{"setup.py": "#!/usr/bin/env python\nimport codecs\nimport os.path\nimport re\n\nfrom setuptools import find_packages, setup\n\nhere = os.path.abspath(os.path.dirname(__file__))\n\n\ndef read(*parts):\n    return codecs.open(os.path.join(here, *parts), 'r').read()\n\n\ndef find_version(*file_paths):\n    version_file = read(*file_paths)\n    version_match = re.search(\n        r\"^__version__ = ['\\\"]([^'\\\"]*)['\\\"]\", version_file, re.M\n    )\n    if version_match:\n        return version_match.group(1)\n    raise RuntimeError(\"Unable to find version string.\")\n\n\nrequires = [\n    'jmespath>=0.7.1,<2.0.0',\n    'python-dateutil>=2.1,<3.0.0',\n    # Prior to Python 3.10, Python doesn't require openssl 1.1.1\n    # but urllib3 2.0+ does. This means all botocore users will be\n    # broken by default on Amazon Linux 2 and AWS Lambda without this pin.\n    'urllib3>=1.25.4,<1.27 ; python_version < \"3.10\"',\n    'urllib3>=1.25.4,!=2.2.0,<3 ; python_version >= \"3.10\"',\n]\n\nextras_require = {\n    'crt': ['awscrt==0.20.11'],\n}\n\nsetup(\n    name='botocore',\n    version=find_version(\"botocore\", \"__init__.py\"),\n    description='Low-level, data-driven core of boto 3.',\n    long_description=open('README.rst').read(),\n    author='Amazon Web Services',\n    url='https://github.com/boto/botocore',\n    scripts=[],\n    packages=find_packages(exclude=['tests*']),\n    package_data={\n        'botocore': ['cacert.pem', 'data/*.json', 'data/*/*.json'],\n        'botocore.vendored.requests': ['*.pem'],\n    },\n    include_package_data=True,\n    install_requires=requires,\n    extras_require=extras_require,\n    license=\"Apache License 2.0\",\n    python_requires=\">= 3.8\",\n    classifiers=[\n        'Development Status :: 5 - Production/Stable',\n        'Intended Audience :: Developers',\n        'Intended Audience :: System Administrators',\n        'Natural Language :: English',\n        'License :: OSI Approved :: Apache Software License',\n        'Programming Language :: Python',\n        'Programming Language :: Python :: 3 :: Only',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.8',\n        'Programming Language :: Python :: 3.9',\n        'Programming Language :: Python :: 3.10',\n        'Programming Language :: Python :: 3.11',\n        'Programming Language :: Python :: 3.12',\n    ],\n)\n", "botocore/response.py": "# Copyright (c) 2012-2013 Mitch Garnaat http://garnaat.org/\n# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport logging\nfrom io import IOBase\n\nfrom urllib3.exceptions import ProtocolError as URLLib3ProtocolError\nfrom urllib3.exceptions import ReadTimeoutError as URLLib3ReadTimeoutError\n\nfrom botocore import parsers\nfrom botocore.compat import set_socket_timeout\nfrom botocore.exceptions import (\n    IncompleteReadError,\n    ReadTimeoutError,\n    ResponseStreamingError,\n)\n\n# Keep these imported.  There's pre-existing code that uses them.\nfrom botocore import ScalarTypes  # noqa\nfrom botocore.compat import XMLParseError  # noqa\nfrom botocore.hooks import first_non_none_response  # noqa\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass StreamingBody(IOBase):\n    \"\"\"Wrapper class for an http response body.\n\n    This provides a few additional conveniences that do not exist\n    in the urllib3 model:\n\n        * Set the timeout on the socket (i.e read() timeouts)\n        * Auto validation of content length, if the amount of bytes\n          we read does not match the content length, an exception\n          is raised.\n\n    \"\"\"\n\n    _DEFAULT_CHUNK_SIZE = 1024\n\n    def __init__(self, raw_stream, content_length):\n        self._raw_stream = raw_stream\n        self._content_length = content_length\n        self._amount_read = 0\n\n    def __del__(self):\n        # Extending destructor in order to preserve the underlying raw_stream.\n        # The ability to add custom cleanup logic introduced in Python3.4+.\n        # https://www.python.org/dev/peps/pep-0442/\n        pass\n\n    def set_socket_timeout(self, timeout):\n        \"\"\"Set the timeout seconds on the socket.\"\"\"\n        # The problem we're trying to solve is to prevent .read() calls from\n        # hanging.  This can happen in rare cases.  What we'd like to ideally\n        # do is set a timeout on the .read() call so that callers can retry\n        # the request.\n        # Unfortunately, this isn't currently possible in requests.\n        # See: https://github.com/kennethreitz/requests/issues/1803\n        # So what we're going to do is reach into the guts of the stream and\n        # grab the socket object, which we can set the timeout on.  We're\n        # putting in a check here so in case this interface goes away, we'll\n        # know.\n        try:\n            set_socket_timeout(self._raw_stream, timeout)\n        except AttributeError:\n            logger.error(\n                \"Cannot access the socket object of \"\n                \"a streaming response.  It's possible \"\n                \"the interface has changed.\",\n                exc_info=True,\n            )\n            raise\n\n    def readable(self):\n        try:\n            return self._raw_stream.readable()\n        except AttributeError:\n            return False\n\n    def read(self, amt=None):\n        \"\"\"Read at most amt bytes from the stream.\n\n        If the amt argument is omitted, read all data.\n        \"\"\"\n        try:\n            chunk = self._raw_stream.read(amt)\n        except URLLib3ReadTimeoutError as e:\n            # TODO: the url will be None as urllib3 isn't setting it yet\n            raise ReadTimeoutError(endpoint_url=e.url, error=e)\n        except URLLib3ProtocolError as e:\n            raise ResponseStreamingError(error=e)\n        self._amount_read += len(chunk)\n        if amt is None or (not chunk and amt > 0):\n            # If the server sends empty contents or\n            # we ask to read all of the contents, then we know\n            # we need to verify the content length.\n            self._verify_content_length()\n        return chunk\n\n    def readlines(self):\n        return self._raw_stream.readlines()\n\n    def __iter__(self):\n        \"\"\"Return an iterator to yield 1k chunks from the raw stream.\"\"\"\n        return self.iter_chunks(self._DEFAULT_CHUNK_SIZE)\n\n    def __next__(self):\n        \"\"\"Return the next 1k chunk from the raw stream.\"\"\"\n        current_chunk = self.read(self._DEFAULT_CHUNK_SIZE)\n        if current_chunk:\n            return current_chunk\n        raise StopIteration()\n\n    def __enter__(self):\n        return self._raw_stream\n\n    def __exit__(self, type, value, traceback):\n        self._raw_stream.close()\n\n    next = __next__\n\n    def iter_lines(self, chunk_size=_DEFAULT_CHUNK_SIZE, keepends=False):\n        \"\"\"Return an iterator to yield lines from the raw stream.\n\n        This is achieved by reading chunk of bytes (of size chunk_size) at a\n        time from the raw stream, and then yielding lines from there.\n        \"\"\"\n        pending = b''\n        for chunk in self.iter_chunks(chunk_size):\n            lines = (pending + chunk).splitlines(True)\n            for line in lines[:-1]:\n                yield line.splitlines(keepends)[0]\n            pending = lines[-1]\n        if pending:\n            yield pending.splitlines(keepends)[0]\n\n    def iter_chunks(self, chunk_size=_DEFAULT_CHUNK_SIZE):\n        \"\"\"Return an iterator to yield chunks of chunk_size bytes from the raw\n        stream.\n        \"\"\"\n        while True:\n            current_chunk = self.read(chunk_size)\n            if current_chunk == b\"\":\n                break\n            yield current_chunk\n\n    def _verify_content_length(self):\n        # See: https://github.com/kennethreitz/requests/issues/1855\n        # Basically, our http library doesn't do this for us, so we have\n        # to do this ourself.\n        if self._content_length is not None and self._amount_read != int(\n            self._content_length\n        ):\n            raise IncompleteReadError(\n                actual_bytes=self._amount_read,\n                expected_bytes=int(self._content_length),\n            )\n\n    def tell(self):\n        return self._raw_stream.tell()\n\n    def close(self):\n        \"\"\"Close the underlying http response stream.\"\"\"\n        self._raw_stream.close()\n\n\ndef get_response(operation_model, http_response):\n    protocol = operation_model.metadata['protocol']\n    response_dict = {\n        'headers': http_response.headers,\n        'status_code': http_response.status_code,\n    }\n    # TODO: Unfortunately, we have to have error logic here.\n    # If it looks like an error, in the streaming response case we\n    # need to actually grab the contents.\n    if response_dict['status_code'] >= 300:\n        response_dict['body'] = http_response.content\n    elif operation_model.has_streaming_output:\n        response_dict['body'] = StreamingBody(\n            http_response.raw, response_dict['headers'].get('content-length')\n        )\n    else:\n        response_dict['body'] = http_response.content\n\n    parser = parsers.create_parser(protocol)\n    return http_response, parser.parse(\n        response_dict, operation_model.output_shape\n    )\n", "botocore/tokens.py": "# Copyright 2022 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport json\nimport logging\nimport os\nimport threading\nfrom datetime import datetime, timedelta\nfrom typing import NamedTuple, Optional\n\nimport dateutil.parser\nfrom dateutil.tz import tzutc\n\nfrom botocore import UNSIGNED\nfrom botocore.compat import total_seconds\nfrom botocore.config import Config\nfrom botocore.exceptions import (\n    ClientError,\n    InvalidConfigError,\n    TokenRetrievalError,\n)\nfrom botocore.utils import CachedProperty, JSONFileCache, SSOTokenLoader\n\nlogger = logging.getLogger(__name__)\n\n\ndef _utc_now():\n    return datetime.now(tzutc())\n\n\ndef create_token_resolver(session):\n    providers = [\n        SSOTokenProvider(session),\n    ]\n    return TokenProviderChain(providers=providers)\n\n\ndef _serialize_utc_timestamp(obj):\n    if isinstance(obj, datetime):\n        return obj.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n    return obj\n\n\ndef _sso_json_dumps(obj):\n    return json.dumps(obj, default=_serialize_utc_timestamp)\n\n\nclass FrozenAuthToken(NamedTuple):\n    token: str\n    expiration: Optional[datetime] = None\n\n\nclass DeferredRefreshableToken:\n    # The time at which we'll attempt to refresh, but not block if someone else\n    # is refreshing.\n    _advisory_refresh_timeout = 15 * 60\n    # The time at which all threads will block waiting for a refreshed token\n    _mandatory_refresh_timeout = 10 * 60\n    # Refresh at most once every minute to avoid blocking every request\n    _attempt_timeout = 60\n\n    def __init__(self, method, refresh_using, time_fetcher=_utc_now):\n        self._time_fetcher = time_fetcher\n        self._refresh_using = refresh_using\n        self.method = method\n\n        # The frozen token is protected by this lock\n        self._refresh_lock = threading.Lock()\n        self._frozen_token = None\n        self._next_refresh = None\n\n    def get_frozen_token(self):\n        self._refresh()\n        return self._frozen_token\n\n    def _refresh(self):\n        # If we don't need to refresh just return\n        refresh_type = self._should_refresh()\n        if not refresh_type:\n            return None\n\n        # Block for refresh if we're in the mandatory refresh window\n        block_for_refresh = refresh_type == \"mandatory\"\n        if self._refresh_lock.acquire(block_for_refresh):\n            try:\n                self._protected_refresh()\n            finally:\n                self._refresh_lock.release()\n\n    def _protected_refresh(self):\n        # This should only be called after acquiring the refresh lock\n        # Another thread may have already refreshed, double check refresh\n        refresh_type = self._should_refresh()\n        if not refresh_type:\n            return None\n\n        try:\n            now = self._time_fetcher()\n            self._next_refresh = now + timedelta(seconds=self._attempt_timeout)\n            self._frozen_token = self._refresh_using()\n        except Exception:\n            logger.warning(\n                \"Refreshing token failed during the %s refresh period.\",\n                refresh_type,\n                exc_info=True,\n            )\n            if refresh_type == \"mandatory\":\n                # This refresh was mandatory, error must be propagated back\n                raise\n\n        if self._is_expired():\n            # Fresh credentials should never be expired\n            raise TokenRetrievalError(\n                provider=self.method,\n                error_msg=\"Token has expired and refresh failed\",\n            )\n\n    def _is_expired(self):\n        if self._frozen_token is None:\n            return False\n\n        expiration = self._frozen_token.expiration\n        remaining = total_seconds(expiration - self._time_fetcher())\n        return remaining <= 0\n\n    def _should_refresh(self):\n        if self._frozen_token is None:\n            # We don't have a token yet, mandatory refresh\n            return \"mandatory\"\n\n        expiration = self._frozen_token.expiration\n        if expiration is None:\n            # No expiration, so assume we don't need to refresh.\n            return None\n\n        now = self._time_fetcher()\n        if now < self._next_refresh:\n            return None\n\n        remaining = total_seconds(expiration - now)\n\n        if remaining < self._mandatory_refresh_timeout:\n            return \"mandatory\"\n        elif remaining < self._advisory_refresh_timeout:\n            return \"advisory\"\n\n        return None\n\n\nclass TokenProviderChain:\n    def __init__(self, providers=None):\n        if providers is None:\n            providers = []\n        self._providers = providers\n\n    def load_token(self):\n        for provider in self._providers:\n            token = provider.load_token()\n            if token is not None:\n                return token\n        return None\n\n\nclass SSOTokenProvider:\n    METHOD = \"sso\"\n    _REFRESH_WINDOW = 15 * 60\n    _SSO_TOKEN_CACHE_DIR = os.path.expanduser(\n        os.path.join(\"~\", \".aws\", \"sso\", \"cache\")\n    )\n    _SSO_CONFIG_VARS = [\n        \"sso_start_url\",\n        \"sso_region\",\n    ]\n    _GRANT_TYPE = \"refresh_token\"\n    DEFAULT_CACHE_CLS = JSONFileCache\n\n    def __init__(\n        self, session, cache=None, time_fetcher=_utc_now, profile_name=None\n    ):\n        self._session = session\n        if cache is None:\n            cache = self.DEFAULT_CACHE_CLS(\n                self._SSO_TOKEN_CACHE_DIR,\n                dumps_func=_sso_json_dumps,\n            )\n        self._now = time_fetcher\n        self._cache = cache\n        self._token_loader = SSOTokenLoader(cache=self._cache)\n        self._profile_name = (\n            profile_name\n            or self._session.get_config_variable(\"profile\")\n            or 'default'\n        )\n\n    def _load_sso_config(self):\n        loaded_config = self._session.full_config\n        profiles = loaded_config.get(\"profiles\", {})\n        sso_sessions = loaded_config.get(\"sso_sessions\", {})\n        profile_config = profiles.get(self._profile_name, {})\n\n        if \"sso_session\" not in profile_config:\n            return\n\n        sso_session_name = profile_config[\"sso_session\"]\n        sso_config = sso_sessions.get(sso_session_name, None)\n\n        if not sso_config:\n            error_msg = (\n                f'The profile \"{self._profile_name}\" is configured to use the SSO '\n                f'token provider but the \"{sso_session_name}\" sso_session '\n                f\"configuration does not exist.\"\n            )\n            raise InvalidConfigError(error_msg=error_msg)\n\n        missing_configs = []\n        for var in self._SSO_CONFIG_VARS:\n            if var not in sso_config:\n                missing_configs.append(var)\n\n        if missing_configs:\n            error_msg = (\n                f'The profile \"{self._profile_name}\" is configured to use the SSO '\n                f\"token provider but is missing the following configuration: \"\n                f\"{missing_configs}.\"\n            )\n            raise InvalidConfigError(error_msg=error_msg)\n\n        return {\n            \"session_name\": sso_session_name,\n            \"sso_region\": sso_config[\"sso_region\"],\n            \"sso_start_url\": sso_config[\"sso_start_url\"],\n        }\n\n    @CachedProperty\n    def _sso_config(self):\n        return self._load_sso_config()\n\n    @CachedProperty\n    def _client(self):\n        config = Config(\n            region_name=self._sso_config[\"sso_region\"],\n            signature_version=UNSIGNED,\n        )\n        return self._session.create_client(\"sso-oidc\", config=config)\n\n    def _attempt_create_token(self, token):\n        response = self._client.create_token(\n            grantType=self._GRANT_TYPE,\n            clientId=token[\"clientId\"],\n            clientSecret=token[\"clientSecret\"],\n            refreshToken=token[\"refreshToken\"],\n        )\n        expires_in = timedelta(seconds=response[\"expiresIn\"])\n        new_token = {\n            \"startUrl\": self._sso_config[\"sso_start_url\"],\n            \"region\": self._sso_config[\"sso_region\"],\n            \"accessToken\": response[\"accessToken\"],\n            \"expiresAt\": self._now() + expires_in,\n            # Cache the registration alongside the token\n            \"clientId\": token[\"clientId\"],\n            \"clientSecret\": token[\"clientSecret\"],\n            \"registrationExpiresAt\": token[\"registrationExpiresAt\"],\n        }\n        if \"refreshToken\" in response:\n            new_token[\"refreshToken\"] = response[\"refreshToken\"]\n        logger.info(\"SSO Token refresh succeeded\")\n        return new_token\n\n    def _refresh_access_token(self, token):\n        keys = (\n            \"refreshToken\",\n            \"clientId\",\n            \"clientSecret\",\n            \"registrationExpiresAt\",\n        )\n        missing_keys = [k for k in keys if k not in token]\n        if missing_keys:\n            msg = f\"Unable to refresh SSO token: missing keys: {missing_keys}\"\n            logger.info(msg)\n            return None\n\n        expiry = dateutil.parser.parse(token[\"registrationExpiresAt\"])\n        if total_seconds(expiry - self._now()) <= 0:\n            logger.info(f\"SSO token registration expired at {expiry}\")\n            return None\n\n        try:\n            return self._attempt_create_token(token)\n        except ClientError:\n            logger.warning(\"SSO token refresh attempt failed\", exc_info=True)\n            return None\n\n    def _refresher(self):\n        start_url = self._sso_config[\"sso_start_url\"]\n        session_name = self._sso_config[\"session_name\"]\n        logger.info(f\"Loading cached SSO token for {session_name}\")\n        token_dict = self._token_loader(start_url, session_name=session_name)\n        expiration = dateutil.parser.parse(token_dict[\"expiresAt\"])\n        logger.debug(f\"Cached SSO token expires at {expiration}\")\n\n        remaining = total_seconds(expiration - self._now())\n        if remaining < self._REFRESH_WINDOW:\n            new_token_dict = self._refresh_access_token(token_dict)\n            if new_token_dict is not None:\n                token_dict = new_token_dict\n                expiration = token_dict[\"expiresAt\"]\n                self._token_loader.save_token(\n                    start_url, token_dict, session_name=session_name\n                )\n\n        return FrozenAuthToken(\n            token_dict[\"accessToken\"], expiration=expiration\n        )\n\n    def load_token(self):\n        if self._sso_config is None:\n            return None\n\n        return DeferredRefreshableToken(\n            self.METHOD, self._refresher, time_fetcher=self._now\n        )\n", "botocore/parsers.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"Response parsers for the various protocol types.\n\nThe module contains classes that can take an HTTP response, and given\nan output shape, parse the response into a dict according to the\nrules in the output shape.\n\nThere are many similarities amongst the different protocols with regard\nto response parsing, and the code is structured in a way to avoid\ncode duplication when possible.  The diagram below is a diagram\nshowing the inheritance hierarchy of the response classes.\n\n::\n\n\n\n                                 +--------------+\n                                 |ResponseParser|\n                                 +--------------+\n                                    ^    ^    ^\n               +--------------------+    |    +-------------------+\n               |                         |                        |\n    +----------+----------+       +------+-------+        +-------+------+\n    |BaseXMLResponseParser|       |BaseRestParser|        |BaseJSONParser|\n    +---------------------+       +--------------+        +--------------+\n              ^         ^          ^           ^           ^        ^\n              |         |          |           |           |        |\n              |         |          |           |           |        |\n              |        ++----------+-+       +-+-----------++       |\n              |        |RestXMLParser|       |RestJSONParser|       |\n        +-----+-----+  +-------------+       +--------------+  +----+-----+\n        |QueryParser|                                          |JSONParser|\n        +-----------+                                          +----------+\n\n\nThe diagram above shows that there is a base class, ``ResponseParser`` that\ncontains logic that is similar amongst all the different protocols (``query``,\n``json``, ``rest-json``, ``rest-xml``).  Amongst the various services there\nis shared logic that can be grouped several ways:\n\n* The ``query`` and ``rest-xml`` both have XML bodies that are parsed in the\n  same way.\n* The ``json`` and ``rest-json`` protocols both have JSON bodies that are\n  parsed in the same way.\n* The ``rest-json`` and ``rest-xml`` protocols have additional attributes\n  besides body parameters that are parsed the same (headers, query string,\n  status code).\n\nThis is reflected in the class diagram above.  The ``BaseXMLResponseParser``\nand the BaseJSONParser contain logic for parsing the XML/JSON body,\nand the BaseRestParser contains logic for parsing out attributes that\ncome from other parts of the HTTP response.  Classes like the\n``RestXMLParser`` inherit from the ``BaseXMLResponseParser`` to get the\nXML body parsing logic and the ``BaseRestParser`` to get the HTTP\nheader/status code/query string parsing.\n\nAdditionally, there are event stream parsers that are used by the other parsers\nto wrap streaming bodies that represent a stream of events. The\nBaseEventStreamParser extends from ResponseParser and defines the logic for\nparsing values from the headers and payload of a message from the underlying\nbinary encoding protocol. Currently, event streams support parsing bodies\nencoded as JSON and XML through the following hierarchy.\n\n\n                                  +--------------+\n                                  |ResponseParser|\n                                  +--------------+\n                                    ^    ^    ^\n               +--------------------+    |    +------------------+\n               |                         |                       |\n    +----------+----------+   +----------+----------+    +-------+------+\n    |BaseXMLResponseParser|   |BaseEventStreamParser|    |BaseJSONParser|\n    +---------------------+   +---------------------+    +--------------+\n                     ^                ^        ^                 ^\n                     |                |        |                 |\n                     |                |        |                 |\n                   +-+----------------+-+    +-+-----------------+-+\n                   |EventStreamXMLParser|    |EventStreamJSONParser|\n                   +--------------------+    +---------------------+\n\nReturn Values\n=============\n\nEach call to ``parse()`` returns a dict has this form::\n\n    Standard Response\n\n    {\n      \"ResponseMetadata\": {\"RequestId\": <requestid>}\n      <response keys>\n    }\n\n    Error response\n\n    {\n      \"ResponseMetadata\": {\"RequestId\": <requestid>}\n      \"Error\": {\n        \"Code\": <string>,\n        \"Message\": <string>,\n        \"Type\": <string>,\n        <additional keys>\n      }\n    }\n\n\"\"\"\nimport base64\nimport http.client\nimport json\nimport logging\nimport re\n\nfrom botocore.compat import ETree, XMLParseError\nfrom botocore.eventstream import EventStream, NoInitialResponseError\nfrom botocore.utils import (\n    is_json_value_header,\n    lowercase_dict,\n    merge_dicts,\n    parse_timestamp,\n)\n\nLOG = logging.getLogger(__name__)\n\nDEFAULT_TIMESTAMP_PARSER = parse_timestamp\n\n\nclass ResponseParserFactory:\n    def __init__(self):\n        self._defaults = {}\n\n    def set_parser_defaults(self, **kwargs):\n        \"\"\"Set default arguments when a parser instance is created.\n\n        You can specify any kwargs that are allowed by a ResponseParser\n        class.  There are currently two arguments:\n\n            * timestamp_parser - A callable that can parse a timestamp string\n            * blob_parser - A callable that can parse a blob type\n\n        \"\"\"\n        self._defaults.update(kwargs)\n\n    def create_parser(self, protocol_name):\n        parser_cls = PROTOCOL_PARSERS[protocol_name]\n        return parser_cls(**self._defaults)\n\n\ndef create_parser(protocol):\n    return ResponseParserFactory().create_parser(protocol)\n\n\ndef _text_content(func):\n    # This decorator hides the difference between\n    # an XML node with text or a plain string.  It's used\n    # to ensure that scalar processing operates only on text\n    # strings, which allows the same scalar handlers to be used\n    # for XML nodes from the body and HTTP headers.\n    def _get_text_content(self, shape, node_or_string):\n        if hasattr(node_or_string, 'text'):\n            text = node_or_string.text\n            if text is None:\n                # If an XML node is empty <foo></foo>,\n                # we want to parse that as an empty string,\n                # not as a null/None value.\n                text = ''\n        else:\n            text = node_or_string\n        return func(self, shape, text)\n\n    return _get_text_content\n\n\nclass ResponseParserError(Exception):\n    pass\n\n\nclass ResponseParser:\n    \"\"\"Base class for response parsing.\n\n    This class represents the interface that all ResponseParsers for the\n    various protocols must implement.\n\n    This class will take an HTTP response and a model shape and parse the\n    HTTP response into a dictionary.\n\n    There is a single public method exposed: ``parse``.  See the ``parse``\n    docstring for more info.\n\n    \"\"\"\n\n    DEFAULT_ENCODING = 'utf-8'\n    EVENT_STREAM_PARSER_CLS = None\n\n    def __init__(self, timestamp_parser=None, blob_parser=None):\n        if timestamp_parser is None:\n            timestamp_parser = DEFAULT_TIMESTAMP_PARSER\n        self._timestamp_parser = timestamp_parser\n        if blob_parser is None:\n            blob_parser = self._default_blob_parser\n        self._blob_parser = blob_parser\n        self._event_stream_parser = None\n        if self.EVENT_STREAM_PARSER_CLS is not None:\n            self._event_stream_parser = self.EVENT_STREAM_PARSER_CLS(\n                timestamp_parser, blob_parser\n            )\n\n    def _default_blob_parser(self, value):\n        # Blobs are always returned as bytes type (this matters on python3).\n        # We don't decode this to a str because it's entirely possible that the\n        # blob contains binary data that actually can't be decoded.\n        return base64.b64decode(value)\n\n    def parse(self, response, shape):\n        \"\"\"Parse the HTTP response given a shape.\n\n        :param response: The HTTP response dictionary.  This is a dictionary\n            that represents the HTTP request.  The dictionary must have the\n            following keys, ``body``, ``headers``, and ``status_code``.\n\n        :param shape: The model shape describing the expected output.\n        :return: Returns a dictionary representing the parsed response\n            described by the model.  In addition to the shape described from\n            the model, each response will also have a ``ResponseMetadata``\n            which contains metadata about the response, which contains at least\n            two keys containing ``RequestId`` and ``HTTPStatusCode``.  Some\n            responses may populate additional keys, but ``RequestId`` will\n            always be present.\n\n        \"\"\"\n        LOG.debug('Response headers: %r', response['headers'])\n        LOG.debug('Response body:\\n%r', response['body'])\n        if response['status_code'] >= 301:\n            if self._is_generic_error_response(response):\n                parsed = self._do_generic_error_parse(response)\n            elif self._is_modeled_error_shape(shape):\n                parsed = self._do_modeled_error_parse(response, shape)\n                # We don't want to decorate the modeled fields with metadata\n                return parsed\n            else:\n                parsed = self._do_error_parse(response, shape)\n        else:\n            parsed = self._do_parse(response, shape)\n\n        # We don't want to decorate event stream responses with metadata\n        if shape and shape.serialization.get('eventstream'):\n            return parsed\n\n        # Add ResponseMetadata if it doesn't exist and inject the HTTP\n        # status code and headers from the response.\n        if isinstance(parsed, dict):\n            response_metadata = parsed.get('ResponseMetadata', {})\n            response_metadata['HTTPStatusCode'] = response['status_code']\n            # Ensure that the http header keys are all lower cased. Older\n            # versions of urllib3 (< 1.11) would unintentionally do this for us\n            # (see urllib3#633). We need to do this conversion manually now.\n            headers = response['headers']\n            response_metadata['HTTPHeaders'] = lowercase_dict(headers)\n            parsed['ResponseMetadata'] = response_metadata\n            self._add_checksum_response_metadata(response, response_metadata)\n        return parsed\n\n    def _add_checksum_response_metadata(self, response, response_metadata):\n        checksum_context = response.get('context', {}).get('checksum', {})\n        algorithm = checksum_context.get('response_algorithm')\n        if algorithm:\n            response_metadata['ChecksumAlgorithm'] = algorithm\n\n    def _is_modeled_error_shape(self, shape):\n        return shape is not None and shape.metadata.get('exception', False)\n\n    def _is_generic_error_response(self, response):\n        # There are times when a service will respond with a generic\n        # error response such as:\n        # '<html><body><b>Http/1.1 Service Unavailable</b></body></html>'\n        #\n        # This can also happen if you're going through a proxy.\n        # In this case the protocol specific _do_error_parse will either\n        # fail to parse the response (in the best case) or silently succeed\n        # and treat the HTML above as an XML response and return\n        # non sensical parsed data.\n        # To prevent this case from happening we first need to check\n        # whether or not this response looks like the generic response.\n        if response['status_code'] >= 500:\n            if 'body' not in response or response['body'] is None:\n                return True\n\n            body = response['body'].strip()\n            return body.startswith(b'<html>') or not body\n\n    def _do_generic_error_parse(self, response):\n        # There's not really much we can do when we get a generic\n        # html response.\n        LOG.debug(\n            \"Received a non protocol specific error response from the \"\n            \"service, unable to populate error code and message.\"\n        )\n        return {\n            'Error': {\n                'Code': str(response['status_code']),\n                'Message': http.client.responses.get(\n                    response['status_code'], ''\n                ),\n            },\n            'ResponseMetadata': {},\n        }\n\n    def _do_parse(self, response, shape):\n        raise NotImplementedError(\"%s._do_parse\" % self.__class__.__name__)\n\n    def _do_error_parse(self, response, shape):\n        raise NotImplementedError(f\"{self.__class__.__name__}._do_error_parse\")\n\n    def _do_modeled_error_parse(self, response, shape, parsed):\n        raise NotImplementedError(\n            f\"{self.__class__.__name__}._do_modeled_error_parse\"\n        )\n\n    def _parse_shape(self, shape, node):\n        handler = getattr(\n            self, f'_handle_{shape.type_name}', self._default_handle\n        )\n        return handler(shape, node)\n\n    def _handle_list(self, shape, node):\n        # Enough implementations share list serialization that it's moved\n        # up here in the base class.\n        parsed = []\n        member_shape = shape.member\n        for item in node:\n            parsed.append(self._parse_shape(member_shape, item))\n        return parsed\n\n    def _default_handle(self, shape, value):\n        return value\n\n    def _create_event_stream(self, response, shape):\n        parser = self._event_stream_parser\n        name = response['context'].get('operation_name')\n        return EventStream(response['body'], shape, parser, name)\n\n    def _get_first_key(self, value):\n        return list(value)[0]\n\n    def _has_unknown_tagged_union_member(self, shape, value):\n        if shape.is_tagged_union:\n            cleaned_value = value.copy()\n            cleaned_value.pop(\"__type\", None)\n            if len(cleaned_value) != 1:\n                error_msg = (\n                    \"Invalid service response: %s must have one and only \"\n                    \"one member set.\"\n                )\n                raise ResponseParserError(error_msg % shape.name)\n            tag = self._get_first_key(cleaned_value)\n            if tag not in shape.members:\n                msg = (\n                    \"Received a tagged union response with member \"\n                    \"unknown to client: %s. Please upgrade SDK for full \"\n                    \"response support.\"\n                )\n                LOG.info(msg % tag)\n                return True\n        return False\n\n    def _handle_unknown_tagged_union_member(self, tag):\n        return {'SDK_UNKNOWN_MEMBER': {'name': tag}}\n\n\nclass BaseXMLResponseParser(ResponseParser):\n    def __init__(self, timestamp_parser=None, blob_parser=None):\n        super().__init__(timestamp_parser, blob_parser)\n        self._namespace_re = re.compile('{.*}')\n\n    def _handle_map(self, shape, node):\n        parsed = {}\n        key_shape = shape.key\n        value_shape = shape.value\n        key_location_name = key_shape.serialization.get('name') or 'key'\n        value_location_name = value_shape.serialization.get('name') or 'value'\n        if shape.serialization.get('flattened') and not isinstance(node, list):\n            node = [node]\n        for keyval_node in node:\n            for single_pair in keyval_node:\n                # Within each <entry> there's a <key> and a <value>\n                tag_name = self._node_tag(single_pair)\n                if tag_name == key_location_name:\n                    key_name = self._parse_shape(key_shape, single_pair)\n                elif tag_name == value_location_name:\n                    val_name = self._parse_shape(value_shape, single_pair)\n                else:\n                    raise ResponseParserError(\"Unknown tag: %s\" % tag_name)\n            parsed[key_name] = val_name\n        return parsed\n\n    def _node_tag(self, node):\n        return self._namespace_re.sub('', node.tag)\n\n    def _handle_list(self, shape, node):\n        # When we use _build_name_to_xml_node, repeated elements are aggregated\n        # into a list.  However, we can't tell the difference between a scalar\n        # value and a single element flattened list.  So before calling the\n        # real _handle_list, we know that \"node\" should actually be a list if\n        # it's flattened, and if it's not, then we make it a one element list.\n        if shape.serialization.get('flattened') and not isinstance(node, list):\n            node = [node]\n        return super()._handle_list(shape, node)\n\n    def _handle_structure(self, shape, node):\n        parsed = {}\n        members = shape.members\n        if shape.metadata.get('exception', False):\n            node = self._get_error_root(node)\n        xml_dict = self._build_name_to_xml_node(node)\n        if self._has_unknown_tagged_union_member(shape, xml_dict):\n            tag = self._get_first_key(xml_dict)\n            return self._handle_unknown_tagged_union_member(tag)\n        for member_name in members:\n            member_shape = members[member_name]\n            if (\n                'location' in member_shape.serialization\n                or member_shape.serialization.get('eventheader')\n            ):\n                # All members with locations have already been handled,\n                # so we don't need to parse these members.\n                continue\n            xml_name = self._member_key_name(member_shape, member_name)\n            member_node = xml_dict.get(xml_name)\n            if member_node is not None:\n                parsed[member_name] = self._parse_shape(\n                    member_shape, member_node\n                )\n            elif member_shape.serialization.get('xmlAttribute'):\n                attribs = {}\n                location_name = member_shape.serialization['name']\n                for key, value in node.attrib.items():\n                    new_key = self._namespace_re.sub(\n                        location_name.split(':')[0] + ':', key\n                    )\n                    attribs[new_key] = value\n                if location_name in attribs:\n                    parsed[member_name] = attribs[location_name]\n        return parsed\n\n    def _get_error_root(self, original_root):\n        if self._node_tag(original_root) == 'ErrorResponse':\n            for child in original_root:\n                if self._node_tag(child) == 'Error':\n                    return child\n        return original_root\n\n    def _member_key_name(self, shape, member_name):\n        # This method is needed because we have to special case flattened list\n        # with a serialization name.  If this is the case we use the\n        # locationName from the list's member shape as the key name for the\n        # surrounding structure.\n        if shape.type_name == 'list' and shape.serialization.get('flattened'):\n            list_member_serialized_name = shape.member.serialization.get(\n                'name'\n            )\n            if list_member_serialized_name is not None:\n                return list_member_serialized_name\n        serialized_name = shape.serialization.get('name')\n        if serialized_name is not None:\n            return serialized_name\n        return member_name\n\n    def _build_name_to_xml_node(self, parent_node):\n        # If the parent node is actually a list. We should not be trying\n        # to serialize it to a dictionary. Instead, return the first element\n        # in the list.\n        if isinstance(parent_node, list):\n            return self._build_name_to_xml_node(parent_node[0])\n        xml_dict = {}\n        for item in parent_node:\n            key = self._node_tag(item)\n            if key in xml_dict:\n                # If the key already exists, the most natural\n                # way to handle this is to aggregate repeated\n                # keys into a single list.\n                # <foo>1</foo><foo>2</foo> -> {'foo': [Node(1), Node(2)]}\n                if isinstance(xml_dict[key], list):\n                    xml_dict[key].append(item)\n                else:\n                    # Convert from a scalar to a list.\n                    xml_dict[key] = [xml_dict[key], item]\n            else:\n                xml_dict[key] = item\n        return xml_dict\n\n    def _parse_xml_string_to_dom(self, xml_string):\n        try:\n            parser = ETree.XMLParser(\n                target=ETree.TreeBuilder(), encoding=self.DEFAULT_ENCODING\n            )\n            parser.feed(xml_string)\n            root = parser.close()\n        except XMLParseError as e:\n            raise ResponseParserError(\n                \"Unable to parse response (%s), \"\n                \"invalid XML received. Further retries may succeed:\\n%s\"\n                % (e, xml_string)\n            )\n        return root\n\n    def _replace_nodes(self, parsed):\n        for key, value in parsed.items():\n            if list(value):\n                sub_dict = self._build_name_to_xml_node(value)\n                parsed[key] = self._replace_nodes(sub_dict)\n            else:\n                parsed[key] = value.text\n        return parsed\n\n    @_text_content\n    def _handle_boolean(self, shape, text):\n        if text == 'true':\n            return True\n        else:\n            return False\n\n    @_text_content\n    def _handle_float(self, shape, text):\n        return float(text)\n\n    @_text_content\n    def _handle_timestamp(self, shape, text):\n        return self._timestamp_parser(text)\n\n    @_text_content\n    def _handle_integer(self, shape, text):\n        return int(text)\n\n    @_text_content\n    def _handle_string(self, shape, text):\n        return text\n\n    @_text_content\n    def _handle_blob(self, shape, text):\n        return self._blob_parser(text)\n\n    _handle_character = _handle_string\n    _handle_double = _handle_float\n    _handle_long = _handle_integer\n\n\nclass QueryParser(BaseXMLResponseParser):\n    def _do_error_parse(self, response, shape):\n        xml_contents = response['body']\n        root = self._parse_xml_string_to_dom(xml_contents)\n        parsed = self._build_name_to_xml_node(root)\n        self._replace_nodes(parsed)\n        # Once we've converted xml->dict, we need to make one or two\n        # more adjustments to extract nested errors and to be consistent\n        # with ResponseMetadata for non-error responses:\n        # 1. {\"Errors\": {\"Error\": {...}}} -> {\"Error\": {...}}\n        # 2. {\"RequestId\": \"id\"} -> {\"ResponseMetadata\": {\"RequestId\": \"id\"}}\n        if 'Errors' in parsed:\n            parsed.update(parsed.pop('Errors'))\n        if 'RequestId' in parsed:\n            parsed['ResponseMetadata'] = {'RequestId': parsed.pop('RequestId')}\n        return parsed\n\n    def _do_modeled_error_parse(self, response, shape):\n        return self._parse_body_as_xml(response, shape, inject_metadata=False)\n\n    def _do_parse(self, response, shape):\n        return self._parse_body_as_xml(response, shape, inject_metadata=True)\n\n    def _parse_body_as_xml(self, response, shape, inject_metadata=True):\n        xml_contents = response['body']\n        root = self._parse_xml_string_to_dom(xml_contents)\n        parsed = {}\n        if shape is not None:\n            start = root\n            if 'resultWrapper' in shape.serialization:\n                start = self._find_result_wrapped_shape(\n                    shape.serialization['resultWrapper'], root\n                )\n            parsed = self._parse_shape(shape, start)\n        if inject_metadata:\n            self._inject_response_metadata(root, parsed)\n        return parsed\n\n    def _find_result_wrapped_shape(self, element_name, xml_root_node):\n        mapping = self._build_name_to_xml_node(xml_root_node)\n        return mapping[element_name]\n\n    def _inject_response_metadata(self, node, inject_into):\n        mapping = self._build_name_to_xml_node(node)\n        child_node = mapping.get('ResponseMetadata')\n        if child_node is not None:\n            sub_mapping = self._build_name_to_xml_node(child_node)\n            for key, value in sub_mapping.items():\n                sub_mapping[key] = value.text\n            inject_into['ResponseMetadata'] = sub_mapping\n\n\nclass EC2QueryParser(QueryParser):\n    def _inject_response_metadata(self, node, inject_into):\n        mapping = self._build_name_to_xml_node(node)\n        child_node = mapping.get('requestId')\n        if child_node is not None:\n            inject_into['ResponseMetadata'] = {'RequestId': child_node.text}\n\n    def _do_error_parse(self, response, shape):\n        # EC2 errors look like:\n        # <Response>\n        #   <Errors>\n        #     <Error>\n        #       <Code>InvalidInstanceID.Malformed</Code>\n        #       <Message>Invalid id: \"1343124\"</Message>\n        #     </Error>\n        #   </Errors>\n        #   <RequestID>12345</RequestID>\n        # </Response>\n        # This is different from QueryParser in that it's RequestID,\n        # not RequestId\n        original = super()._do_error_parse(response, shape)\n        if 'RequestID' in original:\n            original['ResponseMetadata'] = {\n                'RequestId': original.pop('RequestID')\n            }\n        return original\n\n    def _get_error_root(self, original_root):\n        for child in original_root:\n            if self._node_tag(child) == 'Errors':\n                for errors_child in child:\n                    if self._node_tag(errors_child) == 'Error':\n                        return errors_child\n        return original_root\n\n\nclass BaseJSONParser(ResponseParser):\n    def _handle_structure(self, shape, value):\n        final_parsed = {}\n        if shape.is_document_type:\n            final_parsed = value\n        else:\n            member_shapes = shape.members\n            if value is None:\n                # If the comes across the wire as \"null\" (None in python),\n                # we should be returning this unchanged, instead of as an\n                # empty dict.\n                return None\n            final_parsed = {}\n            if self._has_unknown_tagged_union_member(shape, value):\n                tag = self._get_first_key(value)\n                return self._handle_unknown_tagged_union_member(tag)\n            for member_name in member_shapes:\n                member_shape = member_shapes[member_name]\n                json_name = member_shape.serialization.get('name', member_name)\n                raw_value = value.get(json_name)\n                if raw_value is not None:\n                    final_parsed[member_name] = self._parse_shape(\n                        member_shapes[member_name], raw_value\n                    )\n        return final_parsed\n\n    def _handle_map(self, shape, value):\n        parsed = {}\n        key_shape = shape.key\n        value_shape = shape.value\n        for key, value in value.items():\n            actual_key = self._parse_shape(key_shape, key)\n            actual_value = self._parse_shape(value_shape, value)\n            parsed[actual_key] = actual_value\n        return parsed\n\n    def _handle_blob(self, shape, value):\n        return self._blob_parser(value)\n\n    def _handle_timestamp(self, shape, value):\n        return self._timestamp_parser(value)\n\n    def _do_error_parse(self, response, shape):\n        body = self._parse_body_as_json(response['body'])\n        error = {\"Error\": {\"Message\": '', \"Code\": ''}, \"ResponseMetadata\": {}}\n        headers = response['headers']\n        # Error responses can have slightly different structures for json.\n        # The basic structure is:\n        #\n        # {\"__type\":\"ConnectClientException\",\n        #  \"message\":\"The error message.\"}\n\n        # The error message can either come in the 'message' or 'Message' key\n        # so we need to check for both.\n        error['Error']['Message'] = body.get(\n            'message', body.get('Message', '')\n        )\n        # if the message did not contain an error code\n        # include the response status code\n        response_code = response.get('status_code')\n\n        code = body.get('__type', response_code and str(response_code))\n        if code is not None:\n            # code has a couple forms as well:\n            # * \"com.aws.dynamodb.vAPI#ProvisionedThroughputExceededException\"\n            # * \"ResourceNotFoundException\"\n            if '#' in code:\n                code = code.rsplit('#', 1)[1]\n            if 'x-amzn-query-error' in headers:\n                code = self._do_query_compatible_error_parse(\n                    code, headers, error\n                )\n            error['Error']['Code'] = code\n        self._inject_response_metadata(error, response['headers'])\n        return error\n\n    def _do_query_compatible_error_parse(self, code, headers, error):\n        \"\"\"\n        Error response may contain an x-amzn-query-error header to translate\n        errors codes from former `query` services into `json`. We use this to\n        do our lookup in the errorfactory for modeled errors.\n        \"\"\"\n        query_error = headers['x-amzn-query-error']\n        query_error_components = query_error.split(';')\n\n        if len(query_error_components) == 2 and query_error_components[0]:\n            error['Error']['QueryErrorCode'] = code\n            error['Error']['Type'] = query_error_components[1]\n            return query_error_components[0]\n        return code\n\n    def _inject_response_metadata(self, parsed, headers):\n        if 'x-amzn-requestid' in headers:\n            parsed.setdefault('ResponseMetadata', {})['RequestId'] = headers[\n                'x-amzn-requestid'\n            ]\n\n    def _parse_body_as_json(self, body_contents):\n        if not body_contents:\n            return {}\n        body = body_contents.decode(self.DEFAULT_ENCODING)\n        try:\n            original_parsed = json.loads(body)\n            return original_parsed\n        except ValueError:\n            # if the body cannot be parsed, include\n            # the literal string as the message\n            return {'message': body}\n\n\nclass BaseEventStreamParser(ResponseParser):\n    def _do_parse(self, response, shape):\n        final_parsed = {}\n        if shape.serialization.get('eventstream'):\n            event_type = response['headers'].get(':event-type')\n            event_shape = shape.members.get(event_type)\n            if event_shape:\n                final_parsed[event_type] = self._do_parse(\n                    response, event_shape\n                )\n        else:\n            self._parse_non_payload_attrs(\n                response, shape, shape.members, final_parsed\n            )\n            self._parse_payload(response, shape, shape.members, final_parsed)\n        return final_parsed\n\n    def _do_error_parse(self, response, shape):\n        exception_type = response['headers'].get(':exception-type')\n        exception_shape = shape.members.get(exception_type)\n        if exception_shape is not None:\n            original_parsed = self._initial_body_parse(response['body'])\n            body = self._parse_shape(exception_shape, original_parsed)\n            error = {\n                'Error': {\n                    'Code': exception_type,\n                    'Message': body.get('Message', body.get('message', '')),\n                }\n            }\n        else:\n            error = {\n                'Error': {\n                    'Code': response['headers'].get(':error-code', ''),\n                    'Message': response['headers'].get(':error-message', ''),\n                }\n            }\n        return error\n\n    def _parse_payload(self, response, shape, member_shapes, final_parsed):\n        if shape.serialization.get('event'):\n            for name in member_shapes:\n                member_shape = member_shapes[name]\n                if member_shape.serialization.get('eventpayload'):\n                    body = response['body']\n                    if member_shape.type_name == 'blob':\n                        parsed_body = body\n                    elif member_shape.type_name == 'string':\n                        parsed_body = body.decode(self.DEFAULT_ENCODING)\n                    else:\n                        raw_parse = self._initial_body_parse(body)\n                        parsed_body = self._parse_shape(\n                            member_shape, raw_parse\n                        )\n                    final_parsed[name] = parsed_body\n                    return\n            # If we didn't find an explicit payload, use the current shape\n            original_parsed = self._initial_body_parse(response['body'])\n            body_parsed = self._parse_shape(shape, original_parsed)\n            final_parsed.update(body_parsed)\n\n    def _parse_non_payload_attrs(\n        self, response, shape, member_shapes, final_parsed\n    ):\n        headers = response['headers']\n        for name in member_shapes:\n            member_shape = member_shapes[name]\n            if member_shape.serialization.get('eventheader'):\n                if name in headers:\n                    value = headers[name]\n                    if member_shape.type_name == 'timestamp':\n                        # Event stream timestamps are an in milleseconds so we\n                        # divide by 1000 to convert to seconds.\n                        value = self._timestamp_parser(value / 1000.0)\n                    final_parsed[name] = value\n\n    def _initial_body_parse(self, body_contents):\n        # This method should do the initial xml/json parsing of the\n        # body.  We we still need to walk the parsed body in order\n        # to convert types, but this method will do the first round\n        # of parsing.\n        raise NotImplementedError(\"_initial_body_parse\")\n\n\nclass EventStreamJSONParser(BaseEventStreamParser, BaseJSONParser):\n    def _initial_body_parse(self, body_contents):\n        return self._parse_body_as_json(body_contents)\n\n\nclass EventStreamXMLParser(BaseEventStreamParser, BaseXMLResponseParser):\n    def _initial_body_parse(self, xml_string):\n        if not xml_string:\n            return ETree.Element('')\n        return self._parse_xml_string_to_dom(xml_string)\n\n\nclass JSONParser(BaseJSONParser):\n    EVENT_STREAM_PARSER_CLS = EventStreamJSONParser\n\n    \"\"\"Response parser for the \"json\" protocol.\"\"\"\n\n    def _do_parse(self, response, shape):\n        parsed = {}\n        if shape is not None:\n            event_name = shape.event_stream_name\n            if event_name:\n                parsed = self._handle_event_stream(response, shape, event_name)\n            else:\n                parsed = self._handle_json_body(response['body'], shape)\n        self._inject_response_metadata(parsed, response['headers'])\n        return parsed\n\n    def _do_modeled_error_parse(self, response, shape):\n        return self._handle_json_body(response['body'], shape)\n\n    def _handle_event_stream(self, response, shape, event_name):\n        event_stream_shape = shape.members[event_name]\n        event_stream = self._create_event_stream(response, event_stream_shape)\n        try:\n            event = event_stream.get_initial_response()\n        except NoInitialResponseError:\n            error_msg = 'First event was not of type initial-response'\n            raise ResponseParserError(error_msg)\n        parsed = self._handle_json_body(event.payload, shape)\n        parsed[event_name] = event_stream\n        return parsed\n\n    def _handle_json_body(self, raw_body, shape):\n        # The json.loads() gives us the primitive JSON types,\n        # but we need to traverse the parsed JSON data to convert\n        # to richer types (blobs, timestamps, etc.\n        parsed_json = self._parse_body_as_json(raw_body)\n        return self._parse_shape(shape, parsed_json)\n\n\nclass BaseRestParser(ResponseParser):\n    def _do_parse(self, response, shape):\n        final_parsed = {}\n        final_parsed['ResponseMetadata'] = self._populate_response_metadata(\n            response\n        )\n        self._add_modeled_parse(response, shape, final_parsed)\n        return final_parsed\n\n    def _add_modeled_parse(self, response, shape, final_parsed):\n        if shape is None:\n            return final_parsed\n        member_shapes = shape.members\n        self._parse_non_payload_attrs(\n            response, shape, member_shapes, final_parsed\n        )\n        self._parse_payload(response, shape, member_shapes, final_parsed)\n\n    def _do_modeled_error_parse(self, response, shape):\n        final_parsed = {}\n        self._add_modeled_parse(response, shape, final_parsed)\n        return final_parsed\n\n    def _populate_response_metadata(self, response):\n        metadata = {}\n        headers = response['headers']\n        if 'x-amzn-requestid' in headers:\n            metadata['RequestId'] = headers['x-amzn-requestid']\n        elif 'x-amz-request-id' in headers:\n            metadata['RequestId'] = headers['x-amz-request-id']\n            # HostId is what it's called whenever this value is returned\n            # in an XML response body, so to be consistent, we'll always\n            # call is HostId.\n            metadata['HostId'] = headers.get('x-amz-id-2', '')\n        return metadata\n\n    def _parse_payload(self, response, shape, member_shapes, final_parsed):\n        if 'payload' in shape.serialization:\n            # If a payload is specified in the output shape, then only that\n            # shape is used for the body payload.\n            payload_member_name = shape.serialization['payload']\n            body_shape = member_shapes[payload_member_name]\n            if body_shape.serialization.get('eventstream'):\n                body = self._create_event_stream(response, body_shape)\n                final_parsed[payload_member_name] = body\n            elif body_shape.type_name in ['string', 'blob']:\n                # This is a stream\n                body = response['body']\n                if isinstance(body, bytes):\n                    body = body.decode(self.DEFAULT_ENCODING)\n                final_parsed[payload_member_name] = body\n            else:\n                original_parsed = self._initial_body_parse(response['body'])\n                final_parsed[payload_member_name] = self._parse_shape(\n                    body_shape, original_parsed\n                )\n        else:\n            original_parsed = self._initial_body_parse(response['body'])\n            body_parsed = self._parse_shape(shape, original_parsed)\n            final_parsed.update(body_parsed)\n\n    def _parse_non_payload_attrs(\n        self, response, shape, member_shapes, final_parsed\n    ):\n        headers = response['headers']\n        for name in member_shapes:\n            member_shape = member_shapes[name]\n            location = member_shape.serialization.get('location')\n            if location is None:\n                continue\n            elif location == 'statusCode':\n                final_parsed[name] = self._parse_shape(\n                    member_shape, response['status_code']\n                )\n            elif location == 'headers':\n                final_parsed[name] = self._parse_header_map(\n                    member_shape, headers\n                )\n            elif location == 'header':\n                header_name = member_shape.serialization.get('name', name)\n                if header_name in headers:\n                    final_parsed[name] = self._parse_shape(\n                        member_shape, headers[header_name]\n                    )\n\n    def _parse_header_map(self, shape, headers):\n        # Note that headers are case insensitive, so we .lower()\n        # all header names and header prefixes.\n        parsed = {}\n        prefix = shape.serialization.get('name', '').lower()\n        for header_name in headers:\n            if header_name.lower().startswith(prefix):\n                # The key name inserted into the parsed hash\n                # strips off the prefix.\n                name = header_name[len(prefix) :]\n                parsed[name] = headers[header_name]\n        return parsed\n\n    def _initial_body_parse(self, body_contents):\n        # This method should do the initial xml/json parsing of the\n        # body.  We we still need to walk the parsed body in order\n        # to convert types, but this method will do the first round\n        # of parsing.\n        raise NotImplementedError(\"_initial_body_parse\")\n\n    def _handle_string(self, shape, value):\n        parsed = value\n        if is_json_value_header(shape):\n            decoded = base64.b64decode(value).decode(self.DEFAULT_ENCODING)\n            parsed = json.loads(decoded)\n        return parsed\n\n    def _handle_list(self, shape, node):\n        location = shape.serialization.get('location')\n        if location == 'header' and not isinstance(node, list):\n            # List in headers may be a comma separated string as per RFC7230\n            node = [e.strip() for e in node.split(',')]\n        return super()._handle_list(shape, node)\n\n\nclass RestJSONParser(BaseRestParser, BaseJSONParser):\n    EVENT_STREAM_PARSER_CLS = EventStreamJSONParser\n\n    def _initial_body_parse(self, body_contents):\n        return self._parse_body_as_json(body_contents)\n\n    def _do_error_parse(self, response, shape):\n        error = super()._do_error_parse(response, shape)\n        self._inject_error_code(error, response)\n        return error\n\n    def _inject_error_code(self, error, response):\n        # The \"Code\" value can come from either a response\n        # header or a value in the JSON body.\n        body = self._initial_body_parse(response['body'])\n        if 'x-amzn-errortype' in response['headers']:\n            code = response['headers']['x-amzn-errortype']\n            # Could be:\n            # x-amzn-errortype: ValidationException:\n            code = code.split(':')[0]\n            error['Error']['Code'] = code\n        elif 'code' in body or 'Code' in body:\n            error['Error']['Code'] = body.get('code', body.get('Code', ''))\n\n    def _handle_integer(self, shape, value):\n        return int(value)\n\n    _handle_long = _handle_integer\n\n\nclass RestXMLParser(BaseRestParser, BaseXMLResponseParser):\n    EVENT_STREAM_PARSER_CLS = EventStreamXMLParser\n\n    def _initial_body_parse(self, xml_string):\n        if not xml_string:\n            return ETree.Element('')\n        return self._parse_xml_string_to_dom(xml_string)\n\n    def _do_error_parse(self, response, shape):\n        # We're trying to be service agnostic here, but S3 does have a slightly\n        # different response structure for its errors compared to other\n        # rest-xml serivces (route53/cloudfront).  We handle this by just\n        # trying to parse both forms.\n        # First:\n        # <ErrorResponse xmlns=\"...\">\n        #   <Error>\n        #     <Type>Sender</Type>\n        #     <Code>InvalidInput</Code>\n        #     <Message>Invalid resource type: foo</Message>\n        #   </Error>\n        #   <RequestId>request-id</RequestId>\n        # </ErrorResponse>\n        if response['body']:\n            # If the body ends up being invalid xml, the xml parser should not\n            # blow up. It should at least try to pull information about the\n            # the error response from other sources like the HTTP status code.\n            try:\n                return self._parse_error_from_body(response)\n            except ResponseParserError:\n                LOG.debug(\n                    'Exception caught when parsing error response body:',\n                    exc_info=True,\n                )\n        return self._parse_error_from_http_status(response)\n\n    def _parse_error_from_http_status(self, response):\n        return {\n            'Error': {\n                'Code': str(response['status_code']),\n                'Message': http.client.responses.get(\n                    response['status_code'], ''\n                ),\n            },\n            'ResponseMetadata': {\n                'RequestId': response['headers'].get('x-amz-request-id', ''),\n                'HostId': response['headers'].get('x-amz-id-2', ''),\n            },\n        }\n\n    def _parse_error_from_body(self, response):\n        xml_contents = response['body']\n        root = self._parse_xml_string_to_dom(xml_contents)\n        parsed = self._build_name_to_xml_node(root)\n        self._replace_nodes(parsed)\n        if root.tag == 'Error':\n            # This is an S3 error response.  First we'll populate the\n            # response metadata.\n            metadata = self._populate_response_metadata(response)\n            # The RequestId and the HostId are already in the\n            # ResponseMetadata, but are also duplicated in the XML\n            # body.  We don't need these values in both places,\n            # we'll just remove them from the parsed XML body.\n            parsed.pop('RequestId', '')\n            parsed.pop('HostId', '')\n            return {'Error': parsed, 'ResponseMetadata': metadata}\n        elif 'RequestId' in parsed:\n            # Other rest-xml services:\n            parsed['ResponseMetadata'] = {'RequestId': parsed.pop('RequestId')}\n        default = {'Error': {'Message': '', 'Code': ''}}\n        merge_dicts(default, parsed)\n        return default\n\n    @_text_content\n    def _handle_string(self, shape, text):\n        text = super()._handle_string(shape, text)\n        return text\n\n\nPROTOCOL_PARSERS = {\n    'ec2': EC2QueryParser,\n    'query': QueryParser,\n    'json': JSONParser,\n    'rest-json': RestJSONParser,\n    'rest-xml': RestXMLParser,\n}\n", "botocore/configloader.py": "# Copyright (c) 2012-2013 Mitch Garnaat http://garnaat.org/\n# Copyright 2012-2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport configparser\nimport copy\nimport os\nimport shlex\nimport sys\n\nimport botocore.exceptions\n\n\ndef multi_file_load_config(*filenames):\n    \"\"\"Load and combine multiple INI configs with profiles.\n\n    This function will take a list of filesnames and return\n    a single dictionary that represents the merging of the loaded\n    config files.\n\n    If any of the provided filenames does not exist, then that file\n    is ignored.  It is therefore ok to provide a list of filenames,\n    some of which may not exist.\n\n    Configuration files are **not** deep merged, only the top level\n    keys are merged.  The filenames should be passed in order of\n    precedence.  The first config file has precedence over the\n    second config file, which has precedence over the third config file,\n    etc.  The only exception to this is that the \"profiles\" key is\n    merged to combine profiles from multiple config files into a\n    single profiles mapping.  However, if a profile is defined in\n    multiple config files, then the config file with the highest\n    precedence is used.  Profile values themselves are not merged.\n    For example::\n\n        FileA              FileB                FileC\n        [foo]             [foo]                 [bar]\n        a=1               a=2                   a=3\n                          b=2\n\n        [bar]             [baz]                [profile a]\n        a=2               a=3                  region=e\n\n        [profile a]       [profile b]          [profile c]\n        region=c          region=d             region=f\n\n    The final result of ``multi_file_load_config(FileA, FileB, FileC)``\n    would be::\n\n        {\"foo\": {\"a\": 1}, \"bar\": {\"a\": 2}, \"baz\": {\"a\": 3},\n        \"profiles\": {\"a\": {\"region\": \"c\"}}, {\"b\": {\"region\": d\"}},\n                    {\"c\": {\"region\": \"f\"}}}\n\n    Note that the \"foo\" key comes from A, even though it's defined in both\n    FileA and FileB.  Because \"foo\" was defined in FileA first, then the values\n    for \"foo\" from FileA are used and the values for \"foo\" from FileB are\n    ignored.  Also note where the profiles originate from.  Profile \"a\"\n    comes FileA, profile \"b\" comes from FileB, and profile \"c\" comes\n    from FileC.\n\n    \"\"\"\n    configs = []\n    profiles = []\n    for filename in filenames:\n        try:\n            loaded = load_config(filename)\n        except botocore.exceptions.ConfigNotFound:\n            continue\n        profiles.append(loaded.pop('profiles'))\n        configs.append(loaded)\n    merged_config = _merge_list_of_dicts(configs)\n    merged_profiles = _merge_list_of_dicts(profiles)\n    merged_config['profiles'] = merged_profiles\n    return merged_config\n\n\ndef _merge_list_of_dicts(list_of_dicts):\n    merged_dicts = {}\n    for single_dict in list_of_dicts:\n        for key, value in single_dict.items():\n            if key not in merged_dicts:\n                merged_dicts[key] = value\n    return merged_dicts\n\n\ndef load_config(config_filename):\n    \"\"\"Parse a INI config with profiles.\n\n    This will parse an INI config file and map top level profiles\n    into a top level \"profile\" key.\n\n    If you want to parse an INI file and map all section names to\n    top level keys, use ``raw_config_parse`` instead.\n\n    \"\"\"\n    parsed = raw_config_parse(config_filename)\n    return build_profile_map(parsed)\n\n\ndef raw_config_parse(config_filename, parse_subsections=True):\n    \"\"\"Returns the parsed INI config contents.\n\n    Each section name is a top level key.\n\n    :param config_filename: The name of the INI file to parse\n\n    :param parse_subsections: If True, parse indented blocks as\n       subsections that represent their own configuration dictionary.\n       For example, if the config file had the contents::\n\n           s3 =\n              signature_version = s3v4\n              addressing_style = path\n\n        The resulting ``raw_config_parse`` would be::\n\n            {'s3': {'signature_version': 's3v4', 'addressing_style': 'path'}}\n\n       If False, do not try to parse subsections and return the indented\n       block as its literal value::\n\n            {'s3': '\\nsignature_version = s3v4\\naddressing_style = path'}\n\n    :returns: A dict with keys for each profile found in the config\n        file and the value of each key being a dict containing name\n        value pairs found in that profile.\n\n    :raises: ConfigNotFound, ConfigParseError\n    \"\"\"\n    config = {}\n    path = config_filename\n    if path is not None:\n        path = os.path.expandvars(path)\n        path = os.path.expanduser(path)\n        if not os.path.isfile(path):\n            raise botocore.exceptions.ConfigNotFound(path=_unicode_path(path))\n        cp = configparser.RawConfigParser()\n        try:\n            cp.read([path])\n        except (configparser.Error, UnicodeDecodeError) as e:\n            raise botocore.exceptions.ConfigParseError(\n                path=_unicode_path(path), error=e\n            ) from None\n        else:\n            for section in cp.sections():\n                config[section] = {}\n                for option in cp.options(section):\n                    config_value = cp.get(section, option)\n                    if parse_subsections and config_value.startswith('\\n'):\n                        # Then we need to parse the inner contents as\n                        # hierarchical.  We support a single level\n                        # of nesting for now.\n                        try:\n                            config_value = _parse_nested(config_value)\n                        except ValueError as e:\n                            raise botocore.exceptions.ConfigParseError(\n                                path=_unicode_path(path), error=e\n                            ) from None\n                    config[section][option] = config_value\n    return config\n\n\ndef _unicode_path(path):\n    if isinstance(path, str):\n        return path\n    # According to the documentation getfilesystemencoding can return None\n    # on unix in which case the default encoding is used instead.\n    filesystem_encoding = sys.getfilesystemencoding()\n    if filesystem_encoding is None:\n        filesystem_encoding = sys.getdefaultencoding()\n    return path.decode(filesystem_encoding, 'replace')\n\n\ndef _parse_nested(config_value):\n    # Given a value like this:\n    # \\n\n    # foo = bar\n    # bar = baz\n    # We need to parse this into\n    # {'foo': 'bar', 'bar': 'baz}\n    parsed = {}\n    for line in config_value.splitlines():\n        line = line.strip()\n        if not line:\n            continue\n        # The caller will catch ValueError\n        # and raise an appropriate error\n        # if this fails.\n        key, value = line.split('=', 1)\n        parsed[key.strip()] = value.strip()\n    return parsed\n\n\ndef _parse_section(key, values):\n    result = {}\n    try:\n        parts = shlex.split(key)\n    except ValueError:\n        return result\n    if len(parts) == 2:\n        result[parts[1]] = values\n    return result\n\n\ndef build_profile_map(parsed_ini_config):\n    \"\"\"Convert the parsed INI config into a profile map.\n\n    The config file format requires that every profile except the\n    default to be prepended with \"profile\", e.g.::\n\n        [profile test]\n        aws_... = foo\n        aws_... = bar\n\n        [profile bar]\n        aws_... = foo\n        aws_... = bar\n\n        # This is *not* a profile\n        [preview]\n        otherstuff = 1\n\n        # Neither is this\n        [foobar]\n        morestuff = 2\n\n    The build_profile_map will take a parsed INI config file where each top\n    level key represents a section name, and convert into a format where all\n    the profiles are under a single top level \"profiles\" key, and each key in\n    the sub dictionary is a profile name.  For example, the above config file\n    would be converted from::\n\n        {\"profile test\": {\"aws_...\": \"foo\", \"aws...\": \"bar\"},\n         \"profile bar\": {\"aws...\": \"foo\", \"aws...\": \"bar\"},\n         \"preview\": {\"otherstuff\": ...},\n         \"foobar\": {\"morestuff\": ...},\n         }\n\n    into::\n\n        {\"profiles\": {\"test\": {\"aws_...\": \"foo\", \"aws...\": \"bar\"},\n                      \"bar\": {\"aws...\": \"foo\", \"aws...\": \"bar\"},\n         \"preview\": {\"otherstuff\": ...},\n         \"foobar\": {\"morestuff\": ...},\n        }\n\n    If there are no profiles in the provided parsed INI contents, then\n    an empty dict will be the value associated with the ``profiles`` key.\n\n    .. note::\n\n        This will not mutate the passed in parsed_ini_config.  Instead it will\n        make a deepcopy and return that value.\n\n    \"\"\"\n    parsed_config = copy.deepcopy(parsed_ini_config)\n    profiles = {}\n    sso_sessions = {}\n    services = {}\n    final_config = {}\n    for key, values in parsed_config.items():\n        if key.startswith(\"profile\"):\n            profiles.update(_parse_section(key, values))\n        elif key.startswith(\"sso-session\"):\n            sso_sessions.update(_parse_section(key, values))\n        elif key.startswith(\"services\"):\n            services.update(_parse_section(key, values))\n        elif key == 'default':\n            # default section is special and is considered a profile\n            # name but we don't require you use 'profile \"default\"'\n            # as a section.\n            profiles[key] = values\n        else:\n            final_config[key] = values\n    final_config['profiles'] = profiles\n    final_config['sso_sessions'] = sso_sessions\n    final_config['services'] = services\n    return final_config\n", "botocore/discovery.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport logging\nimport time\nimport weakref\n\nfrom botocore import xform_name\nfrom botocore.exceptions import BotoCoreError, ConnectionError, HTTPClientError\nfrom botocore.model import OperationNotFoundError\nfrom botocore.utils import CachedProperty\n\nlogger = logging.getLogger(__name__)\n\n\nclass EndpointDiscoveryException(BotoCoreError):\n    pass\n\n\nclass EndpointDiscoveryRequired(EndpointDiscoveryException):\n    \"\"\"Endpoint Discovery is disabled but is required for this operation.\"\"\"\n\n    fmt = 'Endpoint Discovery is not enabled but this operation requires it.'\n\n\nclass EndpointDiscoveryRefreshFailed(EndpointDiscoveryException):\n    \"\"\"Endpoint Discovery failed to the refresh the known endpoints.\"\"\"\n\n    fmt = 'Endpoint Discovery failed to refresh the required endpoints.'\n\n\ndef block_endpoint_discovery_required_operations(model, **kwargs):\n    endpoint_discovery = model.endpoint_discovery\n    if endpoint_discovery and endpoint_discovery.get('required'):\n        raise EndpointDiscoveryRequired()\n\n\nclass EndpointDiscoveryModel:\n    def __init__(self, service_model):\n        self._service_model = service_model\n\n    @CachedProperty\n    def discovery_operation_name(self):\n        discovery_operation = self._service_model.endpoint_discovery_operation\n        return xform_name(discovery_operation.name)\n\n    @CachedProperty\n    def discovery_operation_keys(self):\n        discovery_operation = self._service_model.endpoint_discovery_operation\n        keys = []\n        if discovery_operation.input_shape:\n            keys = list(discovery_operation.input_shape.members.keys())\n        return keys\n\n    def discovery_required_for(self, operation_name):\n        try:\n            operation_model = self._service_model.operation_model(\n                operation_name\n            )\n            return operation_model.endpoint_discovery.get('required', False)\n        except OperationNotFoundError:\n            return False\n\n    def discovery_operation_kwargs(self, **kwargs):\n        input_keys = self.discovery_operation_keys\n        # Operation and Identifiers are only sent if there are Identifiers\n        if not kwargs.get('Identifiers'):\n            kwargs.pop('Operation', None)\n            kwargs.pop('Identifiers', None)\n        return {k: v for k, v in kwargs.items() if k in input_keys}\n\n    def gather_identifiers(self, operation, params):\n        return self._gather_ids(operation.input_shape, params)\n\n    def _gather_ids(self, shape, params, ids=None):\n        # Traverse the input shape and corresponding parameters, gathering\n        # any input fields labeled as an endpoint discovery id\n        if ids is None:\n            ids = {}\n        for member_name, member_shape in shape.members.items():\n            if member_shape.metadata.get('endpointdiscoveryid'):\n                ids[member_name] = params[member_name]\n            elif (\n                member_shape.type_name == 'structure' and member_name in params\n            ):\n                self._gather_ids(member_shape, params[member_name], ids)\n        return ids\n\n\nclass EndpointDiscoveryManager:\n    def __init__(\n        self, client, cache=None, current_time=None, always_discover=True\n    ):\n        if cache is None:\n            cache = {}\n        self._cache = cache\n        self._failed_attempts = {}\n        if current_time is None:\n            current_time = time.time\n        self._time = current_time\n        self._always_discover = always_discover\n\n        # This needs to be a weak ref in order to prevent memory leaks on\n        # python 2.6\n        self._client = weakref.proxy(client)\n        self._model = EndpointDiscoveryModel(client.meta.service_model)\n\n    def _parse_endpoints(self, response):\n        endpoints = response['Endpoints']\n        current_time = self._time()\n        for endpoint in endpoints:\n            cache_time = endpoint.get('CachePeriodInMinutes')\n            endpoint['Expiration'] = current_time + cache_time * 60\n        return endpoints\n\n    def _cache_item(self, value):\n        if isinstance(value, dict):\n            return tuple(sorted(value.items()))\n        else:\n            return value\n\n    def _create_cache_key(self, **kwargs):\n        kwargs = self._model.discovery_operation_kwargs(**kwargs)\n        return tuple(self._cache_item(v) for k, v in sorted(kwargs.items()))\n\n    def gather_identifiers(self, operation, params):\n        return self._model.gather_identifiers(operation, params)\n\n    def delete_endpoints(self, **kwargs):\n        cache_key = self._create_cache_key(**kwargs)\n        if cache_key in self._cache:\n            del self._cache[cache_key]\n\n    def _describe_endpoints(self, **kwargs):\n        # This is effectively a proxy to whatever name/kwargs the service\n        # supports for endpoint discovery.\n        kwargs = self._model.discovery_operation_kwargs(**kwargs)\n        operation_name = self._model.discovery_operation_name\n        discovery_operation = getattr(self._client, operation_name)\n        logger.debug('Discovering endpoints with kwargs: %s', kwargs)\n        return discovery_operation(**kwargs)\n\n    def _get_current_endpoints(self, key):\n        if key not in self._cache:\n            return None\n        now = self._time()\n        return [e for e in self._cache[key] if now < e['Expiration']]\n\n    def _refresh_current_endpoints(self, **kwargs):\n        cache_key = self._create_cache_key(**kwargs)\n        try:\n            response = self._describe_endpoints(**kwargs)\n            endpoints = self._parse_endpoints(response)\n            self._cache[cache_key] = endpoints\n            self._failed_attempts.pop(cache_key, None)\n            return endpoints\n        except (ConnectionError, HTTPClientError):\n            self._failed_attempts[cache_key] = self._time() + 60\n            return None\n\n    def _recently_failed(self, cache_key):\n        if cache_key in self._failed_attempts:\n            now = self._time()\n            if now < self._failed_attempts[cache_key]:\n                return True\n            del self._failed_attempts[cache_key]\n        return False\n\n    def _select_endpoint(self, endpoints):\n        return endpoints[0]['Address']\n\n    def describe_endpoint(self, **kwargs):\n        operation = kwargs['Operation']\n        discovery_required = self._model.discovery_required_for(operation)\n\n        if not self._always_discover and not discovery_required:\n            # Discovery set to only run on required operations\n            logger.debug(\n                'Optional discovery disabled. Skipping discovery for Operation: %s'\n                % operation\n            )\n            return None\n\n        # Get the endpoint for the provided operation and identifiers\n        cache_key = self._create_cache_key(**kwargs)\n        endpoints = self._get_current_endpoints(cache_key)\n        if endpoints:\n            return self._select_endpoint(endpoints)\n        # All known endpoints are stale\n        recently_failed = self._recently_failed(cache_key)\n        if not recently_failed:\n            # We haven't failed to discover recently, go ahead and refresh\n            endpoints = self._refresh_current_endpoints(**kwargs)\n            if endpoints:\n                return self._select_endpoint(endpoints)\n        # Discovery has failed recently, do our best to get an endpoint\n        logger.debug('Endpoint Discovery has failed for: %s', kwargs)\n        stale_entries = self._cache.get(cache_key, None)\n        if stale_entries:\n            # We have stale entries, use those while discovery is failing\n            return self._select_endpoint(stale_entries)\n        if discovery_required:\n            # It looks strange to be checking recently_failed again but,\n            # this informs us as to whether or not we tried to refresh earlier\n            if recently_failed:\n                # Discovery is required and we haven't already refreshed\n                endpoints = self._refresh_current_endpoints(**kwargs)\n                if endpoints:\n                    return self._select_endpoint(endpoints)\n            # No endpoints even refresh, raise hard error\n            raise EndpointDiscoveryRefreshFailed()\n        # Discovery is optional, just use the default endpoint for now\n        return None\n\n\nclass EndpointDiscoveryHandler:\n    def __init__(self, manager):\n        self._manager = manager\n\n    def register(self, events, service_id):\n        events.register(\n            'before-parameter-build.%s' % service_id, self.gather_identifiers\n        )\n        events.register_first(\n            'request-created.%s' % service_id, self.discover_endpoint\n        )\n        events.register('needs-retry.%s' % service_id, self.handle_retries)\n\n    def gather_identifiers(self, params, model, context, **kwargs):\n        endpoint_discovery = model.endpoint_discovery\n        # Only continue if the operation supports endpoint discovery\n        if endpoint_discovery is None:\n            return\n        ids = self._manager.gather_identifiers(model, params)\n        context['discovery'] = {'identifiers': ids}\n\n    def discover_endpoint(self, request, operation_name, **kwargs):\n        ids = request.context.get('discovery', {}).get('identifiers')\n        if ids is None:\n            return\n        endpoint = self._manager.describe_endpoint(\n            Operation=operation_name, Identifiers=ids\n        )\n        if endpoint is None:\n            logger.debug('Failed to discover and inject endpoint')\n            return\n        if not endpoint.startswith('http'):\n            endpoint = 'https://' + endpoint\n        logger.debug('Injecting discovered endpoint: %s', endpoint)\n        request.url = endpoint\n\n    def handle_retries(self, request_dict, response, operation, **kwargs):\n        if response is None:\n            return None\n\n        _, response = response\n        status = response.get('ResponseMetadata', {}).get('HTTPStatusCode')\n        error_code = response.get('Error', {}).get('Code')\n        if status != 421 and error_code != 'InvalidEndpointException':\n            return None\n\n        context = request_dict.get('context', {})\n        ids = context.get('discovery', {}).get('identifiers')\n        if ids is None:\n            return None\n\n        # Delete the cached endpoints, forcing a refresh on retry\n        # TODO: Improve eviction behavior to only evict the bad endpoint if\n        # there are multiple. This will almost certainly require a lock.\n        self._manager.delete_endpoints(\n            Operation=operation.name, Identifiers=ids\n        )\n        return 0\n", "botocore/model.py": "# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"Abstractions to interact with service models.\"\"\"\nfrom collections import defaultdict\nfrom typing import NamedTuple, Union\n\nfrom botocore.compat import OrderedDict\nfrom botocore.exceptions import (\n    MissingServiceIdError,\n    UndefinedModelAttributeError,\n)\nfrom botocore.utils import CachedProperty, hyphenize_service_id, instance_cache\n\nNOT_SET = object()\n\n\nclass NoShapeFoundError(Exception):\n    pass\n\n\nclass InvalidShapeError(Exception):\n    pass\n\n\nclass OperationNotFoundError(Exception):\n    pass\n\n\nclass InvalidShapeReferenceError(Exception):\n    pass\n\n\nclass ServiceId(str):\n    def hyphenize(self):\n        return hyphenize_service_id(self)\n\n\nclass Shape:\n    \"\"\"Object representing a shape from the service model.\"\"\"\n\n    # To simplify serialization logic, all shape params that are\n    # related to serialization are moved from the top level hash into\n    # a 'serialization' hash.  This list below contains the names of all\n    # the attributes that should be moved.\n    SERIALIZED_ATTRS = [\n        'locationName',\n        'queryName',\n        'flattened',\n        'location',\n        'payload',\n        'streaming',\n        'timestampFormat',\n        'xmlNamespace',\n        'resultWrapper',\n        'xmlAttribute',\n        'eventstream',\n        'event',\n        'eventheader',\n        'eventpayload',\n        'jsonvalue',\n        'timestampFormat',\n        'hostLabel',\n    ]\n    METADATA_ATTRS = [\n        'required',\n        'min',\n        'max',\n        'pattern',\n        'sensitive',\n        'enum',\n        'idempotencyToken',\n        'error',\n        'exception',\n        'endpointdiscoveryid',\n        'retryable',\n        'document',\n        'union',\n        'contextParam',\n        'clientContextParams',\n        'requiresLength',\n    ]\n    MAP_TYPE = OrderedDict\n\n    def __init__(self, shape_name, shape_model, shape_resolver=None):\n        \"\"\"\n\n        :type shape_name: string\n        :param shape_name: The name of the shape.\n\n        :type shape_model: dict\n        :param shape_model: The shape model.  This would be the value\n            associated with the key in the \"shapes\" dict of the\n            service model (i.e ``model['shapes'][shape_name]``)\n\n        :type shape_resolver: botocore.model.ShapeResolver\n        :param shape_resolver: A shape resolver object.  This is used to\n            resolve references to other shapes.  For scalar shape types\n            (string, integer, boolean, etc.), this argument is not\n            required.  If a shape_resolver is not provided for a complex\n            type, then a ``ValueError`` will be raised when an attempt\n            to resolve a shape is made.\n\n        \"\"\"\n        self.name = shape_name\n        self.type_name = shape_model['type']\n        self.documentation = shape_model.get('documentation', '')\n        self._shape_model = shape_model\n        if shape_resolver is None:\n            # If a shape_resolver is not provided, we create an object\n            # that will throw errors if you attempt to resolve\n            # a shape.  This is actually ok for scalar shapes\n            # because they don't need to resolve shapes and shouldn't\n            # be required to provide an object they won't use.\n            shape_resolver = UnresolvableShapeMap()\n        self._shape_resolver = shape_resolver\n        self._cache = {}\n\n    @CachedProperty\n    def serialization(self):\n        \"\"\"Serialization information about the shape.\n\n        This contains information that may be needed for input serialization\n        or response parsing.  This can include:\n\n            * name\n            * queryName\n            * flattened\n            * location\n            * payload\n            * streaming\n            * xmlNamespace\n            * resultWrapper\n            * xmlAttribute\n            * jsonvalue\n            * timestampFormat\n\n        :rtype: dict\n        :return: Serialization information about the shape.\n\n        \"\"\"\n        model = self._shape_model\n        serialization = {}\n        for attr in self.SERIALIZED_ATTRS:\n            if attr in self._shape_model:\n                serialization[attr] = model[attr]\n        # For consistency, locationName is renamed to just 'name'.\n        if 'locationName' in serialization:\n            serialization['name'] = serialization.pop('locationName')\n        return serialization\n\n    @CachedProperty\n    def metadata(self):\n        \"\"\"Metadata about the shape.\n\n        This requires optional information about the shape, including:\n\n            * min\n            * max\n            * pattern\n            * enum\n            * sensitive\n            * required\n            * idempotencyToken\n            * document\n            * union\n            * contextParam\n            * clientContextParams\n            * requiresLength\n\n        :rtype: dict\n        :return: Metadata about the shape.\n\n        \"\"\"\n        model = self._shape_model\n        metadata = {}\n        for attr in self.METADATA_ATTRS:\n            if attr in self._shape_model:\n                metadata[attr] = model[attr]\n        return metadata\n\n    @CachedProperty\n    def required_members(self):\n        \"\"\"A list of members that are required.\n\n        A structure shape can define members that are required.\n        This value will return a list of required members.  If there\n        are no required members an empty list is returned.\n\n        \"\"\"\n        return self.metadata.get('required', [])\n\n    def _resolve_shape_ref(self, shape_ref):\n        return self._shape_resolver.resolve_shape_ref(shape_ref)\n\n    def __repr__(self):\n        return f\"<{self.__class__.__name__}({self.name})>\"\n\n    @property\n    def event_stream_name(self):\n        return None\n\n\nclass StructureShape(Shape):\n    @CachedProperty\n    def members(self):\n        members = self._shape_model.get('members', self.MAP_TYPE())\n        # The members dict looks like:\n        #    'members': {\n        #        'MemberName': {'shape': 'shapeName'},\n        #        'MemberName2': {'shape': 'shapeName'},\n        #    }\n        # We return a dict of member name to Shape object.\n        shape_members = self.MAP_TYPE()\n        for name, shape_ref in members.items():\n            shape_members[name] = self._resolve_shape_ref(shape_ref)\n        return shape_members\n\n    @CachedProperty\n    def event_stream_name(self):\n        for member_name, member in self.members.items():\n            if member.serialization.get('eventstream'):\n                return member_name\n        return None\n\n    @CachedProperty\n    def error_code(self):\n        if not self.metadata.get('exception', False):\n            return None\n        error_metadata = self.metadata.get(\"error\", {})\n        code = error_metadata.get(\"code\")\n        if code:\n            return code\n        # Use the exception name if there is no explicit code modeled\n        return self.name\n\n    @CachedProperty\n    def is_document_type(self):\n        return self.metadata.get('document', False)\n\n    @CachedProperty\n    def is_tagged_union(self):\n        return self.metadata.get('union', False)\n\n\nclass ListShape(Shape):\n    @CachedProperty\n    def member(self):\n        return self._resolve_shape_ref(self._shape_model['member'])\n\n\nclass MapShape(Shape):\n    @CachedProperty\n    def key(self):\n        return self._resolve_shape_ref(self._shape_model['key'])\n\n    @CachedProperty\n    def value(self):\n        return self._resolve_shape_ref(self._shape_model['value'])\n\n\nclass StringShape(Shape):\n    @CachedProperty\n    def enum(self):\n        return self.metadata.get('enum', [])\n\n\nclass StaticContextParameter(NamedTuple):\n    name: str\n    value: Union[bool, str]\n\n\nclass ContextParameter(NamedTuple):\n    name: str\n    member_name: str\n\n\nclass ClientContextParameter(NamedTuple):\n    name: str\n    type: str\n    documentation: str\n\n\nclass ServiceModel:\n    \"\"\"\n\n    :ivar service_description: The parsed service description dictionary.\n\n    \"\"\"\n\n    def __init__(self, service_description, service_name=None):\n        \"\"\"\n\n        :type service_description: dict\n        :param service_description: The service description model.  This value\n            is obtained from a botocore.loader.Loader, or from directly loading\n            the file yourself::\n\n                service_description = json.load(\n                    open('/path/to/service-description-model.json'))\n                model = ServiceModel(service_description)\n\n        :type service_name: str\n        :param service_name: The name of the service.  Normally this is\n            the endpoint prefix defined in the service_description.  However,\n            you can override this value to provide a more convenient name.\n            This is done in a few places in botocore (ses instead of email,\n            emr instead of elasticmapreduce).  If this value is not provided,\n            it will default to the endpointPrefix defined in the model.\n\n        \"\"\"\n        self._service_description = service_description\n        # We want clients to be able to access metadata directly.\n        self.metadata = service_description.get('metadata', {})\n        self._shape_resolver = ShapeResolver(\n            service_description.get('shapes', {})\n        )\n        self._signature_version = NOT_SET\n        self._service_name = service_name\n        self._instance_cache = {}\n\n    def shape_for(self, shape_name, member_traits=None):\n        return self._shape_resolver.get_shape_by_name(\n            shape_name, member_traits\n        )\n\n    def shape_for_error_code(self, error_code):\n        return self._error_code_cache.get(error_code, None)\n\n    @CachedProperty\n    def _error_code_cache(self):\n        error_code_cache = {}\n        for error_shape in self.error_shapes:\n            code = error_shape.error_code\n            error_code_cache[code] = error_shape\n        return error_code_cache\n\n    def resolve_shape_ref(self, shape_ref):\n        return self._shape_resolver.resolve_shape_ref(shape_ref)\n\n    @CachedProperty\n    def shape_names(self):\n        return list(self._service_description.get('shapes', {}))\n\n    @CachedProperty\n    def error_shapes(self):\n        error_shapes = []\n        for shape_name in self.shape_names:\n            error_shape = self.shape_for(shape_name)\n            if error_shape.metadata.get('exception', False):\n                error_shapes.append(error_shape)\n        return error_shapes\n\n    @instance_cache\n    def operation_model(self, operation_name):\n        try:\n            model = self._service_description['operations'][operation_name]\n        except KeyError:\n            raise OperationNotFoundError(operation_name)\n        return OperationModel(model, self, operation_name)\n\n    @CachedProperty\n    def documentation(self):\n        return self._service_description.get('documentation', '')\n\n    @CachedProperty\n    def operation_names(self):\n        return list(self._service_description.get('operations', []))\n\n    @CachedProperty\n    def service_name(self):\n        \"\"\"The name of the service.\n\n        This defaults to the endpointPrefix defined in the service model.\n        However, this value can be overriden when a ``ServiceModel`` is\n        created.  If a service_name was not provided when the ``ServiceModel``\n        was created and if there is no endpointPrefix defined in the\n        service model, then an ``UndefinedModelAttributeError`` exception\n        will be raised.\n\n        \"\"\"\n        if self._service_name is not None:\n            return self._service_name\n        else:\n            return self.endpoint_prefix\n\n    @CachedProperty\n    def service_id(self):\n        try:\n            return ServiceId(self._get_metadata_property('serviceId'))\n        except UndefinedModelAttributeError:\n            raise MissingServiceIdError(service_name=self._service_name)\n\n    @CachedProperty\n    def signing_name(self):\n        \"\"\"The name to use when computing signatures.\n\n        If the model does not define a signing name, this\n        value will be the endpoint prefix defined in the model.\n        \"\"\"\n        signing_name = self.metadata.get('signingName')\n        if signing_name is None:\n            signing_name = self.endpoint_prefix\n        return signing_name\n\n    @CachedProperty\n    def api_version(self):\n        return self._get_metadata_property('apiVersion')\n\n    @CachedProperty\n    def protocol(self):\n        return self._get_metadata_property('protocol')\n\n    @CachedProperty\n    def endpoint_prefix(self):\n        return self._get_metadata_property('endpointPrefix')\n\n    @CachedProperty\n    def endpoint_discovery_operation(self):\n        for operation in self.operation_names:\n            model = self.operation_model(operation)\n            if model.is_endpoint_discovery_operation:\n                return model\n\n    @CachedProperty\n    def endpoint_discovery_required(self):\n        for operation in self.operation_names:\n            model = self.operation_model(operation)\n            if (\n                model.endpoint_discovery is not None\n                and model.endpoint_discovery.get('required')\n            ):\n                return True\n        return False\n\n    @CachedProperty\n    def client_context_parameters(self):\n        params = self._service_description.get('clientContextParams', {})\n        return [\n            ClientContextParameter(\n                name=param_name,\n                type=param_val['type'],\n                documentation=param_val['documentation'],\n            )\n            for param_name, param_val in params.items()\n        ]\n\n    def _get_metadata_property(self, name):\n        try:\n            return self.metadata[name]\n        except KeyError:\n            raise UndefinedModelAttributeError(\n                f'\"{name}\" not defined in the metadata of the model: {self}'\n            )\n\n    # Signature version is one of the rare properties\n    # that can be modified so a CachedProperty is not used here.\n\n    @property\n    def signature_version(self):\n        if self._signature_version is NOT_SET:\n            signature_version = self.metadata.get('signatureVersion')\n            self._signature_version = signature_version\n        return self._signature_version\n\n    @signature_version.setter\n    def signature_version(self, value):\n        self._signature_version = value\n\n    def __repr__(self):\n        return f'{self.__class__.__name__}({self.service_name})'\n\n\nclass OperationModel:\n    def __init__(self, operation_model, service_model, name=None):\n        \"\"\"\n\n        :type operation_model: dict\n        :param operation_model: The operation model.  This comes from the\n            service model, and is the value associated with the operation\n            name in the service model (i.e ``model['operations'][op_name]``).\n\n        :type service_model: botocore.model.ServiceModel\n        :param service_model: The service model associated with the operation.\n\n        :type name: string\n        :param name: The operation name.  This is the operation name exposed to\n            the users of this model.  This can potentially be different from\n            the \"wire_name\", which is the operation name that *must* by\n            provided over the wire.  For example, given::\n\n               \"CreateCloudFrontOriginAccessIdentity\":{\n                 \"name\":\"CreateCloudFrontOriginAccessIdentity2014_11_06\",\n                  ...\n              }\n\n           The ``name`` would be ``CreateCloudFrontOriginAccessIdentity``,\n           but the ``self.wire_name`` would be\n           ``CreateCloudFrontOriginAccessIdentity2014_11_06``, which is the\n           value we must send in the corresponding HTTP request.\n\n        \"\"\"\n        self._operation_model = operation_model\n        self._service_model = service_model\n        self._api_name = name\n        # Clients can access '.name' to get the operation name\n        # and '.metadata' to get the top level metdata of the service.\n        self._wire_name = operation_model.get('name')\n        self.metadata = service_model.metadata\n        self.http = operation_model.get('http', {})\n\n    @CachedProperty\n    def name(self):\n        if self._api_name is not None:\n            return self._api_name\n        else:\n            return self.wire_name\n\n    @property\n    def wire_name(self):\n        \"\"\"The wire name of the operation.\n\n        In many situations this is the same value as the\n        ``name``, value, but in some services, the operation name\n        exposed to the user is different from the operation name\n        we send across the wire (e.g cloudfront).\n\n        Any serialization code should use ``wire_name``.\n\n        \"\"\"\n        return self._operation_model.get('name')\n\n    @property\n    def service_model(self):\n        return self._service_model\n\n    @CachedProperty\n    def documentation(self):\n        return self._operation_model.get('documentation', '')\n\n    @CachedProperty\n    def deprecated(self):\n        return self._operation_model.get('deprecated', False)\n\n    @CachedProperty\n    def endpoint_discovery(self):\n        # Explicit None default. An empty dictionary for this trait means it is\n        # enabled but not required to be used.\n        return self._operation_model.get('endpointdiscovery', None)\n\n    @CachedProperty\n    def is_endpoint_discovery_operation(self):\n        return self._operation_model.get('endpointoperation', False)\n\n    @CachedProperty\n    def input_shape(self):\n        if 'input' not in self._operation_model:\n            # Some operations do not accept any input and do not define an\n            # input shape.\n            return None\n        return self._service_model.resolve_shape_ref(\n            self._operation_model['input']\n        )\n\n    @CachedProperty\n    def output_shape(self):\n        if 'output' not in self._operation_model:\n            # Some operations do not define an output shape,\n            # in which case we return None to indicate the\n            # operation has no expected output.\n            return None\n        return self._service_model.resolve_shape_ref(\n            self._operation_model['output']\n        )\n\n    @CachedProperty\n    def idempotent_members(self):\n        input_shape = self.input_shape\n        if not input_shape:\n            return []\n\n        return [\n            name\n            for (name, shape) in input_shape.members.items()\n            if 'idempotencyToken' in shape.metadata\n            and shape.metadata['idempotencyToken']\n        ]\n\n    @CachedProperty\n    def static_context_parameters(self):\n        params = self._operation_model.get('staticContextParams', {})\n        return [\n            StaticContextParameter(name=name, value=props.get('value'))\n            for name, props in params.items()\n        ]\n\n    @CachedProperty\n    def context_parameters(self):\n        if not self.input_shape:\n            return []\n\n        return [\n            ContextParameter(\n                name=shape.metadata['contextParam']['name'],\n                member_name=name,\n            )\n            for name, shape in self.input_shape.members.items()\n            if 'contextParam' in shape.metadata\n            and 'name' in shape.metadata['contextParam']\n        ]\n\n    @CachedProperty\n    def request_compression(self):\n        return self._operation_model.get('requestcompression')\n\n    @CachedProperty\n    def auth_type(self):\n        return self._operation_model.get('authtype')\n\n    @CachedProperty\n    def error_shapes(self):\n        shapes = self._operation_model.get(\"errors\", [])\n        return list(self._service_model.resolve_shape_ref(s) for s in shapes)\n\n    @CachedProperty\n    def endpoint(self):\n        return self._operation_model.get('endpoint')\n\n    @CachedProperty\n    def http_checksum_required(self):\n        return self._operation_model.get('httpChecksumRequired', False)\n\n    @CachedProperty\n    def http_checksum(self):\n        return self._operation_model.get('httpChecksum', {})\n\n    @CachedProperty\n    def has_event_stream_input(self):\n        return self.get_event_stream_input() is not None\n\n    @CachedProperty\n    def has_event_stream_output(self):\n        return self.get_event_stream_output() is not None\n\n    def get_event_stream_input(self):\n        return self._get_event_stream(self.input_shape)\n\n    def get_event_stream_output(self):\n        return self._get_event_stream(self.output_shape)\n\n    def _get_event_stream(self, shape):\n        \"\"\"Returns the event stream member's shape if any or None otherwise.\"\"\"\n        if shape is None:\n            return None\n        event_name = shape.event_stream_name\n        if event_name:\n            return shape.members[event_name]\n        return None\n\n    @CachedProperty\n    def has_streaming_input(self):\n        return self.get_streaming_input() is not None\n\n    @CachedProperty\n    def has_streaming_output(self):\n        return self.get_streaming_output() is not None\n\n    def get_streaming_input(self):\n        return self._get_streaming_body(self.input_shape)\n\n    def get_streaming_output(self):\n        return self._get_streaming_body(self.output_shape)\n\n    def _get_streaming_body(self, shape):\n        \"\"\"Returns the streaming member's shape if any; or None otherwise.\"\"\"\n        if shape is None:\n            return None\n        payload = shape.serialization.get('payload')\n        if payload is not None:\n            payload_shape = shape.members[payload]\n            if payload_shape.type_name == 'blob':\n                return payload_shape\n        return None\n\n    def __repr__(self):\n        return f'{self.__class__.__name__}(name={self.name})'\n\n\nclass ShapeResolver:\n    \"\"\"Resolves shape references.\"\"\"\n\n    # Any type not in this mapping will default to the Shape class.\n    SHAPE_CLASSES = {\n        'structure': StructureShape,\n        'list': ListShape,\n        'map': MapShape,\n        'string': StringShape,\n    }\n\n    def __init__(self, shape_map):\n        self._shape_map = shape_map\n        self._shape_cache = {}\n\n    def get_shape_by_name(self, shape_name, member_traits=None):\n        try:\n            shape_model = self._shape_map[shape_name]\n        except KeyError:\n            raise NoShapeFoundError(shape_name)\n        try:\n            shape_cls = self.SHAPE_CLASSES.get(shape_model['type'], Shape)\n        except KeyError:\n            raise InvalidShapeError(\n                f\"Shape is missing required key 'type': {shape_model}\"\n            )\n        if member_traits:\n            shape_model = shape_model.copy()\n            shape_model.update(member_traits)\n        result = shape_cls(shape_name, shape_model, self)\n        return result\n\n    def resolve_shape_ref(self, shape_ref):\n        # A shape_ref is a dict that has a 'shape' key that\n        # refers to a shape name as well as any additional\n        # member traits that are then merged over the shape\n        # definition.  For example:\n        # {\"shape\": \"StringType\", \"locationName\": \"Foobar\"}\n        if len(shape_ref) == 1 and 'shape' in shape_ref:\n            # It's just a shape ref with no member traits, we can avoid\n            # a .copy().  This is the common case so it's specifically\n            # called out here.\n            return self.get_shape_by_name(shape_ref['shape'])\n        else:\n            member_traits = shape_ref.copy()\n            try:\n                shape_name = member_traits.pop('shape')\n            except KeyError:\n                raise InvalidShapeReferenceError(\n                    f\"Invalid model, missing shape reference: {shape_ref}\"\n                )\n            return self.get_shape_by_name(shape_name, member_traits)\n\n\nclass UnresolvableShapeMap:\n    \"\"\"A ShapeResolver that will throw ValueErrors when shapes are resolved.\"\"\"\n\n    def get_shape_by_name(self, shape_name, member_traits=None):\n        raise ValueError(\n            f\"Attempted to lookup shape '{shape_name}', but no shape map was provided.\"\n        )\n\n    def resolve_shape_ref(self, shape_ref):\n        raise ValueError(\n            f\"Attempted to resolve shape '{shape_ref}', but no shape \"\n            f\"map was provided.\"\n        )\n\n\nclass DenormalizedStructureBuilder:\n    \"\"\"Build a StructureShape from a denormalized model.\n\n    This is a convenience builder class that makes it easy to construct\n    ``StructureShape``s based on a denormalized model.\n\n    It will handle the details of creating unique shape names and creating\n    the appropriate shape map needed by the ``StructureShape`` class.\n\n    Example usage::\n\n        builder = DenormalizedStructureBuilder()\n        shape = builder.with_members({\n            'A': {\n                'type': 'structure',\n                'members': {\n                    'B': {\n                        'type': 'structure',\n                        'members': {\n                            'C': {\n                                'type': 'string',\n                            }\n                        }\n                    }\n                }\n            }\n        }).build_model()\n        # ``shape`` is now an instance of botocore.model.StructureShape\n\n    :type dict_type: class\n    :param dict_type: The dictionary type to use, allowing you to opt-in\n                      to using OrderedDict or another dict type. This can\n                      be particularly useful for testing when order\n                      matters, such as for documentation.\n\n    \"\"\"\n\n    SCALAR_TYPES = (\n        'string',\n        'integer',\n        'boolean',\n        'blob',\n        'float',\n        'timestamp',\n        'long',\n        'double',\n        'char',\n    )\n\n    def __init__(self, name=None):\n        self.members = OrderedDict()\n        self._name_generator = ShapeNameGenerator()\n        if name is None:\n            self.name = self._name_generator.new_shape_name('structure')\n\n    def with_members(self, members):\n        \"\"\"\n\n        :type members: dict\n        :param members: The denormalized members.\n\n        :return: self\n\n        \"\"\"\n        self._members = members\n        return self\n\n    def build_model(self):\n        \"\"\"Build the model based on the provided members.\n\n        :rtype: botocore.model.StructureShape\n        :return: The built StructureShape object.\n\n        \"\"\"\n        shapes = OrderedDict()\n        denormalized = {\n            'type': 'structure',\n            'members': self._members,\n        }\n        self._build_model(denormalized, shapes, self.name)\n        resolver = ShapeResolver(shape_map=shapes)\n        return StructureShape(\n            shape_name=self.name,\n            shape_model=shapes[self.name],\n            shape_resolver=resolver,\n        )\n\n    def _build_model(self, model, shapes, shape_name):\n        if model['type'] == 'structure':\n            shapes[shape_name] = self._build_structure(model, shapes)\n        elif model['type'] == 'list':\n            shapes[shape_name] = self._build_list(model, shapes)\n        elif model['type'] == 'map':\n            shapes[shape_name] = self._build_map(model, shapes)\n        elif model['type'] in self.SCALAR_TYPES:\n            shapes[shape_name] = self._build_scalar(model)\n        else:\n            raise InvalidShapeError(f\"Unknown shape type: {model['type']}\")\n\n    def _build_structure(self, model, shapes):\n        members = OrderedDict()\n        shape = self._build_initial_shape(model)\n        shape['members'] = members\n\n        for name, member_model in model.get('members', OrderedDict()).items():\n            member_shape_name = self._get_shape_name(member_model)\n            members[name] = {'shape': member_shape_name}\n            self._build_model(member_model, shapes, member_shape_name)\n        return shape\n\n    def _build_list(self, model, shapes):\n        member_shape_name = self._get_shape_name(model)\n        shape = self._build_initial_shape(model)\n        shape['member'] = {'shape': member_shape_name}\n        self._build_model(model['member'], shapes, member_shape_name)\n        return shape\n\n    def _build_map(self, model, shapes):\n        key_shape_name = self._get_shape_name(model['key'])\n        value_shape_name = self._get_shape_name(model['value'])\n        shape = self._build_initial_shape(model)\n        shape['key'] = {'shape': key_shape_name}\n        shape['value'] = {'shape': value_shape_name}\n        self._build_model(model['key'], shapes, key_shape_name)\n        self._build_model(model['value'], shapes, value_shape_name)\n        return shape\n\n    def _build_initial_shape(self, model):\n        shape = {\n            'type': model['type'],\n        }\n        if 'documentation' in model:\n            shape['documentation'] = model['documentation']\n        for attr in Shape.METADATA_ATTRS:\n            if attr in model:\n                shape[attr] = model[attr]\n        return shape\n\n    def _build_scalar(self, model):\n        return self._build_initial_shape(model)\n\n    def _get_shape_name(self, model):\n        if 'shape_name' in model:\n            return model['shape_name']\n        else:\n            return self._name_generator.new_shape_name(model['type'])\n\n\nclass ShapeNameGenerator:\n    \"\"\"Generate unique shape names for a type.\n\n    This class can be used in conjunction with the DenormalizedStructureBuilder\n    to generate unique shape names for a given type.\n\n    \"\"\"\n\n    def __init__(self):\n        self._name_cache = defaultdict(int)\n\n    def new_shape_name(self, type_name):\n        \"\"\"Generate a unique shape name.\n\n        This method will guarantee a unique shape name each time it is\n        called with the same type.\n\n        ::\n\n            >>> s = ShapeNameGenerator()\n            >>> s.new_shape_name('structure')\n            'StructureType1'\n            >>> s.new_shape_name('structure')\n            'StructureType2'\n            >>> s.new_shape_name('list')\n            'ListType1'\n            >>> s.new_shape_name('list')\n            'ListType2'\n\n\n        :type type_name: string\n        :param type_name: The type name (structure, list, map, string, etc.)\n\n        :rtype: string\n        :return: A unique shape name for the given type\n\n        \"\"\"\n        self._name_cache[type_name] += 1\n        current_index = self._name_cache[type_name]\n        return f'{type_name.capitalize()}Type{current_index}'\n", "botocore/awsrequest.py": "# Copyright (c) 2012-2013 Mitch Garnaat http://garnaat.org/\n# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport functools\nimport logging\nfrom collections.abc import Mapping\n\nimport urllib3.util\nfrom urllib3.connection import HTTPConnection, VerifiedHTTPSConnection\nfrom urllib3.connectionpool import HTTPConnectionPool, HTTPSConnectionPool\n\nimport botocore.utils\nfrom botocore.compat import (\n    HTTPHeaders,\n    HTTPResponse,\n    MutableMapping,\n    urlencode,\n    urlparse,\n    urlsplit,\n    urlunsplit,\n)\nfrom botocore.exceptions import UnseekableStreamError\n\nlogger = logging.getLogger(__name__)\n\n\nclass AWSHTTPResponse(HTTPResponse):\n    # The *args, **kwargs is used because the args are slightly\n    # different in py2.6 than in py2.7/py3.\n    def __init__(self, *args, **kwargs):\n        self._status_tuple = kwargs.pop('status_tuple')\n        HTTPResponse.__init__(self, *args, **kwargs)\n\n    def _read_status(self):\n        if self._status_tuple is not None:\n            status_tuple = self._status_tuple\n            self._status_tuple = None\n            return status_tuple\n        else:\n            return HTTPResponse._read_status(self)\n\n\nclass AWSConnection:\n    \"\"\"Mixin for HTTPConnection that supports Expect 100-continue.\n\n    This when mixed with a subclass of httplib.HTTPConnection (though\n    technically we subclass from urllib3, which subclasses\n    httplib.HTTPConnection) and we only override this class to support Expect\n    100-continue, which we need for S3.  As far as I can tell, this is\n    general purpose enough to not be specific to S3, but I'm being\n    tentative and keeping it in botocore because I've only tested\n    this against AWS services.\n\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._original_response_cls = self.response_class\n        # This variable is set when we receive an early response from the\n        # server. If this value is set to True, any calls to send() are noops.\n        # This value is reset to false every time _send_request is called.\n        # This is to workaround changes in urllib3 2.0 which uses separate\n        # send() calls in request() instead of delegating to endheaders(),\n        # which is where the body is sent in CPython's HTTPConnection.\n        self._response_received = False\n        self._expect_header_set = False\n        self._send_called = False\n\n    def close(self):\n        super().close()\n        # Reset all of our instance state we were tracking.\n        self._response_received = False\n        self._expect_header_set = False\n        self._send_called = False\n        self.response_class = self._original_response_cls\n\n    def request(self, method, url, body=None, headers=None, *args, **kwargs):\n        if headers is None:\n            headers = {}\n        self._response_received = False\n        if headers.get('Expect', b'') == b'100-continue':\n            self._expect_header_set = True\n        else:\n            self._expect_header_set = False\n            self.response_class = self._original_response_cls\n        rval = super().request(method, url, body, headers, *args, **kwargs)\n        self._expect_header_set = False\n        return rval\n\n    def _convert_to_bytes(self, mixed_buffer):\n        # Take a list of mixed str/bytes and convert it\n        # all into a single bytestring.\n        # Any str will be encoded as utf-8.\n        bytes_buffer = []\n        for chunk in mixed_buffer:\n            if isinstance(chunk, str):\n                bytes_buffer.append(chunk.encode('utf-8'))\n            else:\n                bytes_buffer.append(chunk)\n        msg = b\"\\r\\n\".join(bytes_buffer)\n        return msg\n\n    def _send_output(self, message_body=None, *args, **kwargs):\n        self._buffer.extend((b\"\", b\"\"))\n        msg = self._convert_to_bytes(self._buffer)\n        del self._buffer[:]\n        # If msg and message_body are sent in a single send() call,\n        # it will avoid performance problems caused by the interaction\n        # between delayed ack and the Nagle algorithm.\n        if isinstance(message_body, bytes):\n            msg += message_body\n            message_body = None\n        self.send(msg)\n        if self._expect_header_set:\n            # This is our custom behavior.  If the Expect header was\n            # set, it will trigger this custom behavior.\n            logger.debug(\"Waiting for 100 Continue response.\")\n            # Wait for 1 second for the server to send a response.\n            if urllib3.util.wait_for_read(self.sock, 1):\n                self._handle_expect_response(message_body)\n                return\n            else:\n                # From the RFC:\n                # Because of the presence of older implementations, the\n                # protocol allows ambiguous situations in which a client may\n                # send \"Expect: 100-continue\" without receiving either a 417\n                # (Expectation Failed) status or a 100 (Continue) status.\n                # Therefore, when a client sends this header field to an origin\n                # server (possibly via a proxy) from which it has never seen a\n                # 100 (Continue) status, the client SHOULD NOT wait for an\n                # indefinite period before sending the request body.\n                logger.debug(\n                    \"No response seen from server, continuing to \"\n                    \"send the response body.\"\n                )\n        if message_body is not None:\n            # message_body was not a string (i.e. it is a file), and\n            # we must run the risk of Nagle.\n            self.send(message_body)\n\n    def _consume_headers(self, fp):\n        # Most servers (including S3) will just return\n        # the CLRF after the 100 continue response.  However,\n        # some servers (I've specifically seen this for squid when\n        # used as a straight HTTP proxy) will also inject a\n        # Connection: keep-alive header.  To account for this\n        # we'll read until we read '\\r\\n', and ignore any headers\n        # that come immediately after the 100 continue response.\n        current = None\n        while current != b'\\r\\n':\n            current = fp.readline()\n\n    def _handle_expect_response(self, message_body):\n        # This is called when we sent the request headers containing\n        # an Expect: 100-continue header and received a response.\n        # We now need to figure out what to do.\n        fp = self.sock.makefile('rb', 0)\n        try:\n            maybe_status_line = fp.readline()\n            parts = maybe_status_line.split(None, 2)\n            if self._is_100_continue_status(maybe_status_line):\n                self._consume_headers(fp)\n                logger.debug(\n                    \"100 Continue response seen, now sending request body.\"\n                )\n                self._send_message_body(message_body)\n            elif len(parts) == 3 and parts[0].startswith(b'HTTP/'):\n                # From the RFC:\n                # Requirements for HTTP/1.1 origin servers:\n                #\n                # - Upon receiving a request which includes an Expect\n                #   request-header field with the \"100-continue\"\n                #   expectation, an origin server MUST either respond with\n                #   100 (Continue) status and continue to read from the\n                #   input stream, or respond with a final status code.\n                #\n                # So if we don't get a 100 Continue response, then\n                # whatever the server has sent back is the final response\n                # and don't send the message_body.\n                logger.debug(\n                    \"Received a non 100 Continue response \"\n                    \"from the server, NOT sending request body.\"\n                )\n                status_tuple = (\n                    parts[0].decode('ascii'),\n                    int(parts[1]),\n                    parts[2].decode('ascii'),\n                )\n                response_class = functools.partial(\n                    AWSHTTPResponse, status_tuple=status_tuple\n                )\n                self.response_class = response_class\n                self._response_received = True\n        finally:\n            fp.close()\n\n    def _send_message_body(self, message_body):\n        if message_body is not None:\n            self.send(message_body)\n\n    def send(self, str):\n        if self._response_received:\n            if not self._send_called:\n                # urllib3 2.0 chunks and calls send potentially\n                # thousands of times inside `request` unlike the\n                # standard library. Only log this once for sanity.\n                logger.debug(\n                    \"send() called, but response already received. \"\n                    \"Not sending data.\"\n                )\n            self._send_called = True\n            return\n        return super().send(str)\n\n    def _is_100_continue_status(self, maybe_status_line):\n        parts = maybe_status_line.split(None, 2)\n        # Check for HTTP/<version> 100 Continue\\r\\n\n        return (\n            len(parts) >= 3\n            and parts[0].startswith(b'HTTP/')\n            and parts[1] == b'100'\n        )\n\n\nclass AWSHTTPConnection(AWSConnection, HTTPConnection):\n    \"\"\"An HTTPConnection that supports 100 Continue behavior.\"\"\"\n\n\nclass AWSHTTPSConnection(AWSConnection, VerifiedHTTPSConnection):\n    \"\"\"An HTTPSConnection that supports 100 Continue behavior.\"\"\"\n\n\nclass AWSHTTPConnectionPool(HTTPConnectionPool):\n    ConnectionCls = AWSHTTPConnection\n\n\nclass AWSHTTPSConnectionPool(HTTPSConnectionPool):\n    ConnectionCls = AWSHTTPSConnection\n\n\ndef prepare_request_dict(\n    request_dict, endpoint_url, context=None, user_agent=None\n):\n    \"\"\"\n    This method prepares a request dict to be created into an\n    AWSRequestObject. This prepares the request dict by adding the\n    url and the user agent to the request dict.\n\n    :type request_dict: dict\n    :param request_dict:  The request dict (created from the\n        ``serialize`` module).\n\n    :type user_agent: string\n    :param user_agent: The user agent to use for this request.\n\n    :type endpoint_url: string\n    :param endpoint_url: The full endpoint url, which contains at least\n        the scheme, the hostname, and optionally any path components.\n    \"\"\"\n    r = request_dict\n    if user_agent is not None:\n        headers = r['headers']\n        headers['User-Agent'] = user_agent\n    host_prefix = r.get('host_prefix')\n    url = _urljoin(endpoint_url, r['url_path'], host_prefix)\n    if r['query_string']:\n        # NOTE: This is to avoid circular import with utils. This is being\n        # done to avoid moving classes to different modules as to not cause\n        # breaking chainges.\n        percent_encode_sequence = botocore.utils.percent_encode_sequence\n        encoded_query_string = percent_encode_sequence(r['query_string'])\n        if '?' not in url:\n            url += '?%s' % encoded_query_string\n        else:\n            url += '&%s' % encoded_query_string\n    r['url'] = url\n    r['context'] = context\n    if context is None:\n        r['context'] = {}\n\n\ndef create_request_object(request_dict):\n    \"\"\"\n    This method takes a request dict and creates an AWSRequest object\n    from it.\n\n    :type request_dict: dict\n    :param request_dict:  The request dict (created from the\n        ``prepare_request_dict`` method).\n\n    :rtype: ``botocore.awsrequest.AWSRequest``\n    :return: An AWSRequest object based on the request_dict.\n\n    \"\"\"\n    r = request_dict\n    request_object = AWSRequest(\n        method=r['method'],\n        url=r['url'],\n        data=r['body'],\n        headers=r['headers'],\n        auth_path=r.get('auth_path'),\n    )\n    request_object.context = r['context']\n    return request_object\n\n\ndef _urljoin(endpoint_url, url_path, host_prefix):\n    p = urlsplit(endpoint_url)\n    # <part>   - <index>\n    # scheme   - p[0]\n    # netloc   - p[1]\n    # path     - p[2]\n    # query    - p[3]\n    # fragment - p[4]\n    if not url_path or url_path == '/':\n        # If there's no path component, ensure the URL ends with\n        # a '/' for backwards compatibility.\n        if not p[2]:\n            new_path = '/'\n        else:\n            new_path = p[2]\n    elif p[2].endswith('/') and url_path.startswith('/'):\n        new_path = p[2][:-1] + url_path\n    else:\n        new_path = p[2] + url_path\n\n    new_netloc = p[1]\n    if host_prefix is not None:\n        new_netloc = host_prefix + new_netloc\n\n    reconstructed = urlunsplit((p[0], new_netloc, new_path, p[3], p[4]))\n    return reconstructed\n\n\nclass AWSRequestPreparer:\n    \"\"\"\n    This class performs preparation on AWSRequest objects similar to that of\n    the PreparedRequest class does in the requests library. However, the logic\n    has been boiled down to meet the specific use cases in botocore. Of note\n    there are the following differences:\n        This class does not heavily prepare the URL. Requests performed many\n        validations and corrections to ensure the URL is properly formatted.\n        Botocore either performs these validations elsewhere or otherwise\n        consistently provides well formatted URLs.\n\n        This class does not heavily prepare the body. Body preperation is\n        simple and supports only the cases that we document: bytes and\n        file-like objects to determine the content-length. This will also\n        additionally prepare a body that is a dict to be url encoded params\n        string as some signers rely on this. Finally, this class does not\n        support multipart file uploads.\n\n        This class does not prepare the method, auth or cookies.\n    \"\"\"\n\n    def prepare(self, original):\n        method = original.method\n        url = self._prepare_url(original)\n        body = self._prepare_body(original)\n        headers = self._prepare_headers(original, body)\n        stream_output = original.stream_output\n\n        return AWSPreparedRequest(method, url, headers, body, stream_output)\n\n    def _prepare_url(self, original):\n        url = original.url\n        if original.params:\n            url_parts = urlparse(url)\n            delim = '&' if url_parts.query else '?'\n            if isinstance(original.params, Mapping):\n                params_to_encode = list(original.params.items())\n            else:\n                params_to_encode = original.params\n            params = urlencode(params_to_encode, doseq=True)\n            url = delim.join((url, params))\n        return url\n\n    def _prepare_headers(self, original, prepared_body=None):\n        headers = HeadersDict(original.headers.items())\n\n        # If the transfer encoding or content length is already set, use that\n        if 'Transfer-Encoding' in headers or 'Content-Length' in headers:\n            return headers\n\n        # Ensure we set the content length when it is expected\n        if original.method not in ('GET', 'HEAD', 'OPTIONS'):\n            length = self._determine_content_length(prepared_body)\n            if length is not None:\n                headers['Content-Length'] = str(length)\n            else:\n                # Failed to determine content length, using chunked\n                # NOTE: This shouldn't ever happen in practice\n                body_type = type(prepared_body)\n                logger.debug('Failed to determine length of %s', body_type)\n                headers['Transfer-Encoding'] = 'chunked'\n\n        return headers\n\n    def _to_utf8(self, item):\n        key, value = item\n        if isinstance(key, str):\n            key = key.encode('utf-8')\n        if isinstance(value, str):\n            value = value.encode('utf-8')\n        return key, value\n\n    def _prepare_body(self, original):\n        \"\"\"Prepares the given HTTP body data.\"\"\"\n        body = original.data\n        if body == b'':\n            body = None\n\n        if isinstance(body, dict):\n            params = [self._to_utf8(item) for item in body.items()]\n            body = urlencode(params, doseq=True)\n\n        return body\n\n    def _determine_content_length(self, body):\n        return botocore.utils.determine_content_length(body)\n\n\nclass AWSRequest:\n    \"\"\"Represents the elements of an HTTP request.\n\n    This class was originally inspired by requests.models.Request, but has been\n    boiled down to meet the specific use cases in botocore. That being said this\n    class (even in requests) is effectively a named-tuple.\n    \"\"\"\n\n    _REQUEST_PREPARER_CLS = AWSRequestPreparer\n\n    def __init__(\n        self,\n        method=None,\n        url=None,\n        headers=None,\n        data=None,\n        params=None,\n        auth_path=None,\n        stream_output=False,\n    ):\n        self._request_preparer = self._REQUEST_PREPARER_CLS()\n\n        # Default empty dicts for dict params.\n        params = {} if params is None else params\n\n        self.method = method\n        self.url = url\n        self.headers = HTTPHeaders()\n        self.data = data\n        self.params = params\n        self.auth_path = auth_path\n        self.stream_output = stream_output\n\n        if headers is not None:\n            for key, value in headers.items():\n                self.headers[key] = value\n\n        # This is a dictionary to hold information that is used when\n        # processing the request. What is inside of ``context`` is open-ended.\n        # For example, it may have a timestamp key that is used for holding\n        # what the timestamp is when signing the request. Note that none\n        # of the information that is inside of ``context`` is directly\n        # sent over the wire; the information is only used to assist in\n        # creating what is sent over the wire.\n        self.context = {}\n\n    def prepare(self):\n        \"\"\"Constructs a :class:`AWSPreparedRequest <AWSPreparedRequest>`.\"\"\"\n        return self._request_preparer.prepare(self)\n\n    @property\n    def body(self):\n        body = self.prepare().body\n        if isinstance(body, str):\n            body = body.encode('utf-8')\n        return body\n\n\nclass AWSPreparedRequest:\n    \"\"\"A data class representing a finalized request to be sent over the wire.\n\n    Requests at this stage should be treated as final, and the properties of\n    the request should not be modified.\n\n    :ivar method: The HTTP Method\n    :ivar url: The full url\n    :ivar headers: The HTTP headers to send.\n    :ivar body: The HTTP body.\n    :ivar stream_output: If the response for this request should be streamed.\n    \"\"\"\n\n    def __init__(self, method, url, headers, body, stream_output):\n        self.method = method\n        self.url = url\n        self.headers = headers\n        self.body = body\n        self.stream_output = stream_output\n\n    def __repr__(self):\n        fmt = (\n            '<AWSPreparedRequest stream_output=%s, method=%s, url=%s, '\n            'headers=%s>'\n        )\n        return fmt % (self.stream_output, self.method, self.url, self.headers)\n\n    def reset_stream(self):\n        \"\"\"Resets the streaming body to it's initial position.\n\n        If the request contains a streaming body (a streamable file-like object)\n        seek to the object's initial position to ensure the entire contents of\n        the object is sent. This is a no-op for static bytes-like body types.\n        \"\"\"\n        # Trying to reset a stream when there is a no stream will\n        # just immediately return.  It's not an error, it will produce\n        # the same result as if we had actually reset the stream (we'll send\n        # the entire body contents again if we need to).\n        # Same case if the body is a string/bytes/bytearray type.\n\n        non_seekable_types = (bytes, str, bytearray)\n        if self.body is None or isinstance(self.body, non_seekable_types):\n            return\n        try:\n            logger.debug(\"Rewinding stream: %s\", self.body)\n            self.body.seek(0)\n        except Exception as e:\n            logger.debug(\"Unable to rewind stream: %s\", e)\n            raise UnseekableStreamError(stream_object=self.body)\n\n\nclass AWSResponse:\n    \"\"\"A data class representing an HTTP response.\n\n    This class was originally inspired by requests.models.Response, but has\n    been boiled down to meet the specific use cases in botocore. This has\n    effectively been reduced to a named tuple.\n\n    :ivar url: The full url.\n    :ivar status_code: The status code of the HTTP response.\n    :ivar headers: The HTTP headers received.\n    :ivar body: The HTTP response body.\n    \"\"\"\n\n    def __init__(self, url, status_code, headers, raw):\n        self.url = url\n        self.status_code = status_code\n        self.headers = HeadersDict(headers)\n        self.raw = raw\n\n        self._content = None\n\n    @property\n    def content(self):\n        \"\"\"Content of the response as bytes.\"\"\"\n\n        if self._content is None:\n            # Read the contents.\n            # NOTE: requests would attempt to call stream and fall back\n            # to a custom generator that would call read in a loop, but\n            # we don't rely on this behavior\n            self._content = b''.join(self.raw.stream()) or b''\n\n        return self._content\n\n    @property\n    def text(self):\n        \"\"\"Content of the response as a proper text type.\n\n        Uses the encoding type provided in the reponse headers to decode the\n        response content into a proper text type. If the encoding is not\n        present in the headers, UTF-8 is used as a default.\n        \"\"\"\n        encoding = botocore.utils.get_encoding_from_headers(self.headers)\n        if encoding:\n            return self.content.decode(encoding)\n        else:\n            return self.content.decode('utf-8')\n\n\nclass _HeaderKey:\n    def __init__(self, key):\n        self._key = key\n        self._lower = key.lower()\n\n    def __hash__(self):\n        return hash(self._lower)\n\n    def __eq__(self, other):\n        return isinstance(other, _HeaderKey) and self._lower == other._lower\n\n    def __str__(self):\n        return self._key\n\n    def __repr__(self):\n        return repr(self._key)\n\n\nclass HeadersDict(MutableMapping):\n    \"\"\"A case-insenseitive dictionary to represent HTTP headers.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        self._dict = {}\n        self.update(*args, **kwargs)\n\n    def __setitem__(self, key, value):\n        self._dict[_HeaderKey(key)] = value\n\n    def __getitem__(self, key):\n        return self._dict[_HeaderKey(key)]\n\n    def __delitem__(self, key):\n        del self._dict[_HeaderKey(key)]\n\n    def __iter__(self):\n        return (str(key) for key in self._dict)\n\n    def __len__(self):\n        return len(self._dict)\n\n    def __repr__(self):\n        return repr(self._dict)\n\n    def copy(self):\n        return HeadersDict(self.items())\n", "botocore/translate.py": "# Copyright (c) 2012-2013 Mitch Garnaat http://garnaat.org/\n# Copyright 2012-2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport copy\n\nfrom botocore.utils import merge_dicts\n\n\ndef build_retry_config(\n    endpoint_prefix, retry_model, definitions, client_retry_config=None\n):\n    service_config = retry_model.get(endpoint_prefix, {})\n    resolve_references(service_config, definitions)\n    # We want to merge the global defaults with the service specific\n    # defaults, with the service specific defaults taking precedence.\n    # So we use the global defaults as the base.\n    #\n    # A deepcopy is done on the retry defaults because it ensures the\n    # retry model has no chance of getting mutated when the service specific\n    # configuration or client retry config is merged in.\n    final_retry_config = {\n        '__default__': copy.deepcopy(retry_model.get('__default__', {}))\n    }\n    resolve_references(final_retry_config, definitions)\n    # The merge the service specific config on top.\n    merge_dicts(final_retry_config, service_config)\n    if client_retry_config is not None:\n        _merge_client_retry_config(final_retry_config, client_retry_config)\n    return final_retry_config\n\n\ndef _merge_client_retry_config(retry_config, client_retry_config):\n    max_retry_attempts_override = client_retry_config.get('max_attempts')\n    if max_retry_attempts_override is not None:\n        # In the retry config, the max_attempts refers to the maximum number\n        # of requests in general will be made. However, for the client's\n        # retry config it refers to how many retry attempts will be made at\n        # most. So to translate this number from the client config, one is\n        # added to convert it to the maximum number request that will be made\n        # by including the initial request.\n        #\n        # It is also important to note that if we ever support per operation\n        # configuration in the retry model via the client, we will need to\n        # revisit this logic to make sure max_attempts gets applied\n        # per operation.\n        retry_config['__default__']['max_attempts'] = (\n            max_retry_attempts_override + 1\n        )\n\n\ndef resolve_references(config, definitions):\n    \"\"\"Recursively replace $ref keys.\n\n    To cut down on duplication, common definitions can be declared\n    (and passed in via the ``definitions`` attribute) and then\n    references as {\"$ref\": \"name\"}, when this happens the reference\n    dict is placed with the value from the ``definition`` dict.\n\n    This is recursively done.\n\n    \"\"\"\n    for key, value in config.items():\n        if isinstance(value, dict):\n            if len(value) == 1 and list(value.keys())[0] == '$ref':\n                # Then we need to resolve this reference.\n                config[key] = definitions[list(value.values())[0]]\n            else:\n                resolve_references(value, definitions)\n", "botocore/config.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport copy\n\nfrom botocore.compat import OrderedDict\nfrom botocore.endpoint import DEFAULT_TIMEOUT, MAX_POOL_CONNECTIONS\nfrom botocore.exceptions import (\n    InvalidMaxRetryAttemptsError,\n    InvalidRetryConfigurationError,\n    InvalidRetryModeError,\n    InvalidS3AddressingStyleError,\n)\n\n\nclass Config:\n    \"\"\"Advanced configuration for Botocore clients.\n\n    :type region_name: str\n    :param region_name: The region to use in instantiating the client\n\n    :type signature_version: str\n    :param signature_version: The signature version when signing requests.\n\n    :type user_agent: str\n    :param user_agent: The value to use in the User-Agent header.\n\n    :type user_agent_extra: str\n    :param user_agent_extra: The value to append to the current User-Agent\n        header value.\n\n    :type user_agent_appid: str\n    :param user_agent_appid: A value that gets included in the User-Agent\n        string in the format \"app/<user_agent_appid>\". Allowed characters are\n        ASCII alphanumerics and ``!$%&'*+-.^_`|~``. All other characters will\n        be replaced by a ``-``.\n\n    :type connect_timeout: float or int\n    :param connect_timeout: The time in seconds till a timeout exception is\n        thrown when attempting to make a connection. The default is 60\n        seconds.\n\n    :type read_timeout: float or int\n    :param read_timeout: The time in seconds till a timeout exception is\n        thrown when attempting to read from a connection. The default is\n        60 seconds.\n\n    :type parameter_validation: bool\n    :param parameter_validation: Whether parameter validation should occur\n        when serializing requests. The default is True.  You can disable\n        parameter validation for performance reasons.  Otherwise, it's\n        recommended to leave parameter validation enabled.\n\n    :type max_pool_connections: int\n    :param max_pool_connections: The maximum number of connections to\n        keep in a connection pool.  If this value is not set, the default\n        value of 10 is used.\n\n    :type proxies: dict\n    :param proxies: A dictionary of proxy servers to use by protocol or\n        endpoint, e.g.:\n        ``{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}``.\n        The proxies are used on each request.\n\n    :type proxies_config: dict\n    :param proxies_config: A dictionary of additional proxy configurations.\n        Valid keys are:\n\n        * ``proxy_ca_bundle`` -- The path to a custom certificate bundle to use\n          when establishing SSL/TLS connections with proxy.\n\n        * ``proxy_client_cert`` -- The path to a certificate for proxy\n          TLS client authentication.\n\n          When a string is provided it is treated as a path to a proxy client\n          certificate. When a two element tuple is provided, it will be\n          interpreted as the path to the client certificate, and the path\n          to the certificate key.\n\n        * ``proxy_use_forwarding_for_https`` -- For HTTPS proxies,\n          forward your requests to HTTPS destinations with an absolute\n          URI. We strongly recommend you only use this option with\n          trusted or corporate proxies. Value must be boolean.\n\n    :type s3: dict\n    :param s3: A dictionary of S3 specific configurations.\n        Valid keys are:\n\n        * ``use_accelerate_endpoint`` -- Refers to whether to use the S3\n          Accelerate endpoint. The value must be a boolean. If True, the\n          client will use the S3 Accelerate endpoint. If the S3 Accelerate\n          endpoint is being used then the addressing style will always\n          be virtual.\n\n        * ``payload_signing_enabled`` -- Refers to whether or not to SHA256\n          sign sigv4 payloads. By default, this is disabled for streaming\n          uploads (UploadPart and PutObject).\n\n        * ``addressing_style`` -- Refers to the style in which to address\n          s3 endpoints. Values must be a string that equals one of:\n\n          * ``auto`` -- Addressing style is chosen for user. Depending\n            on the configuration of client, the endpoint may be addressed in\n            the virtual or the path style. Note that this is the default\n            behavior if no style is specified.\n\n          * ``virtual`` -- Addressing style is always virtual. The name of the\n            bucket must be DNS compatible or an exception will be thrown.\n            Endpoints will be addressed as such: ``mybucket.s3.amazonaws.com``\n\n          * ``path`` -- Addressing style is always by path. Endpoints will be\n            addressed as such: ``s3.amazonaws.com/mybucket``\n\n        * ``us_east_1_regional_endpoint`` -- Refers to what S3 endpoint to use\n          when the region is configured to be us-east-1. Values must be a\n          string that equals:\n\n          * ``regional`` -- Use the us-east-1.amazonaws.com endpoint if the\n            client is configured to use the us-east-1 region.\n\n          * ``legacy`` -- Use the s3.amazonaws.com endpoint if the client is\n            configured to use the us-east-1 region. This is the default if\n            the configuration option is not specified.\n\n\n    :type retries: dict\n    :param retries: A dictionary for configuration related to retry behavior.\n        Valid keys are:\n\n        * ``total_max_attempts`` -- An integer representing the maximum number of\n          total attempts that will be made on a single request.  This includes\n          the initial request, so a value of 1 indicates that no requests\n          will be retried.  If ``total_max_attempts`` and ``max_attempts``\n          are both provided, ``total_max_attempts`` takes precedence.\n          ``total_max_attempts`` is preferred over ``max_attempts`` because\n          it maps to the ``AWS_MAX_ATTEMPTS`` environment variable and\n          the ``max_attempts`` config file value.\n        * ``max_attempts`` -- An integer representing the maximum number of\n          retry attempts that will be made on a single request. For\n          example, setting this value to 2 will result in the request\n          being retried at most two times after the initial request. Setting\n          this value to 0 will result in no retries ever being attempted after\n          the initial request. If not provided, the number of retries will\n          default to the value specified in the service model, which is\n          typically four retries.\n        * ``mode`` -- A string representing the type of retry mode botocore\n          should use.  Valid values are:\n\n          * ``legacy`` - The pre-existing retry behavior.\n\n          * ``standard`` - The standardized set of retry rules. This will also\n            default to 3 max attempts unless overridden.\n\n          * ``adaptive`` - Retries with additional client side throttling.\n\n    :type client_cert: str, (str, str)\n    :param client_cert: The path to a certificate for TLS client authentication.\n\n        When a string is provided it is treated as a path to a client\n        certificate to be used when creating a TLS connection.\n\n        If a client key is to be provided alongside the client certificate the\n        client_cert should be set to a tuple of length two where the first\n        element is the path to the client certificate and the second element is\n        the path to the certificate key.\n\n    :type inject_host_prefix: bool\n    :param inject_host_prefix: Whether host prefix injection should occur.\n\n        Defaults to True.\n\n        Setting this to False disables the injection of operation parameters\n        into the prefix of the hostname. This is useful for clients providing\n        custom endpoints that should not have their host prefix modified.\n\n    :type use_dualstack_endpoint: bool\n    :param use_dualstack_endpoint: Setting to True enables dualstack\n        endpoint resolution.\n\n        Defaults to None.\n\n    :type use_fips_endpoint: bool\n    :param use_fips_endpoint: Setting to True enables fips\n        endpoint resolution.\n\n        Defaults to None.\n\n    :type ignore_configured_endpoint_urls: bool\n    :param ignore_configured_endpoint_urls: Setting to True disables use\n        of endpoint URLs provided via environment variables and\n        the shared configuration file.\n\n        Defaults to None.\n\n    :type tcp_keepalive: bool\n    :param tcp_keepalive: Enables the TCP Keep-Alive socket option used when\n        creating new connections if set to True.\n\n        Defaults to False.\n\n    :type request_min_compression_size_bytes: int\n    :param request_min_compression_size_bytes: The minimum size in bytes that a\n        request body should be to trigger compression. All requests with\n        streaming input that don't contain the ``requiresLength`` trait will be\n        compressed regardless of this setting.\n\n        Defaults to None.\n\n    :type disable_request_compression: bool\n    :param disable_request_compression: Disables request body compression if\n        set to True.\n\n        Defaults to None.\n\n    :type client_context_params: dict\n    :param client_context_params: A dictionary of parameters specific to\n        individual services. If available, valid parameters can be found in\n        the ``Client Context Parameters`` section of the service client's\n        documentation. Invalid parameters or ones that are not used by the\n        specified service will be ignored.\n\n        Defaults to None.\n    \"\"\"\n\n    OPTION_DEFAULTS = OrderedDict(\n        [\n            ('region_name', None),\n            ('signature_version', None),\n            ('user_agent', None),\n            ('user_agent_extra', None),\n            ('user_agent_appid', None),\n            ('connect_timeout', DEFAULT_TIMEOUT),\n            ('read_timeout', DEFAULT_TIMEOUT),\n            ('parameter_validation', True),\n            ('max_pool_connections', MAX_POOL_CONNECTIONS),\n            ('proxies', None),\n            ('proxies_config', None),\n            ('s3', None),\n            ('retries', None),\n            ('client_cert', None),\n            ('inject_host_prefix', True),\n            ('endpoint_discovery_enabled', None),\n            ('use_dualstack_endpoint', None),\n            ('use_fips_endpoint', None),\n            ('ignore_configured_endpoint_urls', None),\n            ('defaults_mode', None),\n            ('tcp_keepalive', None),\n            ('request_min_compression_size_bytes', None),\n            ('disable_request_compression', None),\n            ('client_context_params', None),\n        ]\n    )\n\n    NON_LEGACY_OPTION_DEFAULTS = {\n        'connect_timeout': None,\n    }\n\n    def __init__(self, *args, **kwargs):\n        self._user_provided_options = self._record_user_provided_options(\n            args, kwargs\n        )\n\n        # Merge the user_provided options onto the default options\n        config_vars = copy.copy(self.OPTION_DEFAULTS)\n        defaults_mode = self._user_provided_options.get(\n            'defaults_mode', 'legacy'\n        )\n        if defaults_mode != 'legacy':\n            config_vars.update(self.NON_LEGACY_OPTION_DEFAULTS)\n        config_vars.update(self._user_provided_options)\n\n        # Set the attributes based on the config_vars\n        for key, value in config_vars.items():\n            setattr(self, key, value)\n\n        # Validate the s3 options\n        self._validate_s3_configuration(self.s3)\n\n        self._validate_retry_configuration(self.retries)\n\n    def _record_user_provided_options(self, args, kwargs):\n        option_order = list(self.OPTION_DEFAULTS)\n        user_provided_options = {}\n\n        # Iterate through the kwargs passed through to the constructor and\n        # map valid keys to the dictionary\n        for key, value in kwargs.items():\n            if key in self.OPTION_DEFAULTS:\n                user_provided_options[key] = value\n            # The key must exist in the available options\n            else:\n                raise TypeError(f\"Got unexpected keyword argument '{key}'\")\n\n        # The number of args should not be longer than the allowed\n        # options\n        if len(args) > len(option_order):\n            raise TypeError(\n                f\"Takes at most {len(option_order)} arguments ({len(args)} given)\"\n            )\n\n        # Iterate through the args passed through to the constructor and map\n        # them to appropriate keys.\n        for i, arg in enumerate(args):\n            # If a kwarg was specified for the arg, then error out\n            if option_order[i] in user_provided_options:\n                raise TypeError(\n                    f\"Got multiple values for keyword argument '{option_order[i]}'\"\n                )\n            user_provided_options[option_order[i]] = arg\n\n        return user_provided_options\n\n    def _validate_s3_configuration(self, s3):\n        if s3 is not None:\n            addressing_style = s3.get('addressing_style')\n            if addressing_style not in ['virtual', 'auto', 'path', None]:\n                raise InvalidS3AddressingStyleError(\n                    s3_addressing_style=addressing_style\n                )\n\n    def _validate_retry_configuration(self, retries):\n        valid_options = ('max_attempts', 'mode', 'total_max_attempts')\n        valid_modes = ('legacy', 'standard', 'adaptive')\n        if retries is not None:\n            for key, value in retries.items():\n                if key not in valid_options:\n                    raise InvalidRetryConfigurationError(\n                        retry_config_option=key,\n                        valid_options=valid_options,\n                    )\n                if key == 'max_attempts' and value < 0:\n                    raise InvalidMaxRetryAttemptsError(\n                        provided_max_attempts=value,\n                        min_value=0,\n                    )\n                if key == 'total_max_attempts' and value < 1:\n                    raise InvalidMaxRetryAttemptsError(\n                        provided_max_attempts=value,\n                        min_value=1,\n                    )\n                if key == 'mode' and value not in valid_modes:\n                    raise InvalidRetryModeError(\n                        provided_retry_mode=value,\n                        valid_modes=valid_modes,\n                    )\n\n    def merge(self, other_config):\n        \"\"\"Merges the config object with another config object\n\n        This will merge in all non-default values from the provided config\n        and return a new config object\n\n        :type other_config: botocore.config.Config\n        :param other config: Another config object to merge with. The values\n            in the provided config object will take precedence in the merging\n\n        :returns: A config object built from the merged values of both\n            config objects.\n        \"\"\"\n        # Make a copy of the current attributes in the config object.\n        config_options = copy.copy(self._user_provided_options)\n\n        # Merge in the user provided options from the other config\n        config_options.update(other_config._user_provided_options)\n\n        # Return a new config object with the merged properties.\n        return Config(**config_options)\n", "botocore/compress.py": "# Copyright 2023 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"\nNOTE: All functions in this module are considered private and are\nsubject to abrupt breaking changes. Please do not use them directly.\n\n\"\"\"\n\nimport io\nimport logging\nfrom gzip import GzipFile\nfrom gzip import compress as gzip_compress\n\nfrom botocore.compat import urlencode\nfrom botocore.utils import determine_content_length\n\nlogger = logging.getLogger(__name__)\n\n\ndef maybe_compress_request(config, request_dict, operation_model):\n    \"\"\"Attempt to compress the request body using the modeled encodings.\"\"\"\n    if _should_compress_request(config, request_dict, operation_model):\n        for encoding in operation_model.request_compression['encodings']:\n            encoder = COMPRESSION_MAPPING.get(encoding)\n            if encoder is not None:\n                logger.debug('Compressing request with %s encoding.', encoding)\n                request_dict['body'] = encoder(request_dict['body'])\n                _set_compression_header(request_dict['headers'], encoding)\n                return\n            else:\n                logger.debug('Unsupported compression encoding: %s', encoding)\n\n\ndef _should_compress_request(config, request_dict, operation_model):\n    if (\n        config.disable_request_compression is not True\n        and config.signature_version != 'v2'\n        and operation_model.request_compression is not None\n    ):\n        if not _is_compressible_type(request_dict):\n            body_type = type(request_dict['body'])\n            log_msg = 'Body type %s does not support compression.'\n            logger.debug(log_msg, body_type)\n            return False\n\n        if operation_model.has_streaming_input:\n            streaming_input = operation_model.get_streaming_input()\n            streaming_metadata = streaming_input.metadata\n            return 'requiresLength' not in streaming_metadata\n\n        body_size = _get_body_size(request_dict['body'])\n        min_size = config.request_min_compression_size_bytes\n        return min_size <= body_size\n\n    return False\n\n\ndef _is_compressible_type(request_dict):\n    body = request_dict['body']\n    # Coerce dict to a format compatible with compression.\n    if isinstance(body, dict):\n        body = urlencode(body, doseq=True, encoding='utf-8').encode('utf-8')\n        request_dict['body'] = body\n    is_supported_type = isinstance(body, (str, bytes, bytearray))\n    return is_supported_type or hasattr(body, 'read')\n\n\ndef _get_body_size(body):\n    size = determine_content_length(body)\n    if size is None:\n        logger.debug(\n            'Unable to get length of the request body: %s. '\n            'Skipping compression.',\n            body,\n        )\n        size = 0\n    return size\n\n\ndef _gzip_compress_body(body):\n    if isinstance(body, str):\n        return gzip_compress(body.encode('utf-8'))\n    elif isinstance(body, (bytes, bytearray)):\n        return gzip_compress(body)\n    elif hasattr(body, 'read'):\n        if hasattr(body, 'seek') and hasattr(body, 'tell'):\n            current_position = body.tell()\n            compressed_obj = _gzip_compress_fileobj(body)\n            body.seek(current_position)\n            return compressed_obj\n        return _gzip_compress_fileobj(body)\n\n\ndef _gzip_compress_fileobj(body):\n    compressed_obj = io.BytesIO()\n    with GzipFile(fileobj=compressed_obj, mode='wb') as gz:\n        while True:\n            chunk = body.read(8192)\n            if not chunk:\n                break\n            if isinstance(chunk, str):\n                chunk = chunk.encode('utf-8')\n            gz.write(chunk)\n    compressed_obj.seek(0)\n    return compressed_obj\n\n\ndef _set_compression_header(headers, encoding):\n    ce_header = headers.get('Content-Encoding')\n    if ce_header is None:\n        headers['Content-Encoding'] = encoding\n    else:\n        headers['Content-Encoding'] = f'{ce_header},{encoding}'\n\n\nCOMPRESSION_MAPPING = {'gzip': _gzip_compress_body}\n", "botocore/endpoint.py": "# Copyright (c) 2012-2013 Mitch Garnaat http://garnaat.org/\n# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport datetime\nimport logging\nimport os\nimport threading\nimport time\nimport uuid\n\nfrom botocore import parsers\nfrom botocore.awsrequest import create_request_object\nfrom botocore.exceptions import HTTPClientError\nfrom botocore.history import get_global_history_recorder\nfrom botocore.hooks import first_non_none_response\nfrom botocore.httpchecksum import handle_checksum_body\nfrom botocore.httpsession import URLLib3Session\nfrom botocore.response import StreamingBody\nfrom botocore.utils import (\n    get_environ_proxies,\n    is_valid_endpoint_url,\n    is_valid_ipv6_endpoint_url,\n)\n\nlogger = logging.getLogger(__name__)\nhistory_recorder = get_global_history_recorder()\nDEFAULT_TIMEOUT = 60\nMAX_POOL_CONNECTIONS = 10\n\n\ndef convert_to_response_dict(http_response, operation_model):\n    \"\"\"Convert an HTTP response object to a request dict.\n\n    This converts the requests library's HTTP response object to\n    a dictionary.\n\n    :type http_response: botocore.vendored.requests.model.Response\n    :param http_response: The HTTP response from an AWS service request.\n\n    :rtype: dict\n    :return: A response dictionary which will contain the following keys:\n        * headers (dict)\n        * status_code (int)\n        * body (string or file-like object)\n\n    \"\"\"\n    response_dict = {\n        'headers': http_response.headers,\n        'status_code': http_response.status_code,\n        'context': {\n            'operation_name': operation_model.name,\n        },\n    }\n    if response_dict['status_code'] >= 300:\n        response_dict['body'] = http_response.content\n    elif operation_model.has_event_stream_output:\n        response_dict['body'] = http_response.raw\n    elif operation_model.has_streaming_output:\n        length = response_dict['headers'].get('content-length')\n        response_dict['body'] = StreamingBody(http_response.raw, length)\n    else:\n        response_dict['body'] = http_response.content\n    return response_dict\n\n\nclass Endpoint:\n    \"\"\"\n    Represents an endpoint for a particular service in a specific\n    region.  Only an endpoint can make requests.\n\n    :ivar service: The Service object that describes this endpoints\n        service.\n    :ivar host: The fully qualified endpoint hostname.\n    :ivar session: The session object.\n    \"\"\"\n\n    def __init__(\n        self,\n        host,\n        endpoint_prefix,\n        event_emitter,\n        response_parser_factory=None,\n        http_session=None,\n    ):\n        self._endpoint_prefix = endpoint_prefix\n        self._event_emitter = event_emitter\n        self.host = host\n        self._lock = threading.Lock()\n        if response_parser_factory is None:\n            response_parser_factory = parsers.ResponseParserFactory()\n        self._response_parser_factory = response_parser_factory\n        self.http_session = http_session\n        if self.http_session is None:\n            self.http_session = URLLib3Session()\n\n    def __repr__(self):\n        return f'{self._endpoint_prefix}({self.host})'\n\n    def close(self):\n        self.http_session.close()\n\n    def make_request(self, operation_model, request_dict):\n        logger.debug(\n            \"Making request for %s with params: %s\",\n            operation_model,\n            request_dict,\n        )\n        return self._send_request(request_dict, operation_model)\n\n    def create_request(self, params, operation_model=None):\n        request = create_request_object(params)\n        if operation_model:\n            request.stream_output = any(\n                [\n                    operation_model.has_streaming_output,\n                    operation_model.has_event_stream_output,\n                ]\n            )\n            service_id = operation_model.service_model.service_id.hyphenize()\n            event_name = 'request-created.{service_id}.{op_name}'.format(\n                service_id=service_id, op_name=operation_model.name\n            )\n            self._event_emitter.emit(\n                event_name,\n                request=request,\n                operation_name=operation_model.name,\n            )\n        prepared_request = self.prepare_request(request)\n        return prepared_request\n\n    def _encode_headers(self, headers):\n        # In place encoding of headers to utf-8 if they are unicode.\n        for key, value in headers.items():\n            if isinstance(value, str):\n                headers[key] = value.encode('utf-8')\n\n    def prepare_request(self, request):\n        self._encode_headers(request.headers)\n        return request.prepare()\n\n    def _calculate_ttl(\n        self, response_received_timestamp, date_header, read_timeout\n    ):\n        local_timestamp = datetime.datetime.utcnow()\n        date_conversion = datetime.datetime.strptime(\n            date_header, \"%a, %d %b %Y %H:%M:%S %Z\"\n        )\n        estimated_skew = date_conversion - response_received_timestamp\n        ttl = (\n            local_timestamp\n            + datetime.timedelta(seconds=read_timeout)\n            + estimated_skew\n        )\n        return ttl.strftime('%Y%m%dT%H%M%SZ')\n\n    def _set_ttl(self, retries_context, read_timeout, success_response):\n        response_date_header = success_response[0].headers.get('Date')\n        has_streaming_input = retries_context.get('has_streaming_input')\n        if response_date_header and not has_streaming_input:\n            try:\n                response_received_timestamp = datetime.datetime.utcnow()\n                retries_context['ttl'] = self._calculate_ttl(\n                    response_received_timestamp,\n                    response_date_header,\n                    read_timeout,\n                )\n            except Exception:\n                logger.debug(\n                    \"Exception received when updating retries context with TTL\",\n                    exc_info=True,\n                )\n\n    def _update_retries_context(self, context, attempt, success_response=None):\n        retries_context = context.setdefault('retries', {})\n        retries_context['attempt'] = attempt\n        if 'invocation-id' not in retries_context:\n            retries_context['invocation-id'] = str(uuid.uuid4())\n\n        if success_response:\n            read_timeout = context['client_config'].read_timeout\n            self._set_ttl(retries_context, read_timeout, success_response)\n\n    def _send_request(self, request_dict, operation_model):\n        attempts = 1\n        context = request_dict['context']\n        self._update_retries_context(context, attempts)\n        request = self.create_request(request_dict, operation_model)\n        success_response, exception = self._get_response(\n            request, operation_model, context\n        )\n        while self._needs_retry(\n            attempts,\n            operation_model,\n            request_dict,\n            success_response,\n            exception,\n        ):\n            attempts += 1\n            self._update_retries_context(context, attempts, success_response)\n            # If there is a stream associated with the request, we need\n            # to reset it before attempting to send the request again.\n            # This will ensure that we resend the entire contents of the\n            # body.\n            request.reset_stream()\n            # Create a new request when retried (including a new signature).\n            request = self.create_request(request_dict, operation_model)\n            success_response, exception = self._get_response(\n                request, operation_model, context\n            )\n        if (\n            success_response is not None\n            and 'ResponseMetadata' in success_response[1]\n        ):\n            # We want to share num retries, not num attempts.\n            total_retries = attempts - 1\n            success_response[1]['ResponseMetadata'][\n                'RetryAttempts'\n            ] = total_retries\n        if exception is not None:\n            raise exception\n        else:\n            return success_response\n\n    def _get_response(self, request, operation_model, context):\n        # This will return a tuple of (success_response, exception)\n        # and success_response is itself a tuple of\n        # (http_response, parsed_dict).\n        # If an exception occurs then the success_response is None.\n        # If no exception occurs then exception is None.\n        success_response, exception = self._do_get_response(\n            request, operation_model, context\n        )\n        kwargs_to_emit = {\n            'response_dict': None,\n            'parsed_response': None,\n            'context': context,\n            'exception': exception,\n        }\n        if success_response is not None:\n            http_response, parsed_response = success_response\n            kwargs_to_emit['parsed_response'] = parsed_response\n            kwargs_to_emit['response_dict'] = convert_to_response_dict(\n                http_response, operation_model\n            )\n        service_id = operation_model.service_model.service_id.hyphenize()\n        self._event_emitter.emit(\n            f\"response-received.{service_id}.{operation_model.name}\",\n            **kwargs_to_emit,\n        )\n        return success_response, exception\n\n    def _do_get_response(self, request, operation_model, context):\n        try:\n            logger.debug(\"Sending http request: %s\", request)\n            history_recorder.record(\n                'HTTP_REQUEST',\n                {\n                    'method': request.method,\n                    'headers': request.headers,\n                    'streaming': operation_model.has_streaming_input,\n                    'url': request.url,\n                    'body': request.body,\n                },\n            )\n            service_id = operation_model.service_model.service_id.hyphenize()\n            event_name = f\"before-send.{service_id}.{operation_model.name}\"\n            responses = self._event_emitter.emit(event_name, request=request)\n            http_response = first_non_none_response(responses)\n            if http_response is None:\n                http_response = self._send(request)\n        except HTTPClientError as e:\n            return (None, e)\n        except Exception as e:\n            logger.debug(\n                \"Exception received when sending HTTP request.\", exc_info=True\n            )\n            return (None, e)\n        # This returns the http_response and the parsed_data.\n        response_dict = convert_to_response_dict(\n            http_response, operation_model\n        )\n        handle_checksum_body(\n            http_response,\n            response_dict,\n            context,\n            operation_model,\n        )\n\n        http_response_record_dict = response_dict.copy()\n        http_response_record_dict[\n            'streaming'\n        ] = operation_model.has_streaming_output\n        history_recorder.record('HTTP_RESPONSE', http_response_record_dict)\n\n        protocol = operation_model.metadata['protocol']\n        parser = self._response_parser_factory.create_parser(protocol)\n        parsed_response = parser.parse(\n            response_dict, operation_model.output_shape\n        )\n        # Do a second parsing pass to pick up on any modeled error fields\n        # NOTE: Ideally, we would push this down into the parser classes but\n        # they currently have no reference to the operation or service model\n        # The parsers should probably take the operation model instead of\n        # output shape but we can't change that now\n        if http_response.status_code >= 300:\n            self._add_modeled_error_fields(\n                response_dict,\n                parsed_response,\n                operation_model,\n                parser,\n            )\n        history_recorder.record('PARSED_RESPONSE', parsed_response)\n        return (http_response, parsed_response), None\n\n    def _add_modeled_error_fields(\n        self,\n        response_dict,\n        parsed_response,\n        operation_model,\n        parser,\n    ):\n        error_code = parsed_response.get(\"Error\", {}).get(\"Code\")\n        if error_code is None:\n            return\n        service_model = operation_model.service_model\n        error_shape = service_model.shape_for_error_code(error_code)\n        if error_shape is None:\n            return\n        modeled_parse = parser.parse(response_dict, error_shape)\n        # TODO: avoid naming conflicts with ResponseMetadata and Error\n        parsed_response.update(modeled_parse)\n\n    def _needs_retry(\n        self,\n        attempts,\n        operation_model,\n        request_dict,\n        response=None,\n        caught_exception=None,\n    ):\n        service_id = operation_model.service_model.service_id.hyphenize()\n        event_name = f\"needs-retry.{service_id}.{operation_model.name}\"\n        responses = self._event_emitter.emit(\n            event_name,\n            response=response,\n            endpoint=self,\n            operation=operation_model,\n            attempts=attempts,\n            caught_exception=caught_exception,\n            request_dict=request_dict,\n        )\n        handler_response = first_non_none_response(responses)\n        if handler_response is None:\n            return False\n        else:\n            # Request needs to be retried, and we need to sleep\n            # for the specified number of times.\n            logger.debug(\n                \"Response received to retry, sleeping for %s seconds\",\n                handler_response,\n            )\n            time.sleep(handler_response)\n            return True\n\n    def _send(self, request):\n        return self.http_session.send(request)\n\n\nclass EndpointCreator:\n    def __init__(self, event_emitter):\n        self._event_emitter = event_emitter\n\n    def create_endpoint(\n        self,\n        service_model,\n        region_name,\n        endpoint_url,\n        verify=None,\n        response_parser_factory=None,\n        timeout=DEFAULT_TIMEOUT,\n        max_pool_connections=MAX_POOL_CONNECTIONS,\n        http_session_cls=URLLib3Session,\n        proxies=None,\n        socket_options=None,\n        client_cert=None,\n        proxies_config=None,\n    ):\n        if not is_valid_endpoint_url(\n            endpoint_url\n        ) and not is_valid_ipv6_endpoint_url(endpoint_url):\n            raise ValueError(\"Invalid endpoint: %s\" % endpoint_url)\n\n        if proxies is None:\n            proxies = self._get_proxies(endpoint_url)\n        endpoint_prefix = service_model.endpoint_prefix\n\n        logger.debug('Setting %s timeout as %s', endpoint_prefix, timeout)\n        http_session = http_session_cls(\n            timeout=timeout,\n            proxies=proxies,\n            verify=self._get_verify_value(verify),\n            max_pool_connections=max_pool_connections,\n            socket_options=socket_options,\n            client_cert=client_cert,\n            proxies_config=proxies_config,\n        )\n\n        return Endpoint(\n            endpoint_url,\n            endpoint_prefix=endpoint_prefix,\n            event_emitter=self._event_emitter,\n            response_parser_factory=response_parser_factory,\n            http_session=http_session,\n        )\n\n    def _get_proxies(self, url):\n        # We could also support getting proxies from a config file,\n        # but for now proxy support is taken from the environment.\n        return get_environ_proxies(url)\n\n    def _get_verify_value(self, verify):\n        # This is to account for:\n        # https://github.com/kennethreitz/requests/issues/1436\n        # where we need to honor REQUESTS_CA_BUNDLE because we're creating our\n        # own request objects.\n        # First, if verify is not None, then the user explicitly specified\n        # a value so this automatically wins.\n        if verify is not None:\n            return verify\n        # Otherwise use the value from REQUESTS_CA_BUNDLE, or default to\n        # True if the env var does not exist.\n        return os.environ.get('REQUESTS_CA_BUNDLE', True)\n", "botocore/auth.py": "# Copyright (c) 2012-2013 Mitch Garnaat http://garnaat.org/\n# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport base64\nimport calendar\nimport datetime\nimport functools\nimport hmac\nimport json\nimport logging\nimport time\nfrom collections.abc import Mapping\nfrom email.utils import formatdate\nfrom hashlib import sha1, sha256\nfrom operator import itemgetter\n\nfrom botocore.compat import (\n    HAS_CRT,\n    HTTPHeaders,\n    encodebytes,\n    ensure_unicode,\n    parse_qs,\n    quote,\n    unquote,\n    urlsplit,\n    urlunsplit,\n)\nfrom botocore.exceptions import NoAuthTokenError, NoCredentialsError\nfrom botocore.utils import (\n    is_valid_ipv6_endpoint_url,\n    normalize_url_path,\n    percent_encode_sequence,\n)\n\n# Imports for backwards compatibility\nfrom botocore.compat import MD5_AVAILABLE  # noqa\n\n\nlogger = logging.getLogger(__name__)\n\n\nEMPTY_SHA256_HASH = (\n    'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855'\n)\n# This is the buffer size used when calculating sha256 checksums.\n# Experimenting with various buffer sizes showed that this value generally\n# gave the best result (in terms of performance).\nPAYLOAD_BUFFER = 1024 * 1024\nISO8601 = '%Y-%m-%dT%H:%M:%SZ'\nSIGV4_TIMESTAMP = '%Y%m%dT%H%M%SZ'\nSIGNED_HEADERS_BLACKLIST = [\n    'expect',\n    'user-agent',\n    'x-amzn-trace-id',\n]\nUNSIGNED_PAYLOAD = 'UNSIGNED-PAYLOAD'\nSTREAMING_UNSIGNED_PAYLOAD_TRAILER = 'STREAMING-UNSIGNED-PAYLOAD-TRAILER'\n\n\ndef _host_from_url(url):\n    # Given URL, derive value for host header. Ensure that value:\n    # 1) is lowercase\n    # 2) excludes port, if it was the default port\n    # 3) excludes userinfo\n    url_parts = urlsplit(url)\n    host = url_parts.hostname  # urlsplit's hostname is always lowercase\n    if is_valid_ipv6_endpoint_url(url):\n        host = f'[{host}]'\n    default_ports = {\n        'http': 80,\n        'https': 443,\n    }\n    if url_parts.port is not None:\n        if url_parts.port != default_ports.get(url_parts.scheme):\n            host = '%s:%d' % (host, url_parts.port)\n    return host\n\n\ndef _get_body_as_dict(request):\n    # For query services, request.data is form-encoded and is already a\n    # dict, but for other services such as rest-json it could be a json\n    # string or bytes. In those cases we attempt to load the data as a\n    # dict.\n    data = request.data\n    if isinstance(data, bytes):\n        data = json.loads(data.decode('utf-8'))\n    elif isinstance(data, str):\n        data = json.loads(data)\n    return data\n\n\nclass BaseSigner:\n    REQUIRES_REGION = False\n    REQUIRES_TOKEN = False\n\n    def add_auth(self, request):\n        raise NotImplementedError(\"add_auth\")\n\n\nclass TokenSigner(BaseSigner):\n    REQUIRES_TOKEN = True\n    \"\"\"\n    Signers that expect an authorization token to perform the authorization\n    \"\"\"\n\n    def __init__(self, auth_token):\n        self.auth_token = auth_token\n\n\nclass SigV2Auth(BaseSigner):\n    \"\"\"\n    Sign a request with Signature V2.\n    \"\"\"\n\n    def __init__(self, credentials):\n        self.credentials = credentials\n\n    def calc_signature(self, request, params):\n        logger.debug(\"Calculating signature using v2 auth.\")\n        split = urlsplit(request.url)\n        path = split.path\n        if len(path) == 0:\n            path = '/'\n        string_to_sign = f\"{request.method}\\n{split.netloc}\\n{path}\\n\"\n        lhmac = hmac.new(\n            self.credentials.secret_key.encode(\"utf-8\"), digestmod=sha256\n        )\n        pairs = []\n        for key in sorted(params):\n            # Any previous signature should not be a part of this\n            # one, so we skip that particular key. This prevents\n            # issues during retries.\n            if key == 'Signature':\n                continue\n            value = str(params[key])\n            quoted_key = quote(key.encode('utf-8'), safe='')\n            quoted_value = quote(value.encode('utf-8'), safe='-_~')\n            pairs.append(f'{quoted_key}={quoted_value}')\n        qs = '&'.join(pairs)\n        string_to_sign += qs\n        logger.debug('String to sign: %s', string_to_sign)\n        lhmac.update(string_to_sign.encode('utf-8'))\n        b64 = base64.b64encode(lhmac.digest()).strip().decode('utf-8')\n        return (qs, b64)\n\n    def add_auth(self, request):\n        # The auth handler is the last thing called in the\n        # preparation phase of a prepared request.\n        # Because of this we have to parse the query params\n        # from the request body so we can update them with\n        # the sigv2 auth params.\n        if self.credentials is None:\n            raise NoCredentialsError()\n        if request.data:\n            # POST\n            params = request.data\n        else:\n            # GET\n            params = request.params\n        params['AWSAccessKeyId'] = self.credentials.access_key\n        params['SignatureVersion'] = '2'\n        params['SignatureMethod'] = 'HmacSHA256'\n        params['Timestamp'] = time.strftime(ISO8601, time.gmtime())\n        if self.credentials.token:\n            params['SecurityToken'] = self.credentials.token\n        qs, signature = self.calc_signature(request, params)\n        params['Signature'] = signature\n        return request\n\n\nclass SigV3Auth(BaseSigner):\n    def __init__(self, credentials):\n        self.credentials = credentials\n\n    def add_auth(self, request):\n        if self.credentials is None:\n            raise NoCredentialsError()\n        if 'Date' in request.headers:\n            del request.headers['Date']\n        request.headers['Date'] = formatdate(usegmt=True)\n        if self.credentials.token:\n            if 'X-Amz-Security-Token' in request.headers:\n                del request.headers['X-Amz-Security-Token']\n            request.headers['X-Amz-Security-Token'] = self.credentials.token\n        new_hmac = hmac.new(\n            self.credentials.secret_key.encode('utf-8'), digestmod=sha256\n        )\n        new_hmac.update(request.headers['Date'].encode('utf-8'))\n        encoded_signature = encodebytes(new_hmac.digest()).strip()\n        signature = (\n            f\"AWS3-HTTPS AWSAccessKeyId={self.credentials.access_key},\"\n            f\"Algorithm=HmacSHA256,Signature={encoded_signature.decode('utf-8')}\"\n        )\n        if 'X-Amzn-Authorization' in request.headers:\n            del request.headers['X-Amzn-Authorization']\n        request.headers['X-Amzn-Authorization'] = signature\n\n\nclass SigV4Auth(BaseSigner):\n    \"\"\"\n    Sign a request with Signature V4.\n    \"\"\"\n\n    REQUIRES_REGION = True\n\n    def __init__(self, credentials, service_name, region_name):\n        self.credentials = credentials\n        # We initialize these value here so the unit tests can have\n        # valid values.  But these will get overriden in ``add_auth``\n        # later for real requests.\n        self._region_name = region_name\n        self._service_name = service_name\n\n    def _sign(self, key, msg, hex=False):\n        if hex:\n            sig = hmac.new(key, msg.encode('utf-8'), sha256).hexdigest()\n        else:\n            sig = hmac.new(key, msg.encode('utf-8'), sha256).digest()\n        return sig\n\n    def headers_to_sign(self, request):\n        \"\"\"\n        Select the headers from the request that need to be included\n        in the StringToSign.\n        \"\"\"\n        header_map = HTTPHeaders()\n        for name, value in request.headers.items():\n            lname = name.lower()\n            if lname not in SIGNED_HEADERS_BLACKLIST:\n                header_map[lname] = value\n        if 'host' not in header_map:\n            # TODO: We should set the host ourselves, instead of relying on our\n            # HTTP client to set it for us.\n            header_map['host'] = _host_from_url(request.url)\n        return header_map\n\n    def canonical_query_string(self, request):\n        # The query string can come from two parts.  One is the\n        # params attribute of the request.  The other is from the request\n        # url (in which case we have to re-split the url into its components\n        # and parse out the query string component).\n        if request.params:\n            return self._canonical_query_string_params(request.params)\n        else:\n            return self._canonical_query_string_url(urlsplit(request.url))\n\n    def _canonical_query_string_params(self, params):\n        # [(key, value), (key2, value2)]\n        key_val_pairs = []\n        if isinstance(params, Mapping):\n            params = params.items()\n        for key, value in params:\n            key_val_pairs.append(\n                (quote(key, safe='-_.~'), quote(str(value), safe='-_.~'))\n            )\n        sorted_key_vals = []\n        # Sort by the URI-encoded key names, and in the case of\n        # repeated keys, then sort by the value.\n        for key, value in sorted(key_val_pairs):\n            sorted_key_vals.append(f'{key}={value}')\n        canonical_query_string = '&'.join(sorted_key_vals)\n        return canonical_query_string\n\n    def _canonical_query_string_url(self, parts):\n        canonical_query_string = ''\n        if parts.query:\n            # [(key, value), (key2, value2)]\n            key_val_pairs = []\n            for pair in parts.query.split('&'):\n                key, _, value = pair.partition('=')\n                key_val_pairs.append((key, value))\n            sorted_key_vals = []\n            # Sort by the URI-encoded key names, and in the case of\n            # repeated keys, then sort by the value.\n            for key, value in sorted(key_val_pairs):\n                sorted_key_vals.append(f'{key}={value}')\n            canonical_query_string = '&'.join(sorted_key_vals)\n        return canonical_query_string\n\n    def canonical_headers(self, headers_to_sign):\n        \"\"\"\n        Return the headers that need to be included in the StringToSign\n        in their canonical form by converting all header keys to lower\n        case, sorting them in alphabetical order and then joining\n        them into a string, separated by newlines.\n        \"\"\"\n        headers = []\n        sorted_header_names = sorted(set(headers_to_sign))\n        for key in sorted_header_names:\n            value = ','.join(\n                self._header_value(v) for v in headers_to_sign.get_all(key)\n            )\n            headers.append(f'{key}:{ensure_unicode(value)}')\n        return '\\n'.join(headers)\n\n    def _header_value(self, value):\n        # From the sigv4 docs:\n        # Lowercase(HeaderName) + ':' + Trimall(HeaderValue)\n        #\n        # The Trimall function removes excess white space before and after\n        # values, and converts sequential spaces to a single space.\n        return ' '.join(value.split())\n\n    def signed_headers(self, headers_to_sign):\n        headers = sorted(n.lower().strip() for n in set(headers_to_sign))\n        return ';'.join(headers)\n\n    def _is_streaming_checksum_payload(self, request):\n        checksum_context = request.context.get('checksum', {})\n        algorithm = checksum_context.get('request_algorithm')\n        return isinstance(algorithm, dict) and algorithm.get('in') == 'trailer'\n\n    def payload(self, request):\n        if self._is_streaming_checksum_payload(request):\n            return STREAMING_UNSIGNED_PAYLOAD_TRAILER\n        elif not self._should_sha256_sign_payload(request):\n            # When payload signing is disabled, we use this static string in\n            # place of the payload checksum.\n            return UNSIGNED_PAYLOAD\n        request_body = request.body\n        if request_body and hasattr(request_body, 'seek'):\n            position = request_body.tell()\n            read_chunksize = functools.partial(\n                request_body.read, PAYLOAD_BUFFER\n            )\n            checksum = sha256()\n            for chunk in iter(read_chunksize, b''):\n                checksum.update(chunk)\n            hex_checksum = checksum.hexdigest()\n            request_body.seek(position)\n            return hex_checksum\n        elif request_body:\n            # The request serialization has ensured that\n            # request.body is a bytes() type.\n            return sha256(request_body).hexdigest()\n        else:\n            return EMPTY_SHA256_HASH\n\n    def _should_sha256_sign_payload(self, request):\n        # Payloads will always be signed over insecure connections.\n        if not request.url.startswith('https'):\n            return True\n\n        # Certain operations may have payload signing disabled by default.\n        # Since we don't have access to the operation model, we pass in this\n        # bit of metadata through the request context.\n        return request.context.get('payload_signing_enabled', True)\n\n    def canonical_request(self, request):\n        cr = [request.method.upper()]\n        path = self._normalize_url_path(urlsplit(request.url).path)\n        cr.append(path)\n        cr.append(self.canonical_query_string(request))\n        headers_to_sign = self.headers_to_sign(request)\n        cr.append(self.canonical_headers(headers_to_sign) + '\\n')\n        cr.append(self.signed_headers(headers_to_sign))\n        if 'X-Amz-Content-SHA256' in request.headers:\n            body_checksum = request.headers['X-Amz-Content-SHA256']\n        else:\n            body_checksum = self.payload(request)\n        cr.append(body_checksum)\n        return '\\n'.join(cr)\n\n    def _normalize_url_path(self, path):\n        normalized_path = quote(normalize_url_path(path), safe='/~')\n        return normalized_path\n\n    def scope(self, request):\n        scope = [self.credentials.access_key]\n        scope.append(request.context['timestamp'][0:8])\n        scope.append(self._region_name)\n        scope.append(self._service_name)\n        scope.append('aws4_request')\n        return '/'.join(scope)\n\n    def credential_scope(self, request):\n        scope = []\n        scope.append(request.context['timestamp'][0:8])\n        scope.append(self._region_name)\n        scope.append(self._service_name)\n        scope.append('aws4_request')\n        return '/'.join(scope)\n\n    def string_to_sign(self, request, canonical_request):\n        \"\"\"\n        Return the canonical StringToSign as well as a dict\n        containing the original version of all headers that\n        were included in the StringToSign.\n        \"\"\"\n        sts = ['AWS4-HMAC-SHA256']\n        sts.append(request.context['timestamp'])\n        sts.append(self.credential_scope(request))\n        sts.append(sha256(canonical_request.encode('utf-8')).hexdigest())\n        return '\\n'.join(sts)\n\n    def signature(self, string_to_sign, request):\n        key = self.credentials.secret_key\n        k_date = self._sign(\n            (f\"AWS4{key}\").encode(), request.context[\"timestamp\"][0:8]\n        )\n        k_region = self._sign(k_date, self._region_name)\n        k_service = self._sign(k_region, self._service_name)\n        k_signing = self._sign(k_service, 'aws4_request')\n        return self._sign(k_signing, string_to_sign, hex=True)\n\n    def add_auth(self, request):\n        if self.credentials is None:\n            raise NoCredentialsError()\n        datetime_now = datetime.datetime.utcnow()\n        request.context['timestamp'] = datetime_now.strftime(SIGV4_TIMESTAMP)\n        # This could be a retry.  Make sure the previous\n        # authorization header is removed first.\n        self._modify_request_before_signing(request)\n        canonical_request = self.canonical_request(request)\n        logger.debug(\"Calculating signature using v4 auth.\")\n        logger.debug('CanonicalRequest:\\n%s', canonical_request)\n        string_to_sign = self.string_to_sign(request, canonical_request)\n        logger.debug('StringToSign:\\n%s', string_to_sign)\n        signature = self.signature(string_to_sign, request)\n        logger.debug('Signature:\\n%s', signature)\n\n        self._inject_signature_to_request(request, signature)\n\n    def _inject_signature_to_request(self, request, signature):\n        auth_str = ['AWS4-HMAC-SHA256 Credential=%s' % self.scope(request)]\n        headers_to_sign = self.headers_to_sign(request)\n        auth_str.append(\n            f\"SignedHeaders={self.signed_headers(headers_to_sign)}\"\n        )\n        auth_str.append('Signature=%s' % signature)\n        request.headers['Authorization'] = ', '.join(auth_str)\n        return request\n\n    def _modify_request_before_signing(self, request):\n        if 'Authorization' in request.headers:\n            del request.headers['Authorization']\n        self._set_necessary_date_headers(request)\n        if self.credentials.token:\n            if 'X-Amz-Security-Token' in request.headers:\n                del request.headers['X-Amz-Security-Token']\n            request.headers['X-Amz-Security-Token'] = self.credentials.token\n\n        if not request.context.get('payload_signing_enabled', True):\n            if 'X-Amz-Content-SHA256' in request.headers:\n                del request.headers['X-Amz-Content-SHA256']\n            request.headers['X-Amz-Content-SHA256'] = UNSIGNED_PAYLOAD\n\n    def _set_necessary_date_headers(self, request):\n        # The spec allows for either the Date _or_ the X-Amz-Date value to be\n        # used so we check both.  If there's a Date header, we use the date\n        # header.  Otherwise we use the X-Amz-Date header.\n        if 'Date' in request.headers:\n            del request.headers['Date']\n            datetime_timestamp = datetime.datetime.strptime(\n                request.context['timestamp'], SIGV4_TIMESTAMP\n            )\n            request.headers['Date'] = formatdate(\n                int(calendar.timegm(datetime_timestamp.timetuple()))\n            )\n            if 'X-Amz-Date' in request.headers:\n                del request.headers['X-Amz-Date']\n        else:\n            if 'X-Amz-Date' in request.headers:\n                del request.headers['X-Amz-Date']\n            request.headers['X-Amz-Date'] = request.context['timestamp']\n\n\nclass S3SigV4Auth(SigV4Auth):\n    def _modify_request_before_signing(self, request):\n        super()._modify_request_before_signing(request)\n        if 'X-Amz-Content-SHA256' in request.headers:\n            del request.headers['X-Amz-Content-SHA256']\n\n        request.headers['X-Amz-Content-SHA256'] = self.payload(request)\n\n    def _should_sha256_sign_payload(self, request):\n        # S3 allows optional body signing, so to minimize the performance\n        # impact, we opt to not SHA256 sign the body on streaming uploads,\n        # provided that we're on https.\n        client_config = request.context.get('client_config')\n        s3_config = getattr(client_config, 's3', None)\n\n        # The config could be None if it isn't set, or if the customer sets it\n        # to None.\n        if s3_config is None:\n            s3_config = {}\n\n        # The explicit configuration takes precedence over any implicit\n        # configuration.\n        sign_payload = s3_config.get('payload_signing_enabled', None)\n        if sign_payload is not None:\n            return sign_payload\n\n        # We require that both a checksum be present and https be enabled\n        # to implicitly disable body signing. The combination of TLS and\n        # a checksum is sufficiently secure and durable for us to be\n        # confident in the request without body signing.\n        checksum_header = 'Content-MD5'\n        checksum_context = request.context.get('checksum', {})\n        algorithm = checksum_context.get('request_algorithm')\n        if isinstance(algorithm, dict) and algorithm.get('in') == 'header':\n            checksum_header = algorithm['name']\n        if (\n            not request.url.startswith(\"https\")\n            or checksum_header not in request.headers\n        ):\n            return True\n\n        # If the input is streaming we disable body signing by default.\n        if request.context.get('has_streaming_input', False):\n            return False\n\n        # If the S3-specific checks had no results, delegate to the generic\n        # checks.\n        return super()._should_sha256_sign_payload(request)\n\n    def _normalize_url_path(self, path):\n        # For S3, we do not normalize the path.\n        return path\n\n\nclass S3ExpressAuth(S3SigV4Auth):\n    REQUIRES_IDENTITY_CACHE = True\n\n    def __init__(\n        self, credentials, service_name, region_name, *, identity_cache\n    ):\n        super().__init__(credentials, service_name, region_name)\n        self._identity_cache = identity_cache\n\n    def add_auth(self, request):\n        super().add_auth(request)\n\n    def _modify_request_before_signing(self, request):\n        super()._modify_request_before_signing(request)\n        if 'x-amz-s3session-token' not in request.headers:\n            request.headers['x-amz-s3session-token'] = self.credentials.token\n        # S3Express does not support STS' X-Amz-Security-Token\n        if 'X-Amz-Security-Token' in request.headers:\n            del request.headers['X-Amz-Security-Token']\n\n\nclass S3ExpressPostAuth(S3ExpressAuth):\n    REQUIRES_IDENTITY_CACHE = True\n\n    def add_auth(self, request):\n        datetime_now = datetime.datetime.utcnow()\n        request.context['timestamp'] = datetime_now.strftime(SIGV4_TIMESTAMP)\n\n        fields = {}\n        if request.context.get('s3-presign-post-fields', None) is not None:\n            fields = request.context['s3-presign-post-fields']\n\n        policy = {}\n        conditions = []\n        if request.context.get('s3-presign-post-policy', None) is not None:\n            policy = request.context['s3-presign-post-policy']\n            if policy.get('conditions', None) is not None:\n                conditions = policy['conditions']\n\n        policy['conditions'] = conditions\n\n        fields['x-amz-algorithm'] = 'AWS4-HMAC-SHA256'\n        fields['x-amz-credential'] = self.scope(request)\n        fields['x-amz-date'] = request.context['timestamp']\n\n        conditions.append({'x-amz-algorithm': 'AWS4-HMAC-SHA256'})\n        conditions.append({'x-amz-credential': self.scope(request)})\n        conditions.append({'x-amz-date': request.context['timestamp']})\n\n        if self.credentials.token is not None:\n            fields['X-Amz-S3session-Token'] = self.credentials.token\n            conditions.append(\n                {'X-Amz-S3session-Token': self.credentials.token}\n            )\n\n        # Dump the base64 encoded policy into the fields dictionary.\n        fields['policy'] = base64.b64encode(\n            json.dumps(policy).encode('utf-8')\n        ).decode('utf-8')\n\n        fields['x-amz-signature'] = self.signature(fields['policy'], request)\n\n        request.context['s3-presign-post-fields'] = fields\n        request.context['s3-presign-post-policy'] = policy\n\n\nclass S3ExpressQueryAuth(S3ExpressAuth):\n    DEFAULT_EXPIRES = 300\n    REQUIRES_IDENTITY_CACHE = True\n\n    def __init__(\n        self,\n        credentials,\n        service_name,\n        region_name,\n        *,\n        identity_cache,\n        expires=DEFAULT_EXPIRES,\n    ):\n        super().__init__(\n            credentials,\n            service_name,\n            region_name,\n            identity_cache=identity_cache,\n        )\n        self._expires = expires\n\n    def _modify_request_before_signing(self, request):\n        # We automatically set this header, so if it's the auto-set value we\n        # want to get rid of it since it doesn't make sense for presigned urls.\n        content_type = request.headers.get('content-type')\n        blocklisted_content_type = (\n            'application/x-www-form-urlencoded; charset=utf-8'\n        )\n        if content_type == blocklisted_content_type:\n            del request.headers['content-type']\n\n        # Note that we're not including X-Amz-Signature.\n        # From the docs: \"The Canonical Query String must include all the query\n        # parameters from the preceding table except for X-Amz-Signature.\n        signed_headers = self.signed_headers(self.headers_to_sign(request))\n\n        auth_params = {\n            'X-Amz-Algorithm': 'AWS4-HMAC-SHA256',\n            'X-Amz-Credential': self.scope(request),\n            'X-Amz-Date': request.context['timestamp'],\n            'X-Amz-Expires': self._expires,\n            'X-Amz-SignedHeaders': signed_headers,\n        }\n        if self.credentials.token is not None:\n            auth_params['X-Amz-S3session-Token'] = self.credentials.token\n        # Now parse the original query string to a dict, inject our new query\n        # params, and serialize back to a query string.\n        url_parts = urlsplit(request.url)\n        # parse_qs makes each value a list, but in our case we know we won't\n        # have repeated keys so we know we have single element lists which we\n        # can convert back to scalar values.\n        query_string_parts = parse_qs(url_parts.query, keep_blank_values=True)\n        query_dict = {k: v[0] for k, v in query_string_parts.items()}\n\n        if request.params:\n            query_dict.update(request.params)\n            request.params = {}\n        # The spec is particular about this.  It *has* to be:\n        # https://<endpoint>?<operation params>&<auth params>\n        # You can't mix the two types of params together, i.e just keep doing\n        # new_query_params.update(op_params)\n        # new_query_params.update(auth_params)\n        # percent_encode_sequence(new_query_params)\n        operation_params = ''\n        if request.data:\n            # We also need to move the body params into the query string. To\n            # do this, we first have to convert it to a dict.\n            query_dict.update(_get_body_as_dict(request))\n            request.data = ''\n        if query_dict:\n            operation_params = percent_encode_sequence(query_dict) + '&'\n        new_query_string = (\n            f\"{operation_params}{percent_encode_sequence(auth_params)}\"\n        )\n        # url_parts is a tuple (and therefore immutable) so we need to create\n        # a new url_parts with the new query string.\n        # <part>   - <index>\n        # scheme   - 0\n        # netloc   - 1\n        # path     - 2\n        # query    - 3  <-- we're replacing this.\n        # fragment - 4\n        p = url_parts\n        new_url_parts = (p[0], p[1], p[2], new_query_string, p[4])\n        request.url = urlunsplit(new_url_parts)\n\n    def _inject_signature_to_request(self, request, signature):\n        # Rather than calculating an \"Authorization\" header, for the query\n        # param quth, we just append an 'X-Amz-Signature' param to the end\n        # of the query string.\n        request.url += '&X-Amz-Signature=%s' % signature\n\n    def _normalize_url_path(self, path):\n        # For S3, we do not normalize the path.\n        return path\n\n    def payload(self, request):\n        # From the doc link above:\n        # \"You don't include a payload hash in the Canonical Request, because\n        # when you create a presigned URL, you don't know anything about the\n        # payload. Instead, you use a constant string \"UNSIGNED-PAYLOAD\".\n        return UNSIGNED_PAYLOAD\n\n\nclass SigV4QueryAuth(SigV4Auth):\n    DEFAULT_EXPIRES = 3600\n\n    def __init__(\n        self, credentials, service_name, region_name, expires=DEFAULT_EXPIRES\n    ):\n        super().__init__(credentials, service_name, region_name)\n        self._expires = expires\n\n    def _modify_request_before_signing(self, request):\n        # We automatically set this header, so if it's the auto-set value we\n        # want to get rid of it since it doesn't make sense for presigned urls.\n        content_type = request.headers.get('content-type')\n        blacklisted_content_type = (\n            'application/x-www-form-urlencoded; charset=utf-8'\n        )\n        if content_type == blacklisted_content_type:\n            del request.headers['content-type']\n\n        # Note that we're not including X-Amz-Signature.\n        # From the docs: \"The Canonical Query String must include all the query\n        # parameters from the preceding table except for X-Amz-Signature.\n        signed_headers = self.signed_headers(self.headers_to_sign(request))\n\n        auth_params = {\n            'X-Amz-Algorithm': 'AWS4-HMAC-SHA256',\n            'X-Amz-Credential': self.scope(request),\n            'X-Amz-Date': request.context['timestamp'],\n            'X-Amz-Expires': self._expires,\n            'X-Amz-SignedHeaders': signed_headers,\n        }\n        if self.credentials.token is not None:\n            auth_params['X-Amz-Security-Token'] = self.credentials.token\n        # Now parse the original query string to a dict, inject our new query\n        # params, and serialize back to a query string.\n        url_parts = urlsplit(request.url)\n        # parse_qs makes each value a list, but in our case we know we won't\n        # have repeated keys so we know we have single element lists which we\n        # can convert back to scalar values.\n        query_string_parts = parse_qs(url_parts.query, keep_blank_values=True)\n        query_dict = {k: v[0] for k, v in query_string_parts.items()}\n\n        if request.params:\n            query_dict.update(request.params)\n            request.params = {}\n        # The spec is particular about this.  It *has* to be:\n        # https://<endpoint>?<operation params>&<auth params>\n        # You can't mix the two types of params together, i.e just keep doing\n        # new_query_params.update(op_params)\n        # new_query_params.update(auth_params)\n        # percent_encode_sequence(new_query_params)\n        operation_params = ''\n        if request.data:\n            # We also need to move the body params into the query string. To\n            # do this, we first have to convert it to a dict.\n            query_dict.update(_get_body_as_dict(request))\n            request.data = ''\n        if query_dict:\n            operation_params = percent_encode_sequence(query_dict) + '&'\n        new_query_string = (\n            f\"{operation_params}{percent_encode_sequence(auth_params)}\"\n        )\n        # url_parts is a tuple (and therefore immutable) so we need to create\n        # a new url_parts with the new query string.\n        # <part>   - <index>\n        # scheme   - 0\n        # netloc   - 1\n        # path     - 2\n        # query    - 3  <-- we're replacing this.\n        # fragment - 4\n        p = url_parts\n        new_url_parts = (p[0], p[1], p[2], new_query_string, p[4])\n        request.url = urlunsplit(new_url_parts)\n\n    def _inject_signature_to_request(self, request, signature):\n        # Rather than calculating an \"Authorization\" header, for the query\n        # param quth, we just append an 'X-Amz-Signature' param to the end\n        # of the query string.\n        request.url += '&X-Amz-Signature=%s' % signature\n\n\nclass S3SigV4QueryAuth(SigV4QueryAuth):\n    \"\"\"S3 SigV4 auth using query parameters.\n\n    This signer will sign a request using query parameters and signature\n    version 4, i.e a \"presigned url\" signer.\n\n    Based off of:\n\n    http://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-query-string-auth.html\n\n    \"\"\"\n\n    def _normalize_url_path(self, path):\n        # For S3, we do not normalize the path.\n        return path\n\n    def payload(self, request):\n        # From the doc link above:\n        # \"You don't include a payload hash in the Canonical Request, because\n        # when you create a presigned URL, you don't know anything about the\n        # payload. Instead, you use a constant string \"UNSIGNED-PAYLOAD\".\n        return UNSIGNED_PAYLOAD\n\n\nclass S3SigV4PostAuth(SigV4Auth):\n    \"\"\"\n    Presigns a s3 post\n\n    Implementation doc here:\n    http://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-UsingHTTPPOST.html\n    \"\"\"\n\n    def add_auth(self, request):\n        datetime_now = datetime.datetime.utcnow()\n        request.context['timestamp'] = datetime_now.strftime(SIGV4_TIMESTAMP)\n\n        fields = {}\n        if request.context.get('s3-presign-post-fields', None) is not None:\n            fields = request.context['s3-presign-post-fields']\n\n        policy = {}\n        conditions = []\n        if request.context.get('s3-presign-post-policy', None) is not None:\n            policy = request.context['s3-presign-post-policy']\n            if policy.get('conditions', None) is not None:\n                conditions = policy['conditions']\n\n        policy['conditions'] = conditions\n\n        fields['x-amz-algorithm'] = 'AWS4-HMAC-SHA256'\n        fields['x-amz-credential'] = self.scope(request)\n        fields['x-amz-date'] = request.context['timestamp']\n\n        conditions.append({'x-amz-algorithm': 'AWS4-HMAC-SHA256'})\n        conditions.append({'x-amz-credential': self.scope(request)})\n        conditions.append({'x-amz-date': request.context['timestamp']})\n\n        if self.credentials.token is not None:\n            fields['x-amz-security-token'] = self.credentials.token\n            conditions.append({'x-amz-security-token': self.credentials.token})\n\n        # Dump the base64 encoded policy into the fields dictionary.\n        fields['policy'] = base64.b64encode(\n            json.dumps(policy).encode('utf-8')\n        ).decode('utf-8')\n\n        fields['x-amz-signature'] = self.signature(fields['policy'], request)\n\n        request.context['s3-presign-post-fields'] = fields\n        request.context['s3-presign-post-policy'] = policy\n\n\nclass HmacV1Auth(BaseSigner):\n    # List of Query String Arguments of Interest\n    QSAOfInterest = [\n        'accelerate',\n        'acl',\n        'cors',\n        'defaultObjectAcl',\n        'location',\n        'logging',\n        'partNumber',\n        'policy',\n        'requestPayment',\n        'torrent',\n        'versioning',\n        'versionId',\n        'versions',\n        'website',\n        'uploads',\n        'uploadId',\n        'response-content-type',\n        'response-content-language',\n        'response-expires',\n        'response-cache-control',\n        'response-content-disposition',\n        'response-content-encoding',\n        'delete',\n        'lifecycle',\n        'tagging',\n        'restore',\n        'storageClass',\n        'notification',\n        'replication',\n        'requestPayment',\n        'analytics',\n        'metrics',\n        'inventory',\n        'select',\n        'select-type',\n        'object-lock',\n    ]\n\n    def __init__(self, credentials, service_name=None, region_name=None):\n        self.credentials = credentials\n\n    def sign_string(self, string_to_sign):\n        new_hmac = hmac.new(\n            self.credentials.secret_key.encode('utf-8'), digestmod=sha1\n        )\n        new_hmac.update(string_to_sign.encode('utf-8'))\n        return encodebytes(new_hmac.digest()).strip().decode('utf-8')\n\n    def canonical_standard_headers(self, headers):\n        interesting_headers = ['content-md5', 'content-type', 'date']\n        hoi = []\n        if 'Date' in headers:\n            del headers['Date']\n        headers['Date'] = self._get_date()\n        for ih in interesting_headers:\n            found = False\n            for key in headers:\n                lk = key.lower()\n                if headers[key] is not None and lk == ih:\n                    hoi.append(headers[key].strip())\n                    found = True\n            if not found:\n                hoi.append('')\n        return '\\n'.join(hoi)\n\n    def canonical_custom_headers(self, headers):\n        hoi = []\n        custom_headers = {}\n        for key in headers:\n            lk = key.lower()\n            if headers[key] is not None:\n                if lk.startswith('x-amz-'):\n                    custom_headers[lk] = ','.join(\n                        v.strip() for v in headers.get_all(key)\n                    )\n        sorted_header_keys = sorted(custom_headers.keys())\n        for key in sorted_header_keys:\n            hoi.append(f\"{key}:{custom_headers[key]}\")\n        return '\\n'.join(hoi)\n\n    def unquote_v(self, nv):\n        \"\"\"\n        TODO: Do we need this?\n        \"\"\"\n        if len(nv) == 1:\n            return nv\n        else:\n            return (nv[0], unquote(nv[1]))\n\n    def canonical_resource(self, split, auth_path=None):\n        # don't include anything after the first ? in the resource...\n        # unless it is one of the QSA of interest, defined above\n        # NOTE:\n        # The path in the canonical resource should always be the\n        # full path including the bucket name, even for virtual-hosting\n        # style addressing.  The ``auth_path`` keeps track of the full\n        # path for the canonical resource and would be passed in if\n        # the client was using virtual-hosting style.\n        if auth_path is not None:\n            buf = auth_path\n        else:\n            buf = split.path\n        if split.query:\n            qsa = split.query.split('&')\n            qsa = [a.split('=', 1) for a in qsa]\n            qsa = [\n                self.unquote_v(a) for a in qsa if a[0] in self.QSAOfInterest\n            ]\n            if len(qsa) > 0:\n                qsa.sort(key=itemgetter(0))\n                qsa = ['='.join(a) for a in qsa]\n                buf += '?'\n                buf += '&'.join(qsa)\n        return buf\n\n    def canonical_string(\n        self, method, split, headers, expires=None, auth_path=None\n    ):\n        cs = method.upper() + '\\n'\n        cs += self.canonical_standard_headers(headers) + '\\n'\n        custom_headers = self.canonical_custom_headers(headers)\n        if custom_headers:\n            cs += custom_headers + '\\n'\n        cs += self.canonical_resource(split, auth_path=auth_path)\n        return cs\n\n    def get_signature(\n        self, method, split, headers, expires=None, auth_path=None\n    ):\n        if self.credentials.token:\n            del headers['x-amz-security-token']\n            headers['x-amz-security-token'] = self.credentials.token\n        string_to_sign = self.canonical_string(\n            method, split, headers, auth_path=auth_path\n        )\n        logger.debug('StringToSign:\\n%s', string_to_sign)\n        return self.sign_string(string_to_sign)\n\n    def add_auth(self, request):\n        if self.credentials is None:\n            raise NoCredentialsError\n        logger.debug(\"Calculating signature using hmacv1 auth.\")\n        split = urlsplit(request.url)\n        logger.debug('HTTP request method: %s', request.method)\n        signature = self.get_signature(\n            request.method, split, request.headers, auth_path=request.auth_path\n        )\n        self._inject_signature(request, signature)\n\n    def _get_date(self):\n        return formatdate(usegmt=True)\n\n    def _inject_signature(self, request, signature):\n        if 'Authorization' in request.headers:\n            # We have to do this because request.headers is not\n            # normal dictionary.  It has the (unintuitive) behavior\n            # of aggregating repeated setattr calls for the same\n            # key value.  For example:\n            # headers['foo'] = 'a'; headers['foo'] = 'b'\n            # list(headers) will print ['foo', 'foo'].\n            del request.headers['Authorization']\n\n        auth_header = f\"AWS {self.credentials.access_key}:{signature}\"\n        request.headers['Authorization'] = auth_header\n\n\nclass HmacV1QueryAuth(HmacV1Auth):\n    \"\"\"\n    Generates a presigned request for s3.\n\n    Spec from this document:\n\n    http://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAuthentication.html\n    #RESTAuthenticationQueryStringAuth\n\n    \"\"\"\n\n    DEFAULT_EXPIRES = 3600\n\n    def __init__(self, credentials, expires=DEFAULT_EXPIRES):\n        self.credentials = credentials\n        self._expires = expires\n\n    def _get_date(self):\n        return str(int(time.time() + int(self._expires)))\n\n    def _inject_signature(self, request, signature):\n        query_dict = {}\n        query_dict['AWSAccessKeyId'] = self.credentials.access_key\n        query_dict['Signature'] = signature\n\n        for header_key in request.headers:\n            lk = header_key.lower()\n            # For query string requests, Expires is used instead of the\n            # Date header.\n            if header_key == 'Date':\n                query_dict['Expires'] = request.headers['Date']\n            # We only want to include relevant headers in the query string.\n            # These can be anything that starts with x-amz, is Content-MD5,\n            # or is Content-Type.\n            elif lk.startswith('x-amz-') or lk in (\n                'content-md5',\n                'content-type',\n            ):\n                query_dict[lk] = request.headers[lk]\n        # Combine all of the identified headers into an encoded\n        # query string\n        new_query_string = percent_encode_sequence(query_dict)\n\n        # Create a new url with the presigned url.\n        p = urlsplit(request.url)\n        if p[3]:\n            # If there was a pre-existing query string, we should\n            # add that back before injecting the new query string.\n            new_query_string = f'{p[3]}&{new_query_string}'\n        new_url_parts = (p[0], p[1], p[2], new_query_string, p[4])\n        request.url = urlunsplit(new_url_parts)\n\n\nclass HmacV1PostAuth(HmacV1Auth):\n    \"\"\"\n    Generates a presigned post for s3.\n\n    Spec from this document:\n\n    http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingHTTPPOST.html\n    \"\"\"\n\n    def add_auth(self, request):\n        fields = {}\n        if request.context.get('s3-presign-post-fields', None) is not None:\n            fields = request.context['s3-presign-post-fields']\n\n        policy = {}\n        conditions = []\n        if request.context.get('s3-presign-post-policy', None) is not None:\n            policy = request.context['s3-presign-post-policy']\n            if policy.get('conditions', None) is not None:\n                conditions = policy['conditions']\n\n        policy['conditions'] = conditions\n\n        fields['AWSAccessKeyId'] = self.credentials.access_key\n\n        if self.credentials.token is not None:\n            fields['x-amz-security-token'] = self.credentials.token\n            conditions.append({'x-amz-security-token': self.credentials.token})\n\n        # Dump the base64 encoded policy into the fields dictionary.\n        fields['policy'] = base64.b64encode(\n            json.dumps(policy).encode('utf-8')\n        ).decode('utf-8')\n\n        fields['signature'] = self.sign_string(fields['policy'])\n\n        request.context['s3-presign-post-fields'] = fields\n        request.context['s3-presign-post-policy'] = policy\n\n\nclass BearerAuth(TokenSigner):\n    \"\"\"\n    Performs bearer token authorization by placing the bearer token in the\n    Authorization header as specified by Section 2.1 of RFC 6750.\n\n    https://datatracker.ietf.org/doc/html/rfc6750#section-2.1\n    \"\"\"\n\n    def add_auth(self, request):\n        if self.auth_token is None:\n            raise NoAuthTokenError()\n\n        auth_header = f'Bearer {self.auth_token.token}'\n        if 'Authorization' in request.headers:\n            del request.headers['Authorization']\n        request.headers['Authorization'] = auth_header\n\n\nAUTH_TYPE_MAPS = {\n    'v2': SigV2Auth,\n    'v3': SigV3Auth,\n    'v3https': SigV3Auth,\n    's3': HmacV1Auth,\n    's3-query': HmacV1QueryAuth,\n    's3-presign-post': HmacV1PostAuth,\n    's3v4-presign-post': S3SigV4PostAuth,\n    'v4-s3express': S3ExpressAuth,\n    'v4-s3express-query': S3ExpressQueryAuth,\n    'v4-s3express-presign-post': S3ExpressPostAuth,\n    'bearer': BearerAuth,\n}\n\n# Define v4 signers depending on if CRT is present\nif HAS_CRT:\n    from botocore.crt.auth import CRT_AUTH_TYPE_MAPS\n\n    AUTH_TYPE_MAPS.update(CRT_AUTH_TYPE_MAPS)\nelse:\n    AUTH_TYPE_MAPS.update(\n        {\n            'v4': SigV4Auth,\n            'v4-query': SigV4QueryAuth,\n            's3v4': S3SigV4Auth,\n            's3v4-query': S3SigV4QueryAuth,\n        }\n    )\n", "botocore/serialize.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"Protocol input serializes.\n\nThis module contains classes that implement input serialization\nfor the various AWS protocol types.\n\nThese classes essentially take user input, a model object that\nrepresents what the expected input should look like, and it returns\na dictionary that contains the various parts of a request.  A few\nhigh level design decisions:\n\n\n* Each protocol type maps to a separate class, all inherit from\n  ``Serializer``.\n* The return value for ``serialize_to_request`` (the main entry\n  point) returns a dictionary that represents a request.  This\n  will have keys like ``url_path``, ``query_string``, etc.  This\n  is done so that it's a) easy to test and b) not tied to a\n  particular HTTP library.  See the ``serialize_to_request`` docstring\n  for more details.\n\nUnicode\n-------\n\nThe input to the serializers should be text (str/unicode), not bytes,\nwith the exception of blob types.  Those are assumed to be binary,\nand if a str/unicode type is passed in, it will be encoded as utf-8.\n\"\"\"\nimport base64\nimport calendar\nimport datetime\nimport json\nimport re\nfrom xml.etree import ElementTree\n\nfrom botocore import validate\nfrom botocore.compat import formatdate\nfrom botocore.exceptions import ParamValidationError\nfrom botocore.utils import (\n    has_header,\n    is_json_value_header,\n    parse_to_aware_datetime,\n    percent_encode,\n)\n\n# From the spec, the default timestamp format if not specified is iso8601.\nDEFAULT_TIMESTAMP_FORMAT = 'iso8601'\nISO8601 = '%Y-%m-%dT%H:%M:%SZ'\n# Same as ISO8601, but with microsecond precision.\nISO8601_MICRO = '%Y-%m-%dT%H:%M:%S.%fZ'\nHOST_PREFIX_RE = re.compile(r\"^[A-Za-z0-9\\.\\-]+$\")\n\n\ndef create_serializer(protocol_name, include_validation=True):\n    # TODO: Unknown protocols.\n    serializer = SERIALIZERS[protocol_name]()\n    if include_validation:\n        validator = validate.ParamValidator()\n        serializer = validate.ParamValidationDecorator(validator, serializer)\n    return serializer\n\n\nclass Serializer:\n    DEFAULT_METHOD = 'POST'\n    # Clients can change this to a different MutableMapping\n    # (i.e OrderedDict) if they want.  This is used in the\n    # compliance test to match the hash ordering used in the\n    # tests.\n    MAP_TYPE = dict\n    DEFAULT_ENCODING = 'utf-8'\n\n    def serialize_to_request(self, parameters, operation_model):\n        \"\"\"Serialize parameters into an HTTP request.\n\n        This method takes user provided parameters and a shape\n        model and serializes the parameters to an HTTP request.\n        More specifically, this method returns information about\n        parts of the HTTP request, it does not enforce a particular\n        interface or standard for an HTTP request.  It instead returns\n        a dictionary of:\n\n            * 'url_path'\n            * 'host_prefix'\n            * 'query_string'\n            * 'headers'\n            * 'body'\n            * 'method'\n\n        It is then up to consumers to decide how to map this to a Request\n        object of their HTTP library of choice.  Below is an example\n        return value::\n\n            {'body': {'Action': 'OperationName',\n                      'Bar': 'val2',\n                      'Foo': 'val1',\n                      'Version': '2014-01-01'},\n             'headers': {},\n             'method': 'POST',\n             'query_string': '',\n             'host_prefix': 'value.',\n             'url_path': '/'}\n\n        :param parameters: The dictionary input parameters for the\n            operation (i.e the user input).\n        :param operation_model: The OperationModel object that describes\n            the operation.\n        \"\"\"\n        raise NotImplementedError(\"serialize_to_request\")\n\n    def _create_default_request(self):\n        # Creates a boilerplate default request dict that subclasses\n        # can use as a starting point.\n        serialized = {\n            'url_path': '/',\n            'query_string': '',\n            'method': self.DEFAULT_METHOD,\n            'headers': {},\n            # An empty body is represented as an empty byte string.\n            'body': b'',\n        }\n        return serialized\n\n    # Some extra utility methods subclasses can use.\n\n    def _timestamp_iso8601(self, value):\n        if value.microsecond > 0:\n            timestamp_format = ISO8601_MICRO\n        else:\n            timestamp_format = ISO8601\n        return value.strftime(timestamp_format)\n\n    def _timestamp_unixtimestamp(self, value):\n        return int(calendar.timegm(value.timetuple()))\n\n    def _timestamp_rfc822(self, value):\n        if isinstance(value, datetime.datetime):\n            value = self._timestamp_unixtimestamp(value)\n        return formatdate(value, usegmt=True)\n\n    def _convert_timestamp_to_str(self, value, timestamp_format=None):\n        if timestamp_format is None:\n            timestamp_format = self.TIMESTAMP_FORMAT\n        timestamp_format = timestamp_format.lower()\n        datetime_obj = parse_to_aware_datetime(value)\n        converter = getattr(self, f'_timestamp_{timestamp_format}')\n        final_value = converter(datetime_obj)\n        return final_value\n\n    def _get_serialized_name(self, shape, default_name):\n        # Returns the serialized name for the shape if it exists.\n        # Otherwise it will return the passed in default_name.\n        return shape.serialization.get('name', default_name)\n\n    def _get_base64(self, value):\n        # Returns the base64-encoded version of value, handling\n        # both strings and bytes. The returned value is a string\n        # via the default encoding.\n        if isinstance(value, str):\n            value = value.encode(self.DEFAULT_ENCODING)\n        return base64.b64encode(value).strip().decode(self.DEFAULT_ENCODING)\n\n    def _expand_host_prefix(self, parameters, operation_model):\n        operation_endpoint = operation_model.endpoint\n        if (\n            operation_endpoint is None\n            or 'hostPrefix' not in operation_endpoint\n        ):\n            return None\n\n        host_prefix_expression = operation_endpoint['hostPrefix']\n        input_members = operation_model.input_shape.members\n        host_labels = [\n            member\n            for member, shape in input_members.items()\n            if shape.serialization.get('hostLabel')\n        ]\n        format_kwargs = {}\n        bad_labels = []\n        for name in host_labels:\n            param = parameters[name]\n            if not HOST_PREFIX_RE.match(param):\n                bad_labels.append(name)\n            format_kwargs[name] = param\n        if bad_labels:\n            raise ParamValidationError(\n                report=(\n                    f\"Invalid value for parameter(s): {', '.join(bad_labels)}. \"\n                    \"Must contain only alphanumeric characters, hyphen, \"\n                    \"or period.\"\n                )\n            )\n        return host_prefix_expression.format(**format_kwargs)\n\n\nclass QuerySerializer(Serializer):\n    TIMESTAMP_FORMAT = 'iso8601'\n\n    def serialize_to_request(self, parameters, operation_model):\n        shape = operation_model.input_shape\n        serialized = self._create_default_request()\n        serialized['method'] = operation_model.http.get(\n            'method', self.DEFAULT_METHOD\n        )\n        serialized['headers'] = {\n            'Content-Type': 'application/x-www-form-urlencoded; charset=utf-8'\n        }\n        # The query serializer only deals with body params so\n        # that's what we hand off the _serialize_* methods.\n        body_params = self.MAP_TYPE()\n        body_params['Action'] = operation_model.name\n        body_params['Version'] = operation_model.metadata['apiVersion']\n        if shape is not None:\n            self._serialize(body_params, parameters, shape)\n        serialized['body'] = body_params\n\n        host_prefix = self._expand_host_prefix(parameters, operation_model)\n        if host_prefix is not None:\n            serialized['host_prefix'] = host_prefix\n\n        return serialized\n\n    def _serialize(self, serialized, value, shape, prefix=''):\n        # serialized: The dict that is incrementally added to with the\n        #             final serialized parameters.\n        # value: The current user input value.\n        # shape: The shape object that describes the structure of the\n        #        input.\n        # prefix: The incrementally built up prefix for the serialized\n        #         key (i.e Foo.bar.members.1).\n        method = getattr(\n            self,\n            f'_serialize_type_{shape.type_name}',\n            self._default_serialize,\n        )\n        method(serialized, value, shape, prefix=prefix)\n\n    def _serialize_type_structure(self, serialized, value, shape, prefix=''):\n        members = shape.members\n        for key, value in value.items():\n            member_shape = members[key]\n            member_prefix = self._get_serialized_name(member_shape, key)\n            if prefix:\n                member_prefix = f'{prefix}.{member_prefix}'\n            self._serialize(serialized, value, member_shape, member_prefix)\n\n    def _serialize_type_list(self, serialized, value, shape, prefix=''):\n        if not value:\n            # The query protocol serializes empty lists.\n            serialized[prefix] = ''\n            return\n        if self._is_shape_flattened(shape):\n            list_prefix = prefix\n            if shape.member.serialization.get('name'):\n                name = self._get_serialized_name(shape.member, default_name='')\n                # Replace '.Original' with '.{name}'.\n                list_prefix = '.'.join(prefix.split('.')[:-1] + [name])\n        else:\n            list_name = shape.member.serialization.get('name', 'member')\n            list_prefix = f'{prefix}.{list_name}'\n        for i, element in enumerate(value, 1):\n            element_prefix = f'{list_prefix}.{i}'\n            element_shape = shape.member\n            self._serialize(serialized, element, element_shape, element_prefix)\n\n    def _serialize_type_map(self, serialized, value, shape, prefix=''):\n        if self._is_shape_flattened(shape):\n            full_prefix = prefix\n        else:\n            full_prefix = '%s.entry' % prefix\n        template = full_prefix + '.{i}.{suffix}'\n        key_shape = shape.key\n        value_shape = shape.value\n        key_suffix = self._get_serialized_name(key_shape, default_name='key')\n        value_suffix = self._get_serialized_name(value_shape, 'value')\n        for i, key in enumerate(value, 1):\n            key_prefix = template.format(i=i, suffix=key_suffix)\n            value_prefix = template.format(i=i, suffix=value_suffix)\n            self._serialize(serialized, key, key_shape, key_prefix)\n            self._serialize(serialized, value[key], value_shape, value_prefix)\n\n    def _serialize_type_blob(self, serialized, value, shape, prefix=''):\n        # Blob args must be base64 encoded.\n        serialized[prefix] = self._get_base64(value)\n\n    def _serialize_type_timestamp(self, serialized, value, shape, prefix=''):\n        serialized[prefix] = self._convert_timestamp_to_str(\n            value, shape.serialization.get('timestampFormat')\n        )\n\n    def _serialize_type_boolean(self, serialized, value, shape, prefix=''):\n        if value:\n            serialized[prefix] = 'true'\n        else:\n            serialized[prefix] = 'false'\n\n    def _default_serialize(self, serialized, value, shape, prefix=''):\n        serialized[prefix] = value\n\n    def _is_shape_flattened(self, shape):\n        return shape.serialization.get('flattened')\n\n\nclass EC2Serializer(QuerySerializer):\n    \"\"\"EC2 specific customizations to the query protocol serializers.\n\n    The EC2 model is almost, but not exactly, similar to the query protocol\n    serializer.  This class encapsulates those differences.  The model\n    will have be marked with a ``protocol`` of ``ec2``, so you don't need\n    to worry about wiring this class up correctly.\n\n    \"\"\"\n\n    def _get_serialized_name(self, shape, default_name):\n        # Returns the serialized name for the shape if it exists.\n        # Otherwise it will return the passed in default_name.\n        if 'queryName' in shape.serialization:\n            return shape.serialization['queryName']\n        elif 'name' in shape.serialization:\n            # A locationName is always capitalized\n            # on input for the ec2 protocol.\n            name = shape.serialization['name']\n            return name[0].upper() + name[1:]\n        else:\n            return default_name\n\n    def _serialize_type_list(self, serialized, value, shape, prefix=''):\n        for i, element in enumerate(value, 1):\n            element_prefix = f'{prefix}.{i}'\n            element_shape = shape.member\n            self._serialize(serialized, element, element_shape, element_prefix)\n\n\nclass JSONSerializer(Serializer):\n    TIMESTAMP_FORMAT = 'unixtimestamp'\n\n    def serialize_to_request(self, parameters, operation_model):\n        target = '{}.{}'.format(\n            operation_model.metadata['targetPrefix'],\n            operation_model.name,\n        )\n        json_version = operation_model.metadata['jsonVersion']\n        serialized = self._create_default_request()\n        serialized['method'] = operation_model.http.get(\n            'method', self.DEFAULT_METHOD\n        )\n        serialized['headers'] = {\n            'X-Amz-Target': target,\n            'Content-Type': 'application/x-amz-json-%s' % json_version,\n        }\n        body = self.MAP_TYPE()\n        input_shape = operation_model.input_shape\n        if input_shape is not None:\n            self._serialize(body, parameters, input_shape)\n        serialized['body'] = json.dumps(body).encode(self.DEFAULT_ENCODING)\n\n        host_prefix = self._expand_host_prefix(parameters, operation_model)\n        if host_prefix is not None:\n            serialized['host_prefix'] = host_prefix\n\n        return serialized\n\n    def _serialize(self, serialized, value, shape, key=None):\n        method = getattr(\n            self,\n            '_serialize_type_%s' % shape.type_name,\n            self._default_serialize,\n        )\n        method(serialized, value, shape, key)\n\n    def _serialize_type_structure(self, serialized, value, shape, key):\n        if shape.is_document_type:\n            serialized[key] = value\n        else:\n            if key is not None:\n                # If a key is provided, this is a result of a recursive\n                # call so we need to add a new child dict as the value\n                # of the passed in serialized dict.  We'll then add\n                # all the structure members as key/vals in the new serialized\n                # dictionary we just created.\n                new_serialized = self.MAP_TYPE()\n                serialized[key] = new_serialized\n                serialized = new_serialized\n            members = shape.members\n            for member_key, member_value in value.items():\n                member_shape = members[member_key]\n                if 'name' in member_shape.serialization:\n                    member_key = member_shape.serialization['name']\n                self._serialize(\n                    serialized, member_value, member_shape, member_key\n                )\n\n    def _serialize_type_map(self, serialized, value, shape, key):\n        map_obj = self.MAP_TYPE()\n        serialized[key] = map_obj\n        for sub_key, sub_value in value.items():\n            self._serialize(map_obj, sub_value, shape.value, sub_key)\n\n    def _serialize_type_list(self, serialized, value, shape, key):\n        list_obj = []\n        serialized[key] = list_obj\n        for list_item in value:\n            wrapper = {}\n            # The JSON list serialization is the only case where we aren't\n            # setting a key on a dict.  We handle this by using\n            # a __current__ key on a wrapper dict to serialize each\n            # list item before appending it to the serialized list.\n            self._serialize(wrapper, list_item, shape.member, \"__current__\")\n            list_obj.append(wrapper[\"__current__\"])\n\n    def _default_serialize(self, serialized, value, shape, key):\n        serialized[key] = value\n\n    def _serialize_type_timestamp(self, serialized, value, shape, key):\n        serialized[key] = self._convert_timestamp_to_str(\n            value, shape.serialization.get('timestampFormat')\n        )\n\n    def _serialize_type_blob(self, serialized, value, shape, key):\n        serialized[key] = self._get_base64(value)\n\n\nclass BaseRestSerializer(Serializer):\n    \"\"\"Base class for rest protocols.\n\n    The only variance between the various rest protocols is the\n    way that the body is serialized.  All other aspects (headers, uri, etc.)\n    are the same and logic for serializing those aspects lives here.\n\n    Subclasses must implement the ``_serialize_body_params`` method.\n\n    \"\"\"\n\n    QUERY_STRING_TIMESTAMP_FORMAT = 'iso8601'\n    HEADER_TIMESTAMP_FORMAT = 'rfc822'\n    # This is a list of known values for the \"location\" key in the\n    # serialization dict.  The location key tells us where on the request\n    # to put the serialized value.\n    KNOWN_LOCATIONS = ['uri', 'querystring', 'header', 'headers']\n\n    def serialize_to_request(self, parameters, operation_model):\n        serialized = self._create_default_request()\n        serialized['method'] = operation_model.http.get(\n            'method', self.DEFAULT_METHOD\n        )\n        shape = operation_model.input_shape\n        if shape is None:\n            serialized['url_path'] = operation_model.http['requestUri']\n            return serialized\n        shape_members = shape.members\n        # While the ``serialized`` key holds the final serialized request\n        # data, we need interim dicts for the various locations of the\n        # request.  We need this for the uri_path_kwargs and the\n        # query_string_kwargs because they are templated, so we need\n        # to gather all the needed data for the string template,\n        # then we render the template.  The body_kwargs is needed\n        # because once we've collected them all, we run them through\n        # _serialize_body_params, which for rest-json, creates JSON,\n        # and for rest-xml, will create XML.  This is what the\n        # ``partitioned`` dict below is for.\n        partitioned = {\n            'uri_path_kwargs': self.MAP_TYPE(),\n            'query_string_kwargs': self.MAP_TYPE(),\n            'body_kwargs': self.MAP_TYPE(),\n            'headers': self.MAP_TYPE(),\n        }\n        for param_name, param_value in parameters.items():\n            if param_value is None:\n                # Don't serialize any parameter with a None value.\n                continue\n            self._partition_parameters(\n                partitioned, param_name, param_value, shape_members\n            )\n        serialized['url_path'] = self._render_uri_template(\n            operation_model.http['requestUri'], partitioned['uri_path_kwargs']\n        )\n\n        if 'authPath' in operation_model.http:\n            serialized['auth_path'] = self._render_uri_template(\n                operation_model.http['authPath'],\n                partitioned['uri_path_kwargs'],\n            )\n        # Note that we lean on the http implementation to handle the case\n        # where the requestUri path already has query parameters.\n        # The bundled http client, requests, already supports this.\n        serialized['query_string'] = partitioned['query_string_kwargs']\n        if partitioned['headers']:\n            serialized['headers'] = partitioned['headers']\n        self._serialize_payload(\n            partitioned, parameters, serialized, shape, shape_members\n        )\n        self._serialize_content_type(serialized, shape, shape_members)\n\n        host_prefix = self._expand_host_prefix(parameters, operation_model)\n        if host_prefix is not None:\n            serialized['host_prefix'] = host_prefix\n\n        return serialized\n\n    def _render_uri_template(self, uri_template, params):\n        # We need to handle two cases::\n        #\n        # /{Bucket}/foo\n        # /{Key+}/bar\n        # A label ending with '+' is greedy.  There can only\n        # be one greedy key.\n        encoded_params = {}\n        for template_param in re.findall(r'{(.*?)}', uri_template):\n            if template_param.endswith('+'):\n                encoded_params[template_param] = percent_encode(\n                    params[template_param[:-1]], safe='/~'\n                )\n            else:\n                encoded_params[template_param] = percent_encode(\n                    params[template_param]\n                )\n        return uri_template.format(**encoded_params)\n\n    def _serialize_payload(\n        self, partitioned, parameters, serialized, shape, shape_members\n    ):\n        # partitioned - The user input params partitioned by location.\n        # parameters - The user input params.\n        # serialized - The final serialized request dict.\n        # shape - Describes the expected input shape\n        # shape_members - The members of the input struct shape\n        payload_member = shape.serialization.get('payload')\n        if self._has_streaming_payload(payload_member, shape_members):\n            # If it's streaming, then the body is just the\n            # value of the payload.\n            body_payload = parameters.get(payload_member, b'')\n            body_payload = self._encode_payload(body_payload)\n            serialized['body'] = body_payload\n        elif payload_member is not None:\n            # If there's a payload member, we serialized that\n            # member to they body.\n            body_params = parameters.get(payload_member)\n            if body_params is not None:\n                serialized['body'] = self._serialize_body_params(\n                    body_params, shape_members[payload_member]\n                )\n            else:\n                serialized['body'] = self._serialize_empty_body()\n        elif partitioned['body_kwargs']:\n            serialized['body'] = self._serialize_body_params(\n                partitioned['body_kwargs'], shape\n            )\n        elif self._requires_empty_body(shape):\n            serialized['body'] = self._serialize_empty_body()\n\n    def _serialize_empty_body(self):\n        return b''\n\n    def _serialize_content_type(self, serialized, shape, shape_members):\n        \"\"\"\n        Some protocols require varied Content-Type headers\n        depending on user input. This allows subclasses to apply\n        this conditionally.\n        \"\"\"\n        pass\n\n    def _requires_empty_body(self, shape):\n        \"\"\"\n        Some protocols require a specific body to represent an empty\n        payload. This allows subclasses to apply this conditionally.\n        \"\"\"\n        return False\n\n    def _has_streaming_payload(self, payload, shape_members):\n        \"\"\"Determine if payload is streaming (a blob or string).\"\"\"\n        return payload is not None and shape_members[payload].type_name in (\n            'blob',\n            'string',\n        )\n\n    def _encode_payload(self, body):\n        if isinstance(body, str):\n            return body.encode(self.DEFAULT_ENCODING)\n        return body\n\n    def _partition_parameters(\n        self, partitioned, param_name, param_value, shape_members\n    ):\n        # This takes the user provided input parameter (``param``)\n        # and figures out where they go in the request dict.\n        # Some params are HTTP headers, some are used in the URI, some\n        # are in the request body.  This method deals with this.\n        member = shape_members[param_name]\n        location = member.serialization.get('location')\n        key_name = member.serialization.get('name', param_name)\n        if location == 'uri':\n            partitioned['uri_path_kwargs'][key_name] = param_value\n        elif location == 'querystring':\n            if isinstance(param_value, dict):\n                partitioned['query_string_kwargs'].update(param_value)\n            elif isinstance(param_value, bool):\n                bool_str = str(param_value).lower()\n                partitioned['query_string_kwargs'][key_name] = bool_str\n            elif member.type_name == 'timestamp':\n                timestamp_format = member.serialization.get(\n                    'timestampFormat', self.QUERY_STRING_TIMESTAMP_FORMAT\n                )\n                timestamp = self._convert_timestamp_to_str(\n                    param_value, timestamp_format\n                )\n                partitioned['query_string_kwargs'][key_name] = timestamp\n            else:\n                partitioned['query_string_kwargs'][key_name] = param_value\n        elif location == 'header':\n            shape = shape_members[param_name]\n            if not param_value and shape.type_name == 'list':\n                # Empty lists should not be set on the headers\n                return\n            value = self._convert_header_value(shape, param_value)\n            partitioned['headers'][key_name] = str(value)\n        elif location == 'headers':\n            # 'headers' is a bit of an oddball.  The ``key_name``\n            # is actually really a prefix for the header names:\n            header_prefix = key_name\n            # The value provided by the user is a dict so we'll be\n            # creating multiple header key/val pairs.  The key\n            # name to use for each header is the header_prefix (``key_name``)\n            # plus the key provided by the user.\n            self._do_serialize_header_map(\n                header_prefix, partitioned['headers'], param_value\n            )\n        else:\n            partitioned['body_kwargs'][param_name] = param_value\n\n    def _do_serialize_header_map(self, header_prefix, headers, user_input):\n        for key, val in user_input.items():\n            full_key = header_prefix + key\n            headers[full_key] = val\n\n    def _serialize_body_params(self, params, shape):\n        raise NotImplementedError('_serialize_body_params')\n\n    def _convert_header_value(self, shape, value):\n        if shape.type_name == 'timestamp':\n            datetime_obj = parse_to_aware_datetime(value)\n            timestamp = calendar.timegm(datetime_obj.utctimetuple())\n            timestamp_format = shape.serialization.get(\n                'timestampFormat', self.HEADER_TIMESTAMP_FORMAT\n            )\n            return self._convert_timestamp_to_str(timestamp, timestamp_format)\n        elif shape.type_name == 'list':\n            converted_value = [\n                self._convert_header_value(shape.member, v)\n                for v in value\n                if v is not None\n            ]\n            return \",\".join(converted_value)\n        elif is_json_value_header(shape):\n            # Serialize with no spaces after separators to save space in\n            # the header.\n            return self._get_base64(json.dumps(value, separators=(',', ':')))\n        else:\n            return value\n\n\nclass RestJSONSerializer(BaseRestSerializer, JSONSerializer):\n    def _serialize_empty_body(self):\n        return b'{}'\n\n    def _requires_empty_body(self, shape):\n        \"\"\"\n        Serialize an empty JSON object whenever the shape has\n        members not targeting a location.\n        \"\"\"\n        for member, val in shape.members.items():\n            if 'location' not in val.serialization:\n                return True\n        return False\n\n    def _serialize_content_type(self, serialized, shape, shape_members):\n        \"\"\"Set Content-Type to application/json for all structured bodies.\"\"\"\n        payload = shape.serialization.get('payload')\n        if self._has_streaming_payload(payload, shape_members):\n            # Don't apply content-type to streaming bodies\n            return\n\n        has_body = serialized['body'] != b''\n        has_content_type = has_header('Content-Type', serialized['headers'])\n        if has_body and not has_content_type:\n            serialized['headers']['Content-Type'] = 'application/json'\n\n    def _serialize_body_params(self, params, shape):\n        serialized_body = self.MAP_TYPE()\n        self._serialize(serialized_body, params, shape)\n        return json.dumps(serialized_body).encode(self.DEFAULT_ENCODING)\n\n\nclass RestXMLSerializer(BaseRestSerializer):\n    TIMESTAMP_FORMAT = 'iso8601'\n\n    def _serialize_body_params(self, params, shape):\n        root_name = shape.serialization['name']\n        pseudo_root = ElementTree.Element('')\n        self._serialize(shape, params, pseudo_root, root_name)\n        real_root = list(pseudo_root)[0]\n        return ElementTree.tostring(real_root, encoding=self.DEFAULT_ENCODING)\n\n    def _serialize(self, shape, params, xmlnode, name):\n        method = getattr(\n            self,\n            '_serialize_type_%s' % shape.type_name,\n            self._default_serialize,\n        )\n        method(xmlnode, params, shape, name)\n\n    def _serialize_type_structure(self, xmlnode, params, shape, name):\n        structure_node = ElementTree.SubElement(xmlnode, name)\n\n        if 'xmlNamespace' in shape.serialization:\n            namespace_metadata = shape.serialization['xmlNamespace']\n            attribute_name = 'xmlns'\n            if namespace_metadata.get('prefix'):\n                attribute_name += ':%s' % namespace_metadata['prefix']\n            structure_node.attrib[attribute_name] = namespace_metadata['uri']\n        for key, value in params.items():\n            member_shape = shape.members[key]\n            member_name = member_shape.serialization.get('name', key)\n            # We need to special case member shapes that are marked as an\n            # xmlAttribute.  Rather than serializing into an XML child node,\n            # we instead serialize the shape to an XML attribute of the\n            # *current* node.\n            if value is None:\n                # Don't serialize any param whose value is None.\n                return\n            if member_shape.serialization.get('xmlAttribute'):\n                # xmlAttributes must have a serialization name.\n                xml_attribute_name = member_shape.serialization['name']\n                structure_node.attrib[xml_attribute_name] = value\n                continue\n            self._serialize(member_shape, value, structure_node, member_name)\n\n    def _serialize_type_list(self, xmlnode, params, shape, name):\n        member_shape = shape.member\n        if shape.serialization.get('flattened'):\n            element_name = name\n            list_node = xmlnode\n        else:\n            element_name = member_shape.serialization.get('name', 'member')\n            list_node = ElementTree.SubElement(xmlnode, name)\n        for item in params:\n            self._serialize(member_shape, item, list_node, element_name)\n\n    def _serialize_type_map(self, xmlnode, params, shape, name):\n        # Given the ``name`` of MyMap, and input of {\"key1\": \"val1\"}\n        # we serialize this as:\n        #   <MyMap>\n        #     <entry>\n        #       <key>key1</key>\n        #       <value>val1</value>\n        #     </entry>\n        #  </MyMap>\n        node = ElementTree.SubElement(xmlnode, name)\n        # TODO: handle flattened maps.\n        for key, value in params.items():\n            entry_node = ElementTree.SubElement(node, 'entry')\n            key_name = self._get_serialized_name(shape.key, default_name='key')\n            val_name = self._get_serialized_name(\n                shape.value, default_name='value'\n            )\n            self._serialize(shape.key, key, entry_node, key_name)\n            self._serialize(shape.value, value, entry_node, val_name)\n\n    def _serialize_type_boolean(self, xmlnode, params, shape, name):\n        # For scalar types, the 'params' attr is actually just a scalar\n        # value representing the data we need to serialize as a boolean.\n        # It will either be 'true' or 'false'\n        node = ElementTree.SubElement(xmlnode, name)\n        if params:\n            str_value = 'true'\n        else:\n            str_value = 'false'\n        node.text = str_value\n\n    def _serialize_type_blob(self, xmlnode, params, shape, name):\n        node = ElementTree.SubElement(xmlnode, name)\n        node.text = self._get_base64(params)\n\n    def _serialize_type_timestamp(self, xmlnode, params, shape, name):\n        node = ElementTree.SubElement(xmlnode, name)\n        node.text = self._convert_timestamp_to_str(\n            params, shape.serialization.get('timestampFormat')\n        )\n\n    def _default_serialize(self, xmlnode, params, shape, name):\n        node = ElementTree.SubElement(xmlnode, name)\n        node.text = str(params)\n\n\nSERIALIZERS = {\n    'ec2': EC2Serializer,\n    'query': QuerySerializer,\n    'json': JSONSerializer,\n    'rest-json': RestJSONSerializer,\n    'rest-xml': RestXMLSerializer,\n}\n", "botocore/regions.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"Resolves regions and endpoints.\n\nThis module implements endpoint resolution, including resolving endpoints for a\ngiven service and region and resolving the available endpoints for a service\nin a specific AWS partition.\n\"\"\"\nimport copy\nimport logging\nimport re\nfrom enum import Enum\n\nfrom botocore import UNSIGNED, xform_name\nfrom botocore.auth import AUTH_TYPE_MAPS, HAS_CRT\nfrom botocore.crt import CRT_SUPPORTED_AUTH_TYPES\nfrom botocore.endpoint_provider import EndpointProvider\nfrom botocore.exceptions import (\n    EndpointProviderError,\n    EndpointVariantError,\n    InvalidEndpointConfigurationError,\n    InvalidHostLabelError,\n    MissingDependencyException,\n    NoRegionError,\n    ParamValidationError,\n    UnknownEndpointResolutionBuiltInName,\n    UnknownRegionError,\n    UnknownSignatureVersionError,\n    UnsupportedS3AccesspointConfigurationError,\n    UnsupportedS3ConfigurationError,\n    UnsupportedS3ControlArnError,\n    UnsupportedS3ControlConfigurationError,\n)\nfrom botocore.utils import ensure_boolean, instance_cache\n\nLOG = logging.getLogger(__name__)\nDEFAULT_URI_TEMPLATE = '{service}.{region}.{dnsSuffix}'  # noqa\nDEFAULT_SERVICE_DATA = {'endpoints': {}}\n\n\nclass BaseEndpointResolver:\n    \"\"\"Resolves regions and endpoints. Must be subclassed.\"\"\"\n\n    def construct_endpoint(self, service_name, region_name=None):\n        \"\"\"Resolves an endpoint for a service and region combination.\n\n        :type service_name: string\n        :param service_name: Name of the service to resolve an endpoint for\n            (e.g., s3)\n\n        :type region_name: string\n        :param region_name: Region/endpoint name to resolve (e.g., us-east-1)\n            if no region is provided, the first found partition-wide endpoint\n            will be used if available.\n\n        :rtype: dict\n        :return: Returns a dict containing the following keys:\n            - partition: (string, required) Resolved partition name\n            - endpointName: (string, required) Resolved endpoint name\n            - hostname: (string, required) Hostname to use for this endpoint\n            - sslCommonName: (string) sslCommonName to use for this endpoint.\n            - credentialScope: (dict) Signature version 4 credential scope\n              - region: (string) region name override when signing.\n              - service: (string) service name override when signing.\n            - signatureVersions: (list<string>) A list of possible signature\n              versions, including s3, v4, v2, and s3v4\n            - protocols: (list<string>) A list of supported protocols\n              (e.g., http, https)\n            - ...: Other keys may be included as well based on the metadata\n        \"\"\"\n        raise NotImplementedError\n\n    def get_available_partitions(self):\n        \"\"\"Lists the partitions available to the endpoint resolver.\n\n        :return: Returns a list of partition names (e.g., [\"aws\", \"aws-cn\"]).\n        \"\"\"\n        raise NotImplementedError\n\n    def get_available_endpoints(\n        self, service_name, partition_name='aws', allow_non_regional=False\n    ):\n        \"\"\"Lists the endpoint names of a particular partition.\n\n        :type service_name: string\n        :param service_name: Name of a service to list endpoint for (e.g., s3)\n\n        :type partition_name: string\n        :param partition_name: Name of the partition to limit endpoints to.\n            (e.g., aws for the public AWS endpoints, aws-cn for AWS China\n            endpoints, aws-us-gov for AWS GovCloud (US) Endpoints, etc.\n\n        :type allow_non_regional: bool\n        :param allow_non_regional: Set to True to include endpoints that are\n             not regional endpoints (e.g., s3-external-1,\n             fips-us-gov-west-1, etc).\n        :return: Returns a list of endpoint names (e.g., [\"us-east-1\"]).\n        \"\"\"\n        raise NotImplementedError\n\n\nclass EndpointResolver(BaseEndpointResolver):\n    \"\"\"Resolves endpoints based on partition endpoint metadata\"\"\"\n\n    _UNSUPPORTED_DUALSTACK_PARTITIONS = ['aws-iso', 'aws-iso-b']\n\n    def __init__(self, endpoint_data, uses_builtin_data=False):\n        \"\"\"\n        :type endpoint_data: dict\n        :param endpoint_data: A dict of partition data.\n\n        :type uses_builtin_data: boolean\n        :param uses_builtin_data: Whether the endpoint data originates in the\n            package's data directory.\n        \"\"\"\n        if 'partitions' not in endpoint_data:\n            raise ValueError('Missing \"partitions\" in endpoint data')\n        self._endpoint_data = endpoint_data\n        self.uses_builtin_data = uses_builtin_data\n\n    def get_service_endpoints_data(self, service_name, partition_name='aws'):\n        for partition in self._endpoint_data['partitions']:\n            if partition['partition'] != partition_name:\n                continue\n            services = partition['services']\n            if service_name not in services:\n                continue\n            return services[service_name]['endpoints']\n\n    def get_available_partitions(self):\n        result = []\n        for partition in self._endpoint_data['partitions']:\n            result.append(partition['partition'])\n        return result\n\n    def get_available_endpoints(\n        self,\n        service_name,\n        partition_name='aws',\n        allow_non_regional=False,\n        endpoint_variant_tags=None,\n    ):\n        result = []\n        for partition in self._endpoint_data['partitions']:\n            if partition['partition'] != partition_name:\n                continue\n            services = partition['services']\n            if service_name not in services:\n                continue\n            service_endpoints = services[service_name]['endpoints']\n            for endpoint_name in service_endpoints:\n                is_regional_endpoint = endpoint_name in partition['regions']\n                # Only regional endpoints can be modeled with variants\n                if endpoint_variant_tags and is_regional_endpoint:\n                    variant_data = self._retrieve_variant_data(\n                        service_endpoints[endpoint_name], endpoint_variant_tags\n                    )\n                    if variant_data:\n                        result.append(endpoint_name)\n                elif allow_non_regional or is_regional_endpoint:\n                    result.append(endpoint_name)\n        return result\n\n    def get_partition_dns_suffix(\n        self, partition_name, endpoint_variant_tags=None\n    ):\n        for partition in self._endpoint_data['partitions']:\n            if partition['partition'] == partition_name:\n                if endpoint_variant_tags:\n                    variant = self._retrieve_variant_data(\n                        partition.get('defaults'), endpoint_variant_tags\n                    )\n                    if variant and 'dnsSuffix' in variant:\n                        return variant['dnsSuffix']\n                else:\n                    return partition['dnsSuffix']\n        return None\n\n    def construct_endpoint(\n        self,\n        service_name,\n        region_name=None,\n        partition_name=None,\n        use_dualstack_endpoint=False,\n        use_fips_endpoint=False,\n    ):\n        if (\n            service_name == 's3'\n            and use_dualstack_endpoint\n            and region_name is None\n        ):\n            region_name = 'us-east-1'\n\n        if partition_name is not None:\n            valid_partition = None\n            for partition in self._endpoint_data['partitions']:\n                if partition['partition'] == partition_name:\n                    valid_partition = partition\n\n            if valid_partition is not None:\n                result = self._endpoint_for_partition(\n                    valid_partition,\n                    service_name,\n                    region_name,\n                    use_dualstack_endpoint,\n                    use_fips_endpoint,\n                    True,\n                )\n                return result\n            return None\n\n        # Iterate over each partition until a match is found.\n        for partition in self._endpoint_data['partitions']:\n            if use_dualstack_endpoint and (\n                partition['partition']\n                in self._UNSUPPORTED_DUALSTACK_PARTITIONS\n            ):\n                continue\n            result = self._endpoint_for_partition(\n                partition,\n                service_name,\n                region_name,\n                use_dualstack_endpoint,\n                use_fips_endpoint,\n            )\n            if result:\n                return result\n\n    def get_partition_for_region(self, region_name):\n        for partition in self._endpoint_data['partitions']:\n            if self._region_match(partition, region_name):\n                return partition['partition']\n        raise UnknownRegionError(\n            region_name=region_name,\n            error_msg='No partition found for provided region_name.',\n        )\n\n    def _endpoint_for_partition(\n        self,\n        partition,\n        service_name,\n        region_name,\n        use_dualstack_endpoint,\n        use_fips_endpoint,\n        force_partition=False,\n    ):\n        partition_name = partition[\"partition\"]\n        if (\n            use_dualstack_endpoint\n            and partition_name in self._UNSUPPORTED_DUALSTACK_PARTITIONS\n        ):\n            error_msg = (\n                \"Dualstack endpoints are currently not supported\"\n                \" for %s partition\" % partition_name\n            )\n            raise EndpointVariantError(tags=['dualstack'], error_msg=error_msg)\n\n        # Get the service from the partition, or an empty template.\n        service_data = partition['services'].get(\n            service_name, DEFAULT_SERVICE_DATA\n        )\n        # Use the partition endpoint if no region is supplied.\n        if region_name is None:\n            if 'partitionEndpoint' in service_data:\n                region_name = service_data['partitionEndpoint']\n            else:\n                raise NoRegionError()\n\n        resolve_kwargs = {\n            'partition': partition,\n            'service_name': service_name,\n            'service_data': service_data,\n            'endpoint_name': region_name,\n            'use_dualstack_endpoint': use_dualstack_endpoint,\n            'use_fips_endpoint': use_fips_endpoint,\n        }\n\n        # Attempt to resolve the exact region for this partition.\n        if region_name in service_data['endpoints']:\n            return self._resolve(**resolve_kwargs)\n\n        # Check to see if the endpoint provided is valid for the partition.\n        if self._region_match(partition, region_name) or force_partition:\n            # Use the partition endpoint if set and not regionalized.\n            partition_endpoint = service_data.get('partitionEndpoint')\n            is_regionalized = service_data.get('isRegionalized', True)\n            if partition_endpoint and not is_regionalized:\n                LOG.debug(\n                    'Using partition endpoint for %s, %s: %s',\n                    service_name,\n                    region_name,\n                    partition_endpoint,\n                )\n                resolve_kwargs['endpoint_name'] = partition_endpoint\n                return self._resolve(**resolve_kwargs)\n            LOG.debug(\n                'Creating a regex based endpoint for %s, %s',\n                service_name,\n                region_name,\n            )\n            return self._resolve(**resolve_kwargs)\n\n    def _region_match(self, partition, region_name):\n        if region_name in partition['regions']:\n            return True\n        if 'regionRegex' in partition:\n            return re.compile(partition['regionRegex']).match(region_name)\n        return False\n\n    def _retrieve_variant_data(self, endpoint_data, tags):\n        variants = endpoint_data.get('variants', [])\n        for variant in variants:\n            if set(variant['tags']) == set(tags):\n                result = variant.copy()\n                return result\n\n    def _create_tag_list(self, use_dualstack_endpoint, use_fips_endpoint):\n        tags = []\n        if use_dualstack_endpoint:\n            tags.append('dualstack')\n        if use_fips_endpoint:\n            tags.append('fips')\n        return tags\n\n    def _resolve_variant(\n        self, tags, endpoint_data, service_defaults, partition_defaults\n    ):\n        result = {}\n        for variants in [endpoint_data, service_defaults, partition_defaults]:\n            variant = self._retrieve_variant_data(variants, tags)\n            if variant:\n                self._merge_keys(variant, result)\n        return result\n\n    def _resolve(\n        self,\n        partition,\n        service_name,\n        service_data,\n        endpoint_name,\n        use_dualstack_endpoint,\n        use_fips_endpoint,\n    ):\n        endpoint_data = service_data.get('endpoints', {}).get(\n            endpoint_name, {}\n        )\n\n        if endpoint_data.get('deprecated'):\n            LOG.warning(\n                'Client is configured with the deprecated endpoint: %s'\n                % (endpoint_name)\n            )\n\n        service_defaults = service_data.get('defaults', {})\n        partition_defaults = partition.get('defaults', {})\n        tags = self._create_tag_list(use_dualstack_endpoint, use_fips_endpoint)\n\n        if tags:\n            result = self._resolve_variant(\n                tags, endpoint_data, service_defaults, partition_defaults\n            )\n            if result == {}:\n                error_msg = (\n                    f\"Endpoint does not exist for {service_name} \"\n                    f\"in region {endpoint_name}\"\n                )\n                raise EndpointVariantError(tags=tags, error_msg=error_msg)\n            self._merge_keys(endpoint_data, result)\n        else:\n            result = endpoint_data\n\n        # If dnsSuffix has not already been consumed from a variant definition\n        if 'dnsSuffix' not in result:\n            result['dnsSuffix'] = partition['dnsSuffix']\n\n        result['partition'] = partition['partition']\n        result['endpointName'] = endpoint_name\n\n        # Merge in the service defaults then the partition defaults.\n        self._merge_keys(service_defaults, result)\n        self._merge_keys(partition_defaults, result)\n\n        result['hostname'] = self._expand_template(\n            partition,\n            result['hostname'],\n            service_name,\n            endpoint_name,\n            result['dnsSuffix'],\n        )\n        if 'sslCommonName' in result:\n            result['sslCommonName'] = self._expand_template(\n                partition,\n                result['sslCommonName'],\n                service_name,\n                endpoint_name,\n                result['dnsSuffix'],\n            )\n\n        return result\n\n    def _merge_keys(self, from_data, result):\n        for key in from_data:\n            if key not in result:\n                result[key] = from_data[key]\n\n    def _expand_template(\n        self, partition, template, service_name, endpoint_name, dnsSuffix\n    ):\n        return template.format(\n            service=service_name, region=endpoint_name, dnsSuffix=dnsSuffix\n        )\n\n\nclass EndpointResolverBuiltins(str, Enum):\n    # The AWS Region configured for the SDK client (str)\n    AWS_REGION = \"AWS::Region\"\n    # Whether the UseFIPSEndpoint configuration option has been enabled for\n    # the SDK client (bool)\n    AWS_USE_FIPS = \"AWS::UseFIPS\"\n    # Whether the UseDualStackEndpoint configuration option has been enabled\n    # for the SDK client (bool)\n    AWS_USE_DUALSTACK = \"AWS::UseDualStack\"\n    # Whether the global endpoint should be used with STS, rather the the\n    # regional endpoint for us-east-1 (bool)\n    AWS_STS_USE_GLOBAL_ENDPOINT = \"AWS::STS::UseGlobalEndpoint\"\n    # Whether the global endpoint should be used with S3, rather then the\n    # regional endpoint for us-east-1 (bool)\n    AWS_S3_USE_GLOBAL_ENDPOINT = \"AWS::S3::UseGlobalEndpoint\"\n    # Whether S3 Transfer Acceleration has been requested (bool)\n    AWS_S3_ACCELERATE = \"AWS::S3::Accelerate\"\n    # Whether S3 Force Path Style has been enabled (bool)\n    AWS_S3_FORCE_PATH_STYLE = \"AWS::S3::ForcePathStyle\"\n    # Whether to use the ARN region or raise an error when ARN and client\n    # region differ (for s3 service only, bool)\n    AWS_S3_USE_ARN_REGION = \"AWS::S3::UseArnRegion\"\n    # Whether to use the ARN region or raise an error when ARN and client\n    # region differ (for s3-control service only, bool)\n    AWS_S3CONTROL_USE_ARN_REGION = 'AWS::S3Control::UseArnRegion'\n    # Whether multi-region access points (MRAP) should be disabled (bool)\n    AWS_S3_DISABLE_MRAP = \"AWS::S3::DisableMultiRegionAccessPoints\"\n    # Whether a custom endpoint has been configured (str)\n    SDK_ENDPOINT = \"SDK::Endpoint\"\n\n\nclass EndpointRulesetResolver:\n    \"\"\"Resolves endpoints using a service's endpoint ruleset\"\"\"\n\n    def __init__(\n        self,\n        endpoint_ruleset_data,\n        partition_data,\n        service_model,\n        builtins,\n        client_context,\n        event_emitter,\n        use_ssl=True,\n        requested_auth_scheme=None,\n    ):\n        self._provider = EndpointProvider(\n            ruleset_data=endpoint_ruleset_data,\n            partition_data=partition_data,\n        )\n        self._param_definitions = self._provider.ruleset.parameters\n        self._service_model = service_model\n        self._builtins = builtins\n        self._client_context = client_context\n        self._event_emitter = event_emitter\n        self._use_ssl = use_ssl\n        self._requested_auth_scheme = requested_auth_scheme\n        self._instance_cache = {}\n\n    def construct_endpoint(\n        self,\n        operation_model,\n        call_args,\n        request_context,\n    ):\n        \"\"\"Invokes the provider with params defined in the service's ruleset\"\"\"\n        if call_args is None:\n            call_args = {}\n\n        if request_context is None:\n            request_context = {}\n\n        provider_params = self._get_provider_params(\n            operation_model, call_args, request_context\n        )\n        LOG.debug(\n            'Calling endpoint provider with parameters: %s' % provider_params\n        )\n        try:\n            provider_result = self._provider.resolve_endpoint(\n                **provider_params\n            )\n        except EndpointProviderError as ex:\n            botocore_exception = self.ruleset_error_to_botocore_exception(\n                ex, provider_params\n            )\n            if botocore_exception is None:\n                raise\n            else:\n                raise botocore_exception from ex\n        LOG.debug('Endpoint provider result: %s' % provider_result.url)\n\n        # The endpoint provider does not support non-secure transport.\n        if not self._use_ssl and provider_result.url.startswith('https://'):\n            provider_result = provider_result._replace(\n                url=f'http://{provider_result.url[8:]}'\n            )\n\n        # Multi-valued headers are not supported in botocore. Replace the list\n        # of values returned for each header with just its first entry,\n        # dropping any additionally entries.\n        provider_result = provider_result._replace(\n            headers={\n                key: val[0] for key, val in provider_result.headers.items()\n            }\n        )\n\n        return provider_result\n\n    def _get_provider_params(\n        self, operation_model, call_args, request_context\n    ):\n        \"\"\"Resolve a value for each parameter defined in the service's ruleset\n\n        The resolution order for parameter values is:\n        1. Operation-specific static context values from the service definition\n        2. Operation-specific dynamic context values from API parameters\n        3. Client-specific context parameters\n        4. Built-in values such as region, FIPS usage, ...\n        \"\"\"\n        provider_params = {}\n        # Builtin values can be customized for each operation by hooks\n        # subscribing to the ``before-endpoint-resolution.*`` event.\n        customized_builtins = self._get_customized_builtins(\n            operation_model, call_args, request_context\n        )\n        for param_name, param_def in self._param_definitions.items():\n            param_val = self._resolve_param_from_context(\n                param_name=param_name,\n                operation_model=operation_model,\n                call_args=call_args,\n            )\n            if param_val is None and param_def.builtin is not None:\n                param_val = self._resolve_param_as_builtin(\n                    builtin_name=param_def.builtin,\n                    builtins=customized_builtins,\n                )\n            if param_val is not None:\n                provider_params[param_name] = param_val\n\n        return provider_params\n\n    def _resolve_param_from_context(\n        self, param_name, operation_model, call_args\n    ):\n        static = self._resolve_param_as_static_context_param(\n            param_name, operation_model\n        )\n        if static is not None:\n            return static\n        dynamic = self._resolve_param_as_dynamic_context_param(\n            param_name, operation_model, call_args\n        )\n        if dynamic is not None:\n            return dynamic\n        return self._resolve_param_as_client_context_param(param_name)\n\n    def _resolve_param_as_static_context_param(\n        self, param_name, operation_model\n    ):\n        static_ctx_params = self._get_static_context_params(operation_model)\n        return static_ctx_params.get(param_name)\n\n    def _resolve_param_as_dynamic_context_param(\n        self, param_name, operation_model, call_args\n    ):\n        dynamic_ctx_params = self._get_dynamic_context_params(operation_model)\n        if param_name in dynamic_ctx_params:\n            member_name = dynamic_ctx_params[param_name]\n            return call_args.get(member_name)\n\n    def _resolve_param_as_client_context_param(self, param_name):\n        client_ctx_params = self._get_client_context_params()\n        if param_name in client_ctx_params:\n            client_ctx_varname = client_ctx_params[param_name]\n            return self._client_context.get(client_ctx_varname)\n\n    def _resolve_param_as_builtin(self, builtin_name, builtins):\n        if builtin_name not in EndpointResolverBuiltins.__members__.values():\n            raise UnknownEndpointResolutionBuiltInName(name=builtin_name)\n        return builtins.get(builtin_name)\n\n    @instance_cache\n    def _get_static_context_params(self, operation_model):\n        \"\"\"Mapping of param names to static param value for an operation\"\"\"\n        return {\n            param.name: param.value\n            for param in operation_model.static_context_parameters\n        }\n\n    @instance_cache\n    def _get_dynamic_context_params(self, operation_model):\n        \"\"\"Mapping of param names to member names for an operation\"\"\"\n        return {\n            param.name: param.member_name\n            for param in operation_model.context_parameters\n        }\n\n    @instance_cache\n    def _get_client_context_params(self):\n        \"\"\"Mapping of param names to client configuration variable\"\"\"\n        return {\n            param.name: xform_name(param.name)\n            for param in self._service_model.client_context_parameters\n        }\n\n    def _get_customized_builtins(\n        self, operation_model, call_args, request_context\n    ):\n        service_id = self._service_model.service_id.hyphenize()\n        customized_builtins = copy.copy(self._builtins)\n        # Handlers are expected to modify the builtins dict in place.\n        self._event_emitter.emit(\n            'before-endpoint-resolution.%s' % service_id,\n            builtins=customized_builtins,\n            model=operation_model,\n            params=call_args,\n            context=request_context,\n        )\n        return customized_builtins\n\n    def auth_schemes_to_signing_ctx(self, auth_schemes):\n        \"\"\"Convert an Endpoint's authSchemes property to a signing_context dict\n\n        :type auth_schemes: list\n        :param auth_schemes: A list of dictionaries taken from the\n            ``authSchemes`` property of an Endpoint object returned by\n            ``EndpointProvider``.\n\n        :rtype: str, dict\n        :return: Tuple of auth type string (to be used in\n            ``request_context['auth_type']``) and signing context dict (for use\n            in ``request_context['signing']``).\n        \"\"\"\n        if not isinstance(auth_schemes, list) or len(auth_schemes) == 0:\n            raise TypeError(\"auth_schemes must be a non-empty list.\")\n\n        LOG.debug(\n            'Selecting from endpoint provider\\'s list of auth schemes: %s. '\n            'User selected auth scheme is: \"%s\"',\n            ', '.join([f'\"{s.get(\"name\")}\"' for s in auth_schemes]),\n            self._requested_auth_scheme,\n        )\n\n        if self._requested_auth_scheme == UNSIGNED:\n            return 'none', {}\n\n        auth_schemes = [\n            {**scheme, 'name': self._strip_sig_prefix(scheme['name'])}\n            for scheme in auth_schemes\n        ]\n        if self._requested_auth_scheme is not None:\n            try:\n                # Use the first scheme that matches the requested scheme,\n                # after accounting for naming differences between botocore and\n                # endpoint rulesets. Keep the requested name.\n                name, scheme = next(\n                    (self._requested_auth_scheme, s)\n                    for s in auth_schemes\n                    if self._does_botocore_authname_match_ruleset_authname(\n                        self._requested_auth_scheme, s['name']\n                    )\n                )\n            except StopIteration:\n                # For legacy signers, no match will be found. Do not raise an\n                # exception, instead default to the logic in botocore\n                # customizations.\n                return None, {}\n        else:\n            try:\n                name, scheme = next(\n                    (s['name'], s)\n                    for s in auth_schemes\n                    if s['name'] in AUTH_TYPE_MAPS\n                )\n            except StopIteration:\n                # If no auth scheme was specifically requested and an\n                # authSchemes list is present in the Endpoint object but none\n                # of the entries are supported, raise an exception.\n                fixable_with_crt = False\n                auth_type_options = [s['name'] for s in auth_schemes]\n                if not HAS_CRT:\n                    fixable_with_crt = any(\n                        scheme in CRT_SUPPORTED_AUTH_TYPES\n                        for scheme in auth_type_options\n                    )\n\n                if fixable_with_crt:\n                    raise MissingDependencyException(\n                        msg='This operation requires an additional dependency.'\n                        ' Use pip install botocore[crt] before proceeding.'\n                    )\n                else:\n                    raise UnknownSignatureVersionError(\n                        signature_version=', '.join(auth_type_options)\n                    )\n\n        signing_context = {}\n        if 'signingRegion' in scheme:\n            signing_context['region'] = scheme['signingRegion']\n        elif 'signingRegionSet' in scheme:\n            if len(scheme['signingRegionSet']) > 0:\n                signing_context['region'] = scheme['signingRegionSet'][0]\n        if 'signingName' in scheme:\n            signing_context.update(signing_name=scheme['signingName'])\n        if 'disableDoubleEncoding' in scheme:\n            signing_context['disableDoubleEncoding'] = ensure_boolean(\n                scheme['disableDoubleEncoding']\n            )\n\n        LOG.debug(\n            'Selected auth type \"%s\" as \"%s\" with signing context params: %s',\n            scheme['name'],  # original name without \"sig\"\n            name,  # chosen name can differ when `signature_version` is set\n            signing_context,\n        )\n        return name, signing_context\n\n    def _strip_sig_prefix(self, auth_name):\n        \"\"\"Normalize auth type names by removing any \"sig\" prefix\"\"\"\n        return auth_name[3:] if auth_name.startswith('sig') else auth_name\n\n    def _does_botocore_authname_match_ruleset_authname(self, botoname, rsname):\n        \"\"\"\n        Whether a valid string provided as signature_version parameter for\n        client construction refers to the same auth methods as a string\n        returned by the endpoint ruleset provider. This accounts for:\n\n        * The ruleset prefixes auth names with \"sig\"\n        * The s3 and s3control rulesets don't distinguish between v4[a] and\n          s3v4[a] signers\n        * The v2, v3, and HMAC v1 based signers (s3, s3-*) are botocore legacy\n          features and do not exist in the rulesets\n        * Only characters up to the first dash are considered\n\n        Example matches:\n        * v4, sigv4\n        * v4, v4\n        * s3v4, sigv4\n        * s3v7, sigv7 (hypothetical example)\n        * s3v4a, sigv4a\n        * s3v4-query, sigv4\n\n        Example mismatches:\n        * v4a, sigv4\n        * s3, sigv4\n        * s3-presign-post, sigv4\n        \"\"\"\n        rsname = self._strip_sig_prefix(rsname)\n        botoname = botoname.split('-')[0]\n        if botoname != 's3' and botoname.startswith('s3'):\n            botoname = botoname[2:]\n        return rsname == botoname\n\n    def ruleset_error_to_botocore_exception(self, ruleset_exception, params):\n        \"\"\"Attempts to translate ruleset errors to pre-existing botocore\n        exception types by string matching exception strings.\n        \"\"\"\n        msg = ruleset_exception.kwargs.get('msg')\n        if msg is None:\n            return\n\n        if msg.startswith('Invalid region in ARN: '):\n            # Example message:\n            # \"Invalid region in ARN: `us-we$t-2` (invalid DNS name)\"\n            try:\n                label = msg.split('`')[1]\n            except IndexError:\n                label = msg\n            return InvalidHostLabelError(label=label)\n\n        service_name = self._service_model.service_name\n        if service_name == 's3':\n            if (\n                msg == 'S3 Object Lambda does not support S3 Accelerate'\n                or msg == 'Accelerate cannot be used with FIPS'\n            ):\n                return UnsupportedS3ConfigurationError(msg=msg)\n            if (\n                msg.startswith('S3 Outposts does not support')\n                or msg.startswith('S3 MRAP does not support')\n                or msg.startswith('S3 Object Lambda does not support')\n                or msg.startswith('Access Points do not support')\n                or msg.startswith('Invalid configuration:')\n                or msg.startswith('Client was configured for partition')\n            ):\n                return UnsupportedS3AccesspointConfigurationError(msg=msg)\n            if msg.lower().startswith('invalid arn:'):\n                return ParamValidationError(report=msg)\n        if service_name == 's3control':\n            if msg.startswith('Invalid ARN:'):\n                arn = params.get('Bucket')\n                return UnsupportedS3ControlArnError(arn=arn, msg=msg)\n            if msg.startswith('Invalid configuration:') or msg.startswith(\n                'Client was configured for partition'\n            ):\n                return UnsupportedS3ControlConfigurationError(msg=msg)\n            if msg == \"AccountId is required but not set\":\n                return ParamValidationError(report=msg)\n        if service_name == 'events':\n            if msg.startswith(\n                'Invalid Configuration: FIPS is not supported with '\n                'EventBridge multi-region endpoints.'\n            ):\n                return InvalidEndpointConfigurationError(msg=msg)\n            if msg == 'EndpointId must be a valid host label.':\n                return InvalidEndpointConfigurationError(msg=msg)\n        return None\n", "botocore/exceptions.py": "# Copyright (c) 2012-2013 Mitch Garnaat http://garnaat.org/\n# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nfrom botocore.vendored import requests\nfrom botocore.vendored.requests.packages import urllib3\n\n\ndef _exception_from_packed_args(exception_cls, args=None, kwargs=None):\n    # This is helpful for reducing Exceptions that only accept kwargs as\n    # only positional arguments can be provided for __reduce__\n    # Ideally, this would also be a class method on the BotoCoreError\n    # but instance methods cannot be pickled.\n    if args is None:\n        args = ()\n    if kwargs is None:\n        kwargs = {}\n    return exception_cls(*args, **kwargs)\n\n\nclass BotoCoreError(Exception):\n    \"\"\"\n    The base exception class for BotoCore exceptions.\n\n    :ivar msg: The descriptive message associated with the error.\n    \"\"\"\n\n    fmt = 'An unspecified error occurred'\n\n    def __init__(self, **kwargs):\n        msg = self.fmt.format(**kwargs)\n        Exception.__init__(self, msg)\n        self.kwargs = kwargs\n\n    def __reduce__(self):\n        return _exception_from_packed_args, (self.__class__, None, self.kwargs)\n\n\nclass DataNotFoundError(BotoCoreError):\n    \"\"\"\n    The data associated with a particular path could not be loaded.\n\n    :ivar data_path: The data path that the user attempted to load.\n    \"\"\"\n\n    fmt = 'Unable to load data for: {data_path}'\n\n\nclass UnknownServiceError(DataNotFoundError):\n    \"\"\"Raised when trying to load data for an unknown service.\n\n    :ivar service_name: The name of the unknown service.\n\n    \"\"\"\n\n    fmt = (\n        \"Unknown service: '{service_name}'. Valid service names are: \"\n        \"{known_service_names}\"\n    )\n\n\nclass UnknownRegionError(BotoCoreError):\n    \"\"\"Raised when trying to load data for an unknown region.\n\n    :ivar region_name: The name of the unknown region.\n\n    \"\"\"\n\n    fmt = \"Unknown region: '{region_name}'. {error_msg}\"\n\n\nclass ApiVersionNotFoundError(BotoCoreError):\n    \"\"\"\n    The data associated with either the API version or a compatible one\n    could not be loaded.\n\n    :ivar data_path: The data path that the user attempted to load.\n    :ivar api_version: The API version that the user attempted to load.\n    \"\"\"\n\n    fmt = 'Unable to load data {data_path} for: {api_version}'\n\n\nclass HTTPClientError(BotoCoreError):\n    fmt = 'An HTTP Client raised an unhandled exception: {error}'\n\n    def __init__(self, request=None, response=None, **kwargs):\n        self.request = request\n        self.response = response\n        super().__init__(**kwargs)\n\n    def __reduce__(self):\n        return _exception_from_packed_args, (\n            self.__class__,\n            (self.request, self.response),\n            self.kwargs,\n        )\n\n\nclass ConnectionError(BotoCoreError):\n    fmt = 'An HTTP Client failed to establish a connection: {error}'\n\n\nclass InvalidIMDSEndpointError(BotoCoreError):\n    fmt = 'Invalid endpoint EC2 Instance Metadata endpoint: {endpoint}'\n\n\nclass InvalidIMDSEndpointModeError(BotoCoreError):\n    fmt = (\n        'Invalid EC2 Instance Metadata endpoint mode: {mode}'\n        ' Valid endpoint modes (case-insensitive): {valid_modes}.'\n    )\n\n\nclass EndpointConnectionError(ConnectionError):\n    fmt = 'Could not connect to the endpoint URL: \"{endpoint_url}\"'\n\n\nclass SSLError(ConnectionError, requests.exceptions.SSLError):\n    fmt = 'SSL validation failed for {endpoint_url} {error}'\n\n\nclass ConnectionClosedError(HTTPClientError):\n    fmt = (\n        'Connection was closed before we received a valid response '\n        'from endpoint URL: \"{endpoint_url}\".'\n    )\n\n\nclass ReadTimeoutError(\n    HTTPClientError,\n    requests.exceptions.ReadTimeout,\n    urllib3.exceptions.ReadTimeoutError,\n):\n    fmt = 'Read timeout on endpoint URL: \"{endpoint_url}\"'\n\n\nclass ConnectTimeoutError(ConnectionError, requests.exceptions.ConnectTimeout):\n    fmt = 'Connect timeout on endpoint URL: \"{endpoint_url}\"'\n\n\nclass ProxyConnectionError(ConnectionError, requests.exceptions.ProxyError):\n    fmt = 'Failed to connect to proxy URL: \"{proxy_url}\"'\n\n\nclass ResponseStreamingError(HTTPClientError):\n    fmt = 'An error occurred while reading from response stream: {error}'\n\n\nclass NoCredentialsError(BotoCoreError):\n    \"\"\"\n    No credentials could be found.\n    \"\"\"\n\n    fmt = 'Unable to locate credentials'\n\n\nclass NoAuthTokenError(BotoCoreError):\n    \"\"\"\n    No authorization token could be found.\n    \"\"\"\n\n    fmt = 'Unable to locate authorization token'\n\n\nclass TokenRetrievalError(BotoCoreError):\n    \"\"\"\n    Error attempting to retrieve a token from a remote source.\n\n    :ivar provider: The name of the token provider.\n    :ivar error_msg: The msg explaining why the token could not be retrieved.\n\n    \"\"\"\n\n    fmt = 'Error when retrieving token from {provider}: {error_msg}'\n\n\nclass PartialCredentialsError(BotoCoreError):\n    \"\"\"\n    Only partial credentials were found.\n\n    :ivar cred_var: The missing credential variable name.\n\n    \"\"\"\n\n    fmt = 'Partial credentials found in {provider}, missing: {cred_var}'\n\n\nclass CredentialRetrievalError(BotoCoreError):\n    \"\"\"\n    Error attempting to retrieve credentials from a remote source.\n\n    :ivar provider: The name of the credential provider.\n    :ivar error_msg: The msg explaining why credentials could not be\n        retrieved.\n\n    \"\"\"\n\n    fmt = 'Error when retrieving credentials from {provider}: {error_msg}'\n\n\nclass UnknownSignatureVersionError(BotoCoreError):\n    \"\"\"\n    Requested Signature Version is not known.\n\n    :ivar signature_version: The name of the requested signature version.\n    \"\"\"\n\n    fmt = 'Unknown Signature Version: {signature_version}.'\n\n\nclass ServiceNotInRegionError(BotoCoreError):\n    \"\"\"\n    The service is not available in requested region.\n\n    :ivar service_name: The name of the service.\n    :ivar region_name: The name of the region.\n    \"\"\"\n\n    fmt = 'Service {service_name} not available in region {region_name}'\n\n\nclass BaseEndpointResolverError(BotoCoreError):\n    \"\"\"Base error for endpoint resolving errors.\n\n    Should never be raised directly, but clients can catch\n    this exception if they want to generically handle any errors\n    during the endpoint resolution process.\n\n    \"\"\"\n\n\nclass NoRegionError(BaseEndpointResolverError):\n    \"\"\"No region was specified.\"\"\"\n\n    fmt = 'You must specify a region.'\n\n\nclass EndpointVariantError(BaseEndpointResolverError):\n    \"\"\"\n    Could not construct modeled endpoint variant.\n\n    :ivar error_msg: The message explaining why the modeled endpoint variant\n        is unable to be constructed.\n\n    \"\"\"\n\n    fmt = (\n        'Unable to construct a modeled endpoint with the following '\n        'variant(s) {tags}: '\n    )\n\n\nclass UnknownEndpointError(BaseEndpointResolverError, ValueError):\n    \"\"\"\n    Could not construct an endpoint.\n\n    :ivar service_name: The name of the service.\n    :ivar region_name: The name of the region.\n    \"\"\"\n\n    fmt = (\n        'Unable to construct an endpoint for '\n        '{service_name} in region {region_name}'\n    )\n\n\nclass UnknownFIPSEndpointError(BaseEndpointResolverError):\n    \"\"\"\n    Could not construct a FIPS endpoint.\n\n    :ivar service_name: The name of the service.\n    :ivar region_name: The name of the region.\n    \"\"\"\n\n    fmt = (\n        'The provided FIPS pseudo-region \"{region_name}\" is not known for '\n        'the service \"{service_name}\". A FIPS compliant endpoint cannot be '\n        'constructed.'\n    )\n\n\nclass ProfileNotFound(BotoCoreError):\n    \"\"\"\n    The specified configuration profile was not found in the\n    configuration file.\n\n    :ivar profile: The name of the profile the user attempted to load.\n    \"\"\"\n\n    fmt = 'The config profile ({profile}) could not be found'\n\n\nclass ConfigParseError(BotoCoreError):\n    \"\"\"\n    The configuration file could not be parsed.\n\n    :ivar path: The path to the configuration file.\n    \"\"\"\n\n    fmt = 'Unable to parse config file: {path}'\n\n\nclass ConfigNotFound(BotoCoreError):\n    \"\"\"\n    The specified configuration file could not be found.\n\n    :ivar path: The path to the configuration file.\n    \"\"\"\n\n    fmt = 'The specified config file ({path}) could not be found.'\n\n\nclass MissingParametersError(BotoCoreError):\n    \"\"\"\n    One or more required parameters were not supplied.\n\n    :ivar object: The object that has missing parameters.\n        This can be an operation or a parameter (in the\n        case of inner params).  The str() of this object\n        will be used so it doesn't need to implement anything\n        other than str().\n    :ivar missing: The names of the missing parameters.\n    \"\"\"\n\n    fmt = (\n        'The following required parameters are missing for '\n        '{object_name}: {missing}'\n    )\n\n\nclass ValidationError(BotoCoreError):\n    \"\"\"\n    An exception occurred validating parameters.\n\n    Subclasses must accept a ``value`` and ``param``\n    argument in their ``__init__``.\n\n    :ivar value: The value that was being validated.\n    :ivar param: The parameter that failed validation.\n    :ivar type_name: The name of the underlying type.\n    \"\"\"\n\n    fmt = \"Invalid value ('{value}') for param {param} \" \"of type {type_name} \"\n\n\nclass ParamValidationError(BotoCoreError):\n    fmt = 'Parameter validation failed:\\n{report}'\n\n\n# These exceptions subclass from ValidationError so that code\n# can just 'except ValidationError' to catch any possibly validation\n# error.\nclass UnknownKeyError(ValidationError):\n    \"\"\"\n    Unknown key in a struct parameter.\n\n    :ivar value: The value that was being checked.\n    :ivar param: The name of the parameter.\n    :ivar choices: The valid choices the value can be.\n    \"\"\"\n\n    fmt = (\n        \"Unknown key '{value}' for param '{param}'.  Must be one \"\n        \"of: {choices}\"\n    )\n\n\nclass RangeError(ValidationError):\n    \"\"\"\n    A parameter value was out of the valid range.\n\n    :ivar value: The value that was being checked.\n    :ivar param: The parameter that failed validation.\n    :ivar min_value: The specified minimum value.\n    :ivar max_value: The specified maximum value.\n    \"\"\"\n\n    fmt = (\n        'Value out of range for param {param}: '\n        '{min_value} <= {value} <= {max_value}'\n    )\n\n\nclass UnknownParameterError(ValidationError):\n    \"\"\"\n    Unknown top level parameter.\n\n    :ivar name: The name of the unknown parameter.\n    :ivar operation: The name of the operation.\n    :ivar choices: The valid choices the parameter name can be.\n    \"\"\"\n\n    fmt = (\n        \"Unknown parameter '{name}' for operation {operation}.  Must be one \"\n        \"of: {choices}\"\n    )\n\n\nclass InvalidRegionError(ValidationError, ValueError):\n    \"\"\"\n    Invalid region_name provided to client or resource.\n\n    :ivar region_name: region_name that was being validated.\n    \"\"\"\n\n    fmt = \"Provided region_name '{region_name}' doesn't match a supported format.\"\n\n\nclass AliasConflictParameterError(ValidationError):\n    \"\"\"\n    Error when an alias is provided for a parameter as well as the original.\n\n    :ivar original: The name of the original parameter.\n    :ivar alias: The name of the alias\n    :ivar operation: The name of the operation.\n    \"\"\"\n\n    fmt = (\n        \"Parameter '{original}' and its alias '{alias}' were provided \"\n        \"for operation {operation}.  Only one of them may be used.\"\n    )\n\n\nclass UnknownServiceStyle(BotoCoreError):\n    \"\"\"\n    Unknown style of service invocation.\n\n    :ivar service_style: The style requested.\n    \"\"\"\n\n    fmt = 'The service style ({service_style}) is not understood.'\n\n\nclass PaginationError(BotoCoreError):\n    fmt = 'Error during pagination: {message}'\n\n\nclass OperationNotPageableError(BotoCoreError):\n    fmt = 'Operation cannot be paginated: {operation_name}'\n\n\nclass ChecksumError(BotoCoreError):\n    \"\"\"The expected checksum did not match the calculated checksum.\"\"\"\n\n    fmt = (\n        'Checksum {checksum_type} failed, expected checksum '\n        '{expected_checksum} did not match calculated checksum '\n        '{actual_checksum}.'\n    )\n\n\nclass UnseekableStreamError(BotoCoreError):\n    \"\"\"Need to seek a stream, but stream does not support seeking.\"\"\"\n\n    fmt = (\n        'Need to rewind the stream {stream_object}, but stream '\n        'is not seekable.'\n    )\n\n\nclass WaiterError(BotoCoreError):\n    \"\"\"Waiter failed to reach desired state.\"\"\"\n\n    fmt = 'Waiter {name} failed: {reason}'\n\n    def __init__(self, name, reason, last_response):\n        super().__init__(name=name, reason=reason)\n        self.last_response = last_response\n\n\nclass IncompleteReadError(BotoCoreError):\n    \"\"\"HTTP response did not return expected number of bytes.\"\"\"\n\n    fmt = (\n        '{actual_bytes} read, but total bytes ' 'expected is {expected_bytes}.'\n    )\n\n\nclass InvalidExpressionError(BotoCoreError):\n    \"\"\"Expression is either invalid or too complex.\"\"\"\n\n    fmt = 'Invalid expression {expression}: Only dotted lookups are supported.'\n\n\nclass UnknownCredentialError(BotoCoreError):\n    \"\"\"Tried to insert before/after an unregistered credential type.\"\"\"\n\n    fmt = 'Credential named {name} not found.'\n\n\nclass WaiterConfigError(BotoCoreError):\n    \"\"\"Error when processing waiter configuration.\"\"\"\n\n    fmt = 'Error processing waiter config: {error_msg}'\n\n\nclass UnknownClientMethodError(BotoCoreError):\n    \"\"\"Error when trying to access a method on a client that does not exist.\"\"\"\n\n    fmt = 'Client does not have method: {method_name}'\n\n\nclass UnsupportedSignatureVersionError(BotoCoreError):\n    \"\"\"Error when trying to use an unsupported Signature Version.\"\"\"\n\n    fmt = 'Signature version is not supported: {signature_version}'\n\n\nclass ClientError(Exception):\n    MSG_TEMPLATE = (\n        'An error occurred ({error_code}) when calling the {operation_name} '\n        'operation{retry_info}: {error_message}'\n    )\n\n    def __init__(self, error_response, operation_name):\n        retry_info = self._get_retry_info(error_response)\n        error = error_response.get('Error', {})\n        msg = self.MSG_TEMPLATE.format(\n            error_code=error.get('Code', 'Unknown'),\n            error_message=error.get('Message', 'Unknown'),\n            operation_name=operation_name,\n            retry_info=retry_info,\n        )\n        super().__init__(msg)\n        self.response = error_response\n        self.operation_name = operation_name\n\n    def _get_retry_info(self, response):\n        retry_info = ''\n        if 'ResponseMetadata' in response:\n            metadata = response['ResponseMetadata']\n            if metadata.get('MaxAttemptsReached', False):\n                if 'RetryAttempts' in metadata:\n                    retry_info = (\n                        f\" (reached max retries: {metadata['RetryAttempts']})\"\n                    )\n        return retry_info\n\n    def __reduce__(self):\n        # Subclasses of ClientError's are dynamically generated and\n        # cannot be pickled unless they are attributes of a\n        # module. So at the very least return a ClientError back.\n        return ClientError, (self.response, self.operation_name)\n\n\nclass EventStreamError(ClientError):\n    pass\n\n\nclass UnsupportedTLSVersionWarning(Warning):\n    \"\"\"Warn when an openssl version that uses TLS 1.2 is required\"\"\"\n\n    pass\n\n\nclass ImminentRemovalWarning(Warning):\n    pass\n\n\nclass InvalidDNSNameError(BotoCoreError):\n    \"\"\"Error when virtual host path is forced on a non-DNS compatible bucket\"\"\"\n\n    fmt = (\n        'Bucket named {bucket_name} is not DNS compatible. Virtual '\n        'hosted-style addressing cannot be used. The addressing style '\n        'can be configured by removing the addressing_style value '\n        'or setting that value to \\'path\\' or \\'auto\\' in the AWS Config '\n        'file or in the botocore.client.Config object.'\n    )\n\n\nclass InvalidS3AddressingStyleError(BotoCoreError):\n    \"\"\"Error when an invalid path style is specified\"\"\"\n\n    fmt = (\n        'S3 addressing style {s3_addressing_style} is invalid. Valid options '\n        'are: \\'auto\\', \\'virtual\\', and \\'path\\''\n    )\n\n\nclass UnsupportedS3ArnError(BotoCoreError):\n    \"\"\"Error when S3 ARN provided to Bucket parameter is not supported\"\"\"\n\n    fmt = (\n        'S3 ARN {arn} provided to \"Bucket\" parameter is invalid. Only '\n        'ARNs for S3 access-points are supported.'\n    )\n\n\nclass UnsupportedS3ControlArnError(BotoCoreError):\n    \"\"\"Error when S3 ARN provided to S3 control parameter is not supported\"\"\"\n\n    fmt = 'S3 ARN \"{arn}\" provided is invalid for this operation. {msg}'\n\n\nclass InvalidHostLabelError(BotoCoreError):\n    \"\"\"Error when an invalid host label would be bound to an endpoint\"\"\"\n\n    fmt = (\n        'Invalid host label to be bound to the hostname of the endpoint: '\n        '\"{label}\".'\n    )\n\n\nclass UnsupportedOutpostResourceError(BotoCoreError):\n    \"\"\"Error when S3 Outpost ARN provided to Bucket parameter is incomplete\"\"\"\n\n    fmt = (\n        'S3 Outpost ARN resource \"{resource_name}\" provided to \"Bucket\" '\n        'parameter is invalid. Only ARNs for S3 Outpost arns with an '\n        'access-point sub-resource are supported.'\n    )\n\n\nclass UnsupportedS3ConfigurationError(BotoCoreError):\n    \"\"\"Error when an unsupported configuration is used with access-points\"\"\"\n\n    fmt = 'Unsupported configuration when using S3: {msg}'\n\n\nclass UnsupportedS3AccesspointConfigurationError(BotoCoreError):\n    \"\"\"Error when an unsupported configuration is used with access-points\"\"\"\n\n    fmt = 'Unsupported configuration when using S3 access-points: {msg}'\n\n\nclass InvalidEndpointDiscoveryConfigurationError(BotoCoreError):\n    \"\"\"Error when invalid value supplied for endpoint_discovery_enabled\"\"\"\n\n    fmt = (\n        'Unsupported configuration value for endpoint_discovery_enabled. '\n        'Expected one of (\"true\", \"false\", \"auto\") but got {config_value}.'\n    )\n\n\nclass UnsupportedS3ControlConfigurationError(BotoCoreError):\n    \"\"\"Error when an unsupported configuration is used with S3 Control\"\"\"\n\n    fmt = 'Unsupported configuration when using S3 Control: {msg}'\n\n\nclass InvalidRetryConfigurationError(BotoCoreError):\n    \"\"\"Error when invalid retry configuration is specified\"\"\"\n\n    fmt = (\n        'Cannot provide retry configuration for \"{retry_config_option}\". '\n        'Valid retry configuration options are: {valid_options}'\n    )\n\n\nclass InvalidMaxRetryAttemptsError(InvalidRetryConfigurationError):\n    \"\"\"Error when invalid retry configuration is specified\"\"\"\n\n    fmt = (\n        'Value provided to \"max_attempts\": {provided_max_attempts} must '\n        'be an integer greater than or equal to {min_value}.'\n    )\n\n\nclass InvalidRetryModeError(InvalidRetryConfigurationError):\n    \"\"\"Error when invalid retry mode configuration is specified\"\"\"\n\n    fmt = (\n        'Invalid value provided to \"mode\": \"{provided_retry_mode}\" must '\n        'be one of: {valid_modes}'\n    )\n\n\nclass InvalidS3UsEast1RegionalEndpointConfigError(BotoCoreError):\n    \"\"\"Error for invalid s3 us-east-1 regional endpoints configuration\"\"\"\n\n    fmt = (\n        'S3 us-east-1 regional endpoint option '\n        '{s3_us_east_1_regional_endpoint_config} is '\n        'invalid. Valid options are: \"legacy\", \"regional\"'\n    )\n\n\nclass InvalidSTSRegionalEndpointsConfigError(BotoCoreError):\n    \"\"\"Error when invalid sts regional endpoints configuration is specified\"\"\"\n\n    fmt = (\n        'STS regional endpoints option {sts_regional_endpoints_config} is '\n        'invalid. Valid options are: \"legacy\", \"regional\"'\n    )\n\n\nclass StubResponseError(BotoCoreError):\n    fmt = (\n        'Error getting response stub for operation {operation_name}: {reason}'\n    )\n\n\nclass StubAssertionError(StubResponseError, AssertionError):\n    pass\n\n\nclass UnStubbedResponseError(StubResponseError):\n    pass\n\n\nclass InvalidConfigError(BotoCoreError):\n    fmt = '{error_msg}'\n\n\nclass InfiniteLoopConfigError(InvalidConfigError):\n    fmt = (\n        'Infinite loop in credential configuration detected. Attempting to '\n        'load from profile {source_profile} which has already been visited. '\n        'Visited profiles: {visited_profiles}'\n    )\n\n\nclass RefreshWithMFAUnsupportedError(BotoCoreError):\n    fmt = 'Cannot refresh credentials: MFA token required.'\n\n\nclass MD5UnavailableError(BotoCoreError):\n    fmt = \"This system does not support MD5 generation.\"\n\n\nclass MissingDependencyException(BotoCoreError):\n    fmt = \"Missing Dependency: {msg}\"\n\n\nclass MetadataRetrievalError(BotoCoreError):\n    fmt = \"Error retrieving metadata: {error_msg}\"\n\n\nclass UndefinedModelAttributeError(Exception):\n    pass\n\n\nclass MissingServiceIdError(UndefinedModelAttributeError):\n    fmt = (\n        \"The model being used for the service {service_name} is missing the \"\n        \"serviceId metadata property, which is required.\"\n    )\n\n    def __init__(self, **kwargs):\n        msg = self.fmt.format(**kwargs)\n        Exception.__init__(self, msg)\n        self.kwargs = kwargs\n\n\nclass SSOError(BotoCoreError):\n    fmt = (\n        \"An unspecified error happened when resolving AWS credentials or an \"\n        \"access token from SSO.\"\n    )\n\n\nclass SSOTokenLoadError(SSOError):\n    fmt = \"Error loading SSO Token: {error_msg}\"\n\n\nclass UnauthorizedSSOTokenError(SSOError):\n    fmt = (\n        \"The SSO session associated with this profile has expired or is \"\n        \"otherwise invalid. To refresh this SSO session run aws sso login \"\n        \"with the corresponding profile.\"\n    )\n\n\nclass CapacityNotAvailableError(BotoCoreError):\n    fmt = 'Insufficient request capacity available.'\n\n\nclass InvalidProxiesConfigError(BotoCoreError):\n    fmt = 'Invalid configuration value(s) provided for proxies_config.'\n\n\nclass InvalidDefaultsMode(BotoCoreError):\n    fmt = (\n        'Client configured with invalid defaults mode: {mode}. '\n        'Valid defaults modes include: {valid_modes}.'\n    )\n\n\nclass AwsChunkedWrapperError(BotoCoreError):\n    fmt = '{error_msg}'\n\n\nclass FlexibleChecksumError(BotoCoreError):\n    fmt = '{error_msg}'\n\n\nclass InvalidEndpointConfigurationError(BotoCoreError):\n    fmt = 'Invalid endpoint configuration: {msg}'\n\n\nclass EndpointProviderError(BotoCoreError):\n    \"\"\"Base error for the EndpointProvider class\"\"\"\n\n    fmt = '{msg}'\n\n\nclass EndpointResolutionError(EndpointProviderError):\n    \"\"\"Error when input parameters resolve to an error rule\"\"\"\n\n    fmt = '{msg}'\n\n\nclass UnknownEndpointResolutionBuiltInName(EndpointProviderError):\n    fmt = 'Unknown builtin variable name: {name}'\n", "botocore/retryhandler.py": "# Copyright (c) 2012-2013 Mitch Garnaat http://garnaat.org/\n# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport functools\nimport logging\nimport random\nfrom binascii import crc32\n\nfrom botocore.exceptions import (\n    ChecksumError,\n    ConnectionClosedError,\n    ConnectionError,\n    EndpointConnectionError,\n    ReadTimeoutError,\n)\n\nlogger = logging.getLogger(__name__)\n# The only supported error for now is GENERAL_CONNECTION_ERROR\n# which maps to requests generic ConnectionError.  If we're able\n# to get more specific exceptions from requests we can update\n# this mapping with more specific exceptions.\nEXCEPTION_MAP = {\n    'GENERAL_CONNECTION_ERROR': [\n        ConnectionError,\n        ConnectionClosedError,\n        ReadTimeoutError,\n        EndpointConnectionError,\n    ],\n}\n\n\ndef delay_exponential(base, growth_factor, attempts):\n    \"\"\"Calculate time to sleep based on exponential function.\n\n    The format is::\n\n        base * growth_factor ^ (attempts - 1)\n\n    If ``base`` is set to 'rand' then a random number between\n    0 and 1 will be used as the base.\n    Base must be greater than 0, otherwise a ValueError will be\n    raised.\n\n    \"\"\"\n    if base == 'rand':\n        base = random.random()\n    elif base <= 0:\n        raise ValueError(\n            f\"The 'base' param must be greater than 0, got: {base}\"\n        )\n    time_to_sleep = base * (growth_factor ** (attempts - 1))\n    return time_to_sleep\n\n\ndef create_exponential_delay_function(base, growth_factor):\n    \"\"\"Create an exponential delay function based on the attempts.\n\n    This is used so that you only have to pass it the attempts\n    parameter to calculate the delay.\n\n    \"\"\"\n    return functools.partial(\n        delay_exponential, base=base, growth_factor=growth_factor\n    )\n\n\ndef create_retry_handler(config, operation_name=None):\n    checker = create_checker_from_retry_config(\n        config, operation_name=operation_name\n    )\n    action = create_retry_action_from_config(\n        config, operation_name=operation_name\n    )\n    return RetryHandler(checker=checker, action=action)\n\n\ndef create_retry_action_from_config(config, operation_name=None):\n    # The spec has the possibility of supporting per policy\n    # actions, but right now, we assume this comes from the\n    # default section, which means that delay functions apply\n    # for every policy in the retry config (per service).\n    delay_config = config['__default__']['delay']\n    if delay_config['type'] == 'exponential':\n        return create_exponential_delay_function(\n            base=delay_config['base'],\n            growth_factor=delay_config['growth_factor'],\n        )\n\n\ndef create_checker_from_retry_config(config, operation_name=None):\n    checkers = []\n    max_attempts = None\n    retryable_exceptions = []\n    if '__default__' in config:\n        policies = config['__default__'].get('policies', [])\n        max_attempts = config['__default__']['max_attempts']\n        for key in policies:\n            current_config = policies[key]\n            checkers.append(_create_single_checker(current_config))\n            retry_exception = _extract_retryable_exception(current_config)\n            if retry_exception is not None:\n                retryable_exceptions.extend(retry_exception)\n    if operation_name is not None and config.get(operation_name) is not None:\n        operation_policies = config[operation_name]['policies']\n        for key in operation_policies:\n            checkers.append(_create_single_checker(operation_policies[key]))\n            retry_exception = _extract_retryable_exception(\n                operation_policies[key]\n            )\n            if retry_exception is not None:\n                retryable_exceptions.extend(retry_exception)\n    if len(checkers) == 1:\n        # Don't need to use a MultiChecker\n        return MaxAttemptsDecorator(checkers[0], max_attempts=max_attempts)\n    else:\n        multi_checker = MultiChecker(checkers)\n        return MaxAttemptsDecorator(\n            multi_checker,\n            max_attempts=max_attempts,\n            retryable_exceptions=tuple(retryable_exceptions),\n        )\n\n\ndef _create_single_checker(config):\n    if 'response' in config['applies_when']:\n        return _create_single_response_checker(\n            config['applies_when']['response']\n        )\n    elif 'socket_errors' in config['applies_when']:\n        return ExceptionRaiser()\n\n\ndef _create_single_response_checker(response):\n    if 'service_error_code' in response:\n        checker = ServiceErrorCodeChecker(\n            status_code=response['http_status_code'],\n            error_code=response['service_error_code'],\n        )\n    elif 'http_status_code' in response:\n        checker = HTTPStatusCodeChecker(\n            status_code=response['http_status_code']\n        )\n    elif 'crc32body' in response:\n        checker = CRC32Checker(header=response['crc32body'])\n    else:\n        # TODO: send a signal.\n        raise ValueError(\"Unknown retry policy\")\n    return checker\n\n\ndef _extract_retryable_exception(config):\n    applies_when = config['applies_when']\n    if 'crc32body' in applies_when.get('response', {}):\n        return [ChecksumError]\n    elif 'socket_errors' in applies_when:\n        exceptions = []\n        for name in applies_when['socket_errors']:\n            exceptions.extend(EXCEPTION_MAP[name])\n        return exceptions\n\n\nclass RetryHandler:\n    \"\"\"Retry handler.\n\n    The retry handler takes two params, ``checker`` object\n    and an ``action`` object.\n\n    The ``checker`` object must be a callable object and based on a response\n    and an attempt number, determines whether or not sufficient criteria for\n    a retry has been met.  If this is the case then the ``action`` object\n    (which also is a callable) determines what needs to happen in the event\n    of a retry.\n\n    \"\"\"\n\n    def __init__(self, checker, action):\n        self._checker = checker\n        self._action = action\n\n    def __call__(self, attempts, response, caught_exception, **kwargs):\n        \"\"\"Handler for a retry.\n\n        Intended to be hooked up to an event handler (hence the **kwargs),\n        this will process retries appropriately.\n\n        \"\"\"\n        checker_kwargs = {\n            'attempt_number': attempts,\n            'response': response,\n            'caught_exception': caught_exception,\n        }\n        if isinstance(self._checker, MaxAttemptsDecorator):\n            retries_context = kwargs['request_dict']['context'].get('retries')\n            checker_kwargs.update({'retries_context': retries_context})\n\n        if self._checker(**checker_kwargs):\n            result = self._action(attempts=attempts)\n            logger.debug(\"Retry needed, action of: %s\", result)\n            return result\n        logger.debug(\"No retry needed.\")\n\n\nclass BaseChecker:\n    \"\"\"Base class for retry checkers.\n\n    Each class is responsible for checking a single criteria that determines\n    whether or not a retry should not happen.\n\n    \"\"\"\n\n    def __call__(self, attempt_number, response, caught_exception):\n        \"\"\"Determine if retry criteria matches.\n\n        Note that either ``response`` is not None and ``caught_exception`` is\n        None or ``response`` is None and ``caught_exception`` is not None.\n\n        :type attempt_number: int\n        :param attempt_number: The total number of times we've attempted\n            to send the request.\n\n        :param response: The HTTP response (if one was received).\n\n        :type caught_exception: Exception\n        :param caught_exception: Any exception that was caught while trying to\n            send the HTTP response.\n\n        :return: True, if the retry criteria matches (and therefore a retry\n            should occur.  False if the criteria does not match.\n\n        \"\"\"\n        # The default implementation allows subclasses to not have to check\n        # whether or not response is None or not.\n        if response is not None:\n            return self._check_response(attempt_number, response)\n        elif caught_exception is not None:\n            return self._check_caught_exception(\n                attempt_number, caught_exception\n            )\n        else:\n            raise ValueError(\"Both response and caught_exception are None.\")\n\n    def _check_response(self, attempt_number, response):\n        pass\n\n    def _check_caught_exception(self, attempt_number, caught_exception):\n        pass\n\n\nclass MaxAttemptsDecorator(BaseChecker):\n    \"\"\"Allow retries up to a maximum number of attempts.\n\n    This will pass through calls to the decorated retry checker, provided\n    that the number of attempts does not exceed max_attempts.  It will\n    also catch any retryable_exceptions passed in.  Once max_attempts has\n    been exceeded, then False will be returned or the retryable_exceptions\n    that was previously being caught will be raised.\n\n    \"\"\"\n\n    def __init__(self, checker, max_attempts, retryable_exceptions=None):\n        self._checker = checker\n        self._max_attempts = max_attempts\n        self._retryable_exceptions = retryable_exceptions\n\n    def __call__(\n        self, attempt_number, response, caught_exception, retries_context\n    ):\n        if retries_context:\n            retries_context['max'] = max(\n                retries_context.get('max', 0), self._max_attempts\n            )\n\n        should_retry = self._should_retry(\n            attempt_number, response, caught_exception\n        )\n        if should_retry:\n            if attempt_number >= self._max_attempts:\n                # explicitly set MaxAttemptsReached\n                if response is not None and 'ResponseMetadata' in response[1]:\n                    response[1]['ResponseMetadata'][\n                        'MaxAttemptsReached'\n                    ] = True\n                logger.debug(\n                    \"Reached the maximum number of retry attempts: %s\",\n                    attempt_number,\n                )\n                return False\n            else:\n                return should_retry\n        else:\n            return False\n\n    def _should_retry(self, attempt_number, response, caught_exception):\n        if self._retryable_exceptions and attempt_number < self._max_attempts:\n            try:\n                return self._checker(\n                    attempt_number, response, caught_exception\n                )\n            except self._retryable_exceptions as e:\n                logger.debug(\n                    \"retry needed, retryable exception caught: %s\",\n                    e,\n                    exc_info=True,\n                )\n                return True\n        else:\n            # If we've exceeded the max attempts we just let the exception\n            # propagate if one has occurred.\n            return self._checker(attempt_number, response, caught_exception)\n\n\nclass HTTPStatusCodeChecker(BaseChecker):\n    def __init__(self, status_code):\n        self._status_code = status_code\n\n    def _check_response(self, attempt_number, response):\n        if response[0].status_code == self._status_code:\n            logger.debug(\n                \"retry needed: retryable HTTP status code received: %s\",\n                self._status_code,\n            )\n            return True\n        else:\n            return False\n\n\nclass ServiceErrorCodeChecker(BaseChecker):\n    def __init__(self, status_code, error_code):\n        self._status_code = status_code\n        self._error_code = error_code\n\n    def _check_response(self, attempt_number, response):\n        if response[0].status_code == self._status_code:\n            actual_error_code = response[1].get('Error', {}).get('Code')\n            if actual_error_code == self._error_code:\n                logger.debug(\n                    \"retry needed: matching HTTP status and error code seen: \"\n                    \"%s, %s\",\n                    self._status_code,\n                    self._error_code,\n                )\n                return True\n        return False\n\n\nclass MultiChecker(BaseChecker):\n    def __init__(self, checkers):\n        self._checkers = checkers\n\n    def __call__(self, attempt_number, response, caught_exception):\n        for checker in self._checkers:\n            checker_response = checker(\n                attempt_number, response, caught_exception\n            )\n            if checker_response:\n                return checker_response\n        return False\n\n\nclass CRC32Checker(BaseChecker):\n    def __init__(self, header):\n        # The header where the expected crc32 is located.\n        self._header_name = header\n\n    def _check_response(self, attempt_number, response):\n        http_response = response[0]\n        expected_crc = http_response.headers.get(self._header_name)\n        if expected_crc is None:\n            logger.debug(\n                \"crc32 check skipped, the %s header is not \"\n                \"in the http response.\",\n                self._header_name,\n            )\n        else:\n            actual_crc32 = crc32(response[0].content) & 0xFFFFFFFF\n            if not actual_crc32 == int(expected_crc):\n                logger.debug(\n                    \"retry needed: crc32 check failed, expected != actual: \"\n                    \"%s != %s\",\n                    int(expected_crc),\n                    actual_crc32,\n                )\n                raise ChecksumError(\n                    checksum_type='crc32',\n                    expected_checksum=int(expected_crc),\n                    actual_checksum=actual_crc32,\n                )\n\n\nclass ExceptionRaiser(BaseChecker):\n    \"\"\"Raise any caught exceptions.\n\n    This class will raise any non None ``caught_exception``.\n\n    \"\"\"\n\n    def _check_caught_exception(self, attempt_number, caught_exception):\n        # This is implementation specific, but this class is useful by\n        # coordinating with the MaxAttemptsDecorator.\n        # The MaxAttemptsDecorator has a list of exceptions it should catch\n        # and retry, but something needs to come along and actually raise the\n        # caught_exception.  That's what this class is being used for.  If\n        # the MaxAttemptsDecorator is not interested in retrying the exception\n        # then this exception just propagates out past the retry code.\n        raise caught_exception\n", "botocore/httpchecksum.py": "# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\n\"\"\" The interfaces in this module are not intended for public use.\n\nThis module defines interfaces for applying checksums to HTTP requests within\nthe context of botocore. This involves both resolving the checksum to be used\nbased on client configuration and environment, as well as application of the\nchecksum to the request.\n\"\"\"\nimport base64\nimport io\nimport logging\nfrom binascii import crc32\nfrom hashlib import sha1, sha256\n\nfrom botocore.compat import HAS_CRT\nfrom botocore.exceptions import (\n    AwsChunkedWrapperError,\n    FlexibleChecksumError,\n    MissingDependencyException,\n)\nfrom botocore.response import StreamingBody\nfrom botocore.utils import (\n    conditionally_calculate_md5,\n    determine_content_length,\n)\n\nif HAS_CRT:\n    from awscrt import checksums as crt_checksums\nelse:\n    crt_checksums = None\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseChecksum:\n    _CHUNK_SIZE = 1024 * 1024\n\n    def update(self, chunk):\n        pass\n\n    def digest(self):\n        pass\n\n    def b64digest(self):\n        bs = self.digest()\n        return base64.b64encode(bs).decode(\"ascii\")\n\n    def _handle_fileobj(self, fileobj):\n        start_position = fileobj.tell()\n        for chunk in iter(lambda: fileobj.read(self._CHUNK_SIZE), b\"\"):\n            self.update(chunk)\n        fileobj.seek(start_position)\n\n    def handle(self, body):\n        if isinstance(body, (bytes, bytearray)):\n            self.update(body)\n        else:\n            self._handle_fileobj(body)\n        return self.b64digest()\n\n\nclass Crc32Checksum(BaseChecksum):\n    def __init__(self):\n        self._int_crc32 = 0\n\n    def update(self, chunk):\n        self._int_crc32 = crc32(chunk, self._int_crc32) & 0xFFFFFFFF\n\n    def digest(self):\n        return self._int_crc32.to_bytes(4, byteorder=\"big\")\n\n\nclass CrtCrc32Checksum(BaseChecksum):\n    # Note: This class is only used if the CRT is available\n    def __init__(self):\n        self._int_crc32 = 0\n\n    def update(self, chunk):\n        new_checksum = crt_checksums.crc32(chunk, self._int_crc32)\n        self._int_crc32 = new_checksum & 0xFFFFFFFF\n\n    def digest(self):\n        return self._int_crc32.to_bytes(4, byteorder=\"big\")\n\n\nclass CrtCrc32cChecksum(BaseChecksum):\n    # Note: This class is only used if the CRT is available\n    def __init__(self):\n        self._int_crc32c = 0\n\n    def update(self, chunk):\n        new_checksum = crt_checksums.crc32c(chunk, self._int_crc32c)\n        self._int_crc32c = new_checksum & 0xFFFFFFFF\n\n    def digest(self):\n        return self._int_crc32c.to_bytes(4, byteorder=\"big\")\n\n\nclass Sha1Checksum(BaseChecksum):\n    def __init__(self):\n        self._checksum = sha1()\n\n    def update(self, chunk):\n        self._checksum.update(chunk)\n\n    def digest(self):\n        return self._checksum.digest()\n\n\nclass Sha256Checksum(BaseChecksum):\n    def __init__(self):\n        self._checksum = sha256()\n\n    def update(self, chunk):\n        self._checksum.update(chunk)\n\n    def digest(self):\n        return self._checksum.digest()\n\n\nclass AwsChunkedWrapper:\n    _DEFAULT_CHUNK_SIZE = 1024 * 1024\n\n    def __init__(\n        self,\n        raw,\n        checksum_cls=None,\n        checksum_name=\"x-amz-checksum\",\n        chunk_size=None,\n    ):\n        self._raw = raw\n        self._checksum_name = checksum_name\n        self._checksum_cls = checksum_cls\n        self._reset()\n\n        if chunk_size is None:\n            chunk_size = self._DEFAULT_CHUNK_SIZE\n        self._chunk_size = chunk_size\n\n    def _reset(self):\n        self._remaining = b\"\"\n        self._complete = False\n        self._checksum = None\n        if self._checksum_cls:\n            self._checksum = self._checksum_cls()\n\n    def seek(self, offset, whence=0):\n        if offset != 0 or whence != 0:\n            raise AwsChunkedWrapperError(\n                error_msg=\"Can only seek to start of stream\"\n            )\n        self._reset()\n        self._raw.seek(0)\n\n    def read(self, size=None):\n        # Normalize \"read all\" size values to None\n        if size is not None and size <= 0:\n            size = None\n\n        # If the underlying body is done and we have nothing left then\n        # end the stream\n        if self._complete and not self._remaining:\n            return b\"\"\n\n        # While we're not done and want more bytes\n        want_more_bytes = size is None or size > len(self._remaining)\n        while not self._complete and want_more_bytes:\n            self._remaining += self._make_chunk()\n            want_more_bytes = size is None or size > len(self._remaining)\n\n        # If size was None, we want to return everything\n        if size is None:\n            size = len(self._remaining)\n\n        # Return a chunk up to the size asked for\n        to_return = self._remaining[:size]\n        self._remaining = self._remaining[size:]\n        return to_return\n\n    def _make_chunk(self):\n        # NOTE: Chunk size is not deterministic as read could return less. This\n        # means we cannot know the content length of the encoded aws-chunked\n        # stream ahead of time without ensuring a consistent chunk size\n        raw_chunk = self._raw.read(self._chunk_size)\n        hex_len = hex(len(raw_chunk))[2:].encode(\"ascii\")\n        self._complete = not raw_chunk\n\n        if self._checksum:\n            self._checksum.update(raw_chunk)\n\n        if self._checksum and self._complete:\n            name = self._checksum_name.encode(\"ascii\")\n            checksum = self._checksum.b64digest().encode(\"ascii\")\n            return b\"0\\r\\n%s:%s\\r\\n\\r\\n\" % (name, checksum)\n\n        return b\"%s\\r\\n%s\\r\\n\" % (hex_len, raw_chunk)\n\n    def __iter__(self):\n        while not self._complete:\n            yield self._make_chunk()\n\n\nclass StreamingChecksumBody(StreamingBody):\n    def __init__(self, raw_stream, content_length, checksum, expected):\n        super().__init__(raw_stream, content_length)\n        self._checksum = checksum\n        self._expected = expected\n\n    def read(self, amt=None):\n        chunk = super().read(amt=amt)\n        self._checksum.update(chunk)\n        if amt is None or (not chunk and amt > 0):\n            self._validate_checksum()\n        return chunk\n\n    def _validate_checksum(self):\n        if self._checksum.digest() != base64.b64decode(self._expected):\n            error_msg = (\n                f\"Expected checksum {self._expected} did not match calculated \"\n                f\"checksum: {self._checksum.b64digest()}\"\n            )\n            raise FlexibleChecksumError(error_msg=error_msg)\n\n\ndef resolve_checksum_context(request, operation_model, params):\n    resolve_request_checksum_algorithm(request, operation_model, params)\n    resolve_response_checksum_algorithms(request, operation_model, params)\n\n\ndef resolve_request_checksum_algorithm(\n    request,\n    operation_model,\n    params,\n    supported_algorithms=None,\n):\n    http_checksum = operation_model.http_checksum\n    algorithm_member = http_checksum.get(\"requestAlgorithmMember\")\n    if algorithm_member and algorithm_member in params:\n        # If the client has opted into using flexible checksums and the\n        # request supports it, use that instead of checksum required\n        if supported_algorithms is None:\n            supported_algorithms = _SUPPORTED_CHECKSUM_ALGORITHMS\n\n        algorithm_name = params[algorithm_member].lower()\n        if algorithm_name not in supported_algorithms:\n            if not HAS_CRT and algorithm_name in _CRT_CHECKSUM_ALGORITHMS:\n                raise MissingDependencyException(\n                    msg=(\n                        f\"Using {algorithm_name.upper()} requires an \"\n                        \"additional dependency. You will need to pip install \"\n                        \"botocore[crt] before proceeding.\"\n                    )\n                )\n            raise FlexibleChecksumError(\n                error_msg=\"Unsupported checksum algorithm: %s\" % algorithm_name\n            )\n\n        location_type = \"header\"\n        if operation_model.has_streaming_input:\n            # Operations with streaming input must support trailers.\n            if request[\"url\"].startswith(\"https:\"):\n                # We only support unsigned trailer checksums currently. As this\n                # disables payload signing we'll only use trailers over TLS.\n                location_type = \"trailer\"\n\n        algorithm = {\n            \"algorithm\": algorithm_name,\n            \"in\": location_type,\n            \"name\": \"x-amz-checksum-%s\" % algorithm_name,\n        }\n\n        if algorithm[\"name\"] in request[\"headers\"]:\n            # If the header is already set by the customer, skip calculation\n            return\n\n        checksum_context = request[\"context\"].get(\"checksum\", {})\n        checksum_context[\"request_algorithm\"] = algorithm\n        request[\"context\"][\"checksum\"] = checksum_context\n    elif operation_model.http_checksum_required or http_checksum.get(\n        \"requestChecksumRequired\"\n    ):\n        # Otherwise apply the old http checksum behavior via Content-MD5\n        checksum_context = request[\"context\"].get(\"checksum\", {})\n        checksum_context[\"request_algorithm\"] = \"conditional-md5\"\n        request[\"context\"][\"checksum\"] = checksum_context\n\n\ndef apply_request_checksum(request):\n    checksum_context = request.get(\"context\", {}).get(\"checksum\", {})\n    algorithm = checksum_context.get(\"request_algorithm\")\n\n    if not algorithm:\n        return\n\n    if algorithm == \"conditional-md5\":\n        # Special case to handle the http checksum required trait\n        conditionally_calculate_md5(request)\n    elif algorithm[\"in\"] == \"header\":\n        _apply_request_header_checksum(request)\n    elif algorithm[\"in\"] == \"trailer\":\n        _apply_request_trailer_checksum(request)\n    else:\n        raise FlexibleChecksumError(\n            error_msg=\"Unknown checksum variant: %s\" % algorithm[\"in\"]\n        )\n\n\ndef _apply_request_header_checksum(request):\n    checksum_context = request.get(\"context\", {}).get(\"checksum\", {})\n    algorithm = checksum_context.get(\"request_algorithm\")\n    location_name = algorithm[\"name\"]\n    if location_name in request[\"headers\"]:\n        # If the header is already set by the customer, skip calculation\n        return\n    checksum_cls = _CHECKSUM_CLS.get(algorithm[\"algorithm\"])\n    digest = checksum_cls().handle(request[\"body\"])\n    request[\"headers\"][location_name] = digest\n\n\ndef _apply_request_trailer_checksum(request):\n    checksum_context = request.get(\"context\", {}).get(\"checksum\", {})\n    algorithm = checksum_context.get(\"request_algorithm\")\n    location_name = algorithm[\"name\"]\n    checksum_cls = _CHECKSUM_CLS.get(algorithm[\"algorithm\"])\n\n    headers = request[\"headers\"]\n    body = request[\"body\"]\n\n    if location_name in headers:\n        # If the header is already set by the customer, skip calculation\n        return\n\n    headers[\"Transfer-Encoding\"] = \"chunked\"\n    if \"Content-Encoding\" in headers:\n        # We need to preserve the existing content encoding and add\n        # aws-chunked as a new content encoding.\n        headers[\"Content-Encoding\"] += \",aws-chunked\"\n    else:\n        headers[\"Content-Encoding\"] = \"aws-chunked\"\n    headers[\"X-Amz-Trailer\"] = location_name\n\n    content_length = determine_content_length(body)\n    if content_length is not None:\n        # Send the decoded content length if we can determine it. Some\n        # services such as S3 may require the decoded content length\n        headers[\"X-Amz-Decoded-Content-Length\"] = str(content_length)\n\n    if isinstance(body, (bytes, bytearray)):\n        body = io.BytesIO(body)\n\n    request[\"body\"] = AwsChunkedWrapper(\n        body,\n        checksum_cls=checksum_cls,\n        checksum_name=location_name,\n    )\n\n\ndef resolve_response_checksum_algorithms(\n    request, operation_model, params, supported_algorithms=None\n):\n    http_checksum = operation_model.http_checksum\n    mode_member = http_checksum.get(\"requestValidationModeMember\")\n    if mode_member and mode_member in params:\n        if supported_algorithms is None:\n            supported_algorithms = _SUPPORTED_CHECKSUM_ALGORITHMS\n        response_algorithms = {\n            a.lower() for a in http_checksum.get(\"responseAlgorithms\", [])\n        }\n\n        usable_algorithms = []\n        for algorithm in _ALGORITHMS_PRIORITY_LIST:\n            if algorithm not in response_algorithms:\n                continue\n            if algorithm in supported_algorithms:\n                usable_algorithms.append(algorithm)\n\n        checksum_context = request[\"context\"].get(\"checksum\", {})\n        checksum_context[\"response_algorithms\"] = usable_algorithms\n        request[\"context\"][\"checksum\"] = checksum_context\n\n\ndef handle_checksum_body(http_response, response, context, operation_model):\n    headers = response[\"headers\"]\n    checksum_context = context.get(\"checksum\", {})\n    algorithms = checksum_context.get(\"response_algorithms\")\n\n    if not algorithms:\n        return\n\n    for algorithm in algorithms:\n        header_name = \"x-amz-checksum-%s\" % algorithm\n        # If the header is not found, check the next algorithm\n        if header_name not in headers:\n            continue\n\n        # If a - is in the checksum this is not valid Base64. S3 returns\n        # checksums that include a -# suffix to indicate a checksum derived\n        # from the hash of all part checksums. We cannot wrap this response\n        if \"-\" in headers[header_name]:\n            continue\n\n        if operation_model.has_streaming_output:\n            response[\"body\"] = _handle_streaming_response(\n                http_response, response, algorithm\n            )\n        else:\n            response[\"body\"] = _handle_bytes_response(\n                http_response, response, algorithm\n            )\n\n        # Expose metadata that the checksum check actually occurred\n        checksum_context = response[\"context\"].get(\"checksum\", {})\n        checksum_context[\"response_algorithm\"] = algorithm\n        response[\"context\"][\"checksum\"] = checksum_context\n        return\n\n    logger.info(\n        f'Skipping checksum validation. Response did not contain one of the '\n        f'following algorithms: {algorithms}.'\n    )\n\n\ndef _handle_streaming_response(http_response, response, algorithm):\n    checksum_cls = _CHECKSUM_CLS.get(algorithm)\n    header_name = \"x-amz-checksum-%s\" % algorithm\n    return StreamingChecksumBody(\n        http_response.raw,\n        response[\"headers\"].get(\"content-length\"),\n        checksum_cls(),\n        response[\"headers\"][header_name],\n    )\n\n\ndef _handle_bytes_response(http_response, response, algorithm):\n    body = http_response.content\n    header_name = \"x-amz-checksum-%s\" % algorithm\n    checksum_cls = _CHECKSUM_CLS.get(algorithm)\n    checksum = checksum_cls()\n    checksum.update(body)\n    expected = response[\"headers\"][header_name]\n    if checksum.digest() != base64.b64decode(expected):\n        error_msg = (\n            \"Expected checksum %s did not match calculated checksum: %s\"\n            % (\n                expected,\n                checksum.b64digest(),\n            )\n        )\n        raise FlexibleChecksumError(error_msg=error_msg)\n    return body\n\n\n_CHECKSUM_CLS = {\n    \"crc32\": Crc32Checksum,\n    \"sha1\": Sha1Checksum,\n    \"sha256\": Sha256Checksum,\n}\n_CRT_CHECKSUM_ALGORITHMS = [\"crc32\", \"crc32c\"]\nif HAS_CRT:\n    # Use CRT checksum implementations if available\n    _CRT_CHECKSUM_CLS = {\n        \"crc32\": CrtCrc32Checksum,\n        \"crc32c\": CrtCrc32cChecksum,\n    }\n    _CHECKSUM_CLS.update(_CRT_CHECKSUM_CLS)\n    # Validate this list isn't out of sync with _CRT_CHECKSUM_CLS keys\n    assert all(\n        name in _CRT_CHECKSUM_ALGORITHMS for name in _CRT_CHECKSUM_CLS.keys()\n    )\n_SUPPORTED_CHECKSUM_ALGORITHMS = list(_CHECKSUM_CLS.keys())\n_ALGORITHMS_PRIORITY_LIST = ['crc32c', 'crc32', 'sha1', 'sha256']\n", "botocore/utils.py": "# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport base64\nimport binascii\nimport datetime\nimport email.message\nimport functools\nimport hashlib\nimport io\nimport logging\nimport os\nimport random\nimport re\nimport socket\nimport time\nimport warnings\nimport weakref\nfrom datetime import datetime as _DatetimeClass\nfrom ipaddress import ip_address\nfrom pathlib import Path\nfrom urllib.request import getproxies, proxy_bypass\n\nimport dateutil.parser\nfrom dateutil.tz import tzutc\nfrom urllib3.exceptions import LocationParseError\n\nimport botocore\nimport botocore.awsrequest\nimport botocore.httpsession\n\n# IP Regexes retained for backwards compatibility\nfrom botocore.compat import HEX_PAT  # noqa: F401\nfrom botocore.compat import IPV4_PAT  # noqa: F401\nfrom botocore.compat import IPV6_ADDRZ_PAT  # noqa: F401\nfrom botocore.compat import IPV6_PAT  # noqa: F401\nfrom botocore.compat import LS32_PAT  # noqa: F401\nfrom botocore.compat import UNRESERVED_PAT  # noqa: F401\nfrom botocore.compat import ZONE_ID_PAT  # noqa: F401\nfrom botocore.compat import (\n    HAS_CRT,\n    IPV4_RE,\n    IPV6_ADDRZ_RE,\n    MD5_AVAILABLE,\n    UNSAFE_URL_CHARS,\n    OrderedDict,\n    get_md5,\n    get_tzinfo_options,\n    json,\n    quote,\n    urlparse,\n    urlsplit,\n    urlunsplit,\n    zip_longest,\n)\nfrom botocore.exceptions import (\n    ClientError,\n    ConfigNotFound,\n    ConnectionClosedError,\n    ConnectTimeoutError,\n    EndpointConnectionError,\n    HTTPClientError,\n    InvalidDNSNameError,\n    InvalidEndpointConfigurationError,\n    InvalidExpressionError,\n    InvalidHostLabelError,\n    InvalidIMDSEndpointError,\n    InvalidIMDSEndpointModeError,\n    InvalidRegionError,\n    MetadataRetrievalError,\n    MissingDependencyException,\n    ReadTimeoutError,\n    SSOTokenLoadError,\n    UnsupportedOutpostResourceError,\n    UnsupportedS3AccesspointConfigurationError,\n    UnsupportedS3ArnError,\n    UnsupportedS3ConfigurationError,\n    UnsupportedS3ControlArnError,\n    UnsupportedS3ControlConfigurationError,\n)\n\nlogger = logging.getLogger(__name__)\nDEFAULT_METADATA_SERVICE_TIMEOUT = 1\nMETADATA_BASE_URL = 'http://169.254.169.254/'\nMETADATA_BASE_URL_IPv6 = 'http://[fd00:ec2::254]/'\nMETADATA_ENDPOINT_MODES = ('ipv4', 'ipv6')\n\n# These are chars that do not need to be urlencoded.\n# Based on rfc2986, section 2.3\nSAFE_CHARS = '-._~'\nLABEL_RE = re.compile(r'[a-z0-9][a-z0-9\\-]*[a-z0-9]')\nRETRYABLE_HTTP_ERRORS = (\n    ReadTimeoutError,\n    EndpointConnectionError,\n    ConnectionClosedError,\n    ConnectTimeoutError,\n)\nS3_ACCELERATE_WHITELIST = ['dualstack']\n# In switching events from using service name / endpoint prefix to service\n# id, we have to preserve compatibility. This maps the instances where either\n# is different than the transformed service id.\nEVENT_ALIASES = {\n    \"api.mediatailor\": \"mediatailor\",\n    \"api.pricing\": \"pricing\",\n    \"api.sagemaker\": \"sagemaker\",\n    \"apigateway\": \"api-gateway\",\n    \"application-autoscaling\": \"application-auto-scaling\",\n    \"appstream2\": \"appstream\",\n    \"autoscaling\": \"auto-scaling\",\n    \"autoscaling-plans\": \"auto-scaling-plans\",\n    \"ce\": \"cost-explorer\",\n    \"cloudhsmv2\": \"cloudhsm-v2\",\n    \"cloudsearchdomain\": \"cloudsearch-domain\",\n    \"cognito-idp\": \"cognito-identity-provider\",\n    \"config\": \"config-service\",\n    \"cur\": \"cost-and-usage-report-service\",\n    \"data.iot\": \"iot-data-plane\",\n    \"data.jobs.iot\": \"iot-jobs-data-plane\",\n    \"data.mediastore\": \"mediastore-data\",\n    \"datapipeline\": \"data-pipeline\",\n    \"devicefarm\": \"device-farm\",\n    \"devices.iot1click\": \"iot-1click-devices-service\",\n    \"directconnect\": \"direct-connect\",\n    \"discovery\": \"application-discovery-service\",\n    \"dms\": \"database-migration-service\",\n    \"ds\": \"directory-service\",\n    \"dynamodbstreams\": \"dynamodb-streams\",\n    \"elasticbeanstalk\": \"elastic-beanstalk\",\n    \"elasticfilesystem\": \"efs\",\n    \"elasticloadbalancing\": \"elastic-load-balancing\",\n    \"elasticmapreduce\": \"emr\",\n    \"elastictranscoder\": \"elastic-transcoder\",\n    \"elb\": \"elastic-load-balancing\",\n    \"elbv2\": \"elastic-load-balancing-v2\",\n    \"email\": \"ses\",\n    \"entitlement.marketplace\": \"marketplace-entitlement-service\",\n    \"es\": \"elasticsearch-service\",\n    \"events\": \"eventbridge\",\n    \"cloudwatch-events\": \"eventbridge\",\n    \"iot-data\": \"iot-data-plane\",\n    \"iot-jobs-data\": \"iot-jobs-data-plane\",\n    \"iot1click-devices\": \"iot-1click-devices-service\",\n    \"iot1click-projects\": \"iot-1click-projects\",\n    \"kinesisanalytics\": \"kinesis-analytics\",\n    \"kinesisvideo\": \"kinesis-video\",\n    \"lex-models\": \"lex-model-building-service\",\n    \"lex-runtime\": \"lex-runtime-service\",\n    \"logs\": \"cloudwatch-logs\",\n    \"machinelearning\": \"machine-learning\",\n    \"marketplace-entitlement\": \"marketplace-entitlement-service\",\n    \"marketplacecommerceanalytics\": \"marketplace-commerce-analytics\",\n    \"metering.marketplace\": \"marketplace-metering\",\n    \"meteringmarketplace\": \"marketplace-metering\",\n    \"mgh\": \"migration-hub\",\n    \"models.lex\": \"lex-model-building-service\",\n    \"monitoring\": \"cloudwatch\",\n    \"mturk-requester\": \"mturk\",\n    \"opsworks-cm\": \"opsworkscm\",\n    \"projects.iot1click\": \"iot-1click-projects\",\n    \"resourcegroupstaggingapi\": \"resource-groups-tagging-api\",\n    \"route53\": \"route-53\",\n    \"route53domains\": \"route-53-domains\",\n    \"runtime.lex\": \"lex-runtime-service\",\n    \"runtime.sagemaker\": \"sagemaker-runtime\",\n    \"sdb\": \"simpledb\",\n    \"secretsmanager\": \"secrets-manager\",\n    \"serverlessrepo\": \"serverlessapplicationrepository\",\n    \"servicecatalog\": \"service-catalog\",\n    \"states\": \"sfn\",\n    \"stepfunctions\": \"sfn\",\n    \"storagegateway\": \"storage-gateway\",\n    \"streams.dynamodb\": \"dynamodb-streams\",\n    \"tagging\": \"resource-groups-tagging-api\",\n}\n\n\n# This pattern can be used to detect if a header is a flexible checksum header\nCHECKSUM_HEADER_PATTERN = re.compile(\n    r'^X-Amz-Checksum-([a-z0-9]*)$',\n    flags=re.IGNORECASE,\n)\n\n\ndef ensure_boolean(val):\n    \"\"\"Ensures a boolean value if a string or boolean is provided\n\n    For strings, the value for True/False is case insensitive\n    \"\"\"\n    if isinstance(val, bool):\n        return val\n    elif isinstance(val, str):\n        return val.lower() == 'true'\n    else:\n        return False\n\n\ndef resolve_imds_endpoint_mode(session):\n    \"\"\"Resolving IMDS endpoint mode to either IPv6 or IPv4.\n\n    ec2_metadata_service_endpoint_mode takes precedence over imds_use_ipv6.\n    \"\"\"\n    endpoint_mode = session.get_config_variable(\n        'ec2_metadata_service_endpoint_mode'\n    )\n    if endpoint_mode is not None:\n        lendpoint_mode = endpoint_mode.lower()\n        if lendpoint_mode not in METADATA_ENDPOINT_MODES:\n            error_msg_kwargs = {\n                'mode': endpoint_mode,\n                'valid_modes': METADATA_ENDPOINT_MODES,\n            }\n            raise InvalidIMDSEndpointModeError(**error_msg_kwargs)\n        return lendpoint_mode\n    elif session.get_config_variable('imds_use_ipv6'):\n        return 'ipv6'\n    return 'ipv4'\n\n\ndef is_json_value_header(shape):\n    \"\"\"Determines if the provided shape is the special header type jsonvalue.\n\n    :type shape: botocore.shape\n    :param shape: Shape to be inspected for the jsonvalue trait.\n\n    :return: True if this type is a jsonvalue, False otherwise\n    :rtype: Bool\n    \"\"\"\n    return (\n        hasattr(shape, 'serialization')\n        and shape.serialization.get('jsonvalue', False)\n        and shape.serialization.get('location') == 'header'\n        and shape.type_name == 'string'\n    )\n\n\ndef has_header(header_name, headers):\n    \"\"\"Case-insensitive check for header key.\"\"\"\n    if header_name is None:\n        return False\n    elif isinstance(headers, botocore.awsrequest.HeadersDict):\n        return header_name in headers\n    else:\n        return header_name.lower() in [key.lower() for key in headers.keys()]\n\n\ndef get_service_module_name(service_model):\n    \"\"\"Returns the module name for a service\n\n    This is the value used in both the documentation and client class name\n    \"\"\"\n    name = service_model.metadata.get(\n        'serviceAbbreviation',\n        service_model.metadata.get(\n            'serviceFullName', service_model.service_name\n        ),\n    )\n    name = name.replace('Amazon', '')\n    name = name.replace('AWS', '')\n    name = re.sub(r'\\W+', '', name)\n    return name\n\n\ndef normalize_url_path(path):\n    if not path:\n        return '/'\n    return remove_dot_segments(path)\n\n\ndef normalize_boolean(val):\n    \"\"\"Returns None if val is None, otherwise ensure value\n    converted to boolean\"\"\"\n    if val is None:\n        return val\n    else:\n        return ensure_boolean(val)\n\n\ndef remove_dot_segments(url):\n    # RFC 3986, section 5.2.4 \"Remove Dot Segments\"\n    # Also, AWS services require consecutive slashes to be removed,\n    # so that's done here as well\n    if not url:\n        return ''\n    input_url = url.split('/')\n    output_list = []\n    for x in input_url:\n        if x and x != '.':\n            if x == '..':\n                if output_list:\n                    output_list.pop()\n            else:\n                output_list.append(x)\n\n    if url[0] == '/':\n        first = '/'\n    else:\n        first = ''\n    if url[-1] == '/' and output_list:\n        last = '/'\n    else:\n        last = ''\n    return first + '/'.join(output_list) + last\n\n\ndef validate_jmespath_for_set(expression):\n    # Validates a limited jmespath expression to determine if we can set a\n    # value based on it. Only works with dotted paths.\n    if not expression or expression == '.':\n        raise InvalidExpressionError(expression=expression)\n\n    for invalid in ['[', ']', '*']:\n        if invalid in expression:\n            raise InvalidExpressionError(expression=expression)\n\n\ndef set_value_from_jmespath(source, expression, value, is_first=True):\n    # This takes a (limited) jmespath-like expression & can set a value based\n    # on it.\n    # Limitations:\n    # * Only handles dotted lookups\n    # * No offsets/wildcards/slices/etc.\n    if is_first:\n        validate_jmespath_for_set(expression)\n\n    bits = expression.split('.', 1)\n    current_key, remainder = bits[0], bits[1] if len(bits) > 1 else ''\n\n    if not current_key:\n        raise InvalidExpressionError(expression=expression)\n\n    if remainder:\n        if current_key not in source:\n            # We've got something in the expression that's not present in the\n            # source (new key). If there's any more bits, we'll set the key\n            # with an empty dictionary.\n            source[current_key] = {}\n\n        return set_value_from_jmespath(\n            source[current_key], remainder, value, is_first=False\n        )\n\n    # If we're down to a single key, set it.\n    source[current_key] = value\n\n\ndef is_global_accesspoint(context):\n    \"\"\"Determine if request is intended for an MRAP accesspoint.\"\"\"\n    s3_accesspoint = context.get('s3_accesspoint', {})\n    is_global = s3_accesspoint.get('region') == ''\n    return is_global\n\n\nclass _RetriesExceededError(Exception):\n    \"\"\"Internal exception used when the number of retries are exceeded.\"\"\"\n\n    pass\n\n\nclass BadIMDSRequestError(Exception):\n    def __init__(self, request):\n        self.request = request\n\n\nclass IMDSFetcher:\n    _RETRIES_EXCEEDED_ERROR_CLS = _RetriesExceededError\n    _TOKEN_PATH = 'latest/api/token'\n    _TOKEN_TTL = '21600'\n\n    def __init__(\n        self,\n        timeout=DEFAULT_METADATA_SERVICE_TIMEOUT,\n        num_attempts=1,\n        base_url=METADATA_BASE_URL,\n        env=None,\n        user_agent=None,\n        config=None,\n    ):\n        self._timeout = timeout\n        self._num_attempts = num_attempts\n        if config is None:\n            config = {}\n        self._base_url = self._select_base_url(base_url, config)\n        self._config = config\n\n        if env is None:\n            env = os.environ.copy()\n        self._disabled = (\n            env.get('AWS_EC2_METADATA_DISABLED', 'false').lower() == 'true'\n        )\n        self._imds_v1_disabled = config.get('ec2_metadata_v1_disabled')\n        self._user_agent = user_agent\n        self._session = botocore.httpsession.URLLib3Session(\n            timeout=self._timeout,\n            proxies=get_environ_proxies(self._base_url),\n        )\n\n    def get_base_url(self):\n        return self._base_url\n\n    def _select_base_url(self, base_url, config):\n        if config is None:\n            config = {}\n\n        requires_ipv6 = (\n            config.get('ec2_metadata_service_endpoint_mode') == 'ipv6'\n        )\n        custom_metadata_endpoint = config.get('ec2_metadata_service_endpoint')\n\n        if requires_ipv6 and custom_metadata_endpoint:\n            logger.warning(\n                \"Custom endpoint and IMDS_USE_IPV6 are both set. Using custom endpoint.\"\n            )\n\n        chosen_base_url = None\n\n        if base_url != METADATA_BASE_URL:\n            chosen_base_url = base_url\n        elif custom_metadata_endpoint:\n            chosen_base_url = custom_metadata_endpoint\n        elif requires_ipv6:\n            chosen_base_url = METADATA_BASE_URL_IPv6\n        else:\n            chosen_base_url = METADATA_BASE_URL\n\n        logger.debug(\"IMDS ENDPOINT: %s\" % chosen_base_url)\n        if not is_valid_uri(chosen_base_url):\n            raise InvalidIMDSEndpointError(endpoint=chosen_base_url)\n\n        return chosen_base_url\n\n    def _construct_url(self, path):\n        sep = ''\n        if self._base_url and not self._base_url.endswith('/'):\n            sep = '/'\n        return f'{self._base_url}{sep}{path}'\n\n    def _fetch_metadata_token(self):\n        self._assert_enabled()\n        url = self._construct_url(self._TOKEN_PATH)\n        headers = {\n            'x-aws-ec2-metadata-token-ttl-seconds': self._TOKEN_TTL,\n        }\n        self._add_user_agent(headers)\n        request = botocore.awsrequest.AWSRequest(\n            method='PUT', url=url, headers=headers\n        )\n        for i in range(self._num_attempts):\n            try:\n                response = self._session.send(request.prepare())\n                if response.status_code == 200:\n                    return response.text\n                elif response.status_code in (404, 403, 405):\n                    return None\n                elif response.status_code in (400,):\n                    raise BadIMDSRequestError(request)\n            except ReadTimeoutError:\n                return None\n            except RETRYABLE_HTTP_ERRORS as e:\n                logger.debug(\n                    \"Caught retryable HTTP exception while making metadata \"\n                    \"service request to %s: %s\",\n                    url,\n                    e,\n                    exc_info=True,\n                )\n            except HTTPClientError as e:\n                if isinstance(e.kwargs.get('error'), LocationParseError):\n                    raise InvalidIMDSEndpointError(endpoint=url, error=e)\n                else:\n                    raise\n        return None\n\n    def _get_request(self, url_path, retry_func, token=None):\n        \"\"\"Make a get request to the Instance Metadata Service.\n\n        :type url_path: str\n        :param url_path: The path component of the URL to make a get request.\n            This arg is appended to the base_url that was provided in the\n            initializer.\n\n        :type retry_func: callable\n        :param retry_func: A function that takes the response as an argument\n             and determines if it needs to retry. By default empty and non\n             200 OK responses are retried.\n\n        :type token: str\n        :param token: Metadata token to send along with GET requests to IMDS.\n        \"\"\"\n        self._assert_enabled()\n        if not token:\n            self._assert_v1_enabled()\n        if retry_func is None:\n            retry_func = self._default_retry\n        url = self._construct_url(url_path)\n        headers = {}\n        if token is not None:\n            headers['x-aws-ec2-metadata-token'] = token\n        self._add_user_agent(headers)\n        for i in range(self._num_attempts):\n            try:\n                request = botocore.awsrequest.AWSRequest(\n                    method='GET', url=url, headers=headers\n                )\n                response = self._session.send(request.prepare())\n                if not retry_func(response):\n                    return response\n            except RETRYABLE_HTTP_ERRORS as e:\n                logger.debug(\n                    \"Caught retryable HTTP exception while making metadata \"\n                    \"service request to %s: %s\",\n                    url,\n                    e,\n                    exc_info=True,\n                )\n        raise self._RETRIES_EXCEEDED_ERROR_CLS()\n\n    def _add_user_agent(self, headers):\n        if self._user_agent is not None:\n            headers['User-Agent'] = self._user_agent\n\n    def _assert_enabled(self):\n        if self._disabled:\n            logger.debug(\"Access to EC2 metadata has been disabled.\")\n            raise self._RETRIES_EXCEEDED_ERROR_CLS()\n\n    def _assert_v1_enabled(self):\n        if self._imds_v1_disabled:\n            raise MetadataRetrievalError(\n                error_msg=\"Unable to retrieve token for use in IMDSv2 call and IMDSv1 has been disabled\"\n            )\n\n    def _default_retry(self, response):\n        return self._is_non_ok_response(response) or self._is_empty(response)\n\n    def _is_non_ok_response(self, response):\n        if response.status_code != 200:\n            self._log_imds_response(response, 'non-200', log_body=True)\n            return True\n        return False\n\n    def _is_empty(self, response):\n        if not response.content:\n            self._log_imds_response(response, 'no body', log_body=True)\n            return True\n        return False\n\n    def _log_imds_response(self, response, reason_to_log, log_body=False):\n        statement = (\n            \"Metadata service returned %s response \"\n            \"with status code of %s for url: %s\"\n        )\n        logger_args = [reason_to_log, response.status_code, response.url]\n        if log_body:\n            statement += \", content body: %s\"\n            logger_args.append(response.content)\n        logger.debug(statement, *logger_args)\n\n\nclass InstanceMetadataFetcher(IMDSFetcher):\n    _URL_PATH = 'latest/meta-data/iam/security-credentials/'\n    _REQUIRED_CREDENTIAL_FIELDS = [\n        'AccessKeyId',\n        'SecretAccessKey',\n        'Token',\n        'Expiration',\n    ]\n\n    def retrieve_iam_role_credentials(self):\n        try:\n            token = self._fetch_metadata_token()\n            role_name = self._get_iam_role(token)\n            credentials = self._get_credentials(role_name, token)\n            if self._contains_all_credential_fields(credentials):\n                credentials = {\n                    'role_name': role_name,\n                    'access_key': credentials['AccessKeyId'],\n                    'secret_key': credentials['SecretAccessKey'],\n                    'token': credentials['Token'],\n                    'expiry_time': credentials['Expiration'],\n                }\n                self._evaluate_expiration(credentials)\n                return credentials\n            else:\n                # IMDS can return a 200 response that has a JSON formatted\n                # error message (i.e. if ec2 is not trusted entity for the\n                # attached role). We do not necessarily want to retry for\n                # these and we also do not necessarily want to raise a key\n                # error. So at least log the problematic response and return\n                # an empty dictionary to signal that it was not able to\n                # retrieve credentials. These error will contain both a\n                # Code and Message key.\n                if 'Code' in credentials and 'Message' in credentials:\n                    logger.debug(\n                        'Error response received when retrieving'\n                        'credentials: %s.',\n                        credentials,\n                    )\n                return {}\n        except self._RETRIES_EXCEEDED_ERROR_CLS:\n            logger.debug(\n                \"Max number of attempts exceeded (%s) when \"\n                \"attempting to retrieve data from metadata service.\",\n                self._num_attempts,\n            )\n        except BadIMDSRequestError as e:\n            logger.debug(\"Bad IMDS request: %s\", e.request)\n        return {}\n\n    def _get_iam_role(self, token=None):\n        return self._get_request(\n            url_path=self._URL_PATH,\n            retry_func=self._needs_retry_for_role_name,\n            token=token,\n        ).text\n\n    def _get_credentials(self, role_name, token=None):\n        r = self._get_request(\n            url_path=self._URL_PATH + role_name,\n            retry_func=self._needs_retry_for_credentials,\n            token=token,\n        )\n        return json.loads(r.text)\n\n    def _is_invalid_json(self, response):\n        try:\n            json.loads(response.text)\n            return False\n        except ValueError:\n            self._log_imds_response(response, 'invalid json')\n            return True\n\n    def _needs_retry_for_role_name(self, response):\n        return self._is_non_ok_response(response) or self._is_empty(response)\n\n    def _needs_retry_for_credentials(self, response):\n        return (\n            self._is_non_ok_response(response)\n            or self._is_empty(response)\n            or self._is_invalid_json(response)\n        )\n\n    def _contains_all_credential_fields(self, credentials):\n        for field in self._REQUIRED_CREDENTIAL_FIELDS:\n            if field not in credentials:\n                logger.debug(\n                    'Retrieved credentials is missing required field: %s',\n                    field,\n                )\n                return False\n        return True\n\n    def _evaluate_expiration(self, credentials):\n        expiration = credentials.get(\"expiry_time\")\n        if expiration is None:\n            return\n        try:\n            expiration = datetime.datetime.strptime(\n                expiration, \"%Y-%m-%dT%H:%M:%SZ\"\n            )\n            refresh_interval = self._config.get(\n                \"ec2_credential_refresh_window\", 60 * 10\n            )\n            jitter = random.randint(120, 600)  # Between 2 to 10 minutes\n            refresh_interval_with_jitter = refresh_interval + jitter\n            current_time = datetime.datetime.utcnow()\n            refresh_offset = datetime.timedelta(\n                seconds=refresh_interval_with_jitter\n            )\n            extension_time = expiration - refresh_offset\n            if current_time >= extension_time:\n                new_time = current_time + refresh_offset\n                credentials[\"expiry_time\"] = new_time.strftime(\n                    \"%Y-%m-%dT%H:%M:%SZ\"\n                )\n                logger.info(\n                    f\"Attempting credential expiration extension due to a \"\n                    f\"credential service availability issue. A refresh of \"\n                    f\"these credentials will be attempted again within \"\n                    f\"the next {refresh_interval_with_jitter/60:.0f} minutes.\"\n                )\n        except ValueError:\n            logger.debug(\n                f\"Unable to parse expiry_time in {credentials['expiry_time']}\"\n            )\n\n\nclass IMDSRegionProvider:\n    def __init__(self, session, environ=None, fetcher=None):\n        \"\"\"Initialize IMDSRegionProvider.\n        :type session: :class:`botocore.session.Session`\n        :param session: The session is needed to look up configuration for\n            how to contact the instance metadata service. Specifically the\n            whether or not it should use the IMDS region at all, and if so how\n            to configure the timeout and number of attempts to reach the\n            service.\n        :type environ: None or dict\n        :param environ: A dictionary of environment variables to use. If\n            ``None`` is the argument then ``os.environ`` will be used by\n            default.\n        :type fecther: :class:`botocore.utils.InstanceMetadataRegionFetcher`\n        :param fetcher: The class to actually handle the fetching of the region\n            from the IMDS. If not provided a default one will be created.\n        \"\"\"\n        self._session = session\n        if environ is None:\n            environ = os.environ\n        self._environ = environ\n        self._fetcher = fetcher\n\n    def provide(self):\n        \"\"\"Provide the region value from IMDS.\"\"\"\n        instance_region = self._get_instance_metadata_region()\n        return instance_region\n\n    def _get_instance_metadata_region(self):\n        fetcher = self._get_fetcher()\n        region = fetcher.retrieve_region()\n        return region\n\n    def _get_fetcher(self):\n        if self._fetcher is None:\n            self._fetcher = self._create_fetcher()\n        return self._fetcher\n\n    def _create_fetcher(self):\n        metadata_timeout = self._session.get_config_variable(\n            'metadata_service_timeout'\n        )\n        metadata_num_attempts = self._session.get_config_variable(\n            'metadata_service_num_attempts'\n        )\n        imds_config = {\n            'ec2_metadata_service_endpoint': self._session.get_config_variable(\n                'ec2_metadata_service_endpoint'\n            ),\n            'ec2_metadata_service_endpoint_mode': resolve_imds_endpoint_mode(\n                self._session\n            ),\n            'ec2_metadata_v1_disabled': self._session.get_config_variable(\n                'ec2_metadata_v1_disabled'\n            ),\n        }\n        fetcher = InstanceMetadataRegionFetcher(\n            timeout=metadata_timeout,\n            num_attempts=metadata_num_attempts,\n            env=self._environ,\n            user_agent=self._session.user_agent(),\n            config=imds_config,\n        )\n        return fetcher\n\n\nclass InstanceMetadataRegionFetcher(IMDSFetcher):\n    _URL_PATH = 'latest/meta-data/placement/availability-zone/'\n\n    def retrieve_region(self):\n        \"\"\"Get the current region from the instance metadata service.\n        :rvalue: str\n        :returns: The region the current instance is running in or None\n            if the instance metadata service cannot be contacted or does not\n            give a valid response.\n        :rtype: None or str\n        :returns: Returns the region as a string if it is configured to use\n            IMDS as a region source. Otherwise returns ``None``. It will also\n            return ``None`` if it fails to get the region from IMDS due to\n            exhausting its retries or not being able to connect.\n        \"\"\"\n        try:\n            region = self._get_region()\n            return region\n        except self._RETRIES_EXCEEDED_ERROR_CLS:\n            logger.debug(\n                \"Max number of attempts exceeded (%s) when \"\n                \"attempting to retrieve data from metadata service.\",\n                self._num_attempts,\n            )\n        return None\n\n    def _get_region(self):\n        token = self._fetch_metadata_token()\n        response = self._get_request(\n            url_path=self._URL_PATH,\n            retry_func=self._default_retry,\n            token=token,\n        )\n        availability_zone = response.text\n        region = availability_zone[:-1]\n        return region\n\n\ndef merge_dicts(dict1, dict2, append_lists=False):\n    \"\"\"Given two dict, merge the second dict into the first.\n\n    The dicts can have arbitrary nesting.\n\n    :param append_lists: If true, instead of clobbering a list with the new\n        value, append all of the new values onto the original list.\n    \"\"\"\n    for key in dict2:\n        if isinstance(dict2[key], dict):\n            if key in dict1 and key in dict2:\n                merge_dicts(dict1[key], dict2[key])\n            else:\n                dict1[key] = dict2[key]\n        # If the value is a list and the ``append_lists`` flag is set,\n        # append the new values onto the original list\n        elif isinstance(dict2[key], list) and append_lists:\n            # The value in dict1 must be a list in order to append new\n            # values onto it.\n            if key in dict1 and isinstance(dict1[key], list):\n                dict1[key].extend(dict2[key])\n            else:\n                dict1[key] = dict2[key]\n        else:\n            # At scalar types, we iterate and merge the\n            # current dict that we're on.\n            dict1[key] = dict2[key]\n\n\ndef lowercase_dict(original):\n    \"\"\"Copies the given dictionary ensuring all keys are lowercase strings.\"\"\"\n    copy = {}\n    for key in original:\n        copy[key.lower()] = original[key]\n    return copy\n\n\ndef parse_key_val_file(filename, _open=open):\n    try:\n        with _open(filename) as f:\n            contents = f.read()\n            return parse_key_val_file_contents(contents)\n    except OSError:\n        raise ConfigNotFound(path=filename)\n\n\ndef parse_key_val_file_contents(contents):\n    # This was originally extracted from the EC2 credential provider, which was\n    # fairly lenient in its parsing.  We only try to parse key/val pairs if\n    # there's a '=' in the line.\n    final = {}\n    for line in contents.splitlines():\n        if '=' not in line:\n            continue\n        key, val = line.split('=', 1)\n        key = key.strip()\n        val = val.strip()\n        final[key] = val\n    return final\n\n\ndef percent_encode_sequence(mapping, safe=SAFE_CHARS):\n    \"\"\"Urlencode a dict or list into a string.\n\n    This is similar to urllib.urlencode except that:\n\n    * It uses quote, and not quote_plus\n    * It has a default list of safe chars that don't need\n      to be encoded, which matches what AWS services expect.\n\n    If any value in the input ``mapping`` is a list type,\n    then each list element wil be serialized.  This is the equivalent\n    to ``urlencode``'s ``doseq=True`` argument.\n\n    This function should be preferred over the stdlib\n    ``urlencode()`` function.\n\n    :param mapping: Either a dict to urlencode or a list of\n        ``(key, value)`` pairs.\n\n    \"\"\"\n    encoded_pairs = []\n    if hasattr(mapping, 'items'):\n        pairs = mapping.items()\n    else:\n        pairs = mapping\n    for key, value in pairs:\n        if isinstance(value, list):\n            for element in value:\n                encoded_pairs.append(\n                    f'{percent_encode(key)}={percent_encode(element)}'\n                )\n        else:\n            encoded_pairs.append(\n                f'{percent_encode(key)}={percent_encode(value)}'\n            )\n    return '&'.join(encoded_pairs)\n\n\ndef percent_encode(input_str, safe=SAFE_CHARS):\n    \"\"\"Urlencodes a string.\n\n    Whereas percent_encode_sequence handles taking a dict/sequence and\n    producing a percent encoded string, this function deals only with\n    taking a string (not a dict/sequence) and percent encoding it.\n\n    If given the binary type, will simply URL encode it. If given the\n    text type, will produce the binary type by UTF-8 encoding the\n    text. If given something else, will convert it to the text type\n    first.\n    \"\"\"\n    # If its not a binary or text string, make it a text string.\n    if not isinstance(input_str, (bytes, str)):\n        input_str = str(input_str)\n    # If it's not bytes, make it bytes by UTF-8 encoding it.\n    if not isinstance(input_str, bytes):\n        input_str = input_str.encode('utf-8')\n    return quote(input_str, safe=safe)\n\n\ndef _epoch_seconds_to_datetime(value, tzinfo):\n    \"\"\"Parse numerical epoch timestamps (seconds since 1970) into a\n    ``datetime.datetime`` in UTC using ``datetime.timedelta``. This is intended\n    as fallback when ``fromtimestamp`` raises ``OverflowError`` or ``OSError``.\n\n    :type value: float or int\n    :param value: The Unix timestamps as number.\n\n    :type tzinfo: callable\n    :param tzinfo: A ``datetime.tzinfo`` class or compatible callable.\n    \"\"\"\n    epoch_zero = datetime.datetime(1970, 1, 1, 0, 0, 0, tzinfo=tzutc())\n    epoch_zero_localized = epoch_zero.astimezone(tzinfo())\n    return epoch_zero_localized + datetime.timedelta(seconds=value)\n\n\ndef _parse_timestamp_with_tzinfo(value, tzinfo):\n    \"\"\"Parse timestamp with pluggable tzinfo options.\"\"\"\n    if isinstance(value, (int, float)):\n        # Possibly an epoch time.\n        return datetime.datetime.fromtimestamp(value, tzinfo())\n    else:\n        try:\n            return datetime.datetime.fromtimestamp(float(value), tzinfo())\n        except (TypeError, ValueError):\n            pass\n    try:\n        # In certain cases, a timestamp marked with GMT can be parsed into a\n        # different time zone, so here we provide a context which will\n        # enforce that GMT == UTC.\n        return dateutil.parser.parse(value, tzinfos={'GMT': tzutc()})\n    except (TypeError, ValueError) as e:\n        raise ValueError(f'Invalid timestamp \"{value}\": {e}')\n\n\ndef parse_timestamp(value):\n    \"\"\"Parse a timestamp into a datetime object.\n\n    Supported formats:\n\n        * iso8601\n        * rfc822\n        * epoch (value is an integer)\n\n    This will return a ``datetime.datetime`` object.\n\n    \"\"\"\n    tzinfo_options = get_tzinfo_options()\n    for tzinfo in tzinfo_options:\n        try:\n            return _parse_timestamp_with_tzinfo(value, tzinfo)\n        except (OSError, OverflowError) as e:\n            logger.debug(\n                'Unable to parse timestamp with \"%s\" timezone info.',\n                tzinfo.__name__,\n                exc_info=e,\n            )\n    # For numeric values attempt fallback to using fromtimestamp-free method.\n    # From Python's ``datetime.datetime.fromtimestamp`` documentation: \"This\n    # may raise ``OverflowError``, if the timestamp is out of the range of\n    # values supported by the platform C localtime() function, and ``OSError``\n    # on localtime() failure. It's common for this to be restricted to years\n    # from 1970 through 2038.\"\n    try:\n        numeric_value = float(value)\n    except (TypeError, ValueError):\n        pass\n    else:\n        try:\n            for tzinfo in tzinfo_options:\n                return _epoch_seconds_to_datetime(numeric_value, tzinfo=tzinfo)\n        except (OSError, OverflowError) as e:\n            logger.debug(\n                'Unable to parse timestamp using fallback method with \"%s\" '\n                'timezone info.',\n                tzinfo.__name__,\n                exc_info=e,\n            )\n    raise RuntimeError(\n        'Unable to calculate correct timezone offset for \"%s\"' % value\n    )\n\n\ndef parse_to_aware_datetime(value):\n    \"\"\"Converted the passed in value to a datetime object with tzinfo.\n\n    This function can be used to normalize all timestamp inputs.  This\n    function accepts a number of different types of inputs, but\n    will always return a datetime.datetime object with time zone\n    information.\n\n    The input param ``value`` can be one of several types:\n\n        * A datetime object (both naive and aware)\n        * An integer representing the epoch time (can also be a string\n          of the integer, i.e '0', instead of 0).  The epoch time is\n          considered to be UTC.\n        * An iso8601 formatted timestamp.  This does not need to be\n          a complete timestamp, it can contain just the date portion\n          without the time component.\n\n    The returned value will be a datetime object that will have tzinfo.\n    If no timezone info was provided in the input value, then UTC is\n    assumed, not local time.\n\n    \"\"\"\n    # This is a general purpose method that handles several cases of\n    # converting the provided value to a string timestamp suitable to be\n    # serialized to an http request. It can handle:\n    # 1) A datetime.datetime object.\n    if isinstance(value, _DatetimeClass):\n        datetime_obj = value\n    else:\n        # 2) A string object that's formatted as a timestamp.\n        #    We document this as being an iso8601 timestamp, although\n        #    parse_timestamp is a bit more flexible.\n        datetime_obj = parse_timestamp(value)\n    if datetime_obj.tzinfo is None:\n        # I think a case would be made that if no time zone is provided,\n        # we should use the local time.  However, to restore backwards\n        # compat, the previous behavior was to assume UTC, which is\n        # what we're going to do here.\n        datetime_obj = datetime_obj.replace(tzinfo=tzutc())\n    else:\n        datetime_obj = datetime_obj.astimezone(tzutc())\n    return datetime_obj\n\n\ndef datetime2timestamp(dt, default_timezone=None):\n    \"\"\"Calculate the timestamp based on the given datetime instance.\n\n    :type dt: datetime\n    :param dt: A datetime object to be converted into timestamp\n    :type default_timezone: tzinfo\n    :param default_timezone: If it is provided as None, we treat it as tzutc().\n                             But it is only used when dt is a naive datetime.\n    :returns: The timestamp\n    \"\"\"\n    epoch = datetime.datetime(1970, 1, 1)\n    if dt.tzinfo is None:\n        if default_timezone is None:\n            default_timezone = tzutc()\n        dt = dt.replace(tzinfo=default_timezone)\n    d = dt.replace(tzinfo=None) - dt.utcoffset() - epoch\n    return d.total_seconds()\n\n\ndef calculate_sha256(body, as_hex=False):\n    \"\"\"Calculate a sha256 checksum.\n\n    This method will calculate the sha256 checksum of a file like\n    object.  Note that this method will iterate through the entire\n    file contents.  The caller is responsible for ensuring the proper\n    starting position of the file and ``seek()``'ing the file back\n    to its starting location if other consumers need to read from\n    the file like object.\n\n    :param body: Any file like object.  The file must be opened\n        in binary mode such that a ``.read()`` call returns bytes.\n    :param as_hex: If True, then the hex digest is returned.\n        If False, then the digest (as binary bytes) is returned.\n\n    :returns: The sha256 checksum\n\n    \"\"\"\n    checksum = hashlib.sha256()\n    for chunk in iter(lambda: body.read(1024 * 1024), b''):\n        checksum.update(chunk)\n    if as_hex:\n        return checksum.hexdigest()\n    else:\n        return checksum.digest()\n\n\ndef calculate_tree_hash(body):\n    \"\"\"Calculate a tree hash checksum.\n\n    For more information see:\n\n    http://docs.aws.amazon.com/amazonglacier/latest/dev/checksum-calculations.html\n\n    :param body: Any file like object.  This has the same constraints as\n        the ``body`` param in calculate_sha256\n\n    :rtype: str\n    :returns: The hex version of the calculated tree hash\n\n    \"\"\"\n    chunks = []\n    required_chunk_size = 1024 * 1024\n    sha256 = hashlib.sha256\n    for chunk in iter(lambda: body.read(required_chunk_size), b''):\n        chunks.append(sha256(chunk).digest())\n    if not chunks:\n        return sha256(b'').hexdigest()\n    while len(chunks) > 1:\n        new_chunks = []\n        for first, second in _in_pairs(chunks):\n            if second is not None:\n                new_chunks.append(sha256(first + second).digest())\n            else:\n                # We're at the end of the list and there's no pair left.\n                new_chunks.append(first)\n        chunks = new_chunks\n    return binascii.hexlify(chunks[0]).decode('ascii')\n\n\ndef _in_pairs(iterable):\n    # Creates iterator that iterates over the list in pairs:\n    # for a, b in _in_pairs([0, 1, 2, 3, 4]):\n    #     print(a, b)\n    #\n    # will print:\n    # 0, 1\n    # 2, 3\n    # 4, None\n    shared_iter = iter(iterable)\n    # Note that zip_longest is a compat import that uses\n    # the itertools izip_longest.  This creates an iterator,\n    # this call below does _not_ immediately create the list\n    # of pairs.\n    return zip_longest(shared_iter, shared_iter)\n\n\nclass CachedProperty:\n    \"\"\"A read only property that caches the initially computed value.\n\n    This descriptor will only call the provided ``fget`` function once.\n    Subsequent access to this property will return the cached value.\n\n    \"\"\"\n\n    def __init__(self, fget):\n        self._fget = fget\n\n    def __get__(self, obj, cls):\n        if obj is None:\n            return self\n        else:\n            computed_value = self._fget(obj)\n            obj.__dict__[self._fget.__name__] = computed_value\n            return computed_value\n\n\nclass ArgumentGenerator:\n    \"\"\"Generate sample input based on a shape model.\n\n    This class contains a ``generate_skeleton`` method that will take\n    an input/output shape (created from ``botocore.model``) and generate\n    a sample dictionary corresponding to the input/output shape.\n\n    The specific values used are place holder values. For strings either an\n    empty string or the member name can be used, for numbers 0 or 0.0 is used.\n    The intended usage of this class is to generate the *shape* of the input\n    structure.\n\n    This can be useful for operations that have complex input shapes.\n    This allows a user to just fill in the necessary data instead of\n    worrying about the specific structure of the input arguments.\n\n    Example usage::\n\n        s = botocore.session.get_session()\n        ddb = s.get_service_model('dynamodb')\n        arg_gen = ArgumentGenerator()\n        sample_input = arg_gen.generate_skeleton(\n            ddb.operation_model('CreateTable').input_shape)\n        print(\"Sample input for dynamodb.CreateTable: %s\" % sample_input)\n\n    \"\"\"\n\n    def __init__(self, use_member_names=False):\n        self._use_member_names = use_member_names\n\n    def generate_skeleton(self, shape):\n        \"\"\"Generate a sample input.\n\n        :type shape: ``botocore.model.Shape``\n        :param shape: The input shape.\n\n        :return: The generated skeleton input corresponding to the\n            provided input shape.\n\n        \"\"\"\n        stack = []\n        return self._generate_skeleton(shape, stack)\n\n    def _generate_skeleton(self, shape, stack, name=''):\n        stack.append(shape.name)\n        try:\n            if shape.type_name == 'structure':\n                return self._generate_type_structure(shape, stack)\n            elif shape.type_name == 'list':\n                return self._generate_type_list(shape, stack)\n            elif shape.type_name == 'map':\n                return self._generate_type_map(shape, stack)\n            elif shape.type_name == 'string':\n                if self._use_member_names:\n                    return name\n                if shape.enum:\n                    return random.choice(shape.enum)\n                return ''\n            elif shape.type_name in ['integer', 'long']:\n                return 0\n            elif shape.type_name in ['float', 'double']:\n                return 0.0\n            elif shape.type_name == 'boolean':\n                return True\n            elif shape.type_name == 'timestamp':\n                return datetime.datetime(1970, 1, 1, 0, 0, 0)\n        finally:\n            stack.pop()\n\n    def _generate_type_structure(self, shape, stack):\n        if stack.count(shape.name) > 1:\n            return {}\n        skeleton = OrderedDict()\n        for member_name, member_shape in shape.members.items():\n            skeleton[member_name] = self._generate_skeleton(\n                member_shape, stack, name=member_name\n            )\n        return skeleton\n\n    def _generate_type_list(self, shape, stack):\n        # For list elements we've arbitrarily decided to\n        # return two elements for the skeleton list.\n        name = ''\n        if self._use_member_names:\n            name = shape.member.name\n        return [\n            self._generate_skeleton(shape.member, stack, name),\n        ]\n\n    def _generate_type_map(self, shape, stack):\n        key_shape = shape.key\n        value_shape = shape.value\n        assert key_shape.type_name == 'string'\n        return OrderedDict(\n            [\n                ('KeyName', self._generate_skeleton(value_shape, stack)),\n            ]\n        )\n\n\ndef is_valid_ipv6_endpoint_url(endpoint_url):\n    if UNSAFE_URL_CHARS.intersection(endpoint_url):\n        return False\n    hostname = f'[{urlparse(endpoint_url).hostname}]'\n    return IPV6_ADDRZ_RE.match(hostname) is not None\n\n\ndef is_valid_ipv4_endpoint_url(endpoint_url):\n    hostname = urlparse(endpoint_url).hostname\n    return IPV4_RE.match(hostname) is not None\n\n\ndef is_valid_endpoint_url(endpoint_url):\n    \"\"\"Verify the endpoint_url is valid.\n\n    :type endpoint_url: string\n    :param endpoint_url: An endpoint_url.  Must have at least a scheme\n        and a hostname.\n\n    :return: True if the endpoint url is valid. False otherwise.\n\n    \"\"\"\n    # post-bpo-43882 urlsplit() strips unsafe characters from URL, causing\n    # it to pass hostname validation below.  Detect them early to fix that.\n    if UNSAFE_URL_CHARS.intersection(endpoint_url):\n        return False\n    parts = urlsplit(endpoint_url)\n    hostname = parts.hostname\n    if hostname is None:\n        return False\n    if len(hostname) > 255:\n        return False\n    if hostname[-1] == \".\":\n        hostname = hostname[:-1]\n    allowed = re.compile(\n        r\"^((?!-)[A-Z\\d-]{1,63}(?<!-)\\.)*((?!-)[A-Z\\d-]{1,63}(?<!-))$\",\n        re.IGNORECASE,\n    )\n    return allowed.match(hostname)\n\n\ndef is_valid_uri(endpoint_url):\n    return is_valid_endpoint_url(endpoint_url) or is_valid_ipv6_endpoint_url(\n        endpoint_url\n    )\n\n\ndef validate_region_name(region_name):\n    \"\"\"Provided region_name must be a valid host label.\"\"\"\n    if region_name is None:\n        return\n    valid_host_label = re.compile(r'^(?![0-9]+$)(?!-)[a-zA-Z0-9-]{,63}(?<!-)$')\n    valid = valid_host_label.match(region_name)\n    if not valid:\n        raise InvalidRegionError(region_name=region_name)\n\n\ndef check_dns_name(bucket_name):\n    \"\"\"\n    Check to see if the ``bucket_name`` complies with the\n    restricted DNS naming conventions necessary to allow\n    access via virtual-hosting style.\n\n    Even though \".\" characters are perfectly valid in this DNS\n    naming scheme, we are going to punt on any name containing a\n    \".\" character because these will cause SSL cert validation\n    problems if we try to use virtual-hosting style addressing.\n    \"\"\"\n    if '.' in bucket_name:\n        return False\n    n = len(bucket_name)\n    if n < 3 or n > 63:\n        # Wrong length\n        return False\n    match = LABEL_RE.match(bucket_name)\n    if match is None or match.end() != len(bucket_name):\n        return False\n    return True\n\n\ndef fix_s3_host(\n    request,\n    signature_version,\n    region_name,\n    default_endpoint_url=None,\n    **kwargs,\n):\n    \"\"\"\n    This handler looks at S3 requests just before they are signed.\n    If there is a bucket name on the path (true for everything except\n    ListAllBuckets) it checks to see if that bucket name conforms to\n    the DNS naming conventions.  If it does, it alters the request to\n    use ``virtual hosting`` style addressing rather than ``path-style``\n    addressing.\n\n    \"\"\"\n    if request.context.get('use_global_endpoint', False):\n        default_endpoint_url = 's3.amazonaws.com'\n    try:\n        switch_to_virtual_host_style(\n            request, signature_version, default_endpoint_url\n        )\n    except InvalidDNSNameError as e:\n        bucket_name = e.kwargs['bucket_name']\n        logger.debug(\n            'Not changing URI, bucket is not DNS compatible: %s', bucket_name\n        )\n\n\ndef switch_to_virtual_host_style(\n    request, signature_version, default_endpoint_url=None, **kwargs\n):\n    \"\"\"\n    This is a handler to force virtual host style s3 addressing no matter\n    the signature version (which is taken in consideration for the default\n    case). If the bucket is not DNS compatible an InvalidDNSName is thrown.\n\n    :param request: A AWSRequest object that is about to be sent.\n    :param signature_version: The signature version to sign with\n    :param default_endpoint_url: The endpoint to use when switching to a\n        virtual style. If None is supplied, the virtual host will be\n        constructed from the url of the request.\n    \"\"\"\n    if request.auth_path is not None:\n        # The auth_path has already been applied (this may be a\n        # retried request).  We don't need to perform this\n        # customization again.\n        return\n    elif _is_get_bucket_location_request(request):\n        # For the GetBucketLocation response, we should not be using\n        # the virtual host style addressing so we can avoid any sigv4\n        # issues.\n        logger.debug(\n            \"Request is GetBucketLocation operation, not checking \"\n            \"for DNS compatibility.\"\n        )\n        return\n    parts = urlsplit(request.url)\n    request.auth_path = parts.path\n    path_parts = parts.path.split('/')\n\n    # Retrieve what the endpoint we will be prepending the bucket name to.\n    if default_endpoint_url is None:\n        default_endpoint_url = parts.netloc\n\n    if len(path_parts) > 1:\n        bucket_name = path_parts[1]\n        if not bucket_name:\n            # If the bucket name is empty we should not be checking for\n            # dns compatibility.\n            return\n        logger.debug('Checking for DNS compatible bucket for: %s', request.url)\n        if check_dns_name(bucket_name):\n            # If the operation is on a bucket, the auth_path must be\n            # terminated with a '/' character.\n            if len(path_parts) == 2:\n                if request.auth_path[-1] != '/':\n                    request.auth_path += '/'\n            path_parts.remove(bucket_name)\n            # At the very least the path must be a '/', such as with the\n            # CreateBucket operation when DNS style is being used. If this\n            # is not used you will get an empty path which is incorrect.\n            path = '/'.join(path_parts) or '/'\n            global_endpoint = default_endpoint_url\n            host = bucket_name + '.' + global_endpoint\n            new_tuple = (parts.scheme, host, path, parts.query, '')\n            new_uri = urlunsplit(new_tuple)\n            request.url = new_uri\n            logger.debug('URI updated to: %s', new_uri)\n        else:\n            raise InvalidDNSNameError(bucket_name=bucket_name)\n\n\ndef _is_get_bucket_location_request(request):\n    return request.url.endswith('?location')\n\n\ndef instance_cache(func):\n    \"\"\"Method decorator for caching method calls to a single instance.\n\n    **This is not a general purpose caching decorator.**\n\n    In order to use this, you *must* provide an ``_instance_cache``\n    attribute on the instance.\n\n    This decorator is used to cache method calls.  The cache is only\n    scoped to a single instance though such that multiple instances\n    will maintain their own cache.  In order to keep things simple,\n    this decorator requires that you provide an ``_instance_cache``\n    attribute on your instance.\n\n    \"\"\"\n    func_name = func.__name__\n\n    @functools.wraps(func)\n    def _cache_guard(self, *args, **kwargs):\n        cache_key = (func_name, args)\n        if kwargs:\n            kwarg_items = tuple(sorted(kwargs.items()))\n            cache_key = (func_name, args, kwarg_items)\n        result = self._instance_cache.get(cache_key)\n        if result is not None:\n            return result\n        result = func(self, *args, **kwargs)\n        self._instance_cache[cache_key] = result\n        return result\n\n    return _cache_guard\n\n\ndef lru_cache_weakref(*cache_args, **cache_kwargs):\n    \"\"\"\n    Version of functools.lru_cache that stores a weak reference to ``self``.\n\n    Serves the same purpose as :py:func:`instance_cache` but uses Python's\n    functools implementation which offers ``max_size`` and ``typed`` properties.\n\n    lru_cache is a global cache even when used on a method. The cache's\n    reference to ``self`` will prevent garbace collection of the object. This\n    wrapper around functools.lru_cache replaces the reference to ``self`` with\n    a weak reference to not interfere with garbage collection.\n    \"\"\"\n\n    def wrapper(func):\n        @functools.lru_cache(*cache_args, **cache_kwargs)\n        def func_with_weakref(weakref_to_self, *args, **kwargs):\n            return func(weakref_to_self(), *args, **kwargs)\n\n        @functools.wraps(func)\n        def inner(self, *args, **kwargs):\n            return func_with_weakref(weakref.ref(self), *args, **kwargs)\n\n        inner.cache_info = func_with_weakref.cache_info\n        return inner\n\n    return wrapper\n\n\ndef switch_host_s3_accelerate(request, operation_name, **kwargs):\n    \"\"\"Switches the current s3 endpoint with an S3 Accelerate endpoint\"\"\"\n\n    # Note that when registered the switching of the s3 host happens\n    # before it gets changed to virtual. So we are not concerned with ensuring\n    # that the bucket name is translated to the virtual style here and we\n    # can hard code the Accelerate endpoint.\n    parts = urlsplit(request.url).netloc.split('.')\n    parts = [p for p in parts if p in S3_ACCELERATE_WHITELIST]\n    endpoint = 'https://s3-accelerate.'\n    if len(parts) > 0:\n        endpoint += '.'.join(parts) + '.'\n    endpoint += 'amazonaws.com'\n\n    if operation_name in ['ListBuckets', 'CreateBucket', 'DeleteBucket']:\n        return\n    _switch_hosts(request, endpoint, use_new_scheme=False)\n\n\ndef switch_host_with_param(request, param_name):\n    \"\"\"Switches the host using a parameter value from a JSON request body\"\"\"\n    request_json = json.loads(request.data.decode('utf-8'))\n    if request_json.get(param_name):\n        new_endpoint = request_json[param_name]\n        _switch_hosts(request, new_endpoint)\n\n\ndef _switch_hosts(request, new_endpoint, use_new_scheme=True):\n    final_endpoint = _get_new_endpoint(\n        request.url, new_endpoint, use_new_scheme\n    )\n    request.url = final_endpoint\n\n\ndef _get_new_endpoint(original_endpoint, new_endpoint, use_new_scheme=True):\n    new_endpoint_components = urlsplit(new_endpoint)\n    original_endpoint_components = urlsplit(original_endpoint)\n    scheme = original_endpoint_components.scheme\n    if use_new_scheme:\n        scheme = new_endpoint_components.scheme\n    final_endpoint_components = (\n        scheme,\n        new_endpoint_components.netloc,\n        original_endpoint_components.path,\n        original_endpoint_components.query,\n        '',\n    )\n    final_endpoint = urlunsplit(final_endpoint_components)\n    logger.debug(f'Updating URI from {original_endpoint} to {final_endpoint}')\n    return final_endpoint\n\n\ndef deep_merge(base, extra):\n    \"\"\"Deeply two dictionaries, overriding existing keys in the base.\n\n    :param base: The base dictionary which will be merged into.\n    :param extra: The dictionary to merge into the base. Keys from this\n        dictionary will take precedence.\n    \"\"\"\n    for key in extra:\n        # If the key represents a dict on both given dicts, merge the sub-dicts\n        if (\n            key in base\n            and isinstance(base[key], dict)\n            and isinstance(extra[key], dict)\n        ):\n            deep_merge(base[key], extra[key])\n            continue\n\n        # Otherwise, set the key on the base to be the value of the extra.\n        base[key] = extra[key]\n\n\ndef hyphenize_service_id(service_id):\n    \"\"\"Translate the form used for event emitters.\n\n    :param service_id: The service_id to convert.\n    \"\"\"\n    return service_id.replace(' ', '-').lower()\n\n\nclass IdentityCache:\n    \"\"\"Base IdentityCache implementation for storing and retrieving\n    highly accessed credentials.\n\n    This class is not intended to be instantiated in user code.\n    \"\"\"\n\n    METHOD = \"base_identity_cache\"\n\n    def __init__(self, client, credential_cls):\n        self._client = client\n        self._credential_cls = credential_cls\n\n    def get_credentials(self, **kwargs):\n        callback = self.build_refresh_callback(**kwargs)\n        metadata = callback()\n        credential_entry = self._credential_cls.create_from_metadata(\n            metadata=metadata,\n            refresh_using=callback,\n            method=self.METHOD,\n            advisory_timeout=45,\n            mandatory_timeout=10,\n        )\n        return credential_entry\n\n    def build_refresh_callback(**kwargs):\n        \"\"\"Callback to be implemented by subclasses.\n\n        Returns a set of metadata to be converted into a new\n        credential instance.\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass S3ExpressIdentityCache(IdentityCache):\n    \"\"\"S3Express IdentityCache for retrieving and storing\n    credentials from CreateSession calls.\n\n    This class is not intended to be instantiated in user code.\n    \"\"\"\n\n    METHOD = \"s3express\"\n\n    def __init__(self, client, credential_cls):\n        self._client = client\n        self._credential_cls = credential_cls\n\n    @functools.lru_cache(maxsize=100)\n    def get_credentials(self, bucket):\n        return super().get_credentials(bucket=bucket)\n\n    def build_refresh_callback(self, bucket):\n        def refresher():\n            response = self._client.create_session(Bucket=bucket)\n            creds = response['Credentials']\n            expiration = self._serialize_if_needed(\n                creds['Expiration'], iso=True\n            )\n            return {\n                \"access_key\": creds['AccessKeyId'],\n                \"secret_key\": creds['SecretAccessKey'],\n                \"token\": creds['SessionToken'],\n                \"expiry_time\": expiration,\n            }\n\n        return refresher\n\n    def _serialize_if_needed(self, value, iso=False):\n        if isinstance(value, _DatetimeClass):\n            if iso:\n                return value.isoformat()\n            return value.strftime('%Y-%m-%dT%H:%M:%S%Z')\n        return value\n\n\nclass S3ExpressIdentityResolver:\n    def __init__(self, client, credential_cls, cache=None):\n        self._client = weakref.proxy(client)\n\n        if cache is None:\n            cache = S3ExpressIdentityCache(self._client, credential_cls)\n        self._cache = cache\n\n    def register(self, event_emitter=None):\n        logger.debug('Registering S3Express Identity Resolver')\n        emitter = event_emitter or self._client.meta.events\n        emitter.register('before-call.s3', self.apply_signing_cache_key)\n        emitter.register('before-sign.s3', self.resolve_s3express_identity)\n\n    def apply_signing_cache_key(self, params, context, **kwargs):\n        endpoint_properties = context.get('endpoint_properties', {})\n        backend = endpoint_properties.get('backend', None)\n\n        # Add cache key if Bucket supplied for s3express request\n        bucket_name = context.get('input_params', {}).get('Bucket')\n        if backend == 'S3Express' and bucket_name is not None:\n            context.setdefault('signing', {})\n            context['signing']['cache_key'] = bucket_name\n\n    def resolve_s3express_identity(\n        self,\n        request,\n        signing_name,\n        region_name,\n        signature_version,\n        request_signer,\n        operation_name,\n        **kwargs,\n    ):\n        signing_context = request.context.get('signing', {})\n        signing_name = signing_context.get('signing_name')\n        if signing_name == 's3express' and signature_version.startswith(\n            'v4-s3express'\n        ):\n            signing_context['identity_cache'] = self._cache\n            if 'cache_key' not in signing_context:\n                signing_context['cache_key'] = (\n                    request.context.get('s3_redirect', {})\n                    .get('params', {})\n                    .get('Bucket')\n                )\n\n\nclass S3RegionRedirectorv2:\n    \"\"\"Updated version of S3RegionRedirector for use when\n    EndpointRulesetResolver is in use for endpoint resolution.\n\n    This class is considered private and subject to abrupt breaking changes or\n    removal without prior announcement. Please do not use it directly.\n    \"\"\"\n\n    def __init__(self, endpoint_bridge, client, cache=None):\n        self._cache = cache or {}\n        self._client = weakref.proxy(client)\n\n    def register(self, event_emitter=None):\n        logger.debug('Registering S3 region redirector handler')\n        emitter = event_emitter or self._client.meta.events\n        emitter.register('needs-retry.s3', self.redirect_from_error)\n        emitter.register(\n            'before-parameter-build.s3', self.annotate_request_context\n        )\n        emitter.register(\n            'before-endpoint-resolution.s3', self.redirect_from_cache\n        )\n\n    def redirect_from_error(self, request_dict, response, operation, **kwargs):\n        \"\"\"\n        An S3 request sent to the wrong region will return an error that\n        contains the endpoint the request should be sent to. This handler\n        will add the redirect information to the signing context and then\n        redirect the request.\n        \"\"\"\n        if response is None:\n            # This could be none if there was a ConnectionError or other\n            # transport error.\n            return\n\n        redirect_ctx = request_dict.get('context', {}).get('s3_redirect', {})\n        if ArnParser.is_arn(redirect_ctx.get('bucket')):\n            logger.debug(\n                'S3 request was previously for an Accesspoint ARN, not '\n                'redirecting.'\n            )\n            return\n\n        if redirect_ctx.get('redirected'):\n            logger.debug(\n                'S3 request was previously redirected, not redirecting.'\n            )\n            return\n\n        error = response[1].get('Error', {})\n        error_code = error.get('Code')\n        response_metadata = response[1].get('ResponseMetadata', {})\n\n        # We have to account for 400 responses because\n        # if we sign a Head* request with the wrong region,\n        # we'll get a 400 Bad Request but we won't get a\n        # body saying it's an \"AuthorizationHeaderMalformed\".\n        is_special_head_object = (\n            error_code in ('301', '400') and operation.name == 'HeadObject'\n        )\n        is_special_head_bucket = (\n            error_code in ('301', '400')\n            and operation.name == 'HeadBucket'\n            and 'x-amz-bucket-region'\n            in response_metadata.get('HTTPHeaders', {})\n        )\n        is_wrong_signing_region = (\n            error_code == 'AuthorizationHeaderMalformed' and 'Region' in error\n        )\n        is_redirect_status = response[0] is not None and response[\n            0\n        ].status_code in (301, 302, 307)\n        is_permanent_redirect = error_code == 'PermanentRedirect'\n        if not any(\n            [\n                is_special_head_object,\n                is_wrong_signing_region,\n                is_permanent_redirect,\n                is_special_head_bucket,\n                is_redirect_status,\n            ]\n        ):\n            return\n\n        bucket = request_dict['context']['s3_redirect']['bucket']\n        client_region = request_dict['context'].get('client_region')\n        new_region = self.get_bucket_region(bucket, response)\n\n        if new_region is None:\n            logger.debug(\n                \"S3 client configured for region %s but the bucket %s is not \"\n                \"in that region and the proper region could not be \"\n                \"automatically determined.\" % (client_region, bucket)\n            )\n            return\n\n        logger.debug(\n            \"S3 client configured for region %s but the bucket %s is in region\"\n            \" %s; Please configure the proper region to avoid multiple \"\n            \"unnecessary redirects and signing attempts.\"\n            % (client_region, bucket, new_region)\n        )\n        # Adding the new region to _cache will make construct_endpoint() to\n        # use the new region as value for the AWS::Region builtin parameter.\n        self._cache[bucket] = new_region\n\n        # Re-resolve endpoint with new region and modify request_dict with\n        # the new URL, auth scheme, and signing context.\n        ep_resolver = self._client._ruleset_resolver\n        ep_info = ep_resolver.construct_endpoint(\n            operation_model=operation,\n            call_args=request_dict['context']['s3_redirect']['params'],\n            request_context=request_dict['context'],\n        )\n        request_dict['url'] = self.set_request_url(\n            request_dict['url'], ep_info.url\n        )\n        request_dict['context']['s3_redirect']['redirected'] = True\n        auth_schemes = ep_info.properties.get('authSchemes')\n        if auth_schemes is not None:\n            auth_info = ep_resolver.auth_schemes_to_signing_ctx(auth_schemes)\n            auth_type, signing_context = auth_info\n            request_dict['context']['auth_type'] = auth_type\n            request_dict['context']['signing'] = {\n                **request_dict['context'].get('signing', {}),\n                **signing_context,\n            }\n\n        # Return 0 so it doesn't wait to retry\n        return 0\n\n    def get_bucket_region(self, bucket, response):\n        \"\"\"\n        There are multiple potential sources for the new region to redirect to,\n        but they aren't all universally available for use. This will try to\n        find region from response elements, but will fall back to calling\n        HEAD on the bucket if all else fails.\n\n        :param bucket: The bucket to find the region for. This is necessary if\n            the region is not available in the error response.\n        :param response: A response representing a service request that failed\n            due to incorrect region configuration.\n        \"\"\"\n        # First try to source the region from the headers.\n        service_response = response[1]\n        response_headers = service_response['ResponseMetadata']['HTTPHeaders']\n        if 'x-amz-bucket-region' in response_headers:\n            return response_headers['x-amz-bucket-region']\n\n        # Next, check the error body\n        region = service_response.get('Error', {}).get('Region', None)\n        if region is not None:\n            return region\n\n        # Finally, HEAD the bucket. No other choice sadly.\n        try:\n            response = self._client.head_bucket(Bucket=bucket)\n            headers = response['ResponseMetadata']['HTTPHeaders']\n        except ClientError as e:\n            headers = e.response['ResponseMetadata']['HTTPHeaders']\n\n        region = headers.get('x-amz-bucket-region', None)\n        return region\n\n    def set_request_url(self, old_url, new_endpoint, **kwargs):\n        \"\"\"\n        Splice a new endpoint into an existing URL. Note that some endpoints\n        from the the endpoint provider have a path component which will be\n        discarded by this function.\n        \"\"\"\n        return _get_new_endpoint(old_url, new_endpoint, False)\n\n    def redirect_from_cache(self, builtins, params, **kwargs):\n        \"\"\"\n        If a bucket name has been redirected before, it is in the cache. This\n        handler will update the AWS::Region endpoint resolver builtin param\n        to use the region from cache instead of the client region to avoid the\n        redirect.\n        \"\"\"\n        bucket = params.get('Bucket')\n        if bucket is not None and bucket in self._cache:\n            new_region = self._cache.get(bucket)\n            builtins['AWS::Region'] = new_region\n\n    def annotate_request_context(self, params, context, **kwargs):\n        \"\"\"Store the bucket name in context for later use when redirecting.\n        The bucket name may be an access point ARN or alias.\n        \"\"\"\n        bucket = params.get('Bucket')\n        context['s3_redirect'] = {\n            'redirected': False,\n            'bucket': bucket,\n            'params': params,\n        }\n\n\nclass S3RegionRedirector:\n    \"\"\"This handler has been replaced by S3RegionRedirectorv2. The original\n    version remains in place for any third-party libraries that import it.\n    \"\"\"\n\n    def __init__(self, endpoint_bridge, client, cache=None):\n        self._endpoint_resolver = endpoint_bridge\n        self._cache = cache\n        if self._cache is None:\n            self._cache = {}\n\n        # This needs to be a weak ref in order to prevent memory leaks on\n        # python 2.6\n        self._client = weakref.proxy(client)\n\n        warnings.warn(\n            'The S3RegionRedirector class has been deprecated for a new '\n            'internal replacement. A future version of botocore may remove '\n            'this class.',\n            category=FutureWarning,\n        )\n\n    def register(self, event_emitter=None):\n        emitter = event_emitter or self._client.meta.events\n        emitter.register('needs-retry.s3', self.redirect_from_error)\n        emitter.register('before-call.s3', self.set_request_url)\n        emitter.register('before-parameter-build.s3', self.redirect_from_cache)\n\n    def redirect_from_error(self, request_dict, response, operation, **kwargs):\n        \"\"\"\n        An S3 request sent to the wrong region will return an error that\n        contains the endpoint the request should be sent to. This handler\n        will add the redirect information to the signing context and then\n        redirect the request.\n        \"\"\"\n        if response is None:\n            # This could be none if there was a ConnectionError or other\n            # transport error.\n            return\n\n        if self._is_s3_accesspoint(request_dict.get('context', {})):\n            logger.debug(\n                'S3 request was previously to an accesspoint, not redirecting.'\n            )\n            return\n\n        if request_dict.get('context', {}).get('s3_redirected'):\n            logger.debug(\n                'S3 request was previously redirected, not redirecting.'\n            )\n            return\n\n        error = response[1].get('Error', {})\n        error_code = error.get('Code')\n        response_metadata = response[1].get('ResponseMetadata', {})\n\n        # We have to account for 400 responses because\n        # if we sign a Head* request with the wrong region,\n        # we'll get a 400 Bad Request but we won't get a\n        # body saying it's an \"AuthorizationHeaderMalformed\".\n        is_special_head_object = (\n            error_code in ('301', '400') and operation.name == 'HeadObject'\n        )\n        is_special_head_bucket = (\n            error_code in ('301', '400')\n            and operation.name == 'HeadBucket'\n            and 'x-amz-bucket-region'\n            in response_metadata.get('HTTPHeaders', {})\n        )\n        is_wrong_signing_region = (\n            error_code == 'AuthorizationHeaderMalformed' and 'Region' in error\n        )\n        is_redirect_status = response[0] is not None and response[\n            0\n        ].status_code in (301, 302, 307)\n        is_permanent_redirect = error_code == 'PermanentRedirect'\n        if not any(\n            [\n                is_special_head_object,\n                is_wrong_signing_region,\n                is_permanent_redirect,\n                is_special_head_bucket,\n                is_redirect_status,\n            ]\n        ):\n            return\n\n        bucket = request_dict['context']['signing']['bucket']\n        client_region = request_dict['context'].get('client_region')\n        new_region = self.get_bucket_region(bucket, response)\n\n        if new_region is None:\n            logger.debug(\n                \"S3 client configured for region %s but the bucket %s is not \"\n                \"in that region and the proper region could not be \"\n                \"automatically determined.\" % (client_region, bucket)\n            )\n            return\n\n        logger.debug(\n            \"S3 client configured for region %s but the bucket %s is in region\"\n            \" %s; Please configure the proper region to avoid multiple \"\n            \"unnecessary redirects and signing attempts.\"\n            % (client_region, bucket, new_region)\n        )\n        endpoint = self._endpoint_resolver.resolve('s3', new_region)\n        endpoint = endpoint['endpoint_url']\n\n        signing_context = {\n            'region': new_region,\n            'bucket': bucket,\n            'endpoint': endpoint,\n        }\n        request_dict['context']['signing'] = signing_context\n\n        self._cache[bucket] = signing_context\n        self.set_request_url(request_dict, request_dict['context'])\n\n        request_dict['context']['s3_redirected'] = True\n\n        # Return 0 so it doesn't wait to retry\n        return 0\n\n    def get_bucket_region(self, bucket, response):\n        \"\"\"\n        There are multiple potential sources for the new region to redirect to,\n        but they aren't all universally available for use. This will try to\n        find region from response elements, but will fall back to calling\n        HEAD on the bucket if all else fails.\n\n        :param bucket: The bucket to find the region for. This is necessary if\n            the region is not available in the error response.\n        :param response: A response representing a service request that failed\n            due to incorrect region configuration.\n        \"\"\"\n        # First try to source the region from the headers.\n        service_response = response[1]\n        response_headers = service_response['ResponseMetadata']['HTTPHeaders']\n        if 'x-amz-bucket-region' in response_headers:\n            return response_headers['x-amz-bucket-region']\n\n        # Next, check the error body\n        region = service_response.get('Error', {}).get('Region', None)\n        if region is not None:\n            return region\n\n        # Finally, HEAD the bucket. No other choice sadly.\n        try:\n            response = self._client.head_bucket(Bucket=bucket)\n            headers = response['ResponseMetadata']['HTTPHeaders']\n        except ClientError as e:\n            headers = e.response['ResponseMetadata']['HTTPHeaders']\n\n        region = headers.get('x-amz-bucket-region', None)\n        return region\n\n    def set_request_url(self, params, context, **kwargs):\n        endpoint = context.get('signing', {}).get('endpoint', None)\n        if endpoint is not None:\n            params['url'] = _get_new_endpoint(params['url'], endpoint, False)\n\n    def redirect_from_cache(self, params, context, **kwargs):\n        \"\"\"\n        This handler retrieves a given bucket's signing context from the cache\n        and adds it into the request context.\n        \"\"\"\n        if self._is_s3_accesspoint(context):\n            return\n        bucket = params.get('Bucket')\n        signing_context = self._cache.get(bucket)\n        if signing_context is not None:\n            context['signing'] = signing_context\n        else:\n            context['signing'] = {'bucket': bucket}\n\n    def _is_s3_accesspoint(self, context):\n        return 's3_accesspoint' in context\n\n\nclass InvalidArnException(ValueError):\n    pass\n\n\nclass ArnParser:\n    def parse_arn(self, arn):\n        arn_parts = arn.split(':', 5)\n        if len(arn_parts) < 6:\n            raise InvalidArnException(\n                'Provided ARN: %s must be of the format: '\n                'arn:partition:service:region:account:resource' % arn\n            )\n        return {\n            'partition': arn_parts[1],\n            'service': arn_parts[2],\n            'region': arn_parts[3],\n            'account': arn_parts[4],\n            'resource': arn_parts[5],\n        }\n\n    @staticmethod\n    def is_arn(value):\n        if not isinstance(value, str) or not value.startswith('arn:'):\n            return False\n        arn_parser = ArnParser()\n        try:\n            arn_parser.parse_arn(value)\n            return True\n        except InvalidArnException:\n            return False\n\n\nclass S3ArnParamHandler:\n    _RESOURCE_REGEX = re.compile(\n        r'^(?P<resource_type>accesspoint|outpost)[/:](?P<resource_name>.+)$'\n    )\n    _OUTPOST_RESOURCE_REGEX = re.compile(\n        r'^(?P<outpost_name>[a-zA-Z0-9\\-]{1,63})[/:]accesspoint[/:]'\n        r'(?P<accesspoint_name>[a-zA-Z0-9\\-]{1,63}$)'\n    )\n    _BLACKLISTED_OPERATIONS = ['CreateBucket']\n\n    def __init__(self, arn_parser=None):\n        self._arn_parser = arn_parser\n        if arn_parser is None:\n            self._arn_parser = ArnParser()\n\n    def register(self, event_emitter):\n        event_emitter.register('before-parameter-build.s3', self.handle_arn)\n\n    def handle_arn(self, params, model, context, **kwargs):\n        if model.name in self._BLACKLISTED_OPERATIONS:\n            return\n        arn_details = self._get_arn_details_from_bucket_param(params)\n        if arn_details is None:\n            return\n        if arn_details['resource_type'] == 'accesspoint':\n            self._store_accesspoint(params, context, arn_details)\n        elif arn_details['resource_type'] == 'outpost':\n            self._store_outpost(params, context, arn_details)\n\n    def _get_arn_details_from_bucket_param(self, params):\n        if 'Bucket' in params:\n            try:\n                arn = params['Bucket']\n                arn_details = self._arn_parser.parse_arn(arn)\n                self._add_resource_type_and_name(arn, arn_details)\n                return arn_details\n            except InvalidArnException:\n                pass\n        return None\n\n    def _add_resource_type_and_name(self, arn, arn_details):\n        match = self._RESOURCE_REGEX.match(arn_details['resource'])\n        if match:\n            arn_details['resource_type'] = match.group('resource_type')\n            arn_details['resource_name'] = match.group('resource_name')\n        else:\n            raise UnsupportedS3ArnError(arn=arn)\n\n    def _store_accesspoint(self, params, context, arn_details):\n        # Ideally the access-point would be stored as a parameter in the\n        # request where the serializer would then know how to serialize it,\n        # but access-points are not modeled in S3 operations so it would fail\n        # validation. Instead, we set the access-point to the bucket parameter\n        # to have some value set when serializing the request and additional\n        # information on the context from the arn to use in forming the\n        # access-point endpoint.\n        params['Bucket'] = arn_details['resource_name']\n        context['s3_accesspoint'] = {\n            'name': arn_details['resource_name'],\n            'account': arn_details['account'],\n            'partition': arn_details['partition'],\n            'region': arn_details['region'],\n            'service': arn_details['service'],\n        }\n\n    def _store_outpost(self, params, context, arn_details):\n        resource_name = arn_details['resource_name']\n        match = self._OUTPOST_RESOURCE_REGEX.match(resource_name)\n        if not match:\n            raise UnsupportedOutpostResourceError(resource_name=resource_name)\n        # Because we need to set the bucket name to something to pass\n        # validation we're going to use the access point name to be consistent\n        # with normal access point arns.\n        accesspoint_name = match.group('accesspoint_name')\n        params['Bucket'] = accesspoint_name\n        context['s3_accesspoint'] = {\n            'outpost_name': match.group('outpost_name'),\n            'name': accesspoint_name,\n            'account': arn_details['account'],\n            'partition': arn_details['partition'],\n            'region': arn_details['region'],\n            'service': arn_details['service'],\n        }\n\n\nclass S3EndpointSetter:\n    _DEFAULT_PARTITION = 'aws'\n    _DEFAULT_DNS_SUFFIX = 'amazonaws.com'\n\n    def __init__(\n        self,\n        endpoint_resolver,\n        region=None,\n        s3_config=None,\n        endpoint_url=None,\n        partition=None,\n        use_fips_endpoint=False,\n    ):\n        # This is calling the endpoint_resolver in regions.py\n        self._endpoint_resolver = endpoint_resolver\n        self._region = region\n        self._s3_config = s3_config\n        self._use_fips_endpoint = use_fips_endpoint\n        if s3_config is None:\n            self._s3_config = {}\n        self._endpoint_url = endpoint_url\n        self._partition = partition\n        if partition is None:\n            self._partition = self._DEFAULT_PARTITION\n\n    def register(self, event_emitter):\n        event_emitter.register('before-sign.s3', self.set_endpoint)\n        event_emitter.register('choose-signer.s3', self.set_signer)\n        event_emitter.register(\n            'before-call.s3.WriteGetObjectResponse',\n            self.update_endpoint_to_s3_object_lambda,\n        )\n\n    def update_endpoint_to_s3_object_lambda(self, params, context, **kwargs):\n        if self._use_accelerate_endpoint:\n            raise UnsupportedS3ConfigurationError(\n                msg='S3 client does not support accelerate endpoints for S3 Object Lambda operations',\n            )\n\n        self._override_signing_name(context, 's3-object-lambda')\n        if self._endpoint_url:\n            # Only update the url if an explicit url was not provided\n            return\n\n        resolver = self._endpoint_resolver\n        # Constructing endpoints as s3-object-lambda as region\n        resolved = resolver.construct_endpoint(\n            's3-object-lambda', self._region\n        )\n\n        # Ideally we would be able to replace the endpoint before\n        # serialization but there's no event to do that currently\n        # host_prefix is all the arn/bucket specs\n        new_endpoint = 'https://{host_prefix}{hostname}'.format(\n            host_prefix=params['host_prefix'],\n            hostname=resolved['hostname'],\n        )\n\n        params['url'] = _get_new_endpoint(params['url'], new_endpoint, False)\n\n    def set_endpoint(self, request, **kwargs):\n        if self._use_accesspoint_endpoint(request):\n            self._validate_accesspoint_supported(request)\n            self._validate_fips_supported(request)\n            self._validate_global_regions(request)\n            region_name = self._resolve_region_for_accesspoint_endpoint(\n                request\n            )\n            self._resolve_signing_name_for_accesspoint_endpoint(request)\n            self._switch_to_accesspoint_endpoint(request, region_name)\n            return\n        if self._use_accelerate_endpoint:\n            if self._use_fips_endpoint:\n                raise UnsupportedS3ConfigurationError(\n                    msg=(\n                        'Client is configured to use the FIPS psuedo region '\n                        'for \"%s\", but S3 Accelerate does not have any FIPS '\n                        'compatible endpoints.' % (self._region)\n                    )\n                )\n            switch_host_s3_accelerate(request=request, **kwargs)\n        if self._s3_addressing_handler:\n            self._s3_addressing_handler(request=request, **kwargs)\n\n    def _use_accesspoint_endpoint(self, request):\n        return 's3_accesspoint' in request.context\n\n    def _validate_fips_supported(self, request):\n        if not self._use_fips_endpoint:\n            return\n        if 'fips' in request.context['s3_accesspoint']['region']:\n            raise UnsupportedS3AccesspointConfigurationError(\n                msg={'Invalid ARN, FIPS region not allowed in ARN.'}\n            )\n        if 'outpost_name' in request.context['s3_accesspoint']:\n            raise UnsupportedS3AccesspointConfigurationError(\n                msg=(\n                    'Client is configured to use the FIPS psuedo-region \"%s\", '\n                    'but outpost ARNs do not support FIPS endpoints.'\n                    % (self._region)\n                )\n            )\n        # Transforming psuedo region to actual region\n        accesspoint_region = request.context['s3_accesspoint']['region']\n        if accesspoint_region != self._region:\n            if not self._s3_config.get('use_arn_region', True):\n                # TODO: Update message to reflect use_arn_region\n                # is not set\n                raise UnsupportedS3AccesspointConfigurationError(\n                    msg=(\n                        'Client is configured to use the FIPS psuedo-region '\n                        'for \"%s\", but the access-point ARN provided is for '\n                        'the \"%s\" region. For clients using a FIPS '\n                        'psuedo-region calls to access-point ARNs in another '\n                        'region are not allowed.'\n                        % (self._region, accesspoint_region)\n                    )\n                )\n\n    def _validate_global_regions(self, request):\n        if self._s3_config.get('use_arn_region', True):\n            return\n        if self._region in ['aws-global', 's3-external-1']:\n            raise UnsupportedS3AccesspointConfigurationError(\n                msg=(\n                    'Client is configured to use the global psuedo-region '\n                    '\"%s\". When providing access-point ARNs a regional '\n                    'endpoint must be specified.' % self._region\n                )\n            )\n\n    def _validate_accesspoint_supported(self, request):\n        if self._use_accelerate_endpoint:\n            raise UnsupportedS3AccesspointConfigurationError(\n                msg=(\n                    'Client does not support s3 accelerate configuration '\n                    'when an access-point ARN is specified.'\n                )\n            )\n        request_partition = request.context['s3_accesspoint']['partition']\n        if request_partition != self._partition:\n            raise UnsupportedS3AccesspointConfigurationError(\n                msg=(\n                    'Client is configured for \"%s\" partition, but access-point'\n                    ' ARN provided is for \"%s\" partition. The client and '\n                    ' access-point partition must be the same.'\n                    % (self._partition, request_partition)\n                )\n            )\n        s3_service = request.context['s3_accesspoint'].get('service')\n        if s3_service == 's3-object-lambda' and self._s3_config.get(\n            'use_dualstack_endpoint'\n        ):\n            raise UnsupportedS3AccesspointConfigurationError(\n                msg=(\n                    'Client does not support s3 dualstack configuration '\n                    'when an S3 Object Lambda access point ARN is specified.'\n                )\n            )\n        outpost_name = request.context['s3_accesspoint'].get('outpost_name')\n        if outpost_name and self._s3_config.get('use_dualstack_endpoint'):\n            raise UnsupportedS3AccesspointConfigurationError(\n                msg=(\n                    'Client does not support s3 dualstack configuration '\n                    'when an outpost ARN is specified.'\n                )\n            )\n        self._validate_mrap_s3_config(request)\n\n    def _validate_mrap_s3_config(self, request):\n        if not is_global_accesspoint(request.context):\n            return\n        if self._s3_config.get('s3_disable_multiregion_access_points'):\n            raise UnsupportedS3AccesspointConfigurationError(\n                msg=(\n                    'Invalid configuration, Multi-Region Access Point '\n                    'ARNs are disabled.'\n                )\n            )\n        elif self._s3_config.get('use_dualstack_endpoint'):\n            raise UnsupportedS3AccesspointConfigurationError(\n                msg=(\n                    'Client does not support s3 dualstack configuration '\n                    'when a Multi-Region Access Point ARN is specified.'\n                )\n            )\n\n    def _resolve_region_for_accesspoint_endpoint(self, request):\n        if is_global_accesspoint(request.context):\n            # Requests going to MRAP endpoints MUST be set to any (*) region.\n            self._override_signing_region(request, '*')\n        elif self._s3_config.get('use_arn_region', True):\n            accesspoint_region = request.context['s3_accesspoint']['region']\n            # If we are using the region from the access point,\n            # we will also want to make sure that we set it as the\n            # signing region as well\n            self._override_signing_region(request, accesspoint_region)\n            return accesspoint_region\n        return self._region\n\n    def set_signer(self, context, **kwargs):\n        if is_global_accesspoint(context):\n            if HAS_CRT:\n                return 's3v4a'\n            else:\n                raise MissingDependencyException(\n                    msg=\"Using S3 with an MRAP arn requires an additional \"\n                    \"dependency. You will need to pip install \"\n                    \"botocore[crt] before proceeding.\"\n                )\n\n    def _resolve_signing_name_for_accesspoint_endpoint(self, request):\n        accesspoint_service = request.context['s3_accesspoint']['service']\n        self._override_signing_name(request.context, accesspoint_service)\n\n    def _switch_to_accesspoint_endpoint(self, request, region_name):\n        original_components = urlsplit(request.url)\n        accesspoint_endpoint = urlunsplit(\n            (\n                original_components.scheme,\n                self._get_netloc(request.context, region_name),\n                self._get_accesspoint_path(\n                    original_components.path, request.context\n                ),\n                original_components.query,\n                '',\n            )\n        )\n        logger.debug(\n            f'Updating URI from {request.url} to {accesspoint_endpoint}'\n        )\n        request.url = accesspoint_endpoint\n\n    def _get_netloc(self, request_context, region_name):\n        if is_global_accesspoint(request_context):\n            return self._get_mrap_netloc(request_context)\n        else:\n            return self._get_accesspoint_netloc(request_context, region_name)\n\n    def _get_mrap_netloc(self, request_context):\n        s3_accesspoint = request_context['s3_accesspoint']\n        region_name = 's3-global'\n        mrap_netloc_components = [s3_accesspoint['name']]\n        if self._endpoint_url:\n            endpoint_url_netloc = urlsplit(self._endpoint_url).netloc\n            mrap_netloc_components.append(endpoint_url_netloc)\n        else:\n            partition = s3_accesspoint['partition']\n            mrap_netloc_components.extend(\n                [\n                    'accesspoint',\n                    region_name,\n                    self._get_partition_dns_suffix(partition),\n                ]\n            )\n        return '.'.join(mrap_netloc_components)\n\n    def _get_accesspoint_netloc(self, request_context, region_name):\n        s3_accesspoint = request_context['s3_accesspoint']\n        accesspoint_netloc_components = [\n            '{}-{}'.format(s3_accesspoint['name'], s3_accesspoint['account']),\n        ]\n        outpost_name = s3_accesspoint.get('outpost_name')\n        if self._endpoint_url:\n            if outpost_name:\n                accesspoint_netloc_components.append(outpost_name)\n            endpoint_url_netloc = urlsplit(self._endpoint_url).netloc\n            accesspoint_netloc_components.append(endpoint_url_netloc)\n        else:\n            if outpost_name:\n                outpost_host = [outpost_name, 's3-outposts']\n                accesspoint_netloc_components.extend(outpost_host)\n            elif s3_accesspoint['service'] == 's3-object-lambda':\n                component = self._inject_fips_if_needed(\n                    's3-object-lambda', request_context\n                )\n                accesspoint_netloc_components.append(component)\n            else:\n                component = self._inject_fips_if_needed(\n                    's3-accesspoint', request_context\n                )\n                accesspoint_netloc_components.append(component)\n            if self._s3_config.get('use_dualstack_endpoint'):\n                accesspoint_netloc_components.append('dualstack')\n            accesspoint_netloc_components.extend(\n                [region_name, self._get_dns_suffix(region_name)]\n            )\n        return '.'.join(accesspoint_netloc_components)\n\n    def _inject_fips_if_needed(self, component, request_context):\n        if self._use_fips_endpoint:\n            return '%s-fips' % component\n        return component\n\n    def _get_accesspoint_path(self, original_path, request_context):\n        # The Bucket parameter was substituted with the access-point name as\n        # some value was required in serializing the bucket name. Now that\n        # we are making the request directly to the access point, we will\n        # want to remove that access-point name from the path.\n        name = request_context['s3_accesspoint']['name']\n        # All S3 operations require at least a / in their path.\n        return original_path.replace('/' + name, '', 1) or '/'\n\n    def _get_partition_dns_suffix(self, partition_name):\n        dns_suffix = self._endpoint_resolver.get_partition_dns_suffix(\n            partition_name\n        )\n        if dns_suffix is None:\n            dns_suffix = self._DEFAULT_DNS_SUFFIX\n        return dns_suffix\n\n    def _get_dns_suffix(self, region_name):\n        resolved = self._endpoint_resolver.construct_endpoint(\n            's3', region_name\n        )\n        dns_suffix = self._DEFAULT_DNS_SUFFIX\n        if resolved and 'dnsSuffix' in resolved:\n            dns_suffix = resolved['dnsSuffix']\n        return dns_suffix\n\n    def _override_signing_region(self, request, region_name):\n        signing_context = request.context.get('signing', {})\n        # S3SigV4Auth will use the context['signing']['region'] value to\n        # sign with if present. This is used by the Bucket redirector\n        # as well but we should be fine because the redirector is never\n        # used in combination with the accesspoint setting logic.\n        signing_context['region'] = region_name\n        request.context['signing'] = signing_context\n\n    def _override_signing_name(self, context, signing_name):\n        signing_context = context.get('signing', {})\n        # S3SigV4Auth will use the context['signing']['signing_name'] value to\n        # sign with if present. This is used by the Bucket redirector\n        # as well but we should be fine because the redirector is never\n        # used in combination with the accesspoint setting logic.\n        signing_context['signing_name'] = signing_name\n        context['signing'] = signing_context\n\n    @CachedProperty\n    def _use_accelerate_endpoint(self):\n        # Enable accelerate if the configuration is set to to true or the\n        # endpoint being used matches one of the accelerate endpoints.\n\n        # Accelerate has been explicitly configured.\n        if self._s3_config.get('use_accelerate_endpoint'):\n            return True\n\n        # Accelerate mode is turned on automatically if an endpoint url is\n        # provided that matches the accelerate scheme.\n        if self._endpoint_url is None:\n            return False\n\n        # Accelerate is only valid for Amazon endpoints.\n        netloc = urlsplit(self._endpoint_url).netloc\n        if not netloc.endswith('amazonaws.com'):\n            return False\n\n        # The first part of the url should always be s3-accelerate.\n        parts = netloc.split('.')\n        if parts[0] != 's3-accelerate':\n            return False\n\n        # Url parts between 's3-accelerate' and 'amazonaws.com' which\n        # represent different url features.\n        feature_parts = parts[1:-2]\n\n        # There should be no duplicate url parts.\n        if len(feature_parts) != len(set(feature_parts)):\n            return False\n\n        # Remaining parts must all be in the whitelist.\n        return all(p in S3_ACCELERATE_WHITELIST for p in feature_parts)\n\n    @CachedProperty\n    def _addressing_style(self):\n        # Use virtual host style addressing if accelerate is enabled or if\n        # the given endpoint url is an accelerate endpoint.\n        if self._use_accelerate_endpoint:\n            return 'virtual'\n\n        # If a particular addressing style is configured, use it.\n        configured_addressing_style = self._s3_config.get('addressing_style')\n        if configured_addressing_style:\n            return configured_addressing_style\n\n    @CachedProperty\n    def _s3_addressing_handler(self):\n        # If virtual host style was configured, use it regardless of whether\n        # or not the bucket looks dns compatible.\n        if self._addressing_style == 'virtual':\n            logger.debug(\"Using S3 virtual host style addressing.\")\n            return switch_to_virtual_host_style\n\n        # If path style is configured, no additional steps are needed. If\n        # endpoint_url was specified, don't default to virtual. We could\n        # potentially default provided endpoint urls to virtual hosted\n        # style, but for now it is avoided.\n        if self._addressing_style == 'path' or self._endpoint_url is not None:\n            logger.debug(\"Using S3 path style addressing.\")\n            return None\n\n        logger.debug(\n            \"Defaulting to S3 virtual host style addressing with \"\n            \"path style addressing fallback.\"\n        )\n\n        # By default, try to use virtual style with path fallback.\n        return fix_s3_host\n\n\nclass S3ControlEndpointSetter:\n    _DEFAULT_PARTITION = 'aws'\n    _DEFAULT_DNS_SUFFIX = 'amazonaws.com'\n    _HOST_LABEL_REGEX = re.compile(r'^[a-zA-Z0-9\\-]{1,63}$')\n\n    def __init__(\n        self,\n        endpoint_resolver,\n        region=None,\n        s3_config=None,\n        endpoint_url=None,\n        partition=None,\n        use_fips_endpoint=False,\n    ):\n        self._endpoint_resolver = endpoint_resolver\n        self._region = region\n        self._s3_config = s3_config\n        self._use_fips_endpoint = use_fips_endpoint\n        if s3_config is None:\n            self._s3_config = {}\n        self._endpoint_url = endpoint_url\n        self._partition = partition\n        if partition is None:\n            self._partition = self._DEFAULT_PARTITION\n\n    def register(self, event_emitter):\n        event_emitter.register('before-sign.s3-control', self.set_endpoint)\n\n    def set_endpoint(self, request, **kwargs):\n        if self._use_endpoint_from_arn_details(request):\n            self._validate_endpoint_from_arn_details_supported(request)\n            region_name = self._resolve_region_from_arn_details(request)\n            self._resolve_signing_name_from_arn_details(request)\n            self._resolve_endpoint_from_arn_details(request, region_name)\n            self._add_headers_from_arn_details(request)\n        elif self._use_endpoint_from_outpost_id(request):\n            self._validate_outpost_redirection_valid(request)\n            self._override_signing_name(request, 's3-outposts')\n            new_netloc = self._construct_outpost_endpoint(self._region)\n            self._update_request_netloc(request, new_netloc)\n\n    def _use_endpoint_from_arn_details(self, request):\n        return 'arn_details' in request.context\n\n    def _use_endpoint_from_outpost_id(self, request):\n        return 'outpost_id' in request.context\n\n    def _validate_endpoint_from_arn_details_supported(self, request):\n        if 'fips' in request.context['arn_details']['region']:\n            raise UnsupportedS3ControlArnError(\n                arn=request.context['arn_details']['original'],\n                msg='Invalid ARN, FIPS region not allowed in ARN.',\n            )\n        if not self._s3_config.get('use_arn_region', False):\n            arn_region = request.context['arn_details']['region']\n            if arn_region != self._region:\n                error_msg = (\n                    'The use_arn_region configuration is disabled but '\n                    'received arn for \"%s\" when the client is configured '\n                    'to use \"%s\"'\n                ) % (arn_region, self._region)\n                raise UnsupportedS3ControlConfigurationError(msg=error_msg)\n        request_partion = request.context['arn_details']['partition']\n        if request_partion != self._partition:\n            raise UnsupportedS3ControlConfigurationError(\n                msg=(\n                    'Client is configured for \"%s\" partition, but arn '\n                    'provided is for \"%s\" partition. The client and '\n                    'arn partition must be the same.'\n                    % (self._partition, request_partion)\n                )\n            )\n        if self._s3_config.get('use_accelerate_endpoint'):\n            raise UnsupportedS3ControlConfigurationError(\n                msg='S3 control client does not support accelerate endpoints',\n            )\n        if 'outpost_name' in request.context['arn_details']:\n            self._validate_outpost_redirection_valid(request)\n\n    def _validate_outpost_redirection_valid(self, request):\n        if self._s3_config.get('use_dualstack_endpoint'):\n            raise UnsupportedS3ControlConfigurationError(\n                msg=(\n                    'Client does not support s3 dualstack configuration '\n                    'when an outpost is specified.'\n                )\n            )\n\n    def _resolve_region_from_arn_details(self, request):\n        if self._s3_config.get('use_arn_region', False):\n            arn_region = request.context['arn_details']['region']\n            # If we are using the region from the expanded arn, we will also\n            # want to make sure that we set it as the signing region as well\n            self._override_signing_region(request, arn_region)\n            return arn_region\n        return self._region\n\n    def _resolve_signing_name_from_arn_details(self, request):\n        arn_service = request.context['arn_details']['service']\n        self._override_signing_name(request, arn_service)\n        return arn_service\n\n    def _resolve_endpoint_from_arn_details(self, request, region_name):\n        new_netloc = self._resolve_netloc_from_arn_details(\n            request, region_name\n        )\n        self._update_request_netloc(request, new_netloc)\n\n    def _update_request_netloc(self, request, new_netloc):\n        original_components = urlsplit(request.url)\n        arn_details_endpoint = urlunsplit(\n            (\n                original_components.scheme,\n                new_netloc,\n                original_components.path,\n                original_components.query,\n                '',\n            )\n        )\n        logger.debug(\n            f'Updating URI from {request.url} to {arn_details_endpoint}'\n        )\n        request.url = arn_details_endpoint\n\n    def _resolve_netloc_from_arn_details(self, request, region_name):\n        arn_details = request.context['arn_details']\n        if 'outpost_name' in arn_details:\n            return self._construct_outpost_endpoint(region_name)\n        account = arn_details['account']\n        return self._construct_s3_control_endpoint(region_name, account)\n\n    def _is_valid_host_label(self, label):\n        return self._HOST_LABEL_REGEX.match(label)\n\n    def _validate_host_labels(self, *labels):\n        for label in labels:\n            if not self._is_valid_host_label(label):\n                raise InvalidHostLabelError(label=label)\n\n    def _construct_s3_control_endpoint(self, region_name, account):\n        self._validate_host_labels(region_name, account)\n        if self._endpoint_url:\n            endpoint_url_netloc = urlsplit(self._endpoint_url).netloc\n            netloc = [account, endpoint_url_netloc]\n        else:\n            netloc = [\n                account,\n                's3-control',\n            ]\n            self._add_dualstack(netloc)\n            dns_suffix = self._get_dns_suffix(region_name)\n            netloc.extend([region_name, dns_suffix])\n        return self._construct_netloc(netloc)\n\n    def _construct_outpost_endpoint(self, region_name):\n        self._validate_host_labels(region_name)\n        if self._endpoint_url:\n            return urlsplit(self._endpoint_url).netloc\n        else:\n            netloc = [\n                's3-outposts',\n                region_name,\n                self._get_dns_suffix(region_name),\n            ]\n            self._add_fips(netloc)\n        return self._construct_netloc(netloc)\n\n    def _construct_netloc(self, netloc):\n        return '.'.join(netloc)\n\n    def _add_fips(self, netloc):\n        if self._use_fips_endpoint:\n            netloc[0] = netloc[0] + '-fips'\n\n    def _add_dualstack(self, netloc):\n        if self._s3_config.get('use_dualstack_endpoint'):\n            netloc.append('dualstack')\n\n    def _get_dns_suffix(self, region_name):\n        resolved = self._endpoint_resolver.construct_endpoint(\n            's3', region_name\n        )\n        dns_suffix = self._DEFAULT_DNS_SUFFIX\n        if resolved and 'dnsSuffix' in resolved:\n            dns_suffix = resolved['dnsSuffix']\n        return dns_suffix\n\n    def _override_signing_region(self, request, region_name):\n        signing_context = request.context.get('signing', {})\n        # S3SigV4Auth will use the context['signing']['region'] value to\n        # sign with if present. This is used by the Bucket redirector\n        # as well but we should be fine because the redirector is never\n        # used in combination with the accesspoint setting logic.\n        signing_context['region'] = region_name\n        request.context['signing'] = signing_context\n\n    def _override_signing_name(self, request, signing_name):\n        signing_context = request.context.get('signing', {})\n        # S3SigV4Auth will use the context['signing']['signing_name'] value to\n        # sign with if present. This is used by the Bucket redirector\n        # as well but we should be fine because the redirector is never\n        # used in combination with the accesspoint setting logic.\n        signing_context['signing_name'] = signing_name\n        request.context['signing'] = signing_context\n\n    def _add_headers_from_arn_details(self, request):\n        arn_details = request.context['arn_details']\n        outpost_name = arn_details.get('outpost_name')\n        if outpost_name:\n            self._add_outpost_id_header(request, outpost_name)\n\n    def _add_outpost_id_header(self, request, outpost_name):\n        request.headers['x-amz-outpost-id'] = outpost_name\n\n\nclass S3ControlArnParamHandler:\n    \"\"\"This handler has been replaced by S3ControlArnParamHandlerv2. The\n    original version remains in place for any third-party importers.\n    \"\"\"\n\n    _RESOURCE_SPLIT_REGEX = re.compile(r'[/:]')\n\n    def __init__(self, arn_parser=None):\n        self._arn_parser = arn_parser\n        if arn_parser is None:\n            self._arn_parser = ArnParser()\n        warnings.warn(\n            'The S3ControlArnParamHandler class has been deprecated for a new '\n            'internal replacement. A future version of botocore may remove '\n            'this class.',\n            category=FutureWarning,\n        )\n\n    def register(self, event_emitter):\n        event_emitter.register(\n            'before-parameter-build.s3-control',\n            self.handle_arn,\n        )\n\n    def handle_arn(self, params, model, context, **kwargs):\n        if model.name in ('CreateBucket', 'ListRegionalBuckets'):\n            # CreateBucket and ListRegionalBuckets are special cases that do\n            # not obey ARN based redirection but will redirect based off of the\n            # presence of the OutpostId parameter\n            self._handle_outpost_id_param(params, model, context)\n        else:\n            self._handle_name_param(params, model, context)\n            self._handle_bucket_param(params, model, context)\n\n    def _get_arn_details_from_param(self, params, param_name):\n        if param_name not in params:\n            return None\n        try:\n            arn = params[param_name]\n            arn_details = self._arn_parser.parse_arn(arn)\n            arn_details['original'] = arn\n            arn_details['resources'] = self._split_resource(arn_details)\n            return arn_details\n        except InvalidArnException:\n            return None\n\n    def _split_resource(self, arn_details):\n        return self._RESOURCE_SPLIT_REGEX.split(arn_details['resource'])\n\n    def _override_account_id_param(self, params, arn_details):\n        account_id = arn_details['account']\n        if 'AccountId' in params and params['AccountId'] != account_id:\n            error_msg = (\n                'Account ID in arn does not match the AccountId parameter '\n                'provided: \"%s\"'\n            ) % params['AccountId']\n            raise UnsupportedS3ControlArnError(\n                arn=arn_details['original'],\n                msg=error_msg,\n            )\n        params['AccountId'] = account_id\n\n    def _handle_outpost_id_param(self, params, model, context):\n        if 'OutpostId' not in params:\n            return\n        context['outpost_id'] = params['OutpostId']\n\n    def _handle_name_param(self, params, model, context):\n        # CreateAccessPoint is a special case that does not expand Name\n        if model.name == 'CreateAccessPoint':\n            return\n        arn_details = self._get_arn_details_from_param(params, 'Name')\n        if arn_details is None:\n            return\n        if self._is_outpost_accesspoint(arn_details):\n            self._store_outpost_accesspoint(params, context, arn_details)\n        else:\n            error_msg = 'The Name parameter does not support the provided ARN'\n            raise UnsupportedS3ControlArnError(\n                arn=arn_details['original'],\n                msg=error_msg,\n            )\n\n    def _is_outpost_accesspoint(self, arn_details):\n        if arn_details['service'] != 's3-outposts':\n            return False\n        resources = arn_details['resources']\n        if len(resources) != 4:\n            return False\n        # Resource must be of the form outpost/op-123/accesspoint/name\n        return resources[0] == 'outpost' and resources[2] == 'accesspoint'\n\n    def _store_outpost_accesspoint(self, params, context, arn_details):\n        self._override_account_id_param(params, arn_details)\n        accesspoint_name = arn_details['resources'][3]\n        params['Name'] = accesspoint_name\n        arn_details['accesspoint_name'] = accesspoint_name\n        arn_details['outpost_name'] = arn_details['resources'][1]\n        context['arn_details'] = arn_details\n\n    def _handle_bucket_param(self, params, model, context):\n        arn_details = self._get_arn_details_from_param(params, 'Bucket')\n        if arn_details is None:\n            return\n        if self._is_outpost_bucket(arn_details):\n            self._store_outpost_bucket(params, context, arn_details)\n        else:\n            error_msg = (\n                'The Bucket parameter does not support the provided ARN'\n            )\n            raise UnsupportedS3ControlArnError(\n                arn=arn_details['original'],\n                msg=error_msg,\n            )\n\n    def _is_outpost_bucket(self, arn_details):\n        if arn_details['service'] != 's3-outposts':\n            return False\n        resources = arn_details['resources']\n        if len(resources) != 4:\n            return False\n        # Resource must be of the form outpost/op-123/bucket/name\n        return resources[0] == 'outpost' and resources[2] == 'bucket'\n\n    def _store_outpost_bucket(self, params, context, arn_details):\n        self._override_account_id_param(params, arn_details)\n        bucket_name = arn_details['resources'][3]\n        params['Bucket'] = bucket_name\n        arn_details['bucket_name'] = bucket_name\n        arn_details['outpost_name'] = arn_details['resources'][1]\n        context['arn_details'] = arn_details\n\n\nclass S3ControlArnParamHandlerv2(S3ControlArnParamHandler):\n    \"\"\"Updated version of S3ControlArnParamHandler for use when\n    EndpointRulesetResolver is in use for endpoint resolution.\n\n    This class is considered private and subject to abrupt breaking changes or\n    removal without prior announcement. Please do not use it directly.\n    \"\"\"\n\n    def __init__(self, arn_parser=None):\n        self._arn_parser = arn_parser\n        if arn_parser is None:\n            self._arn_parser = ArnParser()\n\n    def register(self, event_emitter):\n        event_emitter.register(\n            'before-endpoint-resolution.s3-control',\n            self.handle_arn,\n        )\n\n    def _handle_name_param(self, params, model, context):\n        # CreateAccessPoint is a special case that does not expand Name\n        if model.name == 'CreateAccessPoint':\n            return\n        arn_details = self._get_arn_details_from_param(params, 'Name')\n        if arn_details is None:\n            return\n        self._raise_for_fips_pseudo_region(arn_details)\n        self._raise_for_accelerate_endpoint(context)\n        if self._is_outpost_accesspoint(arn_details):\n            self._store_outpost_accesspoint(params, context, arn_details)\n        else:\n            error_msg = 'The Name parameter does not support the provided ARN'\n            raise UnsupportedS3ControlArnError(\n                arn=arn_details['original'],\n                msg=error_msg,\n            )\n\n    def _store_outpost_accesspoint(self, params, context, arn_details):\n        self._override_account_id_param(params, arn_details)\n\n    def _handle_bucket_param(self, params, model, context):\n        arn_details = self._get_arn_details_from_param(params, 'Bucket')\n        if arn_details is None:\n            return\n        self._raise_for_fips_pseudo_region(arn_details)\n        self._raise_for_accelerate_endpoint(context)\n        if self._is_outpost_bucket(arn_details):\n            self._store_outpost_bucket(params, context, arn_details)\n        else:\n            error_msg = (\n                'The Bucket parameter does not support the provided ARN'\n            )\n            raise UnsupportedS3ControlArnError(\n                arn=arn_details['original'],\n                msg=error_msg,\n            )\n\n    def _store_outpost_bucket(self, params, context, arn_details):\n        self._override_account_id_param(params, arn_details)\n\n    def _raise_for_fips_pseudo_region(self, arn_details):\n        # FIPS pseudo region names cannot be used in ARNs\n        arn_region = arn_details['region']\n        if arn_region.startswith('fips-') or arn_region.endswith('fips-'):\n            raise UnsupportedS3ControlArnError(\n                arn=arn_details['original'],\n                msg='Invalid ARN, FIPS region not allowed in ARN.',\n            )\n\n    def _raise_for_accelerate_endpoint(self, context):\n        s3_config = context['client_config'].s3 or {}\n        if s3_config.get('use_accelerate_endpoint'):\n            raise UnsupportedS3ControlConfigurationError(\n                msg='S3 control client does not support accelerate endpoints',\n            )\n\n\nclass ContainerMetadataFetcher:\n    TIMEOUT_SECONDS = 2\n    RETRY_ATTEMPTS = 3\n    SLEEP_TIME = 1\n    IP_ADDRESS = '169.254.170.2'\n    _ALLOWED_HOSTS = [\n        IP_ADDRESS,\n        '169.254.170.23',\n        'fd00:ec2::23',\n        'localhost',\n    ]\n\n    def __init__(self, session=None, sleep=time.sleep):\n        if session is None:\n            session = botocore.httpsession.URLLib3Session(\n                timeout=self.TIMEOUT_SECONDS\n            )\n        self._session = session\n        self._sleep = sleep\n\n    def retrieve_full_uri(self, full_url, headers=None):\n        \"\"\"Retrieve JSON metadata from container metadata.\n\n        :type full_url: str\n        :param full_url: The full URL of the metadata service.\n            This should include the scheme as well, e.g\n            \"http://localhost:123/foo\"\n\n        \"\"\"\n        self._validate_allowed_url(full_url)\n        return self._retrieve_credentials(full_url, headers)\n\n    def _validate_allowed_url(self, full_url):\n        parsed = botocore.compat.urlparse(full_url)\n        if self._is_loopback_address(parsed.hostname):\n            return\n        is_whitelisted_host = self._check_if_whitelisted_host(parsed.hostname)\n        if not is_whitelisted_host:\n            raise ValueError(\n                f\"Unsupported host '{parsed.hostname}'.  Can only retrieve metadata \"\n                f\"from a loopback address or one of these hosts: {', '.join(self._ALLOWED_HOSTS)}\"\n            )\n\n    def _is_loopback_address(self, hostname):\n        try:\n            ip = ip_address(hostname)\n            return ip.is_loopback\n        except ValueError:\n            return False\n\n    def _check_if_whitelisted_host(self, host):\n        if host in self._ALLOWED_HOSTS:\n            return True\n        return False\n\n    def retrieve_uri(self, relative_uri):\n        \"\"\"Retrieve JSON metadata from container metadata.\n\n        :type relative_uri: str\n        :param relative_uri: A relative URI, e.g \"/foo/bar?id=123\"\n\n        :return: The parsed JSON response.\n\n        \"\"\"\n        full_url = self.full_url(relative_uri)\n        return self._retrieve_credentials(full_url)\n\n    def _retrieve_credentials(self, full_url, extra_headers=None):\n        headers = {'Accept': 'application/json'}\n        if extra_headers is not None:\n            headers.update(extra_headers)\n        attempts = 0\n        while True:\n            try:\n                return self._get_response(\n                    full_url, headers, self.TIMEOUT_SECONDS\n                )\n            except MetadataRetrievalError as e:\n                logger.debug(\n                    \"Received error when attempting to retrieve \"\n                    \"container metadata: %s\",\n                    e,\n                    exc_info=True,\n                )\n                self._sleep(self.SLEEP_TIME)\n                attempts += 1\n                if attempts >= self.RETRY_ATTEMPTS:\n                    raise\n\n    def _get_response(self, full_url, headers, timeout):\n        try:\n            AWSRequest = botocore.awsrequest.AWSRequest\n            request = AWSRequest(method='GET', url=full_url, headers=headers)\n            response = self._session.send(request.prepare())\n            response_text = response.content.decode('utf-8')\n            if response.status_code != 200:\n                raise MetadataRetrievalError(\n                    error_msg=(\n                        f\"Received non 200 response {response.status_code} \"\n                        f\"from container metadata: {response_text}\"\n                    )\n                )\n            try:\n                return json.loads(response_text)\n            except ValueError:\n                error_msg = \"Unable to parse JSON returned from container metadata services\"\n                logger.debug('%s:%s', error_msg, response_text)\n                raise MetadataRetrievalError(error_msg=error_msg)\n        except RETRYABLE_HTTP_ERRORS as e:\n            error_msg = (\n                \"Received error when attempting to retrieve \"\n                f\"container metadata: {e}\"\n            )\n            raise MetadataRetrievalError(error_msg=error_msg)\n\n    def full_url(self, relative_uri):\n        return f'http://{self.IP_ADDRESS}{relative_uri}'\n\n\ndef get_environ_proxies(url):\n    if should_bypass_proxies(url):\n        return {}\n    else:\n        return getproxies()\n\n\ndef should_bypass_proxies(url):\n    \"\"\"\n    Returns whether we should bypass proxies or not.\n    \"\"\"\n    # NOTE: requests allowed for ip/cidr entries in no_proxy env that we don't\n    # support current as urllib only checks DNS suffix\n    # If the system proxy settings indicate that this URL should be bypassed,\n    # don't proxy.\n    # The proxy_bypass function is incredibly buggy on OS X in early versions\n    # of Python 2.6, so allow this call to fail. Only catch the specific\n    # exceptions we've seen, though: this call failing in other ways can reveal\n    # legitimate problems.\n    try:\n        if proxy_bypass(urlparse(url).netloc):\n            return True\n    except (TypeError, socket.gaierror):\n        pass\n\n    return False\n\n\ndef determine_content_length(body):\n    # No body, content length of 0\n    if not body:\n        return 0\n\n    # Try asking the body for it's length\n    try:\n        return len(body)\n    except (AttributeError, TypeError):\n        pass\n\n    # Try getting the length from a seekable stream\n    if hasattr(body, 'seek') and hasattr(body, 'tell'):\n        try:\n            orig_pos = body.tell()\n            body.seek(0, 2)\n            end_file_pos = body.tell()\n            body.seek(orig_pos)\n            return end_file_pos - orig_pos\n        except io.UnsupportedOperation:\n            # in case when body is, for example, io.BufferedIOBase object\n            # it has \"seek\" method which throws \"UnsupportedOperation\"\n            # exception in such case we want to fall back to \"chunked\"\n            # encoding\n            pass\n    # Failed to determine the length\n    return None\n\n\ndef get_encoding_from_headers(headers, default='ISO-8859-1'):\n    \"\"\"Returns encodings from given HTTP Header Dict.\n\n    :param headers: dictionary to extract encoding from.\n    :param default: default encoding if the content-type is text\n    \"\"\"\n\n    content_type = headers.get('content-type')\n\n    if not content_type:\n        return None\n\n    message = email.message.Message()\n    message['content-type'] = content_type\n    charset = message.get_param(\"charset\")\n\n    if charset is not None:\n        return charset\n\n    if 'text' in content_type:\n        return default\n\n\ndef calculate_md5(body, **kwargs):\n    if isinstance(body, (bytes, bytearray)):\n        binary_md5 = _calculate_md5_from_bytes(body)\n    else:\n        binary_md5 = _calculate_md5_from_file(body)\n    return base64.b64encode(binary_md5).decode('ascii')\n\n\ndef _calculate_md5_from_bytes(body_bytes):\n    md5 = get_md5(body_bytes)\n    return md5.digest()\n\n\ndef _calculate_md5_from_file(fileobj):\n    start_position = fileobj.tell()\n    md5 = get_md5()\n    for chunk in iter(lambda: fileobj.read(1024 * 1024), b''):\n        md5.update(chunk)\n    fileobj.seek(start_position)\n    return md5.digest()\n\n\ndef _is_s3express_request(params):\n    endpoint_properties = params.get('context', {}).get(\n        'endpoint_properties', {}\n    )\n    return endpoint_properties.get('backend') == 'S3Express'\n\n\ndef _has_checksum_header(params):\n    headers = params['headers']\n    # If a user provided Content-MD5 is present,\n    # don't try to compute a new one.\n    if 'Content-MD5' in headers:\n        return True\n\n    # If a header matching the x-amz-checksum-* pattern is present, we\n    # assume a checksum has already been provided and an md5 is not needed\n    for header in headers:\n        if CHECKSUM_HEADER_PATTERN.match(header):\n            return True\n\n    return False\n\n\ndef conditionally_calculate_checksum(params, **kwargs):\n    if not _has_checksum_header(params):\n        conditionally_calculate_md5(params, **kwargs)\n        conditionally_enable_crc32(params, **kwargs)\n\n\ndef conditionally_enable_crc32(params, **kwargs):\n    checksum_context = params.get('context', {}).get('checksum', {})\n    checksum_algorithm = checksum_context.get('request_algorithm')\n    if (\n        _is_s3express_request(params)\n        and params['body'] is not None\n        and checksum_algorithm in (None, \"conditional-md5\")\n    ):\n        params['context']['checksum'] = {\n            'request_algorithm': {\n                'algorithm': 'crc32',\n                'in': 'header',\n                'name': 'x-amz-checksum-crc32',\n            }\n        }\n\n\ndef conditionally_calculate_md5(params, **kwargs):\n    \"\"\"Only add a Content-MD5 if the system supports it.\"\"\"\n    body = params['body']\n    checksum_context = params.get('context', {}).get('checksum', {})\n    checksum_algorithm = checksum_context.get('request_algorithm')\n    if checksum_algorithm and checksum_algorithm != 'conditional-md5':\n        # Skip for requests that will have a flexible checksum applied\n        return\n\n    if _has_checksum_header(params):\n        # Don't add a new header if one is already available.\n        return\n\n    if _is_s3express_request(params):\n        # S3Express doesn't support MD5\n        return\n\n    if MD5_AVAILABLE and body is not None:\n        md5_digest = calculate_md5(body, **kwargs)\n        params['headers']['Content-MD5'] = md5_digest\n\n\nclass FileWebIdentityTokenLoader:\n    def __init__(self, web_identity_token_path, _open=open):\n        self._web_identity_token_path = web_identity_token_path\n        self._open = _open\n\n    def __call__(self):\n        with self._open(self._web_identity_token_path) as token_file:\n            return token_file.read()\n\n\nclass SSOTokenLoader:\n    def __init__(self, cache=None):\n        if cache is None:\n            cache = {}\n        self._cache = cache\n\n    def _generate_cache_key(self, start_url, session_name):\n        input_str = start_url\n        if session_name is not None:\n            input_str = session_name\n        return hashlib.sha1(input_str.encode('utf-8')).hexdigest()\n\n    def save_token(self, start_url, token, session_name=None):\n        cache_key = self._generate_cache_key(start_url, session_name)\n        self._cache[cache_key] = token\n\n    def __call__(self, start_url, session_name=None):\n        cache_key = self._generate_cache_key(start_url, session_name)\n        logger.debug(f'Checking for cached token at: {cache_key}')\n        if cache_key not in self._cache:\n            name = start_url\n            if session_name is not None:\n                name = session_name\n            error_msg = f'Token for {name} does not exist'\n            raise SSOTokenLoadError(error_msg=error_msg)\n\n        token = self._cache[cache_key]\n        if 'accessToken' not in token or 'expiresAt' not in token:\n            error_msg = f'Token for {start_url} is invalid'\n            raise SSOTokenLoadError(error_msg=error_msg)\n        return token\n\n\nclass EventbridgeSignerSetter:\n    _DEFAULT_PARTITION = 'aws'\n    _DEFAULT_DNS_SUFFIX = 'amazonaws.com'\n\n    def __init__(self, endpoint_resolver, region=None, endpoint_url=None):\n        self._endpoint_resolver = endpoint_resolver\n        self._region = region\n        self._endpoint_url = endpoint_url\n\n    def register(self, event_emitter):\n        event_emitter.register(\n            'before-parameter-build.events.PutEvents',\n            self.check_for_global_endpoint,\n        )\n        event_emitter.register(\n            'before-call.events.PutEvents', self.set_endpoint_url\n        )\n\n    def set_endpoint_url(self, params, context, **kwargs):\n        if 'eventbridge_endpoint' in context:\n            endpoint = context['eventbridge_endpoint']\n            logger.debug(f\"Rewriting URL from {params['url']} to {endpoint}\")\n            params['url'] = endpoint\n\n    def check_for_global_endpoint(self, params, context, **kwargs):\n        endpoint = params.get('EndpointId')\n        if endpoint is None:\n            return\n\n        if len(endpoint) == 0:\n            raise InvalidEndpointConfigurationError(\n                msg='EndpointId must not be a zero length string'\n            )\n\n        if not HAS_CRT:\n            raise MissingDependencyException(\n                msg=\"Using EndpointId requires an additional \"\n                \"dependency. You will need to pip install \"\n                \"botocore[crt] before proceeding.\"\n            )\n\n        config = context.get('client_config')\n        endpoint_variant_tags = None\n        if config is not None:\n            if config.use_fips_endpoint:\n                raise InvalidEndpointConfigurationError(\n                    msg=\"FIPS is not supported with EventBridge \"\n                    \"multi-region endpoints.\"\n                )\n            if config.use_dualstack_endpoint:\n                endpoint_variant_tags = ['dualstack']\n\n        if self._endpoint_url is None:\n            # Validate endpoint is a valid hostname component\n            parts = urlparse(f'https://{endpoint}')\n            if parts.hostname != endpoint:\n                raise InvalidEndpointConfigurationError(\n                    msg='EndpointId is not a valid hostname component.'\n                )\n            resolved_endpoint = self._get_global_endpoint(\n                endpoint, endpoint_variant_tags=endpoint_variant_tags\n            )\n        else:\n            resolved_endpoint = self._endpoint_url\n\n        context['eventbridge_endpoint'] = resolved_endpoint\n        context['auth_type'] = 'v4a'\n\n    def _get_global_endpoint(self, endpoint, endpoint_variant_tags=None):\n        resolver = self._endpoint_resolver\n\n        partition = resolver.get_partition_for_region(self._region)\n        if partition is None:\n            partition = self._DEFAULT_PARTITION\n        dns_suffix = resolver.get_partition_dns_suffix(\n            partition, endpoint_variant_tags=endpoint_variant_tags\n        )\n        if dns_suffix is None:\n            dns_suffix = self._DEFAULT_DNS_SUFFIX\n\n        return f\"https://{endpoint}.endpoint.events.{dns_suffix}/\"\n\n\ndef is_s3_accelerate_url(url):\n    \"\"\"Does the URL match the S3 Accelerate endpoint scheme?\n\n    Virtual host naming style with bucket names in the netloc part of the URL\n    are not allowed by this function.\n    \"\"\"\n    if url is None:\n        return False\n\n    # Accelerate is only valid for Amazon endpoints.\n    url_parts = urlsplit(url)\n    if not url_parts.netloc.endswith(\n        'amazonaws.com'\n    ) or url_parts.scheme not in ['https', 'http']:\n        return False\n\n    # The first part of the URL must be s3-accelerate.\n    parts = url_parts.netloc.split('.')\n    if parts[0] != 's3-accelerate':\n        return False\n\n    # Url parts between 's3-accelerate' and 'amazonaws.com' which\n    # represent different url features.\n    feature_parts = parts[1:-2]\n\n    # There should be no duplicate URL parts.\n    if len(feature_parts) != len(set(feature_parts)):\n        return False\n\n    # Remaining parts must all be in the whitelist.\n    return all(p in S3_ACCELERATE_WHITELIST for p in feature_parts)\n\n\nclass JSONFileCache:\n    \"\"\"JSON file cache.\n    This provides a dict like interface that stores JSON serializable\n    objects.\n    The objects are serialized to JSON and stored in a file.  These\n    values can be retrieved at a later time.\n    \"\"\"\n\n    CACHE_DIR = os.path.expanduser(os.path.join('~', '.aws', 'boto', 'cache'))\n\n    def __init__(self, working_dir=CACHE_DIR, dumps_func=None):\n        self._working_dir = working_dir\n        if dumps_func is None:\n            dumps_func = self._default_dumps\n        self._dumps = dumps_func\n\n    def _default_dumps(self, obj):\n        return json.dumps(obj, default=self._serialize_if_needed)\n\n    def __contains__(self, cache_key):\n        actual_key = self._convert_cache_key(cache_key)\n        return os.path.isfile(actual_key)\n\n    def __getitem__(self, cache_key):\n        \"\"\"Retrieve value from a cache key.\"\"\"\n        actual_key = self._convert_cache_key(cache_key)\n        try:\n            with open(actual_key) as f:\n                return json.load(f)\n        except (OSError, ValueError):\n            raise KeyError(cache_key)\n\n    def __delitem__(self, cache_key):\n        actual_key = self._convert_cache_key(cache_key)\n        try:\n            key_path = Path(actual_key)\n            key_path.unlink()\n        except FileNotFoundError:\n            raise KeyError(cache_key)\n\n    def __setitem__(self, cache_key, value):\n        full_key = self._convert_cache_key(cache_key)\n        try:\n            file_content = self._dumps(value)\n        except (TypeError, ValueError):\n            raise ValueError(\n                f\"Value cannot be cached, must be \"\n                f\"JSON serializable: {value}\"\n            )\n        if not os.path.isdir(self._working_dir):\n            os.makedirs(self._working_dir)\n        with os.fdopen(\n            os.open(full_key, os.O_WRONLY | os.O_CREAT, 0o600), 'w'\n        ) as f:\n            f.truncate()\n            f.write(file_content)\n\n    def _convert_cache_key(self, cache_key):\n        full_path = os.path.join(self._working_dir, cache_key + '.json')\n        return full_path\n\n    def _serialize_if_needed(self, value, iso=False):\n        if isinstance(value, _DatetimeClass):\n            if iso:\n                return value.isoformat()\n            return value.strftime('%Y-%m-%dT%H:%M:%S%Z')\n        return value\n\n\ndef is_s3express_bucket(bucket):\n    if bucket is None:\n        return False\n    return bucket.endswith('--x-s3')\n\n\n# This parameter is not part of the public interface and is subject to abrupt\n# breaking changes or removal without prior announcement.\n# Mapping of services that have been renamed for backwards compatibility reasons.\n# Keys are the previous name that should be allowed, values are the documented\n# and preferred client name.\nSERVICE_NAME_ALIASES = {'runtime.sagemaker': 'sagemaker-runtime'}\n\n\n# This parameter is not part of the public interface and is subject to abrupt\n# breaking changes or removal without prior announcement.\n# Mapping to determine the service ID for services that do not use it as the\n# model data directory name. The keys are the data directory name and the\n# values are the transformed service IDs (lower case and hyphenated).\nCLIENT_NAME_TO_HYPHENIZED_SERVICE_ID_OVERRIDES = {\n    # Actual service name we use -> Allowed computed service name.\n    'apigateway': 'api-gateway',\n    'application-autoscaling': 'application-auto-scaling',\n    'appmesh': 'app-mesh',\n    'autoscaling': 'auto-scaling',\n    'autoscaling-plans': 'auto-scaling-plans',\n    'ce': 'cost-explorer',\n    'cloudhsmv2': 'cloudhsm-v2',\n    'cloudsearchdomain': 'cloudsearch-domain',\n    'cognito-idp': 'cognito-identity-provider',\n    'config': 'config-service',\n    'cur': 'cost-and-usage-report-service',\n    'datapipeline': 'data-pipeline',\n    'directconnect': 'direct-connect',\n    'devicefarm': 'device-farm',\n    'discovery': 'application-discovery-service',\n    'dms': 'database-migration-service',\n    'ds': 'directory-service',\n    'dynamodbstreams': 'dynamodb-streams',\n    'elasticbeanstalk': 'elastic-beanstalk',\n    'elastictranscoder': 'elastic-transcoder',\n    'elb': 'elastic-load-balancing',\n    'elbv2': 'elastic-load-balancing-v2',\n    'es': 'elasticsearch-service',\n    'events': 'eventbridge',\n    'globalaccelerator': 'global-accelerator',\n    'iot-data': 'iot-data-plane',\n    'iot-jobs-data': 'iot-jobs-data-plane',\n    'iot1click-devices': 'iot-1click-devices-service',\n    'iot1click-projects': 'iot-1click-projects',\n    'iotevents-data': 'iot-events-data',\n    'iotevents': 'iot-events',\n    'iotwireless': 'iot-wireless',\n    'kinesisanalytics': 'kinesis-analytics',\n    'kinesisanalyticsv2': 'kinesis-analytics-v2',\n    'kinesisvideo': 'kinesis-video',\n    'lex-models': 'lex-model-building-service',\n    'lexv2-models': 'lex-models-v2',\n    'lex-runtime': 'lex-runtime-service',\n    'lexv2-runtime': 'lex-runtime-v2',\n    'logs': 'cloudwatch-logs',\n    'machinelearning': 'machine-learning',\n    'marketplacecommerceanalytics': 'marketplace-commerce-analytics',\n    'marketplace-entitlement': 'marketplace-entitlement-service',\n    'meteringmarketplace': 'marketplace-metering',\n    'mgh': 'migration-hub',\n    'sms-voice': 'pinpoint-sms-voice',\n    'resourcegroupstaggingapi': 'resource-groups-tagging-api',\n    'route53': 'route-53',\n    'route53domains': 'route-53-domains',\n    's3control': 's3-control',\n    'sdb': 'simpledb',\n    'secretsmanager': 'secrets-manager',\n    'serverlessrepo': 'serverlessapplicationrepository',\n    'servicecatalog': 'service-catalog',\n    'servicecatalog-appregistry': 'service-catalog-appregistry',\n    'stepfunctions': 'sfn',\n    'storagegateway': 'storage-gateway',\n}\n", "botocore/signers.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport base64\nimport datetime\nimport json\nimport weakref\n\nimport botocore\nimport botocore.auth\nfrom botocore.awsrequest import create_request_object, prepare_request_dict\nfrom botocore.compat import OrderedDict\nfrom botocore.exceptions import (\n    UnknownClientMethodError,\n    UnknownSignatureVersionError,\n    UnsupportedSignatureVersionError,\n)\nfrom botocore.utils import ArnParser, datetime2timestamp\n\n# Keep these imported.  There's pre-existing code that uses them.\nfrom botocore.utils import fix_s3_host  # noqa\n\n\nclass RequestSigner:\n    \"\"\"\n    An object to sign requests before they go out over the wire using\n    one of the authentication mechanisms defined in ``auth.py``. This\n    class fires two events scoped to a service and operation name:\n\n    * choose-signer: Allows overriding the auth signer name.\n    * before-sign: Allows mutating the request before signing.\n\n    Together these events allow for customization of the request\n    signing pipeline, including overrides, request path manipulation,\n    and disabling signing per operation.\n\n\n    :type service_id: botocore.model.ServiceId\n    :param service_id: The service id for the service, e.g. ``S3``\n\n    :type region_name: string\n    :param region_name: Name of the service region, e.g. ``us-east-1``\n\n    :type signing_name: string\n    :param signing_name: Service signing name. This is usually the\n                         same as the service name, but can differ. E.g.\n                         ``emr`` vs. ``elasticmapreduce``.\n\n    :type signature_version: string\n    :param signature_version: Signature name like ``v4``.\n\n    :type credentials: :py:class:`~botocore.credentials.Credentials`\n    :param credentials: User credentials with which to sign requests.\n\n    :type event_emitter: :py:class:`~botocore.hooks.BaseEventHooks`\n    :param event_emitter: Extension mechanism to fire events.\n    \"\"\"\n\n    def __init__(\n        self,\n        service_id,\n        region_name,\n        signing_name,\n        signature_version,\n        credentials,\n        event_emitter,\n        auth_token=None,\n    ):\n        self._region_name = region_name\n        self._signing_name = signing_name\n        self._signature_version = signature_version\n        self._credentials = credentials\n        self._auth_token = auth_token\n        self._service_id = service_id\n\n        # We need weakref to prevent leaking memory in Python 2.6 on Linux 2.6\n        self._event_emitter = weakref.proxy(event_emitter)\n\n    @property\n    def region_name(self):\n        return self._region_name\n\n    @property\n    def signature_version(self):\n        return self._signature_version\n\n    @property\n    def signing_name(self):\n        return self._signing_name\n\n    def handler(self, operation_name=None, request=None, **kwargs):\n        # This is typically hooked up to the \"request-created\" event\n        # from a client's event emitter.  When a new request is created\n        # this method is invoked to sign the request.\n        # Don't call this method directly.\n        return self.sign(operation_name, request)\n\n    def sign(\n        self,\n        operation_name,\n        request,\n        region_name=None,\n        signing_type='standard',\n        expires_in=None,\n        signing_name=None,\n    ):\n        \"\"\"Sign a request before it goes out over the wire.\n\n        :type operation_name: string\n        :param operation_name: The name of the current operation, e.g.\n                               ``ListBuckets``.\n        :type request: AWSRequest\n        :param request: The request object to be sent over the wire.\n\n        :type region_name: str\n        :param region_name: The region to sign the request for.\n\n        :type signing_type: str\n        :param signing_type: The type of signing to perform. This can be one of\n            three possible values:\n\n            * 'standard'     - This should be used for most requests.\n            * 'presign-url'  - This should be used when pre-signing a request.\n            * 'presign-post' - This should be used when pre-signing an S3 post.\n\n        :type expires_in: int\n        :param expires_in: The number of seconds the presigned url is valid\n            for. This parameter is only valid for signing type 'presign-url'.\n\n        :type signing_name: str\n        :param signing_name: The name to use for the service when signing.\n        \"\"\"\n        explicit_region_name = region_name\n        if region_name is None:\n            region_name = self._region_name\n\n        if signing_name is None:\n            signing_name = self._signing_name\n\n        signature_version = self._choose_signer(\n            operation_name, signing_type, request.context\n        )\n\n        # Allow mutating request before signing\n        self._event_emitter.emit(\n            'before-sign.{}.{}'.format(\n                self._service_id.hyphenize(), operation_name\n            ),\n            request=request,\n            signing_name=signing_name,\n            region_name=self._region_name,\n            signature_version=signature_version,\n            request_signer=self,\n            operation_name=operation_name,\n        )\n\n        if signature_version != botocore.UNSIGNED:\n            kwargs = {\n                'signing_name': signing_name,\n                'region_name': region_name,\n                'signature_version': signature_version,\n            }\n            if expires_in is not None:\n                kwargs['expires'] = expires_in\n            signing_context = request.context.get('signing', {})\n            if not explicit_region_name and signing_context.get('region'):\n                kwargs['region_name'] = signing_context['region']\n            if signing_context.get('signing_name'):\n                kwargs['signing_name'] = signing_context['signing_name']\n            if signing_context.get('request_credentials'):\n                kwargs['request_credentials'] = signing_context[\n                    'request_credentials'\n                ]\n            if signing_context.get('identity_cache') is not None:\n                self._resolve_identity_cache(\n                    kwargs,\n                    signing_context['identity_cache'],\n                    signing_context['cache_key'],\n                )\n            try:\n                auth = self.get_auth_instance(**kwargs)\n            except UnknownSignatureVersionError as e:\n                if signing_type != 'standard':\n                    raise UnsupportedSignatureVersionError(\n                        signature_version=signature_version\n                    )\n                else:\n                    raise e\n\n            auth.add_auth(request)\n\n    def _resolve_identity_cache(self, kwargs, cache, cache_key):\n        kwargs['identity_cache'] = cache\n        kwargs['cache_key'] = cache_key\n\n    def _choose_signer(self, operation_name, signing_type, context):\n        \"\"\"\n        Allow setting the signature version via the choose-signer event.\n        A value of `botocore.UNSIGNED` means no signing will be performed.\n\n        :param operation_name: The operation to sign.\n        :param signing_type: The type of signing that the signer is to be used\n            for.\n        :return: The signature version to sign with.\n        \"\"\"\n        signing_type_suffix_map = {\n            'presign-post': '-presign-post',\n            'presign-url': '-query',\n        }\n        suffix = signing_type_suffix_map.get(signing_type, '')\n\n        # operation specific signing context takes precedent over client-level\n        # defaults\n        signature_version = context.get('auth_type') or self._signature_version\n        signing = context.get('signing', {})\n        signing_name = signing.get('signing_name', self._signing_name)\n        region_name = signing.get('region', self._region_name)\n        if (\n            signature_version is not botocore.UNSIGNED\n            and not signature_version.endswith(suffix)\n        ):\n            signature_version += suffix\n\n        handler, response = self._event_emitter.emit_until_response(\n            'choose-signer.{}.{}'.format(\n                self._service_id.hyphenize(), operation_name\n            ),\n            signing_name=signing_name,\n            region_name=region_name,\n            signature_version=signature_version,\n            context=context,\n        )\n\n        if response is not None:\n            signature_version = response\n            # The suffix needs to be checked again in case we get an improper\n            # signature version from choose-signer.\n            if (\n                signature_version is not botocore.UNSIGNED\n                and not signature_version.endswith(suffix)\n            ):\n                signature_version += suffix\n\n        return signature_version\n\n    def get_auth_instance(\n        self,\n        signing_name,\n        region_name,\n        signature_version=None,\n        request_credentials=None,\n        **kwargs,\n    ):\n        \"\"\"\n        Get an auth instance which can be used to sign a request\n        using the given signature version.\n\n        :type signing_name: string\n        :param signing_name: Service signing name. This is usually the\n                             same as the service name, but can differ. E.g.\n                             ``emr`` vs. ``elasticmapreduce``.\n\n        :type region_name: string\n        :param region_name: Name of the service region, e.g. ``us-east-1``\n\n        :type signature_version: string\n        :param signature_version: Signature name like ``v4``.\n\n        :rtype: :py:class:`~botocore.auth.BaseSigner`\n        :return: Auth instance to sign a request.\n        \"\"\"\n        if signature_version is None:\n            signature_version = self._signature_version\n\n        cls = botocore.auth.AUTH_TYPE_MAPS.get(signature_version)\n        if cls is None:\n            raise UnknownSignatureVersionError(\n                signature_version=signature_version\n            )\n\n        if cls.REQUIRES_TOKEN is True:\n            frozen_token = None\n            if self._auth_token is not None:\n                frozen_token = self._auth_token.get_frozen_token()\n            auth = cls(frozen_token)\n            return auth\n\n        credentials = request_credentials or self._credentials\n        if getattr(cls, \"REQUIRES_IDENTITY_CACHE\", None) is True:\n            cache = kwargs[\"identity_cache\"]\n            key = kwargs[\"cache_key\"]\n            credentials = cache.get_credentials(key)\n            del kwargs[\"cache_key\"]\n\n        # If there's no credentials provided (i.e credentials is None),\n        # then we'll pass a value of \"None\" over to the auth classes,\n        # which already handle the cases where no credentials have\n        # been provided.\n        frozen_credentials = None\n        if credentials is not None:\n            frozen_credentials = credentials.get_frozen_credentials()\n        kwargs['credentials'] = frozen_credentials\n        if cls.REQUIRES_REGION:\n            if self._region_name is None:\n                raise botocore.exceptions.NoRegionError()\n            kwargs['region_name'] = region_name\n            kwargs['service_name'] = signing_name\n        auth = cls(**kwargs)\n        return auth\n\n    # Alias get_auth for backwards compatibility.\n    get_auth = get_auth_instance\n\n    def generate_presigned_url(\n        self,\n        request_dict,\n        operation_name,\n        expires_in=3600,\n        region_name=None,\n        signing_name=None,\n    ):\n        \"\"\"Generates a presigned url\n\n        :type request_dict: dict\n        :param request_dict: The prepared request dictionary returned by\n            ``botocore.awsrequest.prepare_request_dict()``\n\n        :type operation_name: str\n        :param operation_name: The operation being signed.\n\n        :type expires_in: int\n        :param expires_in: The number of seconds the presigned url is valid\n            for. By default it expires in an hour (3600 seconds)\n\n        :type region_name: string\n        :param region_name: The region name to sign the presigned url.\n\n        :type signing_name: str\n        :param signing_name: The name to use for the service when signing.\n\n        :returns: The presigned url\n        \"\"\"\n        request = create_request_object(request_dict)\n        self.sign(\n            operation_name,\n            request,\n            region_name,\n            'presign-url',\n            expires_in,\n            signing_name,\n        )\n\n        request.prepare()\n        return request.url\n\n\nclass CloudFrontSigner:\n    '''A signer to create a signed CloudFront URL.\n\n    First you create a cloudfront signer based on a normalized RSA signer::\n\n        import rsa\n        def rsa_signer(message):\n            private_key = open('private_key.pem', 'r').read()\n            return rsa.sign(\n                message,\n                rsa.PrivateKey.load_pkcs1(private_key.encode('utf8')),\n                'SHA-1')  # CloudFront requires SHA-1 hash\n        cf_signer = CloudFrontSigner(key_id, rsa_signer)\n\n    To sign with a canned policy::\n\n        signed_url = cf_signer.generate_signed_url(\n            url, date_less_than=datetime(2015, 12, 1))\n\n    To sign with a custom policy::\n\n        signed_url = cf_signer.generate_signed_url(url, policy=my_policy)\n    '''\n\n    def __init__(self, key_id, rsa_signer):\n        \"\"\"Create a CloudFrontSigner.\n\n        :type key_id: str\n        :param key_id: The CloudFront Key Pair ID\n\n        :type rsa_signer: callable\n        :param rsa_signer: An RSA signer.\n               Its only input parameter will be the message to be signed,\n               and its output will be the signed content as a binary string.\n               The hash algorithm needed by CloudFront is SHA-1.\n        \"\"\"\n        self.key_id = key_id\n        self.rsa_signer = rsa_signer\n\n    def generate_presigned_url(self, url, date_less_than=None, policy=None):\n        \"\"\"Creates a signed CloudFront URL based on given parameters.\n\n        :type url: str\n        :param url: The URL of the protected object\n\n        :type date_less_than: datetime\n        :param date_less_than: The URL will expire after that date and time\n\n        :type policy: str\n        :param policy: The custom policy, possibly built by self.build_policy()\n\n        :rtype: str\n        :return: The signed URL.\n        \"\"\"\n        both_args_supplied = date_less_than is not None and policy is not None\n        neither_arg_supplied = date_less_than is None and policy is None\n        if both_args_supplied or neither_arg_supplied:\n            e = 'Need to provide either date_less_than or policy, but not both'\n            raise ValueError(e)\n        if date_less_than is not None:\n            # We still need to build a canned policy for signing purpose\n            policy = self.build_policy(url, date_less_than)\n        if isinstance(policy, str):\n            policy = policy.encode('utf8')\n        if date_less_than is not None:\n            params = ['Expires=%s' % int(datetime2timestamp(date_less_than))]\n        else:\n            params = ['Policy=%s' % self._url_b64encode(policy).decode('utf8')]\n        signature = self.rsa_signer(policy)\n        params.extend(\n            [\n                f\"Signature={self._url_b64encode(signature).decode('utf8')}\",\n                f\"Key-Pair-Id={self.key_id}\",\n            ]\n        )\n        return self._build_url(url, params)\n\n    def _build_url(self, base_url, extra_params):\n        separator = '&' if '?' in base_url else '?'\n        return base_url + separator + '&'.join(extra_params)\n\n    def build_policy(\n        self, resource, date_less_than, date_greater_than=None, ip_address=None\n    ):\n        \"\"\"A helper to build policy.\n\n        :type resource: str\n        :param resource: The URL or the stream filename of the protected object\n\n        :type date_less_than: datetime\n        :param date_less_than: The URL will expire after the time has passed\n\n        :type date_greater_than: datetime\n        :param date_greater_than: The URL will not be valid until this time\n\n        :type ip_address: str\n        :param ip_address: Use 'x.x.x.x' for an IP, or 'x.x.x.x/x' for a subnet\n\n        :rtype: str\n        :return: The policy in a compact string.\n        \"\"\"\n        # Note:\n        # 1. Order in canned policy is significant. Special care has been taken\n        #    to ensure the output will match the order defined by the document.\n        #    There is also a test case to ensure that order.\n        #    SEE: http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-creating-signed-url-canned-policy.html#private-content-canned-policy-creating-policy-statement\n        # 2. Albeit the order in custom policy is not required by CloudFront,\n        #    we still use OrderedDict internally to ensure the result is stable\n        #    and also matches canned policy requirement.\n        #    SEE: http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-creating-signed-url-custom-policy.html\n        moment = int(datetime2timestamp(date_less_than))\n        condition = OrderedDict({\"DateLessThan\": {\"AWS:EpochTime\": moment}})\n        if ip_address:\n            if '/' not in ip_address:\n                ip_address += '/32'\n            condition[\"IpAddress\"] = {\"AWS:SourceIp\": ip_address}\n        if date_greater_than:\n            moment = int(datetime2timestamp(date_greater_than))\n            condition[\"DateGreaterThan\"] = {\"AWS:EpochTime\": moment}\n        ordered_payload = [('Resource', resource), ('Condition', condition)]\n        custom_policy = {\"Statement\": [OrderedDict(ordered_payload)]}\n        return json.dumps(custom_policy, separators=(',', ':'))\n\n    def _url_b64encode(self, data):\n        # Required by CloudFront. See also:\n        # http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-linux-openssl.html\n        return (\n            base64.b64encode(data)\n            .replace(b'+', b'-')\n            .replace(b'=', b'_')\n            .replace(b'/', b'~')\n        )\n\n\ndef add_generate_db_auth_token(class_attributes, **kwargs):\n    class_attributes['generate_db_auth_token'] = generate_db_auth_token\n\n\ndef generate_db_auth_token(self, DBHostname, Port, DBUsername, Region=None):\n    \"\"\"Generates an auth token used to connect to a db with IAM credentials.\n\n    :type DBHostname: str\n    :param DBHostname: The hostname of the database to connect to.\n\n    :type Port: int\n    :param Port: The port number the database is listening on.\n\n    :type DBUsername: str\n    :param DBUsername: The username to log in as.\n\n    :type Region: str\n    :param Region: The region the database is in. If None, the client\n        region will be used.\n\n    :return: A presigned url which can be used as an auth token.\n    \"\"\"\n    region = Region\n    if region is None:\n        region = self.meta.region_name\n\n    params = {\n        'Action': 'connect',\n        'DBUser': DBUsername,\n    }\n\n    request_dict = {\n        'url_path': '/',\n        'query_string': '',\n        'headers': {},\n        'body': params,\n        'method': 'GET',\n    }\n\n    # RDS requires that the scheme not be set when sent over. This can cause\n    # issues when signing because the Python url parsing libraries follow\n    # RFC 1808 closely, which states that a netloc must be introduced by `//`.\n    # Otherwise the url is presumed to be relative, and thus the whole\n    # netloc would be treated as a path component. To work around this we\n    # introduce https here and remove it once we're done processing it.\n    scheme = 'https://'\n    endpoint_url = f'{scheme}{DBHostname}:{Port}'\n    prepare_request_dict(request_dict, endpoint_url)\n    presigned_url = self._request_signer.generate_presigned_url(\n        operation_name='connect',\n        request_dict=request_dict,\n        region_name=region,\n        expires_in=900,\n        signing_name='rds-db',\n    )\n    return presigned_url[len(scheme) :]\n\n\nclass S3PostPresigner:\n    def __init__(self, request_signer):\n        self._request_signer = request_signer\n\n    def generate_presigned_post(\n        self,\n        request_dict,\n        fields=None,\n        conditions=None,\n        expires_in=3600,\n        region_name=None,\n    ):\n        \"\"\"Generates the url and the form fields used for a presigned s3 post\n\n        :type request_dict: dict\n        :param request_dict: The prepared request dictionary returned by\n            ``botocore.awsrequest.prepare_request_dict()``\n\n        :type fields: dict\n        :param fields: A dictionary of prefilled form fields to build on top\n            of.\n\n        :type conditions: list\n        :param conditions: A list of conditions to include in the policy. Each\n            element can be either a list or a structure. For example:\n            [\n             {\"acl\": \"public-read\"},\n             {\"bucket\": \"mybucket\"},\n             [\"starts-with\", \"$key\", \"mykey\"]\n            ]\n\n        :type expires_in: int\n        :param expires_in: The number of seconds the presigned post is valid\n            for.\n\n        :type region_name: string\n        :param region_name: The region name to sign the presigned post to.\n\n        :rtype: dict\n        :returns: A dictionary with two elements: ``url`` and ``fields``.\n            Url is the url to post to. Fields is a dictionary filled with\n            the form fields and respective values to use when submitting the\n            post. For example:\n\n            {'url': 'https://mybucket.s3.amazonaws.com\n             'fields': {'acl': 'public-read',\n                        'key': 'mykey',\n                        'signature': 'mysignature',\n                        'policy': 'mybase64 encoded policy'}\n            }\n        \"\"\"\n        if fields is None:\n            fields = {}\n\n        if conditions is None:\n            conditions = []\n\n        # Create the policy for the post.\n        policy = {}\n\n        # Create an expiration date for the policy\n        datetime_now = datetime.datetime.utcnow()\n        expire_date = datetime_now + datetime.timedelta(seconds=expires_in)\n        policy['expiration'] = expire_date.strftime(botocore.auth.ISO8601)\n\n        # Append all of the conditions that the user supplied.\n        policy['conditions'] = []\n        for condition in conditions:\n            policy['conditions'].append(condition)\n\n        # Store the policy and the fields in the request for signing\n        request = create_request_object(request_dict)\n        request.context['s3-presign-post-fields'] = fields\n        request.context['s3-presign-post-policy'] = policy\n\n        self._request_signer.sign(\n            'PutObject', request, region_name, 'presign-post'\n        )\n        # Return the url and the fields for th form to post.\n        return {'url': request.url, 'fields': fields}\n\n\ndef add_generate_presigned_url(class_attributes, **kwargs):\n    class_attributes['generate_presigned_url'] = generate_presigned_url\n\n\ndef generate_presigned_url(\n    self, ClientMethod, Params=None, ExpiresIn=3600, HttpMethod=None\n):\n    \"\"\"Generate a presigned url given a client, its method, and arguments\n\n    :type ClientMethod: string\n    :param ClientMethod: The client method to presign for\n\n    :type Params: dict\n    :param Params: The parameters normally passed to\n        ``ClientMethod``.\n\n    :type ExpiresIn: int\n    :param ExpiresIn: The number of seconds the presigned url is valid\n        for. By default it expires in an hour (3600 seconds)\n\n    :type HttpMethod: string\n    :param HttpMethod: The http method to use on the generated url. By\n        default, the http method is whatever is used in the method's model.\n\n    :returns: The presigned url\n    \"\"\"\n    client_method = ClientMethod\n    params = Params\n    if params is None:\n        params = {}\n    expires_in = ExpiresIn\n    http_method = HttpMethod\n    context = {\n        'is_presign_request': True,\n        'use_global_endpoint': _should_use_global_endpoint(self),\n    }\n\n    request_signer = self._request_signer\n\n    try:\n        operation_name = self._PY_TO_OP_NAME[client_method]\n    except KeyError:\n        raise UnknownClientMethodError(method_name=client_method)\n\n    operation_model = self.meta.service_model.operation_model(operation_name)\n    params = self._emit_api_params(\n        api_params=params,\n        operation_model=operation_model,\n        context=context,\n    )\n    bucket_is_arn = ArnParser.is_arn(params.get('Bucket', ''))\n    (\n        endpoint_url,\n        additional_headers,\n        properties,\n    ) = self._resolve_endpoint_ruleset(\n        operation_model,\n        params,\n        context,\n        ignore_signing_region=(not bucket_is_arn),\n    )\n\n    request_dict = self._convert_to_request_dict(\n        api_params=params,\n        operation_model=operation_model,\n        endpoint_url=endpoint_url,\n        context=context,\n        headers=additional_headers,\n        set_user_agent_header=False,\n    )\n\n    # Switch out the http method if user specified it.\n    if http_method is not None:\n        request_dict['method'] = http_method\n\n    # Generate the presigned url.\n    return request_signer.generate_presigned_url(\n        request_dict=request_dict,\n        expires_in=expires_in,\n        operation_name=operation_name,\n    )\n\n\ndef add_generate_presigned_post(class_attributes, **kwargs):\n    class_attributes['generate_presigned_post'] = generate_presigned_post\n\n\ndef generate_presigned_post(\n    self, Bucket, Key, Fields=None, Conditions=None, ExpiresIn=3600\n):\n    \"\"\"Builds the url and the form fields used for a presigned s3 post\n\n    :type Bucket: string\n    :param Bucket: The name of the bucket to presign the post to. Note that\n        bucket related conditions should not be included in the\n        ``conditions`` parameter.\n\n    :type Key: string\n    :param Key: Key name, optionally add ${filename} to the end to\n        attach the submitted filename. Note that key related conditions and\n        fields are filled out for you and should not be included in the\n        ``Fields`` or ``Conditions`` parameter.\n\n    :type Fields: dict\n    :param Fields: A dictionary of prefilled form fields to build on top\n        of. Elements that may be included are acl, Cache-Control,\n        Content-Type, Content-Disposition, Content-Encoding, Expires,\n        success_action_redirect, redirect, success_action_status,\n        and x-amz-meta-.\n\n        Note that if a particular element is included in the fields\n        dictionary it will not be automatically added to the conditions\n        list. You must specify a condition for the element as well.\n\n    :type Conditions: list\n    :param Conditions: A list of conditions to include in the policy. Each\n        element can be either a list or a structure. For example:\n\n        [\n         {\"acl\": \"public-read\"},\n         [\"content-length-range\", 2, 5],\n         [\"starts-with\", \"$success_action_redirect\", \"\"]\n        ]\n\n        Conditions that are included may pertain to acl,\n        content-length-range, Cache-Control, Content-Type,\n        Content-Disposition, Content-Encoding, Expires,\n        success_action_redirect, redirect, success_action_status,\n        and/or x-amz-meta-.\n\n        Note that if you include a condition, you must specify\n        the a valid value in the fields dictionary as well. A value will\n        not be added automatically to the fields dictionary based on the\n        conditions.\n\n    :type ExpiresIn: int\n    :param ExpiresIn: The number of seconds the presigned post\n        is valid for.\n\n    :rtype: dict\n    :returns: A dictionary with two elements: ``url`` and ``fields``.\n        Url is the url to post to. Fields is a dictionary filled with\n        the form fields and respective values to use when submitting the\n        post. For example:\n\n        {'url': 'https://mybucket.s3.amazonaws.com\n         'fields': {'acl': 'public-read',\n                    'key': 'mykey',\n                    'signature': 'mysignature',\n                    'policy': 'mybase64 encoded policy'}\n        }\n    \"\"\"\n    bucket = Bucket\n    key = Key\n    fields = Fields\n    conditions = Conditions\n    expires_in = ExpiresIn\n\n    if fields is None:\n        fields = {}\n    else:\n        fields = fields.copy()\n\n    if conditions is None:\n        conditions = []\n\n    context = {\n        'is_presign_request': True,\n        'use_global_endpoint': _should_use_global_endpoint(self),\n    }\n\n    post_presigner = S3PostPresigner(self._request_signer)\n\n    # We choose the CreateBucket operation model because its url gets\n    # serialized to what a presign post requires.\n    operation_model = self.meta.service_model.operation_model('CreateBucket')\n    params = self._emit_api_params(\n        api_params={'Bucket': bucket},\n        operation_model=operation_model,\n        context=context,\n    )\n    bucket_is_arn = ArnParser.is_arn(params.get('Bucket', ''))\n    (\n        endpoint_url,\n        additional_headers,\n        properties,\n    ) = self._resolve_endpoint_ruleset(\n        operation_model,\n        params,\n        context,\n        ignore_signing_region=(not bucket_is_arn),\n    )\n\n    request_dict = self._convert_to_request_dict(\n        api_params=params,\n        operation_model=operation_model,\n        endpoint_url=endpoint_url,\n        context=context,\n        headers=additional_headers,\n        set_user_agent_header=False,\n    )\n\n    # Append that the bucket name to the list of conditions.\n    conditions.append({'bucket': bucket})\n\n    # If the key ends with filename, the only constraint that can be\n    # imposed is if it starts with the specified prefix.\n    if key.endswith('${filename}'):\n        conditions.append([\"starts-with\", '$key', key[: -len('${filename}')]])\n    else:\n        conditions.append({'key': key})\n\n    # Add the key to the fields.\n    fields['key'] = key\n\n    return post_presigner.generate_presigned_post(\n        request_dict=request_dict,\n        fields=fields,\n        conditions=conditions,\n        expires_in=expires_in,\n    )\n\n\ndef _should_use_global_endpoint(client):\n    if client.meta.partition != 'aws':\n        return False\n    s3_config = client.meta.config.s3\n    if s3_config:\n        if s3_config.get('use_dualstack_endpoint', False):\n            return False\n        if (\n            s3_config.get('us_east_1_regional_endpoint') == 'regional'\n            and client.meta.config.region_name == 'us-east-1'\n        ):\n            return False\n        if s3_config.get('addressing_style') == 'virtual':\n            return False\n    return True\n", "botocore/httpsession.py": "import logging\nimport os\nimport os.path\nimport socket\nimport sys\nimport warnings\nfrom base64 import b64encode\n\nfrom urllib3 import PoolManager, Timeout, proxy_from_url\nfrom urllib3.exceptions import (\n    ConnectTimeoutError as URLLib3ConnectTimeoutError,\n)\nfrom urllib3.exceptions import (\n    LocationParseError,\n    NewConnectionError,\n    ProtocolError,\n    ProxyError,\n)\nfrom urllib3.exceptions import ReadTimeoutError as URLLib3ReadTimeoutError\nfrom urllib3.exceptions import SSLError as URLLib3SSLError\nfrom urllib3.util.retry import Retry\nfrom urllib3.util.ssl_ import (\n    OP_NO_COMPRESSION,\n    PROTOCOL_TLS,\n    OP_NO_SSLv2,\n    OP_NO_SSLv3,\n    is_ipaddress,\n    ssl,\n)\nfrom urllib3.util.url import parse_url\n\ntry:\n    from urllib3.util.ssl_ import OP_NO_TICKET, PROTOCOL_TLS_CLIENT\nexcept ImportError:\n    # Fallback directly to ssl for version of urllib3 before 1.26.\n    # They are available in the standard library starting in Python 3.6.\n    from ssl import OP_NO_TICKET, PROTOCOL_TLS_CLIENT\n\ntry:\n    # pyopenssl will be removed in urllib3 2.0, we'll fall back to ssl_ at that point.\n    # This can be removed once our urllib3 floor is raised to >= 2.0.\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", category=DeprecationWarning)\n        # Always import the original SSLContext, even if it has been patched\n        from urllib3.contrib.pyopenssl import (\n            orig_util_SSLContext as SSLContext,\n        )\nexcept ImportError:\n    from urllib3.util.ssl_ import SSLContext\n\ntry:\n    from urllib3.util.ssl_ import DEFAULT_CIPHERS\nexcept ImportError:\n    # Defer to system configuration starting with\n    # urllib3 2.0. This will choose the ciphers provided by\n    # Openssl 1.1.1+ or secure system defaults.\n    DEFAULT_CIPHERS = None\n\nimport botocore.awsrequest\nfrom botocore.compat import (\n    IPV6_ADDRZ_RE,\n    ensure_bytes,\n    filter_ssl_warnings,\n    unquote,\n    urlparse,\n)\nfrom botocore.exceptions import (\n    ConnectionClosedError,\n    ConnectTimeoutError,\n    EndpointConnectionError,\n    HTTPClientError,\n    InvalidProxiesConfigError,\n    ProxyConnectionError,\n    ReadTimeoutError,\n    SSLError,\n)\n\nfilter_ssl_warnings()\nlogger = logging.getLogger(__name__)\nDEFAULT_TIMEOUT = 60\nMAX_POOL_CONNECTIONS = 10\nDEFAULT_CA_BUNDLE = os.path.join(os.path.dirname(__file__), 'cacert.pem')\n\ntry:\n    from certifi import where\nexcept ImportError:\n\n    def where():\n        return DEFAULT_CA_BUNDLE\n\n\ndef get_cert_path(verify):\n    if verify is not True:\n        return verify\n\n    cert_path = where()\n    logger.debug(f\"Certificate path: {cert_path}\")\n\n    return cert_path\n\n\ndef create_urllib3_context(\n    ssl_version=None, cert_reqs=None, options=None, ciphers=None\n):\n    \"\"\"This function is a vendored version of the same function in urllib3\n\n    We vendor this function to ensure that the SSL contexts we construct\n    always use the std lib SSLContext instead of pyopenssl.\n    \"\"\"\n    # PROTOCOL_TLS is deprecated in Python 3.10\n    if not ssl_version or ssl_version == PROTOCOL_TLS:\n        ssl_version = PROTOCOL_TLS_CLIENT\n\n    context = SSLContext(ssl_version)\n\n    if ciphers:\n        context.set_ciphers(ciphers)\n    elif DEFAULT_CIPHERS:\n        context.set_ciphers(DEFAULT_CIPHERS)\n\n    # Setting the default here, as we may have no ssl module on import\n    cert_reqs = ssl.CERT_REQUIRED if cert_reqs is None else cert_reqs\n\n    if options is None:\n        options = 0\n        # SSLv2 is easily broken and is considered harmful and dangerous\n        options |= OP_NO_SSLv2\n        # SSLv3 has several problems and is now dangerous\n        options |= OP_NO_SSLv3\n        # Disable compression to prevent CRIME attacks for OpenSSL 1.0+\n        # (issue urllib3#309)\n        options |= OP_NO_COMPRESSION\n        # TLSv1.2 only. Unless set explicitly, do not request tickets.\n        # This may save some bandwidth on wire, and although the ticket is encrypted,\n        # there is a risk associated with it being on wire,\n        # if the server is not rotating its ticketing keys properly.\n        options |= OP_NO_TICKET\n\n    context.options |= options\n\n    # Enable post-handshake authentication for TLS 1.3, see GH #1634. PHA is\n    # necessary for conditional client cert authentication with TLS 1.3.\n    # The attribute is None for OpenSSL <= 1.1.0 or does not exist in older\n    # versions of Python.  We only enable on Python 3.7.4+ or if certificate\n    # verification is enabled to work around Python issue #37428\n    # See: https://bugs.python.org/issue37428\n    if (\n        cert_reqs == ssl.CERT_REQUIRED or sys.version_info >= (3, 7, 4)\n    ) and getattr(context, \"post_handshake_auth\", None) is not None:\n        context.post_handshake_auth = True\n\n    def disable_check_hostname():\n        if (\n            getattr(context, \"check_hostname\", None) is not None\n        ):  # Platform-specific: Python 3.2\n            # We do our own verification, including fingerprints and alternative\n            # hostnames. So disable it here\n            context.check_hostname = False\n\n    # The order of the below lines setting verify_mode and check_hostname\n    # matter due to safe-guards SSLContext has to prevent an SSLContext with\n    # check_hostname=True, verify_mode=NONE/OPTIONAL. This is made even more\n    # complex because we don't know whether PROTOCOL_TLS_CLIENT will be used\n    # or not so we don't know the initial state of the freshly created SSLContext.\n    if cert_reqs == ssl.CERT_REQUIRED:\n        context.verify_mode = cert_reqs\n        disable_check_hostname()\n    else:\n        disable_check_hostname()\n        context.verify_mode = cert_reqs\n\n    # Enable logging of TLS session keys via defacto standard environment variable\n    # 'SSLKEYLOGFILE', if the feature is available (Python 3.8+). Skip empty values.\n    if hasattr(context, \"keylog_filename\"):\n        sslkeylogfile = os.environ.get(\"SSLKEYLOGFILE\")\n        if sslkeylogfile and not sys.flags.ignore_environment:\n            context.keylog_filename = sslkeylogfile\n\n    return context\n\n\ndef ensure_boolean(val):\n    \"\"\"Ensures a boolean value if a string or boolean is provided\n\n    For strings, the value for True/False is case insensitive\n    \"\"\"\n    if isinstance(val, bool):\n        return val\n    else:\n        return val.lower() == 'true'\n\n\ndef mask_proxy_url(proxy_url):\n    \"\"\"\n    Mask proxy url credentials.\n\n    :type proxy_url: str\n    :param proxy_url: The proxy url, i.e. https://username:password@proxy.com\n\n    :return: Masked proxy url, i.e. https://***:***@proxy.com\n    \"\"\"\n    mask = '*' * 3\n    parsed_url = urlparse(proxy_url)\n    if parsed_url.username:\n        proxy_url = proxy_url.replace(parsed_url.username, mask, 1)\n    if parsed_url.password:\n        proxy_url = proxy_url.replace(parsed_url.password, mask, 1)\n    return proxy_url\n\n\ndef _is_ipaddress(host):\n    \"\"\"Wrap urllib3's is_ipaddress to support bracketed IPv6 addresses.\"\"\"\n    return is_ipaddress(host) or bool(IPV6_ADDRZ_RE.match(host))\n\n\nclass ProxyConfiguration:\n    \"\"\"Represents a proxy configuration dictionary and additional settings.\n\n    This class represents a proxy configuration dictionary and provides utility\n    functions to retrieve well structured proxy urls and proxy headers from the\n    proxy configuration dictionary.\n    \"\"\"\n\n    def __init__(self, proxies=None, proxies_settings=None):\n        if proxies is None:\n            proxies = {}\n        if proxies_settings is None:\n            proxies_settings = {}\n\n        self._proxies = proxies\n        self._proxies_settings = proxies_settings\n\n    def proxy_url_for(self, url):\n        \"\"\"Retrieves the corresponding proxy url for a given url.\"\"\"\n        parsed_url = urlparse(url)\n        proxy = self._proxies.get(parsed_url.scheme)\n        if proxy:\n            proxy = self._fix_proxy_url(proxy)\n        return proxy\n\n    def proxy_headers_for(self, proxy_url):\n        \"\"\"Retrieves the corresponding proxy headers for a given proxy url.\"\"\"\n        headers = {}\n        username, password = self._get_auth_from_url(proxy_url)\n        if username and password:\n            basic_auth = self._construct_basic_auth(username, password)\n            headers['Proxy-Authorization'] = basic_auth\n        return headers\n\n    @property\n    def settings(self):\n        return self._proxies_settings\n\n    def _fix_proxy_url(self, proxy_url):\n        if proxy_url.startswith('http:') or proxy_url.startswith('https:'):\n            return proxy_url\n        elif proxy_url.startswith('//'):\n            return 'http:' + proxy_url\n        else:\n            return 'http://' + proxy_url\n\n    def _construct_basic_auth(self, username, password):\n        auth_str = f'{username}:{password}'\n        encoded_str = b64encode(auth_str.encode('ascii')).strip().decode()\n        return f'Basic {encoded_str}'\n\n    def _get_auth_from_url(self, url):\n        parsed_url = urlparse(url)\n        try:\n            return unquote(parsed_url.username), unquote(parsed_url.password)\n        except (AttributeError, TypeError):\n            return None, None\n\n\nclass URLLib3Session:\n    \"\"\"A basic HTTP client that supports connection pooling and proxies.\n\n    This class is inspired by requests.adapters.HTTPAdapter, but has been\n    boiled down to meet the use cases needed by botocore. For the most part\n    this classes matches the functionality of HTTPAdapter in requests v2.7.0\n    (the same as our vendored version). The only major difference of note is\n    that we currently do not support sending chunked requests. While requests\n    v2.7.0 implemented this themselves, later version urllib3 support this\n    directly via a flag to urlopen so enabling it if needed should be trivial.\n    \"\"\"\n\n    def __init__(\n        self,\n        verify=True,\n        proxies=None,\n        timeout=None,\n        max_pool_connections=MAX_POOL_CONNECTIONS,\n        socket_options=None,\n        client_cert=None,\n        proxies_config=None,\n    ):\n        self._verify = verify\n        self._proxy_config = ProxyConfiguration(\n            proxies=proxies, proxies_settings=proxies_config\n        )\n        self._pool_classes_by_scheme = {\n            'http': botocore.awsrequest.AWSHTTPConnectionPool,\n            'https': botocore.awsrequest.AWSHTTPSConnectionPool,\n        }\n        if timeout is None:\n            timeout = DEFAULT_TIMEOUT\n        if not isinstance(timeout, (int, float)):\n            timeout = Timeout(connect=timeout[0], read=timeout[1])\n\n        self._cert_file = None\n        self._key_file = None\n        if isinstance(client_cert, str):\n            self._cert_file = client_cert\n        elif isinstance(client_cert, tuple):\n            self._cert_file, self._key_file = client_cert\n\n        self._timeout = timeout\n        self._max_pool_connections = max_pool_connections\n        self._socket_options = socket_options\n        if socket_options is None:\n            self._socket_options = []\n        self._proxy_managers = {}\n        self._manager = PoolManager(**self._get_pool_manager_kwargs())\n        self._manager.pool_classes_by_scheme = self._pool_classes_by_scheme\n\n    def _proxies_kwargs(self, **kwargs):\n        proxies_settings = self._proxy_config.settings\n        proxies_kwargs = {\n            'use_forwarding_for_https': proxies_settings.get(\n                'proxy_use_forwarding_for_https'\n            ),\n            **kwargs,\n        }\n        return {k: v for k, v in proxies_kwargs.items() if v is not None}\n\n    def _get_pool_manager_kwargs(self, **extra_kwargs):\n        pool_manager_kwargs = {\n            'timeout': self._timeout,\n            'maxsize': self._max_pool_connections,\n            'ssl_context': self._get_ssl_context(),\n            'socket_options': self._socket_options,\n            'cert_file': self._cert_file,\n            'key_file': self._key_file,\n        }\n        pool_manager_kwargs.update(**extra_kwargs)\n        return pool_manager_kwargs\n\n    def _get_ssl_context(self):\n        return create_urllib3_context()\n\n    def _get_proxy_manager(self, proxy_url):\n        if proxy_url not in self._proxy_managers:\n            proxy_headers = self._proxy_config.proxy_headers_for(proxy_url)\n            proxy_ssl_context = self._setup_proxy_ssl_context(proxy_url)\n            proxy_manager_kwargs = self._get_pool_manager_kwargs(\n                proxy_headers=proxy_headers\n            )\n            proxy_manager_kwargs.update(\n                self._proxies_kwargs(proxy_ssl_context=proxy_ssl_context)\n            )\n            proxy_manager = proxy_from_url(proxy_url, **proxy_manager_kwargs)\n            proxy_manager.pool_classes_by_scheme = self._pool_classes_by_scheme\n            self._proxy_managers[proxy_url] = proxy_manager\n\n        return self._proxy_managers[proxy_url]\n\n    def _path_url(self, url):\n        parsed_url = urlparse(url)\n        path = parsed_url.path\n        if not path:\n            path = '/'\n        if parsed_url.query:\n            path = path + '?' + parsed_url.query\n        return path\n\n    def _setup_ssl_cert(self, conn, url, verify):\n        if url.lower().startswith('https') and verify:\n            conn.cert_reqs = 'CERT_REQUIRED'\n            conn.ca_certs = get_cert_path(verify)\n        else:\n            conn.cert_reqs = 'CERT_NONE'\n            conn.ca_certs = None\n\n    def _setup_proxy_ssl_context(self, proxy_url):\n        proxies_settings = self._proxy_config.settings\n        proxy_ca_bundle = proxies_settings.get('proxy_ca_bundle')\n        proxy_cert = proxies_settings.get('proxy_client_cert')\n        if proxy_ca_bundle is None and proxy_cert is None:\n            return None\n\n        context = self._get_ssl_context()\n        try:\n            url = parse_url(proxy_url)\n            # urllib3 disables this by default but we need it for proper\n            # proxy tls negotiation when proxy_url is not an IP Address\n            if not _is_ipaddress(url.host):\n                context.check_hostname = True\n            if proxy_ca_bundle is not None:\n                context.load_verify_locations(cafile=proxy_ca_bundle)\n\n            if isinstance(proxy_cert, tuple):\n                context.load_cert_chain(proxy_cert[0], keyfile=proxy_cert[1])\n            elif isinstance(proxy_cert, str):\n                context.load_cert_chain(proxy_cert)\n\n            return context\n        except (OSError, URLLib3SSLError, LocationParseError) as e:\n            raise InvalidProxiesConfigError(error=e)\n\n    def _get_connection_manager(self, url, proxy_url=None):\n        if proxy_url:\n            manager = self._get_proxy_manager(proxy_url)\n        else:\n            manager = self._manager\n        return manager\n\n    def _get_request_target(self, url, proxy_url):\n        has_proxy = proxy_url is not None\n\n        if not has_proxy:\n            return self._path_url(url)\n\n        # HTTP proxies expect the request_target to be the absolute url to know\n        # which host to establish a connection to. urllib3 also supports\n        # forwarding for HTTPS through the 'use_forwarding_for_https' parameter.\n        proxy_scheme = urlparse(proxy_url).scheme\n        using_https_forwarding_proxy = (\n            proxy_scheme == 'https'\n            and self._proxies_kwargs().get('use_forwarding_for_https', False)\n        )\n\n        if using_https_forwarding_proxy or url.startswith('http:'):\n            return url\n        else:\n            return self._path_url(url)\n\n    def _chunked(self, headers):\n        transfer_encoding = headers.get('Transfer-Encoding', b'')\n        transfer_encoding = ensure_bytes(transfer_encoding)\n        return transfer_encoding.lower() == b'chunked'\n\n    def close(self):\n        self._manager.clear()\n        for manager in self._proxy_managers.values():\n            manager.clear()\n\n    def send(self, request):\n        try:\n            proxy_url = self._proxy_config.proxy_url_for(request.url)\n            manager = self._get_connection_manager(request.url, proxy_url)\n            conn = manager.connection_from_url(request.url)\n            self._setup_ssl_cert(conn, request.url, self._verify)\n            if ensure_boolean(\n                os.environ.get('BOTO_EXPERIMENTAL__ADD_PROXY_HOST_HEADER', '')\n            ):\n                # This is currently an \"experimental\" feature which provides\n                # no guarantees of backwards compatibility. It may be subject\n                # to change or removal in any patch version. Anyone opting in\n                # to this feature should strictly pin botocore.\n                host = urlparse(request.url).hostname\n                conn.proxy_headers['host'] = host\n\n            request_target = self._get_request_target(request.url, proxy_url)\n            urllib_response = conn.urlopen(\n                method=request.method,\n                url=request_target,\n                body=request.body,\n                headers=request.headers,\n                retries=Retry(False),\n                assert_same_host=False,\n                preload_content=False,\n                decode_content=False,\n                chunked=self._chunked(request.headers),\n            )\n\n            http_response = botocore.awsrequest.AWSResponse(\n                request.url,\n                urllib_response.status,\n                urllib_response.headers,\n                urllib_response,\n            )\n\n            if not request.stream_output:\n                # Cause the raw stream to be exhausted immediately. We do it\n                # this way instead of using preload_content because\n                # preload_content will never buffer chunked responses\n                http_response.content\n\n            return http_response\n        except URLLib3SSLError as e:\n            raise SSLError(endpoint_url=request.url, error=e)\n        except (NewConnectionError, socket.gaierror) as e:\n            raise EndpointConnectionError(endpoint_url=request.url, error=e)\n        except ProxyError as e:\n            raise ProxyConnectionError(\n                proxy_url=mask_proxy_url(proxy_url), error=e\n            )\n        except URLLib3ConnectTimeoutError as e:\n            raise ConnectTimeoutError(endpoint_url=request.url, error=e)\n        except URLLib3ReadTimeoutError as e:\n            raise ReadTimeoutError(endpoint_url=request.url, error=e)\n        except ProtocolError as e:\n            raise ConnectionClosedError(\n                error=e, request=request, endpoint_url=request.url\n            )\n        except Exception as e:\n            message = 'Exception received when sending urllib3 HTTP request'\n            logger.debug(message, exc_info=True)\n            raise HTTPClientError(error=e)\n", "botocore/errorfactory.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom botocore.exceptions import ClientError\nfrom botocore.utils import get_service_module_name\n\n\nclass BaseClientExceptions:\n    ClientError = ClientError\n\n    def __init__(self, code_to_exception):\n        \"\"\"Base class for exceptions object on a client\n\n        :type code_to_exception: dict\n        :param code_to_exception: Mapping of error codes (strings) to exception\n            class that should be raised when encountering a particular\n            error code.\n        \"\"\"\n        self._code_to_exception = code_to_exception\n\n    def from_code(self, error_code):\n        \"\"\"Retrieves the error class based on the error code\n\n        This is helpful for identifying the exception class needing to be\n        caught based on the ClientError.parsed_reponse['Error']['Code'] value\n\n        :type error_code: string\n        :param error_code: The error code associated to a ClientError exception\n\n        :rtype: ClientError or a subclass of ClientError\n        :returns: The appropriate modeled exception class for that error\n            code. If the error code does not match any of the known\n            modeled exceptions then return a generic ClientError.\n        \"\"\"\n        return self._code_to_exception.get(error_code, self.ClientError)\n\n    def __getattr__(self, name):\n        exception_cls_names = [\n            exception_cls.__name__\n            for exception_cls in self._code_to_exception.values()\n        ]\n        raise AttributeError(\n            fr\"{self} object has no attribute {name}. \"\n            fr\"Valid exceptions are: {', '.join(exception_cls_names)}\"\n        )\n\n\nclass ClientExceptionsFactory:\n    def __init__(self):\n        self._client_exceptions_cache = {}\n\n    def create_client_exceptions(self, service_model):\n        \"\"\"Creates a ClientExceptions object for the particular service client\n\n        :type service_model: botocore.model.ServiceModel\n        :param service_model: The service model for the client\n\n        :rtype: object that subclasses from BaseClientExceptions\n        :returns: The exceptions object of a client that can be used\n            to grab the various different modeled exceptions.\n        \"\"\"\n        service_name = service_model.service_name\n        if service_name not in self._client_exceptions_cache:\n            client_exceptions = self._create_client_exceptions(service_model)\n            self._client_exceptions_cache[service_name] = client_exceptions\n        return self._client_exceptions_cache[service_name]\n\n    def _create_client_exceptions(self, service_model):\n        cls_props = {}\n        code_to_exception = {}\n        for error_shape in service_model.error_shapes:\n            exception_name = str(error_shape.name)\n            exception_cls = type(exception_name, (ClientError,), {})\n            cls_props[exception_name] = exception_cls\n            code = str(error_shape.error_code)\n            code_to_exception[code] = exception_cls\n        cls_name = str(get_service_module_name(service_model) + 'Exceptions')\n        client_exceptions_cls = type(\n            cls_name, (BaseClientExceptions,), cls_props\n        )\n        return client_exceptions_cls(code_to_exception)\n", "botocore/eventstream.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"Binary Event Stream Decoding \"\"\"\n\nfrom binascii import crc32\nfrom struct import unpack\n\nfrom botocore.exceptions import EventStreamError\n\n# byte length of the prelude (total_length + header_length + prelude_crc)\n_PRELUDE_LENGTH = 12\n_MAX_HEADERS_LENGTH = 128 * 1024  # 128 Kb\n_MAX_PAYLOAD_LENGTH = 16 * 1024**2  # 16 Mb\n\n\nclass ParserError(Exception):\n    \"\"\"Base binary flow encoding parsing exception.\"\"\"\n\n    pass\n\n\nclass DuplicateHeader(ParserError):\n    \"\"\"Duplicate header found in the event.\"\"\"\n\n    def __init__(self, header):\n        message = 'Duplicate header present: \"%s\"' % header\n        super().__init__(message)\n\n\nclass InvalidHeadersLength(ParserError):\n    \"\"\"Headers length is longer than the maximum.\"\"\"\n\n    def __init__(self, length):\n        message = 'Header length of {} exceeded the maximum of {}'.format(\n            length,\n            _MAX_HEADERS_LENGTH,\n        )\n        super().__init__(message)\n\n\nclass InvalidPayloadLength(ParserError):\n    \"\"\"Payload length is longer than the maximum.\"\"\"\n\n    def __init__(self, length):\n        message = 'Payload length of {} exceeded the maximum of {}'.format(\n            length,\n            _MAX_PAYLOAD_LENGTH,\n        )\n        super().__init__(message)\n\n\nclass ChecksumMismatch(ParserError):\n    \"\"\"Calculated checksum did not match the expected checksum.\"\"\"\n\n    def __init__(self, expected, calculated):\n        message = (\n            'Checksum mismatch: expected 0x{:08x}, calculated 0x{:08x}'.format(\n                expected,\n                calculated,\n            )\n        )\n        super().__init__(message)\n\n\nclass NoInitialResponseError(ParserError):\n    \"\"\"An event of type initial-response was not received.\n\n    This exception is raised when the event stream produced no events or\n    the first event in the stream was not of the initial-response type.\n    \"\"\"\n\n    def __init__(self):\n        message = 'First event was not of the initial-response type'\n        super().__init__(message)\n\n\nclass DecodeUtils:\n    \"\"\"Unpacking utility functions used in the decoder.\n\n    All methods on this class take raw bytes and return  a tuple containing\n    the value parsed from the bytes and the number of bytes consumed to parse\n    that value.\n    \"\"\"\n\n    UINT8_BYTE_FORMAT = '!B'\n    UINT16_BYTE_FORMAT = '!H'\n    UINT32_BYTE_FORMAT = '!I'\n    INT8_BYTE_FORMAT = '!b'\n    INT16_BYTE_FORMAT = '!h'\n    INT32_BYTE_FORMAT = '!i'\n    INT64_BYTE_FORMAT = '!q'\n    PRELUDE_BYTE_FORMAT = '!III'\n\n    # uint byte size to unpack format\n    UINT_BYTE_FORMAT = {\n        1: UINT8_BYTE_FORMAT,\n        2: UINT16_BYTE_FORMAT,\n        4: UINT32_BYTE_FORMAT,\n    }\n\n    @staticmethod\n    def unpack_true(data):\n        \"\"\"This method consumes none of the provided bytes and returns True.\n\n        :type data: bytes\n        :param data: The bytes to parse from. This is ignored in this method.\n\n        :rtype: tuple\n        :rtype: (bool, int)\n        :returns: The tuple (True, 0)\n        \"\"\"\n        return True, 0\n\n    @staticmethod\n    def unpack_false(data):\n        \"\"\"This method consumes none of the provided bytes and returns False.\n\n        :type data: bytes\n        :param data: The bytes to parse from. This is ignored in this method.\n\n        :rtype: tuple\n        :rtype: (bool, int)\n        :returns: The tuple (False, 0)\n        \"\"\"\n        return False, 0\n\n    @staticmethod\n    def unpack_uint8(data):\n        \"\"\"Parse an unsigned 8-bit integer from the bytes.\n\n        :type data: bytes\n        :param data: The bytes to parse from.\n\n        :rtype: (int, int)\n        :returns: A tuple containing the (parsed integer value, bytes consumed)\n        \"\"\"\n        value = unpack(DecodeUtils.UINT8_BYTE_FORMAT, data[:1])[0]\n        return value, 1\n\n    @staticmethod\n    def unpack_uint32(data):\n        \"\"\"Parse an unsigned 32-bit integer from the bytes.\n\n        :type data: bytes\n        :param data: The bytes to parse from.\n\n        :rtype: (int, int)\n        :returns: A tuple containing the (parsed integer value, bytes consumed)\n        \"\"\"\n        value = unpack(DecodeUtils.UINT32_BYTE_FORMAT, data[:4])[0]\n        return value, 4\n\n    @staticmethod\n    def unpack_int8(data):\n        \"\"\"Parse a signed 8-bit integer from the bytes.\n\n        :type data: bytes\n        :param data: The bytes to parse from.\n\n        :rtype: (int, int)\n        :returns: A tuple containing the (parsed integer value, bytes consumed)\n        \"\"\"\n        value = unpack(DecodeUtils.INT8_BYTE_FORMAT, data[:1])[0]\n        return value, 1\n\n    @staticmethod\n    def unpack_int16(data):\n        \"\"\"Parse a signed 16-bit integer from the bytes.\n\n        :type data: bytes\n        :param data: The bytes to parse from.\n\n        :rtype: tuple\n        :rtype: (int, int)\n        :returns: A tuple containing the (parsed integer value, bytes consumed)\n        \"\"\"\n        value = unpack(DecodeUtils.INT16_BYTE_FORMAT, data[:2])[0]\n        return value, 2\n\n    @staticmethod\n    def unpack_int32(data):\n        \"\"\"Parse a signed 32-bit integer from the bytes.\n\n        :type data: bytes\n        :param data: The bytes to parse from.\n\n        :rtype: tuple\n        :rtype: (int, int)\n        :returns: A tuple containing the (parsed integer value, bytes consumed)\n        \"\"\"\n        value = unpack(DecodeUtils.INT32_BYTE_FORMAT, data[:4])[0]\n        return value, 4\n\n    @staticmethod\n    def unpack_int64(data):\n        \"\"\"Parse a signed 64-bit integer from the bytes.\n\n        :type data: bytes\n        :param data: The bytes to parse from.\n\n        :rtype: tuple\n        :rtype: (int, int)\n        :returns: A tuple containing the (parsed integer value, bytes consumed)\n        \"\"\"\n        value = unpack(DecodeUtils.INT64_BYTE_FORMAT, data[:8])[0]\n        return value, 8\n\n    @staticmethod\n    def unpack_byte_array(data, length_byte_size=2):\n        \"\"\"Parse a variable length byte array from the bytes.\n\n        The bytes are expected to be in the following format:\n            [ length ][0 ... length bytes]\n        where length is an unsigned integer represented in the smallest number\n        of bytes to hold the maximum length of the array.\n\n        :type data: bytes\n        :param data: The bytes to parse from.\n\n        :type length_byte_size: int\n        :param length_byte_size: The byte size of the preceding integer that\n        represents the length of the array. Supported values are 1, 2, and 4.\n\n        :rtype: (bytes, int)\n        :returns: A tuple containing the (parsed byte array, bytes consumed).\n        \"\"\"\n        uint_byte_format = DecodeUtils.UINT_BYTE_FORMAT[length_byte_size]\n        length = unpack(uint_byte_format, data[:length_byte_size])[0]\n        bytes_end = length + length_byte_size\n        array_bytes = data[length_byte_size:bytes_end]\n        return array_bytes, bytes_end\n\n    @staticmethod\n    def unpack_utf8_string(data, length_byte_size=2):\n        \"\"\"Parse a variable length utf-8 string from the bytes.\n\n        The bytes are expected to be in the following format:\n            [ length ][0 ... length bytes]\n        where length is an unsigned integer represented in the smallest number\n        of bytes to hold the maximum length of the array and the following\n        bytes are a valid utf-8 string.\n\n        :type data: bytes\n        :param bytes: The bytes to parse from.\n\n        :type length_byte_size: int\n        :param length_byte_size: The byte size of the preceding integer that\n        represents the length of the array. Supported values are 1, 2, and 4.\n\n        :rtype: (str, int)\n        :returns: A tuple containing the (utf-8 string, bytes consumed).\n        \"\"\"\n        array_bytes, consumed = DecodeUtils.unpack_byte_array(\n            data, length_byte_size\n        )\n        return array_bytes.decode('utf-8'), consumed\n\n    @staticmethod\n    def unpack_uuid(data):\n        \"\"\"Parse a 16-byte uuid from the bytes.\n\n        :type data: bytes\n        :param data: The bytes to parse from.\n\n        :rtype: (bytes, int)\n        :returns: A tuple containing the (uuid bytes, bytes consumed).\n        \"\"\"\n        return data[:16], 16\n\n    @staticmethod\n    def unpack_prelude(data):\n        \"\"\"Parse the prelude for an event stream message from the bytes.\n\n        The prelude for an event stream message has the following format:\n            [total_length][header_length][prelude_crc]\n        where each field is an unsigned 32-bit integer.\n\n        :rtype: ((int, int, int), int)\n        :returns: A tuple of ((total_length, headers_length, prelude_crc),\n        consumed)\n        \"\"\"\n        return (unpack(DecodeUtils.PRELUDE_BYTE_FORMAT, data), _PRELUDE_LENGTH)\n\n\ndef _validate_checksum(data, checksum, crc=0):\n    # To generate the same numeric value across all Python versions and\n    # platforms use crc32(data) & 0xffffffff.\n    computed_checksum = crc32(data, crc) & 0xFFFFFFFF\n    if checksum != computed_checksum:\n        raise ChecksumMismatch(checksum, computed_checksum)\n\n\nclass MessagePrelude:\n    \"\"\"Represents the prelude of an event stream message.\"\"\"\n\n    def __init__(self, total_length, headers_length, crc):\n        self.total_length = total_length\n        self.headers_length = headers_length\n        self.crc = crc\n\n    @property\n    def payload_length(self):\n        \"\"\"Calculates the total payload length.\n\n        The extra minus 4 bytes is for the message CRC.\n\n        :rtype: int\n        :returns: The total payload length.\n        \"\"\"\n        return self.total_length - self.headers_length - _PRELUDE_LENGTH - 4\n\n    @property\n    def payload_end(self):\n        \"\"\"Calculates the byte offset for the end of the message payload.\n\n        The extra minus 4 bytes is for the message CRC.\n\n        :rtype: int\n        :returns: The byte offset from the beginning of the event stream\n        message to the end of the payload.\n        \"\"\"\n        return self.total_length - 4\n\n    @property\n    def headers_end(self):\n        \"\"\"Calculates the byte offset for the end of the message headers.\n\n        :rtype: int\n        :returns: The byte offset from the beginning of the event stream\n        message to the end of the headers.\n        \"\"\"\n        return _PRELUDE_LENGTH + self.headers_length\n\n\nclass EventStreamMessage:\n    \"\"\"Represents an event stream message.\"\"\"\n\n    def __init__(self, prelude, headers, payload, crc):\n        self.prelude = prelude\n        self.headers = headers\n        self.payload = payload\n        self.crc = crc\n\n    def to_response_dict(self, status_code=200):\n        message_type = self.headers.get(':message-type')\n        if message_type == 'error' or message_type == 'exception':\n            status_code = 400\n        return {\n            'status_code': status_code,\n            'headers': self.headers,\n            'body': self.payload,\n        }\n\n\nclass EventStreamHeaderParser:\n    \"\"\"Parses the event headers from an event stream message.\n\n    Expects all of the header data upfront and creates a dictionary of headers\n    to return. This object can be reused multiple times to parse the headers\n    from multiple event stream messages.\n    \"\"\"\n\n    # Maps header type to appropriate unpacking function\n    # These unpacking functions return the value and the amount unpacked\n    _HEADER_TYPE_MAP = {\n        # boolean_true\n        0: DecodeUtils.unpack_true,\n        # boolean_false\n        1: DecodeUtils.unpack_false,\n        # byte\n        2: DecodeUtils.unpack_int8,\n        # short\n        3: DecodeUtils.unpack_int16,\n        # integer\n        4: DecodeUtils.unpack_int32,\n        # long\n        5: DecodeUtils.unpack_int64,\n        # byte_array\n        6: DecodeUtils.unpack_byte_array,\n        # string\n        7: DecodeUtils.unpack_utf8_string,\n        # timestamp\n        8: DecodeUtils.unpack_int64,\n        # uuid\n        9: DecodeUtils.unpack_uuid,\n    }\n\n    def __init__(self):\n        self._data = None\n\n    def parse(self, data):\n        \"\"\"Parses the event stream headers from an event stream message.\n\n        :type data: bytes\n        :param data: The bytes that correspond to the headers section of an\n        event stream message.\n\n        :rtype: dict\n        :returns: A dictionary of header key, value pairs.\n        \"\"\"\n        self._data = data\n        return self._parse_headers()\n\n    def _parse_headers(self):\n        headers = {}\n        while self._data:\n            name, value = self._parse_header()\n            if name in headers:\n                raise DuplicateHeader(name)\n            headers[name] = value\n        return headers\n\n    def _parse_header(self):\n        name = self._parse_name()\n        value = self._parse_value()\n        return name, value\n\n    def _parse_name(self):\n        name, consumed = DecodeUtils.unpack_utf8_string(self._data, 1)\n        self._advance_data(consumed)\n        return name\n\n    def _parse_type(self):\n        type, consumed = DecodeUtils.unpack_uint8(self._data)\n        self._advance_data(consumed)\n        return type\n\n    def _parse_value(self):\n        header_type = self._parse_type()\n        value_unpacker = self._HEADER_TYPE_MAP[header_type]\n        value, consumed = value_unpacker(self._data)\n        self._advance_data(consumed)\n        return value\n\n    def _advance_data(self, consumed):\n        self._data = self._data[consumed:]\n\n\nclass EventStreamBuffer:\n    \"\"\"Streaming based event stream buffer\n\n    A buffer class that wraps bytes from an event stream providing parsed\n    messages as they become available via an iterable interface.\n    \"\"\"\n\n    def __init__(self):\n        self._data = b''\n        self._prelude = None\n        self._header_parser = EventStreamHeaderParser()\n\n    def add_data(self, data):\n        \"\"\"Add data to the buffer.\n\n        :type data: bytes\n        :param data: The bytes to add to the buffer to be used when parsing\n        \"\"\"\n        self._data += data\n\n    def _validate_prelude(self, prelude):\n        if prelude.headers_length > _MAX_HEADERS_LENGTH:\n            raise InvalidHeadersLength(prelude.headers_length)\n\n        if prelude.payload_length > _MAX_PAYLOAD_LENGTH:\n            raise InvalidPayloadLength(prelude.payload_length)\n\n    def _parse_prelude(self):\n        prelude_bytes = self._data[:_PRELUDE_LENGTH]\n        raw_prelude, _ = DecodeUtils.unpack_prelude(prelude_bytes)\n        prelude = MessagePrelude(*raw_prelude)\n        self._validate_prelude(prelude)\n        # The minus 4 removes the prelude crc from the bytes to be checked\n        _validate_checksum(prelude_bytes[: _PRELUDE_LENGTH - 4], prelude.crc)\n        return prelude\n\n    def _parse_headers(self):\n        header_bytes = self._data[_PRELUDE_LENGTH : self._prelude.headers_end]\n        return self._header_parser.parse(header_bytes)\n\n    def _parse_payload(self):\n        prelude = self._prelude\n        payload_bytes = self._data[prelude.headers_end : prelude.payload_end]\n        return payload_bytes\n\n    def _parse_message_crc(self):\n        prelude = self._prelude\n        crc_bytes = self._data[prelude.payload_end : prelude.total_length]\n        message_crc, _ = DecodeUtils.unpack_uint32(crc_bytes)\n        return message_crc\n\n    def _parse_message_bytes(self):\n        # The minus 4 includes the prelude crc to the bytes to be checked\n        message_bytes = self._data[\n            _PRELUDE_LENGTH - 4 : self._prelude.payload_end\n        ]\n        return message_bytes\n\n    def _validate_message_crc(self):\n        message_crc = self._parse_message_crc()\n        message_bytes = self._parse_message_bytes()\n        _validate_checksum(message_bytes, message_crc, crc=self._prelude.crc)\n        return message_crc\n\n    def _parse_message(self):\n        crc = self._validate_message_crc()\n        headers = self._parse_headers()\n        payload = self._parse_payload()\n        message = EventStreamMessage(self._prelude, headers, payload, crc)\n        self._prepare_for_next_message()\n        return message\n\n    def _prepare_for_next_message(self):\n        # Advance the data and reset the current prelude\n        self._data = self._data[self._prelude.total_length :]\n        self._prelude = None\n\n    def next(self):\n        \"\"\"Provides the next available message parsed from the stream\n\n        :rtype: EventStreamMessage\n        :returns: The next event stream message\n        \"\"\"\n        if len(self._data) < _PRELUDE_LENGTH:\n            raise StopIteration()\n\n        if self._prelude is None:\n            self._prelude = self._parse_prelude()\n\n        if len(self._data) < self._prelude.total_length:\n            raise StopIteration()\n\n        return self._parse_message()\n\n    def __next__(self):\n        return self.next()\n\n    def __iter__(self):\n        return self\n\n\nclass EventStream:\n    \"\"\"Wrapper class for an event stream body.\n\n    This wraps the underlying streaming body, parsing it for individual events\n    and yielding them as they come available through the iterator interface.\n\n    The following example uses the S3 select API to get structured data out of\n    an object stored in S3 using an event stream.\n\n    **Example:**\n    ::\n        from botocore.session import Session\n\n        s3 = Session().create_client('s3')\n        response = s3.select_object_content(\n            Bucket='bucketname',\n            Key='keyname',\n            ExpressionType='SQL',\n            RequestProgress={'Enabled': True},\n            Expression=\"SELECT * FROM S3Object s\",\n            InputSerialization={'CSV': {}},\n            OutputSerialization={'CSV': {}},\n        )\n        # This is the event stream in the response\n        event_stream = response['Payload']\n        end_event_received = False\n        with open('output', 'wb') as f:\n            # Iterate over events in the event stream as they come\n            for event in event_stream:\n                # If we received a records event, write the data to a file\n                if 'Records' in event:\n                    data = event['Records']['Payload']\n                    f.write(data)\n                # If we received a progress event, print the details\n                elif 'Progress' in event:\n                    print(event['Progress']['Details'])\n                # End event indicates that the request finished successfully\n                elif 'End' in event:\n                    print('Result is complete')\n                    end_event_received = True\n        if not end_event_received:\n            raise Exception(\"End event not received, request incomplete.\")\n    \"\"\"\n\n    def __init__(self, raw_stream, output_shape, parser, operation_name):\n        self._raw_stream = raw_stream\n        self._output_shape = output_shape\n        self._operation_name = operation_name\n        self._parser = parser\n        self._event_generator = self._create_raw_event_generator()\n\n    def __iter__(self):\n        for event in self._event_generator:\n            parsed_event = self._parse_event(event)\n            if parsed_event:\n                yield parsed_event\n\n    def _create_raw_event_generator(self):\n        event_stream_buffer = EventStreamBuffer()\n        for chunk in self._raw_stream.stream():\n            event_stream_buffer.add_data(chunk)\n            yield from event_stream_buffer\n\n    def _parse_event(self, event):\n        response_dict = event.to_response_dict()\n        parsed_response = self._parser.parse(response_dict, self._output_shape)\n        if response_dict['status_code'] == 200:\n            return parsed_response\n        else:\n            raise EventStreamError(parsed_response, self._operation_name)\n\n    def get_initial_response(self):\n        try:\n            initial_event = next(self._event_generator)\n            event_type = initial_event.headers.get(':event-type')\n            if event_type == 'initial-response':\n                return initial_event\n        except StopIteration:\n            pass\n        raise NoInitialResponseError()\n\n    def close(self):\n        \"\"\"Closes the underlying streaming body.\"\"\"\n        self._raw_stream.close()\n", "botocore/waiter.py": "# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport logging\nimport time\n\nimport jmespath\n\nfrom botocore.docs.docstring import WaiterDocstring\nfrom botocore.utils import get_service_module_name\n\nfrom . import xform_name\nfrom .exceptions import ClientError, WaiterConfigError, WaiterError\n\nlogger = logging.getLogger(__name__)\n\n\ndef create_waiter_with_client(waiter_name, waiter_model, client):\n    \"\"\"\n\n    :type waiter_name: str\n    :param waiter_name: The name of the waiter.  The name should match\n        the name (including the casing) of the key name in the waiter\n        model file (typically this is CamelCasing).\n\n    :type waiter_model: botocore.waiter.WaiterModel\n    :param waiter_model: The model for the waiter configuration.\n\n    :type client: botocore.client.BaseClient\n    :param client: The botocore client associated with the service.\n\n    :rtype: botocore.waiter.Waiter\n    :return: The waiter object.\n\n    \"\"\"\n    single_waiter_config = waiter_model.get_waiter(waiter_name)\n    operation_name = xform_name(single_waiter_config.operation)\n    operation_method = NormalizedOperationMethod(\n        getattr(client, operation_name)\n    )\n\n    # Create a new wait method that will serve as a proxy to the underlying\n    # Waiter.wait method. This is needed to attach a docstring to the\n    # method.\n    def wait(self, **kwargs):\n        Waiter.wait(self, **kwargs)\n\n    wait.__doc__ = WaiterDocstring(\n        waiter_name=waiter_name,\n        event_emitter=client.meta.events,\n        service_model=client.meta.service_model,\n        service_waiter_model=waiter_model,\n        include_signature=False,\n    )\n\n    # Rename the waiter class based on the type of waiter.\n    waiter_class_name = str(\n        '%s.Waiter.%s'\n        % (get_service_module_name(client.meta.service_model), waiter_name)\n    )\n\n    # Create the new waiter class\n    documented_waiter_cls = type(waiter_class_name, (Waiter,), {'wait': wait})\n\n    # Return an instance of the new waiter class.\n    return documented_waiter_cls(\n        waiter_name, single_waiter_config, operation_method\n    )\n\n\ndef is_valid_waiter_error(response):\n    error = response.get('Error')\n    if isinstance(error, dict) and 'Code' in error:\n        return True\n    return False\n\n\nclass NormalizedOperationMethod:\n    def __init__(self, client_method):\n        self._client_method = client_method\n\n    def __call__(self, **kwargs):\n        try:\n            return self._client_method(**kwargs)\n        except ClientError as e:\n            return e.response\n\n\nclass WaiterModel:\n    SUPPORTED_VERSION = 2\n\n    def __init__(self, waiter_config):\n        \"\"\"\n\n        Note that the WaiterModel takes ownership of the waiter_config.\n        It may or may not mutate the waiter_config.  If this is a concern,\n        it is best to make a copy of the waiter config before passing it to\n        the WaiterModel.\n\n        :type waiter_config: dict\n        :param waiter_config: The loaded waiter config\n            from the <service>*.waiters.json file.  This can be\n            obtained from a botocore Loader object as well.\n\n        \"\"\"\n        self._waiter_config = waiter_config['waiters']\n\n        # These are part of the public API.  Changing these\n        # will result in having to update the consuming code,\n        # so don't change unless you really need to.\n        version = waiter_config.get('version', 'unknown')\n        self._verify_supported_version(version)\n        self.version = version\n        self.waiter_names = list(sorted(waiter_config['waiters'].keys()))\n\n    def _verify_supported_version(self, version):\n        if version != self.SUPPORTED_VERSION:\n            raise WaiterConfigError(\n                error_msg=(\n                    \"Unsupported waiter version, supported version \"\n                    \"must be: %s, but version of waiter config \"\n                    \"is: %s\" % (self.SUPPORTED_VERSION, version)\n                )\n            )\n\n    def get_waiter(self, waiter_name):\n        try:\n            single_waiter_config = self._waiter_config[waiter_name]\n        except KeyError:\n            raise ValueError(\"Waiter does not exist: %s\" % waiter_name)\n        return SingleWaiterConfig(single_waiter_config)\n\n\nclass SingleWaiterConfig:\n    \"\"\"Represents the waiter configuration for a single waiter.\n\n    A single waiter is considered the configuration for a single\n    value associated with a named waiter (i.e TableExists).\n\n    \"\"\"\n\n    def __init__(self, single_waiter_config):\n        self._config = single_waiter_config\n\n        # These attributes are part of the public API.\n        self.description = single_waiter_config.get('description', '')\n        # Per the spec, these three fields are required.\n        self.operation = single_waiter_config['operation']\n        self.delay = single_waiter_config['delay']\n        self.max_attempts = single_waiter_config['maxAttempts']\n\n    @property\n    def acceptors(self):\n        acceptors = []\n        for acceptor_config in self._config['acceptors']:\n            acceptor = AcceptorConfig(acceptor_config)\n            acceptors.append(acceptor)\n        return acceptors\n\n\nclass AcceptorConfig:\n    def __init__(self, config):\n        self.state = config['state']\n        self.matcher = config['matcher']\n        self.expected = config['expected']\n        self.argument = config.get('argument')\n        self.matcher_func = self._create_matcher_func()\n\n    @property\n    def explanation(self):\n        if self.matcher == 'path':\n            return 'For expression \"{}\" we matched expected path: \"{}\"'.format(\n                self.argument,\n                self.expected,\n            )\n        elif self.matcher == 'pathAll':\n            return (\n                'For expression \"%s\" all members matched excepted path: \"%s\"'\n                % (self.argument, self.expected)\n            )\n        elif self.matcher == 'pathAny':\n            return (\n                'For expression \"%s\" we matched expected path: \"%s\" at least once'\n                % (self.argument, self.expected)\n            )\n        elif self.matcher == 'status':\n            return 'Matched expected HTTP status code: %s' % self.expected\n        elif self.matcher == 'error':\n            return 'Matched expected service error code: %s' % self.expected\n        else:\n            return (\n                'No explanation for unknown waiter type: \"%s\"' % self.matcher\n            )\n\n    def _create_matcher_func(self):\n        # An acceptor function is a callable that takes a single value.  The\n        # parsed AWS response.  Note that the parsed error response is also\n        # provided in the case of errors, so it's entirely possible to\n        # handle all the available matcher capabilities in the future.\n        # There's only three supported matchers, so for now, this is all\n        # contained to a single method.  If this grows, we can expand this\n        # out to separate methods or even objects.\n\n        if self.matcher == 'path':\n            return self._create_path_matcher()\n        elif self.matcher == 'pathAll':\n            return self._create_path_all_matcher()\n        elif self.matcher == 'pathAny':\n            return self._create_path_any_matcher()\n        elif self.matcher == 'status':\n            return self._create_status_matcher()\n        elif self.matcher == 'error':\n            return self._create_error_matcher()\n        else:\n            raise WaiterConfigError(\n                error_msg=\"Unknown acceptor: %s\" % self.matcher\n            )\n\n    def _create_path_matcher(self):\n        expression = jmespath.compile(self.argument)\n        expected = self.expected\n\n        def acceptor_matches(response):\n            if is_valid_waiter_error(response):\n                return\n            return expression.search(response) == expected\n\n        return acceptor_matches\n\n    def _create_path_all_matcher(self):\n        expression = jmespath.compile(self.argument)\n        expected = self.expected\n\n        def acceptor_matches(response):\n            if is_valid_waiter_error(response):\n                return\n            result = expression.search(response)\n            if not isinstance(result, list) or not result:\n                # pathAll matcher must result in a list.\n                # Also we require at least one element in the list,\n                # that is, an empty list should not result in this\n                # acceptor match.\n                return False\n            for element in result:\n                if element != expected:\n                    return False\n            return True\n\n        return acceptor_matches\n\n    def _create_path_any_matcher(self):\n        expression = jmespath.compile(self.argument)\n        expected = self.expected\n\n        def acceptor_matches(response):\n            if is_valid_waiter_error(response):\n                return\n            result = expression.search(response)\n            if not isinstance(result, list) or not result:\n                # pathAny matcher must result in a list.\n                # Also we require at least one element in the list,\n                # that is, an empty list should not result in this\n                # acceptor match.\n                return False\n            for element in result:\n                if element == expected:\n                    return True\n            return False\n\n        return acceptor_matches\n\n    def _create_status_matcher(self):\n        expected = self.expected\n\n        def acceptor_matches(response):\n            # We don't have any requirements on the expected incoming data\n            # other than it is a dict, so we don't assume there's\n            # a ResponseMetadata.HTTPStatusCode.\n            status_code = response.get('ResponseMetadata', {}).get(\n                'HTTPStatusCode'\n            )\n            return status_code == expected\n\n        return acceptor_matches\n\n    def _create_error_matcher(self):\n        expected = self.expected\n\n        def acceptor_matches(response):\n            # When the client encounters an error, it will normally raise\n            # an exception.  However, the waiter implementation will catch\n            # this exception, and instead send us the parsed error\n            # response.  So response is still a dictionary, and in the case\n            # of an error response will contain the \"Error\" and\n            # \"ResponseMetadata\" key.\n            return response.get(\"Error\", {}).get(\"Code\", \"\") == expected\n\n        return acceptor_matches\n\n\nclass Waiter:\n    def __init__(self, name, config, operation_method):\n        \"\"\"\n\n        :type name: string\n        :param name: The name of the waiter\n\n        :type config: botocore.waiter.SingleWaiterConfig\n        :param config: The configuration for the waiter.\n\n        :type operation_method: callable\n        :param operation_method: A callable that accepts **kwargs\n            and returns a response.  For example, this can be\n            a method from a botocore client.\n\n        \"\"\"\n        self._operation_method = operation_method\n        # The two attributes are exposed to allow for introspection\n        # and documentation.\n        self.name = name\n        self.config = config\n\n    def wait(self, **kwargs):\n        acceptors = list(self.config.acceptors)\n        current_state = 'waiting'\n        # pop the invocation specific config\n        config = kwargs.pop('WaiterConfig', {})\n        sleep_amount = config.get('Delay', self.config.delay)\n        max_attempts = config.get('MaxAttempts', self.config.max_attempts)\n        last_matched_acceptor = None\n        num_attempts = 0\n\n        while True:\n            response = self._operation_method(**kwargs)\n            num_attempts += 1\n            for acceptor in acceptors:\n                if acceptor.matcher_func(response):\n                    last_matched_acceptor = acceptor\n                    current_state = acceptor.state\n                    break\n            else:\n                # If none of the acceptors matched, we should\n                # transition to the failure state if an error\n                # response was received.\n                if is_valid_waiter_error(response):\n                    # Transition to a failure state, which we\n                    # can just handle here by raising an exception.\n                    raise WaiterError(\n                        name=self.name,\n                        reason='An error occurred (%s): %s'\n                        % (\n                            response['Error'].get('Code', 'Unknown'),\n                            response['Error'].get('Message', 'Unknown'),\n                        ),\n                        last_response=response,\n                    )\n            if current_state == 'success':\n                logger.debug(\n                    \"Waiting complete, waiter matched the \" \"success state.\"\n                )\n                return\n            if current_state == 'failure':\n                reason = 'Waiter encountered a terminal failure state: %s' % (\n                    acceptor.explanation\n                )\n                raise WaiterError(\n                    name=self.name,\n                    reason=reason,\n                    last_response=response,\n                )\n            if num_attempts >= max_attempts:\n                if last_matched_acceptor is None:\n                    reason = 'Max attempts exceeded'\n                else:\n                    reason = (\n                        'Max attempts exceeded. Previously accepted state: %s'\n                        % (acceptor.explanation)\n                    )\n                raise WaiterError(\n                    name=self.name,\n                    reason=reason,\n                    last_response=response,\n                )\n            time.sleep(sleep_amount)\n", "botocore/credentials.py": "# Copyright (c) 2012-2013 Mitch Garnaat http://garnaat.org/\n# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport datetime\nimport getpass\nimport json\nimport logging\nimport os\nimport subprocess\nimport threading\nimport time\nfrom collections import namedtuple\nfrom copy import deepcopy\nfrom hashlib import sha1\n\nfrom dateutil.parser import parse\nfrom dateutil.tz import tzlocal, tzutc\n\nimport botocore.compat\nimport botocore.configloader\nfrom botocore import UNSIGNED\nfrom botocore.compat import compat_shell_split, total_seconds\nfrom botocore.config import Config\nfrom botocore.exceptions import (\n    ConfigNotFound,\n    CredentialRetrievalError,\n    InfiniteLoopConfigError,\n    InvalidConfigError,\n    MetadataRetrievalError,\n    PartialCredentialsError,\n    RefreshWithMFAUnsupportedError,\n    UnauthorizedSSOTokenError,\n    UnknownCredentialError,\n)\nfrom botocore.tokens import SSOTokenProvider\nfrom botocore.utils import (\n    ContainerMetadataFetcher,\n    FileWebIdentityTokenLoader,\n    InstanceMetadataFetcher,\n    JSONFileCache,\n    SSOTokenLoader,\n    parse_key_val_file,\n    resolve_imds_endpoint_mode,\n)\n\nlogger = logging.getLogger(__name__)\nReadOnlyCredentials = namedtuple(\n    'ReadOnlyCredentials', ['access_key', 'secret_key', 'token']\n)\n\n_DEFAULT_MANDATORY_REFRESH_TIMEOUT = 10 * 60  # 10 min\n_DEFAULT_ADVISORY_REFRESH_TIMEOUT = 15 * 60  # 15 min\n\n\ndef create_credential_resolver(session, cache=None, region_name=None):\n    \"\"\"Create a default credential resolver.\n\n    This creates a pre-configured credential resolver\n    that includes the default lookup chain for\n    credentials.\n\n    \"\"\"\n    profile_name = session.get_config_variable('profile') or 'default'\n    metadata_timeout = session.get_config_variable('metadata_service_timeout')\n    num_attempts = session.get_config_variable('metadata_service_num_attempts')\n    disable_env_vars = session.instance_variables().get('profile') is not None\n\n    imds_config = {\n        'ec2_metadata_service_endpoint': session.get_config_variable(\n            'ec2_metadata_service_endpoint'\n        ),\n        'ec2_metadata_service_endpoint_mode': resolve_imds_endpoint_mode(\n            session\n        ),\n        'ec2_credential_refresh_window': _DEFAULT_ADVISORY_REFRESH_TIMEOUT,\n        'ec2_metadata_v1_disabled': session.get_config_variable(\n            'ec2_metadata_v1_disabled'\n        ),\n    }\n\n    if cache is None:\n        cache = {}\n\n    env_provider = EnvProvider()\n    container_provider = ContainerProvider()\n    instance_metadata_provider = InstanceMetadataProvider(\n        iam_role_fetcher=InstanceMetadataFetcher(\n            timeout=metadata_timeout,\n            num_attempts=num_attempts,\n            user_agent=session.user_agent(),\n            config=imds_config,\n        )\n    )\n\n    profile_provider_builder = ProfileProviderBuilder(\n        session, cache=cache, region_name=region_name\n    )\n    assume_role_provider = AssumeRoleProvider(\n        load_config=lambda: session.full_config,\n        client_creator=_get_client_creator(session, region_name),\n        cache=cache,\n        profile_name=profile_name,\n        credential_sourcer=CanonicalNameCredentialSourcer(\n            [env_provider, container_provider, instance_metadata_provider]\n        ),\n        profile_provider_builder=profile_provider_builder,\n    )\n\n    pre_profile = [\n        env_provider,\n        assume_role_provider,\n    ]\n    profile_providers = profile_provider_builder.providers(\n        profile_name=profile_name,\n        disable_env_vars=disable_env_vars,\n    )\n    post_profile = [\n        OriginalEC2Provider(),\n        BotoProvider(),\n        container_provider,\n        instance_metadata_provider,\n    ]\n    providers = pre_profile + profile_providers + post_profile\n\n    if disable_env_vars:\n        # An explicitly provided profile will negate an EnvProvider.\n        # We will defer to providers that understand the \"profile\"\n        # concept to retrieve credentials.\n        # The one edge case if is all three values are provided via\n        # env vars:\n        # export AWS_ACCESS_KEY_ID=foo\n        # export AWS_SECRET_ACCESS_KEY=bar\n        # export AWS_PROFILE=baz\n        # Then, just like our client() calls, the explicit credentials\n        # will take precedence.\n        #\n        # This precedence is enforced by leaving the EnvProvider in the chain.\n        # This means that the only way a \"profile\" would win is if the\n        # EnvProvider does not return credentials, which is what we want\n        # in this scenario.\n        providers.remove(env_provider)\n        logger.debug(\n            'Skipping environment variable credential check'\n            ' because profile name was explicitly set.'\n        )\n\n    resolver = CredentialResolver(providers=providers)\n    return resolver\n\n\nclass ProfileProviderBuilder:\n    \"\"\"This class handles the creation of profile based providers.\n\n    NOTE: This class is only intended for internal use.\n\n    This class handles the creation and ordering of the various credential\n    providers that primarly source their configuration from the shared config.\n    This is needed to enable sharing between the default credential chain and\n    the source profile chain created by the assume role provider.\n    \"\"\"\n\n    def __init__(\n        self, session, cache=None, region_name=None, sso_token_cache=None\n    ):\n        self._session = session\n        self._cache = cache\n        self._region_name = region_name\n        self._sso_token_cache = sso_token_cache\n\n    def providers(self, profile_name, disable_env_vars=False):\n        return [\n            self._create_web_identity_provider(\n                profile_name,\n                disable_env_vars,\n            ),\n            self._create_sso_provider(profile_name),\n            self._create_shared_credential_provider(profile_name),\n            self._create_process_provider(profile_name),\n            self._create_config_provider(profile_name),\n        ]\n\n    def _create_process_provider(self, profile_name):\n        return ProcessProvider(\n            profile_name=profile_name,\n            load_config=lambda: self._session.full_config,\n        )\n\n    def _create_shared_credential_provider(self, profile_name):\n        credential_file = self._session.get_config_variable('credentials_file')\n        return SharedCredentialProvider(\n            profile_name=profile_name,\n            creds_filename=credential_file,\n        )\n\n    def _create_config_provider(self, profile_name):\n        config_file = self._session.get_config_variable('config_file')\n        return ConfigProvider(\n            profile_name=profile_name,\n            config_filename=config_file,\n        )\n\n    def _create_web_identity_provider(self, profile_name, disable_env_vars):\n        return AssumeRoleWithWebIdentityProvider(\n            load_config=lambda: self._session.full_config,\n            client_creator=_get_client_creator(\n                self._session, self._region_name\n            ),\n            cache=self._cache,\n            profile_name=profile_name,\n            disable_env_vars=disable_env_vars,\n        )\n\n    def _create_sso_provider(self, profile_name):\n        return SSOProvider(\n            load_config=lambda: self._session.full_config,\n            client_creator=self._session.create_client,\n            profile_name=profile_name,\n            cache=self._cache,\n            token_cache=self._sso_token_cache,\n            token_provider=SSOTokenProvider(\n                self._session,\n                cache=self._sso_token_cache,\n                profile_name=profile_name,\n            ),\n        )\n\n\ndef get_credentials(session):\n    resolver = create_credential_resolver(session)\n    return resolver.load_credentials()\n\n\ndef _local_now():\n    return datetime.datetime.now(tzlocal())\n\n\ndef _parse_if_needed(value):\n    if isinstance(value, datetime.datetime):\n        return value\n    return parse(value)\n\n\ndef _serialize_if_needed(value, iso=False):\n    if isinstance(value, datetime.datetime):\n        if iso:\n            return value.isoformat()\n        return value.strftime('%Y-%m-%dT%H:%M:%S%Z')\n    return value\n\n\ndef _get_client_creator(session, region_name):\n    def client_creator(service_name, **kwargs):\n        create_client_kwargs = {'region_name': region_name}\n        create_client_kwargs.update(**kwargs)\n        return session.create_client(service_name, **create_client_kwargs)\n\n    return client_creator\n\n\ndef create_assume_role_refresher(client, params):\n    def refresh():\n        response = client.assume_role(**params)\n        credentials = response['Credentials']\n        # We need to normalize the credential names to\n        # the values expected by the refresh creds.\n        return {\n            'access_key': credentials['AccessKeyId'],\n            'secret_key': credentials['SecretAccessKey'],\n            'token': credentials['SessionToken'],\n            'expiry_time': _serialize_if_needed(credentials['Expiration']),\n        }\n\n    return refresh\n\n\ndef create_mfa_serial_refresher(actual_refresh):\n    class _Refresher:\n        def __init__(self, refresh):\n            self._refresh = refresh\n            self._has_been_called = False\n\n        def __call__(self):\n            if self._has_been_called:\n                # We can explore an option in the future to support\n                # reprompting for MFA, but for now we just error out\n                # when the temp creds expire.\n                raise RefreshWithMFAUnsupportedError()\n            self._has_been_called = True\n            return self._refresh()\n\n    return _Refresher(actual_refresh)\n\n\nclass Credentials:\n    \"\"\"\n    Holds the credentials needed to authenticate requests.\n\n    :param str access_key: The access key part of the credentials.\n    :param str secret_key: The secret key part of the credentials.\n    :param str token: The security token, valid only for session credentials.\n    :param str method: A string which identifies where the credentials\n        were found.\n    \"\"\"\n\n    def __init__(self, access_key, secret_key, token=None, method=None):\n        self.access_key = access_key\n        self.secret_key = secret_key\n        self.token = token\n\n        if method is None:\n            method = 'explicit'\n        self.method = method\n\n        self._normalize()\n\n    def _normalize(self):\n        # Keys would sometimes (accidentally) contain non-ascii characters.\n        # It would cause a confusing UnicodeDecodeError in Python 2.\n        # We explicitly convert them into unicode to avoid such error.\n        #\n        # Eventually the service will decide whether to accept the credential.\n        # This also complies with the behavior in Python 3.\n        self.access_key = botocore.compat.ensure_unicode(self.access_key)\n        self.secret_key = botocore.compat.ensure_unicode(self.secret_key)\n\n    def get_frozen_credentials(self):\n        return ReadOnlyCredentials(\n            self.access_key, self.secret_key, self.token\n        )\n\n\nclass RefreshableCredentials(Credentials):\n    \"\"\"\n    Holds the credentials needed to authenticate requests. In addition, it\n    knows how to refresh itself.\n\n    :param str access_key: The access key part of the credentials.\n    :param str secret_key: The secret key part of the credentials.\n    :param str token: The security token, valid only for session credentials.\n    :param datetime expiry_time: The expiration time of the credentials.\n    :param function refresh_using: Callback function to refresh the credentials.\n    :param str method: A string which identifies where the credentials\n        were found.\n    :param function time_fetcher: Callback function to retrieve current time.\n    \"\"\"\n\n    # The time at which we'll attempt to refresh, but not\n    # block if someone else is refreshing.\n    _advisory_refresh_timeout = _DEFAULT_ADVISORY_REFRESH_TIMEOUT\n    # The time at which all threads will block waiting for\n    # refreshed credentials.\n    _mandatory_refresh_timeout = _DEFAULT_MANDATORY_REFRESH_TIMEOUT\n\n    def __init__(\n        self,\n        access_key,\n        secret_key,\n        token,\n        expiry_time,\n        refresh_using,\n        method,\n        time_fetcher=_local_now,\n        advisory_timeout=None,\n        mandatory_timeout=None,\n    ):\n        self._refresh_using = refresh_using\n        self._access_key = access_key\n        self._secret_key = secret_key\n        self._token = token\n        self._expiry_time = expiry_time\n        self._time_fetcher = time_fetcher\n        self._refresh_lock = threading.Lock()\n        self.method = method\n        self._frozen_credentials = ReadOnlyCredentials(\n            access_key, secret_key, token\n        )\n        self._normalize()\n        if advisory_timeout is not None:\n            self._advisory_refresh_timeout = advisory_timeout\n        if mandatory_timeout is not None:\n            self._mandatory_refresh_timeout = mandatory_timeout\n\n    def _normalize(self):\n        self._access_key = botocore.compat.ensure_unicode(self._access_key)\n        self._secret_key = botocore.compat.ensure_unicode(self._secret_key)\n\n    @classmethod\n    def create_from_metadata(\n        cls,\n        metadata,\n        refresh_using,\n        method,\n        advisory_timeout=None,\n        mandatory_timeout=None,\n    ):\n        kwargs = {}\n        if advisory_timeout is not None:\n            kwargs['advisory_timeout'] = advisory_timeout\n        if mandatory_timeout is not None:\n            kwargs['mandatory_timeout'] = mandatory_timeout\n\n        instance = cls(\n            access_key=metadata['access_key'],\n            secret_key=metadata['secret_key'],\n            token=metadata['token'],\n            expiry_time=cls._expiry_datetime(metadata['expiry_time']),\n            method=method,\n            refresh_using=refresh_using,\n            **kwargs,\n        )\n        return instance\n\n    @property\n    def access_key(self):\n        \"\"\"Warning: Using this property can lead to race conditions if you\n        access another property subsequently along the refresh boundary.\n        Please use get_frozen_credentials instead.\n        \"\"\"\n        self._refresh()\n        return self._access_key\n\n    @access_key.setter\n    def access_key(self, value):\n        self._access_key = value\n\n    @property\n    def secret_key(self):\n        \"\"\"Warning: Using this property can lead to race conditions if you\n        access another property subsequently along the refresh boundary.\n        Please use get_frozen_credentials instead.\n        \"\"\"\n        self._refresh()\n        return self._secret_key\n\n    @secret_key.setter\n    def secret_key(self, value):\n        self._secret_key = value\n\n    @property\n    def token(self):\n        \"\"\"Warning: Using this property can lead to race conditions if you\n        access another property subsequently along the refresh boundary.\n        Please use get_frozen_credentials instead.\n        \"\"\"\n        self._refresh()\n        return self._token\n\n    @token.setter\n    def token(self, value):\n        self._token = value\n\n    def _seconds_remaining(self):\n        delta = self._expiry_time - self._time_fetcher()\n        return total_seconds(delta)\n\n    def refresh_needed(self, refresh_in=None):\n        \"\"\"Check if a refresh is needed.\n\n        A refresh is needed if the expiry time associated\n        with the temporary credentials is less than the\n        provided ``refresh_in``.  If ``time_delta`` is not\n        provided, ``self.advisory_refresh_needed`` will be used.\n\n        For example, if your temporary credentials expire\n        in 10 minutes and the provided ``refresh_in`` is\n        ``15 * 60``, then this function will return ``True``.\n\n        :type refresh_in: int\n        :param refresh_in: The number of seconds before the\n            credentials expire in which refresh attempts should\n            be made.\n\n        :return: True if refresh needed, False otherwise.\n\n        \"\"\"\n        if self._expiry_time is None:\n            # No expiration, so assume we don't need to refresh.\n            return False\n\n        if refresh_in is None:\n            refresh_in = self._advisory_refresh_timeout\n        # The credentials should be refreshed if they're going to expire\n        # in less than 5 minutes.\n        if self._seconds_remaining() >= refresh_in:\n            # There's enough time left. Don't refresh.\n            return False\n        logger.debug(\"Credentials need to be refreshed.\")\n        return True\n\n    def _is_expired(self):\n        # Checks if the current credentials are expired.\n        return self.refresh_needed(refresh_in=0)\n\n    def _refresh(self):\n        # In the common case where we don't need a refresh, we\n        # can immediately exit and not require acquiring the\n        # refresh lock.\n        if not self.refresh_needed(self._advisory_refresh_timeout):\n            return\n\n        # acquire() doesn't accept kwargs, but False is indicating\n        # that we should not block if we can't acquire the lock.\n        # If we aren't able to acquire the lock, we'll trigger\n        # the else clause.\n        if self._refresh_lock.acquire(False):\n            try:\n                if not self.refresh_needed(self._advisory_refresh_timeout):\n                    return\n                is_mandatory_refresh = self.refresh_needed(\n                    self._mandatory_refresh_timeout\n                )\n                self._protected_refresh(is_mandatory=is_mandatory_refresh)\n                return\n            finally:\n                self._refresh_lock.release()\n        elif self.refresh_needed(self._mandatory_refresh_timeout):\n            # If we're within the mandatory refresh window,\n            # we must block until we get refreshed credentials.\n            with self._refresh_lock:\n                if not self.refresh_needed(self._mandatory_refresh_timeout):\n                    return\n                self._protected_refresh(is_mandatory=True)\n\n    def _protected_refresh(self, is_mandatory):\n        # precondition: this method should only be called if you've acquired\n        # the self._refresh_lock.\n        try:\n            metadata = self._refresh_using()\n        except Exception:\n            period_name = 'mandatory' if is_mandatory else 'advisory'\n            logger.warning(\n                \"Refreshing temporary credentials failed \"\n                \"during %s refresh period.\",\n                period_name,\n                exc_info=True,\n            )\n            if is_mandatory:\n                # If this is a mandatory refresh, then\n                # all errors that occur when we attempt to refresh\n                # credentials are propagated back to the user.\n                raise\n            # Otherwise we'll just return.\n            # The end result will be that we'll use the current\n            # set of temporary credentials we have.\n            return\n        self._set_from_data(metadata)\n        self._frozen_credentials = ReadOnlyCredentials(\n            self._access_key, self._secret_key, self._token\n        )\n        if self._is_expired():\n            # We successfully refreshed credentials but for whatever\n            # reason, our refreshing function returned credentials\n            # that are still expired.  In this scenario, the only\n            # thing we can do is let the user know and raise\n            # an exception.\n            msg = (\n                \"Credentials were refreshed, but the \"\n                \"refreshed credentials are still expired.\"\n            )\n            logger.warning(msg)\n            raise RuntimeError(msg)\n\n    @staticmethod\n    def _expiry_datetime(time_str):\n        return parse(time_str)\n\n    def _set_from_data(self, data):\n        expected_keys = ['access_key', 'secret_key', 'token', 'expiry_time']\n        if not data:\n            missing_keys = expected_keys\n        else:\n            missing_keys = [k for k in expected_keys if k not in data]\n\n        if missing_keys:\n            message = \"Credential refresh failed, response did not contain: %s\"\n            raise CredentialRetrievalError(\n                provider=self.method,\n                error_msg=message % ', '.join(missing_keys),\n            )\n\n        self.access_key = data['access_key']\n        self.secret_key = data['secret_key']\n        self.token = data['token']\n        self._expiry_time = parse(data['expiry_time'])\n        logger.debug(\n            \"Retrieved credentials will expire at: %s\", self._expiry_time\n        )\n        self._normalize()\n\n    def get_frozen_credentials(self):\n        \"\"\"Return immutable credentials.\n\n        The ``access_key``, ``secret_key``, and ``token`` properties\n        on this class will always check and refresh credentials if\n        needed before returning the particular credentials.\n\n        This has an edge case where you can get inconsistent\n        credentials.  Imagine this:\n\n            # Current creds are \"t1\"\n            tmp.access_key  ---> expired? no, so return t1.access_key\n            # ---- time is now expired, creds need refreshing to \"t2\" ----\n            tmp.secret_key  ---> expired? yes, refresh and return t2.secret_key\n\n        This means we're using the access key from t1 with the secret key\n        from t2.  To fix this issue, you can request a frozen credential object\n        which is guaranteed not to change.\n\n        The frozen credentials returned from this method should be used\n        immediately and then discarded.  The typical usage pattern would\n        be::\n\n            creds = RefreshableCredentials(...)\n            some_code = SomeSignerObject()\n            # I'm about to sign the request.\n            # The frozen credentials are only used for the\n            # duration of generate_presigned_url and will be\n            # immediately thrown away.\n            request = some_code.sign_some_request(\n                with_credentials=creds.get_frozen_credentials())\n            print(\"Signed request:\", request)\n\n        \"\"\"\n        self._refresh()\n        return self._frozen_credentials\n\n\nclass DeferredRefreshableCredentials(RefreshableCredentials):\n    \"\"\"Refreshable credentials that don't require initial credentials.\n\n    refresh_using will be called upon first access.\n    \"\"\"\n\n    def __init__(self, refresh_using, method, time_fetcher=_local_now):\n        self._refresh_using = refresh_using\n        self._access_key = None\n        self._secret_key = None\n        self._token = None\n        self._expiry_time = None\n        self._time_fetcher = time_fetcher\n        self._refresh_lock = threading.Lock()\n        self.method = method\n        self._frozen_credentials = None\n\n    def refresh_needed(self, refresh_in=None):\n        if self._frozen_credentials is None:\n            return True\n        return super().refresh_needed(refresh_in)\n\n\nclass CachedCredentialFetcher:\n    DEFAULT_EXPIRY_WINDOW_SECONDS = 60 * 15\n\n    def __init__(self, cache=None, expiry_window_seconds=None):\n        if cache is None:\n            cache = {}\n        self._cache = cache\n        self._cache_key = self._create_cache_key()\n        if expiry_window_seconds is None:\n            expiry_window_seconds = self.DEFAULT_EXPIRY_WINDOW_SECONDS\n        self._expiry_window_seconds = expiry_window_seconds\n\n    def _create_cache_key(self):\n        raise NotImplementedError('_create_cache_key()')\n\n    def _make_file_safe(self, filename):\n        # Replace :, path sep, and / to make it the string filename safe.\n        filename = filename.replace(':', '_').replace(os.sep, '_')\n        return filename.replace('/', '_')\n\n    def _get_credentials(self):\n        raise NotImplementedError('_get_credentials()')\n\n    def fetch_credentials(self):\n        return self._get_cached_credentials()\n\n    def _get_cached_credentials(self):\n        \"\"\"Get up-to-date credentials.\n\n        This will check the cache for up-to-date credentials, calling assume\n        role if none are available.\n        \"\"\"\n        response = self._load_from_cache()\n        if response is None:\n            response = self._get_credentials()\n            self._write_to_cache(response)\n        else:\n            logger.debug(\"Credentials for role retrieved from cache.\")\n\n        creds = response['Credentials']\n        expiration = _serialize_if_needed(creds['Expiration'], iso=True)\n        return {\n            'access_key': creds['AccessKeyId'],\n            'secret_key': creds['SecretAccessKey'],\n            'token': creds['SessionToken'],\n            'expiry_time': expiration,\n        }\n\n    def _load_from_cache(self):\n        if self._cache_key in self._cache:\n            creds = deepcopy(self._cache[self._cache_key])\n            if not self._is_expired(creds):\n                return creds\n            else:\n                logger.debug(\n                    \"Credentials were found in cache, but they are expired.\"\n                )\n        return None\n\n    def _write_to_cache(self, response):\n        self._cache[self._cache_key] = deepcopy(response)\n\n    def _is_expired(self, credentials):\n        \"\"\"Check if credentials are expired.\"\"\"\n        end_time = _parse_if_needed(credentials['Credentials']['Expiration'])\n        seconds = total_seconds(end_time - _local_now())\n        return seconds < self._expiry_window_seconds\n\n\nclass BaseAssumeRoleCredentialFetcher(CachedCredentialFetcher):\n    def __init__(\n        self,\n        client_creator,\n        role_arn,\n        extra_args=None,\n        cache=None,\n        expiry_window_seconds=None,\n    ):\n        self._client_creator = client_creator\n        self._role_arn = role_arn\n\n        if extra_args is None:\n            self._assume_kwargs = {}\n        else:\n            self._assume_kwargs = deepcopy(extra_args)\n        self._assume_kwargs['RoleArn'] = self._role_arn\n\n        self._role_session_name = self._assume_kwargs.get('RoleSessionName')\n        self._using_default_session_name = False\n        if not self._role_session_name:\n            self._generate_assume_role_name()\n\n        super().__init__(cache, expiry_window_seconds)\n\n    def _generate_assume_role_name(self):\n        self._role_session_name = 'botocore-session-%s' % (int(time.time()))\n        self._assume_kwargs['RoleSessionName'] = self._role_session_name\n        self._using_default_session_name = True\n\n    def _create_cache_key(self):\n        \"\"\"Create a predictable cache key for the current configuration.\n\n        The cache key is intended to be compatible with file names.\n        \"\"\"\n        args = deepcopy(self._assume_kwargs)\n\n        # The role session name gets randomly generated, so we don't want it\n        # in the hash.\n        if self._using_default_session_name:\n            del args['RoleSessionName']\n\n        if 'Policy' in args:\n            # To have a predictable hash, the keys of the policy must be\n            # sorted, so we have to load it here to make sure it gets sorted\n            # later on.\n            args['Policy'] = json.loads(args['Policy'])\n\n        args = json.dumps(args, sort_keys=True)\n        argument_hash = sha1(args.encode('utf-8')).hexdigest()\n        return self._make_file_safe(argument_hash)\n\n\nclass AssumeRoleCredentialFetcher(BaseAssumeRoleCredentialFetcher):\n    def __init__(\n        self,\n        client_creator,\n        source_credentials,\n        role_arn,\n        extra_args=None,\n        mfa_prompter=None,\n        cache=None,\n        expiry_window_seconds=None,\n    ):\n        \"\"\"\n        :type client_creator: callable\n        :param client_creator: A callable that creates a client taking\n            arguments like ``Session.create_client``.\n\n        :type source_credentials: Credentials\n        :param source_credentials: The credentials to use to create the\n            client for the call to AssumeRole.\n\n        :type role_arn: str\n        :param role_arn: The ARN of the role to be assumed.\n\n        :type extra_args: dict\n        :param extra_args: Any additional arguments to add to the assume\n            role request using the format of the botocore operation.\n            Possible keys include, but may not be limited to,\n            DurationSeconds, Policy, SerialNumber, ExternalId and\n            RoleSessionName.\n\n        :type mfa_prompter: callable\n        :param mfa_prompter: A callable that returns input provided by the\n            user (i.e raw_input, getpass.getpass, etc.).\n\n        :type cache: dict\n        :param cache: An object that supports ``__getitem__``,\n            ``__setitem__``, and ``__contains__``.  An example of this is\n            the ``JSONFileCache`` class in aws-cli.\n\n        :type expiry_window_seconds: int\n        :param expiry_window_seconds: The amount of time, in seconds,\n        \"\"\"\n        self._source_credentials = source_credentials\n        self._mfa_prompter = mfa_prompter\n        if self._mfa_prompter is None:\n            self._mfa_prompter = getpass.getpass\n\n        super().__init__(\n            client_creator,\n            role_arn,\n            extra_args=extra_args,\n            cache=cache,\n            expiry_window_seconds=expiry_window_seconds,\n        )\n\n    def _get_credentials(self):\n        \"\"\"Get credentials by calling assume role.\"\"\"\n        kwargs = self._assume_role_kwargs()\n        client = self._create_client()\n        return client.assume_role(**kwargs)\n\n    def _assume_role_kwargs(self):\n        \"\"\"Get the arguments for assume role based on current configuration.\"\"\"\n        assume_role_kwargs = deepcopy(self._assume_kwargs)\n\n        mfa_serial = assume_role_kwargs.get('SerialNumber')\n\n        if mfa_serial is not None:\n            prompt = 'Enter MFA code for %s: ' % mfa_serial\n            token_code = self._mfa_prompter(prompt)\n            assume_role_kwargs['TokenCode'] = token_code\n\n        duration_seconds = assume_role_kwargs.get('DurationSeconds')\n\n        if duration_seconds is not None:\n            assume_role_kwargs['DurationSeconds'] = duration_seconds\n\n        return assume_role_kwargs\n\n    def _create_client(self):\n        \"\"\"Create an STS client using the source credentials.\"\"\"\n        frozen_credentials = self._source_credentials.get_frozen_credentials()\n        return self._client_creator(\n            'sts',\n            aws_access_key_id=frozen_credentials.access_key,\n            aws_secret_access_key=frozen_credentials.secret_key,\n            aws_session_token=frozen_credentials.token,\n        )\n\n\nclass AssumeRoleWithWebIdentityCredentialFetcher(\n    BaseAssumeRoleCredentialFetcher\n):\n    def __init__(\n        self,\n        client_creator,\n        web_identity_token_loader,\n        role_arn,\n        extra_args=None,\n        cache=None,\n        expiry_window_seconds=None,\n    ):\n        \"\"\"\n        :type client_creator: callable\n        :param client_creator: A callable that creates a client taking\n            arguments like ``Session.create_client``.\n\n        :type web_identity_token_loader: callable\n        :param web_identity_token_loader: A callable that takes no arguments\n        and returns a web identity token str.\n\n        :type role_arn: str\n        :param role_arn: The ARN of the role to be assumed.\n\n        :type extra_args: dict\n        :param extra_args: Any additional arguments to add to the assume\n            role request using the format of the botocore operation.\n            Possible keys include, but may not be limited to,\n            DurationSeconds, Policy, SerialNumber, ExternalId and\n            RoleSessionName.\n\n        :type cache: dict\n        :param cache: An object that supports ``__getitem__``,\n            ``__setitem__``, and ``__contains__``.  An example of this is\n            the ``JSONFileCache`` class in aws-cli.\n\n        :type expiry_window_seconds: int\n        :param expiry_window_seconds: The amount of time, in seconds,\n        \"\"\"\n        self._web_identity_token_loader = web_identity_token_loader\n\n        super().__init__(\n            client_creator,\n            role_arn,\n            extra_args=extra_args,\n            cache=cache,\n            expiry_window_seconds=expiry_window_seconds,\n        )\n\n    def _get_credentials(self):\n        \"\"\"Get credentials by calling assume role.\"\"\"\n        kwargs = self._assume_role_kwargs()\n        # Assume role with web identity does not require credentials other than\n        # the token, explicitly configure the client to not sign requests.\n        config = Config(signature_version=UNSIGNED)\n        client = self._client_creator('sts', config=config)\n        return client.assume_role_with_web_identity(**kwargs)\n\n    def _assume_role_kwargs(self):\n        \"\"\"Get the arguments for assume role based on current configuration.\"\"\"\n        assume_role_kwargs = deepcopy(self._assume_kwargs)\n        identity_token = self._web_identity_token_loader()\n        assume_role_kwargs['WebIdentityToken'] = identity_token\n\n        return assume_role_kwargs\n\n\nclass CredentialProvider:\n    # A short name to identify the provider within botocore.\n    METHOD = None\n\n    # A name to identify the provider for use in cross-sdk features like\n    # assume role's `credential_source` configuration option. These names\n    # are to be treated in a case-insensitive way. NOTE: any providers not\n    # implemented in botocore MUST prefix their canonical names with\n    # 'custom' or we DO NOT guarantee that it will work with any features\n    # that this provides.\n    CANONICAL_NAME = None\n\n    def __init__(self, session=None):\n        self.session = session\n\n    def load(self):\n        \"\"\"\n        Loads the credentials from their source & sets them on the object.\n\n        Subclasses should implement this method (by reading from disk, the\n        environment, the network or wherever), returning ``True`` if they were\n        found & loaded.\n\n        If not found, this method should return ``False``, indictating that the\n        ``CredentialResolver`` should fall back to the next available method.\n\n        The default implementation does nothing, assuming the user has set the\n        ``access_key/secret_key/token`` themselves.\n\n        :returns: Whether credentials were found & set\n        :rtype: Credentials\n        \"\"\"\n        return True\n\n    def _extract_creds_from_mapping(self, mapping, *key_names):\n        found = []\n        for key_name in key_names:\n            try:\n                found.append(mapping[key_name])\n            except KeyError:\n                raise PartialCredentialsError(\n                    provider=self.METHOD, cred_var=key_name\n                )\n        return found\n\n\nclass ProcessProvider(CredentialProvider):\n    METHOD = 'custom-process'\n\n    def __init__(self, profile_name, load_config, popen=subprocess.Popen):\n        self._profile_name = profile_name\n        self._load_config = load_config\n        self._loaded_config = None\n        self._popen = popen\n\n    def load(self):\n        credential_process = self._credential_process\n        if credential_process is None:\n            return\n\n        creds_dict = self._retrieve_credentials_using(credential_process)\n        if creds_dict.get('expiry_time') is not None:\n            return RefreshableCredentials.create_from_metadata(\n                creds_dict,\n                lambda: self._retrieve_credentials_using(credential_process),\n                self.METHOD,\n            )\n\n        return Credentials(\n            access_key=creds_dict['access_key'],\n            secret_key=creds_dict['secret_key'],\n            token=creds_dict.get('token'),\n            method=self.METHOD,\n        )\n\n    def _retrieve_credentials_using(self, credential_process):\n        # We're not using shell=True, so we need to pass the\n        # command and all arguments as a list.\n        process_list = compat_shell_split(credential_process)\n        p = self._popen(\n            process_list, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n        )\n        stdout, stderr = p.communicate()\n        if p.returncode != 0:\n            raise CredentialRetrievalError(\n                provider=self.METHOD, error_msg=stderr.decode('utf-8')\n            )\n        parsed = botocore.compat.json.loads(stdout.decode('utf-8'))\n        version = parsed.get('Version', '<Version key not provided>')\n        if version != 1:\n            raise CredentialRetrievalError(\n                provider=self.METHOD,\n                error_msg=(\n                    f\"Unsupported version '{version}' for credential process \"\n                    f\"provider, supported versions: 1\"\n                ),\n            )\n        try:\n            return {\n                'access_key': parsed['AccessKeyId'],\n                'secret_key': parsed['SecretAccessKey'],\n                'token': parsed.get('SessionToken'),\n                'expiry_time': parsed.get('Expiration'),\n            }\n        except KeyError as e:\n            raise CredentialRetrievalError(\n                provider=self.METHOD,\n                error_msg=f\"Missing required key in response: {e}\",\n            )\n\n    @property\n    def _credential_process(self):\n        if self._loaded_config is None:\n            self._loaded_config = self._load_config()\n        profile_config = self._loaded_config.get('profiles', {}).get(\n            self._profile_name, {}\n        )\n        return profile_config.get('credential_process')\n\n\nclass InstanceMetadataProvider(CredentialProvider):\n    METHOD = 'iam-role'\n    CANONICAL_NAME = 'Ec2InstanceMetadata'\n\n    def __init__(self, iam_role_fetcher):\n        self._role_fetcher = iam_role_fetcher\n\n    def load(self):\n        fetcher = self._role_fetcher\n        # We do the first request, to see if we get useful data back.\n        # If not, we'll pass & move on to whatever's next in the credential\n        # chain.\n        metadata = fetcher.retrieve_iam_role_credentials()\n        if not metadata:\n            return None\n        logger.info(\n            'Found credentials from IAM Role: %s', metadata['role_name']\n        )\n        # We manually set the data here, since we already made the request &\n        # have it. When the expiry is hit, the credentials will auto-refresh\n        # themselves.\n        creds = RefreshableCredentials.create_from_metadata(\n            metadata,\n            method=self.METHOD,\n            refresh_using=fetcher.retrieve_iam_role_credentials,\n        )\n        return creds\n\n\nclass EnvProvider(CredentialProvider):\n    METHOD = 'env'\n    CANONICAL_NAME = 'Environment'\n    ACCESS_KEY = 'AWS_ACCESS_KEY_ID'\n    SECRET_KEY = 'AWS_SECRET_ACCESS_KEY'\n    # The token can come from either of these env var.\n    # AWS_SESSION_TOKEN is what other AWS SDKs have standardized on.\n    TOKENS = ['AWS_SECURITY_TOKEN', 'AWS_SESSION_TOKEN']\n    EXPIRY_TIME = 'AWS_CREDENTIAL_EXPIRATION'\n\n    def __init__(self, environ=None, mapping=None):\n        \"\"\"\n\n        :param environ: The environment variables (defaults to\n            ``os.environ`` if no value is provided).\n        :param mapping: An optional mapping of variable names to\n            environment variable names.  Use this if you want to\n            change the mapping of access_key->AWS_ACCESS_KEY_ID, etc.\n            The dict can have up to 3 keys: ``access_key``, ``secret_key``,\n            ``session_token``.\n        \"\"\"\n        if environ is None:\n            environ = os.environ\n        self.environ = environ\n        self._mapping = self._build_mapping(mapping)\n\n    def _build_mapping(self, mapping):\n        # Mapping of variable name to env var name.\n        var_mapping = {}\n        if mapping is None:\n            # Use the class var default.\n            var_mapping['access_key'] = self.ACCESS_KEY\n            var_mapping['secret_key'] = self.SECRET_KEY\n            var_mapping['token'] = self.TOKENS\n            var_mapping['expiry_time'] = self.EXPIRY_TIME\n        else:\n            var_mapping['access_key'] = mapping.get(\n                'access_key', self.ACCESS_KEY\n            )\n            var_mapping['secret_key'] = mapping.get(\n                'secret_key', self.SECRET_KEY\n            )\n            var_mapping['token'] = mapping.get('token', self.TOKENS)\n            if not isinstance(var_mapping['token'], list):\n                var_mapping['token'] = [var_mapping['token']]\n            var_mapping['expiry_time'] = mapping.get(\n                'expiry_time', self.EXPIRY_TIME\n            )\n        return var_mapping\n\n    def load(self):\n        \"\"\"\n        Search for credentials in explicit environment variables.\n        \"\"\"\n\n        access_key = self.environ.get(self._mapping['access_key'], '')\n\n        if access_key:\n            logger.info('Found credentials in environment variables.')\n            fetcher = self._create_credentials_fetcher()\n            credentials = fetcher(require_expiry=False)\n\n            expiry_time = credentials['expiry_time']\n            if expiry_time is not None:\n                expiry_time = parse(expiry_time)\n                return RefreshableCredentials(\n                    credentials['access_key'],\n                    credentials['secret_key'],\n                    credentials['token'],\n                    expiry_time,\n                    refresh_using=fetcher,\n                    method=self.METHOD,\n                )\n\n            return Credentials(\n                credentials['access_key'],\n                credentials['secret_key'],\n                credentials['token'],\n                method=self.METHOD,\n            )\n        else:\n            return None\n\n    def _create_credentials_fetcher(self):\n        mapping = self._mapping\n        method = self.METHOD\n        environ = self.environ\n\n        def fetch_credentials(require_expiry=True):\n            credentials = {}\n\n            access_key = environ.get(mapping['access_key'], '')\n            if not access_key:\n                raise PartialCredentialsError(\n                    provider=method, cred_var=mapping['access_key']\n                )\n            credentials['access_key'] = access_key\n\n            secret_key = environ.get(mapping['secret_key'], '')\n            if not secret_key:\n                raise PartialCredentialsError(\n                    provider=method, cred_var=mapping['secret_key']\n                )\n            credentials['secret_key'] = secret_key\n\n            credentials['token'] = None\n            for token_env_var in mapping['token']:\n                token = environ.get(token_env_var, '')\n                if token:\n                    credentials['token'] = token\n                    break\n\n            credentials['expiry_time'] = None\n            expiry_time = environ.get(mapping['expiry_time'], '')\n            if expiry_time:\n                credentials['expiry_time'] = expiry_time\n            if require_expiry and not expiry_time:\n                raise PartialCredentialsError(\n                    provider=method, cred_var=mapping['expiry_time']\n                )\n\n            return credentials\n\n        return fetch_credentials\n\n\nclass OriginalEC2Provider(CredentialProvider):\n    METHOD = 'ec2-credentials-file'\n    CANONICAL_NAME = 'Ec2Config'\n\n    CRED_FILE_ENV = 'AWS_CREDENTIAL_FILE'\n    ACCESS_KEY = 'AWSAccessKeyId'\n    SECRET_KEY = 'AWSSecretKey'\n\n    def __init__(self, environ=None, parser=None):\n        if environ is None:\n            environ = os.environ\n        if parser is None:\n            parser = parse_key_val_file\n        self._environ = environ\n        self._parser = parser\n\n    def load(self):\n        \"\"\"\n        Search for a credential file used by original EC2 CLI tools.\n        \"\"\"\n        if 'AWS_CREDENTIAL_FILE' in self._environ:\n            full_path = os.path.expanduser(\n                self._environ['AWS_CREDENTIAL_FILE']\n            )\n            creds = self._parser(full_path)\n            if self.ACCESS_KEY in creds:\n                logger.info('Found credentials in AWS_CREDENTIAL_FILE.')\n                access_key = creds[self.ACCESS_KEY]\n                secret_key = creds[self.SECRET_KEY]\n                # EC2 creds file doesn't support session tokens.\n                return Credentials(access_key, secret_key, method=self.METHOD)\n        else:\n            return None\n\n\nclass SharedCredentialProvider(CredentialProvider):\n    METHOD = 'shared-credentials-file'\n    CANONICAL_NAME = 'SharedCredentials'\n\n    ACCESS_KEY = 'aws_access_key_id'\n    SECRET_KEY = 'aws_secret_access_key'\n    # Same deal as the EnvProvider above.  Botocore originally supported\n    # aws_security_token, but the SDKs are standardizing on aws_session_token\n    # so we support both.\n    TOKENS = ['aws_security_token', 'aws_session_token']\n\n    def __init__(self, creds_filename, profile_name=None, ini_parser=None):\n        self._creds_filename = creds_filename\n        if profile_name is None:\n            profile_name = 'default'\n        self._profile_name = profile_name\n        if ini_parser is None:\n            ini_parser = botocore.configloader.raw_config_parse\n        self._ini_parser = ini_parser\n\n    def load(self):\n        try:\n            available_creds = self._ini_parser(self._creds_filename)\n        except ConfigNotFound:\n            return None\n        if self._profile_name in available_creds:\n            config = available_creds[self._profile_name]\n            if self.ACCESS_KEY in config:\n                logger.info(\n                    \"Found credentials in shared credentials file: %s\",\n                    self._creds_filename,\n                )\n                access_key, secret_key = self._extract_creds_from_mapping(\n                    config, self.ACCESS_KEY, self.SECRET_KEY\n                )\n                token = self._get_session_token(config)\n                return Credentials(\n                    access_key, secret_key, token, method=self.METHOD\n                )\n\n    def _get_session_token(self, config):\n        for token_envvar in self.TOKENS:\n            if token_envvar in config:\n                return config[token_envvar]\n\n\nclass ConfigProvider(CredentialProvider):\n    \"\"\"INI based config provider with profile sections.\"\"\"\n\n    METHOD = 'config-file'\n    CANONICAL_NAME = 'SharedConfig'\n\n    ACCESS_KEY = 'aws_access_key_id'\n    SECRET_KEY = 'aws_secret_access_key'\n    # Same deal as the EnvProvider above.  Botocore originally supported\n    # aws_security_token, but the SDKs are standardizing on aws_session_token\n    # so we support both.\n    TOKENS = ['aws_security_token', 'aws_session_token']\n\n    def __init__(self, config_filename, profile_name, config_parser=None):\n        \"\"\"\n\n        :param config_filename: The session configuration scoped to the current\n            profile.  This is available via ``session.config``.\n        :param profile_name: The name of the current profile.\n        :param config_parser: A config parser callable.\n\n        \"\"\"\n        self._config_filename = config_filename\n        self._profile_name = profile_name\n        if config_parser is None:\n            config_parser = botocore.configloader.load_config\n        self._config_parser = config_parser\n\n    def load(self):\n        \"\"\"\n        If there is are credentials in the configuration associated with\n        the session, use those.\n        \"\"\"\n        try:\n            full_config = self._config_parser(self._config_filename)\n        except ConfigNotFound:\n            return None\n        if self._profile_name in full_config['profiles']:\n            profile_config = full_config['profiles'][self._profile_name]\n            if self.ACCESS_KEY in profile_config:\n                logger.info(\n                    \"Credentials found in config file: %s\",\n                    self._config_filename,\n                )\n                access_key, secret_key = self._extract_creds_from_mapping(\n                    profile_config, self.ACCESS_KEY, self.SECRET_KEY\n                )\n                token = self._get_session_token(profile_config)\n                return Credentials(\n                    access_key, secret_key, token, method=self.METHOD\n                )\n        else:\n            return None\n\n    def _get_session_token(self, profile_config):\n        for token_name in self.TOKENS:\n            if token_name in profile_config:\n                return profile_config[token_name]\n\n\nclass BotoProvider(CredentialProvider):\n    METHOD = 'boto-config'\n    CANONICAL_NAME = 'Boto2Config'\n\n    BOTO_CONFIG_ENV = 'BOTO_CONFIG'\n    DEFAULT_CONFIG_FILENAMES = ['/etc/boto.cfg', '~/.boto']\n    ACCESS_KEY = 'aws_access_key_id'\n    SECRET_KEY = 'aws_secret_access_key'\n\n    def __init__(self, environ=None, ini_parser=None):\n        if environ is None:\n            environ = os.environ\n        if ini_parser is None:\n            ini_parser = botocore.configloader.raw_config_parse\n        self._environ = environ\n        self._ini_parser = ini_parser\n\n    def load(self):\n        \"\"\"\n        Look for credentials in boto config file.\n        \"\"\"\n        if self.BOTO_CONFIG_ENV in self._environ:\n            potential_locations = [self._environ[self.BOTO_CONFIG_ENV]]\n        else:\n            potential_locations = self.DEFAULT_CONFIG_FILENAMES\n        for filename in potential_locations:\n            try:\n                config = self._ini_parser(filename)\n            except ConfigNotFound:\n                # Move on to the next potential config file name.\n                continue\n            if 'Credentials' in config:\n                credentials = config['Credentials']\n                if self.ACCESS_KEY in credentials:\n                    logger.info(\n                        \"Found credentials in boto config file: %s\", filename\n                    )\n                    access_key, secret_key = self._extract_creds_from_mapping(\n                        credentials, self.ACCESS_KEY, self.SECRET_KEY\n                    )\n                    return Credentials(\n                        access_key, secret_key, method=self.METHOD\n                    )\n\n\nclass AssumeRoleProvider(CredentialProvider):\n    METHOD = 'assume-role'\n    # The AssumeRole provider is logically part of the SharedConfig and\n    # SharedCredentials providers. Since the purpose of the canonical name\n    # is to provide cross-sdk compatibility, calling code will need to be\n    # aware that either of those providers should be tied to the AssumeRole\n    # provider as much as possible.\n    CANONICAL_NAME = None\n    ROLE_CONFIG_VAR = 'role_arn'\n    WEB_IDENTITY_TOKE_FILE_VAR = 'web_identity_token_file'\n    # Credentials are considered expired (and will be refreshed) once the total\n    # remaining time left until the credentials expires is less than the\n    # EXPIRY_WINDOW.\n    EXPIRY_WINDOW_SECONDS = 60 * 15\n\n    def __init__(\n        self,\n        load_config,\n        client_creator,\n        cache,\n        profile_name,\n        prompter=getpass.getpass,\n        credential_sourcer=None,\n        profile_provider_builder=None,\n    ):\n        \"\"\"\n        :type load_config: callable\n        :param load_config: A function that accepts no arguments, and\n            when called, will return the full configuration dictionary\n            for the session (``session.full_config``).\n\n        :type client_creator: callable\n        :param client_creator: A factory function that will create\n            a client when called.  Has the same interface as\n            ``botocore.session.Session.create_client``.\n\n        :type cache: dict\n        :param cache: An object that supports ``__getitem__``,\n            ``__setitem__``, and ``__contains__``.  An example\n            of this is the ``JSONFileCache`` class in the CLI.\n\n        :type profile_name: str\n        :param profile_name: The name of the profile.\n\n        :type prompter: callable\n        :param prompter: A callable that returns input provided\n            by the user (i.e raw_input, getpass.getpass, etc.).\n\n        :type credential_sourcer: CanonicalNameCredentialSourcer\n        :param credential_sourcer: A credential provider that takes a\n            configuration, which is used to provide the source credentials\n            for the STS call.\n        \"\"\"\n        #: The cache used to first check for assumed credentials.\n        #: This is checked before making the AssumeRole API\n        #: calls and can be useful if you have short lived\n        #: scripts and you'd like to avoid calling AssumeRole\n        #: until the credentials are expired.\n        self.cache = cache\n        self._load_config = load_config\n        # client_creator is a callable that creates function.\n        # It's basically session.create_client\n        self._client_creator = client_creator\n        self._profile_name = profile_name\n        self._prompter = prompter\n        # The _loaded_config attribute will be populated from the\n        # load_config() function once the configuration is actually\n        # loaded.  The reason we go through all this instead of just\n        # requiring that the loaded_config be passed to us is to that\n        # we can defer configuration loaded until we actually try\n        # to load credentials (as opposed to when the object is\n        # instantiated).\n        self._loaded_config = {}\n        self._credential_sourcer = credential_sourcer\n        self._profile_provider_builder = profile_provider_builder\n        self._visited_profiles = [self._profile_name]\n\n    def load(self):\n        self._loaded_config = self._load_config()\n        profiles = self._loaded_config.get('profiles', {})\n        profile = profiles.get(self._profile_name, {})\n        if self._has_assume_role_config_vars(profile):\n            return self._load_creds_via_assume_role(self._profile_name)\n\n    def _has_assume_role_config_vars(self, profile):\n        return (\n            self.ROLE_CONFIG_VAR in profile\n            and\n            # We need to ensure this provider doesn't look at a profile when\n            # the profile has configuration for web identity. Simply relying on\n            # the order in the credential chain is insufficient as it doesn't\n            # prevent the case when we're doing an assume role chain.\n            self.WEB_IDENTITY_TOKE_FILE_VAR not in profile\n        )\n\n    def _load_creds_via_assume_role(self, profile_name):\n        role_config = self._get_role_config(profile_name)\n        source_credentials = self._resolve_source_credentials(\n            role_config, profile_name\n        )\n\n        extra_args = {}\n        role_session_name = role_config.get('role_session_name')\n        if role_session_name is not None:\n            extra_args['RoleSessionName'] = role_session_name\n\n        external_id = role_config.get('external_id')\n        if external_id is not None:\n            extra_args['ExternalId'] = external_id\n\n        mfa_serial = role_config.get('mfa_serial')\n        if mfa_serial is not None:\n            extra_args['SerialNumber'] = mfa_serial\n\n        duration_seconds = role_config.get('duration_seconds')\n        if duration_seconds is not None:\n            extra_args['DurationSeconds'] = duration_seconds\n\n        fetcher = AssumeRoleCredentialFetcher(\n            client_creator=self._client_creator,\n            source_credentials=source_credentials,\n            role_arn=role_config['role_arn'],\n            extra_args=extra_args,\n            mfa_prompter=self._prompter,\n            cache=self.cache,\n        )\n        refresher = fetcher.fetch_credentials\n        if mfa_serial is not None:\n            refresher = create_mfa_serial_refresher(refresher)\n\n        # The initial credentials are empty and the expiration time is set\n        # to now so that we can delay the call to assume role until it is\n        # strictly needed.\n        return DeferredRefreshableCredentials(\n            method=self.METHOD,\n            refresh_using=refresher,\n            time_fetcher=_local_now,\n        )\n\n    def _get_role_config(self, profile_name):\n        \"\"\"Retrieves and validates the role configuration for the profile.\"\"\"\n        profiles = self._loaded_config.get('profiles', {})\n\n        profile = profiles[profile_name]\n        source_profile = profile.get('source_profile')\n        role_arn = profile['role_arn']\n        credential_source = profile.get('credential_source')\n        mfa_serial = profile.get('mfa_serial')\n        external_id = profile.get('external_id')\n        role_session_name = profile.get('role_session_name')\n        duration_seconds = profile.get('duration_seconds')\n\n        role_config = {\n            'role_arn': role_arn,\n            'external_id': external_id,\n            'mfa_serial': mfa_serial,\n            'role_session_name': role_session_name,\n            'source_profile': source_profile,\n            'credential_source': credential_source,\n        }\n\n        if duration_seconds is not None:\n            try:\n                role_config['duration_seconds'] = int(duration_seconds)\n            except ValueError:\n                pass\n\n        # Either the credential source or the source profile must be\n        # specified, but not both.\n        if credential_source is not None and source_profile is not None:\n            raise InvalidConfigError(\n                error_msg=(\n                    'The profile \"%s\" contains both source_profile and '\n                    'credential_source.' % profile_name\n                )\n            )\n        elif credential_source is None and source_profile is None:\n            raise PartialCredentialsError(\n                provider=self.METHOD,\n                cred_var='source_profile or credential_source',\n            )\n        elif credential_source is not None:\n            self._validate_credential_source(profile_name, credential_source)\n        else:\n            self._validate_source_profile(profile_name, source_profile)\n\n        return role_config\n\n    def _validate_credential_source(self, parent_profile, credential_source):\n        if self._credential_sourcer is None:\n            raise InvalidConfigError(\n                error_msg=(\n                    f\"The credential_source \\\"{credential_source}\\\" is specified \"\n                    f\"in profile \\\"{parent_profile}\\\", \"\n                    f\"but no source provider was configured.\"\n                )\n            )\n        if not self._credential_sourcer.is_supported(credential_source):\n            raise InvalidConfigError(\n                error_msg=(\n                    f\"The credential source \\\"{credential_source}\\\" referenced \"\n                    f\"in profile \\\"{parent_profile}\\\" is not valid.\"\n                )\n            )\n\n    def _source_profile_has_credentials(self, profile):\n        return any(\n            [\n                self._has_static_credentials(profile),\n                self._has_assume_role_config_vars(profile),\n            ]\n        )\n\n    def _validate_source_profile(\n        self, parent_profile_name, source_profile_name\n    ):\n        profiles = self._loaded_config.get('profiles', {})\n        if source_profile_name not in profiles:\n            raise InvalidConfigError(\n                error_msg=(\n                    f\"The source_profile \\\"{source_profile_name}\\\" referenced in \"\n                    f\"the profile \\\"{parent_profile_name}\\\" does not exist.\"\n                )\n            )\n\n        source_profile = profiles[source_profile_name]\n\n        # Make sure we aren't going into an infinite loop. If we haven't\n        # visited the profile yet, we're good.\n        if source_profile_name not in self._visited_profiles:\n            return\n\n        # If we have visited the profile and the profile isn't simply\n        # referencing itself, that's an infinite loop.\n        if source_profile_name != parent_profile_name:\n            raise InfiniteLoopConfigError(\n                source_profile=source_profile_name,\n                visited_profiles=self._visited_profiles,\n            )\n\n        # A profile is allowed to reference itself so that it can source\n        # static credentials and have configuration all in the same\n        # profile. This will only ever work for the top level assume\n        # role because the static credentials will otherwise take\n        # precedence.\n        if not self._has_static_credentials(source_profile):\n            raise InfiniteLoopConfigError(\n                source_profile=source_profile_name,\n                visited_profiles=self._visited_profiles,\n            )\n\n    def _has_static_credentials(self, profile):\n        static_keys = ['aws_secret_access_key', 'aws_access_key_id']\n        return any(static_key in profile for static_key in static_keys)\n\n    def _resolve_source_credentials(self, role_config, profile_name):\n        credential_source = role_config.get('credential_source')\n        if credential_source is not None:\n            return self._resolve_credentials_from_source(\n                credential_source, profile_name\n            )\n\n        source_profile = role_config['source_profile']\n        self._visited_profiles.append(source_profile)\n        return self._resolve_credentials_from_profile(source_profile)\n\n    def _resolve_credentials_from_profile(self, profile_name):\n        profiles = self._loaded_config.get('profiles', {})\n        profile = profiles[profile_name]\n\n        if (\n            self._has_static_credentials(profile)\n            and not self._profile_provider_builder\n        ):\n            # This is only here for backwards compatibility. If this provider\n            # isn't given a profile provider builder we still want to be able\n            # handle the basic static credential case as we would before the\n            # provile provider builder parameter was added.\n            return self._resolve_static_credentials_from_profile(profile)\n        elif self._has_static_credentials(\n            profile\n        ) or not self._has_assume_role_config_vars(profile):\n            profile_providers = self._profile_provider_builder.providers(\n                profile_name=profile_name,\n                disable_env_vars=True,\n            )\n            profile_chain = CredentialResolver(profile_providers)\n            credentials = profile_chain.load_credentials()\n            if credentials is None:\n                error_message = (\n                    'The source profile \"%s\" must have credentials.'\n                )\n                raise InvalidConfigError(\n                    error_msg=error_message % profile_name,\n                )\n            return credentials\n\n        return self._load_creds_via_assume_role(profile_name)\n\n    def _resolve_static_credentials_from_profile(self, profile):\n        try:\n            return Credentials(\n                access_key=profile['aws_access_key_id'],\n                secret_key=profile['aws_secret_access_key'],\n                token=profile.get('aws_session_token'),\n            )\n        except KeyError as e:\n            raise PartialCredentialsError(\n                provider=self.METHOD, cred_var=str(e)\n            )\n\n    def _resolve_credentials_from_source(\n        self, credential_source, profile_name\n    ):\n        credentials = self._credential_sourcer.source_credentials(\n            credential_source\n        )\n        if credentials is None:\n            raise CredentialRetrievalError(\n                provider=credential_source,\n                error_msg=(\n                    'No credentials found in credential_source referenced '\n                    'in profile %s' % profile_name\n                ),\n            )\n        return credentials\n\n\nclass AssumeRoleWithWebIdentityProvider(CredentialProvider):\n    METHOD = 'assume-role-with-web-identity'\n    CANONICAL_NAME = None\n    _CONFIG_TO_ENV_VAR = {\n        'web_identity_token_file': 'AWS_WEB_IDENTITY_TOKEN_FILE',\n        'role_session_name': 'AWS_ROLE_SESSION_NAME',\n        'role_arn': 'AWS_ROLE_ARN',\n    }\n\n    def __init__(\n        self,\n        load_config,\n        client_creator,\n        profile_name,\n        cache=None,\n        disable_env_vars=False,\n        token_loader_cls=None,\n    ):\n        self.cache = cache\n        self._load_config = load_config\n        self._client_creator = client_creator\n        self._profile_name = profile_name\n        self._profile_config = None\n        self._disable_env_vars = disable_env_vars\n        if token_loader_cls is None:\n            token_loader_cls = FileWebIdentityTokenLoader\n        self._token_loader_cls = token_loader_cls\n\n    def load(self):\n        return self._assume_role_with_web_identity()\n\n    def _get_profile_config(self, key):\n        if self._profile_config is None:\n            loaded_config = self._load_config()\n            profiles = loaded_config.get('profiles', {})\n            self._profile_config = profiles.get(self._profile_name, {})\n        return self._profile_config.get(key)\n\n    def _get_env_config(self, key):\n        if self._disable_env_vars:\n            return None\n        env_key = self._CONFIG_TO_ENV_VAR.get(key)\n        if env_key and env_key in os.environ:\n            return os.environ[env_key]\n        return None\n\n    def _get_config(self, key):\n        env_value = self._get_env_config(key)\n        if env_value is not None:\n            return env_value\n        return self._get_profile_config(key)\n\n    def _assume_role_with_web_identity(self):\n        token_path = self._get_config('web_identity_token_file')\n        if not token_path:\n            return None\n        token_loader = self._token_loader_cls(token_path)\n\n        role_arn = self._get_config('role_arn')\n        if not role_arn:\n            error_msg = (\n                'The provided profile or the current environment is '\n                'configured to assume role with web identity but has no '\n                'role ARN configured. Ensure that the profile has the role_arn'\n                'configuration set or the AWS_ROLE_ARN env var is set.'\n            )\n            raise InvalidConfigError(error_msg=error_msg)\n\n        extra_args = {}\n        role_session_name = self._get_config('role_session_name')\n        if role_session_name is not None:\n            extra_args['RoleSessionName'] = role_session_name\n\n        fetcher = AssumeRoleWithWebIdentityCredentialFetcher(\n            client_creator=self._client_creator,\n            web_identity_token_loader=token_loader,\n            role_arn=role_arn,\n            extra_args=extra_args,\n            cache=self.cache,\n        )\n        # The initial credentials are empty and the expiration time is set\n        # to now so that we can delay the call to assume role until it is\n        # strictly needed.\n        return DeferredRefreshableCredentials(\n            method=self.METHOD,\n            refresh_using=fetcher.fetch_credentials,\n        )\n\n\nclass CanonicalNameCredentialSourcer:\n    def __init__(self, providers):\n        self._providers = providers\n\n    def is_supported(self, source_name):\n        \"\"\"Validates a given source name.\n\n        :type source_name: str\n        :param source_name: The value of credential_source in the config\n            file. This is the canonical name of the credential provider.\n\n        :rtype: bool\n        :returns: True if the credential provider is supported,\n            False otherwise.\n        \"\"\"\n        return source_name in [p.CANONICAL_NAME for p in self._providers]\n\n    def source_credentials(self, source_name):\n        \"\"\"Loads source credentials based on the provided configuration.\n\n        :type source_name: str\n        :param source_name: The value of credential_source in the config\n            file. This is the canonical name of the credential provider.\n\n        :rtype: Credentials\n        \"\"\"\n        source = self._get_provider(source_name)\n        if isinstance(source, CredentialResolver):\n            return source.load_credentials()\n        return source.load()\n\n    def _get_provider(self, canonical_name):\n        \"\"\"Return a credential provider by its canonical name.\n\n        :type canonical_name: str\n        :param canonical_name: The canonical name of the provider.\n\n        :raises UnknownCredentialError: Raised if no\n            credential provider by the provided name\n            is found.\n        \"\"\"\n        provider = self._get_provider_by_canonical_name(canonical_name)\n\n        # The AssumeRole provider should really be part of the SharedConfig\n        # provider rather than being its own thing, but it is not. It is\n        # effectively part of both the SharedConfig provider and the\n        # SharedCredentials provider now due to the way it behaves.\n        # Therefore if we want either of those providers we should return\n        # the AssumeRole provider with it.\n        if canonical_name.lower() in ['sharedconfig', 'sharedcredentials']:\n            assume_role_provider = self._get_provider_by_method('assume-role')\n            if assume_role_provider is not None:\n                # The SharedConfig or SharedCredentials provider may not be\n                # present if it was removed for some reason, but the\n                # AssumeRole provider could still be present. In that case,\n                # return the assume role provider by itself.\n                if provider is None:\n                    return assume_role_provider\n\n                # If both are present, return them both as a\n                # CredentialResolver so that calling code can treat them as\n                # a single entity.\n                return CredentialResolver([assume_role_provider, provider])\n\n        if provider is None:\n            raise UnknownCredentialError(name=canonical_name)\n\n        return provider\n\n    def _get_provider_by_canonical_name(self, canonical_name):\n        \"\"\"Return a credential provider by its canonical name.\n\n        This function is strict, it does not attempt to address\n        compatibility issues.\n        \"\"\"\n        for provider in self._providers:\n            name = provider.CANONICAL_NAME\n            # Canonical names are case-insensitive\n            if name and name.lower() == canonical_name.lower():\n                return provider\n\n    def _get_provider_by_method(self, method):\n        \"\"\"Return a credential provider by its METHOD name.\"\"\"\n        for provider in self._providers:\n            if provider.METHOD == method:\n                return provider\n\n\nclass ContainerProvider(CredentialProvider):\n    METHOD = 'container-role'\n    CANONICAL_NAME = 'EcsContainer'\n    ENV_VAR = 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI'\n    ENV_VAR_FULL = 'AWS_CONTAINER_CREDENTIALS_FULL_URI'\n    ENV_VAR_AUTH_TOKEN = 'AWS_CONTAINER_AUTHORIZATION_TOKEN'\n    ENV_VAR_AUTH_TOKEN_FILE = 'AWS_CONTAINER_AUTHORIZATION_TOKEN_FILE'\n\n    def __init__(self, environ=None, fetcher=None):\n        if environ is None:\n            environ = os.environ\n        if fetcher is None:\n            fetcher = ContainerMetadataFetcher()\n        self._environ = environ\n        self._fetcher = fetcher\n\n    def load(self):\n        # This cred provider is only triggered if the self.ENV_VAR is set,\n        # which only happens if you opt into this feature.\n        if self.ENV_VAR in self._environ or self.ENV_VAR_FULL in self._environ:\n            return self._retrieve_or_fail()\n\n    def _retrieve_or_fail(self):\n        if self._provided_relative_uri():\n            full_uri = self._fetcher.full_url(self._environ[self.ENV_VAR])\n        else:\n            full_uri = self._environ[self.ENV_VAR_FULL]\n        fetcher = self._create_fetcher(full_uri)\n        creds = fetcher()\n        return RefreshableCredentials(\n            access_key=creds['access_key'],\n            secret_key=creds['secret_key'],\n            token=creds['token'],\n            method=self.METHOD,\n            expiry_time=_parse_if_needed(creds['expiry_time']),\n            refresh_using=fetcher,\n        )\n\n    def _build_headers(self):\n        auth_token = None\n        if self.ENV_VAR_AUTH_TOKEN_FILE in self._environ:\n            auth_token_file_path = self._environ[self.ENV_VAR_AUTH_TOKEN_FILE]\n            with open(auth_token_file_path) as token_file:\n                auth_token = token_file.read()\n        elif self.ENV_VAR_AUTH_TOKEN in self._environ:\n            auth_token = self._environ[self.ENV_VAR_AUTH_TOKEN]\n        if auth_token is not None:\n            self._validate_auth_token(auth_token)\n            return {'Authorization': auth_token}\n\n    def _validate_auth_token(self, auth_token):\n        if \"\\r\" in auth_token or \"\\n\" in auth_token:\n            raise ValueError(\"Auth token value is not a legal header value\")\n\n    def _create_fetcher(self, full_uri, *args, **kwargs):\n        def fetch_creds():\n            try:\n                headers = self._build_headers()\n                response = self._fetcher.retrieve_full_uri(\n                    full_uri, headers=headers\n                )\n            except MetadataRetrievalError as e:\n                logger.debug(\n                    \"Error retrieving container metadata: %s\", e, exc_info=True\n                )\n                raise CredentialRetrievalError(\n                    provider=self.METHOD, error_msg=str(e)\n                )\n            return {\n                'access_key': response['AccessKeyId'],\n                'secret_key': response['SecretAccessKey'],\n                'token': response['Token'],\n                'expiry_time': response['Expiration'],\n            }\n\n        return fetch_creds\n\n    def _provided_relative_uri(self):\n        return self.ENV_VAR in self._environ\n\n\nclass CredentialResolver:\n    def __init__(self, providers):\n        \"\"\"\n\n        :param providers: A list of ``CredentialProvider`` instances.\n\n        \"\"\"\n        self.providers = providers\n\n    def insert_before(self, name, credential_provider):\n        \"\"\"\n        Inserts a new instance of ``CredentialProvider`` into the chain that\n        will be tried before an existing one.\n\n        :param name: The short name of the credentials you'd like to insert the\n            new credentials before. (ex. ``env`` or ``config``). Existing names\n            & ordering can be discovered via ``self.available_methods``.\n        :type name: string\n\n        :param cred_instance: An instance of the new ``Credentials`` object\n            you'd like to add to the chain.\n        :type cred_instance: A subclass of ``Credentials``\n        \"\"\"\n        try:\n            offset = [p.METHOD for p in self.providers].index(name)\n        except ValueError:\n            raise UnknownCredentialError(name=name)\n        self.providers.insert(offset, credential_provider)\n\n    def insert_after(self, name, credential_provider):\n        \"\"\"\n        Inserts a new type of ``Credentials`` instance into the chain that will\n        be tried after an existing one.\n\n        :param name: The short name of the credentials you'd like to insert the\n            new credentials after. (ex. ``env`` or ``config``). Existing names\n            & ordering can be discovered via ``self.available_methods``.\n        :type name: string\n\n        :param cred_instance: An instance of the new ``Credentials`` object\n            you'd like to add to the chain.\n        :type cred_instance: A subclass of ``Credentials``\n        \"\"\"\n        offset = self._get_provider_offset(name)\n        self.providers.insert(offset + 1, credential_provider)\n\n    def remove(self, name):\n        \"\"\"\n        Removes a given ``Credentials`` instance from the chain.\n\n        :param name: The short name of the credentials instance to remove.\n        :type name: string\n        \"\"\"\n        available_methods = [p.METHOD for p in self.providers]\n        if name not in available_methods:\n            # It's not present. Fail silently.\n            return\n\n        offset = available_methods.index(name)\n        self.providers.pop(offset)\n\n    def get_provider(self, name):\n        \"\"\"Return a credential provider by name.\n\n        :type name: str\n        :param name: The name of the provider.\n\n        :raises UnknownCredentialError: Raised if no\n            credential provider by the provided name\n            is found.\n        \"\"\"\n        return self.providers[self._get_provider_offset(name)]\n\n    def _get_provider_offset(self, name):\n        try:\n            return [p.METHOD for p in self.providers].index(name)\n        except ValueError:\n            raise UnknownCredentialError(name=name)\n\n    def load_credentials(self):\n        \"\"\"\n        Goes through the credentials chain, returning the first ``Credentials``\n        that could be loaded.\n        \"\"\"\n        # First provider to return a non-None response wins.\n        for provider in self.providers:\n            logger.debug(\"Looking for credentials via: %s\", provider.METHOD)\n            creds = provider.load()\n            if creds is not None:\n                return creds\n\n        # If we got here, no credentials could be found.\n        # This feels like it should be an exception, but historically, ``None``\n        # is returned.\n        #\n        # +1\n        # -js\n        return None\n\n\nclass SSOCredentialFetcher(CachedCredentialFetcher):\n    _UTC_DATE_FORMAT = '%Y-%m-%dT%H:%M:%SZ'\n\n    def __init__(\n        self,\n        start_url,\n        sso_region,\n        role_name,\n        account_id,\n        client_creator,\n        token_loader=None,\n        cache=None,\n        expiry_window_seconds=None,\n        token_provider=None,\n        sso_session_name=None,\n    ):\n        self._client_creator = client_creator\n        self._sso_region = sso_region\n        self._role_name = role_name\n        self._account_id = account_id\n        self._start_url = start_url\n        self._token_loader = token_loader\n        self._token_provider = token_provider\n        self._sso_session_name = sso_session_name\n        super().__init__(cache, expiry_window_seconds)\n\n    def _create_cache_key(self):\n        \"\"\"Create a predictable cache key for the current configuration.\n\n        The cache key is intended to be compatible with file names.\n        \"\"\"\n        args = {\n            'roleName': self._role_name,\n            'accountId': self._account_id,\n        }\n        if self._sso_session_name:\n            args['sessionName'] = self._sso_session_name\n        else:\n            args['startUrl'] = self._start_url\n        # NOTE: It would be good to hoist this cache key construction logic\n        # into the CachedCredentialFetcher class as we should be consistent.\n        # Unfortunately, the current assume role fetchers that sub class don't\n        # pass separators resulting in non-minified JSON. In the long term,\n        # all fetchers should use the below caching scheme.\n        args = json.dumps(args, sort_keys=True, separators=(',', ':'))\n        argument_hash = sha1(args.encode('utf-8')).hexdigest()\n        return self._make_file_safe(argument_hash)\n\n    def _parse_timestamp(self, timestamp_ms):\n        # fromtimestamp expects seconds so: milliseconds / 1000 = seconds\n        timestamp_seconds = timestamp_ms / 1000.0\n        timestamp = datetime.datetime.fromtimestamp(timestamp_seconds, tzutc())\n        return timestamp.strftime(self._UTC_DATE_FORMAT)\n\n    def _get_credentials(self):\n        \"\"\"Get credentials by calling SSO get role credentials.\"\"\"\n        config = Config(\n            signature_version=UNSIGNED,\n            region_name=self._sso_region,\n        )\n        client = self._client_creator('sso', config=config)\n        if self._token_provider:\n            initial_token_data = self._token_provider.load_token()\n            token = initial_token_data.get_frozen_token().token\n        else:\n            token = self._token_loader(self._start_url)['accessToken']\n\n        kwargs = {\n            'roleName': self._role_name,\n            'accountId': self._account_id,\n            'accessToken': token,\n        }\n        try:\n            response = client.get_role_credentials(**kwargs)\n        except client.exceptions.UnauthorizedException:\n            raise UnauthorizedSSOTokenError()\n        credentials = response['roleCredentials']\n\n        credentials = {\n            'ProviderType': 'sso',\n            'Credentials': {\n                'AccessKeyId': credentials['accessKeyId'],\n                'SecretAccessKey': credentials['secretAccessKey'],\n                'SessionToken': credentials['sessionToken'],\n                'Expiration': self._parse_timestamp(credentials['expiration']),\n            },\n        }\n        return credentials\n\n\nclass SSOProvider(CredentialProvider):\n    METHOD = 'sso'\n\n    _SSO_TOKEN_CACHE_DIR = os.path.expanduser(\n        os.path.join('~', '.aws', 'sso', 'cache')\n    )\n    _PROFILE_REQUIRED_CONFIG_VARS = (\n        'sso_role_name',\n        'sso_account_id',\n    )\n    _SSO_REQUIRED_CONFIG_VARS = (\n        'sso_start_url',\n        'sso_region',\n    )\n    _ALL_REQUIRED_CONFIG_VARS = (\n        _PROFILE_REQUIRED_CONFIG_VARS + _SSO_REQUIRED_CONFIG_VARS\n    )\n\n    def __init__(\n        self,\n        load_config,\n        client_creator,\n        profile_name,\n        cache=None,\n        token_cache=None,\n        token_provider=None,\n    ):\n        if token_cache is None:\n            token_cache = JSONFileCache(self._SSO_TOKEN_CACHE_DIR)\n        self._token_cache = token_cache\n        self._token_provider = token_provider\n        if cache is None:\n            cache = {}\n        self.cache = cache\n        self._load_config = load_config\n        self._client_creator = client_creator\n        self._profile_name = profile_name\n\n    def _load_sso_config(self):\n        loaded_config = self._load_config()\n        profiles = loaded_config.get('profiles', {})\n        profile_name = self._profile_name\n        profile_config = profiles.get(self._profile_name, {})\n        sso_sessions = loaded_config.get('sso_sessions', {})\n\n        # Role name & Account ID indicate the cred provider should be used\n        if all(\n            c not in profile_config for c in self._PROFILE_REQUIRED_CONFIG_VARS\n        ):\n            return None\n\n        resolved_config, extra_reqs = self._resolve_sso_session_reference(\n            profile_config, sso_sessions\n        )\n\n        config = {}\n        missing_config_vars = []\n        all_required_configs = self._ALL_REQUIRED_CONFIG_VARS + extra_reqs\n        for config_var in all_required_configs:\n            if config_var in resolved_config:\n                config[config_var] = resolved_config[config_var]\n            else:\n                missing_config_vars.append(config_var)\n\n        if missing_config_vars:\n            missing = ', '.join(missing_config_vars)\n            raise InvalidConfigError(\n                error_msg=(\n                    'The profile \"%s\" is configured to use SSO but is missing '\n                    'required configuration: %s' % (profile_name, missing)\n                )\n            )\n        return config\n\n    def _resolve_sso_session_reference(self, profile_config, sso_sessions):\n        sso_session_name = profile_config.get('sso_session')\n        if sso_session_name is None:\n            # No reference to resolve, proceed with legacy flow\n            return profile_config, ()\n\n        if sso_session_name not in sso_sessions:\n            error_msg = f'The specified sso-session does not exist: \"{sso_session_name}\"'\n            raise InvalidConfigError(error_msg=error_msg)\n\n        config = profile_config.copy()\n        session = sso_sessions[sso_session_name]\n        for config_var, val in session.items():\n            # Validate any keys referenced in both profile and sso_session match\n            if config.get(config_var, val) != val:\n                error_msg = (\n                    f\"The value for {config_var} is inconsistent between \"\n                    f\"profile ({config[config_var]}) and sso-session ({val}).\"\n                )\n                raise InvalidConfigError(error_msg=error_msg)\n            config[config_var] = val\n        return config, ('sso_session',)\n\n    def load(self):\n        sso_config = self._load_sso_config()\n        if not sso_config:\n            return None\n\n        fetcher_kwargs = {\n            'start_url': sso_config['sso_start_url'],\n            'sso_region': sso_config['sso_region'],\n            'role_name': sso_config['sso_role_name'],\n            'account_id': sso_config['sso_account_id'],\n            'client_creator': self._client_creator,\n            'token_loader': SSOTokenLoader(cache=self._token_cache),\n            'cache': self.cache,\n        }\n        if 'sso_session' in sso_config:\n            fetcher_kwargs['sso_session_name'] = sso_config['sso_session']\n            fetcher_kwargs['token_provider'] = self._token_provider\n\n        sso_fetcher = SSOCredentialFetcher(**fetcher_kwargs)\n\n        return DeferredRefreshableCredentials(\n            method=self.METHOD,\n            refresh_using=sso_fetcher.fetch_credentials,\n        )\n", "botocore/monitoring.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n#     http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport json\nimport logging\nimport re\nimport time\n\nfrom botocore.compat import ensure_bytes, ensure_unicode, urlparse\nfrom botocore.retryhandler import EXCEPTION_MAP as RETRYABLE_EXCEPTIONS\n\nlogger = logging.getLogger(__name__)\n\n\nclass Monitor:\n    _EVENTS_TO_REGISTER = [\n        'before-parameter-build',\n        'request-created',\n        'response-received',\n        'after-call',\n        'after-call-error',\n    ]\n\n    def __init__(self, adapter, publisher):\n        \"\"\"Abstraction for monitoring clients API calls\n\n        :param adapter: An adapter that takes event emitter events\n            and produces monitor events\n\n        :param publisher: A publisher for generated monitor events\n        \"\"\"\n        self._adapter = adapter\n        self._publisher = publisher\n\n    def register(self, event_emitter):\n        \"\"\"Register an event emitter to the monitor\"\"\"\n        for event_to_register in self._EVENTS_TO_REGISTER:\n            event_emitter.register_last(event_to_register, self.capture)\n\n    def capture(self, event_name, **payload):\n        \"\"\"Captures an incoming event from the event emitter\n\n        It will feed an event emitter event to the monitor's adaptor to create\n        a monitor event and then publish that event to the monitor's publisher.\n        \"\"\"\n        try:\n            monitor_event = self._adapter.feed(event_name, payload)\n            if monitor_event:\n                self._publisher.publish(monitor_event)\n        except Exception as e:\n            logger.debug(\n                'Exception %s raised by client monitor in handling event %s',\n                e,\n                event_name,\n                exc_info=True,\n            )\n\n\nclass MonitorEventAdapter:\n    def __init__(self, time=time.time):\n        \"\"\"Adapts event emitter events to produce monitor events\n\n        :type time: callable\n        :param time: A callable that produces the current time\n        \"\"\"\n        self._time = time\n\n    def feed(self, emitter_event_name, emitter_payload):\n        \"\"\"Feed an event emitter event to generate a monitor event\n\n        :type emitter_event_name: str\n        :param emitter_event_name: The name of the event emitted\n\n        :type emitter_payload: dict\n        :param emitter_payload: The payload to associated to the event\n            emitted\n\n        :rtype: BaseMonitorEvent\n        :returns: A monitor event based on the event emitter events\n            fired\n        \"\"\"\n        return self._get_handler(emitter_event_name)(**emitter_payload)\n\n    def _get_handler(self, event_name):\n        return getattr(\n            self, '_handle_' + event_name.split('.')[0].replace('-', '_')\n        )\n\n    def _handle_before_parameter_build(self, model, context, **kwargs):\n        context['current_api_call_event'] = APICallEvent(\n            service=model.service_model.service_id,\n            operation=model.wire_name,\n            timestamp=self._get_current_time(),\n        )\n\n    def _handle_request_created(self, request, **kwargs):\n        context = request.context\n        new_attempt_event = context[\n            'current_api_call_event'\n        ].new_api_call_attempt(timestamp=self._get_current_time())\n        new_attempt_event.request_headers = request.headers\n        new_attempt_event.url = request.url\n        context['current_api_call_attempt_event'] = new_attempt_event\n\n    def _handle_response_received(\n        self, parsed_response, context, exception, **kwargs\n    ):\n        attempt_event = context.pop('current_api_call_attempt_event')\n        attempt_event.latency = self._get_latency(attempt_event)\n        if parsed_response is not None:\n            attempt_event.http_status_code = parsed_response[\n                'ResponseMetadata'\n            ]['HTTPStatusCode']\n            attempt_event.response_headers = parsed_response[\n                'ResponseMetadata'\n            ]['HTTPHeaders']\n            attempt_event.parsed_error = parsed_response.get('Error')\n        else:\n            attempt_event.wire_exception = exception\n        return attempt_event\n\n    def _handle_after_call(self, context, parsed, **kwargs):\n        context['current_api_call_event'].retries_exceeded = parsed[\n            'ResponseMetadata'\n        ].get('MaxAttemptsReached', False)\n        return self._complete_api_call(context)\n\n    def _handle_after_call_error(self, context, exception, **kwargs):\n        # If the after-call-error was emitted and the error being raised\n        # was a retryable connection error, then the retries must have exceeded\n        # for that exception as this event gets emitted **after** retries\n        # happen.\n        context[\n            'current_api_call_event'\n        ].retries_exceeded = self._is_retryable_exception(exception)\n        return self._complete_api_call(context)\n\n    def _is_retryable_exception(self, exception):\n        return isinstance(\n            exception, tuple(RETRYABLE_EXCEPTIONS['GENERAL_CONNECTION_ERROR'])\n        )\n\n    def _complete_api_call(self, context):\n        call_event = context.pop('current_api_call_event')\n        call_event.latency = self._get_latency(call_event)\n        return call_event\n\n    def _get_latency(self, event):\n        return self._get_current_time() - event.timestamp\n\n    def _get_current_time(self):\n        return int(self._time() * 1000)\n\n\nclass BaseMonitorEvent:\n    def __init__(self, service, operation, timestamp):\n        \"\"\"Base monitor event\n\n        :type service: str\n        :param service: A string identifying the service associated to\n            the event\n\n        :type operation: str\n        :param operation: A string identifying the operation of service\n            associated to the event\n\n        :type timestamp: int\n        :param timestamp: Epoch time in milliseconds from when the event began\n        \"\"\"\n        self.service = service\n        self.operation = operation\n        self.timestamp = timestamp\n\n    def __repr__(self):\n        return f'{self.__class__.__name__}({self.__dict__!r})'\n\n    def __eq__(self, other):\n        if isinstance(other, self.__class__):\n            return self.__dict__ == other.__dict__\n        return False\n\n\nclass APICallEvent(BaseMonitorEvent):\n    def __init__(\n        self,\n        service,\n        operation,\n        timestamp,\n        latency=None,\n        attempts=None,\n        retries_exceeded=False,\n    ):\n        \"\"\"Monitor event for a single API call\n\n        This event corresponds to a single client method call, which includes\n        every HTTP requests attempt made in order to complete the client call\n\n        :type service: str\n        :param service: A string identifying the service associated to\n            the event\n\n        :type operation: str\n        :param operation: A string identifying the operation of service\n            associated to the event\n\n        :type timestamp: int\n        :param timestamp: Epoch time in milliseconds from when the event began\n\n        :type latency: int\n        :param latency: The time in milliseconds to complete the client call\n\n        :type attempts: list\n        :param attempts: The list of APICallAttempts associated to the\n            APICall\n\n        :type retries_exceeded: bool\n        :param retries_exceeded: True if API call exceeded retries. False\n            otherwise\n        \"\"\"\n        super().__init__(\n            service=service, operation=operation, timestamp=timestamp\n        )\n        self.latency = latency\n        self.attempts = attempts\n        if attempts is None:\n            self.attempts = []\n        self.retries_exceeded = retries_exceeded\n\n    def new_api_call_attempt(self, timestamp):\n        \"\"\"Instantiates APICallAttemptEvent associated to the APICallEvent\n\n        :type timestamp: int\n        :param timestamp: Epoch time in milliseconds to associate to the\n            APICallAttemptEvent\n        \"\"\"\n        attempt_event = APICallAttemptEvent(\n            service=self.service, operation=self.operation, timestamp=timestamp\n        )\n        self.attempts.append(attempt_event)\n        return attempt_event\n\n\nclass APICallAttemptEvent(BaseMonitorEvent):\n    def __init__(\n        self,\n        service,\n        operation,\n        timestamp,\n        latency=None,\n        url=None,\n        http_status_code=None,\n        request_headers=None,\n        response_headers=None,\n        parsed_error=None,\n        wire_exception=None,\n    ):\n        \"\"\"Monitor event for a single API call attempt\n\n        This event corresponds to a single HTTP request attempt in completing\n        the entire client method call.\n\n        :type service: str\n        :param service: A string identifying the service associated to\n            the event\n\n        :type operation: str\n        :param operation: A string identifying the operation of service\n            associated to the event\n\n        :type timestamp: int\n        :param timestamp: Epoch time in milliseconds from when the HTTP request\n            started\n\n        :type latency: int\n        :param latency: The time in milliseconds to complete the HTTP request\n            whether it succeeded or failed\n\n        :type url: str\n        :param url: The URL the attempt was sent to\n\n        :type http_status_code: int\n        :param http_status_code: The HTTP status code of the HTTP response\n            if there was a response\n\n        :type request_headers: dict\n        :param request_headers: The HTTP headers sent in making the HTTP\n            request\n\n        :type response_headers: dict\n        :param response_headers: The HTTP headers returned in the HTTP response\n            if there was a response\n\n        :type parsed_error: dict\n        :param parsed_error: The error parsed if the service returned an\n            error back\n\n        :type wire_exception: Exception\n        :param wire_exception: The exception raised in sending the HTTP\n            request (i.e. ConnectionError)\n        \"\"\"\n        super().__init__(\n            service=service, operation=operation, timestamp=timestamp\n        )\n        self.latency = latency\n        self.url = url\n        self.http_status_code = http_status_code\n        self.request_headers = request_headers\n        self.response_headers = response_headers\n        self.parsed_error = parsed_error\n        self.wire_exception = wire_exception\n\n\nclass CSMSerializer:\n    _MAX_CLIENT_ID_LENGTH = 255\n    _MAX_EXCEPTION_CLASS_LENGTH = 128\n    _MAX_ERROR_CODE_LENGTH = 128\n    _MAX_USER_AGENT_LENGTH = 256\n    _MAX_MESSAGE_LENGTH = 512\n    _RESPONSE_HEADERS_TO_EVENT_ENTRIES = {\n        'x-amzn-requestid': 'XAmznRequestId',\n        'x-amz-request-id': 'XAmzRequestId',\n        'x-amz-id-2': 'XAmzId2',\n    }\n    _AUTH_REGEXS = {\n        'v4': re.compile(\n            r'AWS4-HMAC-SHA256 '\n            r'Credential=(?P<access_key>\\w+)/\\d+/'\n            r'(?P<signing_region>[a-z0-9-]+)/'\n        ),\n        's3': re.compile(r'AWS (?P<access_key>\\w+):'),\n    }\n    _SERIALIZEABLE_EVENT_PROPERTIES = [\n        'service',\n        'operation',\n        'timestamp',\n        'attempts',\n        'latency',\n        'retries_exceeded',\n        'url',\n        'request_headers',\n        'http_status_code',\n        'response_headers',\n        'parsed_error',\n        'wire_exception',\n    ]\n\n    def __init__(self, csm_client_id):\n        \"\"\"Serializes monitor events to CSM (Client Side Monitoring) format\n\n        :type csm_client_id: str\n        :param csm_client_id: The application identifier to associate\n            to the serialized events\n        \"\"\"\n        self._validate_client_id(csm_client_id)\n        self.csm_client_id = csm_client_id\n\n    def _validate_client_id(self, csm_client_id):\n        if len(csm_client_id) > self._MAX_CLIENT_ID_LENGTH:\n            raise ValueError(\n                f'The value provided for csm_client_id: {csm_client_id} exceeds '\n                f'the maximum length of {self._MAX_CLIENT_ID_LENGTH} characters'\n            )\n\n    def serialize(self, event):\n        \"\"\"Serializes a monitor event to the CSM format\n\n        :type event: BaseMonitorEvent\n        :param event: The event to serialize to bytes\n\n        :rtype: bytes\n        :returns: The CSM serialized form of the event\n        \"\"\"\n        event_dict = self._get_base_event_dict(event)\n        event_type = self._get_event_type(event)\n        event_dict['Type'] = event_type\n        for attr in self._SERIALIZEABLE_EVENT_PROPERTIES:\n            value = getattr(event, attr, None)\n            if value is not None:\n                getattr(self, '_serialize_' + attr)(\n                    value, event_dict, event_type=event_type\n                )\n        return ensure_bytes(json.dumps(event_dict, separators=(',', ':')))\n\n    def _get_base_event_dict(self, event):\n        return {\n            'Version': 1,\n            'ClientId': self.csm_client_id,\n        }\n\n    def _serialize_service(self, service, event_dict, **kwargs):\n        event_dict['Service'] = service\n\n    def _serialize_operation(self, operation, event_dict, **kwargs):\n        event_dict['Api'] = operation\n\n    def _serialize_timestamp(self, timestamp, event_dict, **kwargs):\n        event_dict['Timestamp'] = timestamp\n\n    def _serialize_attempts(self, attempts, event_dict, **kwargs):\n        event_dict['AttemptCount'] = len(attempts)\n        if attempts:\n            self._add_fields_from_last_attempt(event_dict, attempts[-1])\n\n    def _add_fields_from_last_attempt(self, event_dict, last_attempt):\n        if last_attempt.request_headers:\n            # It does not matter which attempt to use to grab the region\n            # for the ApiCall event, but SDKs typically do the last one.\n            region = self._get_region(last_attempt.request_headers)\n            if region is not None:\n                event_dict['Region'] = region\n            event_dict['UserAgent'] = self._get_user_agent(\n                last_attempt.request_headers\n            )\n        if last_attempt.http_status_code is not None:\n            event_dict['FinalHttpStatusCode'] = last_attempt.http_status_code\n        if last_attempt.parsed_error is not None:\n            self._serialize_parsed_error(\n                last_attempt.parsed_error, event_dict, 'ApiCall'\n            )\n        if last_attempt.wire_exception is not None:\n            self._serialize_wire_exception(\n                last_attempt.wire_exception, event_dict, 'ApiCall'\n            )\n\n    def _serialize_latency(self, latency, event_dict, event_type):\n        if event_type == 'ApiCall':\n            event_dict['Latency'] = latency\n        elif event_type == 'ApiCallAttempt':\n            event_dict['AttemptLatency'] = latency\n\n    def _serialize_retries_exceeded(\n        self, retries_exceeded, event_dict, **kwargs\n    ):\n        event_dict['MaxRetriesExceeded'] = 1 if retries_exceeded else 0\n\n    def _serialize_url(self, url, event_dict, **kwargs):\n        event_dict['Fqdn'] = urlparse(url).netloc\n\n    def _serialize_request_headers(\n        self, request_headers, event_dict, **kwargs\n    ):\n        event_dict['UserAgent'] = self._get_user_agent(request_headers)\n        if self._is_signed(request_headers):\n            event_dict['AccessKey'] = self._get_access_key(request_headers)\n        region = self._get_region(request_headers)\n        if region is not None:\n            event_dict['Region'] = region\n        if 'X-Amz-Security-Token' in request_headers:\n            event_dict['SessionToken'] = request_headers[\n                'X-Amz-Security-Token'\n            ]\n\n    def _serialize_http_status_code(\n        self, http_status_code, event_dict, **kwargs\n    ):\n        event_dict['HttpStatusCode'] = http_status_code\n\n    def _serialize_response_headers(\n        self, response_headers, event_dict, **kwargs\n    ):\n        for header, entry in self._RESPONSE_HEADERS_TO_EVENT_ENTRIES.items():\n            if header in response_headers:\n                event_dict[entry] = response_headers[header]\n\n    def _serialize_parsed_error(\n        self, parsed_error, event_dict, event_type, **kwargs\n    ):\n        field_prefix = 'Final' if event_type == 'ApiCall' else ''\n        event_dict[field_prefix + 'AwsException'] = self._truncate(\n            parsed_error['Code'], self._MAX_ERROR_CODE_LENGTH\n        )\n        event_dict[field_prefix + 'AwsExceptionMessage'] = self._truncate(\n            parsed_error['Message'], self._MAX_MESSAGE_LENGTH\n        )\n\n    def _serialize_wire_exception(\n        self, wire_exception, event_dict, event_type, **kwargs\n    ):\n        field_prefix = 'Final' if event_type == 'ApiCall' else ''\n        event_dict[field_prefix + 'SdkException'] = self._truncate(\n            wire_exception.__class__.__name__, self._MAX_EXCEPTION_CLASS_LENGTH\n        )\n        event_dict[field_prefix + 'SdkExceptionMessage'] = self._truncate(\n            str(wire_exception), self._MAX_MESSAGE_LENGTH\n        )\n\n    def _get_event_type(self, event):\n        if isinstance(event, APICallEvent):\n            return 'ApiCall'\n        elif isinstance(event, APICallAttemptEvent):\n            return 'ApiCallAttempt'\n\n    def _get_access_key(self, request_headers):\n        auth_val = self._get_auth_value(request_headers)\n        _, auth_match = self._get_auth_match(auth_val)\n        return auth_match.group('access_key')\n\n    def _get_region(self, request_headers):\n        if not self._is_signed(request_headers):\n            return None\n        auth_val = self._get_auth_value(request_headers)\n        signature_version, auth_match = self._get_auth_match(auth_val)\n        if signature_version != 'v4':\n            return None\n        return auth_match.group('signing_region')\n\n    def _get_user_agent(self, request_headers):\n        return self._truncate(\n            ensure_unicode(request_headers.get('User-Agent', '')),\n            self._MAX_USER_AGENT_LENGTH,\n        )\n\n    def _is_signed(self, request_headers):\n        return 'Authorization' in request_headers\n\n    def _get_auth_value(self, request_headers):\n        return ensure_unicode(request_headers['Authorization'])\n\n    def _get_auth_match(self, auth_val):\n        for signature_version, regex in self._AUTH_REGEXS.items():\n            match = regex.match(auth_val)\n            if match:\n                return signature_version, match\n        return None, None\n\n    def _truncate(self, text, max_length):\n        if len(text) > max_length:\n            logger.debug(\n                'Truncating following value to maximum length of ' '%s: %s',\n                text,\n                max_length,\n            )\n            return text[:max_length]\n        return text\n\n\nclass SocketPublisher:\n    _MAX_MONITOR_EVENT_LENGTH = 8 * 1024\n\n    def __init__(self, socket, host, port, serializer):\n        \"\"\"Publishes monitor events to a socket\n\n        :type socket: socket.socket\n        :param socket: The socket object to use to publish events\n\n        :type host: string\n        :param host: The host to send events to\n\n        :type port: integer\n        :param port: The port on the host to send events to\n\n        :param serializer: The serializer to use to serialize the event\n            to a form that can be published to the socket. This must\n            have a `serialize()` method that accepts a monitor event\n            and return bytes\n        \"\"\"\n        self._socket = socket\n        self._address = (host, port)\n        self._serializer = serializer\n\n    def publish(self, event):\n        \"\"\"Publishes a specified monitor event\n\n        :type event: BaseMonitorEvent\n        :param event: The monitor event to be sent\n            over the publisher's socket to the desired address.\n        \"\"\"\n        serialized_event = self._serializer.serialize(event)\n        if len(serialized_event) > self._MAX_MONITOR_EVENT_LENGTH:\n            logger.debug(\n                'Serialized event of size %s exceeds the maximum length '\n                'allowed: %s. Not sending event to socket.',\n                len(serialized_event),\n                self._MAX_MONITOR_EVENT_LENGTH,\n            )\n            return\n        self._socket.sendto(serialized_event, self._address)\n", "botocore/configprovider.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"This module contains the interface for controlling how configuration\nis loaded.\n\"\"\"\nimport copy\nimport logging\nimport os\n\nfrom botocore import utils\nfrom botocore.exceptions import InvalidConfigError\n\nlogger = logging.getLogger(__name__)\n\n\n#: A default dictionary that maps the logical names for session variables\n#: to the specific environment variables and configuration file names\n#: that contain the values for these variables.\n#: When creating a new Session object, you can pass in your own dictionary\n#: to remap the logical names or to add new logical names.  You can then\n#: get the current value for these variables by using the\n#: ``get_config_variable`` method of the :class:`botocore.session.Session`\n#: class.\n#: These form the keys of the dictionary.  The values in the dictionary\n#: are tuples of (<config_name>, <environment variable>, <default value>,\n#: <conversion func>).\n#: The conversion func is a function that takes the configuration value\n#: as an argument and returns the converted value.  If this value is\n#: None, then the configuration value is returned unmodified.  This\n#: conversion function can be used to type convert config values to\n#: values other than the default values of strings.\n#: The ``profile`` and ``config_file`` variables should always have a\n#: None value for the first entry in the tuple because it doesn't make\n#: sense to look inside the config file for the location of the config\n#: file or for the default profile to use.\n#: The ``config_name`` is the name to look for in the configuration file,\n#: the ``env var`` is the OS environment variable (``os.environ``) to\n#: use, and ``default_value`` is the value to use if no value is otherwise\n#: found.\n#: NOTE: Fixing the spelling of this variable would be a breaking change.\n#: Please leave as is.\nBOTOCORE_DEFAUT_SESSION_VARIABLES = {\n    # logical:  config_file, env_var,        default_value, conversion_func\n    'profile': (None, ['AWS_DEFAULT_PROFILE', 'AWS_PROFILE'], None, None),\n    'region': ('region', 'AWS_DEFAULT_REGION', None, None),\n    'data_path': ('data_path', 'AWS_DATA_PATH', None, None),\n    'config_file': (None, 'AWS_CONFIG_FILE', '~/.aws/config', None),\n    'ca_bundle': ('ca_bundle', 'AWS_CA_BUNDLE', None, None),\n    'api_versions': ('api_versions', None, {}, None),\n    # This is the shared credentials file amongst sdks.\n    'credentials_file': (\n        None,\n        'AWS_SHARED_CREDENTIALS_FILE',\n        '~/.aws/credentials',\n        None,\n    ),\n    # These variables only exist in the config file.\n    # This is the number of seconds until we time out a request to\n    # the instance metadata service.\n    'metadata_service_timeout': (\n        'metadata_service_timeout',\n        'AWS_METADATA_SERVICE_TIMEOUT',\n        1,\n        int,\n    ),\n    # This is the number of request attempts we make until we give\n    # up trying to retrieve data from the instance metadata service.\n    'metadata_service_num_attempts': (\n        'metadata_service_num_attempts',\n        'AWS_METADATA_SERVICE_NUM_ATTEMPTS',\n        1,\n        int,\n    ),\n    'ec2_metadata_service_endpoint': (\n        'ec2_metadata_service_endpoint',\n        'AWS_EC2_METADATA_SERVICE_ENDPOINT',\n        None,\n        None,\n    ),\n    'ec2_metadata_service_endpoint_mode': (\n        'ec2_metadata_service_endpoint_mode',\n        'AWS_EC2_METADATA_SERVICE_ENDPOINT_MODE',\n        None,\n        None,\n    ),\n    'ec2_metadata_v1_disabled': (\n        'ec2_metadata_v1_disabled',\n        'AWS_EC2_METADATA_V1_DISABLED',\n        False,\n        utils.ensure_boolean,\n    ),\n    'imds_use_ipv6': (\n        'imds_use_ipv6',\n        'AWS_IMDS_USE_IPV6',\n        False,\n        utils.ensure_boolean,\n    ),\n    'use_dualstack_endpoint': (\n        'use_dualstack_endpoint',\n        'AWS_USE_DUALSTACK_ENDPOINT',\n        None,\n        utils.ensure_boolean,\n    ),\n    'use_fips_endpoint': (\n        'use_fips_endpoint',\n        'AWS_USE_FIPS_ENDPOINT',\n        None,\n        utils.ensure_boolean,\n    ),\n    'ignore_configured_endpoint_urls': (\n        'ignore_configured_endpoint_urls',\n        'AWS_IGNORE_CONFIGURED_ENDPOINT_URLS',\n        None,\n        utils.ensure_boolean,\n    ),\n    'parameter_validation': ('parameter_validation', None, True, None),\n    # Client side monitoring configurations.\n    # Note: These configurations are considered internal to botocore.\n    # Do not use them until publicly documented.\n    'csm_enabled': (\n        'csm_enabled',\n        'AWS_CSM_ENABLED',\n        False,\n        utils.ensure_boolean,\n    ),\n    'csm_host': ('csm_host', 'AWS_CSM_HOST', '127.0.0.1', None),\n    'csm_port': ('csm_port', 'AWS_CSM_PORT', 31000, int),\n    'csm_client_id': ('csm_client_id', 'AWS_CSM_CLIENT_ID', '', None),\n    # Endpoint discovery configuration\n    'endpoint_discovery_enabled': (\n        'endpoint_discovery_enabled',\n        'AWS_ENDPOINT_DISCOVERY_ENABLED',\n        'auto',\n        None,\n    ),\n    'sts_regional_endpoints': (\n        'sts_regional_endpoints',\n        'AWS_STS_REGIONAL_ENDPOINTS',\n        'legacy',\n        None,\n    ),\n    'retry_mode': ('retry_mode', 'AWS_RETRY_MODE', 'legacy', None),\n    'defaults_mode': ('defaults_mode', 'AWS_DEFAULTS_MODE', 'legacy', None),\n    # We can't have a default here for v1 because we need to defer to\n    # whatever the defaults are in _retry.json.\n    'max_attempts': ('max_attempts', 'AWS_MAX_ATTEMPTS', None, int),\n    'user_agent_appid': ('sdk_ua_app_id', 'AWS_SDK_UA_APP_ID', None, None),\n    'request_min_compression_size_bytes': (\n        'request_min_compression_size_bytes',\n        'AWS_REQUEST_MIN_COMPRESSION_SIZE_BYTES',\n        10240,\n        None,\n    ),\n    'disable_request_compression': (\n        'disable_request_compression',\n        'AWS_DISABLE_REQUEST_COMPRESSION',\n        False,\n        utils.ensure_boolean,\n    ),\n}\n# A mapping for the s3 specific configuration vars. These are the configuration\n# vars that typically go in the s3 section of the config file. This mapping\n# follows the same schema as the previous session variable mapping.\nDEFAULT_S3_CONFIG_VARS = {\n    'addressing_style': (('s3', 'addressing_style'), None, None, None),\n    'use_accelerate_endpoint': (\n        ('s3', 'use_accelerate_endpoint'),\n        None,\n        None,\n        utils.ensure_boolean,\n    ),\n    'use_dualstack_endpoint': (\n        ('s3', 'use_dualstack_endpoint'),\n        None,\n        None,\n        utils.ensure_boolean,\n    ),\n    'payload_signing_enabled': (\n        ('s3', 'payload_signing_enabled'),\n        None,\n        None,\n        utils.ensure_boolean,\n    ),\n    'use_arn_region': (\n        ['s3_use_arn_region', ('s3', 'use_arn_region')],\n        'AWS_S3_USE_ARN_REGION',\n        None,\n        utils.ensure_boolean,\n    ),\n    'us_east_1_regional_endpoint': (\n        [\n            's3_us_east_1_regional_endpoint',\n            ('s3', 'us_east_1_regional_endpoint'),\n        ],\n        'AWS_S3_US_EAST_1_REGIONAL_ENDPOINT',\n        None,\n        None,\n    ),\n    's3_disable_multiregion_access_points': (\n        ('s3', 's3_disable_multiregion_access_points'),\n        'AWS_S3_DISABLE_MULTIREGION_ACCESS_POINTS',\n        None,\n        utils.ensure_boolean,\n    ),\n}\n# A mapping for the proxy specific configuration vars. These are\n# used to configure how botocore interacts with proxy setups while\n# sending requests.\nDEFAULT_PROXIES_CONFIG_VARS = {\n    'proxy_ca_bundle': ('proxy_ca_bundle', None, None, None),\n    'proxy_client_cert': ('proxy_client_cert', None, None, None),\n    'proxy_use_forwarding_for_https': (\n        'proxy_use_forwarding_for_https',\n        None,\n        None,\n        utils.normalize_boolean,\n    ),\n}\n\n\ndef create_botocore_default_config_mapping(session):\n    chain_builder = ConfigChainFactory(session=session)\n    config_mapping = _create_config_chain_mapping(\n        chain_builder, BOTOCORE_DEFAUT_SESSION_VARIABLES\n    )\n    config_mapping['s3'] = SectionConfigProvider(\n        's3',\n        session,\n        _create_config_chain_mapping(chain_builder, DEFAULT_S3_CONFIG_VARS),\n    )\n    config_mapping['proxies_config'] = SectionConfigProvider(\n        'proxies_config',\n        session,\n        _create_config_chain_mapping(\n            chain_builder, DEFAULT_PROXIES_CONFIG_VARS\n        ),\n    )\n    return config_mapping\n\n\ndef _create_config_chain_mapping(chain_builder, config_variables):\n    mapping = {}\n    for logical_name, config in config_variables.items():\n        mapping[logical_name] = chain_builder.create_config_chain(\n            instance_name=logical_name,\n            env_var_names=config[1],\n            config_property_names=config[0],\n            default=config[2],\n            conversion_func=config[3],\n        )\n    return mapping\n\n\nclass DefaultConfigResolver:\n    def __init__(self, default_config_data):\n        self._base_default_config = default_config_data['base']\n        self._modes = default_config_data['modes']\n        self._resolved_default_configurations = {}\n\n    def _resolve_default_values_by_mode(self, mode):\n        default_config = self._base_default_config.copy()\n        modifications = self._modes.get(mode)\n\n        for config_var in modifications:\n            default_value = default_config[config_var]\n            modification_dict = modifications[config_var]\n            modification = list(modification_dict.keys())[0]\n            modification_value = modification_dict[modification]\n            if modification == 'multiply':\n                default_value *= modification_value\n            elif modification == 'add':\n                default_value += modification_value\n            elif modification == 'override':\n                default_value = modification_value\n            default_config[config_var] = default_value\n        return default_config\n\n    def get_default_modes(self):\n        default_modes = ['legacy', 'auto']\n        default_modes.extend(self._modes.keys())\n        return default_modes\n\n    def get_default_config_values(self, mode):\n        if mode not in self._resolved_default_configurations:\n            defaults = self._resolve_default_values_by_mode(mode)\n            self._resolved_default_configurations[mode] = defaults\n        return self._resolved_default_configurations[mode]\n\n\nclass ConfigChainFactory:\n    \"\"\"Factory class to create our most common configuration chain case.\n\n    This is a convenience class to construct configuration chains that follow\n    our most common pattern. This is to prevent ordering them incorrectly,\n    and to make the config chain construction more readable.\n    \"\"\"\n\n    def __init__(self, session, environ=None):\n        \"\"\"Initialize a ConfigChainFactory.\n\n        :type session: :class:`botocore.session.Session`\n        :param session: This is the session that should be used to look up\n            values from the config file.\n\n        :type environ: dict\n        :param environ: A mapping to use for environment variables. If this\n            is not provided it will default to use os.environ.\n        \"\"\"\n        self._session = session\n        if environ is None:\n            environ = os.environ\n        self._environ = environ\n\n    def create_config_chain(\n        self,\n        instance_name=None,\n        env_var_names=None,\n        config_property_names=None,\n        default=None,\n        conversion_func=None,\n    ):\n        \"\"\"Build a config chain following the standard botocore pattern.\n\n        In botocore most of our config chains follow the the precendence:\n        session_instance_variables, environment, config_file, default_value.\n\n        This is a convenience function for creating a chain that follow\n        that precendence.\n\n        :type instance_name: str\n        :param instance_name: This indicates what session instance variable\n            corresponds to this config value. If it is None it will not be\n            added to the chain.\n\n        :type env_var_names: str or list of str or None\n        :param env_var_names: One or more environment variable names to\n            search for this value. They are searched in order. If it is None\n            it will not be added to the chain.\n\n        :type config_property_names: str/tuple or list of str/tuple or None\n        :param config_property_names: One of more strings or tuples\n            representing the name of the key in the config file for this\n            config option. They are searched in order. If it is None it will\n            not be added to the chain.\n\n        :type default: Any\n        :param default: Any constant value to be returned.\n\n        :type conversion_func: None or callable\n        :param conversion_func: If this value is None then it has no effect on\n            the return type. Otherwise, it is treated as a function that will\n            conversion_func our provided type.\n\n        :rvalue: ConfigChain\n        :returns: A ConfigChain that resolves in the order env_var_names ->\n            config_property_name -> default. Any values that were none are\n            omitted form the chain.\n        \"\"\"\n        providers = []\n        if instance_name is not None:\n            providers.append(\n                InstanceVarProvider(\n                    instance_var=instance_name, session=self._session\n                )\n            )\n        if env_var_names is not None:\n            providers.extend(self._get_env_providers(env_var_names))\n        if config_property_names is not None:\n            providers.extend(\n                self._get_scoped_config_providers(config_property_names)\n            )\n        if default is not None:\n            providers.append(ConstantProvider(value=default))\n\n        return ChainProvider(\n            providers=providers,\n            conversion_func=conversion_func,\n        )\n\n    def _get_env_providers(self, env_var_names):\n        env_var_providers = []\n        if not isinstance(env_var_names, list):\n            env_var_names = [env_var_names]\n        for env_var_name in env_var_names:\n            env_var_providers.append(\n                EnvironmentProvider(name=env_var_name, env=self._environ)\n            )\n        return env_var_providers\n\n    def _get_scoped_config_providers(self, config_property_names):\n        scoped_config_providers = []\n        if not isinstance(config_property_names, list):\n            config_property_names = [config_property_names]\n        for config_property_name in config_property_names:\n            scoped_config_providers.append(\n                ScopedConfigProvider(\n                    config_var_name=config_property_name,\n                    session=self._session,\n                )\n            )\n        return scoped_config_providers\n\n\nclass ConfigValueStore:\n    \"\"\"The ConfigValueStore object stores configuration values.\"\"\"\n\n    def __init__(self, mapping=None):\n        \"\"\"Initialize a ConfigValueStore.\n\n        :type mapping: dict\n        :param mapping: The mapping parameter is a map of string to a subclass\n            of BaseProvider. When a config variable is asked for via the\n            get_config_variable method, the corresponding provider will be\n            invoked to load the value.\n        \"\"\"\n        self._overrides = {}\n        self._mapping = {}\n        if mapping is not None:\n            for logical_name, provider in mapping.items():\n                self.set_config_provider(logical_name, provider)\n\n    def __deepcopy__(self, memo):\n        config_store = ConfigValueStore(copy.deepcopy(self._mapping, memo))\n        for logical_name, override_value in self._overrides.items():\n            config_store.set_config_variable(logical_name, override_value)\n\n        return config_store\n\n    def __copy__(self):\n        config_store = ConfigValueStore(copy.copy(self._mapping))\n        for logical_name, override_value in self._overrides.items():\n            config_store.set_config_variable(logical_name, override_value)\n\n        return config_store\n\n    def get_config_variable(self, logical_name):\n        \"\"\"\n        Retrieve the value associeated with the specified logical_name\n        from the corresponding provider. If no value is found None will\n        be returned.\n\n        :type logical_name: str\n        :param logical_name: The logical name of the session variable\n            you want to retrieve.  This name will be mapped to the\n            appropriate environment variable name for this session as\n            well as the appropriate config file entry.\n\n        :returns: value of variable or None if not defined.\n        \"\"\"\n        if logical_name in self._overrides:\n            return self._overrides[logical_name]\n        if logical_name not in self._mapping:\n            return None\n        provider = self._mapping[logical_name]\n        return provider.provide()\n\n    def get_config_provider(self, logical_name):\n        \"\"\"\n        Retrieve the provider associated with the specified logical_name.\n        If no provider is found None will be returned.\n\n        :type logical_name: str\n        :param logical_name: The logical name of the session variable\n            you want to retrieve.  This name will be mapped to the\n            appropriate environment variable name for this session as\n            well as the appropriate config file entry.\n\n        :returns: configuration provider or None if not defined.\n        \"\"\"\n        if (\n            logical_name in self._overrides\n            or logical_name not in self._mapping\n        ):\n            return None\n        provider = self._mapping[logical_name]\n        return provider\n\n    def set_config_variable(self, logical_name, value):\n        \"\"\"Set a configuration variable to a specific value.\n\n        By using this method, you can override the normal lookup\n        process used in ``get_config_variable`` by explicitly setting\n        a value.  Subsequent calls to ``get_config_variable`` will\n        use the ``value``.  This gives you per-session specific\n        configuration values.\n\n        ::\n            >>> # Assume logical name 'foo' maps to env var 'FOO'\n            >>> os.environ['FOO'] = 'myvalue'\n            >>> s.get_config_variable('foo')\n            'myvalue'\n            >>> s.set_config_variable('foo', 'othervalue')\n            >>> s.get_config_variable('foo')\n            'othervalue'\n\n        :type logical_name: str\n        :param logical_name: The logical name of the session variable\n            you want to set.  These are the keys in ``SESSION_VARIABLES``.\n\n        :param value: The value to associate with the config variable.\n        \"\"\"\n        self._overrides[logical_name] = value\n\n    def clear_config_variable(self, logical_name):\n        \"\"\"Remove an override config variable from the session.\n\n        :type logical_name: str\n        :param logical_name: The name of the parameter to clear the override\n            value from.\n        \"\"\"\n        self._overrides.pop(logical_name, None)\n\n    def set_config_provider(self, logical_name, provider):\n        \"\"\"Set the provider for a config value.\n\n        This provides control over how a particular configuration value is\n        loaded. This replaces the provider for ``logical_name`` with the new\n        ``provider``.\n\n        :type logical_name: str\n        :param logical_name: The name of the config value to change the config\n            provider for.\n\n        :type provider: :class:`botocore.configprovider.BaseProvider`\n        :param provider: The new provider that should be responsible for\n            providing a value for the config named ``logical_name``.\n        \"\"\"\n        self._mapping[logical_name] = provider\n\n\nclass SmartDefaultsConfigStoreFactory:\n    def __init__(self, default_config_resolver, imds_region_provider):\n        self._default_config_resolver = default_config_resolver\n        self._imds_region_provider = imds_region_provider\n        # Initializing _instance_metadata_region as None so we\n        # can fetch region in a lazy fashion only when needed.\n        self._instance_metadata_region = None\n\n    def merge_smart_defaults(self, config_store, mode, region_name):\n        if mode == 'auto':\n            mode = self.resolve_auto_mode(region_name)\n        default_configs = (\n            self._default_config_resolver.get_default_config_values(mode)\n        )\n        for config_var in default_configs:\n            config_value = default_configs[config_var]\n            method = getattr(self, f'_set_{config_var}', None)\n            if method:\n                method(config_store, config_value)\n\n    def resolve_auto_mode(self, region_name):\n        current_region = None\n        if os.environ.get('AWS_EXECUTION_ENV'):\n            default_region = os.environ.get('AWS_DEFAULT_REGION')\n            current_region = os.environ.get('AWS_REGION', default_region)\n        if not current_region:\n            if self._instance_metadata_region:\n                current_region = self._instance_metadata_region\n            else:\n                try:\n                    current_region = self._imds_region_provider.provide()\n                    self._instance_metadata_region = current_region\n                except Exception:\n                    pass\n\n        if current_region:\n            if region_name == current_region:\n                return 'in-region'\n            else:\n                return 'cross-region'\n        return 'standard'\n\n    def _update_provider(self, config_store, variable, value):\n        original_provider = config_store.get_config_provider(variable)\n        default_provider = ConstantProvider(value)\n        if isinstance(original_provider, ChainProvider):\n            chain_provider_copy = copy.deepcopy(original_provider)\n            chain_provider_copy.set_default_provider(default_provider)\n            default_provider = chain_provider_copy\n        elif isinstance(original_provider, BaseProvider):\n            default_provider = ChainProvider(\n                providers=[original_provider, default_provider]\n            )\n        config_store.set_config_provider(variable, default_provider)\n\n    def _update_section_provider(\n        self, config_store, section_name, variable, value\n    ):\n        section_provider_copy = copy.deepcopy(\n            config_store.get_config_provider(section_name)\n        )\n        section_provider_copy.set_default_provider(\n            variable, ConstantProvider(value)\n        )\n        config_store.set_config_provider(section_name, section_provider_copy)\n\n    def _set_retryMode(self, config_store, value):\n        self._update_provider(config_store, 'retry_mode', value)\n\n    def _set_stsRegionalEndpoints(self, config_store, value):\n        self._update_provider(config_store, 'sts_regional_endpoints', value)\n\n    def _set_s3UsEast1RegionalEndpoints(self, config_store, value):\n        self._update_section_provider(\n            config_store, 's3', 'us_east_1_regional_endpoint', value\n        )\n\n    def _set_connectTimeoutInMillis(self, config_store, value):\n        self._update_provider(config_store, 'connect_timeout', value / 1000)\n\n\nclass BaseProvider:\n    \"\"\"Base class for configuration value providers.\n\n    A configuration provider has some method of providing a configuration\n    value.\n    \"\"\"\n\n    def provide(self):\n        \"\"\"Provide a config value.\"\"\"\n        raise NotImplementedError('provide')\n\n\nclass ChainProvider(BaseProvider):\n    \"\"\"This provider wraps one or more other providers.\n\n    Each provider in the chain is called, the first one returning a non-None\n    value is then returned.\n    \"\"\"\n\n    def __init__(self, providers=None, conversion_func=None):\n        \"\"\"Initalize a ChainProvider.\n\n        :type providers: list\n        :param providers: The initial list of providers to check for values\n            when invoked.\n\n        :type conversion_func: None or callable\n        :param conversion_func: If this value is None then it has no affect on\n            the return type. Otherwise, it is treated as a function that will\n            transform provided value.\n        \"\"\"\n        if providers is None:\n            providers = []\n        self._providers = providers\n        self._conversion_func = conversion_func\n\n    def __deepcopy__(self, memo):\n        return ChainProvider(\n            copy.deepcopy(self._providers, memo), self._conversion_func\n        )\n\n    def provide(self):\n        \"\"\"Provide the value from the first provider to return non-None.\n\n        Each provider in the chain has its provide method called. The first\n        one in the chain to return a non-None value is the returned from the\n        ChainProvider. When no non-None value is found, None is returned.\n        \"\"\"\n        for provider in self._providers:\n            value = provider.provide()\n            if value is not None:\n                return self._convert_type(value)\n        return None\n\n    def set_default_provider(self, default_provider):\n        if self._providers and isinstance(\n            self._providers[-1], ConstantProvider\n        ):\n            self._providers[-1] = default_provider\n        else:\n            self._providers.append(default_provider)\n\n        num_of_constants = sum(\n            isinstance(provider, ConstantProvider)\n            for provider in self._providers\n        )\n        if num_of_constants > 1:\n            logger.info(\n                'ChainProvider object contains multiple '\n                'instances of ConstantProvider objects'\n            )\n\n    def _convert_type(self, value):\n        if self._conversion_func is not None:\n            return self._conversion_func(value)\n        return value\n\n    def __repr__(self):\n        return '[%s]' % ', '.join([str(p) for p in self._providers])\n\n\nclass InstanceVarProvider(BaseProvider):\n    \"\"\"This class loads config values from the session instance vars.\"\"\"\n\n    def __init__(self, instance_var, session):\n        \"\"\"Initialize InstanceVarProvider.\n\n        :type instance_var: str\n        :param instance_var: The instance variable to load from the session.\n\n        :type session: :class:`botocore.session.Session`\n        :param session: The botocore session to get the loaded configuration\n            file variables from.\n        \"\"\"\n        self._instance_var = instance_var\n        self._session = session\n\n    def __deepcopy__(self, memo):\n        return InstanceVarProvider(\n            copy.deepcopy(self._instance_var, memo), self._session\n        )\n\n    def provide(self):\n        \"\"\"Provide a config value from the session instance vars.\"\"\"\n        instance_vars = self._session.instance_variables()\n        value = instance_vars.get(self._instance_var)\n        return value\n\n    def __repr__(self):\n        return 'InstanceVarProvider(instance_var={}, session={})'.format(\n            self._instance_var,\n            self._session,\n        )\n\n\nclass ScopedConfigProvider(BaseProvider):\n    def __init__(self, config_var_name, session):\n        \"\"\"Initialize ScopedConfigProvider.\n\n        :type config_var_name: str or tuple\n        :param config_var_name: The name of the config variable to load from\n            the configuration file. If the value is a tuple, it must only\n            consist of two items, where the first item represents the section\n            and the second item represents the config var name in the section.\n\n        :type session: :class:`botocore.session.Session`\n        :param session: The botocore session to get the loaded configuration\n            file variables from.\n        \"\"\"\n        self._config_var_name = config_var_name\n        self._session = session\n\n    def __deepcopy__(self, memo):\n        return ScopedConfigProvider(\n            copy.deepcopy(self._config_var_name, memo), self._session\n        )\n\n    def provide(self):\n        \"\"\"Provide a value from a config file property.\"\"\"\n        scoped_config = self._session.get_scoped_config()\n        if isinstance(self._config_var_name, tuple):\n            section_config = scoped_config.get(self._config_var_name[0])\n            if not isinstance(section_config, dict):\n                return None\n            return section_config.get(self._config_var_name[1])\n        return scoped_config.get(self._config_var_name)\n\n    def __repr__(self):\n        return 'ScopedConfigProvider(config_var_name={}, session={})'.format(\n            self._config_var_name,\n            self._session,\n        )\n\n\nclass EnvironmentProvider(BaseProvider):\n    \"\"\"This class loads config values from environment variables.\"\"\"\n\n    def __init__(self, name, env):\n        \"\"\"Initialize with the keys in the dictionary to check.\n\n        :type name: str\n        :param name: The key with that name will be loaded and returned.\n\n        :type env: dict\n        :param env: Environment variables dictionary to get variables from.\n        \"\"\"\n        self._name = name\n        self._env = env\n\n    def __deepcopy__(self, memo):\n        return EnvironmentProvider(\n            copy.deepcopy(self._name, memo), copy.deepcopy(self._env, memo)\n        )\n\n    def provide(self):\n        \"\"\"Provide a config value from a source dictionary.\"\"\"\n        if self._name in self._env:\n            return self._env[self._name]\n        return None\n\n    def __repr__(self):\n        return f'EnvironmentProvider(name={self._name}, env={self._env})'\n\n\nclass SectionConfigProvider(BaseProvider):\n    \"\"\"Provides a dictionary from a section in the scoped config\n\n    This is useful for retrieving scoped config variables (i.e. s3) that have\n    their own set of config variables and resolving logic.\n    \"\"\"\n\n    def __init__(self, section_name, session, override_providers=None):\n        self._section_name = section_name\n        self._session = session\n        self._scoped_config_provider = ScopedConfigProvider(\n            self._section_name, self._session\n        )\n        self._override_providers = override_providers\n        if self._override_providers is None:\n            self._override_providers = {}\n\n    def __deepcopy__(self, memo):\n        return SectionConfigProvider(\n            copy.deepcopy(self._section_name, memo),\n            self._session,\n            copy.deepcopy(self._override_providers, memo),\n        )\n\n    def provide(self):\n        section_config = self._scoped_config_provider.provide()\n        if section_config and not isinstance(section_config, dict):\n            logger.debug(\n                \"The %s config key is not a dictionary type, \"\n                \"ignoring its value of: %s\",\n                self._section_name,\n                section_config,\n            )\n            return None\n        for section_config_var, provider in self._override_providers.items():\n            provider_val = provider.provide()\n            if provider_val is not None:\n                if section_config is None:\n                    section_config = {}\n                section_config[section_config_var] = provider_val\n        return section_config\n\n    def set_default_provider(self, key, default_provider):\n        provider = self._override_providers.get(key)\n        if isinstance(provider, ChainProvider):\n            provider.set_default_provider(default_provider)\n            return\n        elif isinstance(provider, BaseProvider):\n            default_provider = ChainProvider(\n                providers=[provider, default_provider]\n            )\n        self._override_providers[key] = default_provider\n\n    def __repr__(self):\n        return (\n            f'SectionConfigProvider(section_name={self._section_name}, '\n            f'session={self._session}, '\n            f'override_providers={self._override_providers})'\n        )\n\n\nclass ConstantProvider(BaseProvider):\n    \"\"\"This provider provides a constant value.\"\"\"\n\n    def __init__(self, value):\n        self._value = value\n\n    def __deepcopy__(self, memo):\n        return ConstantProvider(copy.deepcopy(self._value, memo))\n\n    def provide(self):\n        \"\"\"Provide the constant value given during initialization.\"\"\"\n        return self._value\n\n    def __repr__(self):\n        return 'ConstantProvider(value=%s)' % self._value\n\n\nclass ConfiguredEndpointProvider(BaseProvider):\n    \"\"\"Lookup an endpoint URL from environment variable or shared config file.\n\n    NOTE: This class is considered private and is subject to abrupt breaking\n    changes or removal without prior announcement. Please do not use it\n    directly.\n    \"\"\"\n\n    _ENDPOINT_URL_LOOKUP_ORDER = [\n        'environment_service',\n        'environment_global',\n        'config_service',\n        'config_global',\n    ]\n\n    def __init__(\n        self,\n        full_config,\n        scoped_config,\n        client_name,\n        environ=None,\n    ):\n        \"\"\"Initialize a ConfiguredEndpointProviderChain.\n\n        :type full_config: dict\n        :param full_config: This is the dict representing the full\n            configuration file.\n\n        :type scoped_config: dict\n        :param scoped_config: This is the dict representing the configuration\n            for the current profile for the session.\n\n        :type client_name: str\n        :param client_name: The name used to instantiate a client using\n            botocore.session.Session.create_client.\n\n        :type environ: dict\n        :param environ: A mapping to use for environment variables. If this\n            is not provided it will default to use os.environ.\n        \"\"\"\n        self._full_config = full_config\n        self._scoped_config = scoped_config\n        self._client_name = client_name\n        self._transformed_service_id = self._get_snake_case_service_id(\n            self._client_name\n        )\n        if environ is None:\n            environ = os.environ\n        self._environ = environ\n\n    def provide(self):\n        \"\"\"Lookup the configured endpoint URL.\n\n        The order is:\n\n        1. The value provided by a service-specific environment variable.\n        2. The value provided by the global endpoint environment variable\n           (AWS_ENDPOINT_URL).\n        3. The value provided by a service-specific parameter from a services\n           definition section in the shared configuration file.\n        4. The value provided by the global parameter from a services\n           definition section in the shared configuration file.\n        \"\"\"\n        for location in self._ENDPOINT_URL_LOOKUP_ORDER:\n            logger.debug(\n                'Looking for endpoint for %s via: %s',\n                self._client_name,\n                location,\n            )\n\n            endpoint_url = getattr(self, f'_get_endpoint_url_{location}')()\n\n            if endpoint_url:\n                logger.info(\n                    'Found endpoint for %s via: %s.',\n                    self._client_name,\n                    location,\n                )\n                return endpoint_url\n\n        logger.debug('No configured endpoint found.')\n        return None\n\n    def _get_snake_case_service_id(self, client_name):\n        # Get the service ID without loading the service data file, accounting\n        # for any aliases and standardizing the names with hyphens.\n        client_name = utils.SERVICE_NAME_ALIASES.get(client_name, client_name)\n        hyphenized_service_id = (\n            utils.CLIENT_NAME_TO_HYPHENIZED_SERVICE_ID_OVERRIDES.get(\n                client_name, client_name\n            )\n        )\n        return hyphenized_service_id.replace('-', '_')\n\n    def _get_service_env_var_name(self):\n        transformed_service_id_env = self._transformed_service_id.upper()\n        return f'AWS_ENDPOINT_URL_{transformed_service_id_env}'\n\n    def _get_services_config(self):\n        if 'services' not in self._scoped_config:\n            return {}\n\n        section_name = self._scoped_config['services']\n        services_section = self._full_config.get('services', {}).get(\n            section_name\n        )\n\n        if not services_section:\n            error_msg = (\n                f'The profile is configured to use the services '\n                f'section but the \"{section_name}\" services '\n                f'configuration does not exist.'\n            )\n            raise InvalidConfigError(error_msg=error_msg)\n\n        return services_section\n\n    def _get_endpoint_url_config_service(self):\n        snakecase_service_id = self._transformed_service_id.lower()\n        return (\n            self._get_services_config()\n            .get(snakecase_service_id, {})\n            .get('endpoint_url')\n        )\n\n    def _get_endpoint_url_config_global(self):\n        return self._scoped_config.get('endpoint_url')\n\n    def _get_endpoint_url_environment_service(self):\n        return EnvironmentProvider(\n            name=self._get_service_env_var_name(), env=self._environ\n        ).provide()\n\n    def _get_endpoint_url_environment_global(self):\n        return EnvironmentProvider(\n            name='AWS_ENDPOINT_URL', env=self._environ\n        ).provide()\n", "botocore/client.py": "# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport logging\n\nfrom botocore import waiter, xform_name\nfrom botocore.args import ClientArgsCreator\nfrom botocore.auth import AUTH_TYPE_MAPS\nfrom botocore.awsrequest import prepare_request_dict\nfrom botocore.compress import maybe_compress_request\nfrom botocore.config import Config\nfrom botocore.credentials import RefreshableCredentials\nfrom botocore.discovery import (\n    EndpointDiscoveryHandler,\n    EndpointDiscoveryManager,\n    block_endpoint_discovery_required_operations,\n)\nfrom botocore.docs.docstring import ClientMethodDocstring, PaginatorDocstring\nfrom botocore.exceptions import (\n    DataNotFoundError,\n    InvalidEndpointDiscoveryConfigurationError,\n    OperationNotPageableError,\n    UnknownServiceError,\n    UnknownSignatureVersionError,\n)\nfrom botocore.history import get_global_history_recorder\nfrom botocore.hooks import first_non_none_response\nfrom botocore.httpchecksum import (\n    apply_request_checksum,\n    resolve_checksum_context,\n)\nfrom botocore.model import ServiceModel\nfrom botocore.paginate import Paginator\nfrom botocore.retries import adaptive, standard\nfrom botocore.useragent import UserAgentString\nfrom botocore.utils import (\n    CachedProperty,\n    EventbridgeSignerSetter,\n    S3ControlArnParamHandlerv2,\n    S3ExpressIdentityResolver,\n    S3RegionRedirectorv2,\n    ensure_boolean,\n    get_service_module_name,\n)\n\n# Keep these imported.  There's pre-existing code that uses:\n# \"from botocore.client import UNSIGNED\"\n# \"from botocore.client import ClientError\"\n# etc.\nfrom botocore.exceptions import ClientError  # noqa\nfrom botocore.utils import S3ArnParamHandler  # noqa\nfrom botocore.utils import S3ControlArnParamHandler  # noqa\nfrom botocore.utils import S3ControlEndpointSetter  # noqa\nfrom botocore.utils import S3EndpointSetter  # noqa\nfrom botocore.utils import S3RegionRedirector  # noqa\nfrom botocore import UNSIGNED  # noqa\n\n\n_LEGACY_SIGNATURE_VERSIONS = frozenset(\n    (\n        'v2',\n        'v3',\n        'v3https',\n        'v4',\n        's3',\n        's3v4',\n    )\n)\n\n\nlogger = logging.getLogger(__name__)\nhistory_recorder = get_global_history_recorder()\n\n\nclass ClientCreator:\n    \"\"\"Creates client objects for a service.\"\"\"\n\n    def __init__(\n        self,\n        loader,\n        endpoint_resolver,\n        user_agent,\n        event_emitter,\n        retry_handler_factory,\n        retry_config_translator,\n        response_parser_factory=None,\n        exceptions_factory=None,\n        config_store=None,\n        user_agent_creator=None,\n    ):\n        self._loader = loader\n        self._endpoint_resolver = endpoint_resolver\n        self._user_agent = user_agent\n        self._event_emitter = event_emitter\n        self._retry_handler_factory = retry_handler_factory\n        self._retry_config_translator = retry_config_translator\n        self._response_parser_factory = response_parser_factory\n        self._exceptions_factory = exceptions_factory\n        # TODO: Migrate things away from scoped_config in favor of the\n        # config_store.  The config store can pull things from both the scoped\n        # config and environment variables (and potentially more in the\n        # future).\n        self._config_store = config_store\n        self._user_agent_creator = user_agent_creator\n\n    def create_client(\n        self,\n        service_name,\n        region_name,\n        is_secure=True,\n        endpoint_url=None,\n        verify=None,\n        credentials=None,\n        scoped_config=None,\n        api_version=None,\n        client_config=None,\n        auth_token=None,\n    ):\n        responses = self._event_emitter.emit(\n            'choose-service-name', service_name=service_name\n        )\n        service_name = first_non_none_response(responses, default=service_name)\n        service_model = self._load_service_model(service_name, api_version)\n        try:\n            endpoints_ruleset_data = self._load_service_endpoints_ruleset(\n                service_name, api_version\n            )\n            partition_data = self._loader.load_data('partitions')\n        except UnknownServiceError:\n            endpoints_ruleset_data = None\n            partition_data = None\n            logger.info(\n                'No endpoints ruleset found for service %s, falling back to '\n                'legacy endpoint routing.',\n                service_name,\n            )\n\n        cls = self._create_client_class(service_name, service_model)\n        region_name, client_config = self._normalize_fips_region(\n            region_name, client_config\n        )\n        endpoint_bridge = ClientEndpointBridge(\n            self._endpoint_resolver,\n            scoped_config,\n            client_config,\n            service_signing_name=service_model.metadata.get('signingName'),\n            config_store=self._config_store,\n            service_signature_version=service_model.metadata.get(\n                'signatureVersion'\n            ),\n        )\n        client_args = self._get_client_args(\n            service_model,\n            region_name,\n            is_secure,\n            endpoint_url,\n            verify,\n            credentials,\n            scoped_config,\n            client_config,\n            endpoint_bridge,\n            auth_token,\n            endpoints_ruleset_data,\n            partition_data,\n        )\n        service_client = cls(**client_args)\n        self._register_retries(service_client)\n        self._register_s3_events(\n            client=service_client,\n            endpoint_bridge=None,\n            endpoint_url=None,\n            client_config=client_config,\n            scoped_config=scoped_config,\n        )\n        self._register_s3express_events(client=service_client)\n        self._register_s3_control_events(client=service_client)\n        self._register_endpoint_discovery(\n            service_client, endpoint_url, client_config\n        )\n        return service_client\n\n    def create_client_class(self, service_name, api_version=None):\n        service_model = self._load_service_model(service_name, api_version)\n        return self._create_client_class(service_name, service_model)\n\n    def _create_client_class(self, service_name, service_model):\n        class_attributes = self._create_methods(service_model)\n        py_name_to_operation_name = self._create_name_mapping(service_model)\n        class_attributes['_PY_TO_OP_NAME'] = py_name_to_operation_name\n        bases = [BaseClient]\n        service_id = service_model.service_id.hyphenize()\n        self._event_emitter.emit(\n            'creating-client-class.%s' % service_id,\n            class_attributes=class_attributes,\n            base_classes=bases,\n        )\n        class_name = get_service_module_name(service_model)\n        cls = type(str(class_name), tuple(bases), class_attributes)\n        return cls\n\n    def _normalize_fips_region(self, region_name, client_config):\n        if region_name is not None:\n            normalized_region_name = region_name.replace('fips-', '').replace(\n                '-fips', ''\n            )\n            # If region has been transformed then set flag\n            if normalized_region_name != region_name:\n                config_use_fips_endpoint = Config(use_fips_endpoint=True)\n                if client_config:\n                    # Keeping endpoint setting client specific\n                    client_config = client_config.merge(\n                        config_use_fips_endpoint\n                    )\n                else:\n                    client_config = config_use_fips_endpoint\n                logger.warning(\n                    'transforming region from %s to %s and setting '\n                    'use_fips_endpoint to true. client should not '\n                    'be configured with a fips psuedo region.'\n                    % (region_name, normalized_region_name)\n                )\n                region_name = normalized_region_name\n        return region_name, client_config\n\n    def _load_service_model(self, service_name, api_version=None):\n        json_model = self._loader.load_service_model(\n            service_name, 'service-2', api_version=api_version\n        )\n        service_model = ServiceModel(json_model, service_name=service_name)\n        return service_model\n\n    def _load_service_endpoints_ruleset(self, service_name, api_version=None):\n        return self._loader.load_service_model(\n            service_name, 'endpoint-rule-set-1', api_version=api_version\n        )\n\n    def _register_retries(self, client):\n        retry_mode = client.meta.config.retries['mode']\n        if retry_mode == 'standard':\n            self._register_v2_standard_retries(client)\n        elif retry_mode == 'adaptive':\n            self._register_v2_standard_retries(client)\n            self._register_v2_adaptive_retries(client)\n        elif retry_mode == 'legacy':\n            self._register_legacy_retries(client)\n\n    def _register_v2_standard_retries(self, client):\n        max_attempts = client.meta.config.retries.get('total_max_attempts')\n        kwargs = {'client': client}\n        if max_attempts is not None:\n            kwargs['max_attempts'] = max_attempts\n        standard.register_retry_handler(**kwargs)\n\n    def _register_v2_adaptive_retries(self, client):\n        adaptive.register_retry_handler(client)\n\n    def _register_legacy_retries(self, client):\n        endpoint_prefix = client.meta.service_model.endpoint_prefix\n        service_id = client.meta.service_model.service_id\n        service_event_name = service_id.hyphenize()\n\n        # First, we load the entire retry config for all services,\n        # then pull out just the information we need.\n        original_config = self._loader.load_data('_retry')\n        if not original_config:\n            return\n\n        retries = self._transform_legacy_retries(client.meta.config.retries)\n        retry_config = self._retry_config_translator.build_retry_config(\n            endpoint_prefix,\n            original_config.get('retry', {}),\n            original_config.get('definitions', {}),\n            retries,\n        )\n\n        logger.debug(\n            \"Registering retry handlers for service: %s\",\n            client.meta.service_model.service_name,\n        )\n        handler = self._retry_handler_factory.create_retry_handler(\n            retry_config, endpoint_prefix\n        )\n        unique_id = 'retry-config-%s' % service_event_name\n        client.meta.events.register(\n            f\"needs-retry.{service_event_name}\", handler, unique_id=unique_id\n        )\n\n    def _transform_legacy_retries(self, retries):\n        if retries is None:\n            return\n        copied_args = retries.copy()\n        if 'total_max_attempts' in retries:\n            copied_args = retries.copy()\n            copied_args['max_attempts'] = (\n                copied_args.pop('total_max_attempts') - 1\n            )\n        return copied_args\n\n    def _get_retry_mode(self, client, config_store):\n        client_retries = client.meta.config.retries\n        if (\n            client_retries is not None\n            and client_retries.get('mode') is not None\n        ):\n            return client_retries['mode']\n        return config_store.get_config_variable('retry_mode') or 'legacy'\n\n    def _register_endpoint_discovery(self, client, endpoint_url, config):\n        if endpoint_url is not None:\n            # Don't register any handlers in the case of a custom endpoint url\n            return\n        # Only attach handlers if the service supports discovery\n        if client.meta.service_model.endpoint_discovery_operation is None:\n            return\n        events = client.meta.events\n        service_id = client.meta.service_model.service_id.hyphenize()\n        enabled = False\n        if config and config.endpoint_discovery_enabled is not None:\n            enabled = config.endpoint_discovery_enabled\n        elif self._config_store:\n            enabled = self._config_store.get_config_variable(\n                'endpoint_discovery_enabled'\n            )\n\n        enabled = self._normalize_endpoint_discovery_config(enabled)\n        if enabled and self._requires_endpoint_discovery(client, enabled):\n            discover = enabled is True\n            manager = EndpointDiscoveryManager(\n                client, always_discover=discover\n            )\n            handler = EndpointDiscoveryHandler(manager)\n            handler.register(events, service_id)\n        else:\n            events.register(\n                'before-parameter-build',\n                block_endpoint_discovery_required_operations,\n            )\n\n    def _normalize_endpoint_discovery_config(self, enabled):\n        \"\"\"Config must either be a boolean-string or string-literal 'auto'\"\"\"\n        if isinstance(enabled, str):\n            enabled = enabled.lower().strip()\n            if enabled == 'auto':\n                return enabled\n            elif enabled in ('true', 'false'):\n                return ensure_boolean(enabled)\n        elif isinstance(enabled, bool):\n            return enabled\n\n        raise InvalidEndpointDiscoveryConfigurationError(config_value=enabled)\n\n    def _requires_endpoint_discovery(self, client, enabled):\n        if enabled == \"auto\":\n            return client.meta.service_model.endpoint_discovery_required\n        return enabled\n\n    def _register_eventbridge_events(\n        self, client, endpoint_bridge, endpoint_url\n    ):\n        if client.meta.service_model.service_name != 'events':\n            return\n        EventbridgeSignerSetter(\n            endpoint_resolver=self._endpoint_resolver,\n            region=client.meta.region_name,\n            endpoint_url=endpoint_url,\n        ).register(client.meta.events)\n\n    def _register_s3express_events(\n        self,\n        client,\n        endpoint_bridge=None,\n        endpoint_url=None,\n        client_config=None,\n        scoped_config=None,\n    ):\n        if client.meta.service_model.service_name != 's3':\n            return\n        S3ExpressIdentityResolver(client, RefreshableCredentials).register()\n\n    def _register_s3_events(\n        self,\n        client,\n        endpoint_bridge,\n        endpoint_url,\n        client_config,\n        scoped_config,\n    ):\n        if client.meta.service_model.service_name != 's3':\n            return\n        S3RegionRedirectorv2(None, client).register()\n        self._set_s3_presign_signature_version(\n            client.meta, client_config, scoped_config\n        )\n        client.meta.events.register(\n            'before-parameter-build.s3', self._inject_s3_input_parameters\n        )\n\n    def _register_s3_control_events(\n        self,\n        client,\n        endpoint_bridge=None,\n        endpoint_url=None,\n        client_config=None,\n        scoped_config=None,\n    ):\n        if client.meta.service_model.service_name != 's3control':\n            return\n        S3ControlArnParamHandlerv2().register(client.meta.events)\n\n    def _set_s3_presign_signature_version(\n        self, client_meta, client_config, scoped_config\n    ):\n        # This will return the manually configured signature version, or None\n        # if none was manually set. If a customer manually sets the signature\n        # version, we always want to use what they set.\n        provided_signature_version = _get_configured_signature_version(\n            's3', client_config, scoped_config\n        )\n        if provided_signature_version is not None:\n            return\n\n        # Check to see if the region is a region that we know about. If we\n        # don't know about a region, then we can safely assume it's a new\n        # region that is sigv4 only, since all new S3 regions only allow sigv4.\n        # The only exception is aws-global. This is a pseudo-region for the\n        # global endpoint, we should respect the signature versions it\n        # supports, which includes v2.\n        regions = self._endpoint_resolver.get_available_endpoints(\n            's3', client_meta.partition\n        )\n        if (\n            client_meta.region_name != 'aws-global'\n            and client_meta.region_name not in regions\n        ):\n            return\n\n        # If it is a region we know about, we want to default to sigv2, so here\n        # we check to see if it is available.\n        endpoint = self._endpoint_resolver.construct_endpoint(\n            's3', client_meta.region_name\n        )\n        signature_versions = endpoint['signatureVersions']\n        if 's3' not in signature_versions:\n            return\n\n        # We now know that we're in a known region that supports sigv2 and\n        # the customer hasn't set a signature version so we default the\n        # signature version to sigv2.\n        client_meta.events.register(\n            'choose-signer.s3', self._default_s3_presign_to_sigv2\n        )\n\n    def _inject_s3_input_parameters(self, params, context, **kwargs):\n        context['input_params'] = {}\n        inject_parameters = ('Bucket', 'Delete', 'Key', 'Prefix')\n        for inject_parameter in inject_parameters:\n            if inject_parameter in params:\n                context['input_params'][inject_parameter] = params[\n                    inject_parameter\n                ]\n\n    def _default_s3_presign_to_sigv2(self, signature_version, **kwargs):\n        \"\"\"\n        Returns the 's3' (sigv2) signer if presigning an s3 request. This is\n        intended to be used to set the default signature version for the signer\n        to sigv2. Situations where an asymmetric signature is required are the\n        exception, for example MRAP needs v4a.\n\n        :type signature_version: str\n        :param signature_version: The current client signature version.\n\n        :type signing_name: str\n        :param signing_name: The signing name of the service.\n\n        :return: 's3' if the request is an s3 presign request, None otherwise\n        \"\"\"\n        if signature_version.startswith('v4a'):\n            return\n\n        if signature_version.startswith('v4-s3express'):\n            return f'{signature_version}'\n\n        for suffix in ['-query', '-presign-post']:\n            if signature_version.endswith(suffix):\n                return f's3{suffix}'\n\n    def _get_client_args(\n        self,\n        service_model,\n        region_name,\n        is_secure,\n        endpoint_url,\n        verify,\n        credentials,\n        scoped_config,\n        client_config,\n        endpoint_bridge,\n        auth_token,\n        endpoints_ruleset_data,\n        partition_data,\n    ):\n        args_creator = ClientArgsCreator(\n            self._event_emitter,\n            self._user_agent,\n            self._response_parser_factory,\n            self._loader,\n            self._exceptions_factory,\n            config_store=self._config_store,\n            user_agent_creator=self._user_agent_creator,\n        )\n        return args_creator.get_client_args(\n            service_model,\n            region_name,\n            is_secure,\n            endpoint_url,\n            verify,\n            credentials,\n            scoped_config,\n            client_config,\n            endpoint_bridge,\n            auth_token,\n            endpoints_ruleset_data,\n            partition_data,\n        )\n\n    def _create_methods(self, service_model):\n        op_dict = {}\n        for operation_name in service_model.operation_names:\n            py_operation_name = xform_name(operation_name)\n            op_dict[py_operation_name] = self._create_api_method(\n                py_operation_name, operation_name, service_model\n            )\n        return op_dict\n\n    def _create_name_mapping(self, service_model):\n        # py_name -> OperationName, for every operation available\n        # for a service.\n        mapping = {}\n        for operation_name in service_model.operation_names:\n            py_operation_name = xform_name(operation_name)\n            mapping[py_operation_name] = operation_name\n        return mapping\n\n    def _create_api_method(\n        self, py_operation_name, operation_name, service_model\n    ):\n        def _api_call(self, *args, **kwargs):\n            # We're accepting *args so that we can give a more helpful\n            # error message than TypeError: _api_call takes exactly\n            # 1 argument.\n            if args:\n                raise TypeError(\n                    f\"{py_operation_name}() only accepts keyword arguments.\"\n                )\n            # The \"self\" in this scope is referring to the BaseClient.\n            return self._make_api_call(operation_name, kwargs)\n\n        _api_call.__name__ = str(py_operation_name)\n\n        # Add the docstring to the client method\n        operation_model = service_model.operation_model(operation_name)\n        docstring = ClientMethodDocstring(\n            operation_model=operation_model,\n            method_name=operation_name,\n            event_emitter=self._event_emitter,\n            method_description=operation_model.documentation,\n            example_prefix='response = client.%s' % py_operation_name,\n            include_signature=False,\n        )\n        _api_call.__doc__ = docstring\n        return _api_call\n\n\nclass ClientEndpointBridge:\n    \"\"\"Bridges endpoint data and client creation\n\n    This class handles taking out the relevant arguments from the endpoint\n    resolver and determining which values to use, taking into account any\n    client configuration options and scope configuration options.\n\n    This class also handles determining what, if any, region to use if no\n    explicit region setting is provided. For example, Amazon S3 client will\n    utilize \"us-east-1\" by default if no region can be resolved.\"\"\"\n\n    DEFAULT_ENDPOINT = '{service}.{region}.amazonaws.com'\n    _DUALSTACK_CUSTOMIZED_SERVICES = ['s3', 's3-control']\n\n    def __init__(\n        self,\n        endpoint_resolver,\n        scoped_config=None,\n        client_config=None,\n        default_endpoint=None,\n        service_signing_name=None,\n        config_store=None,\n        service_signature_version=None,\n    ):\n        self.service_signing_name = service_signing_name\n        self.endpoint_resolver = endpoint_resolver\n        self.scoped_config = scoped_config\n        self.client_config = client_config\n        self.default_endpoint = default_endpoint or self.DEFAULT_ENDPOINT\n        self.config_store = config_store\n        self.service_signature_version = service_signature_version\n\n    def resolve(\n        self, service_name, region_name=None, endpoint_url=None, is_secure=True\n    ):\n        region_name = self._check_default_region(service_name, region_name)\n        use_dualstack_endpoint = self._resolve_use_dualstack_endpoint(\n            service_name\n        )\n        use_fips_endpoint = self._resolve_endpoint_variant_config_var(\n            'use_fips_endpoint'\n        )\n        resolved = self.endpoint_resolver.construct_endpoint(\n            service_name,\n            region_name,\n            use_dualstack_endpoint=use_dualstack_endpoint,\n            use_fips_endpoint=use_fips_endpoint,\n        )\n\n        # If we can't resolve the region, we'll attempt to get a global\n        # endpoint for non-regionalized services (iam, route53, etc)\n        if not resolved:\n            # TODO: fallback partition_name should be configurable in the\n            # future for users to define as needed.\n            resolved = self.endpoint_resolver.construct_endpoint(\n                service_name,\n                region_name,\n                partition_name='aws',\n                use_dualstack_endpoint=use_dualstack_endpoint,\n                use_fips_endpoint=use_fips_endpoint,\n            )\n\n        if resolved:\n            return self._create_endpoint(\n                resolved, service_name, region_name, endpoint_url, is_secure\n            )\n        else:\n            return self._assume_endpoint(\n                service_name, region_name, endpoint_url, is_secure\n            )\n\n    def resolver_uses_builtin_data(self):\n        return self.endpoint_resolver.uses_builtin_data\n\n    def _check_default_region(self, service_name, region_name):\n        if region_name is not None:\n            return region_name\n        # Use the client_config region if no explicit region was provided.\n        if self.client_config and self.client_config.region_name is not None:\n            return self.client_config.region_name\n\n    def _create_endpoint(\n        self, resolved, service_name, region_name, endpoint_url, is_secure\n    ):\n        region_name, signing_region = self._pick_region_values(\n            resolved, region_name, endpoint_url\n        )\n        if endpoint_url is None:\n            endpoint_url = self._make_url(\n                resolved.get('hostname'),\n                is_secure,\n                resolved.get('protocols', []),\n            )\n        signature_version = self._resolve_signature_version(\n            service_name, resolved\n        )\n        signing_name = self._resolve_signing_name(service_name, resolved)\n        return self._create_result(\n            service_name=service_name,\n            region_name=region_name,\n            signing_region=signing_region,\n            signing_name=signing_name,\n            endpoint_url=endpoint_url,\n            metadata=resolved,\n            signature_version=signature_version,\n        )\n\n    def _resolve_endpoint_variant_config_var(self, config_var):\n        client_config = self.client_config\n        config_val = False\n\n        # Client configuration arg has precedence\n        if client_config and getattr(client_config, config_var) is not None:\n            return getattr(client_config, config_var)\n        elif self.config_store is not None:\n            # Check config store\n            config_val = self.config_store.get_config_variable(config_var)\n        return config_val\n\n    def _resolve_use_dualstack_endpoint(self, service_name):\n        s3_dualstack_mode = self._is_s3_dualstack_mode(service_name)\n        if s3_dualstack_mode is not None:\n            return s3_dualstack_mode\n        return self._resolve_endpoint_variant_config_var(\n            'use_dualstack_endpoint'\n        )\n\n    def _is_s3_dualstack_mode(self, service_name):\n        if service_name not in self._DUALSTACK_CUSTOMIZED_SERVICES:\n            return None\n        # TODO: This normalization logic is duplicated from the\n        # ClientArgsCreator class.  Consolidate everything to\n        # ClientArgsCreator.  _resolve_signature_version also has similarly\n        # duplicated logic.\n        client_config = self.client_config\n        if (\n            client_config is not None\n            and client_config.s3 is not None\n            and 'use_dualstack_endpoint' in client_config.s3\n        ):\n            # Client config trumps scoped config.\n            return client_config.s3['use_dualstack_endpoint']\n        if self.scoped_config is not None:\n            enabled = self.scoped_config.get('s3', {}).get(\n                'use_dualstack_endpoint'\n            )\n            if enabled in [True, 'True', 'true']:\n                return True\n\n    def _assume_endpoint(\n        self, service_name, region_name, endpoint_url, is_secure\n    ):\n        if endpoint_url is None:\n            # Expand the default hostname URI template.\n            hostname = self.default_endpoint.format(\n                service=service_name, region=region_name\n            )\n            endpoint_url = self._make_url(\n                hostname, is_secure, ['http', 'https']\n            )\n        logger.debug(\n            f'Assuming an endpoint for {service_name}, {region_name}: {endpoint_url}'\n        )\n        # We still want to allow the user to provide an explicit version.\n        signature_version = self._resolve_signature_version(\n            service_name, {'signatureVersions': ['v4']}\n        )\n        signing_name = self._resolve_signing_name(service_name, resolved={})\n        return self._create_result(\n            service_name=service_name,\n            region_name=region_name,\n            signing_region=region_name,\n            signing_name=signing_name,\n            signature_version=signature_version,\n            endpoint_url=endpoint_url,\n            metadata={},\n        )\n\n    def _create_result(\n        self,\n        service_name,\n        region_name,\n        signing_region,\n        signing_name,\n        endpoint_url,\n        signature_version,\n        metadata,\n    ):\n        return {\n            'service_name': service_name,\n            'region_name': region_name,\n            'signing_region': signing_region,\n            'signing_name': signing_name,\n            'endpoint_url': endpoint_url,\n            'signature_version': signature_version,\n            'metadata': metadata,\n        }\n\n    def _make_url(self, hostname, is_secure, supported_protocols):\n        if is_secure and 'https' in supported_protocols:\n            scheme = 'https'\n        else:\n            scheme = 'http'\n        return f'{scheme}://{hostname}'\n\n    def _resolve_signing_name(self, service_name, resolved):\n        # CredentialScope overrides everything else.\n        if (\n            'credentialScope' in resolved\n            and 'service' in resolved['credentialScope']\n        ):\n            return resolved['credentialScope']['service']\n        # Use the signingName from the model if present.\n        if self.service_signing_name:\n            return self.service_signing_name\n        # Just assume is the same as the service name.\n        return service_name\n\n    def _pick_region_values(self, resolved, region_name, endpoint_url):\n        signing_region = region_name\n        if endpoint_url is None:\n            # Do not use the region name or signing name from the resolved\n            # endpoint if the user explicitly provides an endpoint_url. This\n            # would happen if we resolve to an endpoint where the service has\n            # a \"defaults\" section that overrides all endpoint with a single\n            # hostname and credentialScope. This has been the case historically\n            # for how STS has worked. The only way to resolve an STS endpoint\n            # was to provide a region_name and an endpoint_url. In that case,\n            # we would still resolve an endpoint, but we would not use the\n            # resolved endpointName or signingRegion because we want to allow\n            # custom endpoints.\n            region_name = resolved['endpointName']\n            signing_region = region_name\n            if (\n                'credentialScope' in resolved\n                and 'region' in resolved['credentialScope']\n            ):\n                signing_region = resolved['credentialScope']['region']\n        return region_name, signing_region\n\n    def _resolve_signature_version(self, service_name, resolved):\n        configured_version = _get_configured_signature_version(\n            service_name, self.client_config, self.scoped_config\n        )\n        if configured_version is not None:\n            return configured_version\n\n        potential_versions = resolved.get('signatureVersions', [])\n        if (\n            self.service_signature_version is not None\n            and self.service_signature_version\n            not in _LEGACY_SIGNATURE_VERSIONS\n        ):\n            # Prefer the service model as most specific\n            # source of truth for new signature versions.\n            potential_versions = [self.service_signature_version]\n\n        # Pick a signature version from the endpoint metadata if present.\n        if 'signatureVersions' in resolved:\n            if service_name == 's3':\n                return 's3v4'\n            if 'v4' in potential_versions:\n                return 'v4'\n            # Now just iterate over the signature versions in order until we\n            # find the first one that is known to Botocore.\n            for known in potential_versions:\n                if known in AUTH_TYPE_MAPS:\n                    return known\n        raise UnknownSignatureVersionError(\n            signature_version=potential_versions\n        )\n\n\nclass BaseClient:\n    # This is actually reassigned with the py->op_name mapping\n    # when the client creator creates the subclass.  This value is used\n    # because calls such as client.get_paginator('list_objects') use the\n    # snake_case name, but we need to know the ListObjects form.\n    # xform_name() does the ListObjects->list_objects conversion, but\n    # we need the reverse mapping here.\n    _PY_TO_OP_NAME = {}\n\n    def __init__(\n        self,\n        serializer,\n        endpoint,\n        response_parser,\n        event_emitter,\n        request_signer,\n        service_model,\n        loader,\n        client_config,\n        partition,\n        exceptions_factory,\n        endpoint_ruleset_resolver=None,\n        user_agent_creator=None,\n    ):\n        self._serializer = serializer\n        self._endpoint = endpoint\n        self._ruleset_resolver = endpoint_ruleset_resolver\n        self._response_parser = response_parser\n        self._request_signer = request_signer\n        self._cache = {}\n        self._loader = loader\n        self._client_config = client_config\n        self.meta = ClientMeta(\n            event_emitter,\n            self._client_config,\n            endpoint.host,\n            service_model,\n            self._PY_TO_OP_NAME,\n            partition,\n        )\n        self._exceptions_factory = exceptions_factory\n        self._exceptions = None\n        self._user_agent_creator = user_agent_creator\n        if self._user_agent_creator is None:\n            self._user_agent_creator = (\n                UserAgentString.from_environment().with_client_config(\n                    self._client_config\n                )\n            )\n        self._register_handlers()\n\n    def __getattr__(self, item):\n        service_id = self._service_model.service_id.hyphenize()\n        event_name = f'getattr.{service_id}.{item}'\n\n        handler, event_response = self.meta.events.emit_until_response(\n            event_name, client=self\n        )\n\n        if event_response is not None:\n            return event_response\n\n        raise AttributeError(\n            f\"'{self.__class__.__name__}' object has no attribute '{item}'\"\n        )\n\n    def close(self):\n        \"\"\"Closes underlying endpoint connections.\"\"\"\n        self._endpoint.close()\n\n    def _register_handlers(self):\n        # Register the handler required to sign requests.\n        service_id = self.meta.service_model.service_id.hyphenize()\n        self.meta.events.register(\n            f\"request-created.{service_id}\", self._request_signer.handler\n        )\n\n    @property\n    def _service_model(self):\n        return self.meta.service_model\n\n    def _make_api_call(self, operation_name, api_params):\n        operation_model = self._service_model.operation_model(operation_name)\n        service_name = self._service_model.service_name\n        history_recorder.record(\n            'API_CALL',\n            {\n                'service': service_name,\n                'operation': operation_name,\n                'params': api_params,\n            },\n        )\n        if operation_model.deprecated:\n            logger.debug(\n                'Warning: %s.%s() is deprecated', service_name, operation_name\n            )\n        request_context = {\n            'client_region': self.meta.region_name,\n            'client_config': self.meta.config,\n            'has_streaming_input': operation_model.has_streaming_input,\n            'auth_type': operation_model.auth_type,\n        }\n        api_params = self._emit_api_params(\n            api_params=api_params,\n            operation_model=operation_model,\n            context=request_context,\n        )\n        (\n            endpoint_url,\n            additional_headers,\n            properties,\n        ) = self._resolve_endpoint_ruleset(\n            operation_model, api_params, request_context\n        )\n        if properties:\n            # Pass arbitrary endpoint info with the Request\n            # for use during construction.\n            request_context['endpoint_properties'] = properties\n        request_dict = self._convert_to_request_dict(\n            api_params=api_params,\n            operation_model=operation_model,\n            endpoint_url=endpoint_url,\n            context=request_context,\n            headers=additional_headers,\n        )\n        resolve_checksum_context(request_dict, operation_model, api_params)\n\n        service_id = self._service_model.service_id.hyphenize()\n        handler, event_response = self.meta.events.emit_until_response(\n            'before-call.{service_id}.{operation_name}'.format(\n                service_id=service_id, operation_name=operation_name\n            ),\n            model=operation_model,\n            params=request_dict,\n            request_signer=self._request_signer,\n            context=request_context,\n        )\n\n        if event_response is not None:\n            http, parsed_response = event_response\n        else:\n            maybe_compress_request(\n                self.meta.config, request_dict, operation_model\n            )\n            apply_request_checksum(request_dict)\n            http, parsed_response = self._make_request(\n                operation_model, request_dict, request_context\n            )\n\n        self.meta.events.emit(\n            'after-call.{service_id}.{operation_name}'.format(\n                service_id=service_id, operation_name=operation_name\n            ),\n            http_response=http,\n            parsed=parsed_response,\n            model=operation_model,\n            context=request_context,\n        )\n\n        if http.status_code >= 300:\n            error_info = parsed_response.get(\"Error\", {})\n            error_code = error_info.get(\"QueryErrorCode\") or error_info.get(\n                \"Code\"\n            )\n            error_class = self.exceptions.from_code(error_code)\n            raise error_class(parsed_response, operation_name)\n        else:\n            return parsed_response\n\n    def _make_request(self, operation_model, request_dict, request_context):\n        try:\n            return self._endpoint.make_request(operation_model, request_dict)\n        except Exception as e:\n            self.meta.events.emit(\n                'after-call-error.{service_id}.{operation_name}'.format(\n                    service_id=self._service_model.service_id.hyphenize(),\n                    operation_name=operation_model.name,\n                ),\n                exception=e,\n                context=request_context,\n            )\n            raise\n\n    def _convert_to_request_dict(\n        self,\n        api_params,\n        operation_model,\n        endpoint_url,\n        context=None,\n        headers=None,\n        set_user_agent_header=True,\n    ):\n        request_dict = self._serializer.serialize_to_request(\n            api_params, operation_model\n        )\n        if not self._client_config.inject_host_prefix:\n            request_dict.pop('host_prefix', None)\n        if headers is not None:\n            request_dict['headers'].update(headers)\n        if set_user_agent_header:\n            user_agent = self._user_agent_creator.to_string()\n        else:\n            user_agent = None\n        prepare_request_dict(\n            request_dict,\n            endpoint_url=endpoint_url,\n            user_agent=user_agent,\n            context=context,\n        )\n        return request_dict\n\n    def _emit_api_params(self, api_params, operation_model, context):\n        # Given the API params provided by the user and the operation_model\n        # we can serialize the request to a request_dict.\n        operation_name = operation_model.name\n\n        # Emit an event that allows users to modify the parameters at the\n        # beginning of the method. It allows handlers to modify existing\n        # parameters or return a new set of parameters to use.\n        service_id = self._service_model.service_id.hyphenize()\n        responses = self.meta.events.emit(\n            f'provide-client-params.{service_id}.{operation_name}',\n            params=api_params,\n            model=operation_model,\n            context=context,\n        )\n        api_params = first_non_none_response(responses, default=api_params)\n\n        self.meta.events.emit(\n            f'before-parameter-build.{service_id}.{operation_name}',\n            params=api_params,\n            model=operation_model,\n            context=context,\n        )\n        return api_params\n\n    def _resolve_endpoint_ruleset(\n        self,\n        operation_model,\n        params,\n        request_context,\n        ignore_signing_region=False,\n    ):\n        \"\"\"Returns endpoint URL and list of additional headers returned from\n        EndpointRulesetResolver for the given operation and params. If the\n        ruleset resolver is not available, for example because the service has\n        no endpoints ruleset file, the legacy endpoint resolver's value is\n        returned.\n\n        Use ignore_signing_region for generating presigned URLs or any other\n        situation where the signing region information from the ruleset\n        resolver should be ignored.\n\n        Returns tuple of URL and headers dictionary. Additionally, the\n        request_context dict is modified in place with any signing information\n        returned from the ruleset resolver.\n        \"\"\"\n        if self._ruleset_resolver is None:\n            endpoint_url = self.meta.endpoint_url\n            additional_headers = {}\n            endpoint_properties = {}\n        else:\n            endpoint_info = self._ruleset_resolver.construct_endpoint(\n                operation_model=operation_model,\n                call_args=params,\n                request_context=request_context,\n            )\n            endpoint_url = endpoint_info.url\n            additional_headers = endpoint_info.headers\n            endpoint_properties = endpoint_info.properties\n            # If authSchemes is present, overwrite default auth type and\n            # signing context derived from service model.\n            auth_schemes = endpoint_info.properties.get('authSchemes')\n            if auth_schemes is not None:\n                auth_info = self._ruleset_resolver.auth_schemes_to_signing_ctx(\n                    auth_schemes\n                )\n                auth_type, signing_context = auth_info\n                request_context['auth_type'] = auth_type\n                if 'region' in signing_context and ignore_signing_region:\n                    del signing_context['region']\n                if 'signing' in request_context:\n                    request_context['signing'].update(signing_context)\n                else:\n                    request_context['signing'] = signing_context\n\n        return endpoint_url, additional_headers, endpoint_properties\n\n    def get_paginator(self, operation_name):\n        \"\"\"Create a paginator for an operation.\n\n        :type operation_name: string\n        :param operation_name: The operation name.  This is the same name\n            as the method name on the client.  For example, if the\n            method name is ``create_foo``, and you'd normally invoke the\n            operation as ``client.create_foo(**kwargs)``, if the\n            ``create_foo`` operation can be paginated, you can use the\n            call ``client.get_paginator(\"create_foo\")``.\n\n        :raise OperationNotPageableError: Raised if the operation is not\n            pageable.  You can use the ``client.can_paginate`` method to\n            check if an operation is pageable.\n\n        :rtype: ``botocore.paginate.Paginator``\n        :return: A paginator object.\n\n        \"\"\"\n        if not self.can_paginate(operation_name):\n            raise OperationNotPageableError(operation_name=operation_name)\n        else:\n            actual_operation_name = self._PY_TO_OP_NAME[operation_name]\n\n            # Create a new paginate method that will serve as a proxy to\n            # the underlying Paginator.paginate method. This is needed to\n            # attach a docstring to the method.\n            def paginate(self, **kwargs):\n                return Paginator.paginate(self, **kwargs)\n\n            paginator_config = self._cache['page_config'][\n                actual_operation_name\n            ]\n            # Add the docstring for the paginate method.\n            paginate.__doc__ = PaginatorDocstring(\n                paginator_name=actual_operation_name,\n                event_emitter=self.meta.events,\n                service_model=self.meta.service_model,\n                paginator_config=paginator_config,\n                include_signature=False,\n            )\n\n            # Rename the paginator class based on the type of paginator.\n            service_module_name = get_service_module_name(\n                self.meta.service_model\n            )\n            paginator_class_name = (\n                f\"{service_module_name}.Paginator.{actual_operation_name}\"\n            )\n\n            # Create the new paginator class\n            documented_paginator_cls = type(\n                paginator_class_name, (Paginator,), {'paginate': paginate}\n            )\n\n            operation_model = self._service_model.operation_model(\n                actual_operation_name\n            )\n            paginator = documented_paginator_cls(\n                getattr(self, operation_name),\n                paginator_config,\n                operation_model,\n            )\n            return paginator\n\n    def can_paginate(self, operation_name):\n        \"\"\"Check if an operation can be paginated.\n\n        :type operation_name: string\n        :param operation_name: The operation name.  This is the same name\n            as the method name on the client.  For example, if the\n            method name is ``create_foo``, and you'd normally invoke the\n            operation as ``client.create_foo(**kwargs)``, if the\n            ``create_foo`` operation can be paginated, you can use the\n            call ``client.get_paginator(\"create_foo\")``.\n\n        :return: ``True`` if the operation can be paginated,\n            ``False`` otherwise.\n\n        \"\"\"\n        if 'page_config' not in self._cache:\n            try:\n                page_config = self._loader.load_service_model(\n                    self._service_model.service_name,\n                    'paginators-1',\n                    self._service_model.api_version,\n                )['pagination']\n                self._cache['page_config'] = page_config\n            except DataNotFoundError:\n                self._cache['page_config'] = {}\n        actual_operation_name = self._PY_TO_OP_NAME[operation_name]\n        return actual_operation_name in self._cache['page_config']\n\n    def _get_waiter_config(self):\n        if 'waiter_config' not in self._cache:\n            try:\n                waiter_config = self._loader.load_service_model(\n                    self._service_model.service_name,\n                    'waiters-2',\n                    self._service_model.api_version,\n                )\n                self._cache['waiter_config'] = waiter_config\n            except DataNotFoundError:\n                self._cache['waiter_config'] = {}\n        return self._cache['waiter_config']\n\n    def get_waiter(self, waiter_name):\n        \"\"\"Returns an object that can wait for some condition.\n\n        :type waiter_name: str\n        :param waiter_name: The name of the waiter to get. See the waiters\n            section of the service docs for a list of available waiters.\n\n        :returns: The specified waiter object.\n        :rtype: ``botocore.waiter.Waiter``\n        \"\"\"\n        config = self._get_waiter_config()\n        if not config:\n            raise ValueError(\"Waiter does not exist: %s\" % waiter_name)\n        model = waiter.WaiterModel(config)\n        mapping = {}\n        for name in model.waiter_names:\n            mapping[xform_name(name)] = name\n        if waiter_name not in mapping:\n            raise ValueError(\"Waiter does not exist: %s\" % waiter_name)\n\n        return waiter.create_waiter_with_client(\n            mapping[waiter_name], model, self\n        )\n\n    @CachedProperty\n    def waiter_names(self):\n        \"\"\"Returns a list of all available waiters.\"\"\"\n        config = self._get_waiter_config()\n        if not config:\n            return []\n        model = waiter.WaiterModel(config)\n        # Waiter configs is a dict, we just want the waiter names\n        # which are the keys in the dict.\n        return [xform_name(name) for name in model.waiter_names]\n\n    @property\n    def exceptions(self):\n        if self._exceptions is None:\n            self._exceptions = self._load_exceptions()\n        return self._exceptions\n\n    def _load_exceptions(self):\n        return self._exceptions_factory.create_client_exceptions(\n            self._service_model\n        )\n\n    def _get_credentials(self):\n        \"\"\"\n        This private interface is subject to abrupt breaking changes, including\n        removal, in any botocore release.\n        \"\"\"\n        return self._request_signer._credentials\n\n\nclass ClientMeta:\n    \"\"\"Holds additional client methods.\n\n    This class holds additional information for clients.  It exists for\n    two reasons:\n\n        * To give advanced functionality to clients\n        * To namespace additional client attributes from the operation\n          names which are mapped to methods at runtime.  This avoids\n          ever running into collisions with operation names.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        events,\n        client_config,\n        endpoint_url,\n        service_model,\n        method_to_api_mapping,\n        partition,\n    ):\n        self.events = events\n        self._client_config = client_config\n        self._endpoint_url = endpoint_url\n        self._service_model = service_model\n        self._method_to_api_mapping = method_to_api_mapping\n        self._partition = partition\n\n    @property\n    def service_model(self):\n        return self._service_model\n\n    @property\n    def region_name(self):\n        return self._client_config.region_name\n\n    @property\n    def endpoint_url(self):\n        return self._endpoint_url\n\n    @property\n    def config(self):\n        return self._client_config\n\n    @property\n    def method_to_api_mapping(self):\n        return self._method_to_api_mapping\n\n    @property\n    def partition(self):\n        return self._partition\n\n\ndef _get_configured_signature_version(\n    service_name, client_config, scoped_config\n):\n    \"\"\"\n    Gets the manually configured signature version.\n\n    :returns: the customer configured signature version, or None if no\n        signature version was configured.\n    \"\"\"\n    # Client config overrides everything.\n    if client_config and client_config.signature_version is not None:\n        return client_config.signature_version\n\n    # Scoped config overrides picking from the endpoint metadata.\n    if scoped_config is not None:\n        # A given service may have service specific configuration in the\n        # config file, so we need to check there as well.\n        service_config = scoped_config.get(service_name)\n        if service_config is not None and isinstance(service_config, dict):\n            version = service_config.get('signature_version')\n            if version:\n                logger.debug(\n                    \"Switching signature version for service %s \"\n                    \"to version %s based on config file override.\",\n                    service_name,\n                    version,\n                )\n                return version\n    return None\n", "botocore/handlers.py": "# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\n\"\"\"Builtin event handlers.\n\nThis module contains builtin handlers for events emitted by botocore.\n\"\"\"\n\nimport base64\nimport copy\nimport logging\nimport os\nimport re\nimport uuid\nimport warnings\nfrom io import BytesIO\n\nimport botocore\nimport botocore.auth\nfrom botocore import utils\nfrom botocore.compat import (\n    ETree,\n    OrderedDict,\n    XMLParseError,\n    ensure_bytes,\n    get_md5,\n    json,\n    quote,\n    unquote,\n    unquote_str,\n    urlsplit,\n    urlunsplit,\n)\nfrom botocore.docs.utils import (\n    AppendParamDocumentation,\n    AutoPopulatedParam,\n    HideParamFromOperations,\n)\nfrom botocore.endpoint_provider import VALID_HOST_LABEL_RE\nfrom botocore.exceptions import (\n    AliasConflictParameterError,\n    ParamValidationError,\n    UnsupportedTLSVersionWarning,\n)\nfrom botocore.regions import EndpointResolverBuiltins\nfrom botocore.signers import (\n    add_generate_db_auth_token,\n    add_generate_presigned_post,\n    add_generate_presigned_url,\n)\nfrom botocore.utils import (\n    SAFE_CHARS,\n    ArnParser,\n    conditionally_calculate_checksum,\n    conditionally_calculate_md5,\n    percent_encode,\n    switch_host_with_param,\n)\n\n# Keep these imported.  There's pre-existing code that uses them.\nfrom botocore import retryhandler  # noqa\nfrom botocore import translate  # noqa\nfrom botocore.compat import MD5_AVAILABLE  # noqa\nfrom botocore.exceptions import MissingServiceIdError  # noqa\nfrom botocore.utils import hyphenize_service_id  # noqa\nfrom botocore.utils import is_global_accesspoint  # noqa\nfrom botocore.utils import SERVICE_NAME_ALIASES  # noqa\n\n\nlogger = logging.getLogger(__name__)\n\nREGISTER_FIRST = object()\nREGISTER_LAST = object()\n# From the S3 docs:\n# The rules for bucket names in the US Standard region allow bucket names\n# to be as long as 255 characters, and bucket names can contain any\n# combination of uppercase letters, lowercase letters, numbers, periods\n# (.), hyphens (-), and underscores (_).\nVALID_BUCKET = re.compile(r'^[a-zA-Z0-9.\\-_]{1,255}$')\n_ACCESSPOINT_ARN = (\n    r'^arn:(aws).*:(s3|s3-object-lambda):[a-z\\-0-9]*:[0-9]{12}:accesspoint[/:]'\n    r'[a-zA-Z0-9\\-.]{1,63}$'\n)\n_OUTPOST_ARN = (\n    r'^arn:(aws).*:s3-outposts:[a-z\\-0-9]+:[0-9]{12}:outpost[/:]'\n    r'[a-zA-Z0-9\\-]{1,63}[/:]accesspoint[/:][a-zA-Z0-9\\-]{1,63}$'\n)\nVALID_S3_ARN = re.compile('|'.join([_ACCESSPOINT_ARN, _OUTPOST_ARN]))\n# signing names used for the services s3 and s3-control, for example in\n# botocore/data/s3/2006-03-01/endpoints-rule-set-1.json\nS3_SIGNING_NAMES = ('s3', 's3-outposts', 's3-object-lambda', 's3express')\nVERSION_ID_SUFFIX = re.compile(r'\\?versionId=[^\\s]+$')\n\n\ndef handle_service_name_alias(service_name, **kwargs):\n    return SERVICE_NAME_ALIASES.get(service_name, service_name)\n\n\ndef add_recursion_detection_header(params, **kwargs):\n    has_lambda_name = 'AWS_LAMBDA_FUNCTION_NAME' in os.environ\n    trace_id = os.environ.get('_X_AMZN_TRACE_ID')\n    if has_lambda_name and trace_id:\n        headers = params['headers']\n        if 'X-Amzn-Trace-Id' not in headers:\n            headers['X-Amzn-Trace-Id'] = quote(trace_id, safe='-=;:+&[]{}\"\\',')\n\n\ndef escape_xml_payload(params, **kwargs):\n    # Replace \\r and \\n with the escaped sequence over the whole XML document\n    # to avoid linebreak normalization modifying customer input when the\n    # document is parsed. Ideally, we would do this in ElementTree.tostring,\n    # but it doesn't allow us to override entity escaping for text fields. For\n    # this operation \\r and \\n can only appear in the XML document if they were\n    # passed as part of the customer input.\n    body = params['body']\n    if b'\\r' in body:\n        body = body.replace(b'\\r', b'&#xD;')\n    if b'\\n' in body:\n        body = body.replace(b'\\n', b'&#xA;')\n\n    params['body'] = body\n\n\ndef check_for_200_error(response, **kwargs):\n    # From: http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html\n    # There are two opportunities for a copy request to return an error. One\n    # can occur when Amazon S3 receives the copy request and the other can\n    # occur while Amazon S3 is copying the files. If the error occurs before\n    # the copy operation starts, you receive a standard Amazon S3 error. If the\n    # error occurs during the copy operation, the error response is embedded in\n    # the 200 OK response. This means that a 200 OK response can contain either\n    # a success or an error. Make sure to design your application to parse the\n    # contents of the response and handle it appropriately.\n    #\n    # So this handler checks for this case.  Even though the server sends a\n    # 200 response, conceptually this should be handled exactly like a\n    # 500 response (with respect to raising exceptions, retries, etc.)\n    # We're connected *before* all the other retry logic handlers, so as long\n    # as we switch the error code to 500, we'll retry the error as expected.\n    if response is None:\n        # A None response can happen if an exception is raised while\n        # trying to retrieve the response.  See Endpoint._get_response().\n        return\n    http_response, parsed = response\n    if _looks_like_special_case_error(http_response):\n        logger.debug(\n            \"Error found for response with 200 status code, \"\n            \"errors: %s, changing status code to \"\n            \"500.\",\n            parsed,\n        )\n        http_response.status_code = 500\n\n\ndef _looks_like_special_case_error(http_response):\n    if http_response.status_code == 200:\n        try:\n            parser = ETree.XMLParser(\n                target=ETree.TreeBuilder(), encoding='utf-8'\n            )\n            parser.feed(http_response.content)\n            root = parser.close()\n        except XMLParseError:\n            # In cases of network disruptions, we may end up with a partial\n            # streamed response from S3. We need to treat these cases as\n            # 500 Service Errors and try again.\n            return True\n        if root.tag == 'Error':\n            return True\n    return False\n\n\ndef set_operation_specific_signer(context, signing_name, **kwargs):\n    \"\"\"Choose the operation-specific signer.\n\n    Individual operations may have a different auth type than the service as a\n    whole. This will most often manifest as operations that should not be\n    authenticated at all, but can include other auth modes such as sigv4\n    without body signing.\n    \"\"\"\n    auth_type = context.get('auth_type')\n\n    # Auth type will be None if the operation doesn't have a configured auth\n    # type.\n    if not auth_type:\n        return\n\n    # Auth type will be the string value 'none' if the operation should not\n    # be signed at all.\n    if auth_type == 'none':\n        return botocore.UNSIGNED\n\n    if auth_type == 'bearer':\n        return 'bearer'\n\n    if auth_type.startswith('v4'):\n        if auth_type == 'v4-s3express':\n            return auth_type\n\n        if auth_type == 'v4a':\n            # If sigv4a is chosen, we must add additional signing config for\n            # global signature.\n            signing = {'region': '*', 'signing_name': signing_name}\n            if 'signing' in context:\n                context['signing'].update(signing)\n            else:\n                context['signing'] = signing\n            signature_version = 'v4a'\n        else:\n            signature_version = 'v4'\n\n        # If the operation needs an unsigned body, we set additional context\n        # allowing the signer to be aware of this.\n        if auth_type == 'v4-unsigned-body':\n            context['payload_signing_enabled'] = False\n\n        # Signing names used by s3 and s3-control use customized signers \"s3v4\"\n        # and \"s3v4a\".\n        if signing_name in S3_SIGNING_NAMES:\n            signature_version = f's3{signature_version}'\n\n        return signature_version\n\n\ndef decode_console_output(parsed, **kwargs):\n    if 'Output' in parsed:\n        try:\n            # We're using 'replace' for errors because it is\n            # possible that console output contains non string\n            # chars we can't utf-8 decode.\n            value = base64.b64decode(\n                bytes(parsed['Output'], 'latin-1')\n            ).decode('utf-8', 'replace')\n            parsed['Output'] = value\n        except (ValueError, TypeError, AttributeError):\n            logger.debug('Error decoding base64', exc_info=True)\n\n\ndef generate_idempotent_uuid(params, model, **kwargs):\n    for name in model.idempotent_members:\n        if name not in params:\n            params[name] = str(uuid.uuid4())\n            logger.debug(\n                \"injecting idempotency token (%s) into param '%s'.\"\n                % (params[name], name)\n            )\n\n\ndef decode_quoted_jsondoc(value):\n    try:\n        value = json.loads(unquote(value))\n    except (ValueError, TypeError):\n        logger.debug('Error loading quoted JSON', exc_info=True)\n    return value\n\n\ndef json_decode_template_body(parsed, **kwargs):\n    if 'TemplateBody' in parsed:\n        try:\n            value = json.loads(\n                parsed['TemplateBody'], object_pairs_hook=OrderedDict\n            )\n            parsed['TemplateBody'] = value\n        except (ValueError, TypeError):\n            logger.debug('error loading JSON', exc_info=True)\n\n\ndef validate_bucket_name(params, **kwargs):\n    if 'Bucket' not in params:\n        return\n    bucket = params['Bucket']\n    if not VALID_BUCKET.search(bucket) and not VALID_S3_ARN.search(bucket):\n        error_msg = (\n            f'Invalid bucket name \"{bucket}\": Bucket name must match '\n            f'the regex \"{VALID_BUCKET.pattern}\" or be an ARN matching '\n            f'the regex \"{VALID_S3_ARN.pattern}\"'\n        )\n        raise ParamValidationError(report=error_msg)\n\n\ndef sse_md5(params, **kwargs):\n    \"\"\"\n    S3 server-side encryption requires the encryption key to be sent to the\n    server base64 encoded, as well as a base64-encoded MD5 hash of the\n    encryption key. This handler does both if the MD5 has not been set by\n    the caller.\n    \"\"\"\n    _sse_md5(params, 'SSECustomer')\n\n\ndef copy_source_sse_md5(params, **kwargs):\n    \"\"\"\n    S3 server-side encryption requires the encryption key to be sent to the\n    server base64 encoded, as well as a base64-encoded MD5 hash of the\n    encryption key. This handler does both if the MD5 has not been set by\n    the caller specifically if the parameter is for the copy-source sse-c key.\n    \"\"\"\n    _sse_md5(params, 'CopySourceSSECustomer')\n\n\ndef _sse_md5(params, sse_member_prefix='SSECustomer'):\n    if not _needs_s3_sse_customization(params, sse_member_prefix):\n        return\n\n    sse_key_member = sse_member_prefix + 'Key'\n    sse_md5_member = sse_member_prefix + 'KeyMD5'\n    key_as_bytes = params[sse_key_member]\n    if isinstance(key_as_bytes, str):\n        key_as_bytes = key_as_bytes.encode('utf-8')\n    key_md5_str = base64.b64encode(get_md5(key_as_bytes).digest()).decode(\n        'utf-8'\n    )\n    key_b64_encoded = base64.b64encode(key_as_bytes).decode('utf-8')\n    params[sse_key_member] = key_b64_encoded\n    params[sse_md5_member] = key_md5_str\n\n\ndef _needs_s3_sse_customization(params, sse_member_prefix):\n    return (\n        params.get(sse_member_prefix + 'Key') is not None\n        and sse_member_prefix + 'KeyMD5' not in params\n    )\n\n\ndef disable_signing(**kwargs):\n    \"\"\"\n    This handler disables request signing by setting the signer\n    name to a special sentinel value.\n    \"\"\"\n    return botocore.UNSIGNED\n\n\ndef add_expect_header(model, params, **kwargs):\n    if model.http.get('method', '') not in ['PUT', 'POST']:\n        return\n    if 'body' in params:\n        body = params['body']\n        if hasattr(body, 'read'):\n            check_body = utils.ensure_boolean(\n                os.environ.get(\n                    'BOTO_EXPERIMENTAL__NO_EMPTY_CONTINUE',\n                    False,\n                )\n            )\n            if check_body and utils.determine_content_length(body) == 0:\n                return\n            # Any file like object will use an expect 100-continue\n            # header regardless of size.\n            logger.debug(\"Adding expect 100 continue header to request.\")\n            params['headers']['Expect'] = '100-continue'\n\n\nclass DeprecatedServiceDocumenter:\n    def __init__(self, replacement_service_name):\n        self._replacement_service_name = replacement_service_name\n\n    def inject_deprecation_notice(self, section, event_name, **kwargs):\n        section.style.start_important()\n        section.write('This service client is deprecated. Please use ')\n        section.style.ref(\n            self._replacement_service_name,\n            self._replacement_service_name,\n        )\n        section.write(' instead.')\n        section.style.end_important()\n\n\ndef document_copy_source_form(section, event_name, **kwargs):\n    if 'request-example' in event_name:\n        parent = section.get_section('structure-value')\n        param_line = parent.get_section('CopySource')\n        value_portion = param_line.get_section('member-value')\n        value_portion.clear_text()\n        value_portion.write(\n            \"'string' or {'Bucket': 'string', \"\n            \"'Key': 'string', 'VersionId': 'string'}\"\n        )\n    elif 'request-params' in event_name:\n        param_section = section.get_section('CopySource')\n        type_section = param_section.get_section('param-type')\n        type_section.clear_text()\n        type_section.write(':type CopySource: str or dict')\n        doc_section = param_section.get_section('param-documentation')\n        doc_section.clear_text()\n        doc_section.write(\n            \"The name of the source bucket, key name of the source object, \"\n            \"and optional version ID of the source object.  You can either \"\n            \"provide this value as a string or a dictionary.  The \"\n            \"string form is {bucket}/{key} or \"\n            \"{bucket}/{key}?versionId={versionId} if you want to copy a \"\n            \"specific version.  You can also provide this value as a \"\n            \"dictionary.  The dictionary format is recommended over \"\n            \"the string format because it is more explicit.  The dictionary \"\n            \"format is: {'Bucket': 'bucket', 'Key': 'key', 'VersionId': 'id'}.\"\n            \"  Note that the VersionId key is optional and may be omitted.\"\n            \" To specify an S3 access point, provide the access point\"\n            \" ARN for the ``Bucket`` key in the copy source dictionary. If you\"\n            \" want to provide the copy source for an S3 access point as a\"\n            \" string instead of a dictionary, the ARN provided must be the\"\n            \" full S3 access point object ARN\"\n            \" (i.e. {accesspoint_arn}/object/{key})\"\n        )\n\n\ndef handle_copy_source_param(params, **kwargs):\n    \"\"\"Convert CopySource param for CopyObject/UploadPartCopy.\n\n    This handler will deal with two cases:\n\n        * CopySource provided as a string.  We'll make a best effort\n          to URL encode the key name as required.  This will require\n          parsing the bucket and version id from the CopySource value\n          and only encoding the key.\n        * CopySource provided as a dict.  In this case we're\n          explicitly given the Bucket, Key, and VersionId so we're\n          able to encode the key and ensure this value is serialized\n          and correctly sent to S3.\n\n    \"\"\"\n    source = params.get('CopySource')\n    if source is None:\n        # The call will eventually fail but we'll let the\n        # param validator take care of this.  It will\n        # give a better error message.\n        return\n    if isinstance(source, str):\n        params['CopySource'] = _quote_source_header(source)\n    elif isinstance(source, dict):\n        params['CopySource'] = _quote_source_header_from_dict(source)\n\n\ndef _quote_source_header_from_dict(source_dict):\n    try:\n        bucket = source_dict['Bucket']\n        key = source_dict['Key']\n        version_id = source_dict.get('VersionId')\n        if VALID_S3_ARN.search(bucket):\n            final = f'{bucket}/object/{key}'\n        else:\n            final = f'{bucket}/{key}'\n    except KeyError as e:\n        raise ParamValidationError(\n            report=f'Missing required parameter: {str(e)}'\n        )\n    final = percent_encode(final, safe=SAFE_CHARS + '/')\n    if version_id is not None:\n        final += '?versionId=%s' % version_id\n    return final\n\n\ndef _quote_source_header(value):\n    result = VERSION_ID_SUFFIX.search(value)\n    if result is None:\n        return percent_encode(value, safe=SAFE_CHARS + '/')\n    else:\n        first, version_id = value[: result.start()], value[result.start() :]\n        return percent_encode(first, safe=SAFE_CHARS + '/') + version_id\n\n\ndef _get_cross_region_presigned_url(\n    request_signer, request_dict, model, source_region, destination_region\n):\n    # The better way to do this is to actually get the\n    # endpoint_resolver and get the endpoint_url given the\n    # source region.  In this specific case, we know that\n    # we can safely replace the dest region with the source\n    # region because of the supported EC2 regions, but in\n    # general this is not a safe assumption to make.\n    # I think eventually we should try to plumb through something\n    # that allows us to resolve endpoints from regions.\n    request_dict_copy = copy.deepcopy(request_dict)\n    request_dict_copy['body']['DestinationRegion'] = destination_region\n    request_dict_copy['url'] = request_dict['url'].replace(\n        destination_region, source_region\n    )\n    request_dict_copy['method'] = 'GET'\n    request_dict_copy['headers'] = {}\n    return request_signer.generate_presigned_url(\n        request_dict_copy, region_name=source_region, operation_name=model.name\n    )\n\n\ndef _get_presigned_url_source_and_destination_regions(request_signer, params):\n    # Gets the source and destination regions to be used\n    destination_region = request_signer._region_name\n    source_region = params.get('SourceRegion')\n    return source_region, destination_region\n\n\ndef inject_presigned_url_ec2(params, request_signer, model, **kwargs):\n    # The customer can still provide this, so we should pass if they do.\n    if 'PresignedUrl' in params['body']:\n        return\n    src, dest = _get_presigned_url_source_and_destination_regions(\n        request_signer, params['body']\n    )\n    url = _get_cross_region_presigned_url(\n        request_signer, params, model, src, dest\n    )\n    params['body']['PresignedUrl'] = url\n    # EC2 Requires that the destination region be sent over the wire in\n    # addition to the source region.\n    params['body']['DestinationRegion'] = dest\n\n\ndef inject_presigned_url_rds(params, request_signer, model, **kwargs):\n    # SourceRegion is not required for RDS operations, so it's possible that\n    # it isn't set. In that case it's probably a local copy so we don't need\n    # to do anything else.\n    if 'SourceRegion' not in params['body']:\n        return\n\n    src, dest = _get_presigned_url_source_and_destination_regions(\n        request_signer, params['body']\n    )\n\n    # Since SourceRegion isn't actually modeled for RDS, it needs to be\n    # removed from the request params before we send the actual request.\n    del params['body']['SourceRegion']\n\n    if 'PreSignedUrl' in params['body']:\n        return\n\n    url = _get_cross_region_presigned_url(\n        request_signer, params, model, src, dest\n    )\n    params['body']['PreSignedUrl'] = url\n\n\ndef json_decode_policies(parsed, model, **kwargs):\n    # Any time an IAM operation returns a policy document\n    # it is a string that is json that has been urlencoded,\n    # i.e urlencode(json.dumps(policy_document)).\n    # To give users something more useful, we will urldecode\n    # this value and json.loads() the result so that they have\n    # the policy document as a dictionary.\n    output_shape = model.output_shape\n    if output_shape is not None:\n        _decode_policy_types(parsed, model.output_shape)\n\n\ndef _decode_policy_types(parsed, shape):\n    # IAM consistently uses the policyDocumentType shape to indicate\n    # strings that have policy documents.\n    shape_name = 'policyDocumentType'\n    if shape.type_name == 'structure':\n        for member_name, member_shape in shape.members.items():\n            if (\n                member_shape.type_name == 'string'\n                and member_shape.name == shape_name\n                and member_name in parsed\n            ):\n                parsed[member_name] = decode_quoted_jsondoc(\n                    parsed[member_name]\n                )\n            elif member_name in parsed:\n                _decode_policy_types(parsed[member_name], member_shape)\n    if shape.type_name == 'list':\n        shape_member = shape.member\n        for item in parsed:\n            _decode_policy_types(item, shape_member)\n\n\ndef parse_get_bucket_location(parsed, http_response, **kwargs):\n    # s3.GetBucketLocation cannot be modeled properly.  To\n    # account for this we just manually parse the XML document.\n    # The \"parsed\" passed in only has the ResponseMetadata\n    # filled out.  This handler will fill in the LocationConstraint\n    # value.\n    if http_response.raw is None:\n        return\n    response_body = http_response.content\n    parser = ETree.XMLParser(target=ETree.TreeBuilder(), encoding='utf-8')\n    parser.feed(response_body)\n    root = parser.close()\n    region = root.text\n    parsed['LocationConstraint'] = region\n\n\ndef base64_encode_user_data(params, **kwargs):\n    if 'UserData' in params:\n        if isinstance(params['UserData'], str):\n            # Encode it to bytes if it is text.\n            params['UserData'] = params['UserData'].encode('utf-8')\n        params['UserData'] = base64.b64encode(params['UserData']).decode(\n            'utf-8'\n        )\n\n\ndef document_base64_encoding(param):\n    description = (\n        '**This value will be base64 encoded automatically. Do '\n        'not base64 encode this value prior to performing the '\n        'operation.**'\n    )\n    append = AppendParamDocumentation(param, description)\n    return append.append_documentation\n\n\ndef validate_ascii_metadata(params, **kwargs):\n    \"\"\"Verify S3 Metadata only contains ascii characters.\n\n    From: http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html\n\n    \"Amazon S3 stores user-defined metadata in lowercase. Each name, value pair\n    must conform to US-ASCII when using REST and UTF-8 when using SOAP or\n    browser-based uploads via POST.\"\n\n    \"\"\"\n    metadata = params.get('Metadata')\n    if not metadata or not isinstance(metadata, dict):\n        # We have to at least type check the metadata as a dict type\n        # because this handler is called before param validation.\n        # We'll go ahead and return because the param validator will\n        # give a descriptive error message for us.\n        # We might need a post-param validation event.\n        return\n    for key, value in metadata.items():\n        try:\n            key.encode('ascii')\n            value.encode('ascii')\n        except UnicodeEncodeError:\n            error_msg = (\n                'Non ascii characters found in S3 metadata '\n                'for key \"%s\", value: \"%s\".  \\nS3 metadata can only '\n                'contain ASCII characters. ' % (key, value)\n            )\n            raise ParamValidationError(report=error_msg)\n\n\ndef fix_route53_ids(params, model, **kwargs):\n    \"\"\"\n    Check for and split apart Route53 resource IDs, setting\n    only the last piece. This allows the output of one operation\n    (e.g. ``'foo/1234'``) to be used as input in another\n    operation (e.g. it expects just ``'1234'``).\n    \"\"\"\n    input_shape = model.input_shape\n    if not input_shape or not hasattr(input_shape, 'members'):\n        return\n\n    members = [\n        name\n        for (name, shape) in input_shape.members.items()\n        if shape.name in ['ResourceId', 'DelegationSetId', 'ChangeId']\n    ]\n\n    for name in members:\n        if name in params:\n            orig_value = params[name]\n            params[name] = orig_value.split('/')[-1]\n            logger.debug('%s %s -> %s', name, orig_value, params[name])\n\n\ndef inject_account_id(params, **kwargs):\n    if params.get('accountId') is None:\n        # Glacier requires accountId, but allows you\n        # to specify '-' for the current owners account.\n        # We add this default value if the user does not\n        # provide the accountId as a convenience.\n        params['accountId'] = '-'\n\n\ndef add_glacier_version(model, params, **kwargs):\n    request_dict = params\n    request_dict['headers']['x-amz-glacier-version'] = model.metadata[\n        'apiVersion'\n    ]\n\n\ndef add_accept_header(model, params, **kwargs):\n    if params['headers'].get('Accept', None) is None:\n        request_dict = params\n        request_dict['headers']['Accept'] = 'application/json'\n\n\ndef add_glacier_checksums(params, **kwargs):\n    \"\"\"Add glacier checksums to the http request.\n\n    This will add two headers to the http request:\n\n        * x-amz-content-sha256\n        * x-amz-sha256-tree-hash\n\n    These values will only be added if they are not present\n    in the HTTP request.\n\n    \"\"\"\n    request_dict = params\n    headers = request_dict['headers']\n    body = request_dict['body']\n    if isinstance(body, bytes):\n        # If the user provided a bytes type instead of a file\n        # like object, we're temporarily create a BytesIO object\n        # so we can use the util functions to calculate the\n        # checksums which assume file like objects.  Note that\n        # we're not actually changing the body in the request_dict.\n        body = BytesIO(body)\n    starting_position = body.tell()\n    if 'x-amz-content-sha256' not in headers:\n        headers['x-amz-content-sha256'] = utils.calculate_sha256(\n            body, as_hex=True\n        )\n    body.seek(starting_position)\n    if 'x-amz-sha256-tree-hash' not in headers:\n        headers['x-amz-sha256-tree-hash'] = utils.calculate_tree_hash(body)\n    body.seek(starting_position)\n\n\ndef document_glacier_tree_hash_checksum():\n    doc = '''\n        This is a required field.\n\n        Ideally you will want to compute this value with checksums from\n        previous uploaded parts, using the algorithm described in\n        `Glacier documentation <http://docs.aws.amazon.com/amazonglacier/latest/dev/checksum-calculations.html>`_.\n\n        But if you prefer, you can also use botocore.utils.calculate_tree_hash()\n        to compute it from raw file by::\n\n            checksum = calculate_tree_hash(open('your_file.txt', 'rb'))\n\n        '''\n    return AppendParamDocumentation('checksum', doc).append_documentation\n\n\ndef document_cloudformation_get_template_return_type(\n    section, event_name, **kwargs\n):\n    if 'response-params' in event_name:\n        template_body_section = section.get_section('TemplateBody')\n        type_section = template_body_section.get_section('param-type')\n        type_section.clear_text()\n        type_section.write('(*dict*) --')\n    elif 'response-example' in event_name:\n        parent = section.get_section('structure-value')\n        param_line = parent.get_section('TemplateBody')\n        value_portion = param_line.get_section('member-value')\n        value_portion.clear_text()\n        value_portion.write('{}')\n\n\ndef switch_host_machinelearning(request, **kwargs):\n    switch_host_with_param(request, 'PredictEndpoint')\n\n\ndef check_openssl_supports_tls_version_1_2(**kwargs):\n    import ssl\n\n    try:\n        openssl_version_tuple = ssl.OPENSSL_VERSION_INFO\n        if openssl_version_tuple < (1, 0, 1):\n            warnings.warn(\n                'Currently installed openssl version: %s does not '\n                'support TLS 1.2, which is required for use of iot-data. '\n                'Please use python installed with openssl version 1.0.1 or '\n                'higher.' % (ssl.OPENSSL_VERSION),\n                UnsupportedTLSVersionWarning,\n            )\n    # We cannot check the openssl version on python2.6, so we should just\n    # pass on this conveniency check.\n    except AttributeError:\n        pass\n\n\ndef change_get_to_post(request, **kwargs):\n    # This is useful when we need to change a potentially large GET request\n    # into a POST with x-www-form-urlencoded encoding.\n    if request.method == 'GET' and '?' in request.url:\n        request.headers['Content-Type'] = 'application/x-www-form-urlencoded'\n        request.method = 'POST'\n        request.url, request.data = request.url.split('?', 1)\n\n\ndef set_list_objects_encoding_type_url(params, context, **kwargs):\n    if 'EncodingType' not in params:\n        # We set this context so that we know it wasn't the customer that\n        # requested the encoding.\n        context['encoding_type_auto_set'] = True\n        params['EncodingType'] = 'url'\n\n\ndef decode_list_object(parsed, context, **kwargs):\n    # This is needed because we are passing url as the encoding type. Since the\n    # paginator is based on the key, we need to handle it before it can be\n    # round tripped.\n    #\n    # From the documentation: If you specify encoding-type request parameter,\n    # Amazon S3 includes this element in the response, and returns encoded key\n    # name values in the following response elements:\n    # Delimiter, Marker, Prefix, NextMarker, Key.\n    _decode_list_object(\n        top_level_keys=['Delimiter', 'Marker', 'NextMarker'],\n        nested_keys=[('Contents', 'Key'), ('CommonPrefixes', 'Prefix')],\n        parsed=parsed,\n        context=context,\n    )\n\n\ndef decode_list_object_v2(parsed, context, **kwargs):\n    # From the documentation: If you specify encoding-type request parameter,\n    # Amazon S3 includes this element in the response, and returns encoded key\n    # name values in the following response elements:\n    # Delimiter, Prefix, ContinuationToken, Key, and StartAfter.\n    _decode_list_object(\n        top_level_keys=['Delimiter', 'Prefix', 'StartAfter'],\n        nested_keys=[('Contents', 'Key'), ('CommonPrefixes', 'Prefix')],\n        parsed=parsed,\n        context=context,\n    )\n\n\ndef decode_list_object_versions(parsed, context, **kwargs):\n    # From the documentation: If you specify encoding-type request parameter,\n    # Amazon S3 includes this element in the response, and returns encoded key\n    # name values in the following response elements:\n    # KeyMarker, NextKeyMarker, Prefix, Key, and Delimiter.\n    _decode_list_object(\n        top_level_keys=[\n            'KeyMarker',\n            'NextKeyMarker',\n            'Prefix',\n            'Delimiter',\n        ],\n        nested_keys=[\n            ('Versions', 'Key'),\n            ('DeleteMarkers', 'Key'),\n            ('CommonPrefixes', 'Prefix'),\n        ],\n        parsed=parsed,\n        context=context,\n    )\n\n\ndef _decode_list_object(top_level_keys, nested_keys, parsed, context):\n    if parsed.get('EncodingType') == 'url' and context.get(\n        'encoding_type_auto_set'\n    ):\n        # URL decode top-level keys in the response if present.\n        for key in top_level_keys:\n            if key in parsed:\n                parsed[key] = unquote_str(parsed[key])\n        # URL decode nested keys from the response if present.\n        for top_key, child_key in nested_keys:\n            if top_key in parsed:\n                for member in parsed[top_key]:\n                    member[child_key] = unquote_str(member[child_key])\n\n\ndef convert_body_to_file_like_object(params, **kwargs):\n    if 'Body' in params:\n        if isinstance(params['Body'], str):\n            params['Body'] = BytesIO(ensure_bytes(params['Body']))\n        elif isinstance(params['Body'], bytes):\n            params['Body'] = BytesIO(params['Body'])\n\n\ndef _add_parameter_aliases(handler_list):\n    # Mapping of original parameter to parameter alias.\n    # The key is <service>.<operation>.parameter\n    # The first part of the key is used for event registration.\n    # The last part is the original parameter name and the value is the\n    # alias to expose in documentation.\n    aliases = {\n        'ec2.*.Filter': 'Filters',\n        'logs.CreateExportTask.from': 'fromTime',\n        'cloudsearchdomain.Search.return': 'returnFields',\n    }\n\n    for original, new_name in aliases.items():\n        event_portion, original_name = original.rsplit('.', 1)\n        parameter_alias = ParameterAlias(original_name, new_name)\n\n        # Add the handlers to the list of handlers.\n        # One handler is to handle when users provide the alias.\n        # The other handler is to update the documentation to show only\n        # the alias.\n        parameter_build_event_handler_tuple = (\n            'before-parameter-build.' + event_portion,\n            parameter_alias.alias_parameter_in_call,\n            REGISTER_FIRST,\n        )\n        docs_event_handler_tuple = (\n            'docs.*.' + event_portion + '.complete-section',\n            parameter_alias.alias_parameter_in_documentation,\n        )\n        handler_list.append(parameter_build_event_handler_tuple)\n        handler_list.append(docs_event_handler_tuple)\n\n\nclass ParameterAlias:\n    def __init__(self, original_name, alias_name):\n        self._original_name = original_name\n        self._alias_name = alias_name\n\n    def alias_parameter_in_call(self, params, model, **kwargs):\n        if model.input_shape:\n            # Only consider accepting the alias if it is modeled in the\n            # input shape.\n            if self._original_name in model.input_shape.members:\n                if self._alias_name in params:\n                    if self._original_name in params:\n                        raise AliasConflictParameterError(\n                            original=self._original_name,\n                            alias=self._alias_name,\n                            operation=model.name,\n                        )\n                    # Remove the alias parameter value and use the old name\n                    # instead.\n                    params[self._original_name] = params.pop(self._alias_name)\n\n    def alias_parameter_in_documentation(self, event_name, section, **kwargs):\n        if event_name.startswith('docs.request-params'):\n            if self._original_name not in section.available_sections:\n                return\n            # Replace the name for parameter type\n            param_section = section.get_section(self._original_name)\n            param_type_section = param_section.get_section('param-type')\n            self._replace_content(param_type_section)\n\n            # Replace the name for the parameter description\n            param_name_section = param_section.get_section('param-name')\n            self._replace_content(param_name_section)\n        elif event_name.startswith('docs.request-example'):\n            section = section.get_section('structure-value')\n            if self._original_name not in section.available_sections:\n                return\n            # Replace the name for the example\n            param_section = section.get_section(self._original_name)\n            self._replace_content(param_section)\n\n    def _replace_content(self, section):\n        content = section.getvalue().decode('utf-8')\n        updated_content = content.replace(\n            self._original_name, self._alias_name\n        )\n        section.clear_text()\n        section.write(updated_content)\n\n\nclass ClientMethodAlias:\n    def __init__(self, actual_name):\n        \"\"\"Aliases a non-extant method to an existing method.\n\n        :param actual_name: The name of the method that actually exists on\n            the client.\n        \"\"\"\n        self._actual = actual_name\n\n    def __call__(self, client, **kwargs):\n        return getattr(client, self._actual)\n\n\n# TODO: Remove this class as it is no longer used\nclass HeaderToHostHoister:\n    \"\"\"Takes a header and moves it to the front of the hoststring.\"\"\"\n\n    _VALID_HOSTNAME = re.compile(r'(?!-)[a-z\\d-]{1,63}(?<!-)$', re.IGNORECASE)\n\n    def __init__(self, header_name):\n        self._header_name = header_name\n\n    def hoist(self, params, **kwargs):\n        \"\"\"Hoist a header to the hostname.\n\n        Hoist a header to the beginning of the hostname with a suffix \".\" after\n        it. The original header should be removed from the header map. This\n        method is intended to be used as a target for the before-call event.\n        \"\"\"\n        if self._header_name not in params['headers']:\n            return\n        header_value = params['headers'][self._header_name]\n        self._ensure_header_is_valid_host(header_value)\n        original_url = params['url']\n        new_url = self._prepend_to_host(original_url, header_value)\n        params['url'] = new_url\n\n    def _ensure_header_is_valid_host(self, header):\n        match = self._VALID_HOSTNAME.match(header)\n        if not match:\n            raise ParamValidationError(\n                report=(\n                    'Hostnames must contain only - and alphanumeric characters, '\n                    'and between 1 and 63 characters long.'\n                )\n            )\n\n    def _prepend_to_host(self, url, prefix):\n        url_components = urlsplit(url)\n        parts = url_components.netloc.split('.')\n        parts = [prefix] + parts\n        new_netloc = '.'.join(parts)\n        new_components = (\n            url_components.scheme,\n            new_netloc,\n            url_components.path,\n            url_components.query,\n            '',\n        )\n        new_url = urlunsplit(new_components)\n        return new_url\n\n\ndef inject_api_version_header_if_needed(model, params, **kwargs):\n    if not model.is_endpoint_discovery_operation:\n        return\n    params['headers']['x-amz-api-version'] = model.service_model.api_version\n\n\ndef remove_lex_v2_start_conversation(class_attributes, **kwargs):\n    \"\"\"Operation requires h2 which is currently unsupported in Python\"\"\"\n    if 'start_conversation' in class_attributes:\n        del class_attributes['start_conversation']\n\n\ndef remove_qbusiness_chat(class_attributes, **kwargs):\n    \"\"\"Operation requires h2 which is currently unsupported in Python\"\"\"\n    if 'chat' in class_attributes:\n        del class_attributes['chat']\n\n\ndef add_retry_headers(request, **kwargs):\n    retries_context = request.context.get('retries')\n    if not retries_context:\n        return\n    headers = request.headers\n    headers['amz-sdk-invocation-id'] = retries_context['invocation-id']\n    sdk_retry_keys = ('ttl', 'attempt', 'max')\n    sdk_request_headers = [\n        f'{key}={retries_context[key]}'\n        for key in sdk_retry_keys\n        if key in retries_context\n    ]\n    headers['amz-sdk-request'] = '; '.join(sdk_request_headers)\n\n\ndef remove_bucket_from_url_paths_from_model(params, model, context, **kwargs):\n    \"\"\"Strips leading `{Bucket}/` from any operations that have it.\n\n    The original value is retained in a separate \"authPath\" field. This is\n    used in the HmacV1Auth signer. See HmacV1Auth.canonical_resource in\n    botocore/auth.py for details.\n\n    This change is applied to the operation model during the first time the\n    operation is invoked and then stays in effect for the lifetime of the\n    client object.\n\n    When the ruleset based endpoint resolver is in effect, both the endpoint\n    ruleset AND the service model place the bucket name in the final URL.\n    The result is an invalid URL. This handler modifies the operation model to\n    no longer place the bucket name. Previous versions of botocore fixed the\n    URL after the fact when necessary. Since the introduction of ruleset based\n    endpoint resolution, the problem exists in ALL URLs that contain a bucket\n    name and can therefore be addressed before the URL gets assembled.\n    \"\"\"\n    req_uri = model.http['requestUri']\n    bucket_path = '/{Bucket}'\n    if req_uri.startswith(bucket_path):\n        model.http['requestUri'] = req_uri[len(bucket_path) :]\n        # Strip query off the requestUri before using as authPath. The\n        # HmacV1Auth signer will append query params to the authPath during\n        # signing.\n        req_uri = req_uri.split('?')[0]\n        # If the request URI is ONLY a bucket, the auth_path must be\n        # terminated with a '/' character to generate a signature that the\n        # server will accept.\n        needs_slash = req_uri == bucket_path\n        model.http['authPath'] = f'{req_uri}/' if needs_slash else req_uri\n\n\ndef remove_accid_host_prefix_from_model(params, model, context, **kwargs):\n    \"\"\"Removes the `{AccountId}.` prefix from the operation model.\n\n    This change is applied to the operation model during the first time the\n    operation is invoked and then stays in effect for the lifetime of the\n    client object.\n\n    When the ruleset based endpoint resolver is in effect, both the endpoint\n    ruleset AND the service model place the {AccountId}. prefix in the URL.\n    The result is an invalid endpoint. This handler modifies the operation\n    model to remove the `endpoint.hostPrefix` field while leaving the\n    `RequiresAccountId` static context parameter in place.\n    \"\"\"\n    has_ctx_param = any(\n        ctx_param.name == 'RequiresAccountId' and ctx_param.value is True\n        for ctx_param in model.static_context_parameters\n    )\n    if (\n        model.endpoint is not None\n        and model.endpoint.get('hostPrefix') == '{AccountId}.'\n        and has_ctx_param\n    ):\n        del model.endpoint['hostPrefix']\n\n\ndef remove_arn_from_signing_path(request, **kwargs):\n    auth_path = request.auth_path\n    if isinstance(auth_path, str) and auth_path.startswith('/arn%3A'):\n        auth_path_parts = auth_path.split('/')\n        if len(auth_path_parts) > 1 and ArnParser.is_arn(\n            unquote(auth_path_parts[1])\n        ):\n            request.auth_path = '/'.join(['', *auth_path_parts[2:]])\n\n\ndef customize_endpoint_resolver_builtins(\n    builtins, model, params, context, **kwargs\n):\n    \"\"\"Modify builtin parameter values for endpoint resolver\n\n    Modifies the builtins dict in place. Changes are in effect for one call.\n    The corresponding event is emitted only if at least one builtin parameter\n    value is required for endpoint resolution for the operation.\n    \"\"\"\n    bucket_name = params.get('Bucket')\n    bucket_is_arn = bucket_name is not None and ArnParser.is_arn(bucket_name)\n    # In some situations the host will return AuthorizationHeaderMalformed\n    # when the signing region of a sigv4 request is not the bucket's\n    # region (which is likely unknown by the user of GetBucketLocation).\n    # Avoid this by always using path-style addressing.\n    if model.name == 'GetBucketLocation':\n        builtins[EndpointResolverBuiltins.AWS_S3_FORCE_PATH_STYLE] = True\n    # All situations where the bucket name is an ARN are not compatible\n    # with path style addressing.\n    elif bucket_is_arn:\n        builtins[EndpointResolverBuiltins.AWS_S3_FORCE_PATH_STYLE] = False\n\n    # Bucket names that are invalid host labels require path-style addressing.\n    # If path-style addressing was specifically requested, the default builtin\n    # value is already set.\n    path_style_required = (\n        bucket_name is not None and not VALID_HOST_LABEL_RE.match(bucket_name)\n    )\n    path_style_requested = builtins[\n        EndpointResolverBuiltins.AWS_S3_FORCE_PATH_STYLE\n    ]\n\n    # Path-style addressing is incompatible with the global endpoint for\n    # presigned URLs. If the bucket name is an ARN, the ARN's region should be\n    # used in the endpoint.\n    if (\n        context.get('use_global_endpoint')\n        and not path_style_required\n        and not path_style_requested\n        and not bucket_is_arn\n        and not utils.is_s3express_bucket(bucket_name)\n    ):\n        builtins[EndpointResolverBuiltins.AWS_REGION] = 'aws-global'\n        builtins[EndpointResolverBuiltins.AWS_S3_USE_GLOBAL_ENDPOINT] = True\n\n\ndef remove_content_type_header_for_presigning(request, **kwargs):\n    if (\n        request.context.get('is_presign_request') is True\n        and 'Content-Type' in request.headers\n    ):\n        del request.headers['Content-Type']\n\n\n# This is a list of (event_name, handler).\n# When a Session is created, everything in this list will be\n# automatically registered with that Session.\n\nBUILTIN_HANDLERS = [\n    ('choose-service-name', handle_service_name_alias),\n    (\n        'getattr.mturk.list_hi_ts_for_qualification_type',\n        ClientMethodAlias('list_hits_for_qualification_type'),\n    ),\n    (\n        'before-parameter-build.s3.UploadPart',\n        convert_body_to_file_like_object,\n        REGISTER_LAST,\n    ),\n    (\n        'before-parameter-build.s3.PutObject',\n        convert_body_to_file_like_object,\n        REGISTER_LAST,\n    ),\n    ('creating-client-class', add_generate_presigned_url),\n    ('creating-client-class.s3', add_generate_presigned_post),\n    ('creating-client-class.iot-data', check_openssl_supports_tls_version_1_2),\n    ('creating-client-class.lex-runtime-v2', remove_lex_v2_start_conversation),\n    ('creating-client-class.qbusiness', remove_qbusiness_chat),\n    ('after-call.iam', json_decode_policies),\n    ('after-call.ec2.GetConsoleOutput', decode_console_output),\n    ('after-call.cloudformation.GetTemplate', json_decode_template_body),\n    ('after-call.s3.GetBucketLocation', parse_get_bucket_location),\n    ('before-parameter-build', generate_idempotent_uuid),\n    ('before-parameter-build.s3', validate_bucket_name),\n    ('before-parameter-build.s3', remove_bucket_from_url_paths_from_model),\n    (\n        'before-parameter-build.s3.ListObjects',\n        set_list_objects_encoding_type_url,\n    ),\n    (\n        'before-parameter-build.s3.ListObjectsV2',\n        set_list_objects_encoding_type_url,\n    ),\n    (\n        'before-parameter-build.s3.ListObjectVersions',\n        set_list_objects_encoding_type_url,\n    ),\n    ('before-parameter-build.s3.CopyObject', handle_copy_source_param),\n    ('before-parameter-build.s3.UploadPartCopy', handle_copy_source_param),\n    ('before-parameter-build.s3.CopyObject', validate_ascii_metadata),\n    ('before-parameter-build.s3.PutObject', validate_ascii_metadata),\n    (\n        'before-parameter-build.s3.CreateMultipartUpload',\n        validate_ascii_metadata,\n    ),\n    ('before-parameter-build.s3-control', remove_accid_host_prefix_from_model),\n    ('docs.*.s3.CopyObject.complete-section', document_copy_source_form),\n    ('docs.*.s3.UploadPartCopy.complete-section', document_copy_source_form),\n    ('before-endpoint-resolution.s3', customize_endpoint_resolver_builtins),\n    ('before-call', add_recursion_detection_header),\n    ('before-call.s3', add_expect_header),\n    ('before-call.glacier', add_glacier_version),\n    ('before-call.apigateway', add_accept_header),\n    ('before-call.s3.PutObject', conditionally_calculate_checksum),\n    ('before-call.s3.UploadPart', conditionally_calculate_md5),\n    ('before-call.s3.DeleteObjects', escape_xml_payload),\n    ('before-call.s3.DeleteObjects', conditionally_calculate_checksum),\n    ('before-call.s3.PutBucketLifecycleConfiguration', escape_xml_payload),\n    ('before-call.glacier.UploadArchive', add_glacier_checksums),\n    ('before-call.glacier.UploadMultipartPart', add_glacier_checksums),\n    ('before-call.ec2.CopySnapshot', inject_presigned_url_ec2),\n    ('request-created', add_retry_headers),\n    ('request-created.machinelearning.Predict', switch_host_machinelearning),\n    ('needs-retry.s3.UploadPartCopy', check_for_200_error, REGISTER_FIRST),\n    ('needs-retry.s3.CopyObject', check_for_200_error, REGISTER_FIRST),\n    (\n        'needs-retry.s3.CompleteMultipartUpload',\n        check_for_200_error,\n        REGISTER_FIRST,\n    ),\n    ('choose-signer.cognito-identity.GetId', disable_signing),\n    ('choose-signer.cognito-identity.GetOpenIdToken', disable_signing),\n    ('choose-signer.cognito-identity.UnlinkIdentity', disable_signing),\n    (\n        'choose-signer.cognito-identity.GetCredentialsForIdentity',\n        disable_signing,\n    ),\n    ('choose-signer.sts.AssumeRoleWithSAML', disable_signing),\n    ('choose-signer.sts.AssumeRoleWithWebIdentity', disable_signing),\n    ('choose-signer', set_operation_specific_signer),\n    ('before-parameter-build.s3.HeadObject', sse_md5),\n    ('before-parameter-build.s3.GetObject', sse_md5),\n    ('before-parameter-build.s3.PutObject', sse_md5),\n    ('before-parameter-build.s3.CopyObject', sse_md5),\n    ('before-parameter-build.s3.CopyObject', copy_source_sse_md5),\n    ('before-parameter-build.s3.CreateMultipartUpload', sse_md5),\n    ('before-parameter-build.s3.UploadPart', sse_md5),\n    ('before-parameter-build.s3.UploadPartCopy', sse_md5),\n    ('before-parameter-build.s3.UploadPartCopy', copy_source_sse_md5),\n    ('before-parameter-build.s3.CompleteMultipartUpload', sse_md5),\n    ('before-parameter-build.s3.SelectObjectContent', sse_md5),\n    ('before-parameter-build.ec2.RunInstances', base64_encode_user_data),\n    (\n        'before-parameter-build.autoscaling.CreateLaunchConfiguration',\n        base64_encode_user_data,\n    ),\n    ('before-parameter-build.route53', fix_route53_ids),\n    ('before-parameter-build.glacier', inject_account_id),\n    ('before-sign.s3', remove_arn_from_signing_path),\n    (\n        'before-sign.polly.SynthesizeSpeech',\n        remove_content_type_header_for_presigning,\n    ),\n    ('after-call.s3.ListObjects', decode_list_object),\n    ('after-call.s3.ListObjectsV2', decode_list_object_v2),\n    ('after-call.s3.ListObjectVersions', decode_list_object_versions),\n    # Cloudsearchdomain search operation will be sent by HTTP POST\n    ('request-created.cloudsearchdomain.Search', change_get_to_post),\n    # Glacier documentation customizations\n    (\n        'docs.*.glacier.*.complete-section',\n        AutoPopulatedParam(\n            'accountId',\n            'Note: this parameter is set to \"-\" by'\n            'default if no value is not specified.',\n        ).document_auto_populated_param,\n    ),\n    (\n        'docs.*.glacier.UploadArchive.complete-section',\n        AutoPopulatedParam('checksum').document_auto_populated_param,\n    ),\n    (\n        'docs.*.glacier.UploadMultipartPart.complete-section',\n        AutoPopulatedParam('checksum').document_auto_populated_param,\n    ),\n    (\n        'docs.request-params.glacier.CompleteMultipartUpload.complete-section',\n        document_glacier_tree_hash_checksum(),\n    ),\n    # Cloudformation documentation customizations\n    (\n        'docs.*.cloudformation.GetTemplate.complete-section',\n        document_cloudformation_get_template_return_type,\n    ),\n    # UserData base64 encoding documentation customizations\n    (\n        'docs.*.ec2.RunInstances.complete-section',\n        document_base64_encoding('UserData'),\n    ),\n    (\n        'docs.*.autoscaling.CreateLaunchConfiguration.complete-section',\n        document_base64_encoding('UserData'),\n    ),\n    # EC2 CopySnapshot documentation customizations\n    (\n        'docs.*.ec2.CopySnapshot.complete-section',\n        AutoPopulatedParam('PresignedUrl').document_auto_populated_param,\n    ),\n    (\n        'docs.*.ec2.CopySnapshot.complete-section',\n        AutoPopulatedParam('DestinationRegion').document_auto_populated_param,\n    ),\n    # S3 SSE documentation modifications\n    (\n        'docs.*.s3.*.complete-section',\n        AutoPopulatedParam('SSECustomerKeyMD5').document_auto_populated_param,\n    ),\n    # S3 SSE Copy Source documentation modifications\n    (\n        'docs.*.s3.*.complete-section',\n        AutoPopulatedParam(\n            'CopySourceSSECustomerKeyMD5'\n        ).document_auto_populated_param,\n    ),\n    # Add base64 information to Lambda\n    (\n        'docs.*.lambda.UpdateFunctionCode.complete-section',\n        document_base64_encoding('ZipFile'),\n    ),\n    # The following S3 operations cannot actually accept a ContentMD5\n    (\n        'docs.*.s3.*.complete-section',\n        HideParamFromOperations(\n            's3',\n            'ContentMD5',\n            [\n                'DeleteObjects',\n                'PutBucketAcl',\n                'PutBucketCors',\n                'PutBucketLifecycle',\n                'PutBucketLogging',\n                'PutBucketNotification',\n                'PutBucketPolicy',\n                'PutBucketReplication',\n                'PutBucketRequestPayment',\n                'PutBucketTagging',\n                'PutBucketVersioning',\n                'PutBucketWebsite',\n                'PutObjectAcl',\n            ],\n        ).hide_param,\n    ),\n    #############\n    # RDS\n    #############\n    ('creating-client-class.rds', add_generate_db_auth_token),\n    ('before-call.rds.CopyDBClusterSnapshot', inject_presigned_url_rds),\n    ('before-call.rds.CreateDBCluster', inject_presigned_url_rds),\n    ('before-call.rds.CopyDBSnapshot', inject_presigned_url_rds),\n    ('before-call.rds.CreateDBInstanceReadReplica', inject_presigned_url_rds),\n    (\n        'before-call.rds.StartDBInstanceAutomatedBackupsReplication',\n        inject_presigned_url_rds,\n    ),\n    # RDS PresignedUrl documentation customizations\n    (\n        'docs.*.rds.CopyDBClusterSnapshot.complete-section',\n        AutoPopulatedParam('PreSignedUrl').document_auto_populated_param,\n    ),\n    (\n        'docs.*.rds.CreateDBCluster.complete-section',\n        AutoPopulatedParam('PreSignedUrl').document_auto_populated_param,\n    ),\n    (\n        'docs.*.rds.CopyDBSnapshot.complete-section',\n        AutoPopulatedParam('PreSignedUrl').document_auto_populated_param,\n    ),\n    (\n        'docs.*.rds.CreateDBInstanceReadReplica.complete-section',\n        AutoPopulatedParam('PreSignedUrl').document_auto_populated_param,\n    ),\n    (\n        'docs.*.rds.StartDBInstanceAutomatedBackupsReplication.complete-section',\n        AutoPopulatedParam('PreSignedUrl').document_auto_populated_param,\n    ),\n    #############\n    # Neptune\n    #############\n    ('before-call.neptune.CopyDBClusterSnapshot', inject_presigned_url_rds),\n    ('before-call.neptune.CreateDBCluster', inject_presigned_url_rds),\n    # Neptune PresignedUrl documentation customizations\n    (\n        'docs.*.neptune.CopyDBClusterSnapshot.complete-section',\n        AutoPopulatedParam('PreSignedUrl').document_auto_populated_param,\n    ),\n    (\n        'docs.*.neptune.CreateDBCluster.complete-section',\n        AutoPopulatedParam('PreSignedUrl').document_auto_populated_param,\n    ),\n    #############\n    # DocDB\n    #############\n    ('before-call.docdb.CopyDBClusterSnapshot', inject_presigned_url_rds),\n    ('before-call.docdb.CreateDBCluster', inject_presigned_url_rds),\n    # DocDB PresignedUrl documentation customizations\n    (\n        'docs.*.docdb.CopyDBClusterSnapshot.complete-section',\n        AutoPopulatedParam('PreSignedUrl').document_auto_populated_param,\n    ),\n    (\n        'docs.*.docdb.CreateDBCluster.complete-section',\n        AutoPopulatedParam('PreSignedUrl').document_auto_populated_param,\n    ),\n    ('before-call', inject_api_version_header_if_needed),\n]\n_add_parameter_aliases(BUILTIN_HANDLERS)\n", "botocore/args.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"Internal module to help with normalizing botocore client args.\n\nThis module (and all function/classes within this module) should be\nconsidered internal, and *not* a public API.\n\n\"\"\"\nimport copy\nimport logging\nimport socket\n\nimport botocore.exceptions\nimport botocore.parsers\nimport botocore.serialize\nfrom botocore.config import Config\nfrom botocore.endpoint import EndpointCreator\nfrom botocore.regions import EndpointResolverBuiltins as EPRBuiltins\nfrom botocore.regions import EndpointRulesetResolver\nfrom botocore.signers import RequestSigner\nfrom botocore.useragent import UserAgentString\nfrom botocore.utils import ensure_boolean, is_s3_accelerate_url\n\nlogger = logging.getLogger(__name__)\n\n\nVALID_REGIONAL_ENDPOINTS_CONFIG = [\n    'legacy',\n    'regional',\n]\nLEGACY_GLOBAL_STS_REGIONS = [\n    'ap-northeast-1',\n    'ap-south-1',\n    'ap-southeast-1',\n    'ap-southeast-2',\n    'aws-global',\n    'ca-central-1',\n    'eu-central-1',\n    'eu-north-1',\n    'eu-west-1',\n    'eu-west-2',\n    'eu-west-3',\n    'sa-east-1',\n    'us-east-1',\n    'us-east-2',\n    'us-west-1',\n    'us-west-2',\n]\n# Maximum allowed length of the ``user_agent_appid`` config field. Longer\n# values result in a warning-level log message.\nUSERAGENT_APPID_MAXLEN = 50\n\n\nclass ClientArgsCreator:\n    def __init__(\n        self,\n        event_emitter,\n        user_agent,\n        response_parser_factory,\n        loader,\n        exceptions_factory,\n        config_store,\n        user_agent_creator=None,\n    ):\n        self._event_emitter = event_emitter\n        self._response_parser_factory = response_parser_factory\n        self._loader = loader\n        self._exceptions_factory = exceptions_factory\n        self._config_store = config_store\n        if user_agent_creator is None:\n            self._session_ua_creator = UserAgentString.from_environment()\n        else:\n            self._session_ua_creator = user_agent_creator\n\n    def get_client_args(\n        self,\n        service_model,\n        region_name,\n        is_secure,\n        endpoint_url,\n        verify,\n        credentials,\n        scoped_config,\n        client_config,\n        endpoint_bridge,\n        auth_token=None,\n        endpoints_ruleset_data=None,\n        partition_data=None,\n    ):\n        final_args = self.compute_client_args(\n            service_model,\n            client_config,\n            endpoint_bridge,\n            region_name,\n            endpoint_url,\n            is_secure,\n            scoped_config,\n        )\n\n        service_name = final_args['service_name']  # noqa\n        parameter_validation = final_args['parameter_validation']\n        endpoint_config = final_args['endpoint_config']\n        protocol = final_args['protocol']\n        config_kwargs = final_args['config_kwargs']\n        s3_config = final_args['s3_config']\n        partition = endpoint_config['metadata'].get('partition', None)\n        socket_options = final_args['socket_options']\n        configured_endpoint_url = final_args['configured_endpoint_url']\n        signing_region = endpoint_config['signing_region']\n        endpoint_region_name = endpoint_config['region_name']\n\n        event_emitter = copy.copy(self._event_emitter)\n        signer = RequestSigner(\n            service_model.service_id,\n            signing_region,\n            endpoint_config['signing_name'],\n            endpoint_config['signature_version'],\n            credentials,\n            event_emitter,\n            auth_token,\n        )\n\n        config_kwargs['s3'] = s3_config\n        new_config = Config(**config_kwargs)\n        endpoint_creator = EndpointCreator(event_emitter)\n\n        endpoint = endpoint_creator.create_endpoint(\n            service_model,\n            region_name=endpoint_region_name,\n            endpoint_url=endpoint_config['endpoint_url'],\n            verify=verify,\n            response_parser_factory=self._response_parser_factory,\n            max_pool_connections=new_config.max_pool_connections,\n            proxies=new_config.proxies,\n            timeout=(new_config.connect_timeout, new_config.read_timeout),\n            socket_options=socket_options,\n            client_cert=new_config.client_cert,\n            proxies_config=new_config.proxies_config,\n        )\n\n        serializer = botocore.serialize.create_serializer(\n            protocol, parameter_validation\n        )\n        response_parser = botocore.parsers.create_parser(protocol)\n\n        ruleset_resolver = self._build_endpoint_resolver(\n            endpoints_ruleset_data,\n            partition_data,\n            client_config,\n            service_model,\n            endpoint_region_name,\n            region_name,\n            configured_endpoint_url,\n            endpoint,\n            is_secure,\n            endpoint_bridge,\n            event_emitter,\n        )\n\n        # Copy the session's user agent factory and adds client configuration.\n        client_ua_creator = self._session_ua_creator.with_client_config(\n            new_config\n        )\n        supplied_ua = client_config.user_agent if client_config else None\n        new_config._supplied_user_agent = supplied_ua\n\n        return {\n            'serializer': serializer,\n            'endpoint': endpoint,\n            'response_parser': response_parser,\n            'event_emitter': event_emitter,\n            'request_signer': signer,\n            'service_model': service_model,\n            'loader': self._loader,\n            'client_config': new_config,\n            'partition': partition,\n            'exceptions_factory': self._exceptions_factory,\n            'endpoint_ruleset_resolver': ruleset_resolver,\n            'user_agent_creator': client_ua_creator,\n        }\n\n    def compute_client_args(\n        self,\n        service_model,\n        client_config,\n        endpoint_bridge,\n        region_name,\n        endpoint_url,\n        is_secure,\n        scoped_config,\n    ):\n        service_name = service_model.endpoint_prefix\n        protocol = service_model.metadata['protocol']\n        parameter_validation = True\n        if client_config and not client_config.parameter_validation:\n            parameter_validation = False\n        elif scoped_config:\n            raw_value = scoped_config.get('parameter_validation')\n            if raw_value is not None:\n                parameter_validation = ensure_boolean(raw_value)\n\n        s3_config = self.compute_s3_config(client_config)\n\n        configured_endpoint_url = self._compute_configured_endpoint_url(\n            client_config=client_config,\n            endpoint_url=endpoint_url,\n        )\n\n        endpoint_config = self._compute_endpoint_config(\n            service_name=service_name,\n            region_name=region_name,\n            endpoint_url=configured_endpoint_url,\n            is_secure=is_secure,\n            endpoint_bridge=endpoint_bridge,\n            s3_config=s3_config,\n        )\n        endpoint_variant_tags = endpoint_config['metadata'].get('tags', [])\n\n        # Some third-party libraries expect the final user-agent string in\n        # ``client.meta.config.user_agent``. To maintain backwards\n        # compatibility, the preliminary user-agent string (before any Config\n        # object modifications and without request-specific user-agent\n        # components) is stored in the new Config object's ``user_agent``\n        # property but not used by Botocore itself.\n        preliminary_ua_string = self._session_ua_creator.with_client_config(\n            client_config\n        ).to_string()\n        # Create a new client config to be passed to the client based\n        # on the final values. We do not want the user to be able\n        # to try to modify an existing client with a client config.\n        config_kwargs = dict(\n            region_name=endpoint_config['region_name'],\n            signature_version=endpoint_config['signature_version'],\n            user_agent=preliminary_ua_string,\n        )\n        if 'dualstack' in endpoint_variant_tags:\n            config_kwargs.update(use_dualstack_endpoint=True)\n        if 'fips' in endpoint_variant_tags:\n            config_kwargs.update(use_fips_endpoint=True)\n        if client_config is not None:\n            config_kwargs.update(\n                connect_timeout=client_config.connect_timeout,\n                read_timeout=client_config.read_timeout,\n                max_pool_connections=client_config.max_pool_connections,\n                proxies=client_config.proxies,\n                proxies_config=client_config.proxies_config,\n                retries=client_config.retries,\n                client_cert=client_config.client_cert,\n                inject_host_prefix=client_config.inject_host_prefix,\n                tcp_keepalive=client_config.tcp_keepalive,\n                user_agent_extra=client_config.user_agent_extra,\n                user_agent_appid=client_config.user_agent_appid,\n                request_min_compression_size_bytes=(\n                    client_config.request_min_compression_size_bytes\n                ),\n                disable_request_compression=(\n                    client_config.disable_request_compression\n                ),\n                client_context_params=client_config.client_context_params,\n            )\n        self._compute_retry_config(config_kwargs)\n        self._compute_connect_timeout(config_kwargs)\n        self._compute_user_agent_appid_config(config_kwargs)\n        self._compute_request_compression_config(config_kwargs)\n        s3_config = self.compute_s3_config(client_config)\n\n        is_s3_service = self._is_s3_service(service_name)\n\n        if is_s3_service and 'dualstack' in endpoint_variant_tags:\n            if s3_config is None:\n                s3_config = {}\n            s3_config['use_dualstack_endpoint'] = True\n\n        return {\n            'service_name': service_name,\n            'parameter_validation': parameter_validation,\n            'configured_endpoint_url': configured_endpoint_url,\n            'endpoint_config': endpoint_config,\n            'protocol': protocol,\n            'config_kwargs': config_kwargs,\n            's3_config': s3_config,\n            'socket_options': self._compute_socket_options(\n                scoped_config, client_config\n            ),\n        }\n\n    def _compute_configured_endpoint_url(self, client_config, endpoint_url):\n        if endpoint_url is not None:\n            return endpoint_url\n\n        if self._ignore_configured_endpoint_urls(client_config):\n            logger.debug(\"Ignoring configured endpoint URLs.\")\n            return endpoint_url\n\n        return self._config_store.get_config_variable('endpoint_url')\n\n    def _ignore_configured_endpoint_urls(self, client_config):\n        if (\n            client_config\n            and client_config.ignore_configured_endpoint_urls is not None\n        ):\n            return client_config.ignore_configured_endpoint_urls\n\n        return self._config_store.get_config_variable(\n            'ignore_configured_endpoint_urls'\n        )\n\n    def compute_s3_config(self, client_config):\n        s3_configuration = self._config_store.get_config_variable('s3')\n\n        # Next specific client config values takes precedence over\n        # specific values in the scoped config.\n        if client_config is not None:\n            if client_config.s3 is not None:\n                if s3_configuration is None:\n                    s3_configuration = client_config.s3\n                else:\n                    # The current s3_configuration dictionary may be\n                    # from a source that only should be read from so\n                    # we want to be safe and just make a copy of it to modify\n                    # before it actually gets updated.\n                    s3_configuration = s3_configuration.copy()\n                    s3_configuration.update(client_config.s3)\n\n        return s3_configuration\n\n    def _is_s3_service(self, service_name):\n        \"\"\"Whether the service is S3 or S3 Control.\n\n        Note that throughout this class, service_name refers to the endpoint\n        prefix, not the folder name of the service in botocore/data. For\n        S3 Control, the folder name is 's3control' but the endpoint prefix is\n        's3-control'.\n        \"\"\"\n        return service_name in ['s3', 's3-control']\n\n    def _compute_endpoint_config(\n        self,\n        service_name,\n        region_name,\n        endpoint_url,\n        is_secure,\n        endpoint_bridge,\n        s3_config,\n    ):\n        resolve_endpoint_kwargs = {\n            'service_name': service_name,\n            'region_name': region_name,\n            'endpoint_url': endpoint_url,\n            'is_secure': is_secure,\n            'endpoint_bridge': endpoint_bridge,\n        }\n        if service_name == 's3':\n            return self._compute_s3_endpoint_config(\n                s3_config=s3_config, **resolve_endpoint_kwargs\n            )\n        if service_name == 'sts':\n            return self._compute_sts_endpoint_config(**resolve_endpoint_kwargs)\n        return self._resolve_endpoint(**resolve_endpoint_kwargs)\n\n    def _compute_s3_endpoint_config(\n        self, s3_config, **resolve_endpoint_kwargs\n    ):\n        force_s3_global = self._should_force_s3_global(\n            resolve_endpoint_kwargs['region_name'], s3_config\n        )\n        if force_s3_global:\n            resolve_endpoint_kwargs['region_name'] = None\n        endpoint_config = self._resolve_endpoint(**resolve_endpoint_kwargs)\n        self._set_region_if_custom_s3_endpoint(\n            endpoint_config, resolve_endpoint_kwargs['endpoint_bridge']\n        )\n        # For backwards compatibility reasons, we want to make sure the\n        # client.meta.region_name will remain us-east-1 if we forced the\n        # endpoint to be the global region. Specifically, if this value\n        # changes to aws-global, it breaks logic where a user is checking\n        # for us-east-1 as the global endpoint such as in creating buckets.\n        if force_s3_global and endpoint_config['region_name'] == 'aws-global':\n            endpoint_config['region_name'] = 'us-east-1'\n        return endpoint_config\n\n    def _should_force_s3_global(self, region_name, s3_config):\n        s3_regional_config = 'legacy'\n        if s3_config and 'us_east_1_regional_endpoint' in s3_config:\n            s3_regional_config = s3_config['us_east_1_regional_endpoint']\n            self._validate_s3_regional_config(s3_regional_config)\n\n        is_global_region = region_name in ('us-east-1', None)\n        return s3_regional_config == 'legacy' and is_global_region\n\n    def _validate_s3_regional_config(self, config_val):\n        if config_val not in VALID_REGIONAL_ENDPOINTS_CONFIG:\n            raise botocore.exceptions.InvalidS3UsEast1RegionalEndpointConfigError(\n                s3_us_east_1_regional_endpoint_config=config_val\n            )\n\n    def _set_region_if_custom_s3_endpoint(\n        self, endpoint_config, endpoint_bridge\n    ):\n        # If a user is providing a custom URL, the endpoint resolver will\n        # refuse to infer a signing region. If we want to default to s3v4,\n        # we have to account for this.\n        if (\n            endpoint_config['signing_region'] is None\n            and endpoint_config['region_name'] is None\n        ):\n            endpoint = endpoint_bridge.resolve('s3')\n            endpoint_config['signing_region'] = endpoint['signing_region']\n            endpoint_config['region_name'] = endpoint['region_name']\n\n    def _compute_sts_endpoint_config(self, **resolve_endpoint_kwargs):\n        endpoint_config = self._resolve_endpoint(**resolve_endpoint_kwargs)\n        if self._should_set_global_sts_endpoint(\n            resolve_endpoint_kwargs['region_name'],\n            resolve_endpoint_kwargs['endpoint_url'],\n            endpoint_config,\n        ):\n            self._set_global_sts_endpoint(\n                endpoint_config, resolve_endpoint_kwargs['is_secure']\n            )\n        return endpoint_config\n\n    def _should_set_global_sts_endpoint(\n        self, region_name, endpoint_url, endpoint_config\n    ):\n        has_variant_tags = endpoint_config and endpoint_config.get(\n            'metadata', {}\n        ).get('tags')\n        if endpoint_url or has_variant_tags:\n            return False\n        return (\n            self._get_sts_regional_endpoints_config() == 'legacy'\n            and region_name in LEGACY_GLOBAL_STS_REGIONS\n        )\n\n    def _get_sts_regional_endpoints_config(self):\n        sts_regional_endpoints_config = self._config_store.get_config_variable(\n            'sts_regional_endpoints'\n        )\n        if not sts_regional_endpoints_config:\n            sts_regional_endpoints_config = 'legacy'\n        if (\n            sts_regional_endpoints_config\n            not in VALID_REGIONAL_ENDPOINTS_CONFIG\n        ):\n            raise botocore.exceptions.InvalidSTSRegionalEndpointsConfigError(\n                sts_regional_endpoints_config=sts_regional_endpoints_config\n            )\n        return sts_regional_endpoints_config\n\n    def _set_global_sts_endpoint(self, endpoint_config, is_secure):\n        scheme = 'https' if is_secure else 'http'\n        endpoint_config['endpoint_url'] = '%s://sts.amazonaws.com' % scheme\n        endpoint_config['signing_region'] = 'us-east-1'\n\n    def _resolve_endpoint(\n        self,\n        service_name,\n        region_name,\n        endpoint_url,\n        is_secure,\n        endpoint_bridge,\n    ):\n        return endpoint_bridge.resolve(\n            service_name, region_name, endpoint_url, is_secure\n        )\n\n    def _compute_socket_options(self, scoped_config, client_config=None):\n        # This disables Nagle's algorithm and is the default socket options\n        # in urllib3.\n        socket_options = [(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)]\n        client_keepalive = client_config and client_config.tcp_keepalive\n        scoped_keepalive = scoped_config and self._ensure_boolean(\n            scoped_config.get(\"tcp_keepalive\", False)\n        )\n        # Enables TCP Keepalive if specified in client config object or shared config file.\n        if client_keepalive or scoped_keepalive:\n            socket_options.append((socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1))\n        return socket_options\n\n    def _compute_retry_config(self, config_kwargs):\n        self._compute_retry_max_attempts(config_kwargs)\n        self._compute_retry_mode(config_kwargs)\n\n    def _compute_retry_max_attempts(self, config_kwargs):\n        # There's a pre-existing max_attempts client config value that actually\n        # means max *retry* attempts.  There's also a `max_attempts` we pull\n        # from the config store that means *total attempts*, which includes the\n        # intitial request.  We can't change what `max_attempts` means in\n        # client config so we try to normalize everything to a new\n        # \"total_max_attempts\" variable.  We ensure that after this, the only\n        # configuration for \"max attempts\" is the 'total_max_attempts' key.\n        # An explicitly provided max_attempts in the client config\n        # overrides everything.\n        retries = config_kwargs.get('retries')\n        if retries is not None:\n            if 'total_max_attempts' in retries:\n                retries.pop('max_attempts', None)\n                return\n            if 'max_attempts' in retries:\n                value = retries.pop('max_attempts')\n                # client config max_attempts means total retries so we\n                # have to add one for 'total_max_attempts' to account\n                # for the initial request.\n                retries['total_max_attempts'] = value + 1\n                return\n        # Otherwise we'll check the config store which checks env vars,\n        # config files, etc.  There is no default value for max_attempts\n        # so if this returns None and we don't set a default value here.\n        max_attempts = self._config_store.get_config_variable('max_attempts')\n        if max_attempts is not None:\n            if retries is None:\n                retries = {}\n                config_kwargs['retries'] = retries\n            retries['total_max_attempts'] = max_attempts\n\n    def _compute_retry_mode(self, config_kwargs):\n        retries = config_kwargs.get('retries')\n        if retries is None:\n            retries = {}\n            config_kwargs['retries'] = retries\n        elif 'mode' in retries:\n            # If there's a retry mode explicitly set in the client config\n            # that overrides everything.\n            return\n        retry_mode = self._config_store.get_config_variable('retry_mode')\n        if retry_mode is None:\n            retry_mode = 'legacy'\n        retries['mode'] = retry_mode\n\n    def _compute_connect_timeout(self, config_kwargs):\n        # Checking if connect_timeout is set on the client config.\n        # If it is not, we check the config_store in case a\n        # non legacy default mode has been configured.\n        connect_timeout = config_kwargs.get('connect_timeout')\n        if connect_timeout is not None:\n            return\n        connect_timeout = self._config_store.get_config_variable(\n            'connect_timeout'\n        )\n        if connect_timeout:\n            config_kwargs['connect_timeout'] = connect_timeout\n\n    def _compute_request_compression_config(self, config_kwargs):\n        min_size = config_kwargs.get('request_min_compression_size_bytes')\n        disabled = config_kwargs.get('disable_request_compression')\n        if min_size is None:\n            min_size = self._config_store.get_config_variable(\n                'request_min_compression_size_bytes'\n            )\n        # conversion func is skipped so input validation must be done here\n        # regardless if the value is coming from the config store or the\n        # config object\n        min_size = self._validate_min_compression_size(min_size)\n        config_kwargs['request_min_compression_size_bytes'] = min_size\n\n        if disabled is None:\n            disabled = self._config_store.get_config_variable(\n                'disable_request_compression'\n            )\n        else:\n            # if the user provided a value we must check if it's a boolean\n            disabled = ensure_boolean(disabled)\n        config_kwargs['disable_request_compression'] = disabled\n\n    def _validate_min_compression_size(self, min_size):\n        min_allowed_min_size = 1\n        max_allowed_min_size = 1048576\n        if min_size is not None:\n            error_msg_base = (\n                f'Invalid value \"{min_size}\" for '\n                'request_min_compression_size_bytes.'\n            )\n            try:\n                min_size = int(min_size)\n            except (ValueError, TypeError):\n                msg = (\n                    f'{error_msg_base} Value must be an integer. '\n                    f'Received {type(min_size)} instead.'\n                )\n                raise botocore.exceptions.InvalidConfigError(error_msg=msg)\n            if not min_allowed_min_size <= min_size <= max_allowed_min_size:\n                msg = (\n                    f'{error_msg_base} Value must be between '\n                    f'{min_allowed_min_size} and {max_allowed_min_size}.'\n                )\n                raise botocore.exceptions.InvalidConfigError(error_msg=msg)\n\n        return min_size\n\n    def _ensure_boolean(self, val):\n        if isinstance(val, bool):\n            return val\n        else:\n            return val.lower() == 'true'\n\n    def _build_endpoint_resolver(\n        self,\n        endpoints_ruleset_data,\n        partition_data,\n        client_config,\n        service_model,\n        endpoint_region_name,\n        region_name,\n        endpoint_url,\n        endpoint,\n        is_secure,\n        endpoint_bridge,\n        event_emitter,\n    ):\n        if endpoints_ruleset_data is None:\n            return None\n\n        # The legacy EndpointResolver is global to the session, but\n        # EndpointRulesetResolver is service-specific. Builtins for\n        # EndpointRulesetResolver must not be derived from the legacy\n        # endpoint resolver's output, including final_args, s3_config,\n        # etc.\n        s3_config_raw = self.compute_s3_config(client_config) or {}\n        service_name_raw = service_model.endpoint_prefix\n        # Maintain complex logic for s3 and sts endpoints for backwards\n        # compatibility.\n        if service_name_raw in ['s3', 'sts'] or region_name is None:\n            eprv2_region_name = endpoint_region_name\n        else:\n            eprv2_region_name = region_name\n        resolver_builtins = self.compute_endpoint_resolver_builtin_defaults(\n            region_name=eprv2_region_name,\n            service_name=service_name_raw,\n            s3_config=s3_config_raw,\n            endpoint_bridge=endpoint_bridge,\n            client_endpoint_url=endpoint_url,\n            legacy_endpoint_url=endpoint.host,\n        )\n        # Client context params for s3 conflict with the available settings\n        # in the `s3` parameter on the `Config` object. If the same parameter\n        # is set in both places, the value in the `s3` parameter takes priority.\n        if client_config is not None:\n            client_context = client_config.client_context_params or {}\n        else:\n            client_context = {}\n        if self._is_s3_service(service_name_raw):\n            client_context.update(s3_config_raw)\n\n        sig_version = (\n            client_config.signature_version\n            if client_config is not None\n            else None\n        )\n        return EndpointRulesetResolver(\n            endpoint_ruleset_data=endpoints_ruleset_data,\n            partition_data=partition_data,\n            service_model=service_model,\n            builtins=resolver_builtins,\n            client_context=client_context,\n            event_emitter=event_emitter,\n            use_ssl=is_secure,\n            requested_auth_scheme=sig_version,\n        )\n\n    def compute_endpoint_resolver_builtin_defaults(\n        self,\n        region_name,\n        service_name,\n        s3_config,\n        endpoint_bridge,\n        client_endpoint_url,\n        legacy_endpoint_url,\n    ):\n        # EndpointRulesetResolver rulesets may accept an \"SDK::Endpoint\" as\n        # input. If the endpoint_url argument of create_client() is set, it\n        # always takes priority.\n        if client_endpoint_url:\n            given_endpoint = client_endpoint_url\n        # If an endpoints.json data file other than the one bundled within\n        # the botocore/data directory is used, the output of legacy\n        # endpoint resolution is provided to EndpointRulesetResolver.\n        elif not endpoint_bridge.resolver_uses_builtin_data():\n            given_endpoint = legacy_endpoint_url\n        else:\n            given_endpoint = None\n\n        # The endpoint rulesets differ from legacy botocore behavior in whether\n        # forcing path style addressing in incompatible situations raises an\n        # exception or silently ignores the config setting. The\n        # AWS_S3_FORCE_PATH_STYLE parameter is adjusted both here and for each\n        # operation so that the ruleset behavior is backwards compatible.\n        if s3_config.get('use_accelerate_endpoint', False):\n            force_path_style = False\n        elif client_endpoint_url is not None and not is_s3_accelerate_url(\n            client_endpoint_url\n        ):\n            force_path_style = s3_config.get('addressing_style') != 'virtual'\n        else:\n            force_path_style = s3_config.get('addressing_style') == 'path'\n\n        return {\n            EPRBuiltins.AWS_REGION: region_name,\n            EPRBuiltins.AWS_USE_FIPS: (\n                # SDK_ENDPOINT cannot be combined with AWS_USE_FIPS\n                given_endpoint is None\n                # use legacy resolver's _resolve_endpoint_variant_config_var()\n                # or default to False if it returns None\n                and endpoint_bridge._resolve_endpoint_variant_config_var(\n                    'use_fips_endpoint'\n                )\n                or False\n            ),\n            EPRBuiltins.AWS_USE_DUALSTACK: (\n                # SDK_ENDPOINT cannot be combined with AWS_USE_DUALSTACK\n                given_endpoint is None\n                # use legacy resolver's _resolve_use_dualstack_endpoint() and\n                # or default to False if it returns None\n                and endpoint_bridge._resolve_use_dualstack_endpoint(\n                    service_name\n                )\n                or False\n            ),\n            EPRBuiltins.AWS_STS_USE_GLOBAL_ENDPOINT: (\n                self._should_set_global_sts_endpoint(\n                    region_name=region_name,\n                    endpoint_url=None,\n                    endpoint_config=None,\n                )\n            ),\n            EPRBuiltins.AWS_S3_USE_GLOBAL_ENDPOINT: (\n                self._should_force_s3_global(region_name, s3_config)\n            ),\n            EPRBuiltins.AWS_S3_ACCELERATE: s3_config.get(\n                'use_accelerate_endpoint', False\n            ),\n            EPRBuiltins.AWS_S3_FORCE_PATH_STYLE: force_path_style,\n            EPRBuiltins.AWS_S3_USE_ARN_REGION: s3_config.get(\n                'use_arn_region', True\n            ),\n            EPRBuiltins.AWS_S3CONTROL_USE_ARN_REGION: s3_config.get(\n                'use_arn_region', False\n            ),\n            EPRBuiltins.AWS_S3_DISABLE_MRAP: s3_config.get(\n                's3_disable_multiregion_access_points', False\n            ),\n            EPRBuiltins.SDK_ENDPOINT: given_endpoint,\n        }\n\n    def _compute_user_agent_appid_config(self, config_kwargs):\n        user_agent_appid = config_kwargs.get('user_agent_appid')\n        if user_agent_appid is None:\n            user_agent_appid = self._config_store.get_config_variable(\n                'user_agent_appid'\n            )\n        if (\n            user_agent_appid is not None\n            and len(user_agent_appid) > USERAGENT_APPID_MAXLEN\n        ):\n            logger.warning(\n                'The configured value for user_agent_appid exceeds the '\n                f'maximum length of {USERAGENT_APPID_MAXLEN} characters.'\n            )\n        config_kwargs['user_agent_appid'] = user_agent_appid\n", "botocore/__init__.py": "# Copyright (c) 2012-2013 Mitch Garnaat http://garnaat.org/\n# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport logging\nimport os\nimport re\n\n__version__ = '1.34.132'\n\n\nclass NullHandler(logging.Handler):\n    def emit(self, record):\n        pass\n\n\n# Configure default logger to do nothing\nlog = logging.getLogger('botocore')\nlog.addHandler(NullHandler())\n\n_INITIALIZERS = []\n\n_first_cap_regex = re.compile('(.)([A-Z][a-z]+)')\n_end_cap_regex = re.compile('([a-z0-9])([A-Z])')\n# The regex below handles the special case where some acronym\n# name is pluralized, e.g GatewayARNs, ListWebACLs, SomeCNAMEs.\n_special_case_transform = re.compile('[A-Z]{2,}s$')\n# Prepopulate the cache with special cases that don't match\n# our regular transformation.\n_xform_cache = {\n    ('CreateCachediSCSIVolume', '_'): 'create_cached_iscsi_volume',\n    ('CreateCachediSCSIVolume', '-'): 'create-cached-iscsi-volume',\n    ('DescribeCachediSCSIVolumes', '_'): 'describe_cached_iscsi_volumes',\n    ('DescribeCachediSCSIVolumes', '-'): 'describe-cached-iscsi-volumes',\n    ('DescribeStorediSCSIVolumes', '_'): 'describe_stored_iscsi_volumes',\n    ('DescribeStorediSCSIVolumes', '-'): 'describe-stored-iscsi-volumes',\n    ('CreateStorediSCSIVolume', '_'): 'create_stored_iscsi_volume',\n    ('CreateStorediSCSIVolume', '-'): 'create-stored-iscsi-volume',\n    ('ListHITsForQualificationType', '_'): 'list_hits_for_qualification_type',\n    ('ListHITsForQualificationType', '-'): 'list-hits-for-qualification-type',\n    ('ExecutePartiQLStatement', '_'): 'execute_partiql_statement',\n    ('ExecutePartiQLStatement', '-'): 'execute-partiql-statement',\n    ('ExecutePartiQLTransaction', '_'): 'execute_partiql_transaction',\n    ('ExecutePartiQLTransaction', '-'): 'execute-partiql-transaction',\n    ('ExecutePartiQLBatch', '_'): 'execute_partiql_batch',\n    ('ExecutePartiQLBatch', '-'): 'execute-partiql-batch',\n}\n# The items in this dict represent partial renames to apply globally to all\n# services which might have a matching argument or operation. This way a\n# common mis-translation can be fixed without having to call out each\n# individual case.\nScalarTypes = ('string', 'integer', 'boolean', 'timestamp', 'float', 'double')\n\nBOTOCORE_ROOT = os.path.dirname(os.path.abspath(__file__))\n\n\n# Used to specify anonymous (unsigned) request signature\nclass UNSIGNED:\n    def __copy__(self):\n        return self\n\n    def __deepcopy__(self, memodict):\n        return self\n\n\nUNSIGNED = UNSIGNED()\n\n\ndef xform_name(name, sep='_', _xform_cache=_xform_cache):\n    \"\"\"Convert camel case to a \"pythonic\" name.\n\n    If the name contains the ``sep`` character, then it is\n    returned unchanged.\n\n    \"\"\"\n    if sep in name:\n        # If the sep is in the name, assume that it's already\n        # transformed and return the string unchanged.\n        return name\n    key = (name, sep)\n    if key not in _xform_cache:\n        if _special_case_transform.search(name) is not None:\n            is_special = _special_case_transform.search(name)\n            matched = is_special.group()\n            # Replace something like ARNs, ACLs with _arns, _acls.\n            name = f\"{name[: -len(matched)]}{sep}{matched.lower()}\"\n        s1 = _first_cap_regex.sub(r'\\1' + sep + r'\\2', name)\n        transformed = _end_cap_regex.sub(r'\\1' + sep + r'\\2', s1).lower()\n        _xform_cache[key] = transformed\n    return _xform_cache[key]\n\n\ndef register_initializer(callback):\n    \"\"\"Register an initializer function for session creation.\n\n    This initializer function will be invoked whenever a new\n    `botocore.session.Session` is instantiated.\n\n    :type callback: callable\n    :param callback: A callable that accepts a single argument\n        of type `botocore.session.Session`.\n\n    \"\"\"\n    _INITIALIZERS.append(callback)\n\n\ndef unregister_initializer(callback):\n    \"\"\"Unregister an initializer function.\n\n    :type callback: callable\n    :param callback: A callable that was previously registered\n        with `botocore.register_initializer`.\n\n    :raises ValueError: If a callback is provided that is not currently\n        registered as an initializer.\n\n    \"\"\"\n    _INITIALIZERS.remove(callback)\n\n\ndef invoke_initializers(session):\n    \"\"\"Invoke all initializers for a session.\n\n    :type session: botocore.session.Session\n    :param session: The session to initialize.\n\n    \"\"\"\n    for initializer in _INITIALIZERS:\n        initializer(session)\n", "botocore/useragent.py": "# Copyright 2023 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"\nNOTE: All classes and functions in this module are considered private and are\nsubject to abrupt breaking changes. Please do not use them directly.\n\nTo modify the User-Agent header sent by botocore, use one of these\nconfiguration options:\n* The ``AWS_SDK_UA_APP_ID`` environment variable.\n* The ``sdk_ua_app_id`` setting in the shared AWS config file.\n* The ``user_agent_appid`` field in the :py:class:`botocore.config.Config`.\n* The ``user_agent_extra`` field in the :py:class:`botocore.config.Config`.\n\n\"\"\"\nimport os\nimport platform\nfrom copy import copy\nfrom string import ascii_letters, digits\nfrom typing import NamedTuple, Optional\n\nfrom botocore import __version__ as botocore_version\nfrom botocore.compat import HAS_CRT\n\n_USERAGENT_ALLOWED_CHARACTERS = ascii_letters + digits + \"!$%&'*+-.^_`|~\"\n_USERAGENT_ALLOWED_OS_NAMES = (\n    'windows',\n    'linux',\n    'macos',\n    'android',\n    'ios',\n    'watchos',\n    'tvos',\n    'other',\n)\n_USERAGENT_PLATFORM_NAME_MAPPINGS = {'darwin': 'macos'}\n# The name by which botocore is identified in the User-Agent header. While most\n# AWS SDKs follow a naming pattern of \"aws-sdk-*\", botocore and boto3 continue\n# using their existing values. Uses uppercase \"B\" with all other characters\n# lowercase.\n_USERAGENT_SDK_NAME = 'Botocore'\n\n\ndef sanitize_user_agent_string_component(raw_str, allow_hash):\n    \"\"\"Replaces all not allowed characters in the string with a dash (\"-\").\n\n    Allowed characters are ASCII alphanumerics and ``!$%&'*+-.^_`|~``. If\n    ``allow_hash`` is ``True``, \"#\"``\" is also allowed.\n\n    :type raw_str: str\n    :param raw_str: The input string to be sanitized.\n\n    :type allow_hash: bool\n    :param allow_hash: Whether \"#\" is considered an allowed character.\n    \"\"\"\n    return ''.join(\n        c\n        if c in _USERAGENT_ALLOWED_CHARACTERS or (allow_hash and c == '#')\n        else '-'\n        for c in raw_str\n    )\n\n\nclass UserAgentComponent(NamedTuple):\n    \"\"\"\n    Component of a Botocore User-Agent header string in the standard format.\n\n    Each component consists of a prefix, a name, and a value. In the string\n    representation these are combined in the format ``prefix/name#value``.\n\n    This class is considered private and is subject to abrupt breaking changes.\n    \"\"\"\n\n    prefix: str\n    name: str\n    value: Optional[str] = None\n\n    def to_string(self):\n        \"\"\"Create string like 'prefix/name#value' from a UserAgentComponent.\"\"\"\n        clean_prefix = sanitize_user_agent_string_component(\n            self.prefix, allow_hash=True\n        )\n        clean_name = sanitize_user_agent_string_component(\n            self.name, allow_hash=False\n        )\n        if self.value is None or self.value == '':\n            return f'{clean_prefix}/{clean_name}'\n        clean_value = sanitize_user_agent_string_component(\n            self.value, allow_hash=True\n        )\n        return f'{clean_prefix}/{clean_name}#{clean_value}'\n\n\nclass RawStringUserAgentComponent:\n    \"\"\"\n    UserAgentComponent interface wrapper around ``str``.\n\n    Use for User-Agent header components that are not constructed from\n    prefix+name+value but instead are provided as strings. No sanitization is\n    performed.\n    \"\"\"\n\n    def __init__(self, value):\n        self._value = value\n\n    def to_string(self):\n        return self._value\n\n\n# This is not a public interface and is subject to abrupt breaking changes.\n# Any usage is not advised or supported in external code bases.\ntry:\n    from botocore.customizations.useragent import modify_components\nexcept ImportError:\n    # Default implementation that returns unmodified User-Agent components.\n    def modify_components(components):\n        return components\n\n\nclass UserAgentString:\n    \"\"\"\n    Generator for AWS SDK User-Agent header strings.\n\n    The User-Agent header format contains information from session, client, and\n    request context. ``UserAgentString`` provides methods for collecting the\n    information and ``to_string`` for assembling it into the standardized\n    string format.\n\n    Example usage:\n\n        ua_session = UserAgentString.from_environment()\n        ua_session.set_session_config(...)\n        ua_client = ua_session.with_client_config(Config(...))\n        ua_string = ua_request.to_string()\n\n    For testing or when information from all sources is available at the same\n    time, the methods can be chained:\n\n        ua_string = (\n            UserAgentString\n            .from_environment()\n            .set_session_config(...)\n            .with_client_config(Config(...))\n            .to_string()\n        )\n\n    \"\"\"\n\n    def __init__(\n        self,\n        platform_name,\n        platform_version,\n        platform_machine,\n        python_version,\n        python_implementation,\n        execution_env,\n        crt_version=None,\n    ):\n        \"\"\"\n        :type platform_name: str\n        :param platform_name: Name of the operating system or equivalent\n            platform name. Should be sourced from :py:meth:`platform.system`.\n        :type platform_version: str\n        :param platform_version: Version of the operating system or equivalent\n            platform name. Should be sourced from :py:meth:`platform.version`.\n        :type platform_machine: str\n        :param platform_version: Processor architecture or machine type. For\n        example \"x86_64\". Should be sourced from :py:meth:`platform.machine`.\n        :type python_version: str\n        :param python_version: Version of the python implementation as str.\n            Should be sourced from :py:meth:`platform.python_version`.\n        :type python_implementation: str\n        :param python_implementation: Name of the python implementation.\n            Should be sourced from :py:meth:`platform.python_implementation`.\n        :type execution_env: str\n        :param execution_env: The value of the AWS execution environment.\n            Should be sourced from the ``AWS_EXECUTION_ENV` environment\n            variable.\n        :type crt_version: str\n        :param crt_version: Version string of awscrt package, if installed.\n        \"\"\"\n        self._platform_name = platform_name\n        self._platform_version = platform_version\n        self._platform_machine = platform_machine\n        self._python_version = python_version\n        self._python_implementation = python_implementation\n        self._execution_env = execution_env\n        self._crt_version = crt_version\n\n        # Components that can be added with ``set_session_config()``\n        self._session_user_agent_name = None\n        self._session_user_agent_version = None\n        self._session_user_agent_extra = None\n\n        self._client_config = None\n        self._uses_paginator = None\n        self._uses_waiter = None\n        self._uses_resource = None\n\n    @classmethod\n    def from_environment(cls):\n        crt_version = None\n        if HAS_CRT:\n            crt_version = _get_crt_version() or 'Unknown'\n        return cls(\n            platform_name=platform.system(),\n            platform_version=platform.release(),\n            platform_machine=platform.machine(),\n            python_version=platform.python_version(),\n            python_implementation=platform.python_implementation(),\n            execution_env=os.environ.get('AWS_EXECUTION_ENV'),\n            crt_version=crt_version,\n        )\n\n    def set_session_config(\n        self,\n        session_user_agent_name,\n        session_user_agent_version,\n        session_user_agent_extra,\n    ):\n        \"\"\"\n        Set the user agent configuration values that apply at session level.\n\n        :param user_agent_name: The user agent name configured in the\n            :py:class:`botocore.session.Session` object. For backwards\n            compatibility, this will always be at the beginning of the\n            User-Agent string, together with ``user_agent_version``.\n        :param user_agent_version: The user agent version configured in the\n            :py:class:`botocore.session.Session` object.\n        :param user_agent_extra: The user agent \"extra\" configured in the\n            :py:class:`botocore.session.Session` object.\n        \"\"\"\n        self._session_user_agent_name = session_user_agent_name\n        self._session_user_agent_version = session_user_agent_version\n        self._session_user_agent_extra = session_user_agent_extra\n        return self\n\n    def with_client_config(self, client_config):\n        \"\"\"\n        Create a copy with all original values and client-specific values.\n\n        :type client_config: botocore.config.Config\n        :param client_config: The client configuration object.\n        \"\"\"\n        cp = copy(self)\n        cp._client_config = client_config\n        return cp\n\n    def to_string(self):\n        \"\"\"\n        Build User-Agent header string from the object's properties.\n        \"\"\"\n        config_ua_override = None\n        if self._client_config:\n            if hasattr(self._client_config, '_supplied_user_agent'):\n                config_ua_override = self._client_config._supplied_user_agent\n            else:\n                config_ua_override = self._client_config.user_agent\n\n        if config_ua_override is not None:\n            return self._build_legacy_ua_string(config_ua_override)\n\n        components = [\n            *self._build_sdk_metadata(),\n            RawStringUserAgentComponent('ua/2.0'),\n            *self._build_os_metadata(),\n            *self._build_architecture_metadata(),\n            *self._build_language_metadata(),\n            *self._build_execution_env_metadata(),\n            *self._build_feature_metadata(),\n            *self._build_config_metadata(),\n            *self._build_app_id(),\n            *self._build_extra(),\n        ]\n\n        components = modify_components(components)\n\n        return ' '.join([comp.to_string() for comp in components])\n\n    def _build_sdk_metadata(self):\n        \"\"\"\n        Build the SDK name and version component of the User-Agent header.\n\n        For backwards-compatibility both session-level and client-level config\n        of custom tool names are honored. If this removes the Botocore\n        information from the start of the string, Botocore's name and version\n        are included as a separate field with \"md\" prefix.\n        \"\"\"\n        sdk_md = []\n        if (\n            self._session_user_agent_name\n            and self._session_user_agent_version\n            and (\n                self._session_user_agent_name != _USERAGENT_SDK_NAME\n                or self._session_user_agent_version != botocore_version\n            )\n        ):\n            sdk_md.extend(\n                [\n                    UserAgentComponent(\n                        self._session_user_agent_name,\n                        self._session_user_agent_version,\n                    ),\n                    UserAgentComponent(\n                        'md', _USERAGENT_SDK_NAME, botocore_version\n                    ),\n                ]\n            )\n        else:\n            sdk_md.append(\n                UserAgentComponent(_USERAGENT_SDK_NAME, botocore_version)\n            )\n\n        if self._crt_version is not None:\n            sdk_md.append(\n                UserAgentComponent('md', 'awscrt', self._crt_version)\n            )\n\n        return sdk_md\n\n    def _build_os_metadata(self):\n        \"\"\"\n        Build the OS/platform components of the User-Agent header string.\n\n        For recognized platform names that match or map to an entry in the list\n        of standardized OS names, a single component with prefix \"os\" is\n        returned. Otherwise, one component \"os/other\" is returned and a second\n        with prefix \"md\" and the raw platform name.\n\n        String representations of example return values:\n         * ``os/macos#10.13.6``\n         * ``os/linux``\n         * ``os/other``\n         * ``os/other md/foobar#1.2.3``\n        \"\"\"\n        if self._platform_name is None:\n            return [UserAgentComponent('os', 'other')]\n\n        plt_name_lower = self._platform_name.lower()\n        if plt_name_lower in _USERAGENT_ALLOWED_OS_NAMES:\n            os_family = plt_name_lower\n        elif plt_name_lower in _USERAGENT_PLATFORM_NAME_MAPPINGS:\n            os_family = _USERAGENT_PLATFORM_NAME_MAPPINGS[plt_name_lower]\n        else:\n            os_family = None\n\n        if os_family is not None:\n            return [\n                UserAgentComponent('os', os_family, self._platform_version)\n            ]\n        else:\n            return [\n                UserAgentComponent('os', 'other'),\n                UserAgentComponent(\n                    'md', self._platform_name, self._platform_version\n                ),\n            ]\n\n    def _build_architecture_metadata(self):\n        \"\"\"\n        Build architecture component of the User-Agent header string.\n\n        Returns the machine type with prefix \"md\" and name \"arch\", if one is\n        available. Common values include \"x86_64\", \"arm64\", \"i386\".\n        \"\"\"\n        if self._platform_machine:\n            return [\n                UserAgentComponent(\n                    'md', 'arch', self._platform_machine.lower()\n                )\n            ]\n        return []\n\n    def _build_language_metadata(self):\n        \"\"\"\n        Build the language components of the User-Agent header string.\n\n        Returns the Python version in a component with prefix \"lang\" and name\n        \"python\". The Python implementation (e.g. CPython, PyPy) is returned as\n        separate metadata component with prefix \"md\" and name \"pyimpl\".\n\n        String representation of an example return value:\n        ``lang/python#3.10.4 md/pyimpl#CPython``\n        \"\"\"\n        lang_md = [\n            UserAgentComponent('lang', 'python', self._python_version),\n        ]\n        if self._python_implementation:\n            lang_md.append(\n                UserAgentComponent('md', 'pyimpl', self._python_implementation)\n            )\n        return lang_md\n\n    def _build_execution_env_metadata(self):\n        \"\"\"\n        Build the execution environment component of the User-Agent header.\n\n        Returns a single component prefixed with \"exec-env\", usually sourced\n        from the environment variable AWS_EXECUTION_ENV.\n        \"\"\"\n        if self._execution_env:\n            return [UserAgentComponent('exec-env', self._execution_env)]\n        else:\n            return []\n\n    def _build_feature_metadata(self):\n        \"\"\"\n        Build the features components of the User-Agent header string.\n\n        Botocore currently does not report any features. This may change in a\n        future version.\n        \"\"\"\n        return []\n\n    def _build_config_metadata(self):\n        \"\"\"\n        Build the configuration components of the User-Agent header string.\n\n        Returns a list of components with prefix \"cfg\" followed by the config\n        setting name and its value. Tracked configuration settings may be\n        added or removed in future versions.\n        \"\"\"\n        if not self._client_config or not self._client_config.retries:\n            return []\n        retry_mode = self._client_config.retries.get('mode')\n        cfg_md = [UserAgentComponent('cfg', 'retry-mode', retry_mode)]\n        if self._client_config.endpoint_discovery_enabled:\n            cfg_md.append(UserAgentComponent('cfg', 'endpoint-discovery'))\n        return cfg_md\n\n    def _build_app_id(self):\n        \"\"\"\n        Build app component of the User-Agent header string.\n\n        Returns a single component with prefix \"app\" and value sourced from the\n        ``user_agent_appid`` field in :py:class:`botocore.config.Config` or\n        the ``sdk_ua_app_id`` setting in the shared configuration file, or the\n        ``AWS_SDK_UA_APP_ID`` environment variable. These are the recommended\n        ways for apps built with Botocore to insert their identifer into the\n        User-Agent header.\n        \"\"\"\n        if self._client_config and self._client_config.user_agent_appid:\n            return [\n                UserAgentComponent('app', self._client_config.user_agent_appid)\n            ]\n        else:\n            return []\n\n    def _build_extra(self):\n        \"\"\"User agent string components based on legacy \"extra\" settings.\n\n        Creates components from the session-level and client-level\n        ``user_agent_extra`` setting, if present. Both are passed through\n        verbatim and should be appended at the end of the string.\n\n        Preferred ways to inject application-specific information into\n        botocore's User-Agent header string are the ``user_agent_appid` field\n        in :py:class:`botocore.config.Config`. The ``AWS_SDK_UA_APP_ID``\n        environment variable and the ``sdk_ua_app_id`` configuration file\n        setting are alternative ways to set the ``user_agent_appid`` config.\n        \"\"\"\n        extra = []\n        if self._session_user_agent_extra:\n            extra.append(\n                RawStringUserAgentComponent(self._session_user_agent_extra)\n            )\n        if self._client_config and self._client_config.user_agent_extra:\n            extra.append(\n                RawStringUserAgentComponent(\n                    self._client_config.user_agent_extra\n                )\n            )\n        return extra\n\n    def _build_legacy_ua_string(self, config_ua_override):\n        components = [config_ua_override]\n        if self._session_user_agent_extra:\n            components.append(self._session_user_agent_extra)\n        if self._client_config.user_agent_extra:\n            components.append(self._client_config.user_agent_extra)\n        return ' '.join(components)\n\n\ndef _get_crt_version():\n    \"\"\"\n    This function is considered private and is subject to abrupt breaking\n    changes.\n    \"\"\"\n    try:\n        import awscrt\n\n        return awscrt.__version__\n    except AttributeError:\n        return None\n", "botocore/compat.py": "# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport copy\nimport datetime\nimport sys\nimport inspect\nimport warnings\nimport hashlib\nfrom http.client import HTTPMessage\nimport logging\nimport shlex\nimport re\nimport os\nfrom collections import OrderedDict\nfrom collections.abc import MutableMapping\nfrom math import floor\n\nfrom botocore.vendored import six\nfrom botocore.exceptions import MD5UnavailableError\nfrom dateutil.tz import tzlocal\nfrom urllib3 import exceptions\n\nlogger = logging.getLogger(__name__)\n\n\nclass HTTPHeaders(HTTPMessage):\n    pass\n\nfrom urllib.parse import (\n    quote,\n    urlencode,\n    unquote,\n    unquote_plus,\n    urlparse,\n    urlsplit,\n    urlunsplit,\n    urljoin,\n    parse_qsl,\n    parse_qs,\n)\nfrom http.client import HTTPResponse\nfrom io import IOBase as _IOBase\nfrom base64 import encodebytes\nfrom email.utils import formatdate\nfrom itertools import zip_longest\nfile_type = _IOBase\nzip = zip\n\n# In python3, unquote takes a str() object, url decodes it,\n# then takes the bytestring and decodes it to utf-8.\nunquote_str = unquote_plus\n\ndef set_socket_timeout(http_response, timeout):\n    \"\"\"Set the timeout of the socket from an HTTPResponse.\n\n    :param http_response: An instance of ``httplib.HTTPResponse``\n\n    \"\"\"\n    http_response._fp.fp.raw._sock.settimeout(timeout)\n\ndef accepts_kwargs(func):\n    # In python3.4.1, there's backwards incompatible\n    # changes when using getargspec with functools.partials.\n    return inspect.getfullargspec(func)[2]\n\ndef ensure_unicode(s, encoding=None, errors=None):\n    # NOOP in Python 3, because every string is already unicode\n    return s\n\ndef ensure_bytes(s, encoding='utf-8', errors='strict'):\n    if isinstance(s, str):\n        return s.encode(encoding, errors)\n    if isinstance(s, bytes):\n        return s\n    raise ValueError(f\"Expected str or bytes, received {type(s)}.\")\n\n\ntry:\n    import xml.etree.cElementTree as ETree\nexcept ImportError:\n    # cElementTree does not exist from Python3.9+\n    import xml.etree.ElementTree as ETree\nXMLParseError = ETree.ParseError\nimport json\n\n\ndef filter_ssl_warnings():\n    # Ignore warnings related to SNI as it is not being used in validations.\n    warnings.filterwarnings(\n        'ignore',\n        message=\"A true SSLContext object is not available.*\",\n        category=exceptions.InsecurePlatformWarning,\n        module=r\".*urllib3\\.util\\.ssl_\",\n    )\n\n\n@classmethod\ndef from_dict(cls, d):\n    new_instance = cls()\n    for key, value in d.items():\n        new_instance[key] = value\n    return new_instance\n\n\n@classmethod\ndef from_pairs(cls, pairs):\n    new_instance = cls()\n    for key, value in pairs:\n        new_instance[key] = value\n    return new_instance\n\n\nHTTPHeaders.from_dict = from_dict\nHTTPHeaders.from_pairs = from_pairs\n\n\ndef copy_kwargs(kwargs):\n    \"\"\"\n    This used to be a compat shim for 2.6 but is now just an alias.\n    \"\"\"\n    copy_kwargs = copy.copy(kwargs)\n    return copy_kwargs\n\n\ndef total_seconds(delta):\n    \"\"\"\n    Returns the total seconds in a ``datetime.timedelta``.\n\n    This used to be a compat shim for 2.6 but is now just an alias.\n\n    :param delta: The timedelta object\n    :type delta: ``datetime.timedelta``\n    \"\"\"\n    return delta.total_seconds()\n\n\n# Checks to see if md5 is available on this system. A given system might not\n# have access to it for various reasons, such as FIPS mode being enabled.\ntry:\n    hashlib.md5()\n    MD5_AVAILABLE = True\nexcept ValueError:\n    MD5_AVAILABLE = False\n\n\ndef get_md5(*args, **kwargs):\n    \"\"\"\n    Attempts to get an md5 hashing object.\n\n    :param args: Args to pass to the MD5 constructor\n    :param kwargs: Key word arguments to pass to the MD5 constructor\n    :return: An MD5 hashing object if available. If it is unavailable, None\n        is returned if raise_error_if_unavailable is set to False.\n    \"\"\"\n    if MD5_AVAILABLE:\n        return hashlib.md5(*args, **kwargs)\n    else:\n        raise MD5UnavailableError()\n\n\ndef compat_shell_split(s, platform=None):\n    if platform is None:\n        platform = sys.platform\n\n    if platform == \"win32\":\n        return _windows_shell_split(s)\n    else:\n        return shlex.split(s)\n\n\ndef _windows_shell_split(s):\n    \"\"\"Splits up a windows command as the built-in command parser would.\n\n    Windows has potentially bizarre rules depending on where you look. When\n    spawning a process via the Windows C runtime (which is what python does\n    when you call popen) the rules are as follows:\n\n    https://docs.microsoft.com/en-us/cpp/cpp/parsing-cpp-command-line-arguments\n\n    To summarize:\n\n    * Only space and tab are valid delimiters\n    * Double quotes are the only valid quotes\n    * Backslash is interpreted literally unless it is part of a chain that\n      leads up to a double quote. Then the backslashes escape the backslashes,\n      and if there is an odd number the final backslash escapes the quote.\n\n    :param s: The command string to split up into parts.\n    :return: A list of command components.\n    \"\"\"\n    if not s:\n        return []\n\n    components = []\n    buff = []\n    is_quoted = False\n    num_backslashes = 0\n    for character in s:\n        if character == '\\\\':\n            # We can't simply append backslashes because we don't know if\n            # they are being used as escape characters or not. Instead we\n            # keep track of how many we've encountered and handle them when\n            # we encounter a different character.\n            num_backslashes += 1\n        elif character == '\"':\n            if num_backslashes > 0:\n                # The backslashes are in a chain leading up to a double\n                # quote, so they are escaping each other.\n                buff.append('\\\\' * int(floor(num_backslashes / 2)))\n                remainder = num_backslashes % 2\n                num_backslashes = 0\n                if remainder == 1:\n                    # The number of backslashes is uneven, so they are also\n                    # escaping the double quote, so it needs to be added to\n                    # the current component buffer.\n                    buff.append('\"')\n                    continue\n\n            # We've encountered a double quote that is not escaped,\n            # so we toggle is_quoted.\n            is_quoted = not is_quoted\n\n            # If there are quotes, then we may want an empty string. To be\n            # safe, we add an empty string to the buffer so that we make\n            # sure it sticks around if there's nothing else between quotes.\n            # If there is other stuff between quotes, the empty string will\n            # disappear during the joining process.\n            buff.append('')\n        elif character in [' ', '\\t'] and not is_quoted:\n            # Since the backslashes aren't leading up to a quote, we put in\n            # the exact number of backslashes.\n            if num_backslashes > 0:\n                buff.append('\\\\' * num_backslashes)\n                num_backslashes = 0\n\n            # Excess whitespace is ignored, so only add the components list\n            # if there is anything in the buffer.\n            if buff:\n                components.append(''.join(buff))\n                buff = []\n        else:\n            # Since the backslashes aren't leading up to a quote, we put in\n            # the exact number of backslashes.\n            if num_backslashes > 0:\n                buff.append('\\\\' * num_backslashes)\n                num_backslashes = 0\n            buff.append(character)\n\n    # Quotes must be terminated.\n    if is_quoted:\n        raise ValueError(f\"No closing quotation in string: {s}\")\n\n    # There may be some leftover backslashes, so we need to add them in.\n    # There's no quote so we add the exact number.\n    if num_backslashes > 0:\n        buff.append('\\\\' * num_backslashes)\n\n    # Add the final component in if there is anything in the buffer.\n    if buff:\n        components.append(''.join(buff))\n\n    return components\n\n\ndef get_tzinfo_options():\n    # Due to dateutil/dateutil#197, Windows may fail to parse times in the past\n    # with the system clock. We can alternatively fallback to tzwininfo when\n    # this happens, which will get time info from the Windows registry.\n    if sys.platform == 'win32':\n        from dateutil.tz import tzwinlocal\n\n        return (tzlocal, tzwinlocal)\n    else:\n        return (tzlocal,)\n\n\n# Detect if CRT is available for use\ntry:\n    import awscrt.auth\n\n    # Allow user opt-out if needed\n    disabled = os.environ.get('BOTO_DISABLE_CRT', \"false\")\n    HAS_CRT = not disabled.lower() == 'true'\nexcept ImportError:\n    HAS_CRT = False\n\n\n########################################################\n#              urllib3 compat backports                #\n########################################################\n\n# Vendoring IPv6 validation regex patterns from urllib3\n# https://github.com/urllib3/urllib3/blob/7e856c0/src/urllib3/util/url.py\nIPV4_PAT = r\"(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\"\nIPV4_RE = re.compile(\"^\" + IPV4_PAT + \"$\")\nHEX_PAT = \"[0-9A-Fa-f]{1,4}\"\nLS32_PAT = \"(?:{hex}:{hex}|{ipv4})\".format(hex=HEX_PAT, ipv4=IPV4_PAT)\n_subs = {\"hex\": HEX_PAT, \"ls32\": LS32_PAT}\n_variations = [\n    #                            6( h16 \":\" ) ls32\n    \"(?:%(hex)s:){6}%(ls32)s\",\n    #                       \"::\" 5( h16 \":\" ) ls32\n    \"::(?:%(hex)s:){5}%(ls32)s\",\n    # [               h16 ] \"::\" 4( h16 \":\" ) ls32\n    \"(?:%(hex)s)?::(?:%(hex)s:){4}%(ls32)s\",\n    # [ *1( h16 \":\" ) h16 ] \"::\" 3( h16 \":\" ) ls32\n    \"(?:(?:%(hex)s:)?%(hex)s)?::(?:%(hex)s:){3}%(ls32)s\",\n    # [ *2( h16 \":\" ) h16 ] \"::\" 2( h16 \":\" ) ls32\n    \"(?:(?:%(hex)s:){0,2}%(hex)s)?::(?:%(hex)s:){2}%(ls32)s\",\n    # [ *3( h16 \":\" ) h16 ] \"::\"    h16 \":\"   ls32\n    \"(?:(?:%(hex)s:){0,3}%(hex)s)?::%(hex)s:%(ls32)s\",\n    # [ *4( h16 \":\" ) h16 ] \"::\"              ls32\n    \"(?:(?:%(hex)s:){0,4}%(hex)s)?::%(ls32)s\",\n    # [ *5( h16 \":\" ) h16 ] \"::\"              h16\n    \"(?:(?:%(hex)s:){0,5}%(hex)s)?::%(hex)s\",\n    # [ *6( h16 \":\" ) h16 ] \"::\"\n    \"(?:(?:%(hex)s:){0,6}%(hex)s)?::\",\n]\n\nUNRESERVED_PAT = (\n    r\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789._!\\-~\"\n)\nIPV6_PAT = \"(?:\" + \"|\".join([x % _subs for x in _variations]) + \")\"\nZONE_ID_PAT = \"(?:%25|%)(?:[\" + UNRESERVED_PAT + \"]|%[a-fA-F0-9]{2})+\"\nIPV6_ADDRZ_PAT = r\"\\[\" + IPV6_PAT + r\"(?:\" + ZONE_ID_PAT + r\")?\\]\"\nIPV6_ADDRZ_RE = re.compile(\"^\" + IPV6_ADDRZ_PAT + \"$\")\n\n# These are the characters that are stripped by post-bpo-43882 urlparse().\nUNSAFE_URL_CHARS = frozenset('\\t\\r\\n')\n\n# Detect if gzip is available for use\ntry:\n    import gzip\n    HAS_GZIP = True\nexcept ImportError:\n    HAS_GZIP = False\n", "botocore/stub.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport copy\nfrom collections import deque\nfrom pprint import pformat\n\nfrom botocore.awsrequest import AWSResponse\nfrom botocore.exceptions import (\n    ParamValidationError,\n    StubAssertionError,\n    StubResponseError,\n    UnStubbedResponseError,\n)\nfrom botocore.validate import validate_parameters\n\n\nclass _ANY:\n    \"\"\"\n    A helper object that compares equal to everything. Copied from\n    unittest.mock\n    \"\"\"\n\n    def __eq__(self, other):\n        return True\n\n    def __ne__(self, other):\n        return False\n\n    def __repr__(self):\n        return '<ANY>'\n\n\nANY = _ANY()\n\n\nclass Stubber:\n    \"\"\"\n    This class will allow you to stub out requests so you don't have to hit\n    an endpoint to write tests. Responses are returned first in, first out.\n    If operations are called out of order, or are called with no remaining\n    queued responses, an error will be raised.\n\n    **Example:**\n    ::\n        import datetime\n        import botocore.session\n        from botocore.stub import Stubber\n\n\n        s3 = botocore.session.get_session().create_client('s3')\n        stubber = Stubber(s3)\n\n        response = {\n            'IsTruncated': False,\n            'Name': 'test-bucket',\n            'MaxKeys': 1000, 'Prefix': '',\n            'Contents': [{\n                'Key': 'test.txt',\n                'ETag': '\"abc123\"',\n                'StorageClass': 'STANDARD',\n                'LastModified': datetime.datetime(2016, 1, 20, 22, 9),\n                'Owner': {'ID': 'abc123', 'DisplayName': 'myname'},\n                'Size': 14814\n            }],\n            'EncodingType': 'url',\n            'ResponseMetadata': {\n                'RequestId': 'abc123',\n                'HTTPStatusCode': 200,\n                'HostId': 'abc123'\n            },\n            'Marker': ''\n        }\n\n        expected_params = {'Bucket': 'test-bucket'}\n\n        stubber.add_response('list_objects', response, expected_params)\n        stubber.activate()\n\n        service_response = s3.list_objects(Bucket='test-bucket')\n        assert service_response == response\n\n\n    This class can also be called as a context manager, which will handle\n    activation / deactivation for you.\n\n    **Example:**\n    ::\n        import datetime\n        import botocore.session\n        from botocore.stub import Stubber\n\n\n        s3 = botocore.session.get_session().create_client('s3')\n\n        response = {\n            \"Owner\": {\n                \"ID\": \"foo\",\n                \"DisplayName\": \"bar\"\n            },\n            \"Buckets\": [{\n                \"CreationDate\": datetime.datetime(2016, 1, 20, 22, 9),\n                \"Name\": \"baz\"\n            }]\n        }\n\n\n        with Stubber(s3) as stubber:\n            stubber.add_response('list_buckets', response, {})\n            service_response = s3.list_buckets()\n\n        assert service_response == response\n\n\n    If you have an input parameter that is a randomly generated value, or you\n    otherwise don't care about its value, you can use ``stub.ANY`` to ignore\n    it in validation.\n\n    **Example:**\n    ::\n        import datetime\n        import botocore.session\n        from botocore.stub import Stubber, ANY\n\n\n        s3 = botocore.session.get_session().create_client('s3')\n        stubber = Stubber(s3)\n\n        response = {\n            'IsTruncated': False,\n            'Name': 'test-bucket',\n            'MaxKeys': 1000, 'Prefix': '',\n            'Contents': [{\n                'Key': 'test.txt',\n                'ETag': '\"abc123\"',\n                'StorageClass': 'STANDARD',\n                'LastModified': datetime.datetime(2016, 1, 20, 22, 9),\n                'Owner': {'ID': 'abc123', 'DisplayName': 'myname'},\n                'Size': 14814\n            }],\n            'EncodingType': 'url',\n            'ResponseMetadata': {\n                'RequestId': 'abc123',\n                'HTTPStatusCode': 200,\n                'HostId': 'abc123'\n            },\n            'Marker': ''\n        }\n\n        expected_params = {'Bucket': ANY}\n        stubber.add_response('list_objects', response, expected_params)\n\n        with stubber:\n            service_response = s3.list_objects(Bucket='test-bucket')\n\n        assert service_response == response\n    \"\"\"\n\n    def __init__(self, client):\n        \"\"\"\n        :param client: The client to add your stubs to.\n        \"\"\"\n        self.client = client\n        self._event_id = 'boto_stubber'\n        self._expected_params_event_id = 'boto_stubber_expected_params'\n        self._queue = deque()\n\n    def __enter__(self):\n        self.activate()\n        return self\n\n    def __exit__(self, exception_type, exception_value, traceback):\n        self.deactivate()\n\n    def activate(self):\n        \"\"\"\n        Activates the stubber on the client\n        \"\"\"\n        self.client.meta.events.register_first(\n            'before-parameter-build.*.*',\n            self._assert_expected_params,\n            unique_id=self._expected_params_event_id,\n        )\n        self.client.meta.events.register(\n            'before-call.*.*',\n            self._get_response_handler,\n            unique_id=self._event_id,\n        )\n\n    def deactivate(self):\n        \"\"\"\n        Deactivates the stubber on the client\n        \"\"\"\n        self.client.meta.events.unregister(\n            'before-parameter-build.*.*',\n            self._assert_expected_params,\n            unique_id=self._expected_params_event_id,\n        )\n        self.client.meta.events.unregister(\n            'before-call.*.*',\n            self._get_response_handler,\n            unique_id=self._event_id,\n        )\n\n    def add_response(self, method, service_response, expected_params=None):\n        \"\"\"\n        Adds a service response to the response queue. This will be validated\n        against the service model to ensure correctness. It should be noted,\n        however, that while missing attributes are often considered correct,\n        your code may not function properly if you leave them out. Therefore\n        you should always fill in every value you see in a typical response for\n        your particular request.\n\n        :param method: The name of the client method to stub.\n        :type method: str\n\n        :param service_response: A dict response stub. Provided parameters will\n            be validated against the service model.\n        :type service_response: dict\n\n        :param expected_params: A dictionary of the expected parameters to\n            be called for the provided service response. The parameters match\n            the names of keyword arguments passed to that client call. If\n            any of the parameters differ a ``StubResponseError`` is thrown.\n            You can use stub.ANY to indicate a particular parameter to ignore\n            in validation. stub.ANY is only valid for top level params.\n        \"\"\"\n        self._add_response(method, service_response, expected_params)\n\n    def _add_response(self, method, service_response, expected_params):\n        if not hasattr(self.client, method):\n            raise ValueError(\n                \"Client %s does not have method: %s\"\n                % (self.client.meta.service_model.service_name, method)\n            )\n\n        # Create a successful http response\n        http_response = AWSResponse(None, 200, {}, None)\n\n        operation_name = self.client.meta.method_to_api_mapping.get(method)\n        self._validate_operation_response(operation_name, service_response)\n\n        # Add the service_response to the queue for returning responses\n        response = {\n            'operation_name': operation_name,\n            'response': (http_response, service_response),\n            'expected_params': expected_params,\n        }\n        self._queue.append(response)\n\n    def add_client_error(\n        self,\n        method,\n        service_error_code='',\n        service_message='',\n        http_status_code=400,\n        service_error_meta=None,\n        expected_params=None,\n        response_meta=None,\n        modeled_fields=None,\n    ):\n        \"\"\"\n        Adds a ``ClientError`` to the response queue.\n\n        :param method: The name of the service method to return the error on.\n        :type method: str\n\n        :param service_error_code: The service error code to return,\n                                   e.g. ``NoSuchBucket``\n        :type service_error_code: str\n\n        :param service_message: The service message to return, e.g.\n                        'The specified bucket does not exist.'\n        :type service_message: str\n\n        :param http_status_code: The HTTP status code to return, e.g. 404, etc\n        :type http_status_code: int\n\n        :param service_error_meta: Additional keys to be added to the\n            service Error\n        :type service_error_meta: dict\n\n        :param expected_params: A dictionary of the expected parameters to\n            be called for the provided service response. The parameters match\n            the names of keyword arguments passed to that client call. If\n            any of the parameters differ a ``StubResponseError`` is thrown.\n            You can use stub.ANY to indicate a particular parameter to ignore\n            in validation.\n\n        :param response_meta: Additional keys to be added to the\n            response's ResponseMetadata\n        :type response_meta: dict\n\n        :param modeled_fields: Additional keys to be added to the response\n            based on fields that are modeled for the particular error code.\n            These keys will be validated against the particular error shape\n            designated by the error code.\n        :type modeled_fields: dict\n\n        \"\"\"\n        http_response = AWSResponse(None, http_status_code, {}, None)\n\n        # We don't look to the model to build this because the caller would\n        # need to know the details of what the HTTP body would need to\n        # look like.\n        parsed_response = {\n            'ResponseMetadata': {'HTTPStatusCode': http_status_code},\n            'Error': {'Message': service_message, 'Code': service_error_code},\n        }\n\n        if service_error_meta is not None:\n            parsed_response['Error'].update(service_error_meta)\n\n        if response_meta is not None:\n            parsed_response['ResponseMetadata'].update(response_meta)\n\n        if modeled_fields is not None:\n            service_model = self.client.meta.service_model\n            shape = service_model.shape_for_error_code(service_error_code)\n            self._validate_response(shape, modeled_fields)\n            parsed_response.update(modeled_fields)\n\n        operation_name = self.client.meta.method_to_api_mapping.get(method)\n        # Note that we do not allow for expected_params while\n        # adding errors into the queue yet.\n        response = {\n            'operation_name': operation_name,\n            'response': (http_response, parsed_response),\n            'expected_params': expected_params,\n        }\n        self._queue.append(response)\n\n    def assert_no_pending_responses(self):\n        \"\"\"\n        Asserts that all expected calls were made.\n        \"\"\"\n        remaining = len(self._queue)\n        if remaining != 0:\n            raise AssertionError(f\"{remaining} responses remaining in queue.\")\n\n    def _assert_expected_call_order(self, model, params):\n        if not self._queue:\n            raise UnStubbedResponseError(\n                operation_name=model.name,\n                reason=(\n                    'Unexpected API Call: A call was made but no additional '\n                    'calls expected. Either the API Call was not stubbed or '\n                    'it was called multiple times.'\n                ),\n            )\n\n        name = self._queue[0]['operation_name']\n        if name != model.name:\n            raise StubResponseError(\n                operation_name=model.name,\n                reason=f'Operation mismatch: found response for {name}.',\n            )\n\n    def _get_response_handler(self, model, params, context, **kwargs):\n        self._assert_expected_call_order(model, params)\n        # Pop off the entire response once everything has been validated\n        return self._queue.popleft()['response']\n\n    def _assert_expected_params(self, model, params, context, **kwargs):\n        if self._should_not_stub(context):\n            return\n        self._assert_expected_call_order(model, params)\n        expected_params = self._queue[0]['expected_params']\n        if expected_params is None:\n            return\n\n        # Validate the parameters are equal\n        for param, value in expected_params.items():\n            if param not in params or expected_params[param] != params[param]:\n                raise StubAssertionError(\n                    operation_name=model.name,\n                    reason='Expected parameters:\\n%s,\\nbut received:\\n%s'\n                    % (pformat(expected_params), pformat(params)),\n                )\n\n        # Ensure there are no extra params hanging around\n        if sorted(expected_params.keys()) != sorted(params.keys()):\n            raise StubAssertionError(\n                operation_name=model.name,\n                reason='Expected parameters:\\n%s,\\nbut received:\\n%s'\n                % (pformat(expected_params), pformat(params)),\n            )\n\n    def _should_not_stub(self, context):\n        # Do not include presign requests when processing stubbed client calls\n        # as a presign request will never have an HTTP request sent over the\n        # wire for it and therefore not receive a response back.\n        if context and context.get('is_presign_request'):\n            return True\n\n    def _validate_operation_response(self, operation_name, service_response):\n        service_model = self.client.meta.service_model\n        operation_model = service_model.operation_model(operation_name)\n        output_shape = operation_model.output_shape\n\n        # Remove ResponseMetadata so that the validator doesn't attempt to\n        # perform validation on it.\n        response = service_response\n        if 'ResponseMetadata' in response:\n            response = copy.copy(service_response)\n            del response['ResponseMetadata']\n\n        self._validate_response(output_shape, response)\n\n    def _validate_response(self, shape, response):\n        if shape is not None:\n            validate_parameters(response, shape)\n        elif response:\n            # If the output shape is None, that means the response should be\n            # empty apart from ResponseMetadata\n            raise ParamValidationError(\n                report=(\n                    \"Service response should only contain ResponseMetadata.\"\n                )\n            )\n", "botocore/session.py": "# Copyright (c) 2012-2013 Mitch Garnaat http://garnaat.org/\n# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"\nThis module contains the main interface to the botocore package, the\nSession object.\n\"\"\"\n\nimport copy\nimport logging\nimport os\nimport platform\nimport socket\nimport warnings\n\nimport botocore.client\nimport botocore.configloader\nimport botocore.credentials\nimport botocore.tokens\nfrom botocore import (\n    UNSIGNED,\n    __version__,\n    handlers,\n    invoke_initializers,\n    monitoring,\n    paginate,\n    retryhandler,\n    translate,\n    waiter,\n)\nfrom botocore.compat import HAS_CRT, MutableMapping\nfrom botocore.configprovider import (\n    BOTOCORE_DEFAUT_SESSION_VARIABLES,\n    ConfigChainFactory,\n    ConfiguredEndpointProvider,\n    ConfigValueStore,\n    DefaultConfigResolver,\n    SmartDefaultsConfigStoreFactory,\n    create_botocore_default_config_mapping,\n)\nfrom botocore.errorfactory import ClientExceptionsFactory\nfrom botocore.exceptions import (\n    ConfigNotFound,\n    InvalidDefaultsMode,\n    PartialCredentialsError,\n    ProfileNotFound,\n    UnknownServiceError,\n)\nfrom botocore.hooks import (\n    EventAliaser,\n    HierarchicalEmitter,\n    first_non_none_response,\n)\nfrom botocore.loaders import create_loader\nfrom botocore.model import ServiceModel\nfrom botocore.parsers import ResponseParserFactory\nfrom botocore.regions import EndpointResolver\nfrom botocore.useragent import UserAgentString\nfrom botocore.utils import (\n    EVENT_ALIASES,\n    IMDSRegionProvider,\n    validate_region_name,\n)\n\nfrom botocore.compat import HAS_CRT  # noqa\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass Session:\n    \"\"\"\n    The Session object collects together useful functionality\n    from `botocore` as well as important data such as configuration\n    information and credentials into a single, easy-to-use object.\n\n    :ivar available_profiles: A list of profiles defined in the config\n        file associated with this session.\n    :ivar profile: The current profile.\n    \"\"\"\n\n    SESSION_VARIABLES = copy.copy(BOTOCORE_DEFAUT_SESSION_VARIABLES)\n\n    #: The default format string to use when configuring the botocore logger.\n    LOG_FORMAT = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n\n    def __init__(\n        self,\n        session_vars=None,\n        event_hooks=None,\n        include_builtin_handlers=True,\n        profile=None,\n    ):\n        \"\"\"\n        Create a new Session object.\n\n        :type session_vars: dict\n        :param session_vars: A dictionary that is used to override some or all\n            of the environment variables associated with this session.  The\n            key/value pairs defined in this dictionary will override the\n            corresponding variables defined in ``SESSION_VARIABLES``.\n\n        :type event_hooks: BaseEventHooks\n        :param event_hooks: The event hooks object to use. If one is not\n            provided, an event hooks object will be automatically created\n            for you.\n\n        :type include_builtin_handlers: bool\n        :param include_builtin_handlers: Indicates whether or not to\n            automatically register builtin handlers.\n\n        :type profile: str\n        :param profile: The name of the profile to use for this\n            session.  Note that the profile can only be set when\n            the session is created.\n\n        \"\"\"\n        if event_hooks is None:\n            self._original_handler = HierarchicalEmitter()\n        else:\n            self._original_handler = event_hooks\n        self._events = EventAliaser(self._original_handler)\n        if include_builtin_handlers:\n            self._register_builtin_handlers(self._events)\n        self.user_agent_name = 'Botocore'\n        self.user_agent_version = __version__\n        self.user_agent_extra = ''\n        # The _profile attribute is just used to cache the value\n        # of the current profile to avoid going through the normal\n        # config lookup process each access time.\n        self._profile = None\n        self._config = None\n        self._credentials = None\n        self._auth_token = None\n        self._profile_map = None\n        # This is a dict that stores per session specific config variable\n        # overrides via set_config_variable().\n        self._session_instance_vars = {}\n        if profile is not None:\n            self._session_instance_vars['profile'] = profile\n        self._client_config = None\n        self._last_client_region_used = None\n        self._components = ComponentLocator()\n        self._internal_components = ComponentLocator()\n        self._register_components()\n        self.session_var_map = SessionVarDict(self, self.SESSION_VARIABLES)\n        if session_vars is not None:\n            self.session_var_map.update(session_vars)\n        invoke_initializers(self)\n\n    def _register_components(self):\n        self._register_credential_provider()\n        self._register_token_provider()\n        self._register_data_loader()\n        self._register_endpoint_resolver()\n        self._register_event_emitter()\n        self._register_response_parser_factory()\n        self._register_exceptions_factory()\n        self._register_config_store()\n        self._register_monitor()\n        self._register_default_config_resolver()\n        self._register_smart_defaults_factory()\n        self._register_user_agent_creator()\n\n    def _register_event_emitter(self):\n        self._components.register_component('event_emitter', self._events)\n\n    def _register_token_provider(self):\n        self._components.lazy_register_component(\n            'token_provider', self._create_token_resolver\n        )\n\n    def _create_token_resolver(self):\n        return botocore.tokens.create_token_resolver(self)\n\n    def _register_credential_provider(self):\n        self._components.lazy_register_component(\n            'credential_provider', self._create_credential_resolver\n        )\n\n    def _create_credential_resolver(self):\n        return botocore.credentials.create_credential_resolver(\n            self, region_name=self._last_client_region_used\n        )\n\n    def _register_data_loader(self):\n        self._components.lazy_register_component(\n            'data_loader',\n            lambda: create_loader(self.get_config_variable('data_path')),\n        )\n\n    def _register_endpoint_resolver(self):\n        def create_default_resolver():\n            loader = self.get_component('data_loader')\n            endpoints, path = loader.load_data_with_path('endpoints')\n            uses_builtin = loader.is_builtin_path(path)\n            return EndpointResolver(endpoints, uses_builtin_data=uses_builtin)\n\n        self._internal_components.lazy_register_component(\n            'endpoint_resolver', create_default_resolver\n        )\n\n    def _register_default_config_resolver(self):\n        def create_default_config_resolver():\n            loader = self.get_component('data_loader')\n            defaults = loader.load_data('sdk-default-configuration')\n            return DefaultConfigResolver(defaults)\n\n        self._internal_components.lazy_register_component(\n            'default_config_resolver', create_default_config_resolver\n        )\n\n    def _register_smart_defaults_factory(self):\n        def create_smart_defaults_factory():\n            default_config_resolver = self._get_internal_component(\n                'default_config_resolver'\n            )\n            imds_region_provider = IMDSRegionProvider(session=self)\n            return SmartDefaultsConfigStoreFactory(\n                default_config_resolver, imds_region_provider\n            )\n\n        self._internal_components.lazy_register_component(\n            'smart_defaults_factory', create_smart_defaults_factory\n        )\n\n    def _register_response_parser_factory(self):\n        self._components.register_component(\n            'response_parser_factory', ResponseParserFactory()\n        )\n\n    def _register_exceptions_factory(self):\n        self._internal_components.register_component(\n            'exceptions_factory', ClientExceptionsFactory()\n        )\n\n    def _register_builtin_handlers(self, events):\n        for spec in handlers.BUILTIN_HANDLERS:\n            if len(spec) == 2:\n                event_name, handler = spec\n                self.register(event_name, handler)\n            else:\n                event_name, handler, register_type = spec\n                if register_type is handlers.REGISTER_FIRST:\n                    self._events.register_first(event_name, handler)\n                elif register_type is handlers.REGISTER_LAST:\n                    self._events.register_last(event_name, handler)\n\n    def _register_config_store(self):\n        config_store_component = ConfigValueStore(\n            mapping=create_botocore_default_config_mapping(self)\n        )\n        self._components.register_component(\n            'config_store', config_store_component\n        )\n\n    def _register_monitor(self):\n        self._internal_components.lazy_register_component(\n            'monitor', self._create_csm_monitor\n        )\n\n    def _register_user_agent_creator(self):\n        uas = UserAgentString.from_environment()\n        self._components.register_component('user_agent_creator', uas)\n\n    def _create_csm_monitor(self):\n        if self.get_config_variable('csm_enabled'):\n            client_id = self.get_config_variable('csm_client_id')\n            host = self.get_config_variable('csm_host')\n            port = self.get_config_variable('csm_port')\n            handler = monitoring.Monitor(\n                adapter=monitoring.MonitorEventAdapter(),\n                publisher=monitoring.SocketPublisher(\n                    socket=socket.socket(socket.AF_INET, socket.SOCK_DGRAM),\n                    host=host,\n                    port=port,\n                    serializer=monitoring.CSMSerializer(\n                        csm_client_id=client_id\n                    ),\n                ),\n            )\n            return handler\n        return None\n\n    def _get_crt_version(self):\n        user_agent_creator = self.get_component('user_agent_creator')\n        return user_agent_creator._crt_version or 'Unknown'\n\n    @property\n    def available_profiles(self):\n        return list(self._build_profile_map().keys())\n\n    def _build_profile_map(self):\n        # This will build the profile map if it has not been created,\n        # otherwise it will return the cached value.  The profile map\n        # is a list of profile names, to the config values for the profile.\n        if self._profile_map is None:\n            self._profile_map = self.full_config['profiles']\n        return self._profile_map\n\n    @property\n    def profile(self):\n        if self._profile is None:\n            profile = self.get_config_variable('profile')\n            self._profile = profile\n        return self._profile\n\n    def get_config_variable(self, logical_name, methods=None):\n        if methods is not None:\n            return self._get_config_variable_with_custom_methods(\n                logical_name, methods\n            )\n        return self.get_component('config_store').get_config_variable(\n            logical_name\n        )\n\n    def _get_config_variable_with_custom_methods(self, logical_name, methods):\n        # If a custom list of methods was supplied we need to perserve the\n        # behavior with the new system. To do so a new chain that is a copy of\n        # the old one will be constructed, but only with the supplied methods\n        # being added to the chain. This chain will be consulted for a value\n        # and then thrown out. This is not efficient, nor is the methods arg\n        # used in botocore, this is just for backwards compatibility.\n        chain_builder = SubsetChainConfigFactory(session=self, methods=methods)\n        mapping = create_botocore_default_config_mapping(self)\n        for name, config_options in self.session_var_map.items():\n            config_name, env_vars, default, typecast = config_options\n            build_chain_config_args = {\n                'conversion_func': typecast,\n                'default': default,\n            }\n            if 'instance' in methods:\n                build_chain_config_args['instance_name'] = name\n            if 'env' in methods:\n                build_chain_config_args['env_var_names'] = env_vars\n            if 'config' in methods:\n                build_chain_config_args['config_property_name'] = config_name\n            mapping[name] = chain_builder.create_config_chain(\n                **build_chain_config_args\n            )\n        config_store_component = ConfigValueStore(mapping=mapping)\n        value = config_store_component.get_config_variable(logical_name)\n        return value\n\n    def set_config_variable(self, logical_name, value):\n        \"\"\"Set a configuration variable to a specific value.\n\n        By using this method, you can override the normal lookup\n        process used in ``get_config_variable`` by explicitly setting\n        a value.  Subsequent calls to ``get_config_variable`` will\n        use the ``value``.  This gives you per-session specific\n        configuration values.\n\n        ::\n            >>> # Assume logical name 'foo' maps to env var 'FOO'\n            >>> os.environ['FOO'] = 'myvalue'\n            >>> s.get_config_variable('foo')\n            'myvalue'\n            >>> s.set_config_variable('foo', 'othervalue')\n            >>> s.get_config_variable('foo')\n            'othervalue'\n\n        :type logical_name: str\n        :param logical_name: The logical name of the session variable\n            you want to set.  These are the keys in ``SESSION_VARIABLES``.\n        :param value: The value to associate with the config variable.\n\n        \"\"\"\n        logger.debug(\n            \"Setting config variable for %s to %r\",\n            logical_name,\n            value,\n        )\n        self._session_instance_vars[logical_name] = value\n\n    def instance_variables(self):\n        return copy.copy(self._session_instance_vars)\n\n    def get_scoped_config(self):\n        \"\"\"\n        Returns the config values from the config file scoped to the current\n        profile.\n\n        The configuration data is loaded **only** from the config file.\n        It does not resolve variables based on different locations\n        (e.g. first from the session instance, then from environment\n        variables, then from the config file).  If you want this lookup\n        behavior, use the ``get_config_variable`` method instead.\n\n        Note that this configuration is specific to a single profile (the\n        ``profile`` session variable).\n\n        If the ``profile`` session variable is set and the profile does\n        not exist in the config file, a ``ProfileNotFound`` exception\n        will be raised.\n\n        :raises: ConfigNotFound, ConfigParseError, ProfileNotFound\n        :rtype: dict\n\n        \"\"\"\n        profile_name = self.get_config_variable('profile')\n        profile_map = self._build_profile_map()\n        # If a profile is not explicitly set return the default\n        # profile config or an empty config dict if we don't have\n        # a default profile.\n        if profile_name is None:\n            return profile_map.get('default', {})\n        elif profile_name not in profile_map:\n            # Otherwise if they specified a profile, it has to\n            # exist (even if it's the default profile) otherwise\n            # we complain.\n            raise ProfileNotFound(profile=profile_name)\n        else:\n            return profile_map[profile_name]\n\n    @property\n    def full_config(self):\n        \"\"\"Return the parsed config file.\n\n        The ``get_config`` method returns the config associated with the\n        specified profile.  This property returns the contents of the\n        **entire** config file.\n\n        :rtype: dict\n        \"\"\"\n        if self._config is None:\n            try:\n                config_file = self.get_config_variable('config_file')\n                self._config = botocore.configloader.load_config(config_file)\n            except ConfigNotFound:\n                self._config = {'profiles': {}}\n            try:\n                # Now we need to inject the profiles from the\n                # credentials file.  We don't actually need the values\n                # in the creds file, only the profile names so that we\n                # can validate the user is not referring to a nonexistent\n                # profile.\n                cred_file = self.get_config_variable('credentials_file')\n                cred_profiles = botocore.configloader.raw_config_parse(\n                    cred_file\n                )\n                for profile in cred_profiles:\n                    cred_vars = cred_profiles[profile]\n                    if profile not in self._config['profiles']:\n                        self._config['profiles'][profile] = cred_vars\n                    else:\n                        self._config['profiles'][profile].update(cred_vars)\n            except ConfigNotFound:\n                pass\n        return self._config\n\n    def get_default_client_config(self):\n        \"\"\"Retrieves the default config for creating clients\n\n        :rtype: botocore.client.Config\n        :returns: The default client config object when creating clients. If\n            the value is ``None`` then there is no default config object\n            attached to the session.\n        \"\"\"\n        return self._client_config\n\n    def set_default_client_config(self, client_config):\n        \"\"\"Sets the default config for creating clients\n\n        :type client_config: botocore.client.Config\n        :param client_config: The default client config object when creating\n            clients. If the value is ``None`` then there is no default config\n            object attached to the session.\n        \"\"\"\n        self._client_config = client_config\n\n    def set_credentials(self, access_key, secret_key, token=None):\n        \"\"\"\n        Manually create credentials for this session.  If you would\n        prefer to use botocore without a config file, environment variables,\n        or IAM roles, you can pass explicit credentials into this\n        method to establish credentials for this session.\n\n        :type access_key: str\n        :param access_key: The access key part of the credentials.\n\n        :type secret_key: str\n        :param secret_key: The secret key part of the credentials.\n\n        :type token: str\n        :param token: An option session token used by STS session\n            credentials.\n        \"\"\"\n        self._credentials = botocore.credentials.Credentials(\n            access_key, secret_key, token\n        )\n\n    def get_credentials(self):\n        \"\"\"\n        Return the :class:`botocore.credential.Credential` object\n        associated with this session.  If the credentials have not\n        yet been loaded, this will attempt to load them.  If they\n        have already been loaded, this will return the cached\n        credentials.\n\n        \"\"\"\n        if self._credentials is None:\n            self._credentials = self._components.get_component(\n                'credential_provider'\n            ).load_credentials()\n        return self._credentials\n\n    def get_auth_token(self):\n        \"\"\"\n        Return the :class:`botocore.tokens.AuthToken` object associated with\n        this session. If the authorization token has not yet been loaded, this\n        will attempt to load it. If it has already been loaded, this will\n        return the cached authorization token.\n\n        \"\"\"\n        if self._auth_token is None:\n            provider = self._components.get_component('token_provider')\n            self._auth_token = provider.load_token()\n        return self._auth_token\n\n    def user_agent(self):\n        \"\"\"\n        Return a string suitable for use as a User-Agent header.\n        The string will be of the form:\n\n        <agent_name>/<agent_version> Python/<py_ver> <plat_name>/<plat_ver> <exec_env>\n\n        Where:\n\n         - agent_name is the value of the `user_agent_name` attribute\n           of the session object (`Botocore` by default).\n         - agent_version is the value of the `user_agent_version`\n           attribute of the session object (the botocore version by default).\n           by default.\n         - py_ver is the version of the Python interpreter beng used.\n         - plat_name is the name of the platform (e.g. Darwin)\n         - plat_ver is the version of the platform\n         - exec_env is exec-env/$AWS_EXECUTION_ENV\n\n        If ``user_agent_extra`` is not empty, then this value will be\n        appended to the end of the user agent string.\n\n        \"\"\"\n        base = (\n            f'{self.user_agent_name}/{self.user_agent_version} '\n            f'Python/{platform.python_version()} '\n            f'{platform.system()}/{platform.release()}'\n        )\n        if HAS_CRT:\n            base += ' awscrt/%s' % self._get_crt_version()\n        if os.environ.get('AWS_EXECUTION_ENV') is not None:\n            base += ' exec-env/%s' % os.environ.get('AWS_EXECUTION_ENV')\n        if self.user_agent_extra:\n            base += ' %s' % self.user_agent_extra\n\n        return base\n\n    def get_data(self, data_path):\n        \"\"\"\n        Retrieve the data associated with `data_path`.\n\n        :type data_path: str\n        :param data_path: The path to the data you wish to retrieve.\n        \"\"\"\n        return self.get_component('data_loader').load_data(data_path)\n\n    def get_service_model(self, service_name, api_version=None):\n        \"\"\"Get the service model object.\n\n        :type service_name: string\n        :param service_name: The service name\n\n        :type api_version: string\n        :param api_version: The API version of the service.  If none is\n            provided, then the latest API version will be used.\n\n        :rtype: L{botocore.model.ServiceModel}\n        :return: The botocore service model for the service.\n\n        \"\"\"\n        service_description = self.get_service_data(service_name, api_version)\n        return ServiceModel(service_description, service_name=service_name)\n\n    def get_waiter_model(self, service_name, api_version=None):\n        loader = self.get_component('data_loader')\n        waiter_config = loader.load_service_model(\n            service_name, 'waiters-2', api_version\n        )\n        return waiter.WaiterModel(waiter_config)\n\n    def get_paginator_model(self, service_name, api_version=None):\n        loader = self.get_component('data_loader')\n        paginator_config = loader.load_service_model(\n            service_name, 'paginators-1', api_version\n        )\n        return paginate.PaginatorModel(paginator_config)\n\n    def get_service_data(self, service_name, api_version=None):\n        \"\"\"\n        Retrieve the fully merged data associated with a service.\n        \"\"\"\n        data_path = service_name\n        service_data = self.get_component('data_loader').load_service_model(\n            data_path, type_name='service-2', api_version=api_version\n        )\n        service_id = EVENT_ALIASES.get(service_name, service_name)\n        self._events.emit(\n            'service-data-loaded.%s' % service_id,\n            service_data=service_data,\n            service_name=service_name,\n            session=self,\n        )\n        return service_data\n\n    def get_available_services(self):\n        \"\"\"\n        Return a list of names of available services.\n        \"\"\"\n        return self.get_component('data_loader').list_available_services(\n            type_name='service-2'\n        )\n\n    def set_debug_logger(self, logger_name='botocore'):\n        \"\"\"\n        Convenience function to quickly configure full debug output\n        to go to the console.\n        \"\"\"\n        self.set_stream_logger(logger_name, logging.DEBUG)\n\n    def set_stream_logger(\n        self, logger_name, log_level, stream=None, format_string=None\n    ):\n        \"\"\"\n        Convenience method to configure a stream logger.\n\n        :type logger_name: str\n        :param logger_name: The name of the logger to configure\n\n        :type log_level: str\n        :param log_level: The log level to set for the logger.  This\n            is any param supported by the ``.setLevel()`` method of\n            a ``Log`` object.\n\n        :type stream: file\n        :param stream: A file like object to log to.  If none is provided\n            then sys.stderr will be used.\n\n        :type format_string: str\n        :param format_string: The format string to use for the log\n            formatter.  If none is provided this will default to\n            ``self.LOG_FORMAT``.\n\n        \"\"\"\n        log = logging.getLogger(logger_name)\n        log.setLevel(logging.DEBUG)\n\n        ch = logging.StreamHandler(stream)\n        ch.setLevel(log_level)\n\n        # create formatter\n        if format_string is None:\n            format_string = self.LOG_FORMAT\n        formatter = logging.Formatter(format_string)\n\n        # add formatter to ch\n        ch.setFormatter(formatter)\n\n        # add ch to logger\n        log.addHandler(ch)\n\n    def set_file_logger(self, log_level, path, logger_name='botocore'):\n        \"\"\"\n        Convenience function to quickly configure any level of logging\n        to a file.\n\n        :type log_level: int\n        :param log_level: A log level as specified in the `logging` module\n\n        :type path: string\n        :param path: Path to the log file.  The file will be created\n            if it doesn't already exist.\n        \"\"\"\n        log = logging.getLogger(logger_name)\n        log.setLevel(logging.DEBUG)\n\n        # create console handler and set level to debug\n        ch = logging.FileHandler(path)\n        ch.setLevel(log_level)\n\n        # create formatter\n        formatter = logging.Formatter(self.LOG_FORMAT)\n\n        # add formatter to ch\n        ch.setFormatter(formatter)\n\n        # add ch to logger\n        log.addHandler(ch)\n\n    def register(\n        self, event_name, handler, unique_id=None, unique_id_uses_count=False\n    ):\n        \"\"\"Register a handler with an event.\n\n        :type event_name: str\n        :param event_name: The name of the event.\n\n        :type handler: callable\n        :param handler: The callback to invoke when the event\n            is emitted.  This object must be callable, and must\n            accept ``**kwargs``.  If either of these preconditions are\n            not met, a ``ValueError`` will be raised.\n\n        :type unique_id: str\n        :param unique_id: An optional identifier to associate with the\n            registration.  A unique_id can only be used once for\n            the entire session registration (unless it is unregistered).\n            This can be used to prevent an event handler from being\n            registered twice.\n\n        :param unique_id_uses_count: boolean\n        :param unique_id_uses_count: Specifies if the event should maintain\n            a count when a ``unique_id`` is registered and unregisted. The\n            event can only be completely unregistered once every register call\n            using the unique id has been matched by an ``unregister`` call.\n            If ``unique_id`` is specified, subsequent ``register``\n            calls must use the same value for  ``unique_id_uses_count``\n            as the ``register`` call that first registered the event.\n\n        :raises ValueError: If the call to ``register`` uses ``unique_id``\n            but the value for ``unique_id_uses_count`` differs from the\n            ``unique_id_uses_count`` value declared by the very first\n            ``register`` call for that ``unique_id``.\n        \"\"\"\n        self._events.register(\n            event_name,\n            handler,\n            unique_id,\n            unique_id_uses_count=unique_id_uses_count,\n        )\n\n    def unregister(\n        self,\n        event_name,\n        handler=None,\n        unique_id=None,\n        unique_id_uses_count=False,\n    ):\n        \"\"\"Unregister a handler with an event.\n\n        :type event_name: str\n        :param event_name: The name of the event.\n\n        :type handler: callable\n        :param handler: The callback to unregister.\n\n        :type unique_id: str\n        :param unique_id: A unique identifier identifying the callback\n            to unregister.  You can provide either the handler or the\n            unique_id, you do not have to provide both.\n\n        :param unique_id_uses_count: boolean\n        :param unique_id_uses_count: Specifies if the event should maintain\n            a count when a ``unique_id`` is registered and unregisted. The\n            event can only be completely unregistered once every ``register``\n            call using the ``unique_id`` has been matched by an ``unregister``\n            call. If the ``unique_id`` is specified, subsequent\n            ``unregister`` calls must use the same value for\n            ``unique_id_uses_count`` as the ``register`` call that first\n            registered the event.\n\n        :raises ValueError: If the call to ``unregister`` uses ``unique_id``\n            but the value for ``unique_id_uses_count`` differs from the\n            ``unique_id_uses_count`` value declared by the very first\n            ``register`` call for that ``unique_id``.\n        \"\"\"\n        self._events.unregister(\n            event_name,\n            handler=handler,\n            unique_id=unique_id,\n            unique_id_uses_count=unique_id_uses_count,\n        )\n\n    def emit(self, event_name, **kwargs):\n        return self._events.emit(event_name, **kwargs)\n\n    def emit_first_non_none_response(self, event_name, **kwargs):\n        responses = self._events.emit(event_name, **kwargs)\n        return first_non_none_response(responses)\n\n    def get_component(self, name):\n        try:\n            return self._components.get_component(name)\n        except ValueError:\n            if name in ['endpoint_resolver', 'exceptions_factory']:\n                warnings.warn(\n                    'Fetching the %s component with the get_component() '\n                    'method is deprecated as the component has always been '\n                    'considered an internal interface of botocore' % name,\n                    DeprecationWarning,\n                )\n                return self._internal_components.get_component(name)\n            raise\n\n    def _get_internal_component(self, name):\n        # While this method may be called by botocore classes outside of the\n        # Session, this method should **never** be used by a class that lives\n        # outside of botocore.\n        return self._internal_components.get_component(name)\n\n    def _register_internal_component(self, name, component):\n        # While this method may be called by botocore classes outside of the\n        # Session, this method should **never** be used by a class that lives\n        # outside of botocore.\n        return self._internal_components.register_component(name, component)\n\n    def register_component(self, name, component):\n        self._components.register_component(name, component)\n\n    def lazy_register_component(self, name, component):\n        self._components.lazy_register_component(name, component)\n\n    def create_client(\n        self,\n        service_name,\n        region_name=None,\n        api_version=None,\n        use_ssl=True,\n        verify=None,\n        endpoint_url=None,\n        aws_access_key_id=None,\n        aws_secret_access_key=None,\n        aws_session_token=None,\n        config=None,\n    ):\n        \"\"\"Create a botocore client.\n\n        :type service_name: string\n        :param service_name: The name of the service for which a client will\n            be created.  You can use the ``Session.get_available_services()``\n            method to get a list of all available service names.\n\n        :type region_name: string\n        :param region_name: The name of the region associated with the client.\n            A client is associated with a single region.\n\n        :type api_version: string\n        :param api_version: The API version to use.  By default, botocore will\n            use the latest API version when creating a client.  You only need\n            to specify this parameter if you want to use a previous API version\n            of the client.\n\n        :type use_ssl: boolean\n        :param use_ssl: Whether or not to use SSL.  By default, SSL is used.\n            Note that not all services support non-ssl connections.\n\n        :type verify: boolean/string\n        :param verify: Whether or not to verify SSL certificates.\n            By default SSL certificates are verified.  You can provide the\n            following values:\n\n            * False - do not validate SSL certificates.  SSL will still be\n              used (unless use_ssl is False), but SSL certificates\n              will not be verified.\n            * path/to/cert/bundle.pem - A filename of the CA cert bundle to\n              uses.  You can specify this argument if you want to use a\n              different CA cert bundle than the one used by botocore.\n\n        :type endpoint_url: string\n        :param endpoint_url: The complete URL to use for the constructed\n            client.  Normally, botocore will automatically construct the\n            appropriate URL to use when communicating with a service.  You can\n            specify a complete URL (including the \"http/https\" scheme) to\n            override this behavior.  If this value is provided, then\n            ``use_ssl`` is ignored.\n\n        :type aws_access_key_id: string\n        :param aws_access_key_id: The access key to use when creating\n            the client.  This is entirely optional, and if not provided,\n            the credentials configured for the session will automatically\n            be used.  You only need to provide this argument if you want\n            to override the credentials used for this specific client.\n\n        :type aws_secret_access_key: string\n        :param aws_secret_access_key: The secret key to use when creating\n            the client.  Same semantics as aws_access_key_id above.\n\n        :type aws_session_token: string\n        :param aws_session_token: The session token to use when creating\n            the client.  Same semantics as aws_access_key_id above.\n\n        :type config: botocore.client.Config\n        :param config: Advanced client configuration options. If a value\n            is specified in the client config, its value will take precedence\n            over environment variables and configuration values, but not over\n            a value passed explicitly to the method. If a default config\n            object is set on the session, the config object used when creating\n            the client will be the result of calling ``merge()`` on the\n            default config with the config provided to this call.\n\n        :rtype: botocore.client.BaseClient\n        :return: A botocore client instance\n\n        \"\"\"\n        default_client_config = self.get_default_client_config()\n        # If a config is provided and a default config is set, then\n        # use the config resulting from merging the two.\n        if config is not None and default_client_config is not None:\n            config = default_client_config.merge(config)\n        # If a config was not provided then use the default\n        # client config from the session\n        elif default_client_config is not None:\n            config = default_client_config\n\n        region_name = self._resolve_region_name(region_name, config)\n\n        # Figure out the verify value base on the various\n        # configuration options.\n        if verify is None:\n            verify = self.get_config_variable('ca_bundle')\n\n        if api_version is None:\n            api_version = self.get_config_variable('api_versions').get(\n                service_name, None\n            )\n\n        loader = self.get_component('data_loader')\n        event_emitter = self.get_component('event_emitter')\n        response_parser_factory = self.get_component('response_parser_factory')\n        if config is not None and config.signature_version is UNSIGNED:\n            credentials = None\n        elif (\n            aws_access_key_id is not None and aws_secret_access_key is not None\n        ):\n            credentials = botocore.credentials.Credentials(\n                access_key=aws_access_key_id,\n                secret_key=aws_secret_access_key,\n                token=aws_session_token,\n            )\n        elif self._missing_cred_vars(aws_access_key_id, aws_secret_access_key):\n            raise PartialCredentialsError(\n                provider='explicit',\n                cred_var=self._missing_cred_vars(\n                    aws_access_key_id, aws_secret_access_key\n                ),\n            )\n        else:\n            credentials = self.get_credentials()\n        auth_token = self.get_auth_token()\n        endpoint_resolver = self._get_internal_component('endpoint_resolver')\n        exceptions_factory = self._get_internal_component('exceptions_factory')\n        config_store = copy.copy(self.get_component('config_store'))\n        user_agent_creator = self.get_component('user_agent_creator')\n        # Session configuration values for the user agent string are applied\n        # just before each client creation because they may have been modified\n        # at any time between session creation and client creation.\n        user_agent_creator.set_session_config(\n            session_user_agent_name=self.user_agent_name,\n            session_user_agent_version=self.user_agent_version,\n            session_user_agent_extra=self.user_agent_extra,\n        )\n        defaults_mode = self._resolve_defaults_mode(config, config_store)\n        if defaults_mode != 'legacy':\n            smart_defaults_factory = self._get_internal_component(\n                'smart_defaults_factory'\n            )\n            smart_defaults_factory.merge_smart_defaults(\n                config_store, defaults_mode, region_name\n            )\n\n        self._add_configured_endpoint_provider(\n            client_name=service_name,\n            config_store=config_store,\n        )\n\n        client_creator = botocore.client.ClientCreator(\n            loader,\n            endpoint_resolver,\n            self.user_agent(),\n            event_emitter,\n            retryhandler,\n            translate,\n            response_parser_factory,\n            exceptions_factory,\n            config_store,\n            user_agent_creator=user_agent_creator,\n        )\n        client = client_creator.create_client(\n            service_name=service_name,\n            region_name=region_name,\n            is_secure=use_ssl,\n            endpoint_url=endpoint_url,\n            verify=verify,\n            credentials=credentials,\n            scoped_config=self.get_scoped_config(),\n            client_config=config,\n            api_version=api_version,\n            auth_token=auth_token,\n        )\n        monitor = self._get_internal_component('monitor')\n        if monitor is not None:\n            monitor.register(client.meta.events)\n        return client\n\n    def _resolve_region_name(self, region_name, config):\n        # Figure out the user-provided region based on the various\n        # configuration options.\n        if region_name is None:\n            if config and config.region_name is not None:\n                region_name = config.region_name\n            else:\n                region_name = self.get_config_variable('region')\n\n        validate_region_name(region_name)\n        # For any client that we create in retrieving credentials\n        # we want to create it using the same region as specified in\n        # creating this client. It is important to note though that the\n        # credentials client is only created once per session. So if a new\n        # client is created with a different region, its credential resolver\n        # will use the region of the first client. However, that is not an\n        # issue as of now because the credential resolver uses only STS and\n        # the credentials returned at regional endpoints are valid across\n        # all regions in the partition.\n        self._last_client_region_used = region_name\n        return region_name\n\n    def _resolve_defaults_mode(self, client_config, config_store):\n        mode = config_store.get_config_variable('defaults_mode')\n\n        if client_config and client_config.defaults_mode:\n            mode = client_config.defaults_mode\n\n        default_config_resolver = self._get_internal_component(\n            'default_config_resolver'\n        )\n        default_modes = default_config_resolver.get_default_modes()\n        lmode = mode.lower()\n        if lmode not in default_modes:\n            raise InvalidDefaultsMode(\n                mode=mode, valid_modes=', '.join(default_modes)\n            )\n\n        return lmode\n\n    def _add_configured_endpoint_provider(self, client_name, config_store):\n        chain = ConfiguredEndpointProvider(\n            full_config=self.full_config,\n            scoped_config=self.get_scoped_config(),\n            client_name=client_name,\n        )\n        config_store.set_config_provider(\n            logical_name='endpoint_url',\n            provider=chain,\n        )\n\n    def _missing_cred_vars(self, access_key, secret_key):\n        if access_key is not None and secret_key is None:\n            return 'aws_secret_access_key'\n        if secret_key is not None and access_key is None:\n            return 'aws_access_key_id'\n        return None\n\n    def get_available_partitions(self):\n        \"\"\"Lists the available partitions found on disk\n\n        :rtype: list\n        :return: Returns a list of partition names (e.g., [\"aws\", \"aws-cn\"])\n        \"\"\"\n        resolver = self._get_internal_component('endpoint_resolver')\n        return resolver.get_available_partitions()\n\n    def get_partition_for_region(self, region_name):\n        \"\"\"Lists the partition name of a particular region.\n\n        :type region_name: string\n        :param region_name: Name of the region to list partition for (e.g.,\n             us-east-1).\n\n        :rtype: string\n        :return: Returns the respective partition name (e.g., aws).\n        \"\"\"\n        resolver = self._get_internal_component('endpoint_resolver')\n        return resolver.get_partition_for_region(region_name)\n\n    def get_available_regions(\n        self, service_name, partition_name='aws', allow_non_regional=False\n    ):\n        \"\"\"Lists the region and endpoint names of a particular partition.\n\n        :type service_name: string\n        :param service_name: Name of a service to list endpoint for (e.g., s3).\n            This parameter accepts a service name (e.g., \"elb\") or endpoint\n            prefix (e.g., \"elasticloadbalancing\").\n\n        :type partition_name: string\n        :param partition_name: Name of the partition to limit endpoints to.\n            (e.g., aws for the public AWS endpoints, aws-cn for AWS China\n            endpoints, aws-us-gov for AWS GovCloud (US) Endpoints, etc.\n\n        :type allow_non_regional: bool\n        :param allow_non_regional: Set to True to include endpoints that are\n             not regional endpoints (e.g., s3-external-1,\n             fips-us-gov-west-1, etc).\n        :return: Returns a list of endpoint names (e.g., [\"us-east-1\"]).\n        \"\"\"\n        resolver = self._get_internal_component('endpoint_resolver')\n        results = []\n        try:\n            service_data = self.get_service_data(service_name)\n            endpoint_prefix = service_data['metadata'].get(\n                'endpointPrefix', service_name\n            )\n            results = resolver.get_available_endpoints(\n                endpoint_prefix, partition_name, allow_non_regional\n            )\n        except UnknownServiceError:\n            pass\n        return results\n\n\nclass ComponentLocator:\n    \"\"\"Service locator for session components.\"\"\"\n\n    def __init__(self):\n        self._components = {}\n        self._deferred = {}\n\n    def get_component(self, name):\n        if name in self._deferred:\n            factory = self._deferred[name]\n            self._components[name] = factory()\n            # Only delete the component from the deferred dict after\n            # successfully creating the object from the factory as well as\n            # injecting the instantiated value into the _components dict.\n            try:\n                del self._deferred[name]\n            except KeyError:\n                # If we get here, it's likely that get_component was called\n                # concurrently from multiple threads, and another thread\n                # already deleted the entry. This means the factory was\n                # probably called twice, but cleaning up the deferred entry\n                # should not crash outright.\n                pass\n        try:\n            return self._components[name]\n        except KeyError:\n            raise ValueError(\"Unknown component: %s\" % name)\n\n    def register_component(self, name, component):\n        self._components[name] = component\n        try:\n            del self._deferred[name]\n        except KeyError:\n            pass\n\n    def lazy_register_component(self, name, no_arg_factory):\n        self._deferred[name] = no_arg_factory\n        try:\n            del self._components[name]\n        except KeyError:\n            pass\n\n\nclass SessionVarDict(MutableMapping):\n    def __init__(self, session, session_vars):\n        self._session = session\n        self._store = copy.copy(session_vars)\n\n    def __getitem__(self, key):\n        return self._store[key]\n\n    def __setitem__(self, key, value):\n        self._store[key] = value\n        self._update_config_store_from_session_vars(key, value)\n\n    def __delitem__(self, key):\n        del self._store[key]\n\n    def __iter__(self):\n        return iter(self._store)\n\n    def __len__(self):\n        return len(self._store)\n\n    def _update_config_store_from_session_vars(\n        self, logical_name, config_options\n    ):\n        # This is for backwards compatibility. The new preferred way to\n        # modify configuration logic is to use the component system to get\n        # the config_store component from the session, and then update\n        # a key with a custom config provider(s).\n        # This backwards compatibility method takes the old session_vars\n        # list of tuples and and transforms that into a set of updates to\n        # the config_store component.\n        config_chain_builder = ConfigChainFactory(session=self._session)\n        config_name, env_vars, default, typecast = config_options\n        config_store = self._session.get_component('config_store')\n        config_store.set_config_provider(\n            logical_name,\n            config_chain_builder.create_config_chain(\n                instance_name=logical_name,\n                env_var_names=env_vars,\n                config_property_names=config_name,\n                default=default,\n                conversion_func=typecast,\n            ),\n        )\n\n\nclass SubsetChainConfigFactory:\n    \"\"\"A class for creating backwards compatible configuration chains.\n\n    This class can be used instead of\n    :class:`botocore.configprovider.ConfigChainFactory` to make it honor the\n    methods argument to get_config_variable. This class can be used to filter\n    out providers that are not in the methods tuple when creating a new config\n    chain.\n    \"\"\"\n\n    def __init__(self, session, methods, environ=None):\n        self._factory = ConfigChainFactory(session, environ)\n        self._supported_methods = methods\n\n    def create_config_chain(\n        self,\n        instance_name=None,\n        env_var_names=None,\n        config_property_name=None,\n        default=None,\n        conversion_func=None,\n    ):\n        \"\"\"Build a config chain following the standard botocore pattern.\n\n        This config chain factory will omit any providers not in the methods\n        tuple provided at initialization. For example if given the tuple\n        ('instance', 'config',) it will not inject the environment provider\n        into the standard config chain. This lets the botocore session support\n        the custom ``methods`` argument for all the default botocore config\n        variables when calling ``get_config_variable``.\n        \"\"\"\n        if 'instance' not in self._supported_methods:\n            instance_name = None\n        if 'env' not in self._supported_methods:\n            env_var_names = None\n        if 'config' not in self._supported_methods:\n            config_property_name = None\n        return self._factory.create_config_chain(\n            instance_name=instance_name,\n            env_var_names=env_var_names,\n            config_property_names=config_property_name,\n            default=default,\n            conversion_func=conversion_func,\n        )\n\n\ndef get_session(env_vars=None):\n    \"\"\"\n    Return a new session object.\n    \"\"\"\n    return Session(env_vars)\n", "botocore/history.py": "# Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport logging\n\nHISTORY_RECORDER = None\nlogger = logging.getLogger(__name__)\n\n\nclass BaseHistoryHandler:\n    def emit(self, event_type, payload, source):\n        raise NotImplementedError('emit()')\n\n\nclass HistoryRecorder:\n    def __init__(self):\n        self._enabled = False\n        self._handlers = []\n\n    def enable(self):\n        self._enabled = True\n\n    def disable(self):\n        self._enabled = False\n\n    def add_handler(self, handler):\n        self._handlers.append(handler)\n\n    def record(self, event_type, payload, source='BOTOCORE'):\n        if self._enabled and self._handlers:\n            for handler in self._handlers:\n                try:\n                    handler.emit(event_type, payload, source)\n                except Exception:\n                    # Never let the process die because we had a failure in\n                    # a record collection handler.\n                    logger.debug(\n                        \"Exception raised in %s.\", handler, exc_info=True\n                    )\n\n\ndef get_global_history_recorder():\n    global HISTORY_RECORDER\n    if HISTORY_RECORDER is None:\n        HISTORY_RECORDER = HistoryRecorder()\n    return HISTORY_RECORDER\n", "botocore/endpoint_provider.py": "# Copyright 2022 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\n\"\"\"\nNOTE: All classes and functions in this module are considered private and are\nsubject to abrupt breaking changes. Please do not use them directly.\n\nTo view the raw JSON that the objects in this module represent, please\ngo to any `endpoint-rule-set.json` file in /botocore/data/<service>/<api version>/\nor you can look at the test files in /tests/unit/data/endpoints/valid-rules/\n\"\"\"\n\n\nimport logging\nimport re\nfrom enum import Enum\nfrom string import Formatter\nfrom typing import NamedTuple\n\nfrom botocore import xform_name\nfrom botocore.compat import IPV4_RE, quote, urlparse\nfrom botocore.exceptions import EndpointResolutionError\nfrom botocore.utils import (\n    ArnParser,\n    InvalidArnException,\n    is_valid_ipv4_endpoint_url,\n    is_valid_ipv6_endpoint_url,\n    lru_cache_weakref,\n    normalize_url_path,\n    percent_encode,\n)\n\nlogger = logging.getLogger(__name__)\n\nTEMPLATE_STRING_RE = re.compile(r\"\\{[a-zA-Z#]+\\}\")\nGET_ATTR_RE = re.compile(r\"(\\w+)\\[(\\d+)\\]\")\nVALID_HOST_LABEL_RE = re.compile(\n    r\"^(?!-)[a-zA-Z\\d-]{1,63}(?<!-)$\",\n)\nCACHE_SIZE = 100\nARN_PARSER = ArnParser()\nSTRING_FORMATTER = Formatter()\n\n\nclass RuleSetStandardLibrary:\n    \"\"\"Rule actions to be performed by the EndpointProvider.\"\"\"\n\n    def __init__(self, partitions_data):\n        self.partitions_data = partitions_data\n\n    def is_func(self, argument):\n        \"\"\"Determine if an object is a function object.\n\n        :type argument: Any\n        :rtype: bool\n        \"\"\"\n        return isinstance(argument, dict) and \"fn\" in argument\n\n    def is_ref(self, argument):\n        \"\"\"Determine if an object is a reference object.\n\n        :type argument: Any\n        :rtype: bool\n        \"\"\"\n        return isinstance(argument, dict) and \"ref\" in argument\n\n    def is_template(self, argument):\n        \"\"\"Determine if an object contains a template string.\n\n        :type argument: Any\n        :rtpe: bool\n        \"\"\"\n        return (\n            isinstance(argument, str)\n            and TEMPLATE_STRING_RE.search(argument) is not None\n        )\n\n    def resolve_template_string(self, value, scope_vars):\n        \"\"\"Resolve and inject values into a template string.\n\n        :type value: str\n        :type scope_vars: dict\n        :rtype: str\n        \"\"\"\n        result = \"\"\n        for literal, reference, _, _ in STRING_FORMATTER.parse(value):\n            if reference is not None:\n                template_value = scope_vars\n                template_params = reference.split(\"#\")\n                for param in template_params:\n                    template_value = template_value[param]\n                result += f\"{literal}{template_value}\"\n            else:\n                result += literal\n        return result\n\n    def resolve_value(self, value, scope_vars):\n        \"\"\"Return evaluated value based on type.\n\n        :type value: Any\n        :type scope_vars: dict\n        :rtype: Any\n        \"\"\"\n        if self.is_func(value):\n            return self.call_function(value, scope_vars)\n        elif self.is_ref(value):\n            return scope_vars.get(value[\"ref\"])\n        elif self.is_template(value):\n            return self.resolve_template_string(value, scope_vars)\n\n        return value\n\n    def convert_func_name(self, value):\n        \"\"\"Normalize function names.\n\n        :type value: str\n        :rtype: str\n        \"\"\"\n        normalized_name = f\"{xform_name(value)}\"\n        if normalized_name == \"not\":\n            normalized_name = f\"_{normalized_name}\"\n        return normalized_name.replace(\".\", \"_\")\n\n    def call_function(self, func_signature, scope_vars):\n        \"\"\"Call the function with the resolved arguments and assign to `scope_vars`\n        when applicable.\n\n        :type func_signature: dict\n        :type scope_vars: dict\n        :rtype: Any\n        \"\"\"\n        func_args = [\n            self.resolve_value(arg, scope_vars)\n            for arg in func_signature[\"argv\"]\n        ]\n        func_name = self.convert_func_name(func_signature[\"fn\"])\n        func = getattr(self, func_name)\n        result = func(*func_args)\n        if \"assign\" in func_signature:\n            assign = func_signature[\"assign\"]\n            if assign in scope_vars:\n                raise EndpointResolutionError(\n                    msg=f\"Assignment {assign} already exists in \"\n                    \"scoped variables and cannot be overwritten\"\n                )\n            scope_vars[assign] = result\n        return result\n\n    def is_set(self, value):\n        \"\"\"Evaluates whether a value is set.\n\n        :type value: Any\n        :rytpe: bool\n        \"\"\"\n        return value is not None\n\n    def get_attr(self, value, path):\n        \"\"\"Find an attribute within a value given a path string. The path can contain\n        the name of the attribute and an index in brackets. A period separating attribute\n        names indicates the one to the right is nested. The index will always occur at\n        the end of the path.\n\n        :type value: dict or list\n        :type path: str\n        :rtype: Any\n        \"\"\"\n        for part in path.split(\".\"):\n            match = GET_ATTR_RE.search(part)\n            if match is not None:\n                name, index = match.groups()\n                index = int(index)\n                value = value.get(name)\n                if value is None or index >= len(value):\n                    return None\n                return value[index]\n            else:\n                value = value[part]\n        return value\n\n    def format_partition_output(self, partition):\n        output = partition[\"outputs\"]\n        output[\"name\"] = partition[\"id\"]\n        return output\n\n    def is_partition_match(self, region, partition):\n        matches_regex = re.match(partition[\"regionRegex\"], region) is not None\n        return region in partition[\"regions\"] or matches_regex\n\n    def aws_partition(self, value):\n        \"\"\"Match a region string to an AWS partition.\n\n        :type value: str\n        :rtype: dict\n        \"\"\"\n        partitions = self.partitions_data['partitions']\n\n        if value is not None:\n            for partition in partitions:\n                if self.is_partition_match(value, partition):\n                    return self.format_partition_output(partition)\n\n        # return the default partition if no matches were found\n        aws_partition = partitions[0]\n        return self.format_partition_output(aws_partition)\n\n    def aws_parse_arn(self, value):\n        \"\"\"Parse and validate string for ARN components.\n\n        :type value: str\n        :rtype: dict\n        \"\"\"\n        if value is None or not value.startswith(\"arn:\"):\n            return None\n\n        try:\n            arn_dict = ARN_PARSER.parse_arn(value)\n        except InvalidArnException:\n            return None\n\n        # partition, resource, and service are required\n        if not all(\n            (arn_dict[\"partition\"], arn_dict[\"service\"], arn_dict[\"resource\"])\n        ):\n            return None\n\n        arn_dict[\"accountId\"] = arn_dict.pop(\"account\")\n\n        resource = arn_dict.pop(\"resource\")\n        arn_dict[\"resourceId\"] = resource.replace(\":\", \"/\").split(\"/\")\n\n        return arn_dict\n\n    def is_valid_host_label(self, value, allow_subdomains):\n        \"\"\"Evaluates whether a value is a valid host label per\n        RFC 1123. If allow_subdomains is True, split on `.` and validate\n        each component separately.\n\n        :type value: str\n        :type allow_subdomains: bool\n        :rtype: bool\n        \"\"\"\n        if value is None or allow_subdomains is False and value.count(\".\") > 0:\n            return False\n\n        if allow_subdomains is True:\n            return all(\n                self.is_valid_host_label(label, False)\n                for label in value.split(\".\")\n            )\n\n        return VALID_HOST_LABEL_RE.match(value) is not None\n\n    def string_equals(self, value1, value2):\n        \"\"\"Evaluates two string values for equality.\n\n        :type value1: str\n        :type value2: str\n        :rtype: bool\n        \"\"\"\n        if not all(isinstance(val, str) for val in (value1, value2)):\n            msg = f\"Both values must be strings, not {type(value1)} and {type(value2)}.\"\n            raise EndpointResolutionError(msg=msg)\n        return value1 == value2\n\n    def uri_encode(self, value):\n        \"\"\"Perform percent-encoding on an input string.\n\n        :type value: str\n        :rytpe: str\n        \"\"\"\n        if value is None:\n            return None\n\n        return percent_encode(value)\n\n    def parse_url(self, value):\n        \"\"\"Parse a URL string into components.\n\n        :type value: str\n        :rtype: dict\n        \"\"\"\n        if value is None:\n            return None\n\n        url_components = urlparse(value)\n        try:\n            # url_parse may assign non-integer values to\n            # `port` and will fail when accessed.\n            url_components.port\n        except ValueError:\n            return None\n\n        scheme = url_components.scheme\n        query = url_components.query\n        # URLs with queries are not supported\n        if scheme not in (\"https\", \"http\") or len(query) > 0:\n            return None\n\n        path = url_components.path\n        normalized_path = quote(normalize_url_path(path))\n        if not normalized_path.endswith(\"/\"):\n            normalized_path = f\"{normalized_path}/\"\n\n        return {\n            \"scheme\": scheme,\n            \"authority\": url_components.netloc,\n            \"path\": path,\n            \"normalizedPath\": normalized_path,\n            \"isIp\": is_valid_ipv4_endpoint_url(value)\n            or is_valid_ipv6_endpoint_url(value),\n        }\n\n    def boolean_equals(self, value1, value2):\n        \"\"\"Evaluates two boolean values for equality.\n\n        :type value1: bool\n        :type value2: bool\n        :rtype: bool\n        \"\"\"\n        if not all(isinstance(val, bool) for val in (value1, value2)):\n            msg = f\"Both arguments must be bools, not {type(value1)} and {type(value2)}.\"\n            raise EndpointResolutionError(msg=msg)\n        return value1 is value2\n\n    def is_ascii(self, value):\n        \"\"\"Evaluates if a string only contains ASCII characters.\n\n        :type value: str\n        :rtype: bool\n        \"\"\"\n        try:\n            value.encode(\"ascii\")\n            return True\n        except UnicodeEncodeError:\n            return False\n\n    def substring(self, value, start, stop, reverse):\n        \"\"\"Computes a substring given the start index and end index. If `reverse` is\n        True, slice the string from the end instead.\n\n        :type value: str\n        :type start: int\n        :type end: int\n        :type reverse: bool\n        :rtype: str\n        \"\"\"\n        if not isinstance(value, str):\n            msg = f\"Input must be a string, not {type(value)}.\"\n            raise EndpointResolutionError(msg=msg)\n        if start >= stop or len(value) < stop or not self.is_ascii(value):\n            return None\n\n        if reverse is True:\n            r_start = len(value) - stop\n            r_stop = len(value) - start\n            return value[r_start:r_stop]\n\n        return value[start:stop]\n\n    def _not(self, value):\n        \"\"\"A function implementation of the logical operator `not`.\n\n        :type value: Any\n        :rtype: bool\n        \"\"\"\n        return not value\n\n    def aws_is_virtual_hostable_s3_bucket(self, value, allow_subdomains):\n        \"\"\"Evaluates whether a value is a valid bucket name for virtual host\n        style bucket URLs. To pass, the value must meet the following criteria:\n        1. is_valid_host_label(value) is True\n        2. length between 3 and 63 characters (inclusive)\n        3. does not contain uppercase characters\n        4. is not formatted as an IP address\n\n        If allow_subdomains is True, split on `.` and validate\n        each component separately.\n\n        :type value: str\n        :type allow_subdomains: bool\n        :rtype: bool\n        \"\"\"\n        if (\n            value is None\n            or len(value) < 3\n            or value.lower() != value\n            or IPV4_RE.match(value) is not None\n        ):\n            return False\n\n        return self.is_valid_host_label(\n            value, allow_subdomains=allow_subdomains\n        )\n\n\n# maintains backwards compatibility as `Library` was misspelled\n# in earlier versions\nRuleSetStandardLibary = RuleSetStandardLibrary\n\n\nclass BaseRule:\n    \"\"\"Base interface for individual endpoint rules.\"\"\"\n\n    def __init__(self, conditions, documentation=None):\n        self.conditions = conditions\n        self.documentation = documentation\n\n    def evaluate(self, scope_vars, rule_lib):\n        raise NotImplementedError()\n\n    def evaluate_conditions(self, scope_vars, rule_lib):\n        \"\"\"Determine if all conditions in a rule are met.\n\n        :type scope_vars: dict\n        :type rule_lib: RuleSetStandardLibrary\n        :rtype: bool\n        \"\"\"\n        for func_signature in self.conditions:\n            result = rule_lib.call_function(func_signature, scope_vars)\n            if result is False or result is None:\n                return False\n        return True\n\n\nclass RuleSetEndpoint(NamedTuple):\n    \"\"\"A resolved endpoint object returned by a rule.\"\"\"\n\n    url: str\n    properties: dict\n    headers: dict\n\n\nclass EndpointRule(BaseRule):\n    def __init__(self, endpoint, **kwargs):\n        super().__init__(**kwargs)\n        self.endpoint = endpoint\n\n    def evaluate(self, scope_vars, rule_lib):\n        \"\"\"Determine if conditions are met to provide a valid endpoint.\n\n        :type scope_vars: dict\n        :rtype: RuleSetEndpoint\n        \"\"\"\n        if self.evaluate_conditions(scope_vars, rule_lib):\n            url = rule_lib.resolve_value(self.endpoint[\"url\"], scope_vars)\n            properties = self.resolve_properties(\n                self.endpoint.get(\"properties\", {}),\n                scope_vars,\n                rule_lib,\n            )\n            headers = self.resolve_headers(scope_vars, rule_lib)\n            return RuleSetEndpoint(\n                url=url, properties=properties, headers=headers\n            )\n\n        return None\n\n    def resolve_properties(self, properties, scope_vars, rule_lib):\n        \"\"\"Traverse `properties` attribute, resolving any template strings.\n\n        :type properties: dict/list/str\n        :type scope_vars: dict\n        :type rule_lib: RuleSetStandardLibrary\n        :rtype: dict\n        \"\"\"\n        if isinstance(properties, list):\n            return [\n                self.resolve_properties(prop, scope_vars, rule_lib)\n                for prop in properties\n            ]\n        elif isinstance(properties, dict):\n            return {\n                key: self.resolve_properties(value, scope_vars, rule_lib)\n                for key, value in properties.items()\n            }\n        elif rule_lib.is_template(properties):\n            return rule_lib.resolve_template_string(properties, scope_vars)\n\n        return properties\n\n    def resolve_headers(self, scope_vars, rule_lib):\n        \"\"\"Iterate through headers attribute resolving all values.\n\n        :type scope_vars: dict\n        :type rule_lib: RuleSetStandardLibrary\n        :rtype: dict\n        \"\"\"\n        resolved_headers = {}\n        headers = self.endpoint.get(\"headers\", {})\n\n        for header, values in headers.items():\n            resolved_headers[header] = [\n                rule_lib.resolve_value(item, scope_vars) for item in values\n            ]\n        return resolved_headers\n\n\nclass ErrorRule(BaseRule):\n    def __init__(self, error, **kwargs):\n        super().__init__(**kwargs)\n        self.error = error\n\n    def evaluate(self, scope_vars, rule_lib):\n        \"\"\"If an error rule's conditions are met, raise an error rule.\n\n        :type scope_vars: dict\n        :type rule_lib: RuleSetStandardLibrary\n        :rtype: EndpointResolutionError\n        \"\"\"\n        if self.evaluate_conditions(scope_vars, rule_lib):\n            error = rule_lib.resolve_value(self.error, scope_vars)\n            raise EndpointResolutionError(msg=error)\n        return None\n\n\nclass TreeRule(BaseRule):\n    \"\"\"A tree rule is non-terminal meaning it will never be returned to a provider.\n    Additionally this means it has no attributes that need to be resolved.\n    \"\"\"\n\n    def __init__(self, rules, **kwargs):\n        super().__init__(**kwargs)\n        self.rules = [RuleCreator.create(**rule) for rule in rules]\n\n    def evaluate(self, scope_vars, rule_lib):\n        \"\"\"If a tree rule's conditions are met, iterate its sub-rules\n        and return first result found.\n\n        :type scope_vars: dict\n        :type rule_lib: RuleSetStandardLibrary\n        :rtype: RuleSetEndpoint/EndpointResolutionError\n        \"\"\"\n        if self.evaluate_conditions(scope_vars, rule_lib):\n            for rule in self.rules:\n                # don't share scope_vars between rules\n                rule_result = rule.evaluate(scope_vars.copy(), rule_lib)\n                if rule_result:\n                    return rule_result\n        return None\n\n\nclass RuleCreator:\n    endpoint = EndpointRule\n    error = ErrorRule\n    tree = TreeRule\n\n    @classmethod\n    def create(cls, **kwargs):\n        \"\"\"Create a rule instance from metadata.\n\n        :rtype: TreeRule/EndpointRule/ErrorRule\n        \"\"\"\n        rule_type = kwargs.pop(\"type\")\n        try:\n            rule_class = getattr(cls, rule_type)\n        except AttributeError:\n            raise EndpointResolutionError(\n                msg=f\"Unknown rule type: {rule_type}. A rule must \"\n                \"be of type tree, endpoint or error.\"\n            )\n        else:\n            return rule_class(**kwargs)\n\n\nclass ParameterType(Enum):\n    \"\"\"Translation from `type` attribute to native Python type.\"\"\"\n\n    string = str\n    boolean = bool\n\n\nclass ParameterDefinition:\n    \"\"\"The spec of an individual parameter defined in a RuleSet.\"\"\"\n\n    def __init__(\n        self,\n        name,\n        parameter_type,\n        documentation=None,\n        builtIn=None,\n        default=None,\n        required=None,\n        deprecated=None,\n    ):\n        self.name = name\n        try:\n            self.parameter_type = getattr(\n                ParameterType, parameter_type.lower()\n            ).value\n        except AttributeError:\n            raise EndpointResolutionError(\n                msg=f\"Unknown parameter type: {parameter_type}. \"\n                \"A parameter must be of type string or boolean.\"\n            )\n        self.documentation = documentation\n        self.builtin = builtIn\n        self.default = default\n        self.required = required\n        self.deprecated = deprecated\n\n    def validate_input(self, value):\n        \"\"\"Perform base validation on parameter input.\n\n        :type value: Any\n        :raises: EndpointParametersError\n        \"\"\"\n\n        if not isinstance(value, self.parameter_type):\n            raise EndpointResolutionError(\n                msg=f\"Value ({self.name}) is the wrong \"\n                f\"type. Must be {self.parameter_type}.\"\n            )\n        if self.deprecated is not None:\n            depr_str = f\"{self.name} has been deprecated.\"\n            msg = self.deprecated.get(\"message\")\n            since = self.deprecated.get(\"since\")\n            if msg:\n                depr_str += f\"\\n{msg}\"\n            if since:\n                depr_str += f\"\\nDeprecated since {since}.\"\n            logger.info(depr_str)\n\n        return None\n\n    def process_input(self, value):\n        \"\"\"Process input against spec, applying default if value is None.\"\"\"\n        if value is None:\n            if self.default is not None:\n                return self.default\n            if self.required:\n                raise EndpointResolutionError(\n                    f\"Cannot find value for required parameter {self.name}\"\n                )\n            # in all other cases, the parameter will keep the value None\n        else:\n            self.validate_input(value)\n        return value\n\n\nclass RuleSet:\n    \"\"\"Collection of rules to derive a routable service endpoint.\"\"\"\n\n    def __init__(\n        self, version, parameters, rules, partitions, documentation=None\n    ):\n        self.version = version\n        self.parameters = self._ingest_parameter_spec(parameters)\n        self.rules = [RuleCreator.create(**rule) for rule in rules]\n        self.rule_lib = RuleSetStandardLibrary(partitions)\n        self.documentation = documentation\n\n    def _ingest_parameter_spec(self, parameters):\n        return {\n            name: ParameterDefinition(\n                name,\n                spec[\"type\"],\n                spec.get(\"documentation\"),\n                spec.get(\"builtIn\"),\n                spec.get(\"default\"),\n                spec.get(\"required\"),\n                spec.get(\"deprecated\"),\n            )\n            for name, spec in parameters.items()\n        }\n\n    def process_input_parameters(self, input_params):\n        \"\"\"Process each input parameter against its spec.\n\n        :type input_params: dict\n        \"\"\"\n        for name, spec in self.parameters.items():\n            value = spec.process_input(input_params.get(name))\n            if value is not None:\n                input_params[name] = value\n        return None\n\n    def evaluate(self, input_parameters):\n        \"\"\"Evaluate input parameters against rules returning first match.\n\n        :type input_parameters: dict\n        \"\"\"\n        self.process_input_parameters(input_parameters)\n        for rule in self.rules:\n            evaluation = rule.evaluate(input_parameters.copy(), self.rule_lib)\n            if evaluation is not None:\n                return evaluation\n        return None\n\n\nclass EndpointProvider:\n    \"\"\"Derives endpoints from a RuleSet for given input parameters.\"\"\"\n\n    def __init__(self, ruleset_data, partition_data):\n        self.ruleset = RuleSet(**ruleset_data, partitions=partition_data)\n\n    @lru_cache_weakref(maxsize=CACHE_SIZE)\n    def resolve_endpoint(self, **input_parameters):\n        \"\"\"Match input parameters to a rule.\n\n        :type input_parameters: dict\n        :rtype: RuleSetEndpoint\n        \"\"\"\n        params_for_error = input_parameters.copy()\n        endpoint = self.ruleset.evaluate(input_parameters)\n        if endpoint is None:\n            param_string = \"\\n\".join(\n                [f\"{key}: {value}\" for key, value in params_for_error.items()]\n            )\n            raise EndpointResolutionError(\n                msg=f\"No endpoint found for parameters:\\n{param_string}\"\n            )\n        return endpoint\n", "botocore/hooks.py": "# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport copy\nimport logging\nfrom collections import deque, namedtuple\n\nfrom botocore.compat import accepts_kwargs\nfrom botocore.utils import EVENT_ALIASES\n\nlogger = logging.getLogger(__name__)\n\n\n_NodeList = namedtuple('NodeList', ['first', 'middle', 'last'])\n_FIRST = 0\n_MIDDLE = 1\n_LAST = 2\n\n\nclass NodeList(_NodeList):\n    def __copy__(self):\n        first_copy = copy.copy(self.first)\n        middle_copy = copy.copy(self.middle)\n        last_copy = copy.copy(self.last)\n        copied = NodeList(first_copy, middle_copy, last_copy)\n        return copied\n\n\ndef first_non_none_response(responses, default=None):\n    \"\"\"Find first non None response in a list of tuples.\n\n    This function can be used to find the first non None response from\n    handlers connected to an event.  This is useful if you are interested\n    in the returned responses from event handlers. Example usage::\n\n        print(first_non_none_response([(func1, None), (func2, 'foo'),\n                                       (func3, 'bar')]))\n        # This will print 'foo'\n\n    :type responses: list of tuples\n    :param responses: The responses from the ``EventHooks.emit`` method.\n        This is a list of tuples, and each tuple is\n        (handler, handler_response).\n\n    :param default: If no non-None responses are found, then this default\n        value will be returned.\n\n    :return: The first non-None response in the list of tuples.\n\n    \"\"\"\n    for response in responses:\n        if response[1] is not None:\n            return response[1]\n    return default\n\n\nclass BaseEventHooks:\n    def emit(self, event_name, **kwargs):\n        \"\"\"Call all handlers subscribed to an event.\n\n        :type event_name: str\n        :param event_name: The name of the event to emit.\n\n        :type **kwargs: dict\n        :param **kwargs: Arbitrary kwargs to pass through to the\n            subscribed handlers.  The ``event_name`` will be injected\n            into the kwargs so it's not necessary to add this to **kwargs.\n\n        :rtype: list of tuples\n        :return: A list of ``(handler_func, handler_func_return_value)``\n\n        \"\"\"\n        return []\n\n    def register(\n        self, event_name, handler, unique_id=None, unique_id_uses_count=False\n    ):\n        \"\"\"Register an event handler for a given event.\n\n        If a ``unique_id`` is given, the handler will not be registered\n        if a handler with the ``unique_id`` has already been registered.\n\n        Handlers are called in the order they have been registered.\n        Note handlers can also be registered with ``register_first()``\n        and ``register_last()``.  All handlers registered with\n        ``register_first()`` are called before handlers registered\n        with ``register()`` which are called before handlers registered\n        with ``register_last()``.\n\n        \"\"\"\n        self._verify_and_register(\n            event_name,\n            handler,\n            unique_id,\n            register_method=self._register,\n            unique_id_uses_count=unique_id_uses_count,\n        )\n\n    def register_first(\n        self, event_name, handler, unique_id=None, unique_id_uses_count=False\n    ):\n        \"\"\"Register an event handler to be called first for an event.\n\n        All event handlers registered with ``register_first()`` will\n        be called before handlers registered with ``register()`` and\n        ``register_last()``.\n\n        \"\"\"\n        self._verify_and_register(\n            event_name,\n            handler,\n            unique_id,\n            register_method=self._register_first,\n            unique_id_uses_count=unique_id_uses_count,\n        )\n\n    def register_last(\n        self, event_name, handler, unique_id=None, unique_id_uses_count=False\n    ):\n        \"\"\"Register an event handler to be called last for an event.\n\n        All event handlers registered with ``register_last()`` will be called\n        after handlers registered with ``register_first()`` and ``register()``.\n\n        \"\"\"\n        self._verify_and_register(\n            event_name,\n            handler,\n            unique_id,\n            register_method=self._register_last,\n            unique_id_uses_count=unique_id_uses_count,\n        )\n\n    def _verify_and_register(\n        self,\n        event_name,\n        handler,\n        unique_id,\n        register_method,\n        unique_id_uses_count,\n    ):\n        self._verify_is_callable(handler)\n        self._verify_accept_kwargs(handler)\n        register_method(event_name, handler, unique_id, unique_id_uses_count)\n\n    def unregister(\n        self,\n        event_name,\n        handler=None,\n        unique_id=None,\n        unique_id_uses_count=False,\n    ):\n        \"\"\"Unregister an event handler for a given event.\n\n        If no ``unique_id`` was given during registration, then the\n        first instance of the event handler is removed (if the event\n        handler has been registered multiple times).\n\n        \"\"\"\n        pass\n\n    def _verify_is_callable(self, func):\n        if not callable(func):\n            raise ValueError(\"Event handler %s must be callable.\" % func)\n\n    def _verify_accept_kwargs(self, func):\n        \"\"\"Verifies a callable accepts kwargs\n\n        :type func: callable\n        :param func: A callable object.\n\n        :returns: True, if ``func`` accepts kwargs, otherwise False.\n\n        \"\"\"\n        try:\n            if not accepts_kwargs(func):\n                raise ValueError(\n                    f\"Event handler {func} must accept keyword \"\n                    f\"arguments (**kwargs)\"\n                )\n        except TypeError:\n            return False\n\n\nclass HierarchicalEmitter(BaseEventHooks):\n    def __init__(self):\n        # We keep a reference to the handlers for quick\n        # read only access (we never modify self._handlers).\n        # A cache of event name to handler list.\n        self._lookup_cache = {}\n        self._handlers = _PrefixTrie()\n        # This is used to ensure that unique_id's are only\n        # registered once.\n        self._unique_id_handlers = {}\n\n    def _emit(self, event_name, kwargs, stop_on_response=False):\n        \"\"\"\n        Emit an event with optional keyword arguments.\n\n        :type event_name: string\n        :param event_name: Name of the event\n        :type kwargs: dict\n        :param kwargs: Arguments to be passed to the handler functions.\n        :type stop_on_response: boolean\n        :param stop_on_response: Whether to stop on the first non-None\n                                response. If False, then all handlers\n                                will be called. This is especially useful\n                                to handlers which mutate data and then\n                                want to stop propagation of the event.\n        :rtype: list\n        :return: List of (handler, response) tuples from all processed\n                 handlers.\n        \"\"\"\n        responses = []\n        # Invoke the event handlers from most specific\n        # to least specific, each time stripping off a dot.\n        handlers_to_call = self._lookup_cache.get(event_name)\n        if handlers_to_call is None:\n            handlers_to_call = self._handlers.prefix_search(event_name)\n            self._lookup_cache[event_name] = handlers_to_call\n        elif not handlers_to_call:\n            # Short circuit and return an empty response is we have\n            # no handlers to call.  This is the common case where\n            # for the majority of signals, nothing is listening.\n            return []\n        kwargs['event_name'] = event_name\n        responses = []\n        for handler in handlers_to_call:\n            logger.debug('Event %s: calling handler %s', event_name, handler)\n            response = handler(**kwargs)\n            responses.append((handler, response))\n            if stop_on_response and response is not None:\n                return responses\n        return responses\n\n    def emit(self, event_name, **kwargs):\n        \"\"\"\n        Emit an event by name with arguments passed as keyword args.\n\n            >>> responses = emitter.emit(\n            ...     'my-event.service.operation', arg1='one', arg2='two')\n\n        :rtype: list\n        :return: List of (handler, response) tuples from all processed\n                 handlers.\n        \"\"\"\n        return self._emit(event_name, kwargs)\n\n    def emit_until_response(self, event_name, **kwargs):\n        \"\"\"\n        Emit an event by name with arguments passed as keyword args,\n        until the first non-``None`` response is received. This\n        method prevents subsequent handlers from being invoked.\n\n            >>> handler, response = emitter.emit_until_response(\n                'my-event.service.operation', arg1='one', arg2='two')\n\n        :rtype: tuple\n        :return: The first (handler, response) tuple where the response\n                 is not ``None``, otherwise (``None``, ``None``).\n        \"\"\"\n        responses = self._emit(event_name, kwargs, stop_on_response=True)\n        if responses:\n            return responses[-1]\n        else:\n            return (None, None)\n\n    def _register(\n        self, event_name, handler, unique_id=None, unique_id_uses_count=False\n    ):\n        self._register_section(\n            event_name,\n            handler,\n            unique_id,\n            unique_id_uses_count,\n            section=_MIDDLE,\n        )\n\n    def _register_first(\n        self, event_name, handler, unique_id=None, unique_id_uses_count=False\n    ):\n        self._register_section(\n            event_name,\n            handler,\n            unique_id,\n            unique_id_uses_count,\n            section=_FIRST,\n        )\n\n    def _register_last(\n        self, event_name, handler, unique_id, unique_id_uses_count=False\n    ):\n        self._register_section(\n            event_name, handler, unique_id, unique_id_uses_count, section=_LAST\n        )\n\n    def _register_section(\n        self, event_name, handler, unique_id, unique_id_uses_count, section\n    ):\n        if unique_id is not None:\n            if unique_id in self._unique_id_handlers:\n                # We've already registered a handler using this unique_id\n                # so we don't need to register it again.\n                count = self._unique_id_handlers[unique_id].get('count', None)\n                if unique_id_uses_count:\n                    if not count:\n                        raise ValueError(\n                            \"Initial registration of  unique id %s was \"\n                            \"specified to use a counter. Subsequent register \"\n                            \"calls to unique id must specify use of a counter \"\n                            \"as well.\" % unique_id\n                        )\n                    else:\n                        self._unique_id_handlers[unique_id]['count'] += 1\n                else:\n                    if count:\n                        raise ValueError(\n                            \"Initial registration of unique id %s was \"\n                            \"specified to not use a counter. Subsequent \"\n                            \"register calls to unique id must specify not to \"\n                            \"use a counter as well.\" % unique_id\n                        )\n                return\n            else:\n                # Note that the trie knows nothing about the unique\n                # id.  We track uniqueness in this class via the\n                # _unique_id_handlers.\n                self._handlers.append_item(\n                    event_name, handler, section=section\n                )\n                unique_id_handler_item = {'handler': handler}\n                if unique_id_uses_count:\n                    unique_id_handler_item['count'] = 1\n                self._unique_id_handlers[unique_id] = unique_id_handler_item\n        else:\n            self._handlers.append_item(event_name, handler, section=section)\n        # Super simple caching strategy for now, if we change the registrations\n        # clear the cache.  This has the opportunity for smarter invalidations.\n        self._lookup_cache = {}\n\n    def unregister(\n        self,\n        event_name,\n        handler=None,\n        unique_id=None,\n        unique_id_uses_count=False,\n    ):\n        if unique_id is not None:\n            try:\n                count = self._unique_id_handlers[unique_id].get('count', None)\n            except KeyError:\n                # There's no handler matching that unique_id so we have\n                # nothing to unregister.\n                return\n            if unique_id_uses_count:\n                if count is None:\n                    raise ValueError(\n                        \"Initial registration of unique id %s was specified to \"\n                        \"use a counter. Subsequent unregister calls to unique \"\n                        \"id must specify use of a counter as well.\" % unique_id\n                    )\n                elif count == 1:\n                    handler = self._unique_id_handlers.pop(unique_id)[\n                        'handler'\n                    ]\n                else:\n                    self._unique_id_handlers[unique_id]['count'] -= 1\n                    return\n            else:\n                if count:\n                    raise ValueError(\n                        \"Initial registration of unique id %s was specified \"\n                        \"to not use a counter. Subsequent unregister calls \"\n                        \"to unique id must specify not to use a counter as \"\n                        \"well.\" % unique_id\n                    )\n                handler = self._unique_id_handlers.pop(unique_id)['handler']\n        try:\n            self._handlers.remove_item(event_name, handler)\n            self._lookup_cache = {}\n        except ValueError:\n            pass\n\n    def __copy__(self):\n        new_instance = self.__class__()\n        new_state = self.__dict__.copy()\n        new_state['_handlers'] = copy.copy(self._handlers)\n        new_state['_unique_id_handlers'] = copy.copy(self._unique_id_handlers)\n        new_instance.__dict__ = new_state\n        return new_instance\n\n\nclass EventAliaser(BaseEventHooks):\n    def __init__(self, event_emitter, event_aliases=None):\n        self._event_aliases = event_aliases\n        if event_aliases is None:\n            self._event_aliases = EVENT_ALIASES\n        self._alias_name_cache = {}\n        self._emitter = event_emitter\n\n    def emit(self, event_name, **kwargs):\n        aliased_event_name = self._alias_event_name(event_name)\n        return self._emitter.emit(aliased_event_name, **kwargs)\n\n    def emit_until_response(self, event_name, **kwargs):\n        aliased_event_name = self._alias_event_name(event_name)\n        return self._emitter.emit_until_response(aliased_event_name, **kwargs)\n\n    def register(\n        self, event_name, handler, unique_id=None, unique_id_uses_count=False\n    ):\n        aliased_event_name = self._alias_event_name(event_name)\n        return self._emitter.register(\n            aliased_event_name, handler, unique_id, unique_id_uses_count\n        )\n\n    def register_first(\n        self, event_name, handler, unique_id=None, unique_id_uses_count=False\n    ):\n        aliased_event_name = self._alias_event_name(event_name)\n        return self._emitter.register_first(\n            aliased_event_name, handler, unique_id, unique_id_uses_count\n        )\n\n    def register_last(\n        self, event_name, handler, unique_id=None, unique_id_uses_count=False\n    ):\n        aliased_event_name = self._alias_event_name(event_name)\n        return self._emitter.register_last(\n            aliased_event_name, handler, unique_id, unique_id_uses_count\n        )\n\n    def unregister(\n        self,\n        event_name,\n        handler=None,\n        unique_id=None,\n        unique_id_uses_count=False,\n    ):\n        aliased_event_name = self._alias_event_name(event_name)\n        return self._emitter.unregister(\n            aliased_event_name, handler, unique_id, unique_id_uses_count\n        )\n\n    def _alias_event_name(self, event_name):\n        if event_name in self._alias_name_cache:\n            return self._alias_name_cache[event_name]\n\n        for old_part, new_part in self._event_aliases.items():\n            # We can't simply do a string replace for everything, otherwise we\n            # might end up translating substrings that we never intended to\n            # translate. When there aren't any dots in the old event name\n            # part, then we can quickly replace the item in the list if it's\n            # there.\n            event_parts = event_name.split('.')\n            if '.' not in old_part:\n                try:\n                    # Theoretically a given event name could have the same part\n                    # repeated, but in practice this doesn't happen\n                    event_parts[event_parts.index(old_part)] = new_part\n                except ValueError:\n                    continue\n\n            # If there's dots in the name, it gets more complicated. Now we\n            # have to replace multiple sections of the original event.\n            elif old_part in event_name:\n                old_parts = old_part.split('.')\n                self._replace_subsection(event_parts, old_parts, new_part)\n            else:\n                continue\n\n            new_name = '.'.join(event_parts)\n            logger.debug(\n                f\"Changing event name from {event_name} to {new_name}\"\n            )\n            self._alias_name_cache[event_name] = new_name\n            return new_name\n\n        self._alias_name_cache[event_name] = event_name\n        return event_name\n\n    def _replace_subsection(self, sections, old_parts, new_part):\n        for i in range(len(sections)):\n            if (\n                sections[i] == old_parts[0]\n                and sections[i : i + len(old_parts)] == old_parts\n            ):\n                sections[i : i + len(old_parts)] = [new_part]\n                return\n\n    def __copy__(self):\n        return self.__class__(\n            copy.copy(self._emitter), copy.copy(self._event_aliases)\n        )\n\n\nclass _PrefixTrie:\n    \"\"\"Specialized prefix trie that handles wildcards.\n\n    The prefixes in this case are based on dot separated\n    names so 'foo.bar.baz' is::\n\n        foo -> bar -> baz\n\n    Wildcard support just means that having a key such as 'foo.bar.*.baz' will\n    be matched with a call to ``get_items(key='foo.bar.ANYTHING.baz')``.\n\n    You can think of this prefix trie as the equivalent as defaultdict(list),\n    except that it can do prefix searches:\n\n        foo.bar.baz -> A\n        foo.bar -> B\n        foo -> C\n\n    Calling ``get_items('foo.bar.baz')`` will return [A + B + C], from\n    most specific to least specific.\n\n    \"\"\"\n\n    def __init__(self):\n        # Each dictionary can be though of as a node, where a node\n        # has values associated with the node, and children is a link\n        # to more nodes.  So 'foo.bar' would have a 'foo' node with\n        # a 'bar' node as a child of foo.\n        # {'foo': {'children': {'bar': {...}}}}.\n        self._root = {'chunk': None, 'children': {}, 'values': None}\n\n    def append_item(self, key, value, section=_MIDDLE):\n        \"\"\"Add an item to a key.\n\n        If a value is already associated with that key, the new\n        value is appended to the list for the key.\n        \"\"\"\n        key_parts = key.split('.')\n        current = self._root\n        for part in key_parts:\n            if part not in current['children']:\n                new_child = {'chunk': part, 'values': None, 'children': {}}\n                current['children'][part] = new_child\n                current = new_child\n            else:\n                current = current['children'][part]\n        if current['values'] is None:\n            current['values'] = NodeList([], [], [])\n        current['values'][section].append(value)\n\n    def prefix_search(self, key):\n        \"\"\"Collect all items that are prefixes of key.\n\n        Prefix in this case are delineated by '.' characters so\n        'foo.bar.baz' is a 3 chunk sequence of 3 \"prefixes\" (\n        \"foo\", \"bar\", and \"baz\").\n\n        \"\"\"\n        collected = deque()\n        key_parts = key.split('.')\n        current = self._root\n        self._get_items(current, key_parts, collected, 0)\n        return collected\n\n    def _get_items(self, starting_node, key_parts, collected, starting_index):\n        stack = [(starting_node, starting_index)]\n        key_parts_len = len(key_parts)\n        # Traverse down the nodes, where at each level we add the\n        # next part from key_parts as well as the wildcard element '*'.\n        # This means for each node we see we potentially add two more\n        # elements to our stack.\n        while stack:\n            current_node, index = stack.pop()\n            if current_node['values']:\n                # We're using extendleft because we want\n                # the values associated with the node furthest\n                # from the root to come before nodes closer\n                # to the root.  extendleft() also adds its items\n                # in right-left order so .extendleft([1, 2, 3])\n                # will result in final_list = [3, 2, 1], which is\n                # why we reverse the lists.\n                node_list = current_node['values']\n                complete_order = (\n                    node_list.first + node_list.middle + node_list.last\n                )\n                collected.extendleft(reversed(complete_order))\n            if not index == key_parts_len:\n                children = current_node['children']\n                directs = children.get(key_parts[index])\n                wildcard = children.get('*')\n                next_index = index + 1\n                if wildcard is not None:\n                    stack.append((wildcard, next_index))\n                if directs is not None:\n                    stack.append((directs, next_index))\n\n    def remove_item(self, key, value):\n        \"\"\"Remove an item associated with a key.\n\n        If the value is not associated with the key a ``ValueError``\n        will be raised.  If the key does not exist in the trie, a\n        ``ValueError`` will be raised.\n\n        \"\"\"\n        key_parts = key.split('.')\n        current = self._root\n        self._remove_item(current, key_parts, value, index=0)\n\n    def _remove_item(self, current_node, key_parts, value, index):\n        if current_node is None:\n            return\n        elif index < len(key_parts):\n            next_node = current_node['children'].get(key_parts[index])\n            if next_node is not None:\n                self._remove_item(next_node, key_parts, value, index + 1)\n                if index == len(key_parts) - 1:\n                    node_list = next_node['values']\n                    if value in node_list.first:\n                        node_list.first.remove(value)\n                    elif value in node_list.middle:\n                        node_list.middle.remove(value)\n                    elif value in node_list.last:\n                        node_list.last.remove(value)\n                if not next_node['children'] and not next_node['values']:\n                    # Then this is a leaf node with no values so\n                    # we can just delete this link from the parent node.\n                    # This makes subsequent search faster in the case\n                    # where a key does not exist.\n                    del current_node['children'][key_parts[index]]\n            else:\n                raise ValueError(f\"key is not in trie: {'.'.join(key_parts)}\")\n\n    def __copy__(self):\n        # The fact that we're using a nested dict under the covers\n        # is an implementation detail, and the user shouldn't have\n        # to know that they'd normally need a deepcopy so we expose\n        # __copy__ instead of __deepcopy__.\n        new_copy = self.__class__()\n        copied_attrs = self._recursive_copy(self.__dict__)\n        new_copy.__dict__ = copied_attrs\n        return new_copy\n\n    def _recursive_copy(self, node):\n        # We can't use copy.deepcopy because we actually only want to copy\n        # the structure of the trie, not the handlers themselves.\n        # Each node has a chunk, children, and values.\n        copied_node = {}\n        for key, value in node.items():\n            if isinstance(value, NodeList):\n                copied_node[key] = copy.copy(value)\n            elif isinstance(value, dict):\n                copied_node[key] = self._recursive_copy(value)\n            else:\n                copied_node[key] = value\n        return copied_node\n", "botocore/validate.py": "\"\"\"User input parameter validation.\n\nThis module handles user input parameter validation\nagainst a provided input model.\n\nNote that the objects in this module do *not* mutate any\narguments.  No type version happens here.  It is up to another\nlayer to properly convert arguments to any required types.\n\nValidation Errors\n-----------------\n\n\n\"\"\"\n\nimport decimal\nimport json\nfrom datetime import datetime\n\nfrom botocore.exceptions import ParamValidationError\nfrom botocore.utils import is_json_value_header, parse_to_aware_datetime\n\n\ndef validate_parameters(params, shape):\n    \"\"\"Validates input parameters against a schema.\n\n    This is a convenience function that validates parameters against a schema.\n    You can also instantiate and use the ParamValidator class directly if you\n    want more control.\n\n    If there are any validation errors then a ParamValidationError\n    will be raised.  If there are no validation errors than no exception\n    is raised and a value of None is returned.\n\n    :param params: The user provided input parameters.\n\n    :type shape: botocore.model.Shape\n    :param shape: The schema which the input parameters should\n        adhere to.\n\n    :raise: ParamValidationError\n\n    \"\"\"\n    validator = ParamValidator()\n    report = validator.validate(params, shape)\n    if report.has_errors():\n        raise ParamValidationError(report=report.generate_report())\n\n\ndef type_check(valid_types):\n    def _create_type_check_guard(func):\n        def _on_passes_type_check(self, param, shape, errors, name):\n            if _type_check(param, errors, name):\n                return func(self, param, shape, errors, name)\n\n        def _type_check(param, errors, name):\n            if not isinstance(param, valid_types):\n                valid_type_names = [str(t) for t in valid_types]\n                errors.report(\n                    name,\n                    'invalid type',\n                    param=param,\n                    valid_types=valid_type_names,\n                )\n                return False\n            return True\n\n        return _on_passes_type_check\n\n    return _create_type_check_guard\n\n\ndef range_check(name, value, shape, error_type, errors):\n    failed = False\n    min_allowed = float('-inf')\n    if 'min' in shape.metadata:\n        min_allowed = shape.metadata['min']\n        if value < min_allowed:\n            failed = True\n    elif hasattr(shape, 'serialization'):\n        # Members that can be bound to the host have an implicit min of 1\n        if shape.serialization.get('hostLabel'):\n            min_allowed = 1\n            if value < min_allowed:\n                failed = True\n    if failed:\n        errors.report(name, error_type, param=value, min_allowed=min_allowed)\n\n\nclass ValidationErrors:\n    def __init__(self):\n        self._errors = []\n\n    def has_errors(self):\n        if self._errors:\n            return True\n        return False\n\n    def generate_report(self):\n        error_messages = []\n        for error in self._errors:\n            error_messages.append(self._format_error(error))\n        return '\\n'.join(error_messages)\n\n    def _format_error(self, error):\n        error_type, name, additional = error\n        name = self._get_name(name)\n        if error_type == 'missing required field':\n            return (\n                f\"Missing required parameter in {name}: \"\n                f\"\\\"{additional['required_name']}\\\"\"\n            )\n        elif error_type == 'unknown field':\n            unknown_param = additional['unknown_param']\n            valid_names = ', '.join(additional['valid_names'])\n            return (\n                f'Unknown parameter in {name}: \"{unknown_param}\", '\n                f'must be one of: {valid_names}'\n            )\n        elif error_type == 'invalid type':\n            param = additional['param']\n            param_type = type(param)\n            valid_types = ', '.join(additional['valid_types'])\n            return (\n                f'Invalid type for parameter {name}, value: {param}, '\n                f'type: {param_type}, valid types: {valid_types}'\n            )\n        elif error_type == 'invalid range':\n            param = additional['param']\n            min_allowed = additional['min_allowed']\n            return (\n                f'Invalid value for parameter {name}, value: {param}, '\n                f'valid min value: {min_allowed}'\n            )\n        elif error_type == 'invalid length':\n            param = additional['param']\n            min_allowed = additional['min_allowed']\n            return (\n                f'Invalid length for parameter {name}, value: {param}, '\n                f'valid min length: {min_allowed}'\n            )\n        elif error_type == 'unable to encode to json':\n            return 'Invalid parameter {} must be json serializable: {}'.format(\n                name,\n                additional['type_error'],\n            )\n        elif error_type == 'invalid type for document':\n            param = additional['param']\n            param_type = type(param)\n            valid_types = ', '.join(additional['valid_types'])\n            return (\n                f'Invalid type for document parameter {name}, value: {param}, '\n                f'type: {param_type}, valid types: {valid_types}'\n            )\n        elif error_type == 'more than one input':\n            members = ', '.join(additional['members'])\n            return (\n                f'Invalid number of parameters set for tagged union structure '\n                f'{name}. Can only set one of the following keys: '\n                f'{members}.'\n            )\n        elif error_type == 'empty input':\n            members = ', '.join(additional['members'])\n            return (\n                f'Must set one of the following keys for tagged union'\n                f'structure {name}: {members}.'\n            )\n\n    def _get_name(self, name):\n        if not name:\n            return 'input'\n        elif name.startswith('.'):\n            return name[1:]\n        else:\n            return name\n\n    def report(self, name, reason, **kwargs):\n        self._errors.append((reason, name, kwargs))\n\n\nclass ParamValidator:\n    \"\"\"Validates parameters against a shape model.\"\"\"\n\n    def validate(self, params, shape):\n        \"\"\"Validate parameters against a shape model.\n\n        This method will validate the parameters against a provided shape model.\n        All errors will be collected before returning to the caller.  This means\n        that this method will not stop at the first error, it will return all\n        possible errors.\n\n        :param params: User provided dict of parameters\n        :param shape: A shape model describing the expected input.\n\n        :return: A list of errors.\n\n        \"\"\"\n        errors = ValidationErrors()\n        self._validate(params, shape, errors, name='')\n        return errors\n\n    def _check_special_validation_cases(self, shape):\n        if is_json_value_header(shape):\n            return self._validate_jsonvalue_string\n        if shape.type_name == 'structure' and shape.is_document_type:\n            return self._validate_document\n\n    def _validate(self, params, shape, errors, name):\n        special_validator = self._check_special_validation_cases(shape)\n        if special_validator:\n            special_validator(params, shape, errors, name)\n        else:\n            getattr(self, '_validate_%s' % shape.type_name)(\n                params, shape, errors, name\n            )\n\n    def _validate_jsonvalue_string(self, params, shape, errors, name):\n        # Check to see if a value marked as a jsonvalue can be dumped to\n        # a json string.\n        try:\n            json.dumps(params)\n        except (ValueError, TypeError) as e:\n            errors.report(name, 'unable to encode to json', type_error=e)\n\n    def _validate_document(self, params, shape, errors, name):\n        if params is None:\n            return\n\n        if isinstance(params, dict):\n            for key in params:\n                self._validate_document(params[key], shape, errors, key)\n        elif isinstance(params, list):\n            for index, entity in enumerate(params):\n                self._validate_document(\n                    entity, shape, errors, '%s[%d]' % (name, index)\n                )\n        elif not isinstance(params, ((str,), int, bool, float)):\n            valid_types = (str, int, bool, float, list, dict)\n            valid_type_names = [str(t) for t in valid_types]\n            errors.report(\n                name,\n                'invalid type for document',\n                param=params,\n                param_type=type(params),\n                valid_types=valid_type_names,\n            )\n\n    @type_check(valid_types=(dict,))\n    def _validate_structure(self, params, shape, errors, name):\n        if shape.is_tagged_union:\n            if len(params) == 0:\n                errors.report(name, 'empty input', members=shape.members)\n            elif len(params) > 1:\n                errors.report(\n                    name, 'more than one input', members=shape.members\n                )\n\n        # Validate required fields.\n        for required_member in shape.metadata.get('required', []):\n            if required_member not in params:\n                errors.report(\n                    name,\n                    'missing required field',\n                    required_name=required_member,\n                    user_params=params,\n                )\n        members = shape.members\n        known_params = []\n        # Validate known params.\n        for param in params:\n            if param not in members:\n                errors.report(\n                    name,\n                    'unknown field',\n                    unknown_param=param,\n                    valid_names=list(members),\n                )\n            else:\n                known_params.append(param)\n        # Validate structure members.\n        for param in known_params:\n            self._validate(\n                params[param],\n                shape.members[param],\n                errors,\n                f'{name}.{param}',\n            )\n\n    @type_check(valid_types=(str,))\n    def _validate_string(self, param, shape, errors, name):\n        # Validate range.  For a string, the min/max constraints\n        # are of the string length.\n        # Looks like:\n        # \"WorkflowId\":{\n        #   \"type\":\"string\",\n        #   \"min\":1,\n        #   \"max\":256\n        #  }\n        range_check(name, len(param), shape, 'invalid length', errors)\n\n    @type_check(valid_types=(list, tuple))\n    def _validate_list(self, param, shape, errors, name):\n        member_shape = shape.member\n        range_check(name, len(param), shape, 'invalid length', errors)\n        for i, item in enumerate(param):\n            self._validate(item, member_shape, errors, f'{name}[{i}]')\n\n    @type_check(valid_types=(dict,))\n    def _validate_map(self, param, shape, errors, name):\n        key_shape = shape.key\n        value_shape = shape.value\n        for key, value in param.items():\n            self._validate(key, key_shape, errors, f\"{name} (key: {key})\")\n            self._validate(value, value_shape, errors, f'{name}.{key}')\n\n    @type_check(valid_types=(int,))\n    def _validate_integer(self, param, shape, errors, name):\n        range_check(name, param, shape, 'invalid range', errors)\n\n    def _validate_blob(self, param, shape, errors, name):\n        if isinstance(param, (bytes, bytearray, str)):\n            return\n        elif hasattr(param, 'read'):\n            # File like objects are also allowed for blob types.\n            return\n        else:\n            errors.report(\n                name,\n                'invalid type',\n                param=param,\n                valid_types=[str(bytes), str(bytearray), 'file-like object'],\n            )\n\n    @type_check(valid_types=(bool,))\n    def _validate_boolean(self, param, shape, errors, name):\n        pass\n\n    @type_check(valid_types=(float, decimal.Decimal) + (int,))\n    def _validate_double(self, param, shape, errors, name):\n        range_check(name, param, shape, 'invalid range', errors)\n\n    _validate_float = _validate_double\n\n    @type_check(valid_types=(int,))\n    def _validate_long(self, param, shape, errors, name):\n        range_check(name, param, shape, 'invalid range', errors)\n\n    def _validate_timestamp(self, param, shape, errors, name):\n        # We don't use @type_check because datetimes are a bit\n        # more flexible.  You can either provide a datetime\n        # object, or a string that parses to a datetime.\n        is_valid_type = self._type_check_datetime(param)\n        if not is_valid_type:\n            valid_type_names = [str(datetime), 'timestamp-string']\n            errors.report(\n                name, 'invalid type', param=param, valid_types=valid_type_names\n            )\n\n    def _type_check_datetime(self, value):\n        try:\n            parse_to_aware_datetime(value)\n            return True\n        except (TypeError, ValueError, AttributeError):\n            # Yes, dateutil can sometimes raise an AttributeError\n            # when parsing timestamps.\n            return False\n\n\nclass ParamValidationDecorator:\n    def __init__(self, param_validator, serializer):\n        self._param_validator = param_validator\n        self._serializer = serializer\n\n    def serialize_to_request(self, parameters, operation_model):\n        input_shape = operation_model.input_shape\n        if input_shape is not None:\n            report = self._param_validator.validate(\n                parameters, operation_model.input_shape\n            )\n            if report.has_errors():\n                raise ParamValidationError(report=report.generate_report())\n        return self._serializer.serialize_to_request(\n            parameters, operation_model\n        )\n", "botocore/loaders.py": "# Copyright 2012-2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"Module for loading various model files.\n\nThis module provides the classes that are used to load models used\nby botocore.  This can include:\n\n    * Service models (e.g. the model for EC2, S3, DynamoDB, etc.)\n    * Service model extras which customize the service models\n    * Other models associated with a service (pagination, waiters)\n    * Non service-specific config (Endpoint data, retry config)\n\nLoading a module is broken down into several steps:\n\n    * Determining the path to load\n    * Search the data_path for files to load\n    * The mechanics of loading the file\n    * Searching for extras and applying them to the loaded file\n\nThe last item is used so that other faster loading mechanism\nbesides the default JSON loader can be used.\n\nThe Search Path\n===============\n\nSimilar to how the PATH environment variable is to finding executables\nand the PYTHONPATH environment variable is to finding python modules\nto import, the botocore loaders have the concept of a data path exposed\nthrough AWS_DATA_PATH.\n\nThis enables end users to provide additional search paths where we\nwill attempt to load models outside of the models we ship with\nbotocore.  When you create a ``Loader``, there are two paths\nautomatically added to the model search path:\n\n    * <botocore root>/data/\n    * ~/.aws/models\n\nThe first value is the path where all the model files shipped with\nbotocore are located.\n\nThe second path is so that users can just drop new model files in\n``~/.aws/models`` without having to mess around with the AWS_DATA_PATH.\n\nThe AWS_DATA_PATH using the platform specific path separator to\nseparate entries (typically ``:`` on linux and ``;`` on windows).\n\n\nDirectory Layout\n================\n\nThe Loader expects a particular directory layout.  In order for any\ndirectory specified in AWS_DATA_PATH to be considered, it must have\nthis structure for service models::\n\n    <root>\n      |\n      |-- servicename1\n      |   |-- 2012-10-25\n      |       |-- service-2.json\n      |-- ec2\n      |   |-- 2014-01-01\n      |   |   |-- paginators-1.json\n      |   |   |-- service-2.json\n      |   |   |-- waiters-2.json\n      |   |-- 2015-03-01\n      |       |-- paginators-1.json\n      |       |-- service-2.json\n      |       |-- waiters-2.json\n      |       |-- service-2.sdk-extras.json\n\n\nThat is:\n\n    * The root directory contains sub directories that are the name\n      of the services.\n    * Within each service directory, there's a sub directory for each\n      available API version.\n    * Within each API version, there are model specific files, including\n      (but not limited to): service-2.json, waiters-2.json, paginators-1.json\n\nThe ``-1`` and ``-2`` suffix at the end of the model files denote which version\nschema is used within the model.  Even though this information is available in\nthe ``version`` key within the model, this version is also part of the filename\nso that code does not need to load the JSON model in order to determine which\nversion to use.\n\nThe ``sdk-extras`` and similar files represent extra data that needs to be\napplied to the model after it is loaded. Data in these files might represent\ninformation that doesn't quite fit in the original models, but is still needed\nfor the sdk. For instance, additional operation parameters might be added here\nwhich don't represent the actual service api.\n\"\"\"\nimport logging\nimport os\n\nfrom botocore import BOTOCORE_ROOT\nfrom botocore.compat import HAS_GZIP, OrderedDict, json\nfrom botocore.exceptions import DataNotFoundError, UnknownServiceError\nfrom botocore.utils import deep_merge\n\n_JSON_OPEN_METHODS = {\n    '.json': open,\n}\n\n\nif HAS_GZIP:\n    from gzip import open as gzip_open\n\n    _JSON_OPEN_METHODS['.json.gz'] = gzip_open\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef instance_cache(func):\n    \"\"\"Cache the result of a method on a per instance basis.\n\n    This is not a general purpose caching decorator.  In order\n    for this to be used, it must be used on methods on an\n    instance, and that instance *must* provide a\n    ``self._cache`` dictionary.\n\n    \"\"\"\n\n    def _wrapper(self, *args, **kwargs):\n        key = (func.__name__,) + args\n        for pair in sorted(kwargs.items()):\n            key += pair\n        if key in self._cache:\n            return self._cache[key]\n        data = func(self, *args, **kwargs)\n        self._cache[key] = data\n        return data\n\n    return _wrapper\n\n\nclass JSONFileLoader:\n    \"\"\"Loader JSON files.\n\n    This class can load the default format of models, which is a JSON file.\n\n    \"\"\"\n\n    def exists(self, file_path):\n        \"\"\"Checks if the file exists.\n\n        :type file_path: str\n        :param file_path: The full path to the file to load without\n            the '.json' extension.\n\n        :return: True if file path exists, False otherwise.\n\n        \"\"\"\n        for ext in _JSON_OPEN_METHODS:\n            if os.path.isfile(file_path + ext):\n                return True\n        return False\n\n    def _load_file(self, full_path, open_method):\n        if not os.path.isfile(full_path):\n            return\n\n        # By default the file will be opened with locale encoding on Python 3.\n        # We specify \"utf8\" here to ensure the correct behavior.\n        with open_method(full_path, 'rb') as fp:\n            payload = fp.read().decode('utf-8')\n\n        logger.debug(\"Loading JSON file: %s\", full_path)\n        return json.loads(payload, object_pairs_hook=OrderedDict)\n\n    def load_file(self, file_path):\n        \"\"\"Attempt to load the file path.\n\n        :type file_path: str\n        :param file_path: The full path to the file to load without\n            the '.json' extension.\n\n        :return: The loaded data if it exists, otherwise None.\n\n        \"\"\"\n        for ext, open_method in _JSON_OPEN_METHODS.items():\n            data = self._load_file(file_path + ext, open_method)\n            if data is not None:\n                return data\n        return None\n\n\ndef create_loader(search_path_string=None):\n    \"\"\"Create a Loader class.\n\n    This factory function creates a loader given a search string path.\n\n    :type search_string_path: str\n    :param search_string_path: The AWS_DATA_PATH value.  A string\n        of data path values separated by the ``os.path.pathsep`` value,\n        which is typically ``:`` on POSIX platforms and ``;`` on\n        windows.\n\n    :return: A ``Loader`` instance.\n\n    \"\"\"\n    if search_path_string is None:\n        return Loader()\n    paths = []\n    extra_paths = search_path_string.split(os.pathsep)\n    for path in extra_paths:\n        path = os.path.expanduser(os.path.expandvars(path))\n        paths.append(path)\n    return Loader(extra_search_paths=paths)\n\n\nclass Loader:\n    \"\"\"Find and load data models.\n\n    This class will handle searching for and loading data models.\n\n    The main method used here is ``load_service_model``, which is a\n    convenience method over ``load_data`` and ``determine_latest_version``.\n\n    \"\"\"\n\n    FILE_LOADER_CLASS = JSONFileLoader\n    # The included models in botocore/data/ that we ship with botocore.\n    BUILTIN_DATA_PATH = os.path.join(BOTOCORE_ROOT, 'data')\n    # For convenience we automatically add ~/.aws/models to the data path.\n    CUSTOMER_DATA_PATH = os.path.join(\n        os.path.expanduser('~'), '.aws', 'models'\n    )\n    BUILTIN_EXTRAS_TYPES = ['sdk']\n\n    def __init__(\n        self,\n        extra_search_paths=None,\n        file_loader=None,\n        cache=None,\n        include_default_search_paths=True,\n        include_default_extras=True,\n    ):\n        self._cache = {}\n        if file_loader is None:\n            file_loader = self.FILE_LOADER_CLASS()\n        self.file_loader = file_loader\n        if extra_search_paths is not None:\n            self._search_paths = extra_search_paths\n        else:\n            self._search_paths = []\n        if include_default_search_paths:\n            self._search_paths.extend(\n                [self.CUSTOMER_DATA_PATH, self.BUILTIN_DATA_PATH]\n            )\n\n        self._extras_types = []\n        if include_default_extras:\n            self._extras_types.extend(self.BUILTIN_EXTRAS_TYPES)\n\n        self._extras_processor = ExtrasProcessor()\n\n    @property\n    def search_paths(self):\n        return self._search_paths\n\n    @property\n    def extras_types(self):\n        return self._extras_types\n\n    @instance_cache\n    def list_available_services(self, type_name):\n        \"\"\"List all known services.\n\n        This will traverse the search path and look for all known\n        services.\n\n        :type type_name: str\n        :param type_name: The type of the service (service-2,\n            paginators-1, waiters-2, etc).  This is needed because\n            the list of available services depends on the service\n            type.  For example, the latest API version available for\n            a resource-1.json file may not be the latest API version\n            available for a services-2.json file.\n\n        :return: A list of all services.  The list of services will\n            be sorted.\n\n        \"\"\"\n        services = set()\n        for possible_path in self._potential_locations():\n            # Any directory in the search path is potentially a service.\n            # We'll collect any initial list of potential services,\n            # but we'll then need to further process these directories\n            # by searching for the corresponding type_name in each\n            # potential directory.\n            possible_services = [\n                d\n                for d in os.listdir(possible_path)\n                if os.path.isdir(os.path.join(possible_path, d))\n            ]\n            for service_name in possible_services:\n                full_dirname = os.path.join(possible_path, service_name)\n                api_versions = os.listdir(full_dirname)\n                for api_version in api_versions:\n                    full_load_path = os.path.join(\n                        full_dirname, api_version, type_name\n                    )\n                    if self.file_loader.exists(full_load_path):\n                        services.add(service_name)\n                        break\n        return sorted(services)\n\n    @instance_cache\n    def determine_latest_version(self, service_name, type_name):\n        \"\"\"Find the latest API version available for a service.\n\n        :type service_name: str\n        :param service_name: The name of the service.\n\n        :type type_name: str\n        :param type_name: The type of the service (service-2,\n            paginators-1, waiters-2, etc).  This is needed because\n            the latest API version available can depend on the service\n            type.  For example, the latest API version available for\n            a resource-1.json file may not be the latest API version\n            available for a services-2.json file.\n\n        :rtype: str\n        :return: The latest API version.  If the service does not exist\n            or does not have any available API data, then a\n            ``DataNotFoundError`` exception will be raised.\n\n        \"\"\"\n        return max(self.list_api_versions(service_name, type_name))\n\n    @instance_cache\n    def list_api_versions(self, service_name, type_name):\n        \"\"\"List all API versions available for a particular service type\n\n        :type service_name: str\n        :param service_name: The name of the service\n\n        :type type_name: str\n        :param type_name: The type name for the service (i.e service-2,\n            paginators-1, etc.)\n\n        :rtype: list\n        :return: A list of API version strings in sorted order.\n\n        \"\"\"\n        known_api_versions = set()\n        for possible_path in self._potential_locations(\n            service_name, must_exist=True, is_dir=True\n        ):\n            for dirname in os.listdir(possible_path):\n                full_path = os.path.join(possible_path, dirname, type_name)\n                # Only add to the known_api_versions if the directory\n                # contains a service-2, paginators-1, etc. file corresponding\n                # to the type_name passed in.\n                if self.file_loader.exists(full_path):\n                    known_api_versions.add(dirname)\n        if not known_api_versions:\n            raise DataNotFoundError(data_path=service_name)\n        return sorted(known_api_versions)\n\n    @instance_cache\n    def load_service_model(self, service_name, type_name, api_version=None):\n        \"\"\"Load a botocore service model\n\n        This is the main method for loading botocore models (e.g. a service\n        model, pagination configs, waiter configs, etc.).\n\n        :type service_name: str\n        :param service_name: The name of the service (e.g ``ec2``, ``s3``).\n\n        :type type_name: str\n        :param type_name: The model type.  Valid types include, but are not\n            limited to: ``service-2``, ``paginators-1``, ``waiters-2``.\n\n        :type api_version: str\n        :param api_version: The API version to load.  If this is not\n            provided, then the latest API version will be used.\n\n        :type load_extras: bool\n        :param load_extras: Whether or not to load the tool extras which\n            contain additional data to be added to the model.\n\n        :raises: UnknownServiceError if there is no known service with\n            the provided service_name.\n\n        :raises: DataNotFoundError if no data could be found for the\n            service_name/type_name/api_version.\n\n        :return: The loaded data, as a python type (e.g. dict, list, etc).\n        \"\"\"\n        # Wrapper around the load_data.  This will calculate the path\n        # to call load_data with.\n        known_services = self.list_available_services(type_name)\n        if service_name not in known_services:\n            raise UnknownServiceError(\n                service_name=service_name,\n                known_service_names=', '.join(sorted(known_services)),\n            )\n        if api_version is None:\n            api_version = self.determine_latest_version(\n                service_name, type_name\n            )\n        full_path = os.path.join(service_name, api_version, type_name)\n        model = self.load_data(full_path)\n\n        # Load in all the extras\n        extras_data = self._find_extras(service_name, type_name, api_version)\n        self._extras_processor.process(model, extras_data)\n\n        return model\n\n    def _find_extras(self, service_name, type_name, api_version):\n        \"\"\"Creates an iterator over all the extras data.\"\"\"\n        for extras_type in self.extras_types:\n            extras_name = f'{type_name}.{extras_type}-extras'\n            full_path = os.path.join(service_name, api_version, extras_name)\n\n            try:\n                yield self.load_data(full_path)\n            except DataNotFoundError:\n                pass\n\n    @instance_cache\n    def load_data_with_path(self, name):\n        \"\"\"Same as ``load_data`` but returns file path as second return value.\n\n        :type name: str\n        :param name: The data path, i.e ``ec2/2015-03-01/service-2``.\n\n        :return: Tuple of the loaded data and the path to the data file\n            where the data was loaded from. If no data could be found then a\n            DataNotFoundError is raised.\n        \"\"\"\n        for possible_path in self._potential_locations(name):\n            found = self.file_loader.load_file(possible_path)\n            if found is not None:\n                return found, possible_path\n\n        # We didn't find anything that matched on any path.\n        raise DataNotFoundError(data_path=name)\n\n    def load_data(self, name):\n        \"\"\"Load data given a data path.\n\n        This is a low level method that will search through the various\n        search paths until it's able to load a value.  This is typically\n        only needed to load *non* model files (such as _endpoints and\n        _retry).  If you need to load model files, you should prefer\n        ``load_service_model``.  Use ``load_data_with_path`` to get the\n        data path of the data file as second return value.\n\n        :type name: str\n        :param name: The data path, i.e ``ec2/2015-03-01/service-2``.\n\n        :return: The loaded data. If no data could be found then\n            a DataNotFoundError is raised.\n        \"\"\"\n        data, _ = self.load_data_with_path(name)\n        return data\n\n    def _potential_locations(self, name=None, must_exist=False, is_dir=False):\n        # Will give an iterator over the full path of potential locations\n        # according to the search path.\n        for path in self.search_paths:\n            if os.path.isdir(path):\n                full_path = path\n                if name is not None:\n                    full_path = os.path.join(path, name)\n                if not must_exist:\n                    yield full_path\n                else:\n                    if is_dir and os.path.isdir(full_path):\n                        yield full_path\n                    elif os.path.exists(full_path):\n                        yield full_path\n\n    def is_builtin_path(self, path):\n        \"\"\"Whether a given path is within the package's data directory.\n\n        This method can be used together with load_data_with_path(name)\n        to determine if data has been loaded from a file bundled with the\n        package, as opposed to a file in a separate location.\n\n        :type path: str\n        :param path: The file path to check.\n\n        :return: Whether the given path is within the package's data directory.\n        \"\"\"\n        path = os.path.expanduser(os.path.expandvars(path))\n        return path.startswith(self.BUILTIN_DATA_PATH)\n\n\nclass ExtrasProcessor:\n    \"\"\"Processes data from extras files into service models.\"\"\"\n\n    def process(self, original_model, extra_models):\n        \"\"\"Processes data from a list of loaded extras files into a model\n\n        :type original_model: dict\n        :param original_model: The service model to load all the extras into.\n\n        :type extra_models: iterable of dict\n        :param extra_models: A list of loaded extras models.\n        \"\"\"\n        for extras in extra_models:\n            self._process(original_model, extras)\n\n    def _process(self, model, extra_model):\n        \"\"\"Process a single extras model into a service model.\"\"\"\n        if 'merge' in extra_model:\n            deep_merge(model, extra_model['merge'])\n", "botocore/paginate.py": "# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport base64\nimport json\nimport logging\nfrom itertools import tee\n\nimport jmespath\n\nfrom botocore.exceptions import PaginationError\nfrom botocore.utils import merge_dicts, set_value_from_jmespath\n\nlog = logging.getLogger(__name__)\n\n\nclass TokenEncoder:\n    \"\"\"Encodes dictionaries into opaque strings.\n\n    This for the most part json dumps + base64 encoding, but also supports\n    having bytes in the dictionary in addition to the types that json can\n    handle by default.\n\n    This is intended for use in encoding pagination tokens, which in some\n    cases can be complex structures and / or contain bytes.\n    \"\"\"\n\n    def encode(self, token):\n        \"\"\"Encodes a dictionary to an opaque string.\n\n        :type token: dict\n        :param token: A dictionary containing pagination information,\n            particularly the service pagination token(s) but also other boto\n            metadata.\n\n        :rtype: str\n        :returns: An opaque string\n        \"\"\"\n        try:\n            # Try just using json dumps first to avoid having to traverse\n            # and encode the dict. In 99.9999% of cases this will work.\n            json_string = json.dumps(token)\n        except (TypeError, UnicodeDecodeError):\n            # If normal dumping failed, go through and base64 encode all bytes.\n            encoded_token, encoded_keys = self._encode(token, [])\n\n            # Save the list of all the encoded key paths. We can safely\n            # assume that no service will ever use this key.\n            encoded_token['boto_encoded_keys'] = encoded_keys\n\n            # Now that the bytes are all encoded, dump the json.\n            json_string = json.dumps(encoded_token)\n\n        # base64 encode the json string to produce an opaque token string.\n        return base64.b64encode(json_string.encode('utf-8')).decode('utf-8')\n\n    def _encode(self, data, path):\n        \"\"\"Encode bytes in given data, keeping track of the path traversed.\"\"\"\n        if isinstance(data, dict):\n            return self._encode_dict(data, path)\n        elif isinstance(data, list):\n            return self._encode_list(data, path)\n        elif isinstance(data, bytes):\n            return self._encode_bytes(data, path)\n        else:\n            return data, []\n\n    def _encode_list(self, data, path):\n        \"\"\"Encode any bytes in a list, noting the index of what is encoded.\"\"\"\n        new_data = []\n        encoded = []\n        for i, value in enumerate(data):\n            new_path = path + [i]\n            new_value, new_encoded = self._encode(value, new_path)\n            new_data.append(new_value)\n            encoded.extend(new_encoded)\n        return new_data, encoded\n\n    def _encode_dict(self, data, path):\n        \"\"\"Encode any bytes in a dict, noting the index of what is encoded.\"\"\"\n        new_data = {}\n        encoded = []\n        for key, value in data.items():\n            new_path = path + [key]\n            new_value, new_encoded = self._encode(value, new_path)\n            new_data[key] = new_value\n            encoded.extend(new_encoded)\n        return new_data, encoded\n\n    def _encode_bytes(self, data, path):\n        \"\"\"Base64 encode a byte string.\"\"\"\n        return base64.b64encode(data).decode('utf-8'), [path]\n\n\nclass TokenDecoder:\n    \"\"\"Decodes token strings back into dictionaries.\n\n    This performs the inverse operation to the TokenEncoder, accepting\n    opaque strings and decoding them into a useable form.\n    \"\"\"\n\n    def decode(self, token):\n        \"\"\"Decodes an opaque string to a dictionary.\n\n        :type token: str\n        :param token: A token string given by the botocore pagination\n            interface.\n\n        :rtype: dict\n        :returns: A dictionary containing pagination information,\n            particularly the service pagination token(s) but also other boto\n            metadata.\n        \"\"\"\n        json_string = base64.b64decode(token.encode('utf-8')).decode('utf-8')\n        decoded_token = json.loads(json_string)\n\n        # Remove the encoding metadata as it is read since it will no longer\n        # be needed.\n        encoded_keys = decoded_token.pop('boto_encoded_keys', None)\n        if encoded_keys is None:\n            return decoded_token\n        else:\n            return self._decode(decoded_token, encoded_keys)\n\n    def _decode(self, token, encoded_keys):\n        \"\"\"Find each encoded value and decode it.\"\"\"\n        for key in encoded_keys:\n            encoded = self._path_get(token, key)\n            decoded = base64.b64decode(encoded.encode('utf-8'))\n            self._path_set(token, key, decoded)\n        return token\n\n    def _path_get(self, data, path):\n        \"\"\"Return the nested data at the given path.\n\n        For instance:\n            data = {'foo': ['bar', 'baz']}\n            path = ['foo', 0]\n            ==> 'bar'\n        \"\"\"\n        # jmespath isn't used here because it would be difficult to actually\n        # create the jmespath query when taking all of the unknowns of key\n        # structure into account. Gross though this is, it is simple and not\n        # very error prone.\n        d = data\n        for step in path:\n            d = d[step]\n        return d\n\n    def _path_set(self, data, path, value):\n        \"\"\"Set the value of a key in the given data.\n\n        Example:\n            data = {'foo': ['bar', 'baz']}\n            path = ['foo', 1]\n            value = 'bin'\n            ==> data = {'foo': ['bar', 'bin']}\n        \"\"\"\n        container = self._path_get(data, path[:-1])\n        container[path[-1]] = value\n\n\nclass PaginatorModel:\n    def __init__(self, paginator_config):\n        self._paginator_config = paginator_config['pagination']\n\n    def get_paginator(self, operation_name):\n        try:\n            single_paginator_config = self._paginator_config[operation_name]\n        except KeyError:\n            raise ValueError(\n                \"Paginator for operation does not exist: %s\" % operation_name\n            )\n        return single_paginator_config\n\n\nclass PageIterator:\n    \"\"\"An iterable object to paginate API results.\n    Please note it is NOT a python iterator.\n    Use ``iter`` to wrap this as a generator.\n    \"\"\"\n\n    def __init__(\n        self,\n        method,\n        input_token,\n        output_token,\n        more_results,\n        result_keys,\n        non_aggregate_keys,\n        limit_key,\n        max_items,\n        starting_token,\n        page_size,\n        op_kwargs,\n    ):\n        self._method = method\n        self._input_token = input_token\n        self._output_token = output_token\n        self._more_results = more_results\n        self._result_keys = result_keys\n        self._max_items = max_items\n        self._limit_key = limit_key\n        self._starting_token = starting_token\n        self._page_size = page_size\n        self._op_kwargs = op_kwargs\n        self._resume_token = None\n        self._non_aggregate_key_exprs = non_aggregate_keys\n        self._non_aggregate_part = {}\n        self._token_encoder = TokenEncoder()\n        self._token_decoder = TokenDecoder()\n\n    @property\n    def result_keys(self):\n        return self._result_keys\n\n    @property\n    def resume_token(self):\n        \"\"\"Token to specify to resume pagination.\"\"\"\n        return self._resume_token\n\n    @resume_token.setter\n    def resume_token(self, value):\n        if not isinstance(value, dict):\n            raise ValueError(\"Bad starting token: %s\" % value)\n\n        if 'boto_truncate_amount' in value:\n            token_keys = sorted(self._input_token + ['boto_truncate_amount'])\n        else:\n            token_keys = sorted(self._input_token)\n        dict_keys = sorted(value.keys())\n\n        if token_keys == dict_keys:\n            self._resume_token = self._token_encoder.encode(value)\n        else:\n            raise ValueError(\"Bad starting token: %s\" % value)\n\n    @property\n    def non_aggregate_part(self):\n        return self._non_aggregate_part\n\n    def __iter__(self):\n        current_kwargs = self._op_kwargs\n        previous_next_token = None\n        next_token = {key: None for key in self._input_token}\n        if self._starting_token is not None:\n            # If the starting token exists, populate the next_token with the\n            # values inside it. This ensures that we have the service's\n            # pagination token on hand if we need to truncate after the\n            # first response.\n            next_token = self._parse_starting_token()[0]\n        # The number of items from result_key we've seen so far.\n        total_items = 0\n        first_request = True\n        primary_result_key = self.result_keys[0]\n        starting_truncation = 0\n        self._inject_starting_params(current_kwargs)\n        while True:\n            response = self._make_request(current_kwargs)\n            parsed = self._extract_parsed_response(response)\n            if first_request:\n                # The first request is handled differently.  We could\n                # possibly have a resume/starting token that tells us where\n                # to index into the retrieved page.\n                if self._starting_token is not None:\n                    starting_truncation = self._handle_first_request(\n                        parsed, primary_result_key, starting_truncation\n                    )\n                first_request = False\n                self._record_non_aggregate_key_values(parsed)\n            else:\n                # If this isn't the first request, we have already sliced into\n                # the first request and had to make additional requests after.\n                # We no longer need to add this to truncation.\n                starting_truncation = 0\n            current_response = primary_result_key.search(parsed)\n            if current_response is None:\n                current_response = []\n            num_current_response = len(current_response)\n            truncate_amount = 0\n            if self._max_items is not None:\n                truncate_amount = (\n                    total_items + num_current_response - self._max_items\n                )\n            if truncate_amount > 0:\n                self._truncate_response(\n                    parsed,\n                    primary_result_key,\n                    truncate_amount,\n                    starting_truncation,\n                    next_token,\n                )\n                yield response\n                break\n            else:\n                yield response\n                total_items += num_current_response\n                next_token = self._get_next_token(parsed)\n                if all(t is None for t in next_token.values()):\n                    break\n                if (\n                    self._max_items is not None\n                    and total_items == self._max_items\n                ):\n                    # We're on a page boundary so we can set the current\n                    # next token to be the resume token.\n                    self.resume_token = next_token\n                    break\n                if (\n                    previous_next_token is not None\n                    and previous_next_token == next_token\n                ):\n                    message = (\n                        f\"The same next token was received \"\n                        f\"twice: {next_token}\"\n                    )\n                    raise PaginationError(message=message)\n                self._inject_token_into_kwargs(current_kwargs, next_token)\n                previous_next_token = next_token\n\n    def search(self, expression):\n        \"\"\"Applies a JMESPath expression to a paginator\n\n        Each page of results is searched using the provided JMESPath\n        expression. If the result is not a list, it is yielded\n        directly. If the result is a list, each element in the result\n        is yielded individually (essentially implementing a flatmap in\n        which the JMESPath search is the mapping function).\n\n        :type expression: str\n        :param expression: JMESPath expression to apply to each page.\n\n        :return: Returns an iterator that yields the individual\n            elements of applying a JMESPath expression to each page of\n            results.\n        \"\"\"\n        compiled = jmespath.compile(expression)\n        for page in self:\n            results = compiled.search(page)\n            if isinstance(results, list):\n                yield from results\n            else:\n                # Yield result directly if it is not a list.\n                yield results\n\n    def _make_request(self, current_kwargs):\n        return self._method(**current_kwargs)\n\n    def _extract_parsed_response(self, response):\n        return response\n\n    def _record_non_aggregate_key_values(self, response):\n        non_aggregate_keys = {}\n        for expression in self._non_aggregate_key_exprs:\n            result = expression.search(response)\n            set_value_from_jmespath(\n                non_aggregate_keys, expression.expression, result\n            )\n        self._non_aggregate_part = non_aggregate_keys\n\n    def _inject_starting_params(self, op_kwargs):\n        # If the user has specified a starting token we need to\n        # inject that into the operation's kwargs.\n        if self._starting_token is not None:\n            # Don't need to do anything special if there is no starting\n            # token specified.\n            next_token = self._parse_starting_token()[0]\n            self._inject_token_into_kwargs(op_kwargs, next_token)\n        if self._page_size is not None:\n            # Pass the page size as the parameter name for limiting\n            # page size, also known as the limit_key.\n            op_kwargs[self._limit_key] = self._page_size\n\n    def _inject_token_into_kwargs(self, op_kwargs, next_token):\n        for name, token in next_token.items():\n            if (token is not None) and (token != 'None'):\n                op_kwargs[name] = token\n            elif name in op_kwargs:\n                del op_kwargs[name]\n\n    def _handle_first_request(\n        self, parsed, primary_result_key, starting_truncation\n    ):\n        # If the payload is an array or string, we need to slice into it\n        # and only return the truncated amount.\n        starting_truncation = self._parse_starting_token()[1]\n        all_data = primary_result_key.search(parsed)\n        if isinstance(all_data, (list, str)):\n            data = all_data[starting_truncation:]\n        else:\n            data = None\n        set_value_from_jmespath(parsed, primary_result_key.expression, data)\n        # We also need to truncate any secondary result keys\n        # because they were not truncated in the previous last\n        # response.\n        for token in self.result_keys:\n            if token == primary_result_key:\n                continue\n            sample = token.search(parsed)\n            if isinstance(sample, list):\n                empty_value = []\n            elif isinstance(sample, str):\n                empty_value = ''\n            elif isinstance(sample, (int, float)):\n                empty_value = 0\n            else:\n                empty_value = None\n            set_value_from_jmespath(parsed, token.expression, empty_value)\n        return starting_truncation\n\n    def _truncate_response(\n        self,\n        parsed,\n        primary_result_key,\n        truncate_amount,\n        starting_truncation,\n        next_token,\n    ):\n        original = primary_result_key.search(parsed)\n        if original is None:\n            original = []\n        amount_to_keep = len(original) - truncate_amount\n        truncated = original[:amount_to_keep]\n        set_value_from_jmespath(\n            parsed, primary_result_key.expression, truncated\n        )\n        # The issue here is that even though we know how much we've truncated\n        # we need to account for this globally including any starting\n        # left truncation. For example:\n        # Raw response: [0,1,2,3]\n        # Starting index: 1\n        # Max items: 1\n        # Starting left truncation: [1, 2, 3]\n        # End right truncation for max items: [1]\n        # However, even though we only kept 1, this is post\n        # left truncation so the next starting index should be 2, not 1\n        # (left_truncation + amount_to_keep).\n        next_token['boto_truncate_amount'] = (\n            amount_to_keep + starting_truncation\n        )\n        self.resume_token = next_token\n\n    def _get_next_token(self, parsed):\n        if self._more_results is not None:\n            if not self._more_results.search(parsed):\n                return {}\n        next_tokens = {}\n        for output_token, input_key in zip(\n            self._output_token, self._input_token\n        ):\n            next_token = output_token.search(parsed)\n            # We do not want to include any empty strings as actual tokens.\n            # Treat them as None.\n            if next_token:\n                next_tokens[input_key] = next_token\n            else:\n                next_tokens[input_key] = None\n        return next_tokens\n\n    def result_key_iters(self):\n        teed_results = tee(self, len(self.result_keys))\n        return [\n            ResultKeyIterator(i, result_key)\n            for i, result_key in zip(teed_results, self.result_keys)\n        ]\n\n    def build_full_result(self):\n        complete_result = {}\n        for response in self:\n            page = response\n            # We want to try to catch operation object pagination\n            # and format correctly for those. They come in the form\n            # of a tuple of two elements: (http_response, parsed_responsed).\n            # We want the parsed_response as that is what the page iterator\n            # uses. We can remove it though once operation objects are removed.\n            if isinstance(response, tuple) and len(response) == 2:\n                page = response[1]\n            # We're incrementally building the full response page\n            # by page.  For each page in the response we need to\n            # inject the necessary components from the page\n            # into the complete_result.\n            for result_expression in self.result_keys:\n                # In order to incrementally update a result key\n                # we need to search the existing value from complete_result,\n                # then we need to search the _current_ page for the\n                # current result key value.  Then we append the current\n                # value onto the existing value, and re-set that value\n                # as the new value.\n                result_value = result_expression.search(page)\n                if result_value is None:\n                    continue\n                existing_value = result_expression.search(complete_result)\n                if existing_value is None:\n                    # Set the initial result\n                    set_value_from_jmespath(\n                        complete_result,\n                        result_expression.expression,\n                        result_value,\n                    )\n                    continue\n                # Now both result_value and existing_value contain something\n                if isinstance(result_value, list):\n                    existing_value.extend(result_value)\n                elif isinstance(result_value, (int, float, str)):\n                    # Modify the existing result with the sum or concatenation\n                    set_value_from_jmespath(\n                        complete_result,\n                        result_expression.expression,\n                        existing_value + result_value,\n                    )\n        merge_dicts(complete_result, self.non_aggregate_part)\n        if self.resume_token is not None:\n            complete_result['NextToken'] = self.resume_token\n        return complete_result\n\n    def _parse_starting_token(self):\n        if self._starting_token is None:\n            return None\n\n        # The starting token is a dict passed as a base64 encoded string.\n        next_token = self._starting_token\n        try:\n            next_token = self._token_decoder.decode(next_token)\n            index = 0\n            if 'boto_truncate_amount' in next_token:\n                index = next_token.get('boto_truncate_amount')\n                del next_token['boto_truncate_amount']\n        except (ValueError, TypeError):\n            next_token, index = self._parse_starting_token_deprecated()\n        return next_token, index\n\n    def _parse_starting_token_deprecated(self):\n        \"\"\"\n        This handles parsing of old style starting tokens, and attempts to\n        coerce them into the new style.\n        \"\"\"\n        log.debug(\n            \"Attempting to fall back to old starting token parser. For \"\n            \"token: %s\" % self._starting_token\n        )\n        if self._starting_token is None:\n            return None\n\n        parts = self._starting_token.split('___')\n        next_token = []\n        index = 0\n        if len(parts) == len(self._input_token) + 1:\n            try:\n                index = int(parts.pop())\n            except ValueError:\n                # This doesn't look like a valid old-style token, so we're\n                # passing it along as an opaque service token.\n                parts = [self._starting_token]\n\n        for part in parts:\n            if part == 'None':\n                next_token.append(None)\n            else:\n                next_token.append(part)\n        return self._convert_deprecated_starting_token(next_token), index\n\n    def _convert_deprecated_starting_token(self, deprecated_token):\n        \"\"\"\n        This attempts to convert a deprecated starting token into the new\n        style.\n        \"\"\"\n        len_deprecated_token = len(deprecated_token)\n        len_input_token = len(self._input_token)\n        if len_deprecated_token > len_input_token:\n            raise ValueError(\"Bad starting token: %s\" % self._starting_token)\n        elif len_deprecated_token < len_input_token:\n            log.debug(\n                \"Old format starting token does not contain all input \"\n                \"tokens. Setting the rest, in order, as None.\"\n            )\n            for i in range(len_input_token - len_deprecated_token):\n                deprecated_token.append(None)\n        return dict(zip(self._input_token, deprecated_token))\n\n\nclass Paginator:\n    PAGE_ITERATOR_CLS = PageIterator\n\n    def __init__(self, method, pagination_config, model):\n        self._model = model\n        self._method = method\n        self._pagination_cfg = pagination_config\n        self._output_token = self._get_output_tokens(self._pagination_cfg)\n        self._input_token = self._get_input_tokens(self._pagination_cfg)\n        self._more_results = self._get_more_results_token(self._pagination_cfg)\n        self._non_aggregate_keys = self._get_non_aggregate_keys(\n            self._pagination_cfg\n        )\n        self._result_keys = self._get_result_keys(self._pagination_cfg)\n        self._limit_key = self._get_limit_key(self._pagination_cfg)\n\n    @property\n    def result_keys(self):\n        return self._result_keys\n\n    def _get_non_aggregate_keys(self, config):\n        keys = []\n        for key in config.get('non_aggregate_keys', []):\n            keys.append(jmespath.compile(key))\n        return keys\n\n    def _get_output_tokens(self, config):\n        output = []\n        output_token = config['output_token']\n        if not isinstance(output_token, list):\n            output_token = [output_token]\n        for config in output_token:\n            output.append(jmespath.compile(config))\n        return output\n\n    def _get_input_tokens(self, config):\n        input_token = self._pagination_cfg['input_token']\n        if not isinstance(input_token, list):\n            input_token = [input_token]\n        return input_token\n\n    def _get_more_results_token(self, config):\n        more_results = config.get('more_results')\n        if more_results is not None:\n            return jmespath.compile(more_results)\n\n    def _get_result_keys(self, config):\n        result_key = config.get('result_key')\n        if result_key is not None:\n            if not isinstance(result_key, list):\n                result_key = [result_key]\n            result_key = [jmespath.compile(rk) for rk in result_key]\n            return result_key\n\n    def _get_limit_key(self, config):\n        return config.get('limit_key')\n\n    def paginate(self, **kwargs):\n        \"\"\"Create paginator object for an operation.\n\n        This returns an iterable object.  Iterating over\n        this object will yield a single page of a response\n        at a time.\n\n        \"\"\"\n        page_params = self._extract_paging_params(kwargs)\n        return self.PAGE_ITERATOR_CLS(\n            self._method,\n            self._input_token,\n            self._output_token,\n            self._more_results,\n            self._result_keys,\n            self._non_aggregate_keys,\n            self._limit_key,\n            page_params['MaxItems'],\n            page_params['StartingToken'],\n            page_params['PageSize'],\n            kwargs,\n        )\n\n    def _extract_paging_params(self, kwargs):\n        pagination_config = kwargs.pop('PaginationConfig', {})\n        max_items = pagination_config.get('MaxItems', None)\n        if max_items is not None:\n            max_items = int(max_items)\n        page_size = pagination_config.get('PageSize', None)\n        if page_size is not None:\n            if self._limit_key is None:\n                raise PaginationError(\n                    message=\"PageSize parameter is not supported for the \"\n                    \"pagination interface for this operation.\"\n                )\n            input_members = self._model.input_shape.members\n            limit_key_shape = input_members.get(self._limit_key)\n            if limit_key_shape.type_name == 'string':\n                if not isinstance(page_size, str):\n                    page_size = str(page_size)\n            else:\n                page_size = int(page_size)\n        return {\n            'MaxItems': max_items,\n            'StartingToken': pagination_config.get('StartingToken', None),\n            'PageSize': page_size,\n        }\n\n\nclass ResultKeyIterator:\n    \"\"\"Iterates over the results of paginated responses.\n\n    Each iterator is associated with a single result key.\n    Iterating over this object will give you each element in\n    the result key list.\n\n    :param pages_iterator: An iterator that will give you\n        pages of results (a ``PageIterator`` class).\n    :param result_key: The JMESPath expression representing\n        the result key.\n\n    \"\"\"\n\n    def __init__(self, pages_iterator, result_key):\n        self._pages_iterator = pages_iterator\n        self.result_key = result_key\n\n    def __iter__(self):\n        for page in self._pages_iterator:\n            results = self.result_key.search(page)\n            if results is None:\n                results = []\n            yield from results\n", "botocore/crt/auth.py": "# Copyright 2022 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport datetime\nfrom io import BytesIO\n\nfrom botocore.auth import (\n    SIGNED_HEADERS_BLACKLIST,\n    STREAMING_UNSIGNED_PAYLOAD_TRAILER,\n    UNSIGNED_PAYLOAD,\n    BaseSigner,\n    _get_body_as_dict,\n    _host_from_url,\n)\nfrom botocore.compat import HTTPHeaders, awscrt, parse_qs, urlsplit, urlunsplit\nfrom botocore.exceptions import NoCredentialsError\nfrom botocore.utils import percent_encode_sequence\n\n\nclass CrtSigV4Auth(BaseSigner):\n    REQUIRES_REGION = True\n    _PRESIGNED_HEADERS_BLOCKLIST = [\n        'Authorization',\n        'X-Amz-Date',\n        'X-Amz-Content-SHA256',\n        'X-Amz-Security-Token',\n    ]\n    _SIGNATURE_TYPE = awscrt.auth.AwsSignatureType.HTTP_REQUEST_HEADERS\n    _USE_DOUBLE_URI_ENCODE = True\n    _SHOULD_NORMALIZE_URI_PATH = True\n\n    def __init__(self, credentials, service_name, region_name):\n        self.credentials = credentials\n        self._service_name = service_name\n        self._region_name = region_name\n        self._expiration_in_seconds = None\n\n    def _is_streaming_checksum_payload(self, request):\n        checksum_context = request.context.get('checksum', {})\n        algorithm = checksum_context.get('request_algorithm')\n        return isinstance(algorithm, dict) and algorithm.get('in') == 'trailer'\n\n    def add_auth(self, request):\n        if self.credentials is None:\n            raise NoCredentialsError()\n\n        # Use utcnow() because that's what gets mocked by tests, but set\n        # timezone because CRT assumes naive datetime is local time.\n        datetime_now = datetime.datetime.utcnow().replace(\n            tzinfo=datetime.timezone.utc\n        )\n\n        # Use existing 'X-Amz-Content-SHA256' header if able\n        existing_sha256 = self._get_existing_sha256(request)\n\n        self._modify_request_before_signing(request)\n\n        credentials_provider = awscrt.auth.AwsCredentialsProvider.new_static(\n            access_key_id=self.credentials.access_key,\n            secret_access_key=self.credentials.secret_key,\n            session_token=self.credentials.token,\n        )\n\n        if self._is_streaming_checksum_payload(request):\n            explicit_payload = STREAMING_UNSIGNED_PAYLOAD_TRAILER\n        elif self._should_sha256_sign_payload(request):\n            if existing_sha256:\n                explicit_payload = existing_sha256\n            else:\n                explicit_payload = None  # to be calculated during signing\n        else:\n            explicit_payload = UNSIGNED_PAYLOAD\n\n        if self._should_add_content_sha256_header(explicit_payload):\n            body_header = (\n                awscrt.auth.AwsSignedBodyHeaderType.X_AMZ_CONTENT_SHA_256\n            )\n        else:\n            body_header = awscrt.auth.AwsSignedBodyHeaderType.NONE\n\n        signing_config = awscrt.auth.AwsSigningConfig(\n            algorithm=awscrt.auth.AwsSigningAlgorithm.V4,\n            signature_type=self._SIGNATURE_TYPE,\n            credentials_provider=credentials_provider,\n            region=self._region_name,\n            service=self._service_name,\n            date=datetime_now,\n            should_sign_header=self._should_sign_header,\n            use_double_uri_encode=self._USE_DOUBLE_URI_ENCODE,\n            should_normalize_uri_path=self._SHOULD_NORMALIZE_URI_PATH,\n            signed_body_value=explicit_payload,\n            signed_body_header_type=body_header,\n            expiration_in_seconds=self._expiration_in_seconds,\n        )\n        crt_request = self._crt_request_from_aws_request(request)\n        future = awscrt.auth.aws_sign_request(crt_request, signing_config)\n        future.result()\n        self._apply_signing_changes(request, crt_request)\n\n    def _crt_request_from_aws_request(self, aws_request):\n        url_parts = urlsplit(aws_request.url)\n        crt_path = url_parts.path if url_parts.path else '/'\n        if aws_request.params:\n            array = []\n            for param, value in aws_request.params.items():\n                value = str(value)\n                array.append(f'{param}={value}')\n            crt_path = crt_path + '?' + '&'.join(array)\n        elif url_parts.query:\n            crt_path = f'{crt_path}?{url_parts.query}'\n\n        crt_headers = awscrt.http.HttpHeaders(aws_request.headers.items())\n\n        # CRT requires body (if it exists) to be an I/O stream.\n        crt_body_stream = None\n        if aws_request.body:\n            if hasattr(aws_request.body, 'seek'):\n                crt_body_stream = aws_request.body\n            else:\n                crt_body_stream = BytesIO(aws_request.body)\n\n        crt_request = awscrt.http.HttpRequest(\n            method=aws_request.method,\n            path=crt_path,\n            headers=crt_headers,\n            body_stream=crt_body_stream,\n        )\n        return crt_request\n\n    def _apply_signing_changes(self, aws_request, signed_crt_request):\n        # Apply changes from signed CRT request to the AWSRequest\n        aws_request.headers = HTTPHeaders.from_pairs(\n            list(signed_crt_request.headers)\n        )\n\n    def _should_sign_header(self, name, **kwargs):\n        return name.lower() not in SIGNED_HEADERS_BLACKLIST\n\n    def _modify_request_before_signing(self, request):\n        # This could be a retry. Make sure the previous\n        # authorization headers are removed first.\n        for h in self._PRESIGNED_HEADERS_BLOCKLIST:\n            if h in request.headers:\n                del request.headers[h]\n        # If necessary, add the host header\n        if 'host' not in request.headers:\n            request.headers['host'] = _host_from_url(request.url)\n\n    def _get_existing_sha256(self, request):\n        return request.headers.get('X-Amz-Content-SHA256')\n\n    def _should_sha256_sign_payload(self, request):\n        # Payloads will always be signed over insecure connections.\n        if not request.url.startswith('https'):\n            return True\n\n        # Certain operations may have payload signing disabled by default.\n        # Since we don't have access to the operation model, we pass in this\n        # bit of metadata through the request context.\n        return request.context.get('payload_signing_enabled', True)\n\n    def _should_add_content_sha256_header(self, explicit_payload):\n        # only add X-Amz-Content-SHA256 header if payload is explicitly set\n        return explicit_payload is not None\n\n\nclass CrtS3SigV4Auth(CrtSigV4Auth):\n    # For S3, we do not normalize the path.\n    _USE_DOUBLE_URI_ENCODE = False\n    _SHOULD_NORMALIZE_URI_PATH = False\n\n    def _get_existing_sha256(self, request):\n        # always recalculate\n        return None\n\n    def _should_sha256_sign_payload(self, request):\n        # S3 allows optional body signing, so to minimize the performance\n        # impact, we opt to not SHA256 sign the body on streaming uploads,\n        # provided that we're on https.\n        client_config = request.context.get('client_config')\n        s3_config = getattr(client_config, 's3', None)\n\n        # The config could be None if it isn't set, or if the customer sets it\n        # to None.\n        if s3_config is None:\n            s3_config = {}\n\n        # The explicit configuration takes precedence over any implicit\n        # configuration.\n        sign_payload = s3_config.get('payload_signing_enabled', None)\n        if sign_payload is not None:\n            return sign_payload\n\n        # We require that both a checksum be present and https be enabled\n        # to implicitly disable body signing. The combination of TLS and\n        # a checksum is sufficiently secure and durable for us to be\n        # confident in the request without body signing.\n        checksum_header = 'Content-MD5'\n        checksum_context = request.context.get('checksum', {})\n        algorithm = checksum_context.get('request_algorithm')\n        if isinstance(algorithm, dict) and algorithm.get('in') == 'header':\n            checksum_header = algorithm['name']\n        if (\n            not request.url.startswith('https')\n            or checksum_header not in request.headers\n        ):\n            return True\n\n        # If the input is streaming we disable body signing by default.\n        if request.context.get('has_streaming_input', False):\n            return False\n\n        # If the S3-specific checks had no results, delegate to the generic\n        # checks.\n        return super()._should_sha256_sign_payload(request)\n\n    def _should_add_content_sha256_header(self, explicit_payload):\n        # Always add X-Amz-Content-SHA256 header\n        return True\n\n\nclass CrtSigV4AsymAuth(BaseSigner):\n    REQUIRES_REGION = True\n    _PRESIGNED_HEADERS_BLOCKLIST = [\n        'Authorization',\n        'X-Amz-Date',\n        'X-Amz-Content-SHA256',\n        'X-Amz-Security-Token',\n    ]\n    _SIGNATURE_TYPE = awscrt.auth.AwsSignatureType.HTTP_REQUEST_HEADERS\n    _USE_DOUBLE_URI_ENCODE = True\n    _SHOULD_NORMALIZE_URI_PATH = True\n\n    def __init__(self, credentials, service_name, region_name):\n        self.credentials = credentials\n        self._service_name = service_name\n        self._region_name = region_name\n        self._expiration_in_seconds = None\n\n    def add_auth(self, request):\n        if self.credentials is None:\n            raise NoCredentialsError()\n\n        # Use utcnow() because that's what gets mocked by tests, but set\n        # timezone because CRT assumes naive datetime is local time.\n        datetime_now = datetime.datetime.utcnow().replace(\n            tzinfo=datetime.timezone.utc\n        )\n\n        # Use existing 'X-Amz-Content-SHA256' header if able\n        existing_sha256 = self._get_existing_sha256(request)\n\n        self._modify_request_before_signing(request)\n\n        credentials_provider = awscrt.auth.AwsCredentialsProvider.new_static(\n            access_key_id=self.credentials.access_key,\n            secret_access_key=self.credentials.secret_key,\n            session_token=self.credentials.token,\n        )\n\n        if self._is_streaming_checksum_payload(request):\n            explicit_payload = STREAMING_UNSIGNED_PAYLOAD_TRAILER\n        elif self._should_sha256_sign_payload(request):\n            if existing_sha256:\n                explicit_payload = existing_sha256\n            else:\n                explicit_payload = None  # to be calculated during signing\n        else:\n            explicit_payload = UNSIGNED_PAYLOAD\n\n        if self._should_add_content_sha256_header(explicit_payload):\n            body_header = (\n                awscrt.auth.AwsSignedBodyHeaderType.X_AMZ_CONTENT_SHA_256\n            )\n        else:\n            body_header = awscrt.auth.AwsSignedBodyHeaderType.NONE\n\n        signing_config = awscrt.auth.AwsSigningConfig(\n            algorithm=awscrt.auth.AwsSigningAlgorithm.V4_ASYMMETRIC,\n            signature_type=self._SIGNATURE_TYPE,\n            credentials_provider=credentials_provider,\n            region=self._region_name,\n            service=self._service_name,\n            date=datetime_now,\n            should_sign_header=self._should_sign_header,\n            use_double_uri_encode=self._USE_DOUBLE_URI_ENCODE,\n            should_normalize_uri_path=self._SHOULD_NORMALIZE_URI_PATH,\n            signed_body_value=explicit_payload,\n            signed_body_header_type=body_header,\n            expiration_in_seconds=self._expiration_in_seconds,\n        )\n        crt_request = self._crt_request_from_aws_request(request)\n        future = awscrt.auth.aws_sign_request(crt_request, signing_config)\n        future.result()\n        self._apply_signing_changes(request, crt_request)\n\n    def _crt_request_from_aws_request(self, aws_request):\n        url_parts = urlsplit(aws_request.url)\n        crt_path = url_parts.path if url_parts.path else '/'\n        if aws_request.params:\n            array = []\n            for param, value in aws_request.params.items():\n                value = str(value)\n                array.append(f'{param}={value}')\n            crt_path = crt_path + '?' + '&'.join(array)\n        elif url_parts.query:\n            crt_path = f'{crt_path}?{url_parts.query}'\n\n        crt_headers = awscrt.http.HttpHeaders(aws_request.headers.items())\n\n        # CRT requires body (if it exists) to be an I/O stream.\n        crt_body_stream = None\n        if aws_request.body:\n            if hasattr(aws_request.body, 'seek'):\n                crt_body_stream = aws_request.body\n            else:\n                crt_body_stream = BytesIO(aws_request.body)\n\n        crt_request = awscrt.http.HttpRequest(\n            method=aws_request.method,\n            path=crt_path,\n            headers=crt_headers,\n            body_stream=crt_body_stream,\n        )\n        return crt_request\n\n    def _apply_signing_changes(self, aws_request, signed_crt_request):\n        # Apply changes from signed CRT request to the AWSRequest\n        aws_request.headers = HTTPHeaders.from_pairs(\n            list(signed_crt_request.headers)\n        )\n\n    def _should_sign_header(self, name, **kwargs):\n        return name.lower() not in SIGNED_HEADERS_BLACKLIST\n\n    def _modify_request_before_signing(self, request):\n        # This could be a retry. Make sure the previous\n        # authorization headers are removed first.\n        for h in self._PRESIGNED_HEADERS_BLOCKLIST:\n            if h in request.headers:\n                del request.headers[h]\n        # If necessary, add the host header\n        if 'host' not in request.headers:\n            request.headers['host'] = _host_from_url(request.url)\n\n    def _get_existing_sha256(self, request):\n        return request.headers.get('X-Amz-Content-SHA256')\n\n    def _is_streaming_checksum_payload(self, request):\n        checksum_context = request.context.get('checksum', {})\n        algorithm = checksum_context.get('request_algorithm')\n        return isinstance(algorithm, dict) and algorithm.get('in') == 'trailer'\n\n    def _should_sha256_sign_payload(self, request):\n        # Payloads will always be signed over insecure connections.\n        if not request.url.startswith('https'):\n            return True\n\n        # Certain operations may have payload signing disabled by default.\n        # Since we don't have access to the operation model, we pass in this\n        # bit of metadata through the request context.\n        return request.context.get('payload_signing_enabled', True)\n\n    def _should_add_content_sha256_header(self, explicit_payload):\n        # only add X-Amz-Content-SHA256 header if payload is explicitly set\n        return explicit_payload is not None\n\n\nclass CrtS3SigV4AsymAuth(CrtSigV4AsymAuth):\n    # For S3, we do not normalize the path.\n    _USE_DOUBLE_URI_ENCODE = False\n    _SHOULD_NORMALIZE_URI_PATH = False\n\n    def _get_existing_sha256(self, request):\n        # always recalculate\n        return None\n\n    def _should_sha256_sign_payload(self, request):\n        # S3 allows optional body signing, so to minimize the performance\n        # impact, we opt to not SHA256 sign the body on streaming uploads,\n        # provided that we're on https.\n        client_config = request.context.get('client_config')\n        s3_config = getattr(client_config, 's3', None)\n\n        # The config could be None if it isn't set, or if the customer sets it\n        # to None.\n        if s3_config is None:\n            s3_config = {}\n\n        # The explicit configuration takes precedence over any implicit\n        # configuration.\n        sign_payload = s3_config.get('payload_signing_enabled', None)\n        if sign_payload is not None:\n            return sign_payload\n\n        # We require that both content-md5 be present and https be enabled\n        # to implicitly disable body signing. The combination of TLS and\n        # content-md5 is sufficiently secure and durable for us to be\n        # confident in the request without body signing.\n        if (\n            not request.url.startswith('https')\n            or 'Content-MD5' not in request.headers\n        ):\n            return True\n\n        # If the input is streaming we disable body signing by default.\n        if request.context.get('has_streaming_input', False):\n            return False\n\n        # If the S3-specific checks had no results, delegate to the generic\n        # checks.\n        return super()._should_sha256_sign_payload(request)\n\n    def _should_add_content_sha256_header(self, explicit_payload):\n        # Always add X-Amz-Content-SHA256 header\n        return True\n\n\nclass CrtSigV4AsymQueryAuth(CrtSigV4AsymAuth):\n    DEFAULT_EXPIRES = 3600\n    _SIGNATURE_TYPE = awscrt.auth.AwsSignatureType.HTTP_REQUEST_QUERY_PARAMS\n\n    def __init__(\n        self, credentials, service_name, region_name, expires=DEFAULT_EXPIRES\n    ):\n        super().__init__(credentials, service_name, region_name)\n        self._expiration_in_seconds = expires\n\n    def _modify_request_before_signing(self, request):\n        super()._modify_request_before_signing(request)\n\n        # We automatically set this header, so if it's the auto-set value we\n        # want to get rid of it since it doesn't make sense for presigned urls.\n        content_type = request.headers.get('content-type')\n        if content_type == 'application/x-www-form-urlencoded; charset=utf-8':\n            del request.headers['content-type']\n\n        # Now parse the original query string to a dict, inject our new query\n        # params, and serialize back to a query string.\n        url_parts = urlsplit(request.url)\n        # parse_qs makes each value a list, but in our case we know we won't\n        # have repeated keys so we know we have single element lists which we\n        # can convert back to scalar values.\n        query_string_parts = parse_qs(url_parts.query, keep_blank_values=True)\n        query_dict = {k: v[0] for k, v in query_string_parts.items()}\n\n        # The spec is particular about this.  It *has* to be:\n        # https://<endpoint>?<operation params>&<auth params>\n        # You can't mix the two types of params together, i.e just keep doing\n        # new_query_params.update(op_params)\n        # new_query_params.update(auth_params)\n        # percent_encode_sequence(new_query_params)\n        if request.data:\n            # We also need to move the body params into the query string. To\n            # do this, we first have to convert it to a dict.\n            query_dict.update(_get_body_as_dict(request))\n            request.data = ''\n        new_query_string = percent_encode_sequence(query_dict)\n        # url_parts is a tuple (and therefore immutable) so we need to create\n        # a new url_parts with the new query string.\n        # <part>   - <index>\n        # scheme   - 0\n        # netloc   - 1\n        # path     - 2\n        # query    - 3  <-- we're replacing this.\n        # fragment - 4\n        p = url_parts\n        new_url_parts = (p[0], p[1], p[2], new_query_string, p[4])\n        request.url = urlunsplit(new_url_parts)\n\n    def _apply_signing_changes(self, aws_request, signed_crt_request):\n        # Apply changes from signed CRT request to the AWSRequest\n        super()._apply_signing_changes(aws_request, signed_crt_request)\n\n        signed_query = urlsplit(signed_crt_request.path).query\n        p = urlsplit(aws_request.url)\n        # urlsplit() returns a tuple (and therefore immutable) so we\n        # need to create new url with the new query string.\n        # <part>   - <index>\n        # scheme   - 0\n        # netloc   - 1\n        # path     - 2\n        # query    - 3  <-- we're replacing this.\n        # fragment - 4\n        aws_request.url = urlunsplit((p[0], p[1], p[2], signed_query, p[4]))\n\n\nclass CrtS3SigV4AsymQueryAuth(CrtSigV4AsymQueryAuth):\n    \"\"\"S3 SigV4A auth using query parameters.\n    This signer will sign a request using query parameters and signature\n    version 4A, i.e a \"presigned url\" signer.\n    \"\"\"\n\n    # For S3, we do not normalize the path.\n    _USE_DOUBLE_URI_ENCODE = False\n    _SHOULD_NORMALIZE_URI_PATH = False\n\n    def _should_sha256_sign_payload(self, request):\n        # From the doc link above:\n        # \"You don't include a payload hash in the Canonical Request, because\n        # when you create a presigned URL, you don't know anything about the\n        # payload. Instead, you use a constant string \"UNSIGNED-PAYLOAD\".\n        return False\n\n    def _should_add_content_sha256_header(self, explicit_payload):\n        # Never add X-Amz-Content-SHA256 header\n        return False\n\n\nclass CrtSigV4QueryAuth(CrtSigV4Auth):\n    DEFAULT_EXPIRES = 3600\n    _SIGNATURE_TYPE = awscrt.auth.AwsSignatureType.HTTP_REQUEST_QUERY_PARAMS\n\n    def __init__(\n        self, credentials, service_name, region_name, expires=DEFAULT_EXPIRES\n    ):\n        super().__init__(credentials, service_name, region_name)\n        self._expiration_in_seconds = expires\n\n    def _modify_request_before_signing(self, request):\n        super()._modify_request_before_signing(request)\n\n        # We automatically set this header, so if it's the auto-set value we\n        # want to get rid of it since it doesn't make sense for presigned urls.\n        content_type = request.headers.get('content-type')\n        if content_type == 'application/x-www-form-urlencoded; charset=utf-8':\n            del request.headers['content-type']\n\n        # Now parse the original query string to a dict, inject our new query\n        # params, and serialize back to a query string.\n        url_parts = urlsplit(request.url)\n        # parse_qs makes each value a list, but in our case we know we won't\n        # have repeated keys so we know we have single element lists which we\n        # can convert back to scalar values.\n        query_dict = {\n            k: v[0]\n            for k, v in parse_qs(\n                url_parts.query, keep_blank_values=True\n            ).items()\n        }\n        if request.params:\n            query_dict.update(request.params)\n            request.params = {}\n        # The spec is particular about this.  It *has* to be:\n        # https://<endpoint>?<operation params>&<auth params>\n        # You can't mix the two types of params together, i.e just keep doing\n        # new_query_params.update(op_params)\n        # new_query_params.update(auth_params)\n        # percent_encode_sequence(new_query_params)\n        if request.data:\n            # We also need to move the body params into the query string. To\n            # do this, we first have to convert it to a dict.\n            query_dict.update(_get_body_as_dict(request))\n            request.data = ''\n        new_query_string = percent_encode_sequence(query_dict)\n        # url_parts is a tuple (and therefore immutable) so we need to create\n        # a new url_parts with the new query string.\n        # <part>   - <index>\n        # scheme   - 0\n        # netloc   - 1\n        # path     - 2\n        # query    - 3  <-- we're replacing this.\n        # fragment - 4\n        p = url_parts\n        new_url_parts = (p[0], p[1], p[2], new_query_string, p[4])\n        request.url = urlunsplit(new_url_parts)\n\n    def _apply_signing_changes(self, aws_request, signed_crt_request):\n        # Apply changes from signed CRT request to the AWSRequest\n        super()._apply_signing_changes(aws_request, signed_crt_request)\n\n        signed_query = urlsplit(signed_crt_request.path).query\n        p = urlsplit(aws_request.url)\n        # urlsplit() returns a tuple (and therefore immutable) so we\n        # need to create new url with the new query string.\n        # <part>   - <index>\n        # scheme   - 0\n        # netloc   - 1\n        # path     - 2\n        # query    - 3  <-- we're replacing this.\n        # fragment - 4\n        aws_request.url = urlunsplit((p[0], p[1], p[2], signed_query, p[4]))\n\n\nclass CrtS3SigV4QueryAuth(CrtSigV4QueryAuth):\n    \"\"\"S3 SigV4 auth using query parameters.\n    This signer will sign a request using query parameters and signature\n    version 4, i.e a \"presigned url\" signer.\n    Based off of:\n    http://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-query-string-auth.html\n    \"\"\"\n\n    # For S3, we do not normalize the path.\n    _USE_DOUBLE_URI_ENCODE = False\n    _SHOULD_NORMALIZE_URI_PATH = False\n\n    def _should_sha256_sign_payload(self, request):\n        # From the doc link above:\n        # \"You don't include a payload hash in the Canonical Request, because\n        # when you create a presigned URL, you don't know anything about the\n        # payload. Instead, you use a constant string \"UNSIGNED-PAYLOAD\".\n        return False\n\n    def _should_add_content_sha256_header(self, explicit_payload):\n        # Never add X-Amz-Content-SHA256 header\n        return False\n\n\n# Defined at the bottom of module to ensure all Auth\n# classes are defined.\nCRT_AUTH_TYPE_MAPS = {\n    'v4': CrtSigV4Auth,\n    'v4-query': CrtSigV4QueryAuth,\n    'v4a': CrtSigV4AsymAuth,\n    's3v4': CrtS3SigV4Auth,\n    's3v4-query': CrtS3SigV4QueryAuth,\n    's3v4a': CrtS3SigV4AsymAuth,\n    's3v4a-query': CrtS3SigV4AsymQueryAuth,\n}\n", "botocore/crt/__init__.py": "# Copyright 2022 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\n# A list of auth types supported by the signers in botocore/crt/auth.py. This\n# should always match the keys of botocore.crt.auth.CRT_AUTH_TYPE_MAPS. The\n# information is duplicated here so that it can be accessed in environments\n# where `awscrt` is not present and any import from botocore.crt.auth would\n# fail.\nCRT_SUPPORTED_AUTH_TYPES = (\n    'v4',\n    'v4-query',\n    'v4a',\n    's3v4',\n    's3v4-query',\n    's3v4a',\n    's3v4a-query',\n)\n", "botocore/retries/special.py": "\"\"\"Special cased retries.\n\nThese are additional retry cases we still have to handle from the legacy\nretry handler.  They don't make sense as part of the standard mode retry\nmodule.  Ideally we should be able to remove this module.\n\n\"\"\"\nimport logging\nfrom binascii import crc32\n\nfrom botocore.retries.base import BaseRetryableChecker\n\nlogger = logging.getLogger(__name__)\n\n\n# TODO: This is an ideal candidate for the retryable trait once that's\n# available.\nclass RetryIDPCommunicationError(BaseRetryableChecker):\n    _SERVICE_NAME = 'sts'\n\n    def is_retryable(self, context):\n        service_name = context.operation_model.service_model.service_name\n        if service_name != self._SERVICE_NAME:\n            return False\n        error_code = context.get_error_code()\n        return error_code == 'IDPCommunicationError'\n\n\nclass RetryDDBChecksumError(BaseRetryableChecker):\n    _CHECKSUM_HEADER = 'x-amz-crc32'\n    _SERVICE_NAME = 'dynamodb'\n\n    def is_retryable(self, context):\n        service_name = context.operation_model.service_model.service_name\n        if service_name != self._SERVICE_NAME:\n            return False\n        if context.http_response is None:\n            return False\n        checksum = context.http_response.headers.get(self._CHECKSUM_HEADER)\n        if checksum is None:\n            return False\n        actual_crc32 = crc32(context.http_response.content) & 0xFFFFFFFF\n        if actual_crc32 != int(checksum):\n            logger.debug(\n                \"DynamoDB crc32 checksum does not match, \"\n                \"expected: %s, actual: %s\",\n                checksum,\n                actual_crc32,\n            )\n            return True\n", "botocore/retries/quota.py": "\"\"\"Retry quota implementation.\n\n\n\"\"\"\nimport threading\n\n\nclass RetryQuota:\n    INITIAL_CAPACITY = 500\n\n    def __init__(self, initial_capacity=INITIAL_CAPACITY, lock=None):\n        self._max_capacity = initial_capacity\n        self._available_capacity = initial_capacity\n        if lock is None:\n            lock = threading.Lock()\n        self._lock = lock\n\n    def acquire(self, capacity_amount):\n        \"\"\"Attempt to aquire a certain amount of capacity.\n\n        If there's not sufficient amount of capacity available, ``False``\n        is returned.  Otherwise, ``True`` is returned, which indicates that\n        capacity was successfully allocated.\n\n        \"\"\"\n        # The acquire() is only called when we encounter a retryable\n        # response so we aren't worried about locking the entire method.\n        with self._lock:\n            if capacity_amount > self._available_capacity:\n                return False\n            self._available_capacity -= capacity_amount\n            return True\n\n    def release(self, capacity_amount):\n        \"\"\"Release capacity back to the retry quota.\n\n        The capacity being released will be truncated if necessary\n        to ensure the max capacity is never exceeded.\n\n        \"\"\"\n        # Implementation note:  The release() method is called as part\n        # of the \"after-call\" event, which means it gets invoked for\n        # every API call.  In the common case where the request is\n        # successful and we're at full capacity, we can avoid locking.\n        # We can't exceed max capacity so there's no work we have to do.\n        if self._max_capacity == self._available_capacity:\n            return\n        with self._lock:\n            amount = min(\n                self._max_capacity - self._available_capacity, capacity_amount\n            )\n            self._available_capacity += amount\n\n    @property\n    def available_capacity(self):\n        return self._available_capacity\n", "botocore/retries/base.py": "class BaseRetryBackoff:\n    def delay_amount(self, context):\n        \"\"\"Calculate how long we should delay before retrying.\n\n        :type context: RetryContext\n\n        \"\"\"\n        raise NotImplementedError(\"delay_amount\")\n\n\nclass BaseRetryableChecker:\n    \"\"\"Base class for determining if a retry should happen.\n\n    This base class checks for specific retryable conditions.\n    A single retryable checker doesn't necessarily indicate a retry\n    will happen.  It's up to the ``RetryPolicy`` to use its\n    ``BaseRetryableCheckers`` to make the final decision on whether a retry\n    should happen.\n    \"\"\"\n\n    def is_retryable(self, context):\n        \"\"\"Returns True if retryable, False if not.\n\n        :type context: RetryContext\n        \"\"\"\n        raise NotImplementedError(\"is_retryable\")\n", "botocore/retries/bucket.py": "\"\"\"This module implements token buckets used for client side throttling.\"\"\"\nimport threading\nimport time\n\nfrom botocore.exceptions import CapacityNotAvailableError\n\n\nclass Clock:\n    def __init__(self):\n        pass\n\n    def sleep(self, amount):\n        time.sleep(amount)\n\n    def current_time(self):\n        return time.time()\n\n\nclass TokenBucket:\n    _MIN_RATE = 0.5\n\n    def __init__(self, max_rate, clock, min_rate=_MIN_RATE):\n        self._fill_rate = None\n        self._max_capacity = None\n        self._current_capacity = 0\n        self._clock = clock\n        self._last_timestamp = None\n        self._min_rate = min_rate\n        self._lock = threading.Lock()\n        self._new_fill_rate_condition = threading.Condition(self._lock)\n        self.max_rate = max_rate\n\n    @property\n    def max_rate(self):\n        return self._fill_rate\n\n    @max_rate.setter\n    def max_rate(self, value):\n        with self._new_fill_rate_condition:\n            # Before we can change the rate we need to fill any pending\n            # tokens we might have based on the current rate.  If we don't\n            # do this it means everything since the last recorded timestamp\n            # will accumulate at the rate we're about to set which isn't\n            # correct.\n            self._refill()\n            self._fill_rate = max(value, self._min_rate)\n            if value >= 1:\n                self._max_capacity = value\n            else:\n                self._max_capacity = 1\n            # If we're scaling down, we also can't have a capacity that's\n            # more than our max_capacity.\n            self._current_capacity = min(\n                self._current_capacity, self._max_capacity\n            )\n            self._new_fill_rate_condition.notify()\n\n    @property\n    def max_capacity(self):\n        return self._max_capacity\n\n    @property\n    def available_capacity(self):\n        return self._current_capacity\n\n    def acquire(self, amount=1, block=True):\n        \"\"\"Acquire token or return amount of time until next token available.\n\n        If block is True, then this method will block until there's sufficient\n        capacity to acquire the desired amount.\n\n        If block is False, then this method will return True is capacity\n        was successfully acquired, False otherwise.\n\n        \"\"\"\n        with self._new_fill_rate_condition:\n            return self._acquire(amount=amount, block=block)\n\n    def _acquire(self, amount, block):\n        self._refill()\n        if amount <= self._current_capacity:\n            self._current_capacity -= amount\n            return True\n        else:\n            if not block:\n                raise CapacityNotAvailableError()\n            # Not enough capacity.\n            sleep_amount = self._sleep_amount(amount)\n            while sleep_amount > 0:\n                # Until python3.2, wait() always returned None so we can't\n                # tell if a timeout occurred waiting on the cond var.\n                # Because of this we'll unconditionally call _refill().\n                # The downside to this is that we were waken up via\n                # a notify(), we're calling unnecessarily calling _refill() an\n                # extra time.\n                self._new_fill_rate_condition.wait(sleep_amount)\n                self._refill()\n                sleep_amount = self._sleep_amount(amount)\n            self._current_capacity -= amount\n            return True\n\n    def _sleep_amount(self, amount):\n        return (amount - self._current_capacity) / self._fill_rate\n\n    def _refill(self):\n        timestamp = self._clock.current_time()\n        if self._last_timestamp is None:\n            self._last_timestamp = timestamp\n            return\n        current_capacity = self._current_capacity\n        fill_amount = (timestamp - self._last_timestamp) * self._fill_rate\n        new_capacity = min(self._max_capacity, current_capacity + fill_amount)\n        self._current_capacity = new_capacity\n        self._last_timestamp = timestamp\n", "botocore/retries/throttling.py": "from collections import namedtuple\n\nCubicParams = namedtuple('CubicParams', ['w_max', 'k', 'last_fail'])\n\n\nclass CubicCalculator:\n    _SCALE_CONSTANT = 0.4\n    _BETA = 0.7\n\n    def __init__(\n        self,\n        starting_max_rate,\n        start_time,\n        scale_constant=_SCALE_CONSTANT,\n        beta=_BETA,\n    ):\n        self._w_max = starting_max_rate\n        self._scale_constant = scale_constant\n        self._beta = beta\n        self._k = self._calculate_zero_point()\n        self._last_fail = start_time\n\n    def _calculate_zero_point(self):\n        scaled_value = (self._w_max * (1 - self._beta)) / self._scale_constant\n        k = scaled_value ** (1 / 3.0)\n        return k\n\n    def success_received(self, timestamp):\n        dt = timestamp - self._last_fail\n        new_rate = self._scale_constant * (dt - self._k) ** 3 + self._w_max\n        return new_rate\n\n    def error_received(self, current_rate, timestamp):\n        # Consider not having this be the current measured rate.\n\n        # We have a new max rate, which is the current rate we were sending\n        # at when we received an error response.\n        self._w_max = current_rate\n        self._k = self._calculate_zero_point()\n        self._last_fail = timestamp\n        return current_rate * self._beta\n\n    def get_params_snapshot(self):\n        \"\"\"Return a read-only object of the current cubic parameters.\n\n        These parameters are intended to be used for debug/troubleshooting\n        purposes.  These object is a read-only snapshot and cannot be used\n        to modify the behavior of the CUBIC calculations.\n\n        New parameters may be added to this object in the future.\n\n        \"\"\"\n        return CubicParams(\n            w_max=self._w_max, k=self._k, last_fail=self._last_fail\n        )\n", "botocore/retries/adaptive.py": "import logging\nimport math\nimport threading\n\nfrom botocore.retries import bucket, standard, throttling\n\nlogger = logging.getLogger(__name__)\n\n\ndef register_retry_handler(client):\n    clock = bucket.Clock()\n    rate_adjustor = throttling.CubicCalculator(\n        starting_max_rate=0, start_time=clock.current_time()\n    )\n    token_bucket = bucket.TokenBucket(max_rate=1, clock=clock)\n    rate_clocker = RateClocker(clock)\n    throttling_detector = standard.ThrottlingErrorDetector(\n        retry_event_adapter=standard.RetryEventAdapter(),\n    )\n    limiter = ClientRateLimiter(\n        rate_adjustor=rate_adjustor,\n        rate_clocker=rate_clocker,\n        token_bucket=token_bucket,\n        throttling_detector=throttling_detector,\n        clock=clock,\n    )\n    client.meta.events.register(\n        'before-send',\n        limiter.on_sending_request,\n    )\n    client.meta.events.register(\n        'needs-retry',\n        limiter.on_receiving_response,\n    )\n    return limiter\n\n\nclass ClientRateLimiter:\n    _MAX_RATE_ADJUST_SCALE = 2.0\n\n    def __init__(\n        self,\n        rate_adjustor,\n        rate_clocker,\n        token_bucket,\n        throttling_detector,\n        clock,\n    ):\n        self._rate_adjustor = rate_adjustor\n        self._rate_clocker = rate_clocker\n        self._token_bucket = token_bucket\n        self._throttling_detector = throttling_detector\n        self._clock = clock\n        self._enabled = False\n        self._lock = threading.Lock()\n\n    def on_sending_request(self, request, **kwargs):\n        if self._enabled:\n            self._token_bucket.acquire()\n\n    # Hooked up to needs-retry.\n    def on_receiving_response(self, **kwargs):\n        measured_rate = self._rate_clocker.record()\n        timestamp = self._clock.current_time()\n        with self._lock:\n            if not self._throttling_detector.is_throttling_error(**kwargs):\n                new_rate = self._rate_adjustor.success_received(timestamp)\n            else:\n                if not self._enabled:\n                    rate_to_use = measured_rate\n                else:\n                    rate_to_use = min(\n                        measured_rate, self._token_bucket.max_rate\n                    )\n                new_rate = self._rate_adjustor.error_received(\n                    rate_to_use, timestamp\n                )\n                logger.debug(\n                    \"Throttling response received, new send rate: %s \"\n                    \"measured rate: %s, token bucket capacity \"\n                    \"available: %s\",\n                    new_rate,\n                    measured_rate,\n                    self._token_bucket.available_capacity,\n                )\n                self._enabled = True\n            self._token_bucket.max_rate = min(\n                new_rate, self._MAX_RATE_ADJUST_SCALE * measured_rate\n            )\n\n\nclass RateClocker:\n    \"\"\"Tracks the rate at which a client is sending a request.\"\"\"\n\n    _DEFAULT_SMOOTHING = 0.8\n    # Update the rate every _TIME_BUCKET_RANGE seconds.\n    _TIME_BUCKET_RANGE = 0.5\n\n    def __init__(\n        self,\n        clock,\n        smoothing=_DEFAULT_SMOOTHING,\n        time_bucket_range=_TIME_BUCKET_RANGE,\n    ):\n        self._clock = clock\n        self._measured_rate = 0\n        self._smoothing = smoothing\n        self._last_bucket = math.floor(self._clock.current_time())\n        self._time_bucket_scale = 1 / self._TIME_BUCKET_RANGE\n        self._count = 0\n        self._lock = threading.Lock()\n\n    def record(self, amount=1):\n        with self._lock:\n            t = self._clock.current_time()\n            bucket = (\n                math.floor(t * self._time_bucket_scale)\n                / self._time_bucket_scale\n            )\n            self._count += amount\n            if bucket > self._last_bucket:\n                current_rate = self._count / float(bucket - self._last_bucket)\n                self._measured_rate = (current_rate * self._smoothing) + (\n                    self._measured_rate * (1 - self._smoothing)\n                )\n                self._count = 0\n                self._last_bucket = bucket\n            return self._measured_rate\n\n    @property\n    def measured_rate(self):\n        return self._measured_rate\n", "botocore/retries/__init__.py": "\"\"\"New retry v2 handlers.\n\nThis package obsoletes the botocore/retryhandler.py module and contains\nnew retry logic.\n\n\"\"\"\n", "botocore/retries/standard.py": "\"\"\"Standard retry behavior.\n\nThis contains the default standard retry behavior.\nIt provides consistent behavior with other AWS SDKs.\n\nThe key base classes uses for retries:\n\n    * ``BaseRetryableChecker`` - Use to check a specific condition that\n    indicates a retry should happen.  This can include things like\n    max attempts, HTTP status code checks, error code checks etc.\n    * ``RetryBackoff`` - Use to determine how long we should backoff until\n    we retry a request.  This is the class that will implement delay such\n    as exponential backoff.\n    * ``RetryPolicy`` - Main class that determines if a retry should\n    happen.  It can combine data from a various BaseRetryableCheckers\n    to make a final call as to whether or not a retry should happen.\n    It then uses a ``BaseRetryBackoff`` to determine how long to delay.\n    * ``RetryHandler`` - The bridge between botocore's event system\n    used by endpoint.py to manage retries and the interfaces defined\n    in this module.\n\nThis allows us to define an API that has minimal coupling to the event\nbased API used by botocore.\n\n\"\"\"\nimport logging\nimport random\n\nfrom botocore.exceptions import (\n    ConnectionError,\n    ConnectTimeoutError,\n    HTTPClientError,\n    ReadTimeoutError,\n)\nfrom botocore.retries import quota, special\nfrom botocore.retries.base import BaseRetryableChecker, BaseRetryBackoff\n\nDEFAULT_MAX_ATTEMPTS = 3\nlogger = logging.getLogger(__name__)\n\n\ndef register_retry_handler(client, max_attempts=DEFAULT_MAX_ATTEMPTS):\n    retry_quota = RetryQuotaChecker(quota.RetryQuota())\n\n    service_id = client.meta.service_model.service_id\n    service_event_name = service_id.hyphenize()\n    client.meta.events.register(\n        f'after-call.{service_event_name}', retry_quota.release_retry_quota\n    )\n\n    handler = RetryHandler(\n        retry_policy=RetryPolicy(\n            retry_checker=StandardRetryConditions(max_attempts=max_attempts),\n            retry_backoff=ExponentialBackoff(),\n        ),\n        retry_event_adapter=RetryEventAdapter(),\n        retry_quota=retry_quota,\n    )\n\n    unique_id = 'retry-config-%s' % service_event_name\n    client.meta.events.register(\n        'needs-retry.%s' % service_event_name,\n        handler.needs_retry,\n        unique_id=unique_id,\n    )\n    return handler\n\n\nclass RetryHandler:\n    \"\"\"Bridge between botocore's event system and this module.\n\n    This class is intended to be hooked to botocore's event system\n    as an event handler.\n    \"\"\"\n\n    def __init__(self, retry_policy, retry_event_adapter, retry_quota):\n        self._retry_policy = retry_policy\n        self._retry_event_adapter = retry_event_adapter\n        self._retry_quota = retry_quota\n\n    def needs_retry(self, **kwargs):\n        \"\"\"Connect as a handler to the needs-retry event.\"\"\"\n        retry_delay = None\n        context = self._retry_event_adapter.create_retry_context(**kwargs)\n        if self._retry_policy.should_retry(context):\n            # Before we can retry we need to ensure we have sufficient\n            # capacity in our retry quota.\n            if self._retry_quota.acquire_retry_quota(context):\n                retry_delay = self._retry_policy.compute_retry_delay(context)\n                logger.debug(\n                    \"Retry needed, retrying request after delay of: %s\",\n                    retry_delay,\n                )\n            else:\n                logger.debug(\n                    \"Retry needed but retry quota reached, \"\n                    \"not retrying request.\"\n                )\n        else:\n            logger.debug(\"Not retrying request.\")\n        self._retry_event_adapter.adapt_retry_response_from_context(context)\n        return retry_delay\n\n\nclass RetryEventAdapter:\n    \"\"\"Adapter to existing retry interface used in the endpoints layer.\n\n    This existing interface for determining if a retry needs to happen\n    is event based and used in ``botocore.endpoint``.  The interface has\n    grown organically over the years and could use some cleanup.  This\n    adapter converts that interface into the interface used by the\n    new retry strategies.\n\n    \"\"\"\n\n    def create_retry_context(self, **kwargs):\n        \"\"\"Create context based on needs-retry kwargs.\"\"\"\n        response = kwargs['response']\n        if response is None:\n            # If response is None it means that an exception was raised\n            # because we never received a response from the service.  This\n            # could be something like a ConnectionError we get from our\n            # http layer.\n            http_response = None\n            parsed_response = None\n        else:\n            http_response, parsed_response = response\n        # This provides isolation between the kwargs emitted in the\n        # needs-retry event, and what this module uses to check for\n        # retries.\n        context = RetryContext(\n            attempt_number=kwargs['attempts'],\n            operation_model=kwargs['operation'],\n            http_response=http_response,\n            parsed_response=parsed_response,\n            caught_exception=kwargs['caught_exception'],\n            request_context=kwargs['request_dict']['context'],\n        )\n        return context\n\n    def adapt_retry_response_from_context(self, context):\n        \"\"\"Modify response back to user back from context.\"\"\"\n        # This will mutate attributes that are returned back to the end\n        # user.  We do it this way so that all the various retry classes\n        # don't mutate any input parameters from the needs-retry event.\n        metadata = context.get_retry_metadata()\n        if context.parsed_response is not None:\n            context.parsed_response.setdefault('ResponseMetadata', {}).update(\n                metadata\n            )\n\n\n# Implementation note: this is meant to encapsulate all the misc stuff\n# that gets sent in the needs-retry event.  This is mapped so that params\n# are more clear and explicit.\nclass RetryContext:\n    \"\"\"Normalize a response that we use to check if a retry should occur.\n\n    This class smoothes over the different types of responses we may get\n    from a service including:\n\n        * A modeled error response from the service that contains a service\n          code and error message.\n        * A raw HTTP response that doesn't contain service protocol specific\n          error keys.\n        * An exception received while attempting to retrieve a response.\n          This could be a ConnectionError we receive from our HTTP layer which\n          could represent that we weren't able to receive a response from\n          the service.\n\n    This class guarantees that at least one of the above attributes will be\n    non None.\n\n    This class is meant to provide a read-only view into the properties\n    associated with a possible retryable response.  None of the properties\n    are meant to be modified directly.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        attempt_number,\n        operation_model=None,\n        parsed_response=None,\n        http_response=None,\n        caught_exception=None,\n        request_context=None,\n    ):\n        # 1-based attempt number.\n        self.attempt_number = attempt_number\n        self.operation_model = operation_model\n        # This is the parsed response dictionary we get from parsing\n        # the HTTP response from the service.\n        self.parsed_response = parsed_response\n        # This is an instance of botocore.awsrequest.AWSResponse.\n        self.http_response = http_response\n        # This is a subclass of Exception that will be non None if\n        # an exception was raised when retrying to retrieve a response.\n        self.caught_exception = caught_exception\n        # This is the request context dictionary that's added to the\n        # request dict.  This is used to story any additional state\n        # about the request.  We use this for storing retry quota\n        # capacity.\n        if request_context is None:\n            request_context = {}\n        self.request_context = request_context\n        self._retry_metadata = {}\n\n    # These are misc helper methods to avoid duplication in the various\n    # checkers.\n    def get_error_code(self):\n        \"\"\"Check if there was a parsed response with an error code.\n\n        If we could not find any error codes, ``None`` is returned.\n\n        \"\"\"\n        if self.parsed_response is None:\n            return\n        error = self.parsed_response.get('Error', {})\n        if not isinstance(error, dict):\n            return\n        return error.get('Code')\n\n    def add_retry_metadata(self, **kwargs):\n        \"\"\"Add key/value pairs to the retry metadata.\n\n        This allows any objects during the retry process to add\n        metadata about any checks/validations that happened.\n\n        This gets added to the response metadata in the retry handler.\n\n        \"\"\"\n        self._retry_metadata.update(**kwargs)\n\n    def get_retry_metadata(self):\n        return self._retry_metadata.copy()\n\n\nclass RetryPolicy:\n    def __init__(self, retry_checker, retry_backoff):\n        self._retry_checker = retry_checker\n        self._retry_backoff = retry_backoff\n\n    def should_retry(self, context):\n        return self._retry_checker.is_retryable(context)\n\n    def compute_retry_delay(self, context):\n        return self._retry_backoff.delay_amount(context)\n\n\nclass ExponentialBackoff(BaseRetryBackoff):\n    _BASE = 2\n    _MAX_BACKOFF = 20\n\n    def __init__(self, max_backoff=20, random=random.random):\n        self._base = self._BASE\n        self._max_backoff = max_backoff\n        self._random = random\n\n    def delay_amount(self, context):\n        \"\"\"Calculates delay based on exponential backoff.\n\n        This class implements truncated binary exponential backoff\n        with jitter::\n\n            t_i = rand(0, 1) * min(2 ** attempt, MAX_BACKOFF)\n\n        where ``i`` is the request attempt (0 based).\n\n        \"\"\"\n        # The context.attempt_number is a 1-based value, but we have\n        # to calculate the delay based on i based a 0-based value.  We\n        # want the first delay to just be ``rand(0, 1)``.\n        return self._random() * min(\n            (self._base ** (context.attempt_number - 1)),\n            self._max_backoff,\n        )\n\n\nclass MaxAttemptsChecker(BaseRetryableChecker):\n    def __init__(self, max_attempts):\n        self._max_attempts = max_attempts\n\n    def is_retryable(self, context):\n        under_max_attempts = context.attempt_number < self._max_attempts\n        retries_context = context.request_context.get('retries')\n        if retries_context:\n            retries_context['max'] = max(\n                retries_context.get('max', 0), self._max_attempts\n            )\n        if not under_max_attempts:\n            logger.debug(\"Max attempts of %s reached.\", self._max_attempts)\n            context.add_retry_metadata(MaxAttemptsReached=True)\n        return under_max_attempts\n\n\nclass TransientRetryableChecker(BaseRetryableChecker):\n    _TRANSIENT_ERROR_CODES = [\n        'RequestTimeout',\n        'RequestTimeoutException',\n        'PriorRequestNotComplete',\n    ]\n    _TRANSIENT_STATUS_CODES = [500, 502, 503, 504]\n    _TRANSIENT_EXCEPTION_CLS = (\n        ConnectionError,\n        HTTPClientError,\n    )\n\n    def __init__(\n        self,\n        transient_error_codes=None,\n        transient_status_codes=None,\n        transient_exception_cls=None,\n    ):\n        if transient_error_codes is None:\n            transient_error_codes = self._TRANSIENT_ERROR_CODES[:]\n        if transient_status_codes is None:\n            transient_status_codes = self._TRANSIENT_STATUS_CODES[:]\n        if transient_exception_cls is None:\n            transient_exception_cls = self._TRANSIENT_EXCEPTION_CLS\n        self._transient_error_codes = transient_error_codes\n        self._transient_status_codes = transient_status_codes\n        self._transient_exception_cls = transient_exception_cls\n\n    def is_retryable(self, context):\n        if context.get_error_code() in self._transient_error_codes:\n            return True\n        if context.http_response is not None:\n            if (\n                context.http_response.status_code\n                in self._transient_status_codes\n            ):\n                return True\n        if context.caught_exception is not None:\n            return isinstance(\n                context.caught_exception, self._transient_exception_cls\n            )\n        return False\n\n\nclass ThrottledRetryableChecker(BaseRetryableChecker):\n    # This is the union of all error codes we've seen that represent\n    # a throttled error.\n    _THROTTLED_ERROR_CODES = [\n        'Throttling',\n        'ThrottlingException',\n        'ThrottledException',\n        'RequestThrottledException',\n        'TooManyRequestsException',\n        'ProvisionedThroughputExceededException',\n        'TransactionInProgressException',\n        'RequestLimitExceeded',\n        'BandwidthLimitExceeded',\n        'LimitExceededException',\n        'RequestThrottled',\n        'SlowDown',\n        'PriorRequestNotComplete',\n        'EC2ThrottledException',\n    ]\n\n    def __init__(self, throttled_error_codes=None):\n        if throttled_error_codes is None:\n            throttled_error_codes = self._THROTTLED_ERROR_CODES[:]\n        self._throttled_error_codes = throttled_error_codes\n\n    def is_retryable(self, context):\n        # Only the error code from a parsed service response is used\n        # to determine if the response is a throttled response.\n        return context.get_error_code() in self._throttled_error_codes\n\n\nclass ModeledRetryableChecker(BaseRetryableChecker):\n    \"\"\"Check if an error has been modeled as retryable.\"\"\"\n\n    def __init__(self):\n        self._error_detector = ModeledRetryErrorDetector()\n\n    def is_retryable(self, context):\n        error_code = context.get_error_code()\n        if error_code is None:\n            return False\n        return self._error_detector.detect_error_type(context) is not None\n\n\nclass ModeledRetryErrorDetector:\n    \"\"\"Checks whether or not an error is a modeled retryable error.\"\"\"\n\n    # There are return values from the detect_error_type() method.\n    TRANSIENT_ERROR = 'TRANSIENT_ERROR'\n    THROTTLING_ERROR = 'THROTTLING_ERROR'\n    # This class is lower level than ModeledRetryableChecker, which\n    # implements BaseRetryableChecker.  This object allows you to distinguish\n    # between the various types of retryable errors.\n\n    def detect_error_type(self, context):\n        \"\"\"Detect the error type associated with an error code and model.\n\n        This will either return:\n\n            * ``self.TRANSIENT_ERROR`` - If the error is a transient error\n            * ``self.THROTTLING_ERROR`` - If the error is a throttling error\n            * ``None`` - If the error is neither type of error.\n\n        \"\"\"\n        error_code = context.get_error_code()\n        op_model = context.operation_model\n        if op_model is None or not op_model.error_shapes:\n            return\n        for shape in op_model.error_shapes:\n            if shape.metadata.get('retryable') is not None:\n                # Check if this error code matches the shape.  This can\n                # be either by name or by a modeled error code.\n                error_code_to_check = (\n                    shape.metadata.get('error', {}).get('code') or shape.name\n                )\n                if error_code == error_code_to_check:\n                    if shape.metadata['retryable'].get('throttling'):\n                        return self.THROTTLING_ERROR\n                    return self.TRANSIENT_ERROR\n\n\nclass ThrottlingErrorDetector:\n    def __init__(self, retry_event_adapter):\n        self._modeled_error_detector = ModeledRetryErrorDetector()\n        self._fixed_error_code_detector = ThrottledRetryableChecker()\n        self._retry_event_adapter = retry_event_adapter\n\n    # This expects the kwargs from needs-retry to be passed through.\n    def is_throttling_error(self, **kwargs):\n        context = self._retry_event_adapter.create_retry_context(**kwargs)\n        if self._fixed_error_code_detector.is_retryable(context):\n            return True\n        error_type = self._modeled_error_detector.detect_error_type(context)\n        return error_type == self._modeled_error_detector.THROTTLING_ERROR\n\n\nclass StandardRetryConditions(BaseRetryableChecker):\n    \"\"\"Concrete class that implements the standard retry policy checks.\n\n    Specifically:\n\n        not max_attempts and (transient or throttled or modeled_retry)\n\n    \"\"\"\n\n    def __init__(self, max_attempts=DEFAULT_MAX_ATTEMPTS):\n        # Note: This class is for convenience so you can have the\n        # standard retry condition in a single class.\n        self._max_attempts_checker = MaxAttemptsChecker(max_attempts)\n        self._additional_checkers = OrRetryChecker(\n            [\n                TransientRetryableChecker(),\n                ThrottledRetryableChecker(),\n                ModeledRetryableChecker(),\n                OrRetryChecker(\n                    [\n                        special.RetryIDPCommunicationError(),\n                        special.RetryDDBChecksumError(),\n                    ]\n                ),\n            ]\n        )\n\n    def is_retryable(self, context):\n        return self._max_attempts_checker.is_retryable(\n            context\n        ) and self._additional_checkers.is_retryable(context)\n\n\nclass OrRetryChecker(BaseRetryableChecker):\n    def __init__(self, checkers):\n        self._checkers = checkers\n\n    def is_retryable(self, context):\n        return any(checker.is_retryable(context) for checker in self._checkers)\n\n\nclass RetryQuotaChecker:\n    _RETRY_COST = 5\n    _NO_RETRY_INCREMENT = 1\n    _TIMEOUT_RETRY_REQUEST = 10\n    _TIMEOUT_EXCEPTIONS = (ConnectTimeoutError, ReadTimeoutError)\n\n    # Implementation note:  We're not making this a BaseRetryableChecker\n    # because this isn't just a check if we can retry.  This also changes\n    # state so we have to careful when/how we call this.  Making it\n    # a BaseRetryableChecker implies you can call .is_retryable(context)\n    # as many times as you want and not affect anything.\n\n    def __init__(self, quota):\n        self._quota = quota\n        # This tracks the last amount\n        self._last_amount_acquired = None\n\n    def acquire_retry_quota(self, context):\n        if self._is_timeout_error(context):\n            capacity_amount = self._TIMEOUT_RETRY_REQUEST\n        else:\n            capacity_amount = self._RETRY_COST\n        success = self._quota.acquire(capacity_amount)\n        if success:\n            # We add the capacity amount to the request context so we know\n            # how much to release later.  The capacity amount can vary based\n            # on the error.\n            context.request_context['retry_quota_capacity'] = capacity_amount\n            return True\n        context.add_retry_metadata(RetryQuotaReached=True)\n        return False\n\n    def _is_timeout_error(self, context):\n        return isinstance(context.caught_exception, self._TIMEOUT_EXCEPTIONS)\n\n    # This is intended to be hooked up to ``after-call``.\n    def release_retry_quota(self, context, http_response, **kwargs):\n        # There's three possible options.\n        # 1. The HTTP response did not have a 2xx response.  In that case we\n        #    give no quota back.\n        # 2. The HTTP request was successful and was never retried.  In\n        #    that case we give _NO_RETRY_INCREMENT back.\n        # 3. The API call had retries, and we eventually receive an HTTP\n        #    response with a 2xx status code.  In that case we give back\n        #    whatever quota was associated with the last acquisition.\n        if http_response is None:\n            return\n        status_code = http_response.status_code\n        if 200 <= status_code < 300:\n            if 'retry_quota_capacity' not in context:\n                self._quota.release(self._NO_RETRY_INCREMENT)\n            else:\n                capacity_amount = context['retry_quota_capacity']\n                self._quota.release(capacity_amount)\n", "botocore/vendored/six.py": "# Copyright (c) 2010-2020 Benjamin Peterson\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n\"\"\"Utilities for writing code that runs on Python 2 and 3\"\"\"\n\nfrom __future__ import absolute_import\n\nimport functools\nimport itertools\nimport operator\nimport sys\nimport types\n\n__author__ = \"Benjamin Peterson <benjamin@python.org>\"\n__version__ = \"1.16.0\"\n\n\n# Useful for very coarse version differentiation.\nPY2 = sys.version_info[0] == 2\nPY3 = sys.version_info[0] == 3\nPY34 = sys.version_info[0:2] >= (3, 4)\n\nif PY3:\n    string_types = str,\n    integer_types = int,\n    class_types = type,\n    text_type = str\n    binary_type = bytes\n\n    MAXSIZE = sys.maxsize\nelse:\n    string_types = basestring,\n    integer_types = (int, long)\n    class_types = (type, types.ClassType)\n    text_type = unicode\n    binary_type = str\n\n    if sys.platform.startswith(\"java\"):\n        # Jython always uses 32 bits.\n        MAXSIZE = int((1 << 31) - 1)\n    else:\n        # It's possible to have sizeof(long) != sizeof(Py_ssize_t).\n        class X(object):\n\n            def __len__(self):\n                return 1 << 31\n        try:\n            len(X())\n        except OverflowError:\n            # 32-bit\n            MAXSIZE = int((1 << 31) - 1)\n        else:\n            # 64-bit\n            MAXSIZE = int((1 << 63) - 1)\n        del X\n\nif PY34:\n    from importlib.util import spec_from_loader\nelse:\n    spec_from_loader = None\n\n\ndef _add_doc(func, doc):\n    \"\"\"Add documentation to a function.\"\"\"\n    func.__doc__ = doc\n\n\ndef _import_module(name):\n    \"\"\"Import module, returning the module after the last dot.\"\"\"\n    __import__(name)\n    return sys.modules[name]\n\n\nclass _LazyDescr(object):\n\n    def __init__(self, name):\n        self.name = name\n\n    def __get__(self, obj, tp):\n        result = self._resolve()\n        setattr(obj, self.name, result)  # Invokes __set__.\n        try:\n            # This is a bit ugly, but it avoids running this again by\n            # removing this descriptor.\n            delattr(obj.__class__, self.name)\n        except AttributeError:\n            pass\n        return result\n\n\nclass MovedModule(_LazyDescr):\n\n    def __init__(self, name, old, new=None):\n        super(MovedModule, self).__init__(name)\n        if PY3:\n            if new is None:\n                new = name\n            self.mod = new\n        else:\n            self.mod = old\n\n    def _resolve(self):\n        return _import_module(self.mod)\n\n    def __getattr__(self, attr):\n        _module = self._resolve()\n        value = getattr(_module, attr)\n        setattr(self, attr, value)\n        return value\n\n\nclass _LazyModule(types.ModuleType):\n\n    def __init__(self, name):\n        super(_LazyModule, self).__init__(name)\n        self.__doc__ = self.__class__.__doc__\n\n    def __dir__(self):\n        attrs = [\"__doc__\", \"__name__\"]\n        attrs += [attr.name for attr in self._moved_attributes]\n        return attrs\n\n    # Subclasses should override this\n    _moved_attributes = []\n\n\nclass MovedAttribute(_LazyDescr):\n\n    def __init__(self, name, old_mod, new_mod, old_attr=None, new_attr=None):\n        super(MovedAttribute, self).__init__(name)\n        if PY3:\n            if new_mod is None:\n                new_mod = name\n            self.mod = new_mod\n            if new_attr is None:\n                if old_attr is None:\n                    new_attr = name\n                else:\n                    new_attr = old_attr\n            self.attr = new_attr\n        else:\n            self.mod = old_mod\n            if old_attr is None:\n                old_attr = name\n            self.attr = old_attr\n\n    def _resolve(self):\n        module = _import_module(self.mod)\n        return getattr(module, self.attr)\n\n\nclass _SixMetaPathImporter(object):\n\n    \"\"\"\n    A meta path importer to import six.moves and its submodules.\n\n    This class implements a PEP302 finder and loader. It should be compatible\n    with Python 2.5 and all existing versions of Python3\n    \"\"\"\n\n    def __init__(self, six_module_name):\n        self.name = six_module_name\n        self.known_modules = {}\n\n    def _add_module(self, mod, *fullnames):\n        for fullname in fullnames:\n            self.known_modules[self.name + \".\" + fullname] = mod\n\n    def _get_module(self, fullname):\n        return self.known_modules[self.name + \".\" + fullname]\n\n    def find_module(self, fullname, path=None):\n        if fullname in self.known_modules:\n            return self\n        return None\n\n    def find_spec(self, fullname, path, target=None):\n        if fullname in self.known_modules:\n            return spec_from_loader(fullname, self)\n        return None\n\n    def __get_module(self, fullname):\n        try:\n            return self.known_modules[fullname]\n        except KeyError:\n            raise ImportError(\"This loader does not know module \" + fullname)\n\n    def load_module(self, fullname):\n        try:\n            # in case of a reload\n            return sys.modules[fullname]\n        except KeyError:\n            pass\n        mod = self.__get_module(fullname)\n        if isinstance(mod, MovedModule):\n            mod = mod._resolve()\n        else:\n            mod.__loader__ = self\n        sys.modules[fullname] = mod\n        return mod\n\n    def is_package(self, fullname):\n        \"\"\"\n        Return true, if the named module is a package.\n\n        We need this method to get correct spec objects with\n        Python 3.4 (see PEP451)\n        \"\"\"\n        return hasattr(self.__get_module(fullname), \"__path__\")\n\n    def get_code(self, fullname):\n        \"\"\"Return None\n\n        Required, if is_package is implemented\"\"\"\n        self.__get_module(fullname)  # eventually raises ImportError\n        return None\n    get_source = get_code  # same as get_code\n\n    def create_module(self, spec):\n        return self.load_module(spec.name)\n\n    def exec_module(self, module):\n        pass\n\n_importer = _SixMetaPathImporter(__name__)\n\n\nclass _MovedItems(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects\"\"\"\n    __path__ = []  # mark as package\n\n\n_moved_attributes = [\n    MovedAttribute(\"cStringIO\", \"cStringIO\", \"io\", \"StringIO\"),\n    MovedAttribute(\"filter\", \"itertools\", \"builtins\", \"ifilter\", \"filter\"),\n    MovedAttribute(\"filterfalse\", \"itertools\", \"itertools\", \"ifilterfalse\", \"filterfalse\"),\n    MovedAttribute(\"input\", \"__builtin__\", \"builtins\", \"raw_input\", \"input\"),\n    MovedAttribute(\"intern\", \"__builtin__\", \"sys\"),\n    MovedAttribute(\"map\", \"itertools\", \"builtins\", \"imap\", \"map\"),\n    MovedAttribute(\"getcwd\", \"os\", \"os\", \"getcwdu\", \"getcwd\"),\n    MovedAttribute(\"getcwdb\", \"os\", \"os\", \"getcwd\", \"getcwdb\"),\n    MovedAttribute(\"getoutput\", \"commands\", \"subprocess\"),\n    MovedAttribute(\"range\", \"__builtin__\", \"builtins\", \"xrange\", \"range\"),\n    MovedAttribute(\"reload_module\", \"__builtin__\", \"importlib\" if PY34 else \"imp\", \"reload\"),\n    MovedAttribute(\"reduce\", \"__builtin__\", \"functools\"),\n    MovedAttribute(\"shlex_quote\", \"pipes\", \"shlex\", \"quote\"),\n    MovedAttribute(\"StringIO\", \"StringIO\", \"io\"),\n    MovedAttribute(\"UserDict\", \"UserDict\", \"collections\"),\n    MovedAttribute(\"UserList\", \"UserList\", \"collections\"),\n    MovedAttribute(\"UserString\", \"UserString\", \"collections\"),\n    MovedAttribute(\"xrange\", \"__builtin__\", \"builtins\", \"xrange\", \"range\"),\n    MovedAttribute(\"zip\", \"itertools\", \"builtins\", \"izip\", \"zip\"),\n    MovedAttribute(\"zip_longest\", \"itertools\", \"itertools\", \"izip_longest\", \"zip_longest\"),\n    MovedModule(\"builtins\", \"__builtin__\"),\n    MovedModule(\"configparser\", \"ConfigParser\"),\n    MovedModule(\"collections_abc\", \"collections\", \"collections.abc\" if sys.version_info >= (3, 3) else \"collections\"),\n    MovedModule(\"copyreg\", \"copy_reg\"),\n    MovedModule(\"dbm_gnu\", \"gdbm\", \"dbm.gnu\"),\n    MovedModule(\"dbm_ndbm\", \"dbm\", \"dbm.ndbm\"),\n    MovedModule(\"_dummy_thread\", \"dummy_thread\", \"_dummy_thread\" if sys.version_info < (3, 9) else \"_thread\"),\n    MovedModule(\"http_cookiejar\", \"cookielib\", \"http.cookiejar\"),\n    MovedModule(\"http_cookies\", \"Cookie\", \"http.cookies\"),\n    MovedModule(\"html_entities\", \"htmlentitydefs\", \"html.entities\"),\n    MovedModule(\"html_parser\", \"HTMLParser\", \"html.parser\"),\n    MovedModule(\"http_client\", \"httplib\", \"http.client\"),\n    MovedModule(\"email_mime_base\", \"email.MIMEBase\", \"email.mime.base\"),\n    MovedModule(\"email_mime_image\", \"email.MIMEImage\", \"email.mime.image\"),\n    MovedModule(\"email_mime_multipart\", \"email.MIMEMultipart\", \"email.mime.multipart\"),\n    MovedModule(\"email_mime_nonmultipart\", \"email.MIMENonMultipart\", \"email.mime.nonmultipart\"),\n    MovedModule(\"email_mime_text\", \"email.MIMEText\", \"email.mime.text\"),\n    MovedModule(\"BaseHTTPServer\", \"BaseHTTPServer\", \"http.server\"),\n    MovedModule(\"CGIHTTPServer\", \"CGIHTTPServer\", \"http.server\"),\n    MovedModule(\"SimpleHTTPServer\", \"SimpleHTTPServer\", \"http.server\"),\n    MovedModule(\"cPickle\", \"cPickle\", \"pickle\"),\n    MovedModule(\"queue\", \"Queue\"),\n    MovedModule(\"reprlib\", \"repr\"),\n    MovedModule(\"socketserver\", \"SocketServer\"),\n    MovedModule(\"_thread\", \"thread\", \"_thread\"),\n    MovedModule(\"tkinter\", \"Tkinter\"),\n    MovedModule(\"tkinter_dialog\", \"Dialog\", \"tkinter.dialog\"),\n    MovedModule(\"tkinter_filedialog\", \"FileDialog\", \"tkinter.filedialog\"),\n    MovedModule(\"tkinter_scrolledtext\", \"ScrolledText\", \"tkinter.scrolledtext\"),\n    MovedModule(\"tkinter_simpledialog\", \"SimpleDialog\", \"tkinter.simpledialog\"),\n    MovedModule(\"tkinter_tix\", \"Tix\", \"tkinter.tix\"),\n    MovedModule(\"tkinter_ttk\", \"ttk\", \"tkinter.ttk\"),\n    MovedModule(\"tkinter_constants\", \"Tkconstants\", \"tkinter.constants\"),\n    MovedModule(\"tkinter_dnd\", \"Tkdnd\", \"tkinter.dnd\"),\n    MovedModule(\"tkinter_colorchooser\", \"tkColorChooser\",\n                \"tkinter.colorchooser\"),\n    MovedModule(\"tkinter_commondialog\", \"tkCommonDialog\",\n                \"tkinter.commondialog\"),\n    MovedModule(\"tkinter_tkfiledialog\", \"tkFileDialog\", \"tkinter.filedialog\"),\n    MovedModule(\"tkinter_font\", \"tkFont\", \"tkinter.font\"),\n    MovedModule(\"tkinter_messagebox\", \"tkMessageBox\", \"tkinter.messagebox\"),\n    MovedModule(\"tkinter_tksimpledialog\", \"tkSimpleDialog\",\n                \"tkinter.simpledialog\"),\n    MovedModule(\"urllib_parse\", __name__ + \".moves.urllib_parse\", \"urllib.parse\"),\n    MovedModule(\"urllib_error\", __name__ + \".moves.urllib_error\", \"urllib.error\"),\n    MovedModule(\"urllib\", __name__ + \".moves.urllib\", __name__ + \".moves.urllib\"),\n    MovedModule(\"urllib_robotparser\", \"robotparser\", \"urllib.robotparser\"),\n    MovedModule(\"xmlrpc_client\", \"xmlrpclib\", \"xmlrpc.client\"),\n    MovedModule(\"xmlrpc_server\", \"SimpleXMLRPCServer\", \"xmlrpc.server\"),\n]\n# Add windows specific modules.\nif sys.platform == \"win32\":\n    _moved_attributes += [\n        MovedModule(\"winreg\", \"_winreg\"),\n    ]\n\nfor attr in _moved_attributes:\n    setattr(_MovedItems, attr.name, attr)\n    if isinstance(attr, MovedModule):\n        _importer._add_module(attr, \"moves.\" + attr.name)\ndel attr\n\n_MovedItems._moved_attributes = _moved_attributes\n\nmoves = _MovedItems(__name__ + \".moves\")\n_importer._add_module(moves, \"moves\")\n\n\nclass Module_six_moves_urllib_parse(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects in six.moves.urllib_parse\"\"\"\n\n\n_urllib_parse_moved_attributes = [\n    MovedAttribute(\"ParseResult\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"SplitResult\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"parse_qs\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"parse_qsl\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urldefrag\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urljoin\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urlparse\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urlsplit\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urlunparse\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urlunsplit\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"quote\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"quote_plus\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"unquote\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"unquote_plus\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"unquote_to_bytes\", \"urllib\", \"urllib.parse\", \"unquote\", \"unquote_to_bytes\"),\n    MovedAttribute(\"urlencode\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"splitquery\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"splittag\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"splituser\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"splitvalue\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"uses_fragment\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"uses_netloc\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"uses_params\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"uses_query\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"uses_relative\", \"urlparse\", \"urllib.parse\"),\n]\nfor attr in _urllib_parse_moved_attributes:\n    setattr(Module_six_moves_urllib_parse, attr.name, attr)\ndel attr\n\nModule_six_moves_urllib_parse._moved_attributes = _urllib_parse_moved_attributes\n\n_importer._add_module(Module_six_moves_urllib_parse(__name__ + \".moves.urllib_parse\"),\n                      \"moves.urllib_parse\", \"moves.urllib.parse\")\n\n\nclass Module_six_moves_urllib_error(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects in six.moves.urllib_error\"\"\"\n\n\n_urllib_error_moved_attributes = [\n    MovedAttribute(\"URLError\", \"urllib2\", \"urllib.error\"),\n    MovedAttribute(\"HTTPError\", \"urllib2\", \"urllib.error\"),\n    MovedAttribute(\"ContentTooShortError\", \"urllib\", \"urllib.error\"),\n]\nfor attr in _urllib_error_moved_attributes:\n    setattr(Module_six_moves_urllib_error, attr.name, attr)\ndel attr\n\nModule_six_moves_urllib_error._moved_attributes = _urllib_error_moved_attributes\n\n_importer._add_module(Module_six_moves_urllib_error(__name__ + \".moves.urllib.error\"),\n                      \"moves.urllib_error\", \"moves.urllib.error\")\n\n\nclass Module_six_moves_urllib_request(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects in six.moves.urllib_request\"\"\"\n\n\n_urllib_request_moved_attributes = [\n    MovedAttribute(\"urlopen\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"install_opener\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"build_opener\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"pathname2url\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"url2pathname\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"getproxies\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"Request\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"OpenerDirector\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPDefaultErrorHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPRedirectHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPCookieProcessor\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"ProxyHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"BaseHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPPasswordMgr\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPPasswordMgrWithDefaultRealm\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"AbstractBasicAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPBasicAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"ProxyBasicAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"AbstractDigestAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPDigestAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"ProxyDigestAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPSHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"FileHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"FTPHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"CacheFTPHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"UnknownHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPErrorProcessor\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"urlretrieve\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"urlcleanup\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"URLopener\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"FancyURLopener\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"proxy_bypass\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"parse_http_list\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"parse_keqv_list\", \"urllib2\", \"urllib.request\"),\n]\nfor attr in _urllib_request_moved_attributes:\n    setattr(Module_six_moves_urllib_request, attr.name, attr)\ndel attr\n\nModule_six_moves_urllib_request._moved_attributes = _urllib_request_moved_attributes\n\n_importer._add_module(Module_six_moves_urllib_request(__name__ + \".moves.urllib.request\"),\n                      \"moves.urllib_request\", \"moves.urllib.request\")\n\n\nclass Module_six_moves_urllib_response(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects in six.moves.urllib_response\"\"\"\n\n\n_urllib_response_moved_attributes = [\n    MovedAttribute(\"addbase\", \"urllib\", \"urllib.response\"),\n    MovedAttribute(\"addclosehook\", \"urllib\", \"urllib.response\"),\n    MovedAttribute(\"addinfo\", \"urllib\", \"urllib.response\"),\n    MovedAttribute(\"addinfourl\", \"urllib\", \"urllib.response\"),\n]\nfor attr in _urllib_response_moved_attributes:\n    setattr(Module_six_moves_urllib_response, attr.name, attr)\ndel attr\n\nModule_six_moves_urllib_response._moved_attributes = _urllib_response_moved_attributes\n\n_importer._add_module(Module_six_moves_urllib_response(__name__ + \".moves.urllib.response\"),\n                      \"moves.urllib_response\", \"moves.urllib.response\")\n\n\nclass Module_six_moves_urllib_robotparser(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects in six.moves.urllib_robotparser\"\"\"\n\n\n_urllib_robotparser_moved_attributes = [\n    MovedAttribute(\"RobotFileParser\", \"robotparser\", \"urllib.robotparser\"),\n]\nfor attr in _urllib_robotparser_moved_attributes:\n    setattr(Module_six_moves_urllib_robotparser, attr.name, attr)\ndel attr\n\nModule_six_moves_urllib_robotparser._moved_attributes = _urllib_robotparser_moved_attributes\n\n_importer._add_module(Module_six_moves_urllib_robotparser(__name__ + \".moves.urllib.robotparser\"),\n                      \"moves.urllib_robotparser\", \"moves.urllib.robotparser\")\n\n\nclass Module_six_moves_urllib(types.ModuleType):\n\n    \"\"\"Create a six.moves.urllib namespace that resembles the Python 3 namespace\"\"\"\n    __path__ = []  # mark as package\n    parse = _importer._get_module(\"moves.urllib_parse\")\n    error = _importer._get_module(\"moves.urllib_error\")\n    request = _importer._get_module(\"moves.urllib_request\")\n    response = _importer._get_module(\"moves.urllib_response\")\n    robotparser = _importer._get_module(\"moves.urllib_robotparser\")\n\n    def __dir__(self):\n        return ['parse', 'error', 'request', 'response', 'robotparser']\n\n_importer._add_module(Module_six_moves_urllib(__name__ + \".moves.urllib\"),\n                      \"moves.urllib\")\n\n\ndef add_move(move):\n    \"\"\"Add an item to six.moves.\"\"\"\n    setattr(_MovedItems, move.name, move)\n\n\ndef remove_move(name):\n    \"\"\"Remove item from six.moves.\"\"\"\n    try:\n        delattr(_MovedItems, name)\n    except AttributeError:\n        try:\n            del moves.__dict__[name]\n        except KeyError:\n            raise AttributeError(\"no such move, %r\" % (name,))\n\n\nif PY3:\n    _meth_func = \"__func__\"\n    _meth_self = \"__self__\"\n\n    _func_closure = \"__closure__\"\n    _func_code = \"__code__\"\n    _func_defaults = \"__defaults__\"\n    _func_globals = \"__globals__\"\nelse:\n    _meth_func = \"im_func\"\n    _meth_self = \"im_self\"\n\n    _func_closure = \"func_closure\"\n    _func_code = \"func_code\"\n    _func_defaults = \"func_defaults\"\n    _func_globals = \"func_globals\"\n\n\ntry:\n    advance_iterator = next\nexcept NameError:\n    def advance_iterator(it):\n        return it.next()\nnext = advance_iterator\n\n\ntry:\n    callable = callable\nexcept NameError:\n    def callable(obj):\n        return any(\"__call__\" in klass.__dict__ for klass in type(obj).__mro__)\n\n\nif PY3:\n    def get_unbound_function(unbound):\n        return unbound\n\n    create_bound_method = types.MethodType\n\n    def create_unbound_method(func, cls):\n        return func\n\n    Iterator = object\nelse:\n    def get_unbound_function(unbound):\n        return unbound.im_func\n\n    def create_bound_method(func, obj):\n        return types.MethodType(func, obj, obj.__class__)\n\n    def create_unbound_method(func, cls):\n        return types.MethodType(func, None, cls)\n\n    class Iterator(object):\n\n        def next(self):\n            return type(self).__next__(self)\n\n    callable = callable\n_add_doc(get_unbound_function,\n         \"\"\"Get the function out of a possibly unbound function\"\"\")\n\n\nget_method_function = operator.attrgetter(_meth_func)\nget_method_self = operator.attrgetter(_meth_self)\nget_function_closure = operator.attrgetter(_func_closure)\nget_function_code = operator.attrgetter(_func_code)\nget_function_defaults = operator.attrgetter(_func_defaults)\nget_function_globals = operator.attrgetter(_func_globals)\n\n\nif PY3:\n    def iterkeys(d, **kw):\n        return iter(d.keys(**kw))\n\n    def itervalues(d, **kw):\n        return iter(d.values(**kw))\n\n    def iteritems(d, **kw):\n        return iter(d.items(**kw))\n\n    def iterlists(d, **kw):\n        return iter(d.lists(**kw))\n\n    viewkeys = operator.methodcaller(\"keys\")\n\n    viewvalues = operator.methodcaller(\"values\")\n\n    viewitems = operator.methodcaller(\"items\")\nelse:\n    def iterkeys(d, **kw):\n        return d.iterkeys(**kw)\n\n    def itervalues(d, **kw):\n        return d.itervalues(**kw)\n\n    def iteritems(d, **kw):\n        return d.iteritems(**kw)\n\n    def iterlists(d, **kw):\n        return d.iterlists(**kw)\n\n    viewkeys = operator.methodcaller(\"viewkeys\")\n\n    viewvalues = operator.methodcaller(\"viewvalues\")\n\n    viewitems = operator.methodcaller(\"viewitems\")\n\n_add_doc(iterkeys, \"Return an iterator over the keys of a dictionary.\")\n_add_doc(itervalues, \"Return an iterator over the values of a dictionary.\")\n_add_doc(iteritems,\n         \"Return an iterator over the (key, value) pairs of a dictionary.\")\n_add_doc(iterlists,\n         \"Return an iterator over the (key, [values]) pairs of a dictionary.\")\n\n\nif PY3:\n    def b(s):\n        return s.encode(\"latin-1\")\n\n    def u(s):\n        return s\n    unichr = chr\n    import struct\n    int2byte = struct.Struct(\">B\").pack\n    del struct\n    byte2int = operator.itemgetter(0)\n    indexbytes = operator.getitem\n    iterbytes = iter\n    import io\n    StringIO = io.StringIO\n    BytesIO = io.BytesIO\n    del io\n    _assertCountEqual = \"assertCountEqual\"\n    if sys.version_info[1] <= 1:\n        _assertRaisesRegex = \"assertRaisesRegexp\"\n        _assertRegex = \"assertRegexpMatches\"\n        _assertNotRegex = \"assertNotRegexpMatches\"\n    else:\n        _assertRaisesRegex = \"assertRaisesRegex\"\n        _assertRegex = \"assertRegex\"\n        _assertNotRegex = \"assertNotRegex\"\nelse:\n    def b(s):\n        return s\n    # Workaround for standalone backslash\n\n    def u(s):\n        return unicode(s.replace(r'\\\\', r'\\\\\\\\'), \"unicode_escape\")\n    unichr = unichr\n    int2byte = chr\n\n    def byte2int(bs):\n        return ord(bs[0])\n\n    def indexbytes(buf, i):\n        return ord(buf[i])\n    iterbytes = functools.partial(itertools.imap, ord)\n    import StringIO\n    StringIO = BytesIO = StringIO.StringIO\n    _assertCountEqual = \"assertItemsEqual\"\n    _assertRaisesRegex = \"assertRaisesRegexp\"\n    _assertRegex = \"assertRegexpMatches\"\n    _assertNotRegex = \"assertNotRegexpMatches\"\n_add_doc(b, \"\"\"Byte literal\"\"\")\n_add_doc(u, \"\"\"Text literal\"\"\")\n\n\ndef assertCountEqual(self, *args, **kwargs):\n    return getattr(self, _assertCountEqual)(*args, **kwargs)\n\n\ndef assertRaisesRegex(self, *args, **kwargs):\n    return getattr(self, _assertRaisesRegex)(*args, **kwargs)\n\n\ndef assertRegex(self, *args, **kwargs):\n    return getattr(self, _assertRegex)(*args, **kwargs)\n\n\ndef assertNotRegex(self, *args, **kwargs):\n    return getattr(self, _assertNotRegex)(*args, **kwargs)\n\n\nif PY3:\n    exec_ = getattr(moves.builtins, \"exec\")\n\n    def reraise(tp, value, tb=None):\n        try:\n            if value is None:\n                value = tp()\n            if value.__traceback__ is not tb:\n                raise value.with_traceback(tb)\n            raise value\n        finally:\n            value = None\n            tb = None\n\nelse:\n    def exec_(_code_, _globs_=None, _locs_=None):\n        \"\"\"Execute code in a namespace.\"\"\"\n        if _globs_ is None:\n            frame = sys._getframe(1)\n            _globs_ = frame.f_globals\n            if _locs_ is None:\n                _locs_ = frame.f_locals\n            del frame\n        elif _locs_ is None:\n            _locs_ = _globs_\n        exec(\"\"\"exec _code_ in _globs_, _locs_\"\"\")\n\n    exec_(\"\"\"def reraise(tp, value, tb=None):\n    try:\n        raise tp, value, tb\n    finally:\n        tb = None\n\"\"\")\n\n\nif sys.version_info[:2] > (3,):\n    exec_(\"\"\"def raise_from(value, from_value):\n    try:\n        raise value from from_value\n    finally:\n        value = None\n\"\"\")\nelse:\n    def raise_from(value, from_value):\n        raise value\n\n\nprint_ = getattr(moves.builtins, \"print\", None)\nif print_ is None:\n    def print_(*args, **kwargs):\n        \"\"\"The new-style print function for Python 2.4 and 2.5.\"\"\"\n        fp = kwargs.pop(\"file\", sys.stdout)\n        if fp is None:\n            return\n\n        def write(data):\n            if not isinstance(data, basestring):\n                data = str(data)\n            # If the file has an encoding, encode unicode with it.\n            if (isinstance(fp, file) and\n                    isinstance(data, unicode) and\n                    fp.encoding is not None):\n                errors = getattr(fp, \"errors\", None)\n                if errors is None:\n                    errors = \"strict\"\n                data = data.encode(fp.encoding, errors)\n            fp.write(data)\n        want_unicode = False\n        sep = kwargs.pop(\"sep\", None)\n        if sep is not None:\n            if isinstance(sep, unicode):\n                want_unicode = True\n            elif not isinstance(sep, str):\n                raise TypeError(\"sep must be None or a string\")\n        end = kwargs.pop(\"end\", None)\n        if end is not None:\n            if isinstance(end, unicode):\n                want_unicode = True\n            elif not isinstance(end, str):\n                raise TypeError(\"end must be None or a string\")\n        if kwargs:\n            raise TypeError(\"invalid keyword arguments to print()\")\n        if not want_unicode:\n            for arg in args:\n                if isinstance(arg, unicode):\n                    want_unicode = True\n                    break\n        if want_unicode:\n            newline = unicode(\"\\n\")\n            space = unicode(\" \")\n        else:\n            newline = \"\\n\"\n            space = \" \"\n        if sep is None:\n            sep = space\n        if end is None:\n            end = newline\n        for i, arg in enumerate(args):\n            if i:\n                write(sep)\n            write(arg)\n        write(end)\nif sys.version_info[:2] < (3, 3):\n    _print = print_\n\n    def print_(*args, **kwargs):\n        fp = kwargs.get(\"file\", sys.stdout)\n        flush = kwargs.pop(\"flush\", False)\n        _print(*args, **kwargs)\n        if flush and fp is not None:\n            fp.flush()\n\n_add_doc(reraise, \"\"\"Reraise an exception.\"\"\")\n\nif sys.version_info[0:2] < (3, 4):\n    # This does exactly the same what the :func:`py3:functools.update_wrapper`\n    # function does on Python versions after 3.2. It sets the ``__wrapped__``\n    # attribute on ``wrapper`` object and it doesn't raise an error if any of\n    # the attributes mentioned in ``assigned`` and ``updated`` are missing on\n    # ``wrapped`` object.\n    def _update_wrapper(wrapper, wrapped,\n                        assigned=functools.WRAPPER_ASSIGNMENTS,\n                        updated=functools.WRAPPER_UPDATES):\n        for attr in assigned:\n            try:\n                value = getattr(wrapped, attr)\n            except AttributeError:\n                continue\n            else:\n                setattr(wrapper, attr, value)\n        for attr in updated:\n            getattr(wrapper, attr).update(getattr(wrapped, attr, {}))\n        wrapper.__wrapped__ = wrapped\n        return wrapper\n    _update_wrapper.__doc__ = functools.update_wrapper.__doc__\n\n    def wraps(wrapped, assigned=functools.WRAPPER_ASSIGNMENTS,\n              updated=functools.WRAPPER_UPDATES):\n        return functools.partial(_update_wrapper, wrapped=wrapped,\n                                 assigned=assigned, updated=updated)\n    wraps.__doc__ = functools.wraps.__doc__\n\nelse:\n    wraps = functools.wraps\n\n\ndef with_metaclass(meta, *bases):\n    \"\"\"Create a base class with a metaclass.\"\"\"\n    # This requires a bit of explanation: the basic idea is to make a dummy\n    # metaclass for one level of class instantiation that replaces itself with\n    # the actual metaclass.\n    class metaclass(type):\n\n        def __new__(cls, name, this_bases, d):\n            if sys.version_info[:2] >= (3, 7):\n                # This version introduced PEP 560 that requires a bit\n                # of extra care (we mimic what is done by __build_class__).\n                resolved_bases = types.resolve_bases(bases)\n                if resolved_bases is not bases:\n                    d['__orig_bases__'] = bases\n            else:\n                resolved_bases = bases\n            return meta(name, resolved_bases, d)\n\n        @classmethod\n        def __prepare__(cls, name, this_bases):\n            return meta.__prepare__(name, bases)\n    return type.__new__(metaclass, 'temporary_class', (), {})\n\n\ndef add_metaclass(metaclass):\n    \"\"\"Class decorator for creating a class with a metaclass.\"\"\"\n    def wrapper(cls):\n        orig_vars = cls.__dict__.copy()\n        slots = orig_vars.get('__slots__')\n        if slots is not None:\n            if isinstance(slots, str):\n                slots = [slots]\n            for slots_var in slots:\n                orig_vars.pop(slots_var)\n        orig_vars.pop('__dict__', None)\n        orig_vars.pop('__weakref__', None)\n        if hasattr(cls, '__qualname__'):\n            orig_vars['__qualname__'] = cls.__qualname__\n        return metaclass(cls.__name__, cls.__bases__, orig_vars)\n    return wrapper\n\n\ndef ensure_binary(s, encoding='utf-8', errors='strict'):\n    \"\"\"Coerce **s** to six.binary_type.\n\n    For Python 2:\n      - `unicode` -> encoded to `str`\n      - `str` -> `str`\n\n    For Python 3:\n      - `str` -> encoded to `bytes`\n      - `bytes` -> `bytes`\n    \"\"\"\n    if isinstance(s, binary_type):\n        return s\n    if isinstance(s, text_type):\n        return s.encode(encoding, errors)\n    raise TypeError(\"not expecting type '%s'\" % type(s))\n\n\ndef ensure_str(s, encoding='utf-8', errors='strict'):\n    \"\"\"Coerce *s* to `str`.\n\n    For Python 2:\n      - `unicode` -> encoded to `str`\n      - `str` -> `str`\n\n    For Python 3:\n      - `str` -> `str`\n      - `bytes` -> decoded to `str`\n    \"\"\"\n    # Optimization: Fast return for the common case.\n    if type(s) is str:\n        return s\n    if PY2 and isinstance(s, text_type):\n        return s.encode(encoding, errors)\n    elif PY3 and isinstance(s, binary_type):\n        return s.decode(encoding, errors)\n    elif not isinstance(s, (text_type, binary_type)):\n        raise TypeError(\"not expecting type '%s'\" % type(s))\n    return s\n\n\ndef ensure_text(s, encoding='utf-8', errors='strict'):\n    \"\"\"Coerce *s* to six.text_type.\n\n    For Python 2:\n      - `unicode` -> `unicode`\n      - `str` -> `unicode`\n\n    For Python 3:\n      - `str` -> `str`\n      - `bytes` -> decoded to `str`\n    \"\"\"\n    if isinstance(s, binary_type):\n        return s.decode(encoding, errors)\n    elif isinstance(s, text_type):\n        return s\n    else:\n        raise TypeError(\"not expecting type '%s'\" % type(s))\n\n\ndef python_2_unicode_compatible(klass):\n    \"\"\"\n    A class decorator that defines __unicode__ and __str__ methods under Python 2.\n    Under Python 3 it does nothing.\n\n    To support Python 2 and 3 with a single code base, define a __str__ method\n    returning text and apply this decorator to the class.\n    \"\"\"\n    if PY2:\n        if '__str__' not in klass.__dict__:\n            raise ValueError(\"@python_2_unicode_compatible cannot be applied \"\n                             \"to %s because it doesn't define __str__().\" %\n                             klass.__name__)\n        klass.__unicode__ = klass.__str__\n        klass.__str__ = lambda self: self.__unicode__().encode('utf-8')\n    return klass\n\n\n# Complete the moves implementation.\n# This code is at the end of this module to speed up module loading.\n# Turn this module into a package.\n__path__ = []  # required for PEP 302 and PEP 451\n__package__ = __name__  # see PEP 366 @ReservedAssignment\nif globals().get(\"__spec__\") is not None:\n    __spec__.submodule_search_locations = []  # PEP 451 @UndefinedVariable\n# Remove other six meta path importers, since they cause problems. This can\n# happen if six is removed from sys.modules and then reloaded. (Setuptools does\n# this for some reason.)\nif sys.meta_path:\n    for i, importer in enumerate(sys.meta_path):\n        # Here's some real nastiness: Another \"instance\" of the six module might\n        # be floating around. Therefore, we can't use isinstance() to check for\n        # the six meta path importer, since the other six instance will have\n        # inserted an importer with different class.\n        if (type(importer).__name__ == \"_SixMetaPathImporter\" and\n                importer.name == __name__):\n            del sys.meta_path[i]\n            break\n    del i, importer\n# Finally, add the importer to the meta path import hook.\nsys.meta_path.append(_importer)\n", "botocore/vendored/__init__.py": "", "botocore/vendored/requests/exceptions.py": "# -*- coding: utf-8 -*-\n\n\"\"\"\nrequests.exceptions\n~~~~~~~~~~~~~~~~~~~\n\nThis module contains the set of Requests' exceptions.\n\n\"\"\"\nfrom .packages.urllib3.exceptions import HTTPError as BaseHTTPError\n\n\nclass RequestException(IOError):\n    \"\"\"There was an ambiguous exception that occurred while handling your\n    request.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Initialize RequestException with `request` and `response` objects.\n        \"\"\"\n        response = kwargs.pop('response', None)\n        self.response = response\n        self.request = kwargs.pop('request', None)\n        if (response is not None and not self.request and\n                hasattr(response, 'request')):\n            self.request = self.response.request\n        super(RequestException, self).__init__(*args, **kwargs)\n\n\nclass HTTPError(RequestException):\n    \"\"\"An HTTP error occurred.\"\"\"\n\n\nclass ConnectionError(RequestException):\n    \"\"\"A Connection error occurred.\"\"\"\n\n\nclass ProxyError(ConnectionError):\n    \"\"\"A proxy error occurred.\"\"\"\n\n\nclass SSLError(ConnectionError):\n    \"\"\"An SSL error occurred.\"\"\"\n\n\nclass Timeout(RequestException):\n    \"\"\"The request timed out.\n\n    Catching this error will catch both\n    :exc:`~requests.exceptions.ConnectTimeout` and\n    :exc:`~requests.exceptions.ReadTimeout` errors.\n    \"\"\"\n\n\nclass ConnectTimeout(ConnectionError, Timeout):\n    \"\"\"The request timed out while trying to connect to the remote server.\n\n    Requests that produced this error are safe to retry.\n    \"\"\"\n\n\nclass ReadTimeout(Timeout):\n    \"\"\"The server did not send any data in the allotted amount of time.\"\"\"\n\n\nclass URLRequired(RequestException):\n    \"\"\"A valid URL is required to make a request.\"\"\"\n\n\nclass TooManyRedirects(RequestException):\n    \"\"\"Too many redirects.\"\"\"\n\n\nclass MissingSchema(RequestException, ValueError):\n    \"\"\"The URL schema (e.g. http or https) is missing.\"\"\"\n\n\nclass InvalidSchema(RequestException, ValueError):\n    \"\"\"See defaults.py for valid schemas.\"\"\"\n\n\nclass InvalidURL(RequestException, ValueError):\n    \"\"\" The URL provided was somehow invalid. \"\"\"\n\n\nclass ChunkedEncodingError(RequestException):\n    \"\"\"The server declared chunked encoding but sent an invalid chunk.\"\"\"\n\n\nclass ContentDecodingError(RequestException, BaseHTTPError):\n    \"\"\"Failed to decode response content\"\"\"\n\n\nclass StreamConsumedError(RequestException, TypeError):\n    \"\"\"The content for this response was already consumed\"\"\"\n\n\nclass RetryError(RequestException):\n    \"\"\"Custom retries logic failed\"\"\"\n", "botocore/vendored/requests/__init__.py": "# -*- coding: utf-8 -*-\n\n#   __\n#  /__)  _  _     _   _ _/   _\n# / (   (- (/ (/ (- _)  /  _)\n#          /\nfrom .exceptions import (\n    RequestException, Timeout, URLRequired,\n    TooManyRedirects, HTTPError, ConnectionError\n)\n", "botocore/vendored/requests/packages/__init__.py": "from __future__ import absolute_import\n\nfrom . import urllib3\n", "botocore/vendored/requests/packages/urllib3/exceptions.py": "\n## Base Exceptions\n\nclass HTTPError(Exception):\n    \"Base exception used by this module.\"\n    pass\n\nclass HTTPWarning(Warning):\n    \"Base warning used by this module.\"\n    pass\n\n\n\nclass PoolError(HTTPError):\n    \"Base exception for errors caused within a pool.\"\n    def __init__(self, pool, message):\n        self.pool = pool\n        HTTPError.__init__(self, \"%s: %s\" % (pool, message))\n\n    def __reduce__(self):\n        # For pickling purposes.\n        return self.__class__, (None, None)\n\n\nclass RequestError(PoolError):\n    \"Base exception for PoolErrors that have associated URLs.\"\n    def __init__(self, pool, url, message):\n        self.url = url\n        PoolError.__init__(self, pool, message)\n\n    def __reduce__(self):\n        # For pickling purposes.\n        return self.__class__, (None, self.url, None)\n\n\nclass SSLError(HTTPError):\n    \"Raised when SSL certificate fails in an HTTPS connection.\"\n    pass\n\n\nclass ProxyError(HTTPError):\n    \"Raised when the connection to a proxy fails.\"\n    pass\n\n\nclass DecodeError(HTTPError):\n    \"Raised when automatic decoding based on Content-Type fails.\"\n    pass\n\n\nclass ProtocolError(HTTPError):\n    \"Raised when something unexpected happens mid-request/response.\"\n    pass\n\n\n#: Renamed to ProtocolError but aliased for backwards compatibility.\nConnectionError = ProtocolError\n\n\n## Leaf Exceptions\n\nclass MaxRetryError(RequestError):\n    \"\"\"Raised when the maximum number of retries is exceeded.\n\n    :param pool: The connection pool\n    :type pool: :class:`~urllib3.connectionpool.HTTPConnectionPool`\n    :param string url: The requested Url\n    :param exceptions.Exception reason: The underlying error\n\n    \"\"\"\n\n    def __init__(self, pool, url, reason=None):\n        self.reason = reason\n\n        message = \"Max retries exceeded with url: %s (Caused by %r)\" % (\n            url, reason)\n\n        RequestError.__init__(self, pool, url, message)\n\n\nclass HostChangedError(RequestError):\n    \"Raised when an existing pool gets a request for a foreign host.\"\n\n    def __init__(self, pool, url, retries=3):\n        message = \"Tried to open a foreign host with url: %s\" % url\n        RequestError.__init__(self, pool, url, message)\n        self.retries = retries\n\n\nclass TimeoutStateError(HTTPError):\n    \"\"\" Raised when passing an invalid state to a timeout \"\"\"\n    pass\n\n\nclass TimeoutError(HTTPError):\n    \"\"\" Raised when a socket timeout error occurs.\n\n    Catching this error will catch both :exc:`ReadTimeoutErrors\n    <ReadTimeoutError>` and :exc:`ConnectTimeoutErrors <ConnectTimeoutError>`.\n    \"\"\"\n    pass\n\n\nclass ReadTimeoutError(TimeoutError, RequestError):\n    \"Raised when a socket timeout occurs while receiving data from a server\"\n    pass\n\n\n# This timeout error does not have a URL attached and needs to inherit from the\n# base HTTPError\nclass ConnectTimeoutError(TimeoutError):\n    \"Raised when a socket timeout occurs while connecting to a server\"\n    pass\n\n\nclass EmptyPoolError(PoolError):\n    \"Raised when a pool runs out of connections and no more are allowed.\"\n    pass\n\n\nclass ClosedPoolError(PoolError):\n    \"Raised when a request enters a pool after the pool has been closed.\"\n    pass\n\n\nclass LocationValueError(ValueError, HTTPError):\n    \"Raised when there is something wrong with a given URL input.\"\n    pass\n\n\nclass LocationParseError(LocationValueError):\n    \"Raised when get_host or similar fails to parse the URL input.\"\n\n    def __init__(self, location):\n        message = \"Failed to parse: %s\" % location\n        HTTPError.__init__(self, message)\n\n        self.location = location\n\n\nclass ResponseError(HTTPError):\n    \"Used as a container for an error reason supplied in a MaxRetryError.\"\n    GENERIC_ERROR = 'too many error responses'\n    SPECIFIC_ERROR = 'too many {status_code} error responses'\n\n\nclass SecurityWarning(HTTPWarning):\n    \"Warned when perfoming security reducing actions\"\n    pass\n\n\nclass InsecureRequestWarning(SecurityWarning):\n    \"Warned when making an unverified HTTPS request.\"\n    pass\n\n\nclass SystemTimeWarning(SecurityWarning):\n    \"Warned when system time is suspected to be wrong\"\n    pass\n\n\nclass InsecurePlatformWarning(SecurityWarning):\n    \"Warned when certain SSL configuration is not available on a platform.\"\n    pass\n\n\nclass ResponseNotChunked(ProtocolError, ValueError):\n    \"Response needs to be chunked in order to read it as chunks.\"\n    pass\n", "botocore/vendored/requests/packages/urllib3/__init__.py": "\"\"\"\nurllib3 - Thread-safe connection pooling and re-using.\n\"\"\"\n\n__author__ = 'Andrey Petrov (andrey.petrov@shazow.net)'\n__license__ = 'MIT'\n__version__ = ''\n\n\nfrom . import exceptions\n"}