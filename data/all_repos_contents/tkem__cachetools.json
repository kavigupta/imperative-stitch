{"setup.py": "from setuptools import setup\n\nsetup()\n", "src/cachetools/func.py": "\"\"\"`functools.lru_cache` compatible memoizing function decorators.\"\"\"\n\n__all__ = (\"fifo_cache\", \"lfu_cache\", \"lru_cache\", \"mru_cache\", \"rr_cache\", \"ttl_cache\")\n\nimport math\nimport random\nimport time\n\ntry:\n    from threading import RLock\nexcept ImportError:  # pragma: no cover\n    from dummy_threading import RLock\n\nfrom . import FIFOCache, LFUCache, LRUCache, MRUCache, RRCache, TTLCache\nfrom . import cached\nfrom . import keys\n\n\nclass _UnboundTTLCache(TTLCache):\n    def __init__(self, ttl, timer):\n        TTLCache.__init__(self, math.inf, ttl, timer)\n\n    @property\n    def maxsize(self):\n        return None\n\n\ndef _cache(cache, maxsize, typed):\n    def decorator(func):\n        key = keys.typedkey if typed else keys.hashkey\n        wrapper = cached(cache=cache, key=key, lock=RLock(), info=True)(func)\n        wrapper.cache_parameters = lambda: {\"maxsize\": maxsize, \"typed\": typed}\n        return wrapper\n\n    return decorator\n\n\ndef fifo_cache(maxsize=128, typed=False):\n    \"\"\"Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a First In First Out (FIFO)\n    algorithm.\n\n    \"\"\"\n    if maxsize is None:\n        return _cache({}, None, typed)\n    elif callable(maxsize):\n        return _cache(FIFOCache(128), 128, typed)(maxsize)\n    else:\n        return _cache(FIFOCache(maxsize), maxsize, typed)\n\n\ndef lfu_cache(maxsize=128, typed=False):\n    \"\"\"Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n\n    \"\"\"\n    if maxsize is None:\n        return _cache({}, None, typed)\n    elif callable(maxsize):\n        return _cache(LFUCache(128), 128, typed)(maxsize)\n    else:\n        return _cache(LFUCache(maxsize), maxsize, typed)\n\n\ndef lru_cache(maxsize=128, typed=False):\n    \"\"\"Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n\n    \"\"\"\n    if maxsize is None:\n        return _cache({}, None, typed)\n    elif callable(maxsize):\n        return _cache(LRUCache(128), 128, typed)(maxsize)\n    else:\n        return _cache(LRUCache(maxsize), maxsize, typed)\n\n\ndef mru_cache(maxsize=128, typed=False):\n    \"\"\"Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU)\n    algorithm.\n    \"\"\"\n    if maxsize is None:\n        return _cache({}, None, typed)\n    elif callable(maxsize):\n        return _cache(MRUCache(128), 128, typed)(maxsize)\n    else:\n        return _cache(MRUCache(maxsize), maxsize, typed)\n\n\ndef rr_cache(maxsize=128, choice=random.choice, typed=False):\n    \"\"\"Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Random Replacement (RR)\n    algorithm.\n\n    \"\"\"\n    if maxsize is None:\n        return _cache({}, None, typed)\n    elif callable(maxsize):\n        return _cache(RRCache(128, choice), 128, typed)(maxsize)\n    else:\n        return _cache(RRCache(maxsize, choice), maxsize, typed)\n\n\ndef ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    if maxsize is None:\n        return _cache(_UnboundTTLCache(ttl, timer), None, typed)\n    elif callable(maxsize):\n        return _cache(TTLCache(128, ttl, timer), 128, typed)(maxsize)\n    else:\n        return _cache(TTLCache(maxsize, ttl, timer), maxsize, typed)\n", "src/cachetools/keys.py": "\"\"\"Key functions for memoizing decorators.\"\"\"\n\n__all__ = (\"hashkey\", \"methodkey\", \"typedkey\")\n\n\nclass _HashedTuple(tuple):\n    \"\"\"A tuple that ensures that hash() will be called no more than once\n    per element, since cache decorators will hash the key multiple\n    times on a cache miss.  See also _HashedSeq in the standard\n    library functools implementation.\n\n    \"\"\"\n\n    __hashvalue = None\n\n    def __hash__(self, hash=tuple.__hash__):\n        hashvalue = self.__hashvalue\n        if hashvalue is None:\n            self.__hashvalue = hashvalue = hash(self)\n        return hashvalue\n\n    def __add__(self, other, add=tuple.__add__):\n        return _HashedTuple(add(self, other))\n\n    def __radd__(self, other, add=tuple.__add__):\n        return _HashedTuple(add(other, self))\n\n    def __getstate__(self):\n        return {}\n\n\n# used for separating keyword arguments; we do not use an object\n# instance here so identity is preserved when pickling/unpickling\n_kwmark = (_HashedTuple,)\n\n\ndef hashkey(*args, **kwargs):\n    \"\"\"Return a cache key for the specified hashable arguments.\"\"\"\n\n    if kwargs:\n        return _HashedTuple(args + sum(sorted(kwargs.items()), _kwmark))\n    else:\n        return _HashedTuple(args)\n\n\ndef methodkey(self, *args, **kwargs):\n    \"\"\"Return a cache key for use with cached methods.\"\"\"\n    return hashkey(*args, **kwargs)\n\n\ndef typedkey(*args, **kwargs):\n    \"\"\"Return a typed cache key for the specified hashable arguments.\"\"\"\n\n    key = hashkey(*args, **kwargs)\n    key += tuple(type(v) for v in args)\n    key += tuple(type(v) for _, v in sorted(kwargs.items()))\n    return key\n", "src/cachetools/__init__.py": "\"\"\"Extensible memoizing collections and decorators.\"\"\"\n\n__all__ = (\n    \"Cache\",\n    \"FIFOCache\",\n    \"LFUCache\",\n    \"LRUCache\",\n    \"MRUCache\",\n    \"RRCache\",\n    \"TLRUCache\",\n    \"TTLCache\",\n    \"cached\",\n    \"cachedmethod\",\n)\n\n__version__ = \"5.3.3\"\n\nimport collections\nimport collections.abc\nimport functools\nimport heapq\nimport random\nimport time\n\nfrom . import keys\n\n\nclass _DefaultSize:\n\n    __slots__ = ()\n\n    def __getitem__(self, _):\n        return 1\n\n    def __setitem__(self, _, value):\n        assert value == 1\n\n    def pop(self, _):\n        return 1\n\n\nclass Cache(collections.abc.MutableMapping):\n    \"\"\"Mutable mapping to serve as a simple cache or cache base class.\"\"\"\n\n    __marker = object()\n\n    __size = _DefaultSize()\n\n    def __init__(self, maxsize, getsizeof=None):\n        if getsizeof:\n            self.getsizeof = getsizeof\n        if self.getsizeof is not Cache.getsizeof:\n            self.__size = dict()\n        self.__data = dict()\n        self.__currsize = 0\n        self.__maxsize = maxsize\n\n    def __repr__(self):\n        return \"%s(%s, maxsize=%r, currsize=%r)\" % (\n            self.__class__.__name__,\n            repr(self.__data),\n            self.__maxsize,\n            self.__currsize,\n        )\n\n    def __getitem__(self, key):\n        try:\n            return self.__data[key]\n        except KeyError:\n            return self.__missing__(key)\n\n    def __setitem__(self, key, value):\n        maxsize = self.__maxsize\n        size = self.getsizeof(value)\n        if size > maxsize:\n            raise ValueError(\"value too large\")\n        if key not in self.__data or self.__size[key] < size:\n            while self.__currsize + size > maxsize:\n                self.popitem()\n        if key in self.__data:\n            diffsize = size - self.__size[key]\n        else:\n            diffsize = size\n        self.__data[key] = value\n        self.__size[key] = size\n        self.__currsize += diffsize\n\n    def __delitem__(self, key):\n        size = self.__size.pop(key)\n        del self.__data[key]\n        self.__currsize -= size\n\n    def __contains__(self, key):\n        return key in self.__data\n\n    def __missing__(self, key):\n        raise KeyError(key)\n\n    def __iter__(self):\n        return iter(self.__data)\n\n    def __len__(self):\n        return len(self.__data)\n\n    def get(self, key, default=None):\n        if key in self:\n            return self[key]\n        else:\n            return default\n\n    def pop(self, key, default=__marker):\n        if key in self:\n            value = self[key]\n            del self[key]\n        elif default is self.__marker:\n            raise KeyError(key)\n        else:\n            value = default\n        return value\n\n    def setdefault(self, key, default=None):\n        if key in self:\n            value = self[key]\n        else:\n            self[key] = value = default\n        return value\n\n    @property\n    def maxsize(self):\n        \"\"\"The maximum size of the cache.\"\"\"\n        return self.__maxsize\n\n    @property\n    def currsize(self):\n        \"\"\"The current size of the cache.\"\"\"\n        return self.__currsize\n\n    @staticmethod\n    def getsizeof(value):\n        \"\"\"Return the size of a cache element's value.\"\"\"\n        return 1\n\n\nclass FIFOCache(Cache):\n    \"\"\"First In First Out (FIFO) cache implementation.\"\"\"\n\n    def __init__(self, maxsize, getsizeof=None):\n        Cache.__init__(self, maxsize, getsizeof)\n        self.__order = collections.OrderedDict()\n\n    def __setitem__(self, key, value, cache_setitem=Cache.__setitem__):\n        cache_setitem(self, key, value)\n        try:\n            self.__order.move_to_end(key)\n        except KeyError:\n            self.__order[key] = None\n\n    def __delitem__(self, key, cache_delitem=Cache.__delitem__):\n        cache_delitem(self, key)\n        del self.__order[key]\n\n    def popitem(self):\n        \"\"\"Remove and return the `(key, value)` pair first inserted.\"\"\"\n        try:\n            key = next(iter(self.__order))\n        except StopIteration:\n            raise KeyError(\"%s is empty\" % type(self).__name__) from None\n        else:\n            return (key, self.pop(key))\n\n\nclass LFUCache(Cache):\n    \"\"\"Least Frequently Used (LFU) cache implementation.\"\"\"\n\n    def __init__(self, maxsize, getsizeof=None):\n        Cache.__init__(self, maxsize, getsizeof)\n        self.__counter = collections.Counter()\n\n    def __getitem__(self, key, cache_getitem=Cache.__getitem__):\n        value = cache_getitem(self, key)\n        if key in self:  # __missing__ may not store item\n            self.__counter[key] -= 1\n        return value\n\n    def __setitem__(self, key, value, cache_setitem=Cache.__setitem__):\n        cache_setitem(self, key, value)\n        self.__counter[key] -= 1\n\n    def __delitem__(self, key, cache_delitem=Cache.__delitem__):\n        cache_delitem(self, key)\n        del self.__counter[key]\n\n    def popitem(self):\n        \"\"\"Remove and return the `(key, value)` pair least frequently used.\"\"\"\n        try:\n            ((key, _),) = self.__counter.most_common(1)\n        except ValueError:\n            raise KeyError(\"%s is empty\" % type(self).__name__) from None\n        else:\n            return (key, self.pop(key))\n\n\nclass LRUCache(Cache):\n    \"\"\"Least Recently Used (LRU) cache implementation.\"\"\"\n\n    def __init__(self, maxsize, getsizeof=None):\n        Cache.__init__(self, maxsize, getsizeof)\n        self.__order = collections.OrderedDict()\n\n    def __getitem__(self, key, cache_getitem=Cache.__getitem__):\n        value = cache_getitem(self, key)\n        if key in self:  # __missing__ may not store item\n            self.__update(key)\n        return value\n\n    def __setitem__(self, key, value, cache_setitem=Cache.__setitem__):\n        cache_setitem(self, key, value)\n        self.__update(key)\n\n    def __delitem__(self, key, cache_delitem=Cache.__delitem__):\n        cache_delitem(self, key)\n        del self.__order[key]\n\n    def popitem(self):\n        \"\"\"Remove and return the `(key, value)` pair least recently used.\"\"\"\n        try:\n            key = next(iter(self.__order))\n        except StopIteration:\n            raise KeyError(\"%s is empty\" % type(self).__name__) from None\n        else:\n            return (key, self.pop(key))\n\n    def __update(self, key):\n        try:\n            self.__order.move_to_end(key)\n        except KeyError:\n            self.__order[key] = None\n\n\nclass MRUCache(Cache):\n    \"\"\"Most Recently Used (MRU) cache implementation.\"\"\"\n\n    def __init__(self, maxsize, getsizeof=None):\n        Cache.__init__(self, maxsize, getsizeof)\n        self.__order = collections.OrderedDict()\n\n    def __getitem__(self, key, cache_getitem=Cache.__getitem__):\n        value = cache_getitem(self, key)\n        if key in self:  # __missing__ may not store item\n            self.__update(key)\n        return value\n\n    def __setitem__(self, key, value, cache_setitem=Cache.__setitem__):\n        cache_setitem(self, key, value)\n        self.__update(key)\n\n    def __delitem__(self, key, cache_delitem=Cache.__delitem__):\n        cache_delitem(self, key)\n        del self.__order[key]\n\n    def popitem(self):\n        \"\"\"Remove and return the `(key, value)` pair most recently used.\"\"\"\n        try:\n            key = next(iter(self.__order))\n        except StopIteration:\n            raise KeyError(\"%s is empty\" % type(self).__name__) from None\n        else:\n            return (key, self.pop(key))\n\n    def __update(self, key):\n        try:\n            self.__order.move_to_end(key, last=False)\n        except KeyError:\n            self.__order[key] = None\n\n\nclass RRCache(Cache):\n    \"\"\"Random Replacement (RR) cache implementation.\"\"\"\n\n    def __init__(self, maxsize, choice=random.choice, getsizeof=None):\n        Cache.__init__(self, maxsize, getsizeof)\n        self.__choice = choice\n\n    @property\n    def choice(self):\n        \"\"\"The `choice` function used by the cache.\"\"\"\n        return self.__choice\n\n    def popitem(self):\n        \"\"\"Remove and return a random `(key, value)` pair.\"\"\"\n        try:\n            key = self.__choice(list(self))\n        except IndexError:\n            raise KeyError(\"%s is empty\" % type(self).__name__) from None\n        else:\n            return (key, self.pop(key))\n\n\nclass _TimedCache(Cache):\n    \"\"\"Base class for time aware cache implementations.\"\"\"\n\n    class _Timer:\n        def __init__(self, timer):\n            self.__timer = timer\n            self.__nesting = 0\n\n        def __call__(self):\n            if self.__nesting == 0:\n                return self.__timer()\n            else:\n                return self.__time\n\n        def __enter__(self):\n            if self.__nesting == 0:\n                self.__time = time = self.__timer()\n            else:\n                time = self.__time\n            self.__nesting += 1\n            return time\n\n        def __exit__(self, *exc):\n            self.__nesting -= 1\n\n        def __reduce__(self):\n            return _TimedCache._Timer, (self.__timer,)\n\n        def __getattr__(self, name):\n            return getattr(self.__timer, name)\n\n    def __init__(self, maxsize, timer=time.monotonic, getsizeof=None):\n        Cache.__init__(self, maxsize, getsizeof)\n        self.__timer = _TimedCache._Timer(timer)\n\n    def __repr__(self, cache_repr=Cache.__repr__):\n        with self.__timer as time:\n            self.expire(time)\n            return cache_repr(self)\n\n    def __len__(self, cache_len=Cache.__len__):\n        with self.__timer as time:\n            self.expire(time)\n            return cache_len(self)\n\n    @property\n    def currsize(self):\n        with self.__timer as time:\n            self.expire(time)\n            return super().currsize\n\n    @property\n    def timer(self):\n        \"\"\"The timer function used by the cache.\"\"\"\n        return self.__timer\n\n    def clear(self):\n        with self.__timer as time:\n            self.expire(time)\n            Cache.clear(self)\n\n    def get(self, *args, **kwargs):\n        with self.__timer:\n            return Cache.get(self, *args, **kwargs)\n\n    def pop(self, *args, **kwargs):\n        with self.__timer:\n            return Cache.pop(self, *args, **kwargs)\n\n    def setdefault(self, *args, **kwargs):\n        with self.__timer:\n            return Cache.setdefault(self, *args, **kwargs)\n\n\nclass TTLCache(_TimedCache):\n    \"\"\"LRU Cache implementation with per-item time-to-live (TTL) value.\"\"\"\n\n    class _Link:\n\n        __slots__ = (\"key\", \"expires\", \"next\", \"prev\")\n\n        def __init__(self, key=None, expires=None):\n            self.key = key\n            self.expires = expires\n\n        def __reduce__(self):\n            return TTLCache._Link, (self.key, self.expires)\n\n        def unlink(self):\n            next = self.next\n            prev = self.prev\n            prev.next = next\n            next.prev = prev\n\n    def __init__(self, maxsize, ttl, timer=time.monotonic, getsizeof=None):\n        _TimedCache.__init__(self, maxsize, timer, getsizeof)\n        self.__root = root = TTLCache._Link()\n        root.prev = root.next = root\n        self.__links = collections.OrderedDict()\n        self.__ttl = ttl\n\n    def __contains__(self, key):\n        try:\n            link = self.__links[key]  # no reordering\n        except KeyError:\n            return False\n        else:\n            return self.timer() < link.expires\n\n    def __getitem__(self, key, cache_getitem=Cache.__getitem__):\n        try:\n            link = self.__getlink(key)\n        except KeyError:\n            expired = False\n        else:\n            expired = not (self.timer() < link.expires)\n        if expired:\n            return self.__missing__(key)\n        else:\n            return cache_getitem(self, key)\n\n    def __setitem__(self, key, value, cache_setitem=Cache.__setitem__):\n        with self.timer as time:\n            self.expire(time)\n            cache_setitem(self, key, value)\n        try:\n            link = self.__getlink(key)\n        except KeyError:\n            self.__links[key] = link = TTLCache._Link(key)\n        else:\n            link.unlink()\n        link.expires = time + self.__ttl\n        link.next = root = self.__root\n        link.prev = prev = root.prev\n        prev.next = root.prev = link\n\n    def __delitem__(self, key, cache_delitem=Cache.__delitem__):\n        cache_delitem(self, key)\n        link = self.__links.pop(key)\n        link.unlink()\n        if not (self.timer() < link.expires):\n            raise KeyError(key)\n\n    def __iter__(self):\n        root = self.__root\n        curr = root.next\n        while curr is not root:\n            # \"freeze\" time for iterator access\n            with self.timer as time:\n                if time < curr.expires:\n                    yield curr.key\n            curr = curr.next\n\n    def __setstate__(self, state):\n        self.__dict__.update(state)\n        root = self.__root\n        root.prev = root.next = root\n        for link in sorted(self.__links.values(), key=lambda obj: obj.expires):\n            link.next = root\n            link.prev = prev = root.prev\n            prev.next = root.prev = link\n        self.expire(self.timer())\n\n    @property\n    def ttl(self):\n        \"\"\"The time-to-live value of the cache's items.\"\"\"\n        return self.__ttl\n\n    def expire(self, time=None):\n        \"\"\"Remove expired items from the cache.\"\"\"\n        if time is None:\n            time = self.timer()\n        root = self.__root\n        curr = root.next\n        links = self.__links\n        cache_delitem = Cache.__delitem__\n        while curr is not root and not (time < curr.expires):\n            cache_delitem(self, curr.key)\n            del links[curr.key]\n            next = curr.next\n            curr.unlink()\n            curr = next\n\n    def popitem(self):\n        \"\"\"Remove and return the `(key, value)` pair least recently used that\n        has not already expired.\n\n        \"\"\"\n        with self.timer as time:\n            self.expire(time)\n            try:\n                key = next(iter(self.__links))\n            except StopIteration:\n                raise KeyError(\"%s is empty\" % type(self).__name__) from None\n            else:\n                return (key, self.pop(key))\n\n    def __getlink(self, key):\n        value = self.__links[key]\n        self.__links.move_to_end(key)\n        return value\n\n\nclass TLRUCache(_TimedCache):\n    \"\"\"Time aware Least Recently Used (TLRU) cache implementation.\"\"\"\n\n    @functools.total_ordering\n    class _Item:\n\n        __slots__ = (\"key\", \"expires\", \"removed\")\n\n        def __init__(self, key=None, expires=None):\n            self.key = key\n            self.expires = expires\n            self.removed = False\n\n        def __lt__(self, other):\n            return self.expires < other.expires\n\n    def __init__(self, maxsize, ttu, timer=time.monotonic, getsizeof=None):\n        _TimedCache.__init__(self, maxsize, timer, getsizeof)\n        self.__items = collections.OrderedDict()\n        self.__order = []\n        self.__ttu = ttu\n\n    def __contains__(self, key):\n        try:\n            item = self.__items[key]  # no reordering\n        except KeyError:\n            return False\n        else:\n            return self.timer() < item.expires\n\n    def __getitem__(self, key, cache_getitem=Cache.__getitem__):\n        try:\n            item = self.__getitem(key)\n        except KeyError:\n            expired = False\n        else:\n            expired = not (self.timer() < item.expires)\n        if expired:\n            return self.__missing__(key)\n        else:\n            return cache_getitem(self, key)\n\n    def __setitem__(self, key, value, cache_setitem=Cache.__setitem__):\n        with self.timer as time:\n            expires = self.__ttu(key, value, time)\n            if not (time < expires):\n                return  # skip expired items\n            self.expire(time)\n            cache_setitem(self, key, value)\n        # removing an existing item would break the heap structure, so\n        # only mark it as removed for now\n        try:\n            self.__getitem(key).removed = True\n        except KeyError:\n            pass\n        self.__items[key] = item = TLRUCache._Item(key, expires)\n        heapq.heappush(self.__order, item)\n\n    def __delitem__(self, key, cache_delitem=Cache.__delitem__):\n        with self.timer as time:\n            # no self.expire() for performance reasons, e.g. self.clear() [#67]\n            cache_delitem(self, key)\n        item = self.__items.pop(key)\n        item.removed = True\n        if not (time < item.expires):\n            raise KeyError(key)\n\n    def __iter__(self):\n        for curr in self.__order:\n            # \"freeze\" time for iterator access\n            with self.timer as time:\n                if time < curr.expires and not curr.removed:\n                    yield curr.key\n\n    @property\n    def ttu(self):\n        \"\"\"The local time-to-use function used by the cache.\"\"\"\n        return self.__ttu\n\n    def expire(self, time=None):\n        \"\"\"Remove expired items from the cache.\"\"\"\n        if time is None:\n            time = self.timer()\n        items = self.__items\n        order = self.__order\n        # clean up the heap if too many items are marked as removed\n        if len(order) > len(items) * 2:\n            self.__order = order = [item for item in order if not item.removed]\n            heapq.heapify(order)\n        cache_delitem = Cache.__delitem__\n        while order and (order[0].removed or not (time < order[0].expires)):\n            item = heapq.heappop(order)\n            if not item.removed:\n                cache_delitem(self, item.key)\n                del items[item.key]\n\n    def popitem(self):\n        \"\"\"Remove and return the `(key, value)` pair least recently used that\n        has not already expired.\n\n        \"\"\"\n        with self.timer as time:\n            self.expire(time)\n            try:\n                key = next(iter(self.__items))\n            except StopIteration:\n                raise KeyError(\"%s is empty\" % self.__class__.__name__) from None\n            else:\n                return (key, self.pop(key))\n\n    def __getitem(self, key):\n        value = self.__items[key]\n        self.__items.move_to_end(key)\n        return value\n\n\n_CacheInfo = collections.namedtuple(\n    \"CacheInfo\", [\"hits\", \"misses\", \"maxsize\", \"currsize\"]\n)\n\n\ndef cached(cache, key=keys.hashkey, lock=None, info=False):\n    \"\"\"Decorator to wrap a function with a memoizing callable that saves\n    results in a cache.\n\n    \"\"\"\n\n    def decorator(func):\n        if info:\n            hits = misses = 0\n\n            if isinstance(cache, Cache):\n\n                def getinfo():\n                    nonlocal hits, misses\n                    return _CacheInfo(hits, misses, cache.maxsize, cache.currsize)\n\n            elif isinstance(cache, collections.abc.Mapping):\n\n                def getinfo():\n                    nonlocal hits, misses\n                    return _CacheInfo(hits, misses, None, len(cache))\n\n            else:\n\n                def getinfo():\n                    nonlocal hits, misses\n                    return _CacheInfo(hits, misses, 0, 0)\n\n            if cache is None:\n\n                def wrapper(*args, **kwargs):\n                    nonlocal misses\n                    misses += 1\n                    return func(*args, **kwargs)\n\n                def cache_clear():\n                    nonlocal hits, misses\n                    hits = misses = 0\n\n                cache_info = getinfo\n\n            elif lock is None:\n\n                def wrapper(*args, **kwargs):\n                    nonlocal hits, misses\n                    k = key(*args, **kwargs)\n                    try:\n                        result = cache[k]\n                        hits += 1\n                        return result\n                    except KeyError:\n                        misses += 1\n                    v = func(*args, **kwargs)\n                    try:\n                        cache[k] = v\n                    except ValueError:\n                        pass  # value too large\n                    return v\n\n                def cache_clear():\n                    nonlocal hits, misses\n                    cache.clear()\n                    hits = misses = 0\n\n                cache_info = getinfo\n\n            else:\n\n                def wrapper(*args, **kwargs):\n                    nonlocal hits, misses\n                    k = key(*args, **kwargs)\n                    try:\n                        with lock:\n                            result = cache[k]\n                            hits += 1\n                            return result\n                    except KeyError:\n                        with lock:\n                            misses += 1\n                    v = func(*args, **kwargs)\n                    # in case of a race, prefer the item already in the cache\n                    try:\n                        with lock:\n                            return cache.setdefault(k, v)\n                    except ValueError:\n                        return v  # value too large\n\n                def cache_clear():\n                    nonlocal hits, misses\n                    with lock:\n                        cache.clear()\n                        hits = misses = 0\n\n                def cache_info():\n                    with lock:\n                        return getinfo()\n\n        else:\n            if cache is None:\n\n                def wrapper(*args, **kwargs):\n                    return func(*args, **kwargs)\n\n                def cache_clear():\n                    pass\n\n            elif lock is None:\n\n                def wrapper(*args, **kwargs):\n                    k = key(*args, **kwargs)\n                    try:\n                        return cache[k]\n                    except KeyError:\n                        pass  # key not found\n                    v = func(*args, **kwargs)\n                    try:\n                        cache[k] = v\n                    except ValueError:\n                        pass  # value too large\n                    return v\n\n                def cache_clear():\n                    cache.clear()\n\n            else:\n\n                def wrapper(*args, **kwargs):\n                    k = key(*args, **kwargs)\n                    try:\n                        with lock:\n                            return cache[k]\n                    except KeyError:\n                        pass  # key not found\n                    v = func(*args, **kwargs)\n                    # in case of a race, prefer the item already in the cache\n                    try:\n                        with lock:\n                            return cache.setdefault(k, v)\n                    except ValueError:\n                        return v  # value too large\n\n                def cache_clear():\n                    with lock:\n                        cache.clear()\n\n            cache_info = None\n\n        wrapper.cache = cache\n        wrapper.cache_key = key\n        wrapper.cache_lock = lock\n        wrapper.cache_clear = cache_clear\n        wrapper.cache_info = cache_info\n\n        return functools.update_wrapper(wrapper, func)\n\n    return decorator\n\n\ndef cachedmethod(cache, key=keys.methodkey, lock=None):\n    \"\"\"Decorator to wrap a class or instance method with a memoizing\n    callable that saves results in a cache.\n\n    \"\"\"\n\n    def decorator(method):\n        if lock is None:\n\n            def wrapper(self, *args, **kwargs):\n                c = cache(self)\n                if c is None:\n                    return method(self, *args, **kwargs)\n                k = key(self, *args, **kwargs)\n                try:\n                    return c[k]\n                except KeyError:\n                    pass  # key not found\n                v = method(self, *args, **kwargs)\n                try:\n                    c[k] = v\n                except ValueError:\n                    pass  # value too large\n                return v\n\n            def clear(self):\n                c = cache(self)\n                if c is not None:\n                    c.clear()\n\n        else:\n\n            def wrapper(self, *args, **kwargs):\n                c = cache(self)\n                if c is None:\n                    return method(self, *args, **kwargs)\n                k = key(self, *args, **kwargs)\n                try:\n                    with lock(self):\n                        return c[k]\n                except KeyError:\n                    pass  # key not found\n                v = method(self, *args, **kwargs)\n                # in case of a race, prefer the item already in the cache\n                try:\n                    with lock(self):\n                        return c.setdefault(k, v)\n                except ValueError:\n                    return v  # value too large\n\n            def clear(self):\n                c = cache(self)\n                if c is not None:\n                    with lock(self):\n                        c.clear()\n\n        wrapper.cache = cache\n        wrapper.cache_key = key\n        wrapper.cache_lock = lock\n        wrapper.cache_clear = clear\n\n        return functools.update_wrapper(wrapper, method)\n\n    return decorator\n"}