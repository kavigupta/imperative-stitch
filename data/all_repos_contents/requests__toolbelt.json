{"setup.py": "# -*- coding: utf-8 -*-\n\nimport os\nimport re\nimport sys\n\nfrom setuptools import setup\n\nif sys.argv[-1].lower() in (\"submit\", \"publish\"):\n    os.system(\"python setup.py bdist_wheel sdist upload\")\n    sys.exit()\n\n\ndef get_version():\n    version = ''\n    with open('requests_toolbelt/__init__.py', 'r') as fd:\n        reg = re.compile(r'__version__ = [\\'\"]([^\\'\"]*)[\\'\"]')\n        for line in fd:\n            m = reg.match(line)\n            if m:\n                version = m.group(1)\n                break\n    return version\n\n__version__ = get_version()\n\nif not __version__:\n    raise RuntimeError('Cannot find version information')\n\n\npackages = [\n    'requests_toolbelt',\n    'requests_toolbelt.adapters',\n    'requests_toolbelt.auth',\n    'requests_toolbelt.downloadutils',\n    'requests_toolbelt.multipart',\n    'requests_toolbelt.threaded',\n    'requests_toolbelt.utils',\n]\n\nsetup(\n    name=\"requests-toolbelt\",\n    version=__version__,\n    description=\"A utility belt for advanced users of python-requests\",\n    long_description=\"\\n\\n\".join([open(\"README.rst\").read(),\n                                  open(\"HISTORY.rst\").read()]),\n    long_description_content_type=\"text/x-rst\",\n    license='Apache 2.0',\n    author='Ian Cordasco, Cory Benfield',\n    author_email=\"graffatcolmingov@gmail.com\",\n    url=\"https://toolbelt.readthedocs.io/\",\n    project_urls={\n        \"Changelog\": \"https://github.com/requests/toolbelt/blob/master/HISTORY.rst\",\n        \"Source\": \"https://github.com/requests/toolbelt\",\n    },\n    packages=packages,\n    package_data={'': ['LICENSE', 'AUTHORS.rst']},\n    include_package_data=True,\n    python_requires='>=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*',\n    install_requires=['requests>=2.0.1,<3.0.0'],\n    classifiers=[\n        'Development Status :: 5 - Production/Stable',\n        'License :: OSI Approved :: Apache Software License',\n        'Intended Audience :: Developers',\n        'Programming Language :: Python',\n        'Programming Language :: Python :: 2',\n        'Programming Language :: Python :: 2.7',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.4',\n        'Programming Language :: Python :: 3.5',\n        'Programming Language :: Python :: 3.6',\n        'Programming Language :: Python :: 3.7',\n        'Programming Language :: Python :: 3.8',\n        'Programming Language :: Python :: 3.9',\n        'Programming Language :: Python :: 3.10',\n        'Programming Language :: Python :: 3.11',\n        'Programming Language :: Python :: Implementation :: CPython',\n        'Programming Language :: Python :: Implementation :: PyPy',\n    ],\n)\n", "examples/threading/threaded_simplified.py": "from requests_toolbelt import threaded\n\nrequests = [{\n    'method': 'GET',\n    'url': 'https://httpbin.org/get',\n    'params': {'foo': 'bar'}\n}, {\n    'method': 'POST',\n    'url': 'https://httpbin.org/post',\n    'json': {'foo': 'bar'}\n}, {\n    'method': 'POST',\n    'url': 'https://httpbin.org/post',\n    'data': {'foo': 'bar'}\n}, {\n    'method': 'PUT',\n    'url': 'https://httpbin.org/put',\n    'files': {'foo': ('', 'bar')}\n}, {\n    'method': 'GET',\n    'url': 'https://httpbin.org/stream/100',\n    'stream': True\n}, {\n    'method': 'GET',\n    'url': 'https://httpbin.org/delay/10',\n    'timeout': 2.0\n}]\n\nurl = 'https://httpbin.org/get'\nrequests.extend([\n    {'method': 'GET', 'url': url, 'params': {'i': str(i)}}\n    for i in range(30)\n])\n\nresponses, exceptions = threaded.map(requests)\n", "examples/threading/threaded.py": "try:\n    import Queue as queue\nexcept ImportError:\n    import queue\n\nfrom requests_toolbelt.threaded import pool\n\nq = queue.Queue()\nq.put({\n    'method': 'GET',\n    'url': 'https://httpbin.org/get',\n    'params': {'foo': 'bar'}\n})\nq.put({\n    'method': 'POST',\n    'url': 'https://httpbin.org/post',\n    'json': {'foo': 'bar'}\n})\nq.put({\n    'method': 'POST',\n    'url': 'https://httpbin.org/post',\n    'data': {'foo': 'bar'}\n})\nq.put({\n    'method': 'PUT',\n    'url': 'https://httpbin.org/put',\n    'files': {'foo': ('', 'bar')}\n})\nq.put({\n    'method': 'GET',\n    'url': 'https://httpbin.org/stream/100',\n    'stream': True\n})\nq.put({\n    'method': 'GET',\n    'url': 'https://httpbin.org/delay/10',\n    'timeout': 5.0\n})\n\nfor i in range(30):\n    q.put({\n        'method': 'GET',\n        'url': 'https://httpbin.org/get',\n        'params': {'i': str(i)},\n    })\n\np = pool.Pool(q)\np.join_all()\n\nresponses = list(p.responses())\nexceptions = list(p.exceptions())\n", "examples/monitor/progress_bar.py": "# -*- coding: utf-8 -*-\n\n# ############################################################################\n# This example demonstrates how to use the MultipartEncoderMonitor to create a\n# progress bar using clint.\n# ############################################################################\n\nfrom clint.textui.progress import Bar as ProgressBar\nfrom requests_toolbelt import MultipartEncoder, MultipartEncoderMonitor\n\nimport requests\n\n\ndef create_callback(encoder):\n    encoder_len = encoder.len\n    bar = ProgressBar(expected_size=encoder_len, filled_char='=')\n\n    def callback(monitor):\n        bar.show(monitor.bytes_read)\n\n    return callback\n\n\ndef create_upload():\n    return MultipartEncoder({\n        'form_field': 'value',\n        'another_form_field': 'another value',\n        'first_file': ('progress_bar.py', open(__file__, 'rb'), 'text/plain'),\n        'second_file': ('progress_bar.py', open(__file__, 'rb'),\n                        'text/plain'),\n        })\n\n\nif __name__ == '__main__':\n    encoder = create_upload()\n    callback = create_callback(encoder)\n    monitor = MultipartEncoderMonitor(encoder, callback)\n    r = requests.post('https://httpbin.org/post', data=monitor,\n                      headers={'Content-Type': monitor.content_type})\n    print('\\nUpload finished! (Returned status {} {})'.format(\n        r.status_code, r.reason\n        ))\n", "requests_toolbelt/exceptions.py": "# -*- coding: utf-8 -*-\n\"\"\"Collection of exceptions raised by requests-toolbelt.\"\"\"\n\n\nclass StreamingError(Exception):\n    \"\"\"Used in :mod:`requests_toolbelt.downloadutils.stream`.\"\"\"\n    pass\n\n\nclass VersionMismatchError(Exception):\n    \"\"\"Used to indicate a version mismatch in the version of requests required.\n\n    The feature in use requires a newer version of Requests to function\n    appropriately but the version installed is not sufficient.\n    \"\"\"\n    pass\n\n\nclass RequestsVersionTooOld(Warning):\n    \"\"\"Used to indicate that the Requests version is too old.\n\n    If the version of Requests is too old to support a feature, we will issue\n    this warning to the user.\n    \"\"\"\n    pass\n", "requests_toolbelt/_compat.py": "\"\"\"Private module full of compatibility hacks.\n\nPrimarily this is for downstream redistributions of requests that unvendor\nurllib3 without providing a shim.\n\n.. warning::\n\n    This module is private. If you use it, and something breaks, you were\n    warned\n\"\"\"\nimport sys\n\nimport requests\n\ntry:\n    from requests.packages.urllib3 import fields\n    from requests.packages.urllib3 import filepost\n    from requests.packages.urllib3 import poolmanager\nexcept ImportError:\n    from urllib3 import fields\n    from urllib3 import filepost\n    from urllib3 import poolmanager\n\ntry:\n    from requests.packages.urllib3.connection import HTTPConnection\n    from requests.packages.urllib3 import connection\nexcept ImportError:\n    try:\n        from urllib3.connection import HTTPConnection\n        from urllib3 import connection\n    except ImportError:\n        HTTPConnection = None\n        connection = None\n\n\nif requests.__build__ < 0x020300:\n    timeout = None\nelse:\n    try:\n        from requests.packages.urllib3.util import timeout\n    except ImportError:\n        from urllib3.util import timeout\n\nPY3 = sys.version_info > (3, 0)\n\nif PY3:\n    from collections.abc import Mapping, MutableMapping\n    import queue\n    from urllib.parse import urlencode, urljoin\nelse:\n    from collections import Mapping, MutableMapping\n    import Queue as queue\n    from urllib import urlencode\n    from urlparse import urljoin\n\ntry:\n    basestring = basestring\nexcept NameError:\n    basestring = (str, bytes)\n\n\nclass HTTPHeaderDict(MutableMapping):\n    \"\"\"\n    :param headers:\n        An iterable of field-value pairs. Must not contain multiple field names\n        when compared case-insensitively.\n\n    :param kwargs:\n        Additional field-value pairs to pass in to ``dict.update``.\n\n    A ``dict`` like container for storing HTTP Headers.\n\n    Field names are stored and compared case-insensitively in compliance with\n    RFC 7230. Iteration provides the first case-sensitive key seen for each\n    case-insensitive pair.\n\n    Using ``__setitem__`` syntax overwrites fields that compare equal\n    case-insensitively in order to maintain ``dict``'s api. For fields that\n    compare equal, instead create a new ``HTTPHeaderDict`` and use ``.add``\n    in a loop.\n\n    If multiple fields that are equal case-insensitively are passed to the\n    constructor or ``.update``, the behavior is undefined and some will be\n    lost.\n\n    >>> headers = HTTPHeaderDict()\n    >>> headers.add('Set-Cookie', 'foo=bar')\n    >>> headers.add('set-cookie', 'baz=quxx')\n    >>> headers['content-length'] = '7'\n    >>> headers['SET-cookie']\n    'foo=bar, baz=quxx'\n    >>> headers['Content-Length']\n    '7'\n    \"\"\"\n\n    def __init__(self, headers=None, **kwargs):\n        super(HTTPHeaderDict, self).__init__()\n        self._container = {}\n        if headers is not None:\n            if isinstance(headers, HTTPHeaderDict):\n                self._copy_from(headers)\n            else:\n                self.extend(headers)\n        if kwargs:\n            self.extend(kwargs)\n\n    def __setitem__(self, key, val):\n        self._container[key.lower()] = (key, val)\n        return self._container[key.lower()]\n\n    def __getitem__(self, key):\n        val = self._container[key.lower()]\n        return ', '.join(val[1:])\n\n    def __delitem__(self, key):\n        del self._container[key.lower()]\n\n    def __contains__(self, key):\n        return key.lower() in self._container\n\n    def __eq__(self, other):\n        if not isinstance(other, Mapping) and not hasattr(other, 'keys'):\n            return False\n        if not isinstance(other, type(self)):\n            other = type(self)(other)\n        return ({k.lower(): v for k, v in self.itermerged()} ==\n                {k.lower(): v for k, v in other.itermerged()})\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    if not PY3:  # Python 2\n        iterkeys = MutableMapping.iterkeys\n        itervalues = MutableMapping.itervalues\n\n    __marker = object()\n\n    def __len__(self):\n        return len(self._container)\n\n    def __iter__(self):\n        # Only provide the originally cased names\n        for vals in self._container.values():\n            yield vals[0]\n\n    def pop(self, key, default=__marker):\n        \"\"\"D.pop(k[,d]) -> v, remove specified key and return its value.\n\n        If key is not found, d is returned if given, otherwise KeyError is\n        raised.\n        \"\"\"\n        # Using the MutableMapping function directly fails due to the private\n        # marker.\n        # Using ordinary dict.pop would expose the internal structures.\n        # So let's reinvent the wheel.\n        try:\n            value = self[key]\n        except KeyError:\n            if default is self.__marker:\n                raise\n            return default\n        else:\n            del self[key]\n            return value\n\n    def discard(self, key):\n        try:\n            del self[key]\n        except KeyError:\n            pass\n\n    def add(self, key, val):\n        \"\"\"Adds a (name, value) pair, doesn't overwrite the value if it already\n        exists.\n\n        >>> headers = HTTPHeaderDict(foo='bar')\n        >>> headers.add('Foo', 'baz')\n        >>> headers['foo']\n        'bar, baz'\n        \"\"\"\n        key_lower = key.lower()\n        new_vals = key, val\n        # Keep the common case aka no item present as fast as possible\n        vals = self._container.setdefault(key_lower, new_vals)\n        if new_vals is not vals:\n            # new_vals was not inserted, as there was a previous one\n            if isinstance(vals, list):\n                # If already several items got inserted, we have a list\n                vals.append(val)\n            else:\n                # vals should be a tuple then, i.e. only one item so far\n                # Need to convert the tuple to list for further extension\n                self._container[key_lower] = [vals[0], vals[1], val]\n\n    def extend(self, *args, **kwargs):\n        \"\"\"Generic import function for any type of header-like object.\n        Adapted version of MutableMapping.update in order to insert items\n        with self.add instead of self.__setitem__\n        \"\"\"\n        if len(args) > 1:\n            raise TypeError(\"extend() takes at most 1 positional \"\n                            \"arguments ({} given)\".format(len(args)))\n        other = args[0] if len(args) >= 1 else ()\n\n        if isinstance(other, HTTPHeaderDict):\n            for key, val in other.iteritems():\n                self.add(key, val)\n        elif isinstance(other, Mapping):\n            for key in other:\n                self.add(key, other[key])\n        elif hasattr(other, \"keys\"):\n            for key in other.keys():\n                self.add(key, other[key])\n        else:\n            for key, value in other:\n                self.add(key, value)\n\n        for key, value in kwargs.items():\n            self.add(key, value)\n\n    def getlist(self, key):\n        \"\"\"Returns a list of all the values for the named field. Returns an\n        empty list if the key doesn't exist.\"\"\"\n        try:\n            vals = self._container[key.lower()]\n        except KeyError:\n            return []\n        else:\n            if isinstance(vals, tuple):\n                return [vals[1]]\n            else:\n                return vals[1:]\n\n    # Backwards compatibility for httplib\n    getheaders = getlist\n    getallmatchingheaders = getlist\n    iget = getlist\n\n    def __repr__(self):\n        return \"%s(%s)\" % (type(self).__name__, dict(self.itermerged()))\n\n    def _copy_from(self, other):\n        for key in other:\n            val = other.getlist(key)\n            if isinstance(val, list):\n                # Don't need to convert tuples\n                val = list(val)\n            self._container[key.lower()] = [key] + val\n\n    def copy(self):\n        clone = type(self)()\n        clone._copy_from(self)\n        return clone\n\n    def iteritems(self):\n        \"\"\"Iterate over all header lines, including duplicate ones.\"\"\"\n        for key in self:\n            vals = self._container[key.lower()]\n            for val in vals[1:]:\n                yield vals[0], val\n\n    def itermerged(self):\n        \"\"\"Iterate over all headers, merging duplicate ones together.\"\"\"\n        for key in self:\n            val = self._container[key.lower()]\n            yield val[0], ', '.join(val[1:])\n\n    def items(self):\n        return list(self.iteritems())\n\n    @classmethod\n    def from_httplib(cls, message):  # Python 2\n        \"\"\"Read headers from a Python 2 httplib message object.\"\"\"\n        # python2.7 does not expose a proper API for exporting multiheaders\n        # efficiently. This function re-reads raw lines from the message\n        # object and extracts the multiheaders properly.\n        headers = []\n\n        for line in message.headers:\n            if line.startswith((' ', '\\t')):\n                key, value = headers[-1]\n                headers[-1] = (key, value + '\\r\\n' + line.rstrip())\n                continue\n\n            key, value = line.split(':', 1)\n            headers.append((key, value.strip()))\n\n        return cls(headers)\n\n\n__all__ = (\n    'basestring',\n    'connection',\n    'fields',\n    'filepost',\n    'poolmanager',\n    'timeout',\n    'HTTPHeaderDict',\n    'queue',\n    'urlencode',\n    'urljoin',\n)\n", "requests_toolbelt/__init__.py": "# -*- coding: utf-8 -*-\n\"\"\"\nrequests-toolbelt\n=================\n\nSee https://toolbelt.readthedocs.io/ for documentation\n\n:copyright: (c) 2014 by Ian Cordasco and Cory Benfield\n:license: Apache v2.0, see LICENSE for more details\n\"\"\"\n\nfrom .adapters import SSLAdapter, SourceAddressAdapter\nfrom .auth.guess import GuessAuth\nfrom .multipart import (\n    MultipartEncoder, MultipartEncoderMonitor, MultipartDecoder,\n    ImproperBodyPartContentException, NonMultipartContentTypeException\n    )\nfrom .streaming_iterator import StreamingIterator\nfrom .utils.user_agent import user_agent\n\n__title__ = 'requests-toolbelt'\n__authors__ = 'Ian Cordasco, Cory Benfield'\n__license__ = 'Apache v2.0'\n__copyright__ = 'Copyright 2014 Ian Cordasco, Cory Benfield'\n__version__ = '1.0.0'\n__version_info__ = tuple(int(i) for i in __version__.split('.'))\n\n__all__ = [\n    'GuessAuth', 'MultipartEncoder', 'MultipartEncoderMonitor',\n    'MultipartDecoder', 'SSLAdapter', 'SourceAddressAdapter',\n    'StreamingIterator', 'user_agent', 'ImproperBodyPartContentException',\n    'NonMultipartContentTypeException', '__title__', '__authors__',\n    '__license__', '__copyright__', '__version__', '__version_info__',\n]\n", "requests_toolbelt/sessions.py": "import requests\n\nfrom ._compat import urljoin\n\n\nclass BaseUrlSession(requests.Session):\n    \"\"\"A Session with a URL that all requests will use as a base.\n\n    Let's start by looking at a few examples:\n\n    .. code-block:: python\n\n        >>> from requests_toolbelt import sessions\n        >>> s = sessions.BaseUrlSession(\n        ...     base_url='https://example.com/resource/')\n        >>> r = s.get('sub-resource/', params={'foo': 'bar'})\n        >>> print(r.request.url)\n        https://example.com/resource/sub-resource/?foo=bar\n\n    Our call to the ``get`` method will make a request to the URL passed in\n    when we created the Session and the partial resource name we provide.\n    We implement this by overriding the ``request`` method of the Session.\n\n    Likewise, we override the ``prepare_request`` method so you can construct\n    a PreparedRequest in the same way:\n\n    .. code-block:: python\n\n        >>> from requests import Request\n        >>> from requests_toolbelt import sessions\n        >>> s = sessions.BaseUrlSession(\n        ...     base_url='https://example.com/resource/')\n        >>> request = Request(method='GET', url='sub-resource/')\n        >>> prepared_request = s.prepare_request(request)\n        >>> r = s.send(prepared_request)\n        >>> print(r.request.url)\n        https://example.com/resource/sub-resource\n\n    .. note::\n\n        The base URL that you provide and the path you provide are **very**\n        important.\n\n    Let's look at another *similar* example\n\n    .. code-block:: python\n\n        >>> from requests_toolbelt import sessions\n        >>> s = sessions.BaseUrlSession(\n        ...     base_url='https://example.com/resource/')\n        >>> r = s.get('/sub-resource/', params={'foo': 'bar'})\n        >>> print(r.request.url)\n        https://example.com/sub-resource/?foo=bar\n\n    The key difference here is that we called ``get`` with ``/sub-resource/``,\n    i.e., there was a leading ``/``. This changes how we create the URL\n    because we rely on :mod:`urllib.parse.urljoin`.\n\n    To override how we generate the URL, sub-class this method and override the\n    ``create_url`` method.\n\n    Based on implementation from\n    https://github.com/kennethreitz/requests/issues/2554#issuecomment-109341010\n    \"\"\"\n\n    base_url = None\n\n    def __init__(self, base_url=None):\n        if base_url:\n            self.base_url = base_url\n        super(BaseUrlSession, self).__init__()\n\n    def request(self, method, url, *args, **kwargs):\n        \"\"\"Send the request after generating the complete URL.\"\"\"\n        url = self.create_url(url)\n        return super(BaseUrlSession, self).request(\n            method, url, *args, **kwargs\n        )\n\n    def prepare_request(self, request, *args, **kwargs):\n        \"\"\"Prepare the request after generating the complete URL.\"\"\"\n        request.url = self.create_url(request.url)\n        return super(BaseUrlSession, self).prepare_request(\n            request, *args, **kwargs\n        )\n\n    def create_url(self, url):\n        \"\"\"Create the URL based off this partial path.\"\"\"\n        return urljoin(self.base_url, url)\n\n    def __getstate__(self):\n        \"\"\"Save base URL as well during the pickle\"\"\"\n        states = super(BaseUrlSession, self).__getstate__()\n        states.update({\"base_url\": self.base_url})\n        return states\n\n    def __setstate__(self, state):\n        \"\"\"Load base URL as well during the unpickle\"\"\"\n        super(BaseUrlSession, self).__setstate__(state)\n        if \"base_url\" in state:\n            self.base_url = state[\"base_url\"]\n", "requests_toolbelt/streaming_iterator.py": "# -*- coding: utf-8 -*-\n\"\"\"\n\nrequests_toolbelt.streaming_iterator\n====================================\n\nThis holds the implementation details for the :class:`StreamingIterator`. It\nis designed for the case where you, the user, know the size of the upload but\nneed to provide the data as an iterator. This class will allow you to specify\nthe size and stream the data without using a chunked transfer-encoding.\n\n\"\"\"\nfrom requests.utils import super_len\n\nfrom .multipart.encoder import CustomBytesIO, encode_with\n\n\nclass StreamingIterator(object):\n\n    \"\"\"\n    This class provides a way of allowing iterators with a known size to be\n    streamed instead of chunked.\n\n    In requests, if you pass in an iterator it assumes you want to use\n    chunked transfer-encoding to upload the data, which not all servers\n    support well. Additionally, you may want to set the content-length\n    yourself to avoid this but that will not work. The only way to preempt\n    requests using a chunked transfer-encoding and forcing it to stream the\n    uploads is to mimic a very specific interace. Instead of having to know\n    these details you can instead just use this class. You simply provide the\n    size and iterator and pass the instance of StreamingIterator to requests\n    via the data parameter like so:\n\n    .. code-block:: python\n\n        from requests_toolbelt import StreamingIterator\n\n        import requests\n\n        # Let iterator be some generator that you already have and size be\n        # the size of the data produced by the iterator\n\n        r = requests.post(url, data=StreamingIterator(size, iterator))\n\n    You can also pass file-like objects to :py:class:`StreamingIterator` in\n    case requests can't determize the filesize itself. This is the case with\n    streaming file objects like ``stdin`` or any sockets. Wrapping e.g. files\n    that are on disk with ``StreamingIterator`` is unnecessary, because\n    requests can determine the filesize itself.\n\n    Naturally, you should also set the `Content-Type` of your upload\n    appropriately because the toolbelt will not attempt to guess that for you.\n    \"\"\"\n\n    def __init__(self, size, iterator, encoding='utf-8'):\n        #: The expected size of the upload\n        self.size = int(size)\n\n        if self.size < 0:\n            raise ValueError(\n                'The size of the upload must be a positive integer'\n                )\n\n        #: Attribute that requests will check to determine the length of the\n        #: body. See bug #80 for more details\n        self.len = self.size\n\n        #: Encoding the input data is using\n        self.encoding = encoding\n\n        #: The iterator used to generate the upload data\n        self.iterator = iterator\n\n        if hasattr(iterator, 'read'):\n            self._file = iterator\n        else:\n            self._file = _IteratorAsBinaryFile(iterator, encoding)\n\n    def read(self, size=-1):\n        return encode_with(self._file.read(size), self.encoding)\n\n\nclass _IteratorAsBinaryFile(object):\n    def __init__(self, iterator, encoding='utf-8'):\n        #: The iterator used to generate the upload data\n        self.iterator = iterator\n\n        #: Encoding the iterator is using\n        self.encoding = encoding\n\n        # The buffer we use to provide the correct number of bytes requested\n        # during a read\n        self._buffer = CustomBytesIO()\n\n    def _get_bytes(self):\n        try:\n            return encode_with(next(self.iterator), self.encoding)\n        except StopIteration:\n            return b''\n\n    def _load_bytes(self, size):\n        self._buffer.smart_truncate()\n        amount_to_load = size - super_len(self._buffer)\n        bytes_to_append = True\n\n        while amount_to_load > 0 and bytes_to_append:\n            bytes_to_append = self._get_bytes()\n            amount_to_load -= self._buffer.append(bytes_to_append)\n\n    def read(self, size=-1):\n        size = int(size)\n        if size == -1:\n            return b''.join(self.iterator)\n\n        self._load_bytes(size)\n        return self._buffer.read(size)\n", "requests_toolbelt/utils/dump.py": "\"\"\"This module provides functions for dumping information about responses.\"\"\"\nimport collections\n\nfrom requests import compat\n\n\n__all__ = ('dump_response', 'dump_all')\n\nHTTP_VERSIONS = {\n    9: b'0.9',\n    10: b'1.0',\n    11: b'1.1',\n}\n\n_PrefixSettings = collections.namedtuple('PrefixSettings',\n                                         ['request', 'response'])\n\n\nclass PrefixSettings(_PrefixSettings):\n    def __new__(cls, request, response):\n        request = _coerce_to_bytes(request)\n        response = _coerce_to_bytes(response)\n        return super(PrefixSettings, cls).__new__(cls, request, response)\n\n\ndef _get_proxy_information(response):\n    if getattr(response.connection, 'proxy_manager', False):\n        proxy_info = {}\n        request_url = response.request.url\n        if request_url.startswith('https://'):\n            proxy_info['method'] = 'CONNECT'\n\n        proxy_info['request_path'] = request_url\n        return proxy_info\n    return None\n\n\ndef _format_header(name, value):\n    return (_coerce_to_bytes(name) + b': ' + _coerce_to_bytes(value) +\n            b'\\r\\n')\n\n\ndef _build_request_path(url, proxy_info):\n    uri = compat.urlparse(url)\n    proxy_url = proxy_info.get('request_path')\n    if proxy_url is not None:\n        request_path = _coerce_to_bytes(proxy_url)\n        return request_path, uri\n\n    request_path = _coerce_to_bytes(uri.path)\n    if uri.query:\n        request_path += b'?' + _coerce_to_bytes(uri.query)\n\n    return request_path, uri\n\n\ndef _dump_request_data(request, prefixes, bytearr, proxy_info=None):\n    if proxy_info is None:\n        proxy_info = {}\n\n    prefix = prefixes.request\n    method = _coerce_to_bytes(proxy_info.pop('method', request.method))\n    request_path, uri = _build_request_path(request.url, proxy_info)\n\n    # <prefix><METHOD> <request-path> HTTP/1.1\n    bytearr.extend(prefix + method + b' ' + request_path + b' HTTP/1.1\\r\\n')\n\n    # <prefix>Host: <request-host> OR host header specified by user\n    headers = request.headers.copy()\n    host_header = _coerce_to_bytes(headers.pop('Host', uri.netloc))\n    bytearr.extend(prefix + b'Host: ' + host_header + b'\\r\\n')\n\n    for name, value in headers.items():\n        bytearr.extend(prefix + _format_header(name, value))\n\n    bytearr.extend(prefix + b'\\r\\n')\n    if request.body:\n        if isinstance(request.body, compat.basestring):\n            bytearr.extend(prefix + _coerce_to_bytes(request.body))\n        else:\n            # In the event that the body is a file-like object, let's not try\n            # to read everything into memory.\n            bytearr.extend(b'<< Request body is not a string-like type >>')\n        bytearr.extend(b'\\r\\n')\n    bytearr.extend(b'\\r\\n')\n\n\ndef _dump_response_data(response, prefixes, bytearr):\n    prefix = prefixes.response\n    # Let's interact almost entirely with urllib3's response\n    raw = response.raw\n\n    # Let's convert the version int from httplib to bytes\n    version_str = HTTP_VERSIONS.get(raw.version, b'?')\n\n    # <prefix>HTTP/<version_str> <status_code> <reason>\n    bytearr.extend(prefix + b'HTTP/' + version_str + b' ' +\n                   str(raw.status).encode('ascii') + b' ' +\n                   _coerce_to_bytes(response.reason) + b'\\r\\n')\n\n    headers = raw.headers\n    for name in headers.keys():\n        for value in headers.getlist(name):\n            bytearr.extend(prefix + _format_header(name, value))\n\n    bytearr.extend(prefix + b'\\r\\n')\n\n    bytearr.extend(response.content)\n\n\ndef _coerce_to_bytes(data):\n    if not isinstance(data, bytes) and hasattr(data, 'encode'):\n        data = data.encode('utf-8')\n    # Don't bail out with an exception if data is None\n    return data if data is not None else b''\n\n\ndef dump_response(response, request_prefix=b'< ', response_prefix=b'> ',\n                  data_array=None):\n    \"\"\"Dump a single request-response cycle's information.\n\n    This will take a response object and dump only the data that requests can\n    see for that single request-response cycle.\n\n    Example::\n\n        import requests\n        from requests_toolbelt.utils import dump\n\n        resp = requests.get('https://api.github.com/users/sigmavirus24')\n        data = dump.dump_response(resp)\n        print(data.decode('utf-8'))\n\n    :param response:\n        The response to format\n    :type response: :class:`requests.Response`\n    :param request_prefix: (*optional*)\n        Bytes to prefix each line of the request data\n    :type request_prefix: :class:`bytes`\n    :param response_prefix: (*optional*)\n        Bytes to prefix each line of the response data\n    :type response_prefix: :class:`bytes`\n    :param data_array: (*optional*)\n        Bytearray to which we append the request-response cycle data\n    :type data_array: :class:`bytearray`\n    :returns: Formatted bytes of request and response information.\n    :rtype: :class:`bytearray`\n    \"\"\"\n    data = data_array if data_array is not None else bytearray()\n    prefixes = PrefixSettings(request_prefix, response_prefix)\n\n    if not hasattr(response, 'request'):\n        raise ValueError('Response has no associated request')\n\n    proxy_info = _get_proxy_information(response)\n    _dump_request_data(response.request, prefixes, data,\n                       proxy_info=proxy_info)\n    _dump_response_data(response, prefixes, data)\n    return data\n\n\ndef dump_all(response, request_prefix=b'< ', response_prefix=b'> '):\n    \"\"\"Dump all requests and responses including redirects.\n\n    This takes the response returned by requests and will dump all\n    request-response pairs in the redirect history in order followed by the\n    final request-response.\n\n    Example::\n\n        import requests\n        from requests_toolbelt.utils import dump\n\n        resp = requests.get('https://httpbin.org/redirect/5')\n        data = dump.dump_all(resp)\n        print(data.decode('utf-8'))\n\n    :param response:\n        The response to format\n    :type response: :class:`requests.Response`\n    :param request_prefix: (*optional*)\n        Bytes to prefix each line of the request data\n    :type request_prefix: :class:`bytes`\n    :param response_prefix: (*optional*)\n        Bytes to prefix each line of the response data\n    :type response_prefix: :class:`bytes`\n    :returns: Formatted bytes of request and response information.\n    :rtype: :class:`bytearray`\n    \"\"\"\n    data = bytearray()\n\n    history = list(response.history[:])\n    history.append(response)\n\n    for response in history:\n        dump_response(response, request_prefix, response_prefix, data)\n\n    return data\n", "requests_toolbelt/utils/deprecated.py": "# -*- coding: utf-8 -*-\n\"\"\"A collection of functions deprecated in requests.utils.\"\"\"\nimport re\nimport sys\n\nfrom requests import utils\n\nfind_charset = re.compile(\n    br'<meta.*?charset=[\"\\']*(.+?)[\"\\'>]', flags=re.I\n).findall\n\nfind_pragma = re.compile(\n    br'<meta.*?content=[\"\\']*;?charset=(.+?)[\"\\'>]', flags=re.I\n).findall\n\nfind_xml = re.compile(\n    br'^<\\?xml.*?encoding=[\"\\']*(.+?)[\"\\'>]'\n).findall\n\n\ndef get_encodings_from_content(content):\n    \"\"\"Return encodings from given content string.\n\n    .. code-block:: python\n\n        import requests\n        from requests_toolbelt.utils import deprecated\n\n        r = requests.get(url)\n        encodings = deprecated.get_encodings_from_content(r)\n\n    :param content: bytestring to extract encodings from\n    :type content: bytes\n    :return: encodings detected in the provided content\n    :rtype: list(str)\n    \"\"\"\n    encodings = (find_charset(content) + find_pragma(content)\n                 + find_xml(content))\n    if (3, 0) <= sys.version_info < (4, 0):\n        encodings = [encoding.decode('utf8') for encoding in encodings]\n    return encodings\n\n\ndef get_unicode_from_response(response):\n    \"\"\"Return the requested content back in unicode.\n\n    This will first attempt to retrieve the encoding from the response\n    headers. If that fails, it will use\n    :func:`requests_toolbelt.utils.deprecated.get_encodings_from_content`\n    to determine encodings from HTML elements.\n\n    .. code-block:: python\n\n        import requests\n        from requests_toolbelt.utils import deprecated\n\n        r = requests.get(url)\n        text = deprecated.get_unicode_from_response(r)\n\n    :param response: Response object to get unicode content from.\n    :type response: requests.models.Response\n    \"\"\"\n    tried_encodings = set()\n\n    # Try charset from content-type\n    encoding = utils.get_encoding_from_headers(response.headers)\n\n    if encoding:\n        try:\n            return str(response.content, encoding)\n        except UnicodeError:\n            tried_encodings.add(encoding.lower())\n\n    encodings = get_encodings_from_content(response.content)\n\n    for _encoding in encodings:\n        _encoding = _encoding.lower()\n        if _encoding in tried_encodings:\n            continue\n        try:\n            return str(response.content, _encoding)\n        except UnicodeError:\n            tried_encodings.add(_encoding)\n\n    # Fall back:\n    if encoding:\n        try:\n            return str(response.content, encoding, errors='replace')\n        except TypeError:\n            pass\n    return response.text\n", "requests_toolbelt/utils/__init__.py": "", "requests_toolbelt/utils/user_agent.py": "# -*- coding: utf-8 -*-\nimport collections\nimport platform\nimport sys\n\n\ndef user_agent(name, version, extras=None):\n    \"\"\"Return an internet-friendly user_agent string.\n\n    The majority of this code has been wilfully stolen from the equivalent\n    function in Requests.\n\n    :param name: The intended name of the user-agent, e.g. \"python-requests\".\n    :param version: The version of the user-agent, e.g. \"0.0.1\".\n    :param extras: List of two-item tuples that are added to the user-agent\n        string.\n    :returns: Formatted user-agent string\n    :rtype: str\n    \"\"\"\n    if extras is None:\n        extras = []\n\n    return UserAgentBuilder(\n            name, version\n        ).include_extras(\n            extras\n        ).include_implementation(\n        ).include_system().build()\n\n\nclass UserAgentBuilder(object):\n    \"\"\"Class to provide a greater level of control than :func:`user_agent`.\n\n    This is used by :func:`user_agent` to build its User-Agent string.\n\n    .. code-block:: python\n\n        user_agent_str = UserAgentBuilder(\n                name='requests-toolbelt',\n                version='17.4.0',\n            ).include_implementation(\n            ).include_system(\n            ).include_extras([\n                ('requests', '2.14.2'),\n                ('urllib3', '1.21.2'),\n            ]).build()\n\n    \"\"\"\n\n    format_string = '%s/%s'\n\n    def __init__(self, name, version):\n        \"\"\"Initialize our builder with the name and version of our user agent.\n\n        :param str name:\n            Name of our user-agent.\n        :param str version:\n            The version string for user-agent.\n        \"\"\"\n        self._pieces = collections.deque([(name, version)])\n\n    def build(self):\n        \"\"\"Finalize the User-Agent string.\n\n        :returns:\n            Formatted User-Agent string.\n        :rtype:\n            str\n        \"\"\"\n        return \" \".join([self.format_string % piece for piece in self._pieces])\n\n    def include_extras(self, extras):\n        \"\"\"Include extra portions of the User-Agent.\n\n        :param list extras:\n            list of tuples of extra-name and extra-version\n        \"\"\"\n        if any(len(extra) != 2 for extra in extras):\n            raise ValueError('Extras should be a sequence of two item tuples.')\n\n        self._pieces.extend(extras)\n        return self\n\n    def include_implementation(self):\n        \"\"\"Append the implementation string to the user-agent string.\n\n        This adds the the information that you're using CPython 2.7.13 to the\n        User-Agent.\n        \"\"\"\n        self._pieces.append(_implementation_tuple())\n        return self\n\n    def include_system(self):\n        \"\"\"Append the information about the Operating System.\"\"\"\n        self._pieces.append(_platform_tuple())\n        return self\n\n\ndef _implementation_tuple():\n    \"\"\"Return the tuple of interpreter name and version.\n\n    Returns a string that provides both the name and the version of the Python\n    implementation currently running. For example, on CPython 2.7.5 it will\n    return \"CPython/2.7.5\".\n\n    This function works best on CPython and PyPy: in particular, it probably\n    doesn't work for Jython or IronPython. Future investigation should be done\n    to work out the correct shape of the code for those platforms.\n    \"\"\"\n    implementation = platform.python_implementation()\n\n    if implementation == 'CPython':\n        implementation_version = platform.python_version()\n    elif implementation == 'PyPy':\n        implementation_version = '%s.%s.%s' % (sys.pypy_version_info.major,\n                                               sys.pypy_version_info.minor,\n                                               sys.pypy_version_info.micro)\n        if sys.pypy_version_info.releaselevel != 'final':\n            implementation_version = ''.join([\n                implementation_version, sys.pypy_version_info.releaselevel\n                ])\n    elif implementation == 'Jython':\n        implementation_version = platform.python_version()  # Complete Guess\n    elif implementation == 'IronPython':\n        implementation_version = platform.python_version()  # Complete Guess\n    else:\n        implementation_version = 'Unknown'\n\n    return (implementation, implementation_version)\n\n\ndef _implementation_string():\n    return \"%s/%s\" % _implementation_tuple()\n\n\ndef _platform_tuple():\n    try:\n        p_system = platform.system()\n        p_release = platform.release()\n    except IOError:\n        p_system = 'Unknown'\n        p_release = 'Unknown'\n    return (p_system, p_release)\n", "requests_toolbelt/cookies/forgetful.py": "\"\"\"The module containing the code for ForgetfulCookieJar.\"\"\"\nfrom requests.cookies import RequestsCookieJar\n\n\nclass ForgetfulCookieJar(RequestsCookieJar):\n    def set_cookie(self, *args, **kwargs):\n        return\n", "requests_toolbelt/cookies/__init__.py": "", "requests_toolbelt/multipart/decoder.py": "# -*- coding: utf-8 -*-\n\"\"\"\n\nrequests_toolbelt.multipart.decoder\n===================================\n\nThis holds all the implementation details of the MultipartDecoder\n\n\"\"\"\n\nimport sys\nimport email.parser\nfrom .encoder import encode_with\nfrom requests.structures import CaseInsensitiveDict\n\n\ndef _split_on_find(content, bound):\n    point = content.find(bound)\n    return content[:point], content[point + len(bound):]\n\n\nclass ImproperBodyPartContentException(Exception):\n    pass\n\n\nclass NonMultipartContentTypeException(Exception):\n    pass\n\n\ndef _header_parser(string, encoding):\n    major = sys.version_info[0]\n    if major == 3:\n        string = string.decode(encoding)\n    headers = email.parser.HeaderParser().parsestr(string).items()\n    return (\n        (encode_with(k, encoding), encode_with(v, encoding))\n        for k, v in headers\n    )\n\n\nclass BodyPart(object):\n    \"\"\"\n\n    The ``BodyPart`` object is a ``Response``-like interface to an individual\n    subpart of a multipart response. It is expected that these will\n    generally be created by objects of the ``MultipartDecoder`` class.\n\n    Like ``Response``, there is a ``CaseInsensitiveDict`` object named headers,\n    ``content`` to access bytes, ``text`` to access unicode, and ``encoding``\n    to access the unicode codec.\n\n    \"\"\"\n\n    def __init__(self, content, encoding):\n        self.encoding = encoding\n        headers = {}\n        # Split into header section (if any) and the content\n        if b'\\r\\n\\r\\n' in content:\n            first, self.content = _split_on_find(content, b'\\r\\n\\r\\n')\n            if first != b'':\n                headers = _header_parser(first.lstrip(), encoding)\n        else:\n            raise ImproperBodyPartContentException(\n                'content does not contain CR-LF-CR-LF'\n            )\n        self.headers = CaseInsensitiveDict(headers)\n\n    @property\n    def text(self):\n        \"\"\"Content of the ``BodyPart`` in unicode.\"\"\"\n        return self.content.decode(self.encoding)\n\n\nclass MultipartDecoder(object):\n    \"\"\"\n\n    The ``MultipartDecoder`` object parses the multipart payload of\n    a bytestring into a tuple of ``Response``-like ``BodyPart`` objects.\n\n    The basic usage is::\n\n        import requests\n        from requests_toolbelt import MultipartDecoder\n\n        response = requests.get(url)\n        decoder = MultipartDecoder.from_response(response)\n        for part in decoder.parts:\n            print(part.headers['content-type'])\n\n    If the multipart content is not from a response, basic usage is::\n\n        from requests_toolbelt import MultipartDecoder\n\n        decoder = MultipartDecoder(content, content_type)\n        for part in decoder.parts:\n            print(part.headers['content-type'])\n\n    For both these usages, there is an optional ``encoding`` parameter. This is\n    a string, which is the name of the unicode codec to use (default is\n    ``'utf-8'``).\n\n    \"\"\"\n    def __init__(self, content, content_type, encoding='utf-8'):\n        #: Original Content-Type header\n        self.content_type = content_type\n        #: Response body encoding\n        self.encoding = encoding\n        #: Parsed parts of the multipart response body\n        self.parts = tuple()\n        self._find_boundary()\n        self._parse_body(content)\n\n    def _find_boundary(self):\n        ct_info = tuple(x.strip() for x in self.content_type.split(';'))\n        mimetype = ct_info[0]\n        if mimetype.split('/')[0].lower() != 'multipart':\n            raise NonMultipartContentTypeException(\n                \"Unexpected mimetype in content-type: '{}'\".format(mimetype)\n            )\n        for item in ct_info[1:]:\n            attr, value = _split_on_find(\n                item,\n                '='\n            )\n            if attr.lower() == 'boundary':\n                self.boundary = encode_with(value.strip('\"'), self.encoding)\n\n    @staticmethod\n    def _fix_first_part(part, boundary_marker):\n        bm_len = len(boundary_marker)\n        if boundary_marker == part[:bm_len]:\n            return part[bm_len:]\n        else:\n            return part\n\n    def _parse_body(self, content):\n        boundary = b''.join((b'--', self.boundary))\n\n        def body_part(part):\n            fixed = MultipartDecoder._fix_first_part(part, boundary)\n            return BodyPart(fixed, self.encoding)\n\n        def test_part(part):\n            return (part != b'' and\n                    part != b'\\r\\n' and\n                    part[:4] != b'--\\r\\n' and\n                    part != b'--')\n\n        parts = content.split(b''.join((b'\\r\\n', boundary)))\n        self.parts = tuple(body_part(x) for x in parts if test_part(x))\n\n    @classmethod\n    def from_response(cls, response, encoding='utf-8'):\n        content = response.content\n        content_type = response.headers.get('content-type', None)\n        return cls(content, content_type, encoding)\n", "requests_toolbelt/multipart/__init__.py": "\"\"\"\nrequests_toolbelt.multipart\n===========================\n\nSee https://toolbelt.readthedocs.io/ for documentation\n\n:copyright: (c) 2014 by Ian Cordasco and Cory Benfield\n:license: Apache v2.0, see LICENSE for more details\n\"\"\"\n\nfrom .encoder import MultipartEncoder, MultipartEncoderMonitor\nfrom .decoder import MultipartDecoder\nfrom .decoder import ImproperBodyPartContentException\nfrom .decoder import NonMultipartContentTypeException\n\n__title__ = 'requests-toolbelt'\n__authors__ = 'Ian Cordasco, Cory Benfield'\n__license__ = 'Apache v2.0'\n__copyright__ = 'Copyright 2014 Ian Cordasco, Cory Benfield'\n\n__all__ = [\n    'MultipartEncoder',\n    'MultipartEncoderMonitor',\n    'MultipartDecoder',\n    'ImproperBodyPartContentException',\n    'NonMultipartContentTypeException',\n    '__title__',\n    '__authors__',\n    '__license__',\n    '__copyright__',\n]\n", "requests_toolbelt/multipart/encoder.py": "# -*- coding: utf-8 -*-\n\"\"\"\n\nrequests_toolbelt.multipart.encoder\n===================================\n\nThis holds all of the implementation details of the MultipartEncoder\n\n\"\"\"\nimport contextlib\nimport io\nimport os\nfrom uuid import uuid4\n\nimport requests\n\nfrom .._compat import fields\n\n\nclass FileNotSupportedError(Exception):\n    \"\"\"File not supported error.\"\"\"\n\n\nclass MultipartEncoder(object):\n\n    \"\"\"\n\n    The ``MultipartEncoder`` object is a generic interface to the engine that\n    will create a ``multipart/form-data`` body for you.\n\n    The basic usage is:\n\n    .. code-block:: python\n\n        import requests\n        from requests_toolbelt import MultipartEncoder\n\n        encoder = MultipartEncoder({'field': 'value',\n                                    'other_field': 'other_value'})\n        r = requests.post('https://httpbin.org/post', data=encoder,\n                          headers={'Content-Type': encoder.content_type})\n\n    If you do not need to take advantage of streaming the post body, you can\n    also do:\n\n    .. code-block:: python\n\n        r = requests.post('https://httpbin.org/post',\n                          data=encoder.to_string(),\n                          headers={'Content-Type': encoder.content_type})\n\n    If you want the encoder to use a specific order, you can use an\n    OrderedDict or more simply, a list of tuples:\n\n    .. code-block:: python\n\n        encoder = MultipartEncoder([('field', 'value'),\n                                    ('other_field', 'other_value')])\n\n    .. versionchanged:: 0.4.0\n\n    You can also provide tuples as part values as you would provide them to\n    requests' ``files`` parameter.\n\n    .. code-block:: python\n\n        encoder = MultipartEncoder({\n            'field': ('file_name', b'{\"a\": \"b\"}', 'application/json',\n                      {'X-My-Header': 'my-value'})\n        ])\n\n    .. warning::\n\n        This object will end up directly in :mod:`httplib`. Currently,\n        :mod:`httplib` has a hard-coded read size of **8192 bytes**. This\n        means that it will loop until the file has been read and your upload\n        could take a while. This is **not** a bug in requests. A feature is\n        being considered for this object to allow you, the user, to specify\n        what size should be returned on a read. If you have opinions on this,\n        please weigh in on `this issue`_.\n\n    .. _this issue:\n        https://github.com/requests/toolbelt/issues/75\n\n    \"\"\"\n\n    def __init__(self, fields, boundary=None, encoding='utf-8'):\n        #: Boundary value either passed in by the user or created\n        self.boundary_value = boundary or uuid4().hex\n\n        # Computed boundary\n        self.boundary = '--{}'.format(self.boundary_value)\n\n        #: Encoding of the data being passed in\n        self.encoding = encoding\n\n        # Pre-encoded boundary\n        self._encoded_boundary = b''.join([\n            encode_with(self.boundary, self.encoding),\n            encode_with('\\r\\n', self.encoding)\n            ])\n\n        #: Fields provided by the user\n        self.fields = fields\n\n        #: Whether or not the encoder is finished\n        self.finished = False\n\n        #: Pre-computed parts of the upload\n        self.parts = []\n\n        # Pre-computed parts iterator\n        self._iter_parts = iter([])\n\n        # The part we're currently working with\n        self._current_part = None\n\n        # Cached computation of the body's length\n        self._len = None\n\n        # Our buffer\n        self._buffer = CustomBytesIO(encoding=encoding)\n\n        # Pre-compute each part's headers\n        self._prepare_parts()\n\n        # Load boundary into buffer\n        self._write_boundary()\n\n    @property\n    def len(self):\n        \"\"\"Length of the multipart/form-data body.\n\n        requests will first attempt to get the length of the body by calling\n        ``len(body)`` and then by checking for the ``len`` attribute.\n\n        On 32-bit systems, the ``__len__`` method cannot return anything\n        larger than an integer (in C) can hold. If the total size of the body\n        is even slightly larger than 4GB users will see an OverflowError. This\n        manifested itself in `bug #80`_.\n\n        As such, we now calculate the length lazily as a property.\n\n        .. _bug #80:\n            https://github.com/requests/toolbelt/issues/80\n        \"\"\"\n        # If _len isn't already calculated, calculate, return, and set it\n        return self._len or self._calculate_length()\n\n    def __repr__(self):\n        return '<MultipartEncoder: {!r}>'.format(self.fields)\n\n    def _calculate_length(self):\n        \"\"\"\n        This uses the parts to calculate the length of the body.\n\n        This returns the calculated length so __len__ can be lazy.\n        \"\"\"\n        boundary_len = len(self.boundary)  # Length of --{boundary}\n        # boundary length + header length + body length + len('\\r\\n') * 2\n        self._len = sum(\n            (boundary_len + total_len(p) + 4) for p in self.parts\n            ) + boundary_len + 4\n        return self._len\n\n    def _calculate_load_amount(self, read_size):\n        \"\"\"This calculates how many bytes need to be added to the buffer.\n\n        When a consumer read's ``x`` from the buffer, there are two cases to\n        satisfy:\n\n            1. Enough data in the buffer to return the requested amount\n            2. Not enough data\n\n        This function uses the amount of unread bytes in the buffer and\n        determines how much the Encoder has to load before it can return the\n        requested amount of bytes.\n\n        :param int read_size: the number of bytes the consumer requests\n        :returns: int -- the number of bytes that must be loaded into the\n            buffer before the read can be satisfied. This will be strictly\n            non-negative\n        \"\"\"\n        amount = read_size - total_len(self._buffer)\n        return amount if amount > 0 else 0\n\n    def _load(self, amount):\n        \"\"\"Load ``amount`` number of bytes into the buffer.\"\"\"\n        self._buffer.smart_truncate()\n        part = self._current_part or self._next_part()\n        while amount == -1 or amount > 0:\n            written = 0\n            if part and not part.bytes_left_to_write():\n                written += self._write(b'\\r\\n')\n                written += self._write_boundary()\n                part = self._next_part()\n\n            if not part:\n                written += self._write_closing_boundary()\n                self.finished = True\n                break\n\n            written += part.write_to(self._buffer, amount)\n\n            if amount != -1:\n                amount -= written\n\n    def _next_part(self):\n        try:\n            p = self._current_part = next(self._iter_parts)\n        except StopIteration:\n            p = None\n        return p\n\n    def _iter_fields(self):\n        _fields = self.fields\n        if hasattr(self.fields, 'items'):\n            _fields = list(self.fields.items())\n        for k, v in _fields:\n            file_name = None\n            file_type = None\n            file_headers = None\n            if isinstance(v, (list, tuple)):\n                if len(v) == 2:\n                    file_name, file_pointer = v\n                elif len(v) == 3:\n                    file_name, file_pointer, file_type = v\n                else:\n                    file_name, file_pointer, file_type, file_headers = v\n            else:\n                file_pointer = v\n\n            field = fields.RequestField(name=k, data=file_pointer,\n                                        filename=file_name,\n                                        headers=file_headers)\n            field.make_multipart(content_type=file_type)\n            yield field\n\n    def _prepare_parts(self):\n        \"\"\"This uses the fields provided by the user and creates Part objects.\n\n        It populates the `parts` attribute and uses that to create a\n        generator for iteration.\n        \"\"\"\n        enc = self.encoding\n        self.parts = [Part.from_field(f, enc) for f in self._iter_fields()]\n        self._iter_parts = iter(self.parts)\n\n    def _write(self, bytes_to_write):\n        \"\"\"Write the bytes to the end of the buffer.\n\n        :param bytes bytes_to_write: byte-string (or bytearray) to append to\n            the buffer\n        :returns: int -- the number of bytes written\n        \"\"\"\n        return self._buffer.append(bytes_to_write)\n\n    def _write_boundary(self):\n        \"\"\"Write the boundary to the end of the buffer.\"\"\"\n        return self._write(self._encoded_boundary)\n\n    def _write_closing_boundary(self):\n        \"\"\"Write the bytes necessary to finish a multipart/form-data body.\"\"\"\n        with reset(self._buffer):\n            self._buffer.seek(-2, 2)\n            self._buffer.write(b'--\\r\\n')\n        return 2\n\n    def _write_headers(self, headers):\n        \"\"\"Write the current part's headers to the buffer.\"\"\"\n        return self._write(encode_with(headers, self.encoding))\n\n    @property\n    def content_type(self):\n        return str(\n            'multipart/form-data; boundary={}'.format(self.boundary_value)\n            )\n\n    def to_string(self):\n        \"\"\"Return the entirety of the data in the encoder.\n\n        .. note::\n\n            This simply reads all of the data it can. If you have started\n            streaming or reading data from the encoder, this method will only\n            return whatever data is left in the encoder.\n\n        .. note::\n\n            This method affects the internal state of the encoder. Calling\n            this method will exhaust the encoder.\n\n        :returns: the multipart message\n        :rtype: bytes\n        \"\"\"\n\n        return self.read()\n\n    def read(self, size=-1):\n        \"\"\"Read data from the streaming encoder.\n\n        :param int size: (optional), If provided, ``read`` will return exactly\n            that many bytes. If it is not provided, it will return the\n            remaining bytes.\n        :returns: bytes\n        \"\"\"\n        if self.finished:\n            return self._buffer.read(size)\n\n        bytes_to_load = size\n        if bytes_to_load != -1 and bytes_to_load is not None:\n            bytes_to_load = self._calculate_load_amount(int(size))\n\n        self._load(bytes_to_load)\n        return self._buffer.read(size)\n\n\ndef IDENTITY(monitor):\n    return monitor\n\n\nclass MultipartEncoderMonitor(object):\n\n    \"\"\"\n    An object used to monitor the progress of a :class:`MultipartEncoder`.\n\n    The :class:`MultipartEncoder` should only be responsible for preparing and\n    streaming the data. For anyone who wishes to monitor it, they shouldn't be\n    using that instance to manage that as well. Using this class, they can\n    monitor an encoder and register a callback. The callback receives the\n    instance of the monitor.\n\n    To use this monitor, you construct your :class:`MultipartEncoder` as you\n    normally would.\n\n    .. code-block:: python\n\n        from requests_toolbelt import (MultipartEncoder,\n                                       MultipartEncoderMonitor)\n        import requests\n\n        def callback(monitor):\n            # Do something with this information\n            pass\n\n        m = MultipartEncoder(fields={'field0': 'value0'})\n        monitor = MultipartEncoderMonitor(m, callback)\n        headers = {'Content-Type': monitor.content_type}\n        r = requests.post('https://httpbin.org/post', data=monitor,\n                          headers=headers)\n\n    Alternatively, if your use case is very simple, you can use the following\n    pattern.\n\n    .. code-block:: python\n\n        from requests_toolbelt import MultipartEncoderMonitor\n        import requests\n\n        def callback(monitor):\n            # Do something with this information\n            pass\n\n        monitor = MultipartEncoderMonitor.from_fields(\n            fields={'field0': 'value0'}, callback\n            )\n        headers = {'Content-Type': montior.content_type}\n        r = requests.post('https://httpbin.org/post', data=monitor,\n                          headers=headers)\n\n    \"\"\"\n\n    def __init__(self, encoder, callback=None):\n        #: Instance of the :class:`MultipartEncoder` being monitored\n        self.encoder = encoder\n\n        #: Optionally function to call after a read\n        self.callback = callback or IDENTITY\n\n        #: Number of bytes already read from the :class:`MultipartEncoder`\n        #: instance\n        self.bytes_read = 0\n\n        #: Avoid the same problem in bug #80\n        self.len = self.encoder.len\n\n    @classmethod\n    def from_fields(cls, fields, boundary=None, encoding='utf-8',\n                    callback=None):\n        encoder = MultipartEncoder(fields, boundary, encoding)\n        return cls(encoder, callback)\n\n    @property\n    def content_type(self):\n        return self.encoder.content_type\n\n    def to_string(self):\n        return self.read()\n\n    def read(self, size=-1):\n        string = self.encoder.read(size)\n        self.bytes_read += len(string)\n        self.callback(self)\n        return string\n\n\ndef encode_with(string, encoding):\n    \"\"\"Encoding ``string`` with ``encoding`` if necessary.\n\n    :param str string: If string is a bytes object, it will not encode it.\n        Otherwise, this function will encode it with the provided encoding.\n    :param str encoding: The encoding with which to encode string.\n    :returns: encoded bytes object\n    \"\"\"\n    if not (string is None or isinstance(string, bytes)):\n        return string.encode(encoding)\n    return string\n\n\ndef readable_data(data, encoding):\n    \"\"\"Coerce the data to an object with a ``read`` method.\"\"\"\n    if hasattr(data, 'read'):\n        return data\n\n    return CustomBytesIO(data, encoding)\n\n\ndef total_len(o):\n    if hasattr(o, '__len__'):\n        return len(o)\n\n    if hasattr(o, 'len'):\n        return o.len\n\n    if hasattr(o, 'fileno'):\n        try:\n            fileno = o.fileno()\n        except io.UnsupportedOperation:\n            pass\n        else:\n            return os.fstat(fileno).st_size\n\n    if hasattr(o, 'getvalue'):\n        # e.g. BytesIO, cStringIO.StringIO\n        return len(o.getvalue())\n\n\n@contextlib.contextmanager\ndef reset(buffer):\n    \"\"\"Keep track of the buffer's current position and write to the end.\n\n    This is a context manager meant to be used when adding data to the buffer.\n    It eliminates the need for every function to be concerned with the\n    position of the cursor in the buffer.\n    \"\"\"\n    original_position = buffer.tell()\n    buffer.seek(0, 2)\n    yield\n    buffer.seek(original_position, 0)\n\n\ndef coerce_data(data, encoding):\n    \"\"\"Ensure that every object's __len__ behaves uniformly.\"\"\"\n    if not isinstance(data, CustomBytesIO):\n        if hasattr(data, 'getvalue'):\n            return CustomBytesIO(data.getvalue(), encoding)\n\n        if hasattr(data, 'fileno'):\n            return FileWrapper(data)\n\n        if not hasattr(data, 'read'):\n            return CustomBytesIO(data, encoding)\n\n    return data\n\n\ndef to_list(fields):\n    if hasattr(fields, 'items'):\n        return list(fields.items())\n    return list(fields)\n\n\nclass Part(object):\n    def __init__(self, headers, body):\n        self.headers = headers\n        self.body = body\n        self.headers_unread = True\n        self.len = len(self.headers) + total_len(self.body)\n\n    @classmethod\n    def from_field(cls, field, encoding):\n        \"\"\"Create a part from a Request Field generated by urllib3.\"\"\"\n        headers = encode_with(field.render_headers(), encoding)\n        body = coerce_data(field.data, encoding)\n        return cls(headers, body)\n\n    def bytes_left_to_write(self):\n        \"\"\"Determine if there are bytes left to write.\n\n        :returns: bool -- ``True`` if there are bytes left to write, otherwise\n            ``False``\n        \"\"\"\n        to_read = 0\n        if self.headers_unread:\n            to_read += len(self.headers)\n\n        return (to_read + total_len(self.body)) > 0\n\n    def write_to(self, buffer, size):\n        \"\"\"Write the requested amount of bytes to the buffer provided.\n\n        The number of bytes written may exceed size on the first read since we\n        load the headers ambitiously.\n\n        :param CustomBytesIO buffer: buffer we want to write bytes to\n        :param int size: number of bytes requested to be written to the buffer\n        :returns: int -- number of bytes actually written\n        \"\"\"\n        written = 0\n        if self.headers_unread:\n            written += buffer.append(self.headers)\n            self.headers_unread = False\n\n        while total_len(self.body) > 0 and (size == -1 or written < size):\n            amount_to_read = size\n            if size != -1:\n                amount_to_read = size - written\n            written += buffer.append(self.body.read(amount_to_read))\n\n        return written\n\n\nclass CustomBytesIO(io.BytesIO):\n    def __init__(self, buffer=None, encoding='utf-8'):\n        buffer = encode_with(buffer, encoding)\n        super(CustomBytesIO, self).__init__(buffer)\n\n    def _get_end(self):\n        current_pos = self.tell()\n        self.seek(0, 2)\n        length = self.tell()\n        self.seek(current_pos, 0)\n        return length\n\n    @property\n    def len(self):\n        length = self._get_end()\n        return length - self.tell()\n\n    def append(self, bytes):\n        with reset(self):\n            written = self.write(bytes)\n        return written\n\n    def smart_truncate(self):\n        to_be_read = total_len(self)\n        already_read = self._get_end() - to_be_read\n\n        if already_read >= to_be_read:\n            old_bytes = self.read()\n            self.seek(0, 0)\n            self.truncate()\n            self.write(old_bytes)\n            self.seek(0, 0)  # We want to be at the beginning\n\n\nclass FileWrapper(object):\n    def __init__(self, file_object):\n        self.fd = file_object\n\n    @property\n    def len(self):\n        return total_len(self.fd) - self.fd.tell()\n\n    def read(self, length=-1):\n        return self.fd.read(length)\n\n\nclass FileFromURLWrapper(object):\n    \"\"\"File from URL wrapper.\n\n    The :class:`FileFromURLWrapper` object gives you the ability to stream file\n    from provided URL in chunks by :class:`MultipartEncoder`.\n    Provide a stateless solution for streaming file from one server to another.\n    You can use the :class:`FileFromURLWrapper` without a session or with\n    a session as demonstated by the examples below:\n\n    .. code-block:: python\n        # no session\n\n        import requests\n        from requests_toolbelt import MultipartEncoder, FileFromURLWrapper\n\n        url = 'https://httpbin.org/image/png'\n        streaming_encoder = MultipartEncoder(\n            fields={\n                'file': FileFromURLWrapper(url)\n            }\n        )\n        r = requests.post(\n            'https://httpbin.org/post', data=streaming_encoder,\n            headers={'Content-Type': streaming_encoder.content_type}\n        )\n\n    .. code-block:: python\n        # using a session\n\n        import requests\n        from requests_toolbelt import MultipartEncoder, FileFromURLWrapper\n\n        session = requests.Session()\n        url = 'https://httpbin.org/image/png'\n        streaming_encoder = MultipartEncoder(\n            fields={\n                'file': FileFromURLWrapper(url, session=session)\n            }\n        )\n        r = session.post(\n            'https://httpbin.org/post', data=streaming_encoder,\n            headers={'Content-Type': streaming_encoder.content_type}\n        )\n\n    \"\"\"\n\n    def __init__(self, file_url, session=None):\n        self.session = session or requests.Session()\n        requested_file = self._request_for_file(file_url)\n        self.len = int(requested_file.headers['content-length'])\n        self.raw_data = requested_file.raw\n\n    def _request_for_file(self, file_url):\n        \"\"\"Make call for file under provided URL.\"\"\"\n        response = self.session.get(file_url, stream=True)\n        content_length = response.headers.get('content-length', None)\n        if content_length is None:\n            error_msg = (\n                \"Data from provided URL {url} is not supported. Lack of \"\n                \"content-length Header in requested file response.\".format(\n                    url=file_url)\n            )\n            raise FileNotSupportedError(error_msg)\n        elif not content_length.isdigit():\n            error_msg = (\n                \"Data from provided URL {url} is not supported. content-length\"\n                \" header value is not a digit.\".format(url=file_url)\n            )\n            raise FileNotSupportedError(error_msg)\n        return response\n\n    def read(self, chunk_size):\n        \"\"\"Read file in chunks.\"\"\"\n        chunk_size = chunk_size if chunk_size >= 0 else self.len\n        chunk = self.raw_data.read(chunk_size) or b''\n        self.len -= len(chunk) if chunk else 0  # left to read\n        return chunk\n", "requests_toolbelt/threaded/pool.py": "\"\"\"Module implementing the Pool for :mod:``requests_toolbelt.threaded``.\"\"\"\nimport multiprocessing\nimport requests\n\nfrom . import thread\nfrom .._compat import queue\n\n\nclass Pool(object):\n    \"\"\"Pool that manages the threads containing sessions.\n\n    :param job_queue:\n        The queue you're expected to use to which you should add items.\n    :type job_queue: queue.Queue\n    :param initializer:\n        Function used to initialize an instance of ``session``.\n    :type initializer: collections.Callable\n    :param auth_generator:\n        Function used to generate new auth credentials for the session.\n    :type auth_generator: collections.Callable\n    :param int num_processes:\n        Number of threads to create.\n    :param session:\n    :type session: requests.Session\n    \"\"\"\n\n    def __init__(self, job_queue, initializer=None, auth_generator=None,\n                 num_processes=None, session=requests.Session):\n        if num_processes is None:\n            num_processes = multiprocessing.cpu_count() or 1\n\n        if num_processes < 1:\n            raise ValueError(\"Number of processes should at least be 1.\")\n\n        self._job_queue = job_queue\n        self._response_queue = queue.Queue()\n        self._exc_queue = queue.Queue()\n        self._processes = num_processes\n        self._initializer = initializer or _identity\n        self._auth = auth_generator or _identity\n        self._session = session\n        self._pool = [\n            thread.SessionThread(self._new_session(), self._job_queue,\n                                 self._response_queue, self._exc_queue)\n            for _ in range(self._processes)\n        ]\n\n    def _new_session(self):\n        return self._auth(self._initializer(self._session()))\n\n    @classmethod\n    def from_exceptions(cls, exceptions, **kwargs):\n        r\"\"\"Create a :class:`~Pool` from an :class:`~ThreadException`\\ s.\n\n        Provided an iterable that provides :class:`~ThreadException` objects,\n        this classmethod will generate a new pool to retry the requests that\n        caused the exceptions.\n\n        :param exceptions:\n            Iterable that returns :class:`~ThreadException`\n        :type exceptions: iterable\n        :param kwargs:\n            Keyword arguments passed to the :class:`~Pool` initializer.\n        :returns: An initialized :class:`~Pool` object.\n        :rtype: :class:`~Pool`\n        \"\"\"\n        job_queue = queue.Queue()\n        for exc in exceptions:\n            job_queue.put(exc.request_kwargs)\n\n        return cls(job_queue=job_queue, **kwargs)\n\n    @classmethod\n    def from_urls(cls, urls, request_kwargs=None, **kwargs):\n        \"\"\"Create a :class:`~Pool` from an iterable of URLs.\n\n        :param urls:\n            Iterable that returns URLs with which we create a pool.\n        :type urls: iterable\n        :param dict request_kwargs:\n            Dictionary of other keyword arguments to provide to the request\n            method.\n        :param kwargs:\n            Keyword arguments passed to the :class:`~Pool` initializer.\n        :returns: An initialized :class:`~Pool` object.\n        :rtype: :class:`~Pool`\n        \"\"\"\n        request_dict = {'method': 'GET'}\n        request_dict.update(request_kwargs or {})\n        job_queue = queue.Queue()\n        for url in urls:\n            job = request_dict.copy()\n            job.update({'url': url})\n            job_queue.put(job)\n\n        return cls(job_queue=job_queue, **kwargs)\n\n    def exceptions(self):\n        \"\"\"Iterate over all the exceptions in the pool.\n\n        :returns: Generator of :class:`~ThreadException`\n        \"\"\"\n        while True:\n            exc = self.get_exception()\n            if exc is None:\n                break\n            yield exc\n\n    def get_exception(self):\n        \"\"\"Get an exception from the pool.\n\n        :rtype: :class:`~ThreadException`\n        \"\"\"\n        try:\n            (request, exc) = self._exc_queue.get_nowait()\n        except queue.Empty:\n            return None\n        else:\n            return ThreadException(request, exc)\n\n    def get_response(self):\n        \"\"\"Get a response from the pool.\n\n        :rtype: :class:`~ThreadResponse`\n        \"\"\"\n        try:\n            (request, response) = self._response_queue.get_nowait()\n        except queue.Empty:\n            return None\n        else:\n            return ThreadResponse(request, response)\n\n    def responses(self):\n        \"\"\"Iterate over all the responses in the pool.\n\n        :returns: Generator of :class:`~ThreadResponse`\n        \"\"\"\n        while True:\n            resp = self.get_response()\n            if resp is None:\n                break\n            yield resp\n\n    def join_all(self):\n        \"\"\"Join all the threads to the master thread.\"\"\"\n        for session_thread in self._pool:\n            session_thread.join()\n\n\nclass ThreadProxy(object):\n    proxied_attr = None\n\n    def __getattr__(self, attr):\n        \"\"\"Proxy attribute accesses to the proxied object.\"\"\"\n        get = object.__getattribute__\n        if attr not in self.attrs:\n            response = get(self, self.proxied_attr)\n            return getattr(response, attr)\n        else:\n            return get(self, attr)\n\n\nclass ThreadResponse(ThreadProxy):\n    \"\"\"A wrapper around a requests Response object.\n\n    This will proxy most attribute access actions to the Response object. For\n    example, if you wanted the parsed JSON from the response, you might do:\n\n    .. code-block:: python\n\n        thread_response = pool.get_response()\n        json = thread_response.json()\n\n    \"\"\"\n    proxied_attr = 'response'\n    attrs = frozenset(['request_kwargs', 'response'])\n\n    def __init__(self, request_kwargs, response):\n        #: The original keyword arguments provided to the queue\n        self.request_kwargs = request_kwargs\n        #: The wrapped response\n        self.response = response\n\n\nclass ThreadException(ThreadProxy):\n    \"\"\"A wrapper around an exception raised during a request.\n\n    This will proxy most attribute access actions to the exception object. For\n    example, if you wanted the message from the exception, you might do:\n\n    .. code-block:: python\n\n        thread_exc = pool.get_exception()\n        msg = thread_exc.message\n\n    \"\"\"\n    proxied_attr = 'exception'\n    attrs = frozenset(['request_kwargs', 'exception'])\n\n    def __init__(self, request_kwargs, exception):\n        #: The original keyword arguments provided to the queue\n        self.request_kwargs = request_kwargs\n        #: The captured and wrapped exception\n        self.exception = exception\n\n\ndef _identity(session_obj):\n    return session_obj\n\n\n__all__ = ['ThreadException', 'ThreadResponse', 'Pool']\n", "requests_toolbelt/threaded/thread.py": "\"\"\"Module containing the SessionThread class.\"\"\"\nimport threading\nimport uuid\n\nimport requests.exceptions as exc\n\nfrom .._compat import queue\n\n\nclass SessionThread(object):\n    def __init__(self, initialized_session, job_queue, response_queue,\n                 exception_queue):\n        self._session = initialized_session\n        self._jobs = job_queue\n        self._create_worker()\n        self._responses = response_queue\n        self._exceptions = exception_queue\n\n    def _create_worker(self):\n        self._worker = threading.Thread(\n            target=self._make_request,\n            name=uuid.uuid4(),\n        )\n        self._worker.daemon = True\n        self._worker._state = 0\n        self._worker.start()\n\n    def _handle_request(self, kwargs):\n        try:\n            response = self._session.request(**kwargs)\n        except exc.RequestException as e:\n            self._exceptions.put((kwargs, e))\n        else:\n            self._responses.put((kwargs, response))\n        finally:\n            self._jobs.task_done()\n\n    def _make_request(self):\n        while True:\n            try:\n                kwargs = self._jobs.get_nowait()\n            except queue.Empty:\n                break\n\n            self._handle_request(kwargs)\n\n    def is_alive(self):\n        \"\"\"Proxy to the thread's ``is_alive`` method.\"\"\"\n        return self._worker.is_alive()\n\n    def join(self):\n        \"\"\"Join this thread to the master thread.\"\"\"\n        self._worker.join()\n", "requests_toolbelt/threaded/__init__.py": "\"\"\"\nThis module provides the API for ``requests_toolbelt.threaded``.\n\nThe module provides a clean and simple API for making requests via a thread\npool. The thread pool will use sessions for increased performance.\n\nA simple use-case is:\n\n.. code-block:: python\n\n    from requests_toolbelt import threaded\n\n    urls_to_get = [{\n        'url': 'https://api.github.com/users/sigmavirus24',\n        'method': 'GET',\n    }, {\n        'url': 'https://api.github.com/repos/requests/toolbelt',\n        'method': 'GET',\n    }, {\n        'url': 'https://google.com',\n        'method': 'GET',\n    }]\n    responses, errors = threaded.map(urls_to_get)\n\nBy default, the threaded submodule will detect the number of CPUs your\ncomputer has and use that if no other number of processes is selected. To\nchange this, always use the keyword argument ``num_processes``. Using the\nabove example, we would expand it like so:\n\n.. code-block:: python\n\n    responses, errors = threaded.map(urls_to_get, num_processes=10)\n\nYou can also customize how a :class:`requests.Session` is initialized by\ncreating a callback function:\n\n.. code-block:: python\n\n    from requests_toolbelt import user_agent\n\n    def initialize_session(session):\n        session.headers['User-Agent'] = user_agent('my-scraper', '0.1')\n        session.headers['Accept'] = 'application/json'\n\n    responses, errors = threaded.map(urls_to_get,\n                                     initializer=initialize_session)\n\n.. autofunction:: requests_toolbelt.threaded.map\n\nInspiration is blatantly drawn from the standard library's multiprocessing\nlibrary. See the following references:\n\n- multiprocessing's `pool source`_\n\n- map and map_async `inspiration`_\n\n.. _pool source:\n    https://hg.python.org/cpython/file/8ef4f75a8018/Lib/multiprocessing/pool.py\n.. _inspiration:\n    https://hg.python.org/cpython/file/8ef4f75a8018/Lib/multiprocessing/pool.py#l340\n\"\"\"\nfrom . import pool\nfrom .._compat import queue\n\n\ndef map(requests, **kwargs):\n    r\"\"\"Simple interface to the threaded Pool object.\n\n    This function takes a list of dictionaries representing requests to make\n    using Sessions in threads and returns a tuple where the first item is\n    a generator of successful responses and the second is a generator of\n    exceptions.\n\n    :param list requests:\n        Collection of dictionaries representing requests to make with the Pool\n        object.\n    :param \\*\\*kwargs:\n        Keyword arguments that are passed to the\n        :class:`~requests_toolbelt.threaded.pool.Pool` object.\n    :returns: Tuple of responses and exceptions from the pool\n    :rtype: (:class:`~requests_toolbelt.threaded.pool.ThreadResponse`,\n        :class:`~requests_toolbelt.threaded.pool.ThreadException`)\n    \"\"\"\n    if not (requests and all(isinstance(r, dict) for r in requests)):\n        raise ValueError('map expects a list of dictionaries.')\n\n    # Build our queue of requests\n    job_queue = queue.Queue()\n    for request in requests:\n        job_queue.put(request)\n\n    # Ensure the user doesn't try to pass their own job_queue\n    kwargs['job_queue'] = job_queue\n\n    threadpool = pool.Pool(**kwargs)\n    threadpool.join_all()\n    return threadpool.responses(), threadpool.exceptions()\n", "requests_toolbelt/auth/guess.py": "# -*- coding: utf-8 -*-\n\"\"\"The module containing the code for GuessAuth.\"\"\"\nfrom requests import auth\nfrom requests import cookies\n\nfrom . import _digest_auth_compat as auth_compat, http_proxy_digest\n\n\nclass GuessAuth(auth.AuthBase):\n    \"\"\"Guesses the auth type by the WWW-Authentication header.\"\"\"\n    def __init__(self, username, password):\n        self.username = username\n        self.password = password\n        self.auth = None\n        self.pos = None\n\n    def _handle_basic_auth_401(self, r, kwargs):\n        if self.pos is not None:\n            r.request.body.seek(self.pos)\n\n        # Consume content and release the original connection\n        # to allow our new request to reuse the same one.\n        r.content\n        r.raw.release_conn()\n        prep = r.request.copy()\n        if not hasattr(prep, '_cookies'):\n            prep._cookies = cookies.RequestsCookieJar()\n        cookies.extract_cookies_to_jar(prep._cookies, r.request, r.raw)\n        prep.prepare_cookies(prep._cookies)\n\n        self.auth = auth.HTTPBasicAuth(self.username, self.password)\n        prep = self.auth(prep)\n        _r = r.connection.send(prep, **kwargs)\n        _r.history.append(r)\n        _r.request = prep\n\n        return _r\n\n    def _handle_digest_auth_401(self, r, kwargs):\n        self.auth = auth_compat.HTTPDigestAuth(self.username, self.password)\n        try:\n            self.auth.init_per_thread_state()\n        except AttributeError:\n            # If we're not on requests 2.8.0+ this method does not exist and\n            # is not relevant.\n            pass\n\n        # Check that the attr exists because much older versions of requests\n        # set this attribute lazily. For example:\n        # https://github.com/kennethreitz/requests/blob/33735480f77891754304e7f13e3cdf83aaaa76aa/requests/auth.py#L59\n        if (hasattr(self.auth, 'num_401_calls') and\n                self.auth.num_401_calls is None):\n            self.auth.num_401_calls = 1\n        # Digest auth would resend the request by itself. We can take a\n        # shortcut here.\n        return self.auth.handle_401(r, **kwargs)\n\n    def handle_401(self, r, **kwargs):\n        \"\"\"Resends a request with auth headers, if needed.\"\"\"\n\n        www_authenticate = r.headers.get('www-authenticate', '').lower()\n\n        if 'basic' in www_authenticate:\n            return self._handle_basic_auth_401(r, kwargs)\n\n        if 'digest' in www_authenticate:\n            return self._handle_digest_auth_401(r, kwargs)\n\n    def __call__(self, request):\n        if self.auth is not None:\n            return self.auth(request)\n\n        try:\n            self.pos = request.body.tell()\n        except AttributeError:\n            pass\n\n        request.register_hook('response', self.handle_401)\n        return request\n\n\nclass GuessProxyAuth(GuessAuth):\n    \"\"\"\n    Guesses the auth type by WWW-Authentication and Proxy-Authentication\n    headers\n    \"\"\"\n    def __init__(self, username=None, password=None,\n                 proxy_username=None, proxy_password=None):\n        super(GuessProxyAuth, self).__init__(username, password)\n        self.proxy_username = proxy_username\n        self.proxy_password = proxy_password\n        self.proxy_auth = None\n\n    def _handle_basic_auth_407(self, r, kwargs):\n        if self.pos is not None:\n            r.request.body.seek(self.pos)\n\n        r.content\n        r.raw.release_conn()\n        prep = r.request.copy()\n        if not hasattr(prep, '_cookies'):\n            prep._cookies = cookies.RequestsCookieJar()\n        cookies.extract_cookies_to_jar(prep._cookies, r.request, r.raw)\n        prep.prepare_cookies(prep._cookies)\n\n        self.proxy_auth = auth.HTTPProxyAuth(self.proxy_username,\n                                             self.proxy_password)\n        prep = self.proxy_auth(prep)\n        _r = r.connection.send(prep, **kwargs)\n        _r.history.append(r)\n        _r.request = prep\n\n        return _r\n\n    def _handle_digest_auth_407(self, r, kwargs):\n        self.proxy_auth = http_proxy_digest.HTTPProxyDigestAuth(\n            username=self.proxy_username,\n            password=self.proxy_password)\n\n        try:\n            self.auth.init_per_thread_state()\n        except AttributeError:\n            pass\n\n        return self.proxy_auth.handle_407(r, **kwargs)\n\n    def handle_407(self, r, **kwargs):\n        proxy_authenticate = r.headers.get('Proxy-Authenticate', '').lower()\n\n        if 'basic' in proxy_authenticate:\n            return self._handle_basic_auth_407(r, kwargs)\n\n        if 'digest' in proxy_authenticate:\n            return self._handle_digest_auth_407(r, kwargs)\n\n    def __call__(self, request):\n        if self.proxy_auth is not None:\n            request = self.proxy_auth(request)\n\n        try:\n            self.pos = request.body.tell()\n        except AttributeError:\n            pass\n\n        request.register_hook('response', self.handle_407)\n        return super(GuessProxyAuth, self).__call__(request)\n", "requests_toolbelt/auth/http_proxy_digest.py": "# -*- coding: utf-8 -*-\n\"\"\"The module containing HTTPProxyDigestAuth.\"\"\"\nimport re\n\nfrom requests import cookies, utils\n\nfrom . import _digest_auth_compat as auth\n\n\nclass HTTPProxyDigestAuth(auth.HTTPDigestAuth):\n    \"\"\"HTTP digest authentication between proxy\n\n    :param stale_rejects: The number of rejects indicate that:\n        the client may wish to simply retry the request\n        with a new encrypted response, without reprompting the user for a\n        new username and password. i.e., retry build_digest_header\n    :type stale_rejects: int\n    \"\"\"\n    _pat = re.compile(r'digest ', flags=re.IGNORECASE)\n\n    def __init__(self, *args, **kwargs):\n        super(HTTPProxyDigestAuth, self).__init__(*args, **kwargs)\n        self.stale_rejects = 0\n\n        self.init_per_thread_state()\n\n    @property\n    def stale_rejects(self):\n        thread_local = getattr(self, '_thread_local', None)\n        if thread_local is None:\n            return self._stale_rejects\n        return thread_local.stale_rejects\n\n    @stale_rejects.setter\n    def stale_rejects(self, value):\n        thread_local = getattr(self, '_thread_local', None)\n        if thread_local is None:\n            self._stale_rejects = value\n        else:\n            thread_local.stale_rejects = value\n\n    def init_per_thread_state(self):\n        try:\n            super(HTTPProxyDigestAuth, self).init_per_thread_state()\n        except AttributeError:\n            # If we're not on requests 2.8.0+ this method does not exist\n            pass\n\n    def handle_407(self, r, **kwargs):\n        \"\"\"Handle HTTP 407 only once, otherwise give up\n\n        :param r: current response\n        :returns: responses, along with the new response\n        \"\"\"\n        if r.status_code == 407 and self.stale_rejects < 2:\n            s_auth = r.headers.get(\"proxy-authenticate\")\n            if s_auth is None:\n                raise IOError(\n                    \"proxy server violated RFC 7235:\"\n                    \"407 response MUST contain header proxy-authenticate\")\n            elif not self._pat.match(s_auth):\n                return r\n\n            self.chal = utils.parse_dict_header(\n                self._pat.sub('', s_auth, count=1))\n\n            # if we present the user/passwd and still get rejected\n            # https://tools.ietf.org/html/rfc2617#section-3.2.1\n            if ('Proxy-Authorization' in r.request.headers and\n                    'stale' in self.chal):\n                if self.chal['stale'].lower() == 'true':  # try again\n                    self.stale_rejects += 1\n                # wrong user/passwd\n                elif self.chal['stale'].lower() == 'false':\n                    raise IOError(\"User or password is invalid\")\n\n            # Consume content and release the original connection\n            # to allow our new request to reuse the same one.\n            r.content\n            r.close()\n            prep = r.request.copy()\n            cookies.extract_cookies_to_jar(prep._cookies, r.request, r.raw)\n            prep.prepare_cookies(prep._cookies)\n\n            prep.headers['Proxy-Authorization'] = self.build_digest_header(\n                prep.method, prep.url)\n            _r = r.connection.send(prep, **kwargs)\n            _r.history.append(r)\n            _r.request = prep\n\n            return _r\n        else:  # give up authenticate\n            return r\n\n    def __call__(self, r):\n        self.init_per_thread_state()\n        # if we have nonce, then just use it, otherwise server will tell us\n        if self.last_nonce:\n            r.headers['Proxy-Authorization'] = self.build_digest_header(\n                r.method, r.url\n            )\n        r.register_hook('response', self.handle_407)\n        return r\n", "requests_toolbelt/auth/_digest_auth_compat.py": "\"\"\"Provide a compatibility layer for requests.auth.HTTPDigestAuth.\"\"\"\nimport requests\n\n\nclass _ThreadingDescriptor(object):\n    def __init__(self, prop, default):\n        self.prop = prop\n        self.default = default\n\n    def __get__(self, obj, objtype=None):\n        return getattr(obj._thread_local, self.prop, self.default)\n\n    def __set__(self, obj, value):\n        setattr(obj._thread_local, self.prop, value)\n\n\nclass _HTTPDigestAuth(requests.auth.HTTPDigestAuth):\n    init = _ThreadingDescriptor('init', True)\n    last_nonce = _ThreadingDescriptor('last_nonce', '')\n    nonce_count = _ThreadingDescriptor('nonce_count', 0)\n    chal = _ThreadingDescriptor('chal', {})\n    pos = _ThreadingDescriptor('pos', None)\n    num_401_calls = _ThreadingDescriptor('num_401_calls', 1)\n\n\nif requests.__build__ < 0x020800:\n    HTTPDigestAuth = requests.auth.HTTPDigestAuth\nelse:\n    HTTPDigestAuth = _HTTPDigestAuth\n", "requests_toolbelt/auth/http_bearer.py": "# -*- coding: utf-8 -*-\n\"\"\"The module containing HTTPBearerAuth.\"\"\"\n\nfrom requests.auth import AuthBase\n\n\nclass HTTPBearerAuth(AuthBase):\n    \"\"\"HTTP Bearer Token Authentication\n    \"\"\"\n\n    def __init__(self, token):\n        self.token = token\n\n    def __eq__(self, other):\n        return self.token == getattr(other, 'token', None)\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __call__(self, r):\n        r.headers['Authorization'] = 'Bearer ' + self.token\n        return r\n", "requests_toolbelt/auth/__init__.py": "", "requests_toolbelt/auth/handler.py": "# -*- coding: utf-8 -*-\n\"\"\"\n\nrequests_toolbelt.auth.handler\n==============================\n\nThis holds all of the implementation details of the Authentication Handler.\n\n\"\"\"\n\nfrom requests.auth import AuthBase, HTTPBasicAuth\nfrom requests.compat import urlparse, urlunparse\n\n\nclass AuthHandler(AuthBase):\n\n    \"\"\"\n\n    The ``AuthHandler`` object takes a dictionary of domains paired with\n    authentication strategies and will use this to determine which credentials\n    to use when making a request. For example, you could do the following:\n\n    .. code-block:: python\n\n        from requests import HTTPDigestAuth\n        from requests_toolbelt.auth.handler import AuthHandler\n\n        import requests\n\n        auth = AuthHandler({\n            'https://api.github.com': ('sigmavirus24', 'fakepassword'),\n            'https://example.com': HTTPDigestAuth('username', 'password')\n        })\n\n        r = requests.get('https://api.github.com/user', auth=auth)\n        # => <Response [200]>\n        r = requests.get('https://example.com/some/path', auth=auth)\n        # => <Response [200]>\n\n        s = requests.Session()\n        s.auth = auth\n        r = s.get('https://api.github.com/user')\n        # => <Response [200]>\n\n    .. warning::\n\n        :class:`requests.auth.HTTPDigestAuth` is not yet thread-safe. If you\n        use :class:`AuthHandler` across multiple threads you should\n        instantiate a new AuthHandler for each thread with a new\n        HTTPDigestAuth instance for each thread.\n\n    \"\"\"\n\n    def __init__(self, strategies):\n        self.strategies = dict(strategies)\n        self._make_uniform()\n\n    def __call__(self, request):\n        auth = self.get_strategy_for(request.url)\n        return auth(request)\n\n    def __repr__(self):\n        return '<AuthHandler({!r})>'.format(self.strategies)\n\n    def _make_uniform(self):\n        existing_strategies = list(self.strategies.items())\n        self.strategies = {}\n\n        for (k, v) in existing_strategies:\n            self.add_strategy(k, v)\n\n    @staticmethod\n    def _key_from_url(url):\n        parsed = urlparse(url)\n        return urlunparse((parsed.scheme.lower(),\n                           parsed.netloc.lower(),\n                           '', '', '', ''))\n\n    def add_strategy(self, domain, strategy):\n        \"\"\"Add a new domain and authentication strategy.\n\n        :param str domain: The domain you wish to match against. For example:\n            ``'https://api.github.com'``\n        :param str strategy: The authentication strategy you wish to use for\n            that domain. For example: ``('username', 'password')`` or\n            ``requests.HTTPDigestAuth('username', 'password')``\n\n        .. code-block:: python\n\n            a = AuthHandler({})\n            a.add_strategy('https://api.github.com', ('username', 'password'))\n\n        \"\"\"\n        # Turn tuples into Basic Authentication objects\n        if isinstance(strategy, tuple):\n            strategy = HTTPBasicAuth(*strategy)\n\n        key = self._key_from_url(domain)\n        self.strategies[key] = strategy\n\n    def get_strategy_for(self, url):\n        \"\"\"Retrieve the authentication strategy for a specified URL.\n\n        :param str url: The full URL you will be making a request against. For\n            example, ``'https://api.github.com/user'``\n        :returns: Callable that adds authentication to a request.\n\n        .. code-block:: python\n\n            import requests\n            a = AuthHandler({'example.com', ('foo', 'bar')})\n            strategy = a.get_strategy_for('http://example.com/example')\n            assert isinstance(strategy, requests.auth.HTTPBasicAuth)\n\n        \"\"\"\n        key = self._key_from_url(url)\n        return self.strategies.get(key, NullAuthStrategy())\n\n    def remove_strategy(self, domain):\n        \"\"\"Remove the domain and strategy from the collection of strategies.\n\n        :param str domain: The domain you wish remove. For example,\n            ``'https://api.github.com'``.\n\n        .. code-block:: python\n\n            a = AuthHandler({'example.com', ('foo', 'bar')})\n            a.remove_strategy('example.com')\n            assert a.strategies == {}\n\n        \"\"\"\n        key = self._key_from_url(domain)\n        if key in self.strategies:\n            del self.strategies[key]\n\n\nclass NullAuthStrategy(AuthBase):\n    def __repr__(self):\n        return '<NullAuthStrategy>'\n\n    def __call__(self, r):\n        return r\n", "requests_toolbelt/downloadutils/tee.py": "\"\"\"Tee function implementations.\"\"\"\nimport io\n\n_DEFAULT_CHUNKSIZE = 65536\n\n__all__ = ['tee', 'tee_to_file', 'tee_to_bytearray']\n\n\ndef _tee(response, callback, chunksize, decode_content):\n    for chunk in response.raw.stream(amt=chunksize,\n                                     decode_content=decode_content):\n        callback(chunk)\n        yield chunk\n\n\ndef tee(response, fileobject, chunksize=_DEFAULT_CHUNKSIZE,\n        decode_content=None):\n    \"\"\"Stream the response both to the generator and a file.\n\n    This will stream the response body while writing the bytes to\n    ``fileobject``.\n\n    Example usage:\n\n    .. code-block:: python\n\n        resp = requests.get(url, stream=True)\n        with open('save_file', 'wb') as save_file:\n            for chunk in tee(resp, save_file):\n                # do stuff with chunk\n\n    .. code-block:: python\n\n        import io\n\n        resp = requests.get(url, stream=True)\n        fileobject = io.BytesIO()\n\n        for chunk in tee(resp, fileobject):\n            # do stuff with chunk\n\n    :param response: Response from requests.\n    :type response: requests.Response\n    :param fileobject: Writable file-like object.\n    :type fileobject: file, io.BytesIO\n    :param int chunksize: (optional), Size of chunk to attempt to stream.\n    :param bool decode_content: (optional), If True, this will decode the\n        compressed content of the response.\n    :raises: TypeError if the fileobject wasn't opened with the right mode\n        or isn't a BytesIO object.\n    \"\"\"\n    # We will be streaming the raw bytes from over the wire, so we need to\n    # ensure that writing to the fileobject will preserve those bytes. On\n    # Python3, if the user passes an io.StringIO, this will fail, so we need\n    # to check for BytesIO instead.\n    if not ('b' in getattr(fileobject, 'mode', '') or\n            isinstance(fileobject, io.BytesIO)):\n        raise TypeError('tee() will write bytes directly to this fileobject'\n                        ', it must be opened with the \"b\" flag if it is a file'\n                        ' or inherit from io.BytesIO.')\n\n    return _tee(response, fileobject.write, chunksize, decode_content)\n\n\ndef tee_to_file(response, filename, chunksize=_DEFAULT_CHUNKSIZE,\n                decode_content=None):\n    \"\"\"Stream the response both to the generator and a file.\n\n    This will open a file named ``filename`` and stream the response body\n    while writing the bytes to the opened file object.\n\n    Example usage:\n\n    .. code-block:: python\n\n        resp = requests.get(url, stream=True)\n        for chunk in tee_to_file(resp, 'save_file'):\n            # do stuff with chunk\n\n    :param response: Response from requests.\n    :type response: requests.Response\n    :param str filename: Name of file in which we write the response content.\n    :param int chunksize: (optional), Size of chunk to attempt to stream.\n    :param bool decode_content: (optional), If True, this will decode the\n        compressed content of the response.\n    \"\"\"\n    with open(filename, 'wb') as fd:\n        for chunk in tee(response, fd, chunksize, decode_content):\n            yield chunk\n\n\ndef tee_to_bytearray(response, bytearr, chunksize=_DEFAULT_CHUNKSIZE,\n                     decode_content=None):\n    \"\"\"Stream the response both to the generator and a bytearray.\n\n    This will stream the response provided to the function, add them to the\n    provided :class:`bytearray` and yield them to the user.\n\n    .. note::\n\n        This uses the :meth:`bytearray.extend` by default instead of passing\n        the bytearray into the ``readinto`` method.\n\n    Example usage:\n\n    .. code-block:: python\n\n        b = bytearray()\n        resp = requests.get(url, stream=True)\n        for chunk in tee_to_bytearray(resp, b):\n            # do stuff with chunk\n\n    :param response: Response from requests.\n    :type response: requests.Response\n    :param bytearray bytearr: Array to add the streamed bytes to.\n    :param int chunksize: (optional), Size of chunk to attempt to stream.\n    :param bool decode_content: (optional), If True, this will decode the\n        compressed content of the response.\n    \"\"\"\n    if not isinstance(bytearr, bytearray):\n        raise TypeError('tee_to_bytearray() expects bytearr to be a '\n                        'bytearray')\n    return _tee(response, bytearr.extend, chunksize, decode_content)\n", "requests_toolbelt/downloadutils/stream.py": "# -*- coding: utf-8 -*-\n\"\"\"Utilities for dealing with streamed requests.\"\"\"\nimport os.path\nimport re\n\nfrom .. import exceptions as exc\n\n# Regular expressions stolen from werkzeug/http.py\n# cd2c97bb0a076da2322f11adce0b2731f9193396 L62-L64\n_QUOTED_STRING_RE = r'\"[^\"\\\\]*(?:\\\\.[^\"\\\\]*)*\"'\n_OPTION_HEADER_PIECE_RE = re.compile(\n    r';\\s*(%s|[^\\s;=]+)\\s*(?:=\\s*(%s|[^;]+))?\\s*' % (_QUOTED_STRING_RE,\n                                                     _QUOTED_STRING_RE)\n)\n_DEFAULT_CHUNKSIZE = 512\n\n\ndef _get_filename(content_disposition):\n    for match in _OPTION_HEADER_PIECE_RE.finditer(content_disposition):\n        k, v = match.groups()\n        if k == 'filename':\n            # ignore any directory paths in the filename\n            return os.path.split(v)[1]\n    return None\n\n\ndef get_download_file_path(response, path):\n    \"\"\"\n    Given a response and a path, return a file path for a download.\n\n    If a ``path`` parameter is a directory, this function will parse the\n    ``Content-Disposition`` header on the response to determine the name of the\n    file as reported by the server, and return a file path in the specified\n    directory.\n\n    If ``path`` is empty or None, this function will return a path relative\n    to the process' current working directory.\n\n    If path is a full file path, return it.\n\n    :param response: A Response object from requests\n    :type response: requests.models.Response\n    :param str path: Directory or file path.\n    :returns: full file path to download as\n    :rtype: str\n    :raises: :class:`requests_toolbelt.exceptions.StreamingError`\n    \"\"\"\n    path_is_dir = path and os.path.isdir(path)\n\n    if path and not path_is_dir:\n        # fully qualified file path\n        filepath = path\n    else:\n        response_filename = _get_filename(\n            response.headers.get('content-disposition', '')\n        )\n        if not response_filename:\n            raise exc.StreamingError('No filename given to stream response to')\n\n        if path_is_dir:\n            # directory to download to\n            filepath = os.path.join(path, response_filename)\n        else:\n            # fallback to downloading to current working directory\n            filepath = response_filename\n\n    return filepath\n\n\ndef stream_response_to_file(response, path=None, chunksize=_DEFAULT_CHUNKSIZE):\n    \"\"\"Stream a response body to the specified file.\n\n    Either use the ``path`` provided or use the name provided in the\n    ``Content-Disposition`` header.\n\n    .. warning::\n\n        If you pass this function an open file-like object as the ``path``\n        parameter, the function will not close that file for you.\n\n    .. warning::\n\n        This function will not automatically close the response object\n        passed in as the ``response`` parameter.\n\n    If a ``path`` parameter is a directory, this function will parse the\n    ``Content-Disposition`` header on the response to determine the name of the\n    file as reported by the server, and return a file path in the specified\n    directory. If no ``path`` parameter is supplied, this function will default\n    to the process' current working directory.\n\n    .. code-block:: python\n\n        import requests\n        from requests_toolbelt import exceptions\n        from requests_toolbelt.downloadutils import stream\n\n        r = requests.get(url, stream=True)\n        try:\n            filename = stream.stream_response_to_file(r)\n        except exceptions.StreamingError as e:\n            # The toolbelt could not find the filename in the\n            # Content-Disposition\n            print(e.message)\n\n    You can also specify the filename as a string. This will be passed to\n    the built-in :func:`open` and we will read the content into the file.\n\n    .. code-block:: python\n\n        import requests\n        from requests_toolbelt.downloadutils import stream\n\n        r = requests.get(url, stream=True)\n        filename = stream.stream_response_to_file(r, path='myfile')\n\n    If the calculated download file path already exists, this function will\n    raise a StreamingError.\n\n    Instead, if you want to manage the file object yourself, you need to\n    provide either a :class:`io.BytesIO` object or a file opened with the\n    `'b'` flag. See the two examples below for more details.\n\n    .. code-block:: python\n\n        import requests\n        from requests_toolbelt.downloadutils import stream\n\n        with open('myfile', 'wb') as fd:\n            r = requests.get(url, stream=True)\n            filename = stream.stream_response_to_file(r, path=fd)\n\n        print('{} saved to {}'.format(url, filename))\n\n    .. code-block:: python\n\n        import io\n        import requests\n        from requests_toolbelt.downloadutils import stream\n\n        b = io.BytesIO()\n        r = requests.get(url, stream=True)\n        filename = stream.stream_response_to_file(r, path=b)\n        assert filename is None\n\n    :param response: A Response object from requests\n    :type response: requests.models.Response\n    :param path: *(optional)*, Either a string with the path to the location\n        to save the response content, or a file-like object expecting bytes.\n    :type path: :class:`str`, or object with a :meth:`write`\n    :param int chunksize: (optional), Size of chunk to attempt to stream\n        (default 512B).\n    :returns: The name of the file, if one can be determined, else None\n    :rtype: str\n    :raises: :class:`requests_toolbelt.exceptions.StreamingError`\n    \"\"\"\n    pre_opened = False\n    fd = None\n    filename = None\n    if path and callable(getattr(path, 'write', None)):\n        pre_opened = True\n        fd = path\n        filename = getattr(fd, 'name', None)\n    else:\n        filename = get_download_file_path(response, path)\n        if os.path.exists(filename):\n            raise exc.StreamingError(\"File already exists: %s\" % filename)\n        fd = open(filename, 'wb')\n\n    for chunk in response.iter_content(chunk_size=chunksize):\n        fd.write(chunk)\n\n    if not pre_opened:\n        fd.close()\n\n    return filename\n", "requests_toolbelt/downloadutils/__init__.py": "", "requests_toolbelt/adapters/host_header_ssl.py": "# -*- coding: utf-8 -*-\n\"\"\"\nrequests_toolbelt.adapters.host_header_ssl\n==========================================\n\nThis file contains an implementation of the HostHeaderSSLAdapter.\n\"\"\"\n\nfrom requests.adapters import HTTPAdapter\n\n\nclass HostHeaderSSLAdapter(HTTPAdapter):\n    \"\"\"\n    A HTTPS Adapter for Python Requests that sets the hostname for certificate\n    verification based on the Host header.\n\n    This allows requesting the IP address directly via HTTPS without getting\n    a \"hostname doesn't match\" exception.\n\n    Example usage:\n\n        >>> s.mount('https://', HostHeaderSSLAdapter())\n        >>> s.get(\"https://93.184.216.34\", headers={\"Host\": \"example.org\"})\n\n    \"\"\"\n\n    def send(self, request, **kwargs):\n        # HTTP headers are case-insensitive (RFC 7230)\n        host_header = None\n        for header in request.headers:\n            if header.lower() == \"host\":\n                host_header = request.headers[header]\n                break\n\n        connection_pool_kwargs = self.poolmanager.connection_pool_kw\n\n        if host_header:\n            connection_pool_kwargs[\"assert_hostname\"] = host_header\n            connection_pool_kwargs[\"server_hostname\"] = host_header\n        elif \"assert_hostname\" in connection_pool_kwargs:\n            # an assert_hostname from a previous request may have been left\n            connection_pool_kwargs.pop(\"assert_hostname\", None)\n            connection_pool_kwargs.pop(\"server_hostname\", None)\n\n        return super(HostHeaderSSLAdapter, self).send(request, **kwargs)\n", "requests_toolbelt/adapters/x509.py": "# -*- coding: utf-8 -*-\n\"\"\"A X509Adapter for use with the requests library.\n\nThis file contains an implementation of the X509Adapter that will\nallow users to authenticate a request using an arbitrary\nX.509 certificate without needing to convert it to a .pem file\n\n\"\"\"\n\nfrom OpenSSL.crypto import PKey, X509\nfrom cryptography import x509\nfrom cryptography.hazmat.primitives.serialization import (load_pem_private_key,\n                                                          load_der_private_key)\nfrom cryptography.hazmat.primitives.serialization import Encoding\nfrom cryptography.hazmat.backends import default_backend\n\nfrom datetime import datetime, timezone\nfrom requests.adapters import HTTPAdapter\nimport requests\n\nfrom .. import exceptions as exc\n\n\"\"\"\nimporting the protocol constants from _ssl instead of ssl because only the\nconstants are needed and to handle issues caused by importing from ssl on\nthe 2.7.x line.\n\"\"\"\ntry:\n    from _ssl import PROTOCOL_TLS as PROTOCOL\nexcept ImportError:\n    from _ssl import PROTOCOL_SSLv23 as PROTOCOL\n\n\nPyOpenSSLContext = None\n\n\nclass X509Adapter(HTTPAdapter):\n    r\"\"\"Adapter for use with X.509 certificates.\n\n    Provides an interface for Requests sessions to contact HTTPS urls and\n    authenticate  with an X.509 cert by implementing the Transport Adapter\n    interface. This class will need to be manually instantiated and mounted\n    to the session\n\n    :param pool_connections: The number of urllib3 connection pools to\n           cache.\n    :param pool_maxsize: The maximum number of connections to save in the\n            pool.\n    :param max_retries: The maximum number of retries each connection\n        should attempt. Note, this applies only to failed DNS lookups,\n        socket connections and connection timeouts, never to requests where\n        data has made it to the server. By default, Requests does not retry\n        failed connections. If you need granular control over the\n        conditions under which we retry a request, import urllib3's\n        ``Retry`` class and pass that instead.\n    :param pool_block: Whether the connection pool should block for\n            connections.\n\n    :param bytes cert_bytes:\n        bytes object containing contents of a cryptography.x509Certificate\n        object using the encoding specified by the ``encoding`` parameter.\n    :param bytes pk_bytes:\n        bytes object containing contents of a object that implements\n        ``cryptography.hazmat.primitives.serialization.PrivateFormat``\n        using the encoding specified by the ``encoding`` parameter.\n    :param password:\n        string or utf8 encoded bytes containing the passphrase used for the\n        private key. None if unencrypted. Defaults to None.\n    :param encoding:\n        Enumeration detailing the encoding method used on the ``cert_bytes``\n        parameter. Can be either PEM or DER. Defaults to PEM.\n    :type encoding:\n        :class: `cryptography.hazmat.primitives.serialization.Encoding`\n\n    Usage::\n\n      >>> import requests\n      >>> from requests_toolbelt.adapters.x509 import X509Adapter\n      >>> s = requests.Session()\n      >>> a = X509Adapter(max_retries=3,\n                cert_bytes=b'...', pk_bytes=b'...', encoding='...'\n      >>> s.mount('https://', a)\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        self._import_pyopensslcontext()\n        self._check_version()\n        cert_bytes = kwargs.pop('cert_bytes', None)\n        pk_bytes = kwargs.pop('pk_bytes', None)\n        password = kwargs.pop('password', None)\n        encoding = kwargs.pop('encoding', Encoding.PEM)\n\n        password_bytes = None\n\n        if cert_bytes is None or not isinstance(cert_bytes, bytes):\n            raise ValueError('Invalid cert content provided. '\n                             'You must provide an X.509 cert '\n                             'formatted as a byte array.')\n        if pk_bytes is None or not isinstance(pk_bytes, bytes):\n            raise ValueError('Invalid private key content provided. '\n                             'You must provide a private key '\n                             'formatted as a byte array.')\n\n        if isinstance(password, bytes):\n            password_bytes = password\n        elif password:\n            password_bytes = password.encode('utf8')\n\n        self.ssl_context = create_ssl_context(cert_bytes, pk_bytes,\n                                              password_bytes, encoding)\n\n        super(X509Adapter, self).__init__(*args, **kwargs)\n\n    def init_poolmanager(self, *args, **kwargs):\n        if self.ssl_context:\n            kwargs['ssl_context'] = self.ssl_context\n        return super(X509Adapter, self).init_poolmanager(*args, **kwargs)\n\n    def proxy_manager_for(self, *args, **kwargs):\n        if self.ssl_context:\n            kwargs['ssl_context'] = self.ssl_context\n        return super(X509Adapter, self).proxy_manager_for(*args, **kwargs)\n\n    def _import_pyopensslcontext(self):\n        global PyOpenSSLContext\n\n        if requests.__build__ < 0x021200:\n            PyOpenSSLContext = None\n        else:\n            try:\n                from requests.packages.urllib3.contrib.pyopenssl \\\n                        import PyOpenSSLContext\n            except ImportError:\n                try:\n                    from urllib3.contrib.pyopenssl import PyOpenSSLContext\n                except ImportError:\n                    PyOpenSSLContext = None\n\n    def _check_version(self):\n        if PyOpenSSLContext is None:\n            raise exc.VersionMismatchError(\n                \"The X509Adapter requires at least Requests 2.12.0 to be \"\n                \"installed. Version {} was found instead.\".format(\n                    requests.__version__\n                )\n            )\n\n\ndef check_cert_dates(cert):\n    \"\"\"Verify that the supplied client cert is not invalid.\"\"\"\n\n    now = datetime.now(timezone.utc)\n    if cert.not_valid_after_utc < now or cert.not_valid_before_utc > now:\n        raise ValueError('Client certificate expired: Not After: '\n                         '{:%Y-%m-%d %H:%M:%SZ} '\n                         'Not Before: {:%Y-%m-%d %H:%M:%SZ}'\n                         .format(cert.not_valid_after, cert.not_valid_before))\n\n\ndef create_ssl_context(cert_byes, pk_bytes, password=None,\n                       encoding=Encoding.PEM):\n    \"\"\"Create an SSL Context with the supplied cert/password.\n\n    :param cert_bytes array of bytes containing the cert encoded\n           using the method supplied in the ``encoding`` parameter\n    :param pk_bytes array of bytes containing the private key encoded\n           using the method supplied in the ``encoding`` parameter\n    :param password array of bytes containing the passphrase to be used\n           with the supplied private key. None if unencrypted.\n           Defaults to None.\n    :param encoding ``cryptography.hazmat.primitives.serialization.Encoding``\n            details the encoding method used on the ``cert_bytes``  and\n            ``pk_bytes`` parameters. Can be either PEM or DER.\n            Defaults to PEM.\n    \"\"\"\n    backend = default_backend()\n\n    cert = None\n    key = None\n    if encoding == Encoding.PEM:\n        cert = x509.load_pem_x509_certificate(cert_byes, backend)\n        key = load_pem_private_key(pk_bytes, password, backend)\n    elif encoding == Encoding.DER:\n        cert = x509.load_der_x509_certificate(cert_byes, backend)\n        key = load_der_private_key(pk_bytes, password, backend)\n    else:\n        raise ValueError('Invalid encoding provided: Must be PEM or DER')\n\n    if not (cert and key):\n        raise ValueError('Cert and key could not be parsed from '\n                         'provided data')\n    check_cert_dates(cert)\n    ssl_context = PyOpenSSLContext(PROTOCOL)\n    ssl_context._ctx.use_certificate(X509.from_cryptography(cert))\n    ssl_context._ctx.use_privatekey(PKey.from_cryptography_key(key))\n    return ssl_context\n", "requests_toolbelt/adapters/source.py": "# -*- coding: utf-8 -*-\n\"\"\"\nrequests_toolbelt.source_adapter\n================================\n\nThis file contains an implementation of the SourceAddressAdapter originally\ndemonstrated on the Requests GitHub page.\n\"\"\"\nfrom requests.adapters import HTTPAdapter\n\nfrom .._compat import poolmanager, basestring\n\n\nclass SourceAddressAdapter(HTTPAdapter):\n    \"\"\"\n    A Source Address Adapter for Python Requests that enables you to choose the\n    local address to bind to. This allows you to send your HTTP requests from a\n    specific interface and IP address.\n\n    Two address formats are accepted. The first is a string: this will set the\n    local IP address to the address given in the string, and will also choose a\n    semi-random high port for the local port number.\n\n    The second is a two-tuple of the form (ip address, port): for example,\n    ``('10.10.10.10', 8999)``. This will set the local IP address to the first\n    element, and the local port to the second element. If ``0`` is used as the\n    port number, a semi-random high port will be selected.\n\n    .. warning:: Setting an explicit local port can have negative interactions\n                 with connection-pooling in Requests: in particular, it risks\n                 the possibility of getting \"Address in use\" errors. The\n                 string-only argument is generally preferred to the tuple-form.\n\n    Example usage:\n\n    .. code-block:: python\n\n        import requests\n        from requests_toolbelt.adapters.source import SourceAddressAdapter\n\n        s = requests.Session()\n        s.mount('http://', SourceAddressAdapter('10.10.10.10'))\n        s.mount('https://', SourceAddressAdapter(('10.10.10.10', 8999)))\n    \"\"\"\n    def __init__(self, source_address, **kwargs):\n        if isinstance(source_address, basestring):\n            self.source_address = (source_address, 0)\n        elif isinstance(source_address, tuple):\n            self.source_address = source_address\n        else:\n            raise TypeError(\n                \"source_address must be IP address string or (ip, port) tuple\"\n            )\n\n        super(SourceAddressAdapter, self).__init__(**kwargs)\n\n    def init_poolmanager(self, connections, maxsize, block=False):\n        self.poolmanager = poolmanager.PoolManager(\n            num_pools=connections,\n            maxsize=maxsize,\n            block=block,\n            source_address=self.source_address)\n\n    def proxy_manager_for(self, *args, **kwargs):\n        kwargs['source_address'] = self.source_address\n        return super(SourceAddressAdapter, self).proxy_manager_for(\n            *args, **kwargs)\n", "requests_toolbelt/adapters/fingerprint.py": "# -*- coding: utf-8 -*-\n\"\"\"Submodule containing the implementation for the FingerprintAdapter.\n\nThis file contains an implementation of a Transport Adapter that validates\nthe fingerprints of SSL certificates presented upon connection.\n\"\"\"\nfrom requests.adapters import HTTPAdapter\n\nfrom .._compat import poolmanager\n\n\nclass FingerprintAdapter(HTTPAdapter):\n    \"\"\"\n    A HTTPS Adapter for Python Requests that verifies certificate fingerprints,\n    instead of certificate hostnames.\n\n    Example usage:\n\n    .. code-block:: python\n\n        import requests\n        import ssl\n        from requests_toolbelt.adapters.fingerprint import FingerprintAdapter\n\n        twitter_fingerprint = '...'\n        s = requests.Session()\n        s.mount(\n            'https://twitter.com',\n            FingerprintAdapter(twitter_fingerprint)\n        )\n\n    The fingerprint should be provided as a hexadecimal string, optionally\n    containing colons.\n    \"\"\"\n\n    __attrs__ = HTTPAdapter.__attrs__ + ['fingerprint']\n\n    def __init__(self, fingerprint, **kwargs):\n        self.fingerprint = fingerprint\n\n        super(FingerprintAdapter, self).__init__(**kwargs)\n\n    def init_poolmanager(self, connections, maxsize, block=False):\n        self.poolmanager = poolmanager.PoolManager(\n            num_pools=connections,\n            maxsize=maxsize,\n            block=block,\n            assert_fingerprint=self.fingerprint)\n", "requests_toolbelt/adapters/ssl.py": "# -*- coding: utf-8 -*-\n\"\"\"\n\nrequests_toolbelt.ssl_adapter\n=============================\n\nThis file contains an implementation of the SSLAdapter originally demonstrated\nin this blog post:\nhttps://lukasa.co.uk/2013/01/Choosing_SSL_Version_In_Requests/\n\n\"\"\"\nimport requests\n\nfrom requests.adapters import HTTPAdapter\n\nfrom .._compat import poolmanager\n\n\nclass SSLAdapter(HTTPAdapter):\n    \"\"\"\n    A HTTPS Adapter for Python Requests that allows the choice of the SSL/TLS\n    version negotiated by Requests. This can be used either to enforce the\n    choice of high-security TLS versions (where supported), or to work around\n    misbehaving servers that fail to correctly negotiate the default TLS\n    version being offered.\n\n    Example usage:\n\n        >>> import requests\n        >>> import ssl\n        >>> from requests_toolbelt import SSLAdapter\n        >>> s = requests.Session()\n        >>> s.mount('https://', SSLAdapter(ssl.PROTOCOL_TLSv1))\n\n    You can replace the chosen protocol with any that are available in the\n    default Python SSL module. All subsequent requests that match the adapter\n    prefix will use the chosen SSL version instead of the default.\n\n    This adapter will also attempt to change the SSL/TLS version negotiated by\n    Requests when using a proxy. However, this may not always be possible:\n    prior to Requests v2.4.0 the adapter did not have access to the proxy setup\n    code. In earlier versions of Requests, this adapter will not function\n    properly when used with proxies.\n    \"\"\"\n\n    __attrs__ = HTTPAdapter.__attrs__ + ['ssl_version']\n\n    def __init__(self, ssl_version=None, **kwargs):\n        self.ssl_version = ssl_version\n\n        super(SSLAdapter, self).__init__(**kwargs)\n\n    def init_poolmanager(self, connections, maxsize, block=False):\n        self.poolmanager = poolmanager.PoolManager(\n            num_pools=connections,\n            maxsize=maxsize,\n            block=block,\n            ssl_version=self.ssl_version)\n\n    if requests.__build__ >= 0x020400:\n        # Earlier versions of requests either don't have this method or, worse,\n        # don't allow passing arbitrary keyword arguments. As a result, only\n        # conditionally define this method.\n        def proxy_manager_for(self, *args, **kwargs):\n            kwargs['ssl_version'] = self.ssl_version\n            return super(SSLAdapter, self).proxy_manager_for(*args, **kwargs)\n", "requests_toolbelt/adapters/socket_options.py": "# -*- coding: utf-8 -*-\n\"\"\"The implementation of the SocketOptionsAdapter.\"\"\"\nimport socket\nimport warnings\nimport sys\n\nimport requests\nfrom requests import adapters\n\nfrom .._compat import connection\nfrom .._compat import poolmanager\nfrom .. import exceptions as exc\n\n\nclass SocketOptionsAdapter(adapters.HTTPAdapter):\n    \"\"\"An adapter for requests that allows users to specify socket options.\n\n    Since version 2.4.0 of requests, it is possible to specify a custom list\n    of socket options that need to be set before establishing the connection.\n\n    Example usage::\n\n        >>> import socket\n        >>> import requests\n        >>> from requests_toolbelt.adapters import socket_options\n        >>> s = requests.Session()\n        >>> opts = [(socket.IPPROTO_TCP, socket.TCP_NODELAY, 0)]\n        >>> adapter = socket_options.SocketOptionsAdapter(socket_options=opts)\n        >>> s.mount('http://', adapter)\n\n    You can also take advantage of the list of default options on this class\n    to keep using the original options in addition to your custom options. In\n    that case, ``opts`` might look like::\n\n        >>> opts = socket_options.SocketOptionsAdapter.default_options + opts\n\n    \"\"\"\n\n    if connection is not None:\n        default_options = getattr(\n            connection.HTTPConnection,\n            'default_socket_options',\n            [(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)]\n        )\n    else:\n        default_options = []\n        warnings.warn(exc.RequestsVersionTooOld,\n                      \"This version of Requests is only compatible with a \"\n                      \"version of urllib3 which is too old to support \"\n                      \"setting options on a socket. This adapter is \"\n                      \"functionally useless.\")\n\n    def __init__(self, **kwargs):\n        self.socket_options = kwargs.pop('socket_options',\n                                         self.default_options)\n\n        super(SocketOptionsAdapter, self).__init__(**kwargs)\n\n    def init_poolmanager(self, connections, maxsize, block=False):\n        if requests.__build__ >= 0x020400:\n            # NOTE(Ian): Perhaps we should raise a warning\n            self.poolmanager = poolmanager.PoolManager(\n                num_pools=connections,\n                maxsize=maxsize,\n                block=block,\n                socket_options=self.socket_options\n            )\n        else:\n            super(SocketOptionsAdapter, self).init_poolmanager(\n                connections, maxsize, block\n            )\n\n\nclass TCPKeepAliveAdapter(SocketOptionsAdapter):\n    \"\"\"An adapter for requests that turns on TCP Keep-Alive by default.\n\n    The adapter sets 4 socket options:\n\n    - ``SOL_SOCKET`` ``SO_KEEPALIVE`` - This turns on TCP Keep-Alive\n    - ``IPPROTO_TCP`` ``TCP_KEEPINTVL`` 20 - Sets the keep alive interval\n    - ``IPPROTO_TCP`` ``TCP_KEEPCNT`` 5 - Sets the number of keep alive probes\n    - ``IPPROTO_TCP`` ``TCP_KEEPIDLE`` 60 - Sets the keep alive time if the\n      socket library has the ``TCP_KEEPIDLE`` constant\n\n    The latter three can be overridden by keyword arguments (respectively):\n\n    - ``interval``\n    - ``count``\n    - ``idle``\n\n    You can use this adapter like so::\n\n       >>> from requests_toolbelt.adapters import socket_options\n       >>> tcp = socket_options.TCPKeepAliveAdapter(idle=120, interval=10)\n       >>> s = requests.Session()\n       >>> s.mount('http://', tcp)\n\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        socket_options = kwargs.pop('socket_options',\n                                    SocketOptionsAdapter.default_options)\n        idle = kwargs.pop('idle', 60)\n        interval = kwargs.pop('interval', 20)\n        count = kwargs.pop('count', 5)\n        socket_options = socket_options + [\n            (socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)\n        ]\n\n        # NOTE(Ian): OSX does not have these constants defined, so we\n        # set them conditionally.\n        if getattr(socket, 'TCP_KEEPINTVL', None) is not None:\n            socket_options += [(socket.IPPROTO_TCP, socket.TCP_KEEPINTVL,\n                                interval)]\n        elif sys.platform == 'darwin':\n            # On OSX, TCP_KEEPALIVE from netinet/tcp.h is not exported\n            # by python's socket module\n            TCP_KEEPALIVE = getattr(socket, 'TCP_KEEPALIVE', 0x10)\n            socket_options += [(socket.IPPROTO_TCP, TCP_KEEPALIVE, interval)]\n\n        if getattr(socket, 'TCP_KEEPCNT', None) is not None:\n            socket_options += [(socket.IPPROTO_TCP, socket.TCP_KEEPCNT, count)]\n\n        if getattr(socket, 'TCP_KEEPIDLE', None) is not None:\n            socket_options += [(socket.IPPROTO_TCP, socket.TCP_KEEPIDLE, idle)]\n\n        super(TCPKeepAliveAdapter, self).__init__(\n            socket_options=socket_options, **kwargs\n        )\n", "requests_toolbelt/adapters/__init__.py": "# -*- coding: utf-8 -*-\n\"\"\"\nrequests-toolbelt.adapters\n==========================\n\nSee https://toolbelt.readthedocs.io/ for documentation\n\n:copyright: (c) 2014 by Ian Cordasco and Cory Benfield\n:license: Apache v2.0, see LICENSE for more details\n\"\"\"\n\nfrom .ssl import SSLAdapter\nfrom .source import SourceAddressAdapter\n\n__all__ = ['SSLAdapter', 'SourceAddressAdapter']\n"}