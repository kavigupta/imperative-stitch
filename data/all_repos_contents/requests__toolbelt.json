{"setup.py": "# -*- coding: utf-8 -*-\n\nimport os\nimport re\nimport sys\n\nfrom setuptools import setup\n\nif sys.argv[-1].lower() in (\"submit\", \"publish\"):\n    os.system(\"python setup.py bdist_wheel sdist upload\")\n    sys.exit()\n\n\ndef get_version():\n    version = ''\n    with open('requests_toolbelt/__init__.py', 'r') as fd:\n        reg = re.compile(r'__version__ = [\\'\"]([^\\'\"]*)[\\'\"]')\n        for line in fd:\n            m = reg.match(line)\n            if m:\n                version = m.group(1)\n                break\n    return version\n\n__version__ = get_version()\n\nif not __version__:\n    raise RuntimeError('Cannot find version information')\n\n\npackages = [\n    'requests_toolbelt',\n    'requests_toolbelt.adapters',\n    'requests_toolbelt.auth',\n    'requests_toolbelt.downloadutils',\n    'requests_toolbelt.multipart',\n    'requests_toolbelt.threaded',\n    'requests_toolbelt.utils',\n]\n\nsetup(\n    name=\"requests-toolbelt\",\n    version=__version__,\n    description=\"A utility belt for advanced users of python-requests\",\n    long_description=\"\\n\\n\".join([open(\"README.rst\").read(),\n                                  open(\"HISTORY.rst\").read()]),\n    long_description_content_type=\"text/x-rst\",\n    license='Apache 2.0',\n    author='Ian Cordasco, Cory Benfield',\n    author_email=\"graffatcolmingov@gmail.com\",\n    url=\"https://toolbelt.readthedocs.io/\",\n    project_urls={\n        \"Changelog\": \"https://github.com/requests/toolbelt/blob/master/HISTORY.rst\",\n        \"Source\": \"https://github.com/requests/toolbelt\",\n    },\n    packages=packages,\n    package_data={'': ['LICENSE', 'AUTHORS.rst']},\n    include_package_data=True,\n    python_requires='>=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*',\n    install_requires=['requests>=2.0.1,<3.0.0'],\n    classifiers=[\n        'Development Status :: 5 - Production/Stable',\n        'License :: OSI Approved :: Apache Software License',\n        'Intended Audience :: Developers',\n        'Programming Language :: Python',\n        'Programming Language :: Python :: 2',\n        'Programming Language :: Python :: 2.7',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.4',\n        'Programming Language :: Python :: 3.5',\n        'Programming Language :: Python :: 3.6',\n        'Programming Language :: Python :: 3.7',\n        'Programming Language :: Python :: 3.8',\n        'Programming Language :: Python :: 3.9',\n        'Programming Language :: Python :: 3.10',\n        'Programming Language :: Python :: 3.11',\n        'Programming Language :: Python :: Implementation :: CPython',\n        'Programming Language :: Python :: Implementation :: PyPy',\n    ],\n)\n", "docs/conf.py": "# -*- coding: utf-8 -*-\n#\n# requests_toolbelt documentation build configuration file, created by\n# sphinx-quickstart on Sun Jan 12 21:24:39 2014.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath('.'))\nsys.path.insert(0, os.path.abspath('..'))\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#needs_sphinx = '1.0'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\n    'sphinx.ext.autodoc',\n    'sphinx.ext.intersphinx',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = ['_templates']\n\n# The suffix of source filenames.\nsource_suffix = '.rst'\n\n# The encoding of source files.\n#source_encoding = 'utf-8-sig'\n\n# The master toctree document.\nmaster_doc = 'index'\n\n# General information about the project.\nproject = u'requests_toolbelt'\ncopyright = u'2015, Ian Cordasco, Cory Benfield'\n\n# The version info for the project you're documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nfrom requests_toolbelt import __version__ as version\n# The full version, including alpha/beta/rc tags.\nrelease = version\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#language = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n#today = ''\n# Else, today_fmt is used as the format for a strftime call.\n#today_fmt = '%B %d, %Y'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns = ['_build']\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n#default_role = None\n\n# If true, '()' will be appended to :func: etc. cross-reference text.\n#add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n#add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n#show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = 'sphinx'\n\n# A list of ignored prefixes for module index sorting.\n#modindex_common_prefix = []\n\n# If true, keep warnings as \"system message\" paragraphs in the built documents.\n#keep_warnings = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\non_rtd = os.environ.get('READTHEDOCS', None) == 'True'\nif not on_rtd:  # only import and set the theme if we're building docs locally\n    import sphinx_rtd_theme\n    html_theme = 'sphinx_rtd_theme'\n    html_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#html_theme = 'alabaster'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#html_theme_options = {}\n\n# Add any paths that contain custom themes here, relative to this directory.\n#html_theme_path = []\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# \"<project> v<release> documentation\".\n#html_title = None\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n#html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n#html_logo = None\n\n# The name of an image file (within the static path) to use as favicon of the\n# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n#html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\n#html_static_path = ['_static']\n\n# Add any extra paths that contain custom files (such as robots.txt or\n# .htaccess) here, relative to this directory. These files are copied\n# directly to the root of the documentation.\n#html_extra_path = []\n\n# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,\n# using the given strftime format.\n#html_last_updated_fmt = '%b %d, %Y'\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n#html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n#html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n#html_additional_pages = {}\n\n# If false, no module index is generated.\n#html_domain_indices = True\n\n# If false, no index is generated.\n#html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n#html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n#html_show_sourcelink = True\n\n# If true, \"Created using Sphinx\" is shown in the HTML footer. Default is True.\n#html_show_sphinx = True\n\n# If true, \"(C) Copyright ...\" is shown in the HTML footer. Default is True.\n#html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n#html_use_opensearch = ''\n\n# This is the file name suffix for HTML files (e.g. \".xhtml\").\n#html_file_suffix = None\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = 'requests_toolbelt-doc'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n# The paper size ('letterpaper' or 'a4paper').\n#'papersize': 'letterpaper',\n\n# The font size ('10pt', '11pt' or '12pt').\n#'pointsize': '10pt',\n\n# Additional stuff for the LaTeX preamble.\n#'preamble': '',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n  ('index', 'requests_toolbelt.tex', u'requests\\\\_toolbelt Documentation',\n   u'Ian Cordasco, Cory Benfield', 'manual'),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n#latex_logo = None\n\n# For \"manual\" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n#latex_use_parts = False\n\n# If true, show page references after internal links.\n#latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n#latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n#latex_appendices = []\n\n# If false, no module index is generated.\n#latex_domain_indices = True\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    ('index', 'requests_toolbelt', u'requests_toolbelt Documentation',\n     [u'Ian Cordasco, Cory Benfield'], 1)\n]\n\n# If true, show URL addresses after external links.\n#man_show_urls = False\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n  ('index', 'requests_toolbelt', u'requests_toolbelt Documentation',\n   u'Ian Cordasco, Cory Benfield', 'requests_toolbelt', 'One line description of project.',\n   'Miscellaneous'),\n]\n\n# Documents to append as an appendix to all manuals.\n#texinfo_appendices = []\n\n# If false, no module index is generated.\n#texinfo_domain_indices = True\n\n# How to display URL addresses: 'footnote', 'no', or 'inline'.\n#texinfo_show_urls = 'footnote'\n\n# If true, do not generate a @detailmenu in the \"Top\" node's menu.\n#texinfo_no_detailmenu = False\n\n\n# Example configuration for intersphinx: refer to the Python standard library.\nintersphinx_mapping = {'python': ('https://docs.python.org/', None)}\n", "tests/test_fingerprintadapter.py": "# -*- coding: utf-8 -*-\nimport requests\nimport unittest\n\nfrom requests_toolbelt.adapters.fingerprint import FingerprintAdapter\nfrom . import get_betamax\n\n\nclass TestFingerprintAdapter(unittest.TestCase):\n    HTTP2BIN_FINGERPRINT = 'abf8683eeba8521ad2e8dc48e92a1cbea3ff8608f1417948fdad75d7b50eb264'\n\n    def setUp(self):\n        self.session = requests.Session()\n        self.session.mount('https://http2bin.org', FingerprintAdapter(self.HTTP2BIN_FINGERPRINT))\n        self.recorder = get_betamax(self.session)\n\n    def test_fingerprint(self):\n        with self.recorder.use_cassette('http2bin_fingerprint'):\n            r = self.session.get('https://http2bin.org/get')\n            assert r.status_code == 200\n", "tests/test_socket_options_adapter.py": "# -*- coding: utf-8 -*-\n\"\"\"Tests for the SocketOptionsAdapter and TCPKeepAliveAdapter.\"\"\"\nimport contextlib\nimport platform\nimport socket\nimport sys\n\nimport pytest\ntry:\n    from unittest import mock\nexcept ImportError:\n    import mock\nimport requests\nfrom requests_toolbelt._compat import poolmanager\n\nfrom requests_toolbelt.adapters import socket_options\n\n\n@contextlib.contextmanager\ndef remove_keepidle():\n    \"\"\"A context manager to remove TCP_KEEPIDLE from socket.\"\"\"\n    TCP_KEEPIDLE = getattr(socket, 'TCP_KEEPIDLE', None)\n    if TCP_KEEPIDLE is not None:\n        del socket.TCP_KEEPIDLE\n\n    yield\n\n    if TCP_KEEPIDLE is not None:\n        socket.TCP_KEEPIDLE = TCP_KEEPIDLE\n\n\n@contextlib.contextmanager\ndef set_keepidle(value):\n    \"\"\"A context manager to set TCP_KEEPALIVE on socket always.\"\"\"\n    TCP_KEEPIDLE = getattr(socket, 'TCP_KEEPIDLE', None)\n    socket.TCP_KEEPIDLE = value\n\n    yield\n\n    if TCP_KEEPIDLE is not None:\n        socket.TCP_KEEPIDLE = TCP_KEEPIDLE\n    else:\n        del socket.TCP_KEEPIDLE\n\n\n@mock.patch.object(requests, '__build__', 0x020500)\n@mock.patch.object(poolmanager, 'PoolManager')\ndef test_options_passing_on_newer_requests(PoolManager):\n    \"\"\"Show that options are passed for a new enough version of requests.\"\"\"\n    fake_opts = [('test', 'options', 'fake')]\n    adapter = socket_options.SocketOptionsAdapter(\n        socket_options=fake_opts,\n        pool_connections=10,\n        pool_maxsize=5,\n        pool_block=True,\n    )\n    PoolManager.assert_called_once_with(\n        num_pools=10, maxsize=5, block=True,\n        socket_options=fake_opts\n    )\n    assert adapter.socket_options == fake_opts\n\n\n@mock.patch.object(requests, '__build__', 0x020300)\n@mock.patch.object(poolmanager, 'PoolManager')\ndef test_options_not_passed_on_older_requests(PoolManager):\n    \"\"\"Show that options are not passed for older versions of requests.\"\"\"\n    fake_opts = [('test', 'options', 'fake')]\n    socket_options.SocketOptionsAdapter(\n        socket_options=fake_opts,\n        pool_connections=10,\n        pool_maxsize=5,\n        pool_block=True,\n    )\n    assert PoolManager.called is False\n\n\n@pytest.mark.xfail(sys.version_info.major == 2 and platform.system() == \"Windows\",\n                   reason=\"Windows does not have TCP_KEEPINTVL in Python 2\")\n@mock.patch.object(requests, '__build__', 0x020500)\n@mock.patch.object(poolmanager, 'PoolManager')\ndef test_keep_alive_on_newer_requests_no_idle(PoolManager):\n    \"\"\"Show that options are generated correctly from kwargs.\"\"\"\n    socket_opts = [\n        (socket.IPPROTO_TCP, socket.TCP_NODELAY, 1),\n        (socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1),\n        (socket.IPPROTO_TCP, socket.TCP_KEEPINTVL, 10),\n        (socket.IPPROTO_TCP, socket.TCP_KEEPCNT, 10),\n    ]\n    with remove_keepidle():\n        adapter = socket_options.TCPKeepAliveAdapter(\n            idle=30, interval=10, count=10,\n            pool_connections=10,\n            pool_maxsize=5,\n            pool_block=True,\n        )\n    PoolManager.assert_called_once_with(\n        num_pools=10, maxsize=5, block=True,\n        socket_options=socket_opts\n    )\n    assert adapter.socket_options == socket_opts\n\n\n@pytest.mark.xfail(sys.version_info.major == 2 and platform.system() == \"Windows\",\n                   reason=\"Windows does not have TCP_KEEPINTVL in Python 2\")\n@mock.patch.object(requests, '__build__', 0x020500)\n@mock.patch.object(poolmanager, 'PoolManager')\ndef test_keep_alive_on_newer_requests_with_idle(PoolManager):\n    \"\"\"Show that options are generated correctly from kwargs with KEEPIDLE.\"\"\"\n    with set_keepidle(3000):\n        socket_opts = [\n            (socket.IPPROTO_TCP, socket.TCP_NODELAY, 1),\n            (socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1),\n            (socket.IPPROTO_TCP, socket.TCP_KEEPINTVL, 10),\n            (socket.IPPROTO_TCP, socket.TCP_KEEPCNT, 10),\n            (socket.IPPROTO_TCP, socket.TCP_KEEPIDLE, 30),\n        ]\n        adapter = socket_options.TCPKeepAliveAdapter(\n            idle=30, interval=10, count=10,\n            pool_connections=10,\n            pool_maxsize=5,\n            pool_block=True,\n        )\n\n    PoolManager.assert_called_once_with(\n        num_pools=10, maxsize=5, block=True,\n        socket_options=socket_opts\n    )\n    assert adapter.socket_options == socket_opts\n", "tests/test_source_adapter.py": "# -*- coding: utf-8 -*-\nfrom requests.adapters import DEFAULT_POOLSIZE, DEFAULT_POOLBLOCK\ntry:\n    from unittest.mock import patch\nexcept ImportError:\n    from mock import patch\nfrom requests_toolbelt.adapters.source import SourceAddressAdapter\n\nimport pytest\n\n\n@patch('requests_toolbelt.adapters.source.poolmanager')\ndef test_source_address_adapter_string(poolmanager):\n    SourceAddressAdapter('10.10.10.10')\n\n    poolmanager.PoolManager.assert_called_once_with(\n        num_pools=DEFAULT_POOLSIZE,\n        maxsize=DEFAULT_POOLSIZE,\n        block=DEFAULT_POOLBLOCK,\n        source_address=('10.10.10.10', 0)\n    )\n\n\n@patch('requests_toolbelt.adapters.source.poolmanager')\ndef test_source_address_adapter_tuple(poolmanager):\n    SourceAddressAdapter(('10.10.10.10', 80))\n\n    poolmanager.PoolManager.assert_called_once_with(\n        num_pools=DEFAULT_POOLSIZE,\n        maxsize=DEFAULT_POOLSIZE,\n        block=DEFAULT_POOLBLOCK,\n        source_address=('10.10.10.10', 80)\n    )\n\n\n@patch('requests_toolbelt.adapters.source.poolmanager')\ndef test_source_address_adapter_type_error(poolmanager):\n    with pytest.raises(TypeError):\n        SourceAddressAdapter({'10.10.10.10': 80})\n\n    assert not poolmanager.PoolManager.called\n", "tests/test_streaming_iterator.py": "import io\n\nfrom requests_toolbelt.streaming_iterator import StreamingIterator\n\nimport pytest\n\n@pytest.fixture(params=[True, False])\ndef get_iterable(request):\n    '''\n    When this fixture is used, the test is run twice -- once with the iterable\n    being a file-like object, once being an iterator.\n    '''\n    is_file = request.param\n    def inner(chunks):\n        if is_file:\n            return io.BytesIO(b''.join(chunks))\n        return iter(chunks)\n    return inner\n\n\nclass TestStreamingIterator(object):\n    @pytest.fixture(autouse=True)\n    def setup(self, get_iterable):\n        self.chunks = [b'here', b'are', b'some', b'chunks']\n        self.size = 17\n        self.uploader = StreamingIterator(self.size, get_iterable(self.chunks))\n\n    def test_read_returns_all_chunks_in_one(self):\n        assert self.uploader.read() == b''.join(self.chunks)\n\n    def test_read_returns_empty_string_after_exhausting_the_iterator(self):\n        for i in range(0, 4):\n            self.uploader.read(8192)\n\n        assert self.uploader.read() == b''\n        assert self.uploader.read(8192) == b''\n\n\nclass TestStreamingIteratorWithLargeChunks(object):\n    @pytest.fixture(autouse=True)\n    def setup(self, get_iterable):\n        self.letters = [b'a', b'b', b'c', b'd', b'e']\n        self.chunks = (letter * 2000 for letter in self.letters)\n        self.size = 5 * 2000\n        self.uploader = StreamingIterator(self.size, get_iterable(self.chunks))\n\n    def test_returns_the_amount_requested(self):\n        chunk_size = 1000\n        bytes_read = 0\n        while True:\n            b = self.uploader.read(chunk_size)\n            if not b:\n                break\n            assert len(b) == chunk_size\n            bytes_read += len(b)\n\n        assert bytes_read == self.size\n\n    def test_returns_all_of_the_bytes(self):\n        chunk_size = 8192\n        bytes_read = 0\n        while True:\n            b = self.uploader.read(chunk_size)\n            if not b:\n                break\n            bytes_read += len(b)\n\n        assert bytes_read == self.size\n", "tests/test_auth.py": "# -*- coding: utf-8 -*-\nimport requests\nimport unittest\ntry:\n    from unittest import mock\nexcept ImportError:\n    import mock\n\nfrom requests_toolbelt.auth.guess import GuessAuth, GuessProxyAuth\nfrom . import get_betamax\n\n\nclass TestGuessAuth(unittest.TestCase):\n    def setUp(self):\n        self.session = requests.Session()\n        self.recorder = get_betamax(self.session)\n\n    def cassette(self, name):\n        return self.recorder.use_cassette(\n            'httpbin_guess_auth_' + name,\n            match_requests_on=['method', 'uri', 'digest-auth']\n        )\n\n    def test_basic(self):\n        with self.cassette('basic'):\n            r = self.session.request(\n                'GET', 'http://httpbin.org/basic-auth/user/passwd',\n                auth=GuessAuth('user', 'passwd'))\n\n        assert r.json() == {'authenticated': True, 'user': 'user'}\n\n    def test_digest(self):\n        with self.cassette('digest'):\n            r = self.session.request(\n                'GET', 'http://httpbin.org/digest-auth/auth/user/passwd',\n                auth=GuessAuth('user', 'passwd'))\n\n        assert r.json() == {'authenticated': True, 'user': 'user'}\n\n    def test_no_auth(self):\n        with self.cassette('none'):\n            url = 'http://httpbin.org/get?a=1'\n            r = self.session.request('GET', url,\n                                     auth=GuessAuth('user', 'passwd'))\n\n            j = r.json()\n            assert j['args'] == {'a': '1'}\n            assert j['url'] == url\n            assert 'user' not in r.text\n            assert 'passwd' not in r.text\n\n\nclass TestGuessProxyAuth(unittest.TestCase):\n\n    @mock.patch('requests_toolbelt.auth.http_proxy_digest.HTTPProxyDigestAuth.handle_407')\n    def test_handle_407_header_digest(self, mock_handle_407):\n        r = requests.Response()\n        r.headers['Proxy-Authenticate'] = 'Digest nonce=\"d2b19757d3d656a283c99762cbd1097b\", opaque=\"1c311ad1cc6e6183b83bc75f95a57893\", realm=\"me@kennethreitz.com\", qop=auth'\n\n        guess_auth = GuessProxyAuth(None, None, \"user\", \"passwd\")\n        guess_auth.handle_407(r)\n\n        mock_handle_407.assert_called_with(r)\n\n    @mock.patch('requests.auth.HTTPProxyAuth.__call__')\n    @mock.patch('requests.cookies.extract_cookies_to_jar')\n    def test_handle_407_header_basic(self, extract_cookies_to_jar, proxy_auth_call):\n        req = mock.Mock()\n        r = mock.Mock()\n        r.headers = dict()\n        r.request.copy.return_value = req\n\n        proxy_auth_call.return_value = requests.Response()\n\n        kwargs = {}\n        r.headers['Proxy-Authenticate'] = 'Basic realm=\"Fake Realm\"'\n        guess_auth = GuessProxyAuth(None, None, \"user\", \"passwd\")\n        guess_auth.handle_407(r, *kwargs)\n\n        proxy_auth_call.assert_called_with(req)\n", "tests/test_auth_handler.py": "import requests\nfrom requests.auth import HTTPBasicAuth\nfrom requests_toolbelt.auth.handler import AuthHandler\nfrom requests_toolbelt.auth.handler import NullAuthStrategy\n\n\ndef test_turns_tuples_into_basic_auth():\n    a = AuthHandler({'http://example.com': ('foo', 'bar')})\n    strategy = a.get_strategy_for('http://example.com')\n    assert not isinstance(strategy, NullAuthStrategy)\n    assert isinstance(strategy, HTTPBasicAuth)\n\n\ndef test_uses_null_strategy_for_non_matching_domains():\n    a = AuthHandler({'http://api.example.com': ('foo', 'bar')})\n    strategy = a.get_strategy_for('http://example.com')\n    assert isinstance(strategy, NullAuthStrategy)\n\n\ndef test_normalizes_domain_keys():\n    a = AuthHandler({'https://API.github.COM': ('foo', 'bar')})\n    assert 'https://api.github.com' in a.strategies\n    assert 'https://API.github.COM' not in a.strategies\n\n\ndef test_can_add_new_strategies():\n    a = AuthHandler({'https://example.com': ('foo', 'bar')})\n    a.add_strategy('https://api.github.com', ('fiz', 'baz'))\n    assert isinstance(\n        a.get_strategy_for('https://api.github.com'),\n        HTTPBasicAuth\n        )\n\n\ndef test_prepares_auth_correctly():\n    # Set up our Session and AuthHandler\n    auth = AuthHandler({\n        'https://api.example.com': ('bar', 'baz'),\n        'https://httpbin.org': ('biz', 'fiz'),\n    })\n    s = requests.Session()\n    s.auth = auth\n    # Set up a valid GET request to https://api.example.com/users\n    r1 = requests.Request('GET', 'https://api.example.com/users')\n    p1 = s.prepare_request(r1)\n    assert p1.headers['Authorization'] == 'Basic YmFyOmJheg=='\n\n    # Set up a valid POST request to https://httpbin.org/post\n    r2 = requests.Request('POST', 'https://httpbin.org/post', data='foo')\n    p2 = s.prepare_request(r2)\n    assert p2.headers['Authorization'] == 'Basic Yml6OmZpeg=='\n\n    # Set up an *invalid* OPTIONS request to http://api.example.com\n    # NOTE(sigmavirus24): This is not because of the verb but instead because\n    # it is the wrong URI scheme.\n    r3 = requests.Request('OPTIONS', 'http://api.example.com/projects')\n    p3 = s.prepare_request(r3)\n    assert p3.headers.get('Authorization') is None\n", "tests/test_proxy_digest_auth.py": "# -*- coding: utf-8 -*-\n\"\"\"Test proxy digest authentication.\"\"\"\n\nimport unittest\ntry:\n    from unittest import mock\nexcept ImportError:\n    import mock\n\nimport requests\nfrom requests_toolbelt.auth import http_proxy_digest\n\n\nclass TestProxyDigestAuth(unittest.TestCase):\n    \"\"\"Tests for the ProxyDigestAuth class.\"\"\"\n\n    def setUp(self):\n        \"\"\"Set up variables for each test.\"\"\"\n        self.username = \"username\"\n        self.password = \"password\"\n        self.auth = http_proxy_digest.HTTPProxyDigestAuth(\n            self.username, self.password\n        )\n        self.prepared_request = requests.Request(\n            'GET',\n            'http://host.org/index.html'\n        ).prepare()\n\n    def test_with_existing_nonce(self):\n        \"\"\"Test if it will generate Proxy-Auth header when nonce present.\n\n        Digest authentication's correctness will not be tested here.\n        \"\"\"\n        self.auth.last_nonce = \"bH3FVAAAAAAg74rL3X8AAI3CyBAAAAAA\"\n        self.auth.chal = {\n            'nonce': self.auth.last_nonce,\n            'realm': 'testrealm@host.org',\n            'qop': 'auth'\n        }\n\n        # prepared_request headers should be clear before calling auth\n        assert self.prepared_request.headers.get('Proxy-Authorization') is None\n        self.auth(self.prepared_request)\n        assert self.prepared_request.headers['Proxy-Authorization'] is not None\n\n    def test_no_challenge(self):\n        \"\"\"Test that a response containing no auth challenge is left alone.\"\"\"\n        connection = MockConnection()\n        first_response = connection.make_response(self.prepared_request)\n        first_response.status_code = 404\n\n        assert self.auth.last_nonce == ''\n        final_response = self.auth.handle_407(first_response)\n        headers = final_response.request.headers\n        assert self.auth.last_nonce == ''\n        assert first_response is final_response\n        assert headers.get('Proxy-Authorization') is None\n\n    def test_digest_challenge(self):\n        \"\"\"Test a response with a digest auth challenge causes a new request.\n\n        This ensures that the auth class generates a new request with a\n        Proxy-Authorization header.\n\n        Digest authentication's correctness will not be tested here.\n        \"\"\"\n        connection = MockConnection()\n        first_response = connection.make_response(self.prepared_request)\n        first_response.status_code = 407\n        first_response.headers['Proxy-Authenticate'] = (\n            'Digest'\n            ' realm=\"Fake Realm\", nonce=\"oS6WVgAAAABw698CAAAAAHAk/HUAAAAA\",'\n            ' qop=\"auth\", stale=false'\n        )\n\n        assert self.auth.last_nonce == ''\n        final_response = self.auth.handle_407(first_response)\n        headers = final_response.request.headers\n        assert self.auth.last_nonce != ''\n        assert first_response is not final_response\n        assert headers.get('Proxy-Authorization') is not None\n\n    def test_ntlm_challenge(self):\n        \"\"\"Test a response without a Digest auth challenge is left alone.\"\"\"\n        connection = MockConnection()\n        first_response = connection.make_response(self.prepared_request)\n        first_response.status_code = 407\n        first_response.headers['Proxy-Authenticate'] = 'NTLM'\n\n        assert self.auth.last_nonce == ''\n        final_response = self.auth.handle_407(first_response)\n        headers = final_response.request.headers\n        assert self.auth.last_nonce == ''\n        assert first_response is final_response\n        assert headers.get('Proxy-Authorization') is None\n\n\nclass MockConnection(object):\n    \"\"\"Fake connection object.\"\"\"\n\n    def send(self, request, **kwargs):\n        \"\"\"Mock out the send method.\"\"\"\n        return self.make_response(request)\n\n    def make_response(self, request):\n        \"\"\"Make a response for us based on the request.\"\"\"\n        response = requests.Response()\n        response.status_code = 200\n        response.request = request\n        response.raw = mock.MagicMock()\n        response.connection = self\n        return response\n\nif __name__ == '__main__':\n    unittest.main()\n", "tests/test_forgetfulcookiejar.py": "# -*- coding: utf-8 -*-\nimport requests\nimport unittest\n\nfrom requests_toolbelt.cookies.forgetful import ForgetfulCookieJar\nfrom . import get_betamax\n\n\nclass TestForgetfulCookieJar(unittest.TestCase):\n\n    def setUp(self):\n        self.session = requests.Session()\n        self.session.cookies = ForgetfulCookieJar()\n        self.recorder = get_betamax(self.session)\n\n    def test_cookies_are_ignored(self):\n        with self.recorder.use_cassette('http2bin_cookies'):\n            url = 'https://httpbin.org/cookies/set'\n            cookies = {\n                'cookie0': 'value0',\n            }\n            r = self.session.request(\n                'GET', url,\n                 params=cookies\n                )\n            assert 'cookie0' not in self.session.cookies\n", "tests/test_dump.py": "\"\"\"Collection of tests for utils.dump.\n\nThe dump utility module only has two public attributes:\n\n- dump_response\n- dump_all\n\nThis module, however, tests many of the private implementation details since\nthose public functions just wrap them and testing the public functions will be\nvery complex and high-level.\n\"\"\"\nfrom requests_toolbelt._compat import HTTPHeaderDict\nfrom requests_toolbelt.utils import dump\n\ntry:\n    from unittest import mock\nexcept ImportError:\n    import mock\nimport pytest\nimport requests\n\nfrom . import get_betamax\n\nHTTP_1_1 = 11\nHTTP_1_0 = 10\nHTTP_0_9 = 9\nHTTP_UNKNOWN = 5000\n\n\nclass TestSimplePrivateFunctions(object):\n\n    \"\"\"Excercise simple private functions in one logical place.\"\"\"\n\n    def test_coerce_to_bytes_skips_byte_strings(self):\n        \"\"\"Show that _coerce_to_bytes skips bytes input.\"\"\"\n        bytestr = b'some bytes'\n        assert dump._coerce_to_bytes(bytestr) is bytestr\n\n    def test_coerce_to_bytes_converts_text(self):\n        \"\"\"Show that _coerce_to_bytes handles text input.\"\"\"\n        bytestr = b'some bytes'\n        text = bytestr.decode('utf-8')\n        assert dump._coerce_to_bytes(text) == bytestr\n\n    def test_format_header(self):\n        \"\"\"Prove that _format_header correctly formats bytes input.\"\"\"\n        header = b'Connection'\n        value = b'close'\n        expected = b'Connection: close\\r\\n'\n        assert dump._format_header(header, value) == expected\n\n    def test_format_header_handles_unicode(self):\n        \"\"\"Prove that _format_header correctly formats text input.\"\"\"\n        header = b'Connection'.decode('utf-8')\n        value = b'close'.decode('utf-8')\n        expected = b'Connection: close\\r\\n'\n        assert dump._format_header(header, value) == expected\n\n    def test_build_request_path(self):\n        \"\"\"Show we get the right request path for a normal request.\"\"\"\n        path, _ = dump._build_request_path(\n            'https://example.com/foo/bar', {}\n        )\n        assert path == b'/foo/bar'\n\n    def test_build_request_path_with_query_string(self):\n        \"\"\"Show we include query strings appropriately.\"\"\"\n        path, _ = dump._build_request_path(\n            'https://example.com/foo/bar?query=data', {}\n        )\n        assert path == b'/foo/bar?query=data'\n\n    def test_build_request_path_with_proxy_info(self):\n        \"\"\"Show that we defer to the proxy request_path info.\"\"\"\n        path, _ = dump._build_request_path(\n            'https://example.com/', {\n                'request_path': b'https://example.com/test'\n            }\n        )\n        assert path == b'https://example.com/test'\n\n\nclass RequestResponseMixin(object):\n\n    \"\"\"Mix-in for test classes needing mocked requests and responses.\"\"\"\n\n    response_spec = [\n        'connection',\n        'content',\n        'raw',\n        'reason',\n        'request',\n        'url',\n    ]\n\n    request_spec = [\n        'body',\n        'headers',\n        'method',\n        'url',\n    ]\n\n    httpresponse_spec = [\n        'headers',\n        'reason',\n        'status',\n        'version',\n    ]\n\n    adapter_spec = [\n        'proxy_manager',\n    ]\n\n    @pytest.fixture(autouse=True)\n    def set_up(self):\n        \"\"\"xUnit style autoused fixture creating mocks.\"\"\"\n        self.response = mock.Mock(spec=self.response_spec)\n        self.request = mock.Mock(spec=self.request_spec)\n        self.httpresponse = mock.Mock(spec=self.httpresponse_spec)\n        self.adapter = mock.Mock(spec=self.adapter_spec)\n\n        self.response.connection = self.adapter\n        self.response.request = self.request\n        self.response.raw = self.httpresponse\n\n    def configure_response(self, content=b'', proxy_manager=None, url=None,\n                           reason=b''):\n        \"\"\"Helper function to configure a mocked response.\"\"\"\n        self.adapter.proxy_manager = proxy_manager or {}\n        self.response.content = content\n        self.response.url = url\n        self.response.reason = reason\n\n    def configure_request(self, body=b'', headers=None, method=None,\n                          url=None):\n        \"\"\"Helper function to configure a mocked request.\"\"\"\n        self.request.body = body\n        self.request.headers = headers or {}\n        self.request.method = method\n        self.request.url = url\n\n    def configure_httpresponse(self, headers=None, reason=b'', status=200,\n                               version=HTTP_1_1):\n        \"\"\"Helper function to configure a mocked urllib3 response.\"\"\"\n        self.httpresponse.headers = HTTPHeaderDict(headers or {})\n        self.httpresponse.reason = reason\n        self.httpresponse.status = status\n        self.httpresponse.version = version\n\n\nclass TestResponsePrivateFunctions(RequestResponseMixin):\n\n    \"\"\"Excercise private functions using responses.\"\"\"\n\n    def test_get_proxy_information_sans_proxy(self):\n        \"\"\"Show no information is returned when not using a proxy.\"\"\"\n        self.configure_response()\n\n        assert dump._get_proxy_information(self.response) is None\n\n    def test_get_proxy_information_with_proxy_over_http(self):\n        \"\"\"Show only the request path is returned for HTTP requests.\n\n        Using HTTP over a proxy doesn't alter anything except the request path\n        of the request. The method doesn't change a dictionary with the\n        request_path is the only thing that should be returned.\n        \"\"\"\n        self.configure_response(\n            proxy_manager={'http://': 'http://local.proxy:3939'},\n        )\n        self.configure_request(\n            url='http://example.com',\n            method='GET',\n        )\n\n        assert dump._get_proxy_information(self.response) == {\n            'request_path': 'http://example.com'\n        }\n\n    def test_get_proxy_information_with_proxy_over_https(self):\n        \"\"\"Show that the request path and method are returned for HTTPS reqs.\n\n        Using HTTPS over a proxy changes the method used and the request path.\n        \"\"\"\n        self.configure_response(\n            proxy_manager={'http://': 'http://local.proxy:3939'},\n        )\n        self.configure_request(\n            url='https://example.com',\n            method='GET',\n        )\n\n        assert dump._get_proxy_information(self.response) == {\n            'method': 'CONNECT',\n            'request_path': 'https://example.com'\n        }\n\n    def test_dump_request_data(self):\n        \"\"\"Build up the request data into a bytearray.\"\"\"\n        self.configure_request(\n            url='http://example.com/',\n            method='GET',\n        )\n\n        array = bytearray()\n        prefixes = dump.PrefixSettings('request:', 'response:')\n        dump._dump_request_data(\n            request=self.request,\n            prefixes=prefixes,\n            bytearr=array,\n            proxy_info={},\n        )\n\n        assert b'request:GET / HTTP/1.1\\r\\n' in array\n        assert b'request:Host: example.com\\r\\n' in array\n\n    def test_dump_non_string_request_data(self):\n        \"\"\"Build up the request data into a bytearray.\"\"\"\n        self.configure_request(\n            url='http://example.com/',\n            method='POST',\n            body=1\n        )\n\n        array = bytearray()\n        prefixes = dump.PrefixSettings('request:', 'response:')\n        dump._dump_request_data(\n            request=self.request,\n            prefixes=prefixes,\n            bytearr=array,\n            proxy_info={},\n        )\n        assert b'request:POST / HTTP/1.1\\r\\n' in array\n        assert b'request:Host: example.com\\r\\n' in array\n        assert b'<< Request body is not a string-like type >>\\r\\n' in array\n\n    def test_dump_request_data_with_proxy_info(self):\n        \"\"\"Build up the request data into a bytearray.\"\"\"\n        self.configure_request(\n            url='http://example.com/',\n            method='GET',\n        )\n\n        array = bytearray()\n        prefixes = dump.PrefixSettings('request:', 'response:')\n        dump._dump_request_data(\n            request=self.request,\n            prefixes=prefixes,\n            bytearr=array,\n            proxy_info={\n                'request_path': b'fake-request-path',\n                'method': b'CONNECT',\n            },\n        )\n\n        assert b'request:CONNECT fake-request-path HTTP/1.1\\r\\n' in array\n        assert b'request:Host: example.com\\r\\n' in array\n\n    def test_dump_response_data(self):\n        \"\"\"Build up the response data into a bytearray.\"\"\"\n        self.configure_response(\n            url='https://example.com/redirected',\n            content=b'foobarbogus',\n            reason=b'OK',\n        )\n        self.configure_httpresponse(\n            headers={'Content-Type': 'application/json'},\n            reason=b'OK',\n            status=201,\n        )\n\n        array = bytearray()\n        prefixes = dump.PrefixSettings('request:', 'response:')\n        dump._dump_response_data(\n            response=self.response,\n            prefixes=prefixes,\n            bytearr=array,\n        )\n\n        assert b'response:HTTP/1.1 201 OK\\r\\n' in array\n        assert b'response:Content-Type: application/json\\r\\n' in array\n\n    def test_dump_response_data_with_older_http_version(self):\n        \"\"\"Build up the response data into a bytearray.\"\"\"\n        self.configure_response(\n            url='https://example.com/redirected',\n            content=b'foobarbogus',\n            reason=b'OK',\n        )\n        self.configure_httpresponse(\n            headers={'Content-Type': 'application/json'},\n            reason=b'OK',\n            status=201,\n            version=HTTP_0_9,\n        )\n\n        array = bytearray()\n        prefixes = dump.PrefixSettings('request:', 'response:')\n        dump._dump_response_data(\n            response=self.response,\n            prefixes=prefixes,\n            bytearr=array,\n        )\n\n        assert b'response:HTTP/0.9 201 OK\\r\\n' in array\n        assert b'response:Content-Type: application/json\\r\\n' in array\n\n    def test_dump_response_data_with_unknown_http_version(self):\n        \"\"\"Build up the response data into a bytearray.\"\"\"\n        self.configure_response(\n            url='https://example.com/redirected',\n            content=b'foobarbogus',\n            reason=b'OK',\n        )\n        self.configure_httpresponse(\n            headers={'Content-Type': 'application/json'},\n            reason=b'OK',\n            status=201,\n            version=HTTP_UNKNOWN,\n        )\n\n        array = bytearray()\n        prefixes = dump.PrefixSettings('request:', 'response:')\n        dump._dump_response_data(\n            response=self.response,\n            prefixes=prefixes,\n            bytearr=array,\n        )\n\n        assert b'response:HTTP/? 201 OK\\r\\n' in array\n        assert b'response:Content-Type: application/json\\r\\n' in array\n\n\nclass TestResponsePublicFunctions(RequestResponseMixin):\n\n    \"\"\"Excercise public functions using responses.\"\"\"\n\n    def test_dump_response_fails_without_request(self):\n        \"\"\"Show that a response without a request raises a ValueError.\"\"\"\n        del self.response.request\n        assert hasattr(self.response, 'request') is False\n\n        with pytest.raises(ValueError):\n            dump.dump_response(self.response)\n\n    def test_dump_response_uses_provided_bytearray(self):\n        \"\"\"Show that users providing bytearrays receive those back.\"\"\"\n        self.configure_request(\n            url='http://example.com/',\n            method='GET',\n        )\n        self.configure_response(\n            url='https://example.com/redirected',\n            content=b'foobarbogus',\n            reason=b'OK',\n        )\n        self.configure_httpresponse(\n            headers={'Content-Type': 'application/json'},\n            reason=b'OK',\n            status=201,\n        )\n        arr = bytearray()\n\n        retarr = dump.dump_response(self.response, data_array=arr)\n        assert retarr is arr\n\n\nclass TestDumpRealResponses(object):\n\n    \"\"\"Exercise dump utilities against real data.\"\"\"\n\n    def test_dump_response(self):\n        session = requests.Session()\n        recorder = get_betamax(session)\n        with recorder.use_cassette('simple_get_request'):\n            response = session.get('https://httpbin.org/get')\n\n        arr = dump.dump_response(response)\n        assert b'< GET /get HTTP/1.1\\r\\n' in arr\n        assert b'< Host: httpbin.org\\r\\n' in arr\n        # NOTE(sigmavirus24): The ? below is only because Betamax doesn't\n        # preserve which HTTP version the server reports as supporting.\n        # When not using Betamax, there should be a different version\n        # reported.\n        assert b'> HTTP/? 200 OK\\r\\n' in arr\n        assert b'> Content-Type: application/json\\r\\n' in arr\n\n    def test_dump_all(self):\n        session = requests.Session()\n        recorder = get_betamax(session)\n        with recorder.use_cassette('redirect_request_for_dump_all'):\n            response = session.get('https://httpbin.org/redirect/5')\n\n        arr = dump.dump_all(response)\n        assert b'< GET /redirect/5 HTTP/1.1\\r\\n' in arr\n        assert b'> Location: /relative-redirect/4\\r\\n' in arr\n        assert b'< GET /relative-redirect/4 HTTP/1.1\\r\\n' in arr\n        assert b'> Location: /relative-redirect/3\\r\\n' in arr\n        assert b'< GET /relative-redirect/3 HTTP/1.1\\r\\n' in arr\n        assert b'> Location: /relative-redirect/2\\r\\n' in arr\n        assert b'< GET /relative-redirect/2 HTTP/1.1\\r\\n' in arr\n        assert b'> Location: /relative-redirect/1\\r\\n' in arr\n        assert b'< GET /relative-redirect/1 HTTP/1.1\\r\\n' in arr\n        assert b'> Location: /get\\r\\n' in arr\n        assert b'< GET /get HTTP/1.1\\r\\n' in arr\n", "tests/test_formdata.py": "\"\"\"Test module for requests_toolbelt.utils.formdata.\"\"\"\ntry:\n    from urllib.parse import parse_qs\nexcept ImportError:\n    from urlparse import parse_qs\n\nfrom requests_toolbelt.utils.formdata import urlencode\n\nimport pytest\n\ndict_query = {\n    'first_nested': {\n        'second_nested': {\n            'third_nested': {\n                'fourth0': 'fourth_value0',\n                'fourth1': 'fourth_value1',\n            },\n            'third0': 'third_value0',\n        },\n        'second0': 'second_value0',\n    },\n    'outter': 'outter_value',\n}\n\nlist_query = [\n    ('first_nested', [\n        ('second_nested', [\n            ('third_nested', [\n                ('fourth0', 'fourth_value0'),\n                ('fourth1', 'fourth_value1'),\n            ]),\n            ('third0', 'third_value0'),\n        ]),\n        ('second0', 'second_value0'),\n    ]),\n    ('outter', 'outter_value'),\n]\n\nmixed_dict_query = {\n    'first_nested': {\n        'second_nested': [\n            ('third_nested', {\n                'fourth0': 'fourth_value0',\n                'fourth1': 'fourth_value1',\n            }),\n            ('third0', 'third_value0'),\n        ],\n        'second0': 'second_value0',\n    },\n    'outter': 'outter_value',\n}\n\nexpected_parsed_query = {\n    'first_nested[second0]': ['second_value0'],\n    'first_nested[second_nested][third0]': ['third_value0'],\n    'first_nested[second_nested][third_nested][fourth0]': ['fourth_value0'],\n    'first_nested[second_nested][third_nested][fourth1]': ['fourth_value1'],\n    'outter': ['outter_value'],\n}\n\n\n@pytest.mark.parametrize(\"query\", [dict_query, list_query, mixed_dict_query])\ndef test_urlencode_flattens_nested_structures(query):\n    \"\"\"Show that when parsed, the structure is conveniently flat.\"\"\"\n    parsed = parse_qs(urlencode(query))\n\n    assert parsed == expected_parsed_query\n\n\ndef test_urlencode_catches_invalid_input():\n    \"\"\"Show that queries are loosely validated.\"\"\"\n    with pytest.raises(ValueError):\n        urlencode(['fo'])\n\n    with pytest.raises(ValueError):\n        urlencode([('foo', 'bar', 'bogus')])\n", "tests/test_host_header_ssl_adapter.py": "import pytest\nimport requests\n\nfrom requests_toolbelt.adapters import host_header_ssl as hhssl\n\n\n@pytest.fixture\ndef session():\n    \"\"\"Create a session with our adapter mounted.\"\"\"\n    session = requests.Session()\n    session.mount('https://', hhssl.HostHeaderSSLAdapter())\n\n\n@pytest.mark.skip\nclass TestHostHeaderSSLAdapter(object):\n    \"\"\"Tests for our HostHeaderSNIAdapter.\"\"\"\n\n    def test_ssladapter(self, session):\n        # normal mode\n        r = session.get('https://example.org')\n        assert r.status_code == 200\n\n        # accessing IP address directly\n        r = session.get('https://93.184.216.34',\n                        headers={\"Host\": \"example.org\"})\n        assert r.status_code == 200\n\n        # vHost\n        r = session.get('https://93.184.216.34',\n                        headers={'Host': 'example.com'})\n        assert r.status_code == 200\n\n    def test_stream(self):\n        self.session.get('https://54.175.219.8/stream/20',\n                         headers={'Host': 'httpbin.org'},\n                         stream=True)\n\n    def test_case_insensitive_header(self):\n        r = self.session.get('https://93.184.216.34',\n                             headers={'hOSt': 'example.org'})\n        assert r.status_code == 200\n\n    def test_plain_requests(self):\n        # test whether the reason for this adapter remains\n        # (may be implemented into requests in the future)\n        with pytest.raises(requests.exceptions.SSLError):\n            requests.get(url='https://93.184.216.34',\n                         headers={'Host': 'example.org'})\n", "tests/test_downloadutils.py": "\"\"\"Tests for the utils module.\"\"\"\nimport io\nimport os\nimport os.path\nimport shutil\nimport tempfile\n\nimport requests\nfrom requests_toolbelt.downloadutils import stream\nfrom requests_toolbelt.downloadutils import tee\ntry:\n    from unittest import mock\nexcept ImportError:\n    import mock\nimport pytest\n\nfrom . import get_betamax\n\n\npreserve_bytes = {'preserve_exact_body_bytes': True}\n\n\ndef test_get_download_file_path_uses_content_disposition():\n    s = requests.Session()\n    recorder = get_betamax(s)\n    url = ('https://api.github.com/repos/sigmavirus24/github3.py/releases/'\n           'assets/37944')\n    filename = 'github3.py-0.7.1-py2.py3-none-any.whl'\n    with recorder.use_cassette('stream_response_to_file', **preserve_bytes):\n        r = s.get(url, headers={'Accept': 'application/octet-stream'})\n        path = stream.get_download_file_path(r, None)\n        r.close()\n        assert path == filename\n\ndef test_get_download_file_path_directory():\n    s = requests.Session()\n    recorder = get_betamax(s)\n    url = ('https://api.github.com/repos/sigmavirus24/github3.py/releases/'\n           'assets/37944')\n    filename = 'github3.py-0.7.1-py2.py3-none-any.whl'\n    with recorder.use_cassette('stream_response_to_file', **preserve_bytes):\n        r = s.get(url, headers={'Accept': 'application/octet-stream'})\n        path = stream.get_download_file_path(r, tempfile.tempdir)\n        r.close()\n        assert path == os.path.join(tempfile.tempdir, filename)\n\n\ndef test_get_download_file_path_specific_file():\n    s = requests.Session()\n    recorder = get_betamax(s)\n    url = ('https://api.github.com/repos/sigmavirus24/github3.py/releases/'\n           'assets/37944')\n    with recorder.use_cassette('stream_response_to_file', **preserve_bytes):\n        r = s.get(url, headers={'Accept': 'application/octet-stream'})\n        path = stream.get_download_file_path(r, '/arbitrary/file.path')\n        r.close()\n        assert path == '/arbitrary/file.path'\n\n\ndef test_stream_response_to_file_uses_content_disposition():\n    s = requests.Session()\n    recorder = get_betamax(s)\n    url = ('https://api.github.com/repos/sigmavirus24/github3.py/releases/'\n           'assets/37944')\n    filename = 'github3.py-0.7.1-py2.py3-none-any.whl'\n    with recorder.use_cassette('stream_response_to_file', **preserve_bytes):\n        r = s.get(url, headers={'Accept': 'application/octet-stream'},\n                  stream=True)\n        stream.stream_response_to_file(r)\n\n    assert os.path.exists(filename)\n    os.unlink(filename)\n\n\ndef test_stream_response_to_specific_filename():\n    s = requests.Session()\n    recorder = get_betamax(s)\n    url = ('https://api.github.com/repos/sigmavirus24/github3.py/releases/'\n           'assets/37944')\n    filename = 'github3.py.whl'\n    with recorder.use_cassette('stream_response_to_file', **preserve_bytes):\n        r = s.get(url, headers={'Accept': 'application/octet-stream'},\n                  stream=True)\n        stream.stream_response_to_file(r, path=filename)\n\n    assert os.path.exists(filename)\n    os.unlink(filename)\n\n\ndef test_stream_response_to_directory():\n    s = requests.Session()\n    recorder = get_betamax(s)\n    url = ('https://api.github.com/repos/sigmavirus24/github3.py/releases/'\n           'assets/37944')\n\n    td = tempfile.mkdtemp()\n    try:\n        filename = 'github3.py-0.7.1-py2.py3-none-any.whl'\n        expected_path = os.path.join(td, filename)\n        with recorder.use_cassette('stream_response_to_file', **preserve_bytes):\n            r = s.get(url, headers={'Accept': 'application/octet-stream'},\n                      stream=True)\n            stream.stream_response_to_file(r, path=td)\n\n        assert os.path.exists(expected_path)\n    finally:\n        shutil.rmtree(td)\n\n\ndef test_stream_response_to_existing_file():\n    s = requests.Session()\n    recorder = get_betamax(s)\n    url = ('https://api.github.com/repos/sigmavirus24/github3.py/releases/'\n           'assets/37944')\n    filename = 'github3.py.whl'\n    with open(filename, 'w') as f_existing:\n        f_existing.write('test')\n\n    with recorder.use_cassette('stream_response_to_file', **preserve_bytes):\n        r = s.get(url, headers={'Accept': 'application/octet-stream'},\n                  stream=True)\n    try:\n        stream.stream_response_to_file(r, path=filename)\n    except stream.exc.StreamingError as e:\n        assert str(e).startswith('File already exists:')\n    else:\n        assert False, \"Should have raised a FileExistsError\"\n    finally:\n        os.unlink(filename)\n\n\ndef test_stream_response_to_file_like_object():\n    s = requests.Session()\n    recorder = get_betamax(s)\n    url = ('https://api.github.com/repos/sigmavirus24/github3.py/releases/'\n           'assets/37944')\n    file_obj = io.BytesIO()\n    with recorder.use_cassette('stream_response_to_file', **preserve_bytes):\n        r = s.get(url, headers={'Accept': 'application/octet-stream'},\n                  stream=True)\n        stream.stream_response_to_file(r, path=file_obj)\n\n    assert 0 < file_obj.tell()\n\n\ndef test_stream_response_to_file_chunksize():\n    s = requests.Session()\n    recorder = get_betamax(s)\n    url = ('https://api.github.com/repos/sigmavirus24/github3.py/releases/'\n           'assets/37944')\n\n    class FileWrapper(io.BytesIO):\n        def __init__(self):\n            super(FileWrapper, self).__init__()\n            self.chunk_sizes = []\n\n        def write(self, data):\n            self.chunk_sizes.append(len(data))\n            return super(FileWrapper, self).write(data)\n\n    file_obj = FileWrapper()\n\n    chunksize = 1231\n\n    with recorder.use_cassette('stream_response_to_file', **preserve_bytes):\n        r = s.get(url, headers={'Accept': 'application/octet-stream'},\n                  stream=True)\n        stream.stream_response_to_file(r, path=file_obj, chunksize=chunksize)\n\n    assert 0 < file_obj.tell()\n\n    assert len(file_obj.chunk_sizes) >= 1\n    assert file_obj.chunk_sizes[0] == chunksize\n\n\n@pytest.fixture\ndef streamed_response(chunks=None):\n    chunks = chunks or [b'chunk'] * 8\n    response = mock.MagicMock()\n    response.raw.stream.return_value = chunks\n    return response\n\n\ndef test_tee(streamed_response):\n    response = streamed_response\n    expected_len = len('chunk') * 8\n    fileobject = io.BytesIO()\n    assert expected_len == sum(len(c) for c in tee.tee(response, fileobject))\n    assert fileobject.getvalue() == b'chunkchunkchunkchunkchunkchunkchunkchunk'\n\n\ndef test_tee_rejects_StringIO():\n    fileobject = io.StringIO()\n    with pytest.raises(TypeError):\n        # The generator needs to be iterated over before the exception will be\n        # raised\n        sum(len(c) for c in tee.tee(None, fileobject))\n\n\ndef test_tee_to_file(streamed_response):\n    response = streamed_response\n    expected_len = len('chunk') * 8\n    assert expected_len == sum(\n        len(c) for c in tee.tee_to_file(response, 'tee.txt')\n        )\n    assert os.path.exists('tee.txt')\n    os.remove('tee.txt')\n\n\ndef test_tee_to_bytearray(streamed_response):\n    response = streamed_response\n    arr = bytearray()\n    expected_arr = bytearray(b'chunk' * 8)\n    expected_len = len(expected_arr)\n    assert expected_len == sum(\n        len(c) for c in tee.tee_to_bytearray(response, arr)\n        )\n    assert expected_arr == arr\n\n\ndef test_tee_to_bytearray_only_accepts_bytearrays():\n    with pytest.raises(TypeError):\n        tee.tee_to_bytearray(None, object())\n", "tests/test_sessions.py": "# -*- coding: utf-8 -*-\nimport unittest\nimport pickle\nimport pytest\n\nfrom requests_toolbelt import sessions\nfrom requests import Request\nfrom . import get_betamax\n\n\nclass TestBasedSession(unittest.TestCase):\n    def test_request_with_base(self):\n        session = sessions.BaseUrlSession('https://httpbin.org/')\n        recorder = get_betamax(session)\n        with recorder.use_cassette('simple_get_request'):\n            response = session.get('/get')\n        response.raise_for_status()\n\n    def test_request_without_base(self):\n        session = sessions.BaseUrlSession()\n        with pytest.raises(ValueError):\n            session.get('/')\n\n    def test_request_override_base(self):\n        session = sessions.BaseUrlSession('https://www.google.com')\n        recorder = get_betamax(session)\n        with recorder.use_cassette('simple_get_request'):\n            response = session.get('https://httpbin.org/get')\n        response.raise_for_status()\n        assert response.json()['headers']['Host'] == 'httpbin.org'\n\n    def test_prepared_request_with_base(self):\n        session = sessions.BaseUrlSession('https://httpbin.org')\n        request = Request(method=\"GET\", url=\"/get\")\n        prepared_request = session.prepare_request(request)\n        recorder = get_betamax(session)\n        with recorder.use_cassette('simple_get_request'):\n            response = session.send(prepared_request)\n        response.raise_for_status()\n\n    def test_prepared_request_without_base(self):\n        session = sessions.BaseUrlSession()\n        request = Request(method=\"GET\", url=\"/\")\n        with pytest.raises(ValueError):\n            prepared_request = session.prepare_request(request)\n            session.send(prepared_request)\n\n    def test_prepared_request_override_base(self):\n        session = sessions.BaseUrlSession('https://www.google.com')\n        request = Request(method=\"GET\", url=\"https://httpbin.org/get\")\n        prepared_request = session.prepare_request(request)\n        recorder = get_betamax(session)\n        with recorder.use_cassette('simple_get_request'):\n            response = session.send(prepared_request)\n        response.raise_for_status()\n        assert response.json()['headers']['Host'] == 'httpbin.org'\n\n    def test_pickle_unpickle_session(self):\n        session = sessions.BaseUrlSession('https://www.google.com')\n        pickled_session = pickle.dumps(session)\n        unpickled_session = pickle.loads(pickled_session)\n        assert session.base_url == unpickled_session.base_url\n", "tests/conftest.py": "# -*- coding: utf-8 -*-\nimport os\nimport sys\n\nimport betamax\n\nsys.path.insert(0, '.')\n\nplaceholders = {\n    '<IPADDR>': os.environ.get('IPADDR', '127.0.0.1'),\n}\n\nwith betamax.Betamax.configure() as config:\n    for placeholder, value in placeholders.items():\n        config.define_cassette_placeholder(placeholder, value)\n", "tests/test_user_agent.py": "# -*- coding: utf-8 -*-\nimport unittest\nimport sys\n\ntry:\n    from unittest.mock import patch\nexcept ImportError:\n    from mock import patch\nimport pytest\n\nfrom requests_toolbelt.utils import user_agent as ua\n\n\nclass Object(object):\n    \"\"\"\n    A simple mock object that can have attributes added to it.\n    \"\"\"\n    pass\n\n\nclass TestUserAgentBuilder(unittest.TestCase):\n    def test_only_user_agent_name(self):\n        assert 'fake/1.0.0' == ua.UserAgentBuilder('fake', '1.0.0').build()\n\n    def test_includes_extras(self):\n        expected = 'fake/1.0.0 another-fake/2.0.1 yet-another-fake/17.1.0'\n        actual = ua.UserAgentBuilder('fake', '1.0.0').include_extras([\n            ('another-fake', '2.0.1'),\n            ('yet-another-fake', '17.1.0'),\n        ]).build()\n        assert expected == actual\n\n    @patch('platform.python_implementation', return_value='CPython')\n    @patch('platform.python_version', return_value='2.7.13')\n    def test_include_implementation(self, *_):\n        expected = 'fake/1.0.0 CPython/2.7.13'\n        actual = ua.UserAgentBuilder('fake', '1.0.0').include_implementation(\n            ).build()\n        assert expected == actual\n\n    @patch('platform.system', return_value='Linux')\n    @patch('platform.release', return_value='4.9.5')\n    def test_include_system(self, *_):\n        expected = 'fake/1.0.0 Linux/4.9.5'\n        actual = ua.UserAgentBuilder('fake', '1.0.0').include_system(\n            ).build()\n        assert expected == actual\n\n\nclass TestUserAgent(unittest.TestCase):\n    def test_user_agent_provides_package_name(self):\n        assert \"my-package\" in ua.user_agent(\"my-package\", \"0.0.1\")\n\n    def test_user_agent_provides_package_version(self):\n        assert \"0.0.1\" in ua.user_agent(\"my-package\", \"0.0.1\")\n\n    def test_user_agent_builds_extras_appropriately(self):\n        assert \"extra/1.0.0\" in ua.user_agent(\n            \"my-package\", \"0.0.1\", extras=[(\"extra\", \"1.0.0\")]\n        )\n\n    def test_user_agent_checks_extras_for_tuples_of_incorrect_length(self):\n        with pytest.raises(ValueError):\n            ua.user_agent(\"my-package\", \"0.0.1\", extras=[\n                (\"extra\", \"1.0.0\", \"oops\")\n            ])\n\n        with pytest.raises(ValueError):\n            ua.user_agent(\"my-package\", \"0.0.1\", extras=[\n                (\"extra\",)\n            ])\n\n\nclass TestImplementationString(unittest.TestCase):\n    @patch('platform.python_implementation')\n    @patch('platform.python_version')\n    def test_cpython_implementation(self, mock_version, mock_implementation):\n        mock_implementation.return_value = 'CPython'\n        mock_version.return_value = '2.7.5'\n        assert 'CPython/2.7.5' == ua._implementation_string()\n\n    @patch('platform.python_implementation')\n    def test_pypy_implementation_final(self, mock_implementation):\n        mock_implementation.return_value = 'PyPy'\n        sys.pypy_version_info = Object()\n        sys.pypy_version_info.major = 2\n        sys.pypy_version_info.minor = 0\n        sys.pypy_version_info.micro = 1\n        sys.pypy_version_info.releaselevel = 'final'\n\n        assert 'PyPy/2.0.1' == ua._implementation_string()\n\n    @patch('platform.python_implementation')\n    def test_pypy_implementation_non_final(self, mock_implementation):\n        mock_implementation.return_value = 'PyPy'\n        sys.pypy_version_info = Object()\n        sys.pypy_version_info.major = 2\n        sys.pypy_version_info.minor = 0\n        sys.pypy_version_info.micro = 1\n        sys.pypy_version_info.releaselevel = 'beta2'\n\n        assert 'PyPy/2.0.1beta2' == ua._implementation_string()\n\n    @patch('platform.python_implementation')\n    def test_unknown_implementation(self, mock_implementation):\n        mock_implementation.return_value = \"Lukasa'sSuperPython\"\n\n        assert \"Lukasa'sSuperPython/Unknown\" == ua._implementation_string()\n", "tests/test_multipart_decoder.py": "# -*- coding: utf-8 -*-\nimport io\nimport sys\nimport unittest\ntry:\n    from unittest import mock\nexcept ImportError:\n    import mock\nimport pytest\nimport requests\nfrom requests_toolbelt.multipart.decoder import BodyPart\nfrom requests_toolbelt.multipart.decoder import (\n    ImproperBodyPartContentException\n)\nfrom requests_toolbelt.multipart.decoder import MultipartDecoder\nfrom requests_toolbelt.multipart.decoder import (\n    NonMultipartContentTypeException\n)\nfrom requests_toolbelt.multipart.encoder import encode_with\nfrom requests_toolbelt.multipart.encoder import MultipartEncoder\n\n\nclass TestBodyPart(unittest.TestCase):\n    @staticmethod\n    def u(content):\n        major = sys.version_info[0]\n        if major == 3:\n            return content\n        else:\n            return unicode(content.replace(r'\\\\', r'\\\\\\\\'), 'unicode_escape')\n\n    @staticmethod\n    def bodypart_bytes_from_headers_and_values(headers, value, encoding):\n        return b'\\r\\n\\r\\n'.join(\n            [\n                b'\\r\\n'.join(\n                    [\n                        b': '.join([encode_with(i, encoding) for i in h])\n                        for h in headers\n                    ]\n                ),\n                encode_with(value, encoding)\n            ]\n        )\n\n    def setUp(self):\n        self.header_1 = (TestBodyPart.u('Snowman'), TestBodyPart.u('\u2603'))\n        self.value_1 = TestBodyPart.u('\u00a9')\n        self.part_1 = BodyPart(\n            TestBodyPart.bodypart_bytes_from_headers_and_values(\n                (self.header_1,), self.value_1, 'utf-8'\n            ),\n            'utf-8'\n        )\n        self.part_2 = BodyPart(\n            TestBodyPart.bodypart_bytes_from_headers_and_values(\n                [], self.value_1, 'utf-16'\n            ),\n            'utf-16'\n        )\n\n    def test_equality_content_should_be_equal(self):\n        part_3 = BodyPart(\n            TestBodyPart.bodypart_bytes_from_headers_and_values(\n                [], self.value_1, 'utf-8'\n            ),\n            'utf-8'\n        )\n        assert self.part_1.content == part_3.content\n\n    def test_equality_content_equals_bytes(self):\n        assert self.part_1.content == encode_with(self.value_1, 'utf-8')\n\n    def test_equality_content_should_not_be_equal(self):\n        assert self.part_1.content != self.part_2.content\n\n    def test_equality_content_does_not_equal_bytes(self):\n        assert self.part_1.content != encode_with(self.value_1, 'latin-1')\n\n    def test_changing_encoding_changes_text(self):\n        part_2_orig_text = self.part_2.text\n        self.part_2.encoding = 'latin-1'\n        assert self.part_2.text != part_2_orig_text\n\n    def test_text_should_be_equal(self):\n        assert self.part_1.text == self.part_2.text\n\n    def test_no_headers(self):\n        sample_1 = b'\\r\\n\\r\\nNo headers\\r\\nTwo lines'\n        part_3 = BodyPart(sample_1, 'utf-8')\n        assert len(part_3.headers) == 0\n        assert part_3.content == b'No headers\\r\\nTwo lines'\n\n    def test_no_crlf_crlf_in_content(self):\n        content = b'no CRLF CRLF here!\\r\\n'\n        with pytest.raises(ImproperBodyPartContentException):\n            BodyPart(content, 'utf-8')\n\n\nclass TestMultipartDecoder(unittest.TestCase):\n    def setUp(self):\n        self.sample_1 = (\n            ('field 1', 'value 1'),\n            ('field 2', 'value 2'),\n            ('field 3', 'value 3'),\n            ('field 4', 'value 4'),\n        )\n        self.boundary = 'test boundary'\n        self.encoded_1 = MultipartEncoder(self.sample_1, self.boundary)\n        self.decoded_1 = MultipartDecoder(\n            self.encoded_1.to_string(),\n            self.encoded_1.content_type\n        )\n\n    def test_non_multipart_response_fails(self):\n        jpeg_response = mock.NonCallableMagicMock(spec=requests.Response)\n        jpeg_response.headers = {'content-type': 'image/jpeg'}\n        with pytest.raises(NonMultipartContentTypeException):\n            MultipartDecoder.from_response(jpeg_response)\n\n    def test_length_of_parts(self):\n        assert len(self.sample_1) == len(self.decoded_1.parts)\n\n    def test_content_of_parts(self):\n        def parts_equal(part, sample):\n            return part.content == encode_with(sample[1], 'utf-8')\n\n        parts_iter = zip(self.decoded_1.parts, self.sample_1)\n        assert all(parts_equal(part, sample) for part, sample in parts_iter)\n\n    def test_header_of_parts(self):\n        def parts_header_equal(part, sample):\n            return part.headers[b'Content-Disposition'] == encode_with(\n                'form-data; name=\"{}\"'.format(sample[0]), 'utf-8'\n            )\n\n        parts_iter = zip(self.decoded_1.parts, self.sample_1)\n        assert all(\n            parts_header_equal(part, sample)\n            for part, sample in parts_iter\n        )\n\n    def test_from_response(self):\n        response = mock.NonCallableMagicMock(spec=requests.Response)\n        response.headers = {\n            'content-type': 'multipart/related; boundary=\"samp1\"'\n        }\n        cnt = io.BytesIO()\n        cnt.write(b'\\r\\n--samp1\\r\\n')\n        cnt.write(b'Header-1: Header-Value-1\\r\\n')\n        cnt.write(b'Header-2: Header-Value-2\\r\\n')\n        cnt.write(b'\\r\\n')\n        cnt.write(b'Body 1, Line 1\\r\\n')\n        cnt.write(b'Body 1, Line 2\\r\\n')\n        cnt.write(b'--samp1\\r\\n')\n        cnt.write(b'\\r\\n')\n        cnt.write(b'Body 2, Line 1\\r\\n')\n        cnt.write(b'--samp1--\\r\\n')\n        response.content = cnt.getvalue()\n        decoder_2 = MultipartDecoder.from_response(response)\n        assert decoder_2.content_type == response.headers['content-type']\n        assert (\n            decoder_2.parts[0].content == b'Body 1, Line 1\\r\\nBody 1, Line 2'\n        )\n        assert decoder_2.parts[0].headers[b'Header-1'] == b'Header-Value-1'\n        assert len(decoder_2.parts[1].headers) == 0\n        assert decoder_2.parts[1].content == b'Body 2, Line 1'\n\n    def test_from_responsecaplarge(self):\n        response = mock.NonCallableMagicMock(spec=requests.Response)\n        response.headers = {\n            'content-type': 'Multipart/Related; boundary=\"samp1\"'\n        }\n        cnt = io.BytesIO()\n        cnt.write(b'\\r\\n--samp1\\r\\n')\n        cnt.write(b'Header-1: Header-Value-1\\r\\n')\n        cnt.write(b'Header-2: Header-Value-2\\r\\n')\n        cnt.write(b'\\r\\n')\n        cnt.write(b'Body 1, Line 1\\r\\n')\n        cnt.write(b'Body 1, Line 2\\r\\n')\n        cnt.write(b'--samp1\\r\\n')\n        cnt.write(b'\\r\\n')\n        cnt.write(b'Body 2, Line 1\\r\\n')\n        cnt.write(b'--samp1--\\r\\n')\n        response.content = cnt.getvalue()\n        decoder_2 = MultipartDecoder.from_response(response)\n        assert decoder_2.content_type == response.headers['content-type']\n        assert (\n            decoder_2.parts[0].content == b'Body 1, Line 1\\r\\nBody 1, Line 2'\n        )\n        assert decoder_2.parts[0].headers[b'Header-1'] == b'Header-Value-1'\n        assert len(decoder_2.parts[1].headers) == 0\n        assert decoder_2.parts[1].content == b'Body 2, Line 1'\n", "tests/test_x509_adapter.py": "# -*- coding: utf-8 -*-\nimport requests\nimport unittest\nimport pytest\n\ntry:\n    import OpenSSL\nexcept ImportError:\n    PYOPENSSL_AVAILABLE = False\nelse:\n    PYOPENSSL_AVAILABLE = True\n    from requests_toolbelt.adapters.x509 import X509Adapter\n    from cryptography import x509\n    from cryptography.hazmat.primitives.serialization import (\n        Encoding,\n        PrivateFormat,\n        BestAvailableEncryption,\n        load_pem_private_key,\n    )\n    import trustme\n\nfrom requests_toolbelt import exceptions as exc\nfrom . import get_betamax\n\nREQUESTS_SUPPORTS_SSL_CONTEXT = requests.__build__ >= 0x021200\n\npytestmark = pytest.mark.filterwarnings(\n    \"ignore:'urllib3.contrib.pyopenssl' module is deprecated:DeprecationWarning\")\n\n\nclass TestX509Adapter(unittest.TestCase):\n    \"\"\"Tests a simple requests.get() call using a .p12 cert.\n    \"\"\"\n    def setUp(self):\n        self.pkcs12_password_bytes = \"test\".encode('utf8')\n        self.session = requests.Session()\n\n    @pytest.mark.skipif(not REQUESTS_SUPPORTS_SSL_CONTEXT,\n                        reason=\"Requires Requests v2.12.0 or later\")\n    @pytest.mark.skipif(not PYOPENSSL_AVAILABLE,\n                        reason=\"Requires OpenSSL\")\n    def test_x509_pem(self):\n        ca = trustme.CA()\n        cert = ca.issue_cert(u'pkiprojecttest01.dev.labs.internal')\n        cert_bytes = cert.cert_chain_pems[0].bytes()\n        pk_bytes = cert.private_key_pem.bytes()\n\n        adapter = X509Adapter(max_retries=3, cert_bytes=cert_bytes, pk_bytes=pk_bytes)\n        self.session.mount('https://', adapter)\n        recorder = get_betamax(self.session)\n        with recorder.use_cassette('test_x509_adapter_pem'):\n            r = self.session.get('https://pkiprojecttest01.dev.labs.internal/', verify=False)\n\n        assert r.status_code == 200\n        assert r.text\n\n    @pytest.mark.skipif(not REQUESTS_SUPPORTS_SSL_CONTEXT,\n                    reason=\"Requires Requests v2.12.0 or later\")\n    @pytest.mark.skipif(not PYOPENSSL_AVAILABLE,\n                    reason=\"Requires OpenSSL\")\n    def test_x509_der_and_password(self):\n        ca = trustme.CA()\n        cert = ca.issue_cert(u'pkiprojecttest01.dev.labs.internal')\n        cert_bytes = x509.load_pem_x509_certificate(\n            cert.cert_chain_pems[0].bytes()).public_bytes(Encoding.DER)\n        pem_pk = load_pem_private_key(cert.private_key_pem.bytes(), password=None)\n        pk_bytes = pem_pk.private_bytes(Encoding.DER, PrivateFormat.PKCS8,\n                                        BestAvailableEncryption(self.pkcs12_password_bytes))\n\n        adapter = X509Adapter(max_retries=3, cert_bytes=cert_bytes, pk_bytes=pk_bytes,\n                              password=self.pkcs12_password_bytes, encoding=Encoding.DER)\n        self.session.mount('https://', adapter)\n        recorder = get_betamax(self.session)\n        with recorder.use_cassette('test_x509_adapter_der'):\n            r = self.session.get('https://pkiprojecttest01.dev.labs.internal/', verify=False)\n\n        assert r.status_code == 200\n        assert r.text\n\n    @pytest.mark.skipif(REQUESTS_SUPPORTS_SSL_CONTEXT, reason=\"Will not raise exc\")\n    def test_requires_new_enough_requests(self):\n        with pytest.raises(exc.VersionMismatchError):\n            X509Adapter()\n", "tests/test_multipart_encoder.py": "# -*- coding: utf-8 -*-\nimport unittest\nimport io\n\nimport requests\n\nimport pytest\nfrom requests_toolbelt.multipart.encoder import (\n    CustomBytesIO, MultipartEncoder, FileFromURLWrapper, FileNotSupportedError)\nfrom requests_toolbelt._compat import filepost\nfrom . import get_betamax\n\n\npreserve_bytes = {'preserve_exact_body_bytes': True}\n\n\nclass LargeFileMock(object):\n    def __init__(self):\n        # Let's keep track of how many bytes we've given\n        self.bytes_read = 0\n        # Our limit (1GB)\n        self.bytes_max = 1024 * 1024 * 1024\n        # Fake name\n        self.name = 'fake_name.py'\n        # Create a fileno attribute\n        self.fileno = None\n\n    def __len__(self):\n        return self.bytes_max\n\n    def read(self, size=None):\n        if self.bytes_read >= self.bytes_max:\n            return b''\n\n        if size is None:\n            length = self.bytes_max - self.bytes_read\n        else:\n            length = size\n\n        length = int(length)\n        length = min([length, self.bytes_max - self.bytes_read])\n\n        self.bytes_read += length\n\n        return b'a' * length\n\n    def tell(self):\n        return self.bytes_read\n\n\nclass TestCustomBytesIO(unittest.TestCase):\n    def setUp(self):\n        self.instance = CustomBytesIO()\n\n    def test_writable(self):\n        assert hasattr(self.instance, 'write')\n        assert self.instance.write(b'example') == 7\n\n    def test_readable(self):\n        assert hasattr(self.instance, 'read')\n        assert self.instance.read() == b''\n        assert self.instance.read(10) == b''\n\n    def test_can_read_after_writing_to(self):\n        self.instance.write(b'example text')\n        self.instance.read() == b'example text'\n\n    def test_can_read_some_after_writing_to(self):\n        self.instance.write(b'example text')\n        self.instance.read(6) == b'exampl'\n\n    def test_can_get_length(self):\n        self.instance.write(b'example')\n        self.instance.seek(0, 0)\n        assert self.instance.len == 7\n\n    def test_truncates_intelligently(self):\n        self.instance.write(b'abcdefghijklmnopqrstuvwxyzabcd')  # 30 bytes\n        assert self.instance.tell() == 30\n        self.instance.seek(-10, 2)\n        self.instance.smart_truncate()\n        assert self.instance.len == 10\n        assert self.instance.read() == b'uvwxyzabcd'\n        assert self.instance.tell() == 10\n\n    def test_accepts_encoded_strings_with_unicode(self):\n        \"\"\"Accepts a string with encoded unicode characters.\"\"\"\n        s = b'this is a unicode string: \\xc3\\xa9 \\xc3\\xa1 \\xc7\\xab \\xc3\\xb3'\n        self.instance = CustomBytesIO(s)\n        assert self.instance.read() == s\n\n\nclass TestFileFromURLWrapper(unittest.TestCase):\n    def setUp(self):\n        self.session = requests.Session()\n        self.recorder = get_betamax(self.session)\n\n    def test_read_file(self):\n        url = ('https://stxnext.com/static/img/logo.830ebe551641.svg')\n        with self.recorder.use_cassette(\n                'file_for_download', **preserve_bytes):\n            self.instance = FileFromURLWrapper(url, session=self.session)\n            assert self.instance.len == 5177\n            chunk = self.instance.read(20)\n            assert chunk == b'<svg xmlns=\"http://w'\n            assert self.instance.len == 5157\n            chunk = self.instance.read(0)\n            assert chunk == b''\n            assert self.instance.len == 5157\n            chunk = self.instance.read(10)\n            assert chunk == b'ww.w3.org/'\n            assert self.instance.len == 5147\n\n    def test_no_content_length_header(self):\n        url = (\n            'https://api.github.com/repos/sigmavirus24/github3.py/releases/'\n            'assets/37944'\n        )\n        with self.recorder.use_cassette(\n                'stream_response_without_content_length_to_file', **preserve_bytes):\n            with self.assertRaises(FileNotSupportedError) as context:\n                FileFromURLWrapper(url, session=self.session)\n            assert context.exception.__str__() == (\n                'Data from provided URL https://api.github.com/repos/s'\n                'igmavirus24/github3.py/releases/assets/37944 is not '\n                'supported. Lack of content-length Header in requested'\n                ' file response.'\n            )\n\n\nclass TestMultipartEncoder(unittest.TestCase):\n    def setUp(self):\n        self.parts = [('field', 'value'), ('other_field', 'other_value')]\n        self.boundary = 'this-is-a-boundary'\n        self.instance = MultipartEncoder(self.parts, boundary=self.boundary)\n\n    def test_to_string(self):\n        assert self.instance.to_string() == (\n            '--this-is-a-boundary\\r\\n'\n            'Content-Disposition: form-data; name=\"field\"\\r\\n\\r\\n'\n            'value\\r\\n'\n            '--this-is-a-boundary\\r\\n'\n            'Content-Disposition: form-data; name=\"other_field\"\\r\\n\\r\\n'\n            'other_value\\r\\n'\n            '--this-is-a-boundary--\\r\\n'\n        ).encode()\n\n    def test_content_type(self):\n        expected = 'multipart/form-data; boundary=this-is-a-boundary'\n        assert self.instance.content_type == expected\n\n    def test_encodes_data_the_same(self):\n        encoded = filepost.encode_multipart_formdata(self.parts,\n                                                     self.boundary)[0]\n        assert encoded == self.instance.read()\n\n    def test_streams_its_data(self):\n        large_file = LargeFileMock()\n        parts = {'some field': 'value',\n                 'some file': large_file,\n                 }\n        encoder = MultipartEncoder(parts)\n        total_size = encoder.len\n        read_size = 1024 * 1024 * 128\n        already_read = 0\n        while True:\n            read = encoder.read(read_size)\n            already_read += len(read)\n            if not read:\n                break\n\n        assert encoder._buffer.tell() <= read_size\n        assert already_read == total_size\n\n    def test_length_is_correct(self):\n        encoded = filepost.encode_multipart_formdata(self.parts,\n                                                     self.boundary)[0]\n        assert len(encoded) == self.instance.len\n\n    def test_encodes_with_readable_data(self):\n        s = io.BytesIO(b'value')\n        m = MultipartEncoder([('field', s)], boundary=self.boundary)\n        assert m.read() == (\n            '--this-is-a-boundary\\r\\n'\n            'Content-Disposition: form-data; name=\"field\"\\r\\n\\r\\n'\n            'value\\r\\n'\n            '--this-is-a-boundary--\\r\\n'\n        ).encode()\n\n    def test_reads_open_file_objects(self):\n        with open('setup.py', 'rb') as fd:\n            m = MultipartEncoder([('field', 'foo'), ('file', fd)])\n            assert m.read() is not None\n\n    def test_reads_file_from_url_wrapper(self):\n        s = requests.Session()\n        recorder = get_betamax(s)\n        url = ('https://stxnext.com/static/img/logo.830ebe551641.svg')\n        with recorder.use_cassette(\n                'file_for_download'):\n            m = MultipartEncoder(\n                [('field', 'foo'), ('file', FileFromURLWrapper(url, session=s))])\n        assert m.read() is not None\n\n    def test_reads_open_file_objects_with_a_specified_filename(self):\n        with open('setup.py', 'rb') as fd:\n            m = MultipartEncoder(\n                [('field', 'foo'), ('file', ('filename', fd, 'text/plain'))]\n                )\n            assert m.read() is not None\n\n    def test_reads_open_file_objects_using_to_string(self):\n        with open('setup.py', 'rb') as fd:\n            m = MultipartEncoder([('field', 'foo'), ('file', fd)])\n            assert m.to_string() is not None\n\n    def test_handles_encoded_unicode_strings(self):\n        m = MultipartEncoder([\n            ('field',\n             b'this is a unicode string: \\xc3\\xa9 \\xc3\\xa1 \\xc7\\xab \\xc3\\xb3')\n        ])\n        assert m.read() is not None\n\n    def test_handles_uncode_strings(self):\n        s = b'this is a unicode string: \\xc3\\xa9 \\xc3\\xa1 \\xc7\\xab \\xc3\\xb3'\n        m = MultipartEncoder([\n            ('field', s.decode('utf-8'))\n        ])\n        assert m.read() is not None\n\n    def test_regresion_1(self):\n        \"\"\"Ensure issue #31 doesn't ever happen again.\"\"\"\n        fields = {\n            \"test\": \"t\" * 100\n        }\n\n        for x in range(30):\n            fields['f%d' % x] = (\n                'test', open('tests/test_multipart_encoder.py', 'rb')\n                )\n\n        m = MultipartEncoder(fields=fields)\n        total_size = m.len\n\n        blocksize = 8192\n        read_so_far = 0\n\n        while True:\n            data = m.read(blocksize)\n            if not data:\n                break\n            read_so_far += len(data)\n\n        assert read_so_far == total_size\n\n    def test_regression_2(self):\n        \"\"\"Ensure issue #31 doesn't ever happen again.\"\"\"\n        fields = {\n            \"test\": \"t\" * 8100\n        }\n\n        m = MultipartEncoder(fields=fields)\n        total_size = m.len\n\n        blocksize = 8192\n        read_so_far = 0\n\n        while True:\n            data = m.read(blocksize)\n            if not data:\n                break\n            read_so_far += len(data)\n\n        assert read_so_far == total_size\n\n    def test_handles_empty_unicode_values(self):\n        \"\"\"Verify that the Encoder can handle empty unicode strings.\n\n        See https://github.com/requests/toolbelt/issues/46 for\n        more context.\n        \"\"\"\n        fields = [(b'test'.decode('utf-8'), b''.decode('utf-8'))]\n        m = MultipartEncoder(fields=fields)\n        assert len(m.read()) > 0\n\n    def test_accepts_custom_content_type(self):\n        \"\"\"Verify that the Encoder handles custom content-types.\n\n        See https://github.com/requests/toolbelt/issues/52\n        \"\"\"\n        fields = [\n            (b'test'.decode('utf-8'), (b'filename'.decode('utf-8'),\n                                       b'filecontent',\n                                       b'application/json'.decode('utf-8')))\n        ]\n        m = MultipartEncoder(fields=fields)\n        output = m.read().decode('utf-8')\n        assert output.index('Content-Type: application/json\\r\\n') > 0\n\n    def test_accepts_custom_headers(self):\n        \"\"\"Verify that the Encoder handles custom headers.\n\n        See https://github.com/requests/toolbelt/issues/52\n        \"\"\"\n        fields = [\n            (b'test'.decode('utf-8'), (b'filename'.decode('utf-8'),\n                                       b'filecontent',\n                                       b'application/json'.decode('utf-8'),\n                                       {'X-My-Header': 'my-value'}))\n        ]\n        m = MultipartEncoder(fields=fields)\n        output = m.read().decode('utf-8')\n        assert output.index('X-My-Header: my-value\\r\\n') > 0\n\n    def test_no_parts(self):\n        fields = []\n        boundary = '--90967316f8404798963cce746a4f4ef9'\n        m = MultipartEncoder(fields=fields, boundary=boundary)\n        output = m.read().decode('utf-8')\n        assert output == '----90967316f8404798963cce746a4f4ef9--\\r\\n'\n\nif __name__ == '__main__':\n    unittest.main()\n", "tests/__init__.py": "# -*- coding: utf-8 -*-\nimport betamax\n\n\ndef get_betamax(session):\n    return betamax.Betamax(\n        session,\n        cassette_library_dir='tests/cassettes')\n", "tests/test_multipart_monitor.py": "# -*- coding: utf-8 -*-\nimport math\nimport unittest\nfrom requests_toolbelt.multipart.encoder import (\n    IDENTITY, MultipartEncoder, MultipartEncoderMonitor\n    )\n\n\nclass TestMultipartEncoderMonitor(unittest.TestCase):\n    def setUp(self):\n        self.fields = {'a': 'b'}\n        self.boundary = 'thisisaboundary'\n        self.encoder = MultipartEncoder(self.fields, self.boundary)\n        self.monitor = MultipartEncoderMonitor(self.encoder)\n\n    def test_content_type(self):\n        assert self.monitor.content_type == self.encoder.content_type\n\n    def test_length(self):\n        assert self.encoder.len == self.monitor.len\n\n    def test_read(self):\n        new_encoder = MultipartEncoder(self.fields, self.boundary)\n        assert new_encoder.read() == self.monitor.read()\n\n    def test_callback_called_when_reading_everything(self):\n        callback = Callback(self.monitor)\n        self.monitor.callback = callback\n        self.monitor.read()\n        assert callback.called == 1\n\n    def test_callback(self):\n        callback = Callback(self.monitor)\n        self.monitor.callback = callback\n        chunk_size = int(math.ceil(self.encoder.len / 4.0))\n        while self.monitor.read(chunk_size):\n            pass\n        assert callback.called == 5\n\n    def test_bytes_read(self):\n        bytes_to_read = self.encoder.len\n        self.monitor.read()\n        assert self.monitor.bytes_read == bytes_to_read\n\n    def test_default_callable_is_the_identity(self):\n        assert self.monitor.callback == IDENTITY\n        assert IDENTITY(1) == 1\n\n    def test_from_fields(self):\n        monitor = MultipartEncoderMonitor.from_fields(\n            self.fields, self.boundary\n            )\n        assert isinstance(monitor, MultipartEncoderMonitor)\n        assert isinstance(monitor.encoder, MultipartEncoder)\n        assert monitor.encoder.boundary_value == self.boundary\n\n\nclass Callback(object):\n    def __init__(self, monitor):\n        self.called = 0\n        self.monitor = monitor\n\n    def __call__(self, monitor):\n        self.called += 1\n        assert monitor == self.monitor\n", "tests/test_ssladapter.py": "# -*- coding: utf-8 -*-\ntry:\n    from unittest import mock\nexcept ImportError:\n    import mock\nimport pytest\nimport requests\nimport unittest\n\nfrom requests_toolbelt import SSLAdapter\nfrom . import get_betamax\n\n\nclass TestSSLAdapter(unittest.TestCase):\n    def setUp(self):\n        self.session = requests.Session()\n        self.session.mount('https://', SSLAdapter('SSLv3'))\n        self.recorder = get_betamax(self.session)\n\n    def test_klevas(self):\n        with self.recorder.use_cassette('klevas_vu_lt_ssl3'):\n            r = self.session.get('https://klevas.vu.lt/')\n            assert r.status_code == 200\n\n    @pytest.mark.skipif(requests.__build__ < 0x020400,\n                        reason=\"Requires Requests v2.4.0 or later\")\n    @mock.patch('requests.packages.urllib3.poolmanager.ProxyManager')\n    def test_proxies(self, ProxyManager):\n        a = SSLAdapter('SSLv3')\n        a.proxy_manager_for('http://127.0.0.1:8888')\n\n        assert ProxyManager.call_count == 1\n        kwargs = ProxyManager.call_args_list[0][1]\n        assert kwargs['ssl_version'] == 'SSLv3'\n", "tests/test_auth_bearer.py": "# -*- coding: utf-8 -*-\nimport requests\nimport unittest\ntry:\n    from unittest import mock\nexcept ImportError:\n    import mock\n\nfrom requests_toolbelt.auth.http_bearer import HTTPBearerAuth\nfrom . import get_betamax\n\n\nclass TestBearerAuth(unittest.TestCase):\n    def setUp(self):\n        self.session = requests.Session()\n        self.recorder = get_betamax(self.session)\n\n    def cassette(self):\n        return self.recorder.use_cassette(\n            'httpbin_bearer_auth',\n            match_requests_on=['method', 'uri']\n        )\n\n    def test_bearer(self):\n        with self.cassette():\n            r = self.session.request(\n                'GET', 'http://httpbin.org/bearer-auth/',\n                auth=HTTPBearerAuth('eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c'))\n\n        assert r.json() == {'authenticated': True, 'user': 'user'}\n", "tests/threaded/test_api.py": "\"\"\"Module containing tests for requests_toolbelt.threaded API.\"\"\"\n\ntry:\n    from unittest import mock\nexcept ImportError:\n    import mock\nimport pytest\n\nfrom requests_toolbelt._compat import queue\nfrom requests_toolbelt import threaded\n\n\ndef test_creates_a_pool_for_the_user():\n    \"\"\"Assert a Pool object is used correctly and as we expect.\n\n    This just ensures that we're not jumping through any extra hoops with our\n    internal usage of a Pool object.\n    \"\"\"\n    mocked_pool = mock.Mock(spec=['join_all', 'responses', 'exceptions'])\n    with mock.patch('requests_toolbelt.threaded.pool.Pool') as Pool:\n        Pool.return_value = mocked_pool\n        threaded.map([{}, {}])\n\n    assert Pool.called is True\n    _, kwargs = Pool.call_args\n    assert 'job_queue' in kwargs\n    assert isinstance(kwargs['job_queue'], queue.Queue)\n    mocked_pool.join_all.assert_called_once_with()\n    mocked_pool.responses.assert_called_once_with()\n    mocked_pool.exceptions.assert_called_once_with()\n\n\ndef test_raises_a_value_error_for_non_dictionaries():\n    \"\"\"Exercise our lazy valdation.\"\"\"\n    with pytest.raises(ValueError):\n        threaded.map([[], []])\n\n\ndef test_raises_a_value_error_for_falsey_requests():\n    \"\"\"Assert that the requests param is truthy.\"\"\"\n    with pytest.raises(ValueError):\n        threaded.map([])\n\n    with pytest.raises(ValueError):\n        threaded.map(None)\n\n\ndef test_passes_on_kwargs():\n    \"\"\"Verify that we pass on kwargs to the Pool constructor.\"\"\"\n    mocked_pool = mock.Mock(spec=['join_all', 'responses', 'exceptions'])\n    with mock.patch('requests_toolbelt.threaded.pool.Pool') as Pool:\n        Pool.return_value = mocked_pool\n        threaded.map([{}, {}], num_processes=1000,\n                     initializer=test_passes_on_kwargs)\n\n    _, kwargs = Pool.call_args\n    assert 'job_queue' in kwargs\n    assert 'num_processes' in kwargs\n    assert 'initializer' in kwargs\n\n    assert kwargs['num_processes'] == 1000\n    assert kwargs['initializer'] == test_passes_on_kwargs\n", "tests/threaded/__init__.py": "", "tests/threaded/test_pool.py": "\"\"\"Module containing the tests for requests_toolbelt.threaded.pool.\"\"\"\ntry:\n    import queue  # Python 3\nexcept ImportError:\n    import Queue as queue\nimport unittest\n\ntry:\n    from unittest import mock\nexcept ImportError:\n    import mock\nimport pytest\n\nfrom requests_toolbelt.threaded import pool\nfrom requests_toolbelt.threaded import thread\n\n\nclass TestPool(unittest.TestCase):\n\n    \"\"\"Collection of tests for requests_toolbelt.threaded.pool.Pool.\"\"\"\n\n    def test_requires_positive_number_of_processes(self):\n        \"\"\"Show that the number of processes has to be > 0.\"\"\"\n        with pytest.raises(ValueError):\n            pool.Pool(None, num_processes=0)\n\n        with pytest.raises(ValueError):\n            pool.Pool(None, num_processes=-1)\n\n    def test_number_of_processes_can_be_arbitrary(self):\n        \"\"\"Show that the number of processes can be set.\"\"\"\n        job_queue = queue.Queue()\n        p = pool.Pool(job_queue, num_processes=100)\n        assert p._processes == 100\n        assert len(p._pool) == 100\n\n        job_queue = queue.Queue()\n        p = pool.Pool(job_queue, num_processes=1)\n        assert p._processes == 1\n        assert len(p._pool) == 1\n\n    def test_initializer_is_called(self):\n        \"\"\"Ensure that the initializer function is called.\"\"\"\n        job_queue = queue.Queue()\n        initializer = mock.MagicMock()\n        pool.Pool(job_queue, num_processes=1, initializer=initializer)\n        assert initializer.called is True\n        initializer.assert_called_once_with(mock.ANY)\n\n    def test_auth_generator_is_called(self):\n        \"\"\"Ensure that the auth_generator function is called.\"\"\"\n        job_queue = queue.Queue()\n        auth_generator = mock.MagicMock()\n        pool.Pool(job_queue, num_processes=1, auth_generator=auth_generator)\n        assert auth_generator.called is True\n        auth_generator.assert_called_once_with(mock.ANY)\n\n    def test_session_is_called(self):\n        \"\"\"Ensure that the session function is called.\"\"\"\n        job_queue = queue.Queue()\n        session = mock.MagicMock()\n        pool.Pool(job_queue, num_processes=1, session=session)\n        assert session.called is True\n        session.assert_called_once_with()\n\n    def test_from_exceptions_populates_a_queue(self):\n        \"\"\"Ensure a Queue is properly populated from exceptions.\"\"\"\n        urls = [\"https://httpbin.org/get?n={}\".format(n) for n in range(5)]\n        Exc = pool.ThreadException\n        excs = (Exc({'method': 'GET', 'url': url}, None) for url in urls)\n\n        job_queue = mock.MagicMock()\n        with mock.patch.object(queue, 'Queue', return_value=job_queue):\n            with mock.patch.object(thread, 'SessionThread'):\n                pool.Pool.from_exceptions(excs)\n\n        assert job_queue.put.call_count == 5\n        assert job_queue.put.mock_calls == [\n            mock.call({'method': 'GET', 'url': url})\n            for url in urls\n        ]\n\n    def test_from_urls_constructs_get_requests(self):\n        \"\"\"Ensure a Queue is properly populated from an iterable of urls.\"\"\"\n        urls = [\"https://httpbin.org/get?n={}\".format(n) for n in range(5)]\n\n        job_queue = mock.MagicMock()\n        with mock.patch.object(queue, 'Queue', return_value=job_queue):\n            with mock.patch.object(thread, 'SessionThread'):\n                pool.Pool.from_urls(urls)\n\n        assert job_queue.put.call_count == 5\n        assert job_queue.put.mock_calls == [\n            mock.call({'method': 'GET', 'url': url})\n            for url in urls\n        ]\n\n    def test_from_urls_constructs_get_requests_with_kwargs(self):\n        \"\"\"Ensure a Queue is properly populated from an iterable of urls.\"\"\"\n        def merge(*args):\n            final = {}\n            for d in args:\n                final.update(d)\n            return final\n\n        urls = [\"https://httpbin.org/get?n={}\".format(n) for n in range(5)]\n\n        kwargs = {'stream': True, 'headers': {'Accept': 'application/json'}}\n        job_queue = mock.MagicMock()\n        with mock.patch.object(queue, 'Queue', return_value=job_queue):\n            with mock.patch.object(thread, 'SessionThread'):\n                pool.Pool.from_urls(urls, kwargs)\n\n        assert job_queue.put.call_count == 5\n        assert job_queue.put.mock_calls == [\n            mock.call(merge({'method': 'GET', 'url': url}, kwargs))\n            for url in urls\n        ]\n\n    def test_join_all(self):\n        \"\"\"Ensure that all threads are joined properly.\"\"\"\n        session_threads = []\n\n        def _side_effect(*args, **kwargs):\n            thread = mock.MagicMock()\n            session_threads.append(thread)\n            return thread\n\n        with mock.patch.object(thread, 'SessionThread',\n                               side_effect=_side_effect):\n            pool.Pool(None).join_all()\n\n        for st in session_threads:\n            st.join.assert_called_once_with()\n\n    def test_get_response_returns_thread_response(self):\n        \"\"\"Ensure that a ThreadResponse is made when there's data.\"\"\"\n        queues = []\n\n        def _side_effect():\n            q = mock.MagicMock()\n            q.get_nowait.return_value = ({}, None)\n            queues.append(q)\n            return q\n\n        with mock.patch.object(queue, 'Queue', side_effect=_side_effect):\n            with mock.patch.object(thread, 'SessionThread'):\n                p = pool.Pool(None)\n\n        assert len(queues) == 2\n\n        assert isinstance(p.get_response(), pool.ThreadResponse)\n        assert len([q for q in queues if q.get_nowait.called]) == 1\n\n    def test_get_exception_returns_thread_exception(self):\n        \"\"\"Ensure that a ThreadException is made when there's data.\"\"\"\n        queues = []\n\n        def _side_effect():\n            q = mock.MagicMock()\n            q.get_nowait.return_value = ({}, None)\n            queues.append(q)\n            return q\n\n        with mock.patch.object(queue, 'Queue', side_effect=_side_effect):\n            with mock.patch.object(thread, 'SessionThread'):\n                p = pool.Pool(None)\n\n        assert len(queues) == 2\n\n        assert isinstance(p.get_exception(), pool.ThreadException)\n        assert len([q for q in queues if q.get_nowait.called]) == 1\n\n    def test_get_response_returns_none_when_queue_is_empty(self):\n        \"\"\"Ensure that None is returned when the response Queue is empty.\"\"\"\n        queues = []\n\n        def _side_effect():\n            q = mock.MagicMock()\n            q.get_nowait.side_effect = queue.Empty()\n            queues.append(q)\n            return q\n\n        with mock.patch.object(queue, 'Queue', side_effect=_side_effect):\n            with mock.patch.object(thread, 'SessionThread'):\n                p = pool.Pool(None)\n\n        assert len(queues) == 2\n\n        assert p.get_response() is None\n        assert len([q for q in queues if q.get_nowait.called]) == 1\n\n    def test_get_exception_returns_none_when_queue_is_empty(self):\n        \"\"\"Ensure that None is returned when the exception Queue is empty.\"\"\"\n        queues = []\n\n        def _side_effect():\n            q = mock.MagicMock()\n            q.get_nowait.side_effect = queue.Empty()\n            queues.append(q)\n            return q\n\n        with mock.patch.object(queue, 'Queue', side_effect=_side_effect):\n            with mock.patch.object(thread, 'SessionThread'):\n                p = pool.Pool(None)\n\n        assert len(queues) == 2\n\n        assert p.get_exception() is None\n        assert len([q for q in queues if q.get_nowait.called]) == 1\n\n    def test_lists_are_correctly_returned(self):\n        \"\"\"Ensure that exceptions and responses return correct lists.\"\"\"\n        def _make_queue():\n            q = queue.Queue()\n            q.put(({}, None))\n            return q\n\n        with mock.patch.object(thread, 'SessionThread'):\n            p = pool.Pool(None)\n\n        # Set up real queues.\n        p._response_queue = _make_queue()\n        p._exc_queue = _make_queue()\n\n        excs = list(p.exceptions())\n        assert len(excs) == 1\n        for exc in excs:\n            assert isinstance(exc, pool.ThreadException)\n\n        resps = list(p.responses())\n        assert len(resps) == 1\n        for resp in resps:\n            assert isinstance(resp, pool.ThreadResponse)\n", "tests/threaded/test_thread.py": "\"\"\"Module containing the tests for requests_toolbelt.threaded.thread.\"\"\"\ntry:\n    import queue  # Python 3\nexcept ImportError:\n    import Queue as queue\nimport threading\nimport unittest\nimport uuid\n\ntry:\n    from unittest import mock\nexcept ImportError:\n    import mock\nimport requests.exceptions\n\nfrom requests_toolbelt.threaded import thread\n\n\ndef _make_mocks():\n    return (mock.MagicMock() for _ in range(4))\n\n\ndef _initialize_a_session_thread(session=None, job_queue=None,\n                                 response_queue=None, exception_queue=None):\n    if job_queue is None:\n        job_queue = queue.Queue()\n    with mock.patch.object(threading, 'Thread') as Thread:\n        thread_instance = mock.MagicMock()\n        Thread.return_value = thread_instance\n        st = thread.SessionThread(\n            initialized_session=session,\n            job_queue=job_queue,\n            response_queue=response_queue,\n            exception_queue=exception_queue,\n            )\n\n    return (st, thread_instance, Thread)\n\n\nclass TestSessionThread(unittest.TestCase):\n\n    \"\"\"Tests for requests_toolbelt.threaded.thread.SessionThread.\"\"\"\n\n    def test_thread_initialization(self):\n        \"\"\"Test the way a SessionThread is initialized.\n\n        We want to ensure that we creat a thread with a name generated by the\n        uuid module, and that we pass the right method to use as a target.\n        \"\"\"\n        with mock.patch.object(uuid, 'uuid4', return_value='test'):\n            (st, thread_instance, Thread) = _initialize_a_session_thread()\n\n        Thread.assert_called_once_with(target=st._make_request, name='test')\n        assert thread_instance.daemon is True\n        assert thread_instance._state is 0\n        thread_instance.start.assert_called_once_with()\n\n    def test_is_alive_proxies_to_worker(self):\n        \"\"\"Test that we proxy the is_alive method to the Thread.\"\"\"\n        job_queue = queue.Queue()\n        with mock.patch.object(threading, 'Thread') as Thread:\n            thread_instance = mock.MagicMock()\n            Thread.return_value = thread_instance\n            st = thread.SessionThread(None, job_queue, None, None)\n\n        st.is_alive()\n        thread_instance.is_alive.assert_called_once_with()\n\n    def test_join_proxies_to_worker(self):\n        \"\"\"Test that we proxy the join method to the Thread.\"\"\"\n        st, thread_instance, _ = _initialize_a_session_thread()\n\n        st.join()\n        thread_instance.join.assert_called_once_with()\n\n    def test_handle_valid_request(self):\n        \"\"\"Test that a response is added to the right queue.\"\"\"\n        session, job_queue, response_queue, exception_queue = _make_mocks()\n        response = mock.MagicMock()\n        session.request.return_value = response\n\n        st, _, _ = _initialize_a_session_thread(\n            session, job_queue, response_queue, exception_queue)\n\n        st._handle_request({'method': 'GET', 'url': 'http://example.com'})\n        session.request.assert_called_once_with(\n            method='GET',\n            url='http://example.com'\n        )\n\n        response_queue.put.assert_called_once_with(\n            ({'method': 'GET', 'url': 'http://example.com'}, response)\n        )\n        assert exception_queue.put.called is False\n        assert job_queue.get.called is False\n        assert job_queue.get_nowait.called is False\n        assert job_queue.get_nowait.called is False\n        assert job_queue.task_done.called is True\n\n    def test_handle_invalid_request(self):\n        \"\"\"Test that exceptions from requests are added to the right queue.\"\"\"\n        session, job_queue, response_queue, exception_queue = _make_mocks()\n        exception = requests.exceptions.InvalidURL()\n\n        def _side_effect(*args, **kwargs):\n            raise exception\n\n        # Make the request raise an exception\n        session.request.side_effect = _side_effect\n\n        st, _, _ = _initialize_a_session_thread(\n            session, job_queue, response_queue, exception_queue)\n\n        st._handle_request({'method': 'GET', 'url': 'http://example.com'})\n        session.request.assert_called_once_with(\n            method='GET',\n            url='http://example.com'\n        )\n\n        exception_queue.put.assert_called_once_with(\n            ({'method': 'GET', 'url': 'http://example.com'}, exception)\n        )\n        assert response_queue.put.called is False\n        assert job_queue.get.called is False\n        assert job_queue.get_nowait.called is False\n        assert job_queue.get_nowait.called is False\n        assert job_queue.task_done.called is True\n\n    def test_make_request(self):\n        \"\"\"Test that _make_request exits when the queue is Empty.\"\"\"\n        job_queue = next(_make_mocks())\n        job_queue.get_nowait.side_effect = queue.Empty()\n\n        st, _, _ = _initialize_a_session_thread(job_queue=job_queue)\n        st._make_request()\n\n        job_queue.get_nowait.assert_called_once_with()\n", "examples/threading/threaded_simplified.py": "from requests_toolbelt import threaded\n\nrequests = [{\n    'method': 'GET',\n    'url': 'https://httpbin.org/get',\n    'params': {'foo': 'bar'}\n}, {\n    'method': 'POST',\n    'url': 'https://httpbin.org/post',\n    'json': {'foo': 'bar'}\n}, {\n    'method': 'POST',\n    'url': 'https://httpbin.org/post',\n    'data': {'foo': 'bar'}\n}, {\n    'method': 'PUT',\n    'url': 'https://httpbin.org/put',\n    'files': {'foo': ('', 'bar')}\n}, {\n    'method': 'GET',\n    'url': 'https://httpbin.org/stream/100',\n    'stream': True\n}, {\n    'method': 'GET',\n    'url': 'https://httpbin.org/delay/10',\n    'timeout': 2.0\n}]\n\nurl = 'https://httpbin.org/get'\nrequests.extend([\n    {'method': 'GET', 'url': url, 'params': {'i': str(i)}}\n    for i in range(30)\n])\n\nresponses, exceptions = threaded.map(requests)\n", "examples/threading/threaded.py": "try:\n    import Queue as queue\nexcept ImportError:\n    import queue\n\nfrom requests_toolbelt.threaded import pool\n\nq = queue.Queue()\nq.put({\n    'method': 'GET',\n    'url': 'https://httpbin.org/get',\n    'params': {'foo': 'bar'}\n})\nq.put({\n    'method': 'POST',\n    'url': 'https://httpbin.org/post',\n    'json': {'foo': 'bar'}\n})\nq.put({\n    'method': 'POST',\n    'url': 'https://httpbin.org/post',\n    'data': {'foo': 'bar'}\n})\nq.put({\n    'method': 'PUT',\n    'url': 'https://httpbin.org/put',\n    'files': {'foo': ('', 'bar')}\n})\nq.put({\n    'method': 'GET',\n    'url': 'https://httpbin.org/stream/100',\n    'stream': True\n})\nq.put({\n    'method': 'GET',\n    'url': 'https://httpbin.org/delay/10',\n    'timeout': 5.0\n})\n\nfor i in range(30):\n    q.put({\n        'method': 'GET',\n        'url': 'https://httpbin.org/get',\n        'params': {'i': str(i)},\n    })\n\np = pool.Pool(q)\np.join_all()\n\nresponses = list(p.responses())\nexceptions = list(p.exceptions())\n", "examples/monitor/progress_bar.py": "# -*- coding: utf-8 -*-\n\n# ############################################################################\n# This example demonstrates how to use the MultipartEncoderMonitor to create a\n# progress bar using clint.\n# ############################################################################\n\nfrom clint.textui.progress import Bar as ProgressBar\nfrom requests_toolbelt import MultipartEncoder, MultipartEncoderMonitor\n\nimport requests\n\n\ndef create_callback(encoder):\n    encoder_len = encoder.len\n    bar = ProgressBar(expected_size=encoder_len, filled_char='=')\n\n    def callback(monitor):\n        bar.show(monitor.bytes_read)\n\n    return callback\n\n\ndef create_upload():\n    return MultipartEncoder({\n        'form_field': 'value',\n        'another_form_field': 'another value',\n        'first_file': ('progress_bar.py', open(__file__, 'rb'), 'text/plain'),\n        'second_file': ('progress_bar.py', open(__file__, 'rb'),\n                        'text/plain'),\n        })\n\n\nif __name__ == '__main__':\n    encoder = create_upload()\n    callback = create_callback(encoder)\n    monitor = MultipartEncoderMonitor(encoder, callback)\n    r = requests.post('https://httpbin.org/post', data=monitor,\n                      headers={'Content-Type': monitor.content_type})\n    print('\\nUpload finished! (Returned status {} {})'.format(\n        r.status_code, r.reason\n        ))\n", "requests_toolbelt/exceptions.py": "# -*- coding: utf-8 -*-\n\"\"\"Collection of exceptions raised by requests-toolbelt.\"\"\"\n\n\nclass StreamingError(Exception):\n    \"\"\"Used in :mod:`requests_toolbelt.downloadutils.stream`.\"\"\"\n    pass\n\n\nclass VersionMismatchError(Exception):\n    \"\"\"Used to indicate a version mismatch in the version of requests required.\n\n    The feature in use requires a newer version of Requests to function\n    appropriately but the version installed is not sufficient.\n    \"\"\"\n    pass\n\n\nclass RequestsVersionTooOld(Warning):\n    \"\"\"Used to indicate that the Requests version is too old.\n\n    If the version of Requests is too old to support a feature, we will issue\n    this warning to the user.\n    \"\"\"\n    pass\n", "requests_toolbelt/_compat.py": "\"\"\"Private module full of compatibility hacks.\n\nPrimarily this is for downstream redistributions of requests that unvendor\nurllib3 without providing a shim.\n\n.. warning::\n\n    This module is private. If you use it, and something breaks, you were\n    warned\n\"\"\"\nimport sys\n\nimport requests\n\ntry:\n    from requests.packages.urllib3 import fields\n    from requests.packages.urllib3 import filepost\n    from requests.packages.urllib3 import poolmanager\nexcept ImportError:\n    from urllib3 import fields\n    from urllib3 import filepost\n    from urllib3 import poolmanager\n\ntry:\n    from requests.packages.urllib3.connection import HTTPConnection\n    from requests.packages.urllib3 import connection\nexcept ImportError:\n    try:\n        from urllib3.connection import HTTPConnection\n        from urllib3 import connection\n    except ImportError:\n        HTTPConnection = None\n        connection = None\n\n\nif requests.__build__ < 0x020300:\n    timeout = None\nelse:\n    try:\n        from requests.packages.urllib3.util import timeout\n    except ImportError:\n        from urllib3.util import timeout\n\nPY3 = sys.version_info > (3, 0)\n\nif PY3:\n    from collections.abc import Mapping, MutableMapping\n    import queue\n    from urllib.parse import urlencode, urljoin\nelse:\n    from collections import Mapping, MutableMapping\n    import Queue as queue\n    from urllib import urlencode\n    from urlparse import urljoin\n\ntry:\n    basestring = basestring\nexcept NameError:\n    basestring = (str, bytes)\n\n\nclass HTTPHeaderDict(MutableMapping):\n    \"\"\"\n    :param headers:\n        An iterable of field-value pairs. Must not contain multiple field names\n        when compared case-insensitively.\n\n    :param kwargs:\n        Additional field-value pairs to pass in to ``dict.update``.\n\n    A ``dict`` like container for storing HTTP Headers.\n\n    Field names are stored and compared case-insensitively in compliance with\n    RFC 7230. Iteration provides the first case-sensitive key seen for each\n    case-insensitive pair.\n\n    Using ``__setitem__`` syntax overwrites fields that compare equal\n    case-insensitively in order to maintain ``dict``'s api. For fields that\n    compare equal, instead create a new ``HTTPHeaderDict`` and use ``.add``\n    in a loop.\n\n    If multiple fields that are equal case-insensitively are passed to the\n    constructor or ``.update``, the behavior is undefined and some will be\n    lost.\n\n    >>> headers = HTTPHeaderDict()\n    >>> headers.add('Set-Cookie', 'foo=bar')\n    >>> headers.add('set-cookie', 'baz=quxx')\n    >>> headers['content-length'] = '7'\n    >>> headers['SET-cookie']\n    'foo=bar, baz=quxx'\n    >>> headers['Content-Length']\n    '7'\n    \"\"\"\n\n    def __init__(self, headers=None, **kwargs):\n        super(HTTPHeaderDict, self).__init__()\n        self._container = {}\n        if headers is not None:\n            if isinstance(headers, HTTPHeaderDict):\n                self._copy_from(headers)\n            else:\n                self.extend(headers)\n        if kwargs:\n            self.extend(kwargs)\n\n    def __setitem__(self, key, val):\n        self._container[key.lower()] = (key, val)\n        return self._container[key.lower()]\n\n    def __getitem__(self, key):\n        val = self._container[key.lower()]\n        return ', '.join(val[1:])\n\n    def __delitem__(self, key):\n        del self._container[key.lower()]\n\n    def __contains__(self, key):\n        return key.lower() in self._container\n\n    def __eq__(self, other):\n        if not isinstance(other, Mapping) and not hasattr(other, 'keys'):\n            return False\n        if not isinstance(other, type(self)):\n            other = type(self)(other)\n        return ({k.lower(): v for k, v in self.itermerged()} ==\n                {k.lower(): v for k, v in other.itermerged()})\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    if not PY3:  # Python 2\n        iterkeys = MutableMapping.iterkeys\n        itervalues = MutableMapping.itervalues\n\n    __marker = object()\n\n    def __len__(self):\n        return len(self._container)\n\n    def __iter__(self):\n        # Only provide the originally cased names\n        for vals in self._container.values():\n            yield vals[0]\n\n    def pop(self, key, default=__marker):\n        \"\"\"D.pop(k[,d]) -> v, remove specified key and return its value.\n\n        If key is not found, d is returned if given, otherwise KeyError is\n        raised.\n        \"\"\"\n        # Using the MutableMapping function directly fails due to the private\n        # marker.\n        # Using ordinary dict.pop would expose the internal structures.\n        # So let's reinvent the wheel.\n        try:\n            value = self[key]\n        except KeyError:\n            if default is self.__marker:\n                raise\n            return default\n        else:\n            del self[key]\n            return value\n\n    def discard(self, key):\n        try:\n            del self[key]\n        except KeyError:\n            pass\n\n    def add(self, key, val):\n        \"\"\"Adds a (name, value) pair, doesn't overwrite the value if it already\n        exists.\n\n        >>> headers = HTTPHeaderDict(foo='bar')\n        >>> headers.add('Foo', 'baz')\n        >>> headers['foo']\n        'bar, baz'\n        \"\"\"\n        key_lower = key.lower()\n        new_vals = key, val\n        # Keep the common case aka no item present as fast as possible\n        vals = self._container.setdefault(key_lower, new_vals)\n        if new_vals is not vals:\n            # new_vals was not inserted, as there was a previous one\n            if isinstance(vals, list):\n                # If already several items got inserted, we have a list\n                vals.append(val)\n            else:\n                # vals should be a tuple then, i.e. only one item so far\n                # Need to convert the tuple to list for further extension\n                self._container[key_lower] = [vals[0], vals[1], val]\n\n    def extend(self, *args, **kwargs):\n        \"\"\"Generic import function for any type of header-like object.\n        Adapted version of MutableMapping.update in order to insert items\n        with self.add instead of self.__setitem__\n        \"\"\"\n        if len(args) > 1:\n            raise TypeError(\"extend() takes at most 1 positional \"\n                            \"arguments ({} given)\".format(len(args)))\n        other = args[0] if len(args) >= 1 else ()\n\n        if isinstance(other, HTTPHeaderDict):\n            for key, val in other.iteritems():\n                self.add(key, val)\n        elif isinstance(other, Mapping):\n            for key in other:\n                self.add(key, other[key])\n        elif hasattr(other, \"keys\"):\n            for key in other.keys():\n                self.add(key, other[key])\n        else:\n            for key, value in other:\n                self.add(key, value)\n\n        for key, value in kwargs.items():\n            self.add(key, value)\n\n    def getlist(self, key):\n        \"\"\"Returns a list of all the values for the named field. Returns an\n        empty list if the key doesn't exist.\"\"\"\n        try:\n            vals = self._container[key.lower()]\n        except KeyError:\n            return []\n        else:\n            if isinstance(vals, tuple):\n                return [vals[1]]\n            else:\n                return vals[1:]\n\n    # Backwards compatibility for httplib\n    getheaders = getlist\n    getallmatchingheaders = getlist\n    iget = getlist\n\n    def __repr__(self):\n        return \"%s(%s)\" % (type(self).__name__, dict(self.itermerged()))\n\n    def _copy_from(self, other):\n        for key in other:\n            val = other.getlist(key)\n            if isinstance(val, list):\n                # Don't need to convert tuples\n                val = list(val)\n            self._container[key.lower()] = [key] + val\n\n    def copy(self):\n        clone = type(self)()\n        clone._copy_from(self)\n        return clone\n\n    def iteritems(self):\n        \"\"\"Iterate over all header lines, including duplicate ones.\"\"\"\n        for key in self:\n            vals = self._container[key.lower()]\n            for val in vals[1:]:\n                yield vals[0], val\n\n    def itermerged(self):\n        \"\"\"Iterate over all headers, merging duplicate ones together.\"\"\"\n        for key in self:\n            val = self._container[key.lower()]\n            yield val[0], ', '.join(val[1:])\n\n    def items(self):\n        return list(self.iteritems())\n\n    @classmethod\n    def from_httplib(cls, message):  # Python 2\n        \"\"\"Read headers from a Python 2 httplib message object.\"\"\"\n        # python2.7 does not expose a proper API for exporting multiheaders\n        # efficiently. This function re-reads raw lines from the message\n        # object and extracts the multiheaders properly.\n        headers = []\n\n        for line in message.headers:\n            if line.startswith((' ', '\\t')):\n                key, value = headers[-1]\n                headers[-1] = (key, value + '\\r\\n' + line.rstrip())\n                continue\n\n            key, value = line.split(':', 1)\n            headers.append((key, value.strip()))\n\n        return cls(headers)\n\n\n__all__ = (\n    'basestring',\n    'connection',\n    'fields',\n    'filepost',\n    'poolmanager',\n    'timeout',\n    'HTTPHeaderDict',\n    'queue',\n    'urlencode',\n    'urljoin',\n)\n", "requests_toolbelt/__init__.py": "# -*- coding: utf-8 -*-\n\"\"\"\nrequests-toolbelt\n=================\n\nSee https://toolbelt.readthedocs.io/ for documentation\n\n:copyright: (c) 2014 by Ian Cordasco and Cory Benfield\n:license: Apache v2.0, see LICENSE for more details\n\"\"\"\n\nfrom .adapters import SSLAdapter, SourceAddressAdapter\nfrom .auth.guess import GuessAuth\nfrom .multipart import (\n    MultipartEncoder, MultipartEncoderMonitor, MultipartDecoder,\n    ImproperBodyPartContentException, NonMultipartContentTypeException\n    )\nfrom .streaming_iterator import StreamingIterator\nfrom .utils.user_agent import user_agent\n\n__title__ = 'requests-toolbelt'\n__authors__ = 'Ian Cordasco, Cory Benfield'\n__license__ = 'Apache v2.0'\n__copyright__ = 'Copyright 2014 Ian Cordasco, Cory Benfield'\n__version__ = '1.0.0'\n__version_info__ = tuple(int(i) for i in __version__.split('.'))\n\n__all__ = [\n    'GuessAuth', 'MultipartEncoder', 'MultipartEncoderMonitor',\n    'MultipartDecoder', 'SSLAdapter', 'SourceAddressAdapter',\n    'StreamingIterator', 'user_agent', 'ImproperBodyPartContentException',\n    'NonMultipartContentTypeException', '__title__', '__authors__',\n    '__license__', '__copyright__', '__version__', '__version_info__',\n]\n", "requests_toolbelt/sessions.py": "import requests\n\nfrom ._compat import urljoin\n\n\nclass BaseUrlSession(requests.Session):\n    \"\"\"A Session with a URL that all requests will use as a base.\n\n    Let's start by looking at a few examples:\n\n    .. code-block:: python\n\n        >>> from requests_toolbelt import sessions\n        >>> s = sessions.BaseUrlSession(\n        ...     base_url='https://example.com/resource/')\n        >>> r = s.get('sub-resource/', params={'foo': 'bar'})\n        >>> print(r.request.url)\n        https://example.com/resource/sub-resource/?foo=bar\n\n    Our call to the ``get`` method will make a request to the URL passed in\n    when we created the Session and the partial resource name we provide.\n    We implement this by overriding the ``request`` method of the Session.\n\n    Likewise, we override the ``prepare_request`` method so you can construct\n    a PreparedRequest in the same way:\n\n    .. code-block:: python\n\n        >>> from requests import Request\n        >>> from requests_toolbelt import sessions\n        >>> s = sessions.BaseUrlSession(\n        ...     base_url='https://example.com/resource/')\n        >>> request = Request(method='GET', url='sub-resource/')\n        >>> prepared_request = s.prepare_request(request)\n        >>> r = s.send(prepared_request)\n        >>> print(r.request.url)\n        https://example.com/resource/sub-resource\n\n    .. note::\n\n        The base URL that you provide and the path you provide are **very**\n        important.\n\n    Let's look at another *similar* example\n\n    .. code-block:: python\n\n        >>> from requests_toolbelt import sessions\n        >>> s = sessions.BaseUrlSession(\n        ...     base_url='https://example.com/resource/')\n        >>> r = s.get('/sub-resource/', params={'foo': 'bar'})\n        >>> print(r.request.url)\n        https://example.com/sub-resource/?foo=bar\n\n    The key difference here is that we called ``get`` with ``/sub-resource/``,\n    i.e., there was a leading ``/``. This changes how we create the URL\n    because we rely on :mod:`urllib.parse.urljoin`.\n\n    To override how we generate the URL, sub-class this method and override the\n    ``create_url`` method.\n\n    Based on implementation from\n    https://github.com/kennethreitz/requests/issues/2554#issuecomment-109341010\n    \"\"\"\n\n    base_url = None\n\n    def __init__(self, base_url=None):\n        if base_url:\n            self.base_url = base_url\n        super(BaseUrlSession, self).__init__()\n\n    def request(self, method, url, *args, **kwargs):\n        \"\"\"Send the request after generating the complete URL.\"\"\"\n        url = self.create_url(url)\n        return super(BaseUrlSession, self).request(\n            method, url, *args, **kwargs\n        )\n\n    def prepare_request(self, request, *args, **kwargs):\n        \"\"\"Prepare the request after generating the complete URL.\"\"\"\n        request.url = self.create_url(request.url)\n        return super(BaseUrlSession, self).prepare_request(\n            request, *args, **kwargs\n        )\n\n    def create_url(self, url):\n        \"\"\"Create the URL based off this partial path.\"\"\"\n        return urljoin(self.base_url, url)\n\n    def __getstate__(self):\n        \"\"\"Save base URL as well during the pickle\"\"\"\n        states = super(BaseUrlSession, self).__getstate__()\n        states.update({\"base_url\": self.base_url})\n        return states\n\n    def __setstate__(self, state):\n        \"\"\"Load base URL as well during the unpickle\"\"\"\n        super(BaseUrlSession, self).__setstate__(state)\n        if \"base_url\" in state:\n            self.base_url = state[\"base_url\"]\n", "requests_toolbelt/streaming_iterator.py": "# -*- coding: utf-8 -*-\n\"\"\"\n\nrequests_toolbelt.streaming_iterator\n====================================\n\nThis holds the implementation details for the :class:`StreamingIterator`. It\nis designed for the case where you, the user, know the size of the upload but\nneed to provide the data as an iterator. This class will allow you to specify\nthe size and stream the data without using a chunked transfer-encoding.\n\n\"\"\"\nfrom requests.utils import super_len\n\nfrom .multipart.encoder import CustomBytesIO, encode_with\n\n\nclass StreamingIterator(object):\n\n    \"\"\"\n    This class provides a way of allowing iterators with a known size to be\n    streamed instead of chunked.\n\n    In requests, if you pass in an iterator it assumes you want to use\n    chunked transfer-encoding to upload the data, which not all servers\n    support well. Additionally, you may want to set the content-length\n    yourself to avoid this but that will not work. The only way to preempt\n    requests using a chunked transfer-encoding and forcing it to stream the\n    uploads is to mimic a very specific interace. Instead of having to know\n    these details you can instead just use this class. You simply provide the\n    size and iterator and pass the instance of StreamingIterator to requests\n    via the data parameter like so:\n\n    .. code-block:: python\n\n        from requests_toolbelt import StreamingIterator\n\n        import requests\n\n        # Let iterator be some generator that you already have and size be\n        # the size of the data produced by the iterator\n\n        r = requests.post(url, data=StreamingIterator(size, iterator))\n\n    You can also pass file-like objects to :py:class:`StreamingIterator` in\n    case requests can't determize the filesize itself. This is the case with\n    streaming file objects like ``stdin`` or any sockets. Wrapping e.g. files\n    that are on disk with ``StreamingIterator`` is unnecessary, because\n    requests can determine the filesize itself.\n\n    Naturally, you should also set the `Content-Type` of your upload\n    appropriately because the toolbelt will not attempt to guess that for you.\n    \"\"\"\n\n    def __init__(self, size, iterator, encoding='utf-8'):\n        #: The expected size of the upload\n        self.size = int(size)\n\n        if self.size < 0:\n            raise ValueError(\n                'The size of the upload must be a positive integer'\n                )\n\n        #: Attribute that requests will check to determine the length of the\n        #: body. See bug #80 for more details\n        self.len = self.size\n\n        #: Encoding the input data is using\n        self.encoding = encoding\n\n        #: The iterator used to generate the upload data\n        self.iterator = iterator\n\n        if hasattr(iterator, 'read'):\n            self._file = iterator\n        else:\n            self._file = _IteratorAsBinaryFile(iterator, encoding)\n\n    def read(self, size=-1):\n        return encode_with(self._file.read(size), self.encoding)\n\n\nclass _IteratorAsBinaryFile(object):\n    def __init__(self, iterator, encoding='utf-8'):\n        #: The iterator used to generate the upload data\n        self.iterator = iterator\n\n        #: Encoding the iterator is using\n        self.encoding = encoding\n\n        # The buffer we use to provide the correct number of bytes requested\n        # during a read\n        self._buffer = CustomBytesIO()\n\n    def _get_bytes(self):\n        try:\n            return encode_with(next(self.iterator), self.encoding)\n        except StopIteration:\n            return b''\n\n    def _load_bytes(self, size):\n        self._buffer.smart_truncate()\n        amount_to_load = size - super_len(self._buffer)\n        bytes_to_append = True\n\n        while amount_to_load > 0 and bytes_to_append:\n            bytes_to_append = self._get_bytes()\n            amount_to_load -= self._buffer.append(bytes_to_append)\n\n    def read(self, size=-1):\n        size = int(size)\n        if size == -1:\n            return b''.join(self.iterator)\n\n        self._load_bytes(size)\n        return self._buffer.read(size)\n", "requests_toolbelt/utils/formdata.py": "# -*- coding: utf-8 -*-\n\"\"\"Implementation of nested form-data encoding function(s).\"\"\"\nfrom .._compat import basestring\nfrom .._compat import urlencode as _urlencode\n\n\n__all__ = ('urlencode',)\n\n\ndef urlencode(query, *args, **kwargs):\n    \"\"\"Handle nested form-data queries and serialize them appropriately.\n\n    There are times when a website expects a nested form data query to be sent\n    but, the standard library's urlencode function does not appropriately\n    handle the nested structures. In that case, you need this function which\n    will flatten the structure first and then properly encode it for you.\n\n    When using this to send data in the body of a request, make sure you\n    specify the appropriate Content-Type header for the request.\n\n    .. code-block:: python\n\n        import requests\n        from requests_toolbelt.utils import formdata\n\n        query = {\n           'my_dict': {\n               'foo': 'bar',\n               'biz': 'baz\",\n            },\n            'a': 'b',\n        }\n\n        resp = requests.get(url, params=formdata.urlencode(query))\n        # or\n        resp = requests.post(\n            url,\n            data=formdata.urlencode(query),\n            headers={\n                'Content-Type': 'application/x-www-form-urlencoded'\n            },\n        )\n\n    Similarly, you can specify a list of nested tuples, e.g.,\n\n    .. code-block:: python\n\n        import requests\n        from requests_toolbelt.utils import formdata\n\n        query = [\n            ('my_list', [\n                ('foo', 'bar'),\n                ('biz', 'baz'),\n            ]),\n            ('a', 'b'),\n        ]\n\n        resp = requests.get(url, params=formdata.urlencode(query))\n        # or\n        resp = requests.post(\n            url,\n            data=formdata.urlencode(query),\n            headers={\n                'Content-Type': 'application/x-www-form-urlencoded'\n            },\n        )\n\n    For additional parameter and return information, see the official\n    `urlencode`_ documentation.\n\n    .. _urlencode:\n        https://docs.python.org/3/library/urllib.parse.html#urllib.parse.urlencode\n    \"\"\"\n    expand_classes = (dict, list, tuple)\n    original_query_list = _to_kv_list(query)\n\n    if not all(_is_two_tuple(i) for i in original_query_list):\n        raise ValueError(\"Expected query to be able to be converted to a \"\n                         \"list comprised of length 2 tuples.\")\n\n    query_list = original_query_list\n    while any(isinstance(v, expand_classes) for _, v in query_list):\n        query_list = _expand_query_values(query_list)\n\n    return _urlencode(query_list, *args, **kwargs)\n\n\ndef _to_kv_list(dict_or_list):\n    if hasattr(dict_or_list, 'items'):\n        return list(dict_or_list.items())\n    return dict_or_list\n\n\ndef _is_two_tuple(item):\n    return isinstance(item, (list, tuple)) and len(item) == 2\n\n\ndef _expand_query_values(original_query_list):\n    query_list = []\n    for key, value in original_query_list:\n        if isinstance(value, basestring):\n            query_list.append((key, value))\n        else:\n            key_fmt = key + '[%s]'\n            value_list = _to_kv_list(value)\n            query_list.extend((key_fmt % k, v) for k, v in value_list)\n    return query_list\n", "requests_toolbelt/utils/dump.py": "\"\"\"This module provides functions for dumping information about responses.\"\"\"\nimport collections\n\nfrom requests import compat\n\n\n__all__ = ('dump_response', 'dump_all')\n\nHTTP_VERSIONS = {\n    9: b'0.9',\n    10: b'1.0',\n    11: b'1.1',\n}\n\n_PrefixSettings = collections.namedtuple('PrefixSettings',\n                                         ['request', 'response'])\n\n\nclass PrefixSettings(_PrefixSettings):\n    def __new__(cls, request, response):\n        request = _coerce_to_bytes(request)\n        response = _coerce_to_bytes(response)\n        return super(PrefixSettings, cls).__new__(cls, request, response)\n\n\ndef _get_proxy_information(response):\n    if getattr(response.connection, 'proxy_manager', False):\n        proxy_info = {}\n        request_url = response.request.url\n        if request_url.startswith('https://'):\n            proxy_info['method'] = 'CONNECT'\n\n        proxy_info['request_path'] = request_url\n        return proxy_info\n    return None\n\n\ndef _format_header(name, value):\n    return (_coerce_to_bytes(name) + b': ' + _coerce_to_bytes(value) +\n            b'\\r\\n')\n\n\ndef _build_request_path(url, proxy_info):\n    uri = compat.urlparse(url)\n    proxy_url = proxy_info.get('request_path')\n    if proxy_url is not None:\n        request_path = _coerce_to_bytes(proxy_url)\n        return request_path, uri\n\n    request_path = _coerce_to_bytes(uri.path)\n    if uri.query:\n        request_path += b'?' + _coerce_to_bytes(uri.query)\n\n    return request_path, uri\n\n\ndef _dump_request_data(request, prefixes, bytearr, proxy_info=None):\n    if proxy_info is None:\n        proxy_info = {}\n\n    prefix = prefixes.request\n    method = _coerce_to_bytes(proxy_info.pop('method', request.method))\n    request_path, uri = _build_request_path(request.url, proxy_info)\n\n    # <prefix><METHOD> <request-path> HTTP/1.1\n    bytearr.extend(prefix + method + b' ' + request_path + b' HTTP/1.1\\r\\n')\n\n    # <prefix>Host: <request-host> OR host header specified by user\n    headers = request.headers.copy()\n    host_header = _coerce_to_bytes(headers.pop('Host', uri.netloc))\n    bytearr.extend(prefix + b'Host: ' + host_header + b'\\r\\n')\n\n    for name, value in headers.items():\n        bytearr.extend(prefix + _format_header(name, value))\n\n    bytearr.extend(prefix + b'\\r\\n')\n    if request.body:\n        if isinstance(request.body, compat.basestring):\n            bytearr.extend(prefix + _coerce_to_bytes(request.body))\n        else:\n            # In the event that the body is a file-like object, let's not try\n            # to read everything into memory.\n            bytearr.extend(b'<< Request body is not a string-like type >>')\n        bytearr.extend(b'\\r\\n')\n    bytearr.extend(b'\\r\\n')\n\n\ndef _dump_response_data(response, prefixes, bytearr):\n    prefix = prefixes.response\n    # Let's interact almost entirely with urllib3's response\n    raw = response.raw\n\n    # Let's convert the version int from httplib to bytes\n    version_str = HTTP_VERSIONS.get(raw.version, b'?')\n\n    # <prefix>HTTP/<version_str> <status_code> <reason>\n    bytearr.extend(prefix + b'HTTP/' + version_str + b' ' +\n                   str(raw.status).encode('ascii') + b' ' +\n                   _coerce_to_bytes(response.reason) + b'\\r\\n')\n\n    headers = raw.headers\n    for name in headers.keys():\n        for value in headers.getlist(name):\n            bytearr.extend(prefix + _format_header(name, value))\n\n    bytearr.extend(prefix + b'\\r\\n')\n\n    bytearr.extend(response.content)\n\n\ndef _coerce_to_bytes(data):\n    if not isinstance(data, bytes) and hasattr(data, 'encode'):\n        data = data.encode('utf-8')\n    # Don't bail out with an exception if data is None\n    return data if data is not None else b''\n\n\ndef dump_response(response, request_prefix=b'< ', response_prefix=b'> ',\n                  data_array=None):\n    \"\"\"Dump a single request-response cycle's information.\n\n    This will take a response object and dump only the data that requests can\n    see for that single request-response cycle.\n\n    Example::\n\n        import requests\n        from requests_toolbelt.utils import dump\n\n        resp = requests.get('https://api.github.com/users/sigmavirus24')\n        data = dump.dump_response(resp)\n        print(data.decode('utf-8'))\n\n    :param response:\n        The response to format\n    :type response: :class:`requests.Response`\n    :param request_prefix: (*optional*)\n        Bytes to prefix each line of the request data\n    :type request_prefix: :class:`bytes`\n    :param response_prefix: (*optional*)\n        Bytes to prefix each line of the response data\n    :type response_prefix: :class:`bytes`\n    :param data_array: (*optional*)\n        Bytearray to which we append the request-response cycle data\n    :type data_array: :class:`bytearray`\n    :returns: Formatted bytes of request and response information.\n    :rtype: :class:`bytearray`\n    \"\"\"\n    data = data_array if data_array is not None else bytearray()\n    prefixes = PrefixSettings(request_prefix, response_prefix)\n\n    if not hasattr(response, 'request'):\n        raise ValueError('Response has no associated request')\n\n    proxy_info = _get_proxy_information(response)\n    _dump_request_data(response.request, prefixes, data,\n                       proxy_info=proxy_info)\n    _dump_response_data(response, prefixes, data)\n    return data\n\n\ndef dump_all(response, request_prefix=b'< ', response_prefix=b'> '):\n    \"\"\"Dump all requests and responses including redirects.\n\n    This takes the response returned by requests and will dump all\n    request-response pairs in the redirect history in order followed by the\n    final request-response.\n\n    Example::\n\n        import requests\n        from requests_toolbelt.utils import dump\n\n        resp = requests.get('https://httpbin.org/redirect/5')\n        data = dump.dump_all(resp)\n        print(data.decode('utf-8'))\n\n    :param response:\n        The response to format\n    :type response: :class:`requests.Response`\n    :param request_prefix: (*optional*)\n        Bytes to prefix each line of the request data\n    :type request_prefix: :class:`bytes`\n    :param response_prefix: (*optional*)\n        Bytes to prefix each line of the response data\n    :type response_prefix: :class:`bytes`\n    :returns: Formatted bytes of request and response information.\n    :rtype: :class:`bytearray`\n    \"\"\"\n    data = bytearray()\n\n    history = list(response.history[:])\n    history.append(response)\n\n    for response in history:\n        dump_response(response, request_prefix, response_prefix, data)\n\n    return data\n", "requests_toolbelt/utils/deprecated.py": "# -*- coding: utf-8 -*-\n\"\"\"A collection of functions deprecated in requests.utils.\"\"\"\nimport re\nimport sys\n\nfrom requests import utils\n\nfind_charset = re.compile(\n    br'<meta.*?charset=[\"\\']*(.+?)[\"\\'>]', flags=re.I\n).findall\n\nfind_pragma = re.compile(\n    br'<meta.*?content=[\"\\']*;?charset=(.+?)[\"\\'>]', flags=re.I\n).findall\n\nfind_xml = re.compile(\n    br'^<\\?xml.*?encoding=[\"\\']*(.+?)[\"\\'>]'\n).findall\n\n\ndef get_encodings_from_content(content):\n    \"\"\"Return encodings from given content string.\n\n    .. code-block:: python\n\n        import requests\n        from requests_toolbelt.utils import deprecated\n\n        r = requests.get(url)\n        encodings = deprecated.get_encodings_from_content(r)\n\n    :param content: bytestring to extract encodings from\n    :type content: bytes\n    :return: encodings detected in the provided content\n    :rtype: list(str)\n    \"\"\"\n    encodings = (find_charset(content) + find_pragma(content)\n                 + find_xml(content))\n    if (3, 0) <= sys.version_info < (4, 0):\n        encodings = [encoding.decode('utf8') for encoding in encodings]\n    return encodings\n\n\ndef get_unicode_from_response(response):\n    \"\"\"Return the requested content back in unicode.\n\n    This will first attempt to retrieve the encoding from the response\n    headers. If that fails, it will use\n    :func:`requests_toolbelt.utils.deprecated.get_encodings_from_content`\n    to determine encodings from HTML elements.\n\n    .. code-block:: python\n\n        import requests\n        from requests_toolbelt.utils import deprecated\n\n        r = requests.get(url)\n        text = deprecated.get_unicode_from_response(r)\n\n    :param response: Response object to get unicode content from.\n    :type response: requests.models.Response\n    \"\"\"\n    tried_encodings = set()\n\n    # Try charset from content-type\n    encoding = utils.get_encoding_from_headers(response.headers)\n\n    if encoding:\n        try:\n            return str(response.content, encoding)\n        except UnicodeError:\n            tried_encodings.add(encoding.lower())\n\n    encodings = get_encodings_from_content(response.content)\n\n    for _encoding in encodings:\n        _encoding = _encoding.lower()\n        if _encoding in tried_encodings:\n            continue\n        try:\n            return str(response.content, _encoding)\n        except UnicodeError:\n            tried_encodings.add(_encoding)\n\n    # Fall back:\n    if encoding:\n        try:\n            return str(response.content, encoding, errors='replace')\n        except TypeError:\n            pass\n    return response.text\n", "requests_toolbelt/utils/__init__.py": "", "requests_toolbelt/utils/user_agent.py": "# -*- coding: utf-8 -*-\nimport collections\nimport platform\nimport sys\n\n\ndef user_agent(name, version, extras=None):\n    \"\"\"Return an internet-friendly user_agent string.\n\n    The majority of this code has been wilfully stolen from the equivalent\n    function in Requests.\n\n    :param name: The intended name of the user-agent, e.g. \"python-requests\".\n    :param version: The version of the user-agent, e.g. \"0.0.1\".\n    :param extras: List of two-item tuples that are added to the user-agent\n        string.\n    :returns: Formatted user-agent string\n    :rtype: str\n    \"\"\"\n    if extras is None:\n        extras = []\n\n    return UserAgentBuilder(\n            name, version\n        ).include_extras(\n            extras\n        ).include_implementation(\n        ).include_system().build()\n\n\nclass UserAgentBuilder(object):\n    \"\"\"Class to provide a greater level of control than :func:`user_agent`.\n\n    This is used by :func:`user_agent` to build its User-Agent string.\n\n    .. code-block:: python\n\n        user_agent_str = UserAgentBuilder(\n                name='requests-toolbelt',\n                version='17.4.0',\n            ).include_implementation(\n            ).include_system(\n            ).include_extras([\n                ('requests', '2.14.2'),\n                ('urllib3', '1.21.2'),\n            ]).build()\n\n    \"\"\"\n\n    format_string = '%s/%s'\n\n    def __init__(self, name, version):\n        \"\"\"Initialize our builder with the name and version of our user agent.\n\n        :param str name:\n            Name of our user-agent.\n        :param str version:\n            The version string for user-agent.\n        \"\"\"\n        self._pieces = collections.deque([(name, version)])\n\n    def build(self):\n        \"\"\"Finalize the User-Agent string.\n\n        :returns:\n            Formatted User-Agent string.\n        :rtype:\n            str\n        \"\"\"\n        return \" \".join([self.format_string % piece for piece in self._pieces])\n\n    def include_extras(self, extras):\n        \"\"\"Include extra portions of the User-Agent.\n\n        :param list extras:\n            list of tuples of extra-name and extra-version\n        \"\"\"\n        if any(len(extra) != 2 for extra in extras):\n            raise ValueError('Extras should be a sequence of two item tuples.')\n\n        self._pieces.extend(extras)\n        return self\n\n    def include_implementation(self):\n        \"\"\"Append the implementation string to the user-agent string.\n\n        This adds the the information that you're using CPython 2.7.13 to the\n        User-Agent.\n        \"\"\"\n        self._pieces.append(_implementation_tuple())\n        return self\n\n    def include_system(self):\n        \"\"\"Append the information about the Operating System.\"\"\"\n        self._pieces.append(_platform_tuple())\n        return self\n\n\ndef _implementation_tuple():\n    \"\"\"Return the tuple of interpreter name and version.\n\n    Returns a string that provides both the name and the version of the Python\n    implementation currently running. For example, on CPython 2.7.5 it will\n    return \"CPython/2.7.5\".\n\n    This function works best on CPython and PyPy: in particular, it probably\n    doesn't work for Jython or IronPython. Future investigation should be done\n    to work out the correct shape of the code for those platforms.\n    \"\"\"\n    implementation = platform.python_implementation()\n\n    if implementation == 'CPython':\n        implementation_version = platform.python_version()\n    elif implementation == 'PyPy':\n        implementation_version = '%s.%s.%s' % (sys.pypy_version_info.major,\n                                               sys.pypy_version_info.minor,\n                                               sys.pypy_version_info.micro)\n        if sys.pypy_version_info.releaselevel != 'final':\n            implementation_version = ''.join([\n                implementation_version, sys.pypy_version_info.releaselevel\n                ])\n    elif implementation == 'Jython':\n        implementation_version = platform.python_version()  # Complete Guess\n    elif implementation == 'IronPython':\n        implementation_version = platform.python_version()  # Complete Guess\n    else:\n        implementation_version = 'Unknown'\n\n    return (implementation, implementation_version)\n\n\ndef _implementation_string():\n    return \"%s/%s\" % _implementation_tuple()\n\n\ndef _platform_tuple():\n    try:\n        p_system = platform.system()\n        p_release = platform.release()\n    except IOError:\n        p_system = 'Unknown'\n        p_release = 'Unknown'\n    return (p_system, p_release)\n", "requests_toolbelt/cookies/forgetful.py": "\"\"\"The module containing the code for ForgetfulCookieJar.\"\"\"\nfrom requests.cookies import RequestsCookieJar\n\n\nclass ForgetfulCookieJar(RequestsCookieJar):\n    def set_cookie(self, *args, **kwargs):\n        return\n", "requests_toolbelt/cookies/__init__.py": "", "requests_toolbelt/multipart/decoder.py": "# -*- coding: utf-8 -*-\n\"\"\"\n\nrequests_toolbelt.multipart.decoder\n===================================\n\nThis holds all the implementation details of the MultipartDecoder\n\n\"\"\"\n\nimport sys\nimport email.parser\nfrom .encoder import encode_with\nfrom requests.structures import CaseInsensitiveDict\n\n\ndef _split_on_find(content, bound):\n    point = content.find(bound)\n    return content[:point], content[point + len(bound):]\n\n\nclass ImproperBodyPartContentException(Exception):\n    pass\n\n\nclass NonMultipartContentTypeException(Exception):\n    pass\n\n\ndef _header_parser(string, encoding):\n    major = sys.version_info[0]\n    if major == 3:\n        string = string.decode(encoding)\n    headers = email.parser.HeaderParser().parsestr(string).items()\n    return (\n        (encode_with(k, encoding), encode_with(v, encoding))\n        for k, v in headers\n    )\n\n\nclass BodyPart(object):\n    \"\"\"\n\n    The ``BodyPart`` object is a ``Response``-like interface to an individual\n    subpart of a multipart response. It is expected that these will\n    generally be created by objects of the ``MultipartDecoder`` class.\n\n    Like ``Response``, there is a ``CaseInsensitiveDict`` object named headers,\n    ``content`` to access bytes, ``text`` to access unicode, and ``encoding``\n    to access the unicode codec.\n\n    \"\"\"\n\n    def __init__(self, content, encoding):\n        self.encoding = encoding\n        headers = {}\n        # Split into header section (if any) and the content\n        if b'\\r\\n\\r\\n' in content:\n            first, self.content = _split_on_find(content, b'\\r\\n\\r\\n')\n            if first != b'':\n                headers = _header_parser(first.lstrip(), encoding)\n        else:\n            raise ImproperBodyPartContentException(\n                'content does not contain CR-LF-CR-LF'\n            )\n        self.headers = CaseInsensitiveDict(headers)\n\n    @property\n    def text(self):\n        \"\"\"Content of the ``BodyPart`` in unicode.\"\"\"\n        return self.content.decode(self.encoding)\n\n\nclass MultipartDecoder(object):\n    \"\"\"\n\n    The ``MultipartDecoder`` object parses the multipart payload of\n    a bytestring into a tuple of ``Response``-like ``BodyPart`` objects.\n\n    The basic usage is::\n\n        import requests\n        from requests_toolbelt import MultipartDecoder\n\n        response = requests.get(url)\n        decoder = MultipartDecoder.from_response(response)\n        for part in decoder.parts:\n            print(part.headers['content-type'])\n\n    If the multipart content is not from a response, basic usage is::\n\n        from requests_toolbelt import MultipartDecoder\n\n        decoder = MultipartDecoder(content, content_type)\n        for part in decoder.parts:\n            print(part.headers['content-type'])\n\n    For both these usages, there is an optional ``encoding`` parameter. This is\n    a string, which is the name of the unicode codec to use (default is\n    ``'utf-8'``).\n\n    \"\"\"\n    def __init__(self, content, content_type, encoding='utf-8'):\n        #: Original Content-Type header\n        self.content_type = content_type\n        #: Response body encoding\n        self.encoding = encoding\n        #: Parsed parts of the multipart response body\n        self.parts = tuple()\n        self._find_boundary()\n        self._parse_body(content)\n\n    def _find_boundary(self):\n        ct_info = tuple(x.strip() for x in self.content_type.split(';'))\n        mimetype = ct_info[0]\n        if mimetype.split('/')[0].lower() != 'multipart':\n            raise NonMultipartContentTypeException(\n                \"Unexpected mimetype in content-type: '{}'\".format(mimetype)\n            )\n        for item in ct_info[1:]:\n            attr, value = _split_on_find(\n                item,\n                '='\n            )\n            if attr.lower() == 'boundary':\n                self.boundary = encode_with(value.strip('\"'), self.encoding)\n\n    @staticmethod\n    def _fix_first_part(part, boundary_marker):\n        bm_len = len(boundary_marker)\n        if boundary_marker == part[:bm_len]:\n            return part[bm_len:]\n        else:\n            return part\n\n    def _parse_body(self, content):\n        boundary = b''.join((b'--', self.boundary))\n\n        def body_part(part):\n            fixed = MultipartDecoder._fix_first_part(part, boundary)\n            return BodyPart(fixed, self.encoding)\n\n        def test_part(part):\n            return (part != b'' and\n                    part != b'\\r\\n' and\n                    part[:4] != b'--\\r\\n' and\n                    part != b'--')\n\n        parts = content.split(b''.join((b'\\r\\n', boundary)))\n        self.parts = tuple(body_part(x) for x in parts if test_part(x))\n\n    @classmethod\n    def from_response(cls, response, encoding='utf-8'):\n        content = response.content\n        content_type = response.headers.get('content-type', None)\n        return cls(content, content_type, encoding)\n", "requests_toolbelt/multipart/__init__.py": "\"\"\"\nrequests_toolbelt.multipart\n===========================\n\nSee https://toolbelt.readthedocs.io/ for documentation\n\n:copyright: (c) 2014 by Ian Cordasco and Cory Benfield\n:license: Apache v2.0, see LICENSE for more details\n\"\"\"\n\nfrom .encoder import MultipartEncoder, MultipartEncoderMonitor\nfrom .decoder import MultipartDecoder\nfrom .decoder import ImproperBodyPartContentException\nfrom .decoder import NonMultipartContentTypeException\n\n__title__ = 'requests-toolbelt'\n__authors__ = 'Ian Cordasco, Cory Benfield'\n__license__ = 'Apache v2.0'\n__copyright__ = 'Copyright 2014 Ian Cordasco, Cory Benfield'\n\n__all__ = [\n    'MultipartEncoder',\n    'MultipartEncoderMonitor',\n    'MultipartDecoder',\n    'ImproperBodyPartContentException',\n    'NonMultipartContentTypeException',\n    '__title__',\n    '__authors__',\n    '__license__',\n    '__copyright__',\n]\n", "requests_toolbelt/multipart/encoder.py": "# -*- coding: utf-8 -*-\n\"\"\"\n\nrequests_toolbelt.multipart.encoder\n===================================\n\nThis holds all of the implementation details of the MultipartEncoder\n\n\"\"\"\nimport contextlib\nimport io\nimport os\nfrom uuid import uuid4\n\nimport requests\n\nfrom .._compat import fields\n\n\nclass FileNotSupportedError(Exception):\n    \"\"\"File not supported error.\"\"\"\n\n\nclass MultipartEncoder(object):\n\n    \"\"\"\n\n    The ``MultipartEncoder`` object is a generic interface to the engine that\n    will create a ``multipart/form-data`` body for you.\n\n    The basic usage is:\n\n    .. code-block:: python\n\n        import requests\n        from requests_toolbelt import MultipartEncoder\n\n        encoder = MultipartEncoder({'field': 'value',\n                                    'other_field': 'other_value'})\n        r = requests.post('https://httpbin.org/post', data=encoder,\n                          headers={'Content-Type': encoder.content_type})\n\n    If you do not need to take advantage of streaming the post body, you can\n    also do:\n\n    .. code-block:: python\n\n        r = requests.post('https://httpbin.org/post',\n                          data=encoder.to_string(),\n                          headers={'Content-Type': encoder.content_type})\n\n    If you want the encoder to use a specific order, you can use an\n    OrderedDict or more simply, a list of tuples:\n\n    .. code-block:: python\n\n        encoder = MultipartEncoder([('field', 'value'),\n                                    ('other_field', 'other_value')])\n\n    .. versionchanged:: 0.4.0\n\n    You can also provide tuples as part values as you would provide them to\n    requests' ``files`` parameter.\n\n    .. code-block:: python\n\n        encoder = MultipartEncoder({\n            'field': ('file_name', b'{\"a\": \"b\"}', 'application/json',\n                      {'X-My-Header': 'my-value'})\n        ])\n\n    .. warning::\n\n        This object will end up directly in :mod:`httplib`. Currently,\n        :mod:`httplib` has a hard-coded read size of **8192 bytes**. This\n        means that it will loop until the file has been read and your upload\n        could take a while. This is **not** a bug in requests. A feature is\n        being considered for this object to allow you, the user, to specify\n        what size should be returned on a read. If you have opinions on this,\n        please weigh in on `this issue`_.\n\n    .. _this issue:\n        https://github.com/requests/toolbelt/issues/75\n\n    \"\"\"\n\n    def __init__(self, fields, boundary=None, encoding='utf-8'):\n        #: Boundary value either passed in by the user or created\n        self.boundary_value = boundary or uuid4().hex\n\n        # Computed boundary\n        self.boundary = '--{}'.format(self.boundary_value)\n\n        #: Encoding of the data being passed in\n        self.encoding = encoding\n\n        # Pre-encoded boundary\n        self._encoded_boundary = b''.join([\n            encode_with(self.boundary, self.encoding),\n            encode_with('\\r\\n', self.encoding)\n            ])\n\n        #: Fields provided by the user\n        self.fields = fields\n\n        #: Whether or not the encoder is finished\n        self.finished = False\n\n        #: Pre-computed parts of the upload\n        self.parts = []\n\n        # Pre-computed parts iterator\n        self._iter_parts = iter([])\n\n        # The part we're currently working with\n        self._current_part = None\n\n        # Cached computation of the body's length\n        self._len = None\n\n        # Our buffer\n        self._buffer = CustomBytesIO(encoding=encoding)\n\n        # Pre-compute each part's headers\n        self._prepare_parts()\n\n        # Load boundary into buffer\n        self._write_boundary()\n\n    @property\n    def len(self):\n        \"\"\"Length of the multipart/form-data body.\n\n        requests will first attempt to get the length of the body by calling\n        ``len(body)`` and then by checking for the ``len`` attribute.\n\n        On 32-bit systems, the ``__len__`` method cannot return anything\n        larger than an integer (in C) can hold. If the total size of the body\n        is even slightly larger than 4GB users will see an OverflowError. This\n        manifested itself in `bug #80`_.\n\n        As such, we now calculate the length lazily as a property.\n\n        .. _bug #80:\n            https://github.com/requests/toolbelt/issues/80\n        \"\"\"\n        # If _len isn't already calculated, calculate, return, and set it\n        return self._len or self._calculate_length()\n\n    def __repr__(self):\n        return '<MultipartEncoder: {!r}>'.format(self.fields)\n\n    def _calculate_length(self):\n        \"\"\"\n        This uses the parts to calculate the length of the body.\n\n        This returns the calculated length so __len__ can be lazy.\n        \"\"\"\n        boundary_len = len(self.boundary)  # Length of --{boundary}\n        # boundary length + header length + body length + len('\\r\\n') * 2\n        self._len = sum(\n            (boundary_len + total_len(p) + 4) for p in self.parts\n            ) + boundary_len + 4\n        return self._len\n\n    def _calculate_load_amount(self, read_size):\n        \"\"\"This calculates how many bytes need to be added to the buffer.\n\n        When a consumer read's ``x`` from the buffer, there are two cases to\n        satisfy:\n\n            1. Enough data in the buffer to return the requested amount\n            2. Not enough data\n\n        This function uses the amount of unread bytes in the buffer and\n        determines how much the Encoder has to load before it can return the\n        requested amount of bytes.\n\n        :param int read_size: the number of bytes the consumer requests\n        :returns: int -- the number of bytes that must be loaded into the\n            buffer before the read can be satisfied. This will be strictly\n            non-negative\n        \"\"\"\n        amount = read_size - total_len(self._buffer)\n        return amount if amount > 0 else 0\n\n    def _load(self, amount):\n        \"\"\"Load ``amount`` number of bytes into the buffer.\"\"\"\n        self._buffer.smart_truncate()\n        part = self._current_part or self._next_part()\n        while amount == -1 or amount > 0:\n            written = 0\n            if part and not part.bytes_left_to_write():\n                written += self._write(b'\\r\\n')\n                written += self._write_boundary()\n                part = self._next_part()\n\n            if not part:\n                written += self._write_closing_boundary()\n                self.finished = True\n                break\n\n            written += part.write_to(self._buffer, amount)\n\n            if amount != -1:\n                amount -= written\n\n    def _next_part(self):\n        try:\n            p = self._current_part = next(self._iter_parts)\n        except StopIteration:\n            p = None\n        return p\n\n    def _iter_fields(self):\n        _fields = self.fields\n        if hasattr(self.fields, 'items'):\n            _fields = list(self.fields.items())\n        for k, v in _fields:\n            file_name = None\n            file_type = None\n            file_headers = None\n            if isinstance(v, (list, tuple)):\n                if len(v) == 2:\n                    file_name, file_pointer = v\n                elif len(v) == 3:\n                    file_name, file_pointer, file_type = v\n                else:\n                    file_name, file_pointer, file_type, file_headers = v\n            else:\n                file_pointer = v\n\n            field = fields.RequestField(name=k, data=file_pointer,\n                                        filename=file_name,\n                                        headers=file_headers)\n            field.make_multipart(content_type=file_type)\n            yield field\n\n    def _prepare_parts(self):\n        \"\"\"This uses the fields provided by the user and creates Part objects.\n\n        It populates the `parts` attribute and uses that to create a\n        generator for iteration.\n        \"\"\"\n        enc = self.encoding\n        self.parts = [Part.from_field(f, enc) for f in self._iter_fields()]\n        self._iter_parts = iter(self.parts)\n\n    def _write(self, bytes_to_write):\n        \"\"\"Write the bytes to the end of the buffer.\n\n        :param bytes bytes_to_write: byte-string (or bytearray) to append to\n            the buffer\n        :returns: int -- the number of bytes written\n        \"\"\"\n        return self._buffer.append(bytes_to_write)\n\n    def _write_boundary(self):\n        \"\"\"Write the boundary to the end of the buffer.\"\"\"\n        return self._write(self._encoded_boundary)\n\n    def _write_closing_boundary(self):\n        \"\"\"Write the bytes necessary to finish a multipart/form-data body.\"\"\"\n        with reset(self._buffer):\n            self._buffer.seek(-2, 2)\n            self._buffer.write(b'--\\r\\n')\n        return 2\n\n    def _write_headers(self, headers):\n        \"\"\"Write the current part's headers to the buffer.\"\"\"\n        return self._write(encode_with(headers, self.encoding))\n\n    @property\n    def content_type(self):\n        return str(\n            'multipart/form-data; boundary={}'.format(self.boundary_value)\n            )\n\n    def to_string(self):\n        \"\"\"Return the entirety of the data in the encoder.\n\n        .. note::\n\n            This simply reads all of the data it can. If you have started\n            streaming or reading data from the encoder, this method will only\n            return whatever data is left in the encoder.\n\n        .. note::\n\n            This method affects the internal state of the encoder. Calling\n            this method will exhaust the encoder.\n\n        :returns: the multipart message\n        :rtype: bytes\n        \"\"\"\n\n        return self.read()\n\n    def read(self, size=-1):\n        \"\"\"Read data from the streaming encoder.\n\n        :param int size: (optional), If provided, ``read`` will return exactly\n            that many bytes. If it is not provided, it will return the\n            remaining bytes.\n        :returns: bytes\n        \"\"\"\n        if self.finished:\n            return self._buffer.read(size)\n\n        bytes_to_load = size\n        if bytes_to_load != -1 and bytes_to_load is not None:\n            bytes_to_load = self._calculate_load_amount(int(size))\n\n        self._load(bytes_to_load)\n        return self._buffer.read(size)\n\n\ndef IDENTITY(monitor):\n    return monitor\n\n\nclass MultipartEncoderMonitor(object):\n\n    \"\"\"\n    An object used to monitor the progress of a :class:`MultipartEncoder`.\n\n    The :class:`MultipartEncoder` should only be responsible for preparing and\n    streaming the data. For anyone who wishes to monitor it, they shouldn't be\n    using that instance to manage that as well. Using this class, they can\n    monitor an encoder and register a callback. The callback receives the\n    instance of the monitor.\n\n    To use this monitor, you construct your :class:`MultipartEncoder` as you\n    normally would.\n\n    .. code-block:: python\n\n        from requests_toolbelt import (MultipartEncoder,\n                                       MultipartEncoderMonitor)\n        import requests\n\n        def callback(monitor):\n            # Do something with this information\n            pass\n\n        m = MultipartEncoder(fields={'field0': 'value0'})\n        monitor = MultipartEncoderMonitor(m, callback)\n        headers = {'Content-Type': monitor.content_type}\n        r = requests.post('https://httpbin.org/post', data=monitor,\n                          headers=headers)\n\n    Alternatively, if your use case is very simple, you can use the following\n    pattern.\n\n    .. code-block:: python\n\n        from requests_toolbelt import MultipartEncoderMonitor\n        import requests\n\n        def callback(monitor):\n            # Do something with this information\n            pass\n\n        monitor = MultipartEncoderMonitor.from_fields(\n            fields={'field0': 'value0'}, callback\n            )\n        headers = {'Content-Type': montior.content_type}\n        r = requests.post('https://httpbin.org/post', data=monitor,\n                          headers=headers)\n\n    \"\"\"\n\n    def __init__(self, encoder, callback=None):\n        #: Instance of the :class:`MultipartEncoder` being monitored\n        self.encoder = encoder\n\n        #: Optionally function to call after a read\n        self.callback = callback or IDENTITY\n\n        #: Number of bytes already read from the :class:`MultipartEncoder`\n        #: instance\n        self.bytes_read = 0\n\n        #: Avoid the same problem in bug #80\n        self.len = self.encoder.len\n\n    @classmethod\n    def from_fields(cls, fields, boundary=None, encoding='utf-8',\n                    callback=None):\n        encoder = MultipartEncoder(fields, boundary, encoding)\n        return cls(encoder, callback)\n\n    @property\n    def content_type(self):\n        return self.encoder.content_type\n\n    def to_string(self):\n        return self.read()\n\n    def read(self, size=-1):\n        string = self.encoder.read(size)\n        self.bytes_read += len(string)\n        self.callback(self)\n        return string\n\n\ndef encode_with(string, encoding):\n    \"\"\"Encoding ``string`` with ``encoding`` if necessary.\n\n    :param str string: If string is a bytes object, it will not encode it.\n        Otherwise, this function will encode it with the provided encoding.\n    :param str encoding: The encoding with which to encode string.\n    :returns: encoded bytes object\n    \"\"\"\n    if not (string is None or isinstance(string, bytes)):\n        return string.encode(encoding)\n    return string\n\n\ndef readable_data(data, encoding):\n    \"\"\"Coerce the data to an object with a ``read`` method.\"\"\"\n    if hasattr(data, 'read'):\n        return data\n\n    return CustomBytesIO(data, encoding)\n\n\ndef total_len(o):\n    if hasattr(o, '__len__'):\n        return len(o)\n\n    if hasattr(o, 'len'):\n        return o.len\n\n    if hasattr(o, 'fileno'):\n        try:\n            fileno = o.fileno()\n        except io.UnsupportedOperation:\n            pass\n        else:\n            return os.fstat(fileno).st_size\n\n    if hasattr(o, 'getvalue'):\n        # e.g. BytesIO, cStringIO.StringIO\n        return len(o.getvalue())\n\n\n@contextlib.contextmanager\ndef reset(buffer):\n    \"\"\"Keep track of the buffer's current position and write to the end.\n\n    This is a context manager meant to be used when adding data to the buffer.\n    It eliminates the need for every function to be concerned with the\n    position of the cursor in the buffer.\n    \"\"\"\n    original_position = buffer.tell()\n    buffer.seek(0, 2)\n    yield\n    buffer.seek(original_position, 0)\n\n\ndef coerce_data(data, encoding):\n    \"\"\"Ensure that every object's __len__ behaves uniformly.\"\"\"\n    if not isinstance(data, CustomBytesIO):\n        if hasattr(data, 'getvalue'):\n            return CustomBytesIO(data.getvalue(), encoding)\n\n        if hasattr(data, 'fileno'):\n            return FileWrapper(data)\n\n        if not hasattr(data, 'read'):\n            return CustomBytesIO(data, encoding)\n\n    return data\n\n\ndef to_list(fields):\n    if hasattr(fields, 'items'):\n        return list(fields.items())\n    return list(fields)\n\n\nclass Part(object):\n    def __init__(self, headers, body):\n        self.headers = headers\n        self.body = body\n        self.headers_unread = True\n        self.len = len(self.headers) + total_len(self.body)\n\n    @classmethod\n    def from_field(cls, field, encoding):\n        \"\"\"Create a part from a Request Field generated by urllib3.\"\"\"\n        headers = encode_with(field.render_headers(), encoding)\n        body = coerce_data(field.data, encoding)\n        return cls(headers, body)\n\n    def bytes_left_to_write(self):\n        \"\"\"Determine if there are bytes left to write.\n\n        :returns: bool -- ``True`` if there are bytes left to write, otherwise\n            ``False``\n        \"\"\"\n        to_read = 0\n        if self.headers_unread:\n            to_read += len(self.headers)\n\n        return (to_read + total_len(self.body)) > 0\n\n    def write_to(self, buffer, size):\n        \"\"\"Write the requested amount of bytes to the buffer provided.\n\n        The number of bytes written may exceed size on the first read since we\n        load the headers ambitiously.\n\n        :param CustomBytesIO buffer: buffer we want to write bytes to\n        :param int size: number of bytes requested to be written to the buffer\n        :returns: int -- number of bytes actually written\n        \"\"\"\n        written = 0\n        if self.headers_unread:\n            written += buffer.append(self.headers)\n            self.headers_unread = False\n\n        while total_len(self.body) > 0 and (size == -1 or written < size):\n            amount_to_read = size\n            if size != -1:\n                amount_to_read = size - written\n            written += buffer.append(self.body.read(amount_to_read))\n\n        return written\n\n\nclass CustomBytesIO(io.BytesIO):\n    def __init__(self, buffer=None, encoding='utf-8'):\n        buffer = encode_with(buffer, encoding)\n        super(CustomBytesIO, self).__init__(buffer)\n\n    def _get_end(self):\n        current_pos = self.tell()\n        self.seek(0, 2)\n        length = self.tell()\n        self.seek(current_pos, 0)\n        return length\n\n    @property\n    def len(self):\n        length = self._get_end()\n        return length - self.tell()\n\n    def append(self, bytes):\n        with reset(self):\n            written = self.write(bytes)\n        return written\n\n    def smart_truncate(self):\n        to_be_read = total_len(self)\n        already_read = self._get_end() - to_be_read\n\n        if already_read >= to_be_read:\n            old_bytes = self.read()\n            self.seek(0, 0)\n            self.truncate()\n            self.write(old_bytes)\n            self.seek(0, 0)  # We want to be at the beginning\n\n\nclass FileWrapper(object):\n    def __init__(self, file_object):\n        self.fd = file_object\n\n    @property\n    def len(self):\n        return total_len(self.fd) - self.fd.tell()\n\n    def read(self, length=-1):\n        return self.fd.read(length)\n\n\nclass FileFromURLWrapper(object):\n    \"\"\"File from URL wrapper.\n\n    The :class:`FileFromURLWrapper` object gives you the ability to stream file\n    from provided URL in chunks by :class:`MultipartEncoder`.\n    Provide a stateless solution for streaming file from one server to another.\n    You can use the :class:`FileFromURLWrapper` without a session or with\n    a session as demonstated by the examples below:\n\n    .. code-block:: python\n        # no session\n\n        import requests\n        from requests_toolbelt import MultipartEncoder, FileFromURLWrapper\n\n        url = 'https://httpbin.org/image/png'\n        streaming_encoder = MultipartEncoder(\n            fields={\n                'file': FileFromURLWrapper(url)\n            }\n        )\n        r = requests.post(\n            'https://httpbin.org/post', data=streaming_encoder,\n            headers={'Content-Type': streaming_encoder.content_type}\n        )\n\n    .. code-block:: python\n        # using a session\n\n        import requests\n        from requests_toolbelt import MultipartEncoder, FileFromURLWrapper\n\n        session = requests.Session()\n        url = 'https://httpbin.org/image/png'\n        streaming_encoder = MultipartEncoder(\n            fields={\n                'file': FileFromURLWrapper(url, session=session)\n            }\n        )\n        r = session.post(\n            'https://httpbin.org/post', data=streaming_encoder,\n            headers={'Content-Type': streaming_encoder.content_type}\n        )\n\n    \"\"\"\n\n    def __init__(self, file_url, session=None):\n        self.session = session or requests.Session()\n        requested_file = self._request_for_file(file_url)\n        self.len = int(requested_file.headers['content-length'])\n        self.raw_data = requested_file.raw\n\n    def _request_for_file(self, file_url):\n        \"\"\"Make call for file under provided URL.\"\"\"\n        response = self.session.get(file_url, stream=True)\n        content_length = response.headers.get('content-length', None)\n        if content_length is None:\n            error_msg = (\n                \"Data from provided URL {url} is not supported. Lack of \"\n                \"content-length Header in requested file response.\".format(\n                    url=file_url)\n            )\n            raise FileNotSupportedError(error_msg)\n        elif not content_length.isdigit():\n            error_msg = (\n                \"Data from provided URL {url} is not supported. content-length\"\n                \" header value is not a digit.\".format(url=file_url)\n            )\n            raise FileNotSupportedError(error_msg)\n        return response\n\n    def read(self, chunk_size):\n        \"\"\"Read file in chunks.\"\"\"\n        chunk_size = chunk_size if chunk_size >= 0 else self.len\n        chunk = self.raw_data.read(chunk_size) or b''\n        self.len -= len(chunk) if chunk else 0  # left to read\n        return chunk\n", "requests_toolbelt/threaded/pool.py": "\"\"\"Module implementing the Pool for :mod:``requests_toolbelt.threaded``.\"\"\"\nimport multiprocessing\nimport requests\n\nfrom . import thread\nfrom .._compat import queue\n\n\nclass Pool(object):\n    \"\"\"Pool that manages the threads containing sessions.\n\n    :param job_queue:\n        The queue you're expected to use to which you should add items.\n    :type job_queue: queue.Queue\n    :param initializer:\n        Function used to initialize an instance of ``session``.\n    :type initializer: collections.Callable\n    :param auth_generator:\n        Function used to generate new auth credentials for the session.\n    :type auth_generator: collections.Callable\n    :param int num_processes:\n        Number of threads to create.\n    :param session:\n    :type session: requests.Session\n    \"\"\"\n\n    def __init__(self, job_queue, initializer=None, auth_generator=None,\n                 num_processes=None, session=requests.Session):\n        if num_processes is None:\n            num_processes = multiprocessing.cpu_count() or 1\n\n        if num_processes < 1:\n            raise ValueError(\"Number of processes should at least be 1.\")\n\n        self._job_queue = job_queue\n        self._response_queue = queue.Queue()\n        self._exc_queue = queue.Queue()\n        self._processes = num_processes\n        self._initializer = initializer or _identity\n        self._auth = auth_generator or _identity\n        self._session = session\n        self._pool = [\n            thread.SessionThread(self._new_session(), self._job_queue,\n                                 self._response_queue, self._exc_queue)\n            for _ in range(self._processes)\n        ]\n\n    def _new_session(self):\n        return self._auth(self._initializer(self._session()))\n\n    @classmethod\n    def from_exceptions(cls, exceptions, **kwargs):\n        r\"\"\"Create a :class:`~Pool` from an :class:`~ThreadException`\\ s.\n\n        Provided an iterable that provides :class:`~ThreadException` objects,\n        this classmethod will generate a new pool to retry the requests that\n        caused the exceptions.\n\n        :param exceptions:\n            Iterable that returns :class:`~ThreadException`\n        :type exceptions: iterable\n        :param kwargs:\n            Keyword arguments passed to the :class:`~Pool` initializer.\n        :returns: An initialized :class:`~Pool` object.\n        :rtype: :class:`~Pool`\n        \"\"\"\n        job_queue = queue.Queue()\n        for exc in exceptions:\n            job_queue.put(exc.request_kwargs)\n\n        return cls(job_queue=job_queue, **kwargs)\n\n    @classmethod\n    def from_urls(cls, urls, request_kwargs=None, **kwargs):\n        \"\"\"Create a :class:`~Pool` from an iterable of URLs.\n\n        :param urls:\n            Iterable that returns URLs with which we create a pool.\n        :type urls: iterable\n        :param dict request_kwargs:\n            Dictionary of other keyword arguments to provide to the request\n            method.\n        :param kwargs:\n            Keyword arguments passed to the :class:`~Pool` initializer.\n        :returns: An initialized :class:`~Pool` object.\n        :rtype: :class:`~Pool`\n        \"\"\"\n        request_dict = {'method': 'GET'}\n        request_dict.update(request_kwargs or {})\n        job_queue = queue.Queue()\n        for url in urls:\n            job = request_dict.copy()\n            job.update({'url': url})\n            job_queue.put(job)\n\n        return cls(job_queue=job_queue, **kwargs)\n\n    def exceptions(self):\n        \"\"\"Iterate over all the exceptions in the pool.\n\n        :returns: Generator of :class:`~ThreadException`\n        \"\"\"\n        while True:\n            exc = self.get_exception()\n            if exc is None:\n                break\n            yield exc\n\n    def get_exception(self):\n        \"\"\"Get an exception from the pool.\n\n        :rtype: :class:`~ThreadException`\n        \"\"\"\n        try:\n            (request, exc) = self._exc_queue.get_nowait()\n        except queue.Empty:\n            return None\n        else:\n            return ThreadException(request, exc)\n\n    def get_response(self):\n        \"\"\"Get a response from the pool.\n\n        :rtype: :class:`~ThreadResponse`\n        \"\"\"\n        try:\n            (request, response) = self._response_queue.get_nowait()\n        except queue.Empty:\n            return None\n        else:\n            return ThreadResponse(request, response)\n\n    def responses(self):\n        \"\"\"Iterate over all the responses in the pool.\n\n        :returns: Generator of :class:`~ThreadResponse`\n        \"\"\"\n        while True:\n            resp = self.get_response()\n            if resp is None:\n                break\n            yield resp\n\n    def join_all(self):\n        \"\"\"Join all the threads to the master thread.\"\"\"\n        for session_thread in self._pool:\n            session_thread.join()\n\n\nclass ThreadProxy(object):\n    proxied_attr = None\n\n    def __getattr__(self, attr):\n        \"\"\"Proxy attribute accesses to the proxied object.\"\"\"\n        get = object.__getattribute__\n        if attr not in self.attrs:\n            response = get(self, self.proxied_attr)\n            return getattr(response, attr)\n        else:\n            return get(self, attr)\n\n\nclass ThreadResponse(ThreadProxy):\n    \"\"\"A wrapper around a requests Response object.\n\n    This will proxy most attribute access actions to the Response object. For\n    example, if you wanted the parsed JSON from the response, you might do:\n\n    .. code-block:: python\n\n        thread_response = pool.get_response()\n        json = thread_response.json()\n\n    \"\"\"\n    proxied_attr = 'response'\n    attrs = frozenset(['request_kwargs', 'response'])\n\n    def __init__(self, request_kwargs, response):\n        #: The original keyword arguments provided to the queue\n        self.request_kwargs = request_kwargs\n        #: The wrapped response\n        self.response = response\n\n\nclass ThreadException(ThreadProxy):\n    \"\"\"A wrapper around an exception raised during a request.\n\n    This will proxy most attribute access actions to the exception object. For\n    example, if you wanted the message from the exception, you might do:\n\n    .. code-block:: python\n\n        thread_exc = pool.get_exception()\n        msg = thread_exc.message\n\n    \"\"\"\n    proxied_attr = 'exception'\n    attrs = frozenset(['request_kwargs', 'exception'])\n\n    def __init__(self, request_kwargs, exception):\n        #: The original keyword arguments provided to the queue\n        self.request_kwargs = request_kwargs\n        #: The captured and wrapped exception\n        self.exception = exception\n\n\ndef _identity(session_obj):\n    return session_obj\n\n\n__all__ = ['ThreadException', 'ThreadResponse', 'Pool']\n", "requests_toolbelt/threaded/thread.py": "\"\"\"Module containing the SessionThread class.\"\"\"\nimport threading\nimport uuid\n\nimport requests.exceptions as exc\n\nfrom .._compat import queue\n\n\nclass SessionThread(object):\n    def __init__(self, initialized_session, job_queue, response_queue,\n                 exception_queue):\n        self._session = initialized_session\n        self._jobs = job_queue\n        self._create_worker()\n        self._responses = response_queue\n        self._exceptions = exception_queue\n\n    def _create_worker(self):\n        self._worker = threading.Thread(\n            target=self._make_request,\n            name=uuid.uuid4(),\n        )\n        self._worker.daemon = True\n        self._worker._state = 0\n        self._worker.start()\n\n    def _handle_request(self, kwargs):\n        try:\n            response = self._session.request(**kwargs)\n        except exc.RequestException as e:\n            self._exceptions.put((kwargs, e))\n        else:\n            self._responses.put((kwargs, response))\n        finally:\n            self._jobs.task_done()\n\n    def _make_request(self):\n        while True:\n            try:\n                kwargs = self._jobs.get_nowait()\n            except queue.Empty:\n                break\n\n            self._handle_request(kwargs)\n\n    def is_alive(self):\n        \"\"\"Proxy to the thread's ``is_alive`` method.\"\"\"\n        return self._worker.is_alive()\n\n    def join(self):\n        \"\"\"Join this thread to the master thread.\"\"\"\n        self._worker.join()\n", "requests_toolbelt/threaded/__init__.py": "\"\"\"\nThis module provides the API for ``requests_toolbelt.threaded``.\n\nThe module provides a clean and simple API for making requests via a thread\npool. The thread pool will use sessions for increased performance.\n\nA simple use-case is:\n\n.. code-block:: python\n\n    from requests_toolbelt import threaded\n\n    urls_to_get = [{\n        'url': 'https://api.github.com/users/sigmavirus24',\n        'method': 'GET',\n    }, {\n        'url': 'https://api.github.com/repos/requests/toolbelt',\n        'method': 'GET',\n    }, {\n        'url': 'https://google.com',\n        'method': 'GET',\n    }]\n    responses, errors = threaded.map(urls_to_get)\n\nBy default, the threaded submodule will detect the number of CPUs your\ncomputer has and use that if no other number of processes is selected. To\nchange this, always use the keyword argument ``num_processes``. Using the\nabove example, we would expand it like so:\n\n.. code-block:: python\n\n    responses, errors = threaded.map(urls_to_get, num_processes=10)\n\nYou can also customize how a :class:`requests.Session` is initialized by\ncreating a callback function:\n\n.. code-block:: python\n\n    from requests_toolbelt import user_agent\n\n    def initialize_session(session):\n        session.headers['User-Agent'] = user_agent('my-scraper', '0.1')\n        session.headers['Accept'] = 'application/json'\n\n    responses, errors = threaded.map(urls_to_get,\n                                     initializer=initialize_session)\n\n.. autofunction:: requests_toolbelt.threaded.map\n\nInspiration is blatantly drawn from the standard library's multiprocessing\nlibrary. See the following references:\n\n- multiprocessing's `pool source`_\n\n- map and map_async `inspiration`_\n\n.. _pool source:\n    https://hg.python.org/cpython/file/8ef4f75a8018/Lib/multiprocessing/pool.py\n.. _inspiration:\n    https://hg.python.org/cpython/file/8ef4f75a8018/Lib/multiprocessing/pool.py#l340\n\"\"\"\nfrom . import pool\nfrom .._compat import queue\n\n\ndef map(requests, **kwargs):\n    r\"\"\"Simple interface to the threaded Pool object.\n\n    This function takes a list of dictionaries representing requests to make\n    using Sessions in threads and returns a tuple where the first item is\n    a generator of successful responses and the second is a generator of\n    exceptions.\n\n    :param list requests:\n        Collection of dictionaries representing requests to make with the Pool\n        object.\n    :param \\*\\*kwargs:\n        Keyword arguments that are passed to the\n        :class:`~requests_toolbelt.threaded.pool.Pool` object.\n    :returns: Tuple of responses and exceptions from the pool\n    :rtype: (:class:`~requests_toolbelt.threaded.pool.ThreadResponse`,\n        :class:`~requests_toolbelt.threaded.pool.ThreadException`)\n    \"\"\"\n    if not (requests and all(isinstance(r, dict) for r in requests)):\n        raise ValueError('map expects a list of dictionaries.')\n\n    # Build our queue of requests\n    job_queue = queue.Queue()\n    for request in requests:\n        job_queue.put(request)\n\n    # Ensure the user doesn't try to pass their own job_queue\n    kwargs['job_queue'] = job_queue\n\n    threadpool = pool.Pool(**kwargs)\n    threadpool.join_all()\n    return threadpool.responses(), threadpool.exceptions()\n", "requests_toolbelt/auth/guess.py": "# -*- coding: utf-8 -*-\n\"\"\"The module containing the code for GuessAuth.\"\"\"\nfrom requests import auth\nfrom requests import cookies\n\nfrom . import _digest_auth_compat as auth_compat, http_proxy_digest\n\n\nclass GuessAuth(auth.AuthBase):\n    \"\"\"Guesses the auth type by the WWW-Authentication header.\"\"\"\n    def __init__(self, username, password):\n        self.username = username\n        self.password = password\n        self.auth = None\n        self.pos = None\n\n    def _handle_basic_auth_401(self, r, kwargs):\n        if self.pos is not None:\n            r.request.body.seek(self.pos)\n\n        # Consume content and release the original connection\n        # to allow our new request to reuse the same one.\n        r.content\n        r.raw.release_conn()\n        prep = r.request.copy()\n        if not hasattr(prep, '_cookies'):\n            prep._cookies = cookies.RequestsCookieJar()\n        cookies.extract_cookies_to_jar(prep._cookies, r.request, r.raw)\n        prep.prepare_cookies(prep._cookies)\n\n        self.auth = auth.HTTPBasicAuth(self.username, self.password)\n        prep = self.auth(prep)\n        _r = r.connection.send(prep, **kwargs)\n        _r.history.append(r)\n        _r.request = prep\n\n        return _r\n\n    def _handle_digest_auth_401(self, r, kwargs):\n        self.auth = auth_compat.HTTPDigestAuth(self.username, self.password)\n        try:\n            self.auth.init_per_thread_state()\n        except AttributeError:\n            # If we're not on requests 2.8.0+ this method does not exist and\n            # is not relevant.\n            pass\n\n        # Check that the attr exists because much older versions of requests\n        # set this attribute lazily. For example:\n        # https://github.com/kennethreitz/requests/blob/33735480f77891754304e7f13e3cdf83aaaa76aa/requests/auth.py#L59\n        if (hasattr(self.auth, 'num_401_calls') and\n                self.auth.num_401_calls is None):\n            self.auth.num_401_calls = 1\n        # Digest auth would resend the request by itself. We can take a\n        # shortcut here.\n        return self.auth.handle_401(r, **kwargs)\n\n    def handle_401(self, r, **kwargs):\n        \"\"\"Resends a request with auth headers, if needed.\"\"\"\n\n        www_authenticate = r.headers.get('www-authenticate', '').lower()\n\n        if 'basic' in www_authenticate:\n            return self._handle_basic_auth_401(r, kwargs)\n\n        if 'digest' in www_authenticate:\n            return self._handle_digest_auth_401(r, kwargs)\n\n    def __call__(self, request):\n        if self.auth is not None:\n            return self.auth(request)\n\n        try:\n            self.pos = request.body.tell()\n        except AttributeError:\n            pass\n\n        request.register_hook('response', self.handle_401)\n        return request\n\n\nclass GuessProxyAuth(GuessAuth):\n    \"\"\"\n    Guesses the auth type by WWW-Authentication and Proxy-Authentication\n    headers\n    \"\"\"\n    def __init__(self, username=None, password=None,\n                 proxy_username=None, proxy_password=None):\n        super(GuessProxyAuth, self).__init__(username, password)\n        self.proxy_username = proxy_username\n        self.proxy_password = proxy_password\n        self.proxy_auth = None\n\n    def _handle_basic_auth_407(self, r, kwargs):\n        if self.pos is not None:\n            r.request.body.seek(self.pos)\n\n        r.content\n        r.raw.release_conn()\n        prep = r.request.copy()\n        if not hasattr(prep, '_cookies'):\n            prep._cookies = cookies.RequestsCookieJar()\n        cookies.extract_cookies_to_jar(prep._cookies, r.request, r.raw)\n        prep.prepare_cookies(prep._cookies)\n\n        self.proxy_auth = auth.HTTPProxyAuth(self.proxy_username,\n                                             self.proxy_password)\n        prep = self.proxy_auth(prep)\n        _r = r.connection.send(prep, **kwargs)\n        _r.history.append(r)\n        _r.request = prep\n\n        return _r\n\n    def _handle_digest_auth_407(self, r, kwargs):\n        self.proxy_auth = http_proxy_digest.HTTPProxyDigestAuth(\n            username=self.proxy_username,\n            password=self.proxy_password)\n\n        try:\n            self.auth.init_per_thread_state()\n        except AttributeError:\n            pass\n\n        return self.proxy_auth.handle_407(r, **kwargs)\n\n    def handle_407(self, r, **kwargs):\n        proxy_authenticate = r.headers.get('Proxy-Authenticate', '').lower()\n\n        if 'basic' in proxy_authenticate:\n            return self._handle_basic_auth_407(r, kwargs)\n\n        if 'digest' in proxy_authenticate:\n            return self._handle_digest_auth_407(r, kwargs)\n\n    def __call__(self, request):\n        if self.proxy_auth is not None:\n            request = self.proxy_auth(request)\n\n        try:\n            self.pos = request.body.tell()\n        except AttributeError:\n            pass\n\n        request.register_hook('response', self.handle_407)\n        return super(GuessProxyAuth, self).__call__(request)\n", "requests_toolbelt/auth/http_proxy_digest.py": "# -*- coding: utf-8 -*-\n\"\"\"The module containing HTTPProxyDigestAuth.\"\"\"\nimport re\n\nfrom requests import cookies, utils\n\nfrom . import _digest_auth_compat as auth\n\n\nclass HTTPProxyDigestAuth(auth.HTTPDigestAuth):\n    \"\"\"HTTP digest authentication between proxy\n\n    :param stale_rejects: The number of rejects indicate that:\n        the client may wish to simply retry the request\n        with a new encrypted response, without reprompting the user for a\n        new username and password. i.e., retry build_digest_header\n    :type stale_rejects: int\n    \"\"\"\n    _pat = re.compile(r'digest ', flags=re.IGNORECASE)\n\n    def __init__(self, *args, **kwargs):\n        super(HTTPProxyDigestAuth, self).__init__(*args, **kwargs)\n        self.stale_rejects = 0\n\n        self.init_per_thread_state()\n\n    @property\n    def stale_rejects(self):\n        thread_local = getattr(self, '_thread_local', None)\n        if thread_local is None:\n            return self._stale_rejects\n        return thread_local.stale_rejects\n\n    @stale_rejects.setter\n    def stale_rejects(self, value):\n        thread_local = getattr(self, '_thread_local', None)\n        if thread_local is None:\n            self._stale_rejects = value\n        else:\n            thread_local.stale_rejects = value\n\n    def init_per_thread_state(self):\n        try:\n            super(HTTPProxyDigestAuth, self).init_per_thread_state()\n        except AttributeError:\n            # If we're not on requests 2.8.0+ this method does not exist\n            pass\n\n    def handle_407(self, r, **kwargs):\n        \"\"\"Handle HTTP 407 only once, otherwise give up\n\n        :param r: current response\n        :returns: responses, along with the new response\n        \"\"\"\n        if r.status_code == 407 and self.stale_rejects < 2:\n            s_auth = r.headers.get(\"proxy-authenticate\")\n            if s_auth is None:\n                raise IOError(\n                    \"proxy server violated RFC 7235:\"\n                    \"407 response MUST contain header proxy-authenticate\")\n            elif not self._pat.match(s_auth):\n                return r\n\n            self.chal = utils.parse_dict_header(\n                self._pat.sub('', s_auth, count=1))\n\n            # if we present the user/passwd and still get rejected\n            # https://tools.ietf.org/html/rfc2617#section-3.2.1\n            if ('Proxy-Authorization' in r.request.headers and\n                    'stale' in self.chal):\n                if self.chal['stale'].lower() == 'true':  # try again\n                    self.stale_rejects += 1\n                # wrong user/passwd\n                elif self.chal['stale'].lower() == 'false':\n                    raise IOError(\"User or password is invalid\")\n\n            # Consume content and release the original connection\n            # to allow our new request to reuse the same one.\n            r.content\n            r.close()\n            prep = r.request.copy()\n            cookies.extract_cookies_to_jar(prep._cookies, r.request, r.raw)\n            prep.prepare_cookies(prep._cookies)\n\n            prep.headers['Proxy-Authorization'] = self.build_digest_header(\n                prep.method, prep.url)\n            _r = r.connection.send(prep, **kwargs)\n            _r.history.append(r)\n            _r.request = prep\n\n            return _r\n        else:  # give up authenticate\n            return r\n\n    def __call__(self, r):\n        self.init_per_thread_state()\n        # if we have nonce, then just use it, otherwise server will tell us\n        if self.last_nonce:\n            r.headers['Proxy-Authorization'] = self.build_digest_header(\n                r.method, r.url\n            )\n        r.register_hook('response', self.handle_407)\n        return r\n", "requests_toolbelt/auth/_digest_auth_compat.py": "\"\"\"Provide a compatibility layer for requests.auth.HTTPDigestAuth.\"\"\"\nimport requests\n\n\nclass _ThreadingDescriptor(object):\n    def __init__(self, prop, default):\n        self.prop = prop\n        self.default = default\n\n    def __get__(self, obj, objtype=None):\n        return getattr(obj._thread_local, self.prop, self.default)\n\n    def __set__(self, obj, value):\n        setattr(obj._thread_local, self.prop, value)\n\n\nclass _HTTPDigestAuth(requests.auth.HTTPDigestAuth):\n    init = _ThreadingDescriptor('init', True)\n    last_nonce = _ThreadingDescriptor('last_nonce', '')\n    nonce_count = _ThreadingDescriptor('nonce_count', 0)\n    chal = _ThreadingDescriptor('chal', {})\n    pos = _ThreadingDescriptor('pos', None)\n    num_401_calls = _ThreadingDescriptor('num_401_calls', 1)\n\n\nif requests.__build__ < 0x020800:\n    HTTPDigestAuth = requests.auth.HTTPDigestAuth\nelse:\n    HTTPDigestAuth = _HTTPDigestAuth\n", "requests_toolbelt/auth/http_bearer.py": "# -*- coding: utf-8 -*-\n\"\"\"The module containing HTTPBearerAuth.\"\"\"\n\nfrom requests.auth import AuthBase\n\n\nclass HTTPBearerAuth(AuthBase):\n    \"\"\"HTTP Bearer Token Authentication\n    \"\"\"\n\n    def __init__(self, token):\n        self.token = token\n\n    def __eq__(self, other):\n        return self.token == getattr(other, 'token', None)\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __call__(self, r):\n        r.headers['Authorization'] = 'Bearer ' + self.token\n        return r\n", "requests_toolbelt/auth/__init__.py": "", "requests_toolbelt/auth/handler.py": "# -*- coding: utf-8 -*-\n\"\"\"\n\nrequests_toolbelt.auth.handler\n==============================\n\nThis holds all of the implementation details of the Authentication Handler.\n\n\"\"\"\n\nfrom requests.auth import AuthBase, HTTPBasicAuth\nfrom requests.compat import urlparse, urlunparse\n\n\nclass AuthHandler(AuthBase):\n\n    \"\"\"\n\n    The ``AuthHandler`` object takes a dictionary of domains paired with\n    authentication strategies and will use this to determine which credentials\n    to use when making a request. For example, you could do the following:\n\n    .. code-block:: python\n\n        from requests import HTTPDigestAuth\n        from requests_toolbelt.auth.handler import AuthHandler\n\n        import requests\n\n        auth = AuthHandler({\n            'https://api.github.com': ('sigmavirus24', 'fakepassword'),\n            'https://example.com': HTTPDigestAuth('username', 'password')\n        })\n\n        r = requests.get('https://api.github.com/user', auth=auth)\n        # => <Response [200]>\n        r = requests.get('https://example.com/some/path', auth=auth)\n        # => <Response [200]>\n\n        s = requests.Session()\n        s.auth = auth\n        r = s.get('https://api.github.com/user')\n        # => <Response [200]>\n\n    .. warning::\n\n        :class:`requests.auth.HTTPDigestAuth` is not yet thread-safe. If you\n        use :class:`AuthHandler` across multiple threads you should\n        instantiate a new AuthHandler for each thread with a new\n        HTTPDigestAuth instance for each thread.\n\n    \"\"\"\n\n    def __init__(self, strategies):\n        self.strategies = dict(strategies)\n        self._make_uniform()\n\n    def __call__(self, request):\n        auth = self.get_strategy_for(request.url)\n        return auth(request)\n\n    def __repr__(self):\n        return '<AuthHandler({!r})>'.format(self.strategies)\n\n    def _make_uniform(self):\n        existing_strategies = list(self.strategies.items())\n        self.strategies = {}\n\n        for (k, v) in existing_strategies:\n            self.add_strategy(k, v)\n\n    @staticmethod\n    def _key_from_url(url):\n        parsed = urlparse(url)\n        return urlunparse((parsed.scheme.lower(),\n                           parsed.netloc.lower(),\n                           '', '', '', ''))\n\n    def add_strategy(self, domain, strategy):\n        \"\"\"Add a new domain and authentication strategy.\n\n        :param str domain: The domain you wish to match against. For example:\n            ``'https://api.github.com'``\n        :param str strategy: The authentication strategy you wish to use for\n            that domain. For example: ``('username', 'password')`` or\n            ``requests.HTTPDigestAuth('username', 'password')``\n\n        .. code-block:: python\n\n            a = AuthHandler({})\n            a.add_strategy('https://api.github.com', ('username', 'password'))\n\n        \"\"\"\n        # Turn tuples into Basic Authentication objects\n        if isinstance(strategy, tuple):\n            strategy = HTTPBasicAuth(*strategy)\n\n        key = self._key_from_url(domain)\n        self.strategies[key] = strategy\n\n    def get_strategy_for(self, url):\n        \"\"\"Retrieve the authentication strategy for a specified URL.\n\n        :param str url: The full URL you will be making a request against. For\n            example, ``'https://api.github.com/user'``\n        :returns: Callable that adds authentication to a request.\n\n        .. code-block:: python\n\n            import requests\n            a = AuthHandler({'example.com', ('foo', 'bar')})\n            strategy = a.get_strategy_for('http://example.com/example')\n            assert isinstance(strategy, requests.auth.HTTPBasicAuth)\n\n        \"\"\"\n        key = self._key_from_url(url)\n        return self.strategies.get(key, NullAuthStrategy())\n\n    def remove_strategy(self, domain):\n        \"\"\"Remove the domain and strategy from the collection of strategies.\n\n        :param str domain: The domain you wish remove. For example,\n            ``'https://api.github.com'``.\n\n        .. code-block:: python\n\n            a = AuthHandler({'example.com', ('foo', 'bar')})\n            a.remove_strategy('example.com')\n            assert a.strategies == {}\n\n        \"\"\"\n        key = self._key_from_url(domain)\n        if key in self.strategies:\n            del self.strategies[key]\n\n\nclass NullAuthStrategy(AuthBase):\n    def __repr__(self):\n        return '<NullAuthStrategy>'\n\n    def __call__(self, r):\n        return r\n", "requests_toolbelt/downloadutils/tee.py": "\"\"\"Tee function implementations.\"\"\"\nimport io\n\n_DEFAULT_CHUNKSIZE = 65536\n\n__all__ = ['tee', 'tee_to_file', 'tee_to_bytearray']\n\n\ndef _tee(response, callback, chunksize, decode_content):\n    for chunk in response.raw.stream(amt=chunksize,\n                                     decode_content=decode_content):\n        callback(chunk)\n        yield chunk\n\n\ndef tee(response, fileobject, chunksize=_DEFAULT_CHUNKSIZE,\n        decode_content=None):\n    \"\"\"Stream the response both to the generator and a file.\n\n    This will stream the response body while writing the bytes to\n    ``fileobject``.\n\n    Example usage:\n\n    .. code-block:: python\n\n        resp = requests.get(url, stream=True)\n        with open('save_file', 'wb') as save_file:\n            for chunk in tee(resp, save_file):\n                # do stuff with chunk\n\n    .. code-block:: python\n\n        import io\n\n        resp = requests.get(url, stream=True)\n        fileobject = io.BytesIO()\n\n        for chunk in tee(resp, fileobject):\n            # do stuff with chunk\n\n    :param response: Response from requests.\n    :type response: requests.Response\n    :param fileobject: Writable file-like object.\n    :type fileobject: file, io.BytesIO\n    :param int chunksize: (optional), Size of chunk to attempt to stream.\n    :param bool decode_content: (optional), If True, this will decode the\n        compressed content of the response.\n    :raises: TypeError if the fileobject wasn't opened with the right mode\n        or isn't a BytesIO object.\n    \"\"\"\n    # We will be streaming the raw bytes from over the wire, so we need to\n    # ensure that writing to the fileobject will preserve those bytes. On\n    # Python3, if the user passes an io.StringIO, this will fail, so we need\n    # to check for BytesIO instead.\n    if not ('b' in getattr(fileobject, 'mode', '') or\n            isinstance(fileobject, io.BytesIO)):\n        raise TypeError('tee() will write bytes directly to this fileobject'\n                        ', it must be opened with the \"b\" flag if it is a file'\n                        ' or inherit from io.BytesIO.')\n\n    return _tee(response, fileobject.write, chunksize, decode_content)\n\n\ndef tee_to_file(response, filename, chunksize=_DEFAULT_CHUNKSIZE,\n                decode_content=None):\n    \"\"\"Stream the response both to the generator and a file.\n\n    This will open a file named ``filename`` and stream the response body\n    while writing the bytes to the opened file object.\n\n    Example usage:\n\n    .. code-block:: python\n\n        resp = requests.get(url, stream=True)\n        for chunk in tee_to_file(resp, 'save_file'):\n            # do stuff with chunk\n\n    :param response: Response from requests.\n    :type response: requests.Response\n    :param str filename: Name of file in which we write the response content.\n    :param int chunksize: (optional), Size of chunk to attempt to stream.\n    :param bool decode_content: (optional), If True, this will decode the\n        compressed content of the response.\n    \"\"\"\n    with open(filename, 'wb') as fd:\n        for chunk in tee(response, fd, chunksize, decode_content):\n            yield chunk\n\n\ndef tee_to_bytearray(response, bytearr, chunksize=_DEFAULT_CHUNKSIZE,\n                     decode_content=None):\n    \"\"\"Stream the response both to the generator and a bytearray.\n\n    This will stream the response provided to the function, add them to the\n    provided :class:`bytearray` and yield them to the user.\n\n    .. note::\n\n        This uses the :meth:`bytearray.extend` by default instead of passing\n        the bytearray into the ``readinto`` method.\n\n    Example usage:\n\n    .. code-block:: python\n\n        b = bytearray()\n        resp = requests.get(url, stream=True)\n        for chunk in tee_to_bytearray(resp, b):\n            # do stuff with chunk\n\n    :param response: Response from requests.\n    :type response: requests.Response\n    :param bytearray bytearr: Array to add the streamed bytes to.\n    :param int chunksize: (optional), Size of chunk to attempt to stream.\n    :param bool decode_content: (optional), If True, this will decode the\n        compressed content of the response.\n    \"\"\"\n    if not isinstance(bytearr, bytearray):\n        raise TypeError('tee_to_bytearray() expects bytearr to be a '\n                        'bytearray')\n    return _tee(response, bytearr.extend, chunksize, decode_content)\n", "requests_toolbelt/downloadutils/stream.py": "# -*- coding: utf-8 -*-\n\"\"\"Utilities for dealing with streamed requests.\"\"\"\nimport os.path\nimport re\n\nfrom .. import exceptions as exc\n\n# Regular expressions stolen from werkzeug/http.py\n# cd2c97bb0a076da2322f11adce0b2731f9193396 L62-L64\n_QUOTED_STRING_RE = r'\"[^\"\\\\]*(?:\\\\.[^\"\\\\]*)*\"'\n_OPTION_HEADER_PIECE_RE = re.compile(\n    r';\\s*(%s|[^\\s;=]+)\\s*(?:=\\s*(%s|[^;]+))?\\s*' % (_QUOTED_STRING_RE,\n                                                     _QUOTED_STRING_RE)\n)\n_DEFAULT_CHUNKSIZE = 512\n\n\ndef _get_filename(content_disposition):\n    for match in _OPTION_HEADER_PIECE_RE.finditer(content_disposition):\n        k, v = match.groups()\n        if k == 'filename':\n            # ignore any directory paths in the filename\n            return os.path.split(v)[1]\n    return None\n\n\ndef get_download_file_path(response, path):\n    \"\"\"\n    Given a response and a path, return a file path for a download.\n\n    If a ``path`` parameter is a directory, this function will parse the\n    ``Content-Disposition`` header on the response to determine the name of the\n    file as reported by the server, and return a file path in the specified\n    directory.\n\n    If ``path`` is empty or None, this function will return a path relative\n    to the process' current working directory.\n\n    If path is a full file path, return it.\n\n    :param response: A Response object from requests\n    :type response: requests.models.Response\n    :param str path: Directory or file path.\n    :returns: full file path to download as\n    :rtype: str\n    :raises: :class:`requests_toolbelt.exceptions.StreamingError`\n    \"\"\"\n    path_is_dir = path and os.path.isdir(path)\n\n    if path and not path_is_dir:\n        # fully qualified file path\n        filepath = path\n    else:\n        response_filename = _get_filename(\n            response.headers.get('content-disposition', '')\n        )\n        if not response_filename:\n            raise exc.StreamingError('No filename given to stream response to')\n\n        if path_is_dir:\n            # directory to download to\n            filepath = os.path.join(path, response_filename)\n        else:\n            # fallback to downloading to current working directory\n            filepath = response_filename\n\n    return filepath\n\n\ndef stream_response_to_file(response, path=None, chunksize=_DEFAULT_CHUNKSIZE):\n    \"\"\"Stream a response body to the specified file.\n\n    Either use the ``path`` provided or use the name provided in the\n    ``Content-Disposition`` header.\n\n    .. warning::\n\n        If you pass this function an open file-like object as the ``path``\n        parameter, the function will not close that file for you.\n\n    .. warning::\n\n        This function will not automatically close the response object\n        passed in as the ``response`` parameter.\n\n    If a ``path`` parameter is a directory, this function will parse the\n    ``Content-Disposition`` header on the response to determine the name of the\n    file as reported by the server, and return a file path in the specified\n    directory. If no ``path`` parameter is supplied, this function will default\n    to the process' current working directory.\n\n    .. code-block:: python\n\n        import requests\n        from requests_toolbelt import exceptions\n        from requests_toolbelt.downloadutils import stream\n\n        r = requests.get(url, stream=True)\n        try:\n            filename = stream.stream_response_to_file(r)\n        except exceptions.StreamingError as e:\n            # The toolbelt could not find the filename in the\n            # Content-Disposition\n            print(e.message)\n\n    You can also specify the filename as a string. This will be passed to\n    the built-in :func:`open` and we will read the content into the file.\n\n    .. code-block:: python\n\n        import requests\n        from requests_toolbelt.downloadutils import stream\n\n        r = requests.get(url, stream=True)\n        filename = stream.stream_response_to_file(r, path='myfile')\n\n    If the calculated download file path already exists, this function will\n    raise a StreamingError.\n\n    Instead, if you want to manage the file object yourself, you need to\n    provide either a :class:`io.BytesIO` object or a file opened with the\n    `'b'` flag. See the two examples below for more details.\n\n    .. code-block:: python\n\n        import requests\n        from requests_toolbelt.downloadutils import stream\n\n        with open('myfile', 'wb') as fd:\n            r = requests.get(url, stream=True)\n            filename = stream.stream_response_to_file(r, path=fd)\n\n        print('{} saved to {}'.format(url, filename))\n\n    .. code-block:: python\n\n        import io\n        import requests\n        from requests_toolbelt.downloadutils import stream\n\n        b = io.BytesIO()\n        r = requests.get(url, stream=True)\n        filename = stream.stream_response_to_file(r, path=b)\n        assert filename is None\n\n    :param response: A Response object from requests\n    :type response: requests.models.Response\n    :param path: *(optional)*, Either a string with the path to the location\n        to save the response content, or a file-like object expecting bytes.\n    :type path: :class:`str`, or object with a :meth:`write`\n    :param int chunksize: (optional), Size of chunk to attempt to stream\n        (default 512B).\n    :returns: The name of the file, if one can be determined, else None\n    :rtype: str\n    :raises: :class:`requests_toolbelt.exceptions.StreamingError`\n    \"\"\"\n    pre_opened = False\n    fd = None\n    filename = None\n    if path and callable(getattr(path, 'write', None)):\n        pre_opened = True\n        fd = path\n        filename = getattr(fd, 'name', None)\n    else:\n        filename = get_download_file_path(response, path)\n        if os.path.exists(filename):\n            raise exc.StreamingError(\"File already exists: %s\" % filename)\n        fd = open(filename, 'wb')\n\n    for chunk in response.iter_content(chunk_size=chunksize):\n        fd.write(chunk)\n\n    if not pre_opened:\n        fd.close()\n\n    return filename\n", "requests_toolbelt/downloadutils/__init__.py": "", "requests_toolbelt/adapters/host_header_ssl.py": "# -*- coding: utf-8 -*-\n\"\"\"\nrequests_toolbelt.adapters.host_header_ssl\n==========================================\n\nThis file contains an implementation of the HostHeaderSSLAdapter.\n\"\"\"\n\nfrom requests.adapters import HTTPAdapter\n\n\nclass HostHeaderSSLAdapter(HTTPAdapter):\n    \"\"\"\n    A HTTPS Adapter for Python Requests that sets the hostname for certificate\n    verification based on the Host header.\n\n    This allows requesting the IP address directly via HTTPS without getting\n    a \"hostname doesn't match\" exception.\n\n    Example usage:\n\n        >>> s.mount('https://', HostHeaderSSLAdapter())\n        >>> s.get(\"https://93.184.216.34\", headers={\"Host\": \"example.org\"})\n\n    \"\"\"\n\n    def send(self, request, **kwargs):\n        # HTTP headers are case-insensitive (RFC 7230)\n        host_header = None\n        for header in request.headers:\n            if header.lower() == \"host\":\n                host_header = request.headers[header]\n                break\n\n        connection_pool_kwargs = self.poolmanager.connection_pool_kw\n\n        if host_header:\n            connection_pool_kwargs[\"assert_hostname\"] = host_header\n            connection_pool_kwargs[\"server_hostname\"] = host_header\n        elif \"assert_hostname\" in connection_pool_kwargs:\n            # an assert_hostname from a previous request may have been left\n            connection_pool_kwargs.pop(\"assert_hostname\", None)\n            connection_pool_kwargs.pop(\"server_hostname\", None)\n\n        return super(HostHeaderSSLAdapter, self).send(request, **kwargs)\n", "requests_toolbelt/adapters/x509.py": "# -*- coding: utf-8 -*-\n\"\"\"A X509Adapter for use with the requests library.\n\nThis file contains an implementation of the X509Adapter that will\nallow users to authenticate a request using an arbitrary\nX.509 certificate without needing to convert it to a .pem file\n\n\"\"\"\n\nfrom OpenSSL.crypto import PKey, X509\nfrom cryptography import x509\nfrom cryptography.hazmat.primitives.serialization import (load_pem_private_key,\n                                                          load_der_private_key)\nfrom cryptography.hazmat.primitives.serialization import Encoding\nfrom cryptography.hazmat.backends import default_backend\n\nfrom datetime import datetime, timezone\nfrom requests.adapters import HTTPAdapter\nimport requests\n\nfrom .. import exceptions as exc\n\n\"\"\"\nimporting the protocol constants from _ssl instead of ssl because only the\nconstants are needed and to handle issues caused by importing from ssl on\nthe 2.7.x line.\n\"\"\"\ntry:\n    from _ssl import PROTOCOL_TLS as PROTOCOL\nexcept ImportError:\n    from _ssl import PROTOCOL_SSLv23 as PROTOCOL\n\n\nPyOpenSSLContext = None\n\n\nclass X509Adapter(HTTPAdapter):\n    r\"\"\"Adapter for use with X.509 certificates.\n\n    Provides an interface for Requests sessions to contact HTTPS urls and\n    authenticate  with an X.509 cert by implementing the Transport Adapter\n    interface. This class will need to be manually instantiated and mounted\n    to the session\n\n    :param pool_connections: The number of urllib3 connection pools to\n           cache.\n    :param pool_maxsize: The maximum number of connections to save in the\n            pool.\n    :param max_retries: The maximum number of retries each connection\n        should attempt. Note, this applies only to failed DNS lookups,\n        socket connections and connection timeouts, never to requests where\n        data has made it to the server. By default, Requests does not retry\n        failed connections. If you need granular control over the\n        conditions under which we retry a request, import urllib3's\n        ``Retry`` class and pass that instead.\n    :param pool_block: Whether the connection pool should block for\n            connections.\n\n    :param bytes cert_bytes:\n        bytes object containing contents of a cryptography.x509Certificate\n        object using the encoding specified by the ``encoding`` parameter.\n    :param bytes pk_bytes:\n        bytes object containing contents of a object that implements\n        ``cryptography.hazmat.primitives.serialization.PrivateFormat``\n        using the encoding specified by the ``encoding`` parameter.\n    :param password:\n        string or utf8 encoded bytes containing the passphrase used for the\n        private key. None if unencrypted. Defaults to None.\n    :param encoding:\n        Enumeration detailing the encoding method used on the ``cert_bytes``\n        parameter. Can be either PEM or DER. Defaults to PEM.\n    :type encoding:\n        :class: `cryptography.hazmat.primitives.serialization.Encoding`\n\n    Usage::\n\n      >>> import requests\n      >>> from requests_toolbelt.adapters.x509 import X509Adapter\n      >>> s = requests.Session()\n      >>> a = X509Adapter(max_retries=3,\n                cert_bytes=b'...', pk_bytes=b'...', encoding='...'\n      >>> s.mount('https://', a)\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        self._import_pyopensslcontext()\n        self._check_version()\n        cert_bytes = kwargs.pop('cert_bytes', None)\n        pk_bytes = kwargs.pop('pk_bytes', None)\n        password = kwargs.pop('password', None)\n        encoding = kwargs.pop('encoding', Encoding.PEM)\n\n        password_bytes = None\n\n        if cert_bytes is None or not isinstance(cert_bytes, bytes):\n            raise ValueError('Invalid cert content provided. '\n                             'You must provide an X.509 cert '\n                             'formatted as a byte array.')\n        if pk_bytes is None or not isinstance(pk_bytes, bytes):\n            raise ValueError('Invalid private key content provided. '\n                             'You must provide a private key '\n                             'formatted as a byte array.')\n\n        if isinstance(password, bytes):\n            password_bytes = password\n        elif password:\n            password_bytes = password.encode('utf8')\n\n        self.ssl_context = create_ssl_context(cert_bytes, pk_bytes,\n                                              password_bytes, encoding)\n\n        super(X509Adapter, self).__init__(*args, **kwargs)\n\n    def init_poolmanager(self, *args, **kwargs):\n        if self.ssl_context:\n            kwargs['ssl_context'] = self.ssl_context\n        return super(X509Adapter, self).init_poolmanager(*args, **kwargs)\n\n    def proxy_manager_for(self, *args, **kwargs):\n        if self.ssl_context:\n            kwargs['ssl_context'] = self.ssl_context\n        return super(X509Adapter, self).proxy_manager_for(*args, **kwargs)\n\n    def _import_pyopensslcontext(self):\n        global PyOpenSSLContext\n\n        if requests.__build__ < 0x021200:\n            PyOpenSSLContext = None\n        else:\n            try:\n                from requests.packages.urllib3.contrib.pyopenssl \\\n                        import PyOpenSSLContext\n            except ImportError:\n                try:\n                    from urllib3.contrib.pyopenssl import PyOpenSSLContext\n                except ImportError:\n                    PyOpenSSLContext = None\n\n    def _check_version(self):\n        if PyOpenSSLContext is None:\n            raise exc.VersionMismatchError(\n                \"The X509Adapter requires at least Requests 2.12.0 to be \"\n                \"installed. Version {} was found instead.\".format(\n                    requests.__version__\n                )\n            )\n\n\ndef check_cert_dates(cert):\n    \"\"\"Verify that the supplied client cert is not invalid.\"\"\"\n\n    now = datetime.now(timezone.utc)\n    if cert.not_valid_after_utc < now or cert.not_valid_before_utc > now:\n        raise ValueError('Client certificate expired: Not After: '\n                         '{:%Y-%m-%d %H:%M:%SZ} '\n                         'Not Before: {:%Y-%m-%d %H:%M:%SZ}'\n                         .format(cert.not_valid_after, cert.not_valid_before))\n\n\ndef create_ssl_context(cert_byes, pk_bytes, password=None,\n                       encoding=Encoding.PEM):\n    \"\"\"Create an SSL Context with the supplied cert/password.\n\n    :param cert_bytes array of bytes containing the cert encoded\n           using the method supplied in the ``encoding`` parameter\n    :param pk_bytes array of bytes containing the private key encoded\n           using the method supplied in the ``encoding`` parameter\n    :param password array of bytes containing the passphrase to be used\n           with the supplied private key. None if unencrypted.\n           Defaults to None.\n    :param encoding ``cryptography.hazmat.primitives.serialization.Encoding``\n            details the encoding method used on the ``cert_bytes``  and\n            ``pk_bytes`` parameters. Can be either PEM or DER.\n            Defaults to PEM.\n    \"\"\"\n    backend = default_backend()\n\n    cert = None\n    key = None\n    if encoding == Encoding.PEM:\n        cert = x509.load_pem_x509_certificate(cert_byes, backend)\n        key = load_pem_private_key(pk_bytes, password, backend)\n    elif encoding == Encoding.DER:\n        cert = x509.load_der_x509_certificate(cert_byes, backend)\n        key = load_der_private_key(pk_bytes, password, backend)\n    else:\n        raise ValueError('Invalid encoding provided: Must be PEM or DER')\n\n    if not (cert and key):\n        raise ValueError('Cert and key could not be parsed from '\n                         'provided data')\n    check_cert_dates(cert)\n    ssl_context = PyOpenSSLContext(PROTOCOL)\n    ssl_context._ctx.use_certificate(X509.from_cryptography(cert))\n    ssl_context._ctx.use_privatekey(PKey.from_cryptography_key(key))\n    return ssl_context\n", "requests_toolbelt/adapters/source.py": "# -*- coding: utf-8 -*-\n\"\"\"\nrequests_toolbelt.source_adapter\n================================\n\nThis file contains an implementation of the SourceAddressAdapter originally\ndemonstrated on the Requests GitHub page.\n\"\"\"\nfrom requests.adapters import HTTPAdapter\n\nfrom .._compat import poolmanager, basestring\n\n\nclass SourceAddressAdapter(HTTPAdapter):\n    \"\"\"\n    A Source Address Adapter for Python Requests that enables you to choose the\n    local address to bind to. This allows you to send your HTTP requests from a\n    specific interface and IP address.\n\n    Two address formats are accepted. The first is a string: this will set the\n    local IP address to the address given in the string, and will also choose a\n    semi-random high port for the local port number.\n\n    The second is a two-tuple of the form (ip address, port): for example,\n    ``('10.10.10.10', 8999)``. This will set the local IP address to the first\n    element, and the local port to the second element. If ``0`` is used as the\n    port number, a semi-random high port will be selected.\n\n    .. warning:: Setting an explicit local port can have negative interactions\n                 with connection-pooling in Requests: in particular, it risks\n                 the possibility of getting \"Address in use\" errors. The\n                 string-only argument is generally preferred to the tuple-form.\n\n    Example usage:\n\n    .. code-block:: python\n\n        import requests\n        from requests_toolbelt.adapters.source import SourceAddressAdapter\n\n        s = requests.Session()\n        s.mount('http://', SourceAddressAdapter('10.10.10.10'))\n        s.mount('https://', SourceAddressAdapter(('10.10.10.10', 8999)))\n    \"\"\"\n    def __init__(self, source_address, **kwargs):\n        if isinstance(source_address, basestring):\n            self.source_address = (source_address, 0)\n        elif isinstance(source_address, tuple):\n            self.source_address = source_address\n        else:\n            raise TypeError(\n                \"source_address must be IP address string or (ip, port) tuple\"\n            )\n\n        super(SourceAddressAdapter, self).__init__(**kwargs)\n\n    def init_poolmanager(self, connections, maxsize, block=False):\n        self.poolmanager = poolmanager.PoolManager(\n            num_pools=connections,\n            maxsize=maxsize,\n            block=block,\n            source_address=self.source_address)\n\n    def proxy_manager_for(self, *args, **kwargs):\n        kwargs['source_address'] = self.source_address\n        return super(SourceAddressAdapter, self).proxy_manager_for(\n            *args, **kwargs)\n", "requests_toolbelt/adapters/fingerprint.py": "# -*- coding: utf-8 -*-\n\"\"\"Submodule containing the implementation for the FingerprintAdapter.\n\nThis file contains an implementation of a Transport Adapter that validates\nthe fingerprints of SSL certificates presented upon connection.\n\"\"\"\nfrom requests.adapters import HTTPAdapter\n\nfrom .._compat import poolmanager\n\n\nclass FingerprintAdapter(HTTPAdapter):\n    \"\"\"\n    A HTTPS Adapter for Python Requests that verifies certificate fingerprints,\n    instead of certificate hostnames.\n\n    Example usage:\n\n    .. code-block:: python\n\n        import requests\n        import ssl\n        from requests_toolbelt.adapters.fingerprint import FingerprintAdapter\n\n        twitter_fingerprint = '...'\n        s = requests.Session()\n        s.mount(\n            'https://twitter.com',\n            FingerprintAdapter(twitter_fingerprint)\n        )\n\n    The fingerprint should be provided as a hexadecimal string, optionally\n    containing colons.\n    \"\"\"\n\n    __attrs__ = HTTPAdapter.__attrs__ + ['fingerprint']\n\n    def __init__(self, fingerprint, **kwargs):\n        self.fingerprint = fingerprint\n\n        super(FingerprintAdapter, self).__init__(**kwargs)\n\n    def init_poolmanager(self, connections, maxsize, block=False):\n        self.poolmanager = poolmanager.PoolManager(\n            num_pools=connections,\n            maxsize=maxsize,\n            block=block,\n            assert_fingerprint=self.fingerprint)\n", "requests_toolbelt/adapters/ssl.py": "# -*- coding: utf-8 -*-\n\"\"\"\n\nrequests_toolbelt.ssl_adapter\n=============================\n\nThis file contains an implementation of the SSLAdapter originally demonstrated\nin this blog post:\nhttps://lukasa.co.uk/2013/01/Choosing_SSL_Version_In_Requests/\n\n\"\"\"\nimport requests\n\nfrom requests.adapters import HTTPAdapter\n\nfrom .._compat import poolmanager\n\n\nclass SSLAdapter(HTTPAdapter):\n    \"\"\"\n    A HTTPS Adapter for Python Requests that allows the choice of the SSL/TLS\n    version negotiated by Requests. This can be used either to enforce the\n    choice of high-security TLS versions (where supported), or to work around\n    misbehaving servers that fail to correctly negotiate the default TLS\n    version being offered.\n\n    Example usage:\n\n        >>> import requests\n        >>> import ssl\n        >>> from requests_toolbelt import SSLAdapter\n        >>> s = requests.Session()\n        >>> s.mount('https://', SSLAdapter(ssl.PROTOCOL_TLSv1))\n\n    You can replace the chosen protocol with any that are available in the\n    default Python SSL module. All subsequent requests that match the adapter\n    prefix will use the chosen SSL version instead of the default.\n\n    This adapter will also attempt to change the SSL/TLS version negotiated by\n    Requests when using a proxy. However, this may not always be possible:\n    prior to Requests v2.4.0 the adapter did not have access to the proxy setup\n    code. In earlier versions of Requests, this adapter will not function\n    properly when used with proxies.\n    \"\"\"\n\n    __attrs__ = HTTPAdapter.__attrs__ + ['ssl_version']\n\n    def __init__(self, ssl_version=None, **kwargs):\n        self.ssl_version = ssl_version\n\n        super(SSLAdapter, self).__init__(**kwargs)\n\n    def init_poolmanager(self, connections, maxsize, block=False):\n        self.poolmanager = poolmanager.PoolManager(\n            num_pools=connections,\n            maxsize=maxsize,\n            block=block,\n            ssl_version=self.ssl_version)\n\n    if requests.__build__ >= 0x020400:\n        # Earlier versions of requests either don't have this method or, worse,\n        # don't allow passing arbitrary keyword arguments. As a result, only\n        # conditionally define this method.\n        def proxy_manager_for(self, *args, **kwargs):\n            kwargs['ssl_version'] = self.ssl_version\n            return super(SSLAdapter, self).proxy_manager_for(*args, **kwargs)\n", "requests_toolbelt/adapters/socket_options.py": "# -*- coding: utf-8 -*-\n\"\"\"The implementation of the SocketOptionsAdapter.\"\"\"\nimport socket\nimport warnings\nimport sys\n\nimport requests\nfrom requests import adapters\n\nfrom .._compat import connection\nfrom .._compat import poolmanager\nfrom .. import exceptions as exc\n\n\nclass SocketOptionsAdapter(adapters.HTTPAdapter):\n    \"\"\"An adapter for requests that allows users to specify socket options.\n\n    Since version 2.4.0 of requests, it is possible to specify a custom list\n    of socket options that need to be set before establishing the connection.\n\n    Example usage::\n\n        >>> import socket\n        >>> import requests\n        >>> from requests_toolbelt.adapters import socket_options\n        >>> s = requests.Session()\n        >>> opts = [(socket.IPPROTO_TCP, socket.TCP_NODELAY, 0)]\n        >>> adapter = socket_options.SocketOptionsAdapter(socket_options=opts)\n        >>> s.mount('http://', adapter)\n\n    You can also take advantage of the list of default options on this class\n    to keep using the original options in addition to your custom options. In\n    that case, ``opts`` might look like::\n\n        >>> opts = socket_options.SocketOptionsAdapter.default_options + opts\n\n    \"\"\"\n\n    if connection is not None:\n        default_options = getattr(\n            connection.HTTPConnection,\n            'default_socket_options',\n            [(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)]\n        )\n    else:\n        default_options = []\n        warnings.warn(exc.RequestsVersionTooOld,\n                      \"This version of Requests is only compatible with a \"\n                      \"version of urllib3 which is too old to support \"\n                      \"setting options on a socket. This adapter is \"\n                      \"functionally useless.\")\n\n    def __init__(self, **kwargs):\n        self.socket_options = kwargs.pop('socket_options',\n                                         self.default_options)\n\n        super(SocketOptionsAdapter, self).__init__(**kwargs)\n\n    def init_poolmanager(self, connections, maxsize, block=False):\n        if requests.__build__ >= 0x020400:\n            # NOTE(Ian): Perhaps we should raise a warning\n            self.poolmanager = poolmanager.PoolManager(\n                num_pools=connections,\n                maxsize=maxsize,\n                block=block,\n                socket_options=self.socket_options\n            )\n        else:\n            super(SocketOptionsAdapter, self).init_poolmanager(\n                connections, maxsize, block\n            )\n\n\nclass TCPKeepAliveAdapter(SocketOptionsAdapter):\n    \"\"\"An adapter for requests that turns on TCP Keep-Alive by default.\n\n    The adapter sets 4 socket options:\n\n    - ``SOL_SOCKET`` ``SO_KEEPALIVE`` - This turns on TCP Keep-Alive\n    - ``IPPROTO_TCP`` ``TCP_KEEPINTVL`` 20 - Sets the keep alive interval\n    - ``IPPROTO_TCP`` ``TCP_KEEPCNT`` 5 - Sets the number of keep alive probes\n    - ``IPPROTO_TCP`` ``TCP_KEEPIDLE`` 60 - Sets the keep alive time if the\n      socket library has the ``TCP_KEEPIDLE`` constant\n\n    The latter three can be overridden by keyword arguments (respectively):\n\n    - ``interval``\n    - ``count``\n    - ``idle``\n\n    You can use this adapter like so::\n\n       >>> from requests_toolbelt.adapters import socket_options\n       >>> tcp = socket_options.TCPKeepAliveAdapter(idle=120, interval=10)\n       >>> s = requests.Session()\n       >>> s.mount('http://', tcp)\n\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        socket_options = kwargs.pop('socket_options',\n                                    SocketOptionsAdapter.default_options)\n        idle = kwargs.pop('idle', 60)\n        interval = kwargs.pop('interval', 20)\n        count = kwargs.pop('count', 5)\n        socket_options = socket_options + [\n            (socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)\n        ]\n\n        # NOTE(Ian): OSX does not have these constants defined, so we\n        # set them conditionally.\n        if getattr(socket, 'TCP_KEEPINTVL', None) is not None:\n            socket_options += [(socket.IPPROTO_TCP, socket.TCP_KEEPINTVL,\n                                interval)]\n        elif sys.platform == 'darwin':\n            # On OSX, TCP_KEEPALIVE from netinet/tcp.h is not exported\n            # by python's socket module\n            TCP_KEEPALIVE = getattr(socket, 'TCP_KEEPALIVE', 0x10)\n            socket_options += [(socket.IPPROTO_TCP, TCP_KEEPALIVE, interval)]\n\n        if getattr(socket, 'TCP_KEEPCNT', None) is not None:\n            socket_options += [(socket.IPPROTO_TCP, socket.TCP_KEEPCNT, count)]\n\n        if getattr(socket, 'TCP_KEEPIDLE', None) is not None:\n            socket_options += [(socket.IPPROTO_TCP, socket.TCP_KEEPIDLE, idle)]\n\n        super(TCPKeepAliveAdapter, self).__init__(\n            socket_options=socket_options, **kwargs\n        )\n", "requests_toolbelt/adapters/__init__.py": "# -*- coding: utf-8 -*-\n\"\"\"\nrequests-toolbelt.adapters\n==========================\n\nSee https://toolbelt.readthedocs.io/ for documentation\n\n:copyright: (c) 2014 by Ian Cordasco and Cory Benfield\n:license: Apache v2.0, see LICENSE for more details\n\"\"\"\n\nfrom .ssl import SSLAdapter\nfrom .source import SourceAddressAdapter\n\n__all__ = ['SSLAdapter', 'SourceAddressAdapter']\n"}