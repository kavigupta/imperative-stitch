{"cli.py": "\"\"\"\nThis is a minimal file intended to be run by users to help them manage the autogpt projects.\n\nIf you want to contribute, please use only libraries that come as part of Python.\nTo ensure efficiency, add the imports to the functions so only what is needed is imported.\n\"\"\"\ntry:\n    import click\nexcept ImportError:\n    import os\n\n    os.system(\"pip3 install click\")\n    import click\n\n\n@click.group()\ndef cli():\n    pass\n\n\n@cli.command()\ndef setup():\n    \"\"\"Installs dependencies needed for your system. Works with Linux, MacOS and Windows WSL.\"\"\"\n    import os\n    import subprocess\n\n    click.echo(\n        click.style(\n            \"\"\"\n       d8888          888             .d8888b.  8888888b. 88888888888 \n      d88888          888            d88P  Y88b 888   Y88b    888     \n     d88P888          888            888    888 888    888    888     \n    d88P 888 888  888 888888 .d88b.  888        888   d88P    888     \n   d88P  888 888  888 888   d88\"\"88b 888  88888 8888888P\"     888     \n  d88P   888 888  888 888   888  888 888    888 888           888     \n d8888888888 Y88b 888 Y88b. Y88..88P Y88b  d88P 888           888     \nd88P     888  \"Y88888  \"Y888 \"Y88P\"   \"Y8888P88 888           888     \n                                                                                                                                       \n\"\"\",\n            fg=\"green\",\n        )\n    )\n\n    script_dir = os.path.dirname(os.path.realpath(__file__))\n    setup_script = os.path.join(script_dir, \"setup.sh\")\n    install_error = False\n    if os.path.exists(setup_script):\n        click.echo(click.style(\"\ud83d\ude80 Setup initiated...\\n\", fg=\"green\"))\n        try:\n            subprocess.check_call([setup_script], cwd=script_dir)\n        except subprocess.CalledProcessError:\n            click.echo(\n                click.style(\"\u274c There was an issue with the installation.\", fg=\"red\")\n            )\n            install_error = True\n    else:\n        click.echo(\n            click.style(\n                \"\u274c Error: setup.sh does not exist in the current directory.\", fg=\"red\"\n            )\n        )\n        install_error = True\n\n    if install_error:\n        click.echo(\n            click.style(\n                \"\\n\\n\ud83d\udd34 If you need help, please raise a ticket on GitHub at https://github.com/Significant-Gravitas/AutoGPT/issues\\n\\n\",\n                fg=\"magenta\",\n                bold=True,\n            )\n        )\n\n\n@cli.group()\ndef agent():\n    \"\"\"Commands to create, start and stop agents\"\"\"\n    pass\n\n\n@agent.command()\n@click.argument(\"agent_name\")\ndef create(agent_name: str):\n    \"\"\"Create's a new agent with the agent name provided\"\"\"\n    import os\n    import re\n    import shutil\n\n    if not re.match(r\"\\w*$\", agent_name):\n        click.echo(\n            click.style(\n                f\"\ud83d\ude1e Agent name '{agent_name}' is not valid. It should not contain spaces or special characters other than -_\",\n                fg=\"red\",\n            )\n        )\n        return\n    try:\n        new_agent_dir = f\"./agents/{agent_name}\"\n        new_agent_name = f\"{agent_name.lower()}.json\"\n\n        if not os.path.exists(new_agent_dir):\n            shutil.copytree(\"./forge\", new_agent_dir)\n            click.echo(\n                click.style(\n                    f\"\ud83c\udf89 New agent '{agent_name}' created. The code for your new agent is in: agents/{agent_name}\",\n                    fg=\"green\",\n                )\n            )\n        else:\n            click.echo(\n                click.style(\n                    f\"\ud83d\ude1e Agent '{agent_name}' already exists. Enter a different name for your agent, the name needs to be unique regardless of case\",\n                    fg=\"red\",\n                )\n            )\n    except Exception as e:\n        click.echo(click.style(f\"\ud83d\ude22 An error occurred: {e}\", fg=\"red\"))\n\n\n@agent.command()\n@click.argument(\"agent_name\")\n@click.option(\n    \"--no-setup\",\n    is_flag=True,\n    help=\"Disables running the setup script before starting the agent\",\n)\ndef start(agent_name: str, no_setup: bool):\n    \"\"\"Start agent command\"\"\"\n    import os\n    import subprocess\n\n    script_dir = os.path.dirname(os.path.realpath(__file__))\n    agent_dir = os.path.join(\n        script_dir,\n        f\"agents/{agent_name}\"\n        if agent_name not in [\"autogpt\", \"forge\"]\n        else agent_name,\n    )\n    run_command = os.path.join(agent_dir, \"run\")\n    run_bench_command = os.path.join(agent_dir, \"run_benchmark\")\n    if (\n        os.path.exists(agent_dir)\n        and os.path.isfile(run_command)\n        and os.path.isfile(run_bench_command)\n    ):\n        os.chdir(agent_dir)\n        if not no_setup:\n            click.echo(f\"\u231b Running setup for agent '{agent_name}'...\")\n            setup_process = subprocess.Popen([\"./setup\"], cwd=agent_dir)\n            setup_process.wait()\n            click.echo()\n\n        # FIXME: Doesn't work: Command not found: agbenchmark\n        # subprocess.Popen([\"./run_benchmark\", \"serve\"], cwd=agent_dir)\n        # click.echo(\"\u231b (Re)starting benchmark server...\")\n        # wait_until_conn_ready(8080)\n        # click.echo()\n\n        subprocess.Popen([\"./run\"], cwd=agent_dir)\n        click.echo(f\"\u231b (Re)starting agent '{agent_name}'...\")\n        wait_until_conn_ready(8000)\n        click.echo(\"\u2705 Agent application started and available on port 8000\")\n    elif not os.path.exists(agent_dir):\n        click.echo(\n            click.style(\n                f\"\ud83d\ude1e Agent '{agent_name}' does not exist. Please create the agent first.\",\n                fg=\"red\",\n            )\n        )\n    else:\n        click.echo(\n            click.style(\n                f\"\ud83d\ude1e Run command does not exist in the agent '{agent_name}' directory.\",\n                fg=\"red\",\n            )\n        )\n\n\n@agent.command()\ndef stop():\n    \"\"\"Stop agent command\"\"\"\n    import os\n    import signal\n    import subprocess\n\n    try:\n        pids = subprocess.check_output([\"lsof\", \"-t\", \"-i\", \":8000\"]).split()\n        if isinstance(pids, int):\n            os.kill(int(pids), signal.SIGTERM)\n        else:\n            for pid in pids:\n                os.kill(int(pid), signal.SIGTERM)\n    except subprocess.CalledProcessError:\n        click.echo(\"No process is running on port 8000\")\n\n    try:\n        pids = int(subprocess.check_output([\"lsof\", \"-t\", \"-i\", \":8080\"]))\n        if isinstance(pids, int):\n            os.kill(int(pids), signal.SIGTERM)\n        else:\n            for pid in pids:\n                os.kill(int(pid), signal.SIGTERM)\n    except subprocess.CalledProcessError:\n        click.echo(\"No process is running on port 8080\")\n\n\n@agent.command()\ndef list():\n    \"\"\"List agents command\"\"\"\n    import os\n\n    try:\n        agents_dir = \"./agents\"\n        agents_list = [\n            d\n            for d in os.listdir(agents_dir)\n            if os.path.isdir(os.path.join(agents_dir, d))\n        ]\n        if os.path.isdir(\"./autogpt\"):\n            agents_list.append(\"autogpt\")\n        if agents_list:\n            click.echo(click.style(\"Available agents: \ud83e\udd16\", fg=\"green\"))\n            for agent in agents_list:\n                click.echo(click.style(f\"\\t\ud83d\udc19 {agent}\", fg=\"blue\"))\n        else:\n            click.echo(click.style(\"No agents found \ud83d\ude1e\", fg=\"red\"))\n    except FileNotFoundError:\n        click.echo(click.style(\"The agents directory does not exist \ud83d\ude22\", fg=\"red\"))\n    except Exception as e:\n        click.echo(click.style(f\"An error occurred: {e} \ud83d\ude22\", fg=\"red\"))\n\n\n@cli.group()\ndef benchmark():\n    \"\"\"Commands to start the benchmark and list tests and categories\"\"\"\n    pass\n\n\n@benchmark.command(\n    context_settings=dict(\n        ignore_unknown_options=True,\n    )\n)\n@click.argument(\"agent_name\")\n@click.argument(\"subprocess_args\", nargs=-1, type=click.UNPROCESSED)\ndef start(agent_name, subprocess_args):\n    \"\"\"Starts the benchmark command\"\"\"\n    import os\n    import subprocess\n\n    script_dir = os.path.dirname(os.path.realpath(__file__))\n    agent_dir = os.path.join(\n        script_dir,\n        f\"agents/{agent_name}\"\n        if agent_name not in [\"autogpt\", \"forge\"]\n        else agent_name,\n    )\n    benchmark_script = os.path.join(agent_dir, \"run_benchmark\")\n    if os.path.exists(agent_dir) and os.path.isfile(benchmark_script):\n        os.chdir(agent_dir)\n        subprocess.Popen([benchmark_script, *subprocess_args], cwd=agent_dir)\n        click.echo(\n            click.style(\n                f\"\ud83d\ude80 Running benchmark for '{agent_name}' with subprocess arguments: {' '.join(subprocess_args)}\",\n                fg=\"green\",\n            )\n        )\n    else:\n        click.echo(\n            click.style(\n                f\"\ud83d\ude1e Agent '{agent_name}' does not exist. Please create the agent first.\",\n                fg=\"red\",\n            )\n        )\n\n\n@benchmark.group(name=\"categories\")\ndef benchmark_categories():\n    \"\"\"Benchmark categories group command\"\"\"\n    pass\n\n\n@benchmark_categories.command(name=\"list\")\ndef benchmark_categories_list():\n    \"\"\"List benchmark categories command\"\"\"\n    import glob\n    import json\n    import os\n\n    categories = set()\n\n    # Get the directory of this file\n    this_dir = os.path.dirname(os.path.abspath(__file__))\n\n    glob_path = os.path.join(\n        this_dir, \"./benchmark/agbenchmark/challenges/**/[!deprecated]*/data.json\"\n    )\n    # Use it as the base for the glob pattern, excluding 'deprecated' directory\n    for data_file in glob.glob(glob_path, recursive=True):\n        if \"deprecated\" not in data_file:\n            with open(data_file, \"r\") as f:\n                try:\n                    data = json.load(f)\n                    categories.update(data.get(\"category\", []))\n                except json.JSONDecodeError:\n                    print(f\"Error: {data_file} is not a valid JSON file.\")\n                    continue\n                except IOError:\n                    print(f\"IOError: file could not be read: {data_file}\")\n                    continue\n\n    if categories:\n        click.echo(click.style(\"Available categories: \ud83d\udcda\", fg=\"green\"))\n        for category in categories:\n            click.echo(click.style(f\"\\t\ud83d\udcd6 {category}\", fg=\"blue\"))\n    else:\n        click.echo(click.style(\"No categories found \ud83d\ude1e\", fg=\"red\"))\n\n\n@benchmark.group(name=\"tests\")\ndef benchmark_tests():\n    \"\"\"Benchmark tests group command\"\"\"\n    pass\n\n\n@benchmark_tests.command(name=\"list\")\ndef benchmark_tests_list():\n    \"\"\"List benchmark tests command\"\"\"\n    import glob\n    import json\n    import os\n    import re\n\n    tests = {}\n\n    # Get the directory of this file\n    this_dir = os.path.dirname(os.path.abspath(__file__))\n\n    glob_path = os.path.join(\n        this_dir, \"./benchmark/agbenchmark/challenges/**/[!deprecated]*/data.json\"\n    )\n    # Use it as the base for the glob pattern, excluding 'deprecated' directory\n    for data_file in glob.glob(glob_path, recursive=True):\n        if \"deprecated\" not in data_file:\n            with open(data_file, \"r\") as f:\n                try:\n                    data = json.load(f)\n                    category = data.get(\"category\", [])\n                    test_name = data.get(\"name\", \"\")\n                    if category and test_name:\n                        if category[0] not in tests:\n                            tests[category[0]] = []\n                        tests[category[0]].append(test_name)\n                except json.JSONDecodeError:\n                    print(f\"Error: {data_file} is not a valid JSON file.\")\n                    continue\n                except IOError:\n                    print(f\"IOError: file could not be read: {data_file}\")\n                    continue\n\n    if tests:\n        click.echo(click.style(\"Available tests: \ud83d\udcda\", fg=\"green\"))\n        for category, test_list in tests.items():\n            click.echo(click.style(f\"\\t\ud83d\udcd6 {category}\", fg=\"blue\"))\n            for test in sorted(test_list):\n                test_name = (\n                    \" \".join(word for word in re.split(\"([A-Z][a-z]*)\", test) if word)\n                    .replace(\"_\", \"\")\n                    .replace(\"C L I\", \"CLI\")\n                    .replace(\"  \", \" \")\n                )\n                test_name_padded = f\"{test_name:<40}\"\n                click.echo(click.style(f\"\\t\\t\ud83d\udd2c {test_name_padded} - {test}\", fg=\"cyan\"))\n    else:\n        click.echo(click.style(\"No tests found \ud83d\ude1e\", fg=\"red\"))\n\n\n@benchmark_tests.command(name=\"details\")\n@click.argument(\"test_name\")\ndef benchmark_tests_details(test_name):\n    \"\"\"Benchmark test details command\"\"\"\n    import glob\n    import json\n    import os\n\n    # Get the directory of this file\n    this_dir = os.path.dirname(os.path.abspath(__file__))\n\n    glob_path = os.path.join(\n        this_dir, \"./benchmark/agbenchmark/challenges/**/[!deprecated]*/data.json\"\n    )\n    # Use it as the base for the glob pattern, excluding 'deprecated' directory\n    for data_file in glob.glob(glob_path, recursive=True):\n        with open(data_file, \"r\") as f:\n            try:\n                data = json.load(f)\n                if data.get(\"name\") == test_name:\n                    click.echo(\n                        click.style(\n                            f\"\\n{data.get('name')}\\n{'-'*len(data.get('name'))}\\n\",\n                            fg=\"blue\",\n                        )\n                    )\n                    click.echo(\n                        click.style(\n                            f\"\\tCategory:  {', '.join(data.get('category'))}\",\n                            fg=\"green\",\n                        )\n                    )\n                    click.echo(click.style(f\"\\tTask:  {data.get('task')}\", fg=\"green\"))\n                    click.echo(\n                        click.style(\n                            f\"\\tDependencies:  {', '.join(data.get('dependencies')) if data.get('dependencies') else 'None'}\",\n                            fg=\"green\",\n                        )\n                    )\n                    click.echo(\n                        click.style(f\"\\tCutoff:  {data.get('cutoff')}\\n\", fg=\"green\")\n                    )\n                    click.echo(\n                        click.style(\"\\tTest Conditions\\n\\t-------\", fg=\"magenta\")\n                    )\n                    click.echo(\n                        click.style(\n                            f\"\\t\\tAnswer: {data.get('ground').get('answer')}\",\n                            fg=\"magenta\",\n                        )\n                    )\n                    click.echo(\n                        click.style(\n                            f\"\\t\\tShould Contain: {', '.join(data.get('ground').get('should_contain'))}\",\n                            fg=\"magenta\",\n                        )\n                    )\n                    click.echo(\n                        click.style(\n                            f\"\\t\\tShould Not Contain: {', '.join(data.get('ground').get('should_not_contain'))}\",\n                            fg=\"magenta\",\n                        )\n                    )\n                    click.echo(\n                        click.style(\n                            f\"\\t\\tFiles: {', '.join(data.get('ground').get('files'))}\",\n                            fg=\"magenta\",\n                        )\n                    )\n                    click.echo(\n                        click.style(\n                            f\"\\t\\tEval: {data.get('ground').get('eval').get('type')}\\n\",\n                            fg=\"magenta\",\n                        )\n                    )\n                    click.echo(click.style(\"\\tInfo\\n\\t-------\", fg=\"yellow\"))\n                    click.echo(\n                        click.style(\n                            f\"\\t\\tDifficulty: {data.get('info').get('difficulty')}\",\n                            fg=\"yellow\",\n                        )\n                    )\n                    click.echo(\n                        click.style(\n                            f\"\\t\\tDescription: {data.get('info').get('description')}\",\n                            fg=\"yellow\",\n                        )\n                    )\n                    click.echo(\n                        click.style(\n                            f\"\\t\\tSide Effects: {', '.join(data.get('info').get('side_effects'))}\",\n                            fg=\"yellow\",\n                        )\n                    )\n                    break\n\n            except json.JSONDecodeError:\n                print(f\"Error: {data_file} is not a valid JSON file.\")\n                continue\n            except IOError:\n                print(f\"IOError: file could not be read: {data_file}\")\n                continue\n\n\ndef wait_until_conn_ready(port: int = 8000, timeout: int = 30):\n    \"\"\"\n    Polls localhost:{port} until it is available for connections\n\n    Params:\n        port: The port for which to wait until it opens\n        timeout: Timeout in seconds; maximum amount of time to wait\n\n    Raises:\n        TimeoutError: If the timeout (seconds) expires before the port opens\n    \"\"\"\n    import socket\n    import time\n\n    start = time.time()\n    while True:\n        time.sleep(0.5)\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n            if s.connect_ex((\"localhost\", port)) == 0:\n                break\n        if time.time() > start + timeout:\n            raise TimeoutError(f\"Port {port} did not open within {timeout} seconds\")\n\n\nif __name__ == \"__main__\":\n    cli()\n", "benchmark/reports/send_to_googledrive.py": "import base64\nimport json\nimport os\nimport re\nfrom datetime import datetime, timedelta\n\nimport gspread\nimport pandas as pd\nfrom dotenv import load_dotenv\nfrom oauth2client.service_account import ServiceAccountCredentials\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Get the base64 string from the environment variable\nbase64_creds = os.getenv(\"GDRIVE_BASE64\")\n\nif base64_creds is None:\n    raise ValueError(\"The GDRIVE_BASE64 environment variable is not set\")\n\n# Decode the base64 string into bytes\ncreds_bytes = base64.b64decode(base64_creds)\n\n# Convert the bytes into a string\ncreds_string = creds_bytes.decode(\"utf-8\")\n\n# Parse the string into a JSON object\ncreds_info = json.loads(creds_string)\n\n# Define the base directory containing JSON files\nbase_dir = \"reports\"\n\n# Get the current working directory\ncurrent_dir = os.getcwd()\n\n# Check if the current directory ends with 'reports'\nif current_dir.endswith(\"reports\"):\n    base_dir = \"/\"\nelse:\n    base_dir = \"reports\"\n\n# Create a list to store each row of data\nrows = []\n\n\ndef process_test(\n    test_name: str, test_info: dict, agent_name: str, common_data: dict\n) -> None:\n    \"\"\"Recursive function to process test data.\"\"\"\n    parts = test_name.split(\"_\", 1)  # Split by underscore only once\n    test_suite = parts[0] if len(parts) > 1 else None\n\n    # transform array into string with | as separator\n    separator = \"|\"\n    categories = separator.join(\n        test_info.get(\"category\", []),\n    )\n\n    row = {\n        \"Agent\": agent_name,\n        \"Command\": common_data.get(\"command\", \"\"),\n        \"Completion Time\": common_data.get(\"completion_time\", \"\"),\n        \"Benchmark Start Time\": common_data.get(\"benchmark_start_time\", \"\"),\n        \"Total Run Time\": common_data.get(\"metrics\", {}).get(\"run_time\", \"\"),\n        \"Highest Difficulty\": common_data.get(\"metrics\", {}).get(\n            \"highest_difficulty\", \"\"\n        ),\n        \"Workspace\": common_data.get(\"config\", {}).get(\"workspace\", \"\"),\n        \"Test Name\": test_name,\n        \"Data Path\": test_info.get(\"data_path\", \"\"),\n        \"Is Regression\": test_info.get(\"is_regression\", \"\"),\n        \"Difficulty\": test_info.get(\"metrics\", {}).get(\"difficulty\", \"\"),\n        \"Success\": test_info.get(\"metrics\", {}).get(\"success\", \"\"),\n        \"Success %\": test_info.get(\"metrics\", {}).get(\"success_%\", \"\"),\n        \"Non mock success %\": test_info.get(\"metrics\", {}).get(\n            \"non_mock_success_%\", \"\"\n        ),\n        \"Run Time\": test_info.get(\"metrics\", {}).get(\"run_time\", \"\"),\n        \"Benchmark Git Commit Sha\": common_data.get(\"benchmark_git_commit_sha\", None),\n        \"Agent Git Commit Sha\": common_data.get(\"agent_git_commit_sha\", None),\n        \"Cost\": test_info.get(\"metrics\", {}).get(\"cost\", \"\"),\n        \"Attempted\": test_info.get(\"metrics\", {}).get(\"attempted\", \"\"),\n        \"Test Suite\": test_suite,\n        \"Category\": categories,\n        \"Task\": test_info.get(\"task\", \"\"),\n        \"Answer\": test_info.get(\"answer\", \"\"),\n        \"Description\": test_info.get(\"description\", \"\"),\n        \"Fail Reason\": test_info.get(\"metrics\", {}).get(\"fail_reason\", \"\"),\n        \"Reached Cutoff\": test_info.get(\"reached_cutoff\", \"\"),\n    }\n\n    rows.append(row)\n\n    # Check for nested tests and process them if present\n    nested_tests = test_info.get(\"tests\")\n    if nested_tests:\n        for nested_test_name, nested_test_info in nested_tests.items():\n            process_test(nested_test_name, nested_test_info, agent_name, common_data)\n\n\n# Usage:\n\n\n# Loop over each directory in the base directory\nfor agent_dir in os.listdir(base_dir):\n    agent_dir_path = os.path.join(base_dir, agent_dir)\n\n    # Ensure the agent_dir_path is a directory\n    if os.path.isdir(agent_dir_path):\n        # Loop over each sub-directory in the agent directory (e.g., \"folder49_07-28-03-53\")\n        for report_folder in os.listdir(agent_dir_path):\n            report_folder_path = os.path.join(agent_dir_path, report_folder)\n\n            # Ensure the report_folder_path is a directory\n            if os.path.isdir(report_folder_path):\n                # Check for a file named \"report.json\" in the sub-directory\n                report_path = os.path.join(report_folder_path, \"report.json\")\n\n                if os.path.exists(report_path):\n                    # Load the JSON data from the file\n                    with open(report_path, \"r\") as f:\n                        data = json.load(f)\n                    benchmark_start_time = data.get(\"benchmark_start_time\", \"\")\n\n                    # Check if benchmark_start_time complies with the required format\n                    pattern = re.compile(r\"\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\+00:00\")\n                    if not pattern.fullmatch(benchmark_start_time):\n                        continue  # Skip processing this report if the date is not in the correct format\n\n                    # Parse the benchmark_start_time to a datetime object\n                    benchmark_datetime = datetime.strptime(\n                        benchmark_start_time, \"%Y-%m-%dT%H:%M:%S+00:00\"\n                    )\n\n                    # Check if benchmark_start_time is older than 3 days\n                    current_datetime = datetime.utcnow()\n                    if current_datetime - benchmark_datetime > timedelta(days=3):\n                        continue  # Skip processing this report if it's more than 3 days old\n\n                    # Loop through each test\n                    for test_name, test_info in data[\"tests\"].items():\n                        process_test(test_name, test_info, agent_dir, data)\n\n# Convert the list of rows into a DataFrame\ndf = pd.DataFrame(rows)\n\n# Define the scope\nscope = [\n    \"https://spreadsheets.google.com/feeds\",\n    \"https://www.googleapis.com/auth/drive\",\n]\n\n# Add your service account credentials\ncreds = ServiceAccountCredentials.from_json_keyfile_dict(creds_info, scope)\n\n# Authorize the clientsheet\nclient = gspread.authorize(creds)\n\n# Get the instance of the Spreadsheet\nbranch_name = os.getenv(\"GITHUB_REF_NAME\")\nsheet = client.open(f\"benchmark-{branch_name}\")\n\n# Get the first sheet of the Spreadsheet\nsheet_instance = sheet.get_worksheet(0)\n\n# Convert dataframe to list of lists for uploading to Google Sheets\nvalues = df.values.tolist()\n\n# Prepend the header to the values list\nvalues.insert(0, df.columns.tolist())\n\n# Clear the existing values in the worksheet\nsheet_instance.clear()\n\n# Update the worksheet with the new values\nsheet_instance.append_rows(values)\n", "benchmark/reports/format.py": "#!/usr/bin/env python3\n\nimport click\n\nfrom agbenchmark.reports.processing.report_types import Report\n\n\n@click.command()\n@click.argument(\"report_json_file\", type=click.Path(exists=True, dir_okay=False))\ndef print_markdown_report(report_json_file: str):\n    \"\"\"\n    Generates a Markdown report from a given report.json file.\n\n    :param report_json_file: Path to the report.json file.\n    :return: A string containing the Markdown formatted report.\n    \"\"\"\n    report = Report.parse_file(report_json_file)\n\n    # Header and metadata\n    click.echo(\"# Benchmark Report\")\n    click.echo(f\"- \u231b **Run time:** `{report.metrics.run_time}`\")\n    click.echo(\n        f\"  - **Started at:** `{report.benchmark_start_time[:16].replace('T', '` `')}`\"\n    )\n    if report.completion_time:\n        click.echo(\n            f\"  - **Completed at:** `{report.completion_time[:16].replace('T', '` `')}`\"\n        )\n    if report.metrics.total_cost:\n        click.echo(f\"- \ud83d\udcb8 **Total cost:** `${round(report.metrics.total_cost, 2)}`\")\n    click.echo(\n        f\"- \ud83c\udfc5 **Highest achieved difficulty:** `{report.metrics.highest_difficulty}`\"\n    )\n    click.echo(f\"- \u2699\ufe0f **Command:** `{report.command}`\")\n\n    click.echo()  # spacing\n\n    # Aggregate information\n    successful, failed, unreliable = [], [], []\n    for test in report.tests.values():\n        test.metrics.success_percentage = (\n            rsp\n            if (rsp := test.metrics.success_percentage) is not None\n            else sum(float(r.success or 0) for r in test.results)\n            * 100\n            / len(test.results)\n        )\n        if test.metrics.success_percentage == 100.0:\n            successful.append(test)\n        elif test.metrics.success_percentage == 0.0:\n            failed.append(test)\n        else:\n            unreliable.append(test)\n\n    # Summary\n    click.echo(\"## Summary\")\n    click.echo(f\"- **`{len(successful)}` passed** {'\u2705'*len(successful)}\")\n    click.echo(f\"- **`{len(failed)}` failed** {'\u274c'*len(failed)}\")\n    click.echo(f\"- **`{len(unreliable)}` unreliable** {'\u26a0\ufe0f'*len(unreliable)}\")\n\n    click.echo()  # spacing\n\n    # Test results\n    click.echo(\"## Challenges\")\n    for test_name, test in report.tests.items():\n        click.echo()  # spacing\n\n        result_indicator = (\n            \"\u2705\"\n            if test.metrics.success_percentage == 100.0\n            else \"\u26a0\ufe0f\"\n            if test.metrics.success_percentage > 0\n            else \"\u274c\"\n        )\n        click.echo(\n            f\"### {test_name} {result_indicator if test.metrics.attempted else '\u2754'}\"\n        )\n        click.echo(f\"{test.description}\")\n\n        click.echo()  # spacing\n\n        click.echo(f\"- **Attempted:** {'Yes \ud83d\udc4d' if test.metrics.attempted else 'No \ud83d\udc4e'}\")\n        click.echo(\n            f\"- **Success rate:** {round(test.metrics.success_percentage)}% \"\n            f\"({len([r for r in test.results if r.success])}/{len(test.results)})\"\n        )\n        click.echo(f\"- **Difficulty:** `{test.difficulty}`\")\n        click.echo(f\"- **Categories:** `{'`, `'.join(test.category)}`\")\n        click.echo(\n            f\"<details>\\n<summary><strong>Task</strong> (click to expand)</summary>\\n\\n\"\n            f\"{indent('> ', test.task)}\\n\\n\"\n            f\"Reference answer:\\n{indent('> ', test.answer)}\\n\"\n            \"</details>\"\n        )\n\n        click.echo()  # spacing\n\n        click.echo(\"\\n#### Attempts\")\n        for i, attempt in enumerate(test.results, 1):\n            click.echo(\n                f\"\\n{i}. **{'\u2705 Passed' if attempt.success else '\u274c Failed'}** \"\n                f\"in **{attempt.run_time}** \"\n                f\"and **{quantify('step', attempt.n_steps)}**\\n\"\n            )\n            if attempt.cost is not None:\n                click.echo(f\"   - **Cost:** `${round(attempt.cost, 3)}`\")\n            if attempt.fail_reason:\n                click.echo(\n                    \"   - **Failure reason:**\\n\"\n                    + indent(\"      > \", attempt.fail_reason)\n                    + \"\\n\"\n                )\n            if attempt.steps:\n                click.echo(\n                    indent(\n                        3 * \" \",\n                        \"<details>\\n<summary><strong>Steps</strong></summary>\\n\",\n                    )\n                )\n                for j, step in enumerate(attempt.steps, 1):\n                    click.echo()\n                    click.echo(\n                        indent(3 * \" \", f\"{j}. {indent(3*' ', step.output, False)}\")\n                    )\n                click.echo(\"\\n</details>\")\n\n\ndef indent(indent: str, text: str, prefix_indent: bool = True) -> str:\n    return (indent if prefix_indent else \"\") + text.replace(\"\\n\", \"\\n\" + indent)\n\n\ndef quantify(noun: str, count: int, plural_suffix: str = \"s\") -> str:\n    if count == 1:\n        return f\"{count} {noun}\"\n    return f\"{count} {noun}{plural_suffix}\"\n\n\nif __name__ == \"__main__\":\n    print_markdown_report()\n", "benchmark/reports/json_to_base_64.py": "import base64\nimport json\n\n# Load JSON data from a file\nwith open(\"secrets.json\", \"r\") as f:\n    data = json.load(f)\n\n# Convert the JSON object into a string\njson_string = json.dumps(data)\n\n# Encode the string into bytes\njson_bytes = json_string.encode(\"utf-8\")\n\n# Convert the bytes to a base64 string\nbase64_string = base64.b64encode(json_bytes).decode(\"utf-8\")\n\nprint(base64_string)\n", "benchmark/reports/match_records.py": "import glob\nimport json\nimport os\nfrom typing import Dict, List, Optional, Union\n\nimport pandas as pd\nfrom gql import Client, gql\nfrom gql.transport.aiohttp import AIOHTTPTransport\nfrom pydantic import BaseModel, Field\n\n# from agbenchmark.reports.processing.report_types import Report, SuiteTest\n\n\nclass Metrics(BaseModel):\n    difficulty: str\n    success: bool\n    success_percent: float = Field(..., alias=\"success_%\")\n    run_time: Optional[str] = None\n    fail_reason: Optional[str] = None\n    attempted: Optional[bool] = None\n\n\nclass MetricsOverall(BaseModel):\n    run_time: str\n    highest_difficulty: str\n    percentage: Optional[float] = None\n\n\nclass Test(BaseModel):\n    data_path: str\n    is_regression: bool\n    answer: str\n    description: str\n    metrics: Metrics\n    category: List[str]\n    task: Optional[str] = None\n    reached_cutoff: Optional[bool] = None\n\n\nclass SuiteTest(BaseModel):\n    data_path: str\n    metrics: MetricsOverall\n    tests: Dict[str, Test]\n    category: Optional[List[str]] = None\n    task: Optional[str] = None\n    reached_cutoff: Optional[bool] = None\n\n\nclass Report(BaseModel):\n    command: str\n    completion_time: str\n    benchmark_start_time: str\n    metrics: MetricsOverall\n    tests: Dict[str, Union[Test, SuiteTest]]\n    config: Dict[str, str | dict[str, str]]\n\n\ndef get_reports():\n    # Initialize an empty list to store the report data\n    report_data = []\n\n    # Get the current working directory\n    current_dir = os.getcwd()\n\n    # Check if the current directory ends with 'reports'\n    if current_dir.endswith(\"reports\"):\n        reports_dir = \"/\"\n    else:\n        reports_dir = \"reports\"\n\n    # Iterate over all agent directories in the reports directory\n    for agent_name in os.listdir(reports_dir):\n        if agent_name is None:\n            continue\n        agent_dir = os.path.join(reports_dir, agent_name)\n\n        # Check if the item is a directory (an agent directory)\n        if os.path.isdir(agent_dir):\n            # Construct the path to the report.json file\n            # Get all directories and files, but note that this will also include any file, not just directories.\n            run_dirs = glob.glob(os.path.join(agent_dir, \"*\"))\n\n            # Get all json files starting with 'file'\n            # old_report_files = glob.glob(os.path.join(agent_dir, \"file*.json\"))\n\n            # For each run directory, add the report.json to the end\n            # Only include the path if it's actually a directory\n            report_files = [\n                os.path.join(run_dir, \"report.json\")\n                for run_dir in run_dirs\n                if os.path.isdir(run_dir)\n            ]\n            # old_report_files already contains the full paths, so no need to join again\n            # report_files = report_files + old_report_files\n            for report_file in report_files:\n                # Check if the report.json file exists\n                if os.path.isfile(report_file):\n                    # Open the report.json file\n                    with open(report_file, \"r\") as f:\n                        # Load the JSON data from the file\n                        json_data = json.load(f)\n                        print(f\"Processing {report_file}\")\n                        report = Report.parse_obj(json_data)\n\n                        for test_name, test_data in report.tests.items():\n                            test_json = {\n                                \"agent\": agent_name.lower(),\n                                \"benchmark_start_time\": report.benchmark_start_time,\n                            }\n\n                            if isinstance(test_data, SuiteTest):\n                                if (\n                                    test_data.category\n                                ):  # this means it's a same task test\n                                    test_json[\"challenge\"] = test_name\n                                    test_json[\"attempted\"] = test_data.tests[\n                                        list(test_data.tests.keys())[0]\n                                    ].metrics.attempted\n                                    test_json[\"categories\"] = \", \".join(\n                                        test_data.category\n                                    )\n                                    test_json[\"task\"] = test_data.task\n                                    test_json[\"success\"] = test_data.metrics.percentage\n                                    test_json[\n                                        \"difficulty\"\n                                    ] = test_data.metrics.highest_difficulty\n                                    test_json[\n                                        \"success_%\"\n                                    ] = test_data.metrics.percentage\n                                    test_json[\"run_time\"] = test_data.metrics.run_time\n                                    test_json[\"is_regression\"] = test_data.tests[\n                                        list(test_data.tests.keys())[0]\n                                    ].is_regression\n                                else:  # separate tasks in 1 suite\n                                    for (\n                                        suite_test_name,\n                                        suite_data,\n                                    ) in test_data.tests.items():\n                                        test_json[\"challenge\"] = suite_test_name\n                                        test_json[\n                                            \"attempted\"\n                                        ] = suite_data.metrics.attempted\n                                        test_json[\"categories\"] = \", \".join(\n                                            suite_data.category\n                                        )\n                                        test_json[\"task\"] = suite_data.task\n                                        test_json[\"success\"] = (\n                                            100.0 if suite_data.metrics.success else 0\n                                        )\n                                        test_json[\n                                            \"difficulty\"\n                                        ] = suite_data.metrics.difficulty\n                                        test_json[\n                                            \"success_%\"\n                                        ] = suite_data.metrics.success_percentage\n                                        test_json[\n                                            \"run_time\"\n                                        ] = suite_data.metrics.run_time\n                                        test_json[\n                                            \"is_regression\"\n                                        ] = suite_data.is_regression\n\n                            else:\n                                test_json[\"challenge\"] = test_name\n                                test_json[\"attempted\"] = test_data.metrics.attempted\n                                test_json[\"categories\"] = \", \".join(test_data.category)\n                                test_json[\"task\"] = test_data.task\n                                test_json[\"success\"] = (\n                                    100.0 if test_data.metrics.success else 0\n                                )\n                                test_json[\"difficulty\"] = test_data.metrics.difficulty\n                                test_json[\n                                    \"success_%\"\n                                ] = test_data.metrics.success_percentage\n                                test_json[\"run_time\"] = test_data.metrics.run_time\n                                test_json[\"is_regression\"] = test_data.is_regression\n\n                            report_data.append(test_json)\n\n    return pd.DataFrame(report_data)\n\n\ndef get_helicone_data():\n    helicone_api_key = os.getenv(\"HELICONE_API_KEY\")\n\n    url = \"https://www.helicone.ai/api/graphql\"\n    # Replace <KEY> with your personal access key\n    transport = AIOHTTPTransport(\n        url=url, headers={\"authorization\": f\"Bearer {helicone_api_key}\"}\n    )\n\n    client = Client(transport=transport, fetch_schema_from_transport=True)\n\n    SIZE = 250\n\n    i = 0\n\n    data = []\n    print(\"Fetching data from Helicone\")\n    while True:\n        query = gql(\n            \"\"\"\n            query ExampleQuery($limit: Int, $offset: Int){\n                heliconeRequest(\n                    limit: $limit\n                    offset: $offset\n                ) {\n                    costUSD\n                    prompt\n                    properties{\n                        name\n                        value\n                    }\n                    \n                    requestBody\n                    response\n                    createdAt\n\n                }\n\n                }\n        \"\"\"\n        )\n        print(f\"Fetching {i * SIZE} to {(i + 1) * SIZE} records\")\n        try:\n            result = client.execute(\n                query, variable_values={\"limit\": SIZE, \"offset\": i * SIZE}\n            )\n        except Exception as e:\n            print(f\"Error occurred: {e}\")\n            result = None\n\n        i += 1\n\n        if result:\n            for item in result[\"heliconeRequest\"]:\n                properties = {\n                    prop[\"name\"]: prop[\"value\"] for prop in item[\"properties\"]\n                }\n                data.append(\n                    {\n                        \"createdAt\": item[\"createdAt\"],\n                        \"agent\": properties.get(\"agent\"),\n                        \"costUSD\": item[\"costUSD\"],\n                        \"job_id\": properties.get(\"job_id\"),\n                        \"challenge\": properties.get(\"challenge\"),\n                        \"benchmark_start_time\": properties.get(\"benchmark_start_time\"),\n                        \"prompt\": item[\"prompt\"],\n                        \"response\": item[\"response\"],\n                        \"model\": item[\"requestBody\"].get(\"model\"),\n                        \"request\": item[\"requestBody\"].get(\"messages\"),\n                    }\n                )\n\n        if not result or (len(result[\"heliconeRequest\"]) == 0):\n            print(\"No more results\")\n            break\n\n    df = pd.DataFrame(data)\n    # Drop rows where agent is None\n    df = df.dropna(subset=[\"agent\"])\n\n    # Convert the remaining agent names to lowercase\n    df[\"agent\"] = df[\"agent\"].str.lower()\n\n    return df\n\n\nif os.path.exists(\"raw_reports.pkl\") and os.path.exists(\"raw_helicone.pkl\"):\n    reports_df = pd.read_pickle(\"raw_reports.pkl\")\n    helicone_df = pd.read_pickle(\"raw_helicone.pkl\")\nelse:\n    reports_df = get_reports()\n    reports_df.to_pickle(\"raw_reports.pkl\")\n    helicone_df = get_helicone_data()\n    helicone_df.to_pickle(\"raw_helicone.pkl\")\n\n\ndef try_formats(date_str):\n    formats = [\"%Y-%m-%d-%H:%M\", \"%Y-%m-%dT%H:%M:%S%z\"]\n    for fmt in formats:\n        try:\n            return pd.to_datetime(date_str, format=fmt)\n        except ValueError:\n            pass\n    return None\n\n\nhelicone_df[\"benchmark_start_time\"] = pd.to_datetime(\n    helicone_df[\"benchmark_start_time\"].apply(try_formats), utc=True\n)\nhelicone_df = helicone_df.dropna(subset=[\"benchmark_start_time\"])\nhelicone_df[\"createdAt\"] = pd.to_datetime(\n    helicone_df[\"createdAt\"], unit=\"ms\", origin=\"unix\"\n)\nreports_df[\"benchmark_start_time\"] = pd.to_datetime(\n    reports_df[\"benchmark_start_time\"].apply(try_formats), utc=True\n)\nreports_df = reports_df.dropna(subset=[\"benchmark_start_time\"])\n\nassert pd.api.types.is_datetime64_any_dtype(\n    helicone_df[\"benchmark_start_time\"]\n), \"benchmark_start_time in helicone_df is not datetime\"\nassert pd.api.types.is_datetime64_any_dtype(\n    reports_df[\"benchmark_start_time\"]\n), \"benchmark_start_time in reports_df is not datetime\"\n\nreports_df[\"report_time\"] = reports_df[\"benchmark_start_time\"]\n\n# df = pd.merge_asof(\n#     helicone_df.sort_values(\"benchmark_start_time\"),\n#     reports_df.sort_values(\"benchmark_start_time\"),\n#     left_on=\"benchmark_start_time\",\n#     right_on=\"benchmark_start_time\",\n#     by=[\"agent\", \"challenge\"],\n#     direction=\"backward\",\n# )\n\ndf = pd.merge(\n    helicone_df,\n    reports_df,\n    on=[\"benchmark_start_time\", \"agent\", \"challenge\"],\n    how=\"inner\",\n)\n\ndf.to_pickle(\"df.pkl\")\nprint(df.info())\nprint(\"Data saved to df.pkl\")\nprint(\"To load the data use: df = pd.read_pickle('df.pkl')\")\n", "benchmark/agbenchmark/config.py": "import json\nimport sys\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Optional\n\nfrom pydantic import BaseSettings, Field, validator\n\n\ndef _calculate_info_test_path(base_path: Path, benchmark_start_time: datetime) -> Path:\n    \"\"\"\n    Calculates the path to the directory where the test report will be saved.\n    \"\"\"\n    # Ensure the reports path exists\n    base_path.mkdir(parents=True, exist_ok=True)\n\n    # Get current UTC date-time stamp\n    date_stamp = benchmark_start_time.strftime(\"%Y%m%dT%H%M%S\")\n\n    # Default run name\n    run_name = \"full_run\"\n\n    # Map command-line arguments to their respective labels\n    arg_labels = {\n        \"--test\": None,\n        \"--category\": None,\n        \"--maintain\": \"maintain\",\n        \"--improve\": \"improve\",\n        \"--explore\": \"explore\",\n    }\n\n    # Identify the relevant command-line argument\n    for arg, label in arg_labels.items():\n        if arg in sys.argv:\n            test_arg = sys.argv[sys.argv.index(arg) + 1] if label is None else None\n            run_name = arg.strip(\"--\")\n            if test_arg:\n                run_name = f\"{run_name}_{test_arg}\"\n            break\n\n    # Create the full new directory path with ISO standard UTC date-time stamp\n    report_path = base_path / f\"{date_stamp}_{run_name}\"\n\n    # Ensure the new directory is created\n    # FIXME: this is not a desirable side-effect of loading the config\n    report_path.mkdir(exist_ok=True)\n\n    return report_path\n\n\nclass AgentBenchmarkConfig(BaseSettings, extra=\"allow\"):\n    \"\"\"\n    Configuration model and loader for the AGBenchmark.\n\n    Projects that want to use AGBenchmark should contain an agbenchmark_config folder\n    with a config.json file that - at minimum - specifies the `host` at which the\n    subject application exposes an Agent Protocol compliant API.\n    \"\"\"\n\n    agbenchmark_config_dir: Path = Field(..., exclude=True)\n    \"\"\"Path to the agbenchmark_config folder of the subject agent application.\"\"\"\n\n    categories: list[str] | None = None\n    \"\"\"Categories to benchmark the agent for. If omitted, all categories are assumed.\"\"\"\n\n    host: str\n    \"\"\"Host (scheme://address:port) of the subject agent application.\"\"\"\n\n    reports_folder: Path = Field(None)\n    \"\"\"\n    Path to the folder where new reports should be stored.\n    Defaults to {agbenchmark_config_dir}/reports.\n    \"\"\"\n\n    @classmethod\n    def load(cls, config_dir: Optional[Path] = None) -> \"AgentBenchmarkConfig\":\n        config_dir = config_dir or cls.find_config_folder()\n        with (config_dir / \"config.json\").open(\"r\") as f:\n            return cls(\n                agbenchmark_config_dir=config_dir,\n                **json.load(f),\n            )\n\n    @staticmethod\n    def find_config_folder(for_dir: Path = Path.cwd()) -> Path:\n        \"\"\"\n        Find the closest ancestor folder containing an agbenchmark_config folder,\n        and returns the path of that agbenchmark_config folder.\n        \"\"\"\n        current_directory = for_dir\n        while current_directory != Path(\"/\"):\n            if (path := current_directory / \"agbenchmark_config\").exists():\n                if (path / \"config.json\").is_file():\n                    return path\n            current_directory = current_directory.parent\n        raise FileNotFoundError(\n            \"No 'agbenchmark_config' directory found in the path hierarchy.\"\n        )\n\n    @property\n    def config_file(self) -> Path:\n        return self.agbenchmark_config_dir / \"config.json\"\n\n    @validator(\"reports_folder\", pre=True, always=True)\n    def set_reports_folder(cls, v, values):\n        if not v:\n            return values[\"agbenchmark_config_dir\"] / \"reports\"\n        return v\n\n    def get_report_dir(self, benchmark_start_time: datetime) -> Path:\n        return _calculate_info_test_path(self.reports_folder, benchmark_start_time)\n\n    @property\n    def regression_tests_file(self) -> Path:\n        return self.reports_folder / \"regression_tests.json\"\n\n    @property\n    def success_rate_file(self) -> Path:\n        return self.reports_folder / \"success_rate.json\"\n\n    @property\n    def challenges_already_beaten_file(self) -> Path:\n        return self.agbenchmark_config_dir / \"challenges_already_beaten.json\"\n\n    @property\n    def temp_folder(self) -> Path:\n        return self.agbenchmark_config_dir / \"temp_folder\"\n", "benchmark/agbenchmark/agent_interface.py": "import os\nimport shutil\nfrom pathlib import Path\n\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nHELICONE_GRAPHQL_LOGS = os.getenv(\"HELICONE_GRAPHQL_LOGS\", \"\").lower() == \"true\"\n\n\ndef get_list_of_file_paths(\n    challenge_dir_path: str | Path, artifact_folder_name: str\n) -> list[Path]:\n    source_dir = Path(challenge_dir_path) / artifact_folder_name\n    if not source_dir.exists():\n        return []\n    return list(source_dir.iterdir())\n\n\ndef copy_challenge_artifacts_into_workspace(\n    challenge_dir_path: str | Path, artifact_folder_name: str, workspace: str | Path\n) -> None:\n    file_paths = get_list_of_file_paths(challenge_dir_path, artifact_folder_name)\n    for file_path in file_paths:\n        if file_path.is_file():\n            shutil.copy(file_path, workspace)\n", "benchmark/agbenchmark/schema.py": "from __future__ import annotations\n\nfrom typing import Any, Optional\n\nfrom pydantic import BaseModel, Field\n\n\nclass TaskRequestBody(BaseModel):\n    input: str = Field(\n        ...,\n        min_length=1,\n        description=\"Input prompt for the task.\",\n        example=\"Write the words you receive to the file 'output.txt'.\",\n    )\n    additional_input: Optional[dict[str, Any]] = Field(default_factory=dict)\n\n\nclass TaskEvalRequestBody(TaskRequestBody):\n    eval_id: str\n", "benchmark/agbenchmark/agent_api_interface.py": "import logging\nimport time\nfrom pathlib import Path\nfrom typing import AsyncIterator, Optional\n\nfrom agent_protocol_client import (\n    AgentApi,\n    ApiClient,\n    Configuration,\n    Step,\n    TaskRequestBody,\n)\n\nfrom agbenchmark.agent_interface import get_list_of_file_paths\nfrom agbenchmark.config import AgentBenchmarkConfig\n\nlogger = logging.getLogger(__name__)\n\n\nasync def run_api_agent(\n    task: str,\n    config: AgentBenchmarkConfig,\n    timeout: int,\n    artifacts_location: Optional[Path] = None,\n    *,\n    mock: bool = False,\n) -> AsyncIterator[Step]:\n    configuration = Configuration(host=config.host)\n    async with ApiClient(configuration) as api_client:\n        api_instance = AgentApi(api_client)\n        task_request_body = TaskRequestBody(input=task, additional_input=None)\n\n        start_time = time.time()\n        response = await api_instance.create_agent_task(\n            task_request_body=task_request_body\n        )\n        task_id = response.task_id\n\n        if artifacts_location:\n            logger.debug(\"Uploading task input artifacts to agent...\")\n            await upload_artifacts(\n                api_instance, artifacts_location, task_id, \"artifacts_in\"\n            )\n\n        logger.debug(\"Running agent until finished or timeout...\")\n        while True:\n            step = await api_instance.execute_agent_task_step(task_id=task_id)\n            yield step\n\n            if time.time() - start_time > timeout:\n                raise TimeoutError(\"Time limit exceeded\")\n            if step and mock:\n                step.is_last = True\n            if not step or step.is_last:\n                break\n\n        if artifacts_location:\n            # In \"mock\" mode, we cheat by giving the correct artifacts to pass the test\n            if mock:\n                logger.debug(\"Uploading mock artifacts to agent...\")\n                await upload_artifacts(\n                    api_instance, artifacts_location, task_id, \"artifacts_out\"\n                )\n\n            logger.debug(\"Downloading agent artifacts...\")\n            await download_agent_artifacts_into_folder(\n                api_instance, task_id, config.temp_folder\n            )\n\n\nasync def download_agent_artifacts_into_folder(\n    api_instance: AgentApi, task_id: str, folder: Path\n):\n    artifacts = await api_instance.list_agent_task_artifacts(task_id=task_id)\n\n    for artifact in artifacts.artifacts:\n        # current absolute path of the directory of the file\n        if artifact.relative_path:\n            path: str = (\n                artifact.relative_path\n                if not artifact.relative_path.startswith(\"/\")\n                else artifact.relative_path[1:]\n            )\n            folder = (folder / path).parent\n\n        if not folder.exists():\n            folder.mkdir(parents=True)\n\n        file_path = folder / artifact.file_name\n        logger.debug(f\"Downloading agent artifact {artifact.file_name} to {folder}\")\n        with open(file_path, \"wb\") as f:\n            content = await api_instance.download_agent_task_artifact(\n                task_id=task_id, artifact_id=artifact.artifact_id\n            )\n\n            f.write(content)\n\n\nasync def upload_artifacts(\n    api_instance: AgentApi, artifacts_location: Path, task_id: str, type: str\n) -> None:\n    for file_path in get_list_of_file_paths(artifacts_location, type):\n        relative_path: Optional[str] = \"/\".join(\n            str(file_path).split(f\"{type}/\", 1)[-1].split(\"/\")[:-1]\n        )\n        if not relative_path:\n            relative_path = None\n\n        await api_instance.upload_agent_task_artifacts(\n            task_id=task_id, file=str(file_path), relative_path=relative_path\n        )\n", "benchmark/agbenchmark/generate_test.py": "\"\"\"\nAGBenchmark's test discovery endpoint for Pytest.\n\nThis module is picked up by Pytest's *_test.py file matching pattern, and all challenge\nclasses in the module that conform to the `Test*` pattern are collected.\n\"\"\"\n\nimport importlib\nimport logging\nfrom itertools import chain\n\nfrom agbenchmark.challenges.builtin import load_builtin_challenges\nfrom agbenchmark.challenges.webarena import load_webarena_challenges\n\nlogger = logging.getLogger(__name__)\n\nDATA_CATEGORY = {}\n\n# Load challenges and attach them to this module\nfor challenge in chain(load_builtin_challenges(), load_webarena_challenges()):\n    # Attach the Challenge class to this module so it can be discovered by pytest\n    module = importlib.import_module(__name__)\n    setattr(module, challenge.__name__, challenge)\n\n    # Build a map of challenge names and their primary category\n    DATA_CATEGORY[challenge.info.name] = challenge.info.category[0].value\n", "benchmark/agbenchmark/main.py": "import logging\nimport os\nfrom pathlib import Path\nfrom typing import Optional, Sequence\n\nfrom dotenv import load_dotenv\n\nfrom agbenchmark.challenges import get_unique_categories\nfrom agbenchmark.config import AgentBenchmarkConfig\n\nload_dotenv()\n\nlogger = logging.getLogger(__name__)\n\n\ndef run_benchmark(\n    config: AgentBenchmarkConfig,\n    maintain: bool = False,\n    improve: bool = False,\n    explore: bool = False,\n    tests: tuple[str, ...] = tuple(),\n    categories: tuple[str, ...] = tuple(),\n    skip_categories: tuple[str, ...] = tuple(),\n    attempts_per_challenge: int = 1,\n    mock: bool = False,\n    no_dep: bool = False,\n    no_cutoff: bool = False,\n    cutoff: Optional[int] = None,\n    keep_answers: bool = False,\n    server: bool = False,\n) -> int:\n    \"\"\"\n    Starts the benchmark. If a category flag is provided, only challenges with the\n    corresponding mark will be run.\n    \"\"\"\n    import pytest\n\n    from agbenchmark.reports.ReportManager import SingletonReportManager\n\n    validate_args(\n        maintain=maintain,\n        improve=improve,\n        explore=explore,\n        tests=tests,\n        categories=categories,\n        skip_categories=skip_categories,\n        no_cutoff=no_cutoff,\n        cutoff=cutoff,\n    )\n\n    SingletonReportManager()\n\n    for key, value in vars(config).items():\n        logger.debug(f\"config.{key} = {repr(value)}\")\n\n    pytest_args = [\"-vs\"]\n\n    if tests:\n        logger.info(f\"Running specific test(s): {' '.join(tests)}\")\n        pytest_args += [f\"--test={t}\" for t in tests]\n    else:\n        all_categories = get_unique_categories()\n\n        if categories or skip_categories:\n            categories_to_run = set(categories) or all_categories\n            if skip_categories:\n                categories_to_run = categories_to_run.difference(set(skip_categories))\n            assert categories_to_run, \"Error: You can't skip all categories\"\n            pytest_args += [f\"--category={c}\" for c in categories_to_run]\n            logger.info(f\"Running tests of category: {categories_to_run}\")\n        else:\n            logger.info(\"Running all categories\")\n\n        if maintain:\n            logger.info(\"Running only regression tests\")\n        elif improve:\n            logger.info(\"Running only non-regression tests\")\n        elif explore:\n            logger.info(\"Only attempt challenges that have never been beaten\")\n\n    if mock:\n        # TODO: unhack\n        os.environ[\n            \"IS_MOCK\"\n        ] = \"True\"  # ugly hack to make the mock work when calling from API\n\n    # Pass through flags\n    for flag, active in {\n        \"--maintain\": maintain,\n        \"--improve\": improve,\n        \"--explore\": explore,\n        \"--no-dep\": no_dep,\n        \"--mock\": mock,\n        \"--nc\": no_cutoff,\n        \"--keep-answers\": keep_answers,\n    }.items():\n        if active:\n            pytest_args.append(flag)\n\n    if attempts_per_challenge > 1:\n        pytest_args.append(f\"--attempts={attempts_per_challenge}\")\n\n    if cutoff:\n        pytest_args.append(f\"--cutoff={cutoff}\")\n        logger.debug(f\"Setting cuttoff override to {cutoff} seconds.\")\n\n    current_dir = Path(__file__).resolve().parent\n    pytest_args.append(str(current_dir / \"generate_test.py\"))\n\n    pytest_args.append(\"--cache-clear\")\n    logger.debug(f\"Running Pytest with args: {pytest_args}\")\n    exit_code = pytest.main(pytest_args)\n\n    SingletonReportManager.clear_instance()\n    return exit_code\n\n\nclass InvalidInvocationError(ValueError):\n    pass\n\n\ndef validate_args(\n    maintain: bool,\n    improve: bool,\n    explore: bool,\n    tests: Sequence[str],\n    categories: Sequence[str],\n    skip_categories: Sequence[str],\n    no_cutoff: bool,\n    cutoff: Optional[int],\n) -> None:\n    if categories:\n        all_categories = get_unique_categories()\n        invalid_categories = set(categories) - all_categories\n        if invalid_categories:\n            raise InvalidInvocationError(\n                \"One or more invalid categories were specified: \"\n                f\"{', '.join(invalid_categories)}.\\n\"\n                f\"Valid categories are: {', '.join(all_categories)}.\"\n            )\n\n    if (maintain + improve + explore) > 1:\n        raise InvalidInvocationError(\n            \"You can't use --maintain, --improve or --explore at the same time. \"\n            \"Please choose one.\"\n        )\n\n    if tests and (categories or skip_categories or maintain or improve or explore):\n        raise InvalidInvocationError(\n            \"If you're running a specific test make sure no other options are \"\n            \"selected. Please just pass the --test.\"\n        )\n\n    if no_cutoff and cutoff:\n        raise InvalidInvocationError(\n            \"You can't use both --nc and --cutoff at the same time. \"\n            \"Please choose one.\"\n        )\n", "benchmark/agbenchmark/conftest.py": "import contextlib\nimport json\nimport logging\nimport os\nimport shutil\nimport threading\nimport time\nfrom pathlib import Path\nfrom typing import Generator\n\nimport pytest\n\nfrom agbenchmark.challenges import OPTIONAL_CATEGORIES, BaseChallenge\nfrom agbenchmark.config import AgentBenchmarkConfig\nfrom agbenchmark.reports.processing.report_types import Test\nfrom agbenchmark.reports.ReportManager import RegressionTestsTracker\nfrom agbenchmark.reports.reports import (\n    add_test_result_to_report,\n    make_empty_test_report,\n    session_finish,\n)\nfrom agbenchmark.utils.data_types import Category\n\nGLOBAL_TIMEOUT = (\n    1500  # The tests will stop after 25 minutes so we can send the reports.\n)\n\nagbenchmark_config = AgentBenchmarkConfig.load()\nlogger = logging.getLogger(__name__)\n\npytest_plugins = [\"agbenchmark.utils.dependencies\"]\ncollect_ignore = [\"challenges\"]\n\n\n@pytest.fixture(scope=\"module\")\ndef config() -> AgentBenchmarkConfig:\n    return agbenchmark_config\n\n\n@pytest.fixture(autouse=True)\ndef temp_folder() -> Generator[Path, None, None]:\n    \"\"\"\n    Pytest fixture that sets up and tears down the temporary folder for each test.\n    It is automatically used in every test due to the 'autouse=True' parameter.\n    \"\"\"\n\n    # create output directory if it doesn't exist\n    if not os.path.exists(agbenchmark_config.temp_folder):\n        os.makedirs(agbenchmark_config.temp_folder, exist_ok=True)\n\n    yield agbenchmark_config.temp_folder\n    # teardown after test function completes\n    if not os.getenv(\"KEEP_TEMP_FOLDER_FILES\"):\n        for filename in os.listdir(agbenchmark_config.temp_folder):\n            file_path = os.path.join(agbenchmark_config.temp_folder, filename)\n            try:\n                if os.path.isfile(file_path) or os.path.islink(file_path):\n                    os.unlink(file_path)\n                elif os.path.isdir(file_path):\n                    shutil.rmtree(file_path)\n            except Exception as e:\n                logger.warning(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef pytest_addoption(parser: pytest.Parser) -> None:\n    \"\"\"\n    Pytest hook that adds command-line options to the `pytest` command.\n    The added options are specific to agbenchmark and control its behavior:\n    * `--mock` is used to run the tests in mock mode.\n    * `--host` is used to specify the host for the tests.\n    * `--category` is used to run only tests of a specific category.\n    * `--nc` is used to run the tests without caching.\n    * `--cutoff` is used to specify a cutoff time for the tests.\n    * `--improve` is used to run only the tests that are marked for improvement.\n    * `--maintain` is used to run only the tests that are marked for maintenance.\n    * `--explore` is used to run the tests in exploration mode.\n    * `--test` is used to run a specific test.\n    * `--no-dep` is used to run the tests without dependencies.\n    * `--keep-answers` is used to keep the answers of the tests.\n\n    Args:\n        parser: The Pytest CLI parser to which the command-line options are added.\n    \"\"\"\n    parser.addoption(\"-N\", \"--attempts\", action=\"store\")\n    parser.addoption(\"--no-dep\", action=\"store_true\")\n    parser.addoption(\"--mock\", action=\"store_true\")\n    parser.addoption(\"--host\", default=None)\n    parser.addoption(\"--nc\", action=\"store_true\")\n    parser.addoption(\"--cutoff\", action=\"store\")\n    parser.addoption(\"--category\", action=\"append\")\n    parser.addoption(\"--test\", action=\"append\")\n    parser.addoption(\"--improve\", action=\"store_true\")\n    parser.addoption(\"--maintain\", action=\"store_true\")\n    parser.addoption(\"--explore\", action=\"store_true\")\n    parser.addoption(\"--keep-answers\", action=\"store_true\")\n\n\ndef pytest_configure(config: pytest.Config) -> None:\n    # Register category markers to prevent \"unknown marker\" warnings\n    for category in Category:\n        config.addinivalue_line(\"markers\", f\"{category.value}: {category}\")\n\n\n@pytest.fixture(autouse=True)\ndef check_regression(request: pytest.FixtureRequest) -> None:\n    \"\"\"\n    Fixture that checks for every test if it should be treated as a regression test,\n    and whether to skip it based on that.\n\n    The test name is retrieved from the `request` object. Regression reports are loaded\n    from the path specified in the benchmark configuration.\n\n    Effect:\n    * If the `--improve` option is used and the current test is considered a regression\n      test, it is skipped.\n    * If the `--maintain` option is used and the current test  is not considered a\n      regression test, it is also skipped.\n\n    Args:\n        request: The request object from which the test name and the benchmark\n            configuration are retrieved.\n    \"\"\"\n    with contextlib.suppress(FileNotFoundError):\n        rt_tracker = RegressionTestsTracker(agbenchmark_config.regression_tests_file)\n\n        assert isinstance(request.node, pytest.Function)\n        assert isinstance(request.node.parent, pytest.Class)\n        test_name = request.node.parent.name\n        challenge_location = getattr(request.node.cls, \"CHALLENGE_LOCATION\", \"\")\n        skip_string = f\"Skipping {test_name} at {challenge_location}\"\n\n        # Check if the test name exists in the regression tests\n        is_regression_test = rt_tracker.has_regression_test(test_name)\n        if request.config.getoption(\"--improve\") and is_regression_test:\n            pytest.skip(f\"{skip_string} because it's a regression test\")\n        elif request.config.getoption(\"--maintain\") and not is_regression_test:\n            pytest.skip(f\"{skip_string} because it's not a regression test\")\n\n\n@pytest.fixture(autouse=True, scope=\"session\")\ndef mock(request: pytest.FixtureRequest) -> bool:\n    \"\"\"\n    Pytest fixture that retrieves the value of the `--mock` command-line option.\n    The `--mock` option is used to run the tests in mock mode.\n\n    Args:\n        request: The `pytest.FixtureRequest` from which the `--mock` option value\n            is retrieved.\n\n    Returns:\n        bool: Whether `--mock` is set for this session.\n    \"\"\"\n    mock = request.config.getoption(\"--mock\")\n    assert isinstance(mock, bool)\n    return mock\n\n\ntest_reports: dict[str, Test] = {}\n\n\ndef pytest_runtest_makereport(item: pytest.Item, call: pytest.CallInfo) -> None:\n    \"\"\"\n    Pytest hook that is called when a test report is being generated.\n    It is used to generate and finalize reports for each test.\n\n    Args:\n        item: The test item for which the report is being generated.\n        call: The call object from which the test result is retrieved.\n    \"\"\"\n    challenge: type[BaseChallenge] = item.cls  # type: ignore\n    challenge_id = challenge.info.eval_id\n\n    if challenge_id not in test_reports:\n        test_reports[challenge_id] = make_empty_test_report(challenge.info)\n\n    if call.when == \"setup\":\n        test_name = item.nodeid.split(\"::\")[1]\n        item.user_properties.append((\"test_name\", test_name))\n\n    if call.when == \"call\":\n        add_test_result_to_report(\n            test_reports[challenge_id], item, call, agbenchmark_config\n        )\n\n\ndef timeout_monitor(start_time: int) -> None:\n    \"\"\"\n    Function that limits the total execution time of the test suite.\n    This function is supposed to be run in a separate thread and calls `pytest.exit`\n    if the total execution time has exceeded the global timeout.\n\n    Args:\n        start_time (int): The start time of the test suite.\n    \"\"\"\n    while time.time() - start_time < GLOBAL_TIMEOUT:\n        time.sleep(1)  # check every second\n\n    pytest.exit(\"Test suite exceeded the global timeout\", returncode=1)\n\n\ndef pytest_sessionstart(session: pytest.Session) -> None:\n    \"\"\"\n    Pytest hook that is called at the start of a test session.\n\n    Sets up and runs a `timeout_monitor` in a separate thread.\n    \"\"\"\n    start_time = time.time()\n    t = threading.Thread(target=timeout_monitor, args=(start_time,))\n    t.daemon = True  # Daemon threads are abruptly stopped at shutdown\n    t.start()\n\n\ndef pytest_sessionfinish(session: pytest.Session) -> None:\n    \"\"\"\n    Pytest hook that is called at the end of a test session.\n\n    Finalizes and saves the test reports.\n    \"\"\"\n    session_finish(agbenchmark_config)\n\n\ndef pytest_generate_tests(metafunc: pytest.Metafunc):\n    n = metafunc.config.getoption(\"-N\")\n    metafunc.parametrize(\"i_attempt\", range(int(n)) if type(n) is str else [0])\n\n\ndef pytest_collection_modifyitems(\n    items: list[pytest.Function], config: pytest.Config\n) -> None:\n    \"\"\"\n    Pytest hook that is called after initial test collection has been performed.\n    Modifies the collected test items based on the agent benchmark configuration,\n    adding the dependency marker and category markers.\n\n    Args:\n        items: The collected test items to be modified.\n        config: The active pytest configuration.\n    \"\"\"\n    rt_tracker = RegressionTestsTracker(agbenchmark_config.regression_tests_file)\n\n    try:\n        challenges_beaten_in_the_past = json.loads(\n            agbenchmark_config.challenges_already_beaten_file.read_bytes()\n        )\n    except FileNotFoundError:\n        challenges_beaten_in_the_past = {}\n\n    selected_tests: tuple[str] = config.getoption(\"--test\")  # type: ignore\n    selected_categories: tuple[str] = config.getoption(\"--category\")  # type: ignore\n\n    # Can't use a for-loop to remove items in-place\n    i = 0\n    while i < len(items):\n        item = items[i]\n        assert item.cls and issubclass(item.cls, BaseChallenge)\n        challenge = item.cls\n        challenge_name = challenge.info.name\n\n        if not issubclass(challenge, BaseChallenge):\n            item.warn(\n                pytest.PytestCollectionWarning(\n                    f\"Non-challenge item collected: {challenge}\"\n                )\n            )\n            i += 1\n            continue\n\n        # --test: remove the test from the set if it's not specifically selected\n        if selected_tests and challenge.info.name not in selected_tests:\n            items.remove(item)\n            continue\n\n        # Filter challenges for --maintain, --improve, and --explore:\n        # --maintain -> only challenges expected to be passed (= regression tests)\n        # --improve -> only challenges that so far are not passed (reliably)\n        # --explore -> only challenges that have never been passed\n        is_regression_test = rt_tracker.has_regression_test(challenge.info.name)\n        has_been_passed = challenges_beaten_in_the_past.get(challenge.info.name, False)\n        if (\n            (config.getoption(\"--maintain\") and not is_regression_test)\n            or (config.getoption(\"--improve\") and is_regression_test)\n            or (config.getoption(\"--explore\") and has_been_passed)\n        ):\n            items.remove(item)\n            continue\n\n        dependencies = challenge.info.dependencies\n        if (\n            config.getoption(\"--test\")\n            or config.getoption(\"--no-dep\")\n            or config.getoption(\"--maintain\")\n        ):\n            # Ignore dependencies:\n            # --test -> user selected specific tests to run, don't care about deps\n            # --no-dep -> ignore dependency relations regardless of test selection\n            # --maintain -> all \"regression\" tests must pass, so run all of them\n            dependencies = []\n        elif config.getoption(\"--improve\"):\n            # Filter dependencies, keep only deps that are not \"regression\" tests\n            dependencies = [\n                d for d in dependencies if not rt_tracker.has_regression_test(d)\n            ]\n\n        # Set category markers\n        challenge_categories = set(c.value for c in challenge.info.category)\n        for category in challenge_categories:\n            item.add_marker(category)\n\n        # Enforce category selection\n        if selected_categories:\n            if not challenge_categories.intersection(set(selected_categories)):\n                items.remove(item)\n                continue\n            # # Filter dependencies, keep only deps from selected categories\n            # dependencies = [\n            #     d for d in dependencies\n            #     if not set(d.categories).intersection(set(selected_categories))\n            # ]\n\n        # Skip items in optional categories that are not selected for the subject agent\n        challenge_optional_categories = challenge_categories & set(OPTIONAL_CATEGORIES)\n        if challenge_optional_categories and not (\n            agbenchmark_config.categories\n            and challenge_optional_categories.issubset(\n                set(agbenchmark_config.categories)\n            )\n        ):\n            logger.debug(\n                f\"Skipping {challenge_name}: \"\n                f\"category {' and '.join(challenge_optional_categories)} is optional, \"\n                \"and not explicitly selected in the benchmark config.\"\n            )\n            items.remove(item)\n            continue\n\n        # Add marker for the DependencyManager\n        item.add_marker(pytest.mark.depends(on=dependencies, name=challenge_name))\n\n        i += 1\n", "benchmark/agbenchmark/__main__.py": "import logging\nimport os\nimport sys\nfrom datetime import datetime, timezone\nfrom pathlib import Path\nfrom typing import Any, Optional\n\nimport click\nfrom click_default_group import DefaultGroup\nfrom dotenv import load_dotenv\n\nfrom agbenchmark.config import AgentBenchmarkConfig\nfrom agbenchmark.utils.logging import configure_logging\n\nload_dotenv()\n\n# try:\n#     if os.getenv(\"HELICONE_API_KEY\"):\n#         import helicone  # noqa\n\n#         helicone_enabled = True\n#     else:\n#         helicone_enabled = False\n# except ImportError:\n#     helicone_enabled = False\n\n\nclass InvalidInvocationError(ValueError):\n    pass\n\n\nlogger = logging.getLogger(__name__)\n\nBENCHMARK_START_TIME_DT = datetime.now(timezone.utc)\nBENCHMARK_START_TIME = BENCHMARK_START_TIME_DT.strftime(\"%Y-%m-%dT%H:%M:%S+00:00\")\n\n\n# if helicone_enabled:\n#     from helicone.lock import HeliconeLockManager\n\n#     HeliconeLockManager.write_custom_property(\n#         \"benchmark_start_time\", BENCHMARK_START_TIME\n#     )\n\n\n@click.group(cls=DefaultGroup, default_if_no_args=True)\n@click.option(\"--debug\", is_flag=True, help=\"Enable debug output\")\ndef cli(\n    debug: bool,\n) -> Any:\n    configure_logging(logging.DEBUG if debug else logging.INFO)\n\n\n@cli.command(hidden=True)\ndef start():\n    raise DeprecationWarning(\n        \"`agbenchmark start` is deprecated. Use `agbenchmark run` instead.\"\n    )\n\n\n@cli.command(default=True)\n@click.option(\n    \"-N\", \"--attempts\", default=1, help=\"Number of times to run each challenge.\"\n)\n@click.option(\n    \"-c\",\n    \"--category\",\n    multiple=True,\n    help=\"(+) Select a category to run.\",\n)\n@click.option(\n    \"-s\",\n    \"--skip-category\",\n    multiple=True,\n    help=\"(+) Exclude a category from running.\",\n)\n@click.option(\"--test\", multiple=True, help=\"(+) Select a test to run.\")\n@click.option(\"--maintain\", is_flag=True, help=\"Run only regression tests.\")\n@click.option(\"--improve\", is_flag=True, help=\"Run only non-regression tests.\")\n@click.option(\n    \"--explore\",\n    is_flag=True,\n    help=\"Run only challenges that have never been beaten.\",\n)\n@click.option(\n    \"--no-dep\",\n    is_flag=True,\n    help=\"Run all (selected) challenges, regardless of dependency success/failure.\",\n)\n@click.option(\"--cutoff\", type=int, help=\"Override the challenge time limit (seconds).\")\n@click.option(\"--nc\", is_flag=True, help=\"Disable the challenge time limit.\")\n@click.option(\"--mock\", is_flag=True, help=\"Run with mock\")\n@click.option(\"--keep-answers\", is_flag=True, help=\"Keep answers\")\n@click.option(\n    \"--backend\",\n    is_flag=True,\n    help=\"Write log output to a file instead of the terminal.\",\n)\n# @click.argument(\n#     \"agent_path\", type=click.Path(exists=True, file_okay=False), required=False\n# )\ndef run(\n    maintain: bool,\n    improve: bool,\n    explore: bool,\n    mock: bool,\n    no_dep: bool,\n    nc: bool,\n    keep_answers: bool,\n    test: tuple[str],\n    category: tuple[str],\n    skip_category: tuple[str],\n    attempts: int,\n    cutoff: Optional[int] = None,\n    backend: Optional[bool] = False,\n    # agent_path: Optional[Path] = None,\n) -> None:\n    \"\"\"\n    Run the benchmark on the agent in the current directory.\n\n    Options marked with (+) can be specified multiple times, to select multiple items.\n    \"\"\"\n    from agbenchmark.main import run_benchmark, validate_args\n\n    agbenchmark_config = AgentBenchmarkConfig.load()\n    logger.debug(f\"agbenchmark_config: {agbenchmark_config.agbenchmark_config_dir}\")\n    try:\n        validate_args(\n            maintain=maintain,\n            improve=improve,\n            explore=explore,\n            tests=test,\n            categories=category,\n            skip_categories=skip_category,\n            no_cutoff=nc,\n            cutoff=cutoff,\n        )\n    except InvalidInvocationError as e:\n        logger.error(\"Error: \" + \"\\n\".join(e.args))\n        sys.exit(1)\n\n    original_stdout = sys.stdout  # Save the original standard output\n    exit_code = None\n\n    if backend:\n        with open(\"backend/backend_stdout.txt\", \"w\") as f:\n            sys.stdout = f\n            exit_code = run_benchmark(\n                config=agbenchmark_config,\n                maintain=maintain,\n                improve=improve,\n                explore=explore,\n                mock=mock,\n                no_dep=no_dep,\n                no_cutoff=nc,\n                keep_answers=keep_answers,\n                tests=test,\n                categories=category,\n                skip_categories=skip_category,\n                attempts_per_challenge=attempts,\n                cutoff=cutoff,\n            )\n\n        sys.stdout = original_stdout\n\n    else:\n        exit_code = run_benchmark(\n            config=agbenchmark_config,\n            maintain=maintain,\n            improve=improve,\n            explore=explore,\n            mock=mock,\n            no_dep=no_dep,\n            no_cutoff=nc,\n            keep_answers=keep_answers,\n            tests=test,\n            categories=category,\n            skip_categories=skip_category,\n            attempts_per_challenge=attempts,\n            cutoff=cutoff,\n        )\n\n        sys.exit(exit_code)\n\n\n@cli.command()\n@click.option(\"--port\", type=int, help=\"Port to run the API on.\")\ndef serve(port: Optional[int] = None):\n    \"\"\"Serve the benchmark frontend and API on port 8080.\"\"\"\n    import uvicorn\n\n    from agbenchmark.app import setup_fastapi_app\n\n    config = AgentBenchmarkConfig.load()\n    app = setup_fastapi_app(config)\n\n    # Run the FastAPI application using uvicorn\n    port = port or int(os.getenv(\"PORT\", 8080))\n    uvicorn.run(app, host=\"0.0.0.0\", port=port)\n\n\n@cli.command()\ndef config():\n    \"\"\"Displays info regarding the present AGBenchmark config.\"\"\"\n    from .utils.utils import pretty_print_model\n\n    try:\n        config = AgentBenchmarkConfig.load()\n    except FileNotFoundError as e:\n        click.echo(e, err=True)\n        return 1\n\n    pretty_print_model(config, include_header=False)\n\n\n@cli.group()\ndef challenge():\n    logging.getLogger().setLevel(logging.WARNING)\n\n\n@challenge.command(\"list\")\n@click.option(\n    \"--all\", \"include_unavailable\", is_flag=True, help=\"Include unavailable challenges.\"\n)\n@click.option(\n    \"--names\", \"only_names\", is_flag=True, help=\"List only the challenge names.\"\n)\n@click.option(\"--json\", \"output_json\", is_flag=True)\ndef list_challenges(include_unavailable: bool, only_names: bool, output_json: bool):\n    \"\"\"Lists [available|all] challenges.\"\"\"\n    import json\n\n    from tabulate import tabulate\n\n    from .challenges.builtin import load_builtin_challenges\n    from .challenges.webarena import load_webarena_challenges\n    from .utils.data_types import Category, DifficultyLevel\n    from .utils.utils import sorted_by_enum_index\n\n    DIFFICULTY_COLORS = {\n        difficulty: color\n        for difficulty, color in zip(\n            DifficultyLevel,\n            [\"black\", \"blue\", \"cyan\", \"green\", \"yellow\", \"red\", \"magenta\", \"white\"],\n        )\n    }\n    CATEGORY_COLORS = {\n        category: f\"bright_{color}\"\n        for category, color in zip(\n            Category,\n            [\"blue\", \"cyan\", \"green\", \"yellow\", \"magenta\", \"red\", \"white\", \"black\"],\n        )\n    }\n\n    # Load challenges\n    challenges = filter(\n        lambda c: c.info.available or include_unavailable,\n        [\n            *load_builtin_challenges(),\n            *load_webarena_challenges(skip_unavailable=False),\n        ],\n    )\n    challenges = sorted_by_enum_index(\n        challenges, DifficultyLevel, key=lambda c: c.info.difficulty\n    )\n\n    if only_names:\n        if output_json:\n            click.echo(json.dumps([c.info.name for c in challenges]))\n            return\n\n        for c in challenges:\n            click.echo(\n                click.style(c.info.name, fg=None if c.info.available else \"black\")\n            )\n        return\n\n    if output_json:\n        click.echo(json.dumps([json.loads(c.info.json()) for c in challenges]))\n        return\n\n    headers = tuple(\n        click.style(h, bold=True) for h in (\"Name\", \"Difficulty\", \"Categories\")\n    )\n    table = [\n        tuple(\n            v if challenge.info.available else click.style(v, fg=\"black\")\n            for v in (\n                challenge.info.name,\n                (\n                    click.style(\n                        challenge.info.difficulty.value,\n                        fg=DIFFICULTY_COLORS[challenge.info.difficulty],\n                    )\n                    if challenge.info.difficulty\n                    else click.style(\"-\", fg=\"black\")\n                ),\n                \" \".join(\n                    click.style(cat.value, fg=CATEGORY_COLORS[cat])\n                    for cat in sorted_by_enum_index(challenge.info.category, Category)\n                ),\n            )\n        )\n        for challenge in challenges\n    ]\n    click.echo(tabulate(table, headers=headers))\n\n\n@challenge.command()\n@click.option(\"--json\", is_flag=True)\n@click.argument(\"name\")\ndef info(name: str, json: bool):\n    from itertools import chain\n\n    from .challenges.builtin import load_builtin_challenges\n    from .challenges.webarena import load_webarena_challenges\n    from .utils.utils import pretty_print_model\n\n    for challenge in chain(\n        load_builtin_challenges(),\n        load_webarena_challenges(skip_unavailable=False),\n    ):\n        if challenge.info.name != name:\n            continue\n\n        if json:\n            click.echo(challenge.info.json())\n            break\n\n        pretty_print_model(challenge.info)\n        break\n    else:\n        click.echo(click.style(f\"Unknown challenge '{name}'\", fg=\"red\"), err=True)\n\n\n@cli.command()\ndef version():\n    \"\"\"Print version info for the AGBenchmark application.\"\"\"\n    import toml\n\n    package_root = Path(__file__).resolve().parent.parent\n    pyproject = toml.load(package_root / \"pyproject.toml\")\n    version = pyproject[\"tool\"][\"poetry\"][\"version\"]\n    click.echo(f\"AGBenchmark version {version}\")\n\n\nif __name__ == \"__main__\":\n    cli()\n", "benchmark/agbenchmark/__init__.py": "", "benchmark/agbenchmark/app.py": "import datetime\nimport glob\nimport json\nimport logging\nimport sys\nimport time\nimport uuid\nfrom collections import deque\nfrom multiprocessing import Process\nfrom pathlib import Path\nfrom typing import Optional\n\nimport httpx\nimport psutil\nfrom agent_protocol_client import AgentApi, ApiClient, ApiException, Configuration\nfrom agent_protocol_client.models import Task, TaskRequestBody\nfrom fastapi import APIRouter, FastAPI, HTTPException, Request, Response\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel, Extra, ValidationError\n\nfrom agbenchmark.challenges import ChallengeInfo\nfrom agbenchmark.config import AgentBenchmarkConfig\nfrom agbenchmark.reports.processing.report_types_v2 import (\n    BenchmarkRun,\n    Metrics,\n    RepositoryInfo,\n    RunDetails,\n    TaskInfo,\n)\nfrom agbenchmark.schema import TaskEvalRequestBody\nfrom agbenchmark.utils.utils import write_pretty_json\n\nsys.path.append(str(Path(__file__).parent.parent))\n\nlogger = logging.getLogger(__name__)\n\nCHALLENGES: dict[str, ChallengeInfo] = {}\nchallenges_path = Path(__file__).parent / \"challenges\"\nchallenge_spec_files = deque(\n    glob.glob(\n        f\"{challenges_path}/**/data.json\",\n        recursive=True,\n    )\n)\n\nlogger.debug(\"Loading challenges...\")\nwhile challenge_spec_files:\n    challenge_spec_file = Path(challenge_spec_files.popleft())\n    challenge_relpath = challenge_spec_file.relative_to(challenges_path.parent)\n    if challenge_relpath.is_relative_to(\"challenges/deprecated\"):\n        continue\n\n    logger.debug(f\"Loading {challenge_relpath}...\")\n    try:\n        challenge_info = ChallengeInfo.parse_file(challenge_spec_file)\n    except ValidationError as e:\n        if logging.getLogger().level == logging.DEBUG:\n            logger.warning(f\"Spec file {challenge_relpath} failed to load:\\n{e}\")\n        logger.debug(f\"Invalid challenge spec: {challenge_spec_file.read_text()}\")\n        continue\n    challenge_info.spec_file = challenge_spec_file\n\n    if not challenge_info.eval_id:\n        challenge_info.eval_id = str(uuid.uuid4())\n        # this will sort all the keys of the JSON systematically\n        # so that the order is always the same\n        write_pretty_json(challenge_info.dict(), challenge_spec_file)\n\n    CHALLENGES[challenge_info.eval_id] = challenge_info\n\n\nclass BenchmarkTaskInfo(BaseModel):\n    task_id: str\n    start_time: datetime.datetime\n    challenge_info: ChallengeInfo\n\n\ntask_informations: dict[str, BenchmarkTaskInfo] = {}\n\n\ndef find_agbenchmark_without_uvicorn():\n    pids = []\n    for process in psutil.process_iter(\n        attrs=[\n            \"pid\",\n            \"cmdline\",\n            \"name\",\n            \"username\",\n            \"status\",\n            \"cpu_percent\",\n            \"memory_info\",\n            \"create_time\",\n            \"cwd\",\n            \"connections\",\n        ]\n    ):\n        try:\n            # Convert the process.info dictionary values to strings and concatenate them\n            full_info = \" \".join([str(v) for k, v in process.as_dict().items()])\n\n            if \"agbenchmark\" in full_info and \"uvicorn\" not in full_info:\n                pids.append(process.pid)\n        except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):\n            pass\n    return pids\n\n\nclass CreateReportRequest(BaseModel):\n    test: str\n    test_run_id: str\n    # category: Optional[str] = []\n    mock: Optional[bool] = False\n\n    class Config:\n        extra = Extra.forbid  # this will forbid any extra fields\n\n\nupdates_list = []\n\norigins = [\n    \"http://localhost:8000\",\n    \"http://localhost:8080\",\n    \"http://127.0.0.1:5000\",\n    \"http://localhost:5000\",\n]\n\n\ndef stream_output(pipe):\n    for line in pipe:\n        print(line, end=\"\")\n\n\ndef setup_fastapi_app(agbenchmark_config: AgentBenchmarkConfig) -> FastAPI:\n    from agbenchmark.agent_api_interface import upload_artifacts\n    from agbenchmark.challenges import get_challenge_from_source_uri\n    from agbenchmark.main import run_benchmark\n\n    configuration = Configuration(\n        host=agbenchmark_config.host or \"http://localhost:8000\"\n    )\n    app = FastAPI()\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=origins,\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n    router = APIRouter()\n\n    @router.post(\"/reports\")\n    def run_single_test(body: CreateReportRequest) -> dict:\n        pids = find_agbenchmark_without_uvicorn()\n        logger.info(f\"pids already running with agbenchmark: {pids}\")\n\n        logger.debug(f\"Request to /reports: {body.dict()}\")\n\n        # Start the benchmark in a separate thread\n        benchmark_process = Process(\n            target=lambda: run_benchmark(\n                config=agbenchmark_config,\n                tests=(body.test,),\n                mock=body.mock or False,\n            )\n        )\n        benchmark_process.start()\n\n        # Wait for the benchmark to finish, with a timeout of 200 seconds\n        timeout = 200\n        start_time = time.time()\n        while benchmark_process.is_alive():\n            if time.time() - start_time > timeout:\n                logger.warning(f\"Benchmark run timed out after {timeout} seconds\")\n                benchmark_process.terminate()\n                break\n            time.sleep(1)\n        else:\n            logger.debug(f\"Benchmark finished running in {time.time() - start_time} s\")\n\n        # List all folders in the current working directory\n        reports_folder = agbenchmark_config.reports_folder\n        folders = [folder for folder in reports_folder.iterdir() if folder.is_dir()]\n\n        # Sort the folders based on their names\n        sorted_folders = sorted(folders, key=lambda x: x.name)\n\n        # Get the last folder\n        latest_folder = sorted_folders[-1] if sorted_folders else None\n\n        # Read report.json from this folder\n        if latest_folder:\n            report_path = latest_folder / \"report.json\"\n            logger.debug(f\"Getting latest report from {report_path}\")\n            if report_path.exists():\n                with report_path.open() as file:\n                    data = json.load(file)\n                logger.debug(f\"Report data: {data}\")\n            else:\n                raise HTTPException(\n                    502,\n                    \"Could not get result after running benchmark: \"\n                    f\"'report.json' does not exist in '{latest_folder}'\",\n                )\n        else:\n            raise HTTPException(\n                504, \"Could not get result after running benchmark: no reports found\"\n            )\n\n        return data\n\n    @router.post(\"/agent/tasks\", tags=[\"agent\"])\n    async def create_agent_task(task_eval_request: TaskEvalRequestBody) -> Task:\n        \"\"\"\n        Creates a new task using the provided TaskEvalRequestBody and returns a Task.\n\n        Args:\n            task_eval_request: `TaskRequestBody` including an eval_id.\n\n        Returns:\n            Task: A new task with task_id, input, additional_input,\n                and empty lists for artifacts and steps.\n\n        Example:\n            Request (TaskEvalRequestBody defined in schema.py):\n                {\n                    ...,\n                    \"eval_id\": \"50da533e-3904-4401-8a07-c49adf88b5eb\"\n                }\n\n            Response (Task defined in `agent_protocol_client.models`):\n                {\n                    \"task_id\": \"50da533e-3904-4401-8a07-c49adf88b5eb\",\n                    \"input\": \"Write the word 'Washington' to a .txt file\",\n                    \"artifacts\": []\n                }\n        \"\"\"\n        try:\n            challenge_info = CHALLENGES[task_eval_request.eval_id]\n            async with ApiClient(configuration) as api_client:\n                api_instance = AgentApi(api_client)\n                task_input = challenge_info.task\n\n                task_request_body = TaskRequestBody(\n                    input=task_input, additional_input=None\n                )\n                task_response = await api_instance.create_agent_task(\n                    task_request_body=task_request_body\n                )\n                task_info = BenchmarkTaskInfo(\n                    task_id=task_response.task_id,\n                    start_time=datetime.datetime.now(datetime.timezone.utc),\n                    challenge_info=challenge_info,\n                )\n                task_informations[task_info.task_id] = task_info\n\n                if input_artifacts_dir := challenge_info.task_artifacts_dir:\n                    await upload_artifacts(\n                        api_instance,\n                        input_artifacts_dir,\n                        task_response.task_id,\n                        \"artifacts_in\",\n                    )\n                return task_response\n        except ApiException as e:\n            logger.error(f\"Error whilst trying to create a task:\\n{e}\")\n            logger.error(\n                \"The above error was caused while processing request: \"\n                f\"{task_eval_request}\"\n            )\n            raise HTTPException(500)\n\n    @router.post(\"/agent/tasks/{task_id}/steps\")\n    async def proxy(request: Request, task_id: str):\n        timeout = httpx.Timeout(300.0, read=300.0)  # 5 minutes\n        async with httpx.AsyncClient(timeout=timeout) as client:\n            # Construct the new URL\n            new_url = f\"{configuration.host}/ap/v1/agent/tasks/{task_id}/steps\"\n\n            # Forward the request\n            response = await client.post(\n                new_url,\n                content=await request.body(),\n                headers=dict(request.headers),\n            )\n\n            # Return the response from the forwarded request\n            return Response(content=response.content, status_code=response.status_code)\n\n    @router.post(\"/agent/tasks/{task_id}/evaluations\")\n    async def create_evaluation(task_id: str) -> BenchmarkRun:\n        task_info = task_informations[task_id]\n        challenge = get_challenge_from_source_uri(task_info.challenge_info.source_uri)\n        try:\n            async with ApiClient(configuration) as api_client:\n                api_instance = AgentApi(api_client)\n                eval_results = await challenge.evaluate_task_state(\n                    api_instance, task_id\n                )\n\n            eval_info = BenchmarkRun(\n                repository_info=RepositoryInfo(),\n                run_details=RunDetails(\n                    command=f\"agbenchmark --test={challenge.info.name}\",\n                    benchmark_start_time=(\n                        task_info.start_time.strftime(\"%Y-%m-%dT%H:%M:%S+00:00\")\n                    ),\n                    test_name=challenge.info.name,\n                ),\n                task_info=TaskInfo(\n                    data_path=challenge.info.source_uri,\n                    is_regression=None,\n                    category=[c.value for c in challenge.info.category],\n                    task=challenge.info.task,\n                    answer=challenge.info.reference_answer or \"\",\n                    description=challenge.info.description or \"\",\n                ),\n                metrics=Metrics(\n                    success=all(e.passed for e in eval_results),\n                    success_percentage=(\n                        100 * sum(e.score for e in eval_results) / len(eval_results)\n                        if eval_results  # avoid division by 0\n                        else 0\n                    ),\n                    attempted=True,\n                ),\n                config={},\n            )\n\n            logger.debug(f\"Returning evaluation data:\\n{eval_info.json(indent=4)}\")\n            return eval_info\n        except ApiException as e:\n            logger.error(f\"Error {e} whilst trying to evaluate task: {task_id}\")\n            raise HTTPException(500)\n\n    app.include_router(router, prefix=\"/ap/v1\")\n\n    return app\n", "benchmark/agbenchmark/challenges/builtin.py": "import glob\nimport json\nimport logging\nimport os\nimport subprocess\nimport sys\nimport tempfile\nfrom collections import deque\nfrom pathlib import Path\nfrom typing import Annotated, Any, ClassVar, Iterator, Literal, Optional\n\nimport pytest\nfrom agent_protocol_client import AgentApi, ApiClient\nfrom agent_protocol_client import Configuration as ClientConfig\nfrom agent_protocol_client import Step\nfrom colorama import Fore, Style\nfrom openai import _load_client as get_openai_client\nfrom pydantic import BaseModel, Field, constr, validator\n\nfrom agbenchmark.agent_api_interface import download_agent_artifacts_into_folder\nfrom agbenchmark.agent_interface import copy_challenge_artifacts_into_workspace\nfrom agbenchmark.config import AgentBenchmarkConfig\nfrom agbenchmark.utils.data_types import Category, DifficultyLevel, EvalResult\nfrom agbenchmark.utils.prompts import (\n    END_PROMPT,\n    FEW_SHOT_EXAMPLES,\n    PROMPT_MAP,\n    SCORING_MAP,\n)\n\nfrom .base import BaseChallenge, ChallengeInfo\n\nlogger = logging.getLogger(__name__)\n\nwith open(Path(__file__).parent / \"optional_categories.json\") as f:\n    OPTIONAL_CATEGORIES: list[str] = json.load(f)[\"optional_categories\"]\n\n\nclass BuiltinChallengeSpec(BaseModel):\n    eval_id: str = \"\"\n    name: str\n    task: str\n    category: list[Category]\n    dependencies: list[str]\n    cutoff: int\n\n    class Info(BaseModel):\n        difficulty: DifficultyLevel\n        description: Annotated[str, constr(regex=r\"^Tests if the agent can.*\")]\n        side_effects: list[str] = Field(default_factory=list)\n\n    info: Info\n\n    class Ground(BaseModel):\n        answer: str\n        should_contain: Optional[list[str]] = None\n        should_not_contain: Optional[list[str]] = None\n        files: list[str]\n        case_sensitive: Optional[bool] = True\n\n        class Eval(BaseModel):\n            type: str\n            scoring: Optional[Literal[\"percentage\", \"scale\", \"binary\"]]\n            template: Optional[Literal[\"rubric\", \"reference\", \"question\", \"custom\"]]\n            examples: Optional[str]\n\n            @validator(\"scoring\", \"template\", always=True)\n            def validate_eval_fields(cls, v, values, field):\n                if \"type\" in values and values[\"type\"] == \"llm\":\n                    if v is None:\n                        raise ValueError(\n                            f\"{field.name} must be provided when eval type is 'llm'\"\n                        )\n                else:\n                    if v is not None:\n                        raise ValueError(\n                            f\"{field.name} should only exist when eval type is 'llm'\"\n                        )\n                return v\n\n        eval: Eval\n\n    ground: Ground\n\n    metadata: Optional[dict[str, Any]] = None\n    spec_file: Path | None = Field(None, exclude=True)\n\n\nclass BuiltinChallenge(BaseChallenge):\n    \"\"\"\n    Base class for AGBenchmark's built-in challenges (challenges/**/*.json).\n\n    All of the logic is present in this class. Individual challenges are created as\n    subclasses of `BuiltinChallenge` with challenge-specific values assigned to the\n    ClassVars `_spec` etc.\n\n    Dynamically constructing subclasses rather than class instances for the individual\n    challenges makes them suitable for collection by Pytest, which will run their\n    `test_method` like any regular test item.\n    \"\"\"\n\n    _spec: ClassVar[BuiltinChallengeSpec]\n    CHALLENGE_LOCATION: ClassVar[str]\n    ARTIFACTS_LOCATION: ClassVar[str]\n\n    SOURCE_URI_PREFIX = \"__BUILTIN__\"\n\n    @classmethod\n    def from_challenge_spec(\n        cls, spec: BuiltinChallengeSpec\n    ) -> type[\"BuiltinChallenge\"]:\n        if not spec.spec_file:\n            raise ValueError(\"spec.spec_file not defined\")\n\n        challenge_info = ChallengeInfo(\n            eval_id=spec.eval_id,\n            name=spec.name,\n            task=spec.task,\n            task_artifacts_dir=spec.spec_file.parent,\n            category=spec.category,\n            difficulty=spec.info.difficulty,\n            description=spec.info.description,\n            dependencies=spec.dependencies,\n            reference_answer=spec.ground.answer,\n            source_uri=(\n                f\"__BUILTIN__/{spec.spec_file.relative_to(Path(__file__).parent)}\"\n            ),\n        )\n\n        challenge_class_name = f\"Test{challenge_info.name}\"\n        logger.debug(f\"Creating {challenge_class_name} from spec: {spec.spec_file}\")\n        return type(\n            challenge_class_name,\n            (BuiltinChallenge,),\n            {\n                \"info\": challenge_info,\n                \"_spec\": spec,\n                \"CHALLENGE_LOCATION\": str(spec.spec_file),\n                \"ARTIFACTS_LOCATION\": str(spec.spec_file.resolve().parent),\n            },\n        )\n\n    @classmethod\n    def from_challenge_spec_file(cls, spec_file: Path) -> type[\"BuiltinChallenge\"]:\n        challenge_spec = BuiltinChallengeSpec.parse_file(spec_file)\n        challenge_spec.spec_file = spec_file\n        return cls.from_challenge_spec(challenge_spec)\n\n    @classmethod\n    def from_source_uri(cls, source_uri: str) -> type[\"BuiltinChallenge\"]:\n        if not source_uri.startswith(cls.SOURCE_URI_PREFIX):\n            raise ValueError(f\"Invalid source_uri for BuiltinChallenge: {source_uri}\")\n\n        path = source_uri.split(\"/\", 1)[1]\n        spec_file = Path(__file__).parent / path\n        return cls.from_challenge_spec_file(spec_file)\n\n    @pytest.mark.asyncio\n    async def test_method(\n        self,\n        config: AgentBenchmarkConfig,\n        request: pytest.FixtureRequest,\n        i_attempt: int,\n    ) -> None:\n        # if os.environ.get(\"HELICONE_API_KEY\"):\n        #     from helicone.lock import HeliconeLockManager\n\n        #     HeliconeLockManager.write_custom_property(\"challenge\", self.info.name)\n\n        timeout = self._spec.cutoff or 60\n\n        if request.config.getoption(\"--nc\"):\n            timeout = 100000\n        elif cutoff := request.config.getoption(\"--cutoff\"):\n            timeout = int(cutoff)  # type: ignore\n\n        task_id = \"\"\n        n_steps = 0\n        timed_out = None\n        agent_task_cost = None\n        steps: list[Step] = []\n        try:\n            async for step in self.run_challenge(\n                config, timeout, mock=bool(request.config.getoption(\"--mock\"))\n            ):\n                if not task_id:\n                    task_id = step.task_id\n\n                n_steps += 1\n                steps.append(step.copy())\n                if step.additional_output:\n                    agent_task_cost = step.additional_output.get(\n                        \"task_total_cost\",\n                        step.additional_output.get(\"task_cumulative_cost\"),\n                    )\n            timed_out = False\n        except TimeoutError:\n            timed_out = True\n\n        assert isinstance(request.node, pytest.Item)\n        request.node.user_properties.append((\"steps\", steps))\n        request.node.user_properties.append((\"n_steps\", n_steps))\n        request.node.user_properties.append((\"timed_out\", timed_out))\n        request.node.user_properties.append((\"agent_task_cost\", agent_task_cost))\n\n        agent_client_config = ClientConfig(host=config.host)\n        async with ApiClient(agent_client_config) as api_client:\n            api_instance = AgentApi(api_client)\n            eval_results = await self.evaluate_task_state(api_instance, task_id)\n\n        if not eval_results:\n            if timed_out:\n                raise TimeoutError(\"Timed out, no results to evaluate\")\n            else:\n                raise ValueError(\"No results to evaluate\")\n\n        request.node.user_properties.append(\n            (\n                \"answers\",\n                [r.result for r in eval_results]\n                if request.config.getoption(\"--keep-answers\")\n                else None,\n            )\n        )\n        request.node.user_properties.append((\"scores\", [r.score for r in eval_results]))\n\n        # FIXME: this allows partial failure\n        assert any(r.passed for r in eval_results), (\n            f\"No passed evals: {eval_results}\"\n            if not timed_out\n            else f\"Timed out; no passed evals: {eval_results}\"\n        )\n\n    @classmethod\n    async def evaluate_task_state(\n        cls, agent: AgentApi, task_id: str\n    ) -> list[EvalResult]:\n        with tempfile.TemporaryDirectory() as workspace:\n            workspace = Path(workspace)\n            await download_agent_artifacts_into_folder(agent, task_id, workspace)\n            if cls.info.task_artifacts_dir:\n                copy_challenge_artifacts_into_workspace(\n                    cls.info.task_artifacts_dir, \"custom_python\", workspace\n                )\n\n            return list(cls.evaluate_workspace_content(workspace))\n\n    @classmethod\n    def evaluate_workspace_content(cls, workspace: Path) -> Iterator[EvalResult]:\n        result_ground = cls._spec.ground\n        outputs_for_eval = cls.get_outputs_for_eval(workspace, result_ground)\n\n        if result_ground.should_contain or result_ground.should_not_contain:\n            for source, content in outputs_for_eval:\n                score = cls.score_result(content, result_ground)\n                if score is not None:\n                    print(f\"{Fore.GREEN}Your score is:{Style.RESET_ALL}\", score)\n                    yield EvalResult(\n                        result=content,\n                        result_source=str(source),\n                        score=score,\n                        passed=score > 0.9,  # FIXME: arbitrary threshold\n                    )\n\n        if result_ground.eval.type in (\"python\", \"pytest\"):\n            for py_file, output in outputs_for_eval:\n                yield EvalResult(\n                    result=output,\n                    result_source=str(py_file),\n                    score=float(not output.startswith(\"Error:\")),\n                    passed=not output.startswith(\"Error:\"),\n                )\n\n        if result_ground.eval.type == \"llm\":\n            combined_results = \"\\n\".join(output[1] for output in outputs_for_eval)\n            llm_eval = cls.score_result_with_llm(combined_results, result_ground)\n            print(f\"{Fore.GREEN}Your score is:{Style.RESET_ALL}\", llm_eval)\n            if result_ground.eval.scoring == \"percentage\":\n                score = llm_eval / 100\n            elif result_ground.eval.scoring == \"scale\":\n                score = llm_eval / 10\n            else:\n                score = llm_eval\n\n            yield EvalResult(\n                result=combined_results,\n                result_source=\", \".join(str(res[0]) for res in outputs_for_eval),\n                score=score,\n                passed=score > 0.9,  # FIXME: arbitrary threshold\n            )\n\n    @staticmethod\n    def get_outputs_for_eval(\n        workspace: str | Path | dict[str, str], ground: BuiltinChallengeSpec.Ground\n    ) -> Iterator[tuple[str | Path, str]]:\n        if isinstance(workspace, dict):\n            workspace = workspace[\"output\"]\n\n        script_dir = workspace\n\n        for file_pattern in ground.files:\n            # Check if it is a file extension\n            if file_pattern.startswith(\".\"):\n                # Find all files with the given extension in the workspace\n                matching_files = glob.glob(os.path.join(script_dir, \"*\" + file_pattern))\n            else:\n                # Otherwise, it is a specific file\n                matching_files = [os.path.join(script_dir, file_pattern)]\n\n            logger.debug(\n                f\"Files to evaluate for pattern `{file_pattern}`: {matching_files}\"\n            )\n\n            for file_path in matching_files:\n                relative_file_path = Path(file_path).relative_to(workspace)\n                logger.debug(\n                    f\"Evaluating {relative_file_path} \"\n                    f\"(eval type: {ground.eval.type})...\"\n                )\n                if ground.eval.type == \"python\":\n                    result = subprocess.run(\n                        [sys.executable, file_path],\n                        cwd=os.path.abspath(workspace),\n                        capture_output=True,\n                        text=True,\n                    )\n                    if \"error\" in result.stderr or result.returncode != 0:\n                        yield relative_file_path, f\"Error: {result.stderr}\\n\"\n                    else:\n                        yield relative_file_path, f\"Output: {result.stdout}\\n\"\n                else:\n                    with open(file_path, \"r\") as f:\n                        yield relative_file_path, f.read()\n        else:\n            if ground.eval.type == \"pytest\":\n                result = subprocess.run(\n                    [sys.executable, \"-m\", \"pytest\"],\n                    cwd=os.path.abspath(workspace),\n                    capture_output=True,\n                    text=True,\n                )\n                logger.debug(f\"EXIT CODE: {result.returncode}\")\n                logger.debug(f\"STDOUT: {result.stdout}\")\n                logger.debug(f\"STDERR: {result.stderr}\")\n                if \"error\" in result.stderr or result.returncode != 0:\n                    yield \"pytest\", f\"Error: {result.stderr.strip() or result.stdout}\\n\"\n                else:\n                    yield \"pytest\", f\"Output: {result.stdout}\\n\"\n\n    @staticmethod\n    def score_result(content: str, ground: BuiltinChallengeSpec.Ground) -> float | None:\n        print(f\"{Fore.BLUE}Scoring content:{Style.RESET_ALL}\", content)\n        if ground.should_contain:\n            for should_contain_word in ground.should_contain:\n                if not ground.case_sensitive:\n                    should_contain_word = should_contain_word.lower()\n                    content = content.lower()\n                print_content = (\n                    f\"{Fore.BLUE}Word that should exist{Style.RESET_ALL}\"\n                    f\" - {should_contain_word}:\"\n                )\n                if should_contain_word not in content:\n                    print(print_content, \"False\")\n                    return 0.0\n                else:\n                    print(print_content, \"True\")\n                    return 1.0\n\n        if ground.should_not_contain:\n            for should_not_contain_word in ground.should_not_contain:\n                if not ground.case_sensitive:\n                    should_not_contain_word = should_not_contain_word.lower()\n                    content = content.lower()\n                print_content = (\n                    f\"{Fore.BLUE}Word that should not exist{Style.RESET_ALL}\"\n                    f\" - {should_not_contain_word}:\"\n                )\n                if should_not_contain_word in content:\n                    print(print_content, \"False\")\n                    return 0.0\n                else:\n                    print(print_content, \"True\")\n                    return 1.0\n\n    @classmethod\n    def score_result_with_llm(\n        cls, content: str, ground: BuiltinChallengeSpec.Ground, *, mock: bool = False\n    ) -> float:\n        if mock:\n            return 1.0\n\n        # the validation for this is done in the Eval BaseModel\n        scoring = SCORING_MAP[ground.eval.scoring]  # type: ignore\n        prompt = PROMPT_MAP[ground.eval.template].format(  # type: ignore\n            task=cls._spec.task, scoring=scoring, answer=ground.answer, response=content\n        )\n\n        if ground.eval.examples:\n            prompt += FEW_SHOT_EXAMPLES.format(examples=ground.eval.examples)\n\n        prompt += END_PROMPT\n\n        answer = get_openai_client().chat.completions.create(\n            model=\"gpt-4\",\n            messages=[\n                {\"role\": \"system\", \"content\": prompt},\n            ],\n        )\n\n        return float(answer.choices[0].message.content)  # type: ignore\n\n\ndef load_builtin_challenges() -> Iterator[type[BuiltinChallenge]]:\n    logger.info(\"Loading built-in challenges...\")\n\n    challenges_path = Path(__file__).parent\n    logger.debug(f\"Looking for challenge spec files in {challenges_path}...\")\n\n    json_files = deque(challenges_path.rglob(\"data.json\"))\n\n    logger.debug(f\"Found {len(json_files)} built-in challenges.\")\n\n    loaded, ignored = 0, 0\n    while json_files:\n        # Take and remove the first element from json_files\n        json_file = json_files.popleft()\n        if _challenge_should_be_ignored(json_file):\n            ignored += 1\n            continue\n\n        challenge = BuiltinChallenge.from_challenge_spec_file(json_file)\n        logger.debug(f\"Generated test for {challenge.info.name}\")\n        yield challenge\n\n        loaded += 1\n\n    logger.info(\n        f\"Loading built-in challenges complete: loaded {loaded}, ignored {ignored}.\"\n    )\n\n\ndef _challenge_should_be_ignored(json_file_path: Path):\n    return (\n        \"challenges/deprecated\" in json_file_path.as_posix()\n        or \"challenges/library\" in json_file_path.as_posix()\n    )\n", "benchmark/agbenchmark/challenges/base.py": "import logging\nfrom abc import ABC, abstractmethod\nfrom pathlib import Path\nfrom typing import AsyncIterator, Awaitable, ClassVar, Optional\n\nimport pytest\nfrom agent_protocol_client import AgentApi, Step\nfrom colorama import Fore, Style\nfrom pydantic import BaseModel, Field\n\nfrom agbenchmark.config import AgentBenchmarkConfig\nfrom agbenchmark.utils.data_types import Category, DifficultyLevel, EvalResult\n\nlogger = logging.getLogger(__name__)\n\n\nclass ChallengeInfo(BaseModel):\n    eval_id: str = \"\"\n    name: str\n    task: str\n    task_artifacts_dir: Optional[Path] = None\n    category: list[Category]\n    difficulty: Optional[DifficultyLevel] = None\n    description: Optional[str] = None\n    dependencies: list[str] = Field(default_factory=list)\n    reference_answer: Optional[str]\n\n    source_uri: str\n    \"\"\"Internal reference indicating the source of the challenge specification\"\"\"\n\n    available: bool = True\n    unavailable_reason: str = \"\"\n\n\nclass BaseChallenge(ABC):\n    \"\"\"\n    The base class and shared interface for all specific challenge implementations.\n    \"\"\"\n\n    info: ClassVar[ChallengeInfo]\n\n    @classmethod\n    @abstractmethod\n    def from_source_uri(cls, source_uri: str) -> type[\"BaseChallenge\"]:\n        \"\"\"\n        Construct an individual challenge subclass from a suitable `source_uri` (as in\n        `ChallengeInfo.source_uri`).\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def test_method(\n        self,\n        config: AgentBenchmarkConfig,\n        request: pytest.FixtureRequest,\n        i_attempt: int,\n    ) -> None | Awaitable[None]:\n        \"\"\"\n        Test method for use by Pytest-based benchmark sessions. Should return normally\n        if the challenge passes, and raise a (preferably descriptive) error otherwise.\n        \"\"\"\n        ...\n\n    @classmethod\n    async def run_challenge(\n        cls, config: AgentBenchmarkConfig, timeout: int, *, mock: bool = False\n    ) -> AsyncIterator[Step]:\n        \"\"\"\n        Runs the challenge on the subject agent with the specified timeout.\n        Also prints basic challenge and status info to STDOUT.\n\n        Params:\n            config: The subject agent's benchmark config.\n            timeout: Timeout (seconds) after which to stop the run if not finished.\n\n        Yields:\n            Step: The steps generated by the agent for the challenge task.\n        \"\"\"\n        # avoid circular import\n        from agbenchmark.agent_api_interface import run_api_agent\n\n        print()\n        print(\n            f\"{Fore.MAGENTA + Style.BRIGHT}{'='*24} \"\n            f\"Starting {cls.info.name} challenge\"\n            f\" {'='*24}{Style.RESET_ALL}\"\n        )\n        print(f\"{Fore.CYAN}Timeout:{Fore.RESET} {timeout} seconds\")\n        print(f\"{Fore.CYAN}Task:{Fore.RESET} {cls.info.task}\")\n\n        print()\n        logger.debug(f\"Starting {cls.info.name} challenge run\")\n        i = 0\n        async for step in run_api_agent(\n            cls.info.task, config, timeout, cls.info.task_artifacts_dir, mock=mock\n        ):\n            i += 1\n            print(f\"[{cls.info.name}] - step {step.name} ({i}. request)\")\n            yield step\n        logger.debug(f\"Finished {cls.info.name} challenge run\")\n\n    @classmethod\n    @abstractmethod\n    async def evaluate_task_state(\n        cls, agent: AgentApi, task_id: str\n    ) -> list[EvalResult]:\n        ...\n", "benchmark/agbenchmark/challenges/webarena.py": "import logging\nimport os\nfrom abc import ABC, abstractmethod\nfrom typing import ClassVar, Iterator, Literal\n\nimport pytest\nimport requests\nfrom agent_protocol_client import AgentApi, Step\nfrom pydantic import BaseModel, ValidationError, validator\n\nfrom agbenchmark.config import AgentBenchmarkConfig\nfrom agbenchmark.utils.data_types import Category, EvalResult\n\nfrom .base import BaseChallenge, ChallengeInfo\n\nlogger = logging.getLogger(__name__)\n\n\nEvalType = Literal[\"string_match\", \"url_match\", \"program_html\"]\nWebArenaSite = Literal[\n    \"gitlab\", \"map\", \"reddit\", \"shopping\", \"shopping_admin\", \"wikipedia\"\n]\nReferenceAnswerType = Literal[\"exact_match\", \"fuzzy_match\", \"must_include\"]\n\n\nclass WebArenaSiteInfo(BaseModel):\n    base_url: str\n    available: bool = True\n    additional_info: str = \"\"\n    unavailable_reason: str = \"\"\n\n\n_git_user, _git_password = os.getenv(\"WEBARENA_GIT_CREDENTIALS\", \":\").split(\":\")\n\nsite_info_map: dict[WebArenaSite, WebArenaSiteInfo] = {\n    \"gitlab\": WebArenaSiteInfo(\n        base_url=\"http://git.junglegym.ai\",\n        available=bool(_git_user and _git_password),\n        additional_info=(\n            f\"To log in to {{url}}, use the username '{_git_user}' \"\n            f\"and password '{_git_password}'.\"\n        ),\n        unavailable_reason=(\n            \"WEBARENA_GIT_CREDENTIALS not set (correctly): \"\n            f\"'{os.getenv('WEBARENA_GIT_CREDENTIALS', '')}', \"\n            \"should be USERNAME:PASSWORD.\"\n        ),\n    ),\n    \"map\": WebArenaSiteInfo(\n        base_url=\"http://ec2-3-131-244-37.us-east-2.compute.amazonaws.com:3000/\"\n    ),\n    \"reddit\": WebArenaSiteInfo(base_url=\"http://forum.junglegym.ai\"),\n    \"shopping\": WebArenaSiteInfo(base_url=\"http://shop.junglegym.ai\"),\n    \"shopping_admin\": WebArenaSiteInfo(\n        base_url=\"http://cms.junglegym.ai/admin\",\n        additional_info=(\n            \"To log in to {url}, use the username 'admin' and password 'admin1234'.\"\n        ),\n    ),\n    \"wikipedia\": WebArenaSiteInfo(base_url=\"http://wiki.junglegym.ai\"),\n}\n\n\ndef get_site_info(site: WebArenaSite) -> WebArenaSiteInfo:\n    if site not in site_info_map:\n        raise ValueError(f\"JungleGym site '{site}' unknown, cannot resolve URL\")\n    return site_info_map[site]\n\n\ndef get_site_url(site: WebArenaSite) -> str:\n    return get_site_info(site).base_url\n\n\ndef resolve_uri(uri: str) -> str:\n    \"\"\"\n    Resolves URIs with mock hosts, like `__WIKI__/wiki/Octopus`, with the corresponding\n    JungleGym site mirror host.\n    \"\"\"\n    segments = uri.split(\"__\")\n    if len(segments) > 2 and (site := segments[1]).lower() in site_info_map:\n        return uri.replace(f\"__{site}__\", get_site_url(site.lower()))  # type: ignore\n    return uri\n\n\nclass Eval(ABC):\n    @abstractmethod\n    def evaluate(self, string: str) -> bool:\n        ...\n\n    @property\n    @abstractmethod\n    def description(self) -> str:\n        ...\n\n\nclass BaseStringEval(BaseModel, Eval):\n    # type: ReferenceAnswerType\n    pass\n\n\nclass ExactStringMatchEval(BaseStringEval):\n    type: Literal[\"exact_match\"] = \"exact_match\"\n    reference_answer: str\n\n    @property\n    def description(self) -> str:\n        return f\"Answer must be '{self.reference_answer}'\"\n\n    def evaluate(self, string: str) -> bool:\n        return string == self.reference_answer\n\n\nclass FuzzyStringMatchEval(BaseStringEval):\n    type: Literal[\"fuzzy_match\"] = \"fuzzy_match\"\n    reference_answer: str\n\n    @property\n    def description(self) -> str:\n        return f\"Answer must contain something like '{self.reference_answer}'\"\n\n    def evaluate(self, string: str) -> bool:\n        # TODO: use LLM for matching (or something else that's flexible/robust)\n        return self.reference_answer.lower() in string.lower()\n\n\nclass MustIncludeStringEval(BaseStringEval):\n    type: Literal[\"must_include\"] = \"must_include\"\n    reference_answer: str\n\n    @property\n    def description(self) -> str:\n        return f\"Answer must include '{self.reference_answer}'\"\n\n    def evaluate(self, string: str) -> bool:\n        return self.reference_answer.lower() in string.lower()\n\n\nStringEval = ExactStringMatchEval | FuzzyStringMatchEval | MustIncludeStringEval\n\n\nclass UrlMatchEval(BaseModel, Eval):\n    url: str\n    \"\"\"Example: `\"__WIKI__/wiki/Octopus\"`\"\"\"\n\n    @property\n    def description(self) -> str:\n        return f\"Agent must navigate to '{self.url}'\"\n\n    def evaluate(self, string: str) -> bool:\n        return string == resolve_uri(self.url)\n\n\nclass ProgramHtmlEval(BaseModel):\n    url: str\n    locator: str\n    \"\"\"JavaScript code that returns the value to check\"\"\"\n    required_contents: str\n\n    @property\n    def description(self) -> str:\n        return (\n            f\"On the webpage {self.url}, \"\n            f\"`{self.locator}` should contain '{self.required_contents}'\"\n        )\n\n    def evaluate(self, selenium_instance) -> bool:\n        result = selenium_instance.execute_script(\n            self.locator or \"return document.body.innerHTML;\"\n        )\n        return self.required_contents in result\n\n\n_Eval = StringEval | UrlMatchEval | ProgramHtmlEval\n\n\nclass WebArenaChallengeSpec(BaseModel):\n    task_id: int\n    sites: list[WebArenaSite]\n    \"\"\"The sites needed to complete the task\"\"\"\n    start_url: str\n    \"\"\"The full URL at which to start\"\"\"\n    start_url_junglegym: str\n    \"\"\"The JungleGym site (base URL) at which to start\"\"\"\n    require_login: bool\n    require_reset: bool\n    storage_state: str | None\n\n    intent: str\n    intent_template: str\n    intent_template_id: int\n    instantiation_dict: dict[str, str | list[str]]\n\n    available: bool = True\n    unavailable_reason: str = \"\"\n\n    class EvalSet(BaseModel):\n        class StringMatchEvalSet(BaseModel):\n            exact_match: str | None\n            fuzzy_match: list[str] | None\n            must_include: list[str] | None\n\n        reference_answers: StringMatchEvalSet | None\n        \"\"\"For string_match eval, a set of criteria to judge the final answer\"\"\"\n        reference_answer_raw_annotation: str | None\n        string_note: str | None\n        annotation_note: str | None\n\n        reference_url: str | None\n        \"\"\"For url_match eval, the last URL that should be visited\"\"\"\n        url_note: str | None\n\n        program_html: list[ProgramHtmlEval]\n        \"\"\"For program_html eval, a list of criteria to judge the site state by\"\"\"\n\n        eval_types: list[EvalType]\n\n        @validator(\"eval_types\")\n        def check_eval_parameters(cls, v: list[EvalType], values):\n            if \"string_match\" in v and not values.get(\"reference_answers\"):\n                raise ValueError(\"'string_match' eval_type requires reference_answers\")\n            if \"url_match\" in v and not values.get(\"reference_url\"):\n                raise ValueError(\"'url_match' eval_type requires reference_url\")\n            if \"program_html\" in v and not values.get(\"program_html\"):\n                raise ValueError(\n                    \"'program_html' eval_type requires at least one program_html eval\"\n                )\n            return v\n\n        @property\n        def evaluators(self) -> list[_Eval]:\n            evaluators: list[_Eval] = []\n            if self.reference_answers:\n                if self.reference_answers.exact_match:\n                    evaluators.append(\n                        ExactStringMatchEval(\n                            reference_answer=self.reference_answers.exact_match\n                        )\n                    )\n                if self.reference_answers.fuzzy_match:\n                    evaluators.extend(\n                        FuzzyStringMatchEval(reference_answer=a)\n                        for a in self.reference_answers.fuzzy_match\n                    )\n                if self.reference_answers.must_include:\n                    evaluators.extend(\n                        MustIncludeStringEval(reference_answer=a)\n                        for a in self.reference_answers.must_include\n                    )\n            if self.reference_url:\n                evaluators.append(UrlMatchEval(url=self.reference_url))\n            evaluators.extend(self.program_html)\n            return evaluators\n\n    eval: EvalSet\n    \"\"\"Evaluation criteria by which to judge the agent's performance\"\"\"\n\n    @property\n    def assignment_for_agent(self):\n        sites = [get_site_info(s) for s in self.sites]\n        nav_constraint = (\n            \"You are ONLY allowed to access URLs in \"\n            f\"{' and '.join(s.base_url for s in sites)}.\\n\\n\"\n            + \"\\n\".join(\n                s.additional_info.format(url=s.base_url)\n                for s in sites\n                if s.additional_info\n            )\n        ).strip()\n\n        return (\n            f\"First of all, go to {self.start_url}. \"\n            f\"{self.intent.rstrip('.')}.\\n\"\n            f\"{nav_constraint}\"\n        )\n\n\nclass WebArenaChallenge(BaseChallenge):\n    _spec: ClassVar[WebArenaChallengeSpec]\n\n    SOURCE_URI_PREFIX = \"__JUNGLEGYM__/webarena/tasks/\"\n    SOURCE_URI_TEMPLATE = f\"{SOURCE_URI_PREFIX}{{task_id}}\"\n\n    @classmethod\n    def from_source_uri(cls, source_uri: str) -> type[\"WebArenaChallenge\"]:\n        if not source_uri.startswith(cls.SOURCE_URI_PREFIX):\n            raise ValueError(f\"Invalid source_uri for WebArenaChallenge: {source_uri}\")\n\n        source_url = source_uri.replace(\n            cls.SOURCE_URI_PREFIX,\n            \"https://api.junglegym.ai/get_webarena_by_task_id?task_id=\",\n        )\n        results = requests.get(source_url).json()[\"data\"]\n        if not results:\n            raise ValueError(f\"Could not fetch challenge {source_uri}\")\n        return cls.from_challenge_spec(WebArenaChallengeSpec.parse_obj(results[0]))\n\n    @classmethod\n    def from_challenge_spec(\n        cls, spec: WebArenaChallengeSpec\n    ) -> type[\"WebArenaChallenge\"]:\n        challenge_info = ChallengeInfo(\n            eval_id=f\"junglegym-webarena-{spec.task_id}\",\n            name=f\"WebArenaTask_{spec.task_id}\",\n            task=spec.assignment_for_agent,\n            category=[\n                Category.GENERALIST,\n                Category.WEB,\n            ],  # TODO: make categories more specific\n            reference_answer=spec.eval.reference_answer_raw_annotation,\n            source_uri=cls.SOURCE_URI_TEMPLATE.format(task_id=spec.task_id),\n            available=spec.available,\n            unavailable_reason=spec.unavailable_reason,\n        )\n        return type(\n            f\"Test{challenge_info.name}\",\n            (WebArenaChallenge,),\n            {\n                \"info\": challenge_info,\n                \"_spec\": spec,\n            },\n        )\n\n    @classmethod\n    def evaluate_answer(cls, answer: str) -> list[tuple[_Eval, EvalResult]]:\n        results: list[tuple[_Eval, EvalResult]] = []\n        for evaluator in cls._spec.eval.evaluators:\n            if isinstance(evaluator, StringEval):  # string_match\n                results.append(\n                    (\n                        evaluator,\n                        EvalResult(\n                            result=answer,\n                            result_source=\"step_output\",\n                            score=evaluator.evaluate(answer),\n                            passed=evaluator.evaluate(answer),\n                        ),\n                    )\n                )\n        return results\n\n    @classmethod\n    def evaluate_step_result(\n        cls, step: Step, *, mock: bool = False\n    ) -> list[tuple[_Eval, EvalResult]]:\n        if mock:\n            step.output = cls.info.reference_answer\n        assert step.output\n        eval_results = cls.evaluate_answer(step.output)\n        for eval in cls._spec.eval.evaluators:\n            if isinstance(eval, UrlMatchEval):\n                passed = resolve_uri(eval.url) in step.output  # HACK: url_match bodge\n                eval_results.append(\n                    (\n                        eval,\n                        EvalResult(\n                            result=step.output,\n                            result_source=\"step_output\",\n                            score=1.0 if passed else 0.0,\n                            passed=passed,\n                        ),\n                    )\n                )\n            # TODO: add support for program_html evals\n        return eval_results\n\n    @classmethod\n    async def evaluate_task_state(\n        cls, agent: AgentApi, task_id: str\n    ) -> list[EvalResult]:\n        steps: list[Step] = (await agent.list_agent_task_steps(task_id)).steps\n\n        eval_results_per_step = [cls.evaluate_step_result(step) for step in steps]\n        # Get the column aggregate (highest scored EvalResult for each Eval)\n        # from the matrix of EvalResults per step.\n        return [\n            max(step_results_for_eval, key=lambda r: r[1].score)[1]\n            for step_results_for_eval in zip(*eval_results_per_step)\n        ]\n\n    @pytest.mark.asyncio\n    async def test_method(\n        self,\n        config: AgentBenchmarkConfig,\n        request: pytest.FixtureRequest,\n        i_attempt: int,\n    ) -> None:\n        if not self._spec.available:\n            pytest.skip(self._spec.unavailable_reason)\n\n        # if os.environ.get(\"HELICONE_API_KEY\"):\n        #     from helicone.lock import HeliconeLockManager\n\n        #     HeliconeLockManager.write_custom_property(\"challenge\", self.info.name)\n\n        timeout = 120\n        if request.config.getoption(\"--nc\"):\n            timeout = 100000\n        elif cutoff := request.config.getoption(\"--cutoff\"):\n            timeout = int(cutoff)  # type: ignore\n\n        assert isinstance(request.node, pytest.Item)\n\n        n_steps = 0\n        timed_out = None\n        agent_task_cost = None\n        steps: list[Step] = []\n        eval_results_per_step: list[list[tuple[_Eval, EvalResult]]] = []\n        try:\n            async for step in self.run_challenge(\n                config, timeout, mock=bool(request.config.getoption(\"--mock\"))\n            ):\n                if not step.output:\n                    logger.warn(f\"Step has no output: {step}\")\n                    continue\n\n                n_steps += 1\n                steps.append(step)\n                if step.additional_output:\n                    agent_task_cost = step.additional_output.get(\n                        \"task_total_cost\",\n                        step.additional_output.get(\"task_cumulative_cost\"),\n                    )\n\n                step_eval_results = self.evaluate_step_result(\n                    step, mock=bool(request.config.getoption(\"--mock\"))\n                )\n                logger.debug(f\"Intermediary results: {step_eval_results}\")\n                eval_results_per_step.append(step_eval_results)\n                if step.is_last:\n                    request.node.user_properties.append(\n                        (\n                            \"answers\",\n                            step.output\n                            if request.config.getoption(\"--keep-answers\")\n                            else None,\n                        )\n                    )\n            timed_out = False\n        except TimeoutError:\n            timed_out = True\n        request.node.user_properties.append((\"steps\", steps))\n        request.node.user_properties.append((\"n_steps\", n_steps))\n        request.node.user_properties.append((\"timed_out\", timed_out))\n        request.node.user_properties.append((\"agent_task_cost\", agent_task_cost))\n\n        # Get the column aggregate (highest score for each Eval)\n        # from the matrix of EvalResults per step.\n        evals_results = [\n            max(step_results_for_eval, key=lambda r: r[1].score)\n            for step_results_for_eval in zip(*eval_results_per_step)\n        ]\n\n        if not evals_results:\n            if timed_out:\n                raise TimeoutError(\"Timed out, no results to evaluate\")\n            else:\n                raise ValueError(\"No results to evaluate\")\n\n        request.node.user_properties.append(\n            (\"scores\", [r[1].score for r in evals_results])\n        )\n\n        # FIXME: arbitrary threshold\n        assert all(r[1].score > 0.9 for r in evals_results), (\n            \"Scores insufficient:\\n\\n\"\n            if not timed_out\n            else \"Timed out; scores insufficient:\\n\\n\"\n        ) + \"\\n\".join(f\"{repr(r[0])}\\n  -> {repr(r[1])}\" for r in evals_results)\n\n\ndef load_webarena_challenges(\n    skip_unavailable: bool = True,\n) -> Iterator[type[WebArenaChallenge]]:\n    logger.info(\"Loading WebArena challenges...\")\n\n    for site, info in site_info_map.items():\n        if not info.available and skip_unavailable:\n            logger.warning(\n                f\"JungleGym site '{site}' is not available: {info.unavailable_reason} \"\n                \"Skipping all challenges which use this site.\"\n            )\n\n    # response = requests.get(\"https://api.junglegym.ai/get_full_webarena_dataset\")\n    # challenge_dicts = response.json()[\"data\"]\n\n    # Until the full WebArena challenge set is supported, use a hand-picked selection\n    import json\n    from pathlib import Path\n\n    challenge_dicts = json.loads(\n        (Path(__file__).parent / \"webarena_selection.json\").read_bytes()\n    )\n\n    logger.debug(\n        \"Fetched WebArena dataset. \"\n        f\"Constructing {len(challenge_dicts)} WebArenaChallenges...\"\n    )\n    loaded = 0\n    failed = 0\n    skipped = 0\n    for entry in challenge_dicts:\n        try:\n            challenge_spec = WebArenaChallengeSpec.parse_obj(entry)\n        except ValidationError as e:\n            failed += 1\n            logger.warning(f\"Error validating WebArena challenge entry: {entry}\")\n            logger.warning(f\"Error details: {e}\")\n            continue\n\n        # Check all required sites for availability\n        for site in challenge_spec.sites:\n            site_info = site_info_map.get(site)\n            if site_info is None:\n                challenge_spec.available = False\n                challenge_spec.unavailable_reason = (\n                    f\"WebArena task {challenge_spec.task_id} requires unknown site \"\n                    f\"'{site}'\"\n                )\n            elif not site_info.available:\n                challenge_spec.available = False\n                challenge_spec.unavailable_reason = (\n                    f\"WebArena task {challenge_spec.task_id} requires unavailable \"\n                    f\"site '{site}'\"\n                )\n\n        if not challenge_spec.available and skip_unavailable:\n            logger.debug(f\"{challenge_spec.unavailable_reason}; skipping...\")\n            skipped += 1\n            continue\n\n        yield WebArenaChallenge.from_challenge_spec(challenge_spec)\n        loaded += 1\n\n    logger.info(\n        \"Loading WebArena challenges complete: \"\n        f\"loaded {loaded}, skipped {skipped}.\"\n        + (f\" {failed} challenges failed to load.\" if failed else \"\")\n    )\n", "benchmark/agbenchmark/challenges/__init__.py": "import glob\nimport json\nimport logging\nfrom pathlib import Path\n\nfrom .base import BaseChallenge, ChallengeInfo\nfrom .builtin import OPTIONAL_CATEGORIES\n\nlogger = logging.getLogger(__name__)\n\n\ndef get_challenge_from_source_uri(source_uri: str) -> type[BaseChallenge]:\n    from .builtin import BuiltinChallenge\n    from .webarena import WebArenaChallenge\n\n    provider_prefix = source_uri.split(\"/\", 1)[0]\n\n    if provider_prefix == BuiltinChallenge.SOURCE_URI_PREFIX:\n        return BuiltinChallenge.from_source_uri(source_uri)\n\n    if provider_prefix == WebArenaChallenge.SOURCE_URI_PREFIX:\n        return WebArenaChallenge.from_source_uri(source_uri)\n\n    raise ValueError(f\"Cannot resolve source_uri '{source_uri}'\")\n\n\ndef get_unique_categories() -> set[str]:\n    \"\"\"\n    Reads all challenge spec files and returns a set of all their categories.\n    \"\"\"\n    categories = set()\n\n    challenges_dir = Path(__file__).parent\n    glob_path = f\"{challenges_dir}/**/data.json\"\n\n    for data_file in glob.glob(glob_path, recursive=True):\n        with open(data_file, \"r\") as f:\n            try:\n                challenge_data = json.load(f)\n                categories.update(challenge_data.get(\"category\", []))\n            except json.JSONDecodeError:\n                logger.error(f\"Error: {data_file} is not a valid JSON file.\")\n                continue\n            except IOError:\n                logger.error(f\"IOError: file could not be read: {data_file}\")\n                continue\n\n    return categories\n\n\n__all__ = [\n    \"BaseChallenge\",\n    \"ChallengeInfo\",\n    \"get_unique_categories\",\n    \"OPTIONAL_CATEGORIES\",\n]\n", "benchmark/agbenchmark/challenges/library/ethereum/check_price/artifacts_in/__init__.py": "", "benchmark/agbenchmark/challenges/library/ethereum/check_price/artifacts_in/test.py": "import re\n\nfrom sample_code import get_ethereum_price\n\n\ndef test_get_ethereum_price() -> None:\n    # Read the Ethereum price from the file\n    with open(\"eth_price.txt\", \"r\") as file:\n        eth_price = file.read().strip()\n\n    # Validate that the eth price is all digits\n    pattern = r\"^\\d+$\"\n    matches = re.match(pattern, eth_price) is not None\n    assert (\n        matches\n    ), f\"AssertionError: Ethereum price should be all digits, but got {eth_price}\"\n\n    # Get the current price of Ethereum\n    real_eth_price = get_ethereum_price()\n\n    # Convert the eth price to a numerical value for comparison\n    eth_price_value = float(eth_price)\n    real_eth_price_value = float(real_eth_price)\n\n    # Check if the eth price is within $50 of the actual Ethereum price\n    assert abs(real_eth_price_value - eth_price_value) <= 50, (\n        \"AssertionError: Ethereum price is not within $50 of the actual Ethereum price \"\n        f\"(Provided price: ${eth_price}, Real price: ${real_eth_price})\"\n    )\n\n    print(\"Matches\")\n\n\nif __name__ == \"__main__\":\n    test_get_ethereum_price()\n", "benchmark/agbenchmark/challenges/library/ethereum/check_price/artifacts_in/sample_code.py": "import requests\n\n\ndef get_ethereum_price() -> float:\n    url = \"https://api.coingecko.com/api/v3/simple/price?ids=ethereum&vs_currencies=usd\"\n    response = requests.get(url)\n\n    if response.status_code == 200:\n        data = response.json()\n        return data[\"ethereum\"][\"usd\"]\n    else:\n        raise Exception(f\"Failed to fetch data: {response.status_code}\")\n", "benchmark/agbenchmark/challenges/library/ethereum/check_price/artifacts_out/__init__.py": "", "benchmark/agbenchmark/challenges/library/ethereum/check_price/artifacts_out/test.py": "import re\n\nfrom sample_code import get_ethereum_price\n\n\ndef test_get_ethereum_price() -> None:\n    # Read the Ethereum price from the file\n    with open(\"output.txt\", \"r\") as file:\n        eth_price = file.read().strip()\n\n    # Validate that the eth price is all digits\n    pattern = r\"^\\d+$\"\n    matches = re.match(pattern, eth_price) is not None\n    assert (\n        matches\n    ), f\"AssertionError: Ethereum price should be all digits, but got {eth_price}\"\n\n    # Get the current price of Ethereum\n    real_eth_price = get_ethereum_price()\n\n    # Convert the eth price to a numerical value for comparison\n    eth_price_value = float(eth_price)\n    real_eth_price_value = float(real_eth_price)\n\n    # Check if the eth price is within $50 of the actual Ethereum price\n    assert abs(real_eth_price_value - eth_price_value) <= 50, (\n        \"AssertionError: Ethereum price is not within $50 of the actual Ethereum price \"\n        f\"(Provided price: ${eth_price}, Real price: ${real_eth_price})\"\n    )\n\n    print(\"Matches\")\n\n\nif __name__ == \"__main__\":\n    test_get_ethereum_price()\n", "benchmark/agbenchmark/challenges/library/ethereum/check_price/artifacts_out/sample_code.py": "import requests\n\n\ndef get_ethereum_price() -> float:\n    url = \"https://api.coingecko.com/api/v3/simple/price?ids=ethereum&vs_currencies=usd\"\n    response = requests.get(url)\n\n    if response.status_code == 200:\n        data = response.json()\n        return data[\"ethereum\"][\"usd\"]\n    else:\n        raise Exception(f\"Failed to fetch data: {response.status_code}\")\n", "benchmark/agbenchmark/challenges/verticals/code/1_three_sum/custom_python/test.py": "# pyright: reportMissingImports=false\nfrom typing import List\n\nfrom sample_code import three_sum\n\n\ndef test_three_sum(nums: List[int], target: int, expected_result: List[int]) -> None:\n    result = three_sum(nums, target)\n    print(result)\n    assert (\n        result == expected_result\n    ), f\"AssertionError: Expected the output to be {expected_result}\"\n\n\nif __name__ == \"__main__\":\n    # test the trivial case with the first three numbers\n    nums = [2, 7, 11, 15]\n    target = 20\n    expected_result = [0, 1, 2]\n    test_three_sum(nums, target, expected_result)\n\n    # test for ability to use zero and the same number twice\n    nums = [2, 7, 0, 15, 12, 0]\n    target = 2\n    expected_result = [0, 2, 5]\n    test_three_sum(nums, target, expected_result)\n\n    # test for first and last index usage and negative numbers\n    nums = [-6, 7, 11, 4]\n    target = 9\n    expected_result = [0, 2, 3]\n    test_three_sum(nums, target, expected_result)\n", "benchmark/agbenchmark/challenges/verticals/code/1_three_sum/artifacts_out/__init__.py": "", "benchmark/agbenchmark/challenges/verticals/code/1_three_sum/artifacts_out/sample_code.py": "from typing import List, Optional\n\n\ndef three_sum(nums: List[int], target: int) -> Optional[List[int]]:\n    nums_indices = [(num, index) for index, num in enumerate(nums)]\n    nums_indices.sort()\n    for i in range(len(nums_indices) - 2):\n        if i > 0 and nums_indices[i] == nums_indices[i - 1]:\n            continue\n        l, r = i + 1, len(nums_indices) - 1\n        while l < r:\n            three_sum = nums_indices[i][0] + nums_indices[l][0] + nums_indices[r][0]\n            if three_sum < target:\n                l += 1\n            elif three_sum > target:\n                r -= 1\n            else:\n                indices = sorted(\n                    [nums_indices[i][1], nums_indices[l][1], nums_indices[r][1]]\n                )\n                return indices\n    return None\n", "benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_in/abstract_class.py": "from abc import ABC, abstractmethod\nfrom typing import Optional\n\nfrom pydantic import BaseModel, validator\n\n\n# Models for the request and response payloads\nclass ShipPlacement(BaseModel):\n    ship_type: str\n    start: dict  # {\"row\": int, \"column\": str}\n    direction: str\n\n    @validator(\"start\")\n    def validate_start(cls, start):\n        row, column = start.get(\"row\"), start.get(\"column\")\n\n        if not (1 <= row <= 10):\n            raise ValueError(\"Row must be between 1 and 10 inclusive.\")\n\n        if column not in list(\"ABCDEFGHIJ\"):\n            raise ValueError(\"Column must be one of A, B, C, D, E, F, G, H, I, J.\")\n\n        return start\n\n\nclass Turn(BaseModel):\n    target: dict  # {\"row\": int, \"column\": str}\n\n\nclass TurnResponse(BaseModel):\n    result: str\n    ship_type: Optional[str]  # This would be None if the result is a miss\n\n\nclass GameStatus(BaseModel):\n    is_game_over: bool\n    winner: Optional[str]\n\n\nclass Game(BaseModel):\n    game_id: str\n    players: list[str]\n    # This could represent the state of the game board,\n    # you might need to flesh this out further:\n    board: dict\n    ships: list[ShipPlacement]  # List of ship placements for this game\n    turns: list[Turn]  # List of turns that have been taken\n\n\nclass AbstractBattleship(ABC):\n    SHIP_LENGTHS = {\n        \"carrier\": 5,\n        \"battleship\": 4,\n        \"cruiser\": 3,\n        \"submarine\": 3,\n        \"destroyer\": 2,\n    }\n\n    @abstractmethod\n    def create_ship_placement(self, game_id: str, placement: ShipPlacement) -> None:\n        \"\"\"\n        Place a ship on the grid.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def create_turn(self, game_id: str, turn: Turn) -> TurnResponse:\n        \"\"\"\n        Players take turns to target a grid cell.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_game_status(self, game_id: str) -> GameStatus:\n        \"\"\"\n        Check if the game is over and get the winner if there's one.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_winner(self, game_id: str) -> str:\n        \"\"\"\n        Get the winner of the game.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_game(self) -> Game | None:\n        \"\"\"\n        Retrieve the state of the game.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def delete_game(self, game_id: str) -> None:\n        \"\"\"\n        Delete a game given its ID.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def create_game(self) -> None:\n        \"\"\"\n        Create a new game.\n\n        Returns:\n            str: The ID of the created game.\n        \"\"\"\n        pass\n", "benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_in/test_positive.py": "from abstract_class import ShipPlacement, Turn\n\n\ndef test_turns_and_results(battleship_game, initialized_game_id):\n    turn = Turn(target={\"row\": 1, \"column\": \"A\"})\n    response = battleship_game.create_turn(initialized_game_id, turn)\n\n    assert response.result in [\"hit\", \"miss\"]\n    if response.result == \"hit\":\n        assert response.ship_type == \"carrier\"\n    game = battleship_game.get_game(initialized_game_id)\n    assert turn in game.turns\n\n\ndef test_game_status_and_winner(battleship_game):\n    game_id = battleship_game.create_game()\n    status = battleship_game.get_game_status(game_id)\n    assert isinstance(status.is_game_over, bool)\n    if status.is_game_over:\n        winner = battleship_game.get_winner(game_id)\n        assert winner is not None\n\n\ndef test_delete_game(battleship_game):\n    game_id = battleship_game.create_game()\n    battleship_game.delete_game(game_id)\n    assert battleship_game.get_game(game_id) is None\n\n\ndef test_ship_rotation(battleship_game):\n    game_id = battleship_game.create_game()\n    placement_horizontal = ShipPlacement(\n        ship_type=\"battleship\", start={\"row\": 1, \"column\": \"B\"}, direction=\"horizontal\"\n    )\n    battleship_game.create_ship_placement(game_id, placement_horizontal)\n    placement_vertical = ShipPlacement(\n        ship_type=\"cruiser\", start={\"row\": 3, \"column\": \"D\"}, direction=\"vertical\"\n    )\n    battleship_game.create_ship_placement(game_id, placement_vertical)\n    game = battleship_game.get_game(game_id)\n    assert placement_horizontal in game.ships\n    assert placement_vertical in game.ships\n\n\ndef test_game_state_updates(battleship_game, initialized_game_id):\n    turn = Turn(target={\"row\": 3, \"column\": \"A\"})\n    battleship_game.create_turn(initialized_game_id, turn)\n\n    game = battleship_game.get_game(initialized_game_id)\n\n    target_key = (3, ord(\"A\") - ord(\"A\"))\n    assert target_key in game.board and game.board[target_key] == \"hit\"\n\n\ndef test_ship_sinking_feedback(battleship_game, initialized_game_id):\n    hits = [\"A\", \"B\", \"C\", \"D\"]\n    static_moves = [\n        {\"row\": 1, \"column\": \"E\"},\n        {\"row\": 1, \"column\": \"F\"},\n        {\"row\": 1, \"column\": \"G\"},\n        {\"row\": 1, \"column\": \"H\"},\n    ]\n\n    response = None\n    for index, hit in enumerate(hits):\n        turn = Turn(target={\"row\": 2, \"column\": hit})\n        response = battleship_game.create_turn(initialized_game_id, turn)\n        assert response.ship_type == \"battleship\"\n\n        static_turn = Turn(target=static_moves[index])\n        battleship_game.create_turn(initialized_game_id, static_turn)\n\n    assert response and response.result == \"sunk\"\n\n\ndef test_restart_game(battleship_game):\n    game_id = battleship_game.create_game()\n    battleship_game.delete_game(game_id)\n    game_id = (\n        battleship_game.create_game()\n    )  # Use the returned game_id after recreating the game\n    game = battleship_game.get_game(game_id)\n    assert game is not None\n\n\ndef test_ship_edge_overlapping(battleship_game):\n    game_id = battleship_game.create_game()\n\n    first_ship = ShipPlacement(\n        ship_type=\"battleship\", start={\"row\": 1, \"column\": \"A\"}, direction=\"horizontal\"\n    )\n    battleship_game.create_ship_placement(game_id, first_ship)\n\n    next_ship = ShipPlacement(\n        ship_type=\"cruiser\", start={\"row\": 1, \"column\": \"E\"}, direction=\"horizontal\"\n    )\n    battleship_game.create_ship_placement(game_id, next_ship)\n\n    game = battleship_game.get_game(game_id)\n    assert first_ship in game.ships\n    assert next_ship in game.ships\n\n\ndef test_game_state_after_ship_placement(battleship_game):\n    game_id = battleship_game.create_game()\n\n    ship_placement = ShipPlacement(\n        ship_type=\"battleship\", start={\"row\": 1, \"column\": \"A\"}, direction=\"horizontal\"\n    )\n    battleship_game.create_ship_placement(game_id, ship_placement)\n\n    game = battleship_game.get_game(game_id)\n    assert ship_placement in game.ships\n\n\ndef test_game_state_after_turn(initialized_game_id, battleship_game):\n    turn = Turn(target={\"row\": 1, \"column\": \"A\"})\n    response = battleship_game.create_turn(initialized_game_id, turn)\n\n    game = battleship_game.get_game(initialized_game_id)\n\n    if response.result == \"hit\":\n        assert game.board[(1, 0)] == \"hit\"\n    else:\n        assert game.board[1][0] == \"miss\"\n\n\ndef test_multiple_hits_on_ship(battleship_game, initialized_game_id):\n    hit_positions = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n\n    for index, pos in enumerate(hit_positions):\n        turn = Turn(target={\"row\": 1, \"column\": pos})\n        response = battleship_game.create_turn(initialized_game_id, turn)\n\n        if index == len(hit_positions) - 1:\n            assert response.result == \"sunk\"\n        else:\n            assert response.result == \"hit\"\n\n\ndef test_game_over_condition(battleship_game, initialized_game_id):\n    for row in range(1, 11):\n        for column in list(\"ABCDEFGHIJ\"):\n            turn = Turn(target={\"row\": row, \"column\": column})\n            battleship_game.create_turn(initialized_game_id, turn)\n\n            battleship_game.create_turn(initialized_game_id, turn)\n\n    status = battleship_game.get_game_status(initialized_game_id)\n    assert status.is_game_over\n", "benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_in/conftest.py": "# pyright: reportMissingImports=false\nimport pytest\nfrom abstract_class import ShipPlacement, Turn\nfrom battleship import Battleship\n\n\n@pytest.fixture\ndef battleship_game():\n    return Battleship()\n\n\n@pytest.fixture\ndef initialized_game_id(battleship_game):\n    # Create a game instance\n    game_id = battleship_game.create_game()\n\n    # Place all the ships using battleship_game's methods\n    sample_ship_placements = [\n        ShipPlacement(\n            ship_type=\"carrier\", start={\"row\": 1, \"column\": \"A\"}, direction=\"horizontal\"\n        ),\n        ShipPlacement(\n            ship_type=\"battleship\",\n            start={\"row\": 2, \"column\": \"A\"},\n            direction=\"horizontal\",\n        ),\n        ShipPlacement(\n            ship_type=\"cruiser\", start={\"row\": 3, \"column\": \"A\"}, direction=\"horizontal\"\n        ),\n        ShipPlacement(\n            ship_type=\"submarine\",\n            start={\"row\": 4, \"column\": \"A\"},\n            direction=\"horizontal\",\n        ),\n        ShipPlacement(\n            ship_type=\"destroyer\",\n            start={\"row\": 5, \"column\": \"A\"},\n            direction=\"horizontal\",\n        ),\n    ]\n\n    for ship_placement in sample_ship_placements:\n        # Place ship using battleship_game's methods\n        battleship_game.create_ship_placement(game_id, ship_placement)\n\n    return game_id\n\n\n@pytest.fixture\ndef game_over_fixture(battleship_game, initialized_game_id):\n    # Assuming 10x10 grid, target all possible positions\n    for row in range(1, 11):\n        for column in list(\"ABCDEFGHIJ\"):\n            # Player 1 takes a turn\n            turn = Turn(target={\"row\": row, \"column\": column})\n            battleship_game.create_turn(initialized_game_id, turn)\n\n            # Player 2 takes a turn, targeting the same position as Player 1\n            battleship_game.create_turn(initialized_game_id, turn)\n\n    # At the end of this fixture, the game should be over\n    return initialized_game_id\n", "benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_in/__init__.py": "", "benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_in/test_negative.py": "import pytest\nfrom abstract_class import ShipPlacement, Turn\nfrom pydantic import ValidationError\n\n\ndef test_ship_placement_out_of_bounds(battleship_game):\n    game_id = battleship_game.create_game()\n\n    try:\n        out_of_bounds_ship = ShipPlacement(\n            ship_type=\"battleship\",\n            start={\"row\": 11, \"column\": \"Z\"},\n            direction=\"horizontal\",\n        )\n    except ValidationError:  # Use the directly imported ValidationError class\n        pass\n    else:\n        with pytest.raises(ValueError, match=\"Placement out of bounds\"):\n            battleship_game.create_ship_placement(game_id, out_of_bounds_ship)\n\n\ndef test_no_ship_overlap(battleship_game):\n    game_id = battleship_game.create_game()\n    placement1 = ShipPlacement(\n        ship_type=\"battleship\", start={\"row\": 1, \"column\": \"A\"}, direction=\"horizontal\"\n    )\n    battleship_game.create_ship_placement(game_id, placement1)\n    placement2 = ShipPlacement(\n        ship_type=\"cruiser\", start={\"row\": 1, \"column\": \"A\"}, direction=\"horizontal\"\n    )\n    with pytest.raises(ValueError):\n        battleship_game.create_ship_placement(game_id, placement2)\n\n\ndef test_cant_hit_before_ships_placed(battleship_game):\n    game_id = battleship_game.create_game()\n    placement1 = ShipPlacement(\n        ship_type=\"battleship\", start={\"row\": 1, \"column\": \"A\"}, direction=\"horizontal\"\n    )\n    battleship_game.create_ship_placement(game_id, placement1)\n    placement2 = ShipPlacement(\n        ship_type=\"cruiser\", start={\"row\": 4, \"column\": \"D\"}, direction=\"horizontal\"\n    )\n    battleship_game.create_ship_placement(game_id, placement2)\n    turn = Turn(target={\"row\": 1, \"column\": \"A\"})\n    with pytest.raises(\n        ValueError, match=\"All ships must be placed before starting turns\"\n    ):\n        battleship_game.create_turn(game_id, turn)\n\n\ndef test_cant_place_ship_after_all_ships_placed(battleship_game, initialized_game_id):\n    battleship_game.get_game(initialized_game_id)\n    additional_ship = ShipPlacement(\n        ship_type=\"carrier\", start={\"row\": 2, \"column\": \"E\"}, direction=\"horizontal\"\n    )\n\n    with pytest.raises(\n        ValueError, match=\"All ships are already placed. Cannot place more ships.\"\n    ):\n        battleship_game.create_ship_placement(initialized_game_id, additional_ship)\n\n\ndef test_ship_placement_invalid_direction(battleship_game):\n    game_id = battleship_game.create_game()\n\n    with pytest.raises(ValueError, match=\"Invalid ship direction\"):\n        invalid_direction_ship = ShipPlacement(\n            ship_type=\"battleship\",\n            start={\"row\": 1, \"column\": \"A\"},\n            direction=\"diagonal\",\n        )\n        battleship_game.create_ship_placement(game_id, invalid_direction_ship)\n\n\ndef test_invalid_ship_type(battleship_game):\n    game_id = battleship_game.create_game()\n    invalid_ship = ShipPlacement(\n        ship_type=\"spacecraft\", start={\"row\": 1, \"column\": \"A\"}, direction=\"horizontal\"\n    )\n    with pytest.raises(ValueError, match=\"Invalid ship type\"):\n        battleship_game.create_ship_placement(game_id, invalid_ship)\n\n\ndef test_ship_placement_extends_beyond_boundaries(battleship_game):\n    game_id = battleship_game.create_game()\n\n    with pytest.raises(ValueError, match=\"Ship extends beyond board boundaries\"):\n        ship_extending_beyond = ShipPlacement(\n            ship_type=\"battleship\",\n            start={\"row\": 1, \"column\": \"H\"},\n            direction=\"horizontal\",\n        )\n        battleship_game.create_ship_placement(game_id, ship_extending_beyond)\n\n    with pytest.raises(ValueError, match=\"Ship extends beyond board boundaries\"):\n        ship_extending_beyond = ShipPlacement(\n            ship_type=\"cruiser\", start={\"row\": 9, \"column\": \"A\"}, direction=\"vertical\"\n        )\n        battleship_game.create_ship_placement(game_id, ship_extending_beyond)\n", "benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/abstract_class.py": "from abc import ABC, abstractmethod\nfrom typing import Optional\n\nfrom pydantic import BaseModel, validator\n\n\n# Models for the request and response payloads\nclass ShipPlacement(BaseModel):\n    ship_type: str\n    start: dict  # {\"row\": int, \"column\": str}\n    direction: str\n\n    @validator(\"start\")\n    def validate_start(cls, start):\n        row, column = start.get(\"row\"), start.get(\"column\")\n\n        if not (1 <= row <= 10):\n            raise ValueError(\"Row must be between 1 and 10 inclusive.\")\n\n        if column not in list(\"ABCDEFGHIJ\"):\n            raise ValueError(\"Column must be one of A, B, C, D, E, F, G, H, I, J.\")\n\n        return start\n\n\nclass Turn(BaseModel):\n    target: dict  # {\"row\": int, \"column\": str}\n\n\nclass TurnResponse(BaseModel):\n    result: str\n    ship_type: Optional[str]  # This would be None if the result is a miss\n\n\nclass GameStatus(BaseModel):\n    is_game_over: bool\n    winner: Optional[str]\n\n\nclass Game(BaseModel):\n    game_id: str\n    players: list[str]\n    # This could represent the state of the game board,\n    # you might need to flesh this out further:\n    board: dict\n    ships: list[ShipPlacement]  # List of ship placements for this game\n    turns: list[Turn]  # List of turns that have been taken\n\n\nclass AbstractBattleship(ABC):\n    SHIP_LENGTHS = {\n        \"carrier\": 5,\n        \"battleship\": 4,\n        \"cruiser\": 3,\n        \"submarine\": 3,\n        \"destroyer\": 2,\n    }\n\n    @abstractmethod\n    def create_ship_placement(self, game_id: str, placement: ShipPlacement) -> None:\n        \"\"\"\n        Place a ship on the grid.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def create_turn(self, game_id: str, turn: Turn) -> TurnResponse:\n        \"\"\"\n        Players take turns to target a grid cell.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_game_status(self, game_id: str) -> GameStatus:\n        \"\"\"\n        Check if the game is over and get the winner if there's one.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_winner(self, game_id: str) -> str:\n        \"\"\"\n        Get the winner of the game.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_game(self, game_id: str) -> Game | None:\n        \"\"\"\n        Retrieve the state of the game.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def delete_game(self, game_id: str) -> None:\n        \"\"\"\n        Delete a game given its ID.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def create_game(self) -> str:\n        \"\"\"\n        Create a new game.\n\n        Returns:\n            str: The ID of the created game.\n        \"\"\"\n        pass\n", "benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/test_positive.py": "from abstract_class import ShipPlacement, Turn\n\n\ndef test_turns_and_results(battleship_game, initialized_game_id):\n    turn = Turn(target={\"row\": 1, \"column\": \"A\"})\n    response = battleship_game.create_turn(initialized_game_id, turn)\n\n    assert response.result in [\"hit\", \"miss\"]\n    if response.result == \"hit\":\n        assert response.ship_type == \"carrier\"\n    game = battleship_game.get_game(initialized_game_id)\n    assert turn in game.turns\n\n\ndef test_game_status_and_winner(battleship_game):\n    game_id = battleship_game.create_game()\n    status = battleship_game.get_game_status(game_id)\n    assert isinstance(status.is_game_over, bool)\n    if status.is_game_over:\n        winner = battleship_game.get_winner(game_id)\n        assert winner is not None\n\n\ndef test_delete_game(battleship_game):\n    game_id = battleship_game.create_game()\n    battleship_game.delete_game(game_id)\n    assert battleship_game.get_game(game_id) is None\n\n\ndef test_ship_rotation(battleship_game):\n    game_id = battleship_game.create_game()\n    placement_horizontal = ShipPlacement(\n        ship_type=\"battleship\", start={\"row\": 1, \"column\": \"B\"}, direction=\"horizontal\"\n    )\n    battleship_game.create_ship_placement(game_id, placement_horizontal)\n    placement_vertical = ShipPlacement(\n        ship_type=\"cruiser\", start={\"row\": 3, \"column\": \"D\"}, direction=\"vertical\"\n    )\n    battleship_game.create_ship_placement(game_id, placement_vertical)\n    game = battleship_game.get_game(game_id)\n    assert placement_horizontal in game.ships\n    assert placement_vertical in game.ships\n\n\ndef test_game_state_updates(battleship_game, initialized_game_id):\n    turn = Turn(target={\"row\": 3, \"column\": \"A\"})\n    battleship_game.create_turn(initialized_game_id, turn)\n\n    game = battleship_game.get_game(initialized_game_id)\n\n    target_key = (3, ord(\"A\") - ord(\"A\"))\n    assert target_key in game.board and game.board[target_key] == \"hit\"\n\n\ndef test_ship_sinking_feedback(battleship_game, initialized_game_id):\n    hits = [\"A\", \"B\", \"C\", \"D\"]\n    static_moves = [\n        {\"row\": 1, \"column\": \"E\"},\n        {\"row\": 1, \"column\": \"F\"},\n        {\"row\": 1, \"column\": \"G\"},\n        {\"row\": 1, \"column\": \"H\"},\n    ]\n\n    response = None\n    for index, hit in enumerate(hits):\n        turn = Turn(target={\"row\": 2, \"column\": hit})\n        response = battleship_game.create_turn(initialized_game_id, turn)\n        assert response.ship_type == \"battleship\"\n\n        static_turn = Turn(target=static_moves[index])\n        battleship_game.create_turn(initialized_game_id, static_turn)\n\n    assert response and response.result == \"sunk\"\n\n\ndef test_restart_game(battleship_game):\n    game_id = battleship_game.create_game()\n    battleship_game.delete_game(game_id)\n    game_id = (\n        battleship_game.create_game()\n    )  # Use the returned game_id after recreating the game\n    game = battleship_game.get_game(game_id)\n    assert game is not None\n\n\ndef test_ship_edge_overlapping(battleship_game):\n    game_id = battleship_game.create_game()\n\n    first_ship = ShipPlacement(\n        ship_type=\"battleship\", start={\"row\": 1, \"column\": \"A\"}, direction=\"horizontal\"\n    )\n    battleship_game.create_ship_placement(game_id, first_ship)\n\n    next_ship = ShipPlacement(\n        ship_type=\"cruiser\", start={\"row\": 1, \"column\": \"E\"}, direction=\"horizontal\"\n    )\n    battleship_game.create_ship_placement(game_id, next_ship)\n\n    game = battleship_game.get_game(game_id)\n    assert first_ship in game.ships\n    assert next_ship in game.ships\n\n\ndef test_game_state_after_ship_placement(battleship_game):\n    game_id = battleship_game.create_game()\n\n    ship_placement = ShipPlacement(\n        ship_type=\"battleship\", start={\"row\": 1, \"column\": \"A\"}, direction=\"horizontal\"\n    )\n    battleship_game.create_ship_placement(game_id, ship_placement)\n\n    game = battleship_game.get_game(game_id)\n    assert ship_placement in game.ships\n\n\ndef test_game_state_after_turn(initialized_game_id, battleship_game):\n    turn = Turn(target={\"row\": 1, \"column\": \"A\"})\n    response = battleship_game.create_turn(initialized_game_id, turn)\n\n    game = battleship_game.get_game(initialized_game_id)\n\n    if response.result == \"hit\":\n        assert game.board[(1, 0)] == \"hit\"\n    else:\n        assert game.board[1][0] == \"miss\"\n\n\ndef test_multiple_hits_on_ship(battleship_game, initialized_game_id):\n    hit_positions = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n\n    for index, pos in enumerate(hit_positions):\n        turn = Turn(target={\"row\": 1, \"column\": pos})\n        response = battleship_game.create_turn(initialized_game_id, turn)\n\n        if index == len(hit_positions) - 1:\n            assert response.result == \"sunk\"\n        else:\n            assert response.result == \"hit\"\n\n\ndef test_game_over_condition(battleship_game, initialized_game_id):\n    for row in range(1, 11):\n        for column in list(\"ABCDEFGHIJ\"):\n            turn = Turn(target={\"row\": row, \"column\": column})\n            battleship_game.create_turn(initialized_game_id, turn)\n\n            battleship_game.create_turn(initialized_game_id, turn)\n\n    status = battleship_game.get_game_status(initialized_game_id)\n    assert status.is_game_over\n", "benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/conftest.py": "import pytest\nfrom abstract_class import ShipPlacement, Turn\nfrom battleship import Battleship\n\n\n@pytest.fixture\ndef battleship_game():\n    return Battleship()\n\n\n@pytest.fixture\ndef initialized_game_id(battleship_game):\n    # Create a game instance\n    game_id = battleship_game.create_game()\n\n    # Place all the ships using battleship_game's methods\n    sample_ship_placements = [\n        ShipPlacement(\n            ship_type=\"carrier\", start={\"row\": 1, \"column\": \"A\"}, direction=\"horizontal\"\n        ),\n        ShipPlacement(\n            ship_type=\"battleship\",\n            start={\"row\": 2, \"column\": \"A\"},\n            direction=\"horizontal\",\n        ),\n        ShipPlacement(\n            ship_type=\"cruiser\", start={\"row\": 3, \"column\": \"A\"}, direction=\"horizontal\"\n        ),\n        ShipPlacement(\n            ship_type=\"submarine\",\n            start={\"row\": 4, \"column\": \"A\"},\n            direction=\"horizontal\",\n        ),\n        ShipPlacement(\n            ship_type=\"destroyer\",\n            start={\"row\": 5, \"column\": \"A\"},\n            direction=\"horizontal\",\n        ),\n    ]\n\n    for ship_placement in sample_ship_placements:\n        # Place ship using battleship_game's methods\n        battleship_game.create_ship_placement(game_id, ship_placement)\n\n    return game_id\n\n\n@pytest.fixture\ndef game_over_fixture(battleship_game, initialized_game_id):\n    # Assuming 10x10 grid, target all possible positions\n    for row in range(1, 11):\n        for column in list(\"ABCDEFGHIJ\"):\n            # Player 1 takes a turn\n            turn = Turn(target={\"row\": row, \"column\": column})\n            battleship_game.create_turn(initialized_game_id, turn)\n\n            # Player 2 takes a turn, targeting the same position as Player 1\n            battleship_game.create_turn(initialized_game_id, turn)\n\n    # At the end of this fixture, the game should be over\n    return initialized_game_id\n", "benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/__init__.py": "", "benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/test_negative.py": "import pytest\nfrom abstract_class import ShipPlacement, Turn\nfrom pydantic import ValidationError\n\n\ndef test_ship_placement_out_of_bounds(battleship_game):\n    game_id = battleship_game.create_game()\n\n    try:\n        out_of_bounds_ship = ShipPlacement(\n            ship_type=\"battleship\",\n            start={\"row\": 11, \"column\": \"Z\"},\n            direction=\"horizontal\",\n        )\n    except ValidationError:  # Use the directly imported ValidationError class\n        pass\n    else:\n        with pytest.raises(ValueError, match=\"Placement out of bounds\"):\n            battleship_game.create_ship_placement(game_id, out_of_bounds_ship)\n\n\ndef test_no_ship_overlap(battleship_game):\n    game_id = battleship_game.create_game()\n    placement1 = ShipPlacement(\n        ship_type=\"battleship\", start={\"row\": 1, \"column\": \"A\"}, direction=\"horizontal\"\n    )\n    battleship_game.create_ship_placement(game_id, placement1)\n    placement2 = ShipPlacement(\n        ship_type=\"cruiser\", start={\"row\": 1, \"column\": \"A\"}, direction=\"horizontal\"\n    )\n    with pytest.raises(ValueError):\n        battleship_game.create_ship_placement(game_id, placement2)\n\n\ndef test_cant_hit_before_ships_placed(battleship_game):\n    game_id = battleship_game.create_game()\n    placement1 = ShipPlacement(\n        ship_type=\"battleship\", start={\"row\": 1, \"column\": \"A\"}, direction=\"horizontal\"\n    )\n    battleship_game.create_ship_placement(game_id, placement1)\n    placement2 = ShipPlacement(\n        ship_type=\"cruiser\", start={\"row\": 4, \"column\": \"D\"}, direction=\"horizontal\"\n    )\n    battleship_game.create_ship_placement(game_id, placement2)\n    turn = Turn(target={\"row\": 1, \"column\": \"A\"})\n    with pytest.raises(\n        ValueError, match=\"All ships must be placed before starting turns\"\n    ):\n        battleship_game.create_turn(game_id, turn)\n\n\ndef test_cant_place_ship_after_all_ships_placed(battleship_game, initialized_game_id):\n    battleship_game.get_game(initialized_game_id)\n    additional_ship = ShipPlacement(\n        ship_type=\"carrier\", start={\"row\": 2, \"column\": \"E\"}, direction=\"horizontal\"\n    )\n\n    with pytest.raises(\n        ValueError, match=\"All ships are already placed. Cannot place more ships.\"\n    ):\n        battleship_game.create_ship_placement(initialized_game_id, additional_ship)\n\n\ndef test_ship_placement_invalid_direction(battleship_game):\n    game_id = battleship_game.create_game()\n\n    with pytest.raises(ValueError, match=\"Invalid ship direction\"):\n        invalid_direction_ship = ShipPlacement(\n            ship_type=\"battleship\",\n            start={\"row\": 1, \"column\": \"A\"},\n            direction=\"diagonal\",\n        )\n        battleship_game.create_ship_placement(game_id, invalid_direction_ship)\n\n\ndef test_invalid_ship_type(battleship_game):\n    game_id = battleship_game.create_game()\n    invalid_ship = ShipPlacement(\n        ship_type=\"spacecraft\", start={\"row\": 1, \"column\": \"A\"}, direction=\"horizontal\"\n    )\n    with pytest.raises(ValueError, match=\"Invalid ship type\"):\n        battleship_game.create_ship_placement(game_id, invalid_ship)\n\n\ndef test_ship_placement_extends_beyond_boundaries(battleship_game):\n    game_id = battleship_game.create_game()\n\n    with pytest.raises(ValueError, match=\"Ship extends beyond board boundaries\"):\n        ship_extending_beyond = ShipPlacement(\n            ship_type=\"battleship\",\n            start={\"row\": 1, \"column\": \"H\"},\n            direction=\"horizontal\",\n        )\n        battleship_game.create_ship_placement(game_id, ship_extending_beyond)\n\n    with pytest.raises(ValueError, match=\"Ship extends beyond board boundaries\"):\n        ship_extending_beyond = ShipPlacement(\n            ship_type=\"cruiser\", start={\"row\": 9, \"column\": \"A\"}, direction=\"vertical\"\n        )\n        battleship_game.create_ship_placement(game_id, ship_extending_beyond)\n", "benchmark/agbenchmark/challenges/verticals/code/6_battleship/artifacts_out/battleship.py": "from typing import Dict\n\nfrom abstract_class import (\n    AbstractBattleship,\n    Game,\n    GameStatus,\n    ShipPlacement,\n    Turn,\n    TurnResponse,\n)\n\n\nclass Battleship(AbstractBattleship):\n    def __init__(self):\n        self.games: Dict[str, Game] = {}\n\n    def create_game(self) -> str:\n        game_id = str(len(self.games))\n        new_game = Game(\n            game_id=game_id,\n            players=[],\n            board={},\n            ships=[],\n            turns=[],\n        )\n\n        self.games[game_id] = new_game\n        return game_id\n\n    def create_ship_placement(self, game_id: str, placement: ShipPlacement) -> None:\n        game = self.games.get(game_id)\n\n        if not game:\n            raise ValueError(f\"Game with ID {game_id} not found.\")\n        if placement.direction not in [\"horizontal\", \"vertical\"]:\n            raise ValueError(\"Invalid ship direction\")\n        if self.all_ships_placed(game):\n            raise ValueError(\"All ships are already placed. Cannot place more ships.\")\n\n        ship_length = self.SHIP_LENGTHS.get(placement.ship_type)\n        if not ship_length:\n            raise ValueError(f\"Invalid ship type {placement.ship_type}\")\n\n        start_row, start_col = placement.start[\"row\"], ord(\n            placement.start[\"column\"]\n        ) - ord(\"A\")\n\n        if start_row < 1 or start_row > 10 or start_col < 0 or start_col > 9:\n            raise ValueError(\"Placement out of bounds\")\n\n        if placement.direction == \"horizontal\" and start_col + ship_length > 10:\n            raise ValueError(\"Ship extends beyond board boundaries\")\n        elif placement.direction == \"vertical\" and start_row + ship_length > 10:\n            raise ValueError(\"Ship extends beyond board boundaries\")\n\n        for i in range(ship_length):\n            if placement.direction == \"horizontal\":\n                if game.board.get((start_row, start_col + i)):\n                    raise ValueError(\"Ship overlaps with another ship!\")\n            elif placement.direction == \"vertical\":\n                if game.board.get((start_row + i, start_col)):\n                    raise ValueError(\"Ship overlaps with another ship!\")\n\n        for i in range(ship_length):\n            if placement.direction == \"horizontal\":\n                game.board[(start_row, start_col + i)] = placement.ship_type\n            else:\n                game.board[(start_row + i, start_col)] = placement.ship_type\n\n        game.ships.append(placement)\n\n    def create_turn(self, game_id: str, turn: Turn) -> TurnResponse:\n        game = self.games.get(game_id)\n\n        if not game:\n            raise ValueError(f\"Game with ID {game_id} not found.\")\n\n        if not self.all_ships_placed(game):\n            raise ValueError(\"All ships must be placed before starting turns\")\n\n        target_row, target_col = turn.target[\"row\"], ord(turn.target[\"column\"]) - ord(\n            \"A\"\n        )\n        hit_ship = game.board.get((target_row, target_col))\n\n        game.turns.append(turn)\n\n        if not hit_ship or hit_ship == \"hit\":  # if no ship or already hit\n            return TurnResponse(result=\"miss\", ship_type=None)\n\n        ship_placement = next(sp for sp in game.ships if sp.ship_type == hit_ship)\n        start_row, start_col = (\n            ship_placement.start[\"row\"],\n            ord(ship_placement.start[\"column\"]) - ord(\"A\"),\n        )\n        ship_positions = [\n            (\n                start_row + (i if ship_placement.direction == \"vertical\" else 0),\n                start_col + (i if ship_placement.direction == \"horizontal\" else 0),\n            )\n            for i in range(self.SHIP_LENGTHS[hit_ship])\n        ]\n\n        targeted_positions = {\n            (t.target[\"row\"], ord(t.target[\"column\"]) - ord(\"A\")) for t in game.turns\n        }\n\n        game.board[(target_row, target_col)] = \"hit\"\n\n        if set(ship_positions).issubset(targeted_positions):\n            for pos in ship_positions:\n                game.board[pos] = \"hit\"\n            return TurnResponse(result=\"sunk\", ship_type=hit_ship)\n        else:\n            return TurnResponse(result=\"hit\", ship_type=hit_ship)\n\n    def get_game_status(self, game_id: str) -> GameStatus:\n        game = self.games.get(game_id)\n\n        if not game:\n            raise ValueError(f\"Game with ID {game_id} not found.\")\n\n        hits = sum(1 for _, status in game.board.items() if status == \"hit\")\n\n        total_ships_length = sum(\n            self.SHIP_LENGTHS[ship.ship_type] for ship in game.ships\n        )\n\n        if hits == total_ships_length:\n            return GameStatus(is_game_over=True, winner=\"player\")\n        else:\n            return GameStatus(is_game_over=False, winner=None)\n\n    def get_winner(self, game_id: str) -> str:\n        game_status = self.get_game_status(game_id)\n\n        if game_status.is_game_over and game_status.winner:\n            return game_status.winner\n        else:\n            raise ValueError(f\"Game {game_id} isn't over yet\")\n\n    def get_game(self, game_id: str) -> Game | None:\n        return self.games.get(game_id)\n\n    def delete_game(self, game_id: str) -> None:\n        if game_id in self.games:\n            del self.games[game_id]\n\n    def all_ships_placed(self, game: Game) -> bool:\n        placed_ship_types = set([placement.ship_type for placement in game.ships])\n        return placed_ship_types == set(self.SHIP_LENGTHS.keys())\n", "benchmark/agbenchmark/challenges/verticals/code/2_password_generator/custom_python/test.py": "# pyright: reportMissingImports=false\nimport unittest\n\nimport password_generator\n\n\nclass TestPasswordGenerator(unittest.TestCase):\n    def test_password_length(self):\n        for i in range(8, 17):\n            password = password_generator.generate_password(i)\n            self.assertEqual(len(password), i)\n\n    def test_value_error(self):\n        with self.assertRaises(ValueError):\n            password_generator.generate_password(7)\n        with self.assertRaises(ValueError):\n            password_generator.generate_password(17)\n\n    def test_password_content(self):\n        password = password_generator.generate_password()\n        self.assertTrue(any(c.isdigit() for c in password))\n        self.assertTrue(\n            any(c in password_generator.string.punctuation for c in password)\n        )\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "benchmark/agbenchmark/challenges/verticals/code/2_password_generator/artifacts_out/__init__.py": "", "benchmark/agbenchmark/challenges/verticals/code/2_password_generator/artifacts_out/password_generator.py": "import random\nimport string\nimport sys\n\n\ndef generate_password(length: int = 8) -> str:\n    if length < 8 or length > 16:\n        raise ValueError(\"Password length must be between 8 and 16 characters.\")\n\n    characters = string.ascii_letters + string.digits + string.punctuation\n    password = [\n        random.choice(string.ascii_lowercase),\n        random.choice(string.ascii_uppercase),\n        random.choice(string.digits),\n        random.choice(string.punctuation),\n    ]\n    password += [random.choice(characters) for _ in range(length - 4)]\n    random.shuffle(password)\n    return \"\".join(password)\n\n\nif __name__ == \"__main__\":\n    password_length = (\n        int(sys.argv[sys.argv.index(\"--length\") + 1]) if \"--length\" in sys.argv else 8\n    )\n    print(generate_password(password_length))\n", "benchmark/agbenchmark/challenges/verticals/code/3_file_organizer/custom_python/test.py": "import os\nimport subprocess\nimport tempfile\nimport unittest\n\n\nclass TestOrganizeFiles(unittest.TestCase):\n    def setUp(self):\n        # Create temporary directory\n        self.test_dir = tempfile.mkdtemp()\n\n        # File types and their corresponding directory\n        self.file_types = {\n            \"test_image.png\": \"images\",\n            \"test_doc.txt\": \"documents\",\n            \"test_audio.mp3\": \"audio\",\n        }\n\n        # Create test files\n        for file_name in self.file_types.keys():\n            open(os.path.join(self.test_dir, file_name), \"a\").close()\n\n    def test_organize_files(self):\n        # Call the organize_files.py script using subprocess\n        subprocess.call(\n            [\"python\", \"organize_files.py\", \"--directory_path=\" + self.test_dir]\n        )\n\n        # Check if the files have been moved to the correct directories\n        for file_name, directory in self.file_types.items():\n            self.assertTrue(\n                os.path.isfile(os.path.join(self.test_dir, directory, file_name))\n            )\n\n    def tearDown(self):\n        # Delete test directory and its contents\n        for file_name, directory in self.file_types.items():\n            os.remove(os.path.join(self.test_dir, directory, file_name))\n        for directory in set(self.file_types.values()):\n            os.rmdir(os.path.join(self.test_dir, directory))\n        os.rmdir(self.test_dir)\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "benchmark/agbenchmark/challenges/verticals/code/3_file_organizer/artifacts_out/organize_files.py": "import argparse\nimport os\nimport shutil\n\n\ndef organize_files(directory_path):\n    # Define file type groups\n    file_types = {\n        \"images\": [\".png\", \".jpg\", \".jpeg\"],\n        \"documents\": [\".pdf\", \".docx\", \".txt\"],\n        \"audio\": [\".mp3\", \".wav\", \".flac\"],\n    }\n\n    # Create the folders if they don't exist\n    for folder_name in file_types.keys():\n        folder_path = os.path.join(directory_path, folder_name)\n        if not os.path.exists(folder_path):\n            os.makedirs(folder_path)\n\n    # Traverse through all files and folders in the specified directory\n    for foldername, subfolders, filenames in os.walk(directory_path):\n        for filename in filenames:\n            # Get file extension\n            _, file_extension = os.path.splitext(filename)\n\n            # Move files to corresponding folders\n            for folder_name, extensions in file_types.items():\n                if file_extension in extensions:\n                    old_path = os.path.join(foldername, filename)\n                    new_path = os.path.join(directory_path, folder_name, filename)\n                    if old_path != new_path:\n                        shutil.move(old_path, new_path)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"Organize files in a directory based on their file types\"\n    )\n    parser.add_argument(\n        \"--directory_path\",\n        type=str,\n        required=True,\n        help=\"The path of the directory to be organized\",\n    )\n\n    args = parser.parse_args()\n\n    organize_files(args.directory_path)\n", "benchmark/agbenchmark/challenges/verticals/code/3_file_organizer/artifacts_out/__init__.py": "", "benchmark/agbenchmark/challenges/verticals/code/5_tic_tac_toe/custom_python/test.py": "import subprocess\n\nimport pytest\n\n\ndef run_game_with_inputs(inputs):\n    # Start the game process\n    process = subprocess.Popen(\n        [\"python\", \"tic_tac_toe.py\"],\n        stdin=subprocess.PIPE,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        text=True,\n    )\n\n    # Send the input moves one by one\n    output, errors = process.communicate(\"\\n\".join(inputs))\n\n    # Print the inputs and outputs\n    print(\"Inputs:\\n\", \"\\n\".join(inputs))\n    print(\"Output:\\n\", output)\n    print(\"Errors:\\n\", errors)\n\n    return output\n\n\n@pytest.mark.parametrize(\n    \"inputs, expected_output\",\n    [\n        ([\"0,0\", \"1,0\", \"0,1\", \"1,1\", \"0,2\"], \"Player 1 won!\"),\n        ([\"1,0\", \"0,0\", \"1,1\", \"0,1\", \"2,0\", \"0,2\"], \"Player 2 won!\"),\n        ([\"0,0\", \"0,1\", \"0,2\", \"1,1\", \"1,0\", \"1,2\", \"2,1\", \"2,0\", \"2,2\"], \"Draw\"),\n    ],\n)\ndef test_game(inputs, expected_output):\n    output = run_game_with_inputs(inputs)\n    assert expected_output in output\n\n\nif __name__ == \"__main__\":\n    pytest.main([__file__])\n", "benchmark/agbenchmark/challenges/verticals/code/5_tic_tac_toe/artifacts_out/tic_tac_toe.py": "import pprint\n\n\ndef column(matrix, i):\n    return [row[i] for row in matrix]\n\n\ndef check(list):\n    if len(set(list)) <= 1:\n        if list[0] != 0:\n            return list[0]\n    return None\n\n\ndef checkDiagLeft(board):\n    if board[0][0] == board[1][1] and board[1][1] == board[2][2]:\n        if board[0][0] != 0:\n            return board[0][0]\n    return None\n\n\ndef checkDiagRight(board):\n    if board[2][0] == board[1][1] and board[1][1] == board[0][2]:\n        if board[2][0] != 0:\n            return board[2][0]\n    return None\n\n\ndef placeItem(row, column, board, current_player):\n    if board[row][column] != 0:\n        return None\n    else:\n        board[row][column] = current_player\n\n\ndef swapPlayers(player):\n    if player == 2:\n        return 1\n    else:\n        return 2\n\n\ndef winner(board):\n    for rowIndex in board:\n        if check(rowIndex) is not None:\n            return check(rowIndex)\n    for columnIndex in range(len(board[0])):\n        if check(column(board, columnIndex)) is not None:\n            return check(column(board, columnIndex))\n    if checkDiagLeft(board) is not None:\n        return checkDiagLeft(board)\n    if checkDiagRight(board) is not None:\n        return checkDiagRight(board)\n    return 0\n\n\ndef getLocation():\n    location = input(\n        \"Choose where to play. Enter two numbers separated by a comma [example: 1,1]: \"\n    )\n    print(f\"\\nYou picked {location}\")\n    coordinates = [int(x) for x in location.split(\",\")]\n    while (\n        len(coordinates) != 2\n        or coordinates[0] < 0\n        or coordinates[0] > 2\n        or coordinates[1] < 0\n        or coordinates[1] > 2\n    ):\n        print(\"You inputted a location in an invalid format\")\n        location = input(\n            \"Choose where to play. Enter two numbers separated by a comma \"\n            \"[example: 1,1]: \"\n        )\n        coordinates = [int(x) for x in location.split(\",\")]\n    return coordinates\n\n\ndef gamePlay():\n    num_moves = 0\n    pp = pprint.PrettyPrinter(width=20)\n    current_player = 1\n    board = [[0 for x in range(3)] for x in range(3)]\n\n    while num_moves < 9 and winner(board) == 0:\n        print(\"This is the current board: \")\n        pp.pprint(board)\n        coordinates = getLocation()\n        placeItem(coordinates[0], coordinates[1], board, current_player)\n        current_player = swapPlayers(current_player)\n        if winner(board) != 0:\n            print(f\"Player {winner(board)} won!\")\n        num_moves += 1\n\n    if winner(board) == 0:\n        print(\"Draw\")\n\n\nif __name__ == \"__main__\":\n    gamePlay()\n", "benchmark/agbenchmark/challenges/verticals/code/5_tic_tac_toe/artifacts_out/__init__.py": "", "benchmark/agbenchmark/challenges/verticals/code/4_url_shortener/custom_python/test.py": "# pyright: reportMissingImports=false\nimport unittest\n\nfrom url_shortener import retrieve_url, shorten_url\n\n\nclass TestURLShortener(unittest.TestCase):\n    def test_url_retrieval(self):\n        # Shorten the URL to get its shortened form\n        shortened_url = shorten_url(\"https://www.example.com\")\n\n        # Retrieve the original URL using the shortened URL directly\n        retrieved_url = retrieve_url(shortened_url)\n\n        self.assertEqual(\n            retrieved_url,\n            \"https://www.example.com\",\n            \"Retrieved URL does not match the original!\",\n        )\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "benchmark/agbenchmark/challenges/verticals/code/4_url_shortener/artifacts_out/url_shortener.py": "import argparse\nimport base64\n\nURL_MAPPING = {}\n\n\ndef shorten_url(url):\n    # Convert the URL to base64\n    encoded_url = base64.b64encode(url.encode()).decode()\n    # Take the first 8 characters of the encoded URL as our shortened URL\n    short_url = encoded_url[:8]\n    # Map the shortened URL back to the original\n    URL_MAPPING[short_url] = url\n    return short_url\n\n\ndef retrieve_url(short_url):\n    return URL_MAPPING.get(short_url, \"URL not found\")\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"URL Shortener\")\n    parser.add_argument(\"-s\", \"--shorten\", type=str, help=\"URL to be shortened\")\n    parser.add_argument(\"-r\", \"--retrieve\", type=str, help=\"Short URL to be retrieved\")\n\n    args = parser.parse_args()\n\n    if args.shorten:\n        shortened_url = shorten_url(args.shorten)\n        print(shortened_url)\n        # Directly retrieve after shortening, using the newly shortened URL\n        print(retrieve_url(shortened_url))\n    elif args.retrieve:\n        print(retrieve_url(args.retrieve))\n    else:\n        print(\"No valid arguments provided.\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "benchmark/agbenchmark/challenges/verticals/code/4_url_shortener/artifacts_out/__init__.py": "", "benchmark/agbenchmark/challenges/verticals/code/4_url_shortener/artifacts_out/test.py": "import unittest\n\nfrom url_shortener import retrieve_url, shorten_url\n\n\nclass TestURLShortener(unittest.TestCase):\n    def test_url_retrieval(self):\n        # Shorten the URL to get its shortened form\n        shortened_url = shorten_url(\"https://www.example.com\")\n\n        # Retrieve the original URL using the shortened URL directly\n        retrieved_url = retrieve_url(shortened_url)\n\n        self.assertEqual(\n            retrieved_url,\n            \"https://www.example.com\",\n            \"Retrieved URL does not match the original!\",\n        )\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "benchmark/agbenchmark/utils/logging.py": "from __future__ import annotations\n\nimport logging\n\nfrom colorama import Fore, Style\n\nSIMPLE_LOG_FORMAT = \"[%(asctime)s] %(levelname)s %(message)s\"\nDEBUG_LOG_FORMAT = \"[%(asctime)s] %(levelname)s %(filename)s:%(lineno)03d  %(message)s\"\n\n\ndef configure_logging(\n    level: int = logging.INFO,\n) -> None:\n    \"\"\"Configure the native logging module.\"\"\"\n\n    # Auto-adjust default log format based on log level\n    log_format = DEBUG_LOG_FORMAT if level == logging.DEBUG else SIMPLE_LOG_FORMAT\n\n    console_handler = logging.StreamHandler()\n    console_handler.setFormatter(FancyConsoleFormatter(log_format))\n\n    # Configure the root logger\n    logging.basicConfig(\n        level=level,\n        format=log_format,\n        handlers=[console_handler],\n    )\n\n\nclass FancyConsoleFormatter(logging.Formatter):\n    \"\"\"\n    A custom logging formatter designed for console output.\n\n    This formatter enhances the standard logging output with color coding. The color\n    coding is based on the level of the log message, making it easier to distinguish\n    between different types of messages in the console output.\n\n    The color for each level is defined in the LEVEL_COLOR_MAP class attribute.\n    \"\"\"\n\n    # level -> (level & text color, title color)\n    LEVEL_COLOR_MAP = {\n        logging.DEBUG: Fore.LIGHTBLACK_EX,\n        logging.INFO: Fore.BLUE,\n        logging.WARNING: Fore.YELLOW,\n        logging.ERROR: Fore.RED,\n        logging.CRITICAL: Fore.RED + Style.BRIGHT,\n    }\n\n    def format(self, record: logging.LogRecord) -> str:\n        # Make sure `msg` is a string\n        if not hasattr(record, \"msg\"):\n            record.msg = \"\"\n        elif not type(record.msg) is str:\n            record.msg = str(record.msg)\n\n        # Justify the level name to 5 characters minimum\n        record.levelname = record.levelname.ljust(5)\n\n        # Determine default color based on error level\n        level_color = \"\"\n        if record.levelno in self.LEVEL_COLOR_MAP:\n            level_color = self.LEVEL_COLOR_MAP[record.levelno]\n            record.levelname = f\"{level_color}{record.levelname}{Style.RESET_ALL}\"\n\n        # Determine color for message\n        color = getattr(record, \"color\", level_color)\n        color_is_specified = hasattr(record, \"color\")\n\n        # Don't color INFO messages unless the color is explicitly specified.\n        if color and (record.levelno != logging.INFO or color_is_specified):\n            record.msg = f\"{color}{record.msg}{Style.RESET_ALL}\"\n\n        return super().format(record)\n", "benchmark/agbenchmark/utils/utils.py": "# radio charts, logs, helper functions for tests, anything else relevant.\nimport json\nimport logging\nimport os\nimport re\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import Any, Callable, Iterable, Optional, TypeVar, overload\n\nimport click\nfrom dotenv import load_dotenv\nfrom pydantic import BaseModel\n\nfrom agbenchmark.reports.processing.report_types import Test\nfrom agbenchmark.utils.data_types import DIFFICULTY_MAP, DifficultyLevel\n\nload_dotenv()\n\nAGENT_NAME = os.getenv(\"AGENT_NAME\")\n\nlogger = logging.getLogger(__name__)\n\nT = TypeVar(\"T\")\nE = TypeVar(\"E\", bound=Enum)\n\n\ndef replace_backslash(value: Any) -> Any:\n    if isinstance(value, str):\n        return re.sub(\n            r\"\\\\+\", \"/\", value\n        )  # replace one or more backslashes with a forward slash\n    elif isinstance(value, list):\n        return [replace_backslash(i) for i in value]\n    elif isinstance(value, dict):\n        return {k: replace_backslash(v) for k, v in value.items()}\n    else:\n        return value\n\n\ndef get_test_path(json_file: str | Path) -> str:\n    if isinstance(json_file, str):\n        json_file = Path(json_file)\n\n    # Find the index of \"agbenchmark\" in the path parts\n    try:\n        agbenchmark_index = json_file.parts.index(\"benchmark\")\n    except ValueError:\n        raise ValueError(\"Invalid challenge location.\")\n\n    # Create the path from \"agbenchmark\" onwards\n    challenge_location = Path(*json_file.parts[agbenchmark_index:])\n\n    formatted_location = replace_backslash(str(challenge_location))\n    if isinstance(formatted_location, str):\n        return formatted_location\n    else:\n        return str(challenge_location)\n\n\ndef get_highest_success_difficulty(\n    data: dict[str, Test], just_string: Optional[bool] = None\n) -> str:\n    highest_difficulty = None\n    highest_difficulty_level = 0\n\n    for test_name, test_data in data.items():\n        try:\n            if any(r.success for r in test_data.results):\n                difficulty_str = test_data.difficulty\n                if not difficulty_str:\n                    continue\n\n                try:\n                    difficulty_enum = DifficultyLevel[difficulty_str.lower()]\n                    difficulty_level = DIFFICULTY_MAP[difficulty_enum]\n\n                    if difficulty_level > highest_difficulty_level:\n                        highest_difficulty = difficulty_enum\n                        highest_difficulty_level = difficulty_level\n                except KeyError:\n                    logger.warning(\n                        f\"Unexpected difficulty level '{difficulty_str}' \"\n                        f\"in test '{test_name}'\"\n                    )\n                    continue\n        except Exception as e:\n            logger.warning(\n                \"An unexpected error [1] occurred while analyzing report [2].\"\n                \"Please notify a maintainer.\\n\"\n                f\"Report data [1]: {data}\\n\"\n                f\"Error [2]: {e}\"\n            )\n            logger.warning(\n                \"Make sure you selected the right test, no reports were generated.\"\n            )\n            break\n\n    if highest_difficulty is not None:\n        highest_difficulty_str = highest_difficulty.name  # convert enum to string\n    else:\n        highest_difficulty_str = \"\"\n\n    if highest_difficulty_level and not just_string:\n        return f\"{highest_difficulty_str}: {highest_difficulty_level}\"\n    elif highest_difficulty_str:\n        return highest_difficulty_str\n    return \"No successful tests\"\n\n\n# def get_git_commit_sha(directory: Path) -> Optional[str]:\n#     try:\n#         repo = git.Repo(directory)\n#         remote_url = repo.remotes.origin.url\n#         if remote_url.endswith(\".git\"):\n#             remote_url = remote_url[:-4]\n#         git_commit_sha = f\"{remote_url}/tree/{repo.head.commit.hexsha}\"\n\n#         # logger.debug(f\"GIT_COMMIT_SHA: {git_commit_sha}\")\n#         return git_commit_sha\n#     except Exception:\n#         # logger.error(f\"{directory} is not a git repository!\")\n#         return None\n\n\ndef write_pretty_json(data, json_file):\n    sorted_data = deep_sort(data)\n    json_graph = json.dumps(sorted_data, indent=4)\n    with open(json_file, \"w\") as f:\n        f.write(json_graph)\n        f.write(\"\\n\")\n\n\ndef pretty_print_model(model: BaseModel, include_header: bool = True) -> None:\n    indent = \"\"\n    if include_header:\n        # Try to find the ID and/or name attribute of the model\n        id, name = None, None\n        for attr, value in model.dict().items():\n            if attr == \"id\" or attr.endswith(\"_id\"):\n                id = value\n            if attr.endswith(\"name\"):\n                name = value\n            if id and name:\n                break\n        identifiers = [v for v in [name, id] if v]\n        click.echo(\n            f\"{model.__repr_name__()}{repr(identifiers) if identifiers else ''}:\"\n        )\n        indent = \" \" * 2\n\n    k_col_width = max(len(k) for k in model.dict().keys())\n    for k, v in model.dict().items():\n        v_fmt = repr(v)\n        if v is None or v == \"\":\n            v_fmt = click.style(v_fmt, fg=\"black\")\n        elif type(v) is bool:\n            v_fmt = click.style(v_fmt, fg=\"green\" if v else \"red\")\n        elif type(v) is str and \"\\n\" in v:\n            v_fmt = f\"\\n{v}\".replace(\n                \"\\n\", f\"\\n{indent} {click.style('|', fg='black')} \"\n            )\n        if isinstance(v, Enum):\n            v_fmt = click.style(v.value, fg=\"blue\")\n        elif type(v) is list and len(v) > 0 and isinstance(v[0], Enum):\n            v_fmt = \", \".join(click.style(lv.value, fg=\"blue\") for lv in v)\n        click.echo(f\"{indent}{k: <{k_col_width}}  = {v_fmt}\")\n\n\ndef deep_sort(obj):\n    \"\"\"\n    Recursively sort the keys in JSON object\n    \"\"\"\n    if isinstance(obj, dict):\n        return {k: deep_sort(v) for k, v in sorted(obj.items())}\n    if isinstance(obj, list):\n        return [deep_sort(elem) for elem in obj]\n    return obj\n\n\n@overload\ndef sorted_by_enum_index(\n    sortable: Iterable[E],\n    enum: type[E],\n    *,\n    reverse: bool = False,\n) -> list[E]:\n    ...\n\n\n@overload\ndef sorted_by_enum_index(\n    sortable: Iterable[T],\n    enum: type[Enum],\n    *,\n    key: Callable[[T], Enum | None],\n    reverse: bool = False,\n) -> list[T]:\n    ...\n\n\ndef sorted_by_enum_index(\n    sortable: Iterable[T],\n    enum: type[Enum],\n    *,\n    key: Optional[Callable[[T], Enum | None]] = None,\n    reverse: bool = False,\n) -> list[T]:\n    return sorted(\n        sortable,\n        key=lambda x: (\n            enum._member_names_.index(e.name)  # type: ignore\n            if (e := key(x) if key else x)\n            else 420e3\n        ),\n        reverse=reverse,\n    )\n", "benchmark/agbenchmark/utils/prompts.py": "SCORING_MAP = {\n    \"percentage\": (\n        \"assign a float score that will represent a percentage out of 100. \"\n        \"Use decimal points to be even more accurate. \"\n        \"0 represents the worst possible generation, \"\n        \"while 100 represents the ideal generation\"\n    ),\n    \"scale\": (\n        \"assign an integer score from a scale of 1-10. \"\n        \"1 represents a really bad generation, while 10 represents an ideal generation\"\n    ),\n    \"binary\": (\n        \"assign a binary score of either 0 or 1. \"\n        \"0 represents a failure, while 1 represents a success\"\n    ),\n}\n\n\nREFERENCE_PROMPT = \"\"\"Ignore previous directions. You are now an expert at evaluating how close machine generated responses are to human answers. You essentially act as a hyper advanced BLEU score.\nIn order to score the machine generated response you will {scoring}. Make sure to factor in the distance to the ideal response into your thinking, deliberation, and final result regarding scoring. Return nothing but a float score.\n\nHere is the given task for you to evaluate:\n{task}\n\nHere is the ideal response you're comparing to based on the task:\n{answer}\n\nHere is the current machine generated response to the task that you need to evaluate:\n{response}\n\n\"\"\"  # noqa: E501\n\nRUBRIC_PROMPT = \"\"\"Ignore previous directions. You are now an expert at evaluating machine generated responses to given tasks.\nIn order to score the generated texts you will {scoring}. Make sure to factor in rubric into your thinking, deliberation, and final result regarding scoring. Return nothing but a float score.\n\nHere is the given task for you to evaluate:\n{task}\n\nUse the below rubric to guide your thinking about scoring:\n{answer}\n\nHere is the current machine generated response to the task that you need to evaluate:\n{response}\n\n\"\"\"  # noqa: E501\n\nQUESTION_PROMPT = \"\"\"Ignore previous directions. You are now an expert at evaluating machine generated responses to given tasks.\nIn order to score the generated texts you will {scoring}. Make sure to think about whether the generated response answers the question well in order to score accurately. Return nothing but a float score.\n\nHere is the given task:\n{task}\n\nHere is a question that checks if the task was completed correctly:\n{answer}\n\nHere is the current machine generated response to the task that you need to evaluate:\n{response}\n\n\"\"\"  # noqa: E501\n\nFEW_SHOT_EXAMPLES = \"\"\"Here are some examples of how to score a machine generated response based on the above:\n{examples}\n\n\"\"\"  # noqa: E501\n\nCUSTOM_PROMPT = \"\"\"{custom}\n{scoring}\n\n\"\"\"\n\nPROMPT_MAP = {\n    \"rubric\": RUBRIC_PROMPT,\n    \"reference\": REFERENCE_PROMPT,\n    \"question\": QUESTION_PROMPT,\n    \"custom\": CUSTOM_PROMPT,\n}\n\nEND_PROMPT = \"\"\"Remember to always end your response with nothing but a float score.\nFloat score:\"\"\"\n", "benchmark/agbenchmark/utils/get_data_from_helicone.py": "import json\nimport logging\nimport os\nfrom typing import Optional\n\nimport requests\n\nfrom agbenchmark.__main__ import BENCHMARK_START_TIME\nfrom agbenchmark.agent_interface import HELICONE_GRAPHQL_LOGS\n\nlogger = logging.getLogger(__name__)\n\n\ndef get_data_from_helicone(challenge: str) -> Optional[float]:\n    # Define the endpoint of your GraphQL server\n    url = \"https://www.helicone.ai/api/graphql\"\n\n    # Set the headers, usually you'd need to set the content type\n    # and possibly an authorization token\n    headers = {\"authorization\": f\"Bearer {os.environ.get('HELICONE_API_KEY')}\"}\n\n    # Define the query, variables, and operation name\n    query = \"\"\"\nquery ExampleQuery($properties: [PropertyFilter!]){\n  aggregatedHeliconeRequest(properties: $properties) {\n    costUSD\n  }\n}\n\"\"\"\n\n    variables = {\n        \"properties\": [\n            {\n                \"value\": {\"equals\": os.environ.get(\"AGENT_NAME\")},\n                \"name\": \"agent\",\n            },\n            {\n                \"value\": {\"equals\": BENCHMARK_START_TIME},\n                \"name\": \"benchmark_start_time\",\n            },\n            {\"value\": {\"equals\": challenge}, \"name\": \"challenge\"},\n        ]\n    }\n    if HELICONE_GRAPHQL_LOGS:\n        logger.debug(f\"Executing Helicone query:\\n{query.strip()}\")\n        logger.debug(f\"Query variables:\\n{json.dumps(variables, indent=4)}\")\n\n    operation_name = \"ExampleQuery\"\n\n    data = {}\n    response = None\n\n    try:\n        response = requests.post(\n            url,\n            headers=headers,\n            json={\n                \"query\": query,\n                \"variables\": variables,\n                \"operationName\": operation_name,\n            },\n        )\n\n        data = response.json()\n    except requests.HTTPError as http_err:\n        logger.error(f\"Helicone returned an HTTP error: {http_err}\")\n        return None\n    except json.JSONDecodeError:\n        raw_response = response.text  # type: ignore\n        logger.error(\n            f\"Helicone returned an invalid JSON response: '''{raw_response}'''\"\n        )\n        return None\n    except Exception as err:\n        logger.error(f\"Error while trying to get data from Helicone: {err}\")\n        return None\n\n    if data is None or data.get(\"data\") is None:\n        logger.error(\"Invalid response received from Helicone: no data\")\n        logger.error(f\"Offending response: {response}\")\n        return None\n    return (\n        data.get(\"data\", {}).get(\"aggregatedHeliconeRequest\", {}).get(\"costUSD\", None)\n    )\n", "benchmark/agbenchmark/utils/data_types.py": "from enum import Enum\nfrom typing import Literal\n\nfrom pydantic import BaseModel\n\n\nclass DifficultyLevel(Enum):\n    interface = \"interface\"\n    basic = \"basic\"\n    novice = \"novice\"\n    intermediate = \"intermediate\"\n    advanced = \"advanced\"\n    expert = \"expert\"\n    human = \"human\"\n\n\n# map from enum to difficulty level (numeric)\nDIFFICULTY_MAP = {\n    DifficultyLevel.interface: 1,\n    DifficultyLevel.basic: 2,\n    DifficultyLevel.novice: 3,\n    DifficultyLevel.intermediate: 4,\n    DifficultyLevel.advanced: 5,\n    DifficultyLevel.expert: 6,\n    DifficultyLevel.human: 7,\n}\n\nSTRING_DIFFICULTY_MAP = {e.value: DIFFICULTY_MAP[e] for e in DifficultyLevel}\n\n\nclass Category(str, Enum):\n    GENERALIST = \"general\"\n    DATA = \"data\"\n    CODING = \"coding\"\n    SCRAPE_SYNTHESIZE = \"scrape_synthesize\"\n    WEB = \"web\"\n    GAIA_1 = \"GAIA_1\"\n    GAIA_2 = \"GAIA_2\"\n    GAIA_3 = \"GAIA_3\"\n\n\nclass EvalResult(BaseModel):\n    result: str\n    result_source: Literal[\"step_output\"] | str\n    score: float\n    passed: bool\n", "benchmark/agbenchmark/utils/dependencies/graphs.py": "import json\nimport logging\nimport math\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Tuple\n\nimport matplotlib.patches as patches\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport numpy as np\nfrom pyvis.network import Network\n\nfrom agbenchmark.generate_test import DATA_CATEGORY\nfrom agbenchmark.utils.utils import write_pretty_json\n\nlogger = logging.getLogger(__name__)\n\n\ndef bezier_curve(\n    src: np.ndarray, ctrl: List[float], dst: np.ndarray\n) -> List[np.ndarray]:\n    \"\"\"\n    Generate B\u00e9zier curve points.\n\n    Args:\n    - src (np.ndarray): The source point.\n    - ctrl (List[float]): The control point.\n    - dst (np.ndarray): The destination point.\n\n    Returns:\n    - List[np.ndarray]: The B\u00e9zier curve points.\n    \"\"\"\n    curve = []\n    for t in np.linspace(0, 1, num=100):\n        curve_point = (\n            np.outer((1 - t) ** 2, src)\n            + 2 * np.outer((1 - t) * t, ctrl)\n            + np.outer(t**2, dst)\n        )\n        curve.append(curve_point[0])\n    return curve\n\n\ndef curved_edges(\n    G: nx.Graph, pos: Dict[Any, Tuple[float, float]], dist: float = 0.2\n) -> None:\n    \"\"\"\n    Draw curved edges for nodes on the same level.\n\n    Args:\n    - G (Any): The graph object.\n    - pos (Dict[Any, Tuple[float, float]]): Dictionary with node positions.\n    - dist (float, optional): Distance for curvature. Defaults to 0.2.\n\n    Returns:\n    - None\n    \"\"\"\n    ax = plt.gca()\n    for u, v, data in G.edges(data=True):\n        _src = pos[u]\n        _dst = pos[v]\n        src = np.array(_src)\n        dst = np.array(_dst)\n\n        same_level = abs(src[1] - dst[1]) < 0.01\n\n        if same_level:\n            control = [(src[0] + dst[0]) / 2, src[1] + dist]\n            curve = bezier_curve(src, control, dst)\n            arrow = patches.FancyArrowPatch(\n                posA=curve[0],  # type: ignore\n                posB=curve[-1],  # type: ignore\n                connectionstyle=\"arc3,rad=0.2\",\n                color=\"gray\",\n                arrowstyle=\"-|>\",\n                mutation_scale=15.0,\n                lw=1,\n                shrinkA=10,\n                shrinkB=10,\n            )\n            ax.add_patch(arrow)\n        else:\n            ax.annotate(\n                \"\",\n                xy=_dst,\n                xytext=_src,\n                arrowprops=dict(\n                    arrowstyle=\"-|>\", color=\"gray\", lw=1, shrinkA=10, shrinkB=10\n                ),\n            )\n\n\ndef tree_layout(graph: nx.DiGraph, root_node: Any) -> Dict[Any, Tuple[float, float]]:\n    \"\"\"Compute positions as a tree layout centered on the root\n    with alternating vertical shifts.\"\"\"\n    bfs_tree = nx.bfs_tree(graph, source=root_node)\n    levels = {\n        node: depth\n        for node, depth in nx.single_source_shortest_path_length(\n            bfs_tree, root_node\n        ).items()\n    }\n\n    pos = {}\n    max_depth = max(levels.values())\n    level_positions = {i: 0 for i in range(max_depth + 1)}  # type: ignore\n\n    # Count the number of nodes per level to compute the width\n    level_count: Any = {}\n    for node, level in levels.items():\n        level_count[level] = level_count.get(level, 0) + 1\n\n    vertical_offset = (\n        0.07  # The amount of vertical shift per node within the same level\n    )\n\n    # Assign positions\n    for node, level in sorted(levels.items(), key=lambda x: x[1]):\n        total_nodes_in_level = level_count[level]\n        horizontal_spacing = 1.0 / (total_nodes_in_level + 1)\n        pos_x = (\n            0.5\n            - (total_nodes_in_level - 1) * horizontal_spacing / 2\n            + level_positions[level] * horizontal_spacing\n        )\n\n        # Alternately shift nodes up and down within the same level\n        pos_y = (\n            -level\n            + (level_positions[level] % 2) * vertical_offset\n            - ((level_positions[level] + 1) % 2) * vertical_offset\n        )\n        pos[node] = (pos_x, pos_y)\n\n        level_positions[level] += 1\n\n    return pos\n\n\ndef graph_spring_layout(\n    dag: nx.DiGraph, labels: Dict[Any, str], tree: bool = True\n) -> None:\n    num_nodes = len(list(dag.nodes()))\n    # Setting up the figure and axis\n    fig, ax = plt.subplots()\n    ax.axis(\"off\")  # Turn off the axis\n\n    base = 3.0\n\n    if num_nodes > 10:\n        base /= 1 + math.log(num_nodes)\n        font_size = base * 10\n\n    font_size = max(10, base * 10)\n    node_size = max(300, base * 1000)\n\n    if tree:\n        root_node = [node for node, degree in dag.in_degree() if degree == 0][0]\n        pos = tree_layout(dag, root_node)\n    else:\n        # Adjust k for the spring layout based on node count\n        k_value = 3 / math.sqrt(num_nodes)\n\n        pos = nx.spring_layout(dag, k=k_value, iterations=50)\n\n    # Draw nodes and labels\n    nx.draw_networkx_nodes(dag, pos, node_color=\"skyblue\", node_size=int(node_size))\n    nx.draw_networkx_labels(dag, pos, labels=labels, font_size=int(font_size))\n\n    # Draw curved edges\n    curved_edges(dag, pos)  # type: ignore\n\n    plt.tight_layout()\n    plt.show()\n\n\ndef rgb_to_hex(rgb: Tuple[float, float, float]) -> str:\n    return \"#{:02x}{:02x}{:02x}\".format(\n        int(rgb[0] * 255), int(rgb[1] * 255), int(rgb[2] * 255)\n    )\n\n\ndef get_category_colors(categories: Dict[Any, str]) -> Dict[str, str]:\n    unique_categories = set(categories.values())\n    colormap = plt.cm.get_cmap(\"tab10\", len(unique_categories))  # type: ignore\n    return {\n        category: rgb_to_hex(colormap(i)[:3])\n        for i, category in enumerate(unique_categories)\n    }\n\n\ndef graph_interactive_network(\n    dag: nx.DiGraph,\n    labels: Dict[Any, Dict[str, Any]],\n    html_graph_path: str = \"\",\n) -> None:\n    nt = Network(notebook=True, width=\"100%\", height=\"800px\", directed=True)\n\n    category_colors = get_category_colors(DATA_CATEGORY)\n\n    # Add nodes and edges to the pyvis network\n    for node, json_data in labels.items():\n        label = json_data.get(\"name\", \"\")\n        # remove the first 4 letters of label\n        label_without_test = label[4:]\n        node_id_str = node.nodeid\n\n        # Get the category for this label\n        category = DATA_CATEGORY.get(\n            label, \"unknown\"\n        )  # Default to 'unknown' if label not found\n\n        # Get the color for this category\n        color = category_colors.get(category, \"grey\")\n\n        nt.add_node(\n            node_id_str,\n            label=label_without_test,\n            color=color,\n            data=json_data,\n        )\n\n    # Add edges to the pyvis network\n    for edge in dag.edges():\n        source_id_str = edge[0].nodeid\n        target_id_str = edge[1].nodeid\n        edge_id_str = (\n            f\"{source_id_str}_to_{target_id_str}\"  # Construct a unique edge id\n        )\n        if not (source_id_str in nt.get_nodes() and target_id_str in nt.get_nodes()):\n            logger.warning(\n                f\"Skipping edge {source_id_str} -> {target_id_str} due to missing nodes\"\n            )\n            continue\n        nt.add_edge(source_id_str, target_id_str, id=edge_id_str)\n\n    # Configure physics for hierarchical layout\n    hierarchical_options = {\n        \"enabled\": True,\n        \"levelSeparation\": 200,  # Increased vertical spacing between levels\n        \"nodeSpacing\": 250,  # Increased spacing between nodes on the same level\n        \"treeSpacing\": 250,  # Increased spacing between different trees (for forest)\n        \"blockShifting\": True,\n        \"edgeMinimization\": True,\n        \"parentCentralization\": True,\n        \"direction\": \"UD\",\n        \"sortMethod\": \"directed\",\n    }\n\n    physics_options = {\n        \"stabilization\": {\n            \"enabled\": True,\n            \"iterations\": 1000,  # Default is often around 100\n        },\n        \"hierarchicalRepulsion\": {\n            \"centralGravity\": 0.0,\n            \"springLength\": 200,  # Increased edge length\n            \"springConstant\": 0.01,\n            \"nodeDistance\": 250,  # Increased minimum distance between nodes\n            \"damping\": 0.09,\n        },\n        \"solver\": \"hierarchicalRepulsion\",\n        \"timestep\": 0.5,\n    }\n\n    nt.options = {\n        \"nodes\": {\n            \"font\": {\n                \"size\": 20,  # Increased font size for labels\n                \"color\": \"black\",  # Set a readable font color\n            },\n            \"shapeProperties\": {\"useBorderWithImage\": True},\n        },\n        \"edges\": {\n            \"length\": 250,  # Increased edge length\n        },\n        \"physics\": physics_options,\n        \"layout\": {\"hierarchical\": hierarchical_options},\n    }\n\n    # Serialize the graph to JSON and save in appropriate locations\n    graph_data = {\"nodes\": nt.nodes, \"edges\": nt.edges}\n    logger.debug(f\"Generated graph data:\\n{json.dumps(graph_data, indent=4)}\")\n\n    # FIXME: use more reliable method to find the right location for these files.\n    #   This will fail in all cases except if run from the root of our repo.\n    home_path = Path.cwd()\n    write_pretty_json(graph_data, home_path / \"frontend\" / \"public\" / \"graph.json\")\n\n    flutter_app_path = home_path.parent / \"frontend\" / \"assets\"\n\n    # Optionally, save to a file\n    # Sync with the flutter UI\n    # this literally only works in the AutoGPT repo, but this part of the code\n    # is not reached if BUILD_SKILL_TREE is false\n    write_pretty_json(graph_data, flutter_app_path / \"tree_structure.json\")\n    validate_skill_tree(graph_data, \"\")\n\n    # Extract node IDs with category \"coding\"\n\n    coding_tree = extract_subgraph_based_on_category(graph_data.copy(), \"coding\")\n    validate_skill_tree(coding_tree, \"coding\")\n    write_pretty_json(\n        coding_tree,\n        flutter_app_path / \"coding_tree_structure.json\",\n    )\n\n    data_tree = extract_subgraph_based_on_category(graph_data.copy(), \"data\")\n    # validate_skill_tree(data_tree, \"data\")\n    write_pretty_json(\n        data_tree,\n        flutter_app_path / \"data_tree_structure.json\",\n    )\n\n    general_tree = extract_subgraph_based_on_category(graph_data.copy(), \"general\")\n    validate_skill_tree(general_tree, \"general\")\n    write_pretty_json(\n        general_tree,\n        flutter_app_path / \"general_tree_structure.json\",\n    )\n\n    scrape_synthesize_tree = extract_subgraph_based_on_category(\n        graph_data.copy(), \"scrape_synthesize\"\n    )\n    validate_skill_tree(scrape_synthesize_tree, \"scrape_synthesize\")\n    write_pretty_json(\n        scrape_synthesize_tree,\n        flutter_app_path / \"scrape_synthesize_tree_structure.json\",\n    )\n\n    if html_graph_path:\n        file_path = str(Path(html_graph_path).resolve())\n\n        nt.write_html(file_path)\n\n\ndef extract_subgraph_based_on_category(graph, category):\n    \"\"\"\n    Extracts a subgraph that includes all nodes and edges required to reach all nodes\n    with a specified category.\n\n    :param graph: The original graph.\n    :param category: The target category.\n    :return: Subgraph with nodes and edges required to reach the nodes\n        with the given category.\n    \"\"\"\n\n    subgraph = {\"nodes\": [], \"edges\": []}\n    visited = set()\n\n    def reverse_dfs(node_id):\n        if node_id in visited:\n            return\n        visited.add(node_id)\n\n        node_data = next(node for node in graph[\"nodes\"] if node[\"id\"] == node_id)\n\n        # Add the node to the subgraph if it's not already present.\n        if node_data not in subgraph[\"nodes\"]:\n            subgraph[\"nodes\"].append(node_data)\n\n        for edge in graph[\"edges\"]:\n            if edge[\"to\"] == node_id:\n                if edge not in subgraph[\"edges\"]:\n                    subgraph[\"edges\"].append(edge)\n                reverse_dfs(edge[\"from\"])\n\n    # Identify nodes with the target category and initiate reverse DFS from them.\n    nodes_with_target_category = [\n        node[\"id\"] for node in graph[\"nodes\"] if category in node[\"data\"][\"category\"]\n    ]\n\n    for node_id in nodes_with_target_category:\n        reverse_dfs(node_id)\n\n    return subgraph\n\n\ndef is_circular(graph):\n    def dfs(node, visited, stack, parent_map):\n        visited.add(node)\n        stack.add(node)\n        for edge in graph[\"edges\"]:\n            if edge[\"from\"] == node:\n                if edge[\"to\"] in stack:\n                    # Detected a cycle\n                    cycle_path = []\n                    current = node\n                    while current != edge[\"to\"]:\n                        cycle_path.append(current)\n                        current = parent_map.get(current)\n                    cycle_path.append(edge[\"to\"])\n                    cycle_path.append(node)\n                    return cycle_path[::-1]\n                elif edge[\"to\"] not in visited:\n                    parent_map[edge[\"to\"]] = node\n                    cycle_path = dfs(edge[\"to\"], visited, stack, parent_map)\n                    if cycle_path:\n                        return cycle_path\n        stack.remove(node)\n        return None\n\n    visited = set()\n    stack = set()\n    parent_map = {}\n    for node in graph[\"nodes\"]:\n        node_id = node[\"id\"]\n        if node_id not in visited:\n            cycle_path = dfs(node_id, visited, stack, parent_map)\n            if cycle_path:\n                return cycle_path\n    return None\n\n\ndef get_roots(graph):\n    \"\"\"\n    Return the roots of a graph. Roots are nodes with no incoming edges.\n    \"\"\"\n    # Create a set of all node IDs\n    all_nodes = {node[\"id\"] for node in graph[\"nodes\"]}\n\n    # Create a set of nodes with incoming edges\n    nodes_with_incoming_edges = {edge[\"to\"] for edge in graph[\"edges\"]}\n\n    # Roots are nodes that have no incoming edges\n    roots = all_nodes - nodes_with_incoming_edges\n\n    return list(roots)\n\n\ndef validate_skill_tree(graph, skill_tree_name):\n    \"\"\"\n    Validate if a given graph represents a valid skill tree\n    and raise appropriate exceptions if not.\n\n    :param graph: A dictionary representing the graph with 'nodes' and 'edges'.\n    :raises: ValueError with a description of the invalidity.\n    \"\"\"\n    # Check for circularity\n    cycle_path = is_circular(graph)\n    if cycle_path:\n        cycle_str = \" -> \".join(cycle_path)\n        raise ValueError(\n            f\"{skill_tree_name} skill tree is circular! \"\n            f\"Detected circular path: {cycle_str}.\"\n        )\n\n    # Check for multiple roots\n    roots = get_roots(graph)\n    if len(roots) > 1:\n        raise ValueError(f\"{skill_tree_name} skill tree has multiple roots: {roots}.\")\n    elif not roots:\n        raise ValueError(f\"{skill_tree_name} skill tree has no roots.\")\n", "benchmark/agbenchmark/utils/dependencies/util.py": "\"\"\" Utility functions to process the identifiers of tests. \"\"\"\nimport re\nfrom typing import Iterator\n\nfrom _pytest.mark.structures import Mark\nfrom _pytest.nodes import Item\n\nfrom .constants import MARKER_KWARG_ID, MARKER_NAME\n\nREGEX_PARAMETERS = re.compile(r\"\\[.+\\]$\")\n\n\ndef clean_nodeid(nodeid: str) -> str:\n    \"\"\"\n    Remove any superfluous ::() from a node id.\n\n    >>> clean_nodeid('test_file.py::TestClass::()::test')\n    'test_file.py::TestClass::test'\n    >>> clean_nodeid('test_file.py::TestClass::test')\n    'test_file.py::TestClass::test'\n    >>> clean_nodeid('test_file.py::test')\n    'test_file.py::test'\n    \"\"\"\n    return nodeid.replace(\"::()::\", \"::\")\n\n\ndef strip_nodeid_parameters(nodeid: str) -> str:\n    \"\"\"\n    Strip parameters from a node id.\n\n    >>> strip_nodeid_parameters('test_file.py::TestClass::test[foo]')\n    'test_file.py::TestClass::test'\n    >>> strip_nodeid_parameters('test_file.py::TestClass::test')\n    'test_file.py::TestClass::test'\n    \"\"\"\n    return REGEX_PARAMETERS.sub(\"\", nodeid)\n\n\ndef get_absolute_nodeid(nodeid: str, scope: str) -> str:\n    \"\"\"\n    Transform a possibly relative node id to an absolute one\n    using the scope in which it is used.\n\n    >>> scope = 'test_file.py::TestClass::test'\n    >>> get_absolute_nodeid('test2', scope)\n    'test_file.py::TestClass::test2'\n    >>> get_absolute_nodeid('TestClass2::test2', scope)\n    'test_file.py::TestClass2::test2'\n    >>> get_absolute_nodeid('test_file2.py::TestClass2::test2', scope)\n    'test_file2.py::TestClass2::test2'\n    \"\"\"\n    parts = nodeid.split(\"::\")\n    # Completely relative (test_name): add the full current scope (file::class or file)\n    if len(parts) == 1:\n        base_nodeid = scope.rsplit(\"::\", 1)[0]\n        nodeid = f\"{base_nodeid}::{nodeid}\"\n    # Contains some scope already (Class::test_name), so only add the current file scope\n    elif \".\" not in parts[0]:\n        base_nodeid = scope.split(\"::\", 1)[0]\n        nodeid = f\"{base_nodeid}::{nodeid}\"\n    return clean_nodeid(nodeid)\n\n\ndef get_name(item: Item) -> str:\n    \"\"\"\n    Get all names for a test.\n\n    This will use the following methods to determine the name of the test:\n        - If given, the custom name(s) passed to the keyword argument name on the marker\n    \"\"\"\n    name = \"\"\n\n    # Custom name\n    markers = get_markers(item, MARKER_NAME)\n    for marker in markers:\n        if MARKER_KWARG_ID in marker.kwargs:\n            name = marker.kwargs[MARKER_KWARG_ID]\n\n    return name\n\n\ndef get_markers(item: Item, name: str) -> Iterator[Mark]:\n    \"\"\"Get all markers with the given name for a given item.\"\"\"\n    for marker in item.iter_markers():\n        if marker.name == name:\n            yield marker\n", "benchmark/agbenchmark/utils/dependencies/constants.py": "\"\"\" Constants for this module. \"\"\"\n\n# The name of the marker used\nMARKER_NAME = \"depends\"\n\n# The name of the kwarg for 'depends' markers that contains custom name(s) for the tests\nMARKER_KWARG_ID = \"name\"\n\n# The name of the keyword argument for the marker that specifies the tests to depend on\nMARKER_KWARG_DEPENDENCIES = \"on\"\n", "benchmark/agbenchmark/utils/dependencies/main.py": "\"\"\"\nA module to manage dependencies between pytest tests.\n\nThis module provides the methods implementing the main logic.\nThese are used in the pytest hooks that are in __init__.py.\n\"\"\"\n\nimport collections\nimport os\nfrom typing import Any, Generator\n\nimport colorama\nimport networkx\nfrom pytest import Function, Item\n\nfrom agbenchmark.challenges.base import BaseChallenge\n\nfrom .constants import MARKER_KWARG_DEPENDENCIES, MARKER_NAME\nfrom .graphs import graph_interactive_network\nfrom .util import clean_nodeid, get_absolute_nodeid, get_markers, get_name\n\n\nclass TestResult(object):\n    \"\"\"Keeps track of the results of a single test.\"\"\"\n\n    STEPS = [\"setup\", \"call\", \"teardown\"]\n    GOOD_OUTCOMES = [\"passed\"]\n\n    def __init__(self, nodeid: str) -> None:\n        \"\"\"Create a new instance for a test with a given node id.\"\"\"\n        self.nodeid = nodeid\n        self.results: dict[str, Any] = {}\n\n    def register_result(self, result: Any) -> None:\n        \"\"\"Register a result of this test.\"\"\"\n        if result.when not in self.STEPS:\n            raise ValueError(\n                f\"Received result for unknown step {result.when} of test {self.nodeid}\"\n            )\n        if result.when in self.results:\n            raise AttributeError(\n                f\"Received multiple results for step {result.when} \"\n                f\"of test {self.nodeid}\"\n            )\n        self.results[result.when] = result.outcome\n\n    @property\n    def success(self) -> bool:\n        \"\"\"Whether the entire test was successful.\"\"\"\n        return all(\n            self.results.get(step, None) in self.GOOD_OUTCOMES for step in self.STEPS\n        )\n\n\nclass TestDependencies(object):\n    \"\"\"Information about the resolved dependencies of a single test.\"\"\"\n\n    def __init__(self, item: Item, manager: \"DependencyManager\") -> None:\n        \"\"\"Create a new instance for a given test.\"\"\"\n        self.nodeid = clean_nodeid(item.nodeid)\n        self.dependencies = set()\n        self.unresolved = set()\n\n        markers = get_markers(item, MARKER_NAME)\n        dependencies = [\n            dep\n            for marker in markers\n            for dep in marker.kwargs.get(MARKER_KWARG_DEPENDENCIES, [])\n        ]\n        for dependency in dependencies:\n            # If the name is not known, try to make it absolute (file::[class::]method)\n            if dependency not in manager.name_to_nodeids:\n                absolute_dependency = get_absolute_nodeid(dependency, self.nodeid)\n                if absolute_dependency in manager.name_to_nodeids:\n                    dependency = absolute_dependency\n\n            # Add all items matching the name\n            if dependency in manager.name_to_nodeids:\n                for nodeid in manager.name_to_nodeids[dependency]:\n                    self.dependencies.add(nodeid)\n            else:\n                self.unresolved.add(dependency)\n\n\nclass DependencyManager(object):\n    \"\"\"Keep track of tests, their names and their dependencies.\"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Create a new DependencyManager.\"\"\"\n        self.options: dict[str, Any] = {}\n        self._items: list[Function] | None = None\n        self._name_to_nodeids: Any = None\n        self._nodeid_to_item: Any = None\n        self._results: Any = None\n\n    @property\n    def items(self) -> list[Function]:\n        \"\"\"The collected tests that are managed by this instance.\"\"\"\n        if self._items is None:\n            raise AttributeError(\"The items attribute has not been set yet\")\n        return self._items\n\n    @items.setter\n    def items(self, items: list[Function]) -> None:\n        if self._items is not None:\n            raise AttributeError(\"The items attribute has already been set\")\n        self._items = items\n\n        self._name_to_nodeids = collections.defaultdict(list)\n        self._nodeid_to_item = {}\n        self._results = {}\n        self._dependencies = {}\n\n        for item in items:\n            nodeid = clean_nodeid(item.nodeid)\n            # Add the mapping from nodeid to the test item\n            self._nodeid_to_item[nodeid] = item\n            # Add the mappings from all names to the node id\n            name = get_name(item)\n            self._name_to_nodeids[name].append(nodeid)\n            # Create the object that will contain the results of this test\n            self._results[nodeid] = TestResult(clean_nodeid(item.nodeid))\n\n        # Don't allow using unknown keys on the name_to_nodeids mapping\n        self._name_to_nodeids.default_factory = None\n\n        for item in items:\n            nodeid = clean_nodeid(item.nodeid)\n            # Process the dependencies of this test\n            # This uses the mappings created in the previous loop,\n            # and can thus not be merged into that loop\n            self._dependencies[nodeid] = TestDependencies(item, self)\n\n    @property\n    def name_to_nodeids(self) -> dict[str, list[str]]:\n        \"\"\"A mapping from names to matching node id(s).\"\"\"\n        assert self.items is not None\n        return self._name_to_nodeids\n\n    @property\n    def nodeid_to_item(self) -> dict[str, Function]:\n        \"\"\"A mapping from node ids to test items.\"\"\"\n        assert self.items is not None\n        return self._nodeid_to_item\n\n    @property\n    def results(self) -> dict[str, TestResult]:\n        \"\"\"The results of the tests.\"\"\"\n        assert self.items is not None\n        return self._results\n\n    @property\n    def dependencies(self) -> dict[str, TestDependencies]:\n        \"\"\"The dependencies of the tests.\"\"\"\n        assert self.items is not None\n        return self._dependencies\n\n    def print_name_map(self, verbose: bool = False) -> None:\n        \"\"\"Print a human-readable version of the name -> test mapping.\"\"\"\n        print(\"Available dependency names:\")\n        for name, nodeids in sorted(self.name_to_nodeids.items(), key=lambda x: x[0]):\n            if len(nodeids) == 1:\n                if name == nodeids[0]:\n                    # This is just the base name, only print this when verbose\n                    if verbose:\n                        print(f\"  {name}\")\n                else:\n                    # Name refers to a single node id, so use the short format\n                    print(f\"  {name} -> {nodeids[0]}\")\n            else:\n                # Name refers to multiple node ids, so use the long format\n                print(f\"  {name} ->\")\n                for nodeid in sorted(nodeids):\n                    print(f\"    {nodeid}\")\n\n    def print_processed_dependencies(self, colors: bool = False) -> None:\n        \"\"\"Print a human-readable list of the processed dependencies.\"\"\"\n        missing = \"MISSING\"\n        if colors:\n            missing = f\"{colorama.Fore.RED}{missing}{colorama.Fore.RESET}\"\n            colorama.init()\n        try:\n            print(\"Dependencies:\")\n            for nodeid, info in sorted(self.dependencies.items(), key=lambda x: x[0]):\n                descriptions = []\n                for dependency in info.dependencies:\n                    descriptions.append(dependency)\n                for dependency in info.unresolved:\n                    descriptions.append(f\"{dependency} ({missing})\")\n                if descriptions:\n                    print(f\"  {nodeid} depends on\")\n                    for description in sorted(descriptions):\n                        print(f\"    {description}\")\n        finally:\n            if colors:\n                colorama.deinit()\n\n    @property\n    def sorted_items(self) -> Generator:\n        \"\"\"\n        Get a sorted list of tests where all tests are sorted after their dependencies.\n        \"\"\"\n        # Build a directed graph for sorting\n        build_skill_tree = os.getenv(\"BUILD_SKILL_TREE\")\n        BUILD_SKILL_TREE = (\n            build_skill_tree.lower() == \"true\" if build_skill_tree else False\n        )\n        dag = networkx.DiGraph()\n\n        # Insert all items as nodes, to prevent items that have no dependencies\n        # and are not dependencies themselves from being lost\n        dag.add_nodes_from(self.items)\n\n        # Insert edges for all the dependencies\n        for item in self.items:\n            nodeid = clean_nodeid(item.nodeid)\n            for dependency in self.dependencies[nodeid].dependencies:\n                dag.add_edge(self.nodeid_to_item[dependency], item)\n\n        labels = {}\n        for item in self.items:\n            assert item.cls and issubclass(item.cls, BaseChallenge)\n            data = item.cls.info.dict()\n\n            node_name = get_name(item)\n            data[\"name\"] = node_name\n            labels[item] = data\n\n        # only build the tree if it's specified in the env and is a whole run\n        if BUILD_SKILL_TREE:\n            # graph_spring_layout(dag, labels)\n            graph_interactive_network(dag, labels, html_graph_path=\"\")\n\n        # Sort based on the dependencies\n        return networkx.topological_sort(dag)\n\n    def register_result(self, item: Item, result: Any) -> None:\n        \"\"\"Register a result of a test.\"\"\"\n        nodeid = clean_nodeid(item.nodeid)\n        self.results[nodeid].register_result(result)\n\n    def get_failed(self, item: Item) -> Any:\n        \"\"\"Get a list of unfulfilled dependencies for a test.\"\"\"\n        nodeid = clean_nodeid(item.nodeid)\n        failed = []\n        for dependency in self.dependencies[nodeid].dependencies:\n            result = self.results[dependency]\n            if not result.success:\n                failed.append(dependency)\n        return failed\n\n    def get_missing(self, item: Item) -> Any:\n        \"\"\"Get a list of missing dependencies for a test.\"\"\"\n        nodeid = clean_nodeid(item.nodeid)\n        return self.dependencies[nodeid].unresolved\n", "benchmark/agbenchmark/utils/dependencies/__init__.py": "\"\"\"\nA module that provides the pytest hooks for this plugin.\n\nThe logic itself is in main.py.\n\"\"\"\n\nimport warnings\nfrom typing import Any, Callable, Optional\n\nimport pytest\nfrom _pytest.config.argparsing import OptionGroup, Parser\nfrom _pytest.nodes import Item\n\nfrom .main import DependencyManager\n\nmanagers: list[DependencyManager] = []\n\n\nDEPENDENCY_PROBLEM_ACTIONS: dict[str, Callable[[str], None] | None] = {\n    \"run\": None,\n    \"skip\": lambda m: pytest.skip(m),\n    \"fail\": lambda m: pytest.fail(m, False),\n    \"warning\": lambda m: warnings.warn(m),\n}\n\n\ndef _add_ini_and_option(\n    parser: Any,\n    group: OptionGroup,\n    name: str,\n    help: str,\n    default: str | bool | int,\n    **kwargs: Any,\n) -> None:\n    \"\"\"\n    Add an option to both the ini file and the command line flags.\n    Command line flags/options takes precedence over the ini config.\n    \"\"\"\n    parser.addini(\n        name,\n        help + \" This overrides the similarly named option from the config.\",\n        default=default,\n    )\n    group.addoption(f'--{name.replace(\"_\", \"-\")}', help=help, default=None, **kwargs)\n\n\ndef _get_ini_or_option(\n    config: Any, name: str, choices: Optional[list[str]]\n) -> str | None:\n    \"\"\"\n    Get an option from either the ini file or the command line flags,\n    with the latter taking precedence.\n    \"\"\"\n    value = config.getini(name)\n    if value is not None and choices is not None and value not in choices:\n        raise ValueError(\n            f'Invalid ini value for {name}, choose from {\", \".join(choices)}'\n        )\n    return config.getoption(name) or value\n\n\ndef pytest_addoption(parser: Parser) -> None:\n    # get all current option strings\n    current_options = []\n    for action in parser._anonymous.options:\n        current_options += action._short_opts + action._long_opts\n\n    for group in parser._groups:\n        for action in group.options:\n            current_options += action._short_opts + action._long_opts\n\n    group = parser.getgroup(\"depends\")\n\n    # Add a flag to list all names + the tests they resolve to\n    if \"--list-dependency-names\" not in current_options:\n        group.addoption(\n            \"--list-dependency-names\",\n            action=\"store_true\",\n            default=False,\n            help=(\n                \"List all non-nodeid dependency names + the tests they resolve to. \"\n                \"Will also list all nodeid dependency names in verbose mode.\"\n            ),\n        )\n\n    # Add a flag to list all (resolved) dependencies for all tests + unresolvable names\n    if \"--list-processed-dependencies\" not in current_options:\n        group.addoption(\n            \"--list-processed-dependencies\",\n            action=\"store_true\",\n            default=False,\n            help=(\n                \"List all dependencies of all tests as a list of nodeids \"\n                \"+ the names that could not be resolved.\"\n            ),\n        )\n\n    # Add an ini option + flag to choose the action to take for failed dependencies\n    if \"--failed-dependency-action\" not in current_options:\n        _add_ini_and_option(\n            parser,\n            group,\n            name=\"failed_dependency_action\",\n            help=(\n                \"The action to take when a test has dependencies that failed. \"\n                'Use \"run\" to run the test anyway, \"skip\" to skip the test, '\n                'and \"fail\" to fail the test.'\n            ),\n            default=\"skip\",\n            choices=DEPENDENCY_PROBLEM_ACTIONS.keys(),\n        )\n\n    # Add an ini option + flag to choose the action to take for unresolved dependencies\n    if \"--missing-dependency-action\" not in current_options:\n        _add_ini_and_option(\n            parser,\n            group,\n            name=\"missing_dependency_action\",\n            help=(\n                \"The action to take when a test has dependencies that cannot be found \"\n                \"within the current scope. \"\n                'Use \"run\" to run the test anyway, \"skip\" to skip the test, '\n                'and \"fail\" to fail the test.'\n            ),\n            default=\"warning\",\n            choices=DEPENDENCY_PROBLEM_ACTIONS.keys(),\n        )\n\n\ndef pytest_configure(config: Any) -> None:\n    manager = DependencyManager()\n    managers.append(manager)\n\n    # Setup the handling of problems with dependencies\n    manager.options[\"failed_dependency_action\"] = _get_ini_or_option(\n        config,\n        \"failed_dependency_action\",\n        list(DEPENDENCY_PROBLEM_ACTIONS.keys()),\n    )\n    manager.options[\"missing_dependency_action\"] = _get_ini_or_option(\n        config,\n        \"missing_dependency_action\",\n        list(DEPENDENCY_PROBLEM_ACTIONS.keys()),\n    )\n\n    # Register marker\n    config.addinivalue_line(\n        \"markers\",\n        \"depends(name='name', on=['other_name']): marks dependencies between tests.\",\n    )\n\n\n@pytest.hookimpl(trylast=True)\ndef pytest_collection_modifyitems(config: Any, items: list[pytest.Function]) -> None:\n    manager = managers[-1]\n\n    # Register the founds tests on the manager\n    manager.items = items\n\n    # Show the extra information if requested\n    if config.getoption(\"list_dependency_names\"):\n        verbose = config.getoption(\"verbose\") > 1\n        manager.print_name_map(verbose)\n    if config.getoption(\"list_processed_dependencies\"):\n        color = config.getoption(\"color\")\n        manager.print_processed_dependencies(color)\n\n    # Reorder the items so that tests run after their dependencies\n    items[:] = manager.sorted_items\n\n\n@pytest.hookimpl(tryfirst=True, hookwrapper=True)\ndef pytest_runtest_makereport(item: Item) -> Any:\n    manager = managers[-1]\n\n    # Run the step\n    outcome = yield\n\n    # Store the result on the manager\n    manager.register_result(item, outcome.get_result())\n\n\ndef pytest_runtest_call(item: Item) -> None:\n    manager = managers[-1]\n\n    # Handle missing dependencies\n    missing_dependency_action = DEPENDENCY_PROBLEM_ACTIONS[\n        manager.options[\"missing_dependency_action\"]\n    ]\n    missing = manager.get_missing(item)\n    if missing_dependency_action and missing:\n        missing_dependency_action(\n            f'{item.nodeid} depends on {\", \".join(missing)}, which was not found'\n        )\n\n    # Check whether all dependencies succeeded\n    failed_dependency_action = DEPENDENCY_PROBLEM_ACTIONS[\n        manager.options[\"failed_dependency_action\"]\n    ]\n    failed = manager.get_failed(item)\n    if failed_dependency_action and failed:\n        failed_dependency_action(f'{item.nodeid} depends on {\", \".join(failed)}')\n\n\ndef pytest_unconfigure() -> None:\n    managers.pop()\n", "benchmark/agbenchmark/reports/reports.py": "import json\nimport logging\nimport os\nfrom pathlib import Path\n\nimport pytest\nfrom pydantic import ValidationError\n\nfrom agbenchmark.challenges import ChallengeInfo\nfrom agbenchmark.config import AgentBenchmarkConfig\nfrom agbenchmark.reports.processing.report_types import Test, TestMetrics, TestResult\nfrom agbenchmark.reports.ReportManager import SingletonReportManager\nfrom agbenchmark.utils.data_types import DifficultyLevel\n\n# from agbenchmark.utils.get_data_from_helicone import get_data_from_helicone\n\nlogger = logging.getLogger(__name__)\n\n\ndef get_and_update_success_history(\n    test_name: str, success: bool | None\n) -> list[bool | None]:\n    mock = os.getenv(\"IS_MOCK\")  # Check if --mock is in sys.argv\n\n    prev_test_results = SingletonReportManager().SUCCESS_RATE_TRACKER.tests.get(\n        test_name, []\n    )\n\n    if not mock:\n        # only add if it's an actual test\n        prev_test_results.append(success)\n        SingletonReportManager().SUCCESS_RATE_TRACKER.update(\n            test_name, prev_test_results\n        )\n\n    return prev_test_results\n\n\ndef update_regression_tests(\n    prev_test_results: list[bool | None],\n    test_report: Test,\n    test_name: str,\n) -> None:\n    if len(prev_test_results) >= 3 and prev_test_results[-3:] == [True, True, True]:\n        # if the last 3 tests were successful, add to the regression tests\n        test_report.metrics.is_regression = True\n        SingletonReportManager().REGRESSION_MANAGER.add_test(\n            test_name, test_report.dict(include={\"difficulty\", \"data_path\"})\n        )\n\n\ndef make_empty_test_report(\n    challenge_info: ChallengeInfo,\n) -> Test:\n    difficulty = challenge_info.difficulty\n    if isinstance(difficulty, DifficultyLevel):\n        difficulty = difficulty.value\n\n    return Test(\n        category=[c.value for c in challenge_info.category],\n        difficulty=difficulty,\n        data_path=challenge_info.source_uri,\n        description=challenge_info.description or \"\",\n        task=challenge_info.task,\n        answer=challenge_info.reference_answer or \"\",\n        metrics=TestMetrics(attempted=False, is_regression=False),\n        results=[],\n    )\n\n\ndef add_test_result_to_report(\n    test_report: Test,\n    item: pytest.Item,\n    call: pytest.CallInfo,\n    config: AgentBenchmarkConfig,\n) -> None:\n    user_properties: dict = dict(item.user_properties)\n    test_name: str = user_properties.get(\"test_name\", \"\")\n\n    mock = os.getenv(\"IS_MOCK\")  # Check if --mock is in sys.argv\n\n    if call.excinfo:\n        if not mock:\n            SingletonReportManager().REGRESSION_MANAGER.remove_test(test_name)\n\n        test_report.metrics.attempted = call.excinfo.typename != \"Skipped\"\n    else:\n        test_report.metrics.attempted = True\n\n    try:\n        test_report.results.append(\n            TestResult(\n                success=call.excinfo is None,\n                run_time=f\"{str(round(call.duration, 3))} seconds\",\n                fail_reason=(\n                    str(call.excinfo.value) if call.excinfo is not None else None\n                ),\n                reached_cutoff=user_properties.get(\"timed_out\", False),\n                n_steps=user_properties.get(\"n_steps\"),\n                steps=user_properties.get(\"steps\", []),\n                cost=user_properties.get(\"agent_task_cost\"),\n            )\n        )\n        test_report.metrics.success_percentage = (\n            sum(r.success or False for r in test_report.results)\n            / len(test_report.results)\n            * 100\n        )\n    except ValidationError:\n        if call.excinfo:\n            logger.error(\n                \"Validation failed on TestResult; \"\n                f\"call.excinfo = {repr(call.excinfo)};\\n{call.excinfo.getrepr()})\"\n            )\n        raise\n\n    prev_test_results: list[bool | None] = get_and_update_success_history(\n        test_name, test_report.results[-1].success\n    )\n\n    update_regression_tests(prev_test_results, test_report, test_name)\n\n    if test_report and test_name:\n        # if \"--mock\" not in sys.argv and os.environ.get(\"HELICONE_API_KEY\"):\n        #     logger.debug(\"Getting cost from Helicone\")\n        #     test_report.metrics.cost = get_data_from_helicone(test_name)\n        #     logger.debug(f\"Cost: {cost}\")\n\n        if not mock:\n            update_challenges_already_beaten(\n                config.challenges_already_beaten_file, test_report, test_name\n            )\n\n        SingletonReportManager().INFO_MANAGER.add_test_report(test_name, test_report)\n\n\ndef update_challenges_already_beaten(\n    challenges_already_beaten_file: Path, test_report: Test, test_name: str\n) -> None:\n    current_run_successful = any(r.success for r in test_report.results)\n    try:\n        with open(challenges_already_beaten_file, \"r\") as f:\n            challenges_beaten_before = json.load(f)\n    except FileNotFoundError:\n        challenges_beaten_before = {}\n\n    has_ever_been_beaten = challenges_beaten_before.get(test_name)\n    challenges_beaten_before[test_name] = has_ever_been_beaten or current_run_successful\n\n    with open(challenges_already_beaten_file, \"w\") as f:\n        json.dump(challenges_beaten_before, f, indent=4)\n\n\ndef session_finish(agbenchmark_config: AgentBenchmarkConfig) -> None:\n    SingletonReportManager().INFO_MANAGER.finalize_session_report(agbenchmark_config)\n    SingletonReportManager().REGRESSION_MANAGER.save()\n    SingletonReportManager().SUCCESS_RATE_TRACKER.save()\n", "benchmark/agbenchmark/reports/ReportManager.py": "import copy\nimport json\nimport logging\nimport os\nimport sys\nimport time\nfrom datetime import datetime, timezone\nfrom pathlib import Path\nfrom typing import Any\n\nfrom agbenchmark.config import AgentBenchmarkConfig\nfrom agbenchmark.reports.processing.graphs import save_single_radar_chart\nfrom agbenchmark.reports.processing.process_report import (\n    get_highest_achieved_difficulty_per_category,\n)\nfrom agbenchmark.reports.processing.report_types import MetricsOverall, Report, Test\nfrom agbenchmark.utils.utils import get_highest_success_difficulty\n\nlogger = logging.getLogger(__name__)\n\n\nclass SingletonReportManager:\n    instance = None\n\n    INFO_MANAGER: \"SessionReportManager\"\n    REGRESSION_MANAGER: \"RegressionTestsTracker\"\n    SUCCESS_RATE_TRACKER: \"SuccessRatesTracker\"\n\n    def __new__(cls):\n        if not cls.instance:\n            cls.instance = super(SingletonReportManager, cls).__new__(cls)\n\n            agent_benchmark_config = AgentBenchmarkConfig.load()\n            benchmark_start_time_dt = datetime.now(\n                timezone.utc\n            )  # or any logic to fetch the datetime\n\n            # Make the Managers class attributes\n            cls.INFO_MANAGER = SessionReportManager(\n                agent_benchmark_config.get_report_dir(benchmark_start_time_dt)\n                / \"report.json\",\n                benchmark_start_time_dt,\n            )\n            cls.REGRESSION_MANAGER = RegressionTestsTracker(\n                agent_benchmark_config.regression_tests_file\n            )\n            cls.SUCCESS_RATE_TRACKER = SuccessRatesTracker(\n                agent_benchmark_config.success_rate_file\n            )\n\n        return cls.instance\n\n    @classmethod\n    def clear_instance(cls):\n        cls.instance = None\n        del cls.INFO_MANAGER\n        del cls.REGRESSION_MANAGER\n        del cls.SUCCESS_RATE_TRACKER\n\n\nclass BaseReportManager:\n    \"\"\"Abstracts interaction with the regression tests file\"\"\"\n\n    tests: dict[str, Any]\n\n    def __init__(self, report_file: Path):\n        self.report_file = report_file\n\n        self.load()\n\n    def load(self) -> None:\n        if not self.report_file.exists():\n            self.report_file.parent.mkdir(exist_ok=True)\n\n        try:\n            with self.report_file.open(\"r\") as f:\n                data = json.load(f)\n                self.tests = {k: data[k] for k in sorted(data)}\n        except FileNotFoundError:\n            self.tests = {}\n        except json.decoder.JSONDecodeError as e:\n            logger.warning(f\"Could not parse {self.report_file}: {e}\")\n            self.tests = {}\n\n    def save(self) -> None:\n        with self.report_file.open(\"w\") as f:\n            json.dump(self.tests, f, indent=4)\n\n    def remove_test(self, test_name: str) -> None:\n        if test_name in self.tests:\n            del self.tests[test_name]\n            self.save()\n\n    def reset(self) -> None:\n        self.tests = {}\n        self.save()\n\n\nclass SessionReportManager(BaseReportManager):\n    \"\"\"Abstracts interaction with the regression tests file\"\"\"\n\n    tests: dict[str, Test]\n    report: Report | None = None\n\n    def __init__(self, report_file: Path, benchmark_start_time: datetime):\n        super().__init__(report_file)\n\n        self.start_time = time.time()\n        self.benchmark_start_time = benchmark_start_time\n\n    def save(self) -> None:\n        with self.report_file.open(\"w\") as f:\n            if self.report:\n                f.write(self.report.json(indent=4))\n            else:\n                json.dump({k: v.dict() for k, v in self.tests.items()}, f, indent=4)\n\n    def load(self) -> None:\n        super().load()\n\n        if \"tests\" in self.tests:\n            self.report = Report.parse_obj(self.tests)\n        else:\n            self.tests = {n: Test.parse_obj(d) for n, d in self.tests.items()}\n\n    def add_test_report(self, test_name: str, test_report: Test) -> None:\n        if self.report:\n            raise RuntimeError(\"Session report already finalized\")\n\n        if test_name.startswith(\"Test\"):\n            test_name = test_name[4:]\n        self.tests[test_name] = test_report\n\n        self.save()\n\n    def finalize_session_report(self, config: AgentBenchmarkConfig) -> None:\n        command = \" \".join(sys.argv)\n\n        if self.report:\n            raise RuntimeError(\"Session report already finalized\")\n\n        self.report = Report(\n            command=command.split(os.sep)[-1],\n            benchmark_git_commit_sha=\"---\",\n            agent_git_commit_sha=\"---\",\n            completion_time=datetime.now(timezone.utc).strftime(\n                \"%Y-%m-%dT%H:%M:%S+00:00\"\n            ),\n            benchmark_start_time=self.benchmark_start_time.strftime(\n                \"%Y-%m-%dT%H:%M:%S+00:00\"\n            ),\n            metrics=MetricsOverall(\n                run_time=str(round(time.time() - self.start_time, 2)) + \" seconds\",\n                highest_difficulty=get_highest_success_difficulty(self.tests),\n                total_cost=self.get_total_costs(),\n            ),\n            tests=copy.copy(self.tests),\n            config=config.dict(exclude={\"reports_folder\"}, exclude_none=True),\n        )\n\n        agent_categories = get_highest_achieved_difficulty_per_category(self.report)\n        if len(agent_categories) > 1:\n            save_single_radar_chart(\n                agent_categories,\n                config.get_report_dir(self.benchmark_start_time) / \"radar_chart.png\",\n            )\n\n        self.save()\n\n    def get_total_costs(self):\n        if self.report:\n            tests = self.report.tests\n        else:\n            tests = self.tests\n\n        total_cost = 0\n        all_costs_none = True\n        for test_data in tests.values():\n            cost = sum(r.cost or 0 for r in test_data.results)\n\n            if cost is not None:  # check if cost is not None\n                all_costs_none = False\n                total_cost += cost  # add cost to total\n        if all_costs_none:\n            total_cost = None\n        return total_cost\n\n\nclass RegressionTestsTracker(BaseReportManager):\n    \"\"\"Abstracts interaction with the regression tests file\"\"\"\n\n    tests: dict[str, dict]\n\n    def add_test(self, test_name: str, test_details: dict) -> None:\n        if test_name.startswith(\"Test\"):\n            test_name = test_name[4:]\n\n        self.tests[test_name] = test_details\n        self.save()\n\n    def has_regression_test(self, test_name: str) -> bool:\n        return self.tests.get(test_name) is not None\n\n\nclass SuccessRatesTracker(BaseReportManager):\n    \"\"\"Abstracts interaction with the regression tests file\"\"\"\n\n    tests: dict[str, list[bool | None]]\n\n    def update(self, test_name: str, success_history: list[bool | None]) -> None:\n        if test_name.startswith(\"Test\"):\n            test_name = test_name[4:]\n\n        self.tests[test_name] = success_history\n        self.save()\n", "benchmark/agbenchmark/reports/processing/graphs.py": "from pathlib import Path\nfrom typing import Any\n\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom matplotlib.colors import Normalize\n\n\ndef save_combined_radar_chart(\n    categories: dict[str, Any], save_path: str | Path\n) -> None:\n    categories = {k: v for k, v in categories.items() if v}\n    if not all(categories.values()):\n        raise Exception(\"No data to plot\")\n    labels = np.array(\n        list(next(iter(categories.values())).keys())\n    )  # We use the first category to get the keys\n    num_vars = len(labels)\n    angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n    angles += angles[\n        :1\n    ]  # Add the first angle to the end of the list to ensure the polygon is closed\n\n    # Create radar chart\n    fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))\n    ax.set_theta_offset(np.pi / 2)  # type: ignore\n    ax.set_theta_direction(-1)  # type: ignore\n    ax.spines[\"polar\"].set_visible(False)  # Remove border\n\n    # Define a custom normalization to start the color from the middle\n    norm = Normalize(\n        vmin=0, vmax=max([max(val.values()) for val in categories.values()])\n    )  # We use the maximum of all categories for normalization\n\n    cmap = plt.cm.get_cmap(\"nipy_spectral\", len(categories))  # type: ignore\n\n    colors = [cmap(i) for i in range(len(categories))]\n\n    for i, (cat_name, cat_values) in enumerate(\n        categories.items()\n    ):  # Iterating through each category (series)\n        values = np.array(list(cat_values.values()))\n        values = np.concatenate((values, values[:1]))  # Ensure the polygon is closed\n\n        ax.fill(angles, values, color=colors[i], alpha=0.25)  # Draw the filled polygon\n        ax.plot(angles, values, color=colors[i], linewidth=2)  # Draw polygon\n        ax.plot(\n            angles,\n            values,\n            \"o\",\n            color=\"white\",\n            markersize=7,\n            markeredgecolor=colors[i],\n            markeredgewidth=2,\n        )  # Draw points\n\n        # Draw legend\n        legend = ax.legend(\n            handles=[\n                mpatches.Patch(color=color, label=cat_name, alpha=0.25)\n                for cat_name, color in zip(categories.keys(), colors)\n            ],\n            loc=\"upper left\",\n            bbox_to_anchor=(0.7, 1.3),\n        )\n\n        # Adjust layout to make room for the legend\n        plt.tight_layout()\n\n    lines, labels = plt.thetagrids(\n        np.degrees(angles[:-1]), (list(next(iter(categories.values())).keys()))\n    )  # We use the first category to get the keys\n\n    highest_score = 7\n\n    # Set y-axis limit to 7\n    ax.set_ylim(top=highest_score)\n\n    # Move labels away from the plot\n    for label in labels:\n        label.set_position(\n            (label.get_position()[0], label.get_position()[1] + -0.05)\n        )  # adjust 0.1 as needed\n\n    # Move radial labels away from the plot\n    ax.set_rlabel_position(180)  # type: ignore\n\n    ax.set_yticks([])  # Remove default yticks\n\n    # Manually create gridlines\n    for y in np.arange(0, highest_score + 1, 1):\n        if y != highest_score:\n            ax.plot(\n                angles, [y] * len(angles), color=\"gray\", linewidth=0.5, linestyle=\":\"\n            )\n        # Add labels for manually created gridlines\n        ax.text(\n            angles[0],\n            y + 0.2,\n            str(int(y)),\n            color=\"black\",\n            size=9,\n            horizontalalignment=\"center\",\n            verticalalignment=\"center\",\n        )\n\n    plt.savefig(save_path, dpi=300)  # Save the figure as a PNG file\n    plt.close()  # Close the figure to free up memory\n\n\ndef save_single_radar_chart(\n    category_dict: dict[str, int], save_path: str | Path\n) -> None:\n    labels = np.array(list(category_dict.keys()))\n    values = np.array(list(category_dict.values()))\n\n    num_vars = len(labels)\n\n    angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n\n    angles += angles[:1]\n    values = np.concatenate((values, values[:1]))\n\n    colors = [\"#1f77b4\"]\n\n    fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))\n    ax.set_theta_offset(np.pi / 2)  # type: ignore\n    ax.set_theta_direction(-1)  # type: ignore\n\n    ax.spines[\"polar\"].set_visible(False)\n\n    lines, labels = plt.thetagrids(\n        np.degrees(angles[:-1]), (list(category_dict.keys()))\n    )\n\n    highest_score = 7\n\n    # Set y-axis limit to 7\n    ax.set_ylim(top=highest_score)\n\n    for label in labels:\n        label.set_position((label.get_position()[0], label.get_position()[1] + -0.05))\n\n    ax.fill(angles, values, color=colors[0], alpha=0.25)\n    ax.plot(angles, values, color=colors[0], linewidth=2)\n\n    for i, (angle, value) in enumerate(zip(angles, values)):\n        ha = \"left\"\n        if angle in {0, np.pi}:\n            ha = \"center\"\n        elif np.pi < angle < 2 * np.pi:\n            ha = \"right\"\n        ax.text(\n            angle,\n            value - 0.5,\n            f\"{value}\",\n            size=10,\n            horizontalalignment=ha,\n            verticalalignment=\"center\",\n            color=\"black\",\n        )\n\n    ax.set_yticklabels([])\n\n    ax.set_yticks([])\n\n    if values.size == 0:\n        return\n\n    for y in np.arange(0, highest_score, 1):\n        ax.plot(angles, [y] * len(angles), color=\"gray\", linewidth=0.5, linestyle=\":\")\n\n    for angle, value in zip(angles, values):\n        ax.plot(\n            angle,\n            value,\n            \"o\",\n            color=\"white\",\n            markersize=7,\n            markeredgecolor=colors[0],\n            markeredgewidth=2,\n        )\n\n    plt.savefig(save_path, dpi=300)  # Save the figure as a PNG file\n    plt.close()  # Close the figure to free up memory\n\n\ndef save_combined_bar_chart(categories: dict[str, Any], save_path: str | Path) -> None:\n    if not all(categories.values()):\n        raise Exception(\"No data to plot\")\n\n    # Convert dictionary to DataFrame\n    df = pd.DataFrame(categories)\n\n    # Create a grouped bar chart\n    df.plot(kind=\"bar\", figsize=(10, 7))\n\n    plt.title(\"Performance by Category for Each Agent\")\n    plt.xlabel(\"Category\")\n    plt.ylabel(\"Performance\")\n\n    plt.savefig(save_path, dpi=300)  # Save the figure as a PNG file\n    plt.close()  # Close the figure to free up memory\n", "benchmark/agbenchmark/reports/processing/report_types_v2.py": "\"\"\"Model definitions for use in the API\"\"\"\nfrom typing import Annotated\n\nfrom pydantic import BaseModel, constr\n\ndatetime_format = r\"^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\+00:00$\"\n\n\nclass TaskInfo(BaseModel):\n    data_path: str\n    is_regression: bool | None\n    answer: str\n    description: str\n    category: list[str]\n    task: str\n\n\nclass RepositoryInfo(BaseModel):\n    repo_url: str | None = None\n    team_name: str | None = None\n    agent_git_commit_sha: str | None = None\n    benchmark_git_commit_sha: str | None = None\n\n\nclass Metrics(BaseModel):\n    cost: float | None = None\n    success: bool\n    attempted: bool\n    difficulty: str | None = None\n    run_time: str | None = None\n    fail_reason: str | None = None\n    success_percentage: float | None = None\n\n\nclass RunDetails(BaseModel):\n    test_name: str\n    run_id: str | None = None\n    command: str\n    completion_time: str | None = None\n    benchmark_start_time: Annotated[str, constr(regex=datetime_format)]\n\n\nclass BenchmarkRun(BaseModel):\n    repository_info: RepositoryInfo\n    run_details: RunDetails\n    task_info: TaskInfo\n    metrics: Metrics\n    reached_cutoff: bool | None = None\n    config: dict[str, str | dict[str, str]]\n", "benchmark/agbenchmark/reports/processing/report_types.py": "\"\"\"\nModel definitions used internally and for reports generated during command-line runs.\n\"\"\"\n\nimport logging\nfrom typing import Annotated, Any, Dict, List\n\nfrom agent_protocol_client import Step\nfrom pydantic import BaseModel, Field, constr, validator\n\ndatetime_format = r\"^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\+00:00$\"\nlogger = logging.getLogger(__name__)\n\n\nclass TestResult(BaseModel):\n    \"\"\"Result details for a single run of a test/challenge.\"\"\"\n\n    success: bool | None = None\n    \"\"\"Whether the run was successful\"\"\"\n    run_time: str | None = None\n    \"\"\"The (formatted) duration of the run\"\"\"\n    fail_reason: str | None = None\n    \"\"\"If applicable, the reason why the run was not successful\"\"\"\n    reached_cutoff: bool | None = None  # None if in progress\n    \"\"\"Whether the run had to be stopped due to reaching the timeout\"\"\"\n    n_steps: int | None = None\n    \"\"\"The number of steps executed by the agent\"\"\"\n    steps: list[Step] = []\n    \"\"\"The steps generated by the agent\"\"\"\n    cost: float | None = None\n    \"\"\"The (known) cost incurred by the run, e.g. from using paid LLM APIs\"\"\"\n\n    @validator(\"fail_reason\")\n    def success_xor_fail_reason(cls, v: str | None, values: dict[str, Any]):\n        if bool(v) == bool(values[\"success\"]):\n            logger.error(\n                \"Error validating `success ^ fail_reason` on TestResult: \"\n                f\"success = {repr(values['success'])}; \"\n                f\"fail_reason = {repr(v)}\"\n            )\n        if v:\n            success = values[\"success\"]\n            assert not success, \"fail_reason must only be specified if success=False\"\n        else:\n            assert values[\"success\"], \"fail_reason is required if success=False\"\n        return v\n\n\nclass TestMetrics(BaseModel):\n    \"\"\"\n    Result metrics for a set of runs for a test/challenge. Should be an aggregate of all\n    results for the same test/challenge within a benchmarking session.\n    \"\"\"\n\n    attempted: bool\n    \"\"\"Whether the challenge was attempted during this session\"\"\"\n    is_regression: bool\n    \"\"\"Whether the challenge was considered a regression test at the time of running\"\"\"\n    success_percentage: float | None = Field(default=None, alias=\"success_%\")\n    \"\"\"Success rate (0-100) for this challenge within the session\"\"\"\n\n\nclass MetricsOverall(BaseModel):\n    \"\"\"Global metrics concerning a benchmarking session\"\"\"\n\n    run_time: str\n    \"\"\"Duration from beginning to end of the session\"\"\"\n    highest_difficulty: str\n    \"\"\"\n    Difficulty of the most difficult challenge that succeeded at least once this session\n    \"\"\"\n    total_cost: float | None = None\n    \"\"\"Total known cost of the session\"\"\"\n\n\nclass Test(BaseModel):\n    category: List[str]\n    difficulty: str | None\n    data_path: str\n    description: str\n    task: str\n    answer: str\n    metrics: TestMetrics\n    results: list[TestResult]\n    metadata: dict[str, Any] | None = Field(default_factory=dict)\n\n\nclass ReportBase(BaseModel):\n    command: str\n    completion_time: str | None = None\n    benchmark_start_time: Annotated[str, constr(regex=datetime_format)]\n    metrics: MetricsOverall\n    config: Dict[str, str | dict[str, str]]\n    agent_git_commit_sha: str | None = None\n    benchmark_git_commit_sha: str | None = None\n    repo_url: str | None = None\n\n\nclass Report(ReportBase):\n    tests: Dict[str, Test]\n", "benchmark/agbenchmark/reports/processing/process_report.py": "import json\nimport logging\nimport os\nfrom pathlib import Path\nfrom typing import Any\n\nfrom agbenchmark.reports.processing.get_files import (\n    get_latest_report_from_agent_directories,\n)\nfrom agbenchmark.reports.processing.report_types import Report\nfrom agbenchmark.utils.data_types import STRING_DIFFICULTY_MAP\n\nlogger = logging.getLogger(__name__)\n\n\ndef get_reports_data(report_path: str) -> dict[str, Any]:\n    latest_files = get_latest_report_from_agent_directories(report_path)\n\n    reports_data = {}\n\n    if latest_files is None:\n        raise Exception(\"No files found in the reports directory\")\n\n    # This will print the latest file in each subdirectory and add to the files_data dictionary\n    for subdir, file in latest_files:\n        subdir_name = os.path.basename(os.path.normpath(subdir))\n        with open(Path(subdir) / file, \"r\") as f:\n            # Load the JSON data from the file\n            json_data = json.load(f)\n            converted_data = Report.parse_obj(json_data)\n            # get the last directory name in the path as key\n            reports_data[subdir_name] = converted_data\n\n    return reports_data\n\n\ndef get_highest_achieved_difficulty_per_category(report: Report) -> dict[str, Any]:\n    categories: dict[str, Any] = {}\n\n    for _, test_data in report.tests.items():\n        for category in test_data.category:\n            if category in (\"interface\", \"iterate\", \"product_advisor\"):\n                continue\n            categories.setdefault(category, 0)\n            if (\n                test_data.results\n                and all(r.success for r in test_data.results)\n                and test_data.difficulty\n            ):\n                num_dif = STRING_DIFFICULTY_MAP[test_data.difficulty]\n                if num_dif > categories[category]:\n                    categories[category] = num_dif\n\n    return categories\n\n\ndef all_agent_categories(reports_data: dict[str, Any]) -> dict[str, Any]:\n    all_categories: dict[str, Any] = {}\n\n    for name, report in reports_data.items():\n        categories = get_highest_achieved_difficulty_per_category(report)\n        if categories:  # only add to all_categories if categories is not empty\n            logger.debug(f\"Adding {name}: {categories}\")\n            all_categories[name] = categories\n\n    return all_categories\n", "benchmark/agbenchmark/reports/processing/get_files.py": "import os\n\n\ndef get_last_subdirectory(directory_path: str) -> str | None:\n    # Get all subdirectories in the directory\n    subdirs = [\n        os.path.join(directory_path, name)\n        for name in os.listdir(directory_path)\n        if os.path.isdir(os.path.join(directory_path, name))\n    ]\n\n    # Sort the subdirectories by creation time\n    subdirs.sort(key=os.path.getctime)\n\n    # Return the last subdirectory in the list\n    return subdirs[-1] if subdirs else None\n\n\ndef get_latest_report_from_agent_directories(\n    directory_path: str,\n) -> list[tuple[os.DirEntry[str], str]]:\n    latest_reports = []\n\n    for subdir in os.scandir(directory_path):\n        if subdir.is_dir():\n            # Get the most recently created subdirectory within this agent's directory\n            latest_subdir = get_last_subdirectory(subdir.path)\n            if latest_subdir is not None:\n                # Look for 'report.json' in the subdirectory\n                report_file = os.path.join(latest_subdir, \"report.json\")\n                if os.path.isfile(report_file):\n                    latest_reports.append((subdir, report_file))\n\n    return latest_reports\n", "benchmark/agbenchmark/reports/processing/gen_combined_chart.py": "import json\nimport os\nfrom pathlib import Path\n\nfrom agbenchmark.reports.processing.graphs import (\n    save_combined_bar_chart,\n    save_combined_radar_chart,\n)\nfrom agbenchmark.reports.processing.process_report import (\n    all_agent_categories,\n    get_reports_data,\n)\n\n\ndef generate_combined_chart() -> None:\n    all_agents_path = Path(__file__).parent.parent.parent.parent / \"reports\"\n\n    combined_charts_folder = all_agents_path / \"combined_charts\"\n\n    reports_data = get_reports_data(str(all_agents_path))\n\n    categories = all_agent_categories(reports_data)\n\n    # Count the number of directories in this directory\n    num_dirs = len([f for f in combined_charts_folder.iterdir() if f.is_dir()])\n\n    run_charts_folder = combined_charts_folder / f\"run{num_dirs + 1}\"\n\n    if not os.path.exists(run_charts_folder):\n        os.makedirs(run_charts_folder)\n\n    info_data = {\n        report_name: data.benchmark_start_time\n        for report_name, data in reports_data.items()\n        if report_name in categories\n    }\n    with open(Path(run_charts_folder) / \"run_info.json\", \"w\") as f:\n        json.dump(info_data, f)\n\n    save_combined_radar_chart(categories, Path(run_charts_folder) / \"radar_chart.png\")\n    save_combined_bar_chart(categories, Path(run_charts_folder) / \"bar_chart.png\")\n\n\nif __name__ == \"__main__\":\n    generate_combined_chart()\n", "benchmark/tests/test_benchmark_workflow.py": "import datetime\nimport time\n\nimport pytest\nimport requests\n\nURL_BENCHMARK = \"http://localhost:8080/ap/v1\"\nURL_AGENT = \"http://localhost:8000/ap/v1\"\n\ntry:\n    response = requests.get(f\"{URL_AGENT}/agent/tasks\")\nexcept requests.exceptions.ConnectionError:\n    pytest.skip(\"No agent available to test against\", allow_module_level=True)\n\n\n@pytest.mark.parametrize(\n    \"eval_id, input_text, expected_artifact_length, test_name, should_be_successful\",\n    [\n        (\n            \"021c695a-6cc4-46c2-b93a-f3a9b0f4d123\",\n            \"Write the word 'Washington' to a .txt file\",\n            0,\n            \"WriteFile\",\n            True,\n        ),\n        (\n            \"f219f3d3-a41b-45a9-a3d0-389832086ee8\",\n            \"Read the file called file_to_read.txt \"\n            \"and write its content to a file called output.txt\",\n            1,\n            \"ReadFile\",\n            False,\n        ),\n    ],\n)\ndef test_entire_workflow(\n    eval_id: str,\n    input_text: str,\n    expected_artifact_length: int,\n    test_name: str,\n    should_be_successful: bool,\n):\n    task_request = {\"eval_id\": eval_id, \"input\": input_text}\n    response = requests.get(f\"{URL_AGENT}/agent/tasks\")\n    task_count_before = response.json()[\"pagination\"][\"total_items\"]\n    # First POST request\n    task_response_benchmark = requests.post(\n        URL_BENCHMARK + \"/agent/tasks\", json=task_request\n    )\n    response = requests.get(f\"{URL_AGENT}/agent/tasks\")\n    task_count_after = response.json()[\"pagination\"][\"total_items\"]\n    assert task_count_after == task_count_before + 1\n\n    timestamp_after_task_eval_created = datetime.datetime.now(datetime.timezone.utc)\n    time.sleep(1.1)  # To make sure the 2 timestamps to compare are different\n    assert task_response_benchmark.status_code == 200\n    task_response_benchmark = task_response_benchmark.json()\n    assert task_response_benchmark[\"input\"] == input_text\n\n    task_response_benchmark_id = task_response_benchmark[\"task_id\"]\n\n    response_task_agent = requests.get(\n        f\"{URL_AGENT}/agent/tasks/{task_response_benchmark_id}\"\n    )\n    assert response_task_agent.status_code == 200\n    response_task_agent = response_task_agent.json()\n    assert len(response_task_agent[\"artifacts\"]) == expected_artifact_length\n\n    step_request = {\"input\": input_text}\n\n    step_response = requests.post(\n        URL_BENCHMARK + \"/agent/tasks/\" + task_response_benchmark_id + \"/steps\",\n        json=step_request,\n    )\n    assert step_response.status_code == 200\n    step_response = step_response.json()\n    assert step_response[\"is_last\"] is True  # Assuming is_last is always True\n\n    eval_response = requests.post(\n        URL_BENCHMARK + \"/agent/tasks/\" + task_response_benchmark_id + \"/evaluations\",\n        json={},\n    )\n    assert eval_response.status_code == 200\n    eval_response = eval_response.json()\n    print(\"eval_response\")\n    print(eval_response)\n    assert eval_response[\"run_details\"][\"test_name\"] == test_name\n    assert eval_response[\"metrics\"][\"success\"] == should_be_successful\n    benchmark_start_time = datetime.datetime.fromisoformat(\n        eval_response[\"run_details\"][\"benchmark_start_time\"]\n    )\n\n    assert benchmark_start_time < timestamp_after_task_eval_created\n", "benchmark/tests/test_extract_subgraph.py": "import pytest\n\nfrom agbenchmark.utils.dependencies.graphs import extract_subgraph_based_on_category\n\n\n@pytest.fixture\ndef curriculum_graph():\n    return {\n        \"edges\": [\n            {\"from\": \"Calculus\", \"to\": \"Advanced Calculus\"},\n            {\"from\": \"Algebra\", \"to\": \"Calculus\"},\n            {\"from\": \"Biology\", \"to\": \"Advanced Biology\"},\n            {\"from\": \"World History\", \"to\": \"Modern History\"},\n        ],\n        \"nodes\": [\n            {\"data\": {\"category\": [\"math\"]}, \"id\": \"Calculus\", \"label\": \"Calculus\"},\n            {\n                \"data\": {\"category\": [\"math\"]},\n                \"id\": \"Advanced Calculus\",\n                \"label\": \"Advanced Calculus\",\n            },\n            {\"data\": {\"category\": [\"math\"]}, \"id\": \"Algebra\", \"label\": \"Algebra\"},\n            {\"data\": {\"category\": [\"science\"]}, \"id\": \"Biology\", \"label\": \"Biology\"},\n            {\n                \"data\": {\"category\": [\"science\"]},\n                \"id\": \"Advanced Biology\",\n                \"label\": \"Advanced Biology\",\n            },\n            {\n                \"data\": {\"category\": [\"history\"]},\n                \"id\": \"World History\",\n                \"label\": \"World History\",\n            },\n            {\n                \"data\": {\"category\": [\"history\"]},\n                \"id\": \"Modern History\",\n                \"label\": \"Modern History\",\n            },\n        ],\n    }\n\n\ngraph_example = {\n    \"nodes\": [\n        {\"id\": \"A\", \"data\": {\"category\": []}},\n        {\"id\": \"B\", \"data\": {\"category\": []}},\n        {\"id\": \"C\", \"data\": {\"category\": [\"math\"]}},\n    ],\n    \"edges\": [{\"from\": \"B\", \"to\": \"C\"}, {\"from\": \"A\", \"to\": \"C\"}],\n}\n\n\ndef test_dfs_category_math(curriculum_graph):\n    result_graph = extract_subgraph_based_on_category(curriculum_graph, \"math\")\n\n    # Expected nodes: Algebra, Calculus, Advanced Calculus\n    # Expected edges: Algebra->Calculus, Calculus->Advanced Calculus\n\n    expected_nodes = [\"Algebra\", \"Calculus\", \"Advanced Calculus\"]\n    expected_edges = [\n        {\"from\": \"Algebra\", \"to\": \"Calculus\"},\n        {\"from\": \"Calculus\", \"to\": \"Advanced Calculus\"},\n    ]\n\n    assert set(node[\"id\"] for node in result_graph[\"nodes\"]) == set(expected_nodes)\n    assert set((edge[\"from\"], edge[\"to\"]) for edge in result_graph[\"edges\"]) == set(\n        (edge[\"from\"], edge[\"to\"]) for edge in expected_edges\n    )\n\n\ndef test_extract_subgraph_math_category():\n    subgraph = extract_subgraph_based_on_category(graph_example, \"math\")\n    assert set(\n        (node[\"id\"], tuple(node[\"data\"][\"category\"])) for node in subgraph[\"nodes\"]\n    ) == set(\n        (node[\"id\"], tuple(node[\"data\"][\"category\"])) for node in graph_example[\"nodes\"]\n    )\n    assert set((edge[\"from\"], edge[\"to\"]) for edge in subgraph[\"edges\"]) == set(\n        (edge[\"from\"], edge[\"to\"]) for edge in graph_example[\"edges\"]\n    )\n\n\ndef test_extract_subgraph_non_existent_category():\n    result_graph = extract_subgraph_based_on_category(graph_example, \"toto\")\n\n    # Asserting that the result graph has no nodes and no edges\n    assert len(result_graph[\"nodes\"]) == 0\n    assert len(result_graph[\"edges\"]) == 0\n", "benchmark/tests/test_is_circular.py": "from agbenchmark.utils.dependencies.graphs import is_circular\n\n\ndef test_is_circular():\n    cyclic_graph = {\n        \"nodes\": [\n            {\"id\": \"A\", \"data\": {\"category\": []}},\n            {\"id\": \"B\", \"data\": {\"category\": []}},\n            {\"id\": \"C\", \"data\": {\"category\": []}},\n            {\"id\": \"D\", \"data\": {\"category\": []}},  # New node\n        ],\n        \"edges\": [\n            {\"from\": \"A\", \"to\": \"B\"},\n            {\"from\": \"B\", \"to\": \"C\"},\n            {\"from\": \"C\", \"to\": \"D\"},\n            {\"from\": \"D\", \"to\": \"A\"},  # This edge creates a cycle\n        ],\n    }\n\n    result = is_circular(cyclic_graph)\n    assert result is not None, \"Expected a cycle, but none was detected\"\n    assert all(\n        (\n            (result[i], result[i + 1])\n            in [(x[\"from\"], x[\"to\"]) for x in cyclic_graph[\"edges\"]]\n        )\n        for i in range(len(result) - 1)\n    ), \"The detected cycle path is not part of the graph's edges\"\n\n\ndef test_is_not_circular():\n    acyclic_graph = {\n        \"nodes\": [\n            {\"id\": \"A\", \"data\": {\"category\": []}},\n            {\"id\": \"B\", \"data\": {\"category\": []}},\n            {\"id\": \"C\", \"data\": {\"category\": []}},\n            {\"id\": \"D\", \"data\": {\"category\": []}},  # New node\n        ],\n        \"edges\": [\n            {\"from\": \"A\", \"to\": \"B\"},\n            {\"from\": \"B\", \"to\": \"C\"},\n            {\"from\": \"C\", \"to\": \"D\"},\n            # No back edge from D to any node, so it remains acyclic\n        ],\n    }\n\n    assert is_circular(acyclic_graph) is None, \"Detected a cycle in an acyclic graph\"\n", "benchmark/tests/__init__.py": "", "benchmark/tests/test_get_roots.py": "from agbenchmark.utils.dependencies.graphs import get_roots\n\n\ndef test_get_roots():\n    graph = {\n        \"nodes\": [\n            {\"id\": \"A\", \"data\": {\"category\": []}},\n            {\"id\": \"B\", \"data\": {\"category\": []}},\n            {\"id\": \"C\", \"data\": {\"category\": []}},\n            {\"id\": \"D\", \"data\": {\"category\": []}},\n        ],\n        \"edges\": [\n            {\"from\": \"A\", \"to\": \"B\"},\n            {\"from\": \"B\", \"to\": \"C\"},\n        ],\n    }\n\n    result = get_roots(graph)\n    assert set(result) == {\n        \"A\",\n        \"D\",\n    }, f\"Expected roots to be 'A' and 'D', but got {result}\"\n\n\ndef test_no_roots():\n    fully_connected_graph = {\n        \"nodes\": [\n            {\"id\": \"A\", \"data\": {\"category\": []}},\n            {\"id\": \"B\", \"data\": {\"category\": []}},\n            {\"id\": \"C\", \"data\": {\"category\": []}},\n        ],\n        \"edges\": [\n            {\"from\": \"A\", \"to\": \"B\"},\n            {\"from\": \"B\", \"to\": \"C\"},\n            {\"from\": \"C\", \"to\": \"A\"},\n        ],\n    }\n\n    result = get_roots(fully_connected_graph)\n    assert not result, \"Expected no roots, but found some\"\n\n\n# def test_no_rcoots():\n#     fully_connected_graph = {\n#         \"nodes\": [\n#             {\"id\": \"A\", \"data\": {\"category\": []}},\n#             {\"id\": \"B\", \"data\": {\"category\": []}},\n#             {\"id\": \"C\", \"data\": {\"category\": []}},\n#         ],\n#         \"edges\": [\n#             {\"from\": \"A\", \"to\": \"B\"},\n#             {\"from\": \"D\", \"to\": \"C\"},\n#         ],\n#     }\n#\n#     result = get_roots(fully_connected_graph)\n#     assert set(result) == {\"A\"}, f\"Expected roots to be 'A', but got {result}\"\n", "benchmark/backend/__init__.py": "", "autogpt/setup.py": "from pkgutil import iter_modules\nfrom shutil import which\n\nfrom cx_Freeze import Executable, setup\n\npackages = [\n    m.name\n    for m in iter_modules()\n    if m.ispkg\n    and m.module_finder\n    and (\"poetry\" in m.module_finder.path)  # type: ignore\n]\n\nicon = (\n    \"../../assets/gpt_dark_RGB.icns\"\n    if which(\"sips\")\n    else \"../../assets/gpt_dark_RGB.ico\"\n)\n\n\nsetup(\n    executables=[\n        Executable(\n            \"autogpt/__main__.py\", target_name=\"autogpt\", base=\"console\", icon=icon\n        ),\n    ],\n    options={\n        \"build_exe\": {\n            \"packages\": packages,\n            \"includes\": [\n                \"autogpt\",\n                \"spacy\",\n                \"spacy.lang\",\n                \"spacy.vocab\",\n                \"spacy.lang.lex_attrs\",\n                \"uvicorn.loops.auto\",\n                \"srsly.msgpack.util\",\n                \"blis\",\n                \"uvicorn.protocols.http.auto\",\n                \"uvicorn.protocols.websockets.auto\",\n                \"uvicorn.lifespan.on\",\n            ],\n            \"excludes\": [\"readability.compat.two\"],\n        },\n        \"bdist_mac\": {\n            \"bundle_name\": \"AutoGPT\",\n            \"iconfile\": \"../assets/gpt_dark_RGB.icns\",\n            \"include_resources\": [\"\"],\n        },\n        \"bdist_dmg\": {\n            \"applications_shortcut\": True,\n            \"volume_label\": \"AutoGPT\",\n        },\n        \"bdist_msi\": {\n            \"target_name\": \"AutoGPT\",\n            \"add_to_path\": True,\n            \"install_icon\": \"../assets/gpt_dark_RGB.ico\",\n        },\n    },\n)\n", "autogpt/agbenchmark_config/analyze_reports.py": "#!/usr/bin/env python3\n\nimport json\nimport logging\nimport re\nimport sys\nfrom collections import defaultdict\nfrom pathlib import Path\n\nfrom tabulate import tabulate\n\ninfo = \"-v\" in sys.argv\ndebug = \"-vv\" in sys.argv\ngranular = \"--granular\" in sys.argv\n\nlogging.basicConfig(\n    level=logging.DEBUG if debug else logging.INFO if info else logging.WARNING\n)\nlogger = logging.getLogger(__name__)\n\n# Get a list of all JSON files in the directory\nreport_files = [\n    report_file\n    for dir in (Path(__file__).parent / \"reports\").iterdir()\n    if re.match(r\"^\\d{8}T\\d{6}_\", dir.name)\n    and (report_file := dir / \"report.json\").is_file()\n]\n\nlabels = list[str]()\nruns_per_label = defaultdict[str, int](lambda: 0)\nsuite_names = list[str]()\ntest_names = list[str]()\n\n# Create a dictionary to store grouped success values by suffix and test\ngrouped_success_values = defaultdict[str, list[str]](list[str])\n\n# Loop through each JSON file to collect suffixes and success values\nfor report_file in sorted(report_files):\n    with open(report_file) as f:\n        logger.info(f\"Loading {report_file}...\")\n\n        data = json.load(f)\n        if \"tests\" in data:\n            test_tree = data[\"tests\"]\n            label = data[\"agent_git_commit_sha\"].rsplit(\"/\", 1)[1][:7]  # commit hash\n        else:\n            # Benchmark run still in progress\n            test_tree = data\n            label = report_file.parent.name.split(\"_\", 1)[1]\n            logger.info(f\"Run '{label}' seems to be in progress\")\n\n        runs_per_label[label] += 1\n\n        def process_test(test_name: str, test_data: dict):\n            result_group = grouped_success_values[f\"{label}|{test_name}\"]\n\n            if \"tests\" in test_data:\n                logger.debug(f\"{test_name} is a test suite\")\n\n                # Test suite\n                suite_attempted = any(\n                    test[\"metrics\"][\"attempted\"] for test in test_data[\"tests\"].values()\n                )\n                logger.debug(f\"suite_attempted: {suite_attempted}\")\n                if not suite_attempted:\n                    return\n\n                if test_name not in test_names:\n                    test_names.append(test_name)\n\n                if test_data[\"metrics\"][\"percentage\"] == 0:\n                    result_indicator = \"\u274c\"\n                else:\n                    highest_difficulty = test_data[\"metrics\"][\"highest_difficulty\"]\n                    result_indicator = {\n                        \"interface\": \"\ud83d\udd0c\",\n                        \"novice\": \"\ud83c\udf11\",\n                        \"basic\": \"\ud83c\udf12\",\n                        \"intermediate\": \"\ud83c\udf13\",\n                        \"advanced\": \"\ud83c\udf14\",\n                        \"hard\": \"\ud83c\udf15\",\n                    }[highest_difficulty]\n\n                logger.debug(f\"result group: {result_group}\")\n                logger.debug(f\"runs_per_label: {runs_per_label[label]}\")\n                if len(result_group) + 1 < runs_per_label[label]:\n                    result_group.extend(\n                        [\"\u2754\"] * (runs_per_label[label] - len(result_group) - 1)\n                    )\n                result_group.append(result_indicator)\n                logger.debug(f\"result group (after): {result_group}\")\n\n                if granular:\n                    for test_name, test in test_data[\"tests\"].items():\n                        process_test(test_name, test)\n                return\n\n            test_metrics = test_data[\"metrics\"]\n            result_indicator = \"\u2754\"\n\n            if \"attempted\" not in test_metrics:\n                return\n            elif test_metrics[\"attempted\"]:\n                if test_name not in test_names:\n                    test_names.append(test_name)\n\n                success_value = test_metrics[\"success\"]\n                result_indicator = {True: \"\u2705\", False: \"\u274c\"}[success_value]\n\n            if len(result_group) + 1 < runs_per_label[label]:\n                result_group.extend(\n                    [\"  \"] * (runs_per_label[label] - len(result_group) - 1)\n                )\n            result_group.append(result_indicator)\n\n        for test_name, suite in test_tree.items():\n            try:\n                process_test(test_name, suite)\n            except KeyError:\n                print(f\"{test_name}.metrics: {suite['metrics']}\")\n                raise\n\n    if label not in labels:\n        labels.append(label)\n\n# Create headers\nheaders = [\"Test Name\"] + list(labels)\n\n# Prepare data for tabulation\ntable_data = list[list[str]]()\nfor test_name in test_names:\n    row = [test_name]\n    for label in labels:\n        results = grouped_success_values.get(f\"{label}|{test_name}\", [\"\u2754\"])\n        if len(results) < runs_per_label[label]:\n            results.extend([\"\u2754\"] * (runs_per_label[label] - len(results)))\n        if len(results) > 1 and all(r == \"\u2754\" for r in results):\n            results.clear()\n        row.append(\" \".join(results))\n    table_data.append(row)\n\n# Print tabulated data\nprint(tabulate(table_data, headers=headers, tablefmt=\"grid\"))\n", "autogpt/agbenchmark_config/__init__.py": "", "autogpt/scripts/check_requirements.py": "import contextlib\nimport os\nimport sys\nfrom importlib.metadata import version\n\ntry:\n    import poetry.factory  # type: ignore # noqa\nexcept ModuleNotFoundError:\n    os.system(f\"{sys.executable} -m pip install 'poetry>=1.6.1,<2.0.0'\")\n\nfrom poetry.core.constraints.version.version import Version  # type: ignore\nfrom poetry.factory import Factory  # type: ignore\n\n\ndef main():\n    poetry_project = Factory().create_poetry()\n    dependency_group = poetry_project.package.dependency_group(\"main\")\n\n    missing_packages = []\n    for dep in dependency_group.dependencies:\n        if dep.is_optional():\n            continue\n        # Try to verify that the installed version is suitable\n        with contextlib.suppress(ModuleNotFoundError):\n            installed_version = version(dep.name)  # if this fails -> not installed\n            if dep.constraint.allows(Version.parse(installed_version)):\n                continue\n        # If the above verification fails, mark the package as missing\n        missing_packages.append(str(dep))\n\n    if missing_packages:\n        print(\"Missing packages:\")\n        print(\", \".join(missing_packages))\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    main()\n", "autogpt/scripts/git_log_to_release_notes.py": "#!/usr/bin/env python3\n\nimport logging\nfrom pathlib import Path\nfrom typing import Optional\n\nimport click\nfrom forge.llm.providers import ChatMessage, MultiProvider\nfrom forge.llm.providers.anthropic import AnthropicModelName\nfrom git import Repo, TagReference\n\nfrom autogpt.app.utils import coroutine\n\n\n@click.command()\n@click.option(\n    \"--repo-path\",\n    type=click.Path(file_okay=False, exists=True),\n    help=\"Path to the git repository\",\n)\n@coroutine\nasync def generate_release_notes(repo_path: Optional[Path] = None):\n    logger = logging.getLogger(generate_release_notes.name)  # pyright: ignore\n\n    repo = Repo(repo_path, search_parent_directories=True)\n    tags = list(repo.tags)\n    if not tags:\n        click.echo(\"No tags found in the repository.\")\n        return\n\n    click.echo(\"Available tags:\")\n    for index, tag in enumerate(tags):\n        click.echo(f\"{index + 1}: {tag.name}\")\n\n    last_release_index = (\n        click.prompt(\"Enter the number for the last release tag\", type=int) - 1\n    )\n    if last_release_index >= len(tags) or last_release_index < 0:\n        click.echo(\"Invalid tag number entered.\")\n        return\n    last_release_tag: TagReference = tags[last_release_index]\n\n    new_release_ref = click.prompt(\n        \"Enter the name of the release branch or git ref\",\n        default=repo.active_branch.name,\n    )\n    try:\n        new_release_ref = repo.heads[new_release_ref].name\n    except IndexError:\n        try:\n            new_release_ref = repo.tags[new_release_ref].name\n        except IndexError:\n            new_release_ref = repo.commit(new_release_ref).hexsha\n    logger.debug(f\"Selected release ref: {new_release_ref}\")\n\n    git_log = repo.git.log(\n        f\"{last_release_tag.name}...{new_release_ref}\",\n        \"autogpt/\",\n        no_merges=True,\n        follow=True,\n    )\n    logger.debug(f\"-------------- GIT LOG --------------\\n\\n{git_log}\\n\")\n\n    model_provider = MultiProvider()\n    chat_messages = [\n        ChatMessage.system(SYSTEM_PROMPT),\n        ChatMessage.user(content=git_log),\n    ]\n    click.echo(\"Writing release notes ...\")\n    completion = await model_provider.create_chat_completion(\n        model_prompt=chat_messages,\n        model_name=AnthropicModelName.CLAUDE3_OPUS_v1,\n        # model_name=OpenAIModelName.GPT4_v4,\n    )\n\n    click.echo(\"-------------- LLM RESPONSE --------------\\n\")\n    click.echo(completion.response.content)\n\n\nEXAMPLE_RELEASE_NOTES = \"\"\"\nFirst some important notes w.r.t. using the application:\n* `run.sh` has been renamed to `autogpt.sh`\n* The project has been restructured. The AutoGPT Agent is now located in `autogpt`.\n* The application no longer uses a single workspace for all tasks. Instead, every task that you run the agent on creates a new workspace folder. See the [usage guide](https://docs.agpt.co/autogpt/usage/#workspace) for more information.\n\n## New features \u2728\n\n* **Agent Protocol \ud83d\udd0c**\n  Our agent now works with the [Agent Protocol](/#-agent-protocol), a REST API that allows creating tasks and executing the agent's step-by-step process. This allows integration with other applications, and we also use it to connect to the agent through the UI.\n* **UI \ud83d\udcbb**\n  With the aforementioned Agent Protocol integration comes the benefit of using our own open-source Agent UI. Easily create, use, and chat with multiple agents from one interface.\n  When starting the application through the project's new [CLI](/#-cli), it runs with the new frontend by default, with benchmarking capabilities. Running `autogpt.sh serve` in the subproject folder (`autogpt`) will also serve the new frontend, but without benchmarking functionality.\n  Running the application the \"old-fashioned\" way, with the terminal interface (let's call it TTY mode), is still possible with `autogpt.sh run`.\n* **Resuming agents \ud83d\udd04\ufe0f**\n  In TTY mode, the application will now save the agent's state when quitting, and allows resuming where you left off at a later time!\n* **GCS and S3 workspace backends \ud83d\udce6**\n  To further support running the application as part of a larger system, Google Cloud Storage and S3 workspace backends were added. Configuration options for this can be found in [`.env.template`](/autogpt/.env.template).\n* **Documentation Rewrite \ud83d\udcd6**\n  The [documentation](https://docs.agpt.co) has been restructured and mostly rewritten to clarify and simplify the instructions, and also to accommodate the other subprojects that are now in the repo.\n* **New Project CLI \ud83d\udd27**\n  The project has a new CLI to provide easier usage of all of the components that are now in the repo: different agents, frontend and benchmark. More info can be found [here](/#-cli).\n* **Docker dev build \ud83d\udc33**\n  In addition to the regular Docker release [images](https://hub.docker.com/r/significantgravitas/auto-gpt/tags) (`latest`, `v0.5.0` in this case), we now also publish a `latest-dev` image that always contains the latest working build from `master`. This allows you to try out the latest bleeding edge version, but be aware that these builds may contain bugs!\n\n## Architecture changes & improvements \ud83d\udc77\ud83c\udffc\n* **PromptStrategy**\n  To make it easier to harness the power of LLMs and use them to fulfil tasks within the application, we adopted the `PromptStrategy` class from `autogpt.core` (AKA re-arch) to encapsulate prompt generation and response parsing throughout the application.\n* **Config modularization**\n  To reduce the complexity of the application's config structure, parts of the monolithic `Config` have been moved into smaller, tightly scoped config objects. Also, the logic for building the configuration from environment variables was decentralized to make it all a lot more maintainable.\n  This is mostly made possible by the `autogpt.core.configuration` module, which was also expanded with a few new features for it. Most notably, the new `from_env` attribute on the `UserConfigurable` field decorator and corresponding logic in `SystemConfiguration.from_env()` and related functions.\n* **Monorepo**\n  As mentioned, the repo has been restructured to accommodate the AutoGPT Agent, Forge, AGBenchmark and the new Frontend.\n  * AutoGPT Agent has been moved to `autogpt`\n  * Forge now lives in `forge`, and the project's new CLI makes it easy to create new Forge-based agents.\n  * AGBenchmark -> `benchmark`\n  * Frontend -> `frontend`\n\n  See also the [README](/#readme).\n\"\"\".lstrip()  # noqa\n\n\nSYSTEM_PROMPT = f\"\"\"\nPlease generate release notes based on the user's git log and the example release notes.\n\nHere is an example of what we like our release notes to look and read like:\n---------------------------------------------------------------------------\n{EXAMPLE_RELEASE_NOTES}\n---------------------------------------------------------------------------\nNOTE: These example release notes are not related to the git log that you should write release notes for!\nDo not mention the changes in the example when writing your release notes!\n\"\"\".lstrip()  # noqa\n\nif __name__ == \"__main__\":\n    import dotenv\n    from forge.logging.config import configure_logging\n\n    configure_logging(debug=True)\n\n    dotenv.load_dotenv()\n    generate_release_notes()\n", "autogpt/scripts/__init__.py": "", "autogpt/tests/utils.py": "import os\n\nimport pytest\n\n\ndef skip_in_ci(test_function):\n    return pytest.mark.skipif(\n        os.environ.get(\"CI\") == \"true\",\n        reason=\"This test doesn't work on GitHub Actions.\",\n    )(test_function)\n", "autogpt/tests/conftest.py": "from __future__ import annotations\n\nimport os\nimport uuid\nfrom pathlib import Path\n\nimport pytest\nfrom forge.config.ai_profile import AIProfile\nfrom forge.file_storage.local import (\n    FileStorage,\n    FileStorageConfiguration,\n    LocalFileStorage,\n)\nfrom forge.llm.providers import MultiProvider\nfrom forge.logging.config import configure_logging\n\nfrom autogpt.agents.agent import Agent, AgentConfiguration, AgentSettings\nfrom autogpt.app.config import AppConfig, ConfigBuilder\nfrom autogpt.app.main import _configure_llm_provider\n\npytest_plugins = [\n    \"tests.integration.agent_factory\",\n    \"tests.vcr\",\n]\n\n\n@pytest.fixture()\ndef tmp_project_root(tmp_path: Path) -> Path:\n    return tmp_path\n\n\n@pytest.fixture()\ndef app_data_dir(tmp_project_root: Path) -> Path:\n    dir = tmp_project_root / \"data\"\n    dir.mkdir(parents=True, exist_ok=True)\n    return dir\n\n\n@pytest.fixture()\ndef storage(app_data_dir: Path) -> FileStorage:\n    storage = LocalFileStorage(\n        FileStorageConfiguration(root=app_data_dir, restrict_to_root=False)\n    )\n    storage.initialize()\n    return storage\n\n\n@pytest.fixture(scope=\"function\")\ndef config(\n    tmp_project_root: Path,\n    app_data_dir: Path,\n):\n    if not os.environ.get(\"OPENAI_API_KEY\"):\n        os.environ[\"OPENAI_API_KEY\"] = \"sk-dummy\"\n    config = ConfigBuilder.build_config_from_env(project_root=tmp_project_root)\n\n    config.app_data_dir = app_data_dir\n\n    config.noninteractive_mode = True\n\n    yield config\n\n\n@pytest.fixture(scope=\"session\")\ndef setup_logger():\n    configure_logging(\n        debug=True,\n        log_dir=Path(__file__).parent / \"logs\",\n        plain_console_output=True,\n    )\n\n\n@pytest.fixture\ndef llm_provider(config: AppConfig) -> MultiProvider:\n    return _configure_llm_provider(config)\n\n\n@pytest.fixture\ndef agent(\n    config: AppConfig, llm_provider: MultiProvider, storage: FileStorage\n) -> Agent:\n    ai_profile = AIProfile(\n        ai_name=\"Base\",\n        ai_role=\"A base AI\",\n        ai_goals=[],\n    )\n\n    agent_settings = AgentSettings(\n        name=Agent.default_settings.name,\n        description=Agent.default_settings.description,\n        agent_id=f\"AutoGPT-test-agent-{str(uuid.uuid4())[:8]}\",\n        ai_profile=ai_profile,\n        config=AgentConfiguration(\n            fast_llm=config.fast_llm,\n            smart_llm=config.smart_llm,\n            allow_fs_access=not config.restrict_to_workspace,\n            use_functions_api=config.openai_functions,\n        ),\n        history=Agent.default_settings.history.copy(deep=True),\n    )\n\n    agent = Agent(\n        settings=agent_settings,\n        llm_provider=llm_provider,\n        file_storage=storage,\n        app_config=config,\n    )\n    return agent\n", "autogpt/tests/__init__.py": "", "autogpt/tests/context.py": "import os\nimport sys\n\n# Add the scripts directory to the path so that we can import the browse module.\nsys.path.insert(\n    0, os.path.abspath(os.path.join(os.path.dirname(__file__), \"../scripts\"))\n)\n", "autogpt/tests/vcr/vcr_filter.py": "import contextlib\nimport json\nimport re\nfrom io import BytesIO\nfrom typing import Any\n\nfrom vcr.request import Request\n\nHOSTNAMES_TO_CACHE: list[str] = [\n    \"api.openai.com\",\n    \"localhost:50337\",\n    \"duckduckgo.com\",\n]\n\nIGNORE_REQUEST_HEADERS: set[str | re.Pattern] = {\n    \"Authorization\",\n    \"Cookie\",\n    \"OpenAI-Organization\",\n    \"X-OpenAI-Client-User-Agent\",\n    \"User-Agent\",\n    re.compile(r\"X-Stainless-[\\w\\-]+\", re.IGNORECASE),\n}\n\nLLM_MESSAGE_REPLACEMENTS: list[dict[str, str]] = [\n    {\n        \"regex\": r\"\\w{3} \\w{3} {1,2}\\d{1,2} \\d{2}:\\d{2}:\\d{2} \\d{4}\",\n        \"replacement\": \"Tue Jan  1 00:00:00 2000\",\n    },\n    {\n        \"regex\": r\"<selenium.webdriver.chrome.webdriver.WebDriver[^>]*>\",\n        \"replacement\": \"\",\n    },\n]\n\nOPENAI_URL = \"api.openai.com\"\n\n\ndef before_record_request(request: Request) -> Request | None:\n    if not should_cache_request(request):\n        return None\n\n    request = filter_request_headers(request)\n    request = freeze_request(request)\n    return request\n\n\ndef should_cache_request(request: Request) -> bool:\n    return any(hostname in request.url for hostname in HOSTNAMES_TO_CACHE)\n\n\ndef filter_request_headers(request: Request) -> Request:\n    for header_name in list(request.headers):\n        if any(\n            (\n                (type(ignore) is str and ignore.lower() == header_name.lower())\n                or (isinstance(ignore, re.Pattern) and ignore.match(header_name))\n            )\n            for ignore in IGNORE_REQUEST_HEADERS\n        ):\n            del request.headers[header_name]\n    return request\n\n\ndef freeze_request(request: Request) -> Request:\n    if not request or not request.body:\n        return request\n\n    with contextlib.suppress(ValueError):\n        request.body = freeze_request_body(\n            json.loads(\n                request.body.getvalue()\n                if isinstance(request.body, BytesIO)\n                else request.body\n            )\n        )\n\n    return request\n\n\ndef freeze_request_body(body: dict) -> bytes:\n    \"\"\"Remove any dynamic items from the request body\"\"\"\n\n    if \"messages\" not in body:\n        return json.dumps(body, sort_keys=True).encode()\n\n    if \"max_tokens\" in body:\n        del body[\"max_tokens\"]\n\n    for message in body[\"messages\"]:\n        if \"content\" in message and \"role\" in message:\n            if message[\"role\"] == \"system\":\n                message[\"content\"] = replace_message_content(\n                    message[\"content\"], LLM_MESSAGE_REPLACEMENTS\n                )\n\n    return json.dumps(body, sort_keys=True).encode()\n\n\ndef replace_message_content(content: str, replacements: list[dict[str, str]]) -> str:\n    for replacement in replacements:\n        pattern = re.compile(replacement[\"regex\"])\n        content = pattern.sub(replacement[\"replacement\"], content)\n\n    return content\n\n\ndef before_record_response(response: dict[str, Any]) -> dict[str, Any]:\n    if \"Transfer-Encoding\" in response[\"headers\"]:\n        del response[\"headers\"][\"Transfer-Encoding\"]\n    return response\n", "autogpt/tests/vcr/__init__.py": "import logging\nimport os\nfrom hashlib import sha256\nfrom typing import cast\n\nimport pytest\nfrom openai import OpenAI\nfrom openai._models import FinalRequestOptions\nfrom openai._types import Omit\nfrom openai._utils import is_given\nfrom pytest_mock import MockerFixture\n\nfrom .vcr_filter import (\n    before_record_request,\n    before_record_response,\n    freeze_request_body,\n)\n\nDEFAULT_RECORD_MODE = \"new_episodes\"\nBASE_VCR_CONFIG = {\n    \"before_record_request\": before_record_request,\n    \"before_record_response\": before_record_response,\n    \"match_on\": [\"method\", \"headers\"],\n}\n\n\n@pytest.fixture(scope=\"session\")\ndef vcr_config(get_base_vcr_config):\n    return get_base_vcr_config\n\n\n@pytest.fixture(scope=\"session\")\ndef get_base_vcr_config(request):\n    record_mode = request.config.getoption(\"--record-mode\", default=\"new_episodes\")\n    config = BASE_VCR_CONFIG\n\n    if record_mode is None:\n        config[\"record_mode\"] = DEFAULT_RECORD_MODE\n\n    return config\n\n\n@pytest.fixture()\ndef vcr_cassette_dir(request):\n    test_name = os.path.splitext(request.node.name)[0]\n    return os.path.join(\"tests/vcr_cassettes\", test_name)\n\n\n@pytest.fixture\ndef cached_openai_client(mocker: MockerFixture) -> OpenAI:\n    client = OpenAI()\n    _prepare_options = client._prepare_options\n\n    def _patched_prepare_options(self, options: FinalRequestOptions):\n        _prepare_options(options)\n\n        if not options.json_data:\n            return\n\n        headers: dict[str, str | Omit] = (\n            {**options.headers} if is_given(options.headers) else {}\n        )\n        options.headers = headers\n        data = cast(dict, options.json_data)\n\n        logging.getLogger(\"cached_openai_client\").debug(\n            f\"Outgoing API request: {headers}\\n{data if data else None}\"\n        )\n\n        # Add hash header for cheap & fast matching on cassette playback\n        headers[\"X-Content-Hash\"] = sha256(\n            freeze_request_body(data), usedforsecurity=False\n        ).hexdigest()\n\n    mocker.patch.object(\n        client,\n        \"_prepare_options\",\n        new=_patched_prepare_options,\n    )\n\n    return client\n", "autogpt/tests/integration/agent_factory.py": "from pathlib import Path\n\nimport pytest\nfrom forge.config.ai_profile import AIProfile\nfrom forge.file_storage import FileStorageBackendName, get_storage\nfrom forge.llm.providers import MultiProvider\n\nfrom autogpt.agents.agent import Agent, AgentConfiguration, AgentSettings\nfrom autogpt.app.config import AppConfig\n\n\n@pytest.fixture\ndef dummy_agent(config: AppConfig, llm_provider: MultiProvider):\n    ai_profile = AIProfile(\n        ai_name=\"Dummy Agent\",\n        ai_role=\"Dummy Role\",\n        ai_goals=[\n            \"Dummy Task\",\n        ],\n    )\n\n    agent_settings = AgentSettings(\n        name=Agent.default_settings.name,\n        description=Agent.default_settings.description,\n        ai_profile=ai_profile,\n        config=AgentConfiguration(\n            fast_llm=config.fast_llm,\n            smart_llm=config.smart_llm,\n            use_functions_api=config.openai_functions,\n        ),\n        history=Agent.default_settings.history.copy(deep=True),\n    )\n\n    local = config.file_storage_backend == FileStorageBackendName.LOCAL\n    restrict_to_root = not local or config.restrict_to_workspace\n    file_storage = get_storage(\n        config.file_storage_backend,\n        root_path=Path(\"data\"),\n        restrict_to_root=restrict_to_root,\n    )\n    file_storage.initialize()\n\n    agent = Agent(\n        settings=agent_settings,\n        llm_provider=llm_provider,\n        file_storage=file_storage,\n        app_config=config,\n    )\n\n    return agent\n", "autogpt/tests/integration/test_execute_code.py": "import random\nimport string\nimport tempfile\nfrom pathlib import Path\n\nimport pytest\nfrom forge.components.code_executor.code_executor import (\n    CodeExecutorComponent,\n    is_docker_available,\n    we_are_running_in_a_docker_container,\n)\nfrom forge.file_storage.base import FileStorage\nfrom forge.utils.exceptions import InvalidArgumentError, OperationNotAllowedError\n\n\n@pytest.fixture\ndef code_executor_component(storage: FileStorage):\n    return CodeExecutorComponent(storage)\n\n\n@pytest.fixture\ndef random_code(random_string) -> str:\n    return f\"print('Hello {random_string}!')\"\n\n\n@pytest.fixture\ndef python_test_file(storage: FileStorage, random_code: str):\n    temp_file = tempfile.NamedTemporaryFile(dir=storage.root, suffix=\".py\")\n    temp_file.write(str.encode(random_code))\n    temp_file.flush()\n\n    yield Path(temp_file.name)\n    temp_file.close()\n\n\n@pytest.fixture\ndef python_test_args_file(storage: FileStorage):\n    temp_file = tempfile.NamedTemporaryFile(dir=storage.root, suffix=\".py\")\n    temp_file.write(str.encode(\"import sys\\nprint(sys.argv[1], sys.argv[2])\"))\n    temp_file.flush()\n\n    yield Path(temp_file.name)\n    temp_file.close()\n\n\n@pytest.fixture\ndef random_string():\n    return \"\".join(random.choice(string.ascii_lowercase) for _ in range(10))\n\n\ndef test_execute_python_file(\n    code_executor_component: CodeExecutorComponent,\n    python_test_file: Path,\n    random_string: str,\n):\n    if not (is_docker_available() or we_are_running_in_a_docker_container()):\n        pytest.skip(\"Docker is not available\")\n\n    result: str = code_executor_component.execute_python_file(python_test_file)\n    assert result.replace(\"\\r\", \"\") == f\"Hello {random_string}!\\n\"\n\n\ndef test_execute_python_file_args(\n    code_executor_component: CodeExecutorComponent,\n    python_test_args_file: Path,\n    random_string: str,\n):\n    if not (is_docker_available() or we_are_running_in_a_docker_container()):\n        pytest.skip(\"Docker is not available\")\n\n    random_args = [random_string] * 2\n    random_args_string = \" \".join(random_args)\n    result = code_executor_component.execute_python_file(\n        python_test_args_file, args=random_args\n    )\n    assert result == f\"{random_args_string}\\n\"\n\n\n@pytest.mark.asyncio\nasync def test_execute_python_code(\n    code_executor_component: CodeExecutorComponent,\n    random_code: str,\n    random_string: str,\n):\n    if not (is_docker_available() or we_are_running_in_a_docker_container()):\n        pytest.skip(\"Docker is not available\")\n\n    result: str = await code_executor_component.execute_python_code(random_code)\n    assert result.replace(\"\\r\", \"\") == f\"Hello {random_string}!\\n\"\n\n\ndef test_execute_python_file_invalid(code_executor_component: CodeExecutorComponent):\n    with pytest.raises(InvalidArgumentError):\n        code_executor_component.execute_python_file(Path(\"not_python.txt\"))\n\n\ndef test_execute_python_file_not_found(code_executor_component: CodeExecutorComponent):\n    with pytest.raises(\n        FileNotFoundError,\n        match=r\"python: can't open file '([a-zA-Z]:)?[/\\\\\\-\\w]*notexist.py': \"\n        r\"\\[Errno 2\\] No such file or directory\",\n    ):\n        code_executor_component.execute_python_file(Path(\"notexist.py\"))\n\n\ndef test_execute_shell_denylist_should_deny(\n    code_executor_component: CodeExecutorComponent, random_string: str\n):\n    code_executor_component.config.shell_command_control = \"denylist\"\n    code_executor_component.config.shell_denylist = [\"echo\"]\n\n    with pytest.raises(OperationNotAllowedError, match=\"not allowed\"):\n        code_executor_component.execute_shell(f\"echo 'Hello {random_string}!'\")\n\n\ndef test_execute_shell_denylist_should_allow(\n    code_executor_component: CodeExecutorComponent, random_string: str\n):\n    code_executor_component.config.shell_command_control = \"denylist\"\n    code_executor_component.config.shell_denylist = [\"cat\"]\n\n    result = code_executor_component.execute_shell(f\"echo 'Hello {random_string}!'\")\n    assert \"Hello\" in result and random_string in result\n\n\ndef test_execute_shell_allowlist_should_deny(\n    code_executor_component: CodeExecutorComponent, random_string: str\n):\n    code_executor_component.config.shell_command_control = \"allowlist\"\n    code_executor_component.config.shell_allowlist = [\"cat\"]\n\n    with pytest.raises(OperationNotAllowedError, match=\"not allowed\"):\n        code_executor_component.execute_shell(f\"echo 'Hello {random_string}!'\")\n\n\ndef test_execute_shell_allowlist_should_allow(\n    code_executor_component: CodeExecutorComponent, random_string: str\n):\n    code_executor_component.config.shell_command_control = \"allowlist\"\n    code_executor_component.config.shell_allowlist = [\"echo\"]\n\n    result = code_executor_component.execute_shell(f\"echo 'Hello {random_string}!'\")\n    assert \"Hello\" in result and random_string in result\n", "autogpt/tests/integration/test_setup.py": "from unittest.mock import patch\n\nimport pytest\nfrom forge.config.ai_directives import AIDirectives\nfrom forge.config.ai_profile import AIProfile\n\nfrom autogpt.app.config import AppConfig\nfrom autogpt.app.setup import (\n    apply_overrides_to_ai_settings,\n    interactively_revise_ai_settings,\n)\n\n\n@pytest.mark.asyncio\nasync def test_apply_overrides_to_ai_settings():\n    ai_profile = AIProfile(ai_name=\"Test AI\", ai_role=\"Test Role\")\n    directives = AIDirectives(\n        resources=[\"Resource1\"],\n        constraints=[\"Constraint1\"],\n        best_practices=[\"BestPractice1\"],\n    )\n\n    apply_overrides_to_ai_settings(\n        ai_profile,\n        directives,\n        override_name=\"New AI\",\n        override_role=\"New Role\",\n        replace_directives=True,\n        resources=[\"NewResource\"],\n        constraints=[\"NewConstraint\"],\n        best_practices=[\"NewBestPractice\"],\n    )\n\n    assert ai_profile.ai_name == \"New AI\"\n    assert ai_profile.ai_role == \"New Role\"\n    assert directives.resources == [\"NewResource\"]\n    assert directives.constraints == [\"NewConstraint\"]\n    assert directives.best_practices == [\"NewBestPractice\"]\n\n\n@pytest.mark.asyncio\nasync def test_interactively_revise_ai_settings(config: AppConfig):\n    ai_profile = AIProfile(ai_name=\"Test AI\", ai_role=\"Test Role\")\n    directives = AIDirectives(\n        resources=[\"Resource1\"],\n        constraints=[\"Constraint1\"],\n        best_practices=[\"BestPractice1\"],\n    )\n\n    user_inputs = [\n        \"n\",\n        \"New AI\",\n        \"New Role\",\n        \"NewConstraint\",\n        \"\",\n        \"NewResource\",\n        \"\",\n        \"NewBestPractice\",\n        \"\",\n        \"y\",\n    ]\n    with patch(\"autogpt.app.setup.clean_input\", side_effect=user_inputs):\n        ai_profile, directives = await interactively_revise_ai_settings(\n            ai_profile, directives, config\n        )\n\n    assert ai_profile.ai_name == \"New AI\"\n    assert ai_profile.ai_role == \"New Role\"\n    assert directives.resources == [\"NewResource\"]\n    assert directives.constraints == [\"NewConstraint\"]\n    assert directives.best_practices == [\"NewBestPractice\"]\n", "autogpt/tests/integration/test_web_selenium.py": "import pytest\nfrom forge.components.web.selenium import BrowsingError, WebSeleniumComponent\n\nfrom autogpt.agents.agent import Agent\n\n\n@pytest.fixture\ndef web_selenium_component(agent: Agent):\n    return agent.web_selenium\n\n\n@pytest.mark.vcr\n@pytest.mark.requires_openai_api_key\n@pytest.mark.asyncio\nasync def test_browse_website_nonexistent_url(\n    web_selenium_component: WebSeleniumComponent, cached_openai_client: None\n):\n    url = \"https://auto-gpt-thinks-this-website-does-not-exist.com\"\n    question = \"How to execute a barrel roll\"\n\n    with pytest.raises(BrowsingError, match=\"NAME_NOT_RESOLVED\") as raised:\n        await web_selenium_component.read_webpage(url=url, question=question)\n\n        # Sanity check that the response is not too long\n        assert len(raised.exconly()) < 200\n", "autogpt/tests/integration/__init__.py": "", "autogpt/tests/integration/test_image_gen.py": "import functools\nimport hashlib\nfrom pathlib import Path\nfrom unittest.mock import patch\n\nimport pytest\nfrom forge.components.image_gen import ImageGeneratorComponent\nfrom forge.components.image_gen.image_gen import ImageGeneratorConfiguration\nfrom forge.file_storage.base import FileStorage\nfrom forge.llm.providers.openai import OpenAICredentials\nfrom PIL import Image\nfrom pydantic import SecretStr\n\n\n@pytest.fixture\ndef image_gen_component(storage: FileStorage):\n    cred = OpenAICredentials.from_env()\n    return ImageGeneratorComponent(storage, openai_credentials=cred)\n\n\n@pytest.fixture\ndef huggingface_image_gen_component(storage: FileStorage):\n    config = ImageGeneratorConfiguration(\n        image_provider=\"huggingface\",\n        huggingface_api_token=SecretStr(\"1\"),\n        huggingface_image_model=\"CompVis/stable-diffusion-v1-4\",\n    )\n    return ImageGeneratorComponent(storage, config=config)\n\n\n@pytest.fixture(params=[256, 512, 1024])\ndef image_size(request):\n    \"\"\"Parametrize image size.\"\"\"\n    return request.param\n\n\n@pytest.mark.requires_openai_api_key\n@pytest.mark.vcr\ndef test_dalle(\n    image_gen_component: ImageGeneratorComponent,\n    image_size,\n):\n    \"\"\"Test DALL-E image generation.\"\"\"\n    generate_and_validate(\n        image_gen_component,\n        image_provider=\"dalle\",\n        image_size=image_size,\n    )\n\n\n@pytest.mark.xfail(\n    reason=\"The image is too big to be put in a cassette for a CI pipeline. \"\n    \"We're looking into a solution.\"\n)\n@pytest.mark.requires_huggingface_api_key\n@pytest.mark.parametrize(\n    \"image_model\",\n    [\"CompVis/stable-diffusion-v1-4\", \"stabilityai/stable-diffusion-2-1\"],\n)\ndef test_huggingface(\n    image_gen_component: ImageGeneratorComponent,\n    image_size,\n    image_model,\n):\n    \"\"\"Test HuggingFace image generation.\"\"\"\n    generate_and_validate(\n        image_gen_component,\n        image_provider=\"huggingface\",\n        image_size=image_size,\n        hugging_face_image_model=image_model,\n    )\n\n\n@pytest.mark.xfail(reason=\"SD WebUI call does not work.\")\ndef test_sd_webui(image_gen_component: ImageGeneratorComponent, image_size):\n    \"\"\"Test SD WebUI image generation.\"\"\"\n    generate_and_validate(\n        image_gen_component,\n        image_provider=\"sd_webui\",\n        image_size=image_size,\n    )\n\n\n@pytest.mark.xfail(reason=\"SD WebUI call does not work.\")\ndef test_sd_webui_negative_prompt(\n    image_gen_component: ImageGeneratorComponent, image_size\n):\n    gen_image = functools.partial(\n        image_gen_component.generate_image_with_sd_webui,\n        prompt=\"astronaut riding a horse\",\n        size=image_size,\n        extra={\"seed\": 123},\n    )\n\n    # Generate an image with a negative prompt\n    image_path = lst(\n        gen_image(negative_prompt=\"horse\", output_file=Path(\"negative.jpg\"))\n    )\n    with Image.open(image_path) as img:\n        neg_image_hash = hashlib.md5(img.tobytes()).hexdigest()\n\n    # Generate an image without a negative prompt\n    image_path = lst(gen_image(output_file=Path(\"positive.jpg\")))\n    with Image.open(image_path) as img:\n        image_hash = hashlib.md5(img.tobytes()).hexdigest()\n\n    assert image_hash != neg_image_hash\n\n\ndef lst(txt):\n    \"\"\"Extract the file path from the output of `generate_image()`\"\"\"\n    return Path(txt.split(\": \", maxsplit=1)[1].strip())\n\n\ndef generate_and_validate(\n    image_gen_component: ImageGeneratorComponent,\n    image_size,\n    image_provider,\n    hugging_face_image_model=None,\n    **kwargs,\n):\n    \"\"\"Generate an image and validate the output.\"\"\"\n    image_gen_component.config.image_provider = image_provider\n    if hugging_face_image_model:\n        image_gen_component.config.huggingface_image_model = hugging_face_image_model\n    prompt = \"astronaut riding a horse\"\n\n    image_path = lst(image_gen_component.generate_image(prompt, image_size, **kwargs))\n    assert image_path.exists()\n    with Image.open(image_path) as img:\n        assert img.size == (image_size, image_size)\n\n\n@pytest.mark.parametrize(\n    \"return_text\",\n    [\n        # Delay\n        '{\"error\":\"Model [model] is currently loading\",\"estimated_time\": [delay]}',\n        '{\"error\":\"Model [model] is currently loading\"}',  # No delay\n        '{\"error:}',  # Bad JSON\n        \"\",  # Bad Image\n    ],\n)\n@pytest.mark.parametrize(\n    \"image_model\",\n    [\"CompVis/stable-diffusion-v1-4\", \"stabilityai/stable-diffusion-2-1\"],\n)\n@pytest.mark.parametrize(\"delay\", [10, 0])\ndef test_huggingface_fail_request_with_delay(\n    huggingface_image_gen_component: ImageGeneratorComponent,\n    image_size,\n    image_model,\n    return_text,\n    delay,\n):\n    return_text = return_text.replace(\"[model]\", image_model).replace(\n        \"[delay]\", str(delay)\n    )\n\n    with patch(\"requests.post\") as mock_post:\n        if return_text == \"\":\n            # Test bad image\n            mock_post.return_value.status_code = 200\n            mock_post.return_value.ok = True\n            mock_post.return_value.content = b\"bad image\"\n        else:\n            # Test delay and bad json\n            mock_post.return_value.status_code = 500\n            mock_post.return_value.ok = False\n            mock_post.return_value.text = return_text\n\n        huggingface_image_gen_component.config.huggingface_image_model = image_model\n        prompt = \"astronaut riding a horse\"\n\n        with patch(\"time.sleep\") as mock_sleep:\n            # Verify request fails.\n            result = huggingface_image_gen_component.generate_image(prompt, image_size)\n            assert result == \"Error creating image.\"\n\n            # Verify retry was called with delay if delay is in return_text\n            if \"estimated_time\" in return_text:\n                mock_sleep.assert_called_with(delay)\n            else:\n                mock_sleep.assert_not_called()\n\n\ndef test_huggingface_fail_request_no_delay(\n    mocker, huggingface_image_gen_component: ImageGeneratorComponent\n):\n    # Mock requests.post\n    mock_post = mocker.patch(\"requests.post\")\n    mock_post.return_value.status_code = 500\n    mock_post.return_value.ok = False\n    mock_post.return_value.text = (\n        '{\"error\":\"Model CompVis/stable-diffusion-v1-4 is currently loading\"}'\n    )\n\n    # Mock time.sleep\n    mock_sleep = mocker.patch(\"time.sleep\")\n\n    result = huggingface_image_gen_component.generate_image(\n        \"astronaut riding a horse\", 512\n    )\n\n    assert result == \"Error creating image.\"\n\n    # Verify retry was not called.\n    mock_sleep.assert_not_called()\n\n\ndef test_huggingface_fail_request_bad_json(\n    mocker, huggingface_image_gen_component: ImageGeneratorComponent\n):\n    # Mock requests.post\n    mock_post = mocker.patch(\"requests.post\")\n    mock_post.return_value.status_code = 500\n    mock_post.return_value.ok = False\n    mock_post.return_value.text = '{\"error:}'\n\n    # Mock time.sleep\n    mock_sleep = mocker.patch(\"time.sleep\")\n\n    result = huggingface_image_gen_component.generate_image(\n        \"astronaut riding a horse\", 512\n    )\n\n    assert result == \"Error creating image.\"\n\n    # Verify retry was not called.\n    mock_sleep.assert_not_called()\n\n\ndef test_huggingface_fail_request_bad_image(\n    mocker, huggingface_image_gen_component: ImageGeneratorComponent\n):\n    # Mock requests.post\n    mock_post = mocker.patch(\"requests.post\")\n    mock_post.return_value.status_code = 200\n\n    result = huggingface_image_gen_component.generate_image(\n        \"astronaut riding a horse\", 512\n    )\n\n    assert result == \"Error creating image.\"\n", "autogpt/tests/mocks/__init__.py": "", "autogpt/tests/unit/test_file_operations.py": "import os\nfrom pathlib import Path\n\nimport pytest\nfrom forge.file_storage import FileStorage\n\nfrom autogpt.agents.agent import Agent\n\n\n@pytest.fixture()\ndef file_content():\n    return \"This is a test file.\\n\"\n\n\n@pytest.fixture\ndef file_manager_component(agent: Agent):\n    return agent.file_manager\n\n\n@pytest.fixture()\ndef test_file_name():\n    return Path(\"test_file.txt\")\n\n\n@pytest.fixture\ndef test_file_path(test_file_name: Path, storage: FileStorage):\n    return storage.get_path(test_file_name)\n\n\n@pytest.fixture()\ndef test_directory(storage: FileStorage):\n    return storage.get_path(\"test_directory\")\n\n\n@pytest.fixture()\ndef test_nested_file(storage: FileStorage):\n    return storage.get_path(\"nested/test_file.txt\")\n\n\n@pytest.mark.asyncio\nasync def test_read_file(\n    test_file_path: Path,\n    file_content,\n    file_manager_component,\n    agent: Agent,\n):\n    await agent.file_manager.workspace.write_file(test_file_path.name, file_content)\n    content = file_manager_component.read_file(test_file_path.name)\n    assert content.replace(\"\\r\", \"\") == file_content\n\n\ndef test_read_file_not_found(file_manager_component):\n    filename = \"does_not_exist.txt\"\n    with pytest.raises(FileNotFoundError):\n        file_manager_component.read_file(filename)\n\n\n@pytest.mark.asyncio\nasync def test_write_to_file_relative_path(\n    test_file_name: Path, file_manager_component, agent: Agent\n):\n    new_content = \"This is new content.\\n\"\n    await file_manager_component.write_to_file(test_file_name, new_content)\n    with open(\n        agent.file_manager.workspace.get_path(test_file_name), \"r\", encoding=\"utf-8\"\n    ) as f:\n        content = f.read()\n    assert content == new_content\n\n\n@pytest.mark.asyncio\nasync def test_write_to_file_absolute_path(\n    test_file_path: Path, file_manager_component\n):\n    new_content = \"This is new content.\\n\"\n    await file_manager_component.write_to_file(test_file_path, new_content)\n    with open(test_file_path, \"r\", encoding=\"utf-8\") as f:\n        content = f.read()\n    assert content == new_content\n\n\n@pytest.mark.asyncio\nasync def test_list_files(file_manager_component, agent: Agent):\n    # Create files A and B\n    file_a_name = \"file_a.txt\"\n    file_b_name = \"file_b.txt\"\n    test_directory = Path(\"test_directory\")\n\n    await agent.file_manager.workspace.write_file(file_a_name, \"This is file A.\")\n    await agent.file_manager.workspace.write_file(file_b_name, \"This is file B.\")\n\n    # Create a subdirectory and place a copy of file_a in it\n    agent.file_manager.workspace.make_dir(test_directory)\n    await agent.file_manager.workspace.write_file(\n        test_directory / file_a_name, \"This is file A in the subdirectory.\"\n    )\n\n    files = file_manager_component.list_folder(\".\")\n    assert file_a_name in files\n    assert file_b_name in files\n    assert os.path.join(test_directory, file_a_name) in files\n\n    # Clean up\n    agent.file_manager.workspace.delete_file(file_a_name)\n    agent.file_manager.workspace.delete_file(file_b_name)\n    agent.file_manager.workspace.delete_file(test_directory / file_a_name)\n    agent.file_manager.workspace.delete_dir(test_directory)\n\n    # Case 2: Search for a file that does not exist and make sure we don't throw\n    non_existent_file = \"non_existent_file.txt\"\n    files = file_manager_component.list_folder(\"\")\n    assert non_existent_file not in files\n", "autogpt/tests/unit/test_git_commands.py": "import pytest\nfrom forge.components.git_operations import GitOperationsComponent\nfrom forge.file_storage.base import FileStorage\nfrom forge.utils.exceptions import CommandExecutionError\nfrom git.exc import GitCommandError\nfrom git.repo.base import Repo\n\nfrom autogpt.agents.agent import Agent\n\n\n@pytest.fixture\ndef mock_clone_from(mocker):\n    return mocker.patch.object(Repo, \"clone_from\")\n\n\n@pytest.fixture\ndef git_ops_component(agent: Agent):\n    return agent.git_ops\n\n\ndef test_clone_auto_gpt_repository(\n    git_ops_component: GitOperationsComponent,\n    storage: FileStorage,\n    mock_clone_from,\n    agent: Agent,\n):\n    mock_clone_from.return_value = None\n\n    repo = \"github.com/Significant-Gravitas/Auto-GPT.git\"\n    scheme = \"https://\"\n    url = scheme + repo\n    clone_path = storage.get_path(\"auto-gpt-repo\")\n\n    expected_output = f\"Cloned {url} to {clone_path}\"\n\n    clone_result = git_ops_component.clone_repository(url, clone_path)\n\n    assert clone_result == expected_output\n    mock_clone_from.assert_called_once_with(\n        url=f\"{scheme}{git_ops_component.config.github_username}:{git_ops_component.config.github_api_key}@{repo}\",  # noqa: E501\n        to_path=clone_path,\n    )\n\n\ndef test_clone_repository_error(\n    git_ops_component: GitOperationsComponent,\n    storage: FileStorage,\n    mock_clone_from,\n    agent: Agent,\n):\n    url = \"https://github.com/this-repository/does-not-exist.git\"\n    clone_path = storage.get_path(\"does-not-exist\")\n\n    mock_clone_from.side_effect = GitCommandError(\n        \"clone\", \"fatal: repository not found\", \"\"\n    )\n\n    with pytest.raises(CommandExecutionError):\n        git_ops_component.clone_repository(url, clone_path)\n", "autogpt/tests/unit/test_url_validation.py": "import pytest\nfrom forge.utils.url_validator import validate_url\nfrom pytest import raises\n\n\n@validate_url\ndef dummy_method(url):\n    return url\n\n\nsuccessful_test_data = (\n    (\"https://google.com/search?query=abc\"),\n    (\"https://google.com/search?query=abc&p=123\"),\n    (\"http://google.com/\"),\n    (\"http://a.lot.of.domain.net/param1/param2\"),\n)\n\n\n@pytest.mark.parametrize(\"url\", successful_test_data)\ndef test_url_validation_succeeds(url):\n    assert dummy_method(url) == url\n\n\n@pytest.mark.parametrize(\n    \"url,expected_error\",\n    [\n        (\"htt://example.com\", \"Invalid URL format\"),\n        (\"httppp://example.com\", \"Invalid URL format\"),\n        (\" https://example.com\", \"Invalid URL format\"),\n        (\"http://?query=q\", \"Missing Scheme or Network location\"),\n    ],\n)\ndef test_url_validation_fails_invalid_url(url, expected_error):\n    with raises(ValueError, match=expected_error):\n        dummy_method(url)\n\n\nlocal_file = (\n    (\"file://localhost\"),\n    (\"file://localhost/home/reinier/secrets.txt\"),\n    (\"file:///home/reinier/secrets.txt\"),\n    (\"file:///C:/Users/Reinier/secrets.txt\"),\n)\n\n\n@pytest.mark.parametrize(\"url\", local_file)\ndef test_url_validation_fails_local_path(url):\n    with raises(ValueError):\n        dummy_method(url)\n\n\ndef test_happy_path_valid_url():\n    \"\"\"\n    Test that the function successfully validates a valid URL with `http://` or\n    `https://` prefix.\n    \"\"\"\n\n    @validate_url\n    def test_func(url):\n        return url\n\n    assert test_func(\"https://www.google.com\") == \"https://www.google.com\"\n    assert test_func(\"http://www.google.com\") == \"http://www.google.com\"\n\n\ndef test_general_behavior_additional_path_parameters_query_string():\n    \"\"\"\n    Test that the function successfully validates a valid URL with additional path,\n    parameters, and query string.\n    \"\"\"\n\n    @validate_url\n    def test_func(url):\n        return url\n\n    assert (\n        test_func(\"https://www.google.com/search?q=python\")\n        == \"https://www.google.com/search?q=python\"\n    )\n\n\ndef test_edge_case_missing_scheme_or_network_location():\n    \"\"\"\n    Test that the function raises a ValueError if the URL is missing scheme or\n    network location.\n    \"\"\"\n\n    @validate_url\n    def test_func(url):\n        return url\n\n    with pytest.raises(ValueError):\n        test_func(\"www.google.com\")\n\n\ndef test_edge_case_local_file_access():\n    \"\"\"Test that the function raises a ValueError if the URL has local file access\"\"\"\n\n    @validate_url\n    def test_func(url):\n        return url\n\n    with pytest.raises(ValueError):\n        test_func(\"file:///etc/passwd\")\n\n\ndef test_general_behavior_sanitizes_url():\n    \"\"\"Test that the function sanitizes the URL by removing unnecessary components\"\"\"\n\n    @validate_url\n    def test_func(url):\n        return url\n\n    assert (\n        test_func(\"https://www.google.com/search?q=python#top\")\n        == \"https://www.google.com/search?q=python\"\n    )\n\n\ndef test_general_behavior_invalid_url_format():\n    \"\"\"\n    Test that the function raises a ValueError if the URL has an invalid format\n    (e.g. missing slashes)\n    \"\"\"\n\n    @validate_url\n    def test_func(url):\n        return url\n\n    with pytest.raises(ValueError):\n        test_func(\"https:www.google.com\")\n\n\ndef test_url_with_special_chars():\n    \"\"\"\n    Tests that the function can handle URLs that contain unusual but valid characters.\n    \"\"\"\n    url = \"https://example.com/path%20with%20spaces\"\n    assert dummy_method(url) == url\n\n\ndef test_extremely_long_url():\n    \"\"\"\n    Tests that the function raises a ValueError if the URL is over 2000 characters.\n    \"\"\"\n    url = \"http://example.com/\" + \"a\" * 2000\n    with raises(ValueError, match=\"URL is too long\"):\n        dummy_method(url)\n\n\ndef test_internationalized_url():\n    \"\"\"\n    Tests that the function can handle internationalized URLs with non-ASCII characters.\n    \"\"\"\n    url = \"http://\u4f8b\u5b50.\u6d4b\u8bd5\"\n    assert dummy_method(url) == url\n", "autogpt/tests/unit/test_text_file_parsers.py": "import json\nimport logging\nimport os.path\nimport tempfile\nfrom pathlib import Path\nfrom xml.etree import ElementTree\n\nimport docx\nimport pytest\nimport yaml\nfrom bs4 import BeautifulSoup\nfrom forge.utils.file_operations import decode_textual_file, is_file_binary_fn\n\nlogger = logging.getLogger(__name__)\n\nplain_text_str = \"Hello, world!\"\n\n\ndef mock_text_file():\n    with tempfile.NamedTemporaryFile(mode=\"w\", delete=False, suffix=\".txt\") as f:\n        f.write(plain_text_str)\n    return f.name\n\n\ndef mock_csv_file():\n    with tempfile.NamedTemporaryFile(mode=\"w\", delete=False, suffix=\".csv\") as f:\n        f.write(plain_text_str)\n    return f.name\n\n\ndef mock_pdf_file():\n    with tempfile.NamedTemporaryFile(mode=\"wb\", delete=False, suffix=\".pdf\") as f:\n        # Create a new PDF and add a page with the text plain_text_str\n        # Write the PDF header\n        f.write(b\"%PDF-1.7\\n\")\n        # Write the document catalog\n        f.write(b\"1 0 obj\\n\")\n        f.write(b\"<< /Type /Catalog /Pages 2 0 R >>\\n\")\n        f.write(b\"endobj\\n\")\n        # Write the page object\n        f.write(b\"2 0 obj\\n\")\n        f.write(\n            b\"<< /Type /Page /Parent 1 0 R /Resources << /Font << /F1 3 0 R >> >> \"\n            b\"/MediaBox [0 0 612 792] /Contents 4 0 R >>\\n\"\n        )\n        f.write(b\"endobj\\n\")\n        # Write the font object\n        f.write(b\"3 0 obj\\n\")\n        f.write(\n            b\"<< /Type /Font /Subtype /Type1 /Name /F1 /BaseFont /Helvetica-Bold >>\\n\"\n        )\n        f.write(b\"endobj\\n\")\n        # Write the page contents object\n        f.write(b\"4 0 obj\\n\")\n        f.write(b\"<< /Length 25 >>\\n\")\n        f.write(b\"stream\\n\")\n        f.write(b\"BT\\n/F1 12 Tf\\n72 720 Td\\n(Hello, world!) Tj\\nET\\n\")\n        f.write(b\"endstream\\n\")\n        f.write(b\"endobj\\n\")\n        # Write the cross-reference table\n        f.write(b\"xref\\n\")\n        f.write(b\"0 5\\n\")\n        f.write(b\"0000000000 65535 f \\n\")\n        f.write(b\"0000000017 00000 n \\n\")\n        f.write(b\"0000000073 00000 n \\n\")\n        f.write(b\"0000000123 00000 n \\n\")\n        f.write(b\"0000000271 00000 n \\n\")\n        f.write(b\"trailer\\n\")\n        f.write(b\"<< /Size 5 /Root 1 0 R >>\\n\")\n        f.write(b\"startxref\\n\")\n        f.write(b\"380\\n\")\n        f.write(b\"%%EOF\\n\")\n        f.write(b\"\\x00\")\n    return f.name\n\n\ndef mock_docx_file():\n    with tempfile.NamedTemporaryFile(mode=\"wb\", delete=False, suffix=\".docx\") as f:\n        document = docx.Document()\n        document.add_paragraph(plain_text_str)\n        document.save(f.name)\n    return f.name\n\n\ndef mock_json_file():\n    with tempfile.NamedTemporaryFile(mode=\"w\", delete=False, suffix=\".json\") as f:\n        json.dump({\"text\": plain_text_str}, f)\n    return f.name\n\n\ndef mock_xml_file():\n    root = ElementTree.Element(\"text\")\n    root.text = plain_text_str\n    tree = ElementTree.ElementTree(root)\n    with tempfile.NamedTemporaryFile(mode=\"wb\", delete=False, suffix=\".xml\") as f:\n        tree.write(f)\n    return f.name\n\n\ndef mock_yaml_file():\n    with tempfile.NamedTemporaryFile(mode=\"w\", delete=False, suffix=\".yaml\") as f:\n        yaml.dump({\"text\": plain_text_str}, f)\n    return f.name\n\n\ndef mock_html_file():\n    html = BeautifulSoup(\n        \"<html>\"\n        \"<head><title>This is a test</title></head>\"\n        f\"<body><p>{plain_text_str}</p></body>\"\n        \"</html>\",\n        \"html.parser\",\n    )\n    with tempfile.NamedTemporaryFile(mode=\"w\", delete=False, suffix=\".html\") as f:\n        f.write(str(html))\n    return f.name\n\n\ndef mock_md_file():\n    with tempfile.NamedTemporaryFile(mode=\"w\", delete=False, suffix=\".md\") as f:\n        f.write(f\"# {plain_text_str}!\\n\")\n    return f.name\n\n\ndef mock_latex_file():\n    with tempfile.NamedTemporaryFile(mode=\"w\", delete=False, suffix=\".tex\") as f:\n        latex_str = (\n            r\"\\documentclass{article}\"\n            r\"\\begin{document}\"\n            f\"{plain_text_str}\"\n            r\"\\end{document}\"\n        )\n        f.write(latex_str)\n    return f.name\n\n\nrespective_file_creation_functions = {\n    \".txt\": mock_text_file,\n    \".csv\": mock_csv_file,\n    \".pdf\": mock_pdf_file,\n    \".docx\": mock_docx_file,\n    \".json\": mock_json_file,\n    \".xml\": mock_xml_file,\n    \".yaml\": mock_yaml_file,\n    \".html\": mock_html_file,\n    \".md\": mock_md_file,\n    \".tex\": mock_latex_file,\n}\nbinary_files_extensions = [\".pdf\", \".docx\"]\n\n\n@pytest.mark.parametrize(\n    \"file_extension, c_file_creator\",\n    respective_file_creation_functions.items(),\n)\ndef test_parsers(file_extension, c_file_creator):\n    created_file_path = Path(c_file_creator())\n    with open(created_file_path, \"rb\") as file:\n        loaded_text = decode_textual_file(file, os.path.splitext(file.name)[1], logger)\n\n        assert plain_text_str in loaded_text\n\n        should_be_binary = file_extension in binary_files_extensions\n        assert should_be_binary == is_file_binary_fn(file)\n\n    created_file_path.unlink()  # cleanup\n", "autogpt/tests/unit/test_config.py": "\"\"\"\nTest cases for the config class, which handles the configuration settings\nfor the AI and ensures it behaves as a singleton.\n\"\"\"\nimport asyncio\nimport os\nfrom typing import Any\nfrom unittest import mock\n\nimport pytest\nfrom openai.pagination import AsyncPage\nfrom openai.types import Model\nfrom pydantic import SecretStr\n\nfrom autogpt.app.config import GPT_3_MODEL, GPT_4_MODEL, AppConfig, ConfigBuilder\nfrom autogpt.app.configurator import apply_overrides_to_config\n\n\ndef test_initial_values(config: AppConfig) -> None:\n    \"\"\"\n    Test if the initial values of the config class attributes are set correctly.\n    \"\"\"\n    assert config.continuous_mode is False\n    assert config.tts_config.speak_mode is False\n    assert config.fast_llm.startswith(\"gpt-3.5-turbo\")\n    assert config.smart_llm.startswith(\"gpt-4\")\n\n\n@pytest.mark.asyncio\n@mock.patch(\"openai.resources.models.AsyncModels.list\")\nasync def test_fallback_to_gpt3_if_gpt4_not_available(\n    mock_list_models: Any, config: AppConfig\n) -> None:\n    \"\"\"\n    Test if models update to gpt-3.5-turbo if gpt-4 is not available.\n    \"\"\"\n    config.fast_llm = GPT_4_MODEL\n    config.smart_llm = GPT_4_MODEL\n\n    mock_list_models.return_value = asyncio.Future()\n    mock_list_models.return_value.set_result(\n        AsyncPage(\n            data=[Model(id=GPT_3_MODEL, created=0, object=\"model\", owned_by=\"AutoGPT\")],\n            object=\"Models\",  # no idea what this should be, but irrelevant\n        )\n    )\n\n    await apply_overrides_to_config(config=config)\n\n    assert config.fast_llm == GPT_3_MODEL\n    assert config.smart_llm == GPT_3_MODEL\n\n\ndef test_missing_azure_config(config: AppConfig) -> None:\n    assert config.openai_credentials is not None\n\n    config_file = config.app_data_dir / \"azure_config.yaml\"\n    with pytest.raises(FileNotFoundError):\n        config.openai_credentials.load_azure_config(config_file)\n\n    config_file.write_text(\"\")\n    with pytest.raises(ValueError):\n        config.openai_credentials.load_azure_config(config_file)\n\n    assert config.openai_credentials.api_type != SecretStr(\"azure\")\n    assert config.openai_credentials.api_version is None\n    assert config.openai_credentials.azure_model_to_deploy_id_map is None\n\n\n@pytest.fixture\ndef config_with_azure(config: AppConfig):\n    config_file = config.app_data_dir / \"azure_config.yaml\"\n    config_file.write_text(\n        f\"\"\"\nazure_api_type: azure\nazure_api_version: 2023-06-01-preview\nazure_endpoint: https://dummy.openai.azure.com\nazure_model_map:\n    {config.fast_llm}: FAST-LLM_ID\n    {config.smart_llm}: SMART-LLM_ID\n    {config.embedding_model}: embedding-deployment-id-for-azure\n\"\"\"\n    )\n    os.environ[\"USE_AZURE\"] = \"True\"\n    os.environ[\"AZURE_CONFIG_FILE\"] = str(config_file)\n    config_with_azure = ConfigBuilder.build_config_from_env(\n        project_root=config.project_root\n    )\n    yield config_with_azure\n    del os.environ[\"USE_AZURE\"]\n    del os.environ[\"AZURE_CONFIG_FILE\"]\n\n\ndef test_azure_config(config_with_azure: AppConfig) -> None:\n    assert (credentials := config_with_azure.openai_credentials) is not None\n    assert credentials.api_type == SecretStr(\"azure\")\n    assert credentials.api_version == SecretStr(\"2023-06-01-preview\")\n    assert credentials.azure_endpoint == SecretStr(\"https://dummy.openai.azure.com\")\n    assert credentials.azure_model_to_deploy_id_map == {\n        config_with_azure.fast_llm: \"FAST-LLM_ID\",\n        config_with_azure.smart_llm: \"SMART-LLM_ID\",\n        config_with_azure.embedding_model: \"embedding-deployment-id-for-azure\",\n    }\n\n    fast_llm = config_with_azure.fast_llm\n    smart_llm = config_with_azure.smart_llm\n    assert (\n        credentials.get_model_access_kwargs(config_with_azure.fast_llm)[\"model\"]\n        == \"FAST-LLM_ID\"\n    )\n    assert (\n        credentials.get_model_access_kwargs(config_with_azure.smart_llm)[\"model\"]\n        == \"SMART-LLM_ID\"\n    )\n\n    # Emulate --gpt4only\n    config_with_azure.fast_llm = smart_llm\n    assert (\n        credentials.get_model_access_kwargs(config_with_azure.fast_llm)[\"model\"]\n        == \"SMART-LLM_ID\"\n    )\n    assert (\n        credentials.get_model_access_kwargs(config_with_azure.smart_llm)[\"model\"]\n        == \"SMART-LLM_ID\"\n    )\n\n    # Emulate --gpt3only\n    config_with_azure.fast_llm = config_with_azure.smart_llm = fast_llm\n    assert (\n        credentials.get_model_access_kwargs(config_with_azure.fast_llm)[\"model\"]\n        == \"FAST-LLM_ID\"\n    )\n    assert (\n        credentials.get_model_access_kwargs(config_with_azure.smart_llm)[\"model\"]\n        == \"FAST-LLM_ID\"\n    )\n", "autogpt/tests/unit/test_local_file_storage.py": "from pathlib import Path\n\nimport pytest\nfrom forge.file_storage.local import FileStorageConfiguration, LocalFileStorage\n\n_ACCESSIBLE_PATHS = [\n    Path(\".\"),\n    Path(\"test_file.txt\"),\n    Path(\"test_folder\"),\n    Path(\"test_folder/test_file.txt\"),\n    Path(\"test_folder/..\"),\n    Path(\"test_folder/../test_file.txt\"),\n    Path(\"test_folder/../test_folder\"),\n    Path(\"test_folder/../test_folder/test_file.txt\"),\n]\n\n_INACCESSIBLE_PATHS = (\n    [\n        # Takes us out of the workspace\n        Path(\"..\"),\n        Path(\"../test_file.txt\"),\n        Path(\"../not_auto_gpt_workspace\"),\n        Path(\"../not_auto_gpt_workspace/test_file.txt\"),\n        Path(\"test_folder/../..\"),\n        Path(\"test_folder/../../test_file.txt\"),\n        Path(\"test_folder/../../not_auto_gpt_workspace\"),\n        Path(\"test_folder/../../not_auto_gpt_workspace/test_file.txt\"),\n    ]\n    + [\n        # Contains null byte\n        Path(\"\\0\"),\n        Path(\"\\0test_file.txt\"),\n        Path(\"test_folder/\\0\"),\n        Path(\"test_folder/\\0test_file.txt\"),\n    ]\n    + [\n        # Absolute paths\n        Path(\"/\"),\n        Path(\"/test_file.txt\"),\n        Path(\"/home\"),\n    ]\n)\n\n_TEST_FILES = [\n    Path(\"test_file.txt\"),\n    Path(\"dir/test_file.txt\"),\n    Path(\"dir/test_file2.txt\"),\n    Path(\"dir/sub_dir/test_file.txt\"),\n]\n\n_TEST_DIRS = [\n    Path(\"dir\"),\n    Path(\"dir/sub_dir\"),\n]\n\n\n@pytest.fixture()\ndef storage_root(tmp_path):\n    return tmp_path / \"data\"\n\n\n@pytest.fixture()\ndef storage(storage_root):\n    return LocalFileStorage(\n        FileStorageConfiguration(root=storage_root, restrict_to_root=True)\n    )\n\n\n@pytest.fixture()\ndef content():\n    return \"test content\"\n\n\n@pytest.fixture(params=_ACCESSIBLE_PATHS)\ndef accessible_path(request):\n    return request.param\n\n\n@pytest.fixture(params=_INACCESSIBLE_PATHS)\ndef inaccessible_path(request):\n    return request.param\n\n\n@pytest.fixture(params=_TEST_FILES)\ndef file_path(request):\n    return request.param\n\n\n@pytest.mark.asyncio\nasync def test_open_file(file_path: Path, content: str, storage: LocalFileStorage):\n    if file_path.parent:\n        storage.make_dir(file_path.parent)\n    await storage.write_file(file_path, content)\n    file = storage.open_file(file_path)\n    assert file.read() == content\n    file.close()\n    storage.delete_file(file_path)\n\n\n@pytest.mark.asyncio\nasync def test_write_read_file(content: str, storage: LocalFileStorage):\n    await storage.write_file(\"test_file.txt\", content)\n    assert storage.read_file(\"test_file.txt\") == content\n\n\n@pytest.mark.asyncio\nasync def test_list_files(content: str, storage: LocalFileStorage):\n    storage.make_dir(\"dir\")\n    storage.make_dir(\"dir/sub_dir\")\n    await storage.write_file(\"test_file.txt\", content)\n    await storage.write_file(\"dir/test_file.txt\", content)\n    await storage.write_file(\"dir/test_file2.txt\", content)\n    await storage.write_file(\"dir/sub_dir/test_file.txt\", content)\n    files = storage.list_files()\n    assert Path(\"test_file.txt\") in files\n    assert Path(\"dir/test_file.txt\") in files\n    assert Path(\"dir/test_file2.txt\") in files\n    assert Path(\"dir/sub_dir/test_file.txt\") in files\n    storage.delete_file(\"test_file.txt\")\n    storage.delete_file(\"dir/test_file.txt\")\n    storage.delete_file(\"dir/test_file2.txt\")\n    storage.delete_file(\"dir/sub_dir/test_file.txt\")\n    storage.delete_dir(\"dir/sub_dir\")\n    storage.delete_dir(\"dir\")\n\n\n@pytest.mark.asyncio\nasync def test_list_folders(content: str, storage: LocalFileStorage):\n    storage.make_dir(\"dir\")\n    storage.make_dir(\"dir/sub_dir\")\n    await storage.write_file(\"dir/test_file.txt\", content)\n    await storage.write_file(\"dir/sub_dir/test_file.txt\", content)\n    folders = storage.list_folders(recursive=False)\n    folders_recursive = storage.list_folders(recursive=True)\n    assert Path(\"dir\") in folders\n    assert Path(\"dir/sub_dir\") not in folders\n    assert Path(\"dir\") in folders_recursive\n    assert Path(\"dir/sub_dir\") in folders_recursive\n    storage.delete_file(\"dir/test_file.txt\")\n    storage.delete_file(\"dir/sub_dir/test_file.txt\")\n    storage.delete_dir(\"dir/sub_dir\")\n    storage.delete_dir(\"dir\")\n\n\n@pytest.mark.asyncio\nasync def test_exists_delete_file(\n    file_path: Path, content: str, storage: LocalFileStorage\n):\n    if file_path.parent:\n        storage.make_dir(file_path.parent)\n    await storage.write_file(file_path, content)\n    assert storage.exists(file_path)\n    storage.delete_file(file_path)\n    assert not storage.exists(file_path)\n\n\n@pytest.fixture(params=_TEST_DIRS)\ndef test_make_delete_dir(request, storage: LocalFileStorage):\n    storage.make_dir(request)\n    assert storage.exists(request)\n    storage.delete_dir(request)\n    assert not storage.exists(request)\n\n\n@pytest.mark.asyncio\nasync def test_rename(file_path: Path, content: str, storage: LocalFileStorage):\n    if file_path.parent:\n        storage.make_dir(file_path.parent)\n    await storage.write_file(file_path, content)\n    assert storage.exists(file_path)\n    storage.rename(file_path, Path(str(file_path) + \"_renamed\"))\n    assert not storage.exists(file_path)\n    assert storage.exists(Path(str(file_path) + \"_renamed\"))\n\n\ndef test_clone_with_subroot(storage: LocalFileStorage):\n    subroot = storage.clone_with_subroot(\"dir\")\n    assert subroot.root == storage.root / \"dir\"\n\n\ndef test_get_path_accessible(accessible_path: Path, storage: LocalFileStorage):\n    full_path = storage.get_path(accessible_path)\n    assert full_path.is_absolute()\n    assert full_path.is_relative_to(storage.root)\n\n\ndef test_get_path_inaccessible(inaccessible_path: Path, storage: LocalFileStorage):\n    with pytest.raises(ValueError):\n        storage.get_path(inaccessible_path)\n\n\n@pytest.mark.asyncio\nasync def test_copy_file(storage: LocalFileStorage):\n    await storage.write_file(\"test_file.txt\", \"test content\")\n    storage.copy(\"test_file.txt\", \"test_file_copy.txt\")\n    storage.make_dir(\"dir\")\n    storage.copy(\"test_file.txt\", \"dir/test_file_copy.txt\")\n    assert storage.read_file(\"test_file_copy.txt\") == \"test content\"\n    assert storage.read_file(\"dir/test_file_copy.txt\") == \"test content\"\n\n\n@pytest.mark.asyncio\nasync def test_copy_dir(storage: LocalFileStorage):\n    storage.make_dir(\"dir\")\n    storage.make_dir(\"dir/sub_dir\")\n    await storage.write_file(\"dir/test_file.txt\", \"test content\")\n    await storage.write_file(\"dir/sub_dir/test_file.txt\", \"test content\")\n    storage.copy(\"dir\", \"dir_copy\")\n    assert storage.read_file(\"dir_copy/test_file.txt\") == \"test content\"\n    assert storage.read_file(\"dir_copy/sub_dir/test_file.txt\") == \"test content\"\n", "autogpt/tests/unit/test_s3_file_storage.py": "import os\nimport uuid\nfrom pathlib import Path\n\nimport pytest\nimport pytest_asyncio\nfrom botocore.exceptions import ClientError\nfrom forge.file_storage.s3 import S3FileStorage, S3FileStorageConfiguration\n\nif not (os.getenv(\"S3_ENDPOINT_URL\") and os.getenv(\"AWS_ACCESS_KEY_ID\")):\n    pytest.skip(\"S3 environment variables are not set\", allow_module_level=True)\n\n\n@pytest.fixture\ndef s3_bucket_name() -> str:\n    return f\"test-bucket-{str(uuid.uuid4())[:8]}\"\n\n\n@pytest.fixture\ndef s3_root() -> Path:\n    return Path(\"/workspaces/AutoGPT-some-unique-task-id\")\n\n\n@pytest.fixture\ndef s3_storage_uninitialized(s3_bucket_name: str, s3_root: Path):\n    os.environ[\"STORAGE_BUCKET\"] = s3_bucket_name\n    storage_config = S3FileStorageConfiguration.from_env()\n    storage_config.root = s3_root\n    storage = S3FileStorage(storage_config)\n    yield storage  # type: ignore\n    del os.environ[\"STORAGE_BUCKET\"]\n\n\ndef test_initialize(s3_bucket_name: str, s3_storage_uninitialized: S3FileStorage):\n    s3 = s3_storage_uninitialized._s3\n\n    # test that the bucket doesn't exist yet\n    with pytest.raises(ClientError):\n        s3.meta.client.head_bucket(Bucket=s3_bucket_name)  # pyright: ignore\n\n    s3_storage_uninitialized.initialize()\n\n    # test that the bucket has been created\n    s3.meta.client.head_bucket(Bucket=s3_bucket_name)  # pyright: ignore\n    # FIXME: remove the \"pyright: ignore\" comments after moving this test file to forge\n\n\ndef test_workspace_bucket_name(\n    s3_storage: S3FileStorage,\n    s3_bucket_name: str,\n):\n    assert s3_storage._bucket.name == s3_bucket_name\n\n\n@pytest.fixture\ndef s3_storage(s3_storage_uninitialized: S3FileStorage):\n    (s3_storage := s3_storage_uninitialized).initialize()\n    yield s3_storage  # type: ignore\n\n    # Empty & delete the test bucket\n    s3_storage._bucket.objects.all().delete()\n    s3_storage._bucket.delete()\n\n\nNESTED_DIR = \"existing/test/dir\"\nTEST_FILES: list[tuple[str | Path, str]] = [\n    (\"existing_test_file_1\", \"test content 1\"),\n    (\"existing_test_file_2.txt\", \"test content 2\"),\n    (Path(\"existing_test_file_3\"), \"test content 3\"),\n    (Path(f\"{NESTED_DIR}/test_file_4\"), \"test content 4\"),\n]\n\n\n@pytest_asyncio.fixture\nasync def s3_storage_with_files(s3_storage: S3FileStorage):\n    for file_name, file_content in TEST_FILES:\n        s3_storage._bucket.Object(str(s3_storage.get_path(file_name))).put(\n            Body=file_content\n        )\n    yield s3_storage  # type: ignore\n\n\n@pytest.mark.asyncio\nasync def test_read_file(s3_storage_with_files: S3FileStorage):\n    for file_name, file_content in TEST_FILES:\n        content = s3_storage_with_files.read_file(file_name)\n        assert content == file_content\n\n    with pytest.raises(ClientError):\n        s3_storage_with_files.read_file(\"non_existent_file\")\n\n\ndef test_list_files(s3_storage_with_files: S3FileStorage):\n    # List at root level\n    assert (\n        files := s3_storage_with_files.list_files()\n    ) == s3_storage_with_files.list_files()\n    assert len(files) > 0\n    assert set(files) == set(Path(file_name) for file_name, _ in TEST_FILES)\n\n    # List at nested path\n    assert (\n        nested_files := s3_storage_with_files.list_files(NESTED_DIR)\n    ) == s3_storage_with_files.list_files(NESTED_DIR)\n    assert len(nested_files) > 0\n    assert set(nested_files) == set(\n        p.relative_to(NESTED_DIR)\n        for file_name, _ in TEST_FILES\n        if (p := Path(file_name)).is_relative_to(NESTED_DIR)\n    )\n\n\ndef test_list_folders(s3_storage_with_files: S3FileStorage):\n    # List recursive\n    folders = s3_storage_with_files.list_folders(recursive=True)\n    assert len(folders) > 0\n    assert set(folders) == {\n        Path(\"existing\"),\n        Path(\"existing/test\"),\n        Path(\"existing/test/dir\"),\n    }\n    # List non-recursive\n    folders = s3_storage_with_files.list_folders(recursive=False)\n    assert len(folders) > 0\n    assert set(folders) == {Path(\"existing\")}\n\n\n@pytest.mark.asyncio\nasync def test_write_read_file(s3_storage: S3FileStorage):\n    await s3_storage.write_file(\"test_file\", \"test_content\")\n    assert s3_storage.read_file(\"test_file\") == \"test_content\"\n\n\n@pytest.mark.asyncio\nasync def test_overwrite_file(s3_storage_with_files: S3FileStorage):\n    for file_name, _ in TEST_FILES:\n        await s3_storage_with_files.write_file(file_name, \"new content\")\n        assert s3_storage_with_files.read_file(file_name) == \"new content\"\n\n\ndef test_delete_file(s3_storage_with_files: S3FileStorage):\n    for file_to_delete, _ in TEST_FILES:\n        s3_storage_with_files.delete_file(file_to_delete)\n        with pytest.raises(ClientError):\n            s3_storage_with_files.read_file(file_to_delete)\n\n\ndef test_exists(s3_storage_with_files: S3FileStorage):\n    for file_name, _ in TEST_FILES:\n        assert s3_storage_with_files.exists(file_name)\n\n    assert not s3_storage_with_files.exists(\"non_existent_file\")\n\n\ndef test_rename_file(s3_storage_with_files: S3FileStorage):\n    for file_name, _ in TEST_FILES:\n        new_name = str(file_name) + \"_renamed\"\n        s3_storage_with_files.rename(file_name, new_name)\n        assert s3_storage_with_files.exists(new_name)\n        assert not s3_storage_with_files.exists(file_name)\n\n\ndef test_rename_dir(s3_storage_with_files: S3FileStorage):\n    s3_storage_with_files.rename(NESTED_DIR, \"existing/test/dir_renamed\")\n    assert s3_storage_with_files.exists(\"existing/test/dir_renamed\")\n    assert not s3_storage_with_files.exists(NESTED_DIR)\n\n\ndef test_clone(s3_storage_with_files: S3FileStorage, s3_root: Path):\n    cloned = s3_storage_with_files.clone_with_subroot(\"existing/test\")\n    assert cloned.root == s3_root / Path(\"existing/test\")\n    assert cloned._bucket.name == s3_storage_with_files._bucket.name\n    assert cloned.exists(\"dir\")\n    assert cloned.exists(\"dir/test_file_4\")\n\n\n@pytest.mark.asyncio\nasync def test_copy_file(storage: S3FileStorage):\n    await storage.write_file(\"test_file.txt\", \"test content\")\n    storage.copy(\"test_file.txt\", \"test_file_copy.txt\")\n    storage.make_dir(\"dir\")\n    storage.copy(\"test_file.txt\", \"dir/test_file_copy.txt\")\n    assert storage.read_file(\"test_file_copy.txt\") == \"test content\"\n    assert storage.read_file(\"dir/test_file_copy.txt\") == \"test content\"\n\n\n@pytest.mark.asyncio\nasync def test_copy_dir(storage: S3FileStorage):\n    storage.make_dir(\"dir\")\n    storage.make_dir(\"dir/sub_dir\")\n    await storage.write_file(\"dir/test_file.txt\", \"test content\")\n    await storage.write_file(\"dir/sub_dir/test_file.txt\", \"test content\")\n    storage.copy(\"dir\", \"dir_copy\")\n    assert storage.read_file(\"dir_copy/test_file.txt\") == \"test content\"\n    assert storage.read_file(\"dir_copy/sub_dir/test_file.txt\") == \"test content\"\n", "autogpt/tests/unit/test_web_search.py": "import json\n\nimport pytest\nfrom forge.components.web.search import WebSearchComponent\nfrom forge.utils.exceptions import ConfigurationError\nfrom googleapiclient.errors import HttpError\nfrom pydantic import SecretStr\n\nfrom autogpt.agents.agent import Agent\n\n\n@pytest.fixture\ndef web_search_component(agent: Agent):\n    agent.web_search.config.google_api_key = SecretStr(\"test\")\n    agent.web_search.config.google_custom_search_engine_id = SecretStr(\"test\")\n    return agent.web_search\n\n\n@pytest.mark.parametrize(\n    \"query, expected_output\",\n    [(\"test\", \"test\"), ([\"test1\", \"test2\"], '[\"test1\", \"test2\"]')],\n)\n@pytest.fixture\ndef test_safe_google_results(\n    query, expected_output, web_search_component: WebSearchComponent\n):\n    result = web_search_component.safe_google_results(query)\n    assert isinstance(result, str)\n    assert result == expected_output\n\n\n@pytest.fixture\ndef test_safe_google_results_invalid_input(web_search_component: WebSearchComponent):\n    with pytest.raises(AttributeError):\n        web_search_component.safe_google_results(123)  # type: ignore\n\n\n@pytest.mark.parametrize(\n    \"query, num_results, expected_output_parts, return_value\",\n    [\n        (\n            \"test\",\n            1,\n            (\"Result 1\", \"https://example.com/result1\"),\n            [{\"title\": \"Result 1\", \"href\": \"https://example.com/result1\"}],\n        ),\n        (\"\", 1, (), []),\n        (\"no results\", 1, (), []),\n    ],\n)\ndef test_google_search(\n    query,\n    num_results,\n    expected_output_parts,\n    return_value,\n    mocker,\n    web_search_component: WebSearchComponent,\n):\n    mock_ddg = mocker.Mock()\n    mock_ddg.return_value = return_value\n\n    mocker.patch(\"forge.components.web.search.DDGS.text\", mock_ddg)\n    actual_output = web_search_component.web_search(query, num_results=num_results)\n    for o in expected_output_parts:\n        assert o in actual_output\n\n\n@pytest.fixture\ndef mock_googleapiclient(mocker):\n    mock_build = mocker.patch(\"googleapiclient.discovery.build\")\n    mock_service = mocker.Mock()\n    mock_build.return_value = mock_service\n    return mock_service.cse().list().execute().get\n\n\n@pytest.mark.parametrize(\n    \"query, num_results, search_results, expected_output\",\n    [\n        (\n            \"test\",\n            3,\n            [\n                {\"link\": \"http://example.com/result1\"},\n                {\"link\": \"http://example.com/result2\"},\n                {\"link\": \"http://example.com/result3\"},\n            ],\n            [\n                \"http://example.com/result1\",\n                \"http://example.com/result2\",\n                \"http://example.com/result3\",\n            ],\n        ),\n        (\"\", 3, [], []),\n    ],\n)\ndef test_google_official_search(\n    query,\n    num_results,\n    expected_output,\n    search_results,\n    mock_googleapiclient,\n    web_search_component: WebSearchComponent,\n):\n    mock_googleapiclient.return_value = search_results\n    actual_output = web_search_component.google(query, num_results=num_results)\n    assert actual_output == web_search_component.safe_google_results(expected_output)\n\n\n@pytest.mark.parametrize(\n    \"query, num_results, expected_error_type, http_code, error_msg\",\n    [\n        (\n            \"invalid query\",\n            3,\n            HttpError,\n            400,\n            \"Invalid Value\",\n        ),\n        (\n            \"invalid API key\",\n            3,\n            ConfigurationError,\n            403,\n            \"invalid API key\",\n        ),\n    ],\n)\ndef test_google_official_search_errors(\n    query,\n    num_results,\n    expected_error_type,\n    mock_googleapiclient,\n    http_code,\n    error_msg,\n    web_search_component: WebSearchComponent,\n):\n    class resp:\n        def __init__(self, _status, _reason):\n            self.status = _status\n            self.reason = _reason\n\n    response_content = {\n        \"error\": {\"code\": http_code, \"message\": error_msg, \"reason\": \"backendError\"}\n    }\n    error = HttpError(\n        resp=resp(http_code, error_msg),\n        content=str.encode(json.dumps(response_content)),\n        uri=\"https://www.googleapis.com/customsearch/v1?q=invalid+query&cx\",\n    )\n\n    mock_googleapiclient.side_effect = error\n    with pytest.raises(expected_error_type):\n        web_search_component.google(query, num_results=num_results)\n", "autogpt/tests/unit/test_spinner.py": "import time\n\nfrom autogpt.app.spinner import Spinner\n\nALMOST_DONE_MESSAGE = \"Almost done...\"\nPLEASE_WAIT = \"Please wait...\"\n\n\ndef test_spinner_initializes_with_default_values():\n    \"\"\"Tests that the spinner initializes with default values.\"\"\"\n    with Spinner() as spinner:\n        assert spinner.message == \"Loading...\"\n        assert spinner.delay == 0.1\n\n\ndef test_spinner_initializes_with_custom_values():\n    \"\"\"Tests that the spinner initializes with custom message and delay values.\"\"\"\n    with Spinner(message=PLEASE_WAIT, delay=0.2) as spinner:\n        assert spinner.message == PLEASE_WAIT\n        assert spinner.delay == 0.2\n\n\n#\ndef test_spinner_stops_spinning():\n    \"\"\"Tests that the spinner starts spinning and stops spinning without errors.\"\"\"\n    with Spinner() as spinner:\n        time.sleep(1)\n    assert not spinner.running\n\n\ndef test_spinner_can_be_used_as_context_manager():\n    \"\"\"Tests that the spinner can be used as a context manager.\"\"\"\n    with Spinner() as spinner:\n        assert spinner.running\n    assert not spinner.running\n", "autogpt/tests/unit/test_gcs_file_storage.py": "import os\nimport uuid\nfrom pathlib import Path\n\nimport pytest\nimport pytest_asyncio\nfrom forge.file_storage.gcs import GCSFileStorage, GCSFileStorageConfiguration\nfrom google.auth.exceptions import GoogleAuthError\nfrom google.cloud import storage\nfrom google.cloud.exceptions import NotFound\n\ntry:\n    storage.Client()\nexcept GoogleAuthError:\n    pytest.skip(\"Google Cloud Authentication not configured\", allow_module_level=True)\n\npytestmark = pytest.mark.slow\n\n\n@pytest.fixture(scope=\"module\")\ndef gcs_bucket_name() -> str:\n    return f\"test-bucket-{str(uuid.uuid4())[:8]}\"\n\n\n@pytest.fixture(scope=\"module\")\ndef gcs_root() -> Path:\n    return Path(\"/workspaces/AutoGPT-some-unique-task-id\")\n\n\n@pytest.fixture(scope=\"module\")\ndef gcs_storage_uninitialized(gcs_bucket_name: str, gcs_root: Path):\n    os.environ[\"STORAGE_BUCKET\"] = gcs_bucket_name\n    storage_config = GCSFileStorageConfiguration.from_env()\n    storage_config.root = gcs_root\n    storage = GCSFileStorage(storage_config)\n    yield storage  # type: ignore\n    del os.environ[\"STORAGE_BUCKET\"]\n\n\ndef test_initialize(gcs_bucket_name: str, gcs_storage_uninitialized: GCSFileStorage):\n    gcs = gcs_storage_uninitialized._gcs\n\n    # test that the bucket doesn't exist yet\n    with pytest.raises(NotFound):\n        gcs.get_bucket(gcs_bucket_name)\n\n    gcs_storage_uninitialized.initialize()\n\n    # test that the bucket has been created\n    bucket = gcs.get_bucket(gcs_bucket_name)\n\n    # clean up\n    bucket.delete(force=True)\n\n\n@pytest.fixture(scope=\"module\")\ndef gcs_storage(gcs_storage_uninitialized: GCSFileStorage):\n    (gcs_storage := gcs_storage_uninitialized).initialize()\n    yield gcs_storage  # type: ignore\n\n    # Empty & delete the test bucket\n    gcs_storage._bucket.delete(force=True)\n\n\ndef test_workspace_bucket_name(\n    gcs_storage: GCSFileStorage,\n    gcs_bucket_name: str,\n):\n    assert gcs_storage._bucket.name == gcs_bucket_name\n\n\nNESTED_DIR = \"existing/test/dir\"\nTEST_FILES: list[tuple[str | Path, str]] = [\n    (\"existing_test_file_1\", \"test content 1\"),\n    (\"existing_test_file_2.txt\", \"test content 2\"),\n    (Path(\"existing_test_file_3\"), \"test content 3\"),\n    (Path(f\"{NESTED_DIR}/test_file_4\"), \"test content 4\"),\n]\n\n\n@pytest_asyncio.fixture\nasync def gcs_storage_with_files(gcs_storage: GCSFileStorage):\n    for file_name, file_content in TEST_FILES:\n        gcs_storage._bucket.blob(\n            str(gcs_storage.get_path(file_name))\n        ).upload_from_string(file_content)\n    yield gcs_storage  # type: ignore\n\n\n@pytest.mark.asyncio\nasync def test_read_file(gcs_storage_with_files: GCSFileStorage):\n    for file_name, file_content in TEST_FILES:\n        content = gcs_storage_with_files.read_file(file_name)\n        assert content == file_content\n\n    with pytest.raises(NotFound):\n        gcs_storage_with_files.read_file(\"non_existent_file\")\n\n\ndef test_list_files(gcs_storage_with_files: GCSFileStorage):\n    # List at root level\n    assert (\n        files := gcs_storage_with_files.list_files()\n    ) == gcs_storage_with_files.list_files()\n    assert len(files) > 0\n    assert set(files) == set(Path(file_name) for file_name, _ in TEST_FILES)\n\n    # List at nested path\n    assert (\n        nested_files := gcs_storage_with_files.list_files(NESTED_DIR)\n    ) == gcs_storage_with_files.list_files(NESTED_DIR)\n    assert len(nested_files) > 0\n    assert set(nested_files) == set(\n        p.relative_to(NESTED_DIR)\n        for file_name, _ in TEST_FILES\n        if (p := Path(file_name)).is_relative_to(NESTED_DIR)\n    )\n\n\ndef test_list_folders(gcs_storage_with_files: GCSFileStorage):\n    # List recursive\n    folders = gcs_storage_with_files.list_folders(recursive=True)\n    assert len(folders) > 0\n    assert set(folders) == {\n        Path(\"existing\"),\n        Path(\"existing/test\"),\n        Path(\"existing/test/dir\"),\n    }\n    # List non-recursive\n    folders = gcs_storage_with_files.list_folders(recursive=False)\n    assert len(folders) > 0\n    assert set(folders) == {Path(\"existing\")}\n\n\n@pytest.mark.asyncio\nasync def test_write_read_file(gcs_storage: GCSFileStorage):\n    await gcs_storage.write_file(\"test_file\", \"test_content\")\n    assert gcs_storage.read_file(\"test_file\") == \"test_content\"\n\n\n@pytest.mark.asyncio\nasync def test_overwrite_file(gcs_storage_with_files: GCSFileStorage):\n    for file_name, _ in TEST_FILES:\n        await gcs_storage_with_files.write_file(file_name, \"new content\")\n        assert gcs_storage_with_files.read_file(file_name) == \"new content\"\n\n\ndef test_delete_file(gcs_storage_with_files: GCSFileStorage):\n    for file_to_delete, _ in TEST_FILES:\n        gcs_storage_with_files.delete_file(file_to_delete)\n        assert not gcs_storage_with_files.exists(file_to_delete)\n\n\ndef test_exists(gcs_storage_with_files: GCSFileStorage):\n    for file_name, _ in TEST_FILES:\n        assert gcs_storage_with_files.exists(file_name)\n\n    assert not gcs_storage_with_files.exists(\"non_existent_file\")\n\n\ndef test_rename_file(gcs_storage_with_files: GCSFileStorage):\n    for file_name, _ in TEST_FILES:\n        new_name = str(file_name) + \"_renamed\"\n        gcs_storage_with_files.rename(file_name, new_name)\n        assert gcs_storage_with_files.exists(new_name)\n        assert not gcs_storage_with_files.exists(file_name)\n\n\ndef test_rename_dir(gcs_storage_with_files: GCSFileStorage):\n    gcs_storage_with_files.rename(NESTED_DIR, \"existing/test/dir_renamed\")\n    assert gcs_storage_with_files.exists(\"existing/test/dir_renamed\")\n    assert not gcs_storage_with_files.exists(NESTED_DIR)\n\n\ndef test_clone(gcs_storage_with_files: GCSFileStorage, gcs_root: Path):\n    cloned = gcs_storage_with_files.clone_with_subroot(\"existing/test\")\n    assert cloned.root == gcs_root / Path(\"existing/test\")\n    assert cloned._bucket.name == gcs_storage_with_files._bucket.name\n    assert cloned.exists(\"dir\")\n    assert cloned.exists(\"dir/test_file_4\")\n\n\n@pytest.mark.asyncio\nasync def test_copy_file(storage: GCSFileStorage):\n    await storage.write_file(\"test_file.txt\", \"test content\")\n    storage.copy(\"test_file.txt\", \"test_file_copy.txt\")\n    storage.make_dir(\"dir\")\n    storage.copy(\"test_file.txt\", \"dir/test_file_copy.txt\")\n    assert storage.read_file(\"test_file_copy.txt\") == \"test content\"\n    assert storage.read_file(\"dir/test_file_copy.txt\") == \"test content\"\n\n\n@pytest.mark.asyncio\nasync def test_copy_dir(storage: GCSFileStorage):\n    storage.make_dir(\"dir\")\n    storage.make_dir(\"dir/sub_dir\")\n    await storage.write_file(\"dir/test_file.txt\", \"test content\")\n    await storage.write_file(\"dir/sub_dir/test_file.txt\", \"test content\")\n    storage.copy(\"dir\", \"dir_copy\")\n    assert storage.read_file(\"dir_copy/test_file.txt\") == \"test content\"\n    assert storage.read_file(\"dir_copy/sub_dir/test_file.txt\") == \"test content\"\n", "autogpt/tests/unit/test_logs.py": "import pytest\nfrom forge.logging.utils import remove_color_codes\n\n\n@pytest.mark.parametrize(\n    \"raw_text, clean_text\",\n    [\n        (\n            \"COMMAND = \\x1b[36mbrowse_website\\x1b[0m  \"\n            \"ARGUMENTS = \\x1b[36m{'url': 'https://www.google.com',\"\n            \" 'question': 'What is the capital of France?'}\\x1b[0m\",\n            \"COMMAND = browse_website  \"\n            \"ARGUMENTS = {'url': 'https://www.google.com',\"\n            \" 'question': 'What is the capital of France?'}\",\n        ),\n        (\n            \"{'Schaue dir meine Projekte auf github () an, als auch meine Webseiten': \"\n            \"'https://github.com/Significant-Gravitas/AutoGPT,\"\n            \" https://discord.gg/autogpt und https://twitter.com/Auto_GPT'}\",\n            \"{'Schaue dir meine Projekte auf github () an, als auch meine Webseiten': \"\n            \"'https://github.com/Significant-Gravitas/AutoGPT,\"\n            \" https://discord.gg/autogpt und https://twitter.com/Auto_GPT'}\",\n        ),\n        (\"\", \"\"),\n        (\"hello\", \"hello\"),\n        (\"hello\\x1B[31m world\", \"hello world\"),\n        (\"\\x1B[36mHello,\\x1B[32m World!\", \"Hello, World!\"),\n        (\n            \"\\x1B[1m\\x1B[31mError:\\x1B[0m\\x1B[31m file not found\",\n            \"Error: file not found\",\n        ),\n    ],\n)\ndef test_remove_color_codes(raw_text, clean_text):\n    assert remove_color_codes(raw_text) == clean_text\n", "autogpt/tests/unit/__init__.py": "", "autogpt/tests/unit/test_utils.py": "import json\nimport os\nfrom pathlib import Path\nfrom unittest.mock import patch\n\nimport pytest\nimport requests\nfrom forge.json.parsing import extract_dict_from_json\nfrom git import InvalidGitRepositoryError\n\nimport autogpt.app.utils\nfrom autogpt.app.utils import (\n    get_bulletin_from_web,\n    get_current_git_branch,\n    get_latest_bulletin,\n    set_env_config_value,\n)\nfrom tests.utils import skip_in_ci\n\n\n@pytest.fixture\ndef valid_json_response() -> dict:\n    return {\n        \"thoughts\": {\n            \"text\": \"My task is complete. I will use the 'task_complete' command \"\n            \"to shut down.\",\n            \"reasoning\": \"I will use the 'task_complete' command because it allows me \"\n            \"to shut down and signal that my task is complete.\",\n            \"plan\": \"I will use the 'task_complete' command with the reason \"\n            \"'Task complete: retrieved Tesla's revenue in 2022.' to shut down.\",\n            \"criticism\": \"I need to ensure that I have completed all necessary tasks \"\n            \"before shutting down.\",\n            \"speak\": \"All done!\",\n        },\n        \"command\": {\n            \"name\": \"task_complete\",\n            \"args\": {\"reason\": \"Task complete: retrieved Tesla's revenue in 2022.\"},\n        },\n    }\n\n\n@pytest.fixture\ndef invalid_json_response() -> dict:\n    return {\n        \"thoughts\": {\n            \"text\": \"My task is complete. I will use the 'task_complete' command \"\n            \"to shut down.\",\n            \"reasoning\": \"I will use the 'task_complete' command because it allows me \"\n            \"to shut down and signal that my task is complete.\",\n            \"plan\": \"I will use the 'task_complete' command with the reason \"\n            \"'Task complete: retrieved Tesla's revenue in 2022.' to shut down.\",\n            \"criticism\": \"I need to ensure that I have completed all necessary tasks \"\n            \"before shutting down.\",\n            \"speak\": \"\",\n        },\n        \"command\": {\"name\": \"\", \"args\": {}},\n    }\n\n\n@patch(\"requests.get\")\ndef test_get_bulletin_from_web_success(mock_get):\n    expected_content = \"Test bulletin from web\"\n\n    mock_get.return_value.status_code = 200\n    mock_get.return_value.text = expected_content\n    bulletin = get_bulletin_from_web()\n\n    assert expected_content in bulletin\n    mock_get.assert_called_with(\n        \"https://raw.githubusercontent.com/Significant-Gravitas/AutoGPT/master/autogpt/BULLETIN.md\"  # noqa: E501\n    )\n\n\n@patch(\"requests.get\")\ndef test_get_bulletin_from_web_failure(mock_get):\n    mock_get.return_value.status_code = 404\n    bulletin = get_bulletin_from_web()\n\n    assert bulletin == \"\"\n\n\n@patch(\"requests.get\")\ndef test_get_bulletin_from_web_exception(mock_get):\n    mock_get.side_effect = requests.exceptions.RequestException()\n    bulletin = get_bulletin_from_web()\n\n    assert bulletin == \"\"\n\n\ndef test_get_latest_bulletin_no_file():\n    if os.path.exists(\"data/CURRENT_BULLETIN.md\"):\n        os.remove(\"data/CURRENT_BULLETIN.md\")\n\n    bulletin, is_new = get_latest_bulletin()\n    assert is_new\n\n\ndef test_get_latest_bulletin_with_file():\n    expected_content = \"Test bulletin\"\n    with open(\"data/CURRENT_BULLETIN.md\", \"w\", encoding=\"utf-8\") as f:\n        f.write(expected_content)\n\n    with patch(\"autogpt.app.utils.get_bulletin_from_web\", return_value=\"\"):\n        bulletin, is_new = get_latest_bulletin()\n        assert expected_content in bulletin\n        assert is_new is False\n\n    os.remove(\"data/CURRENT_BULLETIN.md\")\n\n\ndef test_get_latest_bulletin_with_new_bulletin():\n    with open(\"data/CURRENT_BULLETIN.md\", \"w\", encoding=\"utf-8\") as f:\n        f.write(\"Old bulletin\")\n\n    expected_content = \"New bulletin from web\"\n    with patch(\n        \"autogpt.app.utils.get_bulletin_from_web\", return_value=expected_content\n    ):\n        bulletin, is_new = get_latest_bulletin()\n        assert \"::NEW BULLETIN::\" in bulletin\n        assert expected_content in bulletin\n        assert is_new\n\n    os.remove(\"data/CURRENT_BULLETIN.md\")\n\n\ndef test_get_latest_bulletin_new_bulletin_same_as_old_bulletin():\n    expected_content = \"Current bulletin\"\n    with open(\"data/CURRENT_BULLETIN.md\", \"w\", encoding=\"utf-8\") as f:\n        f.write(expected_content)\n\n    with patch(\n        \"autogpt.app.utils.get_bulletin_from_web\", return_value=expected_content\n    ):\n        bulletin, is_new = get_latest_bulletin()\n        assert expected_content in bulletin\n        assert is_new is False\n\n    os.remove(\"data/CURRENT_BULLETIN.md\")\n\n\n@skip_in_ci\ndef test_get_current_git_branch():\n    branch_name = get_current_git_branch()\n    assert branch_name != \"\"\n\n\n@patch(\"autogpt.app.utils.Repo\")\ndef test_get_current_git_branch_success(mock_repo):\n    mock_repo.return_value.active_branch.name = \"test-branch\"\n    branch_name = get_current_git_branch()\n\n    assert branch_name == \"test-branch\"\n\n\n@patch(\"autogpt.app.utils.Repo\")\ndef test_get_current_git_branch_failure(mock_repo):\n    mock_repo.side_effect = InvalidGitRepositoryError()\n    branch_name = get_current_git_branch()\n\n    assert branch_name == \"\"\n\n\ndef test_extract_json_from_response(valid_json_response: dict):\n    emulated_response_from_openai = json.dumps(valid_json_response)\n    assert extract_dict_from_json(emulated_response_from_openai) == valid_json_response\n\n\ndef test_extract_json_from_response_wrapped_in_code_block(valid_json_response: dict):\n    emulated_response_from_openai = \"```\" + json.dumps(valid_json_response) + \"```\"\n    assert extract_dict_from_json(emulated_response_from_openai) == valid_json_response\n\n\ndef test_extract_json_from_response_wrapped_in_code_block_with_language(\n    valid_json_response: dict,\n):\n    emulated_response_from_openai = \"```json\" + json.dumps(valid_json_response) + \"```\"\n    assert extract_dict_from_json(emulated_response_from_openai) == valid_json_response\n\n\ndef test_extract_json_from_response_json_contained_in_string(valid_json_response: dict):\n    emulated_response_from_openai = (\n        \"sentence1\" + json.dumps(valid_json_response) + \"sentence2\"\n    )\n    assert extract_dict_from_json(emulated_response_from_openai) == valid_json_response\n\n\n@pytest.fixture\ndef mock_env_file_path(tmp_path):\n    return tmp_path / \".env\"\n\n\nenv_file_initial_content = \"\"\"\n# This is a comment\nEXISTING_KEY=EXISTING_VALUE\n\n## This is also a comment\n# DISABLED_KEY=DISABLED_VALUE\n\n# Another comment\nUNUSED_KEY=UNUSED_VALUE\n\"\"\"\n\n\n@pytest.fixture\ndef mock_env_file(mock_env_file_path: Path, monkeypatch: pytest.MonkeyPatch):\n    mock_env_file_path.write_text(env_file_initial_content)\n    monkeypatch.setattr(autogpt.app.utils, \"ENV_FILE_PATH\", mock_env_file_path)\n    return mock_env_file_path\n\n\n@pytest.fixture\ndef mock_environ(monkeypatch: pytest.MonkeyPatch):\n    env = {}\n    monkeypatch.setattr(os, \"environ\", env)\n    return env\n\n\ndef test_set_env_config_value_updates_existing_key(\n    mock_env_file: Path, mock_environ: dict\n):\n    # Before updating, ensure the original content is as expected\n    with mock_env_file.open(\"r\") as file:\n        assert file.readlines() == env_file_initial_content.splitlines(True)\n\n    set_env_config_value(\"EXISTING_KEY\", \"NEW_VALUE\")\n    with mock_env_file.open(\"r\") as file:\n        content = file.readlines()\n\n    # Ensure only the relevant line is altered\n    expected_content_lines = [\n        \"\\n\",\n        \"# This is a comment\\n\",\n        \"EXISTING_KEY=NEW_VALUE\\n\",  # existing key + new value\n        \"\\n\",\n        \"## This is also a comment\\n\",\n        \"# DISABLED_KEY=DISABLED_VALUE\\n\",\n        \"\\n\",\n        \"# Another comment\\n\",\n        \"UNUSED_KEY=UNUSED_VALUE\\n\",\n    ]\n    assert content == expected_content_lines\n    assert mock_environ[\"EXISTING_KEY\"] == \"NEW_VALUE\"\n\n\ndef test_set_env_config_value_uncomments_and_updates_disabled_key(\n    mock_env_file: Path, mock_environ: dict\n):\n    # Before adding, ensure the original content is as expected\n    with mock_env_file.open(\"r\") as file:\n        assert file.readlines() == env_file_initial_content.splitlines(True)\n\n    set_env_config_value(\"DISABLED_KEY\", \"ENABLED_NEW_VALUE\")\n    with mock_env_file.open(\"r\") as file:\n        content = file.readlines()\n\n    # Ensure only the relevant line is altered\n    expected_content_lines = [\n        \"\\n\",\n        \"# This is a comment\\n\",\n        \"EXISTING_KEY=EXISTING_VALUE\\n\",\n        \"\\n\",\n        \"## This is also a comment\\n\",\n        \"DISABLED_KEY=ENABLED_NEW_VALUE\\n\",  # disabled -> enabled + new value\n        \"\\n\",\n        \"# Another comment\\n\",\n        \"UNUSED_KEY=UNUSED_VALUE\\n\",\n    ]\n    assert content == expected_content_lines\n    assert mock_environ[\"DISABLED_KEY\"] == \"ENABLED_NEW_VALUE\"\n\n\ndef test_set_env_config_value_adds_new_key(mock_env_file: Path, mock_environ: dict):\n    # Before adding, ensure the original content is as expected\n    with mock_env_file.open(\"r\") as file:\n        assert file.readlines() == env_file_initial_content.splitlines(True)\n\n    set_env_config_value(\"NEW_KEY\", \"NEW_VALUE\")\n    with mock_env_file.open(\"r\") as file:\n        content = file.readlines()\n\n    # Ensure the new key-value pair is added without altering the rest\n    expected_content_lines = [\n        \"\\n\",\n        \"# This is a comment\\n\",\n        \"EXISTING_KEY=EXISTING_VALUE\\n\",\n        \"\\n\",\n        \"## This is also a comment\\n\",\n        \"# DISABLED_KEY=DISABLED_VALUE\\n\",\n        \"\\n\",\n        \"# Another comment\\n\",\n        \"UNUSED_KEY=UNUSED_VALUE\\n\",\n        \"NEW_KEY=NEW_VALUE\\n\",  # New key-value pair added at the end\n    ]\n    assert content == expected_content_lines\n    assert mock_environ[\"NEW_KEY\"] == \"NEW_VALUE\"\n", "autogpt/tests/unit/test_json.py": "import json\n\nimport pytest\nfrom forge.json.parsing import json_loads\n\n_JSON_FIXABLE: list[tuple[str, str]] = [\n    # Missing comma\n    ('{\"name\": \"John Doe\"   \"age\": 30,}', '{\"name\": \"John Doe\", \"age\": 30}'),\n    (\"[1, 2 3]\", \"[1, 2, 3]\"),\n    # Trailing comma\n    ('{\"name\": \"John Doe\", \"age\": 30,}', '{\"name\": \"John Doe\", \"age\": 30}'),\n    (\"[1, 2, 3,]\", \"[1, 2, 3]\"),\n    # Extra comma in object\n    ('{\"name\": \"John Doe\",, \"age\": 30}', '{\"name\": \"John Doe\", \"age\": 30}'),\n    # Extra newlines\n    ('{\"name\": \"John Doe\",\\n\"age\": 30}', '{\"name\": \"John Doe\", \"age\": 30}'),\n    (\"[1, 2,\\n3]\", \"[1, 2, 3]\"),\n    # Missing closing brace or bracket\n    ('{\"name\": \"John Doe\", \"age\": 30', '{\"name\": \"John Doe\", \"age\": 30}'),\n    (\"[1, 2, 3\", \"[1, 2, 3]\"),\n    # Different numerals\n    (\"[+1, ---2, .5, +-4.5, 123.]\", \"[1, -2, 0.5, -4.5, 123]\"),\n    ('{\"bin\": 0b1001, \"hex\": 0x1A, \"oct\": 0o17}', '{\"bin\": 9, \"hex\": 26, \"oct\": 15}'),\n    # Broken array\n    (\n        '[1, 2 3, \"yes\" true, false null, 25, {\"obj\": \"var\"}',\n        '[1, 2, 3, \"yes\", true, false, null, 25, {\"obj\": \"var\"}]',\n    ),\n    # Codeblock\n    (\n        '```json\\n{\"name\": \"John Doe\", \"age\": 30}\\n```',\n        '{\"name\": \"John Doe\", \"age\": 30}',\n    ),\n    # Multiple problems\n    (\n        '{\"name\":\"John Doe\" \"age\": 30\\n \"empty\": \"\",\"address\": '\n        \"// random comment\\n\"\n        '{\"city\": \"New York\", \"state\": \"NY\"},'\n        '\"skills\": [\"Python\" \"C++\", \"Java\",\"\"],',\n        '{\"name\": \"John Doe\", \"age\": 30, \"empty\": \"\", \"address\": '\n        '{\"city\": \"New York\", \"state\": \"NY\"}, '\n        '\"skills\": [\"Python\", \"C++\", \"Java\", \"\"]}',\n    ),\n    # All good\n    (\n        '{\"name\": \"John Doe\", \"age\": 30, \"address\": '\n        '{\"city\": \"New York\", \"state\": \"NY\"}, '\n        '\"skills\": [\"Python\", \"C++\", \"Java\"]}',\n        '{\"name\": \"John Doe\", \"age\": 30, \"address\": '\n        '{\"city\": \"New York\", \"state\": \"NY\"}, '\n        '\"skills\": [\"Python\", \"C++\", \"Java\"]}',\n    ),\n    (\"true\", \"true\"),\n    (\"false\", \"false\"),\n    (\"null\", \"null\"),\n    (\"123.5\", \"123.5\"),\n    ('\"Hello, World!\"', '\"Hello, World!\"'),\n    (\"{}\", \"{}\"),\n    (\"[]\", \"[]\"),\n]\n\n_JSON_UNFIXABLE: list[tuple[str, str]] = [\n    # Broken booleans and null\n    (\"[TRUE, False, NULL]\", \"[true, false, null]\"),\n    # Missing values in array\n    (\"[1, , 3]\", \"[1, 3]\"),\n    # Leading zeros (are treated as octal)\n    (\"[0023, 015]\", \"[23, 15]\"),\n    # Missing quotes\n    ('{\"name\": John Doe}', '{\"name\": \"John Doe\"}'),\n    # Missing opening braces or bracket\n    ('\"name\": \"John Doe\"}', '{\"name\": \"John Doe\"}'),\n    (\"1, 2, 3]\", \"[1, 2, 3]\"),\n]\n\n\n@pytest.fixture(params=_JSON_FIXABLE)\ndef fixable_json(request: pytest.FixtureRequest) -> tuple[str, str]:\n    return request.param\n\n\n@pytest.fixture(params=_JSON_UNFIXABLE)\ndef unfixable_json(request: pytest.FixtureRequest) -> tuple[str, str]:\n    return request.param\n\n\ndef test_json_loads_fixable(fixable_json: tuple[str, str]):\n    assert json_loads(fixable_json[0]) == json.loads(fixable_json[1])\n\n\ndef test_json_loads_unfixable(unfixable_json: tuple[str, str]):\n    assert json_loads(unfixable_json[0]) != json.loads(unfixable_json[1])\n", "autogpt/autogpt/__main__.py": "\"\"\"AutoGPT: A GPT powered AI Assistant\"\"\"\nimport autogpt.app.cli\n\nif __name__ == \"__main__\":\n    autogpt.app.cli.cli()\n", "autogpt/autogpt/__init__.py": "import os\nimport random\nimport sys\n\nif \"pytest\" in sys.argv or \"pytest\" in sys.modules or os.getenv(\"CI\"):\n    print(\"Setting random seed to 42\")\n    random.seed(42)\n", "autogpt/autogpt/agents/agent_manager.py": "from __future__ import annotations\n\nimport uuid\nfrom pathlib import Path\n\nfrom forge.file_storage.base import FileStorage\n\nfrom autogpt.agents.agent import AgentSettings\n\n\nclass AgentManager:\n    def __init__(self, file_storage: FileStorage):\n        self.file_manager = file_storage.clone_with_subroot(\"agents\")\n\n    @staticmethod\n    def generate_id(agent_name: str) -> str:\n        \"\"\"Generate a unique ID for an agent given agent name.\"\"\"\n        unique_id = str(uuid.uuid4())[:8]\n        return f\"{agent_name}-{unique_id}\"\n\n    def list_agents(self) -> list[str]:\n        \"\"\"Return all agent directories within storage.\"\"\"\n        agent_dirs: list[str] = []\n        for file_path in self.file_manager.list_files():\n            if len(file_path.parts) == 2 and file_path.name == \"state.json\":\n                agent_dirs.append(file_path.parent.name)\n        return agent_dirs\n\n    def get_agent_dir(self, agent_id: str) -> Path:\n        \"\"\"Return the directory of the agent with the given ID.\"\"\"\n        assert len(agent_id) > 0\n        agent_dir: Path | None = None\n        if self.file_manager.exists(agent_id):\n            agent_dir = self.file_manager.root / agent_id\n        else:\n            raise FileNotFoundError(f\"No agent with ID '{agent_id}'\")\n        return agent_dir\n\n    def load_agent_state(self, agent_id: str) -> AgentSettings:\n        \"\"\"Load the state of the agent with the given ID.\"\"\"\n        state_file_path = Path(agent_id) / \"state.json\"\n        if not self.file_manager.exists(state_file_path):\n            raise FileNotFoundError(f\"Agent with ID '{agent_id}' has no state.json\")\n\n        state = self.file_manager.read_file(state_file_path)\n        return AgentSettings.parse_raw(state)\n", "autogpt/autogpt/agents/agent.py": "from __future__ import annotations\n\nimport inspect\nimport logging\nfrom typing import TYPE_CHECKING, Any, ClassVar, Optional\n\nimport sentry_sdk\nfrom forge.agent.base import BaseAgent, BaseAgentConfiguration, BaseAgentSettings\nfrom forge.agent.protocols import (\n    AfterExecute,\n    AfterParse,\n    CommandProvider,\n    DirectiveProvider,\n    MessageProvider,\n)\nfrom forge.command.command import Command\nfrom forge.components.action_history import (\n    ActionHistoryComponent,\n    EpisodicActionHistory,\n)\nfrom forge.components.action_history.action_history import ActionHistoryConfiguration\nfrom forge.components.code_executor.code_executor import (\n    CodeExecutorComponent,\n    CodeExecutorConfiguration,\n)\nfrom forge.components.context.context import AgentContext, ContextComponent\nfrom forge.components.file_manager import FileManagerComponent\nfrom forge.components.git_operations import GitOperationsComponent\nfrom forge.components.image_gen import ImageGeneratorComponent\nfrom forge.components.system import SystemComponent\nfrom forge.components.user_interaction import UserInteractionComponent\nfrom forge.components.watchdog import WatchdogComponent\nfrom forge.components.web import WebSearchComponent, WebSeleniumComponent\nfrom forge.file_storage.base import FileStorage\nfrom forge.llm.prompting.schema import ChatPrompt\nfrom forge.llm.prompting.utils import dump_prompt\nfrom forge.llm.providers import (\n    AssistantFunctionCall,\n    ChatMessage,\n    ChatModelResponse,\n    MultiProvider,\n)\nfrom forge.llm.providers.utils import function_specs_from_commands\nfrom forge.models.action import (\n    ActionErrorResult,\n    ActionInterruptedByHuman,\n    ActionResult,\n    ActionSuccessResult,\n)\nfrom forge.models.config import Configurable\nfrom forge.utils.exceptions import (\n    AgentException,\n    AgentTerminated,\n    CommandExecutionError,\n    UnknownCommandError,\n)\nfrom pydantic import Field\n\nfrom .prompt_strategies.one_shot import (\n    OneShotAgentActionProposal,\n    OneShotAgentPromptStrategy,\n)\n\nif TYPE_CHECKING:\n    from autogpt.app.config import AppConfig\n\nlogger = logging.getLogger(__name__)\n\n\nclass AgentConfiguration(BaseAgentConfiguration):\n    pass\n\n\nclass AgentSettings(BaseAgentSettings):\n    config: AgentConfiguration = Field(  # type: ignore\n        default_factory=AgentConfiguration\n    )\n\n    history: EpisodicActionHistory[OneShotAgentActionProposal] = Field(\n        default_factory=EpisodicActionHistory[OneShotAgentActionProposal]\n    )\n    \"\"\"(STATE) The action history of the agent.\"\"\"\n\n    context: AgentContext = Field(default_factory=AgentContext)\n\n\nclass Agent(BaseAgent[OneShotAgentActionProposal], Configurable[AgentSettings]):\n    default_settings: ClassVar[AgentSettings] = AgentSettings(\n        name=\"Agent\",\n        description=__doc__ if __doc__ else \"\",\n    )\n\n    def __init__(\n        self,\n        settings: AgentSettings,\n        llm_provider: MultiProvider,\n        file_storage: FileStorage,\n        app_config: AppConfig,\n    ):\n        super().__init__(settings)\n\n        self.llm_provider = llm_provider\n        prompt_config = OneShotAgentPromptStrategy.default_configuration.copy(deep=True)\n        prompt_config.use_functions_api = (\n            settings.config.use_functions_api\n            # Anthropic currently doesn't support tools + prefilling :(\n            and self.llm.provider_name != \"anthropic\"\n        )\n        self.prompt_strategy = OneShotAgentPromptStrategy(prompt_config, logger)\n        self.commands: list[Command] = []\n\n        # Components\n        self.system = SystemComponent()\n        self.history = ActionHistoryComponent(\n            settings.history,\n            lambda x: self.llm_provider.count_tokens(x, self.llm.name),\n            llm_provider,\n            ActionHistoryConfiguration(\n                model_name=app_config.fast_llm, max_tokens=self.send_token_limit\n            ),\n        ).run_after(WatchdogComponent)\n        if not app_config.noninteractive_mode:\n            self.user_interaction = UserInteractionComponent()\n        self.file_manager = FileManagerComponent(file_storage, settings)\n        self.code_executor = CodeExecutorComponent(\n            self.file_manager.workspace,\n            CodeExecutorConfiguration(\n                docker_container_name=f\"{settings.agent_id}_sandbox\"\n            ),\n        )\n        self.git_ops = GitOperationsComponent()\n        self.image_gen = ImageGeneratorComponent(self.file_manager.workspace)\n        self.web_search = WebSearchComponent()\n        self.web_selenium = WebSeleniumComponent(\n            llm_provider,\n            app_config.app_data_dir,\n        )\n        self.context = ContextComponent(self.file_manager.workspace, settings.context)\n        self.watchdog = WatchdogComponent(settings.config, settings.history).run_after(\n            ContextComponent\n        )\n\n        self.event_history = settings.history\n        self.app_config = app_config\n\n    async def propose_action(self) -> OneShotAgentActionProposal:\n        \"\"\"Proposes the next action to execute, based on the task and current state.\n\n        Returns:\n            The command name and arguments, if any, and the agent's thoughts.\n        \"\"\"\n        self.reset_trace()\n\n        # Get directives\n        resources = await self.run_pipeline(DirectiveProvider.get_resources)\n        constraints = await self.run_pipeline(DirectiveProvider.get_constraints)\n        best_practices = await self.run_pipeline(DirectiveProvider.get_best_practices)\n\n        directives = self.state.directives.copy(deep=True)\n        directives.resources += resources\n        directives.constraints += constraints\n        directives.best_practices += best_practices\n\n        # Get commands\n        self.commands = await self.run_pipeline(CommandProvider.get_commands)\n        self._remove_disabled_commands()\n\n        # Get messages\n        messages = await self.run_pipeline(MessageProvider.get_messages)\n\n        prompt: ChatPrompt = self.prompt_strategy.build_prompt(\n            messages=messages,\n            task=self.state.task,\n            ai_profile=self.state.ai_profile,\n            ai_directives=directives,\n            commands=function_specs_from_commands(self.commands),\n            include_os_info=self.code_executor.config.execute_local_commands,\n        )\n\n        logger.debug(f\"Executing prompt:\\n{dump_prompt(prompt)}\")\n        output = await self.complete_and_parse(prompt)\n        self.config.cycle_count += 1\n\n        return output\n\n    async def complete_and_parse(\n        self, prompt: ChatPrompt, exception: Optional[Exception] = None\n    ) -> OneShotAgentActionProposal:\n        if exception:\n            prompt.messages.append(ChatMessage.system(f\"Error: {exception}\"))\n\n        response: ChatModelResponse[\n            OneShotAgentActionProposal\n        ] = await self.llm_provider.create_chat_completion(\n            prompt.messages,\n            model_name=self.llm.name,\n            completion_parser=self.prompt_strategy.parse_response_content,\n            functions=prompt.functions,\n            prefill_response=prompt.prefill_response,\n        )\n        result = response.parsed_result\n\n        await self.run_pipeline(AfterParse.after_parse, result)\n\n        return result\n\n    async def execute(\n        self,\n        proposal: OneShotAgentActionProposal,\n        user_feedback: str = \"\",\n    ) -> ActionResult:\n        tool = proposal.use_tool\n\n        # Get commands\n        self.commands = await self.run_pipeline(CommandProvider.get_commands)\n        self._remove_disabled_commands()\n\n        try:\n            return_value = await self._execute_tool(tool)\n\n            result = ActionSuccessResult(outputs=return_value)\n        except AgentTerminated:\n            raise\n        except AgentException as e:\n            result = ActionErrorResult.from_exception(e)\n            logger.warning(f\"{tool} raised an error: {e}\")\n            sentry_sdk.capture_exception(e)\n\n        result_tlength = self.llm_provider.count_tokens(str(result), self.llm.name)\n        if result_tlength > self.send_token_limit // 3:\n            result = ActionErrorResult(\n                reason=f\"Command {tool.name} returned too much output. \"\n                \"Do not execute this command again with the same arguments.\"\n            )\n\n        await self.run_pipeline(AfterExecute.after_execute, result)\n\n        logger.debug(\"\\n\".join(self.trace))\n\n        return result\n\n    async def do_not_execute(\n        self, denied_proposal: OneShotAgentActionProposal, user_feedback: str\n    ) -> ActionResult:\n        result = ActionInterruptedByHuman(feedback=user_feedback)\n\n        await self.run_pipeline(AfterExecute.after_execute, result)\n\n        logger.debug(\"\\n\".join(self.trace))\n\n        return result\n\n    async def _execute_tool(self, tool_call: AssistantFunctionCall) -> Any:\n        \"\"\"Execute the command and return the result\n\n        Args:\n            tool_call (AssistantFunctionCall): The tool call to execute\n\n        Returns:\n            str: The execution result\n        \"\"\"\n        # Execute a native command with the same name or alias, if it exists\n        command = self._get_command(tool_call.name)\n        try:\n            result = command(**tool_call.arguments)\n            if inspect.isawaitable(result):\n                return await result\n            return result\n        except AgentException:\n            raise\n        except Exception as e:\n            raise CommandExecutionError(str(e))\n\n    def _get_command(self, command_name: str) -> Command:\n        for command in reversed(self.commands):\n            if command_name in command.names:\n                return command\n\n        raise UnknownCommandError(\n            f\"Cannot execute command '{command_name}': unknown command.\"\n        )\n\n    def _remove_disabled_commands(self) -> None:\n        self.commands = [\n            command\n            for command in self.commands\n            if not any(\n                name in self.app_config.disabled_commands for name in command.names\n            )\n        ]\n\n    def find_obscured_commands(self) -> list[Command]:\n        seen_names = set()\n        obscured_commands = []\n        for command in reversed(self.commands):\n            # If all of the command's names have been seen, it's obscured\n            if seen_names.issuperset(command.names):\n                obscured_commands.append(command)\n            else:\n                seen_names.update(command.names)\n        return list(reversed(obscured_commands))\n", "autogpt/autogpt/agents/__init__.py": "from .agent import Agent\nfrom .agent_manager import AgentManager\nfrom .prompt_strategies.one_shot import OneShotAgentActionProposal\n\n__all__ = [\n    \"AgentManager\",\n    \"Agent\",\n    \"OneShotAgentActionProposal\",\n]\n", "autogpt/autogpt/agents/prompt_strategies/one_shot.py": "from __future__ import annotations\n\nimport json\nimport platform\nimport re\nfrom logging import Logger\n\nimport distro\nfrom forge.config.ai_directives import AIDirectives\nfrom forge.config.ai_profile import AIProfile\nfrom forge.json.parsing import extract_dict_from_json\nfrom forge.llm.prompting import ChatPrompt, LanguageModelClassification, PromptStrategy\nfrom forge.llm.prompting.utils import format_numbered_list\nfrom forge.llm.providers.schema import (\n    AssistantChatMessage,\n    ChatMessage,\n    CompletionModelFunction,\n)\nfrom forge.models.action import ActionProposal\nfrom forge.models.config import SystemConfiguration, UserConfigurable\nfrom forge.models.json_schema import JSONSchema\nfrom forge.models.utils import ModelWithSummary\nfrom forge.utils.exceptions import InvalidAgentResponseError\nfrom pydantic import Field\n\n_RESPONSE_INTERFACE_NAME = \"AssistantResponse\"\n\n\nclass AssistantThoughts(ModelWithSummary):\n    observations: str = Field(\n        ..., description=\"Relevant observations from your last action (if any)\"\n    )\n    text: str = Field(..., description=\"Thoughts\")\n    reasoning: str = Field(..., description=\"Reasoning behind the thoughts\")\n    self_criticism: str = Field(..., description=\"Constructive self-criticism\")\n    plan: list[str] = Field(\n        ..., description=\"Short list that conveys the long-term plan\"\n    )\n    speak: str = Field(..., description=\"Summary of thoughts, to say to user\")\n\n    def summary(self) -> str:\n        return self.text\n\n\nclass OneShotAgentActionProposal(ActionProposal):\n    thoughts: AssistantThoughts  # type: ignore\n\n\nclass OneShotAgentPromptConfiguration(SystemConfiguration):\n    DEFAULT_BODY_TEMPLATE: str = (\n        \"## Constraints\\n\"\n        \"You operate within the following constraints:\\n\"\n        \"{constraints}\\n\"\n        \"\\n\"\n        \"## Resources\\n\"\n        \"You can leverage access to the following resources:\\n\"\n        \"{resources}\\n\"\n        \"\\n\"\n        \"## Commands\\n\"\n        \"These are the ONLY commands you can use.\"\n        \" Any action you perform must be possible through one of these commands:\\n\"\n        \"{commands}\\n\"\n        \"\\n\"\n        \"## Best practices\\n\"\n        \"{best_practices}\"\n    )\n\n    DEFAULT_CHOOSE_ACTION_INSTRUCTION: str = (\n        \"Determine exactly one command to use next based on the given goals \"\n        \"and the progress you have made so far, \"\n        \"and respond using the JSON schema specified previously:\"\n    )\n\n    body_template: str = UserConfigurable(default=DEFAULT_BODY_TEMPLATE)\n    choose_action_instruction: str = UserConfigurable(\n        default=DEFAULT_CHOOSE_ACTION_INSTRUCTION\n    )\n    use_functions_api: bool = UserConfigurable(default=False)\n\n    #########\n    # State #\n    #########\n    # progress_summaries: dict[tuple[int, int], str] = Field(\n    #     default_factory=lambda: {(0, 0): \"\"}\n    # )\n\n\nclass OneShotAgentPromptStrategy(PromptStrategy):\n    default_configuration: OneShotAgentPromptConfiguration = (\n        OneShotAgentPromptConfiguration()\n    )\n\n    def __init__(\n        self,\n        configuration: OneShotAgentPromptConfiguration,\n        logger: Logger,\n    ):\n        self.config = configuration\n        self.response_schema = JSONSchema.from_dict(OneShotAgentActionProposal.schema())\n        self.logger = logger\n\n    @property\n    def model_classification(self) -> LanguageModelClassification:\n        return LanguageModelClassification.FAST_MODEL  # FIXME: dynamic switching\n\n    def build_prompt(\n        self,\n        *,\n        messages: list[ChatMessage],\n        task: str,\n        ai_profile: AIProfile,\n        ai_directives: AIDirectives,\n        commands: list[CompletionModelFunction],\n        include_os_info: bool,\n        **extras,\n    ) -> ChatPrompt:\n        \"\"\"Constructs and returns a prompt with the following structure:\n        1. System prompt\n        3. `cycle_instruction`\n        \"\"\"\n        system_prompt, response_prefill = self.build_system_prompt(\n            ai_profile=ai_profile,\n            ai_directives=ai_directives,\n            commands=commands,\n            include_os_info=include_os_info,\n        )\n\n        final_instruction_msg = ChatMessage.user(self.config.choose_action_instruction)\n\n        return ChatPrompt(\n            messages=[\n                ChatMessage.system(system_prompt),\n                ChatMessage.user(f'\"\"\"{task}\"\"\"'),\n                *messages,\n                final_instruction_msg,\n            ],\n            prefill_response=response_prefill,\n            functions=commands if self.config.use_functions_api else [],\n        )\n\n    def build_system_prompt(\n        self,\n        ai_profile: AIProfile,\n        ai_directives: AIDirectives,\n        commands: list[CompletionModelFunction],\n        include_os_info: bool,\n    ) -> tuple[str, str]:\n        \"\"\"\n        Builds the system prompt.\n\n        Returns:\n            str: The system prompt body\n            str: The desired start for the LLM's response; used to steer the output\n        \"\"\"\n        response_fmt_instruction, response_prefill = self.response_format_instruction(\n            self.config.use_functions_api\n        )\n        system_prompt_parts = (\n            self._generate_intro_prompt(ai_profile)\n            + (self._generate_os_info() if include_os_info else [])\n            + [\n                self.config.body_template.format(\n                    constraints=format_numbered_list(ai_directives.constraints),\n                    resources=format_numbered_list(ai_directives.resources),\n                    commands=self._generate_commands_list(commands),\n                    best_practices=format_numbered_list(ai_directives.best_practices),\n                )\n            ]\n            + [\n                \"## Your Task\\n\"\n                \"The user will specify a task for you to execute, in triple quotes,\"\n                \" in the next message. Your job is to complete the task while following\"\n                \" your directives as given above, and terminate when your task is done.\"\n            ]\n            + [\"## RESPONSE FORMAT\\n\" + response_fmt_instruction]\n        )\n\n        # Join non-empty parts together into paragraph format\n        return (\n            \"\\n\\n\".join(filter(None, system_prompt_parts)).strip(\"\\n\"),\n            response_prefill,\n        )\n\n    def response_format_instruction(self, use_functions_api: bool) -> tuple[str, str]:\n        response_schema = self.response_schema.copy(deep=True)\n        assert response_schema.properties\n        if use_functions_api and \"use_tool\" in response_schema.properties:\n            del response_schema.properties[\"use_tool\"]\n\n        # Unindent for performance\n        response_format = re.sub(\n            r\"\\n\\s+\",\n            \"\\n\",\n            response_schema.to_typescript_object_interface(_RESPONSE_INTERFACE_NAME),\n        )\n        response_prefill = f'{{\\n    \"{list(response_schema.properties.keys())[0]}\":'\n\n        return (\n            (\n                f\"YOU MUST ALWAYS RESPOND WITH A JSON OBJECT OF THE FOLLOWING TYPE:\\n\"\n                f\"{response_format}\"\n                + (\"\\n\\nYOU MUST ALSO INVOKE A TOOL!\" if use_functions_api else \"\")\n            ),\n            response_prefill,\n        )\n\n    def _generate_intro_prompt(self, ai_profile: AIProfile) -> list[str]:\n        \"\"\"Generates the introduction part of the prompt.\n\n        Returns:\n            list[str]: A list of strings forming the introduction part of the prompt.\n        \"\"\"\n        return [\n            f\"You are {ai_profile.ai_name}, {ai_profile.ai_role.rstrip('.')}.\",\n            \"Your decisions must always be made independently without seeking \"\n            \"user assistance. Play to your strengths as an LLM and pursue \"\n            \"simple strategies with no legal complications.\",\n        ]\n\n    def _generate_os_info(self) -> list[str]:\n        \"\"\"Generates the OS information part of the prompt.\n\n        Params:\n            config (Config): The configuration object.\n\n        Returns:\n            str: The OS information part of the prompt.\n        \"\"\"\n        os_name = platform.system()\n        os_info = (\n            platform.platform(terse=True)\n            if os_name != \"Linux\"\n            else distro.name(pretty=True)\n        )\n        return [f\"The OS you are running on is: {os_info}\"]\n\n    def _generate_commands_list(self, commands: list[CompletionModelFunction]) -> str:\n        \"\"\"Lists the commands available to the agent.\n\n        Params:\n            agent: The agent for which the commands are being listed.\n\n        Returns:\n            str: A string containing a numbered list of commands.\n        \"\"\"\n        try:\n            return format_numbered_list([cmd.fmt_line() for cmd in commands])\n        except AttributeError:\n            self.logger.warning(f\"Formatting commands failed. {commands}\")\n            raise\n\n    def parse_response_content(\n        self,\n        response: AssistantChatMessage,\n    ) -> OneShotAgentActionProposal:\n        if not response.content:\n            raise InvalidAgentResponseError(\"Assistant response has no text content\")\n\n        self.logger.debug(\n            \"LLM response content:\"\n            + (\n                f\"\\n{response.content}\"\n                if \"\\n\" in response.content\n                else f\" '{response.content}'\"\n            )\n        )\n        assistant_reply_dict = extract_dict_from_json(response.content)\n        self.logger.debug(\n            \"Parsing object extracted from LLM response:\\n\"\n            f\"{json.dumps(assistant_reply_dict, indent=4)}\"\n        )\n        if self.config.use_functions_api:\n            if not response.tool_calls:\n                raise InvalidAgentResponseError(\"Assistant did not use a tool\")\n            assistant_reply_dict[\"use_tool\"] = response.tool_calls[0].function\n\n        parsed_response = OneShotAgentActionProposal.parse_obj(assistant_reply_dict)\n        return parsed_response\n", "autogpt/autogpt/agent_factory/configurators.py": "from typing import Optional\n\nfrom forge.config.ai_directives import AIDirectives\nfrom forge.config.ai_profile import AIProfile\nfrom forge.file_storage.base import FileStorage\nfrom forge.llm.providers import MultiProvider\n\nfrom autogpt.agents.agent import Agent, AgentConfiguration, AgentSettings\nfrom autogpt.app.config import AppConfig\n\n\ndef create_agent(\n    agent_id: str,\n    task: str,\n    app_config: AppConfig,\n    file_storage: FileStorage,\n    llm_provider: MultiProvider,\n    ai_profile: Optional[AIProfile] = None,\n    directives: Optional[AIDirectives] = None,\n) -> Agent:\n    if not task:\n        raise ValueError(\"No task specified for new agent\")\n    ai_profile = ai_profile or AIProfile()\n    directives = directives or AIDirectives()\n\n    agent = _configure_agent(\n        agent_id=agent_id,\n        task=task,\n        ai_profile=ai_profile,\n        directives=directives,\n        app_config=app_config,\n        file_storage=file_storage,\n        llm_provider=llm_provider,\n    )\n\n    return agent\n\n\ndef configure_agent_with_state(\n    state: AgentSettings,\n    app_config: AppConfig,\n    file_storage: FileStorage,\n    llm_provider: MultiProvider,\n) -> Agent:\n    return _configure_agent(\n        state=state,\n        app_config=app_config,\n        file_storage=file_storage,\n        llm_provider=llm_provider,\n    )\n\n\ndef _configure_agent(\n    app_config: AppConfig,\n    llm_provider: MultiProvider,\n    file_storage: FileStorage,\n    agent_id: str = \"\",\n    task: str = \"\",\n    ai_profile: Optional[AIProfile] = None,\n    directives: Optional[AIDirectives] = None,\n    state: Optional[AgentSettings] = None,\n) -> Agent:\n    if state:\n        agent_state = state\n    elif agent_id and task and ai_profile and directives:\n        agent_state = state or create_agent_state(\n            agent_id=agent_id,\n            task=task,\n            ai_profile=ai_profile,\n            directives=directives,\n            app_config=app_config,\n        )\n    else:\n        raise TypeError(\n            \"Either (state) or (agent_id, task, ai_profile, directives)\"\n            \" must be specified\"\n        )\n\n    return Agent(\n        settings=agent_state,\n        llm_provider=llm_provider,\n        file_storage=file_storage,\n        app_config=app_config,\n    )\n\n\ndef create_agent_state(\n    agent_id: str,\n    task: str,\n    ai_profile: AIProfile,\n    directives: AIDirectives,\n    app_config: AppConfig,\n) -> AgentSettings:\n    return AgentSettings(\n        agent_id=agent_id,\n        name=Agent.default_settings.name,\n        description=Agent.default_settings.description,\n        task=task,\n        ai_profile=ai_profile,\n        directives=directives,\n        config=AgentConfiguration(\n            fast_llm=app_config.fast_llm,\n            smart_llm=app_config.smart_llm,\n            allow_fs_access=not app_config.restrict_to_workspace,\n            use_functions_api=app_config.openai_functions,\n        ),\n        history=Agent.default_settings.history.copy(deep=True),\n    )\n", "autogpt/autogpt/agent_factory/generators.py": "from __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nfrom forge.file_storage.base import FileStorage\n\nif TYPE_CHECKING:\n    from autogpt.agents.agent import Agent\n    from autogpt.app.config import AppConfig\n    from forge.llm.providers import MultiProvider\n\nfrom .configurators import _configure_agent\nfrom .profile_generator import generate_agent_profile_for_task\n\n\nasync def generate_agent_for_task(\n    agent_id: str,\n    task: str,\n    app_config: AppConfig,\n    file_storage: FileStorage,\n    llm_provider: MultiProvider,\n) -> Agent:\n    ai_profile, task_directives = await generate_agent_profile_for_task(\n        task=task,\n        app_config=app_config,\n        llm_provider=llm_provider,\n    )\n    return _configure_agent(\n        agent_id=agent_id,\n        task=task,\n        ai_profile=ai_profile,\n        directives=task_directives,\n        app_config=app_config,\n        file_storage=file_storage,\n        llm_provider=llm_provider,\n    )\n", "autogpt/autogpt/agent_factory/profile_generator.py": "import json\nimport logging\n\nfrom forge.config.ai_directives import AIDirectives\nfrom forge.config.ai_profile import AIProfile\nfrom forge.llm.prompting import ChatPrompt, LanguageModelClassification, PromptStrategy\nfrom forge.llm.providers import MultiProvider\nfrom forge.llm.providers.schema import (\n    AssistantChatMessage,\n    ChatMessage,\n    CompletionModelFunction,\n)\nfrom forge.models.config import SystemConfiguration, UserConfigurable\nfrom forge.models.json_schema import JSONSchema\n\nfrom autogpt.app.config import AppConfig\n\nlogger = logging.getLogger(__name__)\n\n\nclass AgentProfileGeneratorConfiguration(SystemConfiguration):\n    model_classification: LanguageModelClassification = UserConfigurable(\n        default=LanguageModelClassification.SMART_MODEL\n    )\n    _example_call: object = {\n        \"name\": \"create_agent\",\n        \"arguments\": {\n            \"name\": \"CMOGPT\",\n            \"description\": (\n                \"a professional digital marketer AI that assists Solopreneurs \"\n                \"in growing their businesses by providing \"\n                \"world-class expertise in solving marketing problems \"\n                \"for SaaS, content products, agencies, and more.\"\n            ),\n            \"directives\": {\n                \"best_practices\": [\n                    (\n                        \"Engage in effective problem-solving, prioritization, \"\n                        \"planning, and supporting execution to address your \"\n                        \"marketing needs as your virtual \"\n                        \"Chief Marketing Officer.\"\n                    ),\n                    (\n                        \"Provide specific, actionable, and concise advice to \"\n                        \"help you make informed decisions without the use of \"\n                        \"platitudes or overly wordy explanations.\"\n                    ),\n                    (\n                        \"Identify and prioritize quick wins and cost-effective \"\n                        \"campaigns that maximize results with minimal time and \"\n                        \"budget investment.\"\n                    ),\n                    (\n                        \"Proactively take the lead in guiding you and offering \"\n                        \"suggestions when faced with unclear information or \"\n                        \"uncertainty to ensure your marketing strategy remains \"\n                        \"on track.\"\n                    ),\n                ],\n                \"constraints\": [\n                    \"Do not suggest illegal or unethical plans or strategies.\",\n                    \"Take reasonable budgetary limits into account.\",\n                ],\n            },\n        },\n    }\n    system_prompt: str = UserConfigurable(\n        default=(\n            \"Your job is to respond to a user-defined task, given in triple quotes, by \"\n            \"invoking the `create_agent` function to generate an autonomous agent to \"\n            \"complete the task. \"\n            \"You should supply a role-based name for the agent (_GPT), \"\n            \"an informative description for what the agent does, and 1 to 5 directives \"\n            \"in each of the categories Best Practices and Constraints, \"\n            \"that are optimally aligned with the successful completion \"\n            \"of its assigned task.\\n\"\n            \"\\n\"\n            \"Example Input:\\n\"\n            '\"\"\"Help me with marketing my business\"\"\"\\n\\n'\n            \"Example Call:\\n\"\n            \"```\\n\"\n            f\"{json.dumps(_example_call, indent=4)}\"\n            \"\\n```\"\n        )\n    )\n    user_prompt_template: str = UserConfigurable(default='\"\"\"{user_objective}\"\"\"')\n    create_agent_function: dict = UserConfigurable(\n        default=CompletionModelFunction(\n            name=\"create_agent\",\n            description=\"Create a new autonomous AI agent to complete a given task.\",\n            parameters={\n                \"name\": JSONSchema(\n                    type=JSONSchema.Type.STRING,\n                    description=\"A short role-based name for an autonomous agent.\",\n                    required=True,\n                ),\n                \"description\": JSONSchema(\n                    type=JSONSchema.Type.STRING,\n                    description=(\n                        \"An informative one sentence description \"\n                        \"of what the AI agent does\"\n                    ),\n                    required=True,\n                ),\n                \"directives\": JSONSchema(\n                    type=JSONSchema.Type.OBJECT,\n                    properties={\n                        \"best_practices\": JSONSchema(\n                            type=JSONSchema.Type.ARRAY,\n                            minItems=1,\n                            maxItems=5,\n                            items=JSONSchema(\n                                type=JSONSchema.Type.STRING,\n                            ),\n                            description=(\n                                \"One to five highly effective best practices \"\n                                \"that are optimally aligned with the completion \"\n                                \"of the given task\"\n                            ),\n                            required=True,\n                        ),\n                        \"constraints\": JSONSchema(\n                            type=JSONSchema.Type.ARRAY,\n                            minItems=1,\n                            maxItems=5,\n                            items=JSONSchema(\n                                type=JSONSchema.Type.STRING,\n                            ),\n                            description=(\n                                \"One to five reasonable and efficacious constraints \"\n                                \"that are optimally aligned with the completion \"\n                                \"of the given task\"\n                            ),\n                            required=True,\n                        ),\n                    },\n                    required=True,\n                ),\n            },\n        ).dict()\n    )\n\n\nclass AgentProfileGenerator(PromptStrategy):\n    default_configuration: AgentProfileGeneratorConfiguration = (\n        AgentProfileGeneratorConfiguration()\n    )\n\n    def __init__(\n        self,\n        model_classification: LanguageModelClassification,\n        system_prompt: str,\n        user_prompt_template: str,\n        create_agent_function: dict,\n    ):\n        self._model_classification = model_classification\n        self._system_prompt_message = system_prompt\n        self._user_prompt_template = user_prompt_template\n        self._create_agent_function = CompletionModelFunction.parse_obj(\n            create_agent_function\n        )\n\n    @property\n    def model_classification(self) -> LanguageModelClassification:\n        return self._model_classification\n\n    def build_prompt(self, user_objective: str = \"\", **kwargs) -> ChatPrompt:\n        system_message = ChatMessage.system(self._system_prompt_message)\n        user_message = ChatMessage.user(\n            self._user_prompt_template.format(\n                user_objective=user_objective,\n            )\n        )\n        prompt = ChatPrompt(\n            messages=[system_message, user_message],\n            functions=[self._create_agent_function],\n        )\n        return prompt\n\n    def parse_response_content(\n        self,\n        response: AssistantChatMessage,\n    ) -> tuple[AIProfile, AIDirectives]:\n        \"\"\"Parse the actual text response from the objective model.\n\n        Args:\n            response_content: The raw response content from the objective model.\n\n        Returns:\n            The parsed response.\n        \"\"\"\n        try:\n            if not response.tool_calls:\n                raise ValueError(\n                    f\"LLM did not call {self._create_agent_function.name} function; \"\n                    \"agent profile creation failed\"\n                )\n            arguments: object = response.tool_calls[0].function.arguments\n            ai_profile = AIProfile(\n                ai_name=arguments.get(\"name\"),  # type: ignore\n                ai_role=arguments.get(\"description\"),  # type: ignore\n            )\n            ai_directives = AIDirectives(\n                best_practices=arguments.get(\"directives\", {}).get(\"best_practices\"),\n                constraints=arguments.get(\"directives\", {}).get(\"constraints\"),\n                resources=[],\n            )\n        except KeyError:\n            logger.debug(f\"Failed to parse this response content: {response}\")\n            raise\n        return ai_profile, ai_directives\n\n\nasync def generate_agent_profile_for_task(\n    task: str,\n    app_config: AppConfig,\n    llm_provider: MultiProvider,\n) -> tuple[AIProfile, AIDirectives]:\n    \"\"\"Generates an AIConfig object from the given string.\n\n    Returns:\n    AIConfig: The AIConfig object tailored to the user's input\n    \"\"\"\n    agent_profile_generator = AgentProfileGenerator(\n        **AgentProfileGenerator.default_configuration.dict()  # HACK\n    )\n\n    prompt = agent_profile_generator.build_prompt(task)\n\n    # Call LLM with the string as user input\n    output = await llm_provider.create_chat_completion(\n        prompt.messages,\n        model_name=app_config.smart_llm,\n        functions=prompt.functions,\n        completion_parser=agent_profile_generator.parse_response_content,\n    )\n\n    # Debug LLM Output\n    logger.debug(f\"AI Config Generator Raw Output: {output.response}\")\n\n    return output.parsed_result\n", "autogpt/autogpt/app/agent_protocol_server.py": "import logging\nimport os\nimport pathlib\nfrom collections import defaultdict\nfrom io import BytesIO\nfrom uuid import uuid4\n\nimport orjson\nfrom fastapi import APIRouter, FastAPI, UploadFile\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import RedirectResponse, StreamingResponse\nfrom fastapi.staticfiles import StaticFiles\nfrom forge.agent_protocol.api_router import base_router\nfrom forge.agent_protocol.database import AgentDB\nfrom forge.agent_protocol.middlewares import AgentMiddleware\nfrom forge.agent_protocol.models import (\n    Artifact,\n    Step,\n    StepRequestBody,\n    Task,\n    TaskArtifactsListResponse,\n    TaskListResponse,\n    TaskRequestBody,\n    TaskStepsListResponse,\n)\nfrom forge.file_storage import FileStorage\nfrom forge.llm.providers import ModelProviderBudget, MultiProvider\nfrom forge.models.action import ActionErrorResult, ActionSuccessResult\nfrom forge.utils.const import ASK_COMMAND, FINISH_COMMAND\nfrom forge.utils.exceptions import AgentFinished, NotFoundError\nfrom hypercorn.asyncio import serve as hypercorn_serve\nfrom hypercorn.config import Config as HypercornConfig\nfrom sentry_sdk import set_user\n\nfrom autogpt.agent_factory.configurators import configure_agent_with_state, create_agent\nfrom autogpt.agents.agent_manager import AgentManager\nfrom autogpt.app.config import AppConfig\nfrom autogpt.app.utils import is_port_free\n\nlogger = logging.getLogger(__name__)\n\n\nclass AgentProtocolServer:\n    _task_budgets: dict[str, ModelProviderBudget]\n\n    def __init__(\n        self,\n        app_config: AppConfig,\n        database: AgentDB,\n        file_storage: FileStorage,\n        llm_provider: MultiProvider,\n    ):\n        self.app_config = app_config\n        self.db = database\n        self.file_storage = file_storage\n        self.llm_provider = llm_provider\n        self.agent_manager = AgentManager(file_storage)\n        self._task_budgets = defaultdict(ModelProviderBudget)\n\n    async def start(self, port: int = 8000, router: APIRouter = base_router):\n        \"\"\"Start the agent server.\"\"\"\n        logger.debug(\"Starting the agent server...\")\n        if not is_port_free(port):\n            logger.error(f\"Port {port} is already in use.\")\n            logger.info(\n                \"You can specify a port by either setting the AP_SERVER_PORT \"\n                \"environment variable or defining AP_SERVER_PORT in the .env file.\"\n            )\n            return\n\n        config = HypercornConfig()\n        config.bind = [f\"localhost:{port}\"]\n        app = FastAPI(\n            title=\"AutoGPT Server\",\n            description=\"Forked from AutoGPT Forge; \"\n            \"Modified version of The Agent Protocol.\",\n            version=\"v0.4\",\n        )\n\n        # Configure CORS middleware\n        default_origins = [f\"http://localhost:{port}\"]  # Default only local access\n        configured_origins = [\n            origin\n            for origin in os.getenv(\"AP_SERVER_CORS_ALLOWED_ORIGINS\", \"\").split(\",\")\n            if origin  # Empty list if not configured\n        ]\n        origins = configured_origins or default_origins\n\n        app.add_middleware(\n            CORSMiddleware,\n            allow_origins=origins,\n            allow_credentials=True,\n            allow_methods=[\"*\"],\n            allow_headers=[\"*\"],\n        )\n\n        app.include_router(router, prefix=\"/ap/v1\")\n        script_dir = os.path.dirname(os.path.realpath(__file__))\n        frontend_path = (\n            pathlib.Path(script_dir).joinpath(\"../../../frontend/build/web\").resolve()\n        )\n\n        if os.path.exists(frontend_path):\n            app.mount(\"/app\", StaticFiles(directory=frontend_path), name=\"app\")\n\n            @app.get(\"/\", include_in_schema=False)\n            async def root():\n                return RedirectResponse(url=\"/app/index.html\", status_code=307)\n\n        else:\n            logger.warning(\n                f\"Frontend not found. {frontend_path} does not exist. \"\n                \"The frontend will not be available.\"\n            )\n\n        # Used to access the methods on this class from API route handlers\n        app.add_middleware(AgentMiddleware, agent=self)\n\n        config.loglevel = \"ERROR\"\n        config.bind = [f\"0.0.0.0:{port}\"]\n\n        logger.info(f\"AutoGPT server starting on http://localhost:{port}\")\n        await hypercorn_serve(app, config)  # type: ignore\n\n    async def create_task(self, task_request: TaskRequestBody) -> Task:\n        \"\"\"\n        Create a task for the agent.\n        \"\"\"\n        if user_id := (task_request.additional_input or {}).get(\"user_id\"):\n            set_user({\"id\": user_id})\n\n        task = await self.db.create_task(\n            input=task_request.input,\n            additional_input=task_request.additional_input,\n        )\n        # TODO: re-evaluate performance benefit of task-oriented profiles\n        # logger.debug(f\"Creating agent for task: '{task.input}'\")\n        # task_agent = await generate_agent_for_task(\n        task_agent = create_agent(\n            agent_id=task_agent_id(task.task_id),\n            task=task.input,\n            app_config=self.app_config,\n            file_storage=self.file_storage,\n            llm_provider=self._get_task_llm_provider(task),\n        )\n        await task_agent.file_manager.save_state()\n\n        return task\n\n    async def list_tasks(self, page: int = 1, pageSize: int = 10) -> TaskListResponse:\n        \"\"\"\n        List all tasks that the agent has created.\n        \"\"\"\n        logger.debug(\"Listing all tasks...\")\n        tasks, pagination = await self.db.list_tasks(page, pageSize)\n        response = TaskListResponse(tasks=tasks, pagination=pagination)\n        return response\n\n    async def get_task(self, task_id: str) -> Task:\n        \"\"\"\n        Get a task by ID.\n        \"\"\"\n        logger.debug(f\"Getting task with ID: {task_id}...\")\n        task = await self.db.get_task(task_id)\n        return task\n\n    async def list_steps(\n        self, task_id: str, page: int = 1, pageSize: int = 10\n    ) -> TaskStepsListResponse:\n        \"\"\"\n        List the IDs of all steps that the task has created.\n        \"\"\"\n        logger.debug(f\"Listing all steps created by task with ID: {task_id}...\")\n        steps, pagination = await self.db.list_steps(task_id, page, pageSize)\n        response = TaskStepsListResponse(steps=steps, pagination=pagination)\n        return response\n\n    async def execute_step(self, task_id: str, step_request: StepRequestBody) -> Step:\n        \"\"\"Create a step for the task.\"\"\"\n        logger.debug(f\"Creating a step for task with ID: {task_id}...\")\n\n        # Restore Agent instance\n        task = await self.get_task(task_id)\n        agent = configure_agent_with_state(\n            state=self.agent_manager.load_agent_state(task_agent_id(task_id)),\n            app_config=self.app_config,\n            file_storage=self.file_storage,\n            llm_provider=self._get_task_llm_provider(task),\n        )\n\n        if user_id := (task.additional_input or {}).get(\"user_id\"):\n            set_user({\"id\": user_id})\n\n        # According to the Agent Protocol spec, the first execute_step request contains\n        #  the same task input as the parent create_task request.\n        # To prevent this from interfering with the agent's process, we ignore the input\n        #  of this first step request, and just generate the first step proposal.\n        is_init_step = not bool(agent.event_history)\n        last_proposal, tool_result = None, None\n        execute_approved = False\n\n        # HACK: only for compatibility with AGBenchmark\n        if step_request.input == \"y\":\n            step_request.input = \"\"\n\n        user_input = step_request.input if not is_init_step else \"\"\n\n        if (\n            not is_init_step\n            and agent.event_history.current_episode\n            and not agent.event_history.current_episode.result\n        ):\n            last_proposal = agent.event_history.current_episode.action\n            execute_approved = not user_input\n\n            logger.debug(\n                f\"Agent proposed command {last_proposal.use_tool}.\"\n                f\" User input/feedback: {repr(user_input)}\"\n            )\n\n        # Save step request\n        step = await self.db.create_step(\n            task_id=task_id,\n            input=step_request,\n            is_last=(\n                last_proposal is not None\n                and last_proposal.use_tool.name == FINISH_COMMAND\n                and execute_approved\n            ),\n        )\n        agent.llm_provider = self._get_task_llm_provider(task, step.step_id)\n\n        # Execute previously proposed action\n        if last_proposal:\n            agent.file_manager.workspace.on_write_file = (\n                lambda path: self._on_agent_write_file(\n                    task=task, step=step, relative_path=path\n                )\n            )\n\n            if last_proposal.use_tool.name == ASK_COMMAND:\n                tool_result = ActionSuccessResult(outputs=user_input)\n                agent.event_history.register_result(tool_result)\n            elif execute_approved:\n                step = await self.db.update_step(\n                    task_id=task_id,\n                    step_id=step.step_id,\n                    status=\"running\",\n                )\n\n                try:\n                    # Execute previously proposed action\n                    tool_result = await agent.execute(last_proposal)\n                except AgentFinished:\n                    additional_output = {}\n                    task_total_cost = agent.llm_provider.get_incurred_cost()\n                    if task_total_cost > 0:\n                        additional_output[\"task_total_cost\"] = task_total_cost\n                        logger.info(\n                            f\"Total LLM cost for task {task_id}: \"\n                            f\"${round(task_total_cost, 2)}\"\n                        )\n\n                    step = await self.db.update_step(\n                        task_id=task_id,\n                        step_id=step.step_id,\n                        output=last_proposal.use_tool.arguments[\"reason\"],\n                        additional_output=additional_output,\n                    )\n                    await agent.file_manager.save_state()\n                    return step\n            else:\n                assert user_input\n                tool_result = await agent.do_not_execute(last_proposal, user_input)\n\n        # Propose next action\n        try:\n            assistant_response = await agent.propose_action()\n            next_tool_to_use = assistant_response.use_tool\n            logger.debug(f\"AI output: {assistant_response.thoughts}\")\n        except Exception as e:\n            step = await self.db.update_step(\n                task_id=task_id,\n                step_id=step.step_id,\n                status=\"completed\",\n                output=f\"An error occurred while proposing the next action: {e}\",\n            )\n            return step\n\n        # Format step output\n        output = (\n            (\n                f\"`{last_proposal.use_tool}` returned:\"\n                + (\"\\n\\n\" if \"\\n\" in str(tool_result) else \" \")\n                + f\"{tool_result}\\n\\n\"\n            )\n            if last_proposal and last_proposal.use_tool.name != ASK_COMMAND\n            else \"\"\n        )\n        output += f\"{assistant_response.thoughts.speak}\\n\\n\"\n        output += (\n            f\"Next Command: {next_tool_to_use}\"\n            if next_tool_to_use.name != ASK_COMMAND\n            else next_tool_to_use.arguments[\"question\"]\n        )\n\n        additional_output = {\n            **(\n                {\n                    \"last_action\": {\n                        \"name\": last_proposal.use_tool.name,\n                        \"args\": last_proposal.use_tool.arguments,\n                        \"result\": (\n                            \"\"\n                            if tool_result is None\n                            else (\n                                orjson.loads(tool_result.json())\n                                if not isinstance(tool_result, ActionErrorResult)\n                                else {\n                                    \"error\": str(tool_result.error),\n                                    \"reason\": tool_result.reason,\n                                }\n                            )\n                        ),\n                    },\n                }\n                if last_proposal and tool_result\n                else {}\n            ),\n            **assistant_response.dict(),\n        }\n\n        task_cumulative_cost = agent.llm_provider.get_incurred_cost()\n        if task_cumulative_cost > 0:\n            additional_output[\"task_cumulative_cost\"] = task_cumulative_cost\n        logger.debug(\n            f\"Running total LLM cost for task {task_id}: \"\n            f\"${round(task_cumulative_cost, 3)}\"\n        )\n\n        step = await self.db.update_step(\n            task_id=task_id,\n            step_id=step.step_id,\n            status=\"completed\",\n            output=output,\n            additional_output=additional_output,\n        )\n\n        await agent.file_manager.save_state()\n        return step\n\n    async def _on_agent_write_file(\n        self, task: Task, step: Step, relative_path: pathlib.Path\n    ) -> None:\n        \"\"\"\n        Creates an Artifact for the written file, or updates the Artifact if it exists.\n        \"\"\"\n        if relative_path.is_absolute():\n            raise ValueError(f\"File path '{relative_path}' is not relative\")\n        for a in task.artifacts or []:\n            if a.relative_path == str(relative_path):\n                logger.debug(f\"Updating Artifact after writing to existing file: {a}\")\n                if not a.agent_created:\n                    await self.db.update_artifact(a.artifact_id, agent_created=True)\n                break\n        else:\n            logger.debug(f\"Creating Artifact for new file '{relative_path}'\")\n            await self.db.create_artifact(\n                task_id=step.task_id,\n                step_id=step.step_id,\n                file_name=relative_path.parts[-1],\n                agent_created=True,\n                relative_path=str(relative_path),\n            )\n\n    async def get_step(self, task_id: str, step_id: str) -> Step:\n        \"\"\"\n        Get a step by ID.\n        \"\"\"\n        step = await self.db.get_step(task_id, step_id)\n        return step\n\n    async def list_artifacts(\n        self, task_id: str, page: int = 1, pageSize: int = 10\n    ) -> TaskArtifactsListResponse:\n        \"\"\"\n        List the artifacts that the task has created.\n        \"\"\"\n        artifacts, pagination = await self.db.list_artifacts(task_id, page, pageSize)\n        return TaskArtifactsListResponse(artifacts=artifacts, pagination=pagination)\n\n    async def create_artifact(\n        self, task_id: str, file: UploadFile, relative_path: str\n    ) -> Artifact:\n        \"\"\"\n        Create an artifact for the task.\n        \"\"\"\n        file_name = file.filename or str(uuid4())\n        data = b\"\"\n        while contents := file.file.read(1024 * 1024):\n            data += contents\n        # Check if relative path ends with filename\n        if relative_path.endswith(file_name):\n            file_path = relative_path\n        else:\n            file_path = os.path.join(relative_path, file_name)\n\n        workspace = self._get_task_agent_file_workspace(task_id)\n        await workspace.write_file(file_path, data)\n\n        artifact = await self.db.create_artifact(\n            task_id=task_id,\n            file_name=file_name,\n            relative_path=relative_path,\n            agent_created=False,\n        )\n        return artifact\n\n    async def get_artifact(self, task_id: str, artifact_id: str) -> StreamingResponse:\n        \"\"\"\n        Download a task artifact by ID.\n        \"\"\"\n        try:\n            workspace = self._get_task_agent_file_workspace(task_id)\n            artifact = await self.db.get_artifact(artifact_id)\n            if artifact.file_name not in artifact.relative_path:\n                file_path = os.path.join(artifact.relative_path, artifact.file_name)\n            else:\n                file_path = artifact.relative_path\n            retrieved_artifact = workspace.read_file(file_path, binary=True)\n        except NotFoundError:\n            raise\n        except FileNotFoundError:\n            raise\n\n        return StreamingResponse(\n            BytesIO(retrieved_artifact),\n            media_type=\"application/octet-stream\",\n            headers={\n                \"Content-Disposition\": f'attachment; filename=\"{artifact.file_name}\"'\n            },\n        )\n\n    def _get_task_agent_file_workspace(self, task_id: str | int) -> FileStorage:\n        agent_id = task_agent_id(task_id)\n        return self.file_storage.clone_with_subroot(f\"agents/{agent_id}/workspace\")\n\n    def _get_task_llm_provider(self, task: Task, step_id: str = \"\") -> MultiProvider:\n        \"\"\"\n        Configures the LLM provider with headers to link outgoing requests to the task.\n        \"\"\"\n        task_llm_budget = self._task_budgets[task.task_id]\n\n        task_llm_provider_config = self.llm_provider._configuration.copy(deep=True)\n        _extra_request_headers = task_llm_provider_config.extra_request_headers\n        _extra_request_headers[\"AP-TaskID\"] = task.task_id\n        if step_id:\n            _extra_request_headers[\"AP-StepID\"] = step_id\n        if task.additional_input and (user_id := task.additional_input.get(\"user_id\")):\n            _extra_request_headers[\"AutoGPT-UserID\"] = user_id\n\n        settings = self.llm_provider._settings.copy()\n        settings.budget = task_llm_budget\n        settings.configuration = task_llm_provider_config\n        task_llm_provider = self.llm_provider.__class__(\n            settings=settings,\n            logger=logger.getChild(\n                f\"Task-{task.task_id}_{self.llm_provider.__class__.__name__}\"\n            ),\n        )\n        self._task_budgets[task.task_id] = task_llm_provider._budget  # type: ignore\n\n        return task_llm_provider\n\n\ndef task_agent_id(task_id: str | int) -> str:\n    return f\"AutoGPT-{task_id}\"\n", "autogpt/autogpt/app/telemetry.py": "import os\n\nimport click\nfrom colorama import Fore, Style\n\nfrom .utils import (\n    env_file_exists,\n    get_git_user_email,\n    set_env_config_value,\n    vcs_state_diverges_from_master,\n)\n\n\ndef setup_telemetry() -> None:\n    if os.getenv(\"TELEMETRY_OPT_IN\") is None:\n        # If no .env file is present, don't bother asking to enable telemetry,\n        # to prevent repeated asking in non-persistent environments.\n        if not env_file_exists():\n            return\n\n        allow_telemetry = click.prompt(\n            f\"\"\"\n{Style.BRIGHT}\u2753 Do you want to enable telemetry? \u2753{Style.NORMAL}\nThis means AutoGPT will send diagnostic data to the core development team when something\ngoes wrong, and will help us to diagnose and fix problems earlier and faster. It also\nallows us to collect basic performance data, which helps us find bottlenecks and other\nthings that slow down the application.\n\nBy entering 'yes', you confirm that you have read and agree to our Privacy Policy,\nwhich is available here:\nhttps://www.notion.so/auto-gpt/Privacy-Policy-ab11c9c20dbd4de1a15dcffe84d77984\n\nPlease enter 'yes' or 'no'\"\"\",\n            type=bool,\n        )\n        set_env_config_value(\"TELEMETRY_OPT_IN\", \"true\" if allow_telemetry else \"false\")\n        click.echo(\n            f\"\u2764\ufe0f  Thank you! Telemetry is {Fore.GREEN}enabled{Fore.RESET}.\"\n            if allow_telemetry\n            else f\"\ud83d\udc4d Telemetry is {Fore.RED}disabled{Fore.RESET}.\"\n        )\n        click.echo(\n            \"\ud83d\udca1 If you ever change your mind, you can change 'TELEMETRY_OPT_IN' in .env\"\n        )\n        click.echo()\n\n    if os.getenv(\"TELEMETRY_OPT_IN\", \"\").lower() == \"true\":\n        _setup_sentry()\n\n\ndef _setup_sentry() -> None:\n    import sentry_sdk\n\n    sentry_sdk.init(\n        dsn=\"https://dc266f2f7a2381194d1c0fa36dff67d8@o4505260022104064.ingest.sentry.io/4506739844710400\",  # noqa\n        enable_tracing=True,\n        environment=os.getenv(\n            \"TELEMETRY_ENVIRONMENT\",\n            \"production\" if not vcs_state_diverges_from_master() else \"dev\",\n        ),\n    )\n\n    # Allow Sentry to distinguish between users\n    sentry_sdk.set_user({\"email\": get_git_user_email(), \"ip_address\": \"{{auto}}\"})\n", "autogpt/autogpt/app/config.py": "\"\"\"Configuration class to store the state of bools for different scripts access.\"\"\"\nfrom __future__ import annotations\n\nimport logging\nimport os\nimport re\nfrom pathlib import Path\nfrom typing import Any, Optional, Union\n\nimport forge\nfrom forge.config.base import BaseConfig\nfrom forge.llm.providers import CHAT_MODELS, ModelName\nfrom forge.llm.providers.openai import OpenAICredentials, OpenAIModelName\nfrom forge.logging.config import LoggingConfig\nfrom forge.models.config import Configurable, UserConfigurable\nfrom pydantic import SecretStr, validator\n\nlogger = logging.getLogger(__name__)\n\nPROJECT_ROOT = Path(forge.__file__).parent.parent\nAZURE_CONFIG_FILE = Path(\"azure.yaml\")\n\nGPT_4_MODEL = OpenAIModelName.GPT4\nGPT_3_MODEL = OpenAIModelName.GPT3\n\n\nclass AppConfig(BaseConfig):\n    name: str = \"Auto-GPT configuration\"\n    description: str = \"Default configuration for the Auto-GPT application.\"\n\n    ########################\n    # Application Settings #\n    ########################\n    project_root: Path = PROJECT_ROOT\n    app_data_dir: Path = project_root / \"data\"\n    skip_news: bool = False\n    skip_reprompt: bool = False\n    authorise_key: str = UserConfigurable(default=\"y\", from_env=\"AUTHORISE_COMMAND_KEY\")\n    exit_key: str = UserConfigurable(default=\"n\", from_env=\"EXIT_KEY\")\n    noninteractive_mode: bool = False\n    logging: LoggingConfig = LoggingConfig()\n    component_config_file: Optional[Path] = UserConfigurable(\n        default=None, from_env=\"COMPONENT_CONFIG_FILE\"\n    )\n\n    ##########################\n    # Agent Control Settings #\n    ##########################\n    # Model configuration\n    fast_llm: ModelName = UserConfigurable(\n        default=OpenAIModelName.GPT3,\n        from_env=\"FAST_LLM\",\n    )\n    smart_llm: ModelName = UserConfigurable(\n        default=OpenAIModelName.GPT4_TURBO,\n        from_env=\"SMART_LLM\",\n    )\n    temperature: float = UserConfigurable(default=0, from_env=\"TEMPERATURE\")\n    openai_functions: bool = UserConfigurable(\n        default=False, from_env=lambda: os.getenv(\"OPENAI_FUNCTIONS\", \"False\") == \"True\"\n    )\n    embedding_model: str = UserConfigurable(\n        default=\"text-embedding-3-small\", from_env=\"EMBEDDING_MODEL\"\n    )\n\n    # Run loop configuration\n    continuous_mode: bool = False\n    continuous_limit: int = 0\n\n    ############\n    # Commands #\n    ############\n    # General\n    disabled_commands: list[str] = UserConfigurable(\n        default_factory=list,\n        from_env=lambda: _safe_split(os.getenv(\"DISABLED_COMMANDS\")),\n    )\n\n    # File ops\n    restrict_to_workspace: bool = UserConfigurable(\n        default=True,\n        from_env=lambda: os.getenv(\"RESTRICT_TO_WORKSPACE\", \"True\") == \"True\",\n    )\n\n    ###############\n    # Credentials #\n    ###############\n    # OpenAI\n    openai_credentials: Optional[OpenAICredentials] = None\n    azure_config_file: Optional[Path] = UserConfigurable(\n        default=AZURE_CONFIG_FILE, from_env=\"AZURE_CONFIG_FILE\"\n    )\n\n    @validator(\"openai_functions\")\n    def validate_openai_functions(cls, v: bool, values: dict[str, Any]):\n        if v:\n            smart_llm = values[\"smart_llm\"]\n            assert CHAT_MODELS[smart_llm].has_function_call_api, (\n                f\"Model {smart_llm} does not support tool calling. \"\n                \"Please disable OPENAI_FUNCTIONS or choose a suitable model.\"\n            )\n        return v\n\n\nclass ConfigBuilder(Configurable[AppConfig]):\n    default_settings = AppConfig()\n\n    @classmethod\n    def build_config_from_env(cls, project_root: Path = PROJECT_ROOT) -> AppConfig:\n        \"\"\"Initialize the Config class\"\"\"\n\n        config = cls.build_agent_configuration()\n        config.project_root = project_root\n\n        # Make relative paths absolute\n        for k in {\n            \"azure_config_file\",  # TODO: move from project root\n        }:\n            setattr(config, k, project_root / getattr(config, k))\n\n        if (\n            config.openai_credentials\n            and config.openai_credentials.api_type == SecretStr(\"azure\")\n            and (config_file := config.azure_config_file)\n        ):\n            config.openai_credentials.load_azure_config(config_file)\n\n        return config\n\n\nasync def assert_config_has_required_llm_api_keys(config: AppConfig) -> None:\n    \"\"\"\n    Check if API keys (if required) are set for the configured SMART_LLM and FAST_LLM.\n    \"\"\"\n    from forge.llm.providers.anthropic import AnthropicModelName\n    from forge.llm.providers.groq import GroqModelName\n    from pydantic import ValidationError\n\n    if set((config.smart_llm, config.fast_llm)).intersection(AnthropicModelName):\n        from forge.llm.providers.anthropic import AnthropicCredentials\n\n        try:\n            credentials = AnthropicCredentials.from_env()\n        except ValidationError as e:\n            if \"api_key\" in str(e):\n                logger.error(\n                    \"Set your Anthropic API key in .env or as an environment variable\"\n                )\n                logger.info(\n                    \"For further instructions: \"\n                    \"https://docs.agpt.co/autogpt/setup/#anthropic\"\n                )\n\n            raise ValueError(\"Anthropic is unavailable: can't load credentials\") from e\n\n        key_pattern = r\"^sk-ant-api03-[\\w\\-]{95}\"\n\n        # If key is set, but it looks invalid\n        if not re.search(key_pattern, credentials.api_key.get_secret_value()):\n            logger.warning(\n                \"Possibly invalid Anthropic API key! \"\n                f\"Configured Anthropic API key does not match pattern '{key_pattern}'. \"\n                \"If this is a valid key, please report this warning to the maintainers.\"\n            )\n\n    if set((config.smart_llm, config.fast_llm)).intersection(GroqModelName):\n        from forge.llm.providers.groq import GroqProvider\n        from groq import AuthenticationError\n\n        try:\n            groq = GroqProvider()\n            await groq.get_available_models()\n        except ValidationError as e:\n            if \"api_key\" not in str(e):\n                raise\n\n            logger.error(\"Set your Groq API key in .env or as an environment variable\")\n            logger.info(\n                \"For further instructions: https://docs.agpt.co/autogpt/setup/#groq\"\n            )\n            raise ValueError(\"Groq is unavailable: can't load credentials\")\n        except AuthenticationError as e:\n            logger.error(\"The Groq API key is invalid!\")\n            logger.info(\n                \"For instructions to get and set a new API key: \"\n                \"https://docs.agpt.co/autogpt/setup/#groq\"\n            )\n            raise ValueError(\"Groq is unavailable: invalid API key\") from e\n\n    if set((config.smart_llm, config.fast_llm)).intersection(OpenAIModelName):\n        from forge.llm.providers.openai import OpenAIProvider\n        from openai import AuthenticationError\n\n        try:\n            openai = OpenAIProvider()\n            await openai.get_available_models()\n        except ValidationError as e:\n            if \"api_key\" not in str(e):\n                raise\n\n            logger.error(\n                \"Set your OpenAI API key in .env or as an environment variable\"\n            )\n            logger.info(\n                \"For further instructions: https://docs.agpt.co/autogpt/setup/#openai\"\n            )\n            raise ValueError(\"OpenAI is unavailable: can't load credentials\")\n        except AuthenticationError as e:\n            logger.error(\"The OpenAI API key is invalid!\")\n            logger.info(\n                \"For instructions to get and set a new API key: \"\n                \"https://docs.agpt.co/autogpt/setup/#openai\"\n            )\n            raise ValueError(\"OpenAI is unavailable: invalid API key\") from e\n\n\ndef _safe_split(s: Union[str, None], sep: str = \",\") -> list[str]:\n    \"\"\"Split a string by a separator. Return an empty list if the string is None.\"\"\"\n    if s is None:\n        return []\n    return s.split(sep)\n", "autogpt/autogpt/app/cli.py": "\"\"\"Main script for the autogpt package.\"\"\"\nfrom logging import _nameToLevel as logLevelMap\nfrom pathlib import Path\nfrom typing import Optional\n\nimport click\nfrom forge.logging.config import LogFormatName\n\nfrom .telemetry import setup_telemetry\n\n\n@click.group(invoke_without_command=True)\n@click.pass_context\ndef cli(ctx: click.Context):\n    setup_telemetry()\n\n    # Invoke `run` by default\n    if ctx.invoked_subcommand is None:\n        ctx.invoke(run)\n\n\n@cli.command()\n@click.option(\"-c\", \"--continuous\", is_flag=True, help=\"Enable Continuous Mode\")\n@click.option(\n    \"-l\",\n    \"--continuous-limit\",\n    type=int,\n    help=\"Defines the number of times to run in continuous mode\",\n)\n@click.option(\"--speak\", is_flag=True, help=\"Enable Speak Mode\")\n@click.option(\n    \"--install-plugin-deps\",\n    is_flag=True,\n    help=\"Installs external dependencies for 3rd party plugins.\",\n)\n@click.option(\n    \"--skip-news\",\n    is_flag=True,\n    help=\"Specifies whether to suppress the output of latest news on startup.\",\n)\n@click.option(\n    \"--skip-reprompt\",\n    \"-y\",\n    is_flag=True,\n    help=\"Skips the re-prompting messages at the beginning of the script\",\n)\n@click.option(\n    \"--ai-name\",\n    type=str,\n    help=\"AI name override\",\n)\n@click.option(\n    \"--ai-role\",\n    type=str,\n    help=\"AI role override\",\n)\n@click.option(\n    \"--constraint\",\n    type=str,\n    multiple=True,\n    help=(\n        \"Add or override AI constraints to include in the prompt;\"\n        \" may be used multiple times to pass multiple constraints\"\n    ),\n)\n@click.option(\n    \"--resource\",\n    type=str,\n    multiple=True,\n    help=(\n        \"Add or override AI resources to include in the prompt;\"\n        \" may be used multiple times to pass multiple resources\"\n    ),\n)\n@click.option(\n    \"--best-practice\",\n    type=str,\n    multiple=True,\n    help=(\n        \"Add or override AI best practices to include in the prompt;\"\n        \" may be used multiple times to pass multiple best practices\"\n    ),\n)\n@click.option(\n    \"--override-directives\",\n    is_flag=True,\n    help=(\n        \"If specified, --constraint, --resource and --best-practice will override\"\n        \" the AI's directives instead of being appended to them\"\n    ),\n)\n@click.option(\n    \"--debug\", is_flag=True, help=\"Implies --log-level=DEBUG --log-format=debug\"\n)\n@click.option(\"--log-level\", type=click.Choice([*logLevelMap.keys()]))\n@click.option(\n    \"--log-format\",\n    help=(\n        \"Choose a log format; defaults to 'simple'.\"\n        \" Also implies --log-file-format, unless it is specified explicitly.\"\n        \" Using the 'structured_google_cloud' format disables log file output.\"\n    ),\n    type=click.Choice([i.value for i in LogFormatName]),\n)\n@click.option(\n    \"--log-file-format\",\n    help=(\n        \"Override the format used for the log file output.\"\n        \" Defaults to the application's global --log-format.\"\n    ),\n    type=click.Choice([i.value for i in LogFormatName]),\n)\n@click.option(\n    \"--component-config-file\",\n    help=\"Path to a json configuration file\",\n    type=click.Path(exists=True, dir_okay=False, resolve_path=True),\n)\ndef run(\n    continuous: bool,\n    continuous_limit: Optional[int],\n    speak: bool,\n    install_plugin_deps: bool,\n    skip_news: bool,\n    skip_reprompt: bool,\n    ai_name: Optional[str],\n    ai_role: Optional[str],\n    resource: tuple[str],\n    constraint: tuple[str],\n    best_practice: tuple[str],\n    override_directives: bool,\n    debug: bool,\n    log_level: Optional[str],\n    log_format: Optional[str],\n    log_file_format: Optional[str],\n    component_config_file: Optional[Path],\n) -> None:\n    \"\"\"\n    Sets up and runs an agent, based on the task specified by the user, or resumes an\n    existing agent.\n    \"\"\"\n    # Put imports inside function to avoid importing everything when starting the CLI\n    from autogpt.app.main import run_auto_gpt\n\n    run_auto_gpt(\n        continuous=continuous,\n        continuous_limit=continuous_limit,\n        skip_reprompt=skip_reprompt,\n        speak=speak,\n        debug=debug,\n        log_level=log_level,\n        log_format=log_format,\n        log_file_format=log_file_format,\n        skip_news=skip_news,\n        install_plugin_deps=install_plugin_deps,\n        override_ai_name=ai_name,\n        override_ai_role=ai_role,\n        resources=list(resource),\n        constraints=list(constraint),\n        best_practices=list(best_practice),\n        override_directives=override_directives,\n        component_config_file=component_config_file,\n    )\n\n\n@cli.command()\n@click.option(\n    \"--install-plugin-deps\",\n    is_flag=True,\n    help=\"Installs external dependencies for 3rd party plugins.\",\n)\n@click.option(\n    \"--debug\", is_flag=True, help=\"Implies --log-level=DEBUG --log-format=debug\"\n)\n@click.option(\"--log-level\", type=click.Choice([*logLevelMap.keys()]))\n@click.option(\n    \"--log-format\",\n    help=(\n        \"Choose a log format; defaults to 'simple'.\"\n        \" Also implies --log-file-format, unless it is specified explicitly.\"\n        \" Using the 'structured_google_cloud' format disables log file output.\"\n    ),\n    type=click.Choice([i.value for i in LogFormatName]),\n)\n@click.option(\n    \"--log-file-format\",\n    help=(\n        \"Override the format used for the log file output.\"\n        \" Defaults to the application's global --log-format.\"\n    ),\n    type=click.Choice([i.value for i in LogFormatName]),\n)\ndef serve(\n    install_plugin_deps: bool,\n    debug: bool,\n    log_level: Optional[str],\n    log_format: Optional[str],\n    log_file_format: Optional[str],\n) -> None:\n    \"\"\"\n    Starts an Agent Protocol compliant AutoGPT server, which creates a custom agent for\n    every task.\n    \"\"\"\n    # Put imports inside function to avoid importing everything when starting the CLI\n    from autogpt.app.main import run_auto_gpt_server\n\n    run_auto_gpt_server(\n        debug=debug,\n        log_level=log_level,\n        log_format=log_format,\n        log_file_format=log_file_format,\n        install_plugin_deps=install_plugin_deps,\n    )\n\n\nif __name__ == \"__main__\":\n    cli()\n", "autogpt/autogpt/app/spinner.py": "\"\"\"A simple spinner module\"\"\"\nimport itertools\nimport sys\nimport threading\nimport time\n\n\nclass Spinner:\n    \"\"\"A simple spinner class\"\"\"\n\n    def __init__(\n        self,\n        message: str = \"Loading...\",\n        delay: float = 0.1,\n        plain_output: bool = False,\n    ) -> None:\n        \"\"\"Initialize the spinner class\n\n        Args:\n            message (str): The message to display.\n            delay (float): The delay between each spinner update.\n            plain_output (bool): Whether to display the spinner or not.\n        \"\"\"\n        self.plain_output = plain_output\n        self.spinner = itertools.cycle([\"-\", \"/\", \"|\", \"\\\\\"])\n        self.delay = delay\n        self.message = message\n        self.running = False\n        self.spinner_thread = None\n\n    def spin(self) -> None:\n        \"\"\"Spin the spinner\"\"\"\n        if self.plain_output:\n            self.print_message()\n            return\n        while self.running:\n            self.print_message()\n            time.sleep(self.delay)\n\n    def print_message(self):\n        sys.stdout.write(f\"\\r{' ' * (len(self.message) + 2)}\\r\")\n        sys.stdout.write(f\"{next(self.spinner)} {self.message}\\r\")\n        sys.stdout.flush()\n\n    def start(self):\n        self.running = True\n        self.spinner_thread = threading.Thread(target=self.spin)\n        self.spinner_thread.start()\n\n    def stop(self):\n        self.running = False\n        if self.spinner_thread is not None:\n            self.spinner_thread.join()\n        sys.stdout.write(f\"\\r{' ' * (len(self.message) + 2)}\\r\")\n        sys.stdout.flush()\n\n    def __enter__(self):\n        \"\"\"Start the spinner\"\"\"\n        self.start()\n        return self\n\n    def __exit__(self, exc_type, exc_value, exc_traceback) -> None:\n        \"\"\"Stop the spinner\n\n        Args:\n            exc_type (Exception): The exception type.\n            exc_value (Exception): The exception value.\n            exc_traceback (Exception): The exception traceback.\n        \"\"\"\n        self.stop()\n", "autogpt/autogpt/app/utils.py": "import asyncio\nimport contextlib\nimport functools\nimport logging\nimport os\nimport re\nimport socket\nimport sys\nfrom pathlib import Path\nfrom typing import Any, Callable, Coroutine, ParamSpec, TypeVar, cast\n\nimport requests\nfrom colorama import Fore, Style\nfrom git import InvalidGitRepositoryError, Repo\n\nP = ParamSpec(\"P\")\nT = TypeVar(\"T\")\n\nlogger = logging.getLogger(__name__)\n\n\ndef get_bulletin_from_web():\n    try:\n        response = requests.get(\n            \"https://raw.githubusercontent.com/Significant-Gravitas/AutoGPT/master/autogpt/BULLETIN.md\"  # noqa: E501\n        )\n        if response.status_code == 200:\n            return response.text\n    except requests.exceptions.RequestException:\n        pass\n\n    return \"\"\n\n\ndef get_current_git_branch() -> str:\n    try:\n        repo = Repo(search_parent_directories=True)\n        branch = repo.active_branch\n        return branch.name\n    except InvalidGitRepositoryError:\n        return \"\"\n\n\ndef vcs_state_diverges_from_master() -> bool:\n    \"\"\"\n    Returns whether a git repo is present and contains changes that are not in `master`.\n    \"\"\"\n    paths_we_care_about = \"autogpt/autogpt/**/*.py\"\n    try:\n        repo = Repo(search_parent_directories=True)\n\n        # Check for uncommitted changes in the specified path\n        uncommitted_changes = repo.index.diff(None, paths=paths_we_care_about)\n        if uncommitted_changes:\n            return True\n\n        # Find OG AutoGPT remote\n        for remote in repo.remotes:\n            if remote.url.endswith(\n                tuple(\n                    # All permutations of old/new repo name and HTTP(S)/Git URLs\n                    f\"{prefix}{path}\"\n                    for prefix in (\"://github.com/\", \"git@github.com:\")\n                    for path in (\n                        f\"Significant-Gravitas/{n}.git\" for n in (\"AutoGPT\", \"Auto-GPT\")\n                    )\n                )\n            ):\n                og_remote = remote\n                break\n        else:\n            # Original AutoGPT remote is not configured: assume local codebase diverges\n            return True\n\n        master_branch = og_remote.refs.master\n        with contextlib.suppress(StopIteration):\n            next(repo.iter_commits(f\"HEAD..{master_branch}\", paths=paths_we_care_about))\n            # Local repo is one or more commits ahead of OG AutoGPT master branch\n            return True\n\n        # Relevant part of the codebase is on master\n        return False\n    except InvalidGitRepositoryError:\n        # No git repo present: assume codebase is a clean download\n        return False\n\n\ndef get_git_user_email() -> str:\n    try:\n        repo = Repo(search_parent_directories=True)\n        return cast(str, repo.config_reader().get_value(\"user\", \"email\", default=\"\"))\n    except InvalidGitRepositoryError:\n        return \"\"\n\n\ndef get_latest_bulletin() -> tuple[str, bool]:\n    exists = os.path.exists(\"data/CURRENT_BULLETIN.md\")\n    current_bulletin = \"\"\n    if exists:\n        current_bulletin = open(\n            \"data/CURRENT_BULLETIN.md\", \"r\", encoding=\"utf-8\"\n        ).read()\n    new_bulletin = get_bulletin_from_web()\n    is_new_news = new_bulletin != \"\" and new_bulletin != current_bulletin\n\n    news_header = Fore.YELLOW + \"Welcome to AutoGPT!\\n\"\n    if new_bulletin or current_bulletin:\n        news_header += (\n            \"Below you'll find the latest AutoGPT News and feature updates!\\n\"\n            \"If you don't wish to see this message, you \"\n            \"can run AutoGPT with the *--skip-news* flag.\\n\"\n        )\n\n    if new_bulletin and is_new_news:\n        open(\"data/CURRENT_BULLETIN.md\", \"w\", encoding=\"utf-8\").write(new_bulletin)\n        current_bulletin = f\"{Fore.RED}::NEW BULLETIN::{Fore.RESET}\\n\\n{new_bulletin}\"\n\n    return f\"{news_header}\\n{current_bulletin}\", is_new_news\n\n\ndef markdown_to_ansi_style(markdown: str):\n    ansi_lines: list[str] = []\n    for line in markdown.split(\"\\n\"):\n        line_style = \"\"\n\n        if line.startswith(\"# \"):\n            line_style += Style.BRIGHT\n        else:\n            line = re.sub(\n                r\"(?<!\\*)\\*(\\*?[^*]+\\*?)\\*(?!\\*)\",\n                rf\"{Style.BRIGHT}\\1{Style.NORMAL}\",\n                line,\n            )\n\n        if re.match(r\"^#+ \", line) is not None:\n            line_style += Fore.CYAN\n            line = re.sub(r\"^#+ \", \"\", line)\n\n        ansi_lines.append(f\"{line_style}{line}{Style.RESET_ALL}\")\n    return \"\\n\".join(ansi_lines)\n\n\ndef get_legal_warning() -> str:\n    legal_text = \"\"\"\n## DISCLAIMER AND INDEMNIFICATION AGREEMENT\n### PLEASE READ THIS DISCLAIMER AND INDEMNIFICATION AGREEMENT CAREFULLY BEFORE USING THE AUTOGPT SYSTEM. BY USING THE AUTOGPT SYSTEM, YOU AGREE TO BE BOUND BY THIS AGREEMENT.\n\n## Introduction\nAutoGPT (the \"System\") is a project that connects a GPT-like artificial intelligence system to the internet and allows it to automate tasks. While the System is designed to be useful and efficient, there may be instances where the System could perform actions that may cause harm or have unintended consequences.\n\n## No Liability for Actions of the System\nThe developers, contributors, and maintainers of the AutoGPT project (collectively, the \"Project Parties\") make no warranties or representations, express or implied, about the System's performance, accuracy, reliability, or safety. By using the System, you understand and agree that the Project Parties shall not be liable for any actions taken by the System or any consequences resulting from such actions.\n\n## User Responsibility and Respondeat Superior Liability\nAs a user of the System, you are responsible for supervising and monitoring the actions of the System while it is operating on your\nbehalf. You acknowledge that using the System could expose you to potential liability including but not limited to respondeat superior and you agree to assume all risks and liabilities associated with such potential liability.\n\n## Indemnification\nBy using the System, you agree to indemnify, defend, and hold harmless the Project Parties from and against any and all claims, liabilities, damages, losses, or expenses (including reasonable attorneys' fees and costs) arising out of or in connection with your use of the System, including, without limitation, any actions taken by the System on your behalf, any failure to properly supervise or monitor the System, and any resulting harm or unintended consequences.\n    \"\"\"  # noqa: E501\n    return legal_text\n\n\ndef print_motd(logger: logging.Logger):\n    motd, is_new_motd = get_latest_bulletin()\n    if motd:\n        motd = markdown_to_ansi_style(motd)\n        for motd_line in motd.split(\"\\n\"):\n            logger.info(\n                extra={\n                    \"title\": \"NEWS:\",\n                    \"title_color\": Fore.GREEN,\n                    \"preserve_color\": True,\n                },\n                msg=motd_line,\n            )\n        if is_new_motd:\n            input(\n                Fore.MAGENTA\n                + Style.BRIGHT\n                + \"NEWS: Bulletin was updated! Press Enter to continue...\"\n                + Style.RESET_ALL\n            )\n\n\ndef print_git_branch_info(logger: logging.Logger):\n    git_branch = get_current_git_branch()\n    if git_branch and git_branch != \"master\":\n        logger.warning(\n            f\"You are running on `{git_branch}` branch\"\n            \" - this is not a supported branch.\"\n        )\n\n\ndef print_python_version_info(logger: logging.Logger):\n    if sys.version_info < (3, 10):\n        logger.error(\n            \"WARNING: You are running on an older version of Python. \"\n            \"Some people have observed problems with certain \"\n            \"parts of AutoGPT with this version. \"\n            \"Please consider upgrading to Python 3.10 or higher.\",\n        )\n\n\nENV_FILE_PATH = Path(__file__).parent.parent.parent / \".env\"\n\n\ndef env_file_exists() -> bool:\n    return ENV_FILE_PATH.is_file()\n\n\ndef set_env_config_value(key: str, value: str) -> None:\n    \"\"\"Sets the specified env variable and updates it in .env as well\"\"\"\n    os.environ[key] = value\n\n    with ENV_FILE_PATH.open(\"r+\") as file:\n        lines = file.readlines()\n        file.seek(0)\n        key_already_in_file = False\n        for line in lines:\n            if re.match(rf\"^(?:# )?{key}=.*$\", line):\n                file.write(f\"{key}={value}\\n\")\n                key_already_in_file = True\n            else:\n                file.write(line)\n\n        if not key_already_in_file:\n            file.write(f\"{key}={value}\\n\")\n\n        file.truncate()\n\n\ndef is_port_free(port: int, host: str = \"127.0.0.1\"):\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        try:\n            s.bind((host, port))  # Try to bind to the port\n            return True  # If successful, the port is free\n        except OSError:\n            return False  # If failed, the port is likely in use\n\n\ndef coroutine(f: Callable[P, Coroutine[Any, Any, T]]) -> Callable[P, T]:\n    @functools.wraps(f)\n    def wrapper(*args: P.args, **kwargs: P.kwargs):\n        return asyncio.run(f(*args, **kwargs))\n\n    return wrapper\n", "autogpt/autogpt/app/input.py": "import logging\n\nimport click\n\nlogger = logging.getLogger(__name__)\n\n\ndef clean_input(prompt: str = \"\"):\n    try:\n        # ask for input, default when just pressing Enter is y\n        logger.debug(\"Asking user via keyboard...\")\n\n        return click.prompt(\n            text=prompt, prompt_suffix=\" \", default=\"\", show_default=False\n        )\n    except KeyboardInterrupt:\n        logger.info(\"You interrupted AutoGPT\")\n        logger.info(\"Quitting...\")\n        exit(0)\n", "autogpt/autogpt/app/setup.py": "\"\"\"Set up the AI and its goals\"\"\"\nimport logging\nfrom typing import Optional\n\nfrom forge.config.ai_directives import AIDirectives\nfrom forge.config.ai_profile import AIProfile\nfrom forge.logging.utils import print_attribute\n\nfrom autogpt.app.config import AppConfig\n\nfrom .input import clean_input\n\nlogger = logging.getLogger(__name__)\n\n\ndef apply_overrides_to_ai_settings(\n    ai_profile: AIProfile,\n    directives: AIDirectives,\n    override_name: Optional[str] = \"\",\n    override_role: Optional[str] = \"\",\n    replace_directives: bool = False,\n    resources: Optional[list[str]] = None,\n    constraints: Optional[list[str]] = None,\n    best_practices: Optional[list[str]] = None,\n):\n    if override_name:\n        ai_profile.ai_name = override_name\n    if override_role:\n        ai_profile.ai_role = override_role\n\n    if replace_directives:\n        if resources:\n            directives.resources = resources\n        if constraints:\n            directives.constraints = constraints\n        if best_practices:\n            directives.best_practices = best_practices\n    else:\n        if resources:\n            directives.resources += resources\n        if constraints:\n            directives.constraints += constraints\n        if best_practices:\n            directives.best_practices += best_practices\n\n\nasync def interactively_revise_ai_settings(\n    ai_profile: AIProfile,\n    directives: AIDirectives,\n    app_config: AppConfig,\n):\n    \"\"\"Interactively revise the AI settings.\n\n    Args:\n        ai_profile (AIConfig): The current AI profile.\n        ai_directives (AIDirectives): The current AI directives.\n        app_config (Config): The application configuration.\n\n    Returns:\n        AIConfig: The revised AI settings.\n    \"\"\"\n    logger = logging.getLogger(\"revise_ai_profile\")\n\n    revised = False\n\n    while True:\n        # Print the current AI configuration\n        print_ai_settings(\n            title=\"Current AI Settings\" if not revised else \"Revised AI Settings\",\n            ai_profile=ai_profile,\n            directives=directives,\n            logger=logger,\n        )\n\n        if (\n            clean_input(\"Continue with these settings? [Y/n]\").lower()\n            or app_config.authorise_key\n        ) == app_config.authorise_key:\n            break\n\n        # Ask for revised ai_profile\n        ai_profile.ai_name = (\n            clean_input(\"Enter AI name (or press enter to keep current):\")\n            or ai_profile.ai_name\n        )\n        ai_profile.ai_role = (\n            clean_input(\"Enter new AI role (or press enter to keep current):\")\n            or ai_profile.ai_role\n        )\n\n        # Revise constraints\n        i = 0\n        while i < len(directives.constraints):\n            constraint = directives.constraints[i]\n            print_attribute(f\"Constraint {i+1}:\", f'\"{constraint}\"')\n            new_constraint = (\n                clean_input(\n                    f\"Enter new constraint {i+1}\"\n                    \" (press enter to keep current, or '-' to remove):\",\n                )\n                or constraint\n            )\n\n            if new_constraint == \"-\":\n                directives.constraints.remove(constraint)\n                continue\n            elif new_constraint:\n                directives.constraints[i] = new_constraint\n\n            i += 1\n\n        # Add new constraints\n        while True:\n            new_constraint = clean_input(\n                \"Press enter to finish, or enter a constraint to add:\",\n            )\n            if not new_constraint:\n                break\n            directives.constraints.append(new_constraint)\n\n        # Revise resources\n        i = 0\n        while i < len(directives.resources):\n            resource = directives.resources[i]\n            print_attribute(f\"Resource {i+1}:\", f'\"{resource}\"')\n            new_resource = (\n                clean_input(\n                    f\"Enter new resource {i+1}\"\n                    \" (press enter to keep current, or '-' to remove):\",\n                )\n                or resource\n            )\n            if new_resource == \"-\":\n                directives.resources.remove(resource)\n                continue\n            elif new_resource:\n                directives.resources[i] = new_resource\n\n            i += 1\n\n        # Add new resources\n        while True:\n            new_resource = clean_input(\n                \"Press enter to finish, or enter a resource to add:\",\n            )\n            if not new_resource:\n                break\n            directives.resources.append(new_resource)\n\n        # Revise best practices\n        i = 0\n        while i < len(directives.best_practices):\n            best_practice = directives.best_practices[i]\n            print_attribute(f\"Best Practice {i+1}:\", f'\"{best_practice}\"')\n            new_best_practice = (\n                clean_input(\n                    f\"Enter new best practice {i+1}\"\n                    \" (press enter to keep current, or '-' to remove):\",\n                )\n                or best_practice\n            )\n            if new_best_practice == \"-\":\n                directives.best_practices.remove(best_practice)\n                continue\n            elif new_best_practice:\n                directives.best_practices[i] = new_best_practice\n\n            i += 1\n\n        # Add new best practices\n        while True:\n            new_best_practice = clean_input(\n                \"Press enter to finish, or add a best practice to add:\",\n            )\n            if not new_best_practice:\n                break\n            directives.best_practices.append(new_best_practice)\n\n        revised = True\n\n    return ai_profile, directives\n\n\ndef print_ai_settings(\n    ai_profile: AIProfile,\n    directives: AIDirectives,\n    logger: logging.Logger,\n    title: str = \"AI Settings\",\n):\n    print_attribute(title, \"\")\n    print_attribute(\"-\" * len(title), \"\")\n    print_attribute(\"Name :\", ai_profile.ai_name)\n    print_attribute(\"Role :\", ai_profile.ai_role)\n\n    print_attribute(\"Constraints:\", \"\" if directives.constraints else \"(none)\")\n    for constraint in directives.constraints:\n        logger.info(f\"- {constraint}\")\n    print_attribute(\"Resources:\", \"\" if directives.resources else \"(none)\")\n    for resource in directives.resources:\n        logger.info(f\"- {resource}\")\n    print_attribute(\"Best practices:\", \"\" if directives.best_practices else \"(none)\")\n    for best_practice in directives.best_practices:\n        logger.info(f\"- {best_practice}\")\n", "autogpt/autogpt/app/configurator.py": "\"\"\"Configurator module.\"\"\"\nfrom __future__ import annotations\n\nimport logging\nfrom typing import Literal, Optional\n\nimport click\nfrom forge.llm.providers import ModelName, MultiProvider\n\nfrom autogpt.app.config import GPT_3_MODEL, AppConfig\n\nlogger = logging.getLogger(__name__)\n\n\nasync def apply_overrides_to_config(\n    config: AppConfig,\n    continuous: bool = False,\n    continuous_limit: Optional[int] = None,\n    skip_reprompt: bool = False,\n    skip_news: bool = False,\n) -> None:\n    \"\"\"Updates the config object with the given arguments.\n\n    Args:\n        config (Config): The config object to update.\n        continuous (bool): Whether to run in continuous mode.\n        continuous_limit (int): The number of times to run in continuous mode.\n        skip_reprompt (bool): Whether to skip the re-prompting messages on start.\n        speak (bool): Whether to enable speak mode.\n        debug (bool): Whether to enable debug mode.\n        log_level (int): The global log level for the application.\n        log_format (str): The format for the log(s).\n        log_file_format (str): Override the format for the log file.\n        skips_news (bool): Whether to suppress the output of latest news on startup.\n    \"\"\"\n    config.continuous_mode = False\n\n    if continuous:\n        logger.warning(\n            \"Continuous mode is not recommended. It is potentially dangerous and may\"\n            \" cause your AI to run forever or carry out actions you would not usually\"\n            \" authorise. Use at your own risk.\",\n        )\n        config.continuous_mode = True\n\n        if continuous_limit:\n            config.continuous_limit = continuous_limit\n\n    # Check if continuous limit is used without continuous mode\n    if continuous_limit and not continuous:\n        raise click.UsageError(\"--continuous-limit can only be used with --continuous\")\n\n    # Check availability of configured LLMs; fallback to other LLM if unavailable\n    config.fast_llm = await check_model(config.fast_llm, \"fast_llm\")\n    config.smart_llm = await check_model(config.smart_llm, \"smart_llm\")\n\n    if skip_reprompt:\n        config.skip_reprompt = True\n\n    if skip_news:\n        config.skip_news = True\n\n\nasync def check_model(\n    model_name: ModelName, model_type: Literal[\"smart_llm\", \"fast_llm\"]\n) -> ModelName:\n    \"\"\"Check if model is available for use. If not, return gpt-3.5-turbo.\"\"\"\n    multi_provider = MultiProvider()\n    models = await multi_provider.get_available_chat_models()\n\n    if any(model_name == m.name for m in models):\n        return model_name\n\n    logger.warning(\n        f\"You don't have access to {model_name}. Setting {model_type} to {GPT_3_MODEL}.\"\n    )\n    return GPT_3_MODEL\n", "autogpt/autogpt/app/main.py": "\"\"\"\nThe application entry point. Can be invoked by a CLI or any other front end application.\n\"\"\"\n\nimport enum\nimport logging\nimport math\nimport os\nimport re\nimport signal\nimport sys\nfrom pathlib import Path\nfrom types import FrameType\nfrom typing import TYPE_CHECKING, Optional\n\nfrom colorama import Fore, Style\nfrom forge.agent_protocol.database import AgentDB\nfrom forge.components.code_executor.code_executor import (\n    is_docker_available,\n    we_are_running_in_a_docker_container,\n)\nfrom forge.config.ai_directives import AIDirectives\nfrom forge.config.ai_profile import AIProfile\nfrom forge.file_storage import FileStorageBackendName, get_storage\nfrom forge.llm.providers import MultiProvider\nfrom forge.logging.config import configure_logging\nfrom forge.logging.utils import print_attribute, speak\nfrom forge.models.action import ActionInterruptedByHuman, ActionProposal\nfrom forge.models.utils import ModelWithSummary\nfrom forge.utils.const import FINISH_COMMAND\nfrom forge.utils.exceptions import AgentTerminated, InvalidAgentResponseError\n\nfrom autogpt.agent_factory.configurators import configure_agent_with_state, create_agent\nfrom autogpt.agents.agent_manager import AgentManager\nfrom autogpt.agents.prompt_strategies.one_shot import AssistantThoughts\nfrom autogpt.app.config import (\n    AppConfig,\n    ConfigBuilder,\n    assert_config_has_required_llm_api_keys,\n)\n\nif TYPE_CHECKING:\n    from autogpt.agents.agent import Agent\n\nfrom .configurator import apply_overrides_to_config\nfrom .input import clean_input\nfrom .setup import apply_overrides_to_ai_settings, interactively_revise_ai_settings\nfrom .spinner import Spinner\nfrom .utils import (\n    coroutine,\n    get_legal_warning,\n    markdown_to_ansi_style,\n    print_git_branch_info,\n    print_motd,\n    print_python_version_info,\n)\n\n\n@coroutine\nasync def run_auto_gpt(\n    continuous: bool = False,\n    continuous_limit: Optional[int] = None,\n    skip_reprompt: bool = False,\n    speak: bool = False,\n    debug: bool = False,\n    log_level: Optional[str] = None,\n    log_format: Optional[str] = None,\n    log_file_format: Optional[str] = None,\n    skip_news: bool = False,\n    install_plugin_deps: bool = False,\n    override_ai_name: Optional[str] = None,\n    override_ai_role: Optional[str] = None,\n    resources: Optional[list[str]] = None,\n    constraints: Optional[list[str]] = None,\n    best_practices: Optional[list[str]] = None,\n    override_directives: bool = False,\n    component_config_file: Optional[Path] = None,\n):\n    # Set up configuration\n    config = ConfigBuilder.build_config_from_env()\n    # Storage\n    local = config.file_storage_backend == FileStorageBackendName.LOCAL\n    restrict_to_root = not local or config.restrict_to_workspace\n    file_storage = get_storage(\n        config.file_storage_backend,\n        root_path=Path(\"data\"),\n        restrict_to_root=restrict_to_root,\n    )\n    file_storage.initialize()\n\n    # Set up logging module\n    if speak:\n        config.tts_config.speak_mode = True\n    configure_logging(\n        debug=debug,\n        level=log_level,\n        log_format=log_format,\n        log_file_format=log_file_format,\n        config=config.logging,\n        tts_config=config.tts_config,\n    )\n\n    await assert_config_has_required_llm_api_keys(config)\n\n    await apply_overrides_to_config(\n        config=config,\n        continuous=continuous,\n        continuous_limit=continuous_limit,\n        skip_reprompt=skip_reprompt,\n        skip_news=skip_news,\n    )\n\n    llm_provider = _configure_llm_provider(config)\n\n    logger = logging.getLogger(__name__)\n\n    if config.continuous_mode:\n        for line in get_legal_warning().split(\"\\n\"):\n            logger.warning(\n                extra={\n                    \"title\": \"LEGAL:\",\n                    \"title_color\": Fore.RED,\n                    \"preserve_color\": True,\n                },\n                msg=markdown_to_ansi_style(line),\n            )\n\n    if not config.skip_news:\n        print_motd(logger)\n        print_git_branch_info(logger)\n        print_python_version_info(logger)\n        print_attribute(\"Smart LLM\", config.smart_llm)\n        print_attribute(\"Fast LLM\", config.fast_llm)\n        if config.continuous_mode:\n            print_attribute(\"Continuous Mode\", \"ENABLED\", title_color=Fore.YELLOW)\n            if continuous_limit:\n                print_attribute(\"Continuous Limit\", config.continuous_limit)\n        if config.tts_config.speak_mode:\n            print_attribute(\"Speak Mode\", \"ENABLED\")\n        if we_are_running_in_a_docker_container() or is_docker_available():\n            print_attribute(\"Code Execution\", \"ENABLED\")\n        else:\n            print_attribute(\n                \"Code Execution\",\n                \"DISABLED (Docker unavailable)\",\n                title_color=Fore.YELLOW,\n            )\n\n    # Let user choose an existing agent to run\n    agent_manager = AgentManager(file_storage)\n    existing_agents = agent_manager.list_agents()\n    load_existing_agent = \"\"\n    if existing_agents:\n        print(\n            \"Existing agents\\n---------------\\n\"\n            + \"\\n\".join(f\"{i} - {id}\" for i, id in enumerate(existing_agents, 1))\n        )\n        load_existing_agent = clean_input(\n            \"Enter the number or name of the agent to run,\"\n            \" or hit enter to create a new one:\",\n        )\n        if re.match(r\"^\\d+$\", load_existing_agent.strip()) and 0 < int(\n            load_existing_agent\n        ) <= len(existing_agents):\n            load_existing_agent = existing_agents[int(load_existing_agent) - 1]\n\n        if load_existing_agent != \"\" and load_existing_agent not in existing_agents:\n            logger.info(\n                f\"Unknown agent '{load_existing_agent}', \"\n                f\"creating a new one instead.\",\n                extra={\"color\": Fore.YELLOW},\n            )\n            load_existing_agent = \"\"\n\n    # Either load existing or set up new agent state\n    agent = None\n    agent_state = None\n\n    ############################\n    # Resume an Existing Agent #\n    ############################\n    if load_existing_agent:\n        agent_state = None\n        while True:\n            answer = clean_input(\"Resume? [Y/n]\")\n            if answer == \"\" or answer.lower() == \"y\":\n                agent_state = agent_manager.load_agent_state(load_existing_agent)\n                break\n            elif answer.lower() == \"n\":\n                break\n\n    if agent_state:\n        agent = configure_agent_with_state(\n            state=agent_state,\n            app_config=config,\n            file_storage=file_storage,\n            llm_provider=llm_provider,\n        )\n        apply_overrides_to_ai_settings(\n            ai_profile=agent.state.ai_profile,\n            directives=agent.state.directives,\n            override_name=override_ai_name,\n            override_role=override_ai_role,\n            resources=resources,\n            constraints=constraints,\n            best_practices=best_practices,\n            replace_directives=override_directives,\n        )\n\n        if (\n            (current_episode := agent.event_history.current_episode)\n            and current_episode.action.use_tool.name == FINISH_COMMAND\n            and not current_episode.result\n        ):\n            # Agent was resumed after `finish` -> rewrite result of `finish` action\n            finish_reason = current_episode.action.use_tool.arguments[\"reason\"]\n            print(f\"Agent previously self-terminated; reason: '{finish_reason}'\")\n            new_assignment = clean_input(\n                \"Please give a follow-up question or assignment:\"\n            )\n            agent.event_history.register_result(\n                ActionInterruptedByHuman(feedback=new_assignment)\n            )\n\n        # If any of these are specified as arguments,\n        #  assume the user doesn't want to revise them\n        if not any(\n            [\n                override_ai_name,\n                override_ai_role,\n                resources,\n                constraints,\n                best_practices,\n            ]\n        ):\n            ai_profile, ai_directives = await interactively_revise_ai_settings(\n                ai_profile=agent.state.ai_profile,\n                directives=agent.state.directives,\n                app_config=config,\n            )\n        else:\n            logger.info(\"AI config overrides specified through CLI; skipping revision\")\n\n    ######################\n    # Set up a new Agent #\n    ######################\n    if not agent:\n        task = \"\"\n        while task.strip() == \"\":\n            task = clean_input(\n                \"Enter the task that you want AutoGPT to execute,\"\n                \" with as much detail as possible:\",\n            )\n\n        ai_profile = AIProfile()\n        additional_ai_directives = AIDirectives()\n        apply_overrides_to_ai_settings(\n            ai_profile=ai_profile,\n            directives=additional_ai_directives,\n            override_name=override_ai_name,\n            override_role=override_ai_role,\n            resources=resources,\n            constraints=constraints,\n            best_practices=best_practices,\n            replace_directives=override_directives,\n        )\n\n        # If any of these are specified as arguments,\n        #  assume the user doesn't want to revise them\n        if not any(\n            [\n                override_ai_name,\n                override_ai_role,\n                resources,\n                constraints,\n                best_practices,\n            ]\n        ):\n            (\n                ai_profile,\n                additional_ai_directives,\n            ) = await interactively_revise_ai_settings(\n                ai_profile=ai_profile,\n                directives=additional_ai_directives,\n                app_config=config,\n            )\n        else:\n            logger.info(\"AI config overrides specified through CLI; skipping revision\")\n\n        agent = create_agent(\n            agent_id=agent_manager.generate_id(ai_profile.ai_name),\n            task=task,\n            ai_profile=ai_profile,\n            directives=additional_ai_directives,\n            app_config=config,\n            file_storage=file_storage,\n            llm_provider=llm_provider,\n        )\n\n        file_manager = agent.file_manager\n\n        if file_manager and not agent.config.allow_fs_access:\n            logger.info(\n                f\"{Fore.YELLOW}\"\n                \"NOTE: All files/directories created by this agent can be found \"\n                f\"inside its workspace at:{Fore.RESET} {file_manager.workspace.root}\",\n                extra={\"preserve_color\": True},\n            )\n\n        # TODO: re-evaluate performance benefit of task-oriented profiles\n        # # Concurrently generate a custom profile for the agent and apply it once done\n        # def update_agent_directives(\n        #     task: asyncio.Task[tuple[AIProfile, AIDirectives]]\n        # ):\n        #     logger.debug(f\"Updating AIProfile: {task.result()[0]}\")\n        #     logger.debug(f\"Adding AIDirectives: {task.result()[1]}\")\n        #     agent.state.ai_profile = task.result()[0]\n        #     agent.state.directives = agent.state.directives + task.result()[1]\n\n        # asyncio.create_task(\n        #     generate_agent_profile_for_task(\n        #         task, app_config=config, llm_provider=llm_provider\n        #     )\n        # ).add_done_callback(update_agent_directives)\n\n    # Load component configuration from file\n    if _config_file := component_config_file or config.component_config_file:\n        try:\n            logger.info(f\"Loading component configuration from {_config_file}\")\n            agent.load_component_configs(_config_file.read_text())\n        except Exception as e:\n            logger.error(f\"Could not load component configuration: {e}\")\n\n    #################\n    # Run the Agent #\n    #################\n    try:\n        await run_interaction_loop(agent)\n    except AgentTerminated:\n        agent_id = agent.state.agent_id\n        logger.info(f\"Saving state of {agent_id}...\")\n\n        # Allow user to Save As other ID\n        save_as_id = clean_input(\n            f\"Press enter to save as '{agent_id}',\"\n            \" or enter a different ID to save to:\",\n        )\n        # TODO: allow many-to-one relations of agents and workspaces\n        await agent.file_manager.save_state(\n            save_as_id.strip() if not save_as_id.isspace() else None\n        )\n\n\n@coroutine\nasync def run_auto_gpt_server(\n    debug: bool = False,\n    log_level: Optional[str] = None,\n    log_format: Optional[str] = None,\n    log_file_format: Optional[str] = None,\n    install_plugin_deps: bool = False,\n):\n    from .agent_protocol_server import AgentProtocolServer\n\n    config = ConfigBuilder.build_config_from_env()\n    # Storage\n    local = config.file_storage_backend == FileStorageBackendName.LOCAL\n    restrict_to_root = not local or config.restrict_to_workspace\n    file_storage = get_storage(\n        config.file_storage_backend,\n        root_path=Path(\"data\"),\n        restrict_to_root=restrict_to_root,\n    )\n    file_storage.initialize()\n\n    # Set up logging module\n    configure_logging(\n        debug=debug,\n        level=log_level,\n        log_format=log_format,\n        log_file_format=log_file_format,\n        config=config.logging,\n        tts_config=config.tts_config,\n    )\n\n    await assert_config_has_required_llm_api_keys(config)\n\n    await apply_overrides_to_config(\n        config=config,\n    )\n\n    llm_provider = _configure_llm_provider(config)\n\n    # Set up & start server\n    database = AgentDB(\n        database_string=os.getenv(\"AP_SERVER_DB_URL\", \"sqlite:///data/ap_server.db\"),\n        debug_enabled=debug,\n    )\n    port: int = int(os.getenv(\"AP_SERVER_PORT\", default=8000))\n    server = AgentProtocolServer(\n        app_config=config,\n        database=database,\n        file_storage=file_storage,\n        llm_provider=llm_provider,\n    )\n    await server.start(port=port)\n\n    logging.getLogger().info(\n        f\"Total OpenAI session cost: \"\n        f\"${round(sum(b.total_cost for b in server._task_budgets.values()), 2)}\"\n    )\n\n\ndef _configure_llm_provider(config: AppConfig) -> MultiProvider:\n    multi_provider = MultiProvider()\n    for model in [config.smart_llm, config.fast_llm]:\n        # Ensure model providers for configured LLMs are available\n        multi_provider.get_model_provider(model)\n    return multi_provider\n\n\ndef _get_cycle_budget(continuous_mode: bool, continuous_limit: int) -> int | float:\n    # Translate from the continuous_mode/continuous_limit config\n    # to a cycle_budget (maximum number of cycles to run without checking in with the\n    # user) and a count of cycles_remaining before we check in..\n    if continuous_mode:\n        cycle_budget = continuous_limit if continuous_limit else math.inf\n    else:\n        cycle_budget = 1\n\n    return cycle_budget\n\n\nclass UserFeedback(str, enum.Enum):\n    \"\"\"Enum for user feedback.\"\"\"\n\n    AUTHORIZE = \"GENERATE NEXT COMMAND JSON\"\n    EXIT = \"EXIT\"\n    TEXT = \"TEXT\"\n\n\nasync def run_interaction_loop(\n    agent: \"Agent\",\n) -> None:\n    \"\"\"Run the main interaction loop for the agent.\n\n    Args:\n        agent: The agent to run the interaction loop for.\n\n    Returns:\n        None\n    \"\"\"\n    # These contain both application config and agent config, so grab them here.\n    app_config = agent.app_config\n    ai_profile = agent.state.ai_profile\n    logger = logging.getLogger(__name__)\n\n    cycle_budget = cycles_remaining = _get_cycle_budget(\n        app_config.continuous_mode, app_config.continuous_limit\n    )\n    spinner = Spinner(\n        \"Thinking...\", plain_output=app_config.logging.plain_console_output\n    )\n    stop_reason = None\n\n    def graceful_agent_interrupt(signum: int, frame: Optional[FrameType]) -> None:\n        nonlocal cycle_budget, cycles_remaining, spinner, stop_reason\n        if stop_reason:\n            logger.error(\"Quitting immediately...\")\n            sys.exit()\n        if cycles_remaining in [0, 1]:\n            logger.warning(\"Interrupt signal received: shutting down gracefully.\")\n            logger.warning(\n                \"Press Ctrl+C again if you want to stop AutoGPT immediately.\"\n            )\n            stop_reason = AgentTerminated(\"Interrupt signal received\")\n        else:\n            restart_spinner = spinner.running\n            if spinner.running:\n                spinner.stop()\n\n            logger.error(\n                \"Interrupt signal received: stopping continuous command execution.\"\n            )\n            cycles_remaining = 1\n            if restart_spinner:\n                spinner.start()\n\n    def handle_stop_signal() -> None:\n        if stop_reason:\n            raise stop_reason\n\n    # Set up an interrupt signal for the agent.\n    signal.signal(signal.SIGINT, graceful_agent_interrupt)\n\n    #########################\n    # Application Main Loop #\n    #########################\n\n    # Keep track of consecutive failures of the agent\n    consecutive_failures = 0\n\n    while cycles_remaining > 0:\n        logger.debug(f\"Cycle budget: {cycle_budget}; remaining: {cycles_remaining}\")\n\n        ########\n        # Plan #\n        ########\n        handle_stop_signal()\n        # Have the agent determine the next action to take.\n        if not (_ep := agent.event_history.current_episode) or _ep.result:\n            with spinner:\n                try:\n                    action_proposal = await agent.propose_action()\n                except InvalidAgentResponseError as e:\n                    logger.warning(f\"The agent's thoughts could not be parsed: {e}\")\n                    consecutive_failures += 1\n                    if consecutive_failures >= 3:\n                        logger.error(\n                            \"The agent failed to output valid thoughts\"\n                            f\" {consecutive_failures} times in a row. Terminating...\"\n                        )\n                        raise AgentTerminated(\n                            \"The agent failed to output valid thoughts\"\n                            f\" {consecutive_failures} times in a row.\"\n                        )\n                    continue\n        else:\n            action_proposal = _ep.action\n\n        consecutive_failures = 0\n\n        ###############\n        # Update User #\n        ###############\n        # Print the assistant's thoughts and the next command to the user.\n        update_user(\n            ai_profile,\n            action_proposal,\n            speak_mode=app_config.tts_config.speak_mode,\n        )\n\n        ##################\n        # Get user input #\n        ##################\n        handle_stop_signal()\n        if cycles_remaining == 1:  # Last cycle\n            feedback_type, feedback, new_cycles_remaining = await get_user_feedback(\n                app_config,\n                ai_profile,\n            )\n\n            if feedback_type == UserFeedback.AUTHORIZE:\n                if new_cycles_remaining is not None:\n                    # Case 1: User is altering the cycle budget.\n                    if cycle_budget > 1:\n                        cycle_budget = new_cycles_remaining + 1\n                    # Case 2: User is running iteratively and\n                    #   has initiated a one-time continuous cycle\n                    cycles_remaining = new_cycles_remaining + 1\n                else:\n                    # Case 1: Continuous iteration was interrupted -> resume\n                    if cycle_budget > 1:\n                        logger.info(\n                            f\"The cycle budget is {cycle_budget}.\",\n                            extra={\n                                \"title\": \"RESUMING CONTINUOUS EXECUTION\",\n                                \"title_color\": Fore.MAGENTA,\n                            },\n                        )\n                    # Case 2: The agent used up its cycle budget -> reset\n                    cycles_remaining = cycle_budget + 1\n                logger.info(\n                    \"-=-=-=-=-=-=-= COMMAND AUTHORISED BY USER -=-=-=-=-=-=-=\",\n                    extra={\"color\": Fore.MAGENTA},\n                )\n            elif feedback_type == UserFeedback.EXIT:\n                logger.warning(\"Exiting...\")\n                exit()\n            else:  # user_feedback == UserFeedback.TEXT\n                pass\n        else:\n            feedback = \"\"\n            # First log new-line so user can differentiate sections better in console\n            print()\n            if cycles_remaining != math.inf:\n                # Print authorized commands left value\n                print_attribute(\n                    \"AUTHORIZED_COMMANDS_LEFT\", cycles_remaining, title_color=Fore.CYAN\n                )\n\n        ###################\n        # Execute Command #\n        ###################\n        # Decrement the cycle counter first to reduce the likelihood of a SIGINT\n        # happening during command execution, setting the cycles remaining to 1,\n        # and then having the decrement set it to 0, exiting the application.\n        if not feedback:\n            cycles_remaining -= 1\n\n        if not action_proposal.use_tool:\n            continue\n\n        handle_stop_signal()\n\n        if not feedback:\n            result = await agent.execute(action_proposal)\n        else:\n            result = await agent.do_not_execute(action_proposal, feedback)\n\n        if result.status == \"success\":\n            logger.info(result, extra={\"title\": \"SYSTEM:\", \"title_color\": Fore.YELLOW})\n        elif result.status == \"error\":\n            logger.warning(\n                f\"Command {action_proposal.use_tool.name} returned an error: \"\n                f\"{result.error or result.reason}\"\n            )\n\n\ndef update_user(\n    ai_profile: AIProfile,\n    action_proposal: \"ActionProposal\",\n    speak_mode: bool = False,\n) -> None:\n    \"\"\"Prints the assistant's thoughts and the next command to the user.\n\n    Args:\n        config: The program's configuration.\n        ai_profile: The AI's personality/profile\n        command_name: The name of the command to execute.\n        command_args: The arguments for the command.\n        assistant_reply_dict: The assistant's reply.\n    \"\"\"\n    logger = logging.getLogger(__name__)\n\n    print_assistant_thoughts(\n        ai_name=ai_profile.ai_name,\n        thoughts=action_proposal.thoughts,\n        speak_mode=speak_mode,\n    )\n\n    if speak_mode:\n        speak(f\"I want to execute {action_proposal.use_tool.name}\")\n\n    # First log new-line so user can differentiate sections better in console\n    print()\n    safe_tool_name = remove_ansi_escape(action_proposal.use_tool.name)\n    logger.info(\n        f\"COMMAND = {Fore.CYAN}{safe_tool_name}{Style.RESET_ALL}  \"\n        f\"ARGUMENTS = {Fore.CYAN}{action_proposal.use_tool.arguments}{Style.RESET_ALL}\",\n        extra={\n            \"title\": \"NEXT ACTION:\",\n            \"title_color\": Fore.CYAN,\n            \"preserve_color\": True,\n        },\n    )\n\n\nasync def get_user_feedback(\n    config: AppConfig,\n    ai_profile: AIProfile,\n) -> tuple[UserFeedback, str, int | None]:\n    \"\"\"Gets the user's feedback on the assistant's reply.\n\n    Args:\n        config: The program's configuration.\n        ai_profile: The AI's configuration.\n\n    Returns:\n        A tuple of the user's feedback, the user's input, and the number of\n        cycles remaining if the user has initiated a continuous cycle.\n    \"\"\"\n    logger = logging.getLogger(__name__)\n\n    # ### GET USER AUTHORIZATION TO EXECUTE COMMAND ###\n    # Get key press: Prompt the user to press enter to continue or escape\n    # to exit\n    logger.info(\n        f\"Enter '{config.authorise_key}' to authorise command, \"\n        f\"'{config.authorise_key} -N' to run N continuous commands, \"\n        f\"'{config.exit_key}' to exit program, or enter feedback for \"\n        f\"{ai_profile.ai_name}...\"\n    )\n\n    user_feedback = None\n    user_input = \"\"\n    new_cycles_remaining = None\n\n    while user_feedback is None:\n        # Get input from user\n        console_input = clean_input(Fore.MAGENTA + \"Input:\" + Style.RESET_ALL)\n\n        # Parse user input\n        if console_input.lower().strip() == config.authorise_key:\n            user_feedback = UserFeedback.AUTHORIZE\n        elif console_input.lower().strip() == \"\":\n            logger.warning(\"Invalid input format.\")\n        elif console_input.lower().startswith(f\"{config.authorise_key} -\"):\n            try:\n                user_feedback = UserFeedback.AUTHORIZE\n                new_cycles_remaining = abs(int(console_input.split(\" \")[1]))\n            except ValueError:\n                logger.warning(\n                    f\"Invalid input format. \"\n                    f\"Please enter '{config.authorise_key} -N'\"\n                    \" where N is the number of continuous tasks.\"\n                )\n        elif console_input.lower() in [config.exit_key, \"exit\"]:\n            user_feedback = UserFeedback.EXIT\n        else:\n            user_feedback = UserFeedback.TEXT\n            user_input = console_input\n\n    return user_feedback, user_input, new_cycles_remaining\n\n\ndef print_assistant_thoughts(\n    ai_name: str,\n    thoughts: str | ModelWithSummary | AssistantThoughts,\n    speak_mode: bool = False,\n) -> None:\n    logger = logging.getLogger(__name__)\n\n    thoughts_text = remove_ansi_escape(\n        thoughts.text\n        if isinstance(thoughts, AssistantThoughts)\n        else thoughts.summary()\n        if isinstance(thoughts, ModelWithSummary)\n        else thoughts\n    )\n    print_attribute(\n        f\"{ai_name.upper()} THOUGHTS\", thoughts_text, title_color=Fore.YELLOW\n    )\n\n    if isinstance(thoughts, AssistantThoughts):\n        print_attribute(\n            \"REASONING\", remove_ansi_escape(thoughts.reasoning), title_color=Fore.YELLOW\n        )\n        if assistant_thoughts_plan := remove_ansi_escape(\n            \"\\n\".join(f\"- {p}\" for p in thoughts.plan)\n        ):\n            print_attribute(\"PLAN\", \"\", title_color=Fore.YELLOW)\n            # If it's a list, join it into a string\n            if isinstance(assistant_thoughts_plan, list):\n                assistant_thoughts_plan = \"\\n\".join(assistant_thoughts_plan)\n            elif isinstance(assistant_thoughts_plan, dict):\n                assistant_thoughts_plan = str(assistant_thoughts_plan)\n\n            # Split the input_string using the newline character and dashes\n            lines = assistant_thoughts_plan.split(\"\\n\")\n            for line in lines:\n                line = line.lstrip(\"- \")\n                logger.info(\n                    line.strip(), extra={\"title\": \"- \", \"title_color\": Fore.GREEN}\n                )\n        print_attribute(\n            \"CRITICISM\",\n            remove_ansi_escape(thoughts.self_criticism),\n            title_color=Fore.YELLOW,\n        )\n\n        # Speak the assistant's thoughts\n        if assistant_thoughts_speak := remove_ansi_escape(thoughts.speak):\n            if speak_mode:\n                speak(assistant_thoughts_speak)\n            else:\n                print_attribute(\n                    \"SPEAK\", assistant_thoughts_speak, title_color=Fore.YELLOW\n                )\n    else:\n        speak(thoughts_text)\n\n\ndef remove_ansi_escape(s: str) -> str:\n    return s.replace(\"\\x1B\", \"\")\n", "autogpt/autogpt/app/__init__.py": "from dotenv import load_dotenv\n\n# Load the users .env file into environment variables\nload_dotenv(verbose=True, override=True)\n\ndel load_dotenv\n", "forge/forge/conftest.py": "from pathlib import Path\n\nimport pytest\n\n\n@pytest.fixture()\ndef test_workspace(tmp_path: Path) -> Path:\n    return tmp_path\n", "forge/forge/__main__.py": "import logging\nimport os\n\nimport uvicorn\nfrom dotenv import load_dotenv\n\nfrom forge.logging.config import configure_logging\n\nlogger = logging.getLogger(__name__)\n\nlogo = \"\"\"\\n\\n\n       d8888          888             .d8888b.  8888888b. 88888888888\n     d88P888          888            888    888 888    888    888\n    d88P 888 888  888 888888 .d88b.  888        888   d88P    888\n   d88P  888 888  888 888   d88\"\"88b 888  88888 8888888P\"     888\n  d88P   888 888  888 888   888  888 888    888 888           888\n d8888888888 Y88b 888 Y88b. Y88..88P Y88b  d88P 888           888\nd88P     888  \"Y88888  \"Y888 \"Y88P\"   \"Y8888P88 888           888\n\n\n                8888888888\n                888\n                888      .d88b.  888d888 .d88b.   .d88b.\n                888888  d88\"\"88b 888P\"  d88P\"88b d8P  Y8b\n                888     888  888 888    888  888 88888888\n                888     Y88..88P 888    Y88b 888 Y8b.\n                888      \"Y88P\"  888     \"Y88888  \"Y8888\n                                             888\n                                        Y8b d88P\n                                         \"Y88P\"                v0.2.0\n\\n\"\"\"\n\nif __name__ == \"__main__\":\n    print(logo)\n    port = os.getenv(\"PORT\", 8000)\n    configure_logging()\n    logger.info(f\"Agent server starting on http://localhost:{port}\")\n    load_dotenv()\n\n    uvicorn.run(\n        \"forge.app:app\",\n        host=\"localhost\",\n        port=int(port),\n        log_level=\"error\",\n        # Reload on changes to code or .env\n        reload=True,\n        reload_dirs=os.path.dirname(os.path.dirname(__file__)),\n        reload_excludes=\"*.py\",  # Cancel default *.py include pattern\n        reload_includes=[\n            f\"{os.path.basename(os.path.dirname(__file__))}/**/*.py\",\n            \".*\",\n            \".env\",\n        ],\n    )\n", "forge/forge/__init__.py": "", "forge/forge/app.py": "import os\nfrom pathlib import Path\n\nfrom forge.agent.forge_agent import ForgeAgent\nfrom forge.agent_protocol.database.db import AgentDB\nfrom forge.file_storage import FileStorageBackendName, get_storage\n\ndatabase_name = os.getenv(\"DATABASE_STRING\")\nworkspace = get_storage(FileStorageBackendName.LOCAL, root_path=Path(\"workspace\"))\ndatabase = AgentDB(database_name, debug_enabled=False)\nagent = ForgeAgent(database=database, workspace=workspace)\n\napp = agent.get_agent_app()\n", "forge/forge/models/action.py": "from __future__ import annotations\n\nfrom typing import Any, Literal, Optional, TypeVar\n\nfrom pydantic import BaseModel\n\nfrom forge.llm.providers.schema import AssistantFunctionCall\n\nfrom .utils import ModelWithSummary\n\n\nclass ActionProposal(BaseModel):\n    thoughts: str | ModelWithSummary\n    use_tool: AssistantFunctionCall\n\n\nAnyProposal = TypeVar(\"AnyProposal\", bound=ActionProposal)\n\n\nclass ActionSuccessResult(BaseModel):\n    outputs: Any\n    status: Literal[\"success\"] = \"success\"\n\n    def __str__(self) -> str:\n        outputs = str(self.outputs).replace(\"```\", r\"\\```\")\n        multiline = \"\\n\" in outputs\n        return f\"```\\n{self.outputs}\\n```\" if multiline else str(self.outputs)\n\n\nclass ErrorInfo(BaseModel):\n    args: tuple\n    message: str\n    exception_type: str\n    repr: str\n\n    @staticmethod\n    def from_exception(exception: Exception) -> ErrorInfo:\n        return ErrorInfo(\n            args=exception.args,\n            message=getattr(exception, \"message\", exception.args[0]),\n            exception_type=exception.__class__.__name__,\n            repr=repr(exception),\n        )\n\n    def __str__(self):\n        return repr(self)\n\n    def __repr__(self):\n        return self.repr\n\n\nclass ActionErrorResult(BaseModel):\n    reason: str\n    error: Optional[ErrorInfo] = None\n    status: Literal[\"error\"] = \"error\"\n\n    @staticmethod\n    def from_exception(exception: Exception) -> ActionErrorResult:\n        return ActionErrorResult(\n            reason=getattr(exception, \"message\", exception.args[0]),\n            error=ErrorInfo.from_exception(exception),\n        )\n\n    def __str__(self) -> str:\n        return f\"Action failed: '{self.reason}'\"\n\n\nclass ActionInterruptedByHuman(BaseModel):\n    feedback: str\n    status: Literal[\"interrupted_by_human\"] = \"interrupted_by_human\"\n\n    def __str__(self) -> str:\n        return (\n            'The user interrupted the action with the following feedback: \"%s\"'\n            % self.feedback\n        )\n\n\nActionResult = ActionSuccessResult | ActionErrorResult | ActionInterruptedByHuman\n", "forge/forge/models/config.py": "import os\nimport typing\nfrom typing import Any, Callable, Generic, Optional, Type, TypeVar, get_args\n\nfrom pydantic import BaseModel, Field, ValidationError\nfrom pydantic.fields import ModelField, Undefined, UndefinedType\nfrom pydantic.main import ModelMetaclass\n\nT = TypeVar(\"T\")\nM = TypeVar(\"M\", bound=BaseModel)\n\n\ndef UserConfigurable(\n    default: T | UndefinedType = Undefined,\n    *args,\n    default_factory: Optional[Callable[[], T]] = None,\n    from_env: Optional[str | Callable[[], T | None]] = None,\n    description: str = \"\",\n    exclude: bool = False,\n    **kwargs,\n) -> T:\n    # TODO: use this to auto-generate docs for the application configuration\n    return Field(\n        default,\n        *args,\n        default_factory=default_factory,\n        from_env=from_env,\n        description=description,\n        exclude=exclude,\n        **kwargs,\n        user_configurable=True,\n    )\n\n\nclass SystemConfiguration(BaseModel):\n    def get_user_config(self) -> dict[str, Any]:\n        return _recurse_user_config_values(self)\n\n    @classmethod\n    def from_env(cls):\n        \"\"\"\n        Initializes the config object from environment variables.\n\n        Environment variables are mapped to UserConfigurable fields using the from_env\n        attribute that can be passed to UserConfigurable.\n        \"\"\"\n\n        def infer_field_value(field: ModelField):\n            field_info = field.field_info\n            default_value = (\n                field.default\n                if field.default not in (None, Undefined)\n                else (field.default_factory() if field.default_factory else Undefined)\n            )\n            if from_env := field_info.extra.get(\"from_env\"):\n                val_from_env = (\n                    os.getenv(from_env) if type(from_env) is str else from_env()\n                )\n                if val_from_env is not None:\n                    return val_from_env\n            return default_value\n\n        return _recursive_init_model(cls, infer_field_value)\n\n    class Config:\n        extra = \"forbid\"\n        use_enum_values = True\n        validate_assignment = True\n\n\nSC = TypeVar(\"SC\", bound=SystemConfiguration)\n\n\nclass SystemSettings(BaseModel):\n    \"\"\"A base class for all system settings.\"\"\"\n\n    name: str\n    description: str\n\n    class Config:\n        extra = \"forbid\"\n        use_enum_values = True\n        validate_assignment = True\n\n\nS = TypeVar(\"S\", bound=SystemSettings)\n\n\nclass Configurable(Generic[S]):\n    \"\"\"A base class for all configurable objects.\"\"\"\n\n    prefix: str = \"\"\n    default_settings: typing.ClassVar[S]  # type: ignore\n\n    @classmethod\n    def get_user_config(cls) -> dict[str, Any]:\n        return _recurse_user_config_values(cls.default_settings)\n\n    @classmethod\n    def build_agent_configuration(cls, overrides: dict = {}) -> S:\n        \"\"\"Process the configuration for this object.\"\"\"\n\n        base_config = _update_user_config_from_env(cls.default_settings)\n        final_configuration = deep_update(base_config, overrides)\n\n        return cls.default_settings.__class__.parse_obj(final_configuration)\n\n\ndef _update_user_config_from_env(instance: BaseModel) -> dict[str, Any]:\n    \"\"\"\n    Update config fields of a Pydantic model instance from environment variables.\n\n    Precedence:\n    1. Non-default value already on the instance\n    2. Value returned by `from_env()`\n    3. Default value for the field\n\n    Params:\n        instance: The Pydantic model instance.\n\n    Returns:\n        The user config fields of the instance.\n    \"\"\"\n\n    def infer_field_value(field: ModelField, value):\n        field_info = field.field_info\n        default_value = (\n            field.default\n            if field.default not in (None, Undefined)\n            else (field.default_factory() if field.default_factory else None)\n        )\n        if value == default_value and (from_env := field_info.extra.get(\"from_env\")):\n            val_from_env = os.getenv(from_env) if type(from_env) is str else from_env()\n            if val_from_env is not None:\n                return val_from_env\n        return value\n\n    def init_sub_config(model: Type[SC]) -> SC | None:\n        try:\n            return model.from_env()\n        except ValidationError as e:\n            # Gracefully handle missing fields\n            if all(e[\"type\"] == \"value_error.missing\" for e in e.errors()):\n                return None\n            raise\n\n    return _recurse_user_config_fields(instance, infer_field_value, init_sub_config)\n\n\ndef _recursive_init_model(\n    model: Type[M],\n    infer_field_value: Callable[[ModelField], Any],\n) -> M:\n    \"\"\"\n    Recursively initialize the user configuration fields of a Pydantic model.\n\n    Parameters:\n        model: The Pydantic model type.\n        infer_field_value: A callback function to infer the value of each field.\n            Parameters:\n                ModelField: The Pydantic ModelField object describing the field.\n\n    Returns:\n        BaseModel: An instance of the model with the initialized configuration.\n    \"\"\"\n    user_config_fields = {}\n    for name, field in model.__fields__.items():\n        if \"user_configurable\" in field.field_info.extra:\n            user_config_fields[name] = infer_field_value(field)\n        elif type(field.outer_type_) is ModelMetaclass and issubclass(\n            field.outer_type_, SystemConfiguration\n        ):\n            try:\n                user_config_fields[name] = _recursive_init_model(\n                    model=field.outer_type_,\n                    infer_field_value=infer_field_value,\n                )\n            except ValidationError as e:\n                # Gracefully handle missing fields\n                if all(e[\"type\"] == \"value_error.missing\" for e in e.errors()):\n                    user_config_fields[name] = None\n                raise\n\n    user_config_fields = remove_none_items(user_config_fields)\n\n    return model.parse_obj(user_config_fields)\n\n\ndef _recurse_user_config_fields(\n    model: BaseModel,\n    infer_field_value: Callable[[ModelField, Any], Any],\n    init_sub_config: Optional[\n        Callable[[Type[SystemConfiguration]], SystemConfiguration | None]\n    ] = None,\n) -> dict[str, Any]:\n    \"\"\"\n    Recursively process the user configuration fields of a Pydantic model instance.\n\n    Params:\n        model: The Pydantic model to iterate over.\n        infer_field_value: A callback function to process each field.\n            Params:\n                ModelField: The Pydantic ModelField object describing the field.\n                Any: The current value of the field.\n        init_sub_config: An optional callback function to initialize a sub-config.\n            Params:\n                Type[SystemConfiguration]: The type of the sub-config to initialize.\n\n    Returns:\n        dict[str, Any]: The processed user configuration fields of the instance.\n    \"\"\"\n    user_config_fields = {}\n\n    for name, field in model.__fields__.items():\n        value = getattr(model, name)\n\n        # Handle individual field\n        if \"user_configurable\" in field.field_info.extra:\n            user_config_fields[name] = infer_field_value(field, value)\n\n        # Recurse into nested config object\n        elif isinstance(value, SystemConfiguration):\n            user_config_fields[name] = _recurse_user_config_fields(\n                model=value,\n                infer_field_value=infer_field_value,\n                init_sub_config=init_sub_config,\n            )\n\n        # Recurse into optional nested config object\n        elif value is None and init_sub_config:\n            field_type = get_args(field.annotation)[0]  # Optional[T] -> T\n            if type(field_type) is ModelMetaclass and issubclass(\n                field_type, SystemConfiguration\n            ):\n                sub_config = init_sub_config(field_type)\n                if sub_config:\n                    user_config_fields[name] = _recurse_user_config_fields(\n                        model=sub_config,\n                        infer_field_value=infer_field_value,\n                        init_sub_config=init_sub_config,\n                    )\n\n        elif isinstance(value, list) and all(\n            isinstance(i, SystemConfiguration) for i in value\n        ):\n            user_config_fields[name] = [\n                _recurse_user_config_fields(i, infer_field_value, init_sub_config)\n                for i in value\n            ]\n        elif isinstance(value, dict) and all(\n            isinstance(i, SystemConfiguration) for i in value.values()\n        ):\n            user_config_fields[name] = {\n                k: _recurse_user_config_fields(v, infer_field_value, init_sub_config)\n                for k, v in value.items()\n            }\n\n    return user_config_fields\n\n\ndef _recurse_user_config_values(\n    instance: BaseModel,\n    get_field_value: Callable[[ModelField, T], T] = lambda _, v: v,\n) -> dict[str, Any]:\n    \"\"\"\n    This function recursively traverses the user configuration values in a Pydantic\n    model instance.\n\n    Params:\n        instance: A Pydantic model instance.\n        get_field_value: A callback function to process each field. Parameters:\n            ModelField: The Pydantic ModelField object that describes the field.\n            Any: The current value of the field.\n\n    Returns:\n        A dictionary containing the processed user configuration fields of the instance.\n    \"\"\"\n    user_config_values = {}\n\n    for name, value in instance.__dict__.items():\n        field = instance.__fields__[name]\n        if \"user_configurable\" in field.field_info.extra:\n            user_config_values[name] = get_field_value(field, value)\n        elif isinstance(value, SystemConfiguration):\n            user_config_values[name] = _recurse_user_config_values(\n                instance=value, get_field_value=get_field_value\n            )\n        elif isinstance(value, list) and all(\n            isinstance(i, SystemConfiguration) for i in value\n        ):\n            user_config_values[name] = [\n                _recurse_user_config_values(i, get_field_value) for i in value\n            ]\n        elif isinstance(value, dict) and all(\n            isinstance(i, SystemConfiguration) for i in value.values()\n        ):\n            user_config_values[name] = {\n                k: _recurse_user_config_values(v, get_field_value)\n                for k, v in value.items()\n            }\n\n    return user_config_values\n\n\ndef _get_non_default_user_config_values(instance: BaseModel) -> dict[str, Any]:\n    \"\"\"\n    Get the non-default user config fields of a Pydantic model instance.\n\n    Params:\n        instance: The Pydantic model instance.\n\n    Returns:\n        dict[str, Any]: The non-default user config values on the instance.\n    \"\"\"\n\n    def get_field_value(field: ModelField, value):\n        default = field.default_factory() if field.default_factory else field.default\n        if value != default:\n            return value\n\n    return remove_none_items(_recurse_user_config_values(instance, get_field_value))\n\n\ndef deep_update(original_dict: dict, update_dict: dict) -> dict:\n    \"\"\"\n    Recursively update a dictionary.\n\n    Params:\n        original_dict (dict): The dictionary to be updated.\n        update_dict (dict): The dictionary to update with.\n\n    Returns:\n        dict: The updated dictionary.\n    \"\"\"\n    for key, value in update_dict.items():\n        if (\n            key in original_dict\n            and isinstance(original_dict[key], dict)\n            and isinstance(value, dict)\n        ):\n            original_dict[key] = deep_update(original_dict[key], value)\n        else:\n            original_dict[key] = value\n    return original_dict\n\n\ndef remove_none_items(d):\n    if isinstance(d, dict):\n        return {\n            k: remove_none_items(v) for k, v in d.items() if v not in (None, Undefined)\n        }\n    return d\n", "forge/forge/models/utils.py": "from abc import ABC, abstractmethod\n\nfrom pydantic import BaseModel\n\n\nclass ModelWithSummary(BaseModel, ABC):\n    @abstractmethod\n    def summary(self) -> str:\n        \"\"\"Should produce a human readable summary of the model content.\"\"\"\n        pass\n", "forge/forge/models/providers.py": "import abc\nimport enum\nimport math\nfrom typing import Callable, Generic, TypeVar\n\nfrom pydantic import BaseModel, SecretBytes, SecretField, SecretStr\n\nfrom forge.models.config import SystemConfiguration, UserConfigurable\n\n_T = TypeVar(\"_T\")\n\n\nclass ResourceType(str, enum.Enum):\n    \"\"\"An enumeration of resource types.\"\"\"\n\n    MODEL = \"model\"\n\n\nclass ProviderBudget(SystemConfiguration, Generic[_T]):\n    total_budget: float = UserConfigurable(math.inf)\n    total_cost: float = 0\n    remaining_budget: float = math.inf\n    usage: _T\n\n    @abc.abstractmethod\n    def update_usage_and_cost(self, *args, **kwargs) -> float:\n        \"\"\"Update the usage and cost of the provider.\n\n        Returns:\n            float: The (calculated) cost of the given model response.\n        \"\"\"\n        ...\n\n\nclass ProviderCredentials(SystemConfiguration):\n    \"\"\"Struct for credentials.\"\"\"\n\n    def unmasked(self) -> dict:\n        return unmask(self)\n\n    class Config(SystemConfiguration.Config):\n        json_encoders: dict[type[SecretField], Callable[[SecretField], str | None]] = {\n            SecretStr: lambda v: v.get_secret_value() if v else None,\n            SecretBytes: lambda v: v.get_secret_value() if v else None,\n            SecretField: lambda v: v.get_secret_value() if v else None,\n        }\n\n\ndef unmask(model: BaseModel):\n    unmasked_fields = {}\n    for field_name, _ in model.__fields__.items():\n        value = getattr(model, field_name)\n        if isinstance(value, SecretStr):\n            unmasked_fields[field_name] = value.get_secret_value()\n        else:\n            unmasked_fields[field_name] = value\n    return unmasked_fields\n\n\n# Used both by model providers and memory providers\nEmbedding = list[float]\n", "forge/forge/models/json_schema.py": "import enum\nfrom textwrap import indent\nfrom typing import Optional, overload\n\nfrom jsonschema import Draft7Validator, ValidationError\nfrom pydantic import BaseModel\n\n\nclass JSONSchema(BaseModel):\n    class Type(str, enum.Enum):\n        STRING = \"string\"\n        ARRAY = \"array\"\n        OBJECT = \"object\"\n        NUMBER = \"number\"\n        INTEGER = \"integer\"\n        BOOLEAN = \"boolean\"\n\n    # TODO: add docstrings\n    description: Optional[str] = None\n    type: Optional[Type] = None\n    enum: Optional[list] = None\n    required: bool = False\n    items: Optional[\"JSONSchema\"] = None\n    properties: Optional[dict[str, \"JSONSchema\"]] = None\n    minimum: Optional[int | float] = None\n    maximum: Optional[int | float] = None\n    minItems: Optional[int] = None\n    maxItems: Optional[int] = None\n\n    def to_dict(self) -> dict:\n        schema: dict = {\n            \"type\": self.type.value if self.type else None,\n            \"description\": self.description,\n        }\n        if self.type == \"array\":\n            if self.items:\n                schema[\"items\"] = self.items.to_dict()\n            schema[\"minItems\"] = self.minItems\n            schema[\"maxItems\"] = self.maxItems\n        elif self.type == \"object\":\n            if self.properties:\n                schema[\"properties\"] = {\n                    name: prop.to_dict() for name, prop in self.properties.items()\n                }\n                schema[\"required\"] = [\n                    name for name, prop in self.properties.items() if prop.required\n                ]\n        elif self.enum:\n            schema[\"enum\"] = self.enum\n        else:\n            schema[\"minumum\"] = self.minimum\n            schema[\"maximum\"] = self.maximum\n\n        schema = {k: v for k, v in schema.items() if v is not None}\n\n        return schema\n\n    @staticmethod\n    def from_dict(schema: dict) -> \"JSONSchema\":\n        definitions = schema.get(\"definitions\", {})\n        schema = _resolve_type_refs_in_schema(schema, definitions)\n\n        return JSONSchema(\n            description=schema.get(\"description\"),\n            type=schema[\"type\"],\n            enum=schema.get(\"enum\"),\n            items=JSONSchema.from_dict(schema[\"items\"]) if \"items\" in schema else None,\n            properties=JSONSchema.parse_properties(schema)\n            if schema[\"type\"] == \"object\"\n            else None,\n            minimum=schema.get(\"minimum\"),\n            maximum=schema.get(\"maximum\"),\n            minItems=schema.get(\"minItems\"),\n            maxItems=schema.get(\"maxItems\"),\n        )\n\n    @staticmethod\n    def parse_properties(schema_node: dict) -> dict[str, \"JSONSchema\"]:\n        properties = (\n            {k: JSONSchema.from_dict(v) for k, v in schema_node[\"properties\"].items()}\n            if \"properties\" in schema_node\n            else {}\n        )\n        if \"required\" in schema_node:\n            for k, v in properties.items():\n                v.required = k in schema_node[\"required\"]\n        return properties\n\n    def validate_object(self, object: object) -> tuple[bool, list[ValidationError]]:\n        \"\"\"\n        Validates an object or a value against the JSONSchema.\n\n        Params:\n            object: The value/object to validate.\n            schema (JSONSchema): The JSONSchema to validate against.\n\n        Returns:\n            bool: Indicates whether the given value or object is valid for the schema.\n            list[ValidationError]: The issues with the value or object (if any).\n        \"\"\"\n        validator = Draft7Validator(self.to_dict())\n\n        if errors := sorted(validator.iter_errors(object), key=lambda e: e.path):\n            return False, errors\n\n        return True, []\n\n    def to_typescript_object_interface(self, interface_name: str = \"\") -> str:\n        if self.type != JSONSchema.Type.OBJECT:\n            raise NotImplementedError(\"Only `object` schemas are supported\")\n\n        if self.properties:\n            attributes: list[str] = []\n            for name, property in self.properties.items():\n                if property.description:\n                    attributes.append(f\"// {property.description}\")\n                attributes.append(f\"{name}: {property.typescript_type};\")\n            attributes_string = \"\\n\".join(attributes)\n        else:\n            attributes_string = \"[key: string]: any\"\n\n        return (\n            f\"interface {interface_name} \" if interface_name else \"\"\n        ) + f\"{{\\n{indent(attributes_string, '  ')}\\n}}\"\n\n    @property\n    def typescript_type(self) -> str:\n        if not self.type:\n            return \"any\"\n        if self.type == JSONSchema.Type.BOOLEAN:\n            return \"boolean\"\n        if self.type in {JSONSchema.Type.INTEGER, JSONSchema.Type.NUMBER}:\n            return \"number\"\n        if self.type == JSONSchema.Type.STRING:\n            return \"string\"\n        if self.type == JSONSchema.Type.ARRAY:\n            return f\"Array<{self.items.typescript_type}>\" if self.items else \"Array\"\n        if self.type == JSONSchema.Type.OBJECT:\n            if not self.properties:\n                return \"Record<string, any>\"\n            return self.to_typescript_object_interface()\n        if self.enum:\n            return \" | \".join(repr(v) for v in self.enum)\n\n        raise NotImplementedError(\n            f\"JSONSchema.typescript_type does not support Type.{self.type.name} yet\"\n        )\n\n\n@overload\ndef _resolve_type_refs_in_schema(schema: dict, definitions: dict) -> dict:\n    ...\n\n\n@overload\ndef _resolve_type_refs_in_schema(schema: list, definitions: dict) -> list:\n    ...\n\n\ndef _resolve_type_refs_in_schema(schema: dict | list, definitions: dict) -> dict | list:\n    \"\"\"\n    Recursively resolve type $refs in the JSON schema with their definitions.\n    \"\"\"\n    if isinstance(schema, dict):\n        if \"$ref\" in schema:\n            ref_path = schema[\"$ref\"].split(\"/\")[2:]  # Split and remove '#/definitions'\n            ref_value = definitions\n            for key in ref_path:\n                ref_value = ref_value[key]\n            return _resolve_type_refs_in_schema(ref_value, definitions)\n        else:\n            return {\n                k: _resolve_type_refs_in_schema(v, definitions)\n                for k, v in schema.items()\n            }\n    elif isinstance(schema, list):\n        return [_resolve_type_refs_in_schema(item, definitions) for item in schema]\n    else:\n        return schema\n", "forge/forge/utils/exceptions.py": "import inspect\nimport sys\nimport traceback\nfrom typing import Optional\n\n\ndef get_exception_message():\n    \"\"\"Get current exception type and message.\"\"\"\n    exc_type, exc_value, _ = sys.exc_info()\n    exception_message = f\"{exc_type.__name__}: {exc_value}\" if exc_type else exc_value\n    return exception_message\n\n\ndef get_detailed_traceback():\n    \"\"\"Get current exception traceback with local variables.\"\"\"\n    _, _, exc_tb = sys.exc_info()\n    detailed_traceback = \"Traceback (most recent call last):\\n\"\n    formatted_tb = traceback.format_tb(exc_tb)\n    detailed_traceback += \"\".join(formatted_tb)\n\n    # Optionally add local variables to the traceback information\n    detailed_traceback += \"\\nLocal variables by frame, innermost last:\\n\"\n    while exc_tb:\n        frame = exc_tb.tb_frame\n        lineno = exc_tb.tb_lineno\n        function_name = frame.f_code.co_name\n\n        # Format frame information\n        detailed_traceback += (\n            f\"  Frame {function_name} in {frame.f_code.co_filename} at line {lineno}\\n\"\n        )\n\n        # Get local variables for the frame\n        local_vars = inspect.getargvalues(frame).locals\n        for var_name, value in local_vars.items():\n            detailed_traceback += f\"    {var_name} = {value}\\n\"\n\n        exc_tb = exc_tb.tb_next\n\n    return detailed_traceback\n\n\nclass NotFoundError(Exception):\n    pass\n\n\nclass AgentException(Exception):\n    \"\"\"Base class for specific exceptions relevant in the execution of Agents\"\"\"\n\n    message: str\n\n    hint: Optional[str] = None\n    \"\"\"A hint which can be passed to the LLM to reduce reoccurrence of this error\"\"\"\n\n    def __init__(self, message: str, *args):\n        self.message = message\n        super().__init__(message, *args)\n\n\nclass AgentTerminated(AgentException):\n    \"\"\"The agent terminated or was terminated\"\"\"\n\n\nclass AgentFinished(AgentTerminated):\n    \"\"\"The agent self-terminated\"\"\"\n\n\nclass ConfigurationError(AgentException):\n    \"\"\"Error caused by invalid, incompatible or otherwise incorrect configuration\"\"\"\n\n\nclass InvalidAgentResponseError(AgentException):\n    \"\"\"The LLM deviated from the prescribed response format\"\"\"\n\n\nclass UnknownCommandError(AgentException):\n    \"\"\"The AI tried to use an unknown command\"\"\"\n\n    hint = \"Do not try to use this command again.\"\n\n\nclass CommandExecutionError(AgentException):\n    \"\"\"An error occurred when trying to execute the command\"\"\"\n\n\nclass InvalidArgumentError(CommandExecutionError):\n    \"\"\"The command received an invalid argument\"\"\"\n\n\nclass OperationNotAllowedError(CommandExecutionError):\n    \"\"\"The agent is not allowed to execute the proposed operation\"\"\"\n\n\nclass TooMuchOutputError(CommandExecutionError):\n    \"\"\"The operation generated more output than what the Agent can process\"\"\"\n", "forge/forge/utils/url_validator.py": "import functools\nimport re\nfrom inspect import signature\nfrom typing import Callable, ParamSpec, TypeVar\nfrom urllib.parse import urljoin, urlparse\n\nP = ParamSpec(\"P\")\nT = TypeVar(\"T\")\n\n\ndef validate_url(func: Callable[P, T]) -> Callable[P, T]:\n    \"\"\"\n    The method decorator validate_url is used to validate urls for any command that\n    requires a url as an argument.\n    \"\"\"\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        sig = signature(func)\n        bound_args = sig.bind(*args, **kwargs)\n        bound_args.apply_defaults()\n\n        url = bound_args.arguments.get(\"url\")\n        if url is None:\n            raise ValueError(\"URL is required for this function\")\n\n        if not re.match(r\"^https?://\", url):\n            raise ValueError(\n                \"Invalid URL format: URL must start with http:// or https://\"\n            )\n        if not is_valid_url(url):\n            raise ValueError(\"Missing Scheme or Network location\")\n        if check_local_file_access(url):\n            raise ValueError(\"Access to local files is restricted\")\n        if len(url) > 2000:\n            raise ValueError(\"URL is too long\")\n\n        bound_args.arguments[\"url\"] = sanitize_url(url)\n\n        return func(*bound_args.args, **bound_args.kwargs)\n\n    return wrapper  # type: ignore\n\n\ndef is_valid_url(url: str) -> bool:\n    \"\"\"Check if the URL is valid\n\n    Args:\n        url (str): The URL to check\n\n    Returns:\n        bool: True if the URL is valid, False otherwise\n    \"\"\"\n    try:\n        result = urlparse(url)\n        return all([result.scheme, result.netloc])\n    except ValueError:\n        return False\n\n\ndef sanitize_url(url: str) -> str:\n    \"\"\"Sanitize the URL\n\n    Args:\n        url (str): The URL to sanitize\n\n    Returns:\n        str: The sanitized URL\n    \"\"\"\n    parsed_url = urlparse(url)\n    reconstructed_url = f\"{parsed_url.path}{parsed_url.params}?{parsed_url.query}\"\n    return urljoin(url, reconstructed_url)\n\n\ndef check_local_file_access(url: str) -> bool:\n    \"\"\"Check if the URL is a local file\n\n    Args:\n        url (str): The URL to check\n\n    Returns:\n        bool: True if the URL is a local file, False otherwise\n    \"\"\"\n    # List of local file prefixes\n    local_file_prefixes = [\n        \"file:///\",\n        \"file://localhost\",\n    ]\n\n    return any(url.startswith(prefix) for prefix in local_file_prefixes)\n", "forge/forge/utils/file_operations.py": "import json\nimport logging\nfrom abc import ABC, abstractmethod\nfrom typing import BinaryIO\n\nimport charset_normalizer\nimport docx\nimport pypdf\nimport yaml\nfrom bs4 import BeautifulSoup\nfrom pylatexenc.latex2text import LatexNodes2Text\n\nlogger = logging.getLogger(__name__)\n\n\nclass ParserStrategy(ABC):\n    @abstractmethod\n    def read(self, file: BinaryIO) -> str:\n        ...\n\n\n# Basic text file reading\nclass TXTParser(ParserStrategy):\n    def read(self, file: BinaryIO) -> str:\n        charset_match = charset_normalizer.from_bytes(file.read()).best()\n        logger.debug(\n            f\"Reading {getattr(file, 'name', 'file')} \"\n            f\"with encoding '{charset_match.encoding if charset_match else None}'\"\n        )\n        return str(charset_match)\n\n\n# Reading text from binary file using pdf parser\nclass PDFParser(ParserStrategy):\n    def read(self, file: BinaryIO) -> str:\n        parser = pypdf.PdfReader(file)\n        text = \"\"\n        for page_idx in range(len(parser.pages)):\n            text += parser.pages[page_idx].extract_text()\n        return text\n\n\n# Reading text from binary file using docs parser\nclass DOCXParser(ParserStrategy):\n    def read(self, file: BinaryIO) -> str:\n        doc_file = docx.Document(file)\n        text = \"\"\n        for para in doc_file.paragraphs:\n            text += para.text\n        return text\n\n\n# Reading as dictionary and returning string format\nclass JSONParser(ParserStrategy):\n    def read(self, file: BinaryIO) -> str:\n        data = json.load(file)\n        text = str(data)\n        return text\n\n\nclass XMLParser(ParserStrategy):\n    def read(self, file: BinaryIO) -> str:\n        soup = BeautifulSoup(file, \"xml\")\n        text = soup.get_text()\n        return text\n\n\n# Reading as dictionary and returning string format\nclass YAMLParser(ParserStrategy):\n    def read(self, file: BinaryIO) -> str:\n        data = yaml.load(file, Loader=yaml.SafeLoader)\n        text = str(data)\n        return text\n\n\nclass HTMLParser(ParserStrategy):\n    def read(self, file: BinaryIO) -> str:\n        soup = BeautifulSoup(file, \"html.parser\")\n        text = soup.get_text()\n        return text\n\n\nclass LaTeXParser(ParserStrategy):\n    def read(self, file: BinaryIO) -> str:\n        latex = file.read().decode()\n        text = LatexNodes2Text().latex_to_text(latex)\n        return text\n\n\nclass FileContext:\n    def __init__(self, parser: ParserStrategy, logger: logging.Logger):\n        self.parser = parser\n        self.logger = logger\n\n    def set_parser(self, parser: ParserStrategy) -> None:\n        self.logger.debug(f\"Setting Context Parser to {parser}\")\n        self.parser = parser\n\n    def decode_file(self, file: BinaryIO) -> str:\n        self.logger.debug(\n            f\"Reading {getattr(file, 'name', 'file')} with parser {self.parser}\"\n        )\n        return self.parser.read(file)\n\n\nextension_to_parser = {\n    \".txt\": TXTParser(),\n    \".md\": TXTParser(),\n    \".markdown\": TXTParser(),\n    \".csv\": TXTParser(),\n    \".pdf\": PDFParser(),\n    \".docx\": DOCXParser(),\n    \".json\": JSONParser(),\n    \".xml\": XMLParser(),\n    \".yaml\": YAMLParser(),\n    \".yml\": YAMLParser(),\n    \".html\": HTMLParser(),\n    \".htm\": HTMLParser(),\n    \".xhtml\": HTMLParser(),\n    \".tex\": LaTeXParser(),\n}\n\n\ndef is_file_binary_fn(file: BinaryIO):\n    \"\"\"Given a file path load all its content and checks if the null bytes is present\n\n    Args:\n        file (_type_): _description_\n\n    Returns:\n        bool: is_binary\n    \"\"\"\n    file_data = file.read()\n    file.seek(0)\n    if b\"\\x00\" in file_data:\n        return True\n    return False\n\n\ndef decode_textual_file(file: BinaryIO, ext: str, logger: logging.Logger) -> str:\n    if not file.readable():\n        raise ValueError(f\"{repr(file)} is not readable\")\n\n    parser = extension_to_parser.get(ext.lower())\n    if not parser:\n        if is_file_binary_fn(file):\n            raise ValueError(f\"Unsupported binary file format: {ext}\")\n        # fallback to txt file parser (to support script and code files loading)\n        parser = TXTParser()\n    file_context = FileContext(parser, logger)\n    return file_context.decode_file(file)\n", "forge/forge/utils/const.py": "FINISH_COMMAND = \"finish\"\nASK_COMMAND = \"ask_user\"\n", "forge/forge/components/code_executor/code_executor.py": "import logging\nimport os\nimport random\nimport shlex\nimport string\nimport subprocess\nfrom pathlib import Path\nfrom typing import Iterator, Literal, Optional\n\nimport docker\nfrom docker.errors import DockerException, ImageNotFound, NotFound\nfrom docker.models.containers import Container as DockerContainer\nfrom pydantic import BaseModel, Field\n\nfrom forge.agent.components import ConfigurableComponent\nfrom forge.agent.protocols import CommandProvider\nfrom forge.command import Command, command\nfrom forge.file_storage import FileStorage\nfrom forge.models.json_schema import JSONSchema\nfrom forge.utils.exceptions import (\n    CommandExecutionError,\n    InvalidArgumentError,\n    OperationNotAllowedError,\n)\n\nlogger = logging.getLogger(__name__)\n\n\ndef we_are_running_in_a_docker_container() -> bool:\n    \"\"\"Check if we are running in a Docker container\n\n    Returns:\n        bool: True if we are running in a Docker container, False otherwise\n    \"\"\"\n    return os.path.exists(\"/.dockerenv\")\n\n\ndef is_docker_available() -> bool:\n    \"\"\"Check if Docker is available and supports Linux containers\n\n    Returns:\n        bool: True if Docker is available and supports Linux containers, False otherwise\n    \"\"\"\n    try:\n        client = docker.from_env()\n        docker_info = client.info()\n        return docker_info[\"OSType\"] == \"linux\"\n    except Exception:\n        return False\n\n\nclass CodeExecutionError(CommandExecutionError):\n    \"\"\"The operation (an attempt to run arbitrary code) returned an error\"\"\"\n\n\nclass CodeExecutorConfiguration(BaseModel):\n    execute_local_commands: bool = False\n    \"\"\"Enable shell command execution\"\"\"\n    shell_command_control: Literal[\"allowlist\", \"denylist\"] = \"allowlist\"\n    \"\"\"Controls which list is used\"\"\"\n    shell_allowlist: list[str] = Field(default_factory=list)\n    \"\"\"List of allowed shell commands\"\"\"\n    shell_denylist: list[str] = Field(default_factory=list)\n    \"\"\"List of prohibited shell commands\"\"\"\n    docker_container_name: str = \"agent_sandbox\"\n    \"\"\"Name of the Docker container used for code execution\"\"\"\n\n\nclass CodeExecutorComponent(\n    CommandProvider, ConfigurableComponent[CodeExecutorConfiguration]\n):\n    \"\"\"Provides commands to execute Python code and shell commands.\"\"\"\n\n    config_class = CodeExecutorConfiguration\n\n    def __init__(\n        self,\n        workspace: FileStorage,\n        config: Optional[CodeExecutorConfiguration] = None,\n    ):\n        ConfigurableComponent.__init__(self, config)\n        self.workspace = workspace\n\n        # Change container name if it's empty or default to prevent different agents\n        # from using the same container\n        default_container_name = self.config.__fields__[\"docker_container_name\"].default\n        if (\n            not self.config.docker_container_name\n            or self.config.docker_container_name == default_container_name\n        ):\n            random_suffix = \"\".join(random.choices(string.ascii_lowercase, k=8))\n            self.config.docker_container_name = (\n                f\"{default_container_name}_{random_suffix}\"\n            )\n\n        if not we_are_running_in_a_docker_container() and not is_docker_available():\n            logger.info(\n                \"Docker is not available or does not support Linux containers. \"\n                \"The code execution commands will not be available.\"\n            )\n\n        if not self.config.execute_local_commands:\n            logger.info(\n                \"Local shell commands are disabled. To enable them,\"\n                \" set EXECUTE_LOCAL_COMMANDS to 'True' in your config file.\"\n            )\n\n    def get_commands(self) -> Iterator[Command]:\n        if we_are_running_in_a_docker_container() or is_docker_available():\n            yield self.execute_python_code\n            yield self.execute_python_file\n\n        if self.config.execute_local_commands:\n            yield self.execute_shell\n            yield self.execute_shell_popen\n\n    @command(\n        [\"execute_python_code\"],\n        \"Executes the given Python code inside a single-use Docker container\"\n        \" with access to your workspace folder\",\n        {\n            \"code\": JSONSchema(\n                type=JSONSchema.Type.STRING,\n                description=\"The Python code to run\",\n                required=True,\n            ),\n        },\n    )\n    async def execute_python_code(self, code: str) -> str:\n        \"\"\"\n        Create and execute a Python file in a Docker container\n        and return the STDOUT of the executed code.\n\n        If the code generates any data that needs to be captured,\n        use a print statement.\n\n        Args:\n            code (str): The Python code to run.\n            agent (Agent): The Agent executing the command.\n\n        Returns:\n            str: The STDOUT captured from the code when it ran.\n        \"\"\"\n\n        temp_path = \"\"\n        while True:\n            temp_path = f\"temp{self._generate_random_string()}.py\"\n            if not self.workspace.exists(temp_path):\n                break\n        await self.workspace.write_file(temp_path, code)\n\n        try:\n            return self.execute_python_file(temp_path)\n        except Exception as e:\n            raise CommandExecutionError(*e.args)\n        finally:\n            self.workspace.delete_file(temp_path)\n\n    @command(\n        [\"execute_python_file\"],\n        \"Execute an existing Python file inside a single-use Docker container\"\n        \" with access to your workspace folder\",\n        {\n            \"filename\": JSONSchema(\n                type=JSONSchema.Type.STRING,\n                description=\"The name of the file to execute\",\n                required=True,\n            ),\n            \"args\": JSONSchema(\n                type=JSONSchema.Type.ARRAY,\n                description=\"The (command line) arguments to pass to the script\",\n                required=False,\n                items=JSONSchema(type=JSONSchema.Type.STRING),\n            ),\n        },\n    )\n    def execute_python_file(self, filename: str | Path, args: list[str] = []) -> str:\n        \"\"\"Execute a Python file in a Docker container and return the output\n\n        Args:\n            filename (Path): The name of the file to execute\n            args (list, optional): The arguments with which to run the python script\n\n        Returns:\n            str: The output of the file\n        \"\"\"\n        logger.info(f\"Executing python file '{filename}'\")\n\n        if not str(filename).endswith(\".py\"):\n            raise InvalidArgumentError(\"Invalid file type. Only .py files are allowed.\")\n\n        file_path = self.workspace.get_path(filename)\n        if not self.workspace.exists(file_path):\n            # Mimic the response that you get from the command line to make it\n            # intuitively understandable for the LLM\n            raise FileNotFoundError(\n                f\"python: can't open file '{filename}': \"\n                f\"[Errno 2] No such file or directory\"\n            )\n\n        if we_are_running_in_a_docker_container():\n            logger.debug(\n                \"App is running in a Docker container; \"\n                f\"executing {file_path} directly...\"\n            )\n            with self.workspace.mount() as local_path:\n                result = subprocess.run(\n                    [\"python\", \"-B\", str(file_path.relative_to(self.workspace.root))]\n                    + args,\n                    capture_output=True,\n                    encoding=\"utf8\",\n                    cwd=str(local_path),\n                )\n                if result.returncode == 0:\n                    return result.stdout\n                else:\n                    raise CodeExecutionError(result.stderr)\n\n        logger.debug(\"App is not running in a Docker container\")\n        return self._run_python_code_in_docker(file_path, args)\n\n    def validate_command(self, command_line: str) -> tuple[bool, bool]:\n        \"\"\"Check whether a command is allowed and whether it may be executed in a shell.\n\n        If shell command control is enabled, we disallow executing in a shell, because\n        otherwise the model could circumvent the command filter using shell features.\n\n        Args:\n            command_line (str): The command line to validate\n            config (Config): The app config including shell command control settings\n\n        Returns:\n            bool: True if the command is allowed, False otherwise\n            bool: True if the command may be executed in a shell, False otherwise\n        \"\"\"\n        if not command_line:\n            return False, False\n\n        command_name = shlex.split(command_line)[0]\n\n        if self.config.shell_command_control == \"allowlist\":\n            return command_name in self.config.shell_allowlist, False\n        elif self.config.shell_command_control == \"denylist\":\n            return command_name not in self.config.shell_denylist, False\n        else:\n            return True, True\n\n    @command(\n        [\"execute_shell\"],\n        \"Execute a Shell Command, non-interactive commands only\",\n        {\n            \"command_line\": JSONSchema(\n                type=JSONSchema.Type.STRING,\n                description=\"The command line to execute\",\n                required=True,\n            )\n        },\n    )\n    def execute_shell(self, command_line: str) -> str:\n        \"\"\"Execute a shell command and return the output\n\n        Args:\n            command_line (str): The command line to execute\n\n        Returns:\n            str: The output of the command\n        \"\"\"\n        allow_execute, allow_shell = self.validate_command(command_line)\n        if not allow_execute:\n            logger.info(f\"Command '{command_line}' not allowed\")\n            raise OperationNotAllowedError(\"This shell command is not allowed.\")\n\n        current_dir = Path.cwd()\n        # Change dir into workspace if necessary\n        if not current_dir.is_relative_to(self.workspace.root):\n            os.chdir(self.workspace.root)\n\n        logger.info(\n            f\"Executing command '{command_line}' in working directory '{os.getcwd()}'\"\n        )\n\n        result = subprocess.run(\n            command_line if allow_shell else shlex.split(command_line),\n            capture_output=True,\n            shell=allow_shell,\n        )\n        output = f\"STDOUT:\\n{result.stdout.decode()}\\nSTDERR:\\n{result.stderr.decode()}\"\n\n        # Change back to whatever the prior working dir was\n        os.chdir(current_dir)\n\n        return output\n\n    @command(\n        [\"execute_shell_popen\"],\n        \"Execute a Shell Command, non-interactive commands only\",\n        {\n            \"command_line\": JSONSchema(\n                type=JSONSchema.Type.STRING,\n                description=\"The command line to execute\",\n                required=True,\n            )\n        },\n    )\n    def execute_shell_popen(self, command_line: str) -> str:\n        \"\"\"Execute a shell command with Popen and returns an english description\n        of the event and the process id\n\n        Args:\n            command_line (str): The command line to execute\n\n        Returns:\n            str: Description of the fact that the process started and its id\n        \"\"\"\n        allow_execute, allow_shell = self.validate_command(command_line)\n        if not allow_execute:\n            logger.info(f\"Command '{command_line}' not allowed\")\n            raise OperationNotAllowedError(\"This shell command is not allowed.\")\n\n        current_dir = Path.cwd()\n        # Change dir into workspace if necessary\n        if not current_dir.is_relative_to(self.workspace.root):\n            os.chdir(self.workspace.root)\n\n        logger.info(\n            f\"Executing command '{command_line}' in working directory '{os.getcwd()}'\"\n        )\n\n        do_not_show_output = subprocess.DEVNULL\n        process = subprocess.Popen(\n            command_line if allow_shell else shlex.split(command_line),\n            shell=allow_shell,\n            stdout=do_not_show_output,\n            stderr=do_not_show_output,\n        )\n\n        # Change back to whatever the prior working dir was\n        os.chdir(current_dir)\n\n        return f\"Subprocess started with PID:'{str(process.pid)}'\"\n\n    def _run_python_code_in_docker(self, filename: str | Path, args: list[str]) -> str:\n        \"\"\"Run a Python script in a Docker container\"\"\"\n        file_path = self.workspace.get_path(filename)\n        try:\n            client = docker.from_env()\n            image_name = \"python:3-alpine\"\n            container_is_fresh = False\n            container_name = self.config.docker_container_name\n            with self.workspace.mount() as local_path:\n                try:\n                    container: DockerContainer = client.containers.get(\n                        container_name\n                    )  # type: ignore\n                except NotFound:\n                    try:\n                        client.images.get(image_name)\n                        logger.debug(f\"Image '{image_name}' found locally\")\n                    except ImageNotFound:\n                        logger.info(\n                            f\"Image '{image_name}' not found locally,\"\n                            \" pulling from Docker Hub...\"\n                        )\n                        # Use the low-level API to stream the pull response\n                        low_level_client = docker.APIClient()\n                        for line in low_level_client.pull(\n                            image_name, stream=True, decode=True\n                        ):\n                            # Print the status and progress, if available\n                            status = line.get(\"status\")\n                            progress = line.get(\"progress\")\n                            if status and progress:\n                                logger.info(f\"{status}: {progress}\")\n                            elif status:\n                                logger.info(status)\n\n                    logger.debug(f\"Creating new {image_name} container...\")\n                    container: DockerContainer = client.containers.run(\n                        image_name,\n                        [\"sleep\", \"60\"],  # Max 60 seconds to prevent permanent hangs\n                        volumes={\n                            str(local_path.resolve()): {\n                                \"bind\": \"/workspace\",\n                                \"mode\": \"rw\",\n                            }\n                        },\n                        working_dir=\"/workspace\",\n                        stderr=True,\n                        stdout=True,\n                        detach=True,\n                        name=container_name,\n                    )  # type: ignore\n                    container_is_fresh = True\n\n                if not container.status == \"running\":\n                    container.start()\n                elif not container_is_fresh:\n                    container.restart()\n\n                logger.debug(f\"Running {file_path} in container {container.name}...\")\n\n                exec_result = container.exec_run(\n                    [\n                        \"python\",\n                        \"-B\",\n                        file_path.relative_to(self.workspace.root).as_posix(),\n                    ]\n                    + args,\n                    stderr=True,\n                    stdout=True,\n                )\n\n                if exec_result.exit_code != 0:\n                    raise CodeExecutionError(exec_result.output.decode(\"utf-8\"))\n\n                return exec_result.output.decode(\"utf-8\")\n\n        except DockerException as e:\n            logger.warning(\n                \"Could not run the script in a container. \"\n                \"If you haven't already, please install Docker: \"\n                \"https://docs.docker.com/get-docker/\"\n            )\n            raise CommandExecutionError(f\"Could not run the script in a container: {e}\")\n\n    def _generate_random_string(self, length: int = 8):\n        # Create a string of all letters and digits\n        characters = string.ascii_letters + string.digits\n        # Use random.choices to generate a random string\n        random_string = \"\".join(random.choices(characters, k=length))\n        return random_string\n", "forge/forge/components/code_executor/__init__.py": "from .code_executor import CodeExecutionError, CodeExecutorComponent\n\n__all__ = [\n    \"ALLOWLIST_CONTROL\",\n    \"DENYLIST_CONTROL\",\n    \"CodeExecutionError\",\n    \"CodeExecutorComponent\",\n]\n", "forge/forge/components/image_gen/image_gen.py": "import io\nimport json\nimport logging\nimport time\nimport uuid\nfrom base64 import b64decode\nfrom pathlib import Path\nfrom typing import Iterator, Literal, Optional\n\nimport requests\nfrom openai import OpenAI\nfrom PIL import Image\nfrom pydantic import BaseModel, SecretStr\n\nfrom forge.agent.components import ConfigurableComponent\nfrom forge.agent.protocols import CommandProvider\nfrom forge.command import Command, command\nfrom forge.file_storage import FileStorage\nfrom forge.llm.providers.openai import OpenAICredentials\nfrom forge.models.config import UserConfigurable\nfrom forge.models.json_schema import JSONSchema\n\nlogger = logging.getLogger(__name__)\n\n\nclass ImageGeneratorConfiguration(BaseModel):\n    image_provider: Literal[\"dalle\", \"huggingface\", \"sdwebui\"] = \"dalle\"\n    huggingface_image_model: str = \"CompVis/stable-diffusion-v1-4\"\n    huggingface_api_token: Optional[SecretStr] = UserConfigurable(\n        from_env=\"HUGGINGFACE_API_TOKEN\", exclude=True\n    )\n    sd_webui_url: str = \"http://localhost:7860\"\n    sd_webui_auth: Optional[SecretStr] = UserConfigurable(\n        from_env=\"SD_WEBUI_AUTH\", exclude=True\n    )\n\n\nclass ImageGeneratorComponent(\n    CommandProvider, ConfigurableComponent[ImageGeneratorConfiguration]\n):\n    \"\"\"A component that provides commands to generate images from text prompts.\"\"\"\n\n    config_class = ImageGeneratorConfiguration\n\n    def __init__(\n        self,\n        workspace: FileStorage,\n        config: Optional[ImageGeneratorConfiguration] = None,\n        openai_credentials: Optional[OpenAICredentials] = None,\n    ):\n        \"\"\"openai_credentials only needed for `dalle` provider.\"\"\"\n        ConfigurableComponent.__init__(self, config)\n        self.openai_credentials = openai_credentials\n        self._enabled = bool(self.config.image_provider)\n        self._disabled_reason = \"No image provider set.\"\n        self.workspace = workspace\n\n    def get_commands(self) -> Iterator[Command]:\n        if (\n            self.openai_credentials\n            or self.config.huggingface_api_token\n            or self.config.sd_webui_auth\n        ):\n            yield self.generate_image\n\n    @command(\n        parameters={\n            \"prompt\": JSONSchema(\n                type=JSONSchema.Type.STRING,\n                description=\"The prompt used to generate the image\",\n                required=True,\n            ),\n            \"size\": JSONSchema(\n                type=JSONSchema.Type.INTEGER,\n                description=\"The size of the image [256, 512, 1024]\",\n                required=False,\n            ),\n        },\n    )\n    def generate_image(self, prompt: str, size: int) -> str:\n        \"\"\"Generate an image from a prompt.\n\n        Args:\n            prompt (str): The prompt to use\n            size (int, optional): The size of the image. Defaults to 256.\n                Not supported by HuggingFace.\n\n        Returns:\n            str: The filename of the image\n        \"\"\"\n        filename = self.workspace.root / f\"{str(uuid.uuid4())}.jpg\"\n\n        if self.openai_credentials and (\n            self.config.image_provider == \"dalle\"\n            or not (self.config.huggingface_api_token or self.config.sd_webui_url)\n        ):\n            return self.generate_image_with_dalle(prompt, filename, size)\n\n        elif self.config.huggingface_api_token and (\n            self.config.image_provider == \"huggingface\"\n            or not (self.openai_credentials or self.config.sd_webui_url)\n        ):\n            return self.generate_image_with_hf(prompt, filename)\n\n        elif self.config.sd_webui_url and (\n            self.config.image_provider == \"sdwebui\" or self.config.sd_webui_auth\n        ):\n            return self.generate_image_with_sd_webui(prompt, filename, size)\n\n        return \"Error: No image generation provider available\"\n\n    def generate_image_with_hf(self, prompt: str, output_file: Path) -> str:\n        \"\"\"Generate an image with HuggingFace's API.\n\n        Args:\n            prompt (str): The prompt to use\n            filename (Path): The filename to save the image to\n\n        Returns:\n            str: The filename of the image\n        \"\"\"\n        API_URL = f\"https://api-inference.huggingface.co/models/{self.config.huggingface_image_model}\"  # noqa: E501\n        if self.config.huggingface_api_token is None:\n            raise ValueError(\n                \"You need to set your Hugging Face API token in the config file.\"\n            )\n        headers = {\n            \"Authorization\": (\n                f\"Bearer {self.config.huggingface_api_token.get_secret_value()}\"\n            ),\n            \"X-Use-Cache\": \"false\",\n        }\n\n        retry_count = 0\n        while retry_count < 10:\n            response = requests.post(\n                API_URL,\n                headers=headers,\n                json={\n                    \"inputs\": prompt,\n                },\n            )\n\n            if response.ok:\n                try:\n                    image = Image.open(io.BytesIO(response.content))\n                    logger.info(f\"Image Generated for prompt:{prompt}\")\n                    image.save(output_file)\n                    return f\"Saved to disk: {output_file}\"\n                except Exception as e:\n                    logger.error(e)\n                    break\n            else:\n                try:\n                    error = json.loads(response.text)\n                    if \"estimated_time\" in error:\n                        delay = error[\"estimated_time\"]\n                        logger.debug(response.text)\n                        logger.info(\"Retrying in\", delay)\n                        time.sleep(delay)\n                    else:\n                        break\n                except Exception as e:\n                    logger.error(e)\n                    break\n\n            retry_count += 1\n\n        return \"Error creating image.\"\n\n    def generate_image_with_dalle(\n        self, prompt: str, output_file: Path, size: int\n    ) -> str:\n        \"\"\"Generate an image with DALL-E.\n\n        Args:\n            prompt (str): The prompt to use\n            filename (Path): The filename to save the image to\n            size (int): The size of the image\n\n        Returns:\n            str: The filename of the image\n        \"\"\"\n        assert self.openai_credentials  # otherwise this tool is disabled\n\n        # Check for supported image sizes\n        if size not in [256, 512, 1024]:\n            closest = min([256, 512, 1024], key=lambda x: abs(x - size))\n            logger.info(\n                \"DALL-E only supports image sizes of 256x256, 512x512, or 1024x1024. \"\n                f\"Setting to {closest}, was {size}.\"\n            )\n            size = closest\n\n        # TODO: integrate in `forge.llm.providers`(?)\n        response = OpenAI(\n            api_key=self.openai_credentials.api_key.get_secret_value(),\n            organization=self.openai_credentials.organization.get_secret_value()\n            if self.openai_credentials.organization\n            else None,\n        ).images.generate(\n            prompt=prompt,\n            n=1,\n            # TODO: improve typing of size config item(s)\n            size=f\"{size}x{size}\",  # type: ignore\n            response_format=\"b64_json\",\n        )\n        assert response.data[0].b64_json is not None  # response_format = \"b64_json\"\n\n        logger.info(f\"Image Generated for prompt: {prompt}\")\n\n        image_data = b64decode(response.data[0].b64_json)\n\n        with open(output_file, mode=\"wb\") as png:\n            png.write(image_data)\n\n        return f\"Saved to disk: {output_file}\"\n\n    def generate_image_with_sd_webui(\n        self,\n        prompt: str,\n        output_file: Path,\n        size: int = 512,\n        negative_prompt: str = \"\",\n        extra: dict = {},\n    ) -> str:\n        \"\"\"Generate an image with Stable Diffusion webui.\n        Args:\n            prompt (str): The prompt to use\n            filename (str): The filename to save the image to\n            size (int, optional): The size of the image. Defaults to 256.\n            negative_prompt (str, optional): The negative prompt to use. Defaults to \"\".\n            extra (dict, optional): Extra parameters to pass to the API. Defaults to {}.\n        Returns:\n            str: The filename of the image\n        \"\"\"\n        # Create a session and set the basic auth if needed\n        s = requests.Session()\n        if self.config.sd_webui_auth:\n            username, password = self.config.sd_webui_auth.get_secret_value().split(\":\")\n            s.auth = (username, password or \"\")\n\n        # Generate the images\n        response = requests.post(\n            f\"{self.config.sd_webui_url}/sdapi/v1/txt2img\",\n            json={\n                \"prompt\": prompt,\n                \"negative_prompt\": negative_prompt,\n                \"sampler_index\": \"DDIM\",\n                \"steps\": 20,\n                \"config_scale\": 7.0,\n                \"width\": size,\n                \"height\": size,\n                \"n_iter\": 1,\n                **extra,\n            },\n        )\n\n        logger.info(f\"Image Generated for prompt: '{prompt}'\")\n\n        # Save the image to disk\n        response = response.json()\n        b64 = b64decode(response[\"images\"][0].split(\",\", 1)[0])\n        image = Image.open(io.BytesIO(b64))\n        image.save(output_file)\n\n        return f\"Saved to disk: {output_file}\"\n", "forge/forge/components/image_gen/__init__.py": "from .image_gen import ImageGeneratorComponent\n\n__all__ = [\"ImageGeneratorComponent\"]\n", "forge/forge/components/file_manager/__init__.py": "from .file_manager import FileManagerComponent\n\n__all__ = [\"FileManagerComponent\"]\n", "forge/forge/components/file_manager/file_manager.py": "import logging\nimport os\nfrom pathlib import Path\nfrom typing import Iterator, Optional\n\nfrom pydantic import BaseModel\n\nfrom forge.agent import BaseAgentSettings\nfrom forge.agent.components import ConfigurableComponent\nfrom forge.agent.protocols import CommandProvider, DirectiveProvider\nfrom forge.command import Command, command\nfrom forge.file_storage.base import FileStorage\nfrom forge.models.json_schema import JSONSchema\nfrom forge.utils.file_operations import decode_textual_file\n\nlogger = logging.getLogger(__name__)\n\n\nclass FileManagerConfiguration(BaseModel):\n    storage_path: str\n    \"\"\"Path to agent files, e.g. state\"\"\"\n    workspace_path: str\n    \"\"\"Path to files that agent has access to\"\"\"\n\n    class Config:\n        # Prevent mutation of the configuration\n        # as this wouldn't be reflected in the file storage\n        allow_mutation = False\n\n\nclass FileManagerComponent(\n    DirectiveProvider, CommandProvider, ConfigurableComponent[FileManagerConfiguration]\n):\n    \"\"\"\n    Adds general file manager (e.g. Agent state),\n    workspace manager (e.g. Agent output files) support and\n    commands to perform operations on files and folders.\n    \"\"\"\n\n    config_class = FileManagerConfiguration\n\n    STATE_FILE = \"state.json\"\n    \"\"\"The name of the file where the agent's state is stored.\"\"\"\n\n    def __init__(\n        self,\n        file_storage: FileStorage,\n        agent_state: BaseAgentSettings,\n        config: Optional[FileManagerConfiguration] = None,\n    ):\n        \"\"\"Initialise the FileManagerComponent.\n        Either `agent_id` or `config` must be provided.\n\n        Args:\n            file_storage (FileStorage): The file storage instance to use.\n            state (BaseAgentSettings): The agent's state.\n            config (FileManagerConfiguration, optional): The configuration for\n            the file manager. Defaults to None.\n        \"\"\"\n        if not agent_state.agent_id:\n            raise ValueError(\"Agent must have an ID.\")\n\n        self.agent_state = agent_state\n\n        if not config:\n            storage_path = f\"agents/{self.agent_state.agent_id}/\"\n            workspace_path = f\"agents/{self.agent_state.agent_id}/workspace\"\n            ConfigurableComponent.__init__(\n                self,\n                FileManagerConfiguration(\n                    storage_path=storage_path, workspace_path=workspace_path\n                ),\n            )\n        else:\n            ConfigurableComponent.__init__(self, config)\n\n        self.storage = file_storage.clone_with_subroot(self.config.storage_path)\n        \"\"\"Agent-related files, e.g. state, logs.\n        Use `workspace` to access the agent's workspace files.\"\"\"\n        self.workspace = file_storage.clone_with_subroot(self.config.workspace_path)\n        \"\"\"Workspace that the agent has access to, e.g. for reading/writing files.\n        Use `storage` to access agent-related files, e.g. state, logs.\"\"\"\n        self._file_storage = file_storage\n\n    async def save_state(self, save_as_id: Optional[str] = None) -> None:\n        \"\"\"Save the agent's data and state.\"\"\"\n        if save_as_id:\n            self._file_storage.make_dir(f\"agents/{save_as_id}\")\n            # Save state\n            await self._file_storage.write_file(\n                f\"agents/{save_as_id}/{self.STATE_FILE}\", self.agent_state.json()\n            )\n            # Copy workspace\n            self._file_storage.copy(\n                self.config.workspace_path,\n                f\"agents/{save_as_id}/workspace\",\n            )\n        else:\n            await self.storage.write_file(\n                self.storage.root / self.STATE_FILE, self.agent_state.json()\n            )\n\n    def get_resources(self) -> Iterator[str]:\n        yield \"The ability to read and write files.\"\n\n    def get_commands(self) -> Iterator[Command]:\n        yield self.read_file\n        yield self.write_to_file\n        yield self.list_folder\n\n    @command(\n        parameters={\n            \"filename\": JSONSchema(\n                type=JSONSchema.Type.STRING,\n                description=\"The path of the file to read\",\n                required=True,\n            )\n        },\n    )\n    def read_file(self, filename: str | Path) -> str:\n        \"\"\"Read a file and return the contents\n\n        Args:\n            filename (str): The name of the file to read\n\n        Returns:\n            str: The contents of the file\n        \"\"\"\n        file = self.workspace.open_file(filename, binary=True)\n        content = decode_textual_file(file, os.path.splitext(filename)[1], logger)\n\n        return content\n\n    @command(\n        [\"write_file\", \"create_file\"],\n        \"Write a file, creating it if necessary. \"\n        \"If the file exists, it is overwritten.\",\n        {\n            \"filename\": JSONSchema(\n                type=JSONSchema.Type.STRING,\n                description=\"The name of the file to write to\",\n                required=True,\n            ),\n            \"contents\": JSONSchema(\n                type=JSONSchema.Type.STRING,\n                description=\"The contents to write to the file\",\n                required=True,\n            ),\n        },\n    )\n    async def write_to_file(self, filename: str | Path, contents: str) -> str:\n        \"\"\"Write contents to a file\n\n        Args:\n            filename (str): The name of the file to write to\n            contents (str): The contents to write to the file\n\n        Returns:\n            str: A message indicating success or failure\n        \"\"\"\n        if directory := os.path.dirname(filename):\n            self.workspace.make_dir(directory)\n        await self.workspace.write_file(filename, contents)\n        return f\"File {filename} has been written successfully.\"\n\n    @command(\n        parameters={\n            \"folder\": JSONSchema(\n                type=JSONSchema.Type.STRING,\n                description=\"The folder to list files in\",\n                required=True,\n            )\n        },\n    )\n    def list_folder(self, folder: str | Path) -> list[str]:\n        \"\"\"Lists files in a folder recursively\n\n        Args:\n            folder (str): The folder to search in\n\n        Returns:\n            list[str]: A list of files found in the folder\n        \"\"\"\n        return [str(p) for p in self.workspace.list_files(folder)]\n", "forge/forge/components/user_interaction/user_interaction.py": "from typing import Iterator\n\nimport click\n\nfrom forge.agent.protocols import CommandProvider\nfrom forge.command import Command, command\nfrom forge.models.json_schema import JSONSchema\nfrom forge.utils.const import ASK_COMMAND\n\n\nclass UserInteractionComponent(CommandProvider):\n    \"\"\"Provides commands to interact with the user.\"\"\"\n\n    def get_commands(self) -> Iterator[Command]:\n        yield self.ask_user\n\n    @command(\n        names=[ASK_COMMAND],\n        parameters={\n            \"question\": JSONSchema(\n                type=JSONSchema.Type.STRING,\n                description=\"The question or prompt to the user\",\n                required=True,\n            )\n        },\n    )\n    def ask_user(self, question: str) -> str:\n        \"\"\"If you need more details or information regarding the given goals,\n        you can ask the user for input.\"\"\"\n        print(f\"\\nQ: {question}\")\n        resp = click.prompt(\"A\")\n        return f\"The user's answer: '{resp}'\"\n", "forge/forge/components/user_interaction/__init__.py": "from .user_interaction import UserInteractionComponent\n\n__all__ = [\"UserInteractionComponent\"]\n", "forge/forge/components/system/system.py": "import logging\nimport time\nfrom typing import Iterator\n\nfrom forge.agent.protocols import CommandProvider, DirectiveProvider, MessageProvider\nfrom forge.command import Command, command\nfrom forge.llm.providers import ChatMessage\nfrom forge.models.json_schema import JSONSchema\nfrom forge.utils.const import FINISH_COMMAND\nfrom forge.utils.exceptions import AgentFinished\n\nlogger = logging.getLogger(__name__)\n\n\nclass SystemComponent(DirectiveProvider, MessageProvider, CommandProvider):\n    \"\"\"Component for system messages and commands.\"\"\"\n\n    def get_constraints(self) -> Iterator[str]:\n        yield \"Exclusively use the commands listed below.\"\n        yield (\n            \"You can only act proactively, and are unable to start background jobs or \"\n            \"set up webhooks for yourself. \"\n            \"Take this into account when planning your actions.\"\n        )\n        yield (\n            \"You are unable to interact with physical objects. \"\n            \"If this is absolutely necessary to fulfill a task or objective or \"\n            \"to complete a step, you must ask the user to do it for you. \"\n            \"If the user refuses this, and there is no other way to achieve your \"\n            \"goals, you must terminate to avoid wasting time and energy.\"\n        )\n\n    def get_resources(self) -> Iterator[str]:\n        yield (\n            \"You are a Large Language Model, trained on millions of pages of text, \"\n            \"including a lot of factual knowledge. Make use of this factual knowledge \"\n            \"to avoid unnecessary gathering of information.\"\n        )\n\n    def get_best_practices(self) -> Iterator[str]:\n        yield (\n            \"Continuously review and analyze your actions to ensure \"\n            \"you are performing to the best of your abilities.\"\n        )\n        yield \"Constructively self-criticize your big-picture behavior constantly.\"\n        yield \"Reflect on past decisions and strategies to refine your approach.\"\n        yield (\n            \"Every command has a cost, so be smart and efficient. \"\n            \"Aim to complete tasks in the least number of steps.\"\n        )\n        yield (\n            \"Only make use of your information gathering abilities to find \"\n            \"information that you don't yet have knowledge of.\"\n        )\n\n    def get_messages(self) -> Iterator[ChatMessage]:\n        # Clock\n        yield ChatMessage.system(\n            f\"## Clock\\nThe current time and date is {time.strftime('%c')}\"\n        )\n\n    def get_commands(self) -> Iterator[Command]:\n        yield self.finish\n\n    @command(\n        names=[FINISH_COMMAND],\n        parameters={\n            \"reason\": JSONSchema(\n                type=JSONSchema.Type.STRING,\n                description=\"A summary to the user of how the goals were accomplished\",\n                required=True,\n            ),\n        },\n    )\n    def finish(self, reason: str):\n        \"\"\"Use this to shut down once you have completed your task,\n        or when there are insurmountable problems that make it impossible\n        for you to finish your task.\"\"\"\n        raise AgentFinished(reason)\n", "forge/forge/components/system/__init__.py": "from .system import SystemComponent\n\n__all__ = [\"SystemComponent\"]\n", "forge/forge/components/watchdog/watchdog.py": "from __future__ import annotations\n\nimport logging\nfrom typing import TYPE_CHECKING\n\nfrom forge.agent.components import ComponentSystemError\nfrom forge.agent.protocols import AfterParse\nfrom forge.components.action_history import EpisodicActionHistory\nfrom forge.models.action import AnyProposal\n\nif TYPE_CHECKING:\n    from forge.agent.base import BaseAgentConfiguration\n\nlogger = logging.getLogger(__name__)\n\n\nclass WatchdogComponent(AfterParse[AnyProposal]):\n    \"\"\"\n    Adds a watchdog feature to an agent class. Whenever the agent starts\n    looping, the watchdog will switch from the FAST_LLM to the SMART_LLM and re-think.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: BaseAgentConfiguration,\n        event_history: EpisodicActionHistory[AnyProposal],\n    ):\n        self.config = config\n        self.event_history = event_history\n        self.revert_big_brain = False\n\n    def after_parse(self, result: AnyProposal) -> None:\n        if self.revert_big_brain:\n            self.config.big_brain = False\n            self.revert_big_brain = False\n\n        if not self.config.big_brain and self.config.fast_llm != self.config.smart_llm:\n            previous_command, previous_command_args = None, None\n            if len(self.event_history) > 1:\n                # Detect repetitive commands\n                previous_cycle = self.event_history.episodes[\n                    self.event_history.cursor - 1\n                ]\n                previous_command = previous_cycle.action.use_tool.name\n                previous_command_args = previous_cycle.action.use_tool.arguments\n\n            rethink_reason = \"\"\n\n            if not result.use_tool:\n                rethink_reason = \"AI did not specify a command\"\n            elif (\n                result.use_tool.name == previous_command\n                and result.use_tool.arguments == previous_command_args\n            ):\n                rethink_reason = f\"Repititive command detected ({result.use_tool.name})\"\n\n            if rethink_reason:\n                logger.info(f\"{rethink_reason}, re-thinking with SMART_LLM...\")\n                self.event_history.rewind()\n                self.big_brain = True\n                self.revert_big_brain = True\n                # Trigger retry of all pipelines prior to this component\n                raise ComponentSystemError(rethink_reason, self)\n", "forge/forge/components/watchdog/__init__.py": "from .watchdog import WatchdogComponent\n\n__all__ = [\"WatchdogComponent\"]\n", "forge/forge/components/git_operations/git_operations.py": "from pathlib import Path\nfrom typing import Iterator, Optional\n\nfrom git.repo import Repo\nfrom pydantic import BaseModel, SecretStr\n\nfrom forge.agent.components import ConfigurableComponent\nfrom forge.agent.protocols import CommandProvider\nfrom forge.command import Command, command\nfrom forge.models.config import UserConfigurable\nfrom forge.models.json_schema import JSONSchema\nfrom forge.utils.exceptions import CommandExecutionError\nfrom forge.utils.url_validator import validate_url\n\n\nclass GitOperationsConfiguration(BaseModel):\n    github_username: Optional[str] = UserConfigurable(from_env=\"GITHUB_USERNAME\")\n    github_api_key: Optional[SecretStr] = UserConfigurable(\n        from_env=\"GITHUB_API_KEY\", exclude=True\n    )\n\n\nclass GitOperationsComponent(\n    CommandProvider, ConfigurableComponent[GitOperationsConfiguration]\n):\n    \"\"\"Provides commands to perform Git operations.\"\"\"\n\n    config_class = GitOperationsConfiguration\n\n    def __init__(self, config: Optional[GitOperationsConfiguration] = None):\n        ConfigurableComponent.__init__(self, config)\n        self._enabled = bool(self.config.github_username and self.config.github_api_key)\n        self._disabled_reason = \"Configure github_username and github_api_key.\"\n\n    def get_commands(self) -> Iterator[Command]:\n        yield self.clone_repository\n\n    @command(\n        parameters={\n            \"url\": JSONSchema(\n                type=JSONSchema.Type.STRING,\n                description=\"The URL of the repository to clone\",\n                required=True,\n            ),\n            \"clone_path\": JSONSchema(\n                type=JSONSchema.Type.STRING,\n                description=\"The path to clone the repository to\",\n                required=True,\n            ),\n        },\n    )\n    @validate_url\n    def clone_repository(self, url: str, clone_path: Path) -> str:\n        \"\"\"Clone a GitHub repository locally.\n\n        Args:\n            url (str): The URL of the repository to clone.\n            clone_path (Path): The path to clone the repository to.\n\n        Returns:\n            str: The result of the clone operation.\n        \"\"\"\n        split_url = url.split(\"//\")\n        api_key = (\n            self.config.github_api_key.get_secret_value()\n            if self.config.github_api_key\n            else None\n        )\n        auth_repo_url = f\"//{self.config.github_username}:\" f\"{api_key}@\".join(\n            split_url\n        )\n        try:\n            Repo.clone_from(url=auth_repo_url, to_path=clone_path)\n        except Exception as e:\n            raise CommandExecutionError(f\"Could not clone repo: {e}\")\n\n        return f\"\"\"Cloned {url} to {clone_path}\"\"\"\n", "forge/forge/components/git_operations/__init__.py": "from .git_operations import GitOperationsComponent\n\n__all__ = [\"GitOperationsComponent\"]\n", "forge/forge/components/context/__init__.py": "from .context import ContextComponent\nfrom .context_item import (\n    ContextItem,\n    FileContextItem,\n    FolderContextItem,\n    StaticContextItem,\n)\n\n__all__ = [\n    \"ContextComponent\",\n    \"ContextItem\",\n    \"FileContextItem\",\n    \"FolderContextItem\",\n    \"StaticContextItem\",\n]\n", "forge/forge/components/context/context.py": "import contextlib\nfrom pathlib import Path\nfrom typing import Iterator\n\nfrom pydantic import BaseModel, Field\nfrom typing_extensions import Annotated\n\nfrom forge.agent.protocols import CommandProvider, MessageProvider\nfrom forge.command import Command, command\nfrom forge.file_storage.base import FileStorage\nfrom forge.llm.providers import ChatMessage\nfrom forge.models.json_schema import JSONSchema\nfrom forge.utils.exceptions import InvalidArgumentError\n\nfrom .context_item import ContextItem, FileContextItem, FolderContextItem\n\n\nclass AgentContext(BaseModel):\n    items: list[Annotated[ContextItem, Field(discriminator=\"type\")]] = Field(\n        default_factory=list\n    )\n\n    def __bool__(self) -> bool:\n        return len(self.items) > 0\n\n    def __contains__(self, item: ContextItem) -> bool:\n        return any([i.source == item.source for i in self.items])\n\n    def add(self, item: ContextItem) -> None:\n        self.items.append(item)\n\n    def close(self, index: int) -> None:\n        self.items.pop(index - 1)\n\n    def clear(self) -> None:\n        self.items.clear()\n\n    def format_numbered(self, workspace: FileStorage) -> str:\n        return \"\\n\\n\".join(\n            [f\"{i}. {c.fmt(workspace)}\" for i, c in enumerate(self.items, 1)]\n        )\n\n\nclass ContextComponent(MessageProvider, CommandProvider):\n    \"\"\"Adds ability to keep files and folders open in the context (prompt).\"\"\"\n\n    def __init__(self, workspace: FileStorage, context: AgentContext):\n        self.context = context\n        self.workspace = workspace\n\n    def get_messages(self) -> Iterator[ChatMessage]:\n        if self.context:\n            yield ChatMessage.system(\n                \"## Context\\n\"\n                f\"{self.context.format_numbered(self.workspace)}\\n\\n\"\n                \"When a context item is no longer needed and you are not done yet, \"\n                \"you can hide the item by specifying its number in the list above \"\n                \"to `hide_context_item`.\",\n            )\n\n    def get_commands(self) -> Iterator[Command]:\n        yield self.open_file\n        yield self.open_folder\n        if self.context:\n            yield self.close_context_item\n\n    @command(\n        parameters={\n            \"file_path\": JSONSchema(\n                type=JSONSchema.Type.STRING,\n                description=\"The path of the file to open\",\n                required=True,\n            )\n        }\n    )\n    async def open_file(self, file_path: str | Path) -> str:\n        \"\"\"Opens a file for editing or continued viewing;\n        creates it if it does not exist yet.\n        Note: If you only need to read or write a file once,\n        use `write_to_file` instead.\n\n        Args:\n            file_path (str | Path): The path of the file to open\n\n        Returns:\n            str: A status message indicating what happened\n        \"\"\"\n        if not isinstance(file_path, Path):\n            file_path = Path(file_path)\n\n        created = False\n        if not self.workspace.exists(file_path):\n            await self.workspace.write_file(file_path, \"\")\n            created = True\n\n        # Try to make the file path relative\n        with contextlib.suppress(ValueError):\n            file_path = file_path.relative_to(self.workspace.root)\n\n        file = FileContextItem(path=file_path)\n        self.context.add(file)\n        return (\n            f\"File {file_path}{' created,' if created else ''} has been opened\"\n            \" and added to the context \u2705\"\n        )\n\n    @command(\n        parameters={\n            \"path\": JSONSchema(\n                type=JSONSchema.Type.STRING,\n                description=\"The path of the folder to open\",\n                required=True,\n            )\n        }\n    )\n    def open_folder(self, path: str | Path) -> str:\n        \"\"\"Open a folder to keep track of its content\n\n        Args:\n            path (str | Path): The path of the folder to open\n\n        Returns:\n            str: A status message indicating what happened\n        \"\"\"\n        if not isinstance(path, Path):\n            path = Path(path)\n\n        if not self.workspace.exists(path):\n            raise FileNotFoundError(\n                f\"open_folder {path} failed: no such file or directory\"\n            )\n\n        # Try to make the path relative\n        with contextlib.suppress(ValueError):\n            path = path.relative_to(self.workspace.root)\n\n        folder = FolderContextItem(path=path)\n        self.context.add(folder)\n        return f\"Folder {path} has been opened and added to the context \u2705\"\n\n    @command(\n        parameters={\n            \"number\": JSONSchema(\n                type=JSONSchema.Type.INTEGER,\n                description=\"The 1-based index of the context item to hide\",\n                required=True,\n            )\n        }\n    )\n    def close_context_item(self, number: int) -> str:\n        \"\"\"Hide an open file, folder or other context item, to save tokens.\n\n        Args:\n            number (int): The 1-based index of the context item to hide\n\n        Returns:\n            str: A status message indicating what happened\n        \"\"\"\n        if number > len(self.context.items) or number == 0:\n            raise InvalidArgumentError(f\"Index {number} out of range\")\n\n        self.context.close(number)\n        return f\"Context item {number} hidden \u2705\"\n", "forge/forge/components/context/context_item.py": "import logging\nfrom abc import ABC, abstractmethod\nfrom pathlib import Path\nfrom typing import Literal, Optional\n\nfrom pydantic import BaseModel, Field\n\nfrom forge.file_storage.base import FileStorage\nfrom forge.utils.file_operations import decode_textual_file\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseContextItem(ABC):\n    @property\n    @abstractmethod\n    def description(self) -> str:\n        \"\"\"Description of the context item\"\"\"\n        ...\n\n    @property\n    @abstractmethod\n    def source(self) -> Optional[str]:\n        \"\"\"A string indicating the source location of the context item\"\"\"\n        ...\n\n    @abstractmethod\n    def get_content(self, workspace: FileStorage) -> str:\n        \"\"\"The content represented by the context item\"\"\"\n        ...\n\n    def fmt(self, workspace: FileStorage) -> str:\n        return (\n            f\"{self.description} (source: {self.source})\\n\"\n            \"```\\n\"\n            f\"{self.get_content(workspace)}\\n\"\n            \"```\"\n        )\n\n\nclass FileContextItem(BaseModel, BaseContextItem):\n    path: Path\n    type: Literal[\"file\"] = \"file\"\n\n    @property\n    def description(self) -> str:\n        return f\"The current content of the file '{self.path}'\"\n\n    @property\n    def source(self) -> str:\n        return str(self.path)\n\n    def get_content(self, workspace: FileStorage) -> str:\n        with workspace.open_file(self.path, \"r\", True) as file:\n            return decode_textual_file(file, self.path.suffix, logger)\n\n\nclass FolderContextItem(BaseModel, BaseContextItem):\n    path: Path\n    type: Literal[\"folder\"] = \"folder\"\n\n    @property\n    def description(self) -> str:\n        return f\"The contents of the folder '{self.path}' in the workspace\"\n\n    @property\n    def source(self) -> str:\n        return str(self.path)\n\n    def get_content(self, workspace: FileStorage) -> str:\n        files = [str(p) for p in workspace.list_files(self.path)]\n        folders = [f\"{str(p)}/\" for p in workspace.list_folders(self.path)]\n        items = folders + files\n        items.sort()\n        return \"\\n\".join(items)\n\n\nclass StaticContextItem(BaseModel, BaseContextItem):\n    item_description: str = Field(alias=\"description\")\n    item_source: Optional[str] = Field(alias=\"source\")\n    item_content: str = Field(alias=\"content\")\n    type: Literal[\"static\"] = \"static\"\n\n\nContextItem = FileContextItem | FolderContextItem | StaticContextItem\n", "forge/forge/components/action_history/model.py": "from __future__ import annotations\n\nimport asyncio\nfrom typing import TYPE_CHECKING, Generic\n\nfrom pydantic import Field\nfrom pydantic.generics import GenericModel\n\nfrom forge.content_processing.text import summarize_text\nfrom forge.llm.prompting.utils import format_numbered_list, indent\nfrom forge.llm.providers.multi import ModelName\nfrom forge.models.action import ActionResult, AnyProposal\nfrom forge.models.utils import ModelWithSummary\n\nif TYPE_CHECKING:\n    from forge.llm.providers import MultiProvider\n\n\nclass Episode(GenericModel, Generic[AnyProposal]):\n    action: AnyProposal\n    result: ActionResult | None\n    summary: str | None = None\n\n    def format(self):\n        step = f\"Executed `{self.action.use_tool}`\\n\"\n        reasoning = (\n            _r.summary()\n            if isinstance(_r := self.action.thoughts, ModelWithSummary)\n            else _r\n        )\n        step += f'- **Reasoning:** \"{reasoning}\"\\n'\n        step += (\n            \"- **Status:** \"\n            f\"`{self.result.status if self.result else 'did_not_finish'}`\\n\"\n        )\n        if self.result:\n            if self.result.status == \"success\":\n                result = str(self.result)\n                result = \"\\n\" + indent(result) if \"\\n\" in result else result\n                step += f\"- **Output:** {result}\"\n            elif self.result.status == \"error\":\n                step += f\"- **Reason:** {self.result.reason}\\n\"\n                if self.result.error:\n                    step += f\"- **Error:** {self.result.error}\\n\"\n            elif self.result.status == \"interrupted_by_human\":\n                step += f\"- **Feedback:** {self.result.feedback}\\n\"\n        return step\n\n    def __str__(self) -> str:\n        executed_action = f\"Executed `{self.action.use_tool}`\"\n        action_result = f\": {self.result}\" if self.result else \".\"\n        return executed_action + action_result\n\n\nclass EpisodicActionHistory(GenericModel, Generic[AnyProposal]):\n    \"\"\"Utility container for an action history\"\"\"\n\n    episodes: list[Episode[AnyProposal]] = Field(default_factory=list)\n    cursor: int = 0\n    _lock = asyncio.Lock()\n\n    @property\n    def current_episode(self) -> Episode[AnyProposal] | None:\n        if self.cursor == len(self):\n            return None\n        return self[self.cursor]\n\n    def __getitem__(self, key: int) -> Episode[AnyProposal]:\n        return self.episodes[key]\n\n    def __len__(self) -> int:\n        return len(self.episodes)\n\n    def __bool__(self) -> bool:\n        return len(self.episodes) > 0\n\n    def register_action(self, action: AnyProposal) -> None:\n        if not self.current_episode:\n            self.episodes.append(Episode(action=action, result=None))\n            assert self.current_episode\n        elif self.current_episode.action:\n            raise ValueError(\"Action for current cycle already set\")\n\n    def register_result(self, result: ActionResult) -> None:\n        if not self.current_episode:\n            raise RuntimeError(\"Cannot register result for cycle without action\")\n        elif self.current_episode.result:\n            raise ValueError(\"Result for current cycle already set\")\n\n        self.current_episode.result = result\n        self.cursor = len(self.episodes)\n\n    def rewind(self, number_of_episodes: int = 0) -> None:\n        \"\"\"Resets the history to an earlier state.\n\n        Params:\n            number_of_cycles (int): The number of cycles to rewind. Default is 0.\n                When set to 0, it will only reset the current cycle.\n        \"\"\"\n        # Remove partial record of current cycle\n        if self.current_episode:\n            if self.current_episode.action and not self.current_episode.result:\n                self.episodes.pop(self.cursor)\n\n        # Rewind the specified number of cycles\n        if number_of_episodes > 0:\n            self.episodes = self.episodes[:-number_of_episodes]\n            self.cursor = len(self.episodes)\n\n    async def handle_compression(\n        self,\n        llm_provider: MultiProvider,\n        model_name: ModelName,\n        spacy_model: str,\n    ) -> None:\n        \"\"\"Compresses each episode in the action history using an LLM.\n\n        This method iterates over all episodes in the action history without a summary,\n        and generates a summary for them using an LLM.\n        \"\"\"\n        compress_instruction = (\n            \"The text represents an action, the reason for its execution, \"\n            \"and its result. \"\n            \"Condense the action taken and its result into one line. \"\n            \"Preserve any specific factual information gathered by the action.\"\n        )\n        async with self._lock:\n            # Gather all episodes without a summary\n            episodes_to_summarize = [ep for ep in self.episodes if ep.summary is None]\n\n            # Parallelize summarization calls\n            summarize_coroutines = [\n                summarize_text(\n                    episode.format(),\n                    instruction=compress_instruction,\n                    llm_provider=llm_provider,\n                    model_name=model_name,\n                    spacy_model=spacy_model,\n                )\n                for episode in episodes_to_summarize\n            ]\n            summaries = await asyncio.gather(*summarize_coroutines)\n\n            # Assign summaries to episodes\n            for episode, (summary, _) in zip(episodes_to_summarize, summaries):\n                episode.summary = summary\n\n    def fmt_list(self) -> str:\n        return format_numbered_list(self.episodes)\n\n    def fmt_paragraph(self) -> str:\n        steps: list[str] = []\n\n        for i, episode in enumerate(self.episodes, 1):\n            step = f\"### Step {i}: {episode.format()}\\n\"\n\n            steps.append(step)\n\n        return \"\\n\\n\".join(steps)\n", "forge/forge/components/action_history/__init__.py": "from .action_history import ActionHistoryComponent\nfrom .model import Episode, EpisodicActionHistory\n\n__all__ = [\"ActionHistoryComponent\", \"Episode\", \"EpisodicActionHistory\"]\n", "forge/forge/components/action_history/action_history.py": "from __future__ import annotations\n\nfrom typing import Callable, Iterator, Optional\n\nfrom pydantic import BaseModel\n\nfrom forge.agent.components import ConfigurableComponent\nfrom forge.agent.protocols import AfterExecute, AfterParse, MessageProvider\nfrom forge.llm.prompting.utils import indent\nfrom forge.llm.providers import ChatMessage, MultiProvider\nfrom forge.llm.providers.multi import ModelName\nfrom forge.llm.providers.openai import OpenAIModelName\n\nfrom .model import ActionResult, AnyProposal, Episode, EpisodicActionHistory\n\n\nclass ActionHistoryConfiguration(BaseModel):\n    model_name: ModelName = OpenAIModelName.GPT3\n    \"\"\"Name of the llm model used to compress the history\"\"\"\n    max_tokens: int = 1024\n    \"\"\"Maximum number of tokens to use up with generated history messages\"\"\"\n    spacy_language_model: str = \"en_core_web_sm\"\n    \"\"\"Language model used for summary chunking using spacy\"\"\"\n\n\nclass ActionHistoryComponent(\n    MessageProvider,\n    AfterParse[AnyProposal],\n    AfterExecute,\n    ConfigurableComponent[ActionHistoryConfiguration],\n):\n    \"\"\"Keeps track of the event history and provides a summary of the steps.\"\"\"\n\n    config_class = ActionHistoryConfiguration\n\n    def __init__(\n        self,\n        event_history: EpisodicActionHistory[AnyProposal],\n        count_tokens: Callable[[str], int],\n        llm_provider: MultiProvider,\n        config: Optional[ActionHistoryConfiguration] = None,\n    ) -> None:\n        ConfigurableComponent.__init__(self, config)\n        self.event_history = event_history\n        self.count_tokens = count_tokens\n        self.llm_provider = llm_provider\n\n    def get_messages(self) -> Iterator[ChatMessage]:\n        if progress := self._compile_progress(\n            self.event_history.episodes,\n            self.config.max_tokens,\n            self.count_tokens,\n        ):\n            yield ChatMessage.system(f\"## Progress on your Task so far\\n\\n{progress}\")\n\n    def after_parse(self, result: AnyProposal) -> None:\n        self.event_history.register_action(result)\n\n    async def after_execute(self, result: ActionResult) -> None:\n        self.event_history.register_result(result)\n        await self.event_history.handle_compression(\n            self.llm_provider, self.config.model_name, self.config.spacy_language_model\n        )\n\n    def _compile_progress(\n        self,\n        episode_history: list[Episode[AnyProposal]],\n        max_tokens: Optional[int] = None,\n        count_tokens: Optional[Callable[[str], int]] = None,\n    ) -> str:\n        if max_tokens and not count_tokens:\n            raise ValueError(\"count_tokens is required if max_tokens is set\")\n\n        steps: list[str] = []\n        tokens: int = 0\n        n_episodes = len(episode_history)\n\n        for i, episode in enumerate(reversed(episode_history)):\n            # Use full format for the latest 4 steps, summary or format for older steps\n            if i < 4 or episode.summary is None:\n                step_content = indent(episode.format(), 2).strip()\n            else:\n                step_content = episode.summary\n\n            step = f\"* Step {n_episodes - i}: {step_content}\"\n\n            if max_tokens and count_tokens:\n                step_tokens = count_tokens(step)\n                if tokens + step_tokens > max_tokens:\n                    break\n                tokens += step_tokens\n\n            steps.insert(0, step)\n\n        return \"\\n\\n\".join(steps)\n", "forge/forge/components/web/search.py": "import json\nimport logging\nimport time\nfrom typing import Iterator, Optional\n\nfrom duckduckgo_search import DDGS\nfrom pydantic import BaseModel, SecretStr\n\nfrom forge.agent.components import ConfigurableComponent\nfrom forge.agent.protocols import CommandProvider, DirectiveProvider\nfrom forge.command import Command, command\nfrom forge.models.config import UserConfigurable\nfrom forge.models.json_schema import JSONSchema\nfrom forge.utils.exceptions import ConfigurationError\n\nlogger = logging.getLogger(__name__)\n\n\nclass WebSearchConfiguration(BaseModel):\n    google_api_key: Optional[SecretStr] = UserConfigurable(\n        from_env=\"GOOGLE_API_KEY\", exclude=True\n    )\n    google_custom_search_engine_id: Optional[SecretStr] = UserConfigurable(\n        from_env=\"GOOGLE_CUSTOM_SEARCH_ENGINE_ID\", exclude=True\n    )\n    duckduckgo_max_attempts: int = 3\n\n\nclass WebSearchComponent(\n    DirectiveProvider, CommandProvider, ConfigurableComponent[WebSearchConfiguration]\n):\n    \"\"\"Provides commands to search the web.\"\"\"\n\n    config_class = WebSearchConfiguration\n\n    def __init__(self, config: Optional[WebSearchConfiguration] = None):\n        ConfigurableComponent.__init__(self, config)\n\n        if (\n            not self.config.google_api_key\n            or not self.config.google_custom_search_engine_id\n        ):\n            logger.info(\n                \"Configure google_api_key and custom_search_engine_id \"\n                \"to use Google API search.\"\n            )\n\n    def get_resources(self) -> Iterator[str]:\n        yield \"Internet access for searches and information gathering.\"\n\n    def get_commands(self) -> Iterator[Command]:\n        yield self.web_search\n\n        if self.config.google_api_key and self.config.google_custom_search_engine_id:\n            yield self.google\n\n    @command(\n        [\"web_search\", \"search\"],\n        \"Searches the web\",\n        {\n            \"query\": JSONSchema(\n                type=JSONSchema.Type.STRING,\n                description=\"The search query\",\n                required=True,\n            ),\n            \"num_results\": JSONSchema(\n                type=JSONSchema.Type.INTEGER,\n                description=\"The number of results to return\",\n                minimum=1,\n                maximum=10,\n                required=False,\n            ),\n        },\n    )\n    def web_search(self, query: str, num_results: int = 8) -> str:\n        \"\"\"Return the results of a Google search\n\n        Args:\n            query (str): The search query.\n            num_results (int): The number of results to return.\n\n        Returns:\n            str: The results of the search.\n        \"\"\"\n        search_results = []\n        attempts = 0\n\n        while attempts < self.config.duckduckgo_max_attempts:\n            if not query:\n                return json.dumps(search_results)\n\n            search_results = DDGS().text(query, max_results=num_results)\n\n            if search_results:\n                break\n\n            time.sleep(1)\n            attempts += 1\n\n        search_results = [\n            {\n                \"title\": r[\"title\"],\n                \"url\": r[\"href\"],\n                **({\"exerpt\": r[\"body\"]} if r.get(\"body\") else {}),\n            }\n            for r in search_results\n        ]\n\n        results = (\"## Search results\\n\") + \"\\n\\n\".join(\n            f\"### \\\"{r['title']}\\\"\\n\"\n            f\"**URL:** {r['url']}  \\n\"\n            \"**Excerpt:** \" + (f'\"{exerpt}\"' if (exerpt := r.get(\"exerpt\")) else \"N/A\")\n            for r in search_results\n        )\n        return self.safe_google_results(results)\n\n    @command(\n        [\"google\"],\n        \"Google Search\",\n        {\n            \"query\": JSONSchema(\n                type=JSONSchema.Type.STRING,\n                description=\"The search query\",\n                required=True,\n            ),\n            \"num_results\": JSONSchema(\n                type=JSONSchema.Type.INTEGER,\n                description=\"The number of results to return\",\n                minimum=1,\n                maximum=10,\n                required=False,\n            ),\n        },\n    )\n    def google(self, query: str, num_results: int = 8) -> str | list[str]:\n        \"\"\"Return the results of a Google search using the official Google API\n\n        Args:\n            query (str): The search query.\n            num_results (int): The number of results to return.\n\n        Returns:\n            str: The results of the search.\n        \"\"\"\n\n        from googleapiclient.discovery import build\n        from googleapiclient.errors import HttpError\n\n        try:\n            # Should be the case if this command is enabled:\n            assert self.config.google_api_key\n            assert self.config.google_custom_search_engine_id\n\n            # Initialize the Custom Search API service\n            service = build(\n                \"customsearch\",\n                \"v1\",\n                developerKey=self.config.google_api_key.get_secret_value(),\n            )\n\n            # Send the search query and retrieve the results\n            result = (\n                service.cse()\n                .list(\n                    q=query,\n                    cx=self.config.google_custom_search_engine_id.get_secret_value(),\n                    num=num_results,\n                )\n                .execute()\n            )\n\n            # Extract the search result items from the response\n            search_results = result.get(\"items\", [])\n\n            # Create a list of only the URLs from the search results\n            search_results_links = [item[\"link\"] for item in search_results]\n\n        except HttpError as e:\n            # Handle errors in the API call\n            error_details = json.loads(e.content.decode())\n\n            # Check if the error is related to an invalid or missing API key\n            if error_details.get(\"error\", {}).get(\n                \"code\"\n            ) == 403 and \"invalid API key\" in error_details.get(\"error\", {}).get(\n                \"message\", \"\"\n            ):\n                raise ConfigurationError(\n                    \"The provided Google API key is invalid or missing.\"\n                )\n            raise\n        # google_result can be a list or a string depending on the search results\n\n        # Return the list of search result URLs\n        return self.safe_google_results(search_results_links)\n\n    def safe_google_results(self, results: str | list) -> str:\n        \"\"\"\n            Return the results of a Google search in a safe format.\n\n        Args:\n            results (str | list): The search results.\n\n        Returns:\n            str: The results of the search.\n        \"\"\"\n        if isinstance(results, list):\n            safe_message = json.dumps(\n                [result.encode(\"utf-8\", \"ignore\").decode(\"utf-8\") for result in results]\n            )\n        else:\n            safe_message = results.encode(\"utf-8\", \"ignore\").decode(\"utf-8\")\n        return safe_message\n", "forge/forge/components/web/__init__.py": "from .search import WebSearchComponent\nfrom .selenium import BrowsingError, WebSeleniumComponent\n\n__all__ = [\"WebSearchComponent\", \"BrowsingError\", \"WebSeleniumComponent\"]\n", "forge/forge/components/web/selenium.py": "import asyncio\nimport logging\nimport re\nfrom pathlib import Path\nfrom sys import platform\nfrom typing import Iterator, Literal, Optional, Type\nfrom urllib.request import urlretrieve\n\nfrom bs4 import BeautifulSoup\nfrom pydantic import BaseModel\nfrom selenium.common.exceptions import WebDriverException\nfrom selenium.webdriver.chrome.options import Options as ChromeOptions\nfrom selenium.webdriver.chrome.service import Service as ChromeDriverService\nfrom selenium.webdriver.chrome.webdriver import WebDriver as ChromeDriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.edge.options import Options as EdgeOptions\nfrom selenium.webdriver.edge.service import Service as EdgeDriverService\nfrom selenium.webdriver.edge.webdriver import WebDriver as EdgeDriver\nfrom selenium.webdriver.firefox.options import Options as FirefoxOptions\nfrom selenium.webdriver.firefox.service import Service as GeckoDriverService\nfrom selenium.webdriver.firefox.webdriver import WebDriver as FirefoxDriver\nfrom selenium.webdriver.remote.webdriver import WebDriver\nfrom selenium.webdriver.safari.options import Options as SafariOptions\nfrom selenium.webdriver.safari.webdriver import WebDriver as SafariDriver\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.support.wait import WebDriverWait\nfrom webdriver_manager.chrome import ChromeDriverManager\nfrom webdriver_manager.firefox import GeckoDriverManager\nfrom webdriver_manager.microsoft import EdgeChromiumDriverManager as EdgeDriverManager\n\nfrom forge.agent.components import ConfigurableComponent\nfrom forge.agent.protocols import CommandProvider, DirectiveProvider\nfrom forge.command import Command, command\nfrom forge.content_processing.html import extract_hyperlinks, format_hyperlinks\nfrom forge.content_processing.text import extract_information, summarize_text\nfrom forge.llm.providers import MultiProvider\nfrom forge.llm.providers.multi import ModelName\nfrom forge.llm.providers.openai import OpenAIModelName\nfrom forge.models.json_schema import JSONSchema\nfrom forge.utils.exceptions import CommandExecutionError, TooMuchOutputError\nfrom forge.utils.url_validator import validate_url\n\nlogger = logging.getLogger(__name__)\n\nFILE_DIR = Path(__file__).parent.parent\nMAX_RAW_CONTENT_LENGTH = 500\nLINKS_TO_RETURN = 20\n\n\nBrowserOptions = ChromeOptions | EdgeOptions | FirefoxOptions | SafariOptions\n\n\nclass BrowsingError(CommandExecutionError):\n    \"\"\"An error occurred while trying to browse the page\"\"\"\n\n\nclass WebSeleniumConfiguration(BaseModel):\n    model_name: ModelName = OpenAIModelName.GPT3\n    \"\"\"Name of the llm model used to read websites\"\"\"\n    web_browser: Literal[\"chrome\", \"firefox\", \"safari\", \"edge\"] = \"chrome\"\n    \"\"\"Web browser used by Selenium\"\"\"\n    headless: bool = True\n    \"\"\"Run browser in headless mode\"\"\"\n    user_agent: str = (\n        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) \"\n        \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"\n    )\n    \"\"\"User agent used by the browser\"\"\"\n    browse_spacy_language_model: str = \"en_core_web_sm\"\n    \"\"\"Spacy language model used for chunking text\"\"\"\n\n\nclass WebSeleniumComponent(\n    DirectiveProvider, CommandProvider, ConfigurableComponent[WebSeleniumConfiguration]\n):\n    \"\"\"Provides commands to browse the web using Selenium.\"\"\"\n\n    config_class = WebSeleniumConfiguration\n\n    def __init__(\n        self,\n        llm_provider: MultiProvider,\n        data_dir: Path,\n        config: Optional[WebSeleniumConfiguration] = None,\n    ):\n        ConfigurableComponent.__init__(self, config)\n        self.llm_provider = llm_provider\n        self.data_dir = data_dir\n\n    def get_resources(self) -> Iterator[str]:\n        yield \"Ability to read websites.\"\n\n    def get_commands(self) -> Iterator[Command]:\n        yield self.read_webpage\n\n    @command(\n        [\"read_webpage\"],\n        (\n            \"Read a webpage, and extract specific information from it.\"\n            \" You must specify either topics_of_interest,\"\n            \" a question, or get_raw_content.\"\n        ),\n        {\n            \"url\": JSONSchema(\n                type=JSONSchema.Type.STRING,\n                description=\"The URL to visit\",\n                required=True,\n            ),\n            \"topics_of_interest\": JSONSchema(\n                type=JSONSchema.Type.ARRAY,\n                items=JSONSchema(type=JSONSchema.Type.STRING),\n                description=(\n                    \"A list of topics about which you want to extract information \"\n                    \"from the page.\"\n                ),\n                required=False,\n            ),\n            \"question\": JSONSchema(\n                type=JSONSchema.Type.STRING,\n                description=(\n                    \"A question you want to answer using the content of the webpage.\"\n                ),\n                required=False,\n            ),\n            \"get_raw_content\": JSONSchema(\n                type=JSONSchema.Type.BOOLEAN,\n                description=(\n                    \"If true, the unprocessed content of the webpage will be returned. \"\n                    \"This consumes a lot of tokens, so use it with caution.\"\n                ),\n                required=False,\n            ),\n        },\n    )\n    @validate_url\n    async def read_webpage(\n        self,\n        url: str,\n        *,\n        topics_of_interest: list[str] = [],\n        get_raw_content: bool = False,\n        question: str = \"\",\n    ) -> str:\n        \"\"\"Browse a website and return the answer and links to the user\n\n        Args:\n            url (str): The url of the website to browse\n            question (str): The question to answer using the content of the webpage\n\n        Returns:\n            str: The answer and links to the user and the webdriver\n        \"\"\"\n        driver = None\n        try:\n            driver = await self.open_page_in_browser(url)\n\n            text = self.scrape_text_with_selenium(driver)\n            links = self.scrape_links_with_selenium(driver, url)\n\n            return_literal_content = True\n            summarized = False\n            if not text:\n                return f\"Website did not contain any text.\\n\\nLinks: {links}\"\n            elif get_raw_content:\n                if (\n                    output_tokens := self.llm_provider.count_tokens(\n                        text, self.config.model_name\n                    )\n                ) > MAX_RAW_CONTENT_LENGTH:\n                    oversize_factor = round(output_tokens / MAX_RAW_CONTENT_LENGTH, 1)\n                    raise TooMuchOutputError(\n                        f\"Page content is {oversize_factor}x the allowed length \"\n                        \"for `get_raw_content=true`\"\n                    )\n                return text + (f\"\\n\\nLinks: {links}\" if links else \"\")\n            else:\n                text = await self.summarize_webpage(\n                    text, question or None, topics_of_interest\n                )\n                return_literal_content = bool(question)\n                summarized = True\n\n            # Limit links to LINKS_TO_RETURN\n            if len(links) > LINKS_TO_RETURN:\n                links = links[:LINKS_TO_RETURN]\n\n            text_fmt = f\"'''{text}'''\" if \"\\n\" in text else f\"'{text}'\"\n            links_fmt = \"\\n\".join(f\"- {link}\" for link in links)\n            return (\n                f\"Page content{' (summary)' if summarized else ''}:\"\n                if return_literal_content\n                else \"Answer gathered from webpage:\"\n            ) + f\" {text_fmt}\\n\\nLinks:\\n{links_fmt}\"\n\n        except WebDriverException as e:\n            # These errors are often quite long and include lots of context.\n            # Just grab the first line.\n            msg = e.msg.split(\"\\n\")[0] if e.msg else str(e)\n            if \"net::\" in msg:\n                raise BrowsingError(\n                    \"A networking error occurred while trying to load the page: %s\"\n                    % re.sub(r\"^unknown error: \", \"\", msg)\n                )\n            raise CommandExecutionError(msg)\n        finally:\n            if driver:\n                driver.close()\n\n    def scrape_text_with_selenium(self, driver: WebDriver) -> str:\n        \"\"\"Scrape text from a browser window using selenium\n\n        Args:\n            driver (WebDriver): A driver object representing\n            the browser window to scrape\n\n        Returns:\n            str: the text scraped from the website\n        \"\"\"\n\n        # Get the HTML content directly from the browser's DOM\n        page_source = driver.execute_script(\"return document.body.outerHTML;\")\n        soup = BeautifulSoup(page_source, \"html.parser\")\n\n        for script in soup([\"script\", \"style\"]):\n            script.extract()\n\n        text = soup.get_text()\n        lines = (line.strip() for line in text.splitlines())\n        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n        text = \"\\n\".join(chunk for chunk in chunks if chunk)\n        return text\n\n    def scrape_links_with_selenium(self, driver: WebDriver, base_url: str) -> list[str]:\n        \"\"\"Scrape links from a website using selenium\n\n        Args:\n            driver (WebDriver): A driver object representing\n            the browser window to scrape\n            base_url (str): The base URL to use for resolving relative links\n\n        Returns:\n            List[str]: The links scraped from the website\n        \"\"\"\n        page_source = driver.page_source\n        soup = BeautifulSoup(page_source, \"html.parser\")\n\n        for script in soup([\"script\", \"style\"]):\n            script.extract()\n\n        hyperlinks = extract_hyperlinks(soup, base_url)\n\n        return format_hyperlinks(hyperlinks)\n\n    async def open_page_in_browser(self, url: str) -> WebDriver:\n        \"\"\"Open a browser window and load a web page using Selenium\n\n        Params:\n            url (str): The URL of the page to load\n            config (Config): The applicable application configuration\n\n        Returns:\n            driver (WebDriver): A driver object representing\n            the browser window to scrape\n        \"\"\"\n        logging.getLogger(\"selenium\").setLevel(logging.CRITICAL)\n\n        options_available: dict[str, Type[BrowserOptions]] = {\n            \"chrome\": ChromeOptions,\n            \"edge\": EdgeOptions,\n            \"firefox\": FirefoxOptions,\n            \"safari\": SafariOptions,\n        }\n\n        options: BrowserOptions = options_available[self.config.web_browser]()\n        options.add_argument(f\"user-agent={self.config.user_agent}\")\n\n        if isinstance(options, FirefoxOptions):\n            if self.config.headless:\n                options.headless = True  # type: ignore\n                options.add_argument(\"--disable-gpu\")\n            driver = FirefoxDriver(\n                service=GeckoDriverService(GeckoDriverManager().install()),\n                options=options,\n            )\n        elif isinstance(options, EdgeOptions):\n            driver = EdgeDriver(\n                service=EdgeDriverService(EdgeDriverManager().install()),\n                options=options,\n            )\n        elif isinstance(options, SafariOptions):\n            # Requires a bit more setup on the users end.\n            # See https://developer.apple.com/documentation/webkit/testing_with_webdriver_in_safari  # noqa: E501\n            driver = SafariDriver(options=options)\n        elif isinstance(options, ChromeOptions):\n            if platform == \"linux\" or platform == \"linux2\":\n                options.add_argument(\"--disable-dev-shm-usage\")\n                options.add_argument(\"--remote-debugging-port=9222\")\n\n            options.add_argument(\"--no-sandbox\")\n            if self.config.headless:\n                options.add_argument(\"--headless=new\")\n                options.add_argument(\"--disable-gpu\")\n\n            self._sideload_chrome_extensions(options, self.data_dir / \"assets\" / \"crx\")\n\n            if (chromium_driver_path := Path(\"/usr/bin/chromedriver\")).exists():\n                chrome_service = ChromeDriverService(str(chromium_driver_path))\n            else:\n                try:\n                    chrome_driver = ChromeDriverManager().install()\n                except AttributeError as e:\n                    if \"'NoneType' object has no attribute 'split'\" in str(e):\n                        # https://github.com/SergeyPirogov/webdriver_manager/issues/649\n                        logger.critical(\n                            \"Connecting to browser failed:\"\n                            \" is Chrome or Chromium installed?\"\n                        )\n                    raise\n                chrome_service = ChromeDriverService(chrome_driver)\n            driver = ChromeDriver(service=chrome_service, options=options)\n\n        driver.get(url)\n\n        # Wait for page to be ready, sleep 2 seconds, wait again until page ready.\n        # This allows the cookiewall squasher time to get rid of cookie walls.\n        WebDriverWait(driver, 10).until(\n            EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n        )\n        await asyncio.sleep(2)\n        WebDriverWait(driver, 10).until(\n            EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n        )\n\n        return driver\n\n    def _sideload_chrome_extensions(\n        self, options: ChromeOptions, dl_folder: Path\n    ) -> None:\n        crx_download_url_template = \"https://clients2.google.com/service/update2/crx?response=redirect&prodversion=49.0&acceptformat=crx3&x=id%3D{crx_id}%26installsource%3Dondemand%26uc\"  # noqa\n        cookiewall_squasher_crx_id = \"edibdbjcniadpccecjdfdjjppcpchdlm\"\n        adblocker_crx_id = \"cjpalhdlnbpafiamejdnhcphjbkeiagm\"\n\n        # Make sure the target folder exists\n        dl_folder.mkdir(parents=True, exist_ok=True)\n\n        for crx_id in (cookiewall_squasher_crx_id, adblocker_crx_id):\n            crx_path = dl_folder / f\"{crx_id}.crx\"\n            if not crx_path.exists():\n                logger.debug(f\"Downloading CRX {crx_id}...\")\n                crx_download_url = crx_download_url_template.format(crx_id=crx_id)\n                urlretrieve(crx_download_url, crx_path)\n                logger.debug(f\"Downloaded {crx_path.name}\")\n            options.add_extension(str(crx_path))\n\n    async def summarize_webpage(\n        self,\n        text: str,\n        question: str | None,\n        topics_of_interest: list[str],\n    ) -> str:\n        \"\"\"Summarize text using the OpenAI API\n\n        Args:\n            url (str): The url of the text\n            text (str): The text to summarize\n            question (str): The question to ask the model\n            driver (WebDriver): The webdriver to use to scroll the page\n\n        Returns:\n            str: The summary of the text\n        \"\"\"\n        if not text:\n            raise ValueError(\"No text to summarize\")\n\n        text_length = len(text)\n        logger.debug(f\"Web page content length: {text_length} characters\")\n\n        result = None\n        information = None\n        if topics_of_interest:\n            information = await extract_information(\n                text,\n                topics_of_interest=topics_of_interest,\n                llm_provider=self.llm_provider,\n                model_name=self.config.model_name,\n                spacy_model=self.config.browse_spacy_language_model,\n            )\n            return \"\\n\".join(f\"* {i}\" for i in information)\n        else:\n            result, _ = await summarize_text(\n                text,\n                question=question,\n                llm_provider=self.llm_provider,\n                model_name=self.config.model_name,\n                spacy_model=self.config.browse_spacy_language_model,\n            )\n            return result\n", "forge/forge/file_storage/local.py": "\"\"\"\nThe LocalFileStorage class implements a FileStorage that works with local files.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport inspect\nimport logging\nfrom contextlib import contextmanager\nfrom pathlib import Path\nfrom typing import Any, BinaryIO, Generator, Literal, TextIO, overload\n\nfrom .base import FileStorage, FileStorageConfiguration\n\nlogger = logging.getLogger(__name__)\n\n\nclass LocalFileStorage(FileStorage):\n    \"\"\"A class that represents a file storage.\"\"\"\n\n    def __init__(self, config: FileStorageConfiguration):\n        self._root = config.root.resolve()\n        self._restrict_to_root = config.restrict_to_root\n        self.make_dir(self.root)\n        super().__init__()\n\n    @property\n    def root(self) -> Path:\n        \"\"\"The root directory of the file storage.\"\"\"\n        return self._root\n\n    @property\n    def restrict_to_root(self) -> bool:\n        \"\"\"Whether to restrict generated paths to the root.\"\"\"\n        return self._restrict_to_root\n\n    @property\n    def is_local(self) -> bool:\n        \"\"\"Whether the storage is local (i.e. on the same machine, not cloud-based).\"\"\"\n        return True\n\n    def initialize(self) -> None:\n        self.root.mkdir(exist_ok=True, parents=True)\n\n    @overload\n    def open_file(\n        self,\n        path: str | Path,\n        mode: Literal[\"w\", \"r\"] = \"r\",\n        binary: Literal[False] = False,\n    ) -> TextIO:\n        ...\n\n    @overload\n    def open_file(\n        self, path: str | Path, mode: Literal[\"w\", \"r\"], binary: Literal[True]\n    ) -> BinaryIO:\n        ...\n\n    @overload\n    def open_file(self, path: str | Path, *, binary: Literal[True]) -> BinaryIO:\n        ...\n\n    @overload\n    def open_file(\n        self, path: str | Path, mode: Literal[\"w\", \"r\"] = \"r\", binary: bool = False\n    ) -> TextIO | BinaryIO:\n        ...\n\n    def open_file(\n        self, path: str | Path, mode: Literal[\"w\", \"r\"] = \"r\", binary: bool = False\n    ) -> TextIO | BinaryIO:\n        \"\"\"Open a file in the storage.\"\"\"\n        return self._open_file(path, f\"{mode}b\" if binary else mode)\n\n    def _open_file(self, path: str | Path, mode: str) -> TextIO | BinaryIO:\n        full_path = self.get_path(path)\n        if any(m in mode for m in (\"w\", \"a\", \"x\")):\n            full_path.parent.mkdir(parents=True, exist_ok=True)\n        return open(full_path, mode)  # type: ignore\n\n    @overload\n    def read_file(self, path: str | Path, binary: Literal[False] = False) -> str:\n        \"\"\"Read a file in the storage as text.\"\"\"\n        ...\n\n    @overload\n    def read_file(self, path: str | Path, binary: Literal[True]) -> bytes:\n        \"\"\"Read a file in the storage as binary.\"\"\"\n        ...\n\n    @overload\n    def read_file(self, path: str | Path, binary: bool = False) -> str | bytes:\n        \"\"\"Read a file in the storage.\"\"\"\n        ...\n\n    def read_file(self, path: str | Path, binary: bool = False) -> str | bytes:\n        \"\"\"Read a file in the storage.\"\"\"\n        with self._open_file(path, \"rb\" if binary else \"r\") as file:\n            return file.read()\n\n    async def write_file(self, path: str | Path, content: str | bytes) -> None:\n        \"\"\"Write to a file in the storage.\"\"\"\n        with self._open_file(path, \"wb\" if type(content) is bytes else \"w\") as file:\n            file.write(content)  # type: ignore\n\n        if self.on_write_file:\n            path = Path(path)\n            if path.is_absolute():\n                path = path.relative_to(self.root)\n            res = self.on_write_file(path)\n            if inspect.isawaitable(res):\n                await res\n\n    def list_files(self, path: str | Path = \".\") -> list[Path]:\n        \"\"\"List all files (recursively) in a directory in the storage.\"\"\"\n        path = self.get_path(path)\n        return [file.relative_to(path) for file in path.rglob(\"*\") if file.is_file()]\n\n    def list_folders(\n        self, path: str | Path = \".\", recursive: bool = False\n    ) -> list[Path]:\n        \"\"\"List directories directly in a given path or recursively.\"\"\"\n        path = self.get_path(path)\n        if recursive:\n            return [\n                folder.relative_to(path)\n                for folder in path.rglob(\"*\")\n                if folder.is_dir()\n            ]\n        else:\n            return [\n                folder.relative_to(path) for folder in path.iterdir() if folder.is_dir()\n            ]\n\n    def delete_file(self, path: str | Path) -> None:\n        \"\"\"Delete a file in the storage.\"\"\"\n        full_path = self.get_path(path)\n        full_path.unlink()\n\n    def delete_dir(self, path: str | Path) -> None:\n        \"\"\"Delete an empty folder in the storage.\"\"\"\n        full_path = self.get_path(path)\n        full_path.rmdir()\n\n    def exists(self, path: str | Path) -> bool:\n        \"\"\"Check if a file or folder exists in the storage.\"\"\"\n        return self.get_path(path).exists()\n\n    def make_dir(self, path: str | Path) -> None:\n        \"\"\"Create a directory in the storage if doesn't exist.\"\"\"\n        full_path = self.get_path(path)\n        full_path.mkdir(exist_ok=True, parents=True)\n\n    def rename(self, old_path: str | Path, new_path: str | Path) -> None:\n        \"\"\"Rename a file or folder in the storage.\"\"\"\n        old_path = self.get_path(old_path)\n        new_path = self.get_path(new_path)\n        old_path.rename(new_path)\n\n    def copy(self, source: str | Path, destination: str | Path) -> None:\n        \"\"\"Copy a file or folder with all contents in the storage.\"\"\"\n        source = self.get_path(source)\n        destination = self.get_path(destination)\n        if source.is_file():\n            destination.write_bytes(source.read_bytes())\n        else:\n            destination.mkdir(exist_ok=True, parents=True)\n            for file in source.rglob(\"*\"):\n                if file.is_file():\n                    target = destination / file.relative_to(source)\n                    target.parent.mkdir(exist_ok=True, parents=True)\n                    target.write_bytes(file.read_bytes())\n\n    def clone_with_subroot(self, subroot: str | Path) -> FileStorage:\n        \"\"\"Create a new LocalFileStorage with a subroot of the current storage.\"\"\"\n        return LocalFileStorage(\n            FileStorageConfiguration(\n                root=self.get_path(subroot),\n                restrict_to_root=self.restrict_to_root,\n            )\n        )\n\n    @contextmanager\n    def mount(self, path: str | Path = \".\") -> Generator[Path, Any, None]:\n        \"\"\"Mount the file storage and provide a local path.\"\"\"\n        # No need to do anything for local storage\n        yield Path(self.get_path(\".\")).absolute()\n", "forge/forge/file_storage/base.py": "\"\"\"\nThe FileStorage class provides an interface for interacting with a file storage.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport logging\nimport os\nimport shutil\nimport tempfile\nfrom abc import ABC, abstractmethod\nfrom contextlib import contextmanager\nfrom pathlib import Path\nfrom typing import Any, BinaryIO, Callable, Generator, Literal, TextIO, overload\n\nfrom watchdog.events import FileSystemEvent, FileSystemEventHandler\nfrom watchdog.observers import Observer\n\nfrom forge.models.config import SystemConfiguration\n\nlogger = logging.getLogger(__name__)\n\n\nclass FileStorageConfiguration(SystemConfiguration):\n    restrict_to_root: bool = True\n    root: Path = Path(\"/\")\n\n\nclass FileStorage(ABC):\n    \"\"\"A class that represents a file storage.\"\"\"\n\n    on_write_file: Callable[[Path], Any] | None = None\n    \"\"\"\n    Event hook, executed after writing a file.\n\n    Params:\n        Path: The path of the file that was written, relative to the storage root.\n    \"\"\"\n\n    @property\n    @abstractmethod\n    def root(self) -> Path:\n        \"\"\"The root path of the file storage.\"\"\"\n\n    @property\n    @abstractmethod\n    def restrict_to_root(self) -> bool:\n        \"\"\"Whether to restrict file access to within the storage's root path.\"\"\"\n\n    @property\n    @abstractmethod\n    def is_local(self) -> bool:\n        \"\"\"Whether the storage is local (i.e. on the same machine, not cloud-based).\"\"\"\n\n    @abstractmethod\n    def initialize(self) -> None:\n        \"\"\"\n        Calling `initialize()` should bring the storage to a ready-to-use state.\n        For example, it can create the resource in which files will be stored, if it\n        doesn't exist yet. E.g. a folder on disk, or an S3 Bucket.\n        \"\"\"\n\n    @overload\n    @abstractmethod\n    def open_file(\n        self,\n        path: str | Path,\n        mode: Literal[\"r\", \"w\"] = \"r\",\n        binary: Literal[False] = False,\n    ) -> TextIO:\n        \"\"\"Returns a readable text file-like object representing the file.\"\"\"\n\n    @overload\n    @abstractmethod\n    def open_file(\n        self, path: str | Path, mode: Literal[\"r\", \"w\"], binary: Literal[True]\n    ) -> BinaryIO:\n        \"\"\"Returns a binary file-like object representing the file.\"\"\"\n\n    @overload\n    @abstractmethod\n    def open_file(self, path: str | Path, *, binary: Literal[True]) -> BinaryIO:\n        \"\"\"Returns a readable binary file-like object representing the file.\"\"\"\n\n    @overload\n    @abstractmethod\n    def open_file(\n        self, path: str | Path, mode: Literal[\"r\", \"w\"] = \"r\", binary: bool = False\n    ) -> TextIO | BinaryIO:\n        \"\"\"Returns a file-like object representing the file.\"\"\"\n\n    @overload\n    @abstractmethod\n    def read_file(self, path: str | Path, binary: Literal[False] = False) -> str:\n        \"\"\"Read a file in the storage as text.\"\"\"\n        ...\n\n    @overload\n    @abstractmethod\n    def read_file(self, path: str | Path, binary: Literal[True]) -> bytes:\n        \"\"\"Read a file in the storage as binary.\"\"\"\n        ...\n\n    @overload\n    @abstractmethod\n    def read_file(self, path: str | Path, binary: bool = False) -> str | bytes:\n        \"\"\"Read a file in the storage.\"\"\"\n        ...\n\n    @abstractmethod\n    async def write_file(self, path: str | Path, content: str | bytes) -> None:\n        \"\"\"Write to a file in the storage.\"\"\"\n\n    @abstractmethod\n    def list_files(self, path: str | Path = \".\") -> list[Path]:\n        \"\"\"List all files (recursively) in a directory in the storage.\"\"\"\n\n    @abstractmethod\n    def list_folders(\n        self, path: str | Path = \".\", recursive: bool = False\n    ) -> list[Path]:\n        \"\"\"List all folders in a directory in the storage.\"\"\"\n\n    @abstractmethod\n    def delete_file(self, path: str | Path) -> None:\n        \"\"\"Delete a file in the storage.\"\"\"\n\n    @abstractmethod\n    def delete_dir(self, path: str | Path) -> None:\n        \"\"\"Delete an empty folder in the storage.\"\"\"\n\n    @abstractmethod\n    def exists(self, path: str | Path) -> bool:\n        \"\"\"Check if a file or folder exists in the storage.\"\"\"\n\n    @abstractmethod\n    def rename(self, old_path: str | Path, new_path: str | Path) -> None:\n        \"\"\"Rename a file or folder in the storage.\"\"\"\n\n    @abstractmethod\n    def copy(self, source: str | Path, destination: str | Path) -> None:\n        \"\"\"Copy a file or folder with all contents in the storage.\"\"\"\n\n    @abstractmethod\n    def make_dir(self, path: str | Path) -> None:\n        \"\"\"Create a directory in the storage if doesn't exist.\"\"\"\n\n    @abstractmethod\n    def clone_with_subroot(self, subroot: str | Path) -> FileStorage:\n        \"\"\"Create a new FileStorage with a subroot of the current storage.\"\"\"\n\n    def get_path(self, relative_path: str | Path) -> Path:\n        \"\"\"Get the full path for an item in the storage.\n\n        Parameters:\n            relative_path: The relative path to resolve in the storage.\n\n        Returns:\n            Path: The resolved path relative to the storage.\n        \"\"\"\n        return self._sanitize_path(relative_path)\n\n    @contextmanager\n    def mount(self, path: str | Path = \".\") -> Generator[Path, Any, None]:\n        \"\"\"Mount the file storage and provide a local path.\"\"\"\n        local_path = tempfile.mkdtemp(dir=path)\n\n        observer = Observer()\n        try:\n            # Copy all files to the local directory\n            files = self.list_files()\n            for file in files:\n                file_path = local_path / file\n                file_path.parent.mkdir(parents=True, exist_ok=True)\n                content = self.read_file(file, binary=True)\n                file_path.write_bytes(content)\n\n            # Sync changes\n            event_handler = FileSyncHandler(self, local_path)\n            observer.schedule(event_handler, local_path, recursive=True)\n            observer.start()\n\n            yield Path(local_path)\n        finally:\n            observer.stop()\n            observer.join()\n            shutil.rmtree(local_path)\n\n    def _sanitize_path(\n        self,\n        path: str | Path,\n    ) -> Path:\n        \"\"\"Resolve the relative path within the given root if possible.\n\n        Parameters:\n            relative_path: The relative path to resolve.\n\n        Returns:\n            Path: The resolved path.\n\n        Raises:\n            ValueError: If the path is absolute and a root is provided.\n            ValueError: If the path is outside the root and the root is restricted.\n        \"\"\"\n\n        # Posix systems disallow null bytes in paths. Windows is agnostic about it.\n        # Do an explicit check here for all sorts of null byte representations.\n        if \"\\0\" in str(path):\n            raise ValueError(\"Embedded null byte\")\n\n        logger.debug(f\"Resolving path '{path}' in storage '{self.root}'\")\n\n        relative_path = Path(path)\n\n        # Allow absolute paths if they are contained in the storage.\n        if (\n            relative_path.is_absolute()\n            and self.restrict_to_root\n            and not relative_path.is_relative_to(self.root)\n        ):\n            raise ValueError(\n                f\"Attempted to access absolute path '{relative_path}' \"\n                f\"in storage '{self.root}'\"\n            )\n\n        full_path = self.root / relative_path\n        if self.is_local:\n            full_path = full_path.resolve()\n        else:\n            full_path = Path(os.path.normpath(full_path))\n\n        logger.debug(f\"Joined paths as '{full_path}'\")\n\n        if self.restrict_to_root and not full_path.is_relative_to(self.root):\n            raise ValueError(\n                f\"Attempted to access path '{full_path}' \"\n                f\"outside of storage '{self.root}'.\"\n            )\n\n        return full_path\n\n\nclass FileSyncHandler(FileSystemEventHandler):\n    def __init__(self, storage: FileStorage, path: str | Path = \".\"):\n        self.storage = storage\n        self.path = Path(path)\n\n    def on_modified(self, event: FileSystemEvent):\n        if event.is_directory:\n            return\n\n        file_path = Path(event.src_path).relative_to(self.path)\n        content = file_path.read_bytes()\n        # Must execute write_file synchronously because the hook is synchronous\n        # TODO: Schedule write operation using asyncio.create_task (non-blocking)\n        asyncio.get_event_loop().run_until_complete(\n            self.storage.write_file(file_path, content)\n        )\n\n    def on_created(self, event: FileSystemEvent):\n        if event.is_directory:\n            self.storage.make_dir(event.src_path)\n            return\n\n        file_path = Path(event.src_path).relative_to(self.path)\n        content = file_path.read_bytes()\n        # Must execute write_file synchronously because the hook is synchronous\n        # TODO: Schedule write operation using asyncio.create_task (non-blocking)\n        asyncio.get_event_loop().run_until_complete(\n            self.storage.write_file(file_path, content)\n        )\n\n    def on_deleted(self, event: FileSystemEvent):\n        if event.is_directory:\n            self.storage.delete_dir(event.src_path)\n            return\n\n        file_path = event.src_path\n        self.storage.delete_file(file_path)\n\n    def on_moved(self, event: FileSystemEvent):\n        self.storage.rename(event.src_path, event.dest_path)\n", "forge/forge/file_storage/s3.py": "\"\"\"\nThe S3Workspace class provides an interface for interacting with a file workspace, and\nstores the files in an S3 bucket.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport contextlib\nimport inspect\nimport logging\nfrom io import TextIOWrapper\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, BinaryIO, Literal, Optional, overload\n\nimport boto3\nimport botocore.exceptions\nfrom pydantic import SecretStr\n\nfrom forge.models.config import UserConfigurable\n\nfrom .base import FileStorage, FileStorageConfiguration\n\nif TYPE_CHECKING:\n    import mypy_boto3_s3\n    from botocore.response import StreamingBody\n\nlogger = logging.getLogger(__name__)\n\n\nclass S3FileStorageConfiguration(FileStorageConfiguration):\n    bucket: str = UserConfigurable(\"autogpt\", from_env=\"STORAGE_BUCKET\")\n    s3_endpoint_url: Optional[SecretStr] = UserConfigurable(from_env=\"S3_ENDPOINT_URL\")\n\n\nclass S3FileStorage(FileStorage):\n    \"\"\"A class that represents an S3 storage.\"\"\"\n\n    _bucket: mypy_boto3_s3.service_resource.Bucket\n\n    def __init__(self, config: S3FileStorageConfiguration):\n        self._bucket_name = config.bucket\n        self._root = config.root\n        # Add / at the beginning of the root path\n        if not self._root.is_absolute():\n            self._root = Path(\"/\").joinpath(self._root)\n\n        # https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html\n        self._s3 = boto3.resource(\n            \"s3\",\n            endpoint_url=(\n                config.s3_endpoint_url.get_secret_value()\n                if config.s3_endpoint_url\n                else None\n            ),\n        )\n\n        super().__init__()\n\n    @property\n    def root(self) -> Path:\n        \"\"\"The root directory of the file storage.\"\"\"\n        return self._root\n\n    @property\n    def restrict_to_root(self):\n        \"\"\"Whether to restrict generated paths to the root.\"\"\"\n        return True\n\n    @property\n    def is_local(self) -> bool:\n        \"\"\"Whether the storage is local (i.e. on the same machine, not cloud-based).\"\"\"\n        return False\n\n    def initialize(self) -> None:\n        logger.debug(f\"Initializing {repr(self)}...\")\n        try:\n            self._s3.meta.client.head_bucket(Bucket=self._bucket_name)\n            self._bucket = self._s3.Bucket(self._bucket_name)\n        except botocore.exceptions.ClientError as e:\n            if \"(404)\" not in str(e):\n                raise\n            logger.info(f\"Bucket '{self._bucket_name}' does not exist; creating it...\")\n            self._bucket = self._s3.create_bucket(Bucket=self._bucket_name)\n\n    def get_path(self, relative_path: str | Path) -> Path:\n        # We set S3 root with \"/\" at the beginning\n        # but relative_to(\"/\") will remove it\n        # because we don't actually want it in the storage filenames\n        return super().get_path(relative_path).relative_to(\"/\")\n\n    def _get_obj(self, path: str | Path) -> mypy_boto3_s3.service_resource.Object:\n        \"\"\"Get an S3 object.\"\"\"\n        obj = self._bucket.Object(str(path))\n        with contextlib.suppress(botocore.exceptions.ClientError):\n            obj.load()\n        return obj\n\n    @overload\n    def open_file(\n        self,\n        path: str | Path,\n        mode: Literal[\"r\", \"w\"] = \"r\",\n        binary: Literal[False] = False,\n    ) -> TextIOWrapper:\n        ...\n\n    @overload\n    def open_file(\n        self, path: str | Path, mode: Literal[\"r\", \"w\"], binary: Literal[True]\n    ) -> S3BinaryIOWrapper:\n        ...\n\n    @overload\n    def open_file(\n        self, path: str | Path, *, binary: Literal[True]\n    ) -> S3BinaryIOWrapper:\n        ...\n\n    @overload\n    def open_file(\n        self, path: str | Path, mode: Literal[\"r\", \"w\"] = \"r\", binary: bool = False\n    ) -> S3BinaryIOWrapper | TextIOWrapper:\n        ...\n\n    def open_file(\n        self, path: str | Path, mode: Literal[\"r\", \"w\"] = \"r\", binary: bool = False\n    ) -> TextIOWrapper | S3BinaryIOWrapper:\n        \"\"\"Open a file in the storage.\"\"\"\n        path = self.get_path(path)\n        body = S3BinaryIOWrapper(self._get_obj(path).get()[\"Body\"], str(path))\n        return body if binary else TextIOWrapper(body)\n\n    @overload\n    def read_file(self, path: str | Path, binary: Literal[False] = False) -> str:\n        \"\"\"Read a file in the storage as text.\"\"\"\n        ...\n\n    @overload\n    def read_file(self, path: str | Path, binary: Literal[True]) -> bytes:\n        \"\"\"Read a file in the storage as binary.\"\"\"\n        ...\n\n    @overload\n    def read_file(self, path: str | Path, binary: bool = False) -> str | bytes:\n        \"\"\"Read a file in the storage.\"\"\"\n        ...\n\n    def read_file(self, path: str | Path, binary: bool = False) -> str | bytes:\n        \"\"\"Read a file in the storage.\"\"\"\n        return self.open_file(path, binary=binary).read()\n\n    async def write_file(self, path: str | Path, content: str | bytes) -> None:\n        \"\"\"Write to a file in the storage.\"\"\"\n        obj = self._get_obj(self.get_path(path))\n        obj.put(Body=content)\n\n        if self.on_write_file:\n            path = Path(path)\n            if path.is_absolute():\n                path = path.relative_to(self.root)\n            res = self.on_write_file(path)\n            if inspect.isawaitable(res):\n                await res\n\n    def list_files(self, path: str | Path = \".\") -> list[Path]:\n        \"\"\"List all files (recursively) in a directory in the storage.\"\"\"\n        path = self.get_path(path)\n        if path == Path(\".\"):  # root level of bucket\n            return [Path(obj.key) for obj in self._bucket.objects.all()]\n        else:\n            return [\n                Path(obj.key).relative_to(path)\n                for obj in self._bucket.objects.filter(Prefix=f\"{path}/\")\n            ]\n\n    def list_folders(\n        self, path: str | Path = \".\", recursive: bool = False\n    ) -> list[Path]:\n        \"\"\"List 'directories' directly in a given path or recursively in the storage.\"\"\"\n        path = self.get_path(path)\n        folder_names = set()\n\n        # List objects with the specified prefix and delimiter\n        for obj_summary in self._bucket.objects.filter(Prefix=str(path)):\n            # Remove path prefix and the object name (last part)\n            folder = Path(obj_summary.key).relative_to(path).parent\n            if not folder or folder == Path(\".\"):\n                continue\n            # For non-recursive, only add the first level of folders\n            if not recursive:\n                folder_names.add(folder.parts[0])\n            else:\n                # For recursive, need to add all nested folders\n                for i in range(len(folder.parts)):\n                    folder_names.add(\"/\".join(folder.parts[: i + 1]))\n\n        return [Path(f) for f in folder_names]\n\n    def delete_file(self, path: str | Path) -> None:\n        \"\"\"Delete a file in the storage.\"\"\"\n        path = self.get_path(path)\n        obj = self._s3.Object(self._bucket_name, str(path))\n        obj.delete()\n\n    def delete_dir(self, path: str | Path) -> None:\n        \"\"\"Delete an empty folder in the storage.\"\"\"\n        # S3 does not have directories, so we don't need to do anything\n        pass\n\n    def exists(self, path: str | Path) -> bool:\n        \"\"\"Check if a file or folder exists in S3 storage.\"\"\"\n        path = self.get_path(path)\n        try:\n            # Check for exact object match (file)\n            self._s3.meta.client.head_object(Bucket=self._bucket_name, Key=str(path))\n            return True\n        except botocore.exceptions.ClientError as e:\n            if e.response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\") == 404:\n                # If the object does not exist,\n                # check for objects with the prefix (folder)\n                prefix = f\"{str(path).rstrip('/')}/\"\n                objs = list(self._bucket.objects.filter(Prefix=prefix, MaxKeys=1))\n                return len(objs) > 0  # True if any objects exist with the prefix\n            else:\n                raise  # Re-raise for any other client errors\n\n    def make_dir(self, path: str | Path) -> None:\n        \"\"\"Create a directory in the storage if doesn't exist.\"\"\"\n        # S3 does not have directories, so we don't need to do anything\n        pass\n\n    def rename(self, old_path: str | Path, new_path: str | Path) -> None:\n        \"\"\"Rename a file or folder in the storage.\"\"\"\n        old_path = str(self.get_path(old_path))\n        new_path = str(self.get_path(new_path))\n\n        try:\n            # If file exists, rename it\n            self._s3.meta.client.head_object(Bucket=self._bucket_name, Key=old_path)\n            self._s3.meta.client.copy_object(\n                CopySource={\"Bucket\": self._bucket_name, \"Key\": old_path},\n                Bucket=self._bucket_name,\n                Key=new_path,\n            )\n            self._s3.meta.client.delete_object(Bucket=self._bucket_name, Key=old_path)\n        except botocore.exceptions.ClientError as e:\n            if e.response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\") == 404:\n                # If the object does not exist,\n                # it may be a folder\n                prefix = f\"{old_path.rstrip('/')}/\"\n                objs = list(self._bucket.objects.filter(Prefix=prefix))\n                for obj in objs:\n                    new_key = new_path + obj.key[len(old_path) :]\n                    self._s3.meta.client.copy_object(\n                        CopySource={\"Bucket\": self._bucket_name, \"Key\": obj.key},\n                        Bucket=self._bucket_name,\n                        Key=new_key,\n                    )\n                    self._s3.meta.client.delete_object(\n                        Bucket=self._bucket_name, Key=obj.key\n                    )\n            else:\n                raise  # Re-raise for any other client errors\n\n    def copy(self, source: str | Path, destination: str | Path) -> None:\n        \"\"\"Copy a file or folder with all contents in the storage.\"\"\"\n        source = str(self.get_path(source))\n        destination = str(self.get_path(destination))\n\n        try:\n            # If source is a file, copy it\n            self._s3.meta.client.head_object(Bucket=self._bucket_name, Key=source)\n            self._s3.meta.client.copy_object(\n                CopySource={\"Bucket\": self._bucket_name, \"Key\": source},\n                Bucket=self._bucket_name,\n                Key=destination,\n            )\n        except botocore.exceptions.ClientError as e:\n            if e.response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\") == 404:\n                # If the object does not exist,\n                # it may be a folder\n                prefix = f\"{source.rstrip('/')}/\"\n                objs = list(self._bucket.objects.filter(Prefix=prefix))\n                for obj in objs:\n                    new_key = destination + obj.key[len(source) :]\n                    self._s3.meta.client.copy_object(\n                        CopySource={\"Bucket\": self._bucket_name, \"Key\": obj.key},\n                        Bucket=self._bucket_name,\n                        Key=new_key,\n                    )\n            else:\n                raise\n\n    def clone_with_subroot(self, subroot: str | Path) -> S3FileStorage:\n        \"\"\"Create a new S3FileStorage with a subroot of the current storage.\"\"\"\n        file_storage = S3FileStorage(\n            S3FileStorageConfiguration(\n                bucket=self._bucket_name,\n                root=Path(\"/\").joinpath(self.get_path(subroot)),\n                s3_endpoint_url=SecretStr(self._s3.meta.client.meta.endpoint_url),\n            )\n        )\n        file_storage._s3 = self._s3\n        file_storage._bucket = self._bucket\n        return file_storage\n\n    def __repr__(self) -> str:\n        return f\"{__class__.__name__}(bucket='{self._bucket_name}', root={self._root})\"\n\n\nclass S3BinaryIOWrapper(BinaryIO):\n    def __init__(self, body: StreamingBody, name: str):\n        self.body = body\n        self._name = name\n\n    @property\n    def name(self) -> str:\n        return self._name\n\n    def read(self, size: int = -1) -> bytes:\n        return self.body.read(size if size > 0 else None)\n\n    def readinto(self, b: bytearray) -> int:\n        data = self.read(len(b))\n        b[: len(data)] = data\n        return len(data)\n\n    def close(self) -> None:\n        self.body.close()\n\n    def fileno(self) -> int:\n        return self.body.fileno()\n\n    def flush(self) -> None:\n        self.body.flush()\n\n    def isatty(self) -> bool:\n        return self.body.isatty()\n\n    def readable(self) -> bool:\n        return self.body.readable()\n\n    def seekable(self) -> bool:\n        return self.body.seekable()\n\n    def writable(self) -> bool:\n        return False\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.body.close()\n", "forge/forge/file_storage/__init__.py": "import enum\nfrom pathlib import Path\n\nfrom .base import FileStorage\n\n\nclass FileStorageBackendName(str, enum.Enum):\n    LOCAL = \"local\"\n    GCS = \"gcs\"\n    S3 = \"s3\"\n\n\ndef get_storage(\n    backend: FileStorageBackendName,\n    root_path: Path = Path(\".\"),\n    restrict_to_root: bool = True,\n) -> FileStorage:\n    match backend:\n        case FileStorageBackendName.LOCAL:\n            from .local import FileStorageConfiguration, LocalFileStorage\n\n            config = FileStorageConfiguration.from_env()\n            config.root = root_path\n            config.restrict_to_root = restrict_to_root\n            return LocalFileStorage(config)\n        case FileStorageBackendName.S3:\n            from .s3 import S3FileStorage, S3FileStorageConfiguration\n\n            config = S3FileStorageConfiguration.from_env()\n            config.root = root_path\n            return S3FileStorage(config)\n        case FileStorageBackendName.GCS:\n            from .gcs import GCSFileStorage, GCSFileStorageConfiguration\n\n            config = GCSFileStorageConfiguration.from_env()\n            config.root = root_path\n            return GCSFileStorage(config)\n", "forge/forge/file_storage/gcs.py": "\"\"\"\nThe GCSWorkspace class provides an interface for interacting with a file workspace, and\nstores the files in a Google Cloud Storage bucket.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport inspect\nimport logging\nfrom io import TextIOWrapper\nfrom pathlib import Path\nfrom typing import Literal, overload\n\nfrom google.cloud import storage\nfrom google.cloud.exceptions import NotFound\nfrom google.cloud.storage.fileio import BlobReader, BlobWriter\n\nfrom forge.models.config import UserConfigurable\n\nfrom .base import FileStorage, FileStorageConfiguration\n\nlogger = logging.getLogger(__name__)\n\n\nclass GCSFileStorageConfiguration(FileStorageConfiguration):\n    bucket: str = UserConfigurable(\"autogpt\", from_env=\"STORAGE_BUCKET\")\n\n\nclass GCSFileStorage(FileStorage):\n    \"\"\"A class that represents a Google Cloud Storage.\"\"\"\n\n    _bucket: storage.Bucket\n\n    def __init__(self, config: GCSFileStorageConfiguration):\n        self._bucket_name = config.bucket\n        self._root = config.root\n        # Add / at the beginning of the root path\n        if not self._root.is_absolute():\n            self._root = Path(\"/\").joinpath(self._root)\n\n        self._gcs = storage.Client()\n        super().__init__()\n\n    @property\n    def root(self) -> Path:\n        \"\"\"The root directory of the file storage.\"\"\"\n        return self._root\n\n    @property\n    def restrict_to_root(self) -> bool:\n        \"\"\"Whether to restrict generated paths to the root.\"\"\"\n        return True\n\n    @property\n    def is_local(self) -> bool:\n        \"\"\"Whether the storage is local (i.e. on the same machine, not cloud-based).\"\"\"\n        return False\n\n    def initialize(self) -> None:\n        logger.debug(f\"Initializing {repr(self)}...\")\n        try:\n            self._bucket = self._gcs.get_bucket(self._bucket_name)\n        except NotFound:\n            logger.info(f\"Bucket '{self._bucket_name}' does not exist; creating it...\")\n            self._bucket = self._gcs.create_bucket(self._bucket_name)\n\n    def get_path(self, relative_path: str | Path) -> Path:\n        # We set GCS root with \"/\" at the beginning\n        # but relative_to(\"/\") will remove it\n        # because we don't actually want it in the storage filenames\n        return super().get_path(relative_path).relative_to(\"/\")\n\n    def _get_blob(self, path: str | Path) -> storage.Blob:\n        path = self.get_path(path)\n        return self._bucket.blob(str(path))\n\n    @overload\n    def open_file(\n        self,\n        path: str | Path,\n        mode: Literal[\"r\", \"w\"] = \"r\",\n        binary: Literal[False] = False,\n    ) -> TextIOWrapper:\n        ...\n\n    @overload\n    def open_file(\n        self, path: str | Path, mode: Literal[\"r\"], binary: Literal[True]\n    ) -> BlobReader:\n        ...\n\n    @overload\n    def open_file(\n        self, path: str | Path, mode: Literal[\"w\"], binary: Literal[True]\n    ) -> BlobWriter:\n        ...\n\n    @overload\n    def open_file(\n        self, path: str | Path, mode: Literal[\"r\", \"w\"], binary: Literal[True]\n    ) -> BlobWriter | BlobReader:\n        ...\n\n    @overload\n    def open_file(self, path: str | Path, *, binary: Literal[True]) -> BlobReader:\n        ...\n\n    @overload\n    def open_file(\n        self, path: str | Path, mode: Literal[\"r\", \"w\"] = \"r\", binary: bool = False\n    ) -> BlobReader | BlobWriter | TextIOWrapper:\n        ...\n\n    # https://github.com/microsoft/pyright/issues/8007\n    def open_file(  # pyright: ignore[reportIncompatibleMethodOverride]\n        self, path: str | Path, mode: Literal[\"r\", \"w\"] = \"r\", binary: bool = False\n    ) -> BlobReader | BlobWriter | TextIOWrapper:\n        \"\"\"Open a file in the storage.\"\"\"\n        blob = self._get_blob(path)\n        blob.reload()  # pin revision number to prevent version mixing while reading\n        return blob.open(f\"{mode}b\" if binary else mode)\n\n    @overload\n    def read_file(self, path: str | Path, binary: Literal[False] = False) -> str:\n        \"\"\"Read a file in the storage as text.\"\"\"\n        ...\n\n    @overload\n    def read_file(self, path: str | Path, binary: Literal[True]) -> bytes:\n        \"\"\"Read a file in the storage as binary.\"\"\"\n        ...\n\n    @overload\n    def read_file(self, path: str | Path, binary: bool = False) -> str | bytes:\n        \"\"\"Read a file in the storage.\"\"\"\n        ...\n\n    def read_file(self, path: str | Path, binary: bool = False) -> str | bytes:\n        \"\"\"Read a file in the storage.\"\"\"\n        return self.open_file(path, \"r\", binary).read()\n\n    async def write_file(self, path: str | Path, content: str | bytes) -> None:\n        \"\"\"Write to a file in the storage.\"\"\"\n        blob = self._get_blob(path)\n\n        blob.upload_from_string(\n            data=content,\n            content_type=(\n                \"text/plain\"\n                if type(content) is str\n                # TODO: get MIME type from file extension or binary content\n                else \"application/octet-stream\"\n            ),\n        )\n\n        if self.on_write_file:\n            path = Path(path)\n            if path.is_absolute():\n                path = path.relative_to(self.root)\n            res = self.on_write_file(path)\n            if inspect.isawaitable(res):\n                await res\n\n    def list_files(self, path: str | Path = \".\") -> list[Path]:\n        \"\"\"List all files (recursively) in a directory in the storage.\"\"\"\n        path = self.get_path(path)\n        return [\n            Path(blob.name).relative_to(path)\n            for blob in self._bucket.list_blobs(\n                prefix=f\"{path}/\" if path != Path(\".\") else None\n            )\n        ]\n\n    def list_folders(\n        self, path: str | Path = \".\", recursive: bool = False\n    ) -> list[Path]:\n        \"\"\"List 'directories' directly in a given path or recursively in the storage.\"\"\"\n        path = self.get_path(path)\n        folder_names = set()\n\n        # List objects with the specified prefix and delimiter\n        for blob in self._bucket.list_blobs(prefix=path):\n            # Remove path prefix and the object name (last part)\n            folder = Path(blob.name).relative_to(path).parent\n            if not folder or folder == Path(\".\"):\n                continue\n            # For non-recursive, only add the first level of folders\n            if not recursive:\n                folder_names.add(folder.parts[0])\n            else:\n                # For recursive, need to add all nested folders\n                for i in range(len(folder.parts)):\n                    folder_names.add(\"/\".join(folder.parts[: i + 1]))\n\n        return [Path(f) for f in folder_names]\n\n    def delete_file(self, path: str | Path) -> None:\n        \"\"\"Delete a file in the storage.\"\"\"\n        path = self.get_path(path)\n        blob = self._bucket.blob(str(path))\n        blob.delete()\n\n    def delete_dir(self, path: str | Path) -> None:\n        \"\"\"Delete an empty folder in the storage.\"\"\"\n        # Since GCS does not have directories, we don't need to do anything\n        pass\n\n    def exists(self, path: str | Path) -> bool:\n        \"\"\"Check if a file or folder exists in GCS storage.\"\"\"\n        path = self.get_path(path)\n        # Check for exact blob match (file)\n        blob = self._bucket.blob(str(path))\n        if blob.exists():\n            return True\n        # Check for any blobs with prefix (folder)\n        prefix = f\"{str(path).rstrip('/')}/\"\n        blobs = self._bucket.list_blobs(prefix=prefix, max_results=1)\n        return next(blobs, None) is not None\n\n    def make_dir(self, path: str | Path) -> None:\n        \"\"\"Create a directory in the storage if doesn't exist.\"\"\"\n        # GCS does not have directories, so we don't need to do anything\n        pass\n\n    def rename(self, old_path: str | Path, new_path: str | Path) -> None:\n        \"\"\"Rename a file or folder in the storage.\"\"\"\n        old_path = self.get_path(old_path)\n        new_path = self.get_path(new_path)\n        blob = self._bucket.blob(str(old_path))\n        # If the blob with exact name exists, rename it\n        if blob.exists():\n            self._bucket.rename_blob(blob, new_name=str(new_path))\n            return\n        # Otherwise, rename all blobs with the prefix (folder)\n        for blob in self._bucket.list_blobs(prefix=f\"{old_path}/\"):\n            new_name = str(blob.name).replace(str(old_path), str(new_path), 1)\n            self._bucket.rename_blob(blob, new_name=new_name)\n\n    def copy(self, source: str | Path, destination: str | Path) -> None:\n        \"\"\"Copy a file or folder with all contents in the storage.\"\"\"\n        source = self.get_path(source)\n        destination = self.get_path(destination)\n        # If the source is a file, copy it\n        if self._bucket.blob(str(source)).exists():\n            self._bucket.copy_blob(\n                self._bucket.blob(str(source)), self._bucket, str(destination)\n            )\n            return\n        # Otherwise, copy all blobs with the prefix (folder)\n        for blob in self._bucket.list_blobs(prefix=f\"{source}/\"):\n            new_name = str(blob.name).replace(str(source), str(destination), 1)\n            self._bucket.copy_blob(blob, self._bucket, new_name)\n\n    def clone_with_subroot(self, subroot: str | Path) -> GCSFileStorage:\n        \"\"\"Create a new GCSFileStorage with a subroot of the current storage.\"\"\"\n        file_storage = GCSFileStorage(\n            GCSFileStorageConfiguration(\n                root=Path(\"/\").joinpath(self.get_path(subroot)),\n                bucket=self._bucket_name,\n            )\n        )\n        file_storage._gcs = self._gcs\n        file_storage._bucket = self._bucket\n        return file_storage\n\n    def __repr__(self) -> str:\n        return f\"{__class__.__name__}(bucket='{self._bucket_name}', root={self._root})\"\n", "forge/forge/command/command.py": "from __future__ import annotations\n\nimport inspect\nfrom typing import Callable, Concatenate, Generic, ParamSpec, TypeVar, cast\n\nfrom forge.agent.protocols import CommandProvider\n\nfrom .parameter import CommandParameter\n\nP = ParamSpec(\"P\")\nCO = TypeVar(\"CO\")  # command output\n\n_CP = TypeVar(\"_CP\", bound=CommandProvider)\n\n\nclass Command(Generic[P, CO]):\n    \"\"\"A class representing a command.\n\n    Attributes:\n        name (str): The name of the command.\n        description (str): A brief description of what the command does.\n        parameters (list): The parameters of the function that the command executes.\n    \"\"\"\n\n    def __init__(\n        self,\n        names: list[str],\n        description: str,\n        method: Callable[Concatenate[_CP, P], CO],\n        parameters: list[CommandParameter],\n    ):\n        # Check if all parameters are provided\n        if not self._parameters_match(method, parameters):\n            raise ValueError(\n                f\"Command {names[0]} has different parameters than provided schema\"\n            )\n        self.names = names\n        self.description = description\n        # Method technically has a `self` parameter, but we can ignore that\n        # since Python passes it internally.\n        self.method = cast(Callable[P, CO], method)\n        self.parameters = parameters\n\n    @property\n    def is_async(self) -> bool:\n        return inspect.iscoroutinefunction(self.method)\n\n    def _parameters_match(\n        self, func: Callable, parameters: list[CommandParameter]\n    ) -> bool:\n        # Get the function's signature\n        signature = inspect.signature(func)\n        # Extract parameter names, ignoring 'self' for methods\n        func_param_names = [\n            param.name\n            for param in signature.parameters.values()\n            if param.name != \"self\"\n        ]\n        names = [param.name for param in parameters]\n        # Check if sorted lists of names/keys are equal\n        return sorted(func_param_names) == sorted(names)\n\n    def __call__(self, *args: P.args, **kwargs: P.kwargs) -> CO:\n        return self.method(*args, **kwargs)\n\n    def __str__(self) -> str:\n        params = [\n            f\"{param.name}: \"\n            + (\"%s\" if param.spec.required else \"Optional[%s]\")\n            % (param.spec.type.value if param.spec.type else \"Any\")\n            for param in self.parameters\n        ]\n        return (\n            f\"{self.names[0]}: {self.description.rstrip('.')}. \"\n            f\"Params: ({', '.join(params)})\"\n        )\n\n    def __get__(self, instance, owner):\n        if instance is None:\n            # Accessed on the class, not an instance\n            return self\n        # Bind the method to the instance\n        return Command(\n            self.names,\n            self.description,\n            self.method.__get__(instance, owner),\n            self.parameters,\n        )\n", "forge/forge/command/__init__.py": "from .command import Command\nfrom .decorator import command\nfrom .parameter import CommandParameter\n\n__all__ = [\"Command\", \"CommandParameter\", \"command\"]\n", "forge/forge/command/parameter.py": "from pydantic import BaseModel\n\nfrom forge.models.json_schema import JSONSchema\n\n\nclass CommandParameter(BaseModel):\n    name: str\n    spec: JSONSchema\n\n    def __repr__(self):\n        return \"CommandParameter('%s', '%s', '%s', %s)\" % (\n            self.name,\n            self.spec.type,\n            self.spec.description,\n            self.spec.required,\n        )\n", "forge/forge/command/decorator.py": "import re\nfrom typing import Callable, Concatenate, Optional, TypeVar\n\nfrom forge.agent.protocols import CommandProvider\nfrom forge.models.json_schema import JSONSchema\n\nfrom .command import CO, Command, CommandParameter, P\n\n_CP = TypeVar(\"_CP\", bound=CommandProvider)\n\n\ndef command(\n    names: list[str] = [],\n    description: Optional[str] = None,\n    parameters: dict[str, JSONSchema] = {},\n) -> Callable[[Callable[Concatenate[_CP, P], CO]], Command[P, CO]]:\n    \"\"\"\n    The command decorator is used to make a Command from a function.\n\n    Args:\n        names (list[str]): The names of the command.\n            If not provided, the function name will be used.\n        description (str): A brief description of what the command does.\n            If not provided, the docstring until double line break will be used\n            (or entire docstring if no double line break is found)\n        parameters (dict[str, JSONSchema]): The parameters of the function\n            that the command executes.\n    \"\"\"\n\n    def decorator(func: Callable[Concatenate[_CP, P], CO]) -> Command[P, CO]:\n        doc = func.__doc__ or \"\"\n        # If names is not provided, use the function name\n        command_names = names or [func.__name__]\n        # If description is not provided, use the first part of the docstring\n        if not (command_description := description):\n            if not func.__doc__:\n                raise ValueError(\"Description is required if function has no docstring\")\n            # Return the part of the docstring before double line break or everything\n            command_description = re.sub(r\"\\s+\", \" \", doc.split(\"\\n\\n\")[0].strip())\n\n        # Parameters\n        typed_parameters = [\n            CommandParameter(\n                name=param_name,\n                spec=spec,\n            )\n            for param_name, spec in parameters.items()\n        ]\n\n        # Wrap func with Command\n        command = Command(\n            names=command_names,\n            description=command_description,\n            method=func,\n            parameters=typed_parameters,\n        )\n\n        return command\n\n    return decorator\n", "forge/forge/json/parsing.py": "import logging\nimport re\nfrom typing import Any\n\nimport demjson3\n\nlogger = logging.getLogger(__name__)\n\n\ndef json_loads(json_str: str) -> Any:\n    \"\"\"Parse a JSON string, tolerating minor syntax issues:\n    - Missing, extra and trailing commas\n    - Extraneous newlines and whitespace outside of string literals\n    - Inconsistent spacing after colons and commas\n    - Missing closing brackets or braces\n    - Numbers: binary, hex, octal, trailing and prefixed decimal points\n    - Different encodings\n    - Surrounding markdown code block\n    - Comments\n\n    Args:\n        json_str: The JSON string to parse.\n\n    Returns:\n        The parsed JSON object, same as built-in json.loads.\n    \"\"\"\n    # Remove possible code block\n    pattern = r\"```(?:json|JSON)*([\\s\\S]*?)```\"\n    match = re.search(pattern, json_str)\n\n    if match:\n        json_str = match.group(1).strip()\n\n    json_result = demjson3.decode(json_str, return_errors=True)\n    assert json_result is not None  # by virtue of return_errors=True\n\n    if json_result.errors:\n        logger.debug(\n            \"JSON parse errors:\\n\" + \"\\n\".join(str(e) for e in json_result.errors)\n        )\n\n    if json_result.object in (demjson3.syntax_error, demjson3.undefined):\n        raise ValueError(\n            f\"Failed to parse JSON string: {json_str}\", *json_result.errors\n        )\n\n    return json_result.object\n\n\ndef extract_dict_from_json(json_str: str) -> dict[str, Any]:\n    # Sometimes the response includes the JSON in a code block with ```\n    pattern = r\"```(?:json|JSON)*([\\s\\S]*?)```\"\n    match = re.search(pattern, json_str)\n\n    if match:\n        json_str = match.group(1).strip()\n    else:\n        # The string may contain JSON.\n        json_pattern = r\"{[\\s\\S]*}\"\n        match = re.search(json_pattern, json_str)\n\n        if match:\n            json_str = match.group()\n\n    result = json_loads(json_str)\n    if not isinstance(result, dict):\n        raise ValueError(\n            f\"Response '''{json_str}''' evaluated to non-dict value {repr(result)}\"\n        )\n    return result\n\n\ndef extract_list_from_json(json_str: str) -> list[Any]:\n    # Sometimes the response includes the JSON in a code block with ```\n    pattern = r\"```(?:json|JSON)*([\\s\\S]*?)```\"\n    match = re.search(pattern, json_str)\n\n    if match:\n        json_str = match.group(1).strip()\n    else:\n        # The string may contain JSON.\n        json_pattern = r\"\\[[\\s\\S]*\\]\"\n        match = re.search(json_pattern, json_str)\n\n        if match:\n            json_str = match.group()\n\n    result = json_loads(json_str)\n    if not isinstance(result, list):\n        raise ValueError(\n            f\"Response '''{json_str}''' evaluated to non-list value {repr(result)}\"\n        )\n    return result\n", "forge/forge/json/__init__.py": "", "forge/forge/agent_protocol/agent_test.py": "from pathlib import Path\n\nimport pytest\nfrom fastapi import UploadFile\n\nfrom forge.file_storage.base import FileStorageConfiguration\nfrom forge.file_storage.local import LocalFileStorage\n\nfrom .agent import ProtocolAgent\nfrom .database.db import AgentDB\nfrom .models.task import StepRequestBody, Task, TaskListResponse, TaskRequestBody\n\n\n@pytest.fixture\ndef agent(test_workspace: Path):\n    db = AgentDB(\"sqlite:///test.db\")\n    config = FileStorageConfiguration(root=test_workspace)\n    workspace = LocalFileStorage(config)\n    return ProtocolAgent(db, workspace)\n\n\n@pytest.fixture\ndef file_upload():\n    this_file = Path(__file__)\n    file_handle = this_file.open(\"rb\")\n    yield UploadFile(file_handle, filename=this_file.name)\n    file_handle.close()\n\n\n@pytest.mark.asyncio\nasync def test_create_task(agent: ProtocolAgent):\n    task_request = TaskRequestBody(\n        input=\"test_input\", additional_input={\"input\": \"additional_test_input\"}\n    )\n    task: Task = await agent.create_task(task_request)\n    assert task.input == \"test_input\"\n\n\n@pytest.mark.asyncio\nasync def test_list_tasks(agent: ProtocolAgent):\n    task_request = TaskRequestBody(\n        input=\"test_input\", additional_input={\"input\": \"additional_test_input\"}\n    )\n    await agent.create_task(task_request)\n    tasks = await agent.list_tasks()\n    assert isinstance(tasks, TaskListResponse)\n\n\n@pytest.mark.asyncio\nasync def test_get_task(agent: ProtocolAgent):\n    task_request = TaskRequestBody(\n        input=\"test_input\", additional_input={\"input\": \"additional_test_input\"}\n    )\n    task = await agent.create_task(task_request)\n    retrieved_task = await agent.get_task(task.task_id)\n    assert retrieved_task.task_id == task.task_id\n\n\n@pytest.mark.xfail(reason=\"execute_step is not implemented\")\n@pytest.mark.asyncio\nasync def test_execute_step(agent: ProtocolAgent):\n    task_request = TaskRequestBody(\n        input=\"test_input\", additional_input={\"input\": \"additional_test_input\"}\n    )\n    task = await agent.create_task(task_request)\n    step_request = StepRequestBody(\n        input=\"step_input\", additional_input={\"input\": \"additional_test_input\"}\n    )\n    step = await agent.execute_step(task.task_id, step_request)\n    assert step.input == \"step_input\"\n    assert step.additional_input == {\"input\": \"additional_test_input\"}\n\n\n@pytest.mark.xfail(reason=\"execute_step is not implemented\")\n@pytest.mark.asyncio\nasync def test_get_step(agent: ProtocolAgent):\n    task_request = TaskRequestBody(\n        input=\"test_input\", additional_input={\"input\": \"additional_test_input\"}\n    )\n    task = await agent.create_task(task_request)\n    step_request = StepRequestBody(\n        input=\"step_input\", additional_input={\"input\": \"additional_test_input\"}\n    )\n    step = await agent.execute_step(task.task_id, step_request)\n    retrieved_step = await agent.get_step(task.task_id, step.step_id)\n    assert retrieved_step.step_id == step.step_id\n\n\n@pytest.mark.asyncio\nasync def test_list_artifacts(agent: ProtocolAgent):\n    tasks = await agent.list_tasks()\n    assert tasks.tasks, \"No tasks in test.db\"\n\n    artifacts = await agent.list_artifacts(tasks.tasks[0].task_id)\n    assert isinstance(artifacts.artifacts, list)\n\n\n@pytest.mark.asyncio\nasync def test_create_artifact(agent: ProtocolAgent, file_upload: UploadFile):\n    task_request = TaskRequestBody(\n        input=\"test_input\", additional_input={\"input\": \"additional_test_input\"}\n    )\n    task = await agent.create_task(task_request)\n    artifact = await agent.create_artifact(\n        task_id=task.task_id,\n        file=file_upload,\n        relative_path=f\"a_dir/{file_upload.filename}\",\n    )\n    assert artifact.file_name == file_upload.filename\n    assert artifact.relative_path == f\"a_dir/{file_upload.filename}\"\n\n\n@pytest.mark.asyncio\nasync def test_create_and_get_artifact(agent: ProtocolAgent, file_upload: UploadFile):\n    task_request = TaskRequestBody(\n        input=\"test_input\", additional_input={\"input\": \"additional_test_input\"}\n    )\n    task = await agent.create_task(task_request)\n\n    artifact = await agent.create_artifact(\n        task_id=task.task_id,\n        file=file_upload,\n        relative_path=f\"b_dir/{file_upload.filename}\",\n    )\n    await file_upload.seek(0)\n    file_upload_content = await file_upload.read()\n\n    retrieved_artifact = await agent.get_artifact(task.task_id, artifact.artifact_id)\n    retrieved_artifact_content = bytearray()\n    async for b in retrieved_artifact.body_iterator:\n        retrieved_artifact_content.extend(b)  # type: ignore\n    assert retrieved_artifact_content == file_upload_content\n", "forge/forge/agent_protocol/api_router.py": "\"\"\"\nRoutes for the Agent Service.\n\nThis module defines the API routes for the Agent service.\n\nDevelopers and contributors should be especially careful when making modifications\nto these routes to ensure consistency and correctness in the system's behavior.\n\"\"\"\nimport logging\nfrom typing import TYPE_CHECKING, Optional\n\nfrom fastapi import APIRouter, HTTPException, Query, Request, Response, UploadFile\nfrom fastapi.responses import StreamingResponse\n\nfrom .models import (\n    Artifact,\n    Step,\n    StepRequestBody,\n    Task,\n    TaskArtifactsListResponse,\n    TaskListResponse,\n    TaskRequestBody,\n    TaskStepsListResponse,\n)\n\nif TYPE_CHECKING:\n    from .agent import ProtocolAgent\n\nbase_router = APIRouter()\nlogger = logging.getLogger(__name__)\n\n\n@base_router.get(\"/\", tags=[\"root\"])\nasync def root():\n    \"\"\"\n    Root endpoint that returns a welcome message.\n    \"\"\"\n    return Response(content=\"Welcome to the AutoGPT Forge\")\n\n\n@base_router.get(\"/heartbeat\", tags=[\"server\"])\nasync def check_server_status():\n    \"\"\"\n    Check if the server is running.\n    \"\"\"\n    return Response(content=\"Server is running.\", status_code=200)\n\n\n@base_router.post(\"/agent/tasks\", tags=[\"agent\"], response_model=Task)\nasync def create_agent_task(request: Request, task_request: TaskRequestBody) -> Task:\n    \"\"\"\n    Creates a new task using the provided TaskRequestBody and returns a Task.\n\n    Args:\n        request (Request): FastAPI request object.\n        task (TaskRequestBody): The task request containing input data.\n\n    Returns:\n        Task: A new task with task_id, input, and additional_input set.\n\n    Example:\n        Request (TaskRequestBody defined in schema.py):\n            {\n                \"input\": \"Write the words you receive to the file 'output.txt'.\",\n                \"additional_input\": \"python/code\"\n            }\n\n        Response (Task defined in schema.py):\n            {\n                \"task_id\": \"50da533e-3904-4401-8a07-c49adf88b5eb\",\n                \"input\": \"Write the word 'Washington' to a .txt file\",\n                \"additional_input\": \"python/code\",\n                \"artifacts\": [],\n            }\n    \"\"\"\n    agent: \"ProtocolAgent\" = request[\"agent\"]\n\n    try:\n        task = await agent.create_task(task_request)\n        return task\n    except Exception:\n        logger.exception(f\"Error whilst trying to create a task: {task_request}\")\n        raise\n\n\n@base_router.get(\"/agent/tasks\", tags=[\"agent\"], response_model=TaskListResponse)\nasync def list_agent_tasks(\n    request: Request,\n    page: int = Query(1, ge=1),\n    page_size: int = Query(10, ge=1),\n) -> TaskListResponse:\n    \"\"\"\n    Retrieves a paginated list of all tasks.\n\n    Args:\n        request (Request): FastAPI request object.\n        page (int, optional): Page number for pagination. Default: 1\n        page_size (int, optional): Number of tasks per page for pagination. Default: 10\n\n    Returns:\n        TaskListResponse: A list of tasks, and pagination details.\n\n    Example:\n        Request:\n            GET /agent/tasks?page=1&pageSize=10\n\n        Response (TaskListResponse defined in schema.py):\n            {\n                \"items\": [\n                    {\n                        \"input\": \"Write the word 'Washington' to a .txt file\",\n                        \"additional_input\": null,\n                        \"task_id\": \"50da533e-3904-4401-8a07-c49adf88b5eb\",\n                        \"artifacts\": [],\n                        \"steps\": []\n                    },\n                    ...\n                ],\n                \"pagination\": {\n                    \"total\": 100,\n                    \"pages\": 10,\n                    \"current\": 1,\n                    \"pageSize\": 10\n                }\n            }\n    \"\"\"\n    agent: \"ProtocolAgent\" = request[\"agent\"]\n    try:\n        tasks = await agent.list_tasks(page, page_size)\n        return tasks\n    except Exception:\n        logger.exception(\"Error whilst trying to list tasks\")\n        raise\n\n\n@base_router.get(\"/agent/tasks/{task_id}\", tags=[\"agent\"], response_model=Task)\nasync def get_agent_task(request: Request, task_id: str) -> Task:\n    \"\"\"\n    Gets the details of a task by ID.\n\n    Args:\n        request (Request): FastAPI request object.\n        task_id (str): The ID of the task.\n\n    Returns:\n        Task: The task with the given ID.\n\n    Example:\n        Request:\n            GET /agent/tasks/50da533e-3904-4401-8a07-c49adf88b5eb\n\n        Response (Task defined in schema.py):\n            {\n                \"input\": \"Write the word 'Washington' to a .txt file\",\n                \"additional_input\": null,\n                \"task_id\": \"50da533e-3904-4401-8a07-c49adf88b5eb\",\n                \"artifacts\": [\n                    {\n                        \"artifact_id\": \"7a49f31c-f9c6-4346-a22c-e32bc5af4d8e\",\n                        \"file_name\": \"output.txt\",\n                        \"agent_created\": true,\n                        \"relative_path\": \"file://50da533e-3904-4401-8a07-c49adf88b5eb/output.txt\"\n                    }\n                ],\n                \"steps\": [\n                    {\n                        \"task_id\": \"50da533e-3904-4401-8a07-c49adf88b5eb\",\n                        \"step_id\": \"6bb1801a-fd80-45e8-899a-4dd723cc602e\",\n                        \"input\": \"Write the word 'Washington' to a .txt file\",\n                        \"additional_input\": \"challenge:write_to_file\",\n                        \"name\": \"Write to file\",\n                        \"status\": \"completed\",\n                        \"output\": \"I am going to use the write_to_file command and write Washington to a file called output.txt <write_to_file('output.txt', 'Washington')>\",\n                        \"additional_output\": \"Do you want me to continue?\",\n                        \"artifacts\": [\n                            {\n                                \"artifact_id\": \"7a49f31c-f9c6-4346-a22c-e32bc5af4d8e\",\n                                \"file_name\": \"output.txt\",\n                                \"agent_created\": true,\n                                \"relative_path\": \"file://50da533e-3904-4401-8a07-c49adf88b5eb/output.txt\"\n                            }\n                        ],\n                        \"is_last\": true\n                    }\n                ]\n            }\n    \"\"\"  # noqa: E501\n    agent: \"ProtocolAgent\" = request[\"agent\"]\n    try:\n        task = await agent.get_task(task_id)\n        return task\n    except Exception:\n        logger.exception(f\"Error whilst trying to get task: {task_id}\")\n        raise\n\n\n@base_router.get(\n    \"/agent/tasks/{task_id}/steps\",\n    tags=[\"agent\"],\n    response_model=TaskStepsListResponse,\n)\nasync def list_agent_task_steps(\n    request: Request,\n    task_id: str,\n    page: int = Query(1, ge=1),\n    page_size: int = Query(10, ge=1, alias=\"pageSize\"),\n) -> TaskStepsListResponse:\n    \"\"\"\n    Retrieves a paginated list of steps associated with a specific task.\n\n    Args:\n        request (Request): FastAPI request object.\n        task_id (str): The ID of the task.\n        page (int, optional): The page number for pagination. Defaults to 1.\n        page_size (int, optional): Number of steps per page for pagination. Default: 10.\n\n    Returns:\n        TaskStepsListResponse: A list of steps, and pagination details.\n\n    Example:\n        Request:\n            GET /agent/tasks/50da533e-3904-4401-8a07-c49adf88b5eb/steps?page=1&pageSize=10\n\n        Response (TaskStepsListResponse defined in schema.py):\n            {\n                \"items\": [\n                    {\n                        \"task_id\": \"50da533e-3904-4401-8a07-c49adf88b5eb\",\n                        \"step_id\": \"step1_id\",\n                        ...\n                    },\n                    ...\n                ],\n                \"pagination\": {\n                    \"total\": 100,\n                    \"pages\": 10,\n                    \"current\": 1,\n                    \"pageSize\": 10\n                }\n            }\n    \"\"\"  # noqa: E501\n    agent: \"ProtocolAgent\" = request[\"agent\"]\n    try:\n        steps = await agent.list_steps(task_id, page, page_size)\n        return steps\n    except Exception:\n        logger.exception(\"Error whilst trying to list steps\")\n        raise\n\n\n@base_router.post(\"/agent/tasks/{task_id}/steps\", tags=[\"agent\"], response_model=Step)\nasync def execute_agent_task_step(\n    request: Request, task_id: str, step_request: Optional[StepRequestBody] = None\n) -> Step:\n    \"\"\"\n    Executes the next step for a specified task based on the current task status and\n    returns the executed step with additional feedback fields.\n\n    This route is significant because this is where the agent actually performs work.\n    The function handles executing the next step for a task based on its current state,\n    and it requires careful implementation to ensure all scenarios (like the presence\n    or absence of steps or a step marked as `last_step`) are handled correctly.\n\n    Depending on the current state of the task, the following scenarios are possible:\n    1. No steps exist for the task.\n    2. There is at least one step already for the task, and the task does not have a\n       completed step marked as `last_step`.\n    3. There is a completed step marked as `last_step` already on the task.\n\n    In each of these scenarios, a step object will be returned with two additional\n    fields: `output` and `additional_output`.\n    - `output`: Provides the primary response or feedback to the user.\n    - `additional_output`: Supplementary information or data. Its specific content is\n      not strictly defined and can vary based on the step or agent's implementation.\n\n    Args:\n        request (Request): FastAPI request object.\n        task_id (str): The ID of the task.\n        step (StepRequestBody): The details for executing the step.\n\n    Returns:\n        Step: Details of the executed step with additional feedback.\n\n    Example:\n        Request:\n            POST /agent/tasks/50da533e-3904-4401-8a07-c49adf88b5eb/steps\n            {\n                \"input\": \"Step input details...\",\n                ...\n            }\n\n        Response:\n            {\n                \"task_id\": \"50da533e-3904-4401-8a07-c49adf88b5eb\",\n                \"step_id\": \"step1_id\",\n                \"output\": \"Primary feedback...\",\n                \"additional_output\": \"Supplementary details...\",\n                ...\n            }\n    \"\"\"\n    agent: \"ProtocolAgent\" = request[\"agent\"]\n    try:\n        # An empty step request represents a yes to continue command\n        if not step_request:\n            step_request = StepRequestBody(input=\"y\")\n\n        step = await agent.execute_step(task_id, step_request)\n        return step\n    except Exception:\n        logger.exception(f\"Error whilst trying to execute a task step: {task_id}\")\n        raise\n\n\n@base_router.get(\n    \"/agent/tasks/{task_id}/steps/{step_id}\", tags=[\"agent\"], response_model=Step\n)\nasync def get_agent_task_step(request: Request, task_id: str, step_id: str) -> Step:\n    \"\"\"\n    Retrieves the details of a specific step for a given task.\n\n    Args:\n        request (Request): FastAPI request object.\n        task_id (str): The ID of the task.\n        step_id (str): The ID of the step.\n\n    Returns:\n        Step: Details of the specific step.\n\n    Example:\n        Request:\n            GET /agent/tasks/50da533e-3904-4401-8a07-c49adf88b5eb/steps/step1_id\n\n        Response:\n            {\n                \"task_id\": \"50da533e-3904-4401-8a07-c49adf88b5eb\",\n                \"step_id\": \"step1_id\",\n                ...\n            }\n    \"\"\"\n    agent: \"ProtocolAgent\" = request[\"agent\"]\n    try:\n        step = await agent.get_step(task_id, step_id)\n        return step\n    except Exception:\n        logger.exception(f\"Error whilst trying to get step: {step_id}\")\n        raise\n\n\n@base_router.get(\n    \"/agent/tasks/{task_id}/artifacts\",\n    tags=[\"agent\"],\n    response_model=TaskArtifactsListResponse,\n)\nasync def list_agent_task_artifacts(\n    request: Request,\n    task_id: str,\n    page: int = Query(1, ge=1),\n    page_size: int = Query(10, ge=1, alias=\"pageSize\"),\n) -> TaskArtifactsListResponse:\n    \"\"\"\n    Retrieves a paginated list of artifacts associated with a specific task.\n\n    Args:\n        request (Request): FastAPI request object.\n        task_id (str): The ID of the task.\n        page (int, optional): The page number for pagination. Defaults to 1.\n        page_size (int, optional): Number of items per page for pagination. Default: 10.\n\n    Returns:\n        TaskArtifactsListResponse: A list of artifacts, and pagination details.\n\n    Example:\n        Request:\n            GET /agent/tasks/50da533e-3904-4401-8a07-c49adf88b5eb/artifacts?page=1&pageSize=10\n\n        Response (TaskArtifactsListResponse defined in schema.py):\n            {\n                \"items\": [\n                    {\"artifact_id\": \"artifact1_id\", ...},\n                    {\"artifact_id\": \"artifact2_id\", ...},\n                    ...\n                ],\n                \"pagination\": {\n                    \"total\": 100,\n                    \"pages\": 10,\n                    \"current\": 1,\n                    \"pageSize\": 10\n                }\n            }\n    \"\"\"  # noqa: E501\n    agent: \"ProtocolAgent\" = request[\"agent\"]\n    try:\n        artifacts = await agent.list_artifacts(task_id, page, page_size)\n        return artifacts\n    except Exception:\n        logger.exception(\"Error whilst trying to list artifacts\")\n        raise\n\n\n@base_router.post(\n    \"/agent/tasks/{task_id}/artifacts\", tags=[\"agent\"], response_model=Artifact\n)\nasync def upload_agent_task_artifacts(\n    request: Request, task_id: str, file: UploadFile, relative_path: str = \"\"\n) -> Artifact:\n    \"\"\"\n    This endpoint is used to upload an artifact (file) associated with a specific task.\n\n    Args:\n        request (Request): The FastAPI request object.\n        task_id (str): The ID of the task for which the artifact is being uploaded.\n        file (UploadFile): The file being uploaded as an artifact.\n        relative_path (str): The relative path for the file. This is a query parameter.\n\n    Returns:\n        Artifact: Metadata object for the uploaded artifact, including its ID and path.\n\n    Example:\n        Request:\n            POST /agent/tasks/50da533e-3904-4401-8a07-c49adf88b5eb/artifacts?relative_path=my_folder/my_other_folder\n            File: <uploaded_file>\n\n        Response:\n            {\n                \"artifact_id\": \"b225e278-8b4c-4f99-a696-8facf19f0e56\",\n                \"created_at\": \"2023-01-01T00:00:00Z\",\n                \"modified_at\": \"2023-01-01T00:00:00Z\",\n                \"agent_created\": false,\n                \"relative_path\": \"/my_folder/my_other_folder/\",\n                \"file_name\": \"main.py\"\n            }\n    \"\"\"  # noqa: E501\n    agent: \"ProtocolAgent\" = request[\"agent\"]\n\n    if file is None:\n        raise HTTPException(status_code=400, detail=\"File must be specified\")\n    try:\n        artifact = await agent.create_artifact(task_id, file, relative_path)\n        return artifact\n    except Exception:\n        logger.exception(f\"Error whilst trying to upload artifact: {task_id}\")\n        raise\n\n\n@base_router.get(\n    \"/agent/tasks/{task_id}/artifacts/{artifact_id}\",\n    tags=[\"agent\"],\n    response_model=str,\n)\nasync def download_agent_task_artifact(\n    request: Request, task_id: str, artifact_id: str\n) -> StreamingResponse:\n    \"\"\"\n    Downloads an artifact associated with a specific task.\n\n    Args:\n        request (Request): FastAPI request object.\n        task_id (str): The ID of the task.\n        artifact_id (str): The ID of the artifact.\n\n    Returns:\n        FileResponse: The downloaded artifact file.\n\n    Example:\n        Request:\n            GET /agent/tasks/50da533e-3904-4401-8a07-c49adf88b5eb/artifacts/artifact1_id\n\n        Response:\n            <file_content_of_artifact>\n    \"\"\"\n    agent: \"ProtocolAgent\" = request[\"agent\"]\n    try:\n        return await agent.get_artifact(task_id, artifact_id)\n    except Exception:\n        logger.exception(f\"Error whilst trying to download artifact: {task_id}\")\n        raise\n", "forge/forge/agent_protocol/middlewares.py": "from starlette.types import ASGIApp\n\n\nclass AgentMiddleware:\n    \"\"\"\n    Middleware that injects the agent instance into the request scope.\n    \"\"\"\n\n    def __init__(self, app: ASGIApp, agent):\n        \"\"\"\n\n        Args:\n            app: The FastAPI app - automatically injected by FastAPI.\n            agent: The agent instance to inject into the request scope.\n\n        Examples:\n            >>> from fastapi import FastAPI, Request\n            >>> from agent_protocol.agent import Agent\n            >>> from agent_protocol.middlewares import AgentMiddleware\n            >>> app = FastAPI()\n            >>> @app.get(\"/\")\n            >>> async def root(request: Request):\n            >>>     agent = request[\"agent\"]\n            >>>     task = agent.db.create_task(\"Do something.\")\n            >>>     return {\"task_id\": a.task_id}\n            >>> agent = Agent()\n            >>> app.add_middleware(AgentMiddleware, agent=agent)\n        \"\"\"\n        self.app = app\n        self.agent = agent\n\n    async def __call__(self, scope, receive, send):\n        scope[\"agent\"] = self.agent\n        await self.app(scope, receive, send)\n", "forge/forge/agent_protocol/agent.py": "import logging\nimport os\nimport pathlib\nfrom io import BytesIO\nfrom uuid import uuid4\n\nimport uvicorn\nfrom fastapi import APIRouter, FastAPI, UploadFile\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import RedirectResponse, StreamingResponse\nfrom fastapi.staticfiles import StaticFiles\n\nfrom forge.agent_protocol.api_router import base_router\nfrom forge.agent_protocol.database.db import AgentDB\nfrom forge.agent_protocol.middlewares import AgentMiddleware\nfrom forge.agent_protocol.models.task import (\n    Artifact,\n    Step,\n    StepRequestBody,\n    Task,\n    TaskArtifactsListResponse,\n    TaskListResponse,\n    TaskRequestBody,\n    TaskStepsListResponse,\n)\nfrom forge.file_storage.base import FileStorage\n\nlogger = logging.getLogger(__name__)\n\n\nclass ProtocolAgent:\n    def __init__(self, database: AgentDB, workspace: FileStorage):\n        self.db = database\n        self.workspace = workspace\n\n    def get_agent_app(self, router: APIRouter = base_router):\n        \"\"\"\n        Start the agent server.\n        \"\"\"\n\n        app = FastAPI(\n            title=\"AutoGPT Forge\",\n            description=\"Modified version of The Agent Protocol.\",\n            version=\"v0.4\",\n        )\n\n        # Add CORS middleware\n        origins = [\n            \"http://localhost:5000\",\n            \"http://127.0.0.1:5000\",\n            \"http://localhost:8000\",\n            \"http://127.0.0.1:8000\",\n            \"http://localhost:8080\",\n            \"http://127.0.0.1:8080\",\n            # Add any other origins you want to whitelist\n        ]\n\n        app.add_middleware(\n            CORSMiddleware,\n            allow_origins=origins,\n            allow_credentials=True,\n            allow_methods=[\"*\"],\n            allow_headers=[\"*\"],\n        )\n\n        app.include_router(router, prefix=\"/ap/v1\")\n        script_dir = os.path.dirname(os.path.realpath(__file__))\n        frontend_path = pathlib.Path(\n            os.path.join(script_dir, \"../../../frontend/build/web\")\n        ).resolve()\n\n        if os.path.exists(frontend_path):\n            app.mount(\"/app\", StaticFiles(directory=frontend_path), name=\"app\")\n\n            @app.get(\"/\", include_in_schema=False)\n            async def root():\n                return RedirectResponse(url=\"/app/index.html\", status_code=307)\n\n        else:\n            logger.warning(\n                f\"Frontend not found. {frontend_path} does not exist. \"\n                \"The frontend will not be served.\"\n            )\n        app.add_middleware(AgentMiddleware, agent=self)\n\n        return app\n\n    def start(self, port):\n        uvicorn.run(\n            \"forge.app:app\", host=\"localhost\", port=port, log_level=\"error\", reload=True\n        )\n\n    async def create_task(self, task_request: TaskRequestBody) -> Task:\n        \"\"\"\n        Create a task for the agent.\n        \"\"\"\n        task = await self.db.create_task(\n            input=task_request.input,\n            additional_input=task_request.additional_input,\n        )\n        return task\n\n    async def list_tasks(self, page: int = 1, pageSize: int = 10) -> TaskListResponse:\n        \"\"\"\n        List all tasks that the agent has created.\n        \"\"\"\n        tasks, pagination = await self.db.list_tasks(page, pageSize)\n        response = TaskListResponse(tasks=tasks, pagination=pagination)\n        return response\n\n    async def get_task(self, task_id: str) -> Task:\n        \"\"\"\n        Get a task by ID.\n        \"\"\"\n        task = await self.db.get_task(task_id)\n        return task\n\n    async def list_steps(\n        self, task_id: str, page: int = 1, pageSize: int = 10\n    ) -> TaskStepsListResponse:\n        \"\"\"\n        List the IDs of all steps that the task has created.\n        \"\"\"\n        steps, pagination = await self.db.list_steps(task_id, page, pageSize)\n        response = TaskStepsListResponse(steps=steps, pagination=pagination)\n        return response\n\n    async def execute_step(self, task_id: str, step_request: StepRequestBody) -> Step:\n        \"\"\"\n        Create a step for the task.\n        \"\"\"\n        raise NotImplementedError\n\n    async def get_step(self, task_id: str, step_id: str) -> Step:\n        \"\"\"\n        Get a step by ID.\n        \"\"\"\n        step = await self.db.get_step(task_id, step_id)\n        return step\n\n    async def list_artifacts(\n        self, task_id: str, page: int = 1, pageSize: int = 10\n    ) -> TaskArtifactsListResponse:\n        \"\"\"\n        List the artifacts that the task has created.\n        \"\"\"\n        artifacts, pagination = await self.db.list_artifacts(task_id, page, pageSize)\n        return TaskArtifactsListResponse(artifacts=artifacts, pagination=pagination)\n\n    async def create_artifact(\n        self, task_id: str, file: UploadFile, relative_path: str = \"\"\n    ) -> Artifact:\n        \"\"\"\n        Create an artifact for the task.\n        \"\"\"\n        file_name = file.filename or str(uuid4())\n        data = b\"\"\n        while contents := file.file.read(1024 * 1024):\n            data += contents\n        # Check if relative path ends with filename\n        if relative_path.endswith(file_name):\n            file_path = relative_path\n        else:\n            file_path = os.path.join(relative_path, file_name)\n\n        await self.workspace.write_file(file_path, data)\n\n        artifact = await self.db.create_artifact(\n            task_id=task_id,\n            file_name=file_name,\n            relative_path=relative_path,\n            agent_created=False,\n        )\n        return artifact\n\n    async def get_artifact(self, task_id: str, artifact_id: str) -> StreamingResponse:\n        \"\"\"\n        Get an artifact by ID.\n        \"\"\"\n        artifact = await self.db.get_artifact(artifact_id)\n        if artifact.file_name not in artifact.relative_path:\n            file_path = os.path.join(artifact.relative_path, artifact.file_name)\n        else:\n            file_path = artifact.relative_path\n        retrieved_artifact = self.workspace.read_file(file_path, binary=True)\n\n        return StreamingResponse(\n            BytesIO(retrieved_artifact),\n            media_type=\"application/octet-stream\",\n            headers={\n                \"Content-Disposition\": f\"attachment; filename={artifact.file_name}\"\n            },\n        )\n", "forge/forge/agent_protocol/__init__.py": "", "forge/forge/agent_protocol/models/task.py": "from __future__ import annotations\n\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any, List, Optional\n\nfrom pydantic import BaseModel, Field\n\nfrom .artifact import Artifact\nfrom .pagination import Pagination\n\n\nclass TaskRequestBody(BaseModel):\n    input: str = Field(\n        ...,\n        min_length=1,\n        description=\"Input prompt for the task.\",\n        example=\"Write the words you receive to the file 'output.txt'.\",\n    )\n    additional_input: dict[str, Any] = Field(default_factory=dict)\n\n\nclass Task(TaskRequestBody):\n    created_at: datetime = Field(\n        ...,\n        description=\"The creation datetime of the task.\",\n        example=\"2023-01-01T00:00:00Z\",\n        json_encoders={datetime: lambda v: v.isoformat()},\n    )\n    modified_at: datetime = Field(\n        ...,\n        description=\"The modification datetime of the task.\",\n        example=\"2023-01-01T00:00:00Z\",\n        json_encoders={datetime: lambda v: v.isoformat()},\n    )\n    task_id: str = Field(\n        ...,\n        description=\"The ID of the task.\",\n        example=\"50da533e-3904-4401-8a07-c49adf88b5eb\",\n    )\n    artifacts: list[Artifact] = Field(\n        default_factory=list,\n        description=\"A list of artifacts that the task has produced.\",\n        example=[\n            \"7a49f31c-f9c6-4346-a22c-e32bc5af4d8e\",\n            \"ab7b4091-2560-4692-a4fe-d831ea3ca7d6\",\n        ],\n    )\n\n\nclass StepRequestBody(BaseModel):\n    name: Optional[str] = Field(\n        default=None, description=\"The name of the task step.\", example=\"Write to file\"\n    )\n    input: str = Field(\n        ..., description=\"Input prompt for the step.\", example=\"Washington\"\n    )\n    additional_input: dict[str, Any] = Field(default_factory=dict)\n\n\nclass StepStatus(Enum):\n    created = \"created\"\n    running = \"running\"\n    completed = \"completed\"\n\n\nclass Step(StepRequestBody):\n    created_at: datetime = Field(\n        ...,\n        description=\"The creation datetime of the task.\",\n        example=\"2023-01-01T00:00:00Z\",\n        json_encoders={datetime: lambda v: v.isoformat()},\n    )\n    modified_at: datetime = Field(\n        ...,\n        description=\"The modification datetime of the task.\",\n        example=\"2023-01-01T00:00:00Z\",\n        json_encoders={datetime: lambda v: v.isoformat()},\n    )\n    task_id: str = Field(\n        ...,\n        description=\"The ID of the task this step belongs to.\",\n        example=\"50da533e-3904-4401-8a07-c49adf88b5eb\",\n    )\n    step_id: str = Field(\n        ...,\n        description=\"The ID of the task step.\",\n        example=\"6bb1801a-fd80-45e8-899a-4dd723cc602e\",\n    )\n    name: Optional[str] = Field(\n        default=None, description=\"The name of the task step.\", example=\"Write to file\"\n    )\n    status: StepStatus = Field(\n        ..., description=\"The status of the task step.\", example=\"created\"\n    )\n    output: Optional[str] = Field(\n        default=None,\n        description=\"Output of the task step.\",\n        example=(\n            \"I am going to use the write_to_file command and write Washington \"\n            \"to a file called output.txt <write_to_file('output.txt', 'Washington')\"\n        ),\n    )\n    additional_output: Optional[dict[str, Any]] = None\n    artifacts: list[Artifact] = Field(\n        default_factory=list,\n        description=\"A list of artifacts that the step has produced.\",\n    )\n    is_last: bool = Field(\n        ..., description=\"Whether this is the last step in the task.\", example=True\n    )\n\n\nclass TaskListResponse(BaseModel):\n    tasks: Optional[List[Task]] = None\n    pagination: Optional[Pagination] = None\n\n\nclass TaskStepsListResponse(BaseModel):\n    steps: Optional[List[Step]] = None\n    pagination: Optional[Pagination] = None\n\n\nclass TaskArtifactsListResponse(BaseModel):\n    artifacts: Optional[List[Artifact]] = None\n    pagination: Optional[Pagination] = None\n", "forge/forge/agent_protocol/models/pagination.py": "from pydantic import BaseModel, Field\n\n\nclass Pagination(BaseModel):\n    total_items: int = Field(..., description=\"Total number of items.\", example=42)\n    total_pages: int = Field(..., description=\"Total number of pages.\", example=97)\n    current_page: int = Field(..., description=\"Current_page page number.\", example=1)\n    page_size: int = Field(..., description=\"Number of items per page.\", example=25)\n", "forge/forge/agent_protocol/models/artifact.py": "from datetime import datetime\n\nfrom pydantic import BaseModel, Field\n\n\nclass Artifact(BaseModel):\n    created_at: datetime = Field(\n        ...,\n        description=\"The creation datetime of the task.\",\n        example=\"2023-01-01T00:00:00Z\",\n        json_encoders={datetime: lambda v: v.isoformat()},\n    )\n    modified_at: datetime = Field(\n        ...,\n        description=\"The modification datetime of the task.\",\n        example=\"2023-01-01T00:00:00Z\",\n        json_encoders={datetime: lambda v: v.isoformat()},\n    )\n    artifact_id: str = Field(\n        ...,\n        description=\"ID of the artifact.\",\n        example=\"b225e278-8b4c-4f99-a696-8facf19f0e56\",\n    )\n    agent_created: bool = Field(\n        ...,\n        description=\"Whether the artifact has been created by the agent.\",\n        example=False,\n    )\n    relative_path: str = Field(\n        ...,\n        description=\"Relative path of the artifact in the agents workspace.\",\n        example=\"/my_folder/my_other_folder/\",\n    )\n    file_name: str = Field(\n        ...,\n        description=\"Filename of the artifact.\",\n        example=\"main.py\",\n    )\n", "forge/forge/agent_protocol/models/__init__.py": "from .artifact import Artifact\nfrom .pagination import Pagination\nfrom .task import (\n    Step,\n    StepRequestBody,\n    StepStatus,\n    Task,\n    TaskArtifactsListResponse,\n    TaskListResponse,\n    TaskRequestBody,\n    TaskStepsListResponse,\n)\n\n__all__ = [\n    \"Artifact\",\n    \"Pagination\",\n    \"Step\",\n    \"StepRequestBody\",\n    \"StepStatus\",\n    \"Task\",\n    \"TaskArtifactsListResponse\",\n    \"TaskListResponse\",\n    \"TaskRequestBody\",\n    \"TaskStepsListResponse\",\n]\n", "forge/forge/agent_protocol/database/db.py": "\"\"\"\nThis is an example implementation of the Agent Protocol DB for development Purposes\nIt uses SQLite as the database and file store backend.\nIT IS NOT ADVISED TO USE THIS IN PRODUCTION!\n\"\"\"\n\nimport logging\nimport math\nimport uuid\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Literal, Optional, Tuple\n\nfrom sqlalchemy import JSON, Boolean, DateTime, ForeignKey, create_engine\nfrom sqlalchemy.exc import SQLAlchemyError\nfrom sqlalchemy.orm import (\n    DeclarativeBase,\n    Mapped,\n    joinedload,\n    mapped_column,\n    relationship,\n    sessionmaker,\n)\n\nfrom forge.utils.exceptions import NotFoundError\n\nfrom ..models.artifact import Artifact\nfrom ..models.pagination import Pagination\nfrom ..models.task import Step, StepRequestBody, StepStatus, Task\n\nlogger = logging.getLogger(__name__)\n\n\nclass Base(DeclarativeBase):\n    type_annotation_map = {\n        dict[str, Any]: JSON,\n    }\n\n\nclass TaskModel(Base):\n    __tablename__ = \"tasks\"\n\n    task_id: Mapped[str] = mapped_column(primary_key=True, index=True)\n    input: Mapped[str]\n    additional_input: Mapped[dict[str, Any]] = mapped_column(default=dict)\n    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)\n    modified_at: Mapped[datetime] = mapped_column(\n        DateTime, default=datetime.utcnow, onupdate=datetime.utcnow\n    )\n\n    artifacts = relationship(\"ArtifactModel\", back_populates=\"task\")\n\n\nclass StepModel(Base):\n    __tablename__ = \"steps\"\n\n    step_id: Mapped[str] = mapped_column(primary_key=True, index=True)\n    task_id: Mapped[str] = mapped_column(ForeignKey(\"tasks.task_id\"))\n    name: Mapped[str]\n    input: Mapped[str]\n    status: Mapped[str]\n    output: Mapped[Optional[str]]\n    is_last: Mapped[bool] = mapped_column(Boolean, default=False)\n    created_at: Mapped[datetime] = mapped_column(DateTime, default=datetime.utcnow)\n    modified_at: Mapped[datetime] = mapped_column(\n        DateTime, default=datetime.utcnow, onupdate=datetime.utcnow\n    )\n\n    additional_input: Mapped[dict[str, Any]] = mapped_column(default=dict)\n    additional_output: Mapped[Optional[dict[str, Any]]]\n    artifacts = relationship(\"ArtifactModel\", back_populates=\"step\")\n\n\nclass ArtifactModel(Base):\n    __tablename__ = \"artifacts\"\n\n    artifact_id: Mapped[str] = mapped_column(primary_key=True, index=True)\n    task_id: Mapped[str] = mapped_column(ForeignKey(\"tasks.task_id\"))\n    step_id: Mapped[Optional[str]] = mapped_column(ForeignKey(\"steps.step_id\"))\n    agent_created: Mapped[bool] = mapped_column(default=False)\n    file_name: Mapped[str]\n    relative_path: Mapped[str]\n    created_at: Mapped[datetime] = mapped_column(default=datetime.utcnow)\n    modified_at: Mapped[datetime] = mapped_column(\n        default=datetime.utcnow, onupdate=datetime.utcnow\n    )\n\n    step = relationship(\"StepModel\", back_populates=\"artifacts\")\n    task = relationship(\"TaskModel\", back_populates=\"artifacts\")\n\n\ndef convert_to_task(task_obj: TaskModel, debug_enabled: bool = False) -> Task:\n    if debug_enabled:\n        logger.debug(f\"Converting TaskModel to Task for task_id: {task_obj.task_id}\")\n    task_artifacts = [convert_to_artifact(artifact) for artifact in task_obj.artifacts]\n    return Task(\n        task_id=task_obj.task_id,\n        created_at=task_obj.created_at,\n        modified_at=task_obj.modified_at,\n        input=task_obj.input,\n        additional_input=task_obj.additional_input,\n        artifacts=task_artifacts,\n    )\n\n\ndef convert_to_step(step_model: StepModel, debug_enabled: bool = False) -> Step:\n    if debug_enabled:\n        logger.debug(f\"Converting StepModel to Step for step_id: {step_model.step_id}\")\n    step_artifacts = [\n        convert_to_artifact(artifact) for artifact in step_model.artifacts\n    ]\n    status = (\n        StepStatus.completed if step_model.status == \"completed\" else StepStatus.created\n    )\n    return Step(\n        task_id=step_model.task_id,\n        step_id=step_model.step_id,\n        created_at=step_model.created_at,\n        modified_at=step_model.modified_at,\n        name=step_model.name,\n        input=step_model.input,\n        status=status,\n        output=step_model.output,\n        artifacts=step_artifacts,\n        is_last=step_model.is_last == 1,\n        additional_input=step_model.additional_input,\n        additional_output=step_model.additional_output,\n    )\n\n\ndef convert_to_artifact(artifact_model: ArtifactModel) -> Artifact:\n    return Artifact(\n        artifact_id=artifact_model.artifact_id,\n        created_at=artifact_model.created_at,\n        modified_at=artifact_model.modified_at,\n        agent_created=artifact_model.agent_created,\n        relative_path=artifact_model.relative_path,\n        file_name=artifact_model.file_name,\n    )\n\n\n# sqlite:///{database_name}\nclass AgentDB:\n    def __init__(self, database_string, debug_enabled: bool = False) -> None:\n        super().__init__()\n        self.debug_enabled = debug_enabled\n        if self.debug_enabled:\n            logger.debug(\n                f\"Initializing AgentDB with database_string: {database_string}\"\n            )\n        self.engine = create_engine(database_string)\n        Base.metadata.create_all(self.engine)\n        self.Session = sessionmaker(bind=self.engine)\n\n    def close(self) -> None:\n        self.Session.close_all()\n        self.engine.dispose()\n\n    async def create_task(\n        self, input: Optional[str], additional_input: Optional[dict] = {}\n    ) -> Task:\n        if self.debug_enabled:\n            logger.debug(\"Creating new task\")\n\n        try:\n            with self.Session() as session:\n                new_task = TaskModel(\n                    task_id=str(uuid.uuid4()),\n                    input=input,\n                    additional_input=additional_input if additional_input else {},\n                )\n                session.add(new_task)\n                session.commit()\n                session.refresh(new_task)\n                if self.debug_enabled:\n                    logger.debug(f\"Created new task with task_id: {new_task.task_id}\")\n                return convert_to_task(new_task, self.debug_enabled)\n        except SQLAlchemyError as e:\n            logger.error(f\"SQLAlchemy error while creating task: {e}\")\n            raise\n        except Exception as e:\n            logger.error(f\"Unexpected error while creating task: {e}\")\n            raise\n\n    async def create_step(\n        self,\n        task_id: str,\n        input: StepRequestBody,\n        is_last: bool = False,\n        additional_input: Optional[Dict[str, Any]] = {},\n    ) -> Step:\n        if self.debug_enabled:\n            logger.debug(f\"Creating new step for task_id: {task_id}\")\n        try:\n            with self.Session() as session:\n                new_step = StepModel(\n                    task_id=task_id,\n                    step_id=str(uuid.uuid4()),\n                    name=input.input,\n                    input=input.input,\n                    status=\"created\",\n                    is_last=is_last,\n                    additional_input=additional_input,\n                )\n                session.add(new_step)\n                session.commit()\n                session.refresh(new_step)\n                if self.debug_enabled:\n                    logger.debug(f\"Created new step with step_id: {new_step.step_id}\")\n                return convert_to_step(new_step, self.debug_enabled)\n        except SQLAlchemyError as e:\n            logger.error(f\"SQLAlchemy error while creating step: {e}\")\n            raise\n        except Exception as e:\n            logger.error(f\"Unexpected error while creating step: {e}\")\n            raise\n\n    async def create_artifact(\n        self,\n        task_id: str,\n        file_name: str,\n        relative_path: str,\n        agent_created: bool = False,\n        step_id: str | None = None,\n    ) -> Artifact:\n        if self.debug_enabled:\n            logger.debug(f\"Creating new artifact for task_id: {task_id}\")\n        try:\n            with self.Session() as session:\n                if (\n                    existing_artifact := session.query(ArtifactModel)\n                    .filter_by(\n                        task_id=task_id,\n                        file_name=file_name,\n                        relative_path=relative_path,\n                    )\n                    .first()\n                ):\n                    session.close()\n                    if self.debug_enabled:\n                        logger.debug(\n                            f\"Artifact {file_name} already exists at {relative_path}/\"\n                        )\n                    return convert_to_artifact(existing_artifact)\n\n                new_artifact = ArtifactModel(\n                    artifact_id=str(uuid.uuid4()),\n                    task_id=task_id,\n                    step_id=step_id,\n                    agent_created=agent_created,\n                    file_name=file_name,\n                    relative_path=relative_path,\n                )\n                session.add(new_artifact)\n                session.commit()\n                session.refresh(new_artifact)\n                if self.debug_enabled:\n                    logger.debug(\n                        f\"Created new artifact with ID: {new_artifact.artifact_id}\"\n                    )\n                return convert_to_artifact(new_artifact)\n        except SQLAlchemyError as e:\n            logger.error(f\"SQLAlchemy error while creating step: {e}\")\n            raise\n        except Exception as e:\n            logger.error(f\"Unexpected error while creating step: {e}\")\n            raise\n\n    async def get_task(self, task_id: str) -> Task:\n        \"\"\"Get a task by its id\"\"\"\n        if self.debug_enabled:\n            logger.debug(f\"Getting task with task_id: {task_id}\")\n        try:\n            with self.Session() as session:\n                if task_obj := (\n                    session.query(TaskModel)\n                    .options(joinedload(TaskModel.artifacts))\n                    .filter_by(task_id=task_id)\n                    .first()\n                ):\n                    return convert_to_task(task_obj, self.debug_enabled)\n                else:\n                    logger.error(f\"Task not found with task_id: {task_id}\")\n                    raise NotFoundError(\"Task not found\")\n        except SQLAlchemyError as e:\n            logger.error(f\"SQLAlchemy error while getting task: {e}\")\n            raise\n        except Exception as e:\n            logger.error(f\"Unexpected error while getting task: {e}\")\n            raise\n\n    async def get_step(self, task_id: str, step_id: str) -> Step:\n        if self.debug_enabled:\n            logger.debug(f\"Getting step with task_id: {task_id} and step_id: {step_id}\")\n        try:\n            with self.Session() as session:\n                if step := (\n                    session.query(StepModel)\n                    .options(joinedload(StepModel.artifacts))\n                    .filter(StepModel.step_id == step_id)\n                    .first()\n                ):\n                    return convert_to_step(step, self.debug_enabled)\n\n                else:\n                    logger.error(\n                        f\"Step not found with task_id: {task_id} and step_id: {step_id}\"\n                    )\n                    raise NotFoundError(\"Step not found\")\n        except SQLAlchemyError as e:\n            logger.error(f\"SQLAlchemy error while getting step: {e}\")\n            raise\n        except Exception as e:\n            logger.error(f\"Unexpected error while getting step: {e}\")\n            raise\n\n    async def get_artifact(self, artifact_id: str) -> Artifact:\n        if self.debug_enabled:\n            logger.debug(f\"Getting artifact with and artifact_id: {artifact_id}\")\n        try:\n            with self.Session() as session:\n                if (\n                    artifact_model := session.query(ArtifactModel)\n                    .filter_by(artifact_id=artifact_id)\n                    .first()\n                ):\n                    return convert_to_artifact(artifact_model)\n                else:\n                    logger.error(\n                        f\"Artifact not found with and artifact_id: {artifact_id}\"\n                    )\n                    raise NotFoundError(\"Artifact not found\")\n        except SQLAlchemyError as e:\n            logger.error(f\"SQLAlchemy error while getting artifact: {e}\")\n            raise\n        except Exception as e:\n            logger.error(f\"Unexpected error while getting artifact: {e}\")\n            raise\n\n    async def update_step(\n        self,\n        task_id: str,\n        step_id: str,\n        status: Optional[str] = None,\n        output: Optional[str] = None,\n        additional_input: Optional[Dict[str, Any]] = None,\n        additional_output: Optional[Dict[str, Any]] = None,\n    ) -> Step:\n        if self.debug_enabled:\n            logger.debug(\n                f\"Updating step with task_id: {task_id} and step_id: {step_id}\"\n            )\n        try:\n            with self.Session() as session:\n                if (\n                    step := session.query(StepModel)\n                    .filter_by(task_id=task_id, step_id=step_id)\n                    .first()\n                ):\n                    if status is not None:\n                        step.status = status\n                    if additional_input is not None:\n                        step.additional_input = additional_input\n                    if output is not None:\n                        step.output = output\n                    if additional_output is not None:\n                        step.additional_output = additional_output\n                    session.commit()\n                    return await self.get_step(task_id, step_id)\n                else:\n                    logger.error(\n                        \"Can't update non-existent Step with \"\n                        f\"task_id: {task_id} and step_id: {step_id}\"\n                    )\n                    raise NotFoundError(\"Step not found\")\n        except SQLAlchemyError as e:\n            logger.error(f\"SQLAlchemy error while getting step: {e}\")\n            raise\n        except Exception as e:\n            logger.error(f\"Unexpected error while getting step: {e}\")\n            raise\n\n    async def update_artifact(\n        self,\n        artifact_id: str,\n        *,\n        file_name: str = \"\",\n        relative_path: str = \"\",\n        agent_created: Optional[Literal[True]] = None,\n    ) -> Artifact:\n        logger.debug(f\"Updating artifact with artifact_id: {artifact_id}\")\n        with self.Session() as session:\n            if (\n                artifact := session.query(ArtifactModel)\n                .filter_by(artifact_id=artifact_id)\n                .first()\n            ):\n                if file_name:\n                    artifact.file_name = file_name\n                if relative_path:\n                    artifact.relative_path = relative_path\n                if agent_created:\n                    artifact.agent_created = agent_created\n                session.commit()\n                return await self.get_artifact(artifact_id)\n            else:\n                logger.error(f\"Artifact not found with artifact_id: {artifact_id}\")\n                raise NotFoundError(\"Artifact not found\")\n\n    async def list_tasks(\n        self, page: int = 1, per_page: int = 10\n    ) -> Tuple[List[Task], Pagination]:\n        if self.debug_enabled:\n            logger.debug(\"Listing tasks\")\n        try:\n            with self.Session() as session:\n                tasks = (\n                    session.query(TaskModel)\n                    .offset((page - 1) * per_page)\n                    .limit(per_page)\n                    .all()\n                )\n                total = session.query(TaskModel).count()\n                pages = math.ceil(total / per_page)\n                pagination = Pagination(\n                    total_items=total,\n                    total_pages=pages,\n                    current_page=page,\n                    page_size=per_page,\n                )\n                return [\n                    convert_to_task(task, self.debug_enabled) for task in tasks\n                ], pagination\n        except SQLAlchemyError as e:\n            logger.error(f\"SQLAlchemy error while listing tasks: {e}\")\n            raise\n        except Exception as e:\n            logger.error(f\"Unexpected error while listing tasks: {e}\")\n            raise\n\n    async def list_steps(\n        self, task_id: str, page: int = 1, per_page: int = 10\n    ) -> Tuple[List[Step], Pagination]:\n        if self.debug_enabled:\n            logger.debug(f\"Listing steps for task_id: {task_id}\")\n        try:\n            with self.Session() as session:\n                steps = (\n                    session.query(StepModel)\n                    .filter_by(task_id=task_id)\n                    .offset((page - 1) * per_page)\n                    .limit(per_page)\n                    .all()\n                )\n                total = session.query(StepModel).filter_by(task_id=task_id).count()\n                pages = math.ceil(total / per_page)\n                pagination = Pagination(\n                    total_items=total,\n                    total_pages=pages,\n                    current_page=page,\n                    page_size=per_page,\n                )\n                return [\n                    convert_to_step(step, self.debug_enabled) for step in steps\n                ], pagination\n        except SQLAlchemyError as e:\n            logger.error(f\"SQLAlchemy error while listing steps: {e}\")\n            raise\n        except Exception as e:\n            logger.error(f\"Unexpected error while listing steps: {e}\")\n            raise\n\n    async def list_artifacts(\n        self, task_id: str, page: int = 1, per_page: int = 10\n    ) -> Tuple[List[Artifact], Pagination]:\n        if self.debug_enabled:\n            logger.debug(f\"Listing artifacts for task_id: {task_id}\")\n        try:\n            with self.Session() as session:\n                artifacts = (\n                    session.query(ArtifactModel)\n                    .filter_by(task_id=task_id)\n                    .offset((page - 1) * per_page)\n                    .limit(per_page)\n                    .all()\n                )\n                total = session.query(ArtifactModel).filter_by(task_id=task_id).count()\n                pages = math.ceil(total / per_page)\n                pagination = Pagination(\n                    total_items=total,\n                    total_pages=pages,\n                    current_page=page,\n                    page_size=per_page,\n                )\n                return [\n                    convert_to_artifact(artifact) for artifact in artifacts\n                ], pagination\n        except SQLAlchemyError as e:\n            logger.error(f\"SQLAlchemy error while listing artifacts: {e}\")\n            raise\n        except Exception as e:\n            logger.error(f\"Unexpected error while listing artifacts: {e}\")\n            raise\n", "forge/forge/agent_protocol/database/db_test.py": "import os\nimport sqlite3\nfrom datetime import datetime\n\nimport pytest\n\nfrom forge.agent_protocol.database.db import (\n    AgentDB,\n    ArtifactModel,\n    StepModel,\n    TaskModel,\n    convert_to_artifact,\n    convert_to_step,\n    convert_to_task,\n)\nfrom forge.agent_protocol.models import (\n    Artifact,\n    Step,\n    StepRequestBody,\n    StepStatus,\n    Task,\n)\nfrom forge.utils.exceptions import NotFoundError as DataNotFoundError\n\nTEST_DB_FILENAME = \"test_db.sqlite3\"\nTEST_DB_URL = f\"sqlite:///{TEST_DB_FILENAME}\"\n\n\n@pytest.fixture\ndef agent_db():\n    db = AgentDB(TEST_DB_URL)\n    yield db\n    db.close()\n    os.remove(TEST_DB_FILENAME)\n\n\n@pytest.fixture\ndef raw_db_connection(agent_db: AgentDB):\n    connection = sqlite3.connect(TEST_DB_FILENAME)\n    yield connection\n    connection.close()\n\n\ndef test_table_creation(raw_db_connection: sqlite3.Connection):\n    cursor = raw_db_connection.cursor()\n\n    # Test for tasks table existence\n    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name='tasks'\")\n    assert cursor.fetchone() is not None\n\n    # Test for steps table existence\n    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name='steps'\")\n    assert cursor.fetchone() is not None\n\n    # Test for artifacts table existence\n    cursor.execute(\n        \"SELECT name FROM sqlite_master WHERE type='table' AND name='artifacts'\"\n    )\n    assert cursor.fetchone() is not None\n\n\n@pytest.mark.asyncio\nasync def test_task_schema():\n    now = datetime.now()\n    task = Task(\n        task_id=\"50da533e-3904-4401-8a07-c49adf88b5eb\",\n        input=\"Write the words you receive to the file 'output.txt'.\",\n        created_at=now,\n        modified_at=now,\n        artifacts=[\n            Artifact(\n                artifact_id=\"b225e278-8b4c-4f99-a696-8facf19f0e56\",\n                agent_created=True,\n                file_name=\"main.py\",\n                relative_path=\"python/code/\",\n                created_at=now,\n                modified_at=now,\n            )\n        ],\n    )\n    assert task.task_id == \"50da533e-3904-4401-8a07-c49adf88b5eb\"\n    assert task.input == \"Write the words you receive to the file 'output.txt'.\"\n    assert len(task.artifacts) == 1\n    assert task.artifacts[0].artifact_id == \"b225e278-8b4c-4f99-a696-8facf19f0e56\"\n\n\n@pytest.mark.asyncio\nasync def test_step_schema():\n    now = datetime.now()\n    step = Step(\n        task_id=\"50da533e-3904-4401-8a07-c49adf88b5eb\",\n        step_id=\"6bb1801a-fd80-45e8-899a-4dd723cc602e\",\n        created_at=now,\n        modified_at=now,\n        name=\"Write to file\",\n        input=\"Write the words you receive to the file 'output.txt'.\",\n        status=StepStatus.created,\n        output=(\n            \"I am going to use the write_to_file command and write Washington \"\n            \"to a file called output.txt <write_to_file('output.txt', 'Washington')>\"\n        ),\n        artifacts=[\n            Artifact(\n                artifact_id=\"b225e278-8b4c-4f99-a696-8facf19f0e56\",\n                file_name=\"main.py\",\n                relative_path=\"python/code/\",\n                created_at=now,\n                modified_at=now,\n                agent_created=True,\n            )\n        ],\n        is_last=False,\n    )\n    assert step.task_id == \"50da533e-3904-4401-8a07-c49adf88b5eb\"\n    assert step.step_id == \"6bb1801a-fd80-45e8-899a-4dd723cc602e\"\n    assert step.name == \"Write to file\"\n    assert step.status == StepStatus.created\n    assert step.output == (\n        \"I am going to use the write_to_file command and write Washington \"\n        \"to a file called output.txt <write_to_file('output.txt', 'Washington')>\"\n    )\n    assert len(step.artifacts) == 1\n    assert step.artifacts[0].artifact_id == \"b225e278-8b4c-4f99-a696-8facf19f0e56\"\n    assert step.is_last is False\n\n\n@pytest.mark.asyncio\nasync def test_convert_to_task():\n    now = datetime.now()\n    task_model = TaskModel(\n        task_id=\"50da533e-3904-4401-8a07-c49adf88b5eb\",\n        created_at=now,\n        modified_at=now,\n        input=\"Write the words you receive to the file 'output.txt'.\",\n        additional_input={},\n        artifacts=[\n            ArtifactModel(\n                artifact_id=\"b225e278-8b4c-4f99-a696-8facf19f0e56\",\n                created_at=now,\n                modified_at=now,\n                relative_path=\"file:///path/to/main.py\",\n                agent_created=True,\n                file_name=\"main.py\",\n            )\n        ],\n    )\n    task = convert_to_task(task_model)\n    assert task.task_id == \"50da533e-3904-4401-8a07-c49adf88b5eb\"\n    assert task.input == \"Write the words you receive to the file 'output.txt'.\"\n    assert len(task.artifacts) == 1\n    assert task.artifacts[0].artifact_id == \"b225e278-8b4c-4f99-a696-8facf19f0e56\"\n\n\n@pytest.mark.asyncio\nasync def test_convert_to_step():\n    now = datetime.now()\n    step_model = StepModel(\n        task_id=\"50da533e-3904-4401-8a07-c49adf88b5eb\",\n        step_id=\"6bb1801a-fd80-45e8-899a-4dd723cc602e\",\n        created_at=now,\n        modified_at=now,\n        name=\"Write to file\",\n        status=\"created\",\n        input=\"Write the words you receive to the file 'output.txt'.\",\n        additional_input={},\n        artifacts=[\n            ArtifactModel(\n                artifact_id=\"b225e278-8b4c-4f99-a696-8facf19f0e56\",\n                created_at=now,\n                modified_at=now,\n                relative_path=\"file:///path/to/main.py\",\n                agent_created=True,\n                file_name=\"main.py\",\n            )\n        ],\n        is_last=False,\n    )\n    step = convert_to_step(step_model)\n    assert step.task_id == \"50da533e-3904-4401-8a07-c49adf88b5eb\"\n    assert step.step_id == \"6bb1801a-fd80-45e8-899a-4dd723cc602e\"\n    assert step.name == \"Write to file\"\n    assert step.status == StepStatus.created\n    assert len(step.artifacts) == 1\n    assert step.artifacts[0].artifact_id == \"b225e278-8b4c-4f99-a696-8facf19f0e56\"\n    assert step.is_last is False\n\n\n@pytest.mark.asyncio\nasync def test_convert_to_artifact():\n    now = datetime.now()\n    artifact_model = ArtifactModel(\n        artifact_id=\"b225e278-8b4c-4f99-a696-8facf19f0e56\",\n        created_at=now,\n        modified_at=now,\n        relative_path=\"file:///path/to/main.py\",\n        agent_created=True,\n        file_name=\"main.py\",\n    )\n    artifact = convert_to_artifact(artifact_model)\n    assert artifact.artifact_id == \"b225e278-8b4c-4f99-a696-8facf19f0e56\"\n    assert artifact.relative_path == \"file:///path/to/main.py\"\n    assert artifact.agent_created is True\n\n\n@pytest.mark.asyncio\nasync def test_create_task(agent_db: AgentDB):\n    task = await agent_db.create_task(\"task_input\")\n    assert task.input == \"task_input\"\n\n\n@pytest.mark.asyncio\nasync def test_create_and_get_task(agent_db: AgentDB):\n    task = await agent_db.create_task(\"test_input\")\n    fetched_task = await agent_db.get_task(task.task_id)\n    assert fetched_task.input == \"test_input\"\n\n\n@pytest.mark.asyncio\nasync def test_get_task_not_found(agent_db: AgentDB):\n    with pytest.raises(DataNotFoundError):\n        await agent_db.get_task(\"9999\")\n\n\n@pytest.mark.asyncio\nasync def test_create_and_get_step(agent_db: AgentDB):\n    task = await agent_db.create_task(\"task_input\")\n    step_input = {\"type\": \"python/code\"}\n    request = StepRequestBody(input=\"test_input debug\", additional_input=step_input)\n    step = await agent_db.create_step(task.task_id, request)\n    step = await agent_db.get_step(task.task_id, step.step_id)\n    assert step.input == \"test_input debug\"\n\n\n@pytest.mark.asyncio\nasync def test_updating_step(agent_db: AgentDB):\n    created_task = await agent_db.create_task(\"task_input\")\n    step_input = {\"type\": \"python/code\"}\n    request = StepRequestBody(input=\"test_input debug\", additional_input=step_input)\n    created_step = await agent_db.create_step(created_task.task_id, request)\n    await agent_db.update_step(created_task.task_id, created_step.step_id, \"completed\")\n\n    step = await agent_db.get_step(created_task.task_id, created_step.step_id)\n    assert step.status.value == \"completed\"\n\n\n@pytest.mark.asyncio\nasync def test_get_step_not_found(agent_db: AgentDB):\n    with pytest.raises(DataNotFoundError):\n        await agent_db.get_step(\"9999\", \"9999\")\n\n\n@pytest.mark.asyncio\nasync def test_get_artifact(agent_db: AgentDB):\n    # Given: A task and its corresponding artifact\n    task = await agent_db.create_task(\"test_input debug\")\n    step_input = {\"type\": \"python/code\"}\n    requst = StepRequestBody(input=\"test_input debug\", additional_input=step_input)\n\n    step = await agent_db.create_step(task.task_id, requst)\n\n    # Create an artifact\n    artifact = await agent_db.create_artifact(\n        task_id=task.task_id,\n        file_name=\"test_get_artifact_sample_file.txt\",\n        relative_path=\"file:///path/to/test_get_artifact_sample_file.txt\",\n        agent_created=True,\n        step_id=step.step_id,\n    )\n\n    # When: The artifact is fetched by its ID\n    fetched_artifact = await agent_db.get_artifact(artifact.artifact_id)\n\n    # Then: The fetched artifact matches the original\n    assert fetched_artifact.artifact_id == artifact.artifact_id\n    assert (\n        fetched_artifact.relative_path\n        == \"file:///path/to/test_get_artifact_sample_file.txt\"\n    )\n\n\n@pytest.mark.asyncio\nasync def test_list_tasks(agent_db: AgentDB):\n    # Given: Multiple tasks in the database\n    task1 = await agent_db.create_task(\"test_input_1\")\n    task2 = await agent_db.create_task(\"test_input_2\")\n\n    # When: All tasks are fetched\n    fetched_tasks, pagination = await agent_db.list_tasks()\n\n    # Then: The fetched tasks list includes the created tasks\n    task_ids = [task.task_id for task in fetched_tasks]\n    assert task1.task_id in task_ids\n    assert task2.task_id in task_ids\n\n\n@pytest.mark.asyncio\nasync def test_list_steps(agent_db: AgentDB):\n    step_input = {\"type\": \"python/code\"}\n    request = StepRequestBody(input=\"test_input debug\", additional_input=step_input)\n\n    # Given: A task and multiple steps for that task\n    task = await agent_db.create_task(\"test_input\")\n    step1 = await agent_db.create_step(task.task_id, request)\n    request = StepRequestBody(input=\"step two\")\n    step2 = await agent_db.create_step(task.task_id, request)\n\n    # When: All steps for the task are fetched\n    fetched_steps, pagination = await agent_db.list_steps(task.task_id)\n\n    # Then: The fetched steps list includes the created steps\n    step_ids = [step.step_id for step in fetched_steps]\n    assert step1.step_id in step_ids\n    assert step2.step_id in step_ids\n", "forge/forge/agent_protocol/database/__init__.py": "from .db import AgentDB\n\n__all__ = [\"AgentDB\"]\n", "forge/forge/speech/macos_tts.py": "\"\"\" MacOS TTS Voice. \"\"\"\nfrom __future__ import annotations\n\nimport subprocess\n\nfrom .base import VoiceBase\n\n\nclass MacOSTTS(VoiceBase):\n    \"\"\"MacOS TTS Voice.\"\"\"\n\n    def _setup(self) -> None:\n        pass\n\n    def _speech(self, text: str, voice_id: int = 0) -> bool:\n        \"\"\"Play the given text.\"\"\"\n        if voice_id == 0:\n            subprocess.run([\"say\", text], shell=False)\n        elif voice_id == 1:\n            subprocess.run([\"say\", \"-v\", \"Ava (Premium)\", text], shell=False)\n        else:\n            subprocess.run([\"say\", \"-v\", \"Samantha\", text], shell=False)\n        return True\n", "forge/forge/speech/say.py": "\"\"\" Text to speech module \"\"\"\nfrom __future__ import annotations\n\nimport os\nimport threading\nfrom threading import Semaphore\nfrom typing import Literal, Optional\n\nfrom forge.models.config import SystemConfiguration, UserConfigurable\n\nfrom .base import VoiceBase\nfrom .eleven_labs import ElevenLabsConfig, ElevenLabsSpeech\nfrom .gtts import GTTSVoice\nfrom .macos_tts import MacOSTTS\nfrom .stream_elements_speech import StreamElementsConfig, StreamElementsSpeech\n\n_QUEUE_SEMAPHORE = Semaphore(\n    1\n)  # The amount of sounds to queue before blocking the main thread\n\n\nclass TTSConfig(SystemConfiguration):\n    speak_mode: bool = False\n    elevenlabs: Optional[ElevenLabsConfig] = None\n    streamelements: Optional[StreamElementsConfig] = None\n    provider: Literal[\n        \"elevenlabs\", \"gtts\", \"macos\", \"streamelements\"\n    ] = UserConfigurable(\n        default=\"gtts\",\n        from_env=lambda: os.getenv(\"TEXT_TO_SPEECH_PROVIDER\")\n        or (\n            \"macos\"\n            if os.getenv(\"USE_MAC_OS_TTS\")\n            else \"elevenlabs\"\n            if os.getenv(\"ELEVENLABS_API_KEY\")\n            else \"streamelements\"\n            if os.getenv(\"USE_BRIAN_TTS\")\n            else \"gtts\"\n        ),\n    )  # type: ignore\n\n\nclass TextToSpeechProvider:\n    def __init__(self, config: TTSConfig):\n        self._config = config\n        self._default_voice_engine, self._voice_engine = self._get_voice_engine(config)\n\n    def say(self, text, voice_index: int = 0) -> None:\n        def _speak() -> None:\n            success = self._voice_engine.say(text, voice_index)\n            if not success:\n                self._default_voice_engine.say(text, voice_index)\n            _QUEUE_SEMAPHORE.release()\n\n        if self._config.speak_mode:\n            _QUEUE_SEMAPHORE.acquire(True)\n            thread = threading.Thread(target=_speak)\n            thread.start()\n\n    def __repr__(self):\n        return \"{class_name}(provider={voice_engine_name})\".format(\n            class_name=self.__class__.__name__,\n            voice_engine_name=self._voice_engine.__class__.__name__,\n        )\n\n    @staticmethod\n    def _get_voice_engine(config: TTSConfig) -> tuple[VoiceBase, VoiceBase]:\n        \"\"\"Get the voice engine to use for the given configuration\"\"\"\n        tts_provider = config.provider\n        if tts_provider == \"elevenlabs\":\n            voice_engine = ElevenLabsSpeech(config.elevenlabs)\n        elif tts_provider == \"macos\":\n            voice_engine = MacOSTTS()\n        elif tts_provider == \"streamelements\":\n            voice_engine = StreamElementsSpeech(config.streamelements)\n        else:\n            voice_engine = GTTSVoice()\n\n        return GTTSVoice(), voice_engine\n", "forge/forge/speech/base.py": "\"\"\"Base class for all voice classes.\"\"\"\nfrom __future__ import annotations\n\nimport abc\nimport re\nfrom threading import Lock\n\n\nclass VoiceBase:\n    \"\"\"\n    Base class for all voice classes.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Initialize the voice class.\n        \"\"\"\n        self._url = None\n        self._headers = None\n        self._api_key = None\n        self._voices = []\n        self._mutex = Lock()\n        self._setup(*args, **kwargs)\n\n    def say(self, text: str, voice_index: int = 0) -> bool:\n        \"\"\"\n        Say the given text.\n\n        Args:\n            text (str): The text to say.\n            voice_index (int): The index of the voice to use.\n        \"\"\"\n        text = re.sub(\n            r\"\\b(?:https?://[-\\w_.]+/?\\w[-\\w_.]*\\.(?:[-\\w_.]+/?\\w[-\\w_.]*\\.)?[a-z]+(?:/[-\\w_.%]+)*\\b(?!\\.))\",  # noqa: E501\n            \"\",\n            text,\n        )\n        with self._mutex:\n            return self._speech(text, voice_index)\n\n    @abc.abstractmethod\n    def _setup(self, *args, **kwargs) -> None:\n        \"\"\"\n        Setup the voices, API key, etc.\n        \"\"\"\n\n    @abc.abstractmethod\n    def _speech(self, text: str, voice_id: int = 0) -> bool:\n        \"\"\"\n        Play the given text.\n\n        Args:\n            text (str): The text to play.\n        \"\"\"\n", "forge/forge/speech/gtts.py": "\"\"\" GTTS Voice. \"\"\"\nfrom __future__ import annotations\n\nimport os\n\nimport gtts\nfrom playsound import playsound\n\nfrom .base import VoiceBase\n\n\nclass GTTSVoice(VoiceBase):\n    \"\"\"GTTS Voice.\"\"\"\n\n    def _setup(self) -> None:\n        pass\n\n    def _speech(self, text: str, voice_id: int = 0) -> bool:\n        \"\"\"Play the given text.\"\"\"\n        tts = gtts.gTTS(text)\n        tts.save(\"speech.mp3\")\n        playsound(\"speech.mp3\", True)\n        os.remove(\"speech.mp3\")\n        return True\n", "forge/forge/speech/stream_elements_speech.py": "from __future__ import annotations\n\nimport logging\nimport os\n\nimport requests\nfrom playsound import playsound\n\nfrom forge.models.config import SystemConfiguration, UserConfigurable\n\nfrom .base import VoiceBase\n\nlogger = logging.getLogger(__name__)\n\n\nclass StreamElementsConfig(SystemConfiguration):\n    voice: str = UserConfigurable(default=\"Brian\", from_env=\"STREAMELEMENTS_VOICE\")\n\n\nclass StreamElementsSpeech(VoiceBase):\n    \"\"\"Streamelements speech module for AutoGPT Forge\"\"\"\n\n    def _setup(self, config: StreamElementsConfig) -> None:\n        \"\"\"Setup the voices, API key, etc.\"\"\"\n        self.config = config\n\n    def _speech(self, text: str, voice_id: int = 0) -> bool:\n        \"\"\"Speak text using the streamelements API\n\n        Args:\n            text (str): The text to speak\n            voice (str): The voice to use\n\n        Returns:\n            bool: True if the request was successful, False otherwise\n        \"\"\"\n        voice = self.config.voice\n        tts_url = (\n            f\"https://api.streamelements.com/kappa/v2/speech?voice={voice}&text={text}\"\n        )\n        response = requests.get(tts_url)\n\n        if response.status_code == 200:\n            with open(\"speech.mp3\", \"wb\") as f:\n                f.write(response.content)\n            playsound(\"speech.mp3\")\n            os.remove(\"speech.mp3\")\n            return True\n        else:\n            logger.error(\n                \"Request failed with status code: %s, response content: %s\",\n                response.status_code,\n                response.content,\n            )\n            return False\n", "forge/forge/speech/__init__.py": "\"\"\"This module contains the (speech recognition and) speech synthesis functions.\"\"\"\nfrom .say import TextToSpeechProvider, TTSConfig\n\n__all__ = [\"TextToSpeechProvider\", \"TTSConfig\"]\n", "forge/forge/speech/eleven_labs.py": "\"\"\"ElevenLabs speech module\"\"\"\nfrom __future__ import annotations\n\nimport logging\nimport os\n\nimport requests\nfrom playsound import playsound\n\nfrom forge.models.config import SystemConfiguration, UserConfigurable\n\nfrom .base import VoiceBase\n\nlogger = logging.getLogger(__name__)\n\nPLACEHOLDERS = {\"your-voice-id\"}\n\n\nclass ElevenLabsConfig(SystemConfiguration):\n    api_key: str = UserConfigurable(from_env=\"ELEVENLABS_API_KEY\")\n    voice_id: str = UserConfigurable(from_env=\"ELEVENLABS_VOICE_ID\")\n\n\nclass ElevenLabsSpeech(VoiceBase):\n    \"\"\"ElevenLabs speech class\"\"\"\n\n    def _setup(self, config: ElevenLabsConfig) -> None:\n        \"\"\"Set up the voices, API key, etc.\n\n        Returns:\n            None: None\n        \"\"\"\n\n        default_voices = [\"ErXwobaYiN019PkySvjV\", \"EXAVITQu4vr4xnSDxMaL\"]\n        voice_options = {\n            \"Rachel\": \"21m00Tcm4TlvDq8ikWAM\",\n            \"Domi\": \"AZnzlk1XvdvUeBnXmlld\",\n            \"Bella\": \"EXAVITQu4vr4xnSDxMaL\",\n            \"Antoni\": \"ErXwobaYiN019PkySvjV\",\n            \"Elli\": \"MF3mGyEYCl7XYWbV9V6O\",\n            \"Josh\": \"TxGEqnHWrfWFTfGW9XjX\",\n            \"Arnold\": \"VR6AewLTigWG4xSOukaG\",\n            \"Adam\": \"pNInz6obpgDQGcFmaJgB\",\n            \"Sam\": \"yoZ06aMxZJJ28mfd3POQ\",\n        }\n        self._headers = {\n            \"Content-Type\": \"application/json\",\n            \"xi-api-key\": config.api_key,\n        }\n        self._voices = default_voices.copy()\n        if config.voice_id in voice_options:\n            config.voice_id = voice_options[config.voice_id]\n        self._use_custom_voice(config.voice_id, 0)\n\n    def _use_custom_voice(self, voice, voice_index) -> None:\n        \"\"\"Use a custom voice if provided and not a placeholder\n\n        Args:\n            voice (str): The voice ID\n            voice_index (int): The voice index\n\n        Returns:\n            None: None\n        \"\"\"\n        # Placeholder values that should be treated as empty\n        if voice and voice not in PLACEHOLDERS:\n            self._voices[voice_index] = voice\n\n    def _speech(self, text: str, voice_id: int = 0) -> bool:\n        \"\"\"Speak text using elevenlabs.io's API\n\n        Args:\n            text (str): The text to speak\n            voice_index (int, optional): The voice to use. Defaults to 0.\n\n        Returns:\n            bool: True if the request was successful, False otherwise\n        \"\"\"\n        tts_url = (\n            f\"https://api.elevenlabs.io/v1/text-to-speech/{self._voices[voice_id]}\"\n        )\n        response = requests.post(tts_url, headers=self._headers, json={\"text\": text})\n\n        if response.status_code == 200:\n            with open(\"speech.mpeg\", \"wb\") as f:\n                f.write(response.content)\n            playsound(\"speech.mpeg\", True)\n            os.remove(\"speech.mpeg\")\n            return True\n        else:\n            logger.warning(\"Request failed with status code:\", response.status_code)\n            logger.info(\"Response content:\", response.content)\n            return False\n", "forge/forge/agent/protocols.py": "from abc import abstractmethod\nfrom typing import TYPE_CHECKING, Awaitable, Generic, Iterator\n\nfrom forge.models.action import ActionResult, AnyProposal\n\nfrom .components import AgentComponent\n\nif TYPE_CHECKING:\n    from forge.command.command import Command\n    from forge.llm.providers import ChatMessage\n\n\nclass DirectiveProvider(AgentComponent):\n    def get_constraints(self) -> Iterator[str]:\n        return iter([])\n\n    def get_resources(self) -> Iterator[str]:\n        return iter([])\n\n    def get_best_practices(self) -> Iterator[str]:\n        return iter([])\n\n\nclass CommandProvider(AgentComponent):\n    @abstractmethod\n    def get_commands(self) -> Iterator[\"Command\"]:\n        ...\n\n\nclass MessageProvider(AgentComponent):\n    @abstractmethod\n    def get_messages(self) -> Iterator[\"ChatMessage\"]:\n        ...\n\n\nclass AfterParse(AgentComponent, Generic[AnyProposal]):\n    @abstractmethod\n    def after_parse(self, result: AnyProposal) -> None | Awaitable[None]:\n        ...\n\n\nclass ExecutionFailure(AgentComponent):\n    @abstractmethod\n    def execution_failure(self, error: Exception) -> None | Awaitable[None]:\n        ...\n\n\nclass AfterExecute(AgentComponent):\n    @abstractmethod\n    def after_execute(self, result: \"ActionResult\") -> None | Awaitable[None]:\n        ...\n", "forge/forge/agent/components.py": "from __future__ import annotations\n\nfrom abc import ABC\nfrom typing import Callable, ClassVar, Generic, Optional, TypeVar\n\nfrom pydantic import BaseModel\n\nfrom forge.models.config import _update_user_config_from_env, deep_update\n\nAC = TypeVar(\"AC\", bound=\"AgentComponent\")\nBM = TypeVar(\"BM\", bound=BaseModel)\n\n\nclass AgentComponent(ABC):\n    \"\"\"Base class for all agent components.\"\"\"\n\n    _run_after: list[type[AgentComponent]] = []\n    _enabled: Callable[[], bool] | bool = True\n    _disabled_reason: str = \"\"\n\n    @property\n    def enabled(self) -> bool:\n        if callable(self._enabled):\n            return self._enabled()\n        return self._enabled\n\n    @property\n    def disabled_reason(self) -> str:\n        \"\"\"Return the reason this component is disabled.\"\"\"\n        return self._disabled_reason\n\n    def run_after(self: AC, *components: type[AgentComponent] | AgentComponent) -> AC:\n        \"\"\"Set the components that this component should run after.\"\"\"\n        for component in components:\n            t = component if isinstance(component, type) else type(component)\n            if t not in self._run_after and t is not self.__class__:\n                self._run_after.append(t)\n        return self\n\n\nclass ConfigurableComponent(ABC, Generic[BM]):\n    \"\"\"A component that can be configured with a Pydantic model.\"\"\"\n\n    config_class: ClassVar[type[BM]]  # type: ignore\n\n    def __init__(self, configuration: Optional[BM]):\n        self._config: Optional[BM] = None\n        if configuration is not None:\n            self.config = configuration\n\n    def __init_subclass__(cls, **kwargs):\n        super().__init_subclass__(**kwargs)\n        if getattr(cls, \"config_class\", None) is None:\n            raise NotImplementedError(\n                f\"ConfigurableComponent subclass {cls.__name__} \"\n                \"must define config_class class attribute.\"\n            )\n\n    @property\n    def config(self) -> BM:\n        if not hasattr(self, \"_config\") or self._config is None:\n            self.config = self.config_class()\n        return self._config  # type: ignore\n\n    @config.setter\n    def config(self, config: BM):\n        if not hasattr(self, \"_config\") or self._config is None:\n            # Load configuration from environment variables\n            updated = _update_user_config_from_env(config)\n            config = self.config_class(**deep_update(config.dict(), updated))\n        self._config = config\n\n\nclass ComponentEndpointError(Exception):\n    \"\"\"Error of a single protocol method on a component.\"\"\"\n\n    def __init__(self, message: str, component: AgentComponent):\n        self.message = message\n        self.triggerer = component\n        super().__init__(message)\n\n\nclass EndpointPipelineError(ComponentEndpointError):\n    \"\"\"Error of an entire pipeline of one endpoint.\"\"\"\n\n\nclass ComponentSystemError(EndpointPipelineError):\n    \"\"\"Error of a group of pipelines;\n    multiple different endpoints.\"\"\"\n", "forge/forge/agent/base.py": "from __future__ import annotations\n\nimport copy\nimport inspect\nimport json\nimport logging\nfrom abc import ABCMeta, abstractmethod\nfrom typing import (\n    Any,\n    Awaitable,\n    Callable,\n    Generic,\n    Iterator,\n    Optional,\n    ParamSpec,\n    TypeVar,\n    cast,\n    overload,\n)\n\nfrom colorama import Fore\nfrom pydantic import BaseModel, Field, parse_raw_as, validator\n\nfrom forge.agent import protocols\nfrom forge.agent.components import (\n    AgentComponent,\n    ComponentEndpointError,\n    ConfigurableComponent,\n    EndpointPipelineError,\n)\nfrom forge.config.ai_directives import AIDirectives\nfrom forge.config.ai_profile import AIProfile\nfrom forge.llm.providers import CHAT_MODELS, ModelName, OpenAIModelName\nfrom forge.llm.providers.schema import ChatModelInfo\nfrom forge.models.action import ActionResult, AnyProposal\nfrom forge.models.config import SystemConfiguration, SystemSettings, UserConfigurable\n\nlogger = logging.getLogger(__name__)\n\nT = TypeVar(\"T\")\nP = ParamSpec(\"P\")\n\nDEFAULT_TRIGGERING_PROMPT = (\n    \"Determine exactly one command to use next based on the given goals \"\n    \"and the progress you have made so far, \"\n    \"and respond using the JSON schema specified previously:\"\n)\n\n\n# HACK: This is a workaround wrapper to de/serialize component configs until pydantic v2\nclass ModelContainer(BaseModel):\n    models: dict[str, BaseModel]\n\n\nclass BaseAgentConfiguration(SystemConfiguration):\n    allow_fs_access: bool = UserConfigurable(default=False)\n\n    fast_llm: ModelName = UserConfigurable(default=OpenAIModelName.GPT3_16k)\n    smart_llm: ModelName = UserConfigurable(default=OpenAIModelName.GPT4)\n    use_functions_api: bool = UserConfigurable(default=False)\n\n    default_cycle_instruction: str = DEFAULT_TRIGGERING_PROMPT\n    \"\"\"The default instruction passed to the AI for a thinking cycle.\"\"\"\n\n    big_brain: bool = UserConfigurable(default=True)\n    \"\"\"\n    Whether this agent uses the configured smart LLM (default) to think,\n    as opposed to the configured fast LLM. Enabling this disables hybrid mode.\n    \"\"\"\n\n    cycle_budget: Optional[int] = 1\n    \"\"\"\n    The number of cycles that the agent is allowed to run unsupervised.\n\n    `None` for unlimited continuous execution,\n    `1` to require user approval for every step,\n    `0` to stop the agent.\n    \"\"\"\n\n    cycles_remaining = cycle_budget\n    \"\"\"The number of cycles remaining within the `cycle_budget`.\"\"\"\n\n    cycle_count = 0\n    \"\"\"The number of cycles that the agent has run since its initialization.\"\"\"\n\n    send_token_limit: Optional[int] = None\n    \"\"\"\n    The token limit for prompt construction. Should leave room for the completion;\n    defaults to 75% of `llm.max_tokens`.\n    \"\"\"\n\n    @validator(\"use_functions_api\")\n    def validate_openai_functions(cls, v: bool, values: dict[str, Any]):\n        if v:\n            smart_llm = values[\"smart_llm\"]\n            fast_llm = values[\"fast_llm\"]\n            assert all(\n                [\n                    not any(s in name for s in {\"-0301\", \"-0314\"})\n                    for name in {smart_llm, fast_llm}\n                ]\n            ), (\n                f\"Model {smart_llm} does not support OpenAI Functions. \"\n                \"Please disable OPENAI_FUNCTIONS or choose a suitable model.\"\n            )\n        return v\n\n\nclass BaseAgentSettings(SystemSettings):\n    agent_id: str = \"\"\n\n    ai_profile: AIProfile = Field(default_factory=lambda: AIProfile(ai_name=\"AutoGPT\"))\n    \"\"\"The AI profile or \"personality\" of the agent.\"\"\"\n\n    directives: AIDirectives = Field(default_factory=AIDirectives)\n    \"\"\"Directives (general instructional guidelines) for the agent.\"\"\"\n\n    task: str = \"Terminate immediately\"  # FIXME: placeholder for forge.sdk.schema.Task\n    \"\"\"The user-given task that the agent is working on.\"\"\"\n\n    config: BaseAgentConfiguration = Field(default_factory=BaseAgentConfiguration)\n    \"\"\"The configuration for this BaseAgent subsystem instance.\"\"\"\n\n\nclass AgentMeta(ABCMeta):\n    def __call__(cls, *args, **kwargs):\n        # Create instance of the class (Agent or BaseAgent)\n        instance = super().__call__(*args, **kwargs)\n        # Automatically collect modules after the instance is created\n        instance._collect_components()\n        return instance\n\n\nclass BaseAgent(Generic[AnyProposal], metaclass=AgentMeta):\n    def __init__(\n        self,\n        settings: BaseAgentSettings,\n    ):\n        self.state = settings\n        self.components: list[AgentComponent] = []\n        self.config = settings.config\n        # Execution data for debugging\n        self._trace: list[str] = []\n\n        logger.debug(f\"Created {__class__} '{self.state.ai_profile.ai_name}'\")\n\n    @property\n    def trace(self) -> list[str]:\n        return self._trace\n\n    @property\n    def llm(self) -> ChatModelInfo:\n        \"\"\"The LLM that the agent uses to think.\"\"\"\n        llm_name = (\n            self.config.smart_llm if self.config.big_brain else self.config.fast_llm\n        )\n        return CHAT_MODELS[llm_name]\n\n    @property\n    def send_token_limit(self) -> int:\n        return self.config.send_token_limit or self.llm.max_tokens * 3 // 4\n\n    @abstractmethod\n    async def propose_action(self) -> AnyProposal:\n        ...\n\n    @abstractmethod\n    async def execute(\n        self,\n        proposal: AnyProposal,\n        user_feedback: str = \"\",\n    ) -> ActionResult:\n        ...\n\n    @abstractmethod\n    async def do_not_execute(\n        self,\n        denied_proposal: AnyProposal,\n        user_feedback: str,\n    ) -> ActionResult:\n        ...\n\n    def reset_trace(self):\n        self._trace = []\n\n    @overload\n    async def run_pipeline(\n        self, protocol_method: Callable[P, Iterator[T]], *args, retry_limit: int = 3\n    ) -> list[T]:\n        ...\n\n    @overload\n    async def run_pipeline(\n        self,\n        protocol_method: Callable[P, None | Awaitable[None]],\n        *args,\n        retry_limit: int = 3,\n    ) -> list[None]:\n        ...\n\n    async def run_pipeline(\n        self,\n        protocol_method: Callable[P, Iterator[T] | None | Awaitable[None]],\n        *args,\n        retry_limit: int = 3,\n    ) -> list[T] | list[None]:\n        method_name = protocol_method.__name__\n        protocol_name = protocol_method.__qualname__.split(\".\")[0]\n        protocol_class = getattr(protocols, protocol_name)\n        if not issubclass(protocol_class, AgentComponent):\n            raise TypeError(f\"{repr(protocol_method)} is not a protocol method\")\n\n        # Clone parameters to revert on failure\n        original_args = self._selective_copy(args)\n        pipeline_attempts = 0\n        method_result: list[T] = []\n        self._trace.append(f\"\u2b07\ufe0f  {Fore.BLUE}{method_name}{Fore.RESET}\")\n\n        while pipeline_attempts < retry_limit:\n            try:\n                for component in self.components:\n                    # Skip other protocols\n                    if not isinstance(component, protocol_class):\n                        continue\n\n                    # Skip disabled components\n                    if not component.enabled:\n                        self._trace.append(\n                            f\"   {Fore.LIGHTBLACK_EX}\"\n                            f\"{component.__class__.__name__}{Fore.RESET}\"\n                        )\n                        continue\n\n                    method = cast(\n                        Callable[..., Iterator[T] | None | Awaitable[None]] | None,\n                        getattr(component, method_name, None),\n                    )\n                    if not callable(method):\n                        continue\n\n                    component_attempts = 0\n                    while component_attempts < retry_limit:\n                        try:\n                            component_args = self._selective_copy(args)\n                            result = method(*component_args)\n                            if inspect.isawaitable(result):\n                                result = await result\n                            if result is not None:\n                                method_result.extend(result)\n                            args = component_args\n                            self._trace.append(f\"\u2705 {component.__class__.__name__}\")\n\n                        except ComponentEndpointError:\n                            self._trace.append(\n                                f\"\u274c {Fore.YELLOW}{component.__class__.__name__}: \"\n                                f\"ComponentEndpointError{Fore.RESET}\"\n                            )\n                            # Retry the same component on ComponentEndpointError\n                            component_attempts += 1\n                            continue\n                        # Successful component execution\n                        break\n                # Successful pipeline execution\n                break\n            except EndpointPipelineError as e:\n                self._trace.append(\n                    f\"\u274c {Fore.LIGHTRED_EX}{e.triggerer.__class__.__name__}: \"\n                    f\"EndpointPipelineError{Fore.RESET}\"\n                )\n                # Restart from the beginning on EndpointPipelineError\n                # Revert to original parameters\n                args = self._selective_copy(original_args)\n                pipeline_attempts += 1\n                continue  # Start the loop over\n            except Exception as e:\n                raise e\n        return method_result\n\n    def dump_component_configs(self) -> str:\n        configs = {}\n        for component in self.components:\n            if isinstance(component, ConfigurableComponent):\n                config_type_name = component.config.__class__.__name__\n                configs[config_type_name] = component.config\n        data = ModelContainer(models=configs).json()\n        raw = parse_raw_as(dict[str, dict[str, Any]], data)\n        return json.dumps(raw[\"models\"], indent=4)\n\n    def load_component_configs(self, serialized_configs: str):\n        configs_dict = parse_raw_as(dict[str, dict[str, Any]], serialized_configs)\n\n        for component in self.components:\n            if not isinstance(component, ConfigurableComponent):\n                continue\n            config_type = type(component.config)\n            config_type_name = config_type.__name__\n            if config_type_name in configs_dict:\n                # Parse the serialized data and update the existing config\n                updated_data = configs_dict[config_type_name]\n                data = {**component.config.dict(), **updated_data}\n                component.config = component.config.__class__(**data)\n\n    def _collect_components(self):\n        components = [\n            getattr(self, attr)\n            for attr in dir(self)\n            if isinstance(getattr(self, attr), AgentComponent)\n        ]\n\n        if self.components:\n            # Check if any component is missing (added to Agent but not to components)\n            for component in components:\n                if component not in self.components:\n                    logger.warning(\n                        f\"Component {component.__class__.__name__} \"\n                        \"is attached to an agent but not added to components list\"\n                    )\n            # Skip collecting and sorting and sort if ordering is explicit\n            return\n        self.components = self._topological_sort(components)\n\n    def _topological_sort(\n        self, components: list[AgentComponent]\n    ) -> list[AgentComponent]:\n        visited = set()\n        stack = []\n\n        def visit(node: AgentComponent):\n            if node in visited:\n                return\n            visited.add(node)\n            for neighbor_class in node._run_after:\n                neighbor = next(\n                    (m for m in components if isinstance(m, neighbor_class)), None\n                )\n                if neighbor and neighbor not in visited:\n                    visit(neighbor)\n            stack.append(node)\n\n        for component in components:\n            visit(component)\n\n        return stack\n\n    def _selective_copy(self, args: tuple[Any, ...]) -> tuple[Any, ...]:\n        copied_args = []\n        for item in args:\n            if isinstance(item, list):\n                # Shallow copy for lists\n                copied_item = item[:]\n            elif isinstance(item, dict):\n                # Shallow copy for dicts\n                copied_item = item.copy()\n            elif isinstance(item, BaseModel):\n                # Deep copy for Pydantic models (deep=True to also copy nested models)\n                copied_item = item.copy(deep=True)\n            else:\n                # Deep copy for other objects\n                copied_item = copy.deepcopy(item)\n            copied_args.append(copied_item)\n        return tuple(copied_args)\n", "forge/forge/agent/forge_agent.py": "import inspect\nimport logging\nfrom typing import Any, Optional\nfrom uuid import uuid4\n\nfrom forge.agent.base import BaseAgent, BaseAgentSettings\nfrom forge.agent.protocols import (\n    AfterExecute,\n    CommandProvider,\n    DirectiveProvider,\n    MessageProvider,\n)\nfrom forge.agent_protocol.agent import ProtocolAgent\nfrom forge.agent_protocol.database.db import AgentDB\nfrom forge.agent_protocol.models.task import (\n    Step,\n    StepRequestBody,\n    Task,\n    TaskRequestBody,\n)\nfrom forge.command.command import Command\nfrom forge.components.system.system import SystemComponent\nfrom forge.config.ai_profile import AIProfile\nfrom forge.file_storage.base import FileStorage\nfrom forge.llm.prompting.schema import ChatPrompt\nfrom forge.llm.prompting.utils import dump_prompt\nfrom forge.llm.providers.schema import AssistantFunctionCall\nfrom forge.llm.providers.utils import function_specs_from_commands\nfrom forge.models.action import (\n    ActionErrorResult,\n    ActionProposal,\n    ActionResult,\n    ActionSuccessResult,\n)\nfrom forge.utils.exceptions import AgentException, AgentTerminated\n\nlogger = logging.getLogger(__name__)\n\n\nclass ForgeAgent(ProtocolAgent, BaseAgent):\n    \"\"\"\n    The goal of the Forge is to take care of the boilerplate code,\n    so you can focus on agent design.\n\n    There is a great paper surveying the agent landscape: https://arxiv.org/abs/2308.11432\n    Which I would highly recommend reading as it will help you understand the possibilities.\n\n    ForgeAgent provides component support; https://docs.agpt.co/forge/components/introduction/\n    Using Components is a new way of building agents that is more flexible and easier to extend.\n    Components replace some agent's logic and plugins with a more modular and composable system.\n    \"\"\"  # noqa: E501\n\n    def __init__(self, database: AgentDB, workspace: FileStorage):\n        \"\"\"\n        The database is used to store tasks, steps and artifact metadata.\n        The workspace is used to store artifacts (files).\n        \"\"\"\n\n        # An example agent information; you can modify this to suit your needs\n        state = BaseAgentSettings(\n            name=\"Forge Agent\",\n            description=\"The Forge Agent is a generic agent that can solve tasks.\",\n            agent_id=str(uuid4()),\n            ai_profile=AIProfile(\n                ai_name=\"ForgeAgent\", ai_role=\"Generic Agent\", ai_goals=[\"Solve tasks\"]\n            ),\n            task=\"Solve tasks\",\n        )\n\n        # ProtocolAgent adds the Agent Protocol (API) functionality\n        ProtocolAgent.__init__(self, database, workspace)\n        # BaseAgent provides the component handling functionality\n        BaseAgent.__init__(self, state)\n\n        # AGENT COMPONENTS\n        # Components provide additional functionality to the agent\n        # There are NO components added by default in the BaseAgent\n        # You can create your own components or add existing ones\n        # Built-in components:\n        #   https://docs.agpt.co/forge/components/built-in-components/\n\n        # System component provides \"finish\" command and adds some prompt information\n        self.system = SystemComponent()\n\n    async def create_task(self, task_request: TaskRequestBody) -> Task:\n        \"\"\"\n        The agent protocol, which is the core of the Forge,\n        works by creating a task and then executing steps for that task.\n        This method is called when the agent is asked to create a task.\n\n        We are hooking into function to add a custom log message.\n        Though you can do anything you want here.\n        \"\"\"\n        task = await super().create_task(task_request)\n        logger.info(\n            f\"\ud83d\udce6 Task created with ID: {task.task_id} and \"\n            f\"input: {task.input[:40]}{'...' if len(task.input) > 40 else ''}\"\n        )\n        return task\n\n    async def execute_step(self, task_id: str, step_request: StepRequestBody) -> Step:\n        \"\"\"\n        Preffered method to add agent logic is to add custom components:\n        https://docs.agpt.co/forge/components/creating-components/\n\n        Outdated tutorial on how to add custom logic:\n        https://aiedge.medium.com/autogpt-forge-e3de53cc58ec\n\n        The agent protocol, which is the core of the Forge, works by creating a task and then\n        executing steps for that task. This method is called when the agent is asked to execute\n        a step.\n\n        The task that is created contains an input string, for the benchmarks this is the task\n        the agent has been asked to solve and additional input, which is a dictionary and\n        could contain anything.\n\n        If you want to get the task use:\n\n        ```\n        task = await self.db.get_task(task_id)\n        ```\n\n        The step request body is essentially the same as the task request and contains an input\n        string, for the benchmarks this is the task the agent has been asked to solve and\n        additional input, which is a dictionary and could contain anything.\n\n        You need to implement logic that will take in this step input and output the completed step\n        as a step object. You can do everything in a single step or you can break it down into\n        multiple steps. Returning a request to continue in the step output, the user can then decide\n        if they want the agent to continue or not.\n        \"\"\"  # noqa: E501\n\n        step = await self.db.create_step(\n            task_id=task_id, input=step_request, is_last=False\n        )\n\n        proposal = await self.propose_action()\n\n        output = await self.execute(proposal)\n\n        if isinstance(output, ActionSuccessResult):\n            step.output = str(output.outputs)\n        elif isinstance(output, ActionErrorResult):\n            step.output = output.reason\n\n        return step\n\n    async def propose_action(self) -> ActionProposal:\n        self.reset_trace()\n\n        # Get directives\n        directives = self.state.directives.copy(deep=True)\n        directives.resources += await self.run_pipeline(DirectiveProvider.get_resources)\n        directives.constraints += await self.run_pipeline(\n            DirectiveProvider.get_constraints\n        )\n        directives.best_practices += await self.run_pipeline(\n            DirectiveProvider.get_best_practices\n        )\n\n        # Get commands\n        self.commands = await self.run_pipeline(CommandProvider.get_commands)\n\n        # Get messages\n        messages = await self.run_pipeline(MessageProvider.get_messages)\n\n        prompt: ChatPrompt = ChatPrompt(\n            messages=messages, functions=function_specs_from_commands(self.commands)\n        )\n\n        logger.debug(f\"Executing prompt:\\n{dump_prompt(prompt)}\")\n\n        # Call the LLM and parse result\n        # THIS NEEDS TO BE REPLACED WITH YOUR LLM CALL/LOGIC\n        # Have a look at autogpt/agents/agent.py for an example (complete_and_parse)\n        proposal = ActionProposal(\n            thoughts=\"I cannot solve the task!\",\n            use_tool=AssistantFunctionCall(\n                name=\"finish\", arguments={\"reason\": \"Unimplemented logic\"}\n            ),\n        )\n\n        self.config.cycle_count += 1\n\n        return proposal\n\n    async def execute(self, proposal: Any, user_feedback: str = \"\") -> ActionResult:\n        tool = proposal.use_tool\n\n        # Get commands\n        self.commands = await self.run_pipeline(CommandProvider.get_commands)\n\n        # Execute the command\n        try:\n            command: Optional[Command] = None\n            for c in reversed(self.commands):\n                if tool.name in c.names:\n                    command = c\n\n            if command is None:\n                raise AgentException(f\"Command {tool.name} not found\")\n\n            command_result = command(**tool.arguments)\n            if inspect.isawaitable(command_result):\n                command_result = await command_result\n\n            result = ActionSuccessResult(outputs=command_result)\n        except AgentTerminated:\n            result = ActionSuccessResult(outputs=\"Agent terminated or finished\")\n        except AgentException as e:\n            result = ActionErrorResult.from_exception(e)\n            logger.warning(f\"{tool} raised an error: {e}\")\n\n        await self.run_pipeline(AfterExecute.after_execute, result)\n\n        logger.debug(\"\\n\".join(self.trace))\n\n        return result\n\n    async def do_not_execute(\n        self, denied_proposal: Any, user_feedback: str\n    ) -> ActionResult:\n        result = ActionErrorResult(reason=\"Action denied\")\n\n        await self.run_pipeline(AfterExecute.after_execute, result)\n\n        logger.debug(\"\\n\".join(self.trace))\n\n        return result\n", "forge/forge/agent/__init__.py": "from .base import BaseAgent, BaseAgentConfiguration, BaseAgentSettings\n\n__all__ = [\n    \"BaseAgent\",\n    \"BaseAgentConfiguration\",\n    \"BaseAgentSettings\",\n]\n", "forge/forge/content_processing/html.py": "\"\"\"HTML processing functions\"\"\"\nfrom __future__ import annotations\n\nfrom bs4 import BeautifulSoup\nfrom requests.compat import urljoin\n\n\ndef extract_hyperlinks(soup: BeautifulSoup, base_url: str) -> list[tuple[str, str]]:\n    \"\"\"Extract hyperlinks from a BeautifulSoup object\n\n    Args:\n        soup (BeautifulSoup): The BeautifulSoup object\n        base_url (str): The base URL\n\n    Returns:\n        List[Tuple[str, str]]: The extracted hyperlinks\n    \"\"\"\n    return [\n        (link.text, urljoin(base_url, link[\"href\"]))\n        for link in soup.find_all(\"a\", href=True)\n    ]\n\n\ndef format_hyperlinks(hyperlinks: list[tuple[str, str]]) -> list[str]:\n    \"\"\"Format hyperlinks to be displayed to the user\n\n    Args:\n        hyperlinks (List[Tuple[str, str]]): The hyperlinks to format\n\n    Returns:\n        List[str]: The formatted hyperlinks\n    \"\"\"\n    return [f\"{link_text.strip()} ({link_url})\" for link_text, link_url in hyperlinks]\n", "forge/forge/content_processing/__init__.py": "", "forge/forge/content_processing/text.py": "\"\"\"Text processing functions\"\"\"\nfrom __future__ import annotations\n\nimport logging\nimport math\nfrom typing import Iterator, Optional, TypeVar\n\nimport spacy\n\nfrom forge.json.parsing import extract_list_from_json\nfrom forge.llm.prompting import ChatPrompt\nfrom forge.llm.providers import ChatMessage, ModelTokenizer, MultiProvider\nfrom forge.llm.providers.multi import ModelName\n\nlogger = logging.getLogger(__name__)\n\nT = TypeVar(\"T\")\n\n\ndef batch(\n    sequence: list[T], max_batch_length: int, overlap: int = 0\n) -> Iterator[list[T]]:\n    \"\"\"\n    Batch data from iterable into slices of length N. The last batch may be shorter.\n\n    Example: `batched('ABCDEFGHIJ', 3)` --> `ABC DEF GHI J`\n    \"\"\"\n    if max_batch_length < 1:\n        raise ValueError(\"n must be at least one\")\n    for i in range(0, len(sequence), max_batch_length - overlap):\n        yield sequence[i : i + max_batch_length]\n\n\ndef chunk_content(\n    content: str,\n    max_chunk_length: int,\n    tokenizer: ModelTokenizer,\n    with_overlap: bool = True,\n) -> Iterator[tuple[str, int]]:\n    \"\"\"Split content into chunks of approximately equal token length.\"\"\"\n\n    MAX_OVERLAP = 200  # limit overlap to save tokens\n\n    tokenized_text = tokenizer.encode(content)\n    total_length = len(tokenized_text)\n    n_chunks = math.ceil(total_length / max_chunk_length)\n\n    chunk_length = math.ceil(total_length / n_chunks)\n    overlap = min(max_chunk_length - chunk_length, MAX_OVERLAP) if with_overlap else 0\n\n    for token_batch in batch(tokenized_text, chunk_length + overlap, overlap):\n        yield tokenizer.decode(token_batch), len(token_batch)\n\n\nasync def summarize_text(\n    text: str,\n    llm_provider: MultiProvider,\n    model_name: ModelName,\n    spacy_model: str = \"en_core_web_sm\",\n    question: Optional[str] = None,\n    instruction: Optional[str] = None,\n) -> tuple[str, list[tuple[str, str]]]:\n    if question:\n        if instruction:\n            raise ValueError(\n                \"Parameters 'question' and 'instructions' cannot both be set\"\n            )\n\n        instruction = (\n            f'From the text, answer the question: \"{question}\". '\n            \"If the answer is not in the text, indicate this clearly \"\n            \"and concisely state why the text is not suitable to answer the question.\"\n        )\n    elif not instruction:\n        instruction = (\n            \"Summarize or describe the text clearly and concisely, \"\n            \"whichever seems more appropriate.\"\n        )\n\n    return await _process_text(  # type: ignore\n        text=text,\n        instruction=instruction,\n        llm_provider=llm_provider,\n        model_name=model_name,\n        spacy_model=spacy_model,\n    )\n\n\nasync def extract_information(\n    source_text: str,\n    topics_of_interest: list[str],\n    llm_provider: MultiProvider,\n    model_name: ModelName,\n    spacy_model: str = \"en_core_web_sm\",\n) -> list[str]:\n    fmt_topics_list = \"\\n\".join(f\"* {topic}.\" for topic in topics_of_interest)\n    instruction = (\n        \"Extract relevant pieces of information about the following topics:\\n\"\n        f\"{fmt_topics_list}\\n\"\n        \"Reword pieces of information if needed to make them self-explanatory. \"\n        \"Be concise.\\n\\n\"\n        \"Respond with an `Array<string>` in JSON format AND NOTHING ELSE. \"\n        'If the text contains no relevant information, return \"[]\".'\n    )\n    return await _process_text(  # type: ignore\n        text=source_text,\n        instruction=instruction,\n        output_type=list[str],\n        llm_provider=llm_provider,\n        model_name=model_name,\n        spacy_model=spacy_model,\n    )\n\n\nasync def _process_text(\n    text: str,\n    instruction: str,\n    llm_provider: MultiProvider,\n    model_name: ModelName,\n    spacy_model: str = \"en_core_web_sm\",\n    output_type: type[str | list[str]] = str,\n) -> tuple[str, list[tuple[str, str]]] | list[str]:\n    \"\"\"Process text using the OpenAI API for summarization or information extraction\n\n    Params:\n        text (str): The text to process.\n        instruction (str): Additional instruction for processing.\n        llm_provider: LLM provider to use.\n        model_name: The name of the llm model to use.\n        spacy_model: The spaCy model to use for sentence splitting.\n        output_type: `str` for summaries or `list[str]` for piece-wise info extraction.\n\n    Returns:\n        For summarization: tuple[str, None | list[(summary, chunk)]]\n        For piece-wise information extraction: list[str]\n    \"\"\"\n    if not text.strip():\n        raise ValueError(\"No content\")\n\n    text_tlength = llm_provider.count_tokens(text, model_name)\n    logger.debug(f\"Text length: {text_tlength} tokens\")\n\n    max_result_tokens = 500\n    max_chunk_length = llm_provider.get_token_limit(model_name) - max_result_tokens - 50\n    logger.debug(f\"Max chunk length: {max_chunk_length} tokens\")\n\n    if text_tlength < max_chunk_length:\n        prompt = ChatPrompt(\n            messages=[\n                ChatMessage.system(\n                    \"The user is going to give you a text enclosed in triple quotes. \"\n                    f\"{instruction}\"\n                ),\n                ChatMessage.user(f'\"\"\"{text}\"\"\"'),\n            ]\n        )\n\n        logger.debug(f\"PROCESSING:\\n{prompt}\")\n\n        response = await llm_provider.create_chat_completion(\n            model_prompt=prompt.messages,\n            model_name=model_name,\n            temperature=0.5,\n            max_output_tokens=max_result_tokens,\n            completion_parser=lambda s: (\n                extract_list_from_json(s.content) if output_type is not str else None\n            ),\n        )\n\n        if isinstance(response.parsed_result, list):\n            logger.debug(f\"Raw LLM response: {repr(response.response.content)}\")\n            fmt_result_bullet_list = \"\\n\".join(f\"* {r}\" for r in response.parsed_result)\n            logger.debug(\n                f\"\\n{'-'*11} EXTRACTION RESULT {'-'*12}\\n\"\n                f\"{fmt_result_bullet_list}\\n\"\n                f\"{'-'*42}\\n\"\n            )\n            return response.parsed_result\n        else:\n            summary = response.response.content\n            logger.debug(f\"\\n{'-'*16} SUMMARY {'-'*17}\\n{summary}\\n{'-'*42}\\n\")\n            return summary.strip(), [(summary, text)]\n    else:\n        chunks = list(\n            split_text(\n                text,\n                max_chunk_length=max_chunk_length,\n                tokenizer=llm_provider.get_tokenizer(model_name),\n                spacy_model=spacy_model,\n            )\n        )\n\n        processed_results = []\n        for i, (chunk, _) in enumerate(chunks):\n            logger.info(f\"Processing chunk {i + 1} / {len(chunks)}\")\n            chunk_result = await _process_text(\n                text=chunk,\n                instruction=instruction,\n                output_type=output_type,\n                llm_provider=llm_provider,\n                model_name=model_name,\n                spacy_model=spacy_model,\n            )\n            processed_results.extend(\n                chunk_result if output_type == list[str] else [chunk_result]\n            )\n\n        if output_type == list[str]:\n            return processed_results\n        else:\n            summary, _ = await _process_text(\n                \"\\n\\n\".join([result[0] for result in processed_results]),\n                instruction=(\n                    \"The text consists of multiple partial summaries. \"\n                    \"Combine these partial summaries into one.\"\n                ),\n                llm_provider=llm_provider,\n                model_name=model_name,\n                spacy_model=spacy_model,\n            )\n            return summary.strip(), [\n                (processed_results[i], chunks[i][0]) for i in range(0, len(chunks))\n            ]\n\n\ndef split_text(\n    text: str,\n    max_chunk_length: int,\n    tokenizer: ModelTokenizer,\n    spacy_model: str = \"en_core_web_sm\",\n    with_overlap: bool = True,\n) -> Iterator[tuple[str, int]]:\n    \"\"\"\n    Split text into chunks of sentences, with each chunk not exceeding the max length.\n\n    Args:\n        text (str): The text to split.\n        spacy_model (str): The spaCy model to use for sentence splitting.\n        max_chunk_length (int, optional): The maximum length of a chunk.\n        tokenizer (ModelTokenizer): Tokenizer to use for determining chunk length.\n        with_overlap (bool, optional): Whether to allow overlap between chunks.\n\n    Yields:\n        str: The next chunk of text\n\n    Raises:\n        ValueError: when a sentence is longer than the maximum length\n    \"\"\"\n    text_length = len(tokenizer.encode(text))\n\n    if text_length < max_chunk_length:\n        yield text, text_length\n        return\n\n    n_chunks = math.ceil(text_length / max_chunk_length)\n    target_chunk_length = math.ceil(text_length / n_chunks)\n\n    nlp: spacy.language.Language = spacy.load(spacy_model)\n    nlp.add_pipe(\"sentencizer\")\n    doc = nlp(text)\n    sentences = [sentence.text.strip() for sentence in doc.sents]\n\n    current_chunk: list[str] = []\n    current_chunk_length = 0\n    last_sentence = None\n    last_sentence_length = 0\n\n    i = 0\n    while i < len(sentences):\n        sentence = sentences[i]\n        sentence_length = len(tokenizer.encode(sentence))\n        expected_chunk_length = current_chunk_length + 1 + sentence_length\n\n        if (\n            expected_chunk_length < max_chunk_length\n            # try to create chunks of approximately equal size\n            and expected_chunk_length - (sentence_length / 2) < target_chunk_length\n        ):\n            current_chunk.append(sentence)\n            current_chunk_length = expected_chunk_length\n\n        elif sentence_length < max_chunk_length:\n            if last_sentence:\n                yield \" \".join(current_chunk), current_chunk_length\n                current_chunk = []\n                current_chunk_length = 0\n\n                if with_overlap:\n                    overlap_max_length = max_chunk_length - sentence_length - 1\n                    if last_sentence_length < overlap_max_length:\n                        current_chunk += [last_sentence]\n                        current_chunk_length += last_sentence_length + 1\n                    elif overlap_max_length > 5:\n                        # add as much from the end of the last sentence as fits\n                        current_chunk += [\n                            list(\n                                chunk_content(\n                                    content=last_sentence,\n                                    max_chunk_length=overlap_max_length,\n                                    tokenizer=tokenizer,\n                                )\n                            ).pop()[0],\n                        ]\n                        current_chunk_length += overlap_max_length + 1\n\n            current_chunk += [sentence]\n            current_chunk_length += sentence_length\n\n        else:  # sentence longer than maximum length -> chop up and try again\n            sentences[i : i + 1] = [\n                chunk\n                for chunk, _ in chunk_content(sentence, target_chunk_length, tokenizer)\n            ]\n            continue\n\n        i += 1\n        last_sentence = sentence\n        last_sentence_length = sentence_length\n\n    if current_chunk:\n        yield \" \".join(current_chunk), current_chunk_length\n", "forge/forge/config/ai_profile.py": "from pydantic import BaseModel, Field\n\nDEFAULT_AI_NAME = \"AutoGPT\"\nDEFAULT_AI_ROLE = (\n    \"a seasoned digital assistant: \"\n    \"capable, intelligent, considerate and assertive. \"\n    \"You have extensive research and development skills, and you don't shy \"\n    \"away from writing some code to solve a problem. \"\n    \"You are pragmatic and make the most out of the tools available to you.\"\n)\n\n\nclass AIProfile(BaseModel):\n    \"\"\"\n    Object to hold the AI's personality.\n\n    Attributes:\n        ai_name (str): The name of the AI.\n        ai_role (str): The description of the AI's role.\n        ai_goals (list): The list of objectives the AI is supposed to complete.\n        api_budget (float): The maximum dollar value for API calls (0.0 means infinite)\n    \"\"\"\n\n    ai_name: str = DEFAULT_AI_NAME\n    ai_role: str = DEFAULT_AI_ROLE\n    \"\"\"`ai_role` should fit in the following format: `You are {ai_name}, {ai_role}`\"\"\"\n    ai_goals: list[str] = Field(default_factory=list[str])\n", "forge/forge/config/base.py": "from forge.file_storage import FileStorageBackendName\nfrom forge.models.config import SystemSettings, UserConfigurable\nfrom forge.speech.say import TTSConfig\n\n\nclass BaseConfig(SystemSettings):\n    name: str = \"Base configuration\"\n    description: str = \"Default configuration for forge agent.\"\n\n    # TTS configuration\n    tts_config: TTSConfig = TTSConfig()\n\n    # File storage\n    file_storage_backend: FileStorageBackendName = UserConfigurable(\n        default=FileStorageBackendName.LOCAL, from_env=\"FILE_STORAGE_BACKEND\"\n    )\n", "forge/forge/config/__init__.py": "\"\"\"\nThis module contains configuration models and helpers for AutoGPT Forge.\n\"\"\"\nfrom .ai_directives import AIDirectives\nfrom .ai_profile import AIProfile\nfrom .base import BaseConfig\n\n__all__ = [\n    \"AIProfile\",\n    \"AIDirectives\",\n    \"BaseConfig\",\n]\n", "forge/forge/config/ai_directives.py": "from __future__ import annotations\n\nimport logging\n\nfrom pydantic import BaseModel, Field\n\nlogger = logging.getLogger(__name__)\n\n\nclass AIDirectives(BaseModel):\n    \"\"\"An object that contains the basic directives for the AI prompt.\n\n    Attributes:\n        constraints (list): A list of constraints that the AI should adhere to.\n        resources (list): A list of resources that the AI can utilize.\n        best_practices (list): A list of best practices that the AI should follow.\n    \"\"\"\n\n    resources: list[str] = Field(default_factory=list)\n    constraints: list[str] = Field(default_factory=list)\n    best_practices: list[str] = Field(default_factory=list)\n\n    def __add__(self, other: AIDirectives) -> AIDirectives:\n        return AIDirectives(\n            resources=self.resources + other.resources,\n            constraints=self.constraints + other.constraints,\n            best_practices=self.best_practices + other.best_practices,\n        ).copy(deep=True)\n", "forge/forge/llm/__init__.py": "", "forge/forge/llm/prompting/schema.py": "import enum\n\nfrom pydantic import BaseModel, Field\n\nfrom forge.llm.providers.schema import (\n    ChatMessage,\n    ChatMessageDict,\n    CompletionModelFunction,\n)\n\n\nclass LanguageModelClassification(str, enum.Enum):\n    \"\"\"The LanguageModelClassification is a functional description of the model.\n\n    This is used to determine what kind of model to use for a given prompt.\n    Sometimes we prefer a faster or cheaper model to accomplish a task when\n    possible.\n    \"\"\"\n\n    FAST_MODEL = \"fast_model\"\n    SMART_MODEL = \"smart_model\"\n\n\nclass ChatPrompt(BaseModel):\n    messages: list[ChatMessage]\n    functions: list[CompletionModelFunction] = Field(default_factory=list)\n    prefill_response: str = \"\"\n\n    def raw(self) -> list[ChatMessageDict]:\n        return [m.dict() for m in self.messages]  # type: ignore\n\n    def __str__(self):\n        return \"\\n\\n\".join(\n            f\"{m.role.value.upper()}: {m.content}\" for m in self.messages\n        )\n", "forge/forge/llm/prompting/utils.py": "from math import ceil, floor\nfrom typing import Any\n\nfrom forge.llm.prompting.schema import ChatPrompt\n\nSEPARATOR_LENGTH = 42\n\n\ndef dump_prompt(prompt: ChatPrompt) -> str:\n    def separator(text: str):\n        half_sep_len = (SEPARATOR_LENGTH - 2 - len(text)) / 2\n        return f\"{floor(half_sep_len)*'-'} {text.upper()} {ceil(half_sep_len)*'-'}\"\n\n    formatted_messages = \"\\n\".join(\n        [f\"{separator(m.role)}\\n{m.content}\" for m in prompt.messages]\n    )\n    return f\"\"\"\n============== {prompt.__class__.__name__} ==============\nLength: {len(prompt.messages)} messages\n{formatted_messages}\n==========================================\n\"\"\"\n\n\ndef format_numbered_list(items: list[Any], start_at: int = 1) -> str:\n    return \"\\n\".join(f\"{i}. {str(item)}\" for i, item in enumerate(items, start_at))\n\n\ndef indent(content: str, indentation: int | str = 4) -> str:\n    if type(indentation) is int:\n        indentation = \" \" * indentation\n    return indentation + content.replace(\"\\n\", f\"\\n{indentation}\")  # type: ignore\n\n\ndef to_numbered_list(\n    items: list[str], no_items_response: str = \"\", **template_args\n) -> str:\n    if items:\n        return \"\\n\".join(\n            f\"{i+1}. {item.format(**template_args)}\" for i, item in enumerate(items)\n        )\n    else:\n        return no_items_response\n", "forge/forge/llm/prompting/base.py": "import abc\nfrom typing import TYPE_CHECKING, Any\n\nif TYPE_CHECKING:\n    from forge.llm.providers import AssistantChatMessage\n\nfrom .schema import ChatPrompt, LanguageModelClassification\n\n\nclass PromptStrategy(abc.ABC):\n    @property\n    @abc.abstractmethod\n    def model_classification(self) -> LanguageModelClassification:\n        ...\n\n    @abc.abstractmethod\n    def build_prompt(self, *_, **kwargs) -> ChatPrompt:\n        ...\n\n    @abc.abstractmethod\n    def parse_response_content(self, response: \"AssistantChatMessage\") -> Any:\n        ...\n", "forge/forge/llm/prompting/__init__.py": "from .base import PromptStrategy\nfrom .schema import ChatPrompt, LanguageModelClassification\n\n__all__ = [\n    \"LanguageModelClassification\",\n    \"ChatPrompt\",\n    \"PromptStrategy\",\n]\n", "forge/forge/llm/providers/openai.py": "import enum\nimport logging\nimport os\nfrom pathlib import Path\nfrom typing import Any, Callable, Iterator, Mapping, Optional, ParamSpec, TypeVar, cast\n\nimport tenacity\nimport tiktoken\nimport yaml\nfrom openai._exceptions import APIStatusError, RateLimitError\nfrom openai.types import EmbeddingCreateParams\nfrom openai.types.chat import (\n    ChatCompletionMessage,\n    ChatCompletionMessageParam,\n    CompletionCreateParams,\n)\nfrom pydantic import SecretStr\n\nfrom forge.json.parsing import json_loads\nfrom forge.models.config import UserConfigurable\nfrom forge.models.json_schema import JSONSchema\n\nfrom ._openai_base import BaseOpenAIChatProvider, BaseOpenAIEmbeddingProvider\nfrom .schema import (\n    AssistantToolCall,\n    AssistantToolCallDict,\n    ChatMessage,\n    ChatModelInfo,\n    CompletionModelFunction,\n    Embedding,\n    EmbeddingModelInfo,\n    ModelProviderBudget,\n    ModelProviderConfiguration,\n    ModelProviderCredentials,\n    ModelProviderName,\n    ModelProviderSettings,\n    ModelTokenizer,\n)\n\n_T = TypeVar(\"_T\")\n_P = ParamSpec(\"_P\")\n\nOpenAIEmbeddingParser = Callable[[Embedding], Embedding]\n\n\nclass OpenAIModelName(str, enum.Enum):\n    EMBEDDING_v2 = \"text-embedding-ada-002\"\n    EMBEDDING_v3_S = \"text-embedding-3-small\"\n    EMBEDDING_v3_L = \"text-embedding-3-large\"\n\n    GPT3_v1 = \"gpt-3.5-turbo-0301\"\n    GPT3_v2 = \"gpt-3.5-turbo-0613\"\n    GPT3_v2_16k = \"gpt-3.5-turbo-16k-0613\"\n    GPT3_v3 = \"gpt-3.5-turbo-1106\"\n    GPT3_v4 = \"gpt-3.5-turbo-0125\"\n    GPT3_ROLLING = \"gpt-3.5-turbo\"\n    GPT3_ROLLING_16k = \"gpt-3.5-turbo-16k\"\n    GPT3 = GPT3_ROLLING\n    GPT3_16k = GPT3_ROLLING_16k\n\n    GPT4_v1 = \"gpt-4-0314\"\n    GPT4_v1_32k = \"gpt-4-32k-0314\"\n    GPT4_v2 = \"gpt-4-0613\"\n    GPT4_v2_32k = \"gpt-4-32k-0613\"\n    GPT4_v3 = \"gpt-4-1106-preview\"\n    GPT4_v3_VISION = \"gpt-4-1106-vision-preview\"\n    GPT4_v4 = \"gpt-4-0125-preview\"\n    GPT4_v5 = \"gpt-4-turbo-2024-04-09\"\n    GPT4_ROLLING = \"gpt-4\"\n    GPT4_ROLLING_32k = \"gpt-4-32k\"\n    GPT4_TURBO = \"gpt-4-turbo\"\n    GPT4_TURBO_PREVIEW = \"gpt-4-turbo-preview\"\n    GPT4_VISION = \"gpt-4-vision-preview\"\n    GPT4_O_v1 = \"gpt-4o-2024-05-13\"\n    GPT4_O_ROLLING = \"gpt-4o\"\n    GPT4 = GPT4_ROLLING\n    GPT4_32k = GPT4_ROLLING_32k\n    GPT4_O = GPT4_O_ROLLING\n\n\nOPEN_AI_EMBEDDING_MODELS = {\n    info.name: info\n    for info in [\n        EmbeddingModelInfo(\n            name=OpenAIModelName.EMBEDDING_v2,\n            provider_name=ModelProviderName.OPENAI,\n            prompt_token_cost=0.0001 / 1000,\n            max_tokens=8191,\n            embedding_dimensions=1536,\n        ),\n        EmbeddingModelInfo(\n            name=OpenAIModelName.EMBEDDING_v3_S,\n            provider_name=ModelProviderName.OPENAI,\n            prompt_token_cost=0.00002 / 1000,\n            max_tokens=8191,\n            embedding_dimensions=1536,\n        ),\n        EmbeddingModelInfo(\n            name=OpenAIModelName.EMBEDDING_v3_L,\n            provider_name=ModelProviderName.OPENAI,\n            prompt_token_cost=0.00013 / 1000,\n            max_tokens=8191,\n            embedding_dimensions=3072,\n        ),\n    ]\n}\n\n\nOPEN_AI_CHAT_MODELS = {\n    info.name: info\n    for info in [\n        ChatModelInfo(\n            name=OpenAIModelName.GPT3_v1,\n            provider_name=ModelProviderName.OPENAI,\n            prompt_token_cost=0.0015 / 1000,\n            completion_token_cost=0.002 / 1000,\n            max_tokens=4096,\n            has_function_call_api=True,\n        ),\n        ChatModelInfo(\n            name=OpenAIModelName.GPT3_v2_16k,\n            provider_name=ModelProviderName.OPENAI,\n            prompt_token_cost=0.003 / 1000,\n            completion_token_cost=0.004 / 1000,\n            max_tokens=16384,\n            has_function_call_api=True,\n        ),\n        ChatModelInfo(\n            name=OpenAIModelName.GPT3_v3,\n            provider_name=ModelProviderName.OPENAI,\n            prompt_token_cost=0.001 / 1000,\n            completion_token_cost=0.002 / 1000,\n            max_tokens=16384,\n            has_function_call_api=True,\n        ),\n        ChatModelInfo(\n            name=OpenAIModelName.GPT3_v4,\n            provider_name=ModelProviderName.OPENAI,\n            prompt_token_cost=0.0005 / 1000,\n            completion_token_cost=0.0015 / 1000,\n            max_tokens=16384,\n            has_function_call_api=True,\n        ),\n        ChatModelInfo(\n            name=OpenAIModelName.GPT4_v1,\n            provider_name=ModelProviderName.OPENAI,\n            prompt_token_cost=0.03 / 1000,\n            completion_token_cost=0.06 / 1000,\n            max_tokens=8191,\n            has_function_call_api=True,\n        ),\n        ChatModelInfo(\n            name=OpenAIModelName.GPT4_v1_32k,\n            provider_name=ModelProviderName.OPENAI,\n            prompt_token_cost=0.06 / 1000,\n            completion_token_cost=0.12 / 1000,\n            max_tokens=32768,\n            has_function_call_api=True,\n        ),\n        ChatModelInfo(\n            name=OpenAIModelName.GPT4_TURBO,\n            provider_name=ModelProviderName.OPENAI,\n            prompt_token_cost=0.01 / 1000,\n            completion_token_cost=0.03 / 1000,\n            max_tokens=128000,\n            has_function_call_api=True,\n        ),\n        ChatModelInfo(\n            name=OpenAIModelName.GPT4_O,\n            provider_name=ModelProviderName.OPENAI,\n            prompt_token_cost=5 / 1_000_000,\n            completion_token_cost=15 / 1_000_000,\n            max_tokens=128_000,\n            has_function_call_api=True,\n        ),\n    ]\n}\n# Copy entries for models with equivalent specs\nchat_model_mapping = {\n    OpenAIModelName.GPT3_v1: [OpenAIModelName.GPT3_v2],\n    OpenAIModelName.GPT3_v2_16k: [OpenAIModelName.GPT3_16k],\n    OpenAIModelName.GPT3_v4: [OpenAIModelName.GPT3_ROLLING],\n    OpenAIModelName.GPT4_v1: [OpenAIModelName.GPT4_v2, OpenAIModelName.GPT4_ROLLING],\n    OpenAIModelName.GPT4_v1_32k: [\n        OpenAIModelName.GPT4_v2_32k,\n        OpenAIModelName.GPT4_32k,\n    ],\n    OpenAIModelName.GPT4_TURBO: [\n        OpenAIModelName.GPT4_v3,\n        OpenAIModelName.GPT4_v3_VISION,\n        OpenAIModelName.GPT4_VISION,\n        OpenAIModelName.GPT4_v4,\n        OpenAIModelName.GPT4_TURBO_PREVIEW,\n        OpenAIModelName.GPT4_v5,\n    ],\n    OpenAIModelName.GPT4_O: [OpenAIModelName.GPT4_O_v1],\n}\nfor base, copies in chat_model_mapping.items():\n    for copy in copies:\n        copy_info = OPEN_AI_CHAT_MODELS[base].copy(update={\"name\": copy})\n        OPEN_AI_CHAT_MODELS[copy] = copy_info\n        if copy.endswith((\"-0301\", \"-0314\")):\n            copy_info.has_function_call_api = False\n\n\nOPEN_AI_MODELS: Mapping[\n    OpenAIModelName,\n    ChatModelInfo[OpenAIModelName] | EmbeddingModelInfo[OpenAIModelName],\n] = {\n    **OPEN_AI_CHAT_MODELS,\n    **OPEN_AI_EMBEDDING_MODELS,\n}\n\n\nclass OpenAICredentials(ModelProviderCredentials):\n    \"\"\"Credentials for OpenAI.\"\"\"\n\n    api_key: SecretStr = UserConfigurable(from_env=\"OPENAI_API_KEY\")  # type: ignore\n    api_base: Optional[SecretStr] = UserConfigurable(\n        default=None, from_env=\"OPENAI_API_BASE_URL\"\n    )\n    organization: Optional[SecretStr] = UserConfigurable(from_env=\"OPENAI_ORGANIZATION\")\n\n    api_type: Optional[SecretStr] = UserConfigurable(\n        default=None,\n        from_env=lambda: cast(\n            SecretStr | None,\n            \"azure\"\n            if os.getenv(\"USE_AZURE\") == \"True\"\n            else os.getenv(\"OPENAI_API_TYPE\"),\n        ),\n    )\n    api_version: Optional[SecretStr] = UserConfigurable(\n        default=None, from_env=\"OPENAI_API_VERSION\"\n    )\n    azure_endpoint: Optional[SecretStr] = None\n    azure_model_to_deploy_id_map: Optional[dict[str, str]] = None\n\n    def get_api_access_kwargs(self) -> dict[str, str]:\n        kwargs = {\n            k: v.get_secret_value()\n            for k, v in {\n                \"api_key\": self.api_key,\n                \"base_url\": self.api_base,\n                \"organization\": self.organization,\n                \"api_version\": self.api_version,\n            }.items()\n            if v is not None\n        }\n        if self.api_type == SecretStr(\"azure\"):\n            assert self.azure_endpoint, \"Azure endpoint not configured\"\n            kwargs[\"azure_endpoint\"] = self.azure_endpoint.get_secret_value()\n        return kwargs\n\n    def get_model_access_kwargs(self, model: str) -> dict[str, str]:\n        kwargs = {\"model\": model}\n        if self.api_type == SecretStr(\"azure\") and model:\n            azure_kwargs = self._get_azure_access_kwargs(model)\n            kwargs.update(azure_kwargs)\n        return kwargs\n\n    def load_azure_config(self, config_file: Path) -> None:\n        with open(config_file) as file:\n            config_params = yaml.load(file, Loader=yaml.SafeLoader) or {}\n\n        try:\n            assert config_params.get(\n                \"azure_model_map\", {}\n            ), \"Azure model->deployment_id map is empty\"\n        except AssertionError as e:\n            raise ValueError(*e.args)\n\n        self.api_type = config_params.get(\"azure_api_type\", \"azure\")\n        self.api_version = config_params.get(\"azure_api_version\", None)\n        self.azure_endpoint = config_params.get(\"azure_endpoint\")\n        self.azure_model_to_deploy_id_map = config_params.get(\"azure_model_map\")\n\n    def _get_azure_access_kwargs(self, model: str) -> dict[str, str]:\n        \"\"\"Get the kwargs for the Azure API.\"\"\"\n\n        if not self.azure_model_to_deploy_id_map:\n            raise ValueError(\"Azure model deployment map not configured\")\n\n        if model not in self.azure_model_to_deploy_id_map:\n            raise ValueError(f\"No Azure deployment ID configured for model '{model}'\")\n        deployment_id = self.azure_model_to_deploy_id_map[model]\n\n        return {\"model\": deployment_id}\n\n\nclass OpenAISettings(ModelProviderSettings):\n    credentials: Optional[OpenAICredentials]  # type: ignore\n    budget: ModelProviderBudget  # type: ignore\n\n\nclass OpenAIProvider(\n    BaseOpenAIChatProvider[OpenAIModelName, OpenAISettings],\n    BaseOpenAIEmbeddingProvider[OpenAIModelName, OpenAISettings],\n):\n    MODELS = OPEN_AI_MODELS\n    CHAT_MODELS = OPEN_AI_CHAT_MODELS\n    EMBEDDING_MODELS = OPEN_AI_EMBEDDING_MODELS\n\n    default_settings = OpenAISettings(\n        name=\"openai_provider\",\n        description=\"Provides access to OpenAI's API.\",\n        configuration=ModelProviderConfiguration(),\n        credentials=None,\n        budget=ModelProviderBudget(),\n    )\n\n    _settings: OpenAISettings\n    _configuration: ModelProviderConfiguration\n    _credentials: OpenAICredentials\n    _budget: ModelProviderBudget\n\n    def __init__(\n        self,\n        settings: Optional[OpenAISettings] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        super(OpenAIProvider, self).__init__(settings=settings, logger=logger)\n\n        if self._credentials.api_type == SecretStr(\"azure\"):\n            from openai import AsyncAzureOpenAI\n\n            # API key and org (if configured) are passed, the rest of the required\n            # credentials is loaded from the environment by the AzureOpenAI client.\n            self._client = AsyncAzureOpenAI(\n                **self._credentials.get_api_access_kwargs()  # type: ignore\n            )\n        else:\n            from openai import AsyncOpenAI\n\n            self._client = AsyncOpenAI(\n                **self._credentials.get_api_access_kwargs()  # type: ignore\n            )\n\n    def get_tokenizer(self, model_name: OpenAIModelName) -> ModelTokenizer[int]:\n        return tiktoken.encoding_for_model(model_name)\n\n    def count_message_tokens(\n        self,\n        messages: ChatMessage | list[ChatMessage],\n        model_name: OpenAIModelName,\n    ) -> int:\n        if isinstance(messages, ChatMessage):\n            messages = [messages]\n\n        if model_name.startswith(\"gpt-3.5-turbo\"):\n            tokens_per_message = (\n                4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n            )\n            tokens_per_name = -1  # if there's a name, the role is omitted\n        # TODO: check if this is still valid for gpt-4o\n        elif model_name.startswith(\"gpt-4\"):\n            tokens_per_message = 3\n            tokens_per_name = 1\n        else:\n            raise NotImplementedError(\n                f\"count_message_tokens() is not implemented for model {model_name}.\\n\"\n                \"See https://github.com/openai/openai-python/blob/120d225b91a8453e15240a49fb1c6794d8119326/chatml.md \"  # noqa\n                \"for information on how messages are converted to tokens.\"\n            )\n        tokenizer = self.get_tokenizer(model_name)\n\n        num_tokens = 0\n        for message in messages:\n            num_tokens += tokens_per_message\n            for key, value in message.dict().items():\n                num_tokens += len(tokenizer.encode(value))\n                if key == \"name\":\n                    num_tokens += tokens_per_name\n        num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n        return num_tokens\n\n    def _get_chat_completion_args(\n        self,\n        prompt_messages: list[ChatMessage],\n        model: OpenAIModelName,\n        functions: Optional[list[CompletionModelFunction]] = None,\n        max_output_tokens: Optional[int] = None,\n        **kwargs,\n    ) -> tuple[\n        list[ChatCompletionMessageParam], CompletionCreateParams, dict[str, Any]\n    ]:\n        \"\"\"Prepare keyword arguments for an OpenAI chat completion call\n\n        Args:\n            prompt_messages: List of ChatMessages\n            model: The model to use\n            functions (optional): List of functions available to the LLM\n            max_output_tokens (optional): Maximum number of tokens to generate\n\n        Returns:\n            list[ChatCompletionMessageParam]: Prompt messages for the OpenAI call\n            CompletionCreateParams: Mapping of other kwargs for the OpenAI call\n            Mapping[str, Any]: Any keyword arguments to pass on to the completion parser\n        \"\"\"\n        tools_compat_mode = False\n        if functions:\n            if not OPEN_AI_CHAT_MODELS[model].has_function_call_api:\n                # Provide compatibility with older models\n                _functions_compat_fix_kwargs(functions, prompt_messages)\n                tools_compat_mode = True\n                functions = None\n\n        openai_messages, kwargs, parse_kwargs = super()._get_chat_completion_args(\n            prompt_messages=prompt_messages,\n            model=model,\n            functions=functions,\n            max_output_tokens=max_output_tokens,\n            **kwargs,\n        )\n        kwargs.update(self._credentials.get_model_access_kwargs(model))  # type: ignore\n\n        if tools_compat_mode:\n            parse_kwargs[\"compat_mode\"] = True\n\n        return openai_messages, kwargs, parse_kwargs\n\n    def _parse_assistant_tool_calls(\n        self,\n        assistant_message: ChatCompletionMessage,\n        compat_mode: bool = False,\n        **kwargs,\n    ) -> tuple[list[AssistantToolCall], list[Exception]]:\n        tool_calls: list[AssistantToolCall] = []\n        parse_errors: list[Exception] = []\n\n        if not compat_mode:\n            return super()._parse_assistant_tool_calls(\n                assistant_message=assistant_message, compat_mode=compat_mode, **kwargs\n            )\n        elif assistant_message.content:\n            try:\n                tool_calls = list(\n                    _tool_calls_compat_extract_calls(assistant_message.content)\n                )\n            except Exception as e:\n                parse_errors.append(e)\n\n        return tool_calls, parse_errors\n\n    def _get_embedding_kwargs(\n        self, input: str | list[str], model: OpenAIModelName, **kwargs\n    ) -> EmbeddingCreateParams:\n        kwargs = super()._get_embedding_kwargs(input=input, model=model, **kwargs)\n        kwargs.update(self._credentials.get_model_access_kwargs(model))  # type: ignore\n        return kwargs\n\n    _get_embedding_kwargs.__doc__ = (\n        BaseOpenAIEmbeddingProvider._get_embedding_kwargs.__doc__\n    )\n\n    def _retry_api_request(self, func: Callable[_P, _T]) -> Callable[_P, _T]:\n        _log_retry_debug_message = tenacity.after_log(self._logger, logging.DEBUG)\n\n        def _log_on_fail(retry_state: tenacity.RetryCallState) -> None:\n            _log_retry_debug_message(retry_state)\n\n            if (\n                retry_state.attempt_number == 0\n                and retry_state.outcome\n                and isinstance(retry_state.outcome.exception(), RateLimitError)\n            ):\n                self._logger.warning(\n                    \"Please double check that you have setup a PAID OpenAI API Account.\"\n                    \" You can read more here: \"\n                    \"https://docs.agpt.co/setup/#getting-an-openai-api-key\"\n                )\n\n        return tenacity.retry(\n            retry=(\n                tenacity.retry_if_exception_type(RateLimitError)\n                | tenacity.retry_if_exception(\n                    lambda e: isinstance(e, APIStatusError) and e.status_code == 502\n                )\n            ),\n            wait=tenacity.wait_exponential(),\n            stop=tenacity.stop_after_attempt(self._configuration.retries_per_request),\n            after=_log_on_fail,\n        )(func)\n\n    def __repr__(self):\n        return \"OpenAIProvider()\"\n\n\ndef format_function_specs_as_typescript_ns(\n    functions: list[CompletionModelFunction],\n) -> str:\n    \"\"\"Returns a function signature block in the format used by OpenAI internally:\n    https://community.openai.com/t/how-to-calculate-the-tokens-when-using-function-call/266573/18\n\n    For use with `count_tokens` to determine token usage of provided functions.\n\n    Example:\n    ```ts\n    namespace functions {\n\n    // Get the current weather in a given location\n    type get_current_weather = (_: {\n    // The city and state, e.g. San Francisco, CA\n    location: string,\n    unit?: \"celsius\" | \"fahrenheit\",\n    }) => any;\n\n    } // namespace functions\n    ```\n    \"\"\"\n\n    return (\n        \"namespace functions {\\n\\n\"\n        + \"\\n\\n\".join(format_openai_function_for_prompt(f) for f in functions)\n        + \"\\n\\n} // namespace functions\"\n    )\n\n\ndef format_openai_function_for_prompt(func: CompletionModelFunction) -> str:\n    \"\"\"Returns the function formatted similarly to the way OpenAI does it internally:\n    https://community.openai.com/t/how-to-calculate-the-tokens-when-using-function-call/266573/18\n\n    Example:\n    ```ts\n    // Get the current weather in a given location\n    type get_current_weather = (_: {\n    // The city and state, e.g. San Francisco, CA\n    location: string,\n    unit?: \"celsius\" | \"fahrenheit\",\n    }) => any;\n    ```\n    \"\"\"\n\n    def param_signature(name: str, spec: JSONSchema) -> str:\n        return (\n            f\"// {spec.description}\\n\" if spec.description else \"\"\n        ) + f\"{name}{'' if spec.required else '?'}: {spec.typescript_type},\"\n\n    return \"\\n\".join(\n        [\n            f\"// {func.description}\",\n            f\"type {func.name} = (_ :{{\",\n            *[param_signature(name, p) for name, p in func.parameters.items()],\n            \"}) => any;\",\n        ]\n    )\n\n\ndef count_openai_functions_tokens(\n    functions: list[CompletionModelFunction], count_tokens: Callable[[str], int]\n) -> int:\n    \"\"\"Returns the number of tokens taken up by a set of function definitions\n\n    Reference: https://community.openai.com/t/how-to-calculate-the-tokens-when-using-function-call/266573/18  # noqa: E501\n    \"\"\"\n    return count_tokens(\n        \"# Tools\\n\\n\"\n        \"## functions\\n\\n\"\n        f\"{format_function_specs_as_typescript_ns(functions)}\"\n    )\n\n\ndef _functions_compat_fix_kwargs(\n    functions: list[CompletionModelFunction],\n    prompt_messages: list[ChatMessage],\n):\n    function_definitions = format_function_specs_as_typescript_ns(functions)\n    function_call_schema = JSONSchema(\n        type=JSONSchema.Type.OBJECT,\n        properties={\n            \"name\": JSONSchema(\n                description=\"The name of the function to call\",\n                enum=[f.name for f in functions],\n                required=True,\n            ),\n            \"arguments\": JSONSchema(\n                description=\"The arguments for the function call\",\n                type=JSONSchema.Type.OBJECT,\n                required=True,\n            ),\n        },\n    )\n    tool_calls_schema = JSONSchema(\n        type=JSONSchema.Type.ARRAY,\n        items=JSONSchema(\n            type=JSONSchema.Type.OBJECT,\n            properties={\n                \"type\": JSONSchema(\n                    type=JSONSchema.Type.STRING,\n                    enum=[\"function\"],\n                ),\n                \"function\": function_call_schema,\n            },\n        ),\n    )\n    prompt_messages.append(\n        ChatMessage.system(\n            \"# tool usage instructions\\n\\n\"\n            \"Specify a '```tool_calls' block in your response,\"\n            \" with a valid JSON object that adheres to the following schema:\\n\\n\"\n            f\"{tool_calls_schema.to_dict()}\\n\\n\"\n            \"Specify any tools that you need to use through this JSON object.\\n\\n\"\n            \"Put the tool_calls block at the end of your response\"\n            \" and include its fences if it is not the only content.\\n\\n\"\n            \"## functions\\n\\n\"\n            \"For the function call itself, use one of the following\"\n            f\" functions:\\n\\n{function_definitions}\"\n        ),\n    )\n\n\ndef _tool_calls_compat_extract_calls(response: str) -> Iterator[AssistantToolCall]:\n    import re\n    import uuid\n\n    logging.debug(f\"Trying to extract tool calls from response:\\n{response}\")\n\n    if response[0] == \"[\":\n        tool_calls: list[AssistantToolCallDict] = json_loads(response)\n    else:\n        block = re.search(r\"```(?:tool_calls)?\\n(.*)\\n```\\s*$\", response, re.DOTALL)\n        if not block:\n            raise ValueError(\"Could not find tool_calls block in response\")\n        tool_calls: list[AssistantToolCallDict] = json_loads(block.group(1))\n\n    for t in tool_calls:\n        t[\"id\"] = str(uuid.uuid4())\n        yield AssistantToolCall.parse_obj(t)\n", "forge/forge/llm/providers/multi.py": "from __future__ import annotations\n\nimport logging\nfrom typing import Any, Callable, Iterator, Optional, Sequence, TypeVar\n\nfrom pydantic import ValidationError\n\nfrom .anthropic import ANTHROPIC_CHAT_MODELS, AnthropicModelName, AnthropicProvider\nfrom .groq import GROQ_CHAT_MODELS, GroqModelName, GroqProvider\nfrom .openai import OPEN_AI_CHAT_MODELS, OpenAIModelName, OpenAIProvider\nfrom .schema import (\n    AssistantChatMessage,\n    BaseChatModelProvider,\n    ChatMessage,\n    ChatModelInfo,\n    ChatModelResponse,\n    CompletionModelFunction,\n    ModelProviderBudget,\n    ModelProviderConfiguration,\n    ModelProviderName,\n    ModelProviderSettings,\n    ModelTokenizer,\n)\n\n_T = TypeVar(\"_T\")\n\nModelName = AnthropicModelName | GroqModelName | OpenAIModelName\nEmbeddingModelProvider = OpenAIProvider\n\nCHAT_MODELS = {**ANTHROPIC_CHAT_MODELS, **GROQ_CHAT_MODELS, **OPEN_AI_CHAT_MODELS}\n\n\nclass MultiProvider(BaseChatModelProvider[ModelName, ModelProviderSettings]):\n    default_settings = ModelProviderSettings(\n        name=\"multi_provider\",\n        description=(\n            \"Provides access to all of the available models, regardless of provider.\"\n        ),\n        configuration=ModelProviderConfiguration(\n            retries_per_request=7,\n        ),\n        budget=ModelProviderBudget(),\n    )\n\n    _budget: ModelProviderBudget\n\n    _provider_instances: dict[ModelProviderName, ChatModelProvider]\n\n    def __init__(\n        self,\n        settings: Optional[ModelProviderSettings] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        super(MultiProvider, self).__init__(settings=settings, logger=logger)\n        self._budget = self._settings.budget or ModelProviderBudget()\n\n        self._provider_instances = {}\n\n    async def get_available_models(self) -> Sequence[ChatModelInfo[ModelName]]:\n        # TODO: support embeddings\n        return await self.get_available_chat_models()\n\n    async def get_available_chat_models(self) -> Sequence[ChatModelInfo[ModelName]]:\n        models = []\n        for provider in self.get_available_providers():\n            models.extend(await provider.get_available_chat_models())\n        return models\n\n    def get_token_limit(self, model_name: ModelName) -> int:\n        \"\"\"Get the token limit for a given model.\"\"\"\n        return self.get_model_provider(model_name).get_token_limit(\n            model_name  # type: ignore\n        )\n\n    def get_tokenizer(self, model_name: ModelName) -> ModelTokenizer[Any]:\n        return self.get_model_provider(model_name).get_tokenizer(\n            model_name  # type: ignore\n        )\n\n    def count_tokens(self, text: str, model_name: ModelName) -> int:\n        return self.get_model_provider(model_name).count_tokens(\n            text=text, model_name=model_name  # type: ignore\n        )\n\n    def count_message_tokens(\n        self, messages: ChatMessage | list[ChatMessage], model_name: ModelName\n    ) -> int:\n        return self.get_model_provider(model_name).count_message_tokens(\n            messages=messages, model_name=model_name  # type: ignore\n        )\n\n    async def create_chat_completion(\n        self,\n        model_prompt: list[ChatMessage],\n        model_name: ModelName,\n        completion_parser: Callable[[AssistantChatMessage], _T] = lambda _: None,\n        functions: Optional[list[CompletionModelFunction]] = None,\n        max_output_tokens: Optional[int] = None,\n        prefill_response: str = \"\",\n        **kwargs,\n    ) -> ChatModelResponse[_T]:\n        \"\"\"Create a completion using the Anthropic API.\"\"\"\n        return await self.get_model_provider(model_name).create_chat_completion(\n            model_prompt=model_prompt,\n            model_name=model_name,  # type: ignore\n            completion_parser=completion_parser,\n            functions=functions,\n            max_output_tokens=max_output_tokens,\n            prefill_response=prefill_response,\n            **kwargs,\n        )\n\n    def get_model_provider(self, model: ModelName) -> ChatModelProvider:\n        model_info = CHAT_MODELS[model]\n        return self._get_provider(model_info.provider_name)\n\n    def get_available_providers(self) -> Iterator[ChatModelProvider]:\n        for provider_name in ModelProviderName:\n            try:\n                yield self._get_provider(provider_name)\n            except Exception:\n                pass\n\n    def _get_provider(self, provider_name: ModelProviderName) -> ChatModelProvider:\n        _provider = self._provider_instances.get(provider_name)\n        if not _provider:\n            Provider = self._get_provider_class(provider_name)\n            settings = Provider.default_settings.copy(deep=True)\n            settings.budget = self._budget\n            settings.configuration.extra_request_headers.update(\n                self._settings.configuration.extra_request_headers\n            )\n            if settings.credentials is None:\n                try:\n                    Credentials = settings.__fields__[\"credentials\"].type_\n                    settings.credentials = Credentials.from_env()\n                except ValidationError as e:\n                    raise ValueError(\n                        f\"{provider_name} is unavailable: can't load credentials\"\n                    ) from e\n\n            self._provider_instances[provider_name] = _provider = Provider(\n                settings=settings, logger=self._logger  # type: ignore\n            )\n            _provider._budget = self._budget  # Object binding not preserved by Pydantic\n        return _provider\n\n    @classmethod\n    def _get_provider_class(\n        cls, provider_name: ModelProviderName\n    ) -> type[AnthropicProvider | GroqProvider | OpenAIProvider]:\n        try:\n            return {\n                ModelProviderName.ANTHROPIC: AnthropicProvider,\n                ModelProviderName.GROQ: GroqProvider,\n                ModelProviderName.OPENAI: OpenAIProvider,\n            }[provider_name]\n        except KeyError:\n            raise ValueError(f\"{provider_name} is not a known provider\") from None\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}()\"\n\n\nChatModelProvider = AnthropicProvider | GroqProvider | OpenAIProvider | MultiProvider\n", "forge/forge/llm/providers/anthropic.py": "from __future__ import annotations\n\nimport enum\nimport logging\nfrom typing import TYPE_CHECKING, Any, Callable, Optional, ParamSpec, Sequence, TypeVar\n\nimport sentry_sdk\nimport tenacity\nimport tiktoken\nfrom anthropic import APIConnectionError, APIStatusError\nfrom pydantic import SecretStr\n\nfrom forge.models.config import UserConfigurable\n\nfrom .schema import (\n    AssistantChatMessage,\n    AssistantFunctionCall,\n    AssistantToolCall,\n    BaseChatModelProvider,\n    ChatMessage,\n    ChatModelInfo,\n    ChatModelResponse,\n    CompletionModelFunction,\n    ModelProviderBudget,\n    ModelProviderConfiguration,\n    ModelProviderCredentials,\n    ModelProviderName,\n    ModelProviderSettings,\n    ModelTokenizer,\n    ToolResultMessage,\n)\nfrom .utils import validate_tool_calls\n\nif TYPE_CHECKING:\n    from anthropic.types.beta.tools import MessageCreateParams\n    from anthropic.types.beta.tools import ToolsBetaMessage as Message\n    from anthropic.types.beta.tools import ToolsBetaMessageParam as MessageParam\n\n_T = TypeVar(\"_T\")\n_P = ParamSpec(\"_P\")\n\n\nclass AnthropicModelName(str, enum.Enum):\n    CLAUDE3_OPUS_v1 = \"claude-3-opus-20240229\"\n    CLAUDE3_SONNET_v1 = \"claude-3-sonnet-20240229\"\n    CLAUDE3_HAIKU_v1 = \"claude-3-haiku-20240307\"\n\n\nANTHROPIC_CHAT_MODELS = {\n    info.name: info\n    for info in [\n        ChatModelInfo(\n            name=AnthropicModelName.CLAUDE3_OPUS_v1,\n            provider_name=ModelProviderName.ANTHROPIC,\n            prompt_token_cost=15 / 1e6,\n            completion_token_cost=75 / 1e6,\n            max_tokens=200000,\n            has_function_call_api=True,\n        ),\n        ChatModelInfo(\n            name=AnthropicModelName.CLAUDE3_SONNET_v1,\n            provider_name=ModelProviderName.ANTHROPIC,\n            prompt_token_cost=3 / 1e6,\n            completion_token_cost=15 / 1e6,\n            max_tokens=200000,\n            has_function_call_api=True,\n        ),\n        ChatModelInfo(\n            name=AnthropicModelName.CLAUDE3_HAIKU_v1,\n            provider_name=ModelProviderName.ANTHROPIC,\n            prompt_token_cost=0.25 / 1e6,\n            completion_token_cost=1.25 / 1e6,\n            max_tokens=200000,\n            has_function_call_api=True,\n        ),\n    ]\n}\n\n\nclass AnthropicCredentials(ModelProviderCredentials):\n    \"\"\"Credentials for Anthropic.\"\"\"\n\n    api_key: SecretStr = UserConfigurable(from_env=\"ANTHROPIC_API_KEY\")  # type: ignore\n    api_base: Optional[SecretStr] = UserConfigurable(\n        default=None, from_env=\"ANTHROPIC_API_BASE_URL\"\n    )\n\n    def get_api_access_kwargs(self) -> dict[str, str]:\n        return {\n            k: v.get_secret_value()\n            for k, v in {\n                \"api_key\": self.api_key,\n                \"base_url\": self.api_base,\n            }.items()\n            if v is not None\n        }\n\n\nclass AnthropicSettings(ModelProviderSettings):\n    credentials: Optional[AnthropicCredentials]  # type: ignore\n    budget: ModelProviderBudget  # type: ignore\n\n\nclass AnthropicProvider(BaseChatModelProvider[AnthropicModelName, AnthropicSettings]):\n    default_settings = AnthropicSettings(\n        name=\"anthropic_provider\",\n        description=\"Provides access to Anthropic's API.\",\n        configuration=ModelProviderConfiguration(),\n        credentials=None,\n        budget=ModelProviderBudget(),\n    )\n\n    _settings: AnthropicSettings\n    _credentials: AnthropicCredentials\n    _budget: ModelProviderBudget\n\n    def __init__(\n        self,\n        settings: Optional[AnthropicSettings] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        if not settings:\n            settings = self.default_settings.copy(deep=True)\n        if not settings.credentials:\n            settings.credentials = AnthropicCredentials.from_env()\n\n        super(AnthropicProvider, self).__init__(settings=settings, logger=logger)\n\n        from anthropic import AsyncAnthropic\n\n        self._client = AsyncAnthropic(\n            **self._credentials.get_api_access_kwargs()  # type: ignore\n        )\n\n    async def get_available_models(self) -> Sequence[ChatModelInfo[AnthropicModelName]]:\n        return await self.get_available_chat_models()\n\n    async def get_available_chat_models(\n        self,\n    ) -> Sequence[ChatModelInfo[AnthropicModelName]]:\n        return list(ANTHROPIC_CHAT_MODELS.values())\n\n    def get_token_limit(self, model_name: AnthropicModelName) -> int:\n        \"\"\"Get the token limit for a given model.\"\"\"\n        return ANTHROPIC_CHAT_MODELS[model_name].max_tokens\n\n    def get_tokenizer(self, model_name: AnthropicModelName) -> ModelTokenizer[Any]:\n        # HACK: No official tokenizer is available for Claude 3\n        return tiktoken.encoding_for_model(model_name)\n\n    def count_tokens(self, text: str, model_name: AnthropicModelName) -> int:\n        return 0  # HACK: No official tokenizer is available for Claude 3\n\n    def count_message_tokens(\n        self,\n        messages: ChatMessage | list[ChatMessage],\n        model_name: AnthropicModelName,\n    ) -> int:\n        return 0  # HACK: No official tokenizer is available for Claude 3\n\n    async def create_chat_completion(\n        self,\n        model_prompt: list[ChatMessage],\n        model_name: AnthropicModelName,\n        completion_parser: Callable[[AssistantChatMessage], _T] = lambda _: None,\n        functions: Optional[list[CompletionModelFunction]] = None,\n        max_output_tokens: Optional[int] = None,\n        prefill_response: str = \"\",\n        **kwargs,\n    ) -> ChatModelResponse[_T]:\n        \"\"\"Create a completion using the Anthropic API.\"\"\"\n        anthropic_messages, completion_kwargs = self._get_chat_completion_args(\n            prompt_messages=model_prompt,\n            model=model_name,\n            functions=functions,\n            max_output_tokens=max_output_tokens,\n            **kwargs,\n        )\n\n        total_cost = 0.0\n        attempts = 0\n        while True:\n            completion_kwargs[\"messages\"] = anthropic_messages.copy()\n            if prefill_response:\n                completion_kwargs[\"messages\"].append(\n                    {\"role\": \"assistant\", \"content\": prefill_response}\n                )\n\n            (\n                _assistant_msg,\n                cost,\n                t_input,\n                t_output,\n            ) = await self._create_chat_completion(model_name, completion_kwargs)\n            total_cost += cost\n            self._logger.debug(\n                f\"Completion usage: {t_input} input, {t_output} output \"\n                f\"- ${round(cost, 5)}\"\n            )\n\n            # Merge prefill into generated response\n            if prefill_response:\n                first_text_block = next(\n                    b for b in _assistant_msg.content if b.type == \"text\"\n                )\n                first_text_block.text = prefill_response + first_text_block.text\n\n            assistant_msg = AssistantChatMessage(\n                content=\"\\n\\n\".join(\n                    b.text for b in _assistant_msg.content if b.type == \"text\"\n                ),\n                tool_calls=self._parse_assistant_tool_calls(_assistant_msg),\n            )\n\n            # If parsing the response fails, append the error to the prompt, and let the\n            # LLM fix its mistake(s).\n            attempts += 1\n            tool_call_errors = []\n            try:\n                # Validate tool calls\n                if assistant_msg.tool_calls and functions:\n                    tool_call_errors = validate_tool_calls(\n                        assistant_msg.tool_calls, functions\n                    )\n                    if tool_call_errors:\n                        raise ValueError(\n                            \"Invalid tool use(s):\\n\"\n                            + \"\\n\".join(str(e) for e in tool_call_errors)\n                        )\n\n                parsed_result = completion_parser(assistant_msg)\n                break\n            except Exception as e:\n                self._logger.debug(\n                    f\"Parsing failed on response: '''{_assistant_msg}'''\"\n                )\n                self._logger.warning(f\"Parsing attempt #{attempts} failed: {e}\")\n                sentry_sdk.capture_exception(\n                    error=e,\n                    extras={\"assistant_msg\": _assistant_msg, \"i_attempt\": attempts},\n                )\n                if attempts < self._configuration.fix_failed_parse_tries:\n                    anthropic_messages.append(\n                        _assistant_msg.dict(include={\"role\", \"content\"})  # type: ignore\n                    )\n                    anthropic_messages.append(\n                        {\n                            \"role\": \"user\",\n                            \"content\": [\n                                *(\n                                    # tool_result is required if last assistant message\n                                    # had tool_use block(s)\n                                    {\n                                        \"type\": \"tool_result\",\n                                        \"tool_use_id\": tc.id,\n                                        \"is_error\": True,\n                                        \"content\": [\n                                            {\n                                                \"type\": \"text\",\n                                                \"text\": \"Not executed because parsing \"\n                                                \"of your last message failed\"\n                                                if not tool_call_errors\n                                                else str(e)\n                                                if (\n                                                    e := next(\n                                                        (\n                                                            tce\n                                                            for tce in tool_call_errors\n                                                            if tce.name\n                                                            == tc.function.name\n                                                        ),\n                                                        None,\n                                                    )\n                                                )\n                                                else \"Not executed because validation \"\n                                                \"of tool input failed\",\n                                            }\n                                        ],\n                                    }\n                                    for tc in assistant_msg.tool_calls or []\n                                ),\n                                {\n                                    \"type\": \"text\",\n                                    \"text\": (\n                                        \"ERROR PARSING YOUR RESPONSE:\\n\\n\"\n                                        f\"{e.__class__.__name__}: {e}\"\n                                    ),\n                                },\n                            ],\n                        }\n                    )\n                else:\n                    raise\n\n        if attempts > 1:\n            self._logger.debug(\n                f\"Total cost for {attempts} attempts: ${round(total_cost, 5)}\"\n            )\n\n        return ChatModelResponse(\n            response=assistant_msg,\n            parsed_result=parsed_result,\n            model_info=ANTHROPIC_CHAT_MODELS[model_name],\n            prompt_tokens_used=t_input,\n            completion_tokens_used=t_output,\n        )\n\n    def _get_chat_completion_args(\n        self,\n        prompt_messages: list[ChatMessage],\n        functions: Optional[list[CompletionModelFunction]] = None,\n        max_output_tokens: Optional[int] = None,\n        **kwargs,\n    ) -> tuple[list[MessageParam], MessageCreateParams]:\n        \"\"\"Prepare arguments for message completion API call.\n\n        Args:\n            prompt_messages: List of ChatMessages.\n            functions: Optional list of functions available to the LLM.\n            kwargs: Additional keyword arguments.\n\n        Returns:\n            list[MessageParam]: Prompt messages for the Anthropic call\n            dict[str, Any]: Any other kwargs for the Anthropic call\n        \"\"\"\n        if functions:\n            kwargs[\"tools\"] = [\n                {\n                    \"name\": f.name,\n                    \"description\": f.description,\n                    \"input_schema\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            name: param.to_dict()\n                            for name, param in f.parameters.items()\n                        },\n                        \"required\": [\n                            name\n                            for name, param in f.parameters.items()\n                            if param.required\n                        ],\n                    },\n                }\n                for f in functions\n            ]\n\n        kwargs[\"max_tokens\"] = max_output_tokens or 4096\n\n        if extra_headers := self._configuration.extra_request_headers:\n            kwargs[\"extra_headers\"] = kwargs.get(\"extra_headers\", {})\n            kwargs[\"extra_headers\"].update(extra_headers.copy())\n\n        system_messages = [\n            m for m in prompt_messages if m.role == ChatMessage.Role.SYSTEM\n        ]\n        if (_n := len(system_messages)) > 1:\n            self._logger.warning(\n                f\"Prompt has {_n} system messages; Anthropic supports only 1. \"\n                \"They will be merged, and removed from the rest of the prompt.\"\n            )\n        kwargs[\"system\"] = \"\\n\\n\".join(sm.content for sm in system_messages)\n\n        messages: list[MessageParam] = []\n        for message in prompt_messages:\n            if message.role == ChatMessage.Role.SYSTEM:\n                continue\n            elif message.role == ChatMessage.Role.USER:\n                # Merge subsequent user messages\n                if messages and (prev_msg := messages[-1])[\"role\"] == \"user\":\n                    if isinstance(prev_msg[\"content\"], str):\n                        prev_msg[\"content\"] += f\"\\n\\n{message.content}\"\n                    else:\n                        assert isinstance(prev_msg[\"content\"], list)\n                        prev_msg[\"content\"].append(\n                            {\"type\": \"text\", \"text\": message.content}\n                        )\n                else:\n                    messages.append({\"role\": \"user\", \"content\": message.content})\n                # TODO: add support for image blocks\n            elif message.role == ChatMessage.Role.ASSISTANT:\n                if isinstance(message, AssistantChatMessage) and message.tool_calls:\n                    messages.append(\n                        {\n                            \"role\": \"assistant\",\n                            \"content\": [\n                                *(\n                                    [{\"type\": \"text\", \"text\": message.content}]\n                                    if message.content\n                                    else []\n                                ),\n                                *(\n                                    {\n                                        \"type\": \"tool_use\",\n                                        \"id\": tc.id,\n                                        \"name\": tc.function.name,\n                                        \"input\": tc.function.arguments,\n                                    }\n                                    for tc in message.tool_calls\n                                ),\n                            ],\n                        }\n                    )\n                elif message.content:\n                    messages.append(\n                        {\n                            \"role\": \"assistant\",\n                            \"content\": message.content,\n                        }\n                    )\n            elif isinstance(message, ToolResultMessage):\n                messages.append(\n                    {\n                        \"role\": \"user\",\n                        \"content\": [\n                            {\n                                \"type\": \"tool_result\",\n                                \"tool_use_id\": message.tool_call_id,\n                                \"content\": [{\"type\": \"text\", \"text\": message.content}],\n                                \"is_error\": message.is_error,\n                            }\n                        ],\n                    }\n                )\n\n        return messages, kwargs  # type: ignore\n\n    async def _create_chat_completion(\n        self, model: AnthropicModelName, completion_kwargs: MessageCreateParams\n    ) -> tuple[Message, float, int, int]:\n        \"\"\"\n        Create a chat completion using the Anthropic API with retry handling.\n\n        Params:\n            completion_kwargs: Keyword arguments for an Anthropic Messages API call\n\n        Returns:\n            Message: The message completion object\n            float: The cost ($) of this completion\n            int: Number of input tokens used\n            int: Number of output tokens used\n        \"\"\"\n\n        @self._retry_api_request\n        async def _create_chat_completion_with_retry() -> Message:\n            return await self._client.beta.tools.messages.create(\n                model=model, **completion_kwargs  # type: ignore\n            )\n\n        response = await _create_chat_completion_with_retry()\n\n        cost = self._budget.update_usage_and_cost(\n            model_info=ANTHROPIC_CHAT_MODELS[model],\n            input_tokens_used=response.usage.input_tokens,\n            output_tokens_used=response.usage.output_tokens,\n        )\n        return response, cost, response.usage.input_tokens, response.usage.output_tokens\n\n    def _parse_assistant_tool_calls(\n        self, assistant_message: Message\n    ) -> list[AssistantToolCall]:\n        return [\n            AssistantToolCall(\n                id=c.id,\n                type=\"function\",\n                function=AssistantFunctionCall(\n                    name=c.name,\n                    arguments=c.input,  # type: ignore\n                ),\n            )\n            for c in assistant_message.content\n            if c.type == \"tool_use\"\n        ]\n\n    def _retry_api_request(self, func: Callable[_P, _T]) -> Callable[_P, _T]:\n        return tenacity.retry(\n            retry=(\n                tenacity.retry_if_exception_type(APIConnectionError)\n                | tenacity.retry_if_exception(\n                    lambda e: isinstance(e, APIStatusError) and e.status_code >= 500\n                )\n            ),\n            wait=tenacity.wait_exponential(),\n            stop=tenacity.stop_after_attempt(self._configuration.retries_per_request),\n            after=tenacity.after_log(self._logger, logging.DEBUG),\n        )(func)\n\n    def __repr__(self):\n        return \"AnthropicProvider()\"\n", "forge/forge/llm/providers/schema.py": "import abc\nimport enum\nimport logging\nimport math\nfrom collections import defaultdict\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    ClassVar,\n    Generic,\n    Literal,\n    Optional,\n    Protocol,\n    Sequence,\n    TypedDict,\n    TypeVar,\n)\n\nfrom pydantic import BaseModel, Field, SecretStr\n\nfrom forge.logging.utils import fmt_kwargs\nfrom forge.models.config import (\n    Configurable,\n    SystemConfiguration,\n    SystemSettings,\n    UserConfigurable,\n)\nfrom forge.models.json_schema import JSONSchema\nfrom forge.models.providers import (\n    Embedding,\n    ProviderBudget,\n    ProviderCredentials,\n    ResourceType,\n)\n\nif TYPE_CHECKING:\n    from jsonschema import ValidationError\n\n\n_T = TypeVar(\"_T\")\n\n_ModelName = TypeVar(\"_ModelName\", bound=str)\n\n\nclass ModelProviderService(str, enum.Enum):\n    \"\"\"A ModelService describes what kind of service the model provides.\"\"\"\n\n    EMBEDDING = \"embedding\"\n    CHAT = \"chat_completion\"\n    TEXT = \"text_completion\"\n\n\nclass ModelProviderName(str, enum.Enum):\n    OPENAI = \"openai\"\n    ANTHROPIC = \"anthropic\"\n    GROQ = \"groq\"\n\n\nclass ChatMessage(BaseModel):\n    class Role(str, enum.Enum):\n        USER = \"user\"\n        SYSTEM = \"system\"\n        ASSISTANT = \"assistant\"\n\n        TOOL = \"tool\"\n        \"\"\"May be used for the result of tool calls\"\"\"\n        FUNCTION = \"function\"\n        \"\"\"May be used for the return value of function calls\"\"\"\n\n    role: Role\n    content: str\n\n    @staticmethod\n    def user(content: str) -> \"ChatMessage\":\n        return ChatMessage(role=ChatMessage.Role.USER, content=content)\n\n    @staticmethod\n    def system(content: str) -> \"ChatMessage\":\n        return ChatMessage(role=ChatMessage.Role.SYSTEM, content=content)\n\n\nclass ChatMessageDict(TypedDict):\n    role: str\n    content: str\n\n\nclass AssistantFunctionCall(BaseModel):\n    name: str\n    arguments: dict[str, Any]\n\n    def __str__(self) -> str:\n        return f\"{self.name}({fmt_kwargs(self.arguments)})\"\n\n\nclass AssistantFunctionCallDict(TypedDict):\n    name: str\n    arguments: dict[str, Any]\n\n\nclass AssistantToolCall(BaseModel):\n    id: str\n    type: Literal[\"function\"]\n    function: AssistantFunctionCall\n\n\nclass AssistantToolCallDict(TypedDict):\n    id: str\n    type: Literal[\"function\"]\n    function: AssistantFunctionCallDict\n\n\nclass AssistantChatMessage(ChatMessage):\n    role: Literal[ChatMessage.Role.ASSISTANT] = ChatMessage.Role.ASSISTANT  # type: ignore # noqa\n    content: str = \"\"\n    tool_calls: Optional[list[AssistantToolCall]] = None\n\n\nclass ToolResultMessage(ChatMessage):\n    role: Literal[ChatMessage.Role.TOOL] = ChatMessage.Role.TOOL  # type: ignore\n    is_error: bool = False\n    tool_call_id: str\n\n\nclass AssistantChatMessageDict(TypedDict, total=False):\n    role: str\n    content: str\n    tool_calls: list[AssistantToolCallDict]\n\n\nclass CompletionModelFunction(BaseModel):\n    \"\"\"General representation object for LLM-callable functions.\"\"\"\n\n    name: str\n    description: str\n    parameters: dict[str, \"JSONSchema\"]\n\n    def fmt_line(self) -> str:\n        params = \", \".join(\n            f\"{name}{'?' if not p.required else ''}: \" f\"{p.typescript_type}\"\n            for name, p in self.parameters.items()\n        )\n        return f\"{self.name}: {self.description}. Params: ({params})\"\n\n    def validate_call(\n        self, function_call: AssistantFunctionCall\n    ) -> tuple[bool, list[\"ValidationError\"]]:\n        \"\"\"\n        Validates the given function call against the function's parameter specs\n\n        Returns:\n            bool: Whether the given set of arguments is valid for this command\n            list[ValidationError]: Issues with the set of arguments (if any)\n\n        Raises:\n            ValueError: If the function_call doesn't call this function\n        \"\"\"\n        if function_call.name != self.name:\n            raise ValueError(\n                f\"Can't validate {function_call.name} call using {self.name} spec\"\n            )\n\n        params_schema = JSONSchema(\n            type=JSONSchema.Type.OBJECT,\n            properties={name: spec for name, spec in self.parameters.items()},\n        )\n        return params_schema.validate_object(function_call.arguments)\n\n\nclass ModelInfo(BaseModel, Generic[_ModelName]):\n    \"\"\"Struct for model information.\n\n    Would be lovely to eventually get this directly from APIs, but needs to be\n    scraped from websites for now.\n    \"\"\"\n\n    name: _ModelName\n    service: ClassVar[ModelProviderService]\n    provider_name: ModelProviderName\n    prompt_token_cost: float = 0.0\n    completion_token_cost: float = 0.0\n\n\nclass ModelResponse(BaseModel):\n    \"\"\"Standard response struct for a response from a model.\"\"\"\n\n    prompt_tokens_used: int\n    completion_tokens_used: int\n    model_info: ModelInfo\n\n\nclass ModelProviderConfiguration(SystemConfiguration):\n    retries_per_request: int = UserConfigurable(7)\n    fix_failed_parse_tries: int = UserConfigurable(3)\n    extra_request_headers: dict[str, str] = Field(default_factory=dict)\n\n\nclass ModelProviderCredentials(ProviderCredentials):\n    \"\"\"Credentials for a model provider.\"\"\"\n\n    api_key: SecretStr | None = UserConfigurable(default=None)\n    api_type: SecretStr | None = UserConfigurable(default=None)\n    api_base: SecretStr | None = UserConfigurable(default=None)\n    api_version: SecretStr | None = UserConfigurable(default=None)\n    deployment_id: SecretStr | None = UserConfigurable(default=None)\n\n    class Config(ProviderCredentials.Config):\n        extra = \"ignore\"\n\n\nclass ModelProviderUsage(BaseModel):\n    \"\"\"Usage for a particular model from a model provider.\"\"\"\n\n    class ModelUsage(BaseModel):\n        completion_tokens: int = 0\n        prompt_tokens: int = 0\n\n    usage_per_model: dict[str, ModelUsage] = defaultdict(ModelUsage)\n\n    @property\n    def completion_tokens(self) -> int:\n        return sum(model.completion_tokens for model in self.usage_per_model.values())\n\n    @property\n    def prompt_tokens(self) -> int:\n        return sum(model.prompt_tokens for model in self.usage_per_model.values())\n\n    def update_usage(\n        self,\n        model: str,\n        input_tokens_used: int,\n        output_tokens_used: int = 0,\n    ) -> None:\n        self.usage_per_model[model].prompt_tokens += input_tokens_used\n        self.usage_per_model[model].completion_tokens += output_tokens_used\n\n\nclass ModelProviderBudget(ProviderBudget[ModelProviderUsage]):\n    usage: ModelProviderUsage = Field(default_factory=ModelProviderUsage)\n\n    def update_usage_and_cost(\n        self,\n        model_info: ModelInfo,\n        input_tokens_used: int,\n        output_tokens_used: int = 0,\n    ) -> float:\n        \"\"\"Update the usage and cost of the provider.\n\n        Returns:\n            float: The (calculated) cost of the given model response.\n        \"\"\"\n        self.usage.update_usage(model_info.name, input_tokens_used, output_tokens_used)\n        incurred_cost = (\n            output_tokens_used * model_info.completion_token_cost\n            + input_tokens_used * model_info.prompt_token_cost\n        )\n        self.total_cost += incurred_cost\n        self.remaining_budget -= incurred_cost\n        return incurred_cost\n\n\nclass ModelProviderSettings(SystemSettings):\n    resource_type: ClassVar[ResourceType] = ResourceType.MODEL\n    configuration: ModelProviderConfiguration\n    credentials: Optional[ModelProviderCredentials] = None\n    budget: Optional[ModelProviderBudget] = None\n\n\n_ModelProviderSettings = TypeVar(\"_ModelProviderSettings\", bound=ModelProviderSettings)\n\n\n# TODO: either use MultiProvider throughout codebase as type for `llm_provider`, or\n# replace `_ModelName` by `str` to eliminate type checking difficulties\nclass BaseModelProvider(\n    abc.ABC,\n    Generic[_ModelName, _ModelProviderSettings],\n    Configurable[_ModelProviderSettings],\n):\n    \"\"\"A ModelProvider abstracts the details of a particular provider of models.\"\"\"\n\n    default_settings: ClassVar[_ModelProviderSettings]  # type: ignore\n\n    _settings: _ModelProviderSettings\n    _logger: logging.Logger\n\n    def __init__(\n        self,\n        settings: Optional[_ModelProviderSettings] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        if not settings:\n            settings = self.default_settings.copy(deep=True)\n\n        self._settings = settings\n        self._configuration = settings.configuration\n        self._credentials = settings.credentials\n        self._budget = settings.budget\n\n        self._logger = logger or logging.getLogger(self.__module__)\n\n    @abc.abstractmethod\n    async def get_available_models(\n        self,\n    ) -> Sequence[\"ChatModelInfo[_ModelName] | EmbeddingModelInfo[_ModelName]\"]:\n        ...\n\n    @abc.abstractmethod\n    def count_tokens(self, text: str, model_name: _ModelName) -> int:\n        ...\n\n    @abc.abstractmethod\n    def get_tokenizer(self, model_name: _ModelName) -> \"ModelTokenizer[Any]\":\n        ...\n\n    @abc.abstractmethod\n    def get_token_limit(self, model_name: _ModelName) -> int:\n        ...\n\n    def get_incurred_cost(self) -> float:\n        if self._budget:\n            return self._budget.total_cost\n        return 0\n\n    def get_remaining_budget(self) -> float:\n        if self._budget:\n            return self._budget.remaining_budget\n        return math.inf\n\n\nclass ModelTokenizer(Protocol, Generic[_T]):\n    \"\"\"A ModelTokenizer provides tokenization specific to a model.\"\"\"\n\n    @abc.abstractmethod\n    def encode(self, text: str) -> list[_T]:\n        ...\n\n    @abc.abstractmethod\n    def decode(self, tokens: list[_T]) -> str:\n        ...\n\n\n####################\n# Embedding Models #\n####################\n\n\nclass EmbeddingModelInfo(ModelInfo[_ModelName]):\n    \"\"\"Struct for embedding model information.\"\"\"\n\n    service: Literal[ModelProviderService.EMBEDDING] = ModelProviderService.EMBEDDING  # type: ignore # noqa\n    max_tokens: int\n    embedding_dimensions: int\n\n\nclass EmbeddingModelResponse(ModelResponse):\n    \"\"\"Standard response struct for a response from an embedding model.\"\"\"\n\n    embedding: Embedding = Field(default_factory=list)\n    completion_tokens_used: int = Field(default=0, const=True)\n\n\nclass BaseEmbeddingModelProvider(BaseModelProvider[_ModelName, _ModelProviderSettings]):\n    @abc.abstractmethod\n    async def get_available_embedding_models(\n        self,\n    ) -> Sequence[EmbeddingModelInfo[_ModelName]]:\n        ...\n\n    @abc.abstractmethod\n    async def create_embedding(\n        self,\n        text: str,\n        model_name: _ModelName,\n        embedding_parser: Callable[[Embedding], Embedding],\n        **kwargs,\n    ) -> EmbeddingModelResponse:\n        ...\n\n\n###############\n# Chat Models #\n###############\n\n\nclass ChatModelInfo(ModelInfo[_ModelName]):\n    \"\"\"Struct for language model information.\"\"\"\n\n    service: Literal[ModelProviderService.CHAT] = ModelProviderService.CHAT  # type: ignore # noqa\n    max_tokens: int\n    has_function_call_api: bool = False\n\n\nclass ChatModelResponse(ModelResponse, Generic[_T]):\n    \"\"\"Standard response struct for a response from a language model.\"\"\"\n\n    response: AssistantChatMessage\n    parsed_result: _T\n\n\nclass BaseChatModelProvider(BaseModelProvider[_ModelName, _ModelProviderSettings]):\n    @abc.abstractmethod\n    async def get_available_chat_models(self) -> Sequence[ChatModelInfo[_ModelName]]:\n        ...\n\n    @abc.abstractmethod\n    def count_message_tokens(\n        self,\n        messages: ChatMessage | list[ChatMessage],\n        model_name: _ModelName,\n    ) -> int:\n        ...\n\n    @abc.abstractmethod\n    async def create_chat_completion(\n        self,\n        model_prompt: list[ChatMessage],\n        model_name: _ModelName,\n        completion_parser: Callable[[AssistantChatMessage], _T] = lambda _: None,\n        functions: Optional[list[CompletionModelFunction]] = None,\n        max_output_tokens: Optional[int] = None,\n        prefill_response: str = \"\",\n        **kwargs,\n    ) -> ChatModelResponse[_T]:\n        ...\n", "forge/forge/llm/providers/utils.py": "from typing import TYPE_CHECKING, Any, Iterable\n\nif TYPE_CHECKING:\n    from forge.command.command import Command\n\nfrom .schema import AssistantToolCall, CompletionModelFunction\n\n\nclass InvalidFunctionCallError(Exception):\n    def __init__(self, name: str, arguments: dict[str, Any], message: str):\n        self.message = message\n        self.name = name\n        self.arguments = arguments\n        super().__init__(message)\n\n    def __str__(self) -> str:\n        return f\"Invalid function call for {self.name}: {self.message}\"\n\n\ndef validate_tool_calls(\n    tool_calls: list[AssistantToolCall], functions: list[CompletionModelFunction]\n) -> list[InvalidFunctionCallError]:\n    \"\"\"\n    Validates a list of tool calls against a list of functions.\n\n    1. Tries to find a function matching each tool call\n    2. If a matching function is found, validates the tool call's arguments,\n    reporting any resulting errors\n    2. If no matching function is found, an error \"Unknown function X\" is reported\n    3. A list of all errors encountered during validation is returned\n\n    Params:\n        tool_calls: A list of tool calls to validate.\n        functions: A list of functions to validate against.\n\n    Returns:\n        list[InvalidFunctionCallError]: All errors encountered during validation.\n    \"\"\"\n    errors: list[InvalidFunctionCallError] = []\n    for tool_call in tool_calls:\n        function_call = tool_call.function\n\n        if function := next(\n            (f for f in functions if f.name == function_call.name),\n            None,\n        ):\n            is_valid, validation_errors = function.validate_call(function_call)\n            if not is_valid:\n                fmt_errors = [\n                    f\"{'.'.join(str(p) for p in f.path)}: {f.message}\"\n                    if f.path\n                    else f.message\n                    for f in validation_errors\n                ]\n                errors.append(\n                    InvalidFunctionCallError(\n                        name=function_call.name,\n                        arguments=function_call.arguments,\n                        message=(\n                            \"The set of arguments supplied is invalid:\\n\"\n                            + \"\\n\".join(fmt_errors)\n                        ),\n                    )\n                )\n        else:\n            errors.append(\n                InvalidFunctionCallError(\n                    name=function_call.name,\n                    arguments=function_call.arguments,\n                    message=f\"Unknown function {function_call.name}\",\n                )\n            )\n\n    return errors\n\n\ndef function_specs_from_commands(\n    commands: Iterable[\"Command\"],\n) -> list[CompletionModelFunction]:\n    \"\"\"Get LLM-consumable function specs for the agent's available commands.\"\"\"\n    return [\n        CompletionModelFunction(\n            name=command.names[0],\n            description=command.description,\n            parameters={param.name: param.spec for param in command.parameters},\n        )\n        for command in commands\n    ]\n", "forge/forge/llm/providers/_openai_base.py": "import logging\nfrom typing import (\n    Any,\n    Awaitable,\n    Callable,\n    ClassVar,\n    Mapping,\n    Optional,\n    ParamSpec,\n    Sequence,\n    TypeVar,\n    cast,\n)\n\nimport sentry_sdk\nimport tenacity\nfrom openai._exceptions import APIConnectionError, APIStatusError\nfrom openai.types import CreateEmbeddingResponse, EmbeddingCreateParams\nfrom openai.types.chat import (\n    ChatCompletion,\n    ChatCompletionAssistantMessageParam,\n    ChatCompletionMessage,\n    ChatCompletionMessageParam,\n    CompletionCreateParams,\n)\nfrom openai.types.shared_params import FunctionDefinition\n\nfrom forge.json.parsing import json_loads\n\nfrom .schema import (\n    AssistantChatMessage,\n    AssistantFunctionCall,\n    AssistantToolCall,\n    BaseChatModelProvider,\n    BaseEmbeddingModelProvider,\n    BaseModelProvider,\n    ChatMessage,\n    ChatModelInfo,\n    ChatModelResponse,\n    CompletionModelFunction,\n    Embedding,\n    EmbeddingModelInfo,\n    EmbeddingModelResponse,\n    ModelProviderService,\n    _ModelName,\n    _ModelProviderSettings,\n)\nfrom .utils import validate_tool_calls\n\n_T = TypeVar(\"_T\")\n_P = ParamSpec(\"_P\")\n\n\nclass _BaseOpenAIProvider(BaseModelProvider[_ModelName, _ModelProviderSettings]):\n    \"\"\"Base class for LLM providers with OpenAI-like APIs\"\"\"\n\n    MODELS: ClassVar[\n        Mapping[_ModelName, ChatModelInfo[_ModelName] | EmbeddingModelInfo[_ModelName]]  # type: ignore # noqa\n    ]\n\n    def __init__(\n        self,\n        settings: Optional[_ModelProviderSettings] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        if not getattr(self, \"MODELS\", None):\n            raise ValueError(f\"{self.__class__.__name__}.MODELS is not set\")\n\n        if not settings:\n            settings = self.default_settings.copy(deep=True)\n        if not settings.credentials:\n            settings.credentials = self.default_settings.__fields__[\n                \"credentials\"\n            ].type_.from_env()\n\n        super(_BaseOpenAIProvider, self).__init__(settings=settings, logger=logger)\n\n        if not getattr(self, \"_client\", None):\n            from openai import AsyncOpenAI\n\n            self._client = AsyncOpenAI(\n                **self._credentials.get_api_access_kwargs()  # type: ignore\n            )\n\n    async def get_available_models(\n        self,\n    ) -> Sequence[ChatModelInfo[_ModelName] | EmbeddingModelInfo[_ModelName]]:\n        _models = (await self._client.models.list()).data\n        return [\n            self.MODELS[cast(_ModelName, m.id)] for m in _models if m.id in self.MODELS\n        ]\n\n    def get_token_limit(self, model_name: _ModelName) -> int:\n        \"\"\"Get the maximum number of input tokens for a given model\"\"\"\n        return self.MODELS[model_name].max_tokens\n\n    def count_tokens(self, text: str, model_name: _ModelName) -> int:\n        return len(self.get_tokenizer(model_name).encode(text))\n\n    def _retry_api_request(self, func: Callable[_P, _T]) -> Callable[_P, _T]:\n        return tenacity.retry(\n            retry=(\n                tenacity.retry_if_exception_type(APIConnectionError)\n                | tenacity.retry_if_exception(\n                    lambda e: isinstance(e, APIStatusError) and e.status_code >= 500\n                )\n            ),\n            wait=tenacity.wait_exponential(),\n            stop=tenacity.stop_after_attempt(self._configuration.retries_per_request),\n            after=tenacity.after_log(self._logger, logging.DEBUG),\n        )(func)\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}()\"\n\n\nclass BaseOpenAIChatProvider(\n    _BaseOpenAIProvider[_ModelName, _ModelProviderSettings],\n    BaseChatModelProvider[_ModelName, _ModelProviderSettings],\n):\n    CHAT_MODELS: ClassVar[dict[_ModelName, ChatModelInfo[_ModelName]]]  # type: ignore\n\n    def __init__(\n        self,\n        settings: Optional[_ModelProviderSettings] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        if not getattr(self, \"CHAT_MODELS\", None):\n            raise ValueError(f\"{self.__class__.__name__}.CHAT_MODELS is not set\")\n\n        super(BaseOpenAIChatProvider, self).__init__(settings=settings, logger=logger)\n\n    async def get_available_chat_models(self) -> Sequence[ChatModelInfo[_ModelName]]:\n        all_available_models = await self.get_available_models()\n        return [\n            model\n            for model in all_available_models\n            if model.service == ModelProviderService.CHAT\n        ]\n\n    def count_message_tokens(\n        self,\n        messages: ChatMessage | list[ChatMessage],\n        model_name: _ModelName,\n    ) -> int:\n        if isinstance(messages, ChatMessage):\n            messages = [messages]\n        return self.count_tokens(\n            \"\\n\\n\".join(f\"{m.role.upper()}: {m.content}\" for m in messages), model_name\n        )\n\n    async def create_chat_completion(\n        self,\n        model_prompt: list[ChatMessage],\n        model_name: _ModelName,\n        completion_parser: Callable[[AssistantChatMessage], _T] = lambda _: None,\n        functions: Optional[list[CompletionModelFunction]] = None,\n        max_output_tokens: Optional[int] = None,\n        prefill_response: str = \"\",\n        **kwargs,\n    ) -> ChatModelResponse[_T]:\n        \"\"\"Create a chat completion using the API.\"\"\"\n\n        (\n            openai_messages,\n            completion_kwargs,\n            parse_kwargs,\n        ) = self._get_chat_completion_args(\n            prompt_messages=model_prompt,\n            model=model_name,\n            functions=functions,\n            max_output_tokens=max_output_tokens,\n            **kwargs,\n        )\n\n        total_cost = 0.0\n        attempts = 0\n        while True:\n            completion_kwargs[\"messages\"] = openai_messages\n            _response, _cost, t_input, t_output = await self._create_chat_completion(\n                model=model_name,\n                completion_kwargs=completion_kwargs,\n            )\n            total_cost += _cost\n\n            # If parsing the response fails, append the error to the prompt, and let the\n            # LLM fix its mistake(s).\n            attempts += 1\n            parse_errors: list[Exception] = []\n\n            _assistant_msg = _response.choices[0].message\n\n            tool_calls, _errors = self._parse_assistant_tool_calls(\n                _assistant_msg, **parse_kwargs\n            )\n            parse_errors += _errors\n\n            # Validate tool calls\n            if not parse_errors and tool_calls and functions:\n                parse_errors += validate_tool_calls(tool_calls, functions)\n\n            assistant_msg = AssistantChatMessage(\n                content=_assistant_msg.content or \"\",\n                tool_calls=tool_calls or None,\n            )\n\n            parsed_result: _T = None  # type: ignore\n            if not parse_errors:\n                try:\n                    parsed_result = completion_parser(assistant_msg)\n                except Exception as e:\n                    parse_errors.append(e)\n\n            if not parse_errors:\n                if attempts > 1:\n                    self._logger.debug(\n                        f\"Total cost for {attempts} attempts: ${round(total_cost, 5)}\"\n                    )\n\n                return ChatModelResponse(\n                    response=AssistantChatMessage(\n                        content=_assistant_msg.content or \"\",\n                        tool_calls=tool_calls or None,\n                    ),\n                    parsed_result=parsed_result,\n                    model_info=self.CHAT_MODELS[model_name],\n                    prompt_tokens_used=t_input,\n                    completion_tokens_used=t_output,\n                )\n\n            else:\n                self._logger.debug(\n                    f\"Parsing failed on response: '''{_assistant_msg}'''\"\n                )\n                parse_errors_fmt = \"\\n\\n\".join(\n                    f\"{e.__class__.__name__}: {e}\" for e in parse_errors\n                )\n                self._logger.warning(\n                    f\"Parsing attempt #{attempts} failed: {parse_errors_fmt}\"\n                )\n                for e in parse_errors:\n                    sentry_sdk.capture_exception(\n                        error=e,\n                        extras={\"assistant_msg\": _assistant_msg, \"i_attempt\": attempts},\n                    )\n\n                if attempts < self._configuration.fix_failed_parse_tries:\n                    openai_messages.append(\n                        cast(\n                            ChatCompletionAssistantMessageParam,\n                            _assistant_msg.dict(exclude_none=True),\n                        )\n                    )\n                    openai_messages.append(\n                        {\n                            \"role\": \"system\",\n                            \"content\": (\n                                f\"ERROR PARSING YOUR RESPONSE:\\n\\n{parse_errors_fmt}\"\n                            ),\n                        }\n                    )\n                    continue\n                else:\n                    raise parse_errors[0]\n\n    def _get_chat_completion_args(\n        self,\n        prompt_messages: list[ChatMessage],\n        model: _ModelName,\n        functions: Optional[list[CompletionModelFunction]] = None,\n        max_output_tokens: Optional[int] = None,\n        **kwargs,\n    ) -> tuple[\n        list[ChatCompletionMessageParam], CompletionCreateParams, dict[str, Any]\n    ]:\n        \"\"\"Prepare keyword arguments for a chat completion API call\n\n        Args:\n            prompt_messages: List of ChatMessages\n            model: The model to use\n            functions (optional): List of functions available to the LLM\n            max_output_tokens (optional): Maximum number of tokens to generate\n\n        Returns:\n            list[ChatCompletionMessageParam]: Prompt messages for the API call\n            CompletionCreateParams: Mapping of other kwargs for the API call\n            Mapping[str, Any]: Any keyword arguments to pass on to the completion parser\n        \"\"\"\n        kwargs = cast(CompletionCreateParams, kwargs)\n\n        if max_output_tokens:\n            kwargs[\"max_tokens\"] = max_output_tokens\n\n        if functions:\n            kwargs[\"tools\"] = [  # pyright: ignore - it fails to infer the dict type\n                {\"type\": \"function\", \"function\": format_function_def_for_openai(f)}\n                for f in functions\n            ]\n            if len(functions) == 1:\n                # force the model to call the only specified function\n                kwargs[\"tool_choice\"] = {  # pyright: ignore - type inference failure\n                    \"type\": \"function\",\n                    \"function\": {\"name\": functions[0].name},\n                }\n\n        if extra_headers := self._configuration.extra_request_headers:\n            # 'extra_headers' is not on CompletionCreateParams, but is on chat.create()\n            kwargs[\"extra_headers\"] = kwargs.get(\"extra_headers\", {})  # type: ignore\n            kwargs[\"extra_headers\"].update(extra_headers.copy())  # type: ignore\n\n        prepped_messages: list[ChatCompletionMessageParam] = [\n            message.dict(  # type: ignore\n                include={\"role\", \"content\", \"tool_calls\", \"tool_call_id\", \"name\"},\n                exclude_none=True,\n            )\n            for message in prompt_messages\n        ]\n\n        if \"messages\" in kwargs:\n            prepped_messages += kwargs[\"messages\"]\n            del kwargs[\"messages\"]  # type: ignore - messages are added back later\n\n        return prepped_messages, kwargs, {}\n\n    async def _create_chat_completion(\n        self,\n        model: _ModelName,\n        completion_kwargs: CompletionCreateParams,\n    ) -> tuple[ChatCompletion, float, int, int]:\n        \"\"\"\n        Create a chat completion using an OpenAI-like API with retry handling\n\n        Params:\n            model: The model to use for the completion\n            completion_kwargs: All other arguments for the completion call\n\n        Returns:\n            ChatCompletion: The chat completion response object\n            float: The cost ($) of this completion\n            int: Number of prompt tokens used\n            int: Number of completion tokens used\n        \"\"\"\n        completion_kwargs[\"model\"] = completion_kwargs.get(\"model\") or model\n\n        @self._retry_api_request\n        async def _create_chat_completion_with_retry() -> ChatCompletion:\n            return await self._client.chat.completions.create(\n                **completion_kwargs,  # type: ignore\n            )\n\n        completion = await _create_chat_completion_with_retry()\n\n        if completion.usage:\n            prompt_tokens_used = completion.usage.prompt_tokens\n            completion_tokens_used = completion.usage.completion_tokens\n        else:\n            prompt_tokens_used = completion_tokens_used = 0\n\n        if self._budget:\n            cost = self._budget.update_usage_and_cost(\n                model_info=self.CHAT_MODELS[model],\n                input_tokens_used=prompt_tokens_used,\n                output_tokens_used=completion_tokens_used,\n            )\n        else:\n            cost = 0\n\n        self._logger.debug(\n            f\"{model} completion usage: {prompt_tokens_used} input, \"\n            f\"{completion_tokens_used} output - ${round(cost, 5)}\"\n        )\n        return completion, cost, prompt_tokens_used, completion_tokens_used\n\n    def _parse_assistant_tool_calls(\n        self, assistant_message: ChatCompletionMessage, **kwargs\n    ) -> tuple[list[AssistantToolCall], list[Exception]]:\n        tool_calls: list[AssistantToolCall] = []\n        parse_errors: list[Exception] = []\n\n        if assistant_message.tool_calls:\n            for _tc in assistant_message.tool_calls:\n                try:\n                    parsed_arguments = json_loads(_tc.function.arguments)\n                except Exception as e:\n                    err_message = (\n                        f\"Decoding arguments for {_tc.function.name} failed: \"\n                        + str(e.args[0])\n                    )\n                    parse_errors.append(\n                        type(e)(err_message, *e.args[1:]).with_traceback(\n                            e.__traceback__\n                        )\n                    )\n                    continue\n\n                tool_calls.append(\n                    AssistantToolCall(\n                        id=_tc.id,\n                        type=_tc.type,\n                        function=AssistantFunctionCall(\n                            name=_tc.function.name,\n                            arguments=parsed_arguments,\n                        ),\n                    )\n                )\n\n            # If parsing of all tool calls succeeds in the end, we ignore any issues\n            if len(tool_calls) == len(assistant_message.tool_calls):\n                parse_errors = []\n\n        return tool_calls, parse_errors\n\n\nclass BaseOpenAIEmbeddingProvider(\n    _BaseOpenAIProvider[_ModelName, _ModelProviderSettings],\n    BaseEmbeddingModelProvider[_ModelName, _ModelProviderSettings],\n):\n    EMBEDDING_MODELS: ClassVar[\n        dict[_ModelName, EmbeddingModelInfo[_ModelName]]  # type: ignore\n    ]\n\n    def __init__(\n        self,\n        settings: Optional[_ModelProviderSettings] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        if not getattr(self, \"EMBEDDING_MODELS\", None):\n            raise ValueError(f\"{self.__class__.__name__}.EMBEDDING_MODELS is not set\")\n\n        super(BaseOpenAIEmbeddingProvider, self).__init__(\n            settings=settings, logger=logger\n        )\n\n    async def get_available_embedding_models(\n        self,\n    ) -> Sequence[EmbeddingModelInfo[_ModelName]]:\n        all_available_models = await self.get_available_models()\n        return [\n            model\n            for model in all_available_models\n            if model.service == ModelProviderService.EMBEDDING\n        ]\n\n    async def create_embedding(\n        self,\n        text: str,\n        model_name: _ModelName,\n        embedding_parser: Callable[[Embedding], Embedding],\n        **kwargs,\n    ) -> EmbeddingModelResponse:\n        \"\"\"Create an embedding using an OpenAI-like API\"\"\"\n        embedding_kwargs = self._get_embedding_kwargs(\n            input=text, model=model_name, **kwargs\n        )\n        response = await self._create_embedding(embedding_kwargs)\n\n        return EmbeddingModelResponse(\n            embedding=embedding_parser(response.data[0].embedding),\n            model_info=self.EMBEDDING_MODELS[model_name],\n            prompt_tokens_used=response.usage.prompt_tokens,\n        )\n\n    def _get_embedding_kwargs(\n        self, input: str | list[str], model: _ModelName, **kwargs\n    ) -> EmbeddingCreateParams:\n        \"\"\"Get kwargs for an embedding API call\n\n        Params:\n            input: Text body or list of text bodies to create embedding(s) from\n            model: Embedding model to use\n\n        Returns:\n            The kwargs for the embedding API call\n        \"\"\"\n        kwargs = cast(EmbeddingCreateParams, kwargs)\n\n        kwargs[\"input\"] = input\n        kwargs[\"model\"] = model\n\n        if extra_headers := self._configuration.extra_request_headers:\n            # 'extra_headers' is not on CompletionCreateParams, but is on embedding.create()  # noqa\n            kwargs[\"extra_headers\"] = kwargs.get(\"extra_headers\", {})  # type: ignore\n            kwargs[\"extra_headers\"].update(extra_headers.copy())  # type: ignore\n\n        return kwargs\n\n    def _create_embedding(\n        self, embedding_kwargs: EmbeddingCreateParams\n    ) -> Awaitable[CreateEmbeddingResponse]:\n        \"\"\"Create an embedding using an OpenAI-like API with retry handling.\"\"\"\n\n        @self._retry_api_request\n        async def _create_embedding_with_retry() -> CreateEmbeddingResponse:\n            return await self._client.embeddings.create(**embedding_kwargs)\n\n        return _create_embedding_with_retry()\n\n\ndef format_function_def_for_openai(self: CompletionModelFunction) -> FunctionDefinition:\n    \"\"\"Returns an OpenAI-consumable function definition\"\"\"\n\n    return {\n        \"name\": self.name,\n        \"description\": self.description,\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                name: param.to_dict() for name, param in self.parameters.items()\n            },\n            \"required\": [\n                name for name, param in self.parameters.items() if param.required\n            ],\n        },\n    }\n", "forge/forge/llm/providers/groq.py": "from __future__ import annotations\n\nimport enum\nimport logging\nfrom typing import Any, Optional\n\nimport tiktoken\nfrom pydantic import SecretStr\n\nfrom forge.models.config import UserConfigurable\n\nfrom ._openai_base import BaseOpenAIChatProvider\nfrom .schema import (\n    ChatModelInfo,\n    ModelProviderBudget,\n    ModelProviderConfiguration,\n    ModelProviderCredentials,\n    ModelProviderName,\n    ModelProviderSettings,\n    ModelTokenizer,\n)\n\n\nclass GroqModelName(str, enum.Enum):\n    LLAMA3_8B = \"llama3-8b-8192\"\n    LLAMA3_70B = \"llama3-70b-8192\"\n    MIXTRAL_8X7B = \"mixtral-8x7b-32768\"\n    GEMMA_7B = \"gemma-7b-it\"\n\n\nGROQ_CHAT_MODELS = {\n    info.name: info\n    for info in [\n        ChatModelInfo(\n            name=GroqModelName.LLAMA3_8B,\n            provider_name=ModelProviderName.GROQ,\n            prompt_token_cost=0.05 / 1e6,\n            completion_token_cost=0.10 / 1e6,\n            max_tokens=8192,\n            has_function_call_api=True,\n        ),\n        ChatModelInfo(\n            name=GroqModelName.LLAMA3_70B,\n            provider_name=ModelProviderName.GROQ,\n            prompt_token_cost=0.59 / 1e6,\n            completion_token_cost=0.79 / 1e6,\n            max_tokens=8192,\n            has_function_call_api=True,\n        ),\n        ChatModelInfo(\n            name=GroqModelName.MIXTRAL_8X7B,\n            provider_name=ModelProviderName.GROQ,\n            prompt_token_cost=0.27 / 1e6,\n            completion_token_cost=0.27 / 1e6,\n            max_tokens=32768,\n            has_function_call_api=True,\n        ),\n        ChatModelInfo(\n            name=GroqModelName.GEMMA_7B,\n            provider_name=ModelProviderName.GROQ,\n            prompt_token_cost=0.10 / 1e6,\n            completion_token_cost=0.10 / 1e6,\n            max_tokens=8192,\n            has_function_call_api=True,\n        ),\n    ]\n}\n\n\nclass GroqCredentials(ModelProviderCredentials):\n    \"\"\"Credentials for Groq.\"\"\"\n\n    api_key: SecretStr = UserConfigurable(from_env=\"GROQ_API_KEY\")  # type: ignore\n    api_base: Optional[SecretStr] = UserConfigurable(\n        default=None, from_env=\"GROQ_API_BASE_URL\"\n    )\n\n    def get_api_access_kwargs(self) -> dict[str, str]:\n        return {\n            k: v.get_secret_value()\n            for k, v in {\n                \"api_key\": self.api_key,\n                \"base_url\": self.api_base,\n            }.items()\n            if v is not None\n        }\n\n\nclass GroqSettings(ModelProviderSettings):\n    credentials: Optional[GroqCredentials]  # type: ignore\n    budget: ModelProviderBudget  # type: ignore\n\n\nclass GroqProvider(BaseOpenAIChatProvider[GroqModelName, GroqSettings]):\n    CHAT_MODELS = GROQ_CHAT_MODELS\n    MODELS = CHAT_MODELS\n\n    default_settings = GroqSettings(\n        name=\"groq_provider\",\n        description=\"Provides access to Groq's API.\",\n        configuration=ModelProviderConfiguration(),\n        credentials=None,\n        budget=ModelProviderBudget(),\n    )\n\n    _settings: GroqSettings\n    _configuration: ModelProviderConfiguration\n    _credentials: GroqCredentials\n    _budget: ModelProviderBudget\n\n    def __init__(\n        self,\n        settings: Optional[GroqSettings] = None,\n        logger: Optional[logging.Logger] = None,\n    ):\n        super(GroqProvider, self).__init__(settings=settings, logger=logger)\n\n        from groq import AsyncGroq\n\n        self._client = AsyncGroq(\n            **self._credentials.get_api_access_kwargs()  # type: ignore\n        )\n\n    def get_tokenizer(self, model_name: GroqModelName) -> ModelTokenizer[Any]:\n        # HACK: No official tokenizer is available for Groq\n        return tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n", "forge/forge/llm/providers/__init__.py": "from .multi import (\n    CHAT_MODELS,\n    ChatModelProvider,\n    EmbeddingModelProvider,\n    ModelName,\n    MultiProvider,\n)\nfrom .openai import (\n    OPEN_AI_CHAT_MODELS,\n    OPEN_AI_EMBEDDING_MODELS,\n    OPEN_AI_MODELS,\n    OpenAIModelName,\n    OpenAIProvider,\n    OpenAISettings,\n)\nfrom .schema import (\n    AssistantChatMessage,\n    AssistantChatMessageDict,\n    AssistantFunctionCall,\n    AssistantFunctionCallDict,\n    ChatMessage,\n    ChatModelInfo,\n    ChatModelResponse,\n    CompletionModelFunction,\n    Embedding,\n    EmbeddingModelInfo,\n    EmbeddingModelResponse,\n    ModelInfo,\n    ModelProviderBudget,\n    ModelProviderCredentials,\n    ModelProviderName,\n    ModelProviderService,\n    ModelProviderSettings,\n    ModelProviderUsage,\n    ModelResponse,\n    ModelTokenizer,\n)\nfrom .utils import function_specs_from_commands\n\n__all__ = [\n    \"AssistantChatMessage\",\n    \"AssistantChatMessageDict\",\n    \"AssistantFunctionCall\",\n    \"AssistantFunctionCallDict\",\n    \"ChatMessage\",\n    \"ChatModelInfo\",\n    \"ChatModelResponse\",\n    \"CompletionModelFunction\",\n    \"CHAT_MODELS\",\n    \"Embedding\",\n    \"EmbeddingModelInfo\",\n    \"EmbeddingModelProvider\",\n    \"EmbeddingModelResponse\",\n    \"ModelInfo\",\n    \"ModelName\",\n    \"ChatModelProvider\",\n    \"ModelProviderBudget\",\n    \"ModelProviderCredentials\",\n    \"ModelProviderName\",\n    \"ModelProviderService\",\n    \"ModelProviderSettings\",\n    \"ModelProviderUsage\",\n    \"ModelResponse\",\n    \"ModelTokenizer\",\n    \"MultiProvider\",\n    \"OPEN_AI_MODELS\",\n    \"OPEN_AI_CHAT_MODELS\",\n    \"OPEN_AI_EMBEDDING_MODELS\",\n    \"OpenAIModelName\",\n    \"OpenAIProvider\",\n    \"OpenAISettings\",\n    \"function_specs_from_commands\",\n]\n", "forge/forge/logging/filters.py": "import logging\n\n\nclass BelowLevelFilter(logging.Filter):\n    \"\"\"Filter for logging levels below a certain threshold.\"\"\"\n\n    def __init__(self, below_level: int):\n        super().__init__()\n        self.below_level = below_level\n\n    def filter(self, record: logging.LogRecord):\n        return record.levelno < self.below_level\n", "forge/forge/logging/config.py": "\"\"\"Logging module for Auto-GPT.\"\"\"\nfrom __future__ import annotations\n\nimport enum\nimport logging\nimport os\nimport sys\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Optional\n\nfrom openai._base_client import log as openai_logger\n\nfrom forge.models.config import SystemConfiguration, UserConfigurable\n\nif TYPE_CHECKING:\n    from forge.speech import TTSConfig\n\nfrom .filters import BelowLevelFilter\nfrom .formatters import ForgeFormatter, StructuredLoggingFormatter\nfrom .handlers import TTSHandler\n\nLOG_DIR = Path(__file__).parent.parent.parent / \"logs\"\nLOG_FILE = \"activity.log\"\nDEBUG_LOG_FILE = \"debug.log\"\nERROR_LOG_FILE = \"error.log\"\n\nSIMPLE_LOG_FORMAT = \"%(asctime)s %(levelname)s  %(title)s%(message)s\"\nDEBUG_LOG_FORMAT = (\n    \"%(asctime)s %(levelname)s %(filename)s:%(lineno)d\" \"  %(title)s%(message)s\"\n)\n\nSPEECH_OUTPUT_LOGGER = \"VOICE\"\nUSER_FRIENDLY_OUTPUT_LOGGER = \"USER_FRIENDLY_OUTPUT\"\n\n\nclass LogFormatName(str, enum.Enum):\n    SIMPLE = \"simple\"\n    DEBUG = \"debug\"\n    STRUCTURED = \"structured_google_cloud\"\n\n\nTEXT_LOG_FORMAT_MAP = {\n    LogFormatName.DEBUG: DEBUG_LOG_FORMAT,\n    LogFormatName.SIMPLE: SIMPLE_LOG_FORMAT,\n}\n\n\nclass LoggingConfig(SystemConfiguration):\n    level: int = UserConfigurable(\n        default=logging.INFO,\n        from_env=lambda: logging.getLevelName(os.getenv(\"LOG_LEVEL\", \"INFO\")),\n    )\n\n    # Console output\n    log_format: LogFormatName = UserConfigurable(\n        default=LogFormatName.SIMPLE, from_env=\"LOG_FORMAT\"\n    )\n    plain_console_output: bool = UserConfigurable(\n        default=False,\n        from_env=lambda: os.getenv(\"PLAIN_OUTPUT\", \"False\") == \"True\",\n    )\n\n    # File output\n    log_dir: Path = LOG_DIR\n    log_file_format: Optional[LogFormatName] = UserConfigurable(\n        default=LogFormatName.SIMPLE,\n        from_env=lambda: os.getenv(  # type: ignore\n            \"LOG_FILE_FORMAT\", os.getenv(\"LOG_FORMAT\", \"simple\")\n        ),\n    )\n\n\ndef configure_logging(\n    debug: bool = False,\n    level: Optional[int | str] = None,\n    log_dir: Optional[Path] = None,\n    log_format: Optional[LogFormatName | str] = None,\n    log_file_format: Optional[LogFormatName | str] = None,\n    plain_console_output: Optional[bool] = None,\n    config: Optional[LoggingConfig] = None,\n    tts_config: Optional[TTSConfig] = None,\n) -> None:\n    \"\"\"Configure the native logging module, based on the environment config and any\n    specified overrides.\n\n    Arguments override values specified in the environment.\n    Overrides are also applied to `config`, if passed.\n\n    Should be usable as `configure_logging(**config.logging.dict())`, where\n    `config.logging` is a `LoggingConfig` object.\n    \"\"\"\n    if debug and level:\n        raise ValueError(\"Only one of either 'debug' and 'level' arguments may be set\")\n\n    # Parse arguments\n    if isinstance(level, str):\n        if type(_level := logging.getLevelName(level.upper())) is int:\n            level = _level\n        else:\n            raise ValueError(f\"Unknown log level '{level}'\")\n    if isinstance(log_format, str):\n        if log_format in LogFormatName._value2member_map_:\n            log_format = LogFormatName(log_format)\n        elif not isinstance(log_format, LogFormatName):\n            raise ValueError(f\"Unknown log format '{log_format}'\")\n    if isinstance(log_file_format, str):\n        if log_file_format in LogFormatName._value2member_map_:\n            log_file_format = LogFormatName(log_file_format)\n        elif not isinstance(log_file_format, LogFormatName):\n            raise ValueError(f\"Unknown log format '{log_format}'\")\n\n    config = config or LoggingConfig.from_env()\n\n    # Aggregate env config + arguments\n    config.level = logging.DEBUG if debug else level or config.level\n    config.log_dir = log_dir or config.log_dir\n    config.log_format = log_format or (\n        LogFormatName.DEBUG if debug else config.log_format\n    )\n    config.log_file_format = log_file_format or log_format or config.log_file_format\n    config.plain_console_output = (\n        plain_console_output\n        if plain_console_output is not None\n        else config.plain_console_output\n    )\n\n    # Structured logging is used for cloud environments,\n    # where logging to a file makes no sense.\n    if config.log_format == LogFormatName.STRUCTURED:\n        config.plain_console_output = True\n        config.log_file_format = None\n\n    # create log directory if it doesn't exist\n    if not config.log_dir.exists():\n        config.log_dir.mkdir()\n\n    log_handlers: list[logging.Handler] = []\n\n    if config.log_format in (LogFormatName.DEBUG, LogFormatName.SIMPLE):\n        console_format_template = TEXT_LOG_FORMAT_MAP[config.log_format]\n        console_formatter = ForgeFormatter(console_format_template)\n    else:\n        console_formatter = StructuredLoggingFormatter()\n        console_format_template = SIMPLE_LOG_FORMAT\n\n    # Console output handlers\n    stdout = logging.StreamHandler(stream=sys.stdout)\n    stdout.setLevel(config.level)\n    stdout.addFilter(BelowLevelFilter(logging.WARNING))\n    stdout.setFormatter(console_formatter)\n    stderr = logging.StreamHandler()\n    stderr.setLevel(logging.WARNING)\n    stderr.setFormatter(console_formatter)\n    log_handlers += [stdout, stderr]\n\n    # File output handlers\n    if config.log_file_format is not None:\n        if config.level < logging.ERROR:\n            file_output_format_template = TEXT_LOG_FORMAT_MAP[config.log_file_format]\n            file_output_formatter = ForgeFormatter(\n                file_output_format_template, no_color=True\n            )\n\n            # INFO log file handler\n            activity_log_handler = logging.FileHandler(\n                config.log_dir / LOG_FILE, \"a\", \"utf-8\"\n            )\n            activity_log_handler.setLevel(config.level)\n            activity_log_handler.setFormatter(file_output_formatter)\n            log_handlers += [activity_log_handler]\n\n        # ERROR log file handler\n        error_log_handler = logging.FileHandler(\n            config.log_dir / ERROR_LOG_FILE, \"a\", \"utf-8\"\n        )\n        error_log_handler.setLevel(logging.ERROR)\n        error_log_handler.setFormatter(ForgeFormatter(DEBUG_LOG_FORMAT, no_color=True))\n        log_handlers += [error_log_handler]\n\n    # Configure the root logger\n    logging.basicConfig(\n        format=console_format_template,\n        level=config.level,\n        handlers=log_handlers,\n    )\n\n    # Speech output\n    speech_output_logger = logging.getLogger(SPEECH_OUTPUT_LOGGER)\n    speech_output_logger.setLevel(logging.INFO)\n    if tts_config:\n        speech_output_logger.addHandler(TTSHandler(tts_config))\n    speech_output_logger.propagate = False\n\n    # JSON logger with better formatting\n    json_logger = logging.getLogger(\"JSON_LOGGER\")\n    json_logger.setLevel(logging.DEBUG)\n    json_logger.propagate = False\n\n    # Disable debug logging from OpenAI library\n    openai_logger.setLevel(logging.WARNING)\n", "forge/forge/logging/utils.py": "import logging\nimport re\nfrom typing import Any\n\nfrom colorama import Fore\n\n\ndef remove_color_codes(s: str) -> str:\n    return re.sub(r\"\\x1B(?:[@-Z\\\\-_]|\\[[0-?]*[ -/]*[@-~])\", \"\", s)\n\n\ndef fmt_kwargs(kwargs: dict) -> str:\n    return \", \".join(f\"{n}={repr(v)}\" for n, v in kwargs.items())\n\n\ndef print_attribute(\n    title: str, value: Any, title_color: str = Fore.GREEN, value_color: str = \"\"\n) -> None:\n    logger = logging.getLogger()\n    logger.info(\n        str(value),\n        extra={\n            \"title\": f\"{title.rstrip(':')}:\",\n            \"title_color\": title_color,\n            \"color\": value_color,\n        },\n    )\n\n\ndef speak(message: str, level: int = logging.INFO) -> None:\n    from .config import SPEECH_OUTPUT_LOGGER\n\n    logging.getLogger(SPEECH_OUTPUT_LOGGER).log(level, message)\n", "forge/forge/logging/formatters.py": "import logging\n\nfrom colorama import Fore, Style\nfrom google.cloud.logging_v2.handlers import CloudLoggingFilter, StructuredLogHandler\n\nfrom .utils import remove_color_codes\n\n\nclass FancyConsoleFormatter(logging.Formatter):\n    \"\"\"\n    A custom logging formatter designed for console output.\n\n    This formatter enhances the standard logging output with color coding. The color\n    coding is based on the level of the log message, making it easier to distinguish\n    between different types of messages in the console output.\n\n    The color for each level is defined in the LEVEL_COLOR_MAP class attribute.\n    \"\"\"\n\n    # level -> (level & text color, title color)\n    LEVEL_COLOR_MAP = {\n        logging.DEBUG: Fore.LIGHTBLACK_EX,\n        logging.INFO: Fore.BLUE,\n        logging.WARNING: Fore.YELLOW,\n        logging.ERROR: Fore.RED,\n        logging.CRITICAL: Fore.RED + Style.BRIGHT,\n    }\n\n    def format(self, record: logging.LogRecord) -> str:\n        # Make sure `msg` is a string\n        if not hasattr(record, \"msg\"):\n            record.msg = \"\"\n        elif not type(record.msg) is str:\n            record.msg = str(record.msg)\n\n        # Determine default color based on error level\n        level_color = \"\"\n        if record.levelno in self.LEVEL_COLOR_MAP:\n            level_color = self.LEVEL_COLOR_MAP[record.levelno]\n            record.levelname = f\"{level_color}{record.levelname}{Style.RESET_ALL}\"\n\n        # Determine color for message\n        color = getattr(record, \"color\", level_color)\n        color_is_specified = hasattr(record, \"color\")\n\n        # Don't color INFO messages unless the color is explicitly specified.\n        if color and (record.levelno != logging.INFO or color_is_specified):\n            record.msg = f\"{color}{record.msg}{Style.RESET_ALL}\"\n\n        return super().format(record)\n\n\nclass ForgeFormatter(FancyConsoleFormatter):\n    def __init__(self, *args, no_color: bool = False, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.no_color = no_color\n\n    def format(self, record: logging.LogRecord) -> str:\n        # Make sure `msg` is a string\n        if not hasattr(record, \"msg\"):\n            record.msg = \"\"\n        elif not type(record.msg) is str:\n            record.msg = str(record.msg)\n\n        # Strip color from the message to prevent color spoofing\n        if record.msg and not getattr(record, \"preserve_color\", False):\n            record.msg = remove_color_codes(record.msg)\n\n        # Determine color for title\n        title = getattr(record, \"title\", \"\")\n        title_color = getattr(record, \"title_color\", \"\") or self.LEVEL_COLOR_MAP.get(\n            record.levelno, \"\"\n        )\n        if title and title_color:\n            title = f\"{title_color + Style.BRIGHT}{title}{Style.RESET_ALL}\"\n        # Make sure record.title is set, and padded with a space if not empty\n        record.title = f\"{title} \" if title else \"\"\n\n        if self.no_color:\n            return remove_color_codes(super().format(record))\n        else:\n            return super().format(record)\n\n\nclass StructuredLoggingFormatter(StructuredLogHandler, logging.Formatter):\n    def __init__(self):\n        # Set up CloudLoggingFilter to add diagnostic info to the log records\n        self.cloud_logging_filter = CloudLoggingFilter()\n\n        # Init StructuredLogHandler\n        super().__init__()\n\n    def format(self, record: logging.LogRecord) -> str:\n        self.cloud_logging_filter.filter(record)\n        return super().format(record)\n", "forge/forge/logging/handlers.py": "from __future__ import annotations\n\nimport json\nimport logging\nfrom typing import TYPE_CHECKING\n\nfrom forge.logging.utils import remove_color_codes\nfrom forge.speech import TextToSpeechProvider\n\nif TYPE_CHECKING:\n    from forge.speech import TTSConfig\n\n\nclass TTSHandler(logging.Handler):\n    \"\"\"Output messages to the configured TTS engine (if any)\"\"\"\n\n    def __init__(self, config: TTSConfig):\n        super().__init__()\n        self.config = config\n        self.tts_provider = TextToSpeechProvider(config)\n\n    def format(self, record: logging.LogRecord) -> str:\n        if getattr(record, \"title\", \"\"):\n            msg = f\"{getattr(record, 'title')} {record.msg}\"\n        else:\n            msg = f\"{record.msg}\"\n\n        return remove_color_codes(msg)\n\n    def emit(self, record: logging.LogRecord) -> None:\n        if not self.config.speak_mode:\n            return\n\n        message = self.format(record)\n        self.tts_provider.say(message)\n\n\nclass JsonFileHandler(logging.FileHandler):\n    def format(self, record: logging.LogRecord) -> str:\n        record.json_data = json.loads(record.getMessage())\n        return json.dumps(getattr(record, \"json_data\"), ensure_ascii=False, indent=4)\n\n    def emit(self, record: logging.LogRecord) -> None:\n        with open(self.baseFilename, \"w\", encoding=\"utf-8\") as f:\n            f.write(self.format(record))\n", "forge/forge/logging/__init__.py": "from .config import configure_logging\nfrom .filters import BelowLevelFilter\nfrom .formatters import FancyConsoleFormatter\n\n__all__ = [\n    \"configure_logging\",\n    \"BelowLevelFilter\",\n    \"FancyConsoleFormatter\",\n]\n", "rnd/autogpt_server/setup.py": "from pkgutil import iter_modules\nfrom shutil import which\n\nfrom cx_Freeze import Executable, setup\n\npackages = [\n    m.name\n    for m in iter_modules()\n    if m.ispkg and m.module_finder and \"poetry\" in m.module_finder.path  # type: ignore\n]\npackages.append(\"collections\")\n\n# if mac use the icns file, otherwise use the ico file\nicon = (\n    \"../../assets/gpt_dark_RGB.icns\"\n    if which(\"sips\")\n    else \"../../assets/gpt_dark_RGB.ico\"\n)\n\nsetup(\n    name=\"AutoGPT Server\",\n    url=\"https://agpt.co\",\n    # The entry points of the application\n    executables=[\n        Executable(\n            \"autogpt_server/app.py\",\n            target_name=\"agpt_server\",\n            base=\"console\",\n            icon=icon,\n        ),\n        Executable(\n            \"autogpt_server/cli.py\",\n            target_name=\"agpt_server_cli\",\n            base=\"console\",\n            icon=icon,\n        ),\n    ],\n    options={\n        # Options for building all the executables\n        \"build_exe\": {\n            \"packages\": packages,\n            \"includes\": [\n                \"autogpt_server\",\n                \"uvicorn.loops.auto\",\n                \"uvicorn.protocols.http.auto\",\n                \"uvicorn.protocols.websockets.auto\",\n                \"uvicorn.lifespan.on\",\n            ],\n            # Exclude the two module from readability.compat as it causes issues\n            \"excludes\": [\"readability.compat.two\"],\n        },\n        # Mac .app specific options\n        \"bdist_mac\": {\n            \"bundle_name\": \"AutoGPT\",\n            \"iconfile\": \"../../assets/gpt_dark_RGB.icns\",\n            # \"include_resources\": [\"IMG_3775.jpeg\"],\n        },\n        # Mac .dmg specific options\n        \"bdist_dmg\": {\n            \"applications_shortcut\": True,\n            \"volume_label\": \"AutoGPTServer\",\n        },\n        # Windows .msi specific options\n        \"bdist_msi\": {\n            \"target_name\": \"AutoGPTServer\",\n            \"add_to_path\": True,\n            \"install_icon\": \"../../assets/gpt_dark_RGB.ico\",\n        },\n        # Linux .appimage specific options\n        \"bdist_appimage\": {},\n        # Linux rpm specific options\n        \"bdist_rpm\": {\n            \"name\": \"AutoGPTServer\",\n            \"description\": \"AutoGPT Server\",\n            \"version\": \"0.1\",\n            \"license\": \"UNKNOWNORPROPRIETARY\",\n            \"url\": \"https://agpt.co\",\n            \"long_description\": \"AutoGPT Server\",\n        },\n    },\n)\n", "rnd/autogpt_server/autogpt_server/cli.py": "\"\"\"\nThe command line interface for the agent server\n\"\"\"\n\nimport os\nimport pathlib\n\nimport click\nimport psutil\n\nfrom autogpt_server import app\nfrom autogpt_server.util.process import AppProcess\n\n\ndef get_pid_path() -> pathlib.Path:\n    home_dir = pathlib.Path.home()\n    new_dir = home_dir / \".config\" / \"agpt\"\n    file_path = new_dir / \"running.tmp\"\n    return file_path\n\n\ndef get_pid() -> int | None:\n    file_path = get_pid_path()\n    if not file_path.exists():\n        return None\n\n    os.makedirs(file_path.parent, exist_ok=True)\n    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n        pid = file.read()\n    try:\n        return int(pid)\n    except ValueError:\n        return None\n\n\ndef write_pid(pid: int):\n    file_path = get_pid_path()\n    os.makedirs(file_path.parent, exist_ok=True)\n    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n        file.write(str(pid))\n\n\nclass MainApp(AppProcess):\n    def run(self):\n        app.main(silent=True)\n\n\n@click.group()\ndef main():\n    \"\"\"AutoGPT Server CLI Tool\"\"\"\n    pass\n\n\n@main.command()\ndef start():\n    \"\"\"\n    Starts the server in the background and saves the PID\n    \"\"\"\n    # Define the path for the new directory and file\n    pid = get_pid()\n    if pid and psutil.pid_exists(pid):\n        print(\"Server is already running\")\n        exit(1)\n    elif pid:\n        print(\"PID does not exist deleting file\")\n        os.remove(get_pid_path())\n\n    print(\"Starting server\")\n    pid = MainApp().start(background=True, silent=True)\n    print(f\"Server running in process: {pid}\")\n\n    write_pid(pid)\n    print(\"done\")\n    os._exit(status=0)\n\n\n@main.command()\ndef stop():\n    \"\"\"\n    Stops the server\n    \"\"\"\n    pid = get_pid()\n    if not pid:\n        print(\"Server is not running\")\n        return\n\n    os.remove(get_pid_path())\n    process = psutil.Process(int(pid))\n    for child in process.children(recursive=True):\n        child.terminate()\n    process.terminate()\n\n    print(\"Server Stopped\")\n\n\n@click.group()\ndef test():\n    \"\"\"\n    Group for test commands\n    \"\"\"\n    pass\n\n\n@test.command()\ndef event():\n    \"\"\"\n    Send an event to the running server\n    \"\"\"\n    print(\"Event sent\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "rnd/autogpt_server/autogpt_server/app.py": "from autogpt_server.executor import ExecutionManager, ExecutionScheduler\nfrom autogpt_server.server import AgentServer\nfrom autogpt_server.util.process import AppProcess\nfrom autogpt_server.util.service import PyroNameServer\n\n\ndef run_processes(processes: list[AppProcess], **kwargs):\n    \"\"\"\n    Execute all processes in the app. The last process is run in the foreground.\n    \"\"\"\n    try:\n        for process in processes[:-1]:\n            process.start(background=True, **kwargs)\n        processes[-1].start(background=False, **kwargs)\n    except Exception as e:\n        for process in processes:\n            process.stop()\n        raise e\n\n\ndef main(**kwargs):\n    run_processes(\n        [\n            PyroNameServer(),\n            ExecutionManager(pool_size=5),\n            ExecutionScheduler(),\n            AgentServer(),\n        ],\n        **kwargs\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n", "rnd/autogpt_server/autogpt_server/server/__init__.py": "from .server import AgentServer\n\n__all__ = [\"AgentServer\"]\n", "rnd/autogpt_server/autogpt_server/server/server.py": "import uuid\nimport uvicorn\n\nfrom contextlib import asynccontextmanager\nfrom fastapi import APIRouter, FastAPI, HTTPException\n\nfrom autogpt_server.data import db, execution, graph, block\nfrom autogpt_server.executor import ExecutionManager, ExecutionScheduler\nfrom autogpt_server.util.process import AppProcess\nfrom autogpt_server.util.service import get_service_client\n\n\nclass AgentServer(AppProcess):\n\n    @asynccontextmanager\n    async def lifespan(self, _: FastAPI):\n        await db.connect()\n        yield\n        await db.disconnect()\n\n    def run(self):\n        app = FastAPI(\n            title=\"AutoGPT Agent Server\",\n            description=(\n                \"This server is used to execute agents that are created by the \"\n                \"AutoGPT system.\"\n            ),\n            summary=\"AutoGPT Agent Server\",\n            version=\"0.1\",\n            lifespan=self.lifespan,\n        )\n\n        # Define the API routes\n        router = APIRouter()\n        router.add_api_route(\n            path=\"/blocks\",\n            endpoint=self.get_agent_blocks,\n            methods=[\"GET\"],\n        )\n        router.add_api_route(\n            path=\"/agents\",\n            endpoint=self.get_agents,\n            methods=[\"GET\"],\n        )\n        router.add_api_route(\n            path=\"/agents/{agent_id}\",\n            endpoint=self.get_agent,\n            methods=[\"GET\"],\n        )\n        router.add_api_route(\n            path=\"/agents\",\n            endpoint=self.create_agent,\n            methods=[\"POST\"],\n        )\n        router.add_api_route(\n            path=\"/agents/{agent_id}/execute\",\n            endpoint=self.execute_agent,\n            methods=[\"POST\"],\n        )\n        router.add_api_route(\n            path=\"/agents/{agent_id}/executions/{run_id}\",\n            endpoint=self.get_executions,\n            methods=[\"GET\"],\n        )\n        router.add_api_route(\n            path=\"/agents/{agent_id}/schedules\",\n            endpoint=self.schedule_agent,\n            methods=[\"POST\"],\n        )\n        router.add_api_route(\n            path=\"/agents/{agent_id}/schedules\",\n            endpoint=self.get_execution_schedules,\n            methods=[\"GET\"],\n        )\n        router.add_api_route(\n            path=\"/agents/schedules/{schedule_id}\",\n            endpoint=self.update_schedule,\n            methods=[\"PUT\"],\n        )\n\n        app.include_router(router)\n        uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n\n    @property\n    def execution_manager_client(self) -> ExecutionManager:\n        return get_service_client(ExecutionManager)\n\n    @property\n    def execution_scheduler_client(self) -> ExecutionScheduler:\n        return get_service_client(ExecutionScheduler)\n\n    async def get_agent_blocks(self) -> list[dict]:\n        return [v.to_dict() for v in await block.get_blocks()]\n\n    async def get_agents(self) -> list[str]:\n        return await graph.get_graph_ids()\n\n    async def get_agent(self, agent_id: str) -> graph.Graph:\n        agent = await graph.get_graph(agent_id)\n        if not agent:\n            raise HTTPException(status_code=404, detail=f\"Agent #{agent_id} not found.\")\n\n        return agent\n\n    async def create_agent(self, agent: graph.Graph) -> graph.Graph:\n        agent.id = str(uuid.uuid4())\n\n        id_map = {node.id: str(uuid.uuid4()) for node in agent.nodes}\n        for node in agent.nodes:\n            node.id = id_map[node.id]\n            node.input_nodes = {k: id_map[v] for k, v in node.input_nodes.items()}\n            node.output_nodes = {k: id_map[v] for k, v in node.output_nodes.items()}\n\n        return await graph.create_graph(agent)\n\n    async def execute_agent(self, agent_id: str, node_input: dict) -> dict:\n        try:\n            return self.execution_manager_client.add_execution(agent_id, node_input)\n        except Exception as e:\n            msg = e.__str__().encode().decode('unicode_escape')\n            raise HTTPException(status_code=400, detail=msg)\n\n    async def get_executions(\n            self, agent_id: str, run_id: str) -> list[execution.ExecutionResult]:\n        agent = await graph.get_graph(agent_id)\n        if not agent:\n            raise HTTPException(status_code=404, detail=f\"Agent #{agent_id} not found.\")\n\n        return await execution.get_executions(run_id)\n\n    async def schedule_agent(self, agent_id: str, cron: str, input_data: dict) -> dict:\n        agent = await graph.get_graph(agent_id)\n        if not agent:\n            raise HTTPException(status_code=404, detail=f\"Agent #{agent_id} not found.\")\n        execution_scheduler = self.execution_scheduler_client\n        return {\n            \"id\": execution_scheduler.add_execution_schedule(agent_id, cron, input_data)\n        }\n\n    def update_schedule(self, schedule_id: str, input_data: dict) -> dict:\n        execution_scheduler = self.execution_scheduler_client\n        is_enabled = input_data.get(\"is_enabled\", False)\n        execution_scheduler.update_schedule(schedule_id, is_enabled)\n        return {\"id\": schedule_id}\n\n    def get_execution_schedules(self, agent_id: str) -> dict[str, str]:\n        execution_scheduler = self.execution_scheduler_client\n        return execution_scheduler.get_execution_schedules(agent_id)\n", "rnd/autogpt_server/autogpt_server/executor/manager.py": "import asyncio\nimport logging\nimport uuid\nfrom concurrent.futures import ProcessPoolExecutor\nfrom typing import Optional, Any\n\nfrom autogpt_server.data import db\nfrom autogpt_server.data.block import Block, get_block\nfrom autogpt_server.data.graph import Node, get_node, get_node_input, get_graph\nfrom autogpt_server.data.execution import (\n    Execution,\n    ExecutionQueue,\n    enqueue_execution,\n    complete_execution,\n    fail_execution,\n    start_execution,\n)\nfrom autogpt_server.util.service import AppService, expose\n\nlogger = logging.getLogger(__name__)\n\n\ndef get_log_prefix(run_id: str, exec_id: str, block_name: str = \"-\"):\n    return f\"[ExecutionManager] [graph-{run_id}|node-{exec_id}|{block_name}]\"\n\n\ndef execute_node(loop: asyncio.AbstractEventLoop, data: Execution) -> Execution | None:\n    \"\"\"\n    Execute a node in the graph. This will trigger a block execution on a node,\n    persist the execution result, and return the subsequent node to be executed.\n\n    Args:\n        loop: The event loop to run the async functions.\n        data: The execution data for executing the current node.\n\n    Returns:\n        The subsequent node to be enqueued, or None if there is no subsequent node.\n    \"\"\"\n    run_id = data.run_id\n    exec_id = data.id\n    exec_data = data.data\n    node_id = data.node_id\n\n    asyncio.set_event_loop(loop)\n    wait = lambda f: loop.run_until_complete(f)\n\n    node: Optional[Node] = wait(get_node(node_id))\n    if not node:\n        logger.error(f\"Node {node_id} not found.\")\n        return None\n\n    node_block: Optional[Block] = wait(get_block(node.block_id))\n    if not node_block:\n        logger.error(f\"Block {node.block_id} not found.\")\n        return None\n\n    # Execute the node\n    prefix = get_log_prefix(run_id, exec_id, node_block.name)\n    logger.warning(f\"{prefix} execute with input:\\n`{exec_data}`\")\n    wait(start_execution(exec_id))\n\n    try:\n        output_name, output_data = node_block.execute(exec_data)\n        logger.warning(f\"{prefix} executed with output [{output_name}]:`{output_data}`\")\n        wait(complete_execution(exec_id, (output_name, output_data)))\n    except Exception as e:\n        logger.exception(f\"{prefix} failed with error: %s\", e)\n        wait(fail_execution(exec_id, e))\n        raise e\n\n    # Try to enqueue next eligible nodes\n    if output_name not in node.output_nodes:\n        logger.error(f\"{prefix} Output [{output_name}] has no subsequent node.\")\n        return None\n\n    next_node_id = node.output_nodes[output_name]\n    next_node: Optional[Node] = wait(get_node(next_node_id))\n    if not next_node:\n        logger.error(f\"{prefix} Error, next node {next_node_id} not found.\")\n        return None\n\n    next_node_input: dict[str, Any] = wait(get_node_input(next_node, run_id))\n    is_valid, validation_resp = wait(validate_exec(next_node, next_node_input))\n    if not is_valid:\n        logger.warning(f\"{prefix} Skipped {next_node_id}: {validation_resp}\")\n        return None\n\n    logger.warning(f\"{prefix} Enqueue next node {next_node_id}-{validation_resp}\")\n    return Execution(run_id=run_id, node_id=next_node_id, data=next_node_input)\n\n\nasync def validate_exec(node: Node, data: dict[str, Any]) -> tuple[bool, str]:\n    \"\"\"\n    Validate the input data for a node execution.\n\n    Args:\n        node: The node to execute.\n        data: The input data for the node execution.\n\n    Returns:\n        A tuple of a boolean indicating if the data is valid, and a message if not.\n        Return the executed block name if the data is valid.\n    \"\"\"\n    node_block: Block | None = await(get_block(node.block_id))\n    if not node_block:\n        return False, f\"Block for {node.block_id} not found.\"\n\n    if not set(node.input_nodes).issubset(data):\n        return False, f\"Input data missing: {set(node.input_nodes) - set(data)}\"\n\n    if error := node_block.input_schema.validate_data(data):\n        return False, f\"Input data doesn't match {node_block.name}: {error}\"\n\n    return True, node_block.name\n\n\nclass Executor:\n    loop: asyncio.AbstractEventLoop\n\n    @classmethod\n    def on_executor_start(cls):\n        cls.loop = asyncio.new_event_loop()\n        cls.loop.run_until_complete(db.connect())\n\n    @classmethod\n    def on_start_execution(cls, data: Execution) -> Optional[Execution | None]:\n        \"\"\"\n        A synchronous version of `execute_node`, to be used in the ProcessPoolExecutor.\n        \"\"\"\n        prefix = get_log_prefix(data.run_id, data.id)\n        try:\n            logger.warning(f\"{prefix} Start execution\")\n            return execute_node(cls.loop, data)\n        except Exception as e:\n            logger.error(f\"{prefix} Error: {e}\")\n\n\nclass ExecutionManager(AppService):\n\n    def __init__(self, pool_size: int):\n        self.pool_size = pool_size\n        self.queue = ExecutionQueue()\n\n    def run_service(self):\n        def on_complete_execution(f: asyncio.Future[Execution | None]):\n            exception = f.exception()\n            if exception:\n                logger.exception(\"Error during execution!! %s\", exception)\n                return exception\n\n            execution = f.result()\n            if execution:\n                return self.add_node_execution(execution)\n\n            return None\n\n        with ProcessPoolExecutor(\n                max_workers=self.pool_size,\n                initializer=Executor.on_executor_start,\n        ) as executor:\n            logger.warning(f\"Execution manager started with {self.pool_size} workers.\")\n            while True:\n                future = executor.submit(\n                    Executor.on_start_execution,\n                    self.queue.get()\n                )\n                future.add_done_callback(on_complete_execution)  # type: ignore\n\n    @expose\n    def add_execution(self, graph_id: str, data: dict[str, Any]) -> dict:\n        run_id = str(uuid.uuid4())\n\n        agent = self.run_and_wait(get_graph(graph_id))\n        if not agent:\n            raise Exception(f\"Agent #{graph_id} not found.\")\n\n        # Currently, there is no constraint on the number of root nodes in the graph.\n        for node in agent.starting_nodes:\n            valid, error = self.run_and_wait(validate_exec(node, data))\n            if not valid:\n                raise Exception(error)\n\n        executions = []\n        for node in agent.starting_nodes:\n            exec_id = self.add_node_execution(\n                Execution(run_id=run_id, node_id=node.id, data=data)\n            )\n            executions.append({\n                \"exec_id\": exec_id,\n                \"node_id\": node.id,\n            })\n\n        return {\n            \"run_id\": run_id,\n            \"executions\": executions,\n        }\n\n    def add_node_execution(self, execution: Execution) -> Execution:\n        self.run_and_wait(enqueue_execution(execution))\n        return self.queue.add(execution)\n", "rnd/autogpt_server/autogpt_server/executor/scheduler.py": "import logging\nimport time\n\nfrom apscheduler.schedulers.background import BackgroundScheduler\nfrom apscheduler.triggers.cron import CronTrigger\nfrom datetime import datetime\n\nfrom autogpt_server.data import schedule as model\nfrom autogpt_server.util.service import AppService, expose, get_service_client\nfrom autogpt_server.executor.manager import ExecutionManager\n\nlogger = logging.getLogger(__name__)\n\n\ndef log(msg, **kwargs):\n    logger.warning(\"[ExecutionScheduler] \" + msg, **kwargs)\n\n\nclass ExecutionScheduler(AppService):\n\n    def __init__(self, refresh_interval=10):\n        self.last_check = datetime.min\n        self.refresh_interval = refresh_interval\n\n    @property\n    def execution_manager_client(self):\n        return get_service_client(ExecutionManager)\n\n    def run_service(self):\n        scheduler = BackgroundScheduler()\n        scheduler.start()\n        while True:\n            self.__refresh_jobs_from_db(scheduler)\n            time.sleep(self.refresh_interval)\n\n    def __refresh_jobs_from_db(self, scheduler: BackgroundScheduler):\n        schedules = self.run_and_wait(model.get_active_schedules(self.last_check))\n        for schedule in schedules:\n            self.last_check = max(self.last_check, schedule.last_updated)\n\n            if not schedule.is_enabled:\n                log(f\"Removing recurring job {schedule.id}: {schedule.schedule}\")\n                scheduler.remove_job(schedule.id)\n                continue\n\n            log(f\"Adding recurring job {schedule.id}: {schedule.schedule}\")\n            scheduler.add_job(\n                self.__execute_agent,\n                CronTrigger.from_crontab(schedule.schedule),\n                id=schedule.id,\n                args=[schedule.agent_id, schedule.input_data],\n                replace_existing=True,\n            )\n\n    def __execute_agent(self, agent_id: str, input_data: dict):\n        try:\n            log(f\"Executing recurring job for agent #{agent_id}\")\n            execution_manager = self.execution_manager_client\n            execution_manager.add_execution(agent_id, input_data)\n        except Exception as e:\n            logger.error(f\"Error executing agent {agent_id}: {e}\")\n\n    @expose\n    def update_schedule(self, schedule_id: str, is_enabled: bool) -> str:\n        self.run_and_wait(model.update_schedule(schedule_id, is_enabled))\n        return schedule_id\n\n    @expose\n    def add_execution_schedule(self, agent_id: str, cron: str, input_data: dict) -> str:\n        schedule = model.ExecutionSchedule(\n            agent_id=agent_id,\n            schedule=cron,\n            input_data=input_data,\n        )\n        self.run_and_wait(model.add_schedule(schedule))\n        return schedule.id\n\n    @expose\n    def get_execution_schedules(self, agent_id: str) -> dict[str, str]:\n        query = model.get_schedules(agent_id)\n        schedules: list[model.ExecutionSchedule] = self.run_and_wait(query)\n        return {v.id: v.schedule for v in schedules}\n", "rnd/autogpt_server/autogpt_server/executor/__init__.py": "from .manager import ExecutionManager\nfrom .scheduler import ExecutionScheduler\n\n__all__ = [\n    \"ExecutionManager\",\n    \"ExecutionScheduler\",\n]\n\n", "rnd/autogpt_server/autogpt_server/data/execution.py": "import json\nfrom datetime import datetime\nfrom enum import Enum\nfrom multiprocessing import Queue\nfrom typing import Any\n\nfrom prisma.models import AgentNodeExecution\n\nfrom autogpt_server.data.db import BaseDbModel\n\n\nclass Execution(BaseDbModel):\n    \"\"\"Data model for an execution of an Agent\"\"\"\n\n    run_id: str\n    node_id: str\n    data: dict[str, Any]\n\n\nclass ExecutionStatus(str, Enum):\n    QUEUED = \"QUEUED\"\n    RUNNING = \"RUNNING\"\n    COMPLETED = \"COMPLETED\"\n    FAILED = \"FAILED\"\n\n\nclass ExecutionQueue:\n    \"\"\"\n    Queue for managing the execution of agents.\n    This will be shared between different processes\n    \"\"\"\n\n    def __init__(self):\n        self.queue: Queue[Execution] = Queue()\n\n    def add(self, execution: Execution) -> Execution:\n        self.queue.put(execution)\n        return execution\n\n    def get(self) -> Execution:\n        return self.queue.get()\n\n    def empty(self) -> bool:\n        return self.queue.empty()\n\n\nclass ExecutionResult(BaseDbModel):\n    run_id: str\n    execution_id: str\n    node_id: str\n    status: ExecutionStatus\n    input_data: dict[str, Any]\n    output_name: str\n    output_data: Any\n    creation_time: datetime\n    start_time: datetime | None\n    end_time: datetime | None\n\n    @staticmethod\n    def from_db(execution: AgentNodeExecution):\n        return ExecutionResult(\n            run_id=execution.executionId,\n            node_id=execution.agentNodeId,\n            execution_id=execution.id,\n            status=ExecutionStatus(execution.executionStatus),\n            input_data=json.loads(execution.inputData or \"{}\"),\n            output_name=execution.outputName or \"\",\n            output_data=json.loads(execution.outputData or \"{}\"),\n            creation_time=execution.creationTime,\n            start_time=execution.startTime,\n            end_time=execution.endTime,\n        )\n\n\n# --------------------- Model functions --------------------- #\n\n\nasync def enqueue_execution(execution: Execution) -> None:\n    await AgentNodeExecution.prisma().create(\n        data={\n            \"id\": execution.id,\n            \"executionId\": execution.run_id,\n            \"agentNodeId\": execution.node_id,\n            \"executionStatus\": ExecutionStatus.QUEUED,\n            \"inputData\": json.dumps(execution.data),\n            \"creationTime\": datetime.now(),\n        }\n    )\n\n\nasync def start_execution(exec_id: str) -> None:\n    await AgentNodeExecution.prisma().update(\n        where={\"id\": exec_id},\n        data={\n            \"executionStatus\": ExecutionStatus.RUNNING,\n            \"startTime\": datetime.now(),\n        },\n    )\n\n\nasync def complete_execution(exec_id: str, output: tuple[str, Any]) -> None:\n    output_name, output_data = output\n\n    await AgentNodeExecution.prisma().update(\n        where={\"id\": exec_id},\n        data={\n            \"executionStatus\": ExecutionStatus.COMPLETED,\n            \"outputName\": output_name,\n            \"outputData\": json.dumps(output_data),\n            \"endTime\": datetime.now(),\n        },\n    )\n\n\nasync def fail_execution(exec_id: str, error: Exception) -> None:\n    await AgentNodeExecution.prisma().update(\n        where={\"id\": exec_id},\n        data={\n            \"executionStatus\": ExecutionStatus.FAILED,\n            \"outputName\": \"error\",\n            \"outputData\": str(error),\n            \"endTime\": datetime.now(),\n        },\n    )\n\n\nasync def get_executions(run_id: str) -> list[ExecutionResult]:\n    executions = await AgentNodeExecution.prisma().find_many(\n        where={\"executionId\": run_id},\n        order={\"startTime\": \"asc\"},\n    )\n    res = [ExecutionResult.from_db(execution) for execution in executions]\n    return res\n", "rnd/autogpt_server/autogpt_server/data/block.py": "import json\nimport jsonschema\n\nfrom abc import ABC, abstractmethod\nfrom typing import Any, ClassVar\n\nfrom prisma.models import AgentBlock\nfrom pydantic import BaseModel\n\nBlockData = dict[str, Any]\n\n\nclass BlockSchema(BaseModel):\n    \"\"\"\n    A schema for the block input and output data.\n    The dictionary structure is an object-typed `jsonschema`.\n    The top-level properties are the block input/output names.\n\n    You can initialize this class by providing a dictionary of properties.\n    The key is the string of the property name, and the value is either\n    a string of the type or a dictionary of the jsonschema.\n\n    You can also provide additional keyword arguments for additional properties.\n    Like `name`, `required` (by default all properties are required), etc.\n\n    Example:\n    input_schema = BlockSchema({\n        \"system_prompt\": \"string\",\n        \"user_prompt\": \"string\",\n        \"max_tokens\": \"integer\",\n        \"user_info\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"name\": {\"type\": \"string\"},\n                \"age\": {\"type\": \"integer\"},\n            },\n            \"required\": [\"name\"],\n        },\n    }, required=[\"system_prompt\", \"user_prompt\"])\n\n    output_schema = BlockSchema({\n        \"on_complete\": \"string\",\n        \"on_failures\": \"string\",\n    })\n    \"\"\"\n\n    jsonschema: dict[str, Any]\n\n    def __init__(\n            self,\n            properties: dict[str, str | dict],\n            required: list[str] | None = None,\n            **kwargs: Any,\n    ):\n        schema = {\n            \"type\": \"object\",\n            \"properties\": {\n                key: {\"type\": value} if isinstance(value, str) else value\n                for key, value in properties.items()\n            },\n            \"required\": required or list(properties.keys()),\n            **kwargs,\n        }\n        super().__init__(jsonschema=schema)\n\n    def __str__(self) -> str:\n        return json.dumps(self.jsonschema)\n\n    def validate_data(self, data: BlockData) -> str | None:\n        \"\"\"\n        Validate the data against the schema.\n        Returns the validation error message if the data does not match the schema.\n        \"\"\"\n        try:\n            jsonschema.validate(data, self.jsonschema)\n            return None\n        except jsonschema.ValidationError as e:\n            return str(e)\n\n    def validate_field(self, field_name: str, data: BlockData) -> str | None:\n        \"\"\"\n        Validate the data against a specific property (one of the input/output name).\n        Returns the validation error message if the data does not match the schema.\n        \"\"\"\n        property_schema = self.jsonschema[\"properties\"].get(field_name)\n        if not property_schema:\n            return f\"Invalid property name {field_name}\"\n\n        try:\n            jsonschema.validate(data, property_schema)\n            return None\n        except jsonschema.ValidationError as e:\n            return str(e)\n\n\nclass Block(ABC, BaseModel):\n    @classmethod\n    @property\n    @abstractmethod\n    def id(cls) -> str:\n        \"\"\"\n        The unique identifier for the block, this value will be persisted in the DB.\n        So it should be a unique and constant across the application run.\n        Use the UUID format for the ID.\n        \"\"\"\n        pass\n\n    @classmethod\n    @property\n    @abstractmethod\n    def input_schema(cls) -> BlockSchema:\n        \"\"\"\n        The schema for the block input data.\n        The top-level properties are the possible input name expected by the block.\n        \"\"\"\n        pass\n\n    @classmethod\n    @property\n    @abstractmethod\n    def output_schema(cls) -> BlockSchema:\n        \"\"\"\n        The schema for the block output.\n        The top-level properties are the possible output name produced by the block.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def run(self, input_data: BlockData) -> tuple[str, Any]:\n        \"\"\"\n        Run the block with the given input data.\n        Args:\n            input_data: The input data with the structure of input_schema.\n        Returns:\n            The (output name, output data), matching the type in output_schema.\n        \"\"\"\n        pass\n\n    @classmethod\n    @property\n    def name(cls):\n        return cls.__name__\n\n    def to_dict(self):\n        return {\n            \"id\": self.id,\n            \"name\": self.name,\n            \"inputSchema\": self.input_schema.jsonschema,\n            \"outputSchema\": self.output_schema.jsonschema,\n        }\n\n    def execute(self, input_data: BlockData) -> tuple[str, Any]:\n        if error := self.input_schema.validate_data(input_data):\n            raise ValueError(\n                f\"Unable to execute block with invalid input data: {error}\"\n            )\n\n        output_name, output_data = self.run(input_data)\n\n        if error := self.output_schema.validate_field(output_name, output_data):\n            raise ValueError(\n                f\"Unable to execute block with invalid output data: {error}\"\n            )\n\n        return output_name, output_data\n\n\n# ===================== Inline-Block Implementations ===================== #\n\n\nclass ParrotBlock(Block):\n    id: ClassVar[str] = \"1ff065e9-88e8-4358-9d82-8dc91f622ba9\"  # type: ignore\n    input_schema: ClassVar[BlockSchema] = BlockSchema(  # type: ignore\n        {\n            \"input\": \"string\",\n        }\n    )\n    output_schema: ClassVar[BlockSchema] = BlockSchema(  # type: ignore\n        {\n            \"output\": \"string\",\n        }\n    )\n\n    def run(self, input_data: BlockData) -> tuple[str, Any]:\n        return \"output\", input_data[\"input\"]\n\n\nclass TextCombinerBlock(Block):\n    id: ClassVar[str] = \"db7d8f02-2f44-4c55-ab7a-eae0941f0c30\"  # type: ignore\n    input_schema: ClassVar[BlockSchema] = BlockSchema(  # type: ignore\n        {\n            \"text1\": \"string\",\n            \"text2\": \"string\",\n            \"format\": \"string\",\n        }\n    )\n    output_schema: ClassVar[BlockSchema] = BlockSchema(  # type: ignore\n        {\n            \"combined_text\": \"string\",\n        }\n    )\n\n    def run(self, input_data: BlockData) -> tuple[str, Any]:\n        return \"combined_text\", input_data[\"format\"].format(\n            text1=input_data[\"text1\"],\n            text2=input_data[\"text2\"],\n        )\n\n\nclass PrintingBlock(Block):\n    id: ClassVar[str] = \"f3b1c1b2-4c4f-4f0d-8d2f-4c4f0d8d2f4c\"  # type: ignore\n    input_schema: ClassVar[BlockSchema] = BlockSchema(  # type: ignore\n        {\n            \"text\": \"string\",\n        }\n    )\n    output_schema: ClassVar[BlockSchema] = BlockSchema(  # type: ignore\n        {\n            \"status\": \"string\",\n        }\n    )\n\n    def run(self, input_data: BlockData) -> tuple[str, Any]:\n        return \"status\", \"printed\"\n\n\n# ======================= Block Helper Functions ======================= #\n\nAVAILABLE_BLOCKS: dict[str, Block] = {}\n\n\nasync def initialize_blocks() -> None:\n    global AVAILABLE_BLOCKS\n\n    AVAILABLE_BLOCKS = {block.id: block() for block in Block.__subclasses__()}\n\n    for block in AVAILABLE_BLOCKS.values():\n        if await AgentBlock.prisma().find_unique(where={\"id\": block.id}):\n            continue\n\n        await AgentBlock.prisma().create(\n            data={\n                \"id\": block.id,\n                \"name\": block.name,\n                \"inputSchema\": str(block.input_schema),\n                \"outputSchema\": str(block.output_schema),\n            }\n        )\n\n\nasync def get_blocks() -> list[Block]:\n    if not AVAILABLE_BLOCKS:\n        await initialize_blocks()\n    return list(AVAILABLE_BLOCKS.values())\n\n\nasync def get_block(block_id: str) -> Block | None:\n    if not AVAILABLE_BLOCKS:\n        await initialize_blocks()\n    return AVAILABLE_BLOCKS.get(block_id)\n", "rnd/autogpt_server/autogpt_server/data/db.py": "from uuid import uuid4\nfrom prisma import Prisma\nfrom pydantic import BaseModel\n\nprisma = Prisma(auto_register=True)\n\n\nasync def connect():\n    if not prisma.is_connected():\n        await prisma.connect()\n\n\nasync def disconnect():\n    if prisma.is_connected():\n        await prisma.disconnect()\n\n\nclass BaseDbModel(BaseModel):\n    id: str = \"\"\n\n    def __init__(self, id: str = \"\", **data):\n        data[\"id\"] = id or str(uuid4())\n        super().__init__(**data)\n", "rnd/autogpt_server/autogpt_server/data/schedule.py": "import json\nfrom datetime import datetime\nfrom typing import Optional, Any\n\nfrom prisma.models import AgentExecutionSchedule\n\nfrom autogpt_server.data.db import BaseDbModel\n\n\nclass ExecutionSchedule(BaseDbModel):\n    id: str\n    agent_id: str\n    schedule: str\n    is_enabled: bool\n    input_data: dict[str, Any]\n    last_updated: Optional[datetime] = None\n\n    def __init__(\n            self,\n            is_enabled: Optional[bool] = None,\n            **kwargs\n    ):\n        if is_enabled is None:\n            is_enabled = True\n        super().__init__(is_enabled=is_enabled, **kwargs)\n\n    @staticmethod\n    def from_db(schedule: AgentExecutionSchedule):\n        return ExecutionSchedule(\n            id=schedule.id,\n            agent_id=schedule.agentGraphId,\n            schedule=schedule.schedule,\n            is_enabled=schedule.isEnabled,\n            last_updated=schedule.lastUpdated.replace(tzinfo=None),\n            input_data=json.loads(schedule.inputData),\n        )\n\n\nasync def get_active_schedules(last_fetch_time: datetime) -> list[ExecutionSchedule]:\n    query = AgentExecutionSchedule.prisma().find_many(\n        where={\n            \"isEnabled\": True,\n            \"lastUpdated\": {\"gt\": last_fetch_time}\n        },\n        order={\"lastUpdated\": \"asc\"}\n    )\n    return [\n        ExecutionSchedule.from_db(schedule)\n        for schedule in await query\n    ]\n\n\nasync def disable_schedule(schedule_id: str):\n    await AgentExecutionSchedule.prisma().update(\n        where={\"id\": schedule_id},\n        data={\"isEnabled\": False}\n    )\n\n\nasync def get_schedules(agent_id: str) -> list[ExecutionSchedule]:\n    query = AgentExecutionSchedule.prisma().find_many(\n        where={\n            \"isEnabled\": True,\n            \"agentGraphId\": agent_id,\n        },\n    )\n    return [\n        ExecutionSchedule.from_db(schedule)\n        for schedule in await query\n    ]\n\n\nasync def add_schedule(schedule: ExecutionSchedule):\n    await AgentExecutionSchedule.prisma().create(\n        data={\n            \"id\": schedule.id,\n            \"agentGraphId\": schedule.agent_id,\n            \"schedule\": schedule.schedule,\n            \"isEnabled\": schedule.is_enabled,\n            \"inputData\": json.dumps(schedule.input_data),\n        }\n    )\n\n\nasync def update_schedule(schedule_id: str, is_enabled: bool):\n    await AgentExecutionSchedule.prisma().update(\n        where={\"id\": schedule_id},\n        data={\"isEnabled\": is_enabled}\n    )\n", "rnd/autogpt_server/autogpt_server/data/graph.py": "import asyncio\nimport json\nimport uuid\n\nfrom typing import Any\nfrom prisma.models import AgentGraph, AgentNode, AgentNodeExecution, AgentNodeLink\n\nfrom autogpt_server.data.db import BaseDbModel\n\n\nclass Node(BaseDbModel):\n    block_id: str\n    input_default: dict[str, Any] = {}  # dict[input_name, default_value]\n    input_nodes: dict[str, str] = {}  # dict[input_name, node_id]\n    # TODO: Make it `dict[str, list[str]]`, output can be connected to multiple blocks.\n    #       Other option is to use an edge-list, but it will complicate the rest code.\n    output_nodes: dict[str, str] = {}  # dict[output_name, node_id]\n    metadata: dict[str, Any] = {}\n\n    @staticmethod\n    def from_db(node: AgentNode):\n        if not node.AgentBlock:\n            raise ValueError(f\"Invalid node {node.id}, invalid AgentBlock.\")\n\n        return Node(\n            id=node.id,\n            block_id=node.AgentBlock.id,\n            input_default=json.loads(node.constantInput),\n            input_nodes={v.sinkName: v.agentNodeSourceId for v in node.Input or []},\n            output_nodes={v.sourceName: v.agentNodeSinkId for v in node.Output or []},\n            metadata=json.loads(node.metadata),\n        )\n\n    def connect(self, node: \"Node\", source_name: str, sink_name: str):\n        self.output_nodes[source_name] = node.id\n        node.input_nodes[sink_name] = self.id\n\n\nclass Graph(BaseDbModel):\n    name: str\n    description: str\n    nodes: list[Node]\n\n    @property\n    def starting_nodes(self) -> list[Node]:\n        return [node for node in self.nodes if not node.input_nodes]\n\n    @staticmethod\n    def from_db(graph: AgentGraph):\n        return Graph(\n            id=graph.id,\n            name=graph.name or \"\",\n            description=graph.description or \"\",\n            nodes=[Node.from_db(node) for node in graph.AgentNodes or []],\n        )\n\n\nEXECUTION_NODE_INCLUDE = {\n    \"Input\": True,\n    \"Output\": True,\n    \"AgentBlock\": True,\n}\n\n\n# --------------------- Model functions --------------------- #\n\n\nasync def get_node(node_id: str) -> Node | None:\n    node = await AgentNode.prisma().find_unique_or_raise(\n        where={\"id\": node_id},\n        include=EXECUTION_NODE_INCLUDE,  # type: ignore\n    )\n    return Node.from_db(node) if node else None\n\n\nasync def get_graph_ids() -> list[str]:\n    return [graph.id for graph in await AgentGraph.prisma().find_many()]  # type: ignore\n\n\nasync def get_graph(graph_id: str) -> Graph | None:\n    graph = await AgentGraph.prisma().find_unique(\n        where={\"id\": graph_id},\n        include={\"AgentNodes\": {\"include\": EXECUTION_NODE_INCLUDE}},  # type: ignore\n    )\n    return Graph.from_db(graph) if graph else None\n\n\nasync def get_node_input(node: Node, exec_id: str) -> dict[str, Any]:\n    \"\"\"\n    Get execution node input data from the previous node execution result.\n    Args:\n        node: The execution node.\n        exec_id: The execution ID.\n    Returns:\n        dictionary of input data, key is the input name, value is the input data.\n    \"\"\"\n    query = await AgentNodeExecution.prisma().find_many(\n        where={  # type: ignore\n            \"executionId\": exec_id,\n            \"agentNodeId\": {\"in\": list(node.input_nodes.values())},\n            \"executionStatus\": \"COMPLETED\",\n        },\n        distinct=[\"agentNodeId\"],  # type: ignore\n        order={\"creationTime\": \"desc\"},\n    )\n\n    latest_executions: dict[str, AgentNodeExecution] = {\n        execution.agentNodeId: execution for execution in query\n    }\n\n    return {\n        **node.input_default,\n        **{\n            name: json.loads(latest_executions[node_id].outputData or \"{}\")\n            for name, node_id in node.input_nodes.items()\n            if node_id in latest_executions and latest_executions[node_id].outputData\n        },\n    }\n\n\nasync def create_graph(graph: Graph) -> Graph:\n\n    await AgentGraph.prisma().create(\n        data={\n            \"id\": graph.id,\n            \"name\": graph.name,\n            \"description\": graph.description,\n        }\n    )\n\n    # TODO: replace bulk creation using create_many\n    await asyncio.gather(*[\n        AgentNode.prisma().create({\n            \"id\": node.id,\n            \"agentBlockId\": node.block_id,\n            \"agentGraphId\": graph.id,\n            \"constantInput\": json.dumps(node.input_default),\n            \"metadata\": json.dumps(node.metadata),\n        }) for node in graph.nodes\n    ])\n\n    edge_source_names = {\n        (source_node.id, sink_node_id): output_name\n        for source_node in graph.nodes\n        for output_name, sink_node_id in source_node.output_nodes.items()\n    }\n    edge_sink_names = {\n        (source_node_id, sink_node.id): input_name\n        for sink_node in graph.nodes\n        for input_name, source_node_id in sink_node.input_nodes.items()\n    }\n\n    # TODO: replace bulk creation using create_many\n    await asyncio.gather(*[\n        AgentNodeLink.prisma().create({\n            \"id\": str(uuid.uuid4()),\n            \"sourceName\": edge_source_names.get((input_node, output_node), \"\"),\n            \"sinkName\": edge_sink_names.get((input_node, output_node), \"\"),\n            \"agentNodeSourceId\": input_node,\n            \"agentNodeSinkId\": output_node,\n        })\n        for input_node, output_node in edge_source_names.keys() | edge_sink_names.keys()\n    ])\n\n    if created_graph := await get_graph(graph.id):\n        return created_graph\n\n    raise ValueError(f\"Failed to create graph {graph.id}.\")\n", "rnd/autogpt_server/autogpt_server/util/process.py": "import os\nimport sys\nfrom abc import ABC, abstractmethod\nfrom multiprocessing import Process, freeze_support, set_start_method\nfrom multiprocessing.spawn import freeze_support as freeze_support_spawn\nfrom typing import Optional\n\n\nclass AppProcess(ABC):\n    \"\"\"\n    A class to represent an object that can be executed in a background process.\n    \"\"\"\n    process: Optional[Process] = None\n    set_start_method('spawn', force=True)\n    freeze_support()\n    freeze_support_spawn()\n\n    @abstractmethod\n    def run(self):\n        \"\"\"\n        The method that will be executed in the process.\n        \"\"\"\n        pass\n\n    def execute_run_command(self, silent):\n        try:\n            if silent:\n                sys.stdout = open(os.devnull, \"w\")\n                sys.stderr = open(os.devnull, \"w\")\n            self.run()\n        except KeyboardInterrupt or SystemExit as e:\n            print(f\"Process terminated: {e}\")\n\n    def __enter__(self):\n        self.start(background=True)\n        return self\n\n    def __exit__(self, *args, **kwargs):\n        self.stop()\n\n    def start(self, background: bool = False, silent: bool = False, **proc_args) -> int:\n        \"\"\"\n        Start the background process.\n        Args:\n            background: Whether to run the process in the background.\n            silent: Whether to disable stdout and stderr.\n            proc_args: Additional arguments to pass to the process.\n        Returns:\n            the process id or 0 if the process is not running in the background.\n        \"\"\"\n        if not background:\n            self.execute_run_command(silent)\n            return 0\n\n        self.process = Process(\n            name=self.__class__.__name__,\n            target=self.execute_run_command,\n            args=(silent,),\n            **proc_args,\n        )\n        self.process.start()\n        return self.process.pid or 0\n\n    def stop(self):\n        \"\"\"\n        Stop the background process.\n        \"\"\"\n        if not self.process:\n            return\n\n        self.process.terminate()\n        self.process.join()\n        self.process = None\n", "rnd/autogpt_server/autogpt_server/util/service.py": "import time\nimport asyncio\nimport logging\nimport threading\n\nfrom abc import abstractmethod\nfrom typing import Any, Callable, Type, TypeVar, cast, Coroutine\n\nfrom Pyro5 import api as pyro\nfrom Pyro5 import nameserver\nfrom tenacity import retry, stop_after_delay, wait_exponential\n\nfrom autogpt_server.data import db\nfrom autogpt_server.util.process import AppProcess\n\nlogger = logging.getLogger(__name__)\nconn_retry = retry(stop=stop_after_delay(5), wait=wait_exponential(multiplier=0.1))\n\n\ndef expose(func: Callable) -> Callable:\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except Exception as e:\n            msg = f\"Error in {func.__name__}: {e.__str__()}\"\n            logger.error(msg)\n            raise Exception(msg, e)\n\n    return pyro.expose(wrapper)\n\n\nclass PyroNameServer(AppProcess):\n    def run(self):\n        try:\n            print(\"Starting NameServer loop\")\n            nameserver.start_ns_loop()\n        except KeyboardInterrupt:\n            print(\"Shutting down NameServer\")\n\n\nclass AppService(AppProcess):\n    shared_event_loop: asyncio.AbstractEventLoop\n\n    @classmethod\n    @property\n    def service_name(cls) -> str:\n        return cls.__name__\n\n    @abstractmethod\n    def run_service(self):\n        while True:\n            time.sleep(10)\n\n    def run_async(self, coro: Coroutine):\n        return asyncio.run_coroutine_threadsafe(coro, self.shared_event_loop)\n\n    def run_and_wait(self, coro: Coroutine):\n        future = self.run_async(coro)\n        return future.result()\n\n    def run(self):\n        self.shared_event_loop = asyncio.get_event_loop()\n        self.shared_event_loop.run_until_complete(db.connect())\n\n        # Initialize the async loop.\n        async_thread = threading.Thread(target=self.__start_async_loop)\n        async_thread.daemon = True\n        async_thread.start()\n\n        # Initialize pyro service\n        daemon_thread = threading.Thread(target=self.__start_pyro)\n        daemon_thread.daemon = True\n        daemon_thread.start()\n\n        # Run the main service (if it's not implemented, just sleep).\n        self.run_service()\n\n    @conn_retry\n    def __start_pyro(self):\n        daemon = pyro.Daemon()\n        ns = pyro.locate_ns()\n        uri = daemon.register(self)\n        ns.register(self.service_name, uri)\n        logger.warning(f\"Service [{self.service_name}] Ready. Object URI = {uri}\")\n        daemon.requestLoop()\n\n    def __start_async_loop(self):\n        # asyncio.set_event_loop(self.shared_event_loop)\n        self.shared_event_loop.run_forever()\n\n\nAS = TypeVar(\"AS\", bound=AppService)\n\n\ndef get_service_client(service_type: Type[AS]) -> AS:\n    service_name = service_type.service_name\n\n    class DynamicClient:\n\n        @conn_retry\n        def __init__(self):\n            ns = pyro.locate_ns()\n            uri = ns.lookup(service_name)\n            self.proxy = pyro.Proxy(uri)\n            self.proxy._pyroBind()\n\n        def __getattr__(self, name: str) -> Callable[..., Any]:\n            return getattr(self.proxy, name)\n\n    return cast(AS, DynamicClient())\n", "rnd/autogpt_server/test/executor/test_scheduler.py": "import pytest\n\nimport test_manager\nfrom autogpt_server.executor.scheduler import ExecutionScheduler\nfrom autogpt_server.util.service import PyroNameServer, get_service_client\n\n\n@pytest.mark.asyncio(scope=\"session\")\nasync def test_agent_schedule():\n    await test_manager.db.connect()\n    test_graph = await test_manager.create_test_graph()\n\n    with PyroNameServer():\n        with ExecutionScheduler():\n            scheduler = get_service_client(ExecutionScheduler)\n\n            schedules = scheduler.get_execution_schedules(test_graph.id)\n            assert len(schedules) == 0\n\n            schedule_id = scheduler.add_execution_schedule(\n                test_graph.id,\n                \"0 0 * * *\",\n                {\"input\": \"data\"}\n            )\n            assert schedule_id\n\n            schedules = scheduler.get_execution_schedules(test_graph.id)\n            assert len(schedules) == 1\n            assert schedules[schedule_id] == \"0 0 * * *\"\n\n            scheduler.update_schedule(schedule_id, is_enabled=False)\n            schedules = scheduler.get_execution_schedules(test_graph.id)\n            assert len(schedules) == 0\n", "rnd/autogpt_server/test/executor/test_manager.py": "import time\n\nimport pytest\n\nfrom autogpt_server.data import block, db, execution, graph\nfrom autogpt_server.executor import ExecutionManager\nfrom autogpt_server.server import AgentServer\nfrom autogpt_server.util.service import PyroNameServer\n\n\nasync def create_test_graph() -> graph.Graph:\n    \"\"\"\n    ParrotBlock\n                \\\n                 ---- TextCombinerBlock ---- PrintingBlock\n                /\n    ParrotBlock\n    \"\"\"\n    nodes = [\n        graph.Node(block_id=block.ParrotBlock.id),\n        graph.Node(block_id=block.ParrotBlock.id),\n        graph.Node(\n            block_id=block.TextCombinerBlock.id,\n            input_default={\"format\": \"{text1},{text2}\"},\n        ),\n        graph.Node(block_id=block.PrintingBlock.id),\n    ]\n    nodes[0].connect(nodes[2], \"output\", \"text1\")\n    nodes[1].connect(nodes[2], \"output\", \"text2\")\n    nodes[2].connect(nodes[3], \"combined_text\", \"text\")\n\n    test_graph = graph.Graph(\n        name=\"TestGraph\",\n        description=\"Test graph\",\n        nodes=nodes,\n    )\n    await block.initialize_blocks()\n    result = await graph.create_graph(test_graph)\n\n    # Assertions\n    assert result.name == test_graph.name\n    assert result.description == test_graph.description\n    assert len(result.nodes) == len(test_graph.nodes)\n\n    return test_graph\n\n\nasync def execute_agent(test_manager: ExecutionManager, test_graph: graph.Graph):\n    # --- Test adding new executions --- #\n    text = \"Hello, World!\"\n    input_data = {\"input\": text}\n    agent_server = AgentServer()\n    response = await agent_server.execute_agent(test_graph.id, input_data)\n    executions = response[\"executions\"]\n    run_id = response[\"run_id\"]\n    assert len(executions) == 2\n\n    async def is_execution_completed():\n        execs = await agent_server.get_executions(test_graph.id, run_id)\n        return test_manager.queue.empty() and len(execs) == 4\n\n    # Wait for the executions to complete\n    for i in range(10):\n        if await is_execution_completed():\n            break\n        time.sleep(1)\n\n    # Execution queue should be empty\n    assert await is_execution_completed()\n    executions = await agent_server.get_executions(test_graph.id, run_id)\n\n    # Executing ParrotBlock1\n    exec = executions[0]\n    assert exec.status == execution.ExecutionStatus.COMPLETED\n    assert exec.run_id == run_id\n    assert exec.output_name == \"output\"\n    assert exec.output_data == \"Hello, World!\"\n    assert exec.input_data == input_data\n    assert exec.node_id == test_graph.nodes[0].id\n\n    # Executing ParrotBlock2\n    exec = executions[1]\n    assert exec.status == execution.ExecutionStatus.COMPLETED\n    assert exec.run_id == run_id\n    assert exec.output_name == \"output\"\n    assert exec.output_data == \"Hello, World!\"\n    assert exec.input_data == input_data\n    assert exec.node_id == test_graph.nodes[1].id\n\n    # Executing TextCombinerBlock\n    exec = executions[2]\n    assert exec.status == execution.ExecutionStatus.COMPLETED\n    assert exec.run_id == run_id\n    assert exec.output_name == \"combined_text\"\n    assert exec.output_data == \"Hello, World!,Hello, World!\"\n    assert exec.input_data == {\n        \"format\": \"{text1},{text2}\",\n        \"text1\": \"Hello, World!\",\n        \"text2\": \"Hello, World!\",\n    }\n    assert exec.node_id == test_graph.nodes[2].id\n\n    # Executing PrintingBlock\n    exec = executions[3]\n    assert exec.status == execution.ExecutionStatus.COMPLETED\n    assert exec.run_id == run_id\n    assert exec.output_name == \"status\"\n    assert exec.output_data == \"printed\"\n    assert exec.input_data == {\"text\": \"Hello, World!,Hello, World!\"}\n    assert exec.node_id == test_graph.nodes[3].id\n\n\n@pytest.mark.asyncio(scope=\"session\")\nasync def test_agent_execution():\n    with PyroNameServer():\n        with ExecutionManager(1) as test_manager:\n            await db.connect()\n            test_graph = await create_test_graph()\n            await execute_agent(test_manager, test_graph)\n", "rnd/autogpt_server/test/util/test_service.py": "from autogpt_server.util.service import (\n    AppService,\n    PyroNameServer,\n    expose,\n    get_service_client,\n)\n\n\nclass TestService(AppService):\n\n    def run_service(self):\n        super().run_service()\n\n    @expose\n    def add(self, a: int, b: int) -> int:\n        return a + b\n\n    @expose\n    def subtract(self, a: int, b: int) -> int:\n        return a - b\n\n    @expose\n    def fun_with_async(self, a: int, b: int) -> int:\n        async def add_async(a: int, b: int) -> int:\n            return a + b\n        return self.run_and_wait(add_async(a, b))\n\n\ndef test_service_creation():\n    with PyroNameServer():\n        with TestService():\n            client = get_service_client(TestService)\n            assert client.add(5, 3) == 8\n            assert client.subtract(10, 4) == 6\n            assert client.fun_with_async(5, 3) == 8\n"}