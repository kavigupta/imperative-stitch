{"scripts/utils.py": "import argparse\nimport os\nimport shutil\nfrom typing import Any, ClassVar\n\nfrom private_gpt.paths import local_data_path\nfrom private_gpt.settings.settings import settings\n\n\ndef wipe_file(file: str) -> None:\n    if os.path.isfile(file):\n        os.remove(file)\n        print(f\" - Deleted {file}\")\n\n\ndef wipe_tree(path: str) -> None:\n    if not os.path.exists(path):\n        print(f\"Warning: Path not found {path}\")\n        return\n    print(f\"Wiping {path}...\")\n    all_files = os.listdir(path)\n\n    files_to_remove = [file for file in all_files if file != \".gitignore\"]\n    for file_name in files_to_remove:\n        file_path = os.path.join(path, file_name)\n        try:\n            if os.path.isfile(file_path):\n                os.remove(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n            print(f\" - Deleted {file_path}\")\n        except PermissionError:\n            print(\n                f\"PermissionError: Unable to remove {file_path}. It is in use by another process.\"\n            )\n            continue\n\n\nclass Postgres:\n    tables: ClassVar[dict[str, list[str]]] = {\n        \"nodestore\": [\"data_docstore\", \"data_indexstore\"],\n        \"vectorstore\": [\"data_embeddings\"],\n    }\n\n    def __init__(self) -> None:\n        try:\n            import psycopg2\n        except ModuleNotFoundError:\n            raise ModuleNotFoundError(\"Postgres dependencies not found\") from None\n\n        connection = settings().postgres.model_dump(exclude_none=True)\n        self.schema = connection.pop(\"schema_name\")\n        self.conn = psycopg2.connect(**connection)\n\n    def wipe(self, storetype: str) -> None:\n        cur = self.conn.cursor()\n        try:\n            for table in self.tables[storetype]:\n                sql = f\"DROP TABLE IF EXISTS {self.schema}.{table}\"\n                cur.execute(sql)\n                print(f\"Table {self.schema}.{table} dropped.\")\n            self.conn.commit()\n        finally:\n            cur.close()\n\n    def stats(self, store_type: str) -> None:\n        template = \"SELECT '{table}', COUNT(*), pg_size_pretty(pg_total_relation_size('{table}')) FROM {table}\"\n        sql = \" UNION ALL \".join(\n            template.format(table=tbl) for tbl in self.tables[store_type]\n        )\n\n        cur = self.conn.cursor()\n        try:\n            print(f\"Storage for Postgres {store_type}.\")\n            print(\"{:<15} | {:>15} | {:>9}\".format(\"Table\", \"Rows\", \"Size\"))\n            print(\"-\" * 45)  # Print a line separator\n\n            cur.execute(sql)\n            for row in cur.fetchall():\n                formatted_row_count = f\"{row[1]:,}\"\n                print(f\"{row[0]:<15} | {formatted_row_count:>15} | {row[2]:>9}\")\n\n            print()\n        finally:\n            cur.close()\n\n    def __del__(self):\n        if hasattr(self, \"conn\") and self.conn:\n            self.conn.close()\n\n\nclass Simple:\n    def wipe(self, store_type: str) -> None:\n        assert store_type == \"nodestore\"\n        from llama_index.core.storage.docstore.types import (\n            DEFAULT_PERSIST_FNAME as DOCSTORE,\n        )\n        from llama_index.core.storage.index_store.types import (\n            DEFAULT_PERSIST_FNAME as INDEXSTORE,\n        )\n\n        for store in (DOCSTORE, INDEXSTORE):\n            wipe_file(str((local_data_path / store).absolute()))\n\n\nclass Chroma:\n    def wipe(self, store_type: str) -> None:\n        assert store_type == \"vectorstore\"\n        wipe_tree(str((local_data_path / \"chroma_db\").absolute()))\n\n\nclass Qdrant:\n    COLLECTION = (\n        \"make_this_parameterizable_per_api_call\"  # ?! see vector_store_component.py\n    )\n\n    def __init__(self) -> None:\n        try:\n            from qdrant_client import QdrantClient  # type: ignore\n        except ImportError:\n            raise ImportError(\"Qdrant dependencies not found\") from None\n        self.client = QdrantClient(**settings().qdrant.model_dump(exclude_none=True))\n\n    def wipe(self, store_type: str) -> None:\n        assert store_type == \"vectorstore\"\n        try:\n            self.client.delete_collection(self.COLLECTION)\n            print(\"Collection dropped successfully.\")\n        except Exception as e:\n            print(\"Error dropping collection:\", e)\n\n    def stats(self, store_type: str) -> None:\n        print(f\"Storage for Qdrant {store_type}.\")\n        try:\n            collection_data = self.client.get_collection(self.COLLECTION)\n            if collection_data:\n                # Collection Info\n                # https://qdrant.tech/documentation/concepts/collections/\n                print(f\"\\tPoints:        {collection_data.points_count:,}\")\n                print(f\"\\tVectors:       {collection_data.vectors_count:,}\")\n                print(f\"\\tIndex Vectors: {collection_data.indexed_vectors_count:,}\")\n                return\n        except ValueError:\n            pass\n        print(\"\\t- Qdrant collection not found or empty\")\n\n\nclass Command:\n    DB_HANDLERS: ClassVar[dict[str, Any]] = {\n        \"simple\": Simple,  # node store\n        \"chroma\": Chroma,  # vector store\n        \"postgres\": Postgres,  # node, index and vector store\n        \"qdrant\": Qdrant,  # vector store\n    }\n\n    def for_each_store(self, cmd: str):\n        for store_type in (\"nodestore\", \"vectorstore\"):\n            database = getattr(settings(), store_type).database\n            handler_class = self.DB_HANDLERS.get(database)\n            if handler_class is None:\n                print(f\"No handler found for database '{database}'\")\n                continue\n            handler_instance = handler_class()  # Instantiate the class\n            # If the DB can handle this cmd dispatch it.\n            if hasattr(handler_instance, cmd) and callable(\n                func := getattr(handler_instance, cmd)\n            ):\n                func(store_type)\n            else:\n                print(\n                    f\"Unable to execute command '{cmd}' on '{store_type}' in database '{database}'\"\n                )\n\n    def execute(self, cmd: str) -> None:\n        if cmd in (\"wipe\", \"stats\"):\n            self.for_each_store(cmd)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"mode\", help=\"select a mode to run\", choices=[\"wipe\", \"stats\"])\n    args = parser.parse_args()\n\n    Command().execute(args.mode.lower())\n", "scripts/extract_openapi.py": "import argparse\nimport json\nimport sys\n\nimport yaml\nfrom uvicorn.importer import import_from_string\n\nparser = argparse.ArgumentParser(prog=\"extract_openapi.py\")\nparser.add_argument(\"app\", help='App import string. Eg. \"main:app\"', default=\"main:app\")\nparser.add_argument(\"--app-dir\", help=\"Directory containing the app\", default=None)\nparser.add_argument(\n    \"--out\", help=\"Output file ending in .json or .yaml\", default=\"openapi.yaml\"\n)\n\nif __name__ == \"__main__\":\n    args = parser.parse_args()\n\n    if args.app_dir is not None:\n        print(f\"adding {args.app_dir} to sys.path\")\n        sys.path.insert(0, args.app_dir)\n\n    print(f\"importing app from {args.app}\")\n    app = import_from_string(args.app)\n    openapi = app.openapi()\n    version = openapi.get(\"openapi\", \"unknown version\")\n\n    print(f\"writing openapi spec v{version}\")\n    with open(args.out, \"w\") as f:\n        if args.out.endswith(\".json\"):\n            json.dump(openapi, f, indent=2)\n        else:\n            yaml.dump(openapi, f, sort_keys=False)\n\n    print(f\"spec written to {args.out}\")\n", "scripts/ingest_folder.py": "#!/usr/bin/env python3\n\nimport argparse\nimport logging\nfrom pathlib import Path\n\nfrom private_gpt.di import global_injector\nfrom private_gpt.server.ingest.ingest_service import IngestService\nfrom private_gpt.server.ingest.ingest_watcher import IngestWatcher\n\nlogger = logging.getLogger(__name__)\n\n\nclass LocalIngestWorker:\n    def __init__(self, ingest_service: IngestService) -> None:\n        self.ingest_service = ingest_service\n\n        self.total_documents = 0\n        self.current_document_count = 0\n\n        self._files_under_root_folder: list[Path] = []\n\n    def _find_all_files_in_folder(self, root_path: Path, ignored: list[str]) -> None:\n        \"\"\"Search all files under the root folder recursively.\n\n        Count them at the same time\n        \"\"\"\n        for file_path in root_path.iterdir():\n            if file_path.is_file() and file_path.name not in ignored:\n                self.total_documents += 1\n                self._files_under_root_folder.append(file_path)\n            elif file_path.is_dir() and file_path.name not in ignored:\n                self._find_all_files_in_folder(file_path, ignored)\n\n    def ingest_folder(self, folder_path: Path, ignored: list[str]) -> None:\n        # Count total documents before ingestion\n        self._find_all_files_in_folder(folder_path, ignored)\n        self._ingest_all(self._files_under_root_folder)\n\n    def _ingest_all(self, files_to_ingest: list[Path]) -> None:\n        logger.info(\"Ingesting files=%s\", [f.name for f in files_to_ingest])\n        self.ingest_service.bulk_ingest([(str(p.name), p) for p in files_to_ingest])\n\n    def ingest_on_watch(self, changed_path: Path) -> None:\n        logger.info(\"Detected change in at path=%s, ingesting\", changed_path)\n        self._do_ingest_one(changed_path)\n\n    def _do_ingest_one(self, changed_path: Path) -> None:\n        try:\n            if changed_path.exists():\n                logger.info(f\"Started ingesting file={changed_path}\")\n                self.ingest_service.ingest_file(changed_path.name, changed_path)\n                logger.info(f\"Completed ingesting file={changed_path}\")\n        except Exception:\n            logger.exception(\n                f\"Failed to ingest document: {changed_path}, find the exception attached\"\n            )\n\n\nparser = argparse.ArgumentParser(prog=\"ingest_folder.py\")\nparser.add_argument(\"folder\", help=\"Folder to ingest\")\nparser.add_argument(\n    \"--watch\",\n    help=\"Watch for changes\",\n    action=argparse.BooleanOptionalAction,\n    default=False,\n)\nparser.add_argument(\n    \"--ignored\",\n    nargs=\"*\",\n    help=\"List of files/directories to ignore\",\n    default=[],\n)\nparser.add_argument(\n    \"--log-file\",\n    help=\"Optional path to a log file. If provided, logs will be written to this file.\",\n    type=str,\n    default=None,\n)\n\nargs = parser.parse_args()\n\n# Set up logging to a file if a path is provided\nif args.log_file:\n    file_handler = logging.FileHandler(args.log_file, mode=\"a\")\n    file_handler.setFormatter(\n        logging.Formatter(\n            \"[%(asctime)s.%(msecs)03d] [%(levelname)s] %(message)s\",\n            datefmt=\"%Y-%m-%d %H:%M:%S\",\n        )\n    )\n    logger.addHandler(file_handler)\n\nif __name__ == \"__main__\":\n\n    root_path = Path(args.folder)\n    if not root_path.exists():\n        raise ValueError(f\"Path {args.folder} does not exist\")\n\n    ingest_service = global_injector.get(IngestService)\n    worker = LocalIngestWorker(ingest_service)\n    worker.ingest_folder(root_path, args.ignored)\n\n    if args.ignored:\n        logger.info(f\"Skipping following files and directories: {args.ignored}\")\n\n    if args.watch:\n        logger.info(f\"Watching {args.folder} for changes, press Ctrl+C to stop...\")\n        directories_to_watch = [\n            dir\n            for dir in root_path.iterdir()\n            if dir.is_dir() and dir.name not in args.ignored\n        ]\n        watcher = IngestWatcher(args.folder, worker.ingest_on_watch)\n        watcher.start()\n", "scripts/__init__.py": "\"\"\"PrivateGPT scripts.\"\"\"\n", "tests/conftest.py": "import os\nimport pathlib\nfrom glob import glob\n\nroot_path = pathlib.Path(__file__).parents[1]\n# This is to prevent a bug in intellij that uses the wrong working directory\nos.chdir(root_path)\n\n\ndef _as_module(fixture_path: str) -> str:\n    return fixture_path.replace(\"/\", \".\").replace(\"\\\\\", \".\").replace(\".py\", \"\")\n\n\npytest_plugins = [_as_module(fixture) for fixture in glob(\"tests/fixtures/[!_]*.py\")]\n", "tests/__init__.py": "\"\"\"Tests.\"\"\"\n", "tests/test_prompt_helper.py": "import pytest\nfrom llama_index.core.llms import ChatMessage, MessageRole\n\nfrom private_gpt.components.llm.prompt_helper import (\n    ChatMLPromptStyle,\n    DefaultPromptStyle,\n    Llama2PromptStyle,\n    MistralPromptStyle,\n    TagPromptStyle,\n    get_prompt_style,\n)\n\n\n@pytest.mark.parametrize(\n    (\"prompt_style\", \"expected_prompt_style\"),\n    [\n        (\"default\", DefaultPromptStyle),\n        (\"llama2\", Llama2PromptStyle),\n        (\"tag\", TagPromptStyle),\n        (\"mistral\", MistralPromptStyle),\n        (\"chatml\", ChatMLPromptStyle),\n    ],\n)\ndef test_get_prompt_style_success(prompt_style, expected_prompt_style):\n    assert isinstance(get_prompt_style(prompt_style), expected_prompt_style)\n\n\ndef test_get_prompt_style_failure():\n    prompt_style = \"unknown\"\n    with pytest.raises(ValueError) as exc_info:\n        get_prompt_style(prompt_style)\n    assert str(exc_info.value) == f\"Unknown prompt_style='{prompt_style}'\"\n\n\ndef test_tag_prompt_style_format():\n    prompt_style = TagPromptStyle()\n    messages = [\n        ChatMessage(content=\"You are an AI assistant.\", role=MessageRole.SYSTEM),\n        ChatMessage(content=\"Hello, how are you doing?\", role=MessageRole.USER),\n    ]\n\n    expected_prompt = (\n        \"<|system|>: You are an AI assistant.\\n\"\n        \"<|user|>: Hello, how are you doing?\\n\"\n        \"<|assistant|>: \"\n    )\n\n    assert prompt_style.messages_to_prompt(messages) == expected_prompt\n\n\ndef test_tag_prompt_style_format_with_system_prompt():\n    prompt_style = TagPromptStyle()\n    messages = [\n        ChatMessage(\n            content=\"FOO BAR Custom sys prompt from messages.\", role=MessageRole.SYSTEM\n        ),\n        ChatMessage(content=\"Hello, how are you doing?\", role=MessageRole.USER),\n    ]\n\n    expected_prompt = (\n        \"<|system|>: FOO BAR Custom sys prompt from messages.\\n\"\n        \"<|user|>: Hello, how are you doing?\\n\"\n        \"<|assistant|>: \"\n    )\n\n    assert prompt_style.messages_to_prompt(messages) == expected_prompt\n\n\ndef test_mistral_prompt_style_format():\n    prompt_style = MistralPromptStyle()\n    messages = [\n        ChatMessage(content=\"A\", role=MessageRole.SYSTEM),\n        ChatMessage(content=\"B\", role=MessageRole.USER),\n    ]\n    expected_prompt = \"<s>[INST] A\\nB [/INST]\"\n    assert prompt_style.messages_to_prompt(messages) == expected_prompt\n\n    messages2 = [\n        ChatMessage(content=\"A\", role=MessageRole.SYSTEM),\n        ChatMessage(content=\"B\", role=MessageRole.USER),\n        ChatMessage(content=\"C\", role=MessageRole.ASSISTANT),\n        ChatMessage(content=\"D\", role=MessageRole.USER),\n    ]\n    expected_prompt2 = \"<s>[INST] A\\nB [/INST] C</s><s>[INST] D [/INST]\"\n    assert prompt_style.messages_to_prompt(messages2) == expected_prompt2\n\n\ndef test_chatml_prompt_style_format():\n    prompt_style = ChatMLPromptStyle()\n    messages = [\n        ChatMessage(content=\"You are an AI assistant.\", role=MessageRole.SYSTEM),\n        ChatMessage(content=\"Hello, how are you doing?\", role=MessageRole.USER),\n    ]\n\n    expected_prompt = (\n        \"<|im_start|>system\\n\"\n        \"You are an AI assistant.<|im_end|>\\n\"\n        \"<|im_start|>user\\n\"\n        \"Hello, how are you doing?<|im_end|>\\n\"\n        \"<|im_start|>assistant\\n\"\n    )\n\n    assert prompt_style.messages_to_prompt(messages) == expected_prompt\n\n\ndef test_llama2_prompt_style_format():\n    prompt_style = Llama2PromptStyle()\n    messages = [\n        ChatMessage(content=\"You are an AI assistant.\", role=MessageRole.SYSTEM),\n        ChatMessage(content=\"Hello, how are you doing?\", role=MessageRole.USER),\n    ]\n\n    expected_prompt = (\n        \"<s> [INST] <<SYS>>\\n\"\n        \" You are an AI assistant. \\n\"\n        \"<</SYS>>\\n\"\n        \"\\n\"\n        \" Hello, how are you doing? [/INST]\"\n    )\n\n    assert prompt_style.messages_to_prompt(messages) == expected_prompt\n\n\ndef test_llama2_prompt_style_with_system_prompt():\n    prompt_style = Llama2PromptStyle()\n    messages = [\n        ChatMessage(\n            content=\"FOO BAR Custom sys prompt from messages.\", role=MessageRole.SYSTEM\n        ),\n        ChatMessage(content=\"Hello, how are you doing?\", role=MessageRole.USER),\n    ]\n\n    expected_prompt = (\n        \"<s> [INST] <<SYS>>\\n\"\n        \" FOO BAR Custom sys prompt from messages. \\n\"\n        \"<</SYS>>\\n\"\n        \"\\n\"\n        \" Hello, how are you doing? [/INST]\"\n    )\n\n    assert prompt_style.messages_to_prompt(messages) == expected_prompt\n", "tests/settings/test_settings_loader.py": "import io\nimport os\n\nimport pytest\n\nfrom private_gpt.settings.yaml import load_yaml_with_envvars\n\n\ndef test_environment_variables_are_loaded() -> None:\n    sample_yaml = \"\"\"\n    replaced: ${TEST_REPLACE_ME}\n    \"\"\"\n    env = {\"TEST_REPLACE_ME\": \"replaced\"}\n    loaded = load_yaml_with_envvars(io.StringIO(sample_yaml), env)\n    os.environ.copy()\n    assert loaded[\"replaced\"] == \"replaced\"\n\n\ndef test_environment_defaults_variables_are_loaded() -> None:\n    sample_yaml = \"\"\"\n    replaced: ${PGPT_EMBEDDING_HF_MODEL_NAME:BAAI/bge-small-en-v1.5}\n    \"\"\"\n    loaded = load_yaml_with_envvars(io.StringIO(sample_yaml), {})\n    assert loaded[\"replaced\"] == \"BAAI/bge-small-en-v1.5\"\n\n\ndef test_environment_defaults_variables_are_loaded_with_duplicated_delimiters() -> None:\n    sample_yaml = \"\"\"\n    replaced: ${PGPT_EMBEDDING_HF_MODEL_NAME::duped::}\n    \"\"\"\n    loaded = load_yaml_with_envvars(io.StringIO(sample_yaml), {})\n    assert loaded[\"replaced\"] == \":duped::\"\n\n\ndef test_environment_without_defaults_fails() -> None:\n    sample_yaml = \"\"\"\n    replaced: ${TEST_REPLACE_ME}\n    \"\"\"\n    with pytest.raises(ValueError) as error:\n        load_yaml_with_envvars(io.StringIO(sample_yaml), {})\n    assert error is not None\n", "tests/settings/test_settings.py": "from private_gpt.settings.settings import Settings, settings\nfrom tests.fixtures.mock_injector import MockInjector\n\n\ndef test_settings_are_loaded_and_merged() -> None:\n    assert settings().server.env_name == \"test\"\n\n\ndef test_settings_can_be_overriden(injector: MockInjector) -> None:\n    injector.bind_settings({\"server\": {\"env_name\": \"overriden\"}})\n    mocked_settings = injector.get(Settings)\n    assert mocked_settings.server.env_name == \"overriden\"\n", "tests/ui/test_ui.py": "import pytest\nfrom fastapi.testclient import TestClient\n\n\n@pytest.mark.parametrize(\n    \"test_client\", [{\"ui\": {\"enabled\": True, \"path\": \"/ui\"}}], indirect=True\n)\ndef test_ui_starts_in_the_given_endpoint(test_client: TestClient) -> None:\n    response = test_client.get(\"/ui\")\n    assert response.status_code == 200\n", "tests/server/chat/test_chat_routes.py": "from fastapi.testclient import TestClient\n\nfrom private_gpt.open_ai.openai_models import OpenAICompletion, OpenAIMessage\nfrom private_gpt.server.chat.chat_router import ChatBody\n\n\ndef test_chat_route_produces_a_stream(test_client: TestClient) -> None:\n    body = ChatBody(\n        messages=[OpenAIMessage(content=\"test\", role=\"user\")],\n        use_context=False,\n        stream=True,\n    )\n    response = test_client.post(\"/v1/chat/completions\", json=body.model_dump())\n\n    raw_events = response.text.split(\"\\n\\n\")\n    events = [\n        item.removeprefix(\"data: \") for item in raw_events if item.startswith(\"data: \")\n    ]\n    assert response.status_code == 200\n    assert \"text/event-stream\" in response.headers[\"content-type\"]\n    assert len(events) > 0\n    assert events[-1] == \"[DONE]\"\n\n\ndef test_chat_route_produces_a_single_value(test_client: TestClient) -> None:\n    body = ChatBody(\n        messages=[OpenAIMessage(content=\"test\", role=\"user\")],\n        use_context=False,\n        stream=False,\n    )\n    response = test_client.post(\"/v1/chat/completions\", json=body.model_dump())\n\n    # No asserts, if it validates it's good\n    OpenAICompletion.model_validate(response.json())\n    assert response.status_code == 200\n", "tests/server/utils/test_auth.py": "from fastapi.testclient import TestClient\n\n\ndef test_default_does_not_require_auth(test_client: TestClient) -> None:\n    response_before = test_client.get(\"/v1/ingest/list\")\n    assert response_before.status_code == 200\n", "tests/server/utils/test_simple_auth.py": "\"\"\"Tests to validate that the simple authentication mechanism is working.\n\nNOTE: We are not testing the switch based on the config in\n      `private_gpt.server.utils.auth`. This is not done because of the way the code\n      is currently architecture (it is hard to patch the `settings` and the app while\n      the tests are directly importing them).\n\"\"\"\n\nfrom typing import Annotated\n\nimport pytest\nfrom fastapi import Depends\nfrom fastapi.testclient import TestClient\n\nfrom private_gpt.server.utils.auth import (\n    NOT_AUTHENTICATED,\n    _simple_authentication,\n    authenticated,\n)\nfrom private_gpt.settings.settings import settings\n\n\ndef _copy_simple_authenticated(\n    _simple_authentication: Annotated[bool, Depends(_simple_authentication)]\n) -> bool:\n    \"\"\"Check if the request is authenticated.\"\"\"\n    if not _simple_authentication:\n        raise NOT_AUTHENTICATED\n    return True\n\n\n@pytest.fixture(autouse=True)\ndef _patch_authenticated_dependency(test_client: TestClient):\n    # Patch the server to use simple authentication\n\n    test_client.app.dependency_overrides[authenticated] = _copy_simple_authenticated\n\n    # Call the actual test\n    yield\n\n    # Remove the patch for other tests\n    test_client.app.dependency_overrides = {}\n\n\ndef test_default_auth_working_when_enabled_401(test_client: TestClient) -> None:\n    response = test_client.get(\"/v1/ingest/list\")\n    assert response.status_code == 401\n\n\ndef test_default_auth_working_when_enabled_200(test_client: TestClient) -> None:\n    response_fail = test_client.get(\"/v1/ingest/list\")\n    assert response_fail.status_code == 401\n\n    response_success = test_client.get(\n        \"/v1/ingest/list\", headers={\"Authorization\": settings().server.auth.secret}\n    )\n    assert response_success.status_code == 200\n", "tests/server/chunks/test_chunk_routes.py": "from pathlib import Path\n\nfrom fastapi.testclient import TestClient\n\nfrom private_gpt.server.chunks.chunks_router import ChunksBody, ChunksResponse\nfrom tests.fixtures.ingest_helper import IngestHelper\n\n\ndef test_chunks_retrieval(test_client: TestClient, ingest_helper: IngestHelper) -> None:\n    # Make sure there is at least some chunk to query in the database\n    path = Path(__file__).parents[0] / \"chunk_test.txt\"\n    ingest_helper.ingest_file(path)\n\n    body = ChunksBody(text=\"b483dd15-78c4-4d67-b546-21a0d690bf43\")\n    response = test_client.post(\"/v1/chunks\", json=body.model_dump())\n    assert response.status_code == 200\n    chunk_response = ChunksResponse.model_validate(response.json())\n    assert len(chunk_response.data) > 0\n", "tests/server/ingest/test_ingest_routes.py": "import tempfile\nfrom pathlib import Path\n\nfrom fastapi.testclient import TestClient\n\nfrom private_gpt.server.ingest.ingest_router import IngestResponse\nfrom tests.fixtures.ingest_helper import IngestHelper\n\n\ndef test_ingest_accepts_txt_files(ingest_helper: IngestHelper) -> None:\n    path = Path(__file__).parents[0] / \"test.txt\"\n    ingest_result = ingest_helper.ingest_file(path)\n    assert len(ingest_result.data) == 1\n\n\ndef test_ingest_accepts_pdf_files(ingest_helper: IngestHelper) -> None:\n    path = Path(__file__).parents[0] / \"test.pdf\"\n    ingest_result = ingest_helper.ingest_file(path)\n    assert len(ingest_result.data) == 1\n\n\ndef test_ingest_list_returns_something_after_ingestion(\n    test_client: TestClient, ingest_helper: IngestHelper\n) -> None:\n    response_before = test_client.get(\"/v1/ingest/list\")\n    count_ingest_before = len(response_before.json()[\"data\"])\n    with tempfile.NamedTemporaryFile(\"w\", suffix=\".txt\") as test_file:\n        test_file.write(\"Foo bar; hello there!\")\n        test_file.flush()\n        test_file.seek(0)\n        ingest_result = ingest_helper.ingest_file(Path(test_file.name))\n    assert len(ingest_result.data) == 1, \"The temp doc should have been ingested\"\n    response_after = test_client.get(\"/v1/ingest/list\")\n    count_ingest_after = len(response_after.json()[\"data\"])\n    assert (\n        count_ingest_after == count_ingest_before + 1\n    ), \"The temp doc should be returned\"\n\n\ndef test_ingest_plain_text(test_client: TestClient) -> None:\n    response = test_client.post(\n        \"/v1/ingest/text\", json={\"file_name\": \"file_name\", \"text\": \"text\"}\n    )\n    assert response.status_code == 200\n    ingest_result = IngestResponse.model_validate(response.json())\n    assert len(ingest_result.data) == 1\n", "tests/server/embeddings/test_embedding_routes.py": "from fastapi.testclient import TestClient\n\nfrom private_gpt.server.embeddings.embeddings_router import (\n    EmbeddingsBody,\n    EmbeddingsResponse,\n)\n\n\ndef test_embeddings_generation(test_client: TestClient) -> None:\n    body = EmbeddingsBody(input=\"Embed me\")\n    response = test_client.post(\"/v1/embeddings\", json=body.model_dump())\n\n    assert response.status_code == 200\n    embedding_response = EmbeddingsResponse.model_validate(response.json())\n    assert len(embedding_response.data) > 0\n    assert len(embedding_response.data[0].embedding) > 0\n", "tests/fixtures/fast_api_test_client.py": "import pytest\nfrom fastapi.testclient import TestClient\n\nfrom private_gpt.launcher import create_app\nfrom tests.fixtures.mock_injector import MockInjector\n\n\n@pytest.fixture()\ndef test_client(request: pytest.FixtureRequest, injector: MockInjector) -> TestClient:\n    if request is not None and hasattr(request, \"param\"):\n        injector.bind_settings(request.param or {})\n\n    app_under_test = create_app(injector.test_injector)\n    return TestClient(app_under_test)\n", "tests/fixtures/auto_close_qdrant.py": "import pytest\n\nfrom private_gpt.components.vector_store.vector_store_component import (\n    VectorStoreComponent,\n)\nfrom tests.fixtures.mock_injector import MockInjector\n\n\n@pytest.fixture(autouse=True)\ndef _auto_close_vector_store_client(injector: MockInjector) -> None:\n    \"\"\"Auto close VectorStore client after each test.\n\n    VectorStore client (qdrant/chromadb) opens a connection the\n    Database that causes issues when running tests too fast,\n    so close explicitly after each test.\n    \"\"\"\n    yield\n    injector.get(VectorStoreComponent).close()\n", "tests/fixtures/__init__.py": "\"\"\"Global fixtures.\"\"\"\n", "tests/fixtures/ingest_helper.py": "from pathlib import Path\n\nimport pytest\nfrom fastapi.testclient import TestClient\n\nfrom private_gpt.server.ingest.ingest_router import IngestResponse\n\n\nclass IngestHelper:\n    def __init__(self, test_client: TestClient):\n        self.test_client = test_client\n\n    def ingest_file(self, path: Path) -> IngestResponse:\n        files = {\"file\": (path.name, path.open(\"rb\"))}\n\n        response = self.test_client.post(\"/v1/ingest/file\", files=files)\n        assert response.status_code == 200\n        ingest_result = IngestResponse.model_validate(response.json())\n        return ingest_result\n\n\n@pytest.fixture()\ndef ingest_helper(test_client: TestClient) -> IngestHelper:\n    return IngestHelper(test_client)\n", "tests/fixtures/mock_injector.py": "from collections.abc import Callable\nfrom typing import Any\nfrom unittest.mock import MagicMock\n\nimport pytest\nfrom injector import Provider, ScopeDecorator, singleton\n\nfrom private_gpt.di import create_application_injector\nfrom private_gpt.settings.settings import Settings, unsafe_settings\nfrom private_gpt.settings.settings_loader import merge_settings\nfrom private_gpt.utils.typing import T\n\n\nclass MockInjector:\n    def __init__(self) -> None:\n        self.test_injector = create_application_injector()\n\n    def bind_mock(\n        self,\n        interface: type[T],\n        mock: (T | (Callable[..., T] | Provider[T])) | None = None,\n        *,\n        scope: ScopeDecorator = singleton,\n    ) -> T:\n        if mock is None:\n            mock = MagicMock()\n        self.test_injector.binder.bind(interface, to=mock, scope=scope)\n        return mock  # type: ignore\n\n    def bind_settings(self, settings: dict[str, Any]) -> Settings:\n        merged = merge_settings([unsafe_settings, settings])\n        new_settings = Settings(**merged)\n        self.test_injector.binder.bind(Settings, new_settings)\n        return new_settings\n\n    def get(self, interface: type[T]) -> T:\n        return self.test_injector.get(interface)\n\n\n@pytest.fixture()\ndef injector() -> MockInjector:\n    return MockInjector()\n", "private_gpt/paths.py": "from pathlib import Path\n\nfrom private_gpt.constants import PROJECT_ROOT_PATH\nfrom private_gpt.settings.settings import settings\n\n\ndef _absolute_or_from_project_root(path: str) -> Path:\n    if path.startswith(\"/\"):\n        return Path(path)\n    return PROJECT_ROOT_PATH / path\n\n\nmodels_path: Path = PROJECT_ROOT_PATH / \"models\"\nmodels_cache_path: Path = models_path / \"cache\"\ndocs_path: Path = PROJECT_ROOT_PATH / \"docs\"\nlocal_data_path: Path = _absolute_or_from_project_root(\n    settings().data.local_data_folder\n)\n", "private_gpt/di.py": "from injector import Injector\n\nfrom private_gpt.settings.settings import Settings, unsafe_typed_settings\n\n\ndef create_application_injector() -> Injector:\n    _injector = Injector(auto_bind=True)\n    _injector.binder.bind(Settings, to=unsafe_typed_settings)\n    return _injector\n\n\n\"\"\"\nGlobal injector for the application.\n\nAvoid using this reference, it will make your code harder to test.\n\nInstead, use the `request.state.injector` reference, which is bound to every request\n\"\"\"\nglobal_injector: Injector = create_application_injector()\n", "private_gpt/constants.py": "from pathlib import Path\n\nPROJECT_ROOT_PATH: Path = Path(__file__).parents[1]\n", "private_gpt/main.py": "\"\"\"FastAPI app creation, logger configuration and main API routes.\"\"\"\n\nfrom private_gpt.di import global_injector\nfrom private_gpt.launcher import create_app\n\napp = create_app(global_injector)\n", "private_gpt/__main__.py": "# start a fastapi server with uvicorn\n\nimport uvicorn\n\nfrom private_gpt.main import app\nfrom private_gpt.settings.settings import settings\n\n# Set log_config=None to do not use the uvicorn logging configuration, and\n# use ours instead. For reference, see below:\n# https://github.com/tiangolo/fastapi/discussions/7457#discussioncomment-5141108\nuvicorn.run(app, host=\"0.0.0.0\", port=settings().server.port, log_config=None)\n", "private_gpt/__init__.py": "\"\"\"private-gpt.\"\"\"\n\nimport logging\nimport os\n\n# Set to 'DEBUG' to have extensive logging turned on, even for libraries\nROOT_LOG_LEVEL = \"INFO\"\n\nPRETTY_LOG_FORMAT = (\n    \"%(asctime)s.%(msecs)03d [%(levelname)-8s] %(name)+25s - %(message)s\"\n)\nlogging.basicConfig(level=ROOT_LOG_LEVEL, format=PRETTY_LOG_FORMAT, datefmt=\"%H:%M:%S\")\nlogging.captureWarnings(True)\n\n# Disable gradio analytics\n# This is done this way because gradio does not solely rely on what values are\n# passed to gr.Blocks(enable_analytics=...) but also on the environment\n# variable GRADIO_ANALYTICS_ENABLED. `gradio.strings` actually reads this env\n# directly, so to fully disable gradio analytics we need to set this env var.\nos.environ[\"GRADIO_ANALYTICS_ENABLED\"] = \"False\"\n\n# Disable chromaDB telemetry\n# It is already disabled, see PR#1144\n# os.environ[\"ANONYMIZED_TELEMETRY\"] = \"False\"\n\n# adding tiktoken cache path within repo to be able to run in offline environment.\nos.environ[\"TIKTOKEN_CACHE_DIR\"] = \"tiktoken_cache\"\n", "private_gpt/launcher.py": "\"\"\"FastAPI app creation, logger configuration and main API routes.\"\"\"\n\nimport logging\n\nfrom fastapi import Depends, FastAPI, Request\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom injector import Injector\nfrom llama_index.core.callbacks import CallbackManager\nfrom llama_index.core.callbacks.global_handlers import create_global_handler\nfrom llama_index.core.settings import Settings as LlamaIndexSettings\n\nfrom private_gpt.server.chat.chat_router import chat_router\nfrom private_gpt.server.chunks.chunks_router import chunks_router\nfrom private_gpt.server.completions.completions_router import completions_router\nfrom private_gpt.server.embeddings.embeddings_router import embeddings_router\nfrom private_gpt.server.health.health_router import health_router\nfrom private_gpt.server.ingest.ingest_router import ingest_router\nfrom private_gpt.settings.settings import Settings\n\nlogger = logging.getLogger(__name__)\n\n\ndef create_app(root_injector: Injector) -> FastAPI:\n\n    # Start the API\n    async def bind_injector_to_request(request: Request) -> None:\n        request.state.injector = root_injector\n\n    app = FastAPI(dependencies=[Depends(bind_injector_to_request)])\n\n    app.include_router(completions_router)\n    app.include_router(chat_router)\n    app.include_router(chunks_router)\n    app.include_router(ingest_router)\n    app.include_router(embeddings_router)\n    app.include_router(health_router)\n\n    # Add LlamaIndex simple observability\n    global_handler = create_global_handler(\"simple\")\n    LlamaIndexSettings.callback_manager = CallbackManager([global_handler])\n\n    settings = root_injector.get(Settings)\n    if settings.server.cors.enabled:\n        logger.debug(\"Setting up CORS middleware\")\n        app.add_middleware(\n            CORSMiddleware,\n            allow_credentials=settings.server.cors.allow_credentials,\n            allow_origins=settings.server.cors.allow_origins,\n            allow_origin_regex=settings.server.cors.allow_origin_regex,\n            allow_methods=settings.server.cors.allow_methods,\n            allow_headers=settings.server.cors.allow_headers,\n        )\n\n    if settings.ui.enabled:\n        logger.debug(\"Importing the UI module\")\n        try:\n            from private_gpt.ui.ui import PrivateGptUi\n        except ImportError as e:\n            raise ImportError(\n                \"UI dependencies not found, install with `poetry install --extras ui`\"\n            ) from e\n\n        ui = root_injector.get(PrivateGptUi)\n        ui.mount_in_app(app, settings.ui.path)\n\n    return app\n", "private_gpt/utils/eta.py": "import datetime\nimport logging\nimport math\nimport time\nfrom collections import deque\nfrom typing import Any\n\nlogger = logging.getLogger(__name__)\n\n\ndef human_time(*args: Any, **kwargs: Any) -> str:\n    def timedelta_total_seconds(timedelta: datetime.timedelta) -> float:\n        return (\n            timedelta.microseconds\n            + 0.0\n            + (timedelta.seconds + timedelta.days * 24 * 3600) * 10**6\n        ) / 10**6\n\n    secs = float(timedelta_total_seconds(datetime.timedelta(*args, **kwargs)))\n    # We want (ms) precision below 2 seconds\n    if secs < 2:\n        return f\"{secs * 1000}ms\"\n    units = [(\"y\", 86400 * 365), (\"d\", 86400), (\"h\", 3600), (\"m\", 60), (\"s\", 1)]\n    parts = []\n    for unit, mul in units:\n        if secs / mul >= 1 or mul == 1:\n            if mul > 1:\n                n = int(math.floor(secs / mul))\n                secs -= n * mul\n            else:\n                # >2s we drop the (ms) component.\n                n = int(secs)\n            if n:\n                parts.append(f\"{n}{unit}\")\n    return \" \".join(parts)\n\n\ndef eta(iterator: list[Any]) -> Any:\n    \"\"\"Report an ETA after 30s and every 60s thereafter.\"\"\"\n    total = len(iterator)\n    _eta = ETA(total)\n    _eta.needReport(30)\n    for processed, data in enumerate(iterator, start=1):\n        yield data\n        _eta.update(processed)\n        if _eta.needReport(60):\n            logger.info(f\"{processed}/{total} - ETA {_eta.human_time()}\")\n\n\nclass ETA:\n    \"\"\"Predict how long something will take to complete.\"\"\"\n\n    def __init__(self, total: int):\n        self.total: int = total  # Total expected records.\n        self.rate: float = 0.0  # per second\n        self._timing_data: deque[tuple[float, int]] = deque(maxlen=100)\n        self.secondsLeft: float = 0.0\n        self.nexttime: float = 0.0\n\n    def human_time(self) -> str:\n        if self._calc():\n            return f\"{human_time(seconds=self.secondsLeft)} @ {int(self.rate * 60)}/min\"\n        return \"(computing)\"\n\n    def update(self, count: int) -> None:\n        # count should be in the range 0 to self.total\n        assert count > 0\n        assert count <= self.total\n        self._timing_data.append((time.time(), count))  # (X,Y) for pearson\n\n    def needReport(self, whenSecs: int) -> bool:\n        now = time.time()\n        if now > self.nexttime:\n            self.nexttime = now + whenSecs\n            return True\n        return False\n\n    def _calc(self) -> bool:\n        # A sample before a prediction.   Need two points to compute slope!\n        if len(self._timing_data) < 3:\n            return False\n\n        # http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient\n        # Calculate means and standard deviations.\n        samples = len(self._timing_data)\n        # column wise sum of the timing tuples to compute their mean.\n        mean_x, mean_y = (\n            sum(i) / samples for i in zip(*self._timing_data, strict=False)\n        )\n        std_x = math.sqrt(\n            sum(pow(i[0] - mean_x, 2) for i in self._timing_data) / (samples - 1)\n        )\n        std_y = math.sqrt(\n            sum(pow(i[1] - mean_y, 2) for i in self._timing_data) / (samples - 1)\n        )\n\n        # Calculate coefficient.\n        sum_xy, sum_sq_v_x, sum_sq_v_y = 0.0, 0.0, 0\n        for x, y in self._timing_data:\n            x -= mean_x\n            y -= mean_y\n            sum_xy += x * y\n            sum_sq_v_x += pow(x, 2)\n            sum_sq_v_y += pow(y, 2)\n        pearson_r = sum_xy / math.sqrt(sum_sq_v_x * sum_sq_v_y)\n\n        # Calculate regression line.\n        # y = mx + b where m is the slope and b is the y-intercept.\n        m = self.rate = pearson_r * (std_y / std_x)\n        y = self.total\n        b = mean_y - m * mean_x\n        x = (y - b) / m\n\n        # Calculate fitted line (transformed/shifted regression line horizontally).\n        fitted_b = self._timing_data[-1][1] - (m * self._timing_data[-1][0])\n        fitted_x = (y - fitted_b) / m\n        _, count = self._timing_data[-1]  # adjust last data point progress count\n        adjusted_x = ((fitted_x - x) * (count / self.total)) + x\n        eta_epoch = adjusted_x\n\n        self.secondsLeft = max([eta_epoch - time.time(), 0])\n        return True\n", "private_gpt/utils/typing.py": "from typing import TypeVar\n\nT = TypeVar(\"T\")\nK = TypeVar(\"K\")\nV = TypeVar(\"V\")\n", "private_gpt/utils/__init__.py": "\"\"\"general utils.\"\"\"\n", "private_gpt/components/__init__.py": "", "private_gpt/components/embedding/embedding_component.py": "import logging\n\nfrom injector import inject, singleton\nfrom llama_index.core.embeddings import BaseEmbedding, MockEmbedding\n\nfrom private_gpt.paths import models_cache_path\nfrom private_gpt.settings.settings import Settings\n\nlogger = logging.getLogger(__name__)\n\n\n@singleton\nclass EmbeddingComponent:\n    embedding_model: BaseEmbedding\n\n    @inject\n    def __init__(self, settings: Settings) -> None:\n        embedding_mode = settings.embedding.mode\n        logger.info(\"Initializing the embedding model in mode=%s\", embedding_mode)\n        match embedding_mode:\n            case \"huggingface\":\n                try:\n                    from llama_index.embeddings.huggingface import (  # type: ignore\n                        HuggingFaceEmbedding,\n                    )\n                except ImportError as e:\n                    raise ImportError(\n                        \"Local dependencies not found, install with `poetry install --extras embeddings-huggingface`\"\n                    ) from e\n\n                self.embedding_model = HuggingFaceEmbedding(\n                    model_name=settings.huggingface.embedding_hf_model_name,\n                    cache_folder=str(models_cache_path),\n                )\n            case \"sagemaker\":\n                try:\n                    from private_gpt.components.embedding.custom.sagemaker import (\n                        SagemakerEmbedding,\n                    )\n                except ImportError as e:\n                    raise ImportError(\n                        \"Sagemaker dependencies not found, install with `poetry install --extras embeddings-sagemaker`\"\n                    ) from e\n\n                self.embedding_model = SagemakerEmbedding(\n                    endpoint_name=settings.sagemaker.embedding_endpoint_name,\n                )\n            case \"openai\":\n                try:\n                    from llama_index.embeddings.openai import (  # type: ignore\n                        OpenAIEmbedding,\n                    )\n                except ImportError as e:\n                    raise ImportError(\n                        \"OpenAI dependencies not found, install with `poetry install --extras embeddings-openai`\"\n                    ) from e\n\n                api_base = (\n                    settings.openai.embedding_api_base or settings.openai.api_base\n                )\n                api_key = settings.openai.embedding_api_key or settings.openai.api_key\n                model = settings.openai.embedding_model\n\n                self.embedding_model = OpenAIEmbedding(\n                    api_base=api_base,\n                    api_key=api_key,\n                    model=model,\n                )\n            case \"ollama\":\n                try:\n                    from llama_index.embeddings.ollama import (  # type: ignore\n                        OllamaEmbedding,\n                    )\n                except ImportError as e:\n                    raise ImportError(\n                        \"Local dependencies not found, install with `poetry install --extras embeddings-ollama`\"\n                    ) from e\n\n                ollama_settings = settings.ollama\n                self.embedding_model = OllamaEmbedding(\n                    model_name=ollama_settings.embedding_model,\n                    base_url=ollama_settings.embedding_api_base,\n                )\n            case \"azopenai\":\n                try:\n                    from llama_index.embeddings.azure_openai import (  # type: ignore\n                        AzureOpenAIEmbedding,\n                    )\n                except ImportError as e:\n                    raise ImportError(\n                        \"Azure OpenAI dependencies not found, install with `poetry install --extras embeddings-azopenai`\"\n                    ) from e\n\n                azopenai_settings = settings.azopenai\n                self.embedding_model = AzureOpenAIEmbedding(\n                    model=azopenai_settings.embedding_model,\n                    deployment_name=azopenai_settings.embedding_deployment_name,\n                    api_key=azopenai_settings.api_key,\n                    azure_endpoint=azopenai_settings.azure_endpoint,\n                    api_version=azopenai_settings.api_version,\n                )\n            case \"mock\":\n                # Not a random number, is the dimensionality used by\n                # the default embedding model\n                self.embedding_model = MockEmbedding(384)\n", "private_gpt/components/embedding/__init__.py": "", "private_gpt/components/embedding/custom/__init__.py": "", "private_gpt/components/embedding/custom/sagemaker.py": "# mypy: ignore-errors\nimport json\nfrom typing import Any\n\nimport boto3\nfrom llama_index.core.base.embeddings.base import BaseEmbedding\nfrom pydantic import Field, PrivateAttr\n\n\nclass SagemakerEmbedding(BaseEmbedding):\n    \"\"\"Sagemaker Embedding Endpoint.\n\n    To use, you must supply the endpoint name from your deployed\n    Sagemaker embedding model & the region where it is deployed.\n\n    To authenticate, the AWS client uses the following methods to\n    automatically load credentials:\n    https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html\n\n    If a specific credential profile should be used, you must pass\n    the name of the profile from the ~/.aws/credentials file that is to be used.\n\n    Make sure the credentials / roles used have the required policies to\n    access the Sagemaker endpoint.\n    See: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html\n    \"\"\"\n\n    endpoint_name: str = Field(description=\"\")\n\n    _boto_client: Any = boto3.client(\n        \"sagemaker-runtime\",\n    )  # TODO make it an optional field\n\n    _async_not_implemented_warned: bool = PrivateAttr(default=False)\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"SagemakerEmbedding\"\n\n    def _async_not_implemented_warn_once(self) -> None:\n        if not self._async_not_implemented_warned:\n            print(\"Async embedding not available, falling back to sync method.\")\n            self._async_not_implemented_warned = True\n\n    def _embed(self, sentences: list[str]) -> list[list[float]]:\n        request_params = {\n            \"inputs\": sentences,\n        }\n\n        resp = self._boto_client.invoke_endpoint(\n            EndpointName=self.endpoint_name,\n            Body=json.dumps(request_params),\n            ContentType=\"application/json\",\n        )\n\n        response_body = resp[\"Body\"]\n        response_str = response_body.read().decode(\"utf-8\")\n        response_json = json.loads(response_str)\n\n        return response_json[\"vectors\"]\n\n    def _get_query_embedding(self, query: str) -> list[float]:\n        \"\"\"Get query embedding.\"\"\"\n        return self._embed([query])[0]\n\n    async def _aget_query_embedding(self, query: str) -> list[float]:\n        # Warn the user that sync is being used\n        self._async_not_implemented_warn_once()\n        return self._get_query_embedding(query)\n\n    async def _aget_text_embedding(self, text: str) -> list[float]:\n        # Warn the user that sync is being used\n        self._async_not_implemented_warn_once()\n        return self._get_text_embedding(text)\n\n    def _get_text_embedding(self, text: str) -> list[float]:\n        \"\"\"Get text embedding.\"\"\"\n        return self._embed([text])[0]\n\n    def _get_text_embeddings(self, texts: list[str]) -> list[list[float]]:\n        \"\"\"Get text embeddings.\"\"\"\n        return self._embed(texts)\n", "private_gpt/components/ingest/ingest_component.py": "import abc\nimport itertools\nimport logging\nimport multiprocessing\nimport multiprocessing.pool\nimport os\nimport threading\nfrom pathlib import Path\nfrom queue import Queue\nfrom typing import Any\n\nfrom llama_index.core.data_structs import IndexDict\nfrom llama_index.core.embeddings.utils import EmbedType\nfrom llama_index.core.indices import VectorStoreIndex, load_index_from_storage\nfrom llama_index.core.indices.base import BaseIndex\nfrom llama_index.core.ingestion import run_transformations\nfrom llama_index.core.schema import BaseNode, Document, TransformComponent\nfrom llama_index.core.storage import StorageContext\n\nfrom private_gpt.components.ingest.ingest_helper import IngestionHelper\nfrom private_gpt.paths import local_data_path\nfrom private_gpt.settings.settings import Settings\nfrom private_gpt.utils.eta import eta\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseIngestComponent(abc.ABC):\n    def __init__(\n        self,\n        storage_context: StorageContext,\n        embed_model: EmbedType,\n        transformations: list[TransformComponent],\n        *args: Any,\n        **kwargs: Any,\n    ) -> None:\n        logger.debug(\"Initializing base ingest component type=%s\", type(self).__name__)\n        self.storage_context = storage_context\n        self.embed_model = embed_model\n        self.transformations = transformations\n\n    @abc.abstractmethod\n    def ingest(self, file_name: str, file_data: Path) -> list[Document]:\n        pass\n\n    @abc.abstractmethod\n    def bulk_ingest(self, files: list[tuple[str, Path]]) -> list[Document]:\n        pass\n\n    @abc.abstractmethod\n    def delete(self, doc_id: str) -> None:\n        pass\n\n\nclass BaseIngestComponentWithIndex(BaseIngestComponent, abc.ABC):\n    def __init__(\n        self,\n        storage_context: StorageContext,\n        embed_model: EmbedType,\n        transformations: list[TransformComponent],\n        *args: Any,\n        **kwargs: Any,\n    ) -> None:\n        super().__init__(storage_context, embed_model, transformations, *args, **kwargs)\n\n        self.show_progress = True\n        self._index_thread_lock = (\n            threading.Lock()\n        )  # Thread lock! Not Multiprocessing lock\n        self._index = self._initialize_index()\n\n    def _initialize_index(self) -> BaseIndex[IndexDict]:\n        \"\"\"Initialize the index from the storage context.\"\"\"\n        try:\n            # Load the index with store_nodes_override=True to be able to delete them\n            index = load_index_from_storage(\n                storage_context=self.storage_context,\n                store_nodes_override=True,  # Force store nodes in index and document stores\n                show_progress=self.show_progress,\n                embed_model=self.embed_model,\n                transformations=self.transformations,\n            )\n        except ValueError:\n            # There are no index in the storage context, creating a new one\n            logger.info(\"Creating a new vector store index\")\n            index = VectorStoreIndex.from_documents(\n                [],\n                storage_context=self.storage_context,\n                store_nodes_override=True,  # Force store nodes in index and document stores\n                show_progress=self.show_progress,\n                embed_model=self.embed_model,\n                transformations=self.transformations,\n            )\n            index.storage_context.persist(persist_dir=local_data_path)\n        return index\n\n    def _save_index(self) -> None:\n        self._index.storage_context.persist(persist_dir=local_data_path)\n\n    def delete(self, doc_id: str) -> None:\n        with self._index_thread_lock:\n            # Delete the document from the index\n            self._index.delete_ref_doc(doc_id, delete_from_docstore=True)\n\n            # Save the index\n            self._save_index()\n\n\nclass SimpleIngestComponent(BaseIngestComponentWithIndex):\n    def __init__(\n        self,\n        storage_context: StorageContext,\n        embed_model: EmbedType,\n        transformations: list[TransformComponent],\n        *args: Any,\n        **kwargs: Any,\n    ) -> None:\n        super().__init__(storage_context, embed_model, transformations, *args, **kwargs)\n\n    def ingest(self, file_name: str, file_data: Path) -> list[Document]:\n        logger.info(\"Ingesting file_name=%s\", file_name)\n        documents = IngestionHelper.transform_file_into_documents(file_name, file_data)\n        logger.info(\n            \"Transformed file=%s into count=%s documents\", file_name, len(documents)\n        )\n        logger.debug(\"Saving the documents in the index and doc store\")\n        return self._save_docs(documents)\n\n    def bulk_ingest(self, files: list[tuple[str, Path]]) -> list[Document]:\n        saved_documents = []\n        for file_name, file_data in files:\n            documents = IngestionHelper.transform_file_into_documents(\n                file_name, file_data\n            )\n            saved_documents.extend(self._save_docs(documents))\n        return saved_documents\n\n    def _save_docs(self, documents: list[Document]) -> list[Document]:\n        logger.debug(\"Transforming count=%s documents into nodes\", len(documents))\n        with self._index_thread_lock:\n            for document in documents:\n                self._index.insert(document, show_progress=True)\n            logger.debug(\"Persisting the index and nodes\")\n            # persist the index and nodes\n            self._save_index()\n            logger.debug(\"Persisted the index and nodes\")\n        return documents\n\n\nclass BatchIngestComponent(BaseIngestComponentWithIndex):\n    \"\"\"Parallelize the file reading and parsing on multiple CPU core.\n\n    This also makes the embeddings to be computed in batches (on GPU or CPU).\n    \"\"\"\n\n    def __init__(\n        self,\n        storage_context: StorageContext,\n        embed_model: EmbedType,\n        transformations: list[TransformComponent],\n        count_workers: int,\n        *args: Any,\n        **kwargs: Any,\n    ) -> None:\n        super().__init__(storage_context, embed_model, transformations, *args, **kwargs)\n        # Make an efficient use of the CPU and GPU, the embedding\n        # must be in the transformations\n        assert (\n            len(self.transformations) >= 2\n        ), \"Embeddings must be in the transformations\"\n        assert count_workers > 0, \"count_workers must be > 0\"\n        self.count_workers = count_workers\n\n        self._file_to_documents_work_pool = multiprocessing.Pool(\n            processes=self.count_workers\n        )\n\n    def ingest(self, file_name: str, file_data: Path) -> list[Document]:\n        logger.info(\"Ingesting file_name=%s\", file_name)\n        documents = IngestionHelper.transform_file_into_documents(file_name, file_data)\n        logger.info(\n            \"Transformed file=%s into count=%s documents\", file_name, len(documents)\n        )\n        logger.debug(\"Saving the documents in the index and doc store\")\n        return self._save_docs(documents)\n\n    def bulk_ingest(self, files: list[tuple[str, Path]]) -> list[Document]:\n        documents = list(\n            itertools.chain.from_iterable(\n                self._file_to_documents_work_pool.starmap(\n                    IngestionHelper.transform_file_into_documents, files\n                )\n            )\n        )\n        logger.info(\n            \"Transformed count=%s files into count=%s documents\",\n            len(files),\n            len(documents),\n        )\n        return self._save_docs(documents)\n\n    def _save_docs(self, documents: list[Document]) -> list[Document]:\n        logger.debug(\"Transforming count=%s documents into nodes\", len(documents))\n        nodes = run_transformations(\n            documents,  # type: ignore[arg-type]\n            self.transformations,\n            show_progress=self.show_progress,\n        )\n        # Locking the index to avoid concurrent writes\n        with self._index_thread_lock:\n            logger.info(\"Inserting count=%s nodes in the index\", len(nodes))\n            self._index.insert_nodes(nodes, show_progress=True)\n            for document in documents:\n                self._index.docstore.set_document_hash(\n                    document.get_doc_id(), document.hash\n                )\n            logger.debug(\"Persisting the index and nodes\")\n            # persist the index and nodes\n            self._save_index()\n            logger.debug(\"Persisted the index and nodes\")\n        return documents\n\n\nclass ParallelizedIngestComponent(BaseIngestComponentWithIndex):\n    \"\"\"Parallelize the file ingestion (file reading, embeddings, and index insertion).\n\n    This use the CPU and GPU in parallel (both running at the same time), and\n    reduce the memory pressure by not loading all the files in memory at the same time.\n    \"\"\"\n\n    def __init__(\n        self,\n        storage_context: StorageContext,\n        embed_model: EmbedType,\n        transformations: list[TransformComponent],\n        count_workers: int,\n        *args: Any,\n        **kwargs: Any,\n    ) -> None:\n        super().__init__(storage_context, embed_model, transformations, *args, **kwargs)\n        # To make an efficient use of the CPU and GPU, the embeddings\n        # must be in the transformations (to be computed in batches)\n        assert (\n            len(self.transformations) >= 2\n        ), \"Embeddings must be in the transformations\"\n        assert count_workers > 0, \"count_workers must be > 0\"\n        self.count_workers = count_workers\n        # We are doing our own multiprocessing\n        # To do not collide with the multiprocessing of huggingface, we disable it\n        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n        self._ingest_work_pool = multiprocessing.pool.ThreadPool(\n            processes=self.count_workers\n        )\n\n        self._file_to_documents_work_pool = multiprocessing.Pool(\n            processes=self.count_workers\n        )\n\n    def ingest(self, file_name: str, file_data: Path) -> list[Document]:\n        logger.info(\"Ingesting file_name=%s\", file_name)\n        # Running in a single (1) process to release the current\n        # thread, and take a dedicated CPU core for computation\n        documents = self._file_to_documents_work_pool.apply(\n            IngestionHelper.transform_file_into_documents, (file_name, file_data)\n        )\n        logger.info(\n            \"Transformed file=%s into count=%s documents\", file_name, len(documents)\n        )\n        logger.debug(\"Saving the documents in the index and doc store\")\n        return self._save_docs(documents)\n\n    def bulk_ingest(self, files: list[tuple[str, Path]]) -> list[Document]:\n        # Lightweight threads, used for parallelize the\n        # underlying IO calls made in the ingestion\n\n        documents = list(\n            itertools.chain.from_iterable(\n                self._ingest_work_pool.starmap(self.ingest, files)\n            )\n        )\n        return documents\n\n    def _save_docs(self, documents: list[Document]) -> list[Document]:\n        logger.debug(\"Transforming count=%s documents into nodes\", len(documents))\n        nodes = run_transformations(\n            documents,  # type: ignore[arg-type]\n            self.transformations,\n            show_progress=self.show_progress,\n        )\n        # Locking the index to avoid concurrent writes\n        with self._index_thread_lock:\n            logger.info(\"Inserting count=%s nodes in the index\", len(nodes))\n            self._index.insert_nodes(nodes, show_progress=True)\n            for document in documents:\n                self._index.docstore.set_document_hash(\n                    document.get_doc_id(), document.hash\n                )\n            logger.debug(\"Persisting the index and nodes\")\n            # persist the index and nodes\n            self._save_index()\n            logger.debug(\"Persisted the index and nodes\")\n        return documents\n\n    def __del__(self) -> None:\n        # We need to do the appropriate cleanup of the multiprocessing pools\n        # when the object is deleted. Using root logger to avoid\n        # the logger to be deleted before the pool\n        logging.debug(\"Closing the ingest work pool\")\n        self._ingest_work_pool.close()\n        self._ingest_work_pool.join()\n        self._ingest_work_pool.terminate()\n        logging.debug(\"Closing the file to documents work pool\")\n        self._file_to_documents_work_pool.close()\n        self._file_to_documents_work_pool.join()\n        self._file_to_documents_work_pool.terminate()\n\n\nclass PipelineIngestComponent(BaseIngestComponentWithIndex):\n    \"\"\"Pipeline ingestion - keeping the embedding worker pool as busy as possible.\n\n    This class implements a threaded ingestion pipeline, which comprises two threads\n    and two queues. The primary thread is responsible for reading and parsing files\n    into documents. These documents are then placed into a queue, which is\n    distributed to a pool of worker processes for embedding computation. After\n    embedding, the documents are transferred to another queue where they are\n    accumulated until a threshold is reached. Upon reaching this threshold, the\n    accumulated documents are flushed to the document store, index, and vector\n    store.\n\n    Exception handling ensures robustness against erroneous files. However, in the\n    pipelined design, one error can lead to the discarding of multiple files. Any\n    discarded files will be reported.\n    \"\"\"\n\n    NODE_FLUSH_COUNT = 5000  # Save the index every # nodes.\n\n    def __init__(\n        self,\n        storage_context: StorageContext,\n        embed_model: EmbedType,\n        transformations: list[TransformComponent],\n        count_workers: int,\n        *args: Any,\n        **kwargs: Any,\n    ) -> None:\n        super().__init__(storage_context, embed_model, transformations, *args, **kwargs)\n        self.count_workers = count_workers\n        assert (\n            len(self.transformations) >= 2\n        ), \"Embeddings must be in the transformations\"\n        assert count_workers > 0, \"count_workers must be > 0\"\n        self.count_workers = count_workers\n        # We are doing our own multiprocessing\n        # To do not collide with the multiprocessing of huggingface, we disable it\n        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n        # doc_q stores parsed files as Document chunks.\n        # Using a shallow queue causes the filesystem parser to block\n        # when it reaches capacity. This ensures it doesn't outpace the\n        # computationally intensive embeddings phase, avoiding unnecessary\n        # memory consumption.  The semaphore is used to bound the async worker\n        # embedding computations to cause the doc Q to fill and block.\n        self.doc_semaphore = multiprocessing.Semaphore(\n            self.count_workers\n        )  # limit the doc queue to # items.\n        self.doc_q: Queue[tuple[str, str | None, list[Document] | None]] = Queue(20)\n        # node_q stores documents parsed into nodes (embeddings).\n        # Larger queue size so we don't block the embedding workers during a slow\n        # index update.\n        self.node_q: Queue[\n            tuple[str, str | None, list[Document] | None, list[BaseNode] | None]\n        ] = Queue(40)\n        threading.Thread(target=self._doc_to_node, daemon=True).start()\n        threading.Thread(target=self._write_nodes, daemon=True).start()\n\n    def _doc_to_node(self) -> None:\n        # Parse documents into nodes\n        with multiprocessing.pool.ThreadPool(processes=self.count_workers) as pool:\n            while True:\n                try:\n                    cmd, file_name, documents = self.doc_q.get(\n                        block=True\n                    )  # Documents for a file\n                    if cmd == \"process\":\n                        # Push CPU/GPU embedding work to the worker pool\n                        # Acquire semaphore to control access to worker pool\n                        self.doc_semaphore.acquire()\n                        pool.apply_async(\n                            self._doc_to_node_worker, (file_name, documents)\n                        )\n                    elif cmd == \"quit\":\n                        break\n                finally:\n                    if cmd != \"process\":\n                        self.doc_q.task_done()  # unblock Q joins\n\n    def _doc_to_node_worker(self, file_name: str, documents: list[Document]) -> None:\n        # CPU/GPU intensive work in its own process\n        try:\n            nodes = run_transformations(\n                documents,  # type: ignore[arg-type]\n                self.transformations,\n                show_progress=self.show_progress,\n            )\n            self.node_q.put((\"process\", file_name, documents, nodes))\n        finally:\n            self.doc_semaphore.release()\n            self.doc_q.task_done()  # unblock Q joins\n\n    def _save_docs(\n        self, files: list[str], documents: list[Document], nodes: list[BaseNode]\n    ) -> None:\n        try:\n            logger.info(\n                f\"Saving {len(files)} files ({len(documents)} documents / {len(nodes)} nodes)\"\n            )\n            self._index.insert_nodes(nodes)\n            for document in documents:\n                self._index.docstore.set_document_hash(\n                    document.get_doc_id(), document.hash\n                )\n            self._save_index()\n        except Exception:\n            # Tell the user so they can investigate these files\n            logger.exception(f\"Processing files {files}\")\n        finally:\n            # Clearing work, even on exception, maintains a clean state.\n            nodes.clear()\n            documents.clear()\n            files.clear()\n\n    def _write_nodes(self) -> None:\n        # Save nodes to index.  I/O intensive.\n        node_stack: list[BaseNode] = []\n        doc_stack: list[Document] = []\n        file_stack: list[str] = []\n        while True:\n            try:\n                cmd, file_name, documents, nodes = self.node_q.get(block=True)\n                if cmd in (\"flush\", \"quit\"):\n                    if file_stack:\n                        self._save_docs(file_stack, doc_stack, node_stack)\n                    if cmd == \"quit\":\n                        break\n                elif cmd == \"process\":\n                    node_stack.extend(nodes)  # type: ignore[arg-type]\n                    doc_stack.extend(documents)  # type: ignore[arg-type]\n                    file_stack.append(file_name)  # type: ignore[arg-type]\n                    # Constant saving is heavy on I/O - accumulate to a threshold\n                    if len(node_stack) >= self.NODE_FLUSH_COUNT:\n                        self._save_docs(file_stack, doc_stack, node_stack)\n            finally:\n                self.node_q.task_done()\n\n    def _flush(self) -> None:\n        self.doc_q.put((\"flush\", None, None))\n        self.doc_q.join()\n        self.node_q.put((\"flush\", None, None, None))\n        self.node_q.join()\n\n    def ingest(self, file_name: str, file_data: Path) -> list[Document]:\n        documents = IngestionHelper.transform_file_into_documents(file_name, file_data)\n        self.doc_q.put((\"process\", file_name, documents))\n        self._flush()\n        return documents\n\n    def bulk_ingest(self, files: list[tuple[str, Path]]) -> list[Document]:\n        docs = []\n        for file_name, file_data in eta(files):\n            try:\n                documents = IngestionHelper.transform_file_into_documents(\n                    file_name, file_data\n                )\n                self.doc_q.put((\"process\", file_name, documents))\n                docs.extend(documents)\n            except Exception:\n                logger.exception(f\"Skipping {file_data.name}\")\n        self._flush()\n        return docs\n\n\ndef get_ingestion_component(\n    storage_context: StorageContext,\n    embed_model: EmbedType,\n    transformations: list[TransformComponent],\n    settings: Settings,\n) -> BaseIngestComponent:\n    \"\"\"Get the ingestion component for the given configuration.\"\"\"\n    ingest_mode = settings.embedding.ingest_mode\n    if ingest_mode == \"batch\":\n        return BatchIngestComponent(\n            storage_context=storage_context,\n            embed_model=embed_model,\n            transformations=transformations,\n            count_workers=settings.embedding.count_workers,\n        )\n    elif ingest_mode == \"parallel\":\n        return ParallelizedIngestComponent(\n            storage_context=storage_context,\n            embed_model=embed_model,\n            transformations=transformations,\n            count_workers=settings.embedding.count_workers,\n        )\n    elif ingest_mode == \"pipeline\":\n        return PipelineIngestComponent(\n            storage_context=storage_context,\n            embed_model=embed_model,\n            transformations=transformations,\n            count_workers=settings.embedding.count_workers,\n        )\n    else:\n        return SimpleIngestComponent(\n            storage_context=storage_context,\n            embed_model=embed_model,\n            transformations=transformations,\n        )\n", "private_gpt/components/ingest/__init__.py": "", "private_gpt/components/ingest/ingest_helper.py": "import logging\nfrom pathlib import Path\n\nfrom llama_index.core.readers import StringIterableReader\nfrom llama_index.core.readers.base import BaseReader\nfrom llama_index.core.readers.json import JSONReader\nfrom llama_index.core.schema import Document\n\nlogger = logging.getLogger(__name__)\n\n\n# Inspired by the `llama_index.core.readers.file.base` module\ndef _try_loading_included_file_formats() -> dict[str, type[BaseReader]]:\n    try:\n        from llama_index.readers.file.docs import (  # type: ignore\n            DocxReader,\n            HWPReader,\n            PDFReader,\n        )\n        from llama_index.readers.file.epub import EpubReader  # type: ignore\n        from llama_index.readers.file.image import ImageReader  # type: ignore\n        from llama_index.readers.file.ipynb import IPYNBReader  # type: ignore\n        from llama_index.readers.file.markdown import MarkdownReader  # type: ignore\n        from llama_index.readers.file.mbox import MboxReader  # type: ignore\n        from llama_index.readers.file.slides import PptxReader  # type: ignore\n        from llama_index.readers.file.tabular import PandasCSVReader  # type: ignore\n        from llama_index.readers.file.video_audio import (  # type: ignore\n            VideoAudioReader,\n        )\n    except ImportError as e:\n        raise ImportError(\"`llama-index-readers-file` package not found\") from e\n\n    default_file_reader_cls: dict[str, type[BaseReader]] = {\n        \".hwp\": HWPReader,\n        \".pdf\": PDFReader,\n        \".docx\": DocxReader,\n        \".pptx\": PptxReader,\n        \".ppt\": PptxReader,\n        \".pptm\": PptxReader,\n        \".jpg\": ImageReader,\n        \".png\": ImageReader,\n        \".jpeg\": ImageReader,\n        \".mp3\": VideoAudioReader,\n        \".mp4\": VideoAudioReader,\n        \".csv\": PandasCSVReader,\n        \".epub\": EpubReader,\n        \".md\": MarkdownReader,\n        \".mbox\": MboxReader,\n        \".ipynb\": IPYNBReader,\n    }\n    return default_file_reader_cls\n\n\n# Patching the default file reader to support other file types\nFILE_READER_CLS = _try_loading_included_file_formats()\nFILE_READER_CLS.update(\n    {\n        \".json\": JSONReader,\n    }\n)\n\n\nclass IngestionHelper:\n    \"\"\"Helper class to transform a file into a list of documents.\n\n    This class should be used to transform a file into a list of documents.\n    These methods are thread-safe (and multiprocessing-safe).\n    \"\"\"\n\n    @staticmethod\n    def transform_file_into_documents(\n        file_name: str, file_data: Path\n    ) -> list[Document]:\n        documents = IngestionHelper._load_file_to_documents(file_name, file_data)\n        for document in documents:\n            document.metadata[\"file_name\"] = file_name\n        IngestionHelper._exclude_metadata(documents)\n        return documents\n\n    @staticmethod\n    def _load_file_to_documents(file_name: str, file_data: Path) -> list[Document]:\n        logger.debug(\"Transforming file_name=%s into documents\", file_name)\n        extension = Path(file_name).suffix\n        reader_cls = FILE_READER_CLS.get(extension)\n        if reader_cls is None:\n            logger.debug(\n                \"No reader found for extension=%s, using default string reader\",\n                extension,\n            )\n            # Read as a plain text\n            string_reader = StringIterableReader()\n            return string_reader.load_data([file_data.read_text()])\n\n        logger.debug(\"Specific reader found for extension=%s\", extension)\n        return reader_cls().load_data(file_data)\n\n    @staticmethod\n    def _exclude_metadata(documents: list[Document]) -> None:\n        logger.debug(\"Excluding metadata from count=%s documents\", len(documents))\n        for document in documents:\n            document.metadata[\"doc_id\"] = document.doc_id\n            # We don't want the Embeddings search to receive this metadata\n            document.excluded_embed_metadata_keys = [\"doc_id\"]\n            # We don't want the LLM to receive these metadata in the context\n            document.excluded_llm_metadata_keys = [\"file_name\", \"doc_id\", \"page_label\"]\n", "private_gpt/components/node_store/node_store_component.py": "import logging\n\nfrom injector import inject, singleton\nfrom llama_index.core.storage.docstore import BaseDocumentStore, SimpleDocumentStore\nfrom llama_index.core.storage.index_store import SimpleIndexStore\nfrom llama_index.core.storage.index_store.types import BaseIndexStore\n\nfrom private_gpt.paths import local_data_path\nfrom private_gpt.settings.settings import Settings\n\nlogger = logging.getLogger(__name__)\n\n\n@singleton\nclass NodeStoreComponent:\n    index_store: BaseIndexStore\n    doc_store: BaseDocumentStore\n\n    @inject\n    def __init__(self, settings: Settings) -> None:\n        match settings.nodestore.database:\n            case \"simple\":\n                try:\n                    self.index_store = SimpleIndexStore.from_persist_dir(\n                        persist_dir=str(local_data_path)\n                    )\n                except FileNotFoundError:\n                    logger.debug(\"Local index store not found, creating a new one\")\n                    self.index_store = SimpleIndexStore()\n\n                try:\n                    self.doc_store = SimpleDocumentStore.from_persist_dir(\n                        persist_dir=str(local_data_path)\n                    )\n                except FileNotFoundError:\n                    logger.debug(\"Local document store not found, creating a new one\")\n                    self.doc_store = SimpleDocumentStore()\n\n            case \"postgres\":\n                try:\n                    from llama_index.core.storage.docstore.postgres_docstore import (\n                        PostgresDocumentStore,\n                    )\n                    from llama_index.core.storage.index_store.postgres_index_store import (\n                        PostgresIndexStore,\n                    )\n                except ImportError:\n                    raise ImportError(\n                        \"Postgres dependencies not found, install with `poetry install --extras storage-nodestore-postgres`\"\n                    ) from None\n\n                if settings.postgres is None:\n                    raise ValueError(\"Postgres index/doc store settings not found.\")\n\n                self.index_store = PostgresIndexStore.from_params(\n                    **settings.postgres.model_dump(exclude_none=True)\n                )\n                self.doc_store = PostgresDocumentStore.from_params(\n                    **settings.postgres.model_dump(exclude_none=True)\n                )\n\n            case _:\n                # Should be unreachable\n                # The settings validator should have caught this\n                raise ValueError(\n                    f\"Database {settings.nodestore.database} not supported\"\n                )\n", "private_gpt/components/node_store/__init__.py": "", "private_gpt/components/vector_store/batched_chroma.py": "from collections.abc import Generator\nfrom typing import Any\n\nfrom llama_index.core.schema import BaseNode, MetadataMode\nfrom llama_index.core.vector_stores.utils import node_to_metadata_dict\nfrom llama_index.vector_stores.chroma import ChromaVectorStore  # type: ignore\n\n\ndef chunk_list(\n    lst: list[BaseNode], max_chunk_size: int\n) -> Generator[list[BaseNode], None, None]:\n    \"\"\"Yield successive max_chunk_size-sized chunks from lst.\n\n    Args:\n        lst (List[BaseNode]): list of nodes with embeddings\n        max_chunk_size (int): max chunk size\n\n    Yields:\n        Generator[List[BaseNode], None, None]: list of nodes with embeddings\n    \"\"\"\n    for i in range(0, len(lst), max_chunk_size):\n        yield lst[i : i + max_chunk_size]\n\n\nclass BatchedChromaVectorStore(ChromaVectorStore):  # type: ignore\n    \"\"\"Chroma vector store, batching additions to avoid reaching the max batch limit.\n\n    In this vector store, embeddings are stored within a ChromaDB collection.\n\n    During query time, the index uses ChromaDB to query for the top\n    k most similar nodes.\n\n    Args:\n        chroma_client (from chromadb.api.API):\n            API instance\n        chroma_collection (chromadb.api.models.Collection.Collection):\n            ChromaDB collection instance\n\n    \"\"\"\n\n    chroma_client: Any | None\n\n    def __init__(\n        self,\n        chroma_client: Any,\n        chroma_collection: Any,\n        host: str | None = None,\n        port: str | None = None,\n        ssl: bool = False,\n        headers: dict[str, str] | None = None,\n        collection_kwargs: dict[Any, Any] | None = None,\n    ) -> None:\n        super().__init__(\n            chroma_collection=chroma_collection,\n            host=host,\n            port=port,\n            ssl=ssl,\n            headers=headers,\n            collection_kwargs=collection_kwargs or {},\n        )\n        self.chroma_client = chroma_client\n\n    def add(self, nodes: list[BaseNode], **add_kwargs: Any) -> list[str]:\n        \"\"\"Add nodes to index, batching the insertion to avoid issues.\n\n        Args:\n            nodes: List[BaseNode]: list of nodes with embeddings\n            add_kwargs: _\n        \"\"\"\n        if not self.chroma_client:\n            raise ValueError(\"Client not initialized\")\n\n        if not self._collection:\n            raise ValueError(\"Collection not initialized\")\n\n        max_chunk_size = self.chroma_client.max_batch_size\n        node_chunks = chunk_list(nodes, max_chunk_size)\n\n        all_ids = []\n        for node_chunk in node_chunks:\n            embeddings = []\n            metadatas = []\n            ids = []\n            documents = []\n            for node in node_chunk:\n                embeddings.append(node.get_embedding())\n                metadatas.append(\n                    node_to_metadata_dict(\n                        node, remove_text=True, flat_metadata=self.flat_metadata\n                    )\n                )\n                ids.append(node.node_id)\n                documents.append(node.get_content(metadata_mode=MetadataMode.NONE))\n\n            self._collection.add(\n                embeddings=embeddings,\n                ids=ids,\n                metadatas=metadatas,\n                documents=documents,\n            )\n            all_ids.extend(ids)\n\n        return all_ids\n", "private_gpt/components/vector_store/vector_store_component.py": "import logging\nimport typing\n\nfrom injector import inject, singleton\nfrom llama_index.core.indices.vector_store import VectorIndexRetriever, VectorStoreIndex\nfrom llama_index.core.vector_stores.types import (\n    FilterCondition,\n    MetadataFilter,\n    MetadataFilters,\n    VectorStore,\n)\n\nfrom private_gpt.open_ai.extensions.context_filter import ContextFilter\nfrom private_gpt.paths import local_data_path\nfrom private_gpt.settings.settings import Settings\n\nlogger = logging.getLogger(__name__)\n\n\ndef _doc_id_metadata_filter(\n    context_filter: ContextFilter | None,\n) -> MetadataFilters:\n    filters = MetadataFilters(filters=[], condition=FilterCondition.OR)\n\n    if context_filter is not None and context_filter.docs_ids is not None:\n        for doc_id in context_filter.docs_ids:\n            filters.filters.append(MetadataFilter(key=\"doc_id\", value=doc_id))\n\n    return filters\n\n\n@singleton\nclass VectorStoreComponent:\n    settings: Settings\n    vector_store: VectorStore\n\n    @inject\n    def __init__(self, settings: Settings) -> None:\n        self.settings = settings\n        match settings.vectorstore.database:\n            case \"postgres\":\n                try:\n                    from llama_index.vector_stores.postgres import (  # type: ignore\n                        PGVectorStore,\n                    )\n                except ImportError as e:\n                    raise ImportError(\n                        \"Postgres dependencies not found, install with `poetry install --extras vector-stores-postgres`\"\n                    ) from e\n\n                if settings.postgres is None:\n                    raise ValueError(\n                        \"Postgres settings not found. Please provide settings.\"\n                    )\n\n                self.vector_store = typing.cast(\n                    VectorStore,\n                    PGVectorStore.from_params(\n                        **settings.postgres.model_dump(exclude_none=True),\n                        table_name=\"embeddings\",\n                        embed_dim=settings.embedding.embed_dim,\n                    ),\n                )\n\n            case \"chroma\":\n                try:\n                    import chromadb  # type: ignore\n                    from chromadb.config import (  # type: ignore\n                        Settings as ChromaSettings,\n                    )\n\n                    from private_gpt.components.vector_store.batched_chroma import (\n                        BatchedChromaVectorStore,\n                    )\n                except ImportError as e:\n                    raise ImportError(\n                        \"ChromaDB dependencies not found, install with `poetry install --extras vector-stores-chroma`\"\n                    ) from e\n\n                chroma_settings = ChromaSettings(anonymized_telemetry=False)\n                chroma_client = chromadb.PersistentClient(\n                    path=str((local_data_path / \"chroma_db\").absolute()),\n                    settings=chroma_settings,\n                )\n                chroma_collection = chroma_client.get_or_create_collection(\n                    \"make_this_parameterizable_per_api_call\"\n                )  # TODO\n\n                self.vector_store = typing.cast(\n                    VectorStore,\n                    BatchedChromaVectorStore(\n                        chroma_client=chroma_client, chroma_collection=chroma_collection\n                    ),\n                )\n\n            case \"qdrant\":\n                try:\n                    from llama_index.vector_stores.qdrant import (  # type: ignore\n                        QdrantVectorStore,\n                    )\n                    from qdrant_client import QdrantClient  # type: ignore\n                except ImportError as e:\n                    raise ImportError(\n                        \"Qdrant dependencies not found, install with `poetry install --extras vector-stores-qdrant`\"\n                    ) from e\n\n                if settings.qdrant is None:\n                    logger.info(\n                        \"Qdrant config not found. Using default settings.\"\n                        \"Trying to connect to Qdrant at localhost:6333.\"\n                    )\n                    client = QdrantClient()\n                else:\n                    client = QdrantClient(\n                        **settings.qdrant.model_dump(exclude_none=True)\n                    )\n                self.vector_store = typing.cast(\n                    VectorStore,\n                    QdrantVectorStore(\n                        client=client,\n                        collection_name=\"make_this_parameterizable_per_api_call\",\n                    ),  # TODO\n                )\n            case _:\n                # Should be unreachable\n                # The settings validator should have caught this\n                raise ValueError(\n                    f\"Vectorstore database {settings.vectorstore.database} not supported\"\n                )\n\n    def get_retriever(\n        self,\n        index: VectorStoreIndex,\n        context_filter: ContextFilter | None = None,\n        similarity_top_k: int = 2,\n    ) -> VectorIndexRetriever:\n        # This way we support qdrant (using doc_ids) and the rest (using filters)\n        return VectorIndexRetriever(\n            index=index,\n            similarity_top_k=similarity_top_k,\n            doc_ids=context_filter.docs_ids if context_filter else None,\n            filters=(\n                _doc_id_metadata_filter(context_filter)\n                if self.settings.vectorstore.database != \"qdrant\"\n                else None\n            ),\n        )\n\n    def close(self) -> None:\n        if hasattr(self.vector_store.client, \"close\"):\n            self.vector_store.client.close()\n", "private_gpt/components/vector_store/__init__.py": "", "private_gpt/components/llm/llm_component.py": "import logging\nfrom collections.abc import Callable\nfrom typing import Any\n\nfrom injector import inject, singleton\nfrom llama_index.core.llms import LLM, MockLLM\nfrom llama_index.core.settings import Settings as LlamaIndexSettings\nfrom llama_index.core.utils import set_global_tokenizer\nfrom transformers import AutoTokenizer  # type: ignore\n\nfrom private_gpt.components.llm.prompt_helper import get_prompt_style\nfrom private_gpt.paths import models_cache_path, models_path\nfrom private_gpt.settings.settings import Settings\n\nlogger = logging.getLogger(__name__)\n\n\n@singleton\nclass LLMComponent:\n    llm: LLM\n\n    @inject\n    def __init__(self, settings: Settings) -> None:\n        llm_mode = settings.llm.mode\n        if settings.llm.tokenizer and settings.llm.mode != \"mock\":\n            # Try to download the tokenizer. If it fails, the LLM will still work\n            # using the default one, which is less accurate.\n            try:\n                set_global_tokenizer(\n                    AutoTokenizer.from_pretrained(\n                        pretrained_model_name_or_path=settings.llm.tokenizer,\n                        cache_dir=str(models_cache_path),\n                        token=settings.huggingface.access_token,\n                    )\n                )\n            except Exception as e:\n                logger.warning(\n                    \"Failed to download tokenizer %s. Falling back to \"\n                    \"default tokenizer.\",\n                    settings.llm.tokenizer,\n                    e,\n                )\n\n        logger.info(\"Initializing the LLM in mode=%s\", llm_mode)\n        match settings.llm.mode:\n            case \"llamacpp\":\n                try:\n                    from llama_index.llms.llama_cpp import LlamaCPP  # type: ignore\n                except ImportError as e:\n                    raise ImportError(\n                        \"Local dependencies not found, install with `poetry install --extras llms-llama-cpp`\"\n                    ) from e\n\n                prompt_style = get_prompt_style(settings.llm.prompt_style)\n                settings_kwargs = {\n                    \"tfs_z\": settings.llamacpp.tfs_z,  # ollama and llama-cpp\n                    \"top_k\": settings.llamacpp.top_k,  # ollama and llama-cpp\n                    \"top_p\": settings.llamacpp.top_p,  # ollama and llama-cpp\n                    \"repeat_penalty\": settings.llamacpp.repeat_penalty,  # ollama llama-cpp\n                    \"n_gpu_layers\": -1,\n                    \"offload_kqv\": True,\n                }\n                self.llm = LlamaCPP(\n                    model_path=str(models_path / settings.llamacpp.llm_hf_model_file),\n                    temperature=settings.llm.temperature,\n                    max_new_tokens=settings.llm.max_new_tokens,\n                    context_window=settings.llm.context_window,\n                    generate_kwargs={},\n                    callback_manager=LlamaIndexSettings.callback_manager,\n                    # All to GPU\n                    model_kwargs=settings_kwargs,\n                    # transform inputs into Llama2 format\n                    messages_to_prompt=prompt_style.messages_to_prompt,\n                    completion_to_prompt=prompt_style.completion_to_prompt,\n                    verbose=True,\n                )\n\n            case \"sagemaker\":\n                try:\n                    from private_gpt.components.llm.custom.sagemaker import SagemakerLLM\n                except ImportError as e:\n                    raise ImportError(\n                        \"Sagemaker dependencies not found, install with `poetry install --extras llms-sagemaker`\"\n                    ) from e\n\n                self.llm = SagemakerLLM(\n                    endpoint_name=settings.sagemaker.llm_endpoint_name,\n                    max_new_tokens=settings.llm.max_new_tokens,\n                    context_window=settings.llm.context_window,\n                )\n            case \"openai\":\n                try:\n                    from llama_index.llms.openai import OpenAI  # type: ignore\n                except ImportError as e:\n                    raise ImportError(\n                        \"OpenAI dependencies not found, install with `poetry install --extras llms-openai`\"\n                    ) from e\n\n                openai_settings = settings.openai\n                self.llm = OpenAI(\n                    api_base=openai_settings.api_base,\n                    api_key=openai_settings.api_key,\n                    model=openai_settings.model,\n                )\n            case \"openailike\":\n                try:\n                    from llama_index.llms.openai_like import OpenAILike  # type: ignore\n                except ImportError as e:\n                    raise ImportError(\n                        \"OpenAILike dependencies not found, install with `poetry install --extras llms-openai-like`\"\n                    ) from e\n                prompt_style = get_prompt_style(settings.llm.prompt_style)\n                openai_settings = settings.openai\n                self.llm = OpenAILike(\n                    api_base=openai_settings.api_base,\n                    api_key=openai_settings.api_key,\n                    model=openai_settings.model,\n                    is_chat_model=True,\n                    max_tokens=settings.llm.max_new_tokens,\n                    api_version=\"\",\n                    temperature=settings.llm.temperature,\n                    context_window=settings.llm.context_window,\n                    max_new_tokens=settings.llm.max_new_tokens,\n                    messages_to_prompt=prompt_style.messages_to_prompt,\n                    completion_to_prompt=prompt_style.completion_to_prompt,\n                    tokenizer=settings.llm.tokenizer,\n                    timeout=openai_settings.request_timeout,\n                    reuse_client=False,\n                )\n            case \"ollama\":\n                try:\n                    from llama_index.llms.ollama import Ollama  # type: ignore\n                except ImportError as e:\n                    raise ImportError(\n                        \"Ollama dependencies not found, install with `poetry install --extras llms-ollama`\"\n                    ) from e\n\n                ollama_settings = settings.ollama\n\n                settings_kwargs = {\n                    \"tfs_z\": ollama_settings.tfs_z,  # ollama and llama-cpp\n                    \"num_predict\": ollama_settings.num_predict,  # ollama only\n                    \"top_k\": ollama_settings.top_k,  # ollama and llama-cpp\n                    \"top_p\": ollama_settings.top_p,  # ollama and llama-cpp\n                    \"repeat_last_n\": ollama_settings.repeat_last_n,  # ollama\n                    \"repeat_penalty\": ollama_settings.repeat_penalty,  # ollama llama-cpp\n                }\n\n                self.llm = Ollama(\n                    model=ollama_settings.llm_model,\n                    base_url=ollama_settings.api_base,\n                    temperature=settings.llm.temperature,\n                    context_window=settings.llm.context_window,\n                    additional_kwargs=settings_kwargs,\n                    request_timeout=ollama_settings.request_timeout,\n                )\n\n                if (\n                    ollama_settings.keep_alive\n                    != ollama_settings.model_fields[\"keep_alive\"].default\n                ):\n                    # Modify Ollama methods to use the \"keep_alive\" field.\n                    def add_keep_alive(func: Callable[..., Any]) -> Callable[..., Any]:\n                        def wrapper(*args: Any, **kwargs: Any) -> Any:\n                            kwargs[\"keep_alive\"] = ollama_settings.keep_alive\n                            return func(*args, **kwargs)\n\n                        return wrapper\n\n                    Ollama.chat = add_keep_alive(Ollama.chat)\n                    Ollama.stream_chat = add_keep_alive(Ollama.stream_chat)\n                    Ollama.complete = add_keep_alive(Ollama.complete)\n                    Ollama.stream_complete = add_keep_alive(Ollama.stream_complete)\n\n            case \"azopenai\":\n                try:\n                    from llama_index.llms.azure_openai import (  # type: ignore\n                        AzureOpenAI,\n                    )\n                except ImportError as e:\n                    raise ImportError(\n                        \"Azure OpenAI dependencies not found, install with `poetry install --extras llms-azopenai`\"\n                    ) from e\n\n                azopenai_settings = settings.azopenai\n                self.llm = AzureOpenAI(\n                    model=azopenai_settings.llm_model,\n                    deployment_name=azopenai_settings.llm_deployment_name,\n                    api_key=azopenai_settings.api_key,\n                    azure_endpoint=azopenai_settings.azure_endpoint,\n                    api_version=azopenai_settings.api_version,\n                )\n            case \"mock\":\n                self.llm = MockLLM()\n", "private_gpt/components/llm/prompt_helper.py": "import abc\nimport logging\nfrom collections.abc import Sequence\nfrom typing import Any, Literal\n\nfrom llama_index.core.llms import ChatMessage, MessageRole\n\nlogger = logging.getLogger(__name__)\n\n\nclass AbstractPromptStyle(abc.ABC):\n    \"\"\"Abstract class for prompt styles.\n\n    This class is used to format a series of messages into a prompt that can be\n    understood by the models. A series of messages represents the interaction(s)\n    between a user and an assistant. This series of messages can be considered as a\n    session between a user X and an assistant Y.This session holds, through the\n    messages, the state of the conversation. This session, to be understood by the\n    model, needs to be formatted into a prompt (i.e. a string that the models\n    can understand). Prompts can be formatted in different ways,\n    depending on the model.\n\n    The implementations of this class represent the different ways to format a\n    series of messages into a prompt.\n    \"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        logger.debug(\"Initializing prompt_style=%s\", self.__class__.__name__)\n\n    @abc.abstractmethod\n    def _messages_to_prompt(self, messages: Sequence[ChatMessage]) -> str:\n        pass\n\n    @abc.abstractmethod\n    def _completion_to_prompt(self, completion: str) -> str:\n        pass\n\n    def messages_to_prompt(self, messages: Sequence[ChatMessage]) -> str:\n        prompt = self._messages_to_prompt(messages)\n        logger.debug(\"Got for messages='%s' the prompt='%s'\", messages, prompt)\n        return prompt\n\n    def completion_to_prompt(self, completion: str) -> str:\n        prompt = self._completion_to_prompt(completion)\n        logger.debug(\"Got for completion='%s' the prompt='%s'\", completion, prompt)\n        return prompt\n\n\nclass DefaultPromptStyle(AbstractPromptStyle):\n    \"\"\"Default prompt style that uses the defaults from llama_utils.\n\n    It basically passes None to the LLM, indicating it should use\n    the default functions.\n    \"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        super().__init__(*args, **kwargs)\n\n        # Hacky way to override the functions\n        # Override the functions to be None, and pass None to the LLM.\n        self.messages_to_prompt = None  # type: ignore[method-assign, assignment]\n        self.completion_to_prompt = None  # type: ignore[method-assign, assignment]\n\n    def _messages_to_prompt(self, messages: Sequence[ChatMessage]) -> str:\n        return \"\"\n\n    def _completion_to_prompt(self, completion: str) -> str:\n        return \"\"\n\n\nclass Llama2PromptStyle(AbstractPromptStyle):\n    \"\"\"Simple prompt style that uses llama 2 prompt style.\n\n    Inspired by llama_index/legacy/llms/llama_utils.py\n\n    It transforms the sequence of messages into a prompt that should look like:\n    ```text\n    <s> [INST] <<SYS>> your system prompt here. <</SYS>>\n\n    user message here [/INST] assistant (model) response here </s>\n    ```\n    \"\"\"\n\n    BOS, EOS = \"<s>\", \"</s>\"\n    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n    B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n    DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n    You are a helpful, respectful and honest assistant. \\\n    Always answer as helpfully as possible and follow ALL given instructions. \\\n    Do not speculate or make up information. \\\n    Do not reference any given instructions or context. \\\n    \"\"\"\n\n    def _messages_to_prompt(self, messages: Sequence[ChatMessage]) -> str:\n        string_messages: list[str] = []\n        if messages[0].role == MessageRole.SYSTEM:\n            # pull out the system message (if it exists in messages)\n            system_message_str = messages[0].content or \"\"\n            messages = messages[1:]\n        else:\n            system_message_str = self.DEFAULT_SYSTEM_PROMPT\n\n        system_message_str = f\"{self.B_SYS} {system_message_str.strip()} {self.E_SYS}\"\n\n        for i in range(0, len(messages), 2):\n            # first message should always be a user\n            user_message = messages[i]\n            assert user_message.role == MessageRole.USER\n\n            if i == 0:\n                # make sure system prompt is included at the start\n                str_message = f\"{self.BOS} {self.B_INST} {system_message_str} \"\n            else:\n                # end previous user-assistant interaction\n                string_messages[-1] += f\" {self.EOS}\"\n                # no need to include system prompt\n                str_message = f\"{self.BOS} {self.B_INST} \"\n\n            # include user message content\n            str_message += f\"{user_message.content} {self.E_INST}\"\n\n            if len(messages) > (i + 1):\n                # if assistant message exists, add to str_message\n                assistant_message = messages[i + 1]\n                assert assistant_message.role == MessageRole.ASSISTANT\n                str_message += f\" {assistant_message.content}\"\n\n            string_messages.append(str_message)\n\n        return \"\".join(string_messages)\n\n    def _completion_to_prompt(self, completion: str) -> str:\n        system_prompt_str = self.DEFAULT_SYSTEM_PROMPT\n\n        return (\n            f\"{self.BOS} {self.B_INST} {self.B_SYS} {system_prompt_str.strip()} {self.E_SYS} \"\n            f\"{completion.strip()} {self.E_INST}\"\n        )\n\n\nclass TagPromptStyle(AbstractPromptStyle):\n    \"\"\"Tag prompt style (used by Vigogne) that uses the prompt style `<|ROLE|>`.\n\n    It transforms the sequence of messages into a prompt that should look like:\n    ```text\n    <|system|>: your system prompt here.\n    <|user|>: user message here\n    (possibly with context and question)\n    <|assistant|>: assistant (model) response here.\n    ```\n\n    FIXME: should we add surrounding `<s>` and `</s>` tags, like in llama2?\n    \"\"\"\n\n    def _messages_to_prompt(self, messages: Sequence[ChatMessage]) -> str:\n        \"\"\"Format message to prompt with `<|ROLE|>: MSG` style.\"\"\"\n        prompt = \"\"\n        for message in messages:\n            role = message.role\n            content = message.content or \"\"\n            message_from_user = f\"<|{role.lower()}|>: {content.strip()}\"\n            message_from_user += \"\\n\"\n            prompt += message_from_user\n        # we are missing the last <|assistant|> tag that will trigger a completion\n        prompt += \"<|assistant|>: \"\n        return prompt\n\n    def _completion_to_prompt(self, completion: str) -> str:\n        return self._messages_to_prompt(\n            [ChatMessage(content=completion, role=MessageRole.USER)]\n        )\n\n\nclass MistralPromptStyle(AbstractPromptStyle):\n    def _messages_to_prompt(self, messages: Sequence[ChatMessage]) -> str:\n        inst_buffer = []\n        text = \"\"\n        for message in messages:\n            if message.role == MessageRole.SYSTEM or message.role == MessageRole.USER:\n                inst_buffer.append(str(message.content).strip())\n            elif message.role == MessageRole.ASSISTANT:\n                text += \"<s>[INST] \" + \"\\n\".join(inst_buffer) + \" [/INST]\"\n                text += \" \" + str(message.content).strip() + \"</s>\"\n                inst_buffer.clear()\n            else:\n                raise ValueError(f\"Unknown message role {message.role}\")\n\n        if len(inst_buffer) > 0:\n            text += \"<s>[INST] \" + \"\\n\".join(inst_buffer) + \" [/INST]\"\n\n        return text\n\n    def _completion_to_prompt(self, completion: str) -> str:\n        return self._messages_to_prompt(\n            [ChatMessage(content=completion, role=MessageRole.USER)]\n        )\n\n\nclass ChatMLPromptStyle(AbstractPromptStyle):\n    def _messages_to_prompt(self, messages: Sequence[ChatMessage]) -> str:\n        prompt = \"<|im_start|>system\\n\"\n        for message in messages:\n            role = message.role\n            content = message.content or \"\"\n            if role.lower() == \"system\":\n                message_from_user = f\"{content.strip()}\"\n                prompt += message_from_user\n            elif role.lower() == \"user\":\n                prompt += \"<|im_end|>\\n<|im_start|>user\\n\"\n                message_from_user = f\"{content.strip()}<|im_end|>\\n\"\n                prompt += message_from_user\n        prompt += \"<|im_start|>assistant\\n\"\n        return prompt\n\n    def _completion_to_prompt(self, completion: str) -> str:\n        return self._messages_to_prompt(\n            [ChatMessage(content=completion, role=MessageRole.USER)]\n        )\n\n\ndef get_prompt_style(\n    prompt_style: Literal[\"default\", \"llama2\", \"tag\", \"mistral\", \"chatml\"] | None\n) -> AbstractPromptStyle:\n    \"\"\"Get the prompt style to use from the given string.\n\n    :param prompt_style: The prompt style to use.\n    :return: The prompt style to use.\n    \"\"\"\n    if prompt_style is None or prompt_style == \"default\":\n        return DefaultPromptStyle()\n    elif prompt_style == \"llama2\":\n        return Llama2PromptStyle()\n    elif prompt_style == \"tag\":\n        return TagPromptStyle()\n    elif prompt_style == \"mistral\":\n        return MistralPromptStyle()\n    elif prompt_style == \"chatml\":\n        return ChatMLPromptStyle()\n    raise ValueError(f\"Unknown prompt_style='{prompt_style}'\")\n", "private_gpt/components/llm/__init__.py": "\"\"\"LLM implementations.\"\"\"\n", "private_gpt/components/llm/custom/__init__.py": "", "private_gpt/components/llm/custom/sagemaker.py": "# mypy: ignore-errors\nfrom __future__ import annotations\n\nimport io\nimport json\nimport logging\nfrom typing import TYPE_CHECKING, Any\n\nimport boto3  # type: ignore\nfrom llama_index.core.base.llms.generic_utils import (\n    completion_response_to_chat_response,\n    stream_completion_response_to_chat_response,\n)\nfrom llama_index.core.bridge.pydantic import Field\nfrom llama_index.core.llms import (\n    CompletionResponse,\n    CustomLLM,\n    LLMMetadata,\n)\nfrom llama_index.core.llms.callbacks import (\n    llm_chat_callback,\n    llm_completion_callback,\n)\n\nif TYPE_CHECKING:\n    from collections.abc import Sequence\n\n    from llama_index.callbacks import CallbackManager\n    from llama_index.llms import (\n        ChatMessage,\n        ChatResponse,\n        ChatResponseGen,\n        CompletionResponseGen,\n    )\n\nlogger = logging.getLogger(__name__)\n\n\nclass LineIterator:\n    r\"\"\"A helper class for parsing the byte stream input from TGI container.\n\n    The output of the model will be in the following format:\n    ```\n    b'data:{\"token\": {\"text\": \" a\"}}\\n\\n'\n    b'data:{\"token\": {\"text\": \" challenging\"}}\\n\\n'\n    b'data:{\"token\": {\"text\": \" problem\"\n    b'}}'\n    ...\n    ```\n\n    While usually each PayloadPart event from the event stream will contain a byte array\n    with a full json, this is not guaranteed and some of the json objects may be split\n    across PayloadPart events. For example:\n    ```\n    {'PayloadPart': {'Bytes': b'{\"outputs\": '}}\n    {'PayloadPart': {'Bytes': b'[\" problem\"]}\\n'}}\n    ```\n\n\n    This class accounts for this by concatenating bytes written via the 'write' function\n    and then exposing a method which will return lines (ending with a '\\n' character)\n    within the buffer via the 'scan_lines' function. It maintains the position of the\n    last read position to ensure that previous bytes are not exposed again. It will\n    also save any pending lines that doe not end with a '\\n' to make sure truncations\n    are concatinated\n    \"\"\"\n\n    def __init__(self, stream: Any) -> None:\n        \"\"\"Line iterator initializer.\"\"\"\n        self.byte_iterator = iter(stream)\n        self.buffer = io.BytesIO()\n        self.read_pos = 0\n\n    def __iter__(self) -> Any:\n        \"\"\"Self iterator.\"\"\"\n        return self\n\n    def __next__(self) -> Any:\n        \"\"\"Next element from iterator.\"\"\"\n        while True:\n            self.buffer.seek(self.read_pos)\n            line = self.buffer.readline()\n            if line and line[-1] == ord(\"\\n\"):\n                self.read_pos += len(line)\n                return line[:-1]\n            try:\n                chunk = next(self.byte_iterator)\n            except StopIteration:\n                if self.read_pos < self.buffer.getbuffer().nbytes:\n                    continue\n                raise\n            if \"PayloadPart\" not in chunk:\n                logger.warning(\"Unknown event type=%s\", chunk)\n                continue\n            self.buffer.seek(0, io.SEEK_END)\n            self.buffer.write(chunk[\"PayloadPart\"][\"Bytes\"])\n\n\nclass SagemakerLLM(CustomLLM):\n    \"\"\"Sagemaker Inference Endpoint models.\n\n    To use, you must supply the endpoint name from your deployed\n    Sagemaker model & the region where it is deployed.\n\n    To authenticate, the AWS client uses the following methods to\n    automatically load credentials:\n    https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html\n\n    If a specific credential profile should be used, you must pass\n    the name of the profile from the ~/.aws/credentials file that is to be used.\n\n    Make sure the credentials / roles used have the required policies to\n    access the Sagemaker endpoint.\n    See: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html\n    \"\"\"\n\n    endpoint_name: str = Field(description=\"\")\n    temperature: float = Field(description=\"The temperature to use for sampling.\")\n    max_new_tokens: int = Field(description=\"The maximum number of tokens to generate.\")\n    context_window: int = Field(\n        description=\"The maximum number of context tokens for the model.\"\n    )\n    messages_to_prompt: Any = Field(\n        description=\"The function to convert messages to a prompt.\", exclude=True\n    )\n    completion_to_prompt: Any = Field(\n        description=\"The function to convert a completion to a prompt.\", exclude=True\n    )\n    generate_kwargs: dict[str, Any] = Field(\n        default_factory=dict, description=\"Kwargs used for generation.\"\n    )\n    model_kwargs: dict[str, Any] = Field(\n        default_factory=dict, description=\"Kwargs used for model initialization.\"\n    )\n    verbose: bool = Field(description=\"Whether to print verbose output.\")\n\n    _boto_client: Any = boto3.client(\n        \"sagemaker-runtime\",\n    )  # TODO make it an optional field\n\n    def __init__(\n        self,\n        endpoint_name: str | None = \"\",\n        temperature: float = 0.1,\n        max_new_tokens: int = 512,  # to review defaults\n        context_window: int = 2048,  # to review defaults\n        messages_to_prompt: Any = None,\n        completion_to_prompt: Any = None,\n        callback_manager: CallbackManager | None = None,\n        generate_kwargs: dict[str, Any] | None = None,\n        model_kwargs: dict[str, Any] | None = None,\n        verbose: bool = True,\n    ) -> None:\n        \"\"\"SagemakerLLM initializer.\"\"\"\n        model_kwargs = model_kwargs or {}\n        model_kwargs.update({\"n_ctx\": context_window, \"verbose\": verbose})\n\n        messages_to_prompt = messages_to_prompt or {}\n        completion_to_prompt = completion_to_prompt or {}\n\n        generate_kwargs = generate_kwargs or {}\n        generate_kwargs.update(\n            {\"temperature\": temperature, \"max_tokens\": max_new_tokens}\n        )\n\n        super().__init__(\n            endpoint_name=endpoint_name,\n            temperature=temperature,\n            context_window=context_window,\n            max_new_tokens=max_new_tokens,\n            messages_to_prompt=messages_to_prompt,\n            completion_to_prompt=completion_to_prompt,\n            callback_manager=callback_manager,\n            generate_kwargs=generate_kwargs,\n            model_kwargs=model_kwargs,\n            verbose=verbose,\n        )\n\n    @property\n    def inference_params(self):\n        # TODO expose the rest of params\n        return {\n            \"do_sample\": True,\n            \"top_p\": 0.7,\n            \"temperature\": self.temperature,\n            \"top_k\": 50,\n            \"max_new_tokens\": self.max_new_tokens,\n        }\n\n    @property\n    def metadata(self) -> LLMMetadata:\n        \"\"\"Get LLM metadata.\"\"\"\n        return LLMMetadata(\n            context_window=self.context_window,\n            num_output=self.max_new_tokens,\n            model_name=\"Sagemaker LLama 2\",\n        )\n\n    @llm_completion_callback()\n    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\n        self.generate_kwargs.update({\"stream\": False})\n\n        is_formatted = kwargs.pop(\"formatted\", False)\n        if not is_formatted:\n            prompt = self.completion_to_prompt(prompt)\n\n        request_params = {\n            \"inputs\": prompt,\n            \"stream\": False,\n            \"parameters\": self.inference_params,\n        }\n\n        resp = self._boto_client.invoke_endpoint(\n            EndpointName=self.endpoint_name,\n            Body=json.dumps(request_params),\n            ContentType=\"application/json\",\n        )\n\n        response_body = resp[\"Body\"]\n        response_str = response_body.read().decode(\"utf-8\")\n        response_dict = json.loads(response_str)\n\n        return CompletionResponse(\n            text=response_dict[0][\"generated_text\"][len(prompt) :], raw=resp\n        )\n\n    @llm_completion_callback()\n    def stream_complete(self, prompt: str, **kwargs: Any) -> CompletionResponseGen:\n        def get_stream():\n            text = \"\"\n\n            request_params = {\n                \"inputs\": prompt,\n                \"stream\": True,\n                \"parameters\": self.inference_params,\n            }\n            resp = self._boto_client.invoke_endpoint_with_response_stream(\n                EndpointName=self.endpoint_name,\n                Body=json.dumps(request_params),\n                ContentType=\"application/json\",\n            )\n\n            event_stream = resp[\"Body\"]\n            start_json = b\"{\"\n            stop_token = \"<|endoftext|>\"\n            first_token = True\n\n            for line in LineIterator(event_stream):\n                if line != b\"\" and start_json in line:\n                    data = json.loads(line[line.find(start_json) :].decode(\"utf-8\"))\n                    special = data[\"token\"][\"special\"]\n                    stop = data[\"token\"][\"text\"] == stop_token\n                    if not special and not stop:\n                        delta = data[\"token\"][\"text\"]\n                        # trim the leading space for the first token if present\n                        if first_token:\n                            delta = delta.lstrip()\n                            first_token = False\n                        text += delta\n                        yield CompletionResponse(delta=delta, text=text, raw=data)\n\n        return get_stream()\n\n    @llm_chat_callback()\n    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:\n        prompt = self.messages_to_prompt(messages)\n        completion_response = self.complete(prompt, formatted=True, **kwargs)\n        return completion_response_to_chat_response(completion_response)\n\n    @llm_chat_callback()\n    def stream_chat(\n        self, messages: Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseGen:\n        prompt = self.messages_to_prompt(messages)\n        completion_response = self.stream_complete(prompt, formatted=True, **kwargs)\n        return stream_completion_response_to_chat_response(completion_response)\n", "private_gpt/settings/yaml.py": "import os\nimport re\nimport typing\nfrom typing import Any, TextIO\n\nfrom yaml import SafeLoader\n\n_env_replace_matcher = re.compile(r\"\\$\\{(\\w|_)+:?.*}\")\n\n\n@typing.no_type_check  # pyaml does not have good hints, everything is Any\ndef load_yaml_with_envvars(\n    stream: TextIO, environ: dict[str, Any] = os.environ\n) -> dict[str, Any]:\n    \"\"\"Load yaml file with environment variable expansion.\n\n    The pattern ${VAR} or ${VAR:default} will be replaced with\n    the value of the environment variable.\n    \"\"\"\n    loader = SafeLoader(stream)\n\n    def load_env_var(_, node) -> str:\n        \"\"\"Extract the matched value, expand env variable, and replace the match.\"\"\"\n        value = str(node.value).removeprefix(\"${\").removesuffix(\"}\")\n        split = value.split(\":\", 1)\n        env_var = split[0]\n        value = environ.get(env_var)\n        default = None if len(split) == 1 else split[1]\n        if value is None and default is None:\n            raise ValueError(\n                f\"Environment variable {env_var} is not set and not default was provided\"\n            )\n        return value or default\n\n    loader.add_implicit_resolver(\"env_var_replacer\", _env_replace_matcher, None)\n    loader.add_constructor(\"env_var_replacer\", load_env_var)\n\n    try:\n        return loader.get_single_data()\n    finally:\n        loader.dispose()\n", "private_gpt/settings/settings_loader.py": "import functools\nimport logging\nimport os\nimport sys\nfrom collections.abc import Iterable\nfrom pathlib import Path\nfrom typing import Any\n\nfrom pydantic.v1.utils import deep_update, unique_list\n\nfrom private_gpt.constants import PROJECT_ROOT_PATH\nfrom private_gpt.settings.yaml import load_yaml_with_envvars\n\nlogger = logging.getLogger(__name__)\n\n_settings_folder = os.environ.get(\"PGPT_SETTINGS_FOLDER\", PROJECT_ROOT_PATH)\n\n# if running in unittest, use the test profile\n_test_profile = [\"test\"] if \"tests.fixtures\" in sys.modules else []\n\nactive_profiles: list[str] = unique_list(\n    [\"default\"]\n    + [\n        item.strip()\n        for item in os.environ.get(\"PGPT_PROFILES\", \"\").split(\",\")\n        if item.strip()\n    ]\n    + _test_profile\n)\n\n\ndef merge_settings(settings: Iterable[dict[str, Any]]) -> dict[str, Any]:\n    return functools.reduce(deep_update, settings, {})\n\n\ndef load_settings_from_profile(profile: str) -> dict[str, Any]:\n    if profile == \"default\":\n        profile_file_name = \"settings.yaml\"\n    else:\n        profile_file_name = f\"settings-{profile}.yaml\"\n\n    path = Path(_settings_folder) / profile_file_name\n    with Path(path).open(\"r\") as f:\n        config = load_yaml_with_envvars(f)\n    if not isinstance(config, dict):\n        raise TypeError(f\"Config file has no top-level mapping: {path}\")\n    return config\n\n\ndef load_active_settings() -> dict[str, Any]:\n    \"\"\"Load active profiles and merge them.\"\"\"\n    logger.info(\"Starting application with profiles=%s\", active_profiles)\n    loaded_profiles = [\n        load_settings_from_profile(profile) for profile in active_profiles\n    ]\n    merged: dict[str, Any] = merge_settings(loaded_profiles)\n    return merged\n", "private_gpt/settings/settings.py": "from typing import Literal\n\nfrom pydantic import BaseModel, Field\n\nfrom private_gpt.settings.settings_loader import load_active_settings\n\n\nclass CorsSettings(BaseModel):\n    \"\"\"CORS configuration.\n\n    For more details on the CORS configuration, see:\n    # * https://fastapi.tiangolo.com/tutorial/cors/\n    # * https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS\n    \"\"\"\n\n    enabled: bool = Field(\n        description=\"Flag indicating if CORS headers are set or not.\"\n        \"If set to True, the CORS headers will be set to allow all origins, methods and headers.\",\n        default=False,\n    )\n    allow_credentials: bool = Field(\n        description=\"Indicate that cookies should be supported for cross-origin requests\",\n        default=False,\n    )\n    allow_origins: list[str] = Field(\n        description=\"A list of origins that should be permitted to make cross-origin requests.\",\n        default=[],\n    )\n    allow_origin_regex: list[str] = Field(\n        description=\"A regex string to match against origins that should be permitted to make cross-origin requests.\",\n        default=None,\n    )\n    allow_methods: list[str] = Field(\n        description=\"A list of HTTP methods that should be allowed for cross-origin requests.\",\n        default=[\n            \"GET\",\n        ],\n    )\n    allow_headers: list[str] = Field(\n        description=\"A list of HTTP request headers that should be supported for cross-origin requests.\",\n        default=[],\n    )\n\n\nclass AuthSettings(BaseModel):\n    \"\"\"Authentication configuration.\n\n    The implementation of the authentication strategy must\n    \"\"\"\n\n    enabled: bool = Field(\n        description=\"Flag indicating if authentication is enabled or not.\",\n        default=False,\n    )\n    secret: str = Field(\n        description=\"The secret to be used for authentication. \"\n        \"It can be any non-blank string. For HTTP basic authentication, \"\n        \"this value should be the whole 'Authorization' header that is expected\"\n    )\n\n\nclass ServerSettings(BaseModel):\n    env_name: str = Field(\n        description=\"Name of the environment (prod, staging, local...)\"\n    )\n    port: int = Field(description=\"Port of PrivateGPT FastAPI server, defaults to 8001\")\n    cors: CorsSettings = Field(\n        description=\"CORS configuration\", default=CorsSettings(enabled=False)\n    )\n    auth: AuthSettings = Field(\n        description=\"Authentication configuration\",\n        default_factory=lambda: AuthSettings(enabled=False, secret=\"secret-key\"),\n    )\n\n\nclass DataSettings(BaseModel):\n    local_data_folder: str = Field(\n        description=\"Path to local storage.\"\n        \"It will be treated as an absolute path if it starts with /\"\n    )\n\n\nclass LLMSettings(BaseModel):\n    mode: Literal[\n        \"llamacpp\", \"openai\", \"openailike\", \"azopenai\", \"sagemaker\", \"mock\", \"ollama\"\n    ]\n    max_new_tokens: int = Field(\n        256,\n        description=\"The maximum number of token that the LLM is authorized to generate in one completion.\",\n    )\n    context_window: int = Field(\n        3900,\n        description=\"The maximum number of context tokens for the model.\",\n    )\n    tokenizer: str = Field(\n        None,\n        description=\"The model id of a predefined tokenizer hosted inside a model repo on \"\n        \"huggingface.co. Valid model ids can be located at the root-level, like \"\n        \"`bert-base-uncased`, or namespaced under a user or organization name, \"\n        \"like `HuggingFaceH4/zephyr-7b-beta`. If not set, will load a tokenizer matching \"\n        \"gpt-3.5-turbo LLM.\",\n    )\n    temperature: float = Field(\n        0.1,\n        description=\"The temperature of the model. Increasing the temperature will make the model answer more creatively. A value of 0.1 would be more factual.\",\n    )\n    prompt_style: Literal[\"default\", \"llama2\", \"tag\", \"mistral\", \"chatml\"] = Field(\n        \"llama2\",\n        description=(\n            \"The prompt style to use for the chat engine. \"\n            \"If `default` - use the default prompt style from the llama_index. It should look like `role: message`.\\n\"\n            \"If `llama2` - use the llama2 prompt style from the llama_index. Based on `<s>`, `[INST]` and `<<SYS>>`.\\n\"\n            \"If `tag` - use the `tag` prompt style. It should look like `<|role|>: message`. \\n\"\n            \"If `mistral` - use the `mistral prompt style. It shoudl look like <s>[INST] {System Prompt} [/INST]</s>[INST] { UserInstructions } [/INST]\"\n            \"`llama2` is the historic behaviour. `default` might work better with your custom models.\"\n        ),\n    )\n\n\nclass VectorstoreSettings(BaseModel):\n    database: Literal[\"chroma\", \"qdrant\", \"postgres\"]\n\n\nclass NodeStoreSettings(BaseModel):\n    database: Literal[\"simple\", \"postgres\"]\n\n\nclass LlamaCPPSettings(BaseModel):\n    llm_hf_repo_id: str\n    llm_hf_model_file: str\n    tfs_z: float = Field(\n        1.0,\n        description=\"Tail free sampling is used to reduce the impact of less probable tokens from the output. A higher value (e.g., 2.0) will reduce the impact more, while a value of 1.0 disables this setting.\",\n    )\n    top_k: int = Field(\n        40,\n        description=\"Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40)\",\n    )\n    top_p: float = Field(\n        0.9,\n        description=\"Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)\",\n    )\n    repeat_penalty: float = Field(\n        1.1,\n        description=\"Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. (Default: 1.1)\",\n    )\n\n\nclass HuggingFaceSettings(BaseModel):\n    embedding_hf_model_name: str = Field(\n        description=\"Name of the HuggingFace model to use for embeddings\"\n    )\n    access_token: str = Field(\n        None,\n        description=\"Huggingface access token, required to download some models\",\n    )\n\n\nclass EmbeddingSettings(BaseModel):\n    mode: Literal[\"huggingface\", \"openai\", \"azopenai\", \"sagemaker\", \"ollama\", \"mock\"]\n    ingest_mode: Literal[\"simple\", \"batch\", \"parallel\", \"pipeline\"] = Field(\n        \"simple\",\n        description=(\n            \"The ingest mode to use for the embedding engine:\\n\"\n            \"If `simple` - ingest files sequentially and one by one. It is the historic behaviour.\\n\"\n            \"If `batch` - if multiple files, parse all the files in parallel, \"\n            \"and send them in batch to the embedding model.\\n\"\n            \"In `pipeline` - The Embedding engine is kept as busy as possible\\n\"\n            \"If `parallel` - parse the files in parallel using multiple cores, and embedd them in parallel.\\n\"\n            \"`parallel` is the fastest mode for local setup, as it parallelize IO RW in the index.\\n\"\n            \"For modes that leverage parallelization, you can specify the number of \"\n            \"workers to use with `count_workers`.\\n\"\n        ),\n    )\n    count_workers: int = Field(\n        2,\n        description=(\n            \"The number of workers to use for file ingestion.\\n\"\n            \"In `batch` mode, this is the number of workers used to parse the files.\\n\"\n            \"In `parallel` mode, this is the number of workers used to parse the files and embed them.\\n\"\n            \"In `pipeline` mode, this is the number of workers that can perform embeddings.\\n\"\n            \"This is only used if `ingest_mode` is not `simple`.\\n\"\n            \"Do not go too high with this number, as it might cause memory issues. (especially in `parallel` mode)\\n\"\n            \"Do not set it higher than your number of threads of your CPU.\"\n        ),\n    )\n    embed_dim: int = Field(\n        384,\n        description=\"The dimension of the embeddings stored in the Postgres database\",\n    )\n\n\nclass SagemakerSettings(BaseModel):\n    llm_endpoint_name: str\n    embedding_endpoint_name: str\n\n\nclass OpenAISettings(BaseModel):\n    api_base: str = Field(\n        None,\n        description=\"Base URL of OpenAI API. Example: 'https://api.openai.com/v1'.\",\n    )\n    api_key: str\n    model: str = Field(\n        \"gpt-3.5-turbo\",\n        description=\"OpenAI Model to use. Example: 'gpt-4'.\",\n    )\n    request_timeout: float = Field(\n        120.0,\n        description=\"Time elapsed until openailike server times out the request. Default is 120s. Format is float. \",\n    )\n    embedding_api_base: str = Field(\n        None,\n        description=\"Base URL of OpenAI API. Example: 'https://api.openai.com/v1'.\",\n    )\n    embedding_api_key: str\n    embedding_model: str = Field(\n        \"text-embedding-ada-002\",\n        description=\"OpenAI embedding Model to use. Example: 'text-embedding-3-large'.\",\n    )\n\n\nclass OllamaSettings(BaseModel):\n    api_base: str = Field(\n        \"http://localhost:11434\",\n        description=\"Base URL of Ollama API. Example: 'https://localhost:11434'.\",\n    )\n    embedding_api_base: str = Field(\n        \"http://localhost:11434\",\n        description=\"Base URL of Ollama embedding API. Example: 'https://localhost:11434'.\",\n    )\n    llm_model: str = Field(\n        None,\n        description=\"Model to use. Example: 'llama2-uncensored'.\",\n    )\n    embedding_model: str = Field(\n        None,\n        description=\"Model to use. Example: 'nomic-embed-text'.\",\n    )\n    keep_alive: str = Field(\n        \"5m\",\n        description=\"Time the model will stay loaded in memory after a request. examples: 5m, 5h, '-1' \",\n    )\n    tfs_z: float = Field(\n        1.0,\n        description=\"Tail free sampling is used to reduce the impact of less probable tokens from the output. A higher value (e.g., 2.0) will reduce the impact more, while a value of 1.0 disables this setting.\",\n    )\n    num_predict: int = Field(\n        None,\n        description=\"Maximum number of tokens to predict when generating text. (Default: 128, -1 = infinite generation, -2 = fill context)\",\n    )\n    top_k: int = Field(\n        40,\n        description=\"Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40)\",\n    )\n    top_p: float = Field(\n        0.9,\n        description=\"Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)\",\n    )\n    repeat_last_n: int = Field(\n        64,\n        description=\"Sets how far back for the model to look back to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)\",\n    )\n    repeat_penalty: float = Field(\n        1.1,\n        description=\"Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. (Default: 1.1)\",\n    )\n    request_timeout: float = Field(\n        120.0,\n        description=\"Time elapsed until ollama times out the request. Default is 120s. Format is float. \",\n    )\n\n\nclass AzureOpenAISettings(BaseModel):\n    api_key: str\n    azure_endpoint: str\n    api_version: str = Field(\n        \"2023_05_15\",\n        description=\"The API version to use for this operation. This follows the YYYY-MM-DD format.\",\n    )\n    embedding_deployment_name: str\n    embedding_model: str = Field(\n        \"text-embedding-ada-002\",\n        description=\"OpenAI Model to use. Example: 'text-embedding-ada-002'.\",\n    )\n    llm_deployment_name: str\n    llm_model: str = Field(\n        \"gpt-35-turbo\",\n        description=\"OpenAI Model to use. Example: 'gpt-4'.\",\n    )\n\n\nclass UISettings(BaseModel):\n    enabled: bool\n    path: str\n    default_chat_system_prompt: str = Field(\n        None,\n        description=\"The default system prompt to use for the chat mode.\",\n    )\n    default_query_system_prompt: str = Field(\n        None, description=\"The default system prompt to use for the query mode.\"\n    )\n    delete_file_button_enabled: bool = Field(\n        True, description=\"If the button to delete a file is enabled or not.\"\n    )\n    delete_all_files_button_enabled: bool = Field(\n        False, description=\"If the button to delete all files is enabled or not.\"\n    )\n\n\nclass RerankSettings(BaseModel):\n    enabled: bool = Field(\n        False,\n        description=\"This value controls whether a reranker should be included in the RAG pipeline.\",\n    )\n    model: str = Field(\n        \"cross-encoder/ms-marco-MiniLM-L-2-v2\",\n        description=\"Rerank model to use. Limited to SentenceTransformer cross-encoder models.\",\n    )\n    top_n: int = Field(\n        2,\n        description=\"This value controls the number of documents returned by the RAG pipeline.\",\n    )\n\n\nclass RagSettings(BaseModel):\n    similarity_top_k: int = Field(\n        2,\n        description=\"This value controls the number of documents returned by the RAG pipeline or considered for reranking if enabled.\",\n    )\n    similarity_value: float = Field(\n        None,\n        description=\"If set, any documents retrieved from the RAG must meet a certain match score. Acceptable values are between 0 and 1.\",\n    )\n    rerank: RerankSettings\n\n\nclass PostgresSettings(BaseModel):\n    host: str = Field(\n        \"localhost\",\n        description=\"The server hosting the Postgres database\",\n    )\n    port: int = Field(\n        5432,\n        description=\"The port on which the Postgres database is accessible\",\n    )\n    user: str = Field(\n        \"postgres\",\n        description=\"The user to use to connect to the Postgres database\",\n    )\n    password: str = Field(\n        \"postgres\",\n        description=\"The password to use to connect to the Postgres database\",\n    )\n    database: str = Field(\n        \"postgres\",\n        description=\"The database to use to connect to the Postgres database\",\n    )\n    schema_name: str = Field(\n        \"public\",\n        description=\"The name of the schema in the Postgres database to use\",\n    )\n\n\nclass QdrantSettings(BaseModel):\n    location: str | None = Field(\n        None,\n        description=(\n            \"If `:memory:` - use in-memory Qdrant instance.\\n\"\n            \"If `str` - use it as a `url` parameter.\\n\"\n        ),\n    )\n    url: str | None = Field(\n        None,\n        description=(\n            \"Either host or str of 'Optional[scheme], host, Optional[port], Optional[prefix]'.\"\n        ),\n    )\n    port: int | None = Field(6333, description=\"Port of the REST API interface.\")\n    grpc_port: int | None = Field(6334, description=\"Port of the gRPC interface.\")\n    prefer_grpc: bool | None = Field(\n        False,\n        description=\"If `true` - use gRPC interface whenever possible in custom methods.\",\n    )\n    https: bool | None = Field(\n        None,\n        description=\"If `true` - use HTTPS(SSL) protocol.\",\n    )\n    api_key: str | None = Field(\n        None,\n        description=\"API key for authentication in Qdrant Cloud.\",\n    )\n    prefix: str | None = Field(\n        None,\n        description=(\n            \"Prefix to add to the REST URL path.\"\n            \"Example: `service/v1` will result in \"\n            \"'http://localhost:6333/service/v1/{qdrant-endpoint}' for REST API.\"\n        ),\n    )\n    timeout: float | None = Field(\n        None,\n        description=\"Timeout for REST and gRPC API requests.\",\n    )\n    host: str | None = Field(\n        None,\n        description=\"Host name of Qdrant service. If url and host are None, set to 'localhost'.\",\n    )\n    path: str | None = Field(None, description=\"Persistence path for QdrantLocal.\")\n    force_disable_check_same_thread: bool | None = Field(\n        True,\n        description=(\n            \"For QdrantLocal, force disable check_same_thread. Default: `True`\"\n            \"Only use this if you can guarantee that you can resolve the thread safety outside QdrantClient.\"\n        ),\n    )\n\n\nclass Settings(BaseModel):\n    server: ServerSettings\n    data: DataSettings\n    ui: UISettings\n    llm: LLMSettings\n    embedding: EmbeddingSettings\n    llamacpp: LlamaCPPSettings\n    huggingface: HuggingFaceSettings\n    sagemaker: SagemakerSettings\n    openai: OpenAISettings\n    ollama: OllamaSettings\n    azopenai: AzureOpenAISettings\n    vectorstore: VectorstoreSettings\n    nodestore: NodeStoreSettings\n    rag: RagSettings\n    qdrant: QdrantSettings | None = None\n    postgres: PostgresSettings | None = None\n\n\n\"\"\"\nThis is visible just for DI or testing purposes.\n\nUse dependency injection or `settings()` method instead.\n\"\"\"\nunsafe_settings = load_active_settings()\n\n\"\"\"\nThis is visible just for DI or testing purposes.\n\nUse dependency injection or `settings()` method instead.\n\"\"\"\nunsafe_typed_settings = Settings(**unsafe_settings)\n\n\ndef settings() -> Settings:\n    \"\"\"Get the current loaded settings from the DI container.\n\n    This method exists to keep compatibility with the existing code,\n    that require global access to the settings.\n\n    For regular components use dependency injection instead.\n    \"\"\"\n    from private_gpt.di import global_injector\n\n    return global_injector.get(Settings)\n", "private_gpt/settings/__init__.py": "\"\"\"Settings.\"\"\"\n", "private_gpt/ui/ui.py": "\"\"\"This file should be imported if and only if you want to run the UI locally.\"\"\"\n\nimport itertools\nimport logging\nimport time\nfrom collections.abc import Iterable\nfrom pathlib import Path\nfrom typing import Any\n\nimport gradio as gr  # type: ignore\nfrom fastapi import FastAPI\nfrom gradio.themes.utils.colors import slate  # type: ignore\nfrom injector import inject, singleton\nfrom llama_index.core.llms import ChatMessage, ChatResponse, MessageRole\nfrom pydantic import BaseModel\n\nfrom private_gpt.constants import PROJECT_ROOT_PATH\nfrom private_gpt.di import global_injector\nfrom private_gpt.open_ai.extensions.context_filter import ContextFilter\nfrom private_gpt.server.chat.chat_service import ChatService, CompletionGen\nfrom private_gpt.server.chunks.chunks_service import Chunk, ChunksService\nfrom private_gpt.server.ingest.ingest_service import IngestService\nfrom private_gpt.settings.settings import settings\nfrom private_gpt.ui.images import logo_svg\n\nlogger = logging.getLogger(__name__)\n\nTHIS_DIRECTORY_RELATIVE = Path(__file__).parent.relative_to(PROJECT_ROOT_PATH)\n# Should be \"private_gpt/ui/avatar-bot.ico\"\nAVATAR_BOT = THIS_DIRECTORY_RELATIVE / \"avatar-bot.ico\"\n\nUI_TAB_TITLE = \"My Private GPT\"\n\nSOURCES_SEPARATOR = \"\\n\\n Sources: \\n\"\n\nMODES = [\"Query Files\", \"Search Files\", \"LLM Chat (no context from files)\"]\n\n\nclass Source(BaseModel):\n    file: str\n    page: str\n    text: str\n\n    class Config:\n        frozen = True\n\n    @staticmethod\n    def curate_sources(sources: list[Chunk]) -> list[\"Source\"]:\n        curated_sources = []\n\n        for chunk in sources:\n            doc_metadata = chunk.document.doc_metadata\n\n            file_name = doc_metadata.get(\"file_name\", \"-\") if doc_metadata else \"-\"\n            page_label = doc_metadata.get(\"page_label\", \"-\") if doc_metadata else \"-\"\n\n            source = Source(file=file_name, page=page_label, text=chunk.text)\n            curated_sources.append(source)\n            curated_sources = list(\n                dict.fromkeys(curated_sources).keys()\n            )  # Unique sources only\n\n        return curated_sources\n\n\n@singleton\nclass PrivateGptUi:\n    @inject\n    def __init__(\n        self,\n        ingest_service: IngestService,\n        chat_service: ChatService,\n        chunks_service: ChunksService,\n    ) -> None:\n        self._ingest_service = ingest_service\n        self._chat_service = chat_service\n        self._chunks_service = chunks_service\n\n        # Cache the UI blocks\n        self._ui_block = None\n\n        self._selected_filename = None\n\n        # Initialize system prompt based on default mode\n        self.mode = MODES[0]\n        self._system_prompt = self._get_default_system_prompt(self.mode)\n\n    def _chat(self, message: str, history: list[list[str]], mode: str, *_: Any) -> Any:\n        def yield_deltas(completion_gen: CompletionGen) -> Iterable[str]:\n            full_response: str = \"\"\n            stream = completion_gen.response\n            for delta in stream:\n                if isinstance(delta, str):\n                    full_response += str(delta)\n                elif isinstance(delta, ChatResponse):\n                    full_response += delta.delta or \"\"\n                yield full_response\n                time.sleep(0.02)\n\n            if completion_gen.sources:\n                full_response += SOURCES_SEPARATOR\n                cur_sources = Source.curate_sources(completion_gen.sources)\n                sources_text = \"\\n\\n\\n\"\n                used_files = set()\n                for index, source in enumerate(cur_sources, start=1):\n                    if f\"{source.file}-{source.page}\" not in used_files:\n                        sources_text = (\n                            sources_text\n                            + f\"{index}. {source.file} (page {source.page}) \\n\\n\"\n                        )\n                        used_files.add(f\"{source.file}-{source.page}\")\n                full_response += sources_text\n            yield full_response\n\n        def build_history() -> list[ChatMessage]:\n            history_messages: list[ChatMessage] = list(\n                itertools.chain(\n                    *[\n                        [\n                            ChatMessage(content=interaction[0], role=MessageRole.USER),\n                            ChatMessage(\n                                # Remove from history content the Sources information\n                                content=interaction[1].split(SOURCES_SEPARATOR)[0],\n                                role=MessageRole.ASSISTANT,\n                            ),\n                        ]\n                        for interaction in history\n                    ]\n                )\n            )\n\n            # max 20 messages to try to avoid context overflow\n            return history_messages[:20]\n\n        new_message = ChatMessage(content=message, role=MessageRole.USER)\n        all_messages = [*build_history(), new_message]\n        # If a system prompt is set, add it as a system message\n        if self._system_prompt:\n            all_messages.insert(\n                0,\n                ChatMessage(\n                    content=self._system_prompt,\n                    role=MessageRole.SYSTEM,\n                ),\n            )\n        match mode:\n            case \"Query Files\":\n\n                # Use only the selected file for the query\n                context_filter = None\n                if self._selected_filename is not None:\n                    docs_ids = []\n                    for ingested_document in self._ingest_service.list_ingested():\n                        if (\n                            ingested_document.doc_metadata[\"file_name\"]\n                            == self._selected_filename\n                        ):\n                            docs_ids.append(ingested_document.doc_id)\n                    context_filter = ContextFilter(docs_ids=docs_ids)\n\n                query_stream = self._chat_service.stream_chat(\n                    messages=all_messages,\n                    use_context=True,\n                    context_filter=context_filter,\n                )\n                yield from yield_deltas(query_stream)\n            case \"LLM Chat (no context from files)\":\n                llm_stream = self._chat_service.stream_chat(\n                    messages=all_messages,\n                    use_context=False,\n                )\n                yield from yield_deltas(llm_stream)\n\n            case \"Search Files\":\n                response = self._chunks_service.retrieve_relevant(\n                    text=message, limit=4, prev_next_chunks=0\n                )\n\n                sources = Source.curate_sources(response)\n\n                yield \"\\n\\n\\n\".join(\n                    f\"{index}. **{source.file} \"\n                    f\"(page {source.page})**\\n \"\n                    f\"{source.text}\"\n                    for index, source in enumerate(sources, start=1)\n                )\n\n    # On initialization and on mode change, this function set the system prompt\n    # to the default prompt based on the mode (and user settings).\n    @staticmethod\n    def _get_default_system_prompt(mode: str) -> str:\n        p = \"\"\n        match mode:\n            # For query chat mode, obtain default system prompt from settings\n            case \"Query Files\":\n                p = settings().ui.default_query_system_prompt\n            # For chat mode, obtain default system prompt from settings\n            case \"LLM Chat (no context from files)\":\n                p = settings().ui.default_chat_system_prompt\n            # For any other mode, clear the system prompt\n            case _:\n                p = \"\"\n        return p\n\n    def _set_system_prompt(self, system_prompt_input: str) -> None:\n        logger.info(f\"Setting system prompt to: {system_prompt_input}\")\n        self._system_prompt = system_prompt_input\n\n    def _set_current_mode(self, mode: str) -> Any:\n        self.mode = mode\n        self._set_system_prompt(self._get_default_system_prompt(mode))\n        # Update placeholder and allow interaction if default system prompt is set\n        if self._system_prompt:\n            return gr.update(placeholder=self._system_prompt, interactive=True)\n        # Update placeholder and disable interaction if no default system prompt is set\n        else:\n            return gr.update(placeholder=self._system_prompt, interactive=False)\n\n    def _list_ingested_files(self) -> list[list[str]]:\n        files = set()\n        for ingested_document in self._ingest_service.list_ingested():\n            if ingested_document.doc_metadata is None:\n                # Skipping documents without metadata\n                continue\n            file_name = ingested_document.doc_metadata.get(\n                \"file_name\", \"[FILE NAME MISSING]\"\n            )\n            files.add(file_name)\n        return [[row] for row in files]\n\n    def _upload_file(self, files: list[str]) -> None:\n        logger.debug(\"Loading count=%s files\", len(files))\n        paths = [Path(file) for file in files]\n\n        # remove all existing Documents with name identical to a new file upload:\n        file_names = [path.name for path in paths]\n        doc_ids_to_delete = []\n        for ingested_document in self._ingest_service.list_ingested():\n            if (\n                ingested_document.doc_metadata\n                and ingested_document.doc_metadata[\"file_name\"] in file_names\n            ):\n                doc_ids_to_delete.append(ingested_document.doc_id)\n        if len(doc_ids_to_delete) > 0:\n            logger.info(\n                \"Uploading file(s) which were already ingested: %s document(s) will be replaced.\",\n                len(doc_ids_to_delete),\n            )\n            for doc_id in doc_ids_to_delete:\n                self._ingest_service.delete(doc_id)\n\n        self._ingest_service.bulk_ingest([(str(path.name), path) for path in paths])\n\n    def _delete_all_files(self) -> Any:\n        ingested_files = self._ingest_service.list_ingested()\n        logger.debug(\"Deleting count=%s files\", len(ingested_files))\n        for ingested_document in ingested_files:\n            self._ingest_service.delete(ingested_document.doc_id)\n        return [\n            gr.List(self._list_ingested_files()),\n            gr.components.Button(interactive=False),\n            gr.components.Button(interactive=False),\n            gr.components.Textbox(\"All files\"),\n        ]\n\n    def _delete_selected_file(self) -> Any:\n        logger.debug(\"Deleting selected %s\", self._selected_filename)\n        # Note: keep looping for pdf's (each page became a Document)\n        for ingested_document in self._ingest_service.list_ingested():\n            if (\n                ingested_document.doc_metadata\n                and ingested_document.doc_metadata[\"file_name\"]\n                == self._selected_filename\n            ):\n                self._ingest_service.delete(ingested_document.doc_id)\n        return [\n            gr.List(self._list_ingested_files()),\n            gr.components.Button(interactive=False),\n            gr.components.Button(interactive=False),\n            gr.components.Textbox(\"All files\"),\n        ]\n\n    def _deselect_selected_file(self) -> Any:\n        self._selected_filename = None\n        return [\n            gr.components.Button(interactive=False),\n            gr.components.Button(interactive=False),\n            gr.components.Textbox(\"All files\"),\n        ]\n\n    def _selected_a_file(self, select_data: gr.SelectData) -> Any:\n        self._selected_filename = select_data.value\n        return [\n            gr.components.Button(interactive=True),\n            gr.components.Button(interactive=True),\n            gr.components.Textbox(self._selected_filename),\n        ]\n\n    def _build_ui_blocks(self) -> gr.Blocks:\n        logger.debug(\"Creating the UI blocks\")\n        with gr.Blocks(\n            title=UI_TAB_TITLE,\n            theme=gr.themes.Soft(primary_hue=slate),\n            css=\".logo { \"\n            \"display:flex;\"\n            \"background-color: #C7BAFF;\"\n            \"height: 80px;\"\n            \"border-radius: 8px;\"\n            \"align-content: center;\"\n            \"justify-content: center;\"\n            \"align-items: center;\"\n            \"}\"\n            \".logo img { height: 25% }\"\n            \".contain { display: flex !important; flex-direction: column !important; }\"\n            \"#component-0, #component-3, #component-10, #component-8  { height: 100% !important; }\"\n            \"#chatbot { flex-grow: 1 !important; overflow: auto !important;}\"\n            \"#col { height: calc(100vh - 112px - 16px) !important; }\",\n        ) as blocks:\n            with gr.Row():\n                gr.HTML(f\"<div class='logo'/><img src={logo_svg} alt=PrivateGPT></div\")\n\n            with gr.Row(equal_height=False):\n                with gr.Column(scale=3):\n                    mode = gr.Radio(\n                        MODES,\n                        label=\"Mode\",\n                        value=\"Query Files\",\n                    )\n                    upload_button = gr.components.UploadButton(\n                        \"Upload File(s)\",\n                        type=\"filepath\",\n                        file_count=\"multiple\",\n                        size=\"sm\",\n                    )\n                    ingested_dataset = gr.List(\n                        self._list_ingested_files,\n                        headers=[\"File name\"],\n                        label=\"Ingested Files\",\n                        height=235,\n                        interactive=False,\n                        render=False,  # Rendered under the button\n                    )\n                    upload_button.upload(\n                        self._upload_file,\n                        inputs=upload_button,\n                        outputs=ingested_dataset,\n                    )\n                    ingested_dataset.change(\n                        self._list_ingested_files,\n                        outputs=ingested_dataset,\n                    )\n                    ingested_dataset.render()\n                    deselect_file_button = gr.components.Button(\n                        \"De-select selected file\", size=\"sm\", interactive=False\n                    )\n                    selected_text = gr.components.Textbox(\n                        \"All files\", label=\"Selected for Query or Deletion\", max_lines=1\n                    )\n                    delete_file_button = gr.components.Button(\n                        \"\ud83d\uddd1\ufe0f Delete selected file\",\n                        size=\"sm\",\n                        visible=settings().ui.delete_file_button_enabled,\n                        interactive=False,\n                    )\n                    delete_files_button = gr.components.Button(\n                        \"\u26a0\ufe0f Delete ALL files\",\n                        size=\"sm\",\n                        visible=settings().ui.delete_all_files_button_enabled,\n                    )\n                    deselect_file_button.click(\n                        self._deselect_selected_file,\n                        outputs=[\n                            delete_file_button,\n                            deselect_file_button,\n                            selected_text,\n                        ],\n                    )\n                    ingested_dataset.select(\n                        fn=self._selected_a_file,\n                        outputs=[\n                            delete_file_button,\n                            deselect_file_button,\n                            selected_text,\n                        ],\n                    )\n                    delete_file_button.click(\n                        self._delete_selected_file,\n                        outputs=[\n                            ingested_dataset,\n                            delete_file_button,\n                            deselect_file_button,\n                            selected_text,\n                        ],\n                    )\n                    delete_files_button.click(\n                        self._delete_all_files,\n                        outputs=[\n                            ingested_dataset,\n                            delete_file_button,\n                            deselect_file_button,\n                            selected_text,\n                        ],\n                    )\n                    system_prompt_input = gr.Textbox(\n                        placeholder=self._system_prompt,\n                        label=\"System Prompt\",\n                        lines=2,\n                        interactive=True,\n                        render=False,\n                    )\n                    # When mode changes, set default system prompt\n                    mode.change(\n                        self._set_current_mode, inputs=mode, outputs=system_prompt_input\n                    )\n                    # On blur, set system prompt to use in queries\n                    system_prompt_input.blur(\n                        self._set_system_prompt,\n                        inputs=system_prompt_input,\n                    )\n\n                    def get_model_label() -> str | None:\n                        \"\"\"Get model label from llm mode setting YAML.\n\n                        Raises:\n                            ValueError: If an invalid 'llm_mode' is encountered.\n\n                        Returns:\n                            str: The corresponding model label.\n                        \"\"\"\n                        # Get model label from llm mode setting YAML\n                        # Labels: local, openai, openailike, sagemaker, mock, ollama\n                        config_settings = settings()\n                        if config_settings is None:\n                            raise ValueError(\"Settings are not configured.\")\n\n                        # Get llm_mode from settings\n                        llm_mode = config_settings.llm.mode\n\n                        # Mapping of 'llm_mode' to corresponding model labels\n                        model_mapping = {\n                            \"llamacpp\": config_settings.llamacpp.llm_hf_model_file,\n                            \"openai\": config_settings.openai.model,\n                            \"openailike\": config_settings.openai.model,\n                            \"sagemaker\": config_settings.sagemaker.llm_endpoint_name,\n                            \"mock\": llm_mode,\n                            \"ollama\": config_settings.ollama.llm_model,\n                        }\n\n                        if llm_mode not in model_mapping:\n                            print(f\"Invalid 'llm mode': {llm_mode}\")\n                            return None\n\n                        return model_mapping[llm_mode]\n\n                with gr.Column(scale=7, elem_id=\"col\"):\n                    # Determine the model label based on the value of PGPT_PROFILES\n                    model_label = get_model_label()\n                    if model_label is not None:\n                        label_text = (\n                            f\"LLM: {settings().llm.mode} | Model: {model_label}\"\n                        )\n                    else:\n                        label_text = f\"LLM: {settings().llm.mode}\"\n\n                    _ = gr.ChatInterface(\n                        self._chat,\n                        chatbot=gr.Chatbot(\n                            label=label_text,\n                            show_copy_button=True,\n                            elem_id=\"chatbot\",\n                            render=False,\n                            avatar_images=(\n                                None,\n                                AVATAR_BOT,\n                            ),\n                        ),\n                        additional_inputs=[mode, upload_button, system_prompt_input],\n                    )\n        return blocks\n\n    def get_ui_blocks(self) -> gr.Blocks:\n        if self._ui_block is None:\n            self._ui_block = self._build_ui_blocks()\n        return self._ui_block\n\n    def mount_in_app(self, app: FastAPI, path: str) -> None:\n        blocks = self.get_ui_blocks()\n        blocks.queue()\n        logger.info(\"Mounting the gradio UI, at path=%s\", path)\n        gr.mount_gradio_app(app, blocks, path=path)\n\n\nif __name__ == \"__main__\":\n    ui = global_injector.get(PrivateGptUi)\n    _blocks = ui.get_ui_blocks()\n    _blocks.queue()\n    _blocks.launch(debug=False, show_api=False)\n", "private_gpt/ui/__init__.py": "\"\"\"Gradio based UI.\"\"\"\n", "private_gpt/ui/images.py": "logo_svg = \"data:image/svg+xml;base64,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\"\n", "private_gpt/server/__init__.py": "\"\"\"private-gpt server.\"\"\"\n", "private_gpt/server/chat/chat_service.py": "from dataclasses import dataclass\n\nfrom injector import inject, singleton\nfrom llama_index.core.chat_engine import ContextChatEngine, SimpleChatEngine\nfrom llama_index.core.chat_engine.types import (\n    BaseChatEngine,\n)\nfrom llama_index.core.indices import VectorStoreIndex\nfrom llama_index.core.indices.postprocessor import MetadataReplacementPostProcessor\nfrom llama_index.core.llms import ChatMessage, MessageRole\nfrom llama_index.core.postprocessor import (\n    SentenceTransformerRerank,\n    SimilarityPostprocessor,\n)\nfrom llama_index.core.storage import StorageContext\nfrom llama_index.core.types import TokenGen\nfrom pydantic import BaseModel\n\nfrom private_gpt.components.embedding.embedding_component import EmbeddingComponent\nfrom private_gpt.components.llm.llm_component import LLMComponent\nfrom private_gpt.components.node_store.node_store_component import NodeStoreComponent\nfrom private_gpt.components.vector_store.vector_store_component import (\n    VectorStoreComponent,\n)\nfrom private_gpt.open_ai.extensions.context_filter import ContextFilter\nfrom private_gpt.server.chunks.chunks_service import Chunk\nfrom private_gpt.settings.settings import Settings\n\n\nclass Completion(BaseModel):\n    response: str\n    sources: list[Chunk] | None = None\n\n\nclass CompletionGen(BaseModel):\n    response: TokenGen\n    sources: list[Chunk] | None = None\n\n\n@dataclass\nclass ChatEngineInput:\n    system_message: ChatMessage | None = None\n    last_message: ChatMessage | None = None\n    chat_history: list[ChatMessage] | None = None\n\n    @classmethod\n    def from_messages(cls, messages: list[ChatMessage]) -> \"ChatEngineInput\":\n        # Detect if there is a system message, extract the last message and chat history\n        system_message = (\n            messages[0]\n            if len(messages) > 0 and messages[0].role == MessageRole.SYSTEM\n            else None\n        )\n        last_message = (\n            messages[-1]\n            if len(messages) > 0 and messages[-1].role == MessageRole.USER\n            else None\n        )\n        # Remove from messages list the system message and last message,\n        # if they exist. The rest is the chat history.\n        if system_message:\n            messages.pop(0)\n        if last_message:\n            messages.pop(-1)\n        chat_history = messages if len(messages) > 0 else None\n\n        return cls(\n            system_message=system_message,\n            last_message=last_message,\n            chat_history=chat_history,\n        )\n\n\n@singleton\nclass ChatService:\n    settings: Settings\n\n    @inject\n    def __init__(\n        self,\n        settings: Settings,\n        llm_component: LLMComponent,\n        vector_store_component: VectorStoreComponent,\n        embedding_component: EmbeddingComponent,\n        node_store_component: NodeStoreComponent,\n    ) -> None:\n        self.settings = settings\n        self.llm_component = llm_component\n        self.embedding_component = embedding_component\n        self.vector_store_component = vector_store_component\n        self.storage_context = StorageContext.from_defaults(\n            vector_store=vector_store_component.vector_store,\n            docstore=node_store_component.doc_store,\n            index_store=node_store_component.index_store,\n        )\n        self.index = VectorStoreIndex.from_vector_store(\n            vector_store_component.vector_store,\n            storage_context=self.storage_context,\n            llm=llm_component.llm,\n            embed_model=embedding_component.embedding_model,\n            show_progress=True,\n        )\n\n    def _chat_engine(\n        self,\n        system_prompt: str | None = None,\n        use_context: bool = False,\n        context_filter: ContextFilter | None = None,\n    ) -> BaseChatEngine:\n        settings = self.settings\n        if use_context:\n            vector_index_retriever = self.vector_store_component.get_retriever(\n                index=self.index,\n                context_filter=context_filter,\n                similarity_top_k=self.settings.rag.similarity_top_k,\n            )\n            node_postprocessors = [\n                MetadataReplacementPostProcessor(target_metadata_key=\"window\"),\n                SimilarityPostprocessor(\n                    similarity_cutoff=settings.rag.similarity_value\n                ),\n            ]\n\n            if settings.rag.rerank.enabled:\n                rerank_postprocessor = SentenceTransformerRerank(\n                    model=settings.rag.rerank.model, top_n=settings.rag.rerank.top_n\n                )\n                node_postprocessors.append(rerank_postprocessor)\n\n            return ContextChatEngine.from_defaults(\n                system_prompt=system_prompt,\n                retriever=vector_index_retriever,\n                llm=self.llm_component.llm,  # Takes no effect at the moment\n                node_postprocessors=node_postprocessors,\n            )\n        else:\n            return SimpleChatEngine.from_defaults(\n                system_prompt=system_prompt,\n                llm=self.llm_component.llm,\n            )\n\n    def stream_chat(\n        self,\n        messages: list[ChatMessage],\n        use_context: bool = False,\n        context_filter: ContextFilter | None = None,\n    ) -> CompletionGen:\n        chat_engine_input = ChatEngineInput.from_messages(messages)\n        last_message = (\n            chat_engine_input.last_message.content\n            if chat_engine_input.last_message\n            else None\n        )\n        system_prompt = (\n            chat_engine_input.system_message.content\n            if chat_engine_input.system_message\n            else None\n        )\n        chat_history = (\n            chat_engine_input.chat_history if chat_engine_input.chat_history else None\n        )\n\n        chat_engine = self._chat_engine(\n            system_prompt=system_prompt,\n            use_context=use_context,\n            context_filter=context_filter,\n        )\n        streaming_response = chat_engine.stream_chat(\n            message=last_message if last_message is not None else \"\",\n            chat_history=chat_history,\n        )\n        sources = [Chunk.from_node(node) for node in streaming_response.source_nodes]\n        completion_gen = CompletionGen(\n            response=streaming_response.response_gen, sources=sources\n        )\n        return completion_gen\n\n    def chat(\n        self,\n        messages: list[ChatMessage],\n        use_context: bool = False,\n        context_filter: ContextFilter | None = None,\n    ) -> Completion:\n        chat_engine_input = ChatEngineInput.from_messages(messages)\n        last_message = (\n            chat_engine_input.last_message.content\n            if chat_engine_input.last_message\n            else None\n        )\n        system_prompt = (\n            chat_engine_input.system_message.content\n            if chat_engine_input.system_message\n            else None\n        )\n        chat_history = (\n            chat_engine_input.chat_history if chat_engine_input.chat_history else None\n        )\n\n        chat_engine = self._chat_engine(\n            system_prompt=system_prompt,\n            use_context=use_context,\n            context_filter=context_filter,\n        )\n        wrapped_response = chat_engine.chat(\n            message=last_message if last_message is not None else \"\",\n            chat_history=chat_history,\n        )\n        sources = [Chunk.from_node(node) for node in wrapped_response.source_nodes]\n        completion = Completion(response=wrapped_response.response, sources=sources)\n        return completion\n", "private_gpt/server/chat/chat_router.py": "from fastapi import APIRouter, Depends, Request\nfrom llama_index.core.llms import ChatMessage, MessageRole\nfrom pydantic import BaseModel\nfrom starlette.responses import StreamingResponse\n\nfrom private_gpt.open_ai.extensions.context_filter import ContextFilter\nfrom private_gpt.open_ai.openai_models import (\n    OpenAICompletion,\n    OpenAIMessage,\n    to_openai_response,\n    to_openai_sse_stream,\n)\nfrom private_gpt.server.chat.chat_service import ChatService\nfrom private_gpt.server.utils.auth import authenticated\n\nchat_router = APIRouter(prefix=\"/v1\", dependencies=[Depends(authenticated)])\n\n\nclass ChatBody(BaseModel):\n    messages: list[OpenAIMessage]\n    use_context: bool = False\n    context_filter: ContextFilter | None = None\n    include_sources: bool = True\n    stream: bool = False\n\n    model_config = {\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"messages\": [\n                        {\n                            \"role\": \"system\",\n                            \"content\": \"You are a rapper. Always answer with a rap.\",\n                        },\n                        {\n                            \"role\": \"user\",\n                            \"content\": \"How do you fry an egg?\",\n                        },\n                    ],\n                    \"stream\": False,\n                    \"use_context\": True,\n                    \"include_sources\": True,\n                    \"context_filter\": {\n                        \"docs_ids\": [\"c202d5e6-7b69-4869-81cc-dd574ee8ee11\"]\n                    },\n                }\n            ]\n        }\n    }\n\n\n@chat_router.post(\n    \"/chat/completions\",\n    response_model=None,\n    responses={200: {\"model\": OpenAICompletion}},\n    tags=[\"Contextual Completions\"],\n    openapi_extra={\n        \"x-fern-streaming\": {\n            \"stream-condition\": \"stream\",\n            \"response\": {\"$ref\": \"#/components/schemas/OpenAICompletion\"},\n            \"response-stream\": {\"$ref\": \"#/components/schemas/OpenAICompletion\"},\n        }\n    },\n)\ndef chat_completion(\n    request: Request, body: ChatBody\n) -> OpenAICompletion | StreamingResponse:\n    \"\"\"Given a list of messages comprising a conversation, return a response.\n\n    Optionally include an initial `role: system` message to influence the way\n    the LLM answers.\n\n    If `use_context` is set to `true`, the model will use context coming\n    from the ingested documents to create the response. The documents being used can\n    be filtered using the `context_filter` and passing the document IDs to be used.\n    Ingested documents IDs can be found using `/ingest/list` endpoint. If you want\n    all ingested documents to be used, remove `context_filter` altogether.\n\n    When using `'include_sources': true`, the API will return the source Chunks used\n    to create the response, which come from the context provided.\n\n    When using `'stream': true`, the API will return data chunks following [OpenAI's\n    streaming model](https://platform.openai.com/docs/api-reference/chat/streaming):\n    ```\n    {\"id\":\"12345\",\"object\":\"completion.chunk\",\"created\":1694268190,\n    \"model\":\"private-gpt\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"Hello\"},\n    \"finish_reason\":null}]}\n    ```\n    \"\"\"\n    service = request.state.injector.get(ChatService)\n    all_messages = [\n        ChatMessage(content=m.content, role=MessageRole(m.role)) for m in body.messages\n    ]\n    if body.stream:\n        completion_gen = service.stream_chat(\n            messages=all_messages,\n            use_context=body.use_context,\n            context_filter=body.context_filter,\n        )\n        return StreamingResponse(\n            to_openai_sse_stream(\n                completion_gen.response,\n                completion_gen.sources if body.include_sources else None,\n            ),\n            media_type=\"text/event-stream\",\n        )\n    else:\n        completion = service.chat(\n            messages=all_messages,\n            use_context=body.use_context,\n            context_filter=body.context_filter,\n        )\n        return to_openai_response(\n            completion.response, completion.sources if body.include_sources else None\n        )\n", "private_gpt/server/chat/__init__.py": "", "private_gpt/server/utils/auth.py": "\"\"\"Authentication mechanism for the API.\n\nDefine a simple mechanism to authenticate requests.\nMore complex authentication mechanisms can be defined here, and be placed in the\n`authenticated` method (being a 'bean' injected in fastapi routers).\n\nAuthorization can also be made after the authentication, and depends on\nthe authentication. Authorization should not be implemented in this file.\n\nAuthorization can be done by following fastapi's guides:\n* https://fastapi.tiangolo.com/advanced/security/oauth2-scopes/\n* https://fastapi.tiangolo.com/tutorial/security/\n* https://fastapi.tiangolo.com/tutorial/dependencies/dependencies-in-path-operation-decorators/\n\"\"\"\n\n# mypy: ignore-errors\n# Disabled mypy error: All conditional function variants must have identical signatures\n# We are changing the implementation of the authenticated method, based on\n# the config. If the auth is not enabled, we are not defining the complex method\n# with its dependencies.\nimport logging\nimport secrets\nfrom typing import Annotated\n\nfrom fastapi import Depends, Header, HTTPException\n\nfrom private_gpt.settings.settings import settings\n\n# 401 signify that the request requires authentication.\n# 403 signify that the authenticated user is not authorized to perform the operation.\nNOT_AUTHENTICATED = HTTPException(\n    status_code=401,\n    detail=\"Not authenticated\",\n    headers={\"WWW-Authenticate\": 'Basic realm=\"All the API\", charset=\"UTF-8\"'},\n)\n\nlogger = logging.getLogger(__name__)\n\n\ndef _simple_authentication(authorization: Annotated[str, Header()] = \"\") -> bool:\n    \"\"\"Check if the request is authenticated.\"\"\"\n    if not secrets.compare_digest(authorization, settings().server.auth.secret):\n        # If the \"Authorization\" header is not the expected one, raise an exception.\n        raise NOT_AUTHENTICATED\n    return True\n\n\nif not settings().server.auth.enabled:\n    logger.debug(\n        \"Defining a dummy authentication mechanism for fastapi, always authenticating requests\"\n    )\n\n    # Define a dummy authentication method that always returns True.\n    def authenticated() -> bool:\n        \"\"\"Check if the request is authenticated.\"\"\"\n        return True\n\nelse:\n    logger.info(\"Defining the given authentication mechanism for the API\")\n\n    # Method to be used as a dependency to check if the request is authenticated.\n    def authenticated(\n        _simple_authentication: Annotated[bool, Depends(_simple_authentication)]\n    ) -> bool:\n        \"\"\"Check if the request is authenticated.\"\"\"\n        assert settings().server.auth.enabled\n        if not _simple_authentication:\n            raise NOT_AUTHENTICATED\n        return True\n", "private_gpt/server/utils/__init__.py": "", "private_gpt/server/chunks/chunks_service.py": "from typing import TYPE_CHECKING, Literal\n\nfrom injector import inject, singleton\nfrom llama_index.core.indices import VectorStoreIndex\nfrom llama_index.core.schema import NodeWithScore\nfrom llama_index.core.storage import StorageContext\nfrom pydantic import BaseModel, Field\n\nfrom private_gpt.components.embedding.embedding_component import EmbeddingComponent\nfrom private_gpt.components.llm.llm_component import LLMComponent\nfrom private_gpt.components.node_store.node_store_component import NodeStoreComponent\nfrom private_gpt.components.vector_store.vector_store_component import (\n    VectorStoreComponent,\n)\nfrom private_gpt.open_ai.extensions.context_filter import ContextFilter\nfrom private_gpt.server.ingest.model import IngestedDoc\n\nif TYPE_CHECKING:\n    from llama_index.core.schema import RelatedNodeInfo\n\n\nclass Chunk(BaseModel):\n    object: Literal[\"context.chunk\"]\n    score: float = Field(examples=[0.023])\n    document: IngestedDoc\n    text: str = Field(examples=[\"Outbound sales increased 20%, driven by new leads.\"])\n    previous_texts: list[str] | None = Field(\n        default=None,\n        examples=[[\"SALES REPORT 2023\", \"Inbound didn't show major changes.\"]],\n    )\n    next_texts: list[str] | None = Field(\n        default=None,\n        examples=[\n            [\n                \"New leads came from Google Ads campaign.\",\n                \"The campaign was run by the Marketing Department\",\n            ]\n        ],\n    )\n\n    @classmethod\n    def from_node(cls: type[\"Chunk\"], node: NodeWithScore) -> \"Chunk\":\n        doc_id = node.node.ref_doc_id if node.node.ref_doc_id is not None else \"-\"\n        return cls(\n            object=\"context.chunk\",\n            score=node.score or 0.0,\n            document=IngestedDoc(\n                object=\"ingest.document\",\n                doc_id=doc_id,\n                doc_metadata=node.metadata,\n            ),\n            text=node.get_content(),\n        )\n\n\n@singleton\nclass ChunksService:\n    @inject\n    def __init__(\n        self,\n        llm_component: LLMComponent,\n        vector_store_component: VectorStoreComponent,\n        embedding_component: EmbeddingComponent,\n        node_store_component: NodeStoreComponent,\n    ) -> None:\n        self.vector_store_component = vector_store_component\n        self.llm_component = llm_component\n        self.embedding_component = embedding_component\n        self.storage_context = StorageContext.from_defaults(\n            vector_store=vector_store_component.vector_store,\n            docstore=node_store_component.doc_store,\n            index_store=node_store_component.index_store,\n        )\n\n    def _get_sibling_nodes_text(\n        self, node_with_score: NodeWithScore, related_number: int, forward: bool = True\n    ) -> list[str]:\n        explored_nodes_texts = []\n        current_node = node_with_score.node\n        for _ in range(related_number):\n            explored_node_info: RelatedNodeInfo | None = (\n                current_node.next_node if forward else current_node.prev_node\n            )\n            if explored_node_info is None:\n                break\n\n            explored_node = self.storage_context.docstore.get_node(\n                explored_node_info.node_id\n            )\n\n            explored_nodes_texts.append(explored_node.get_content())\n            current_node = explored_node\n\n        return explored_nodes_texts\n\n    def retrieve_relevant(\n        self,\n        text: str,\n        context_filter: ContextFilter | None = None,\n        limit: int = 10,\n        prev_next_chunks: int = 0,\n    ) -> list[Chunk]:\n        index = VectorStoreIndex.from_vector_store(\n            self.vector_store_component.vector_store,\n            storage_context=self.storage_context,\n            llm=self.llm_component.llm,\n            embed_model=self.embedding_component.embedding_model,\n            show_progress=True,\n        )\n        vector_index_retriever = self.vector_store_component.get_retriever(\n            index=index, context_filter=context_filter, similarity_top_k=limit\n        )\n        nodes = vector_index_retriever.retrieve(text)\n        nodes.sort(key=lambda n: n.score or 0.0, reverse=True)\n\n        retrieved_nodes = []\n        for node in nodes:\n            chunk = Chunk.from_node(node)\n            chunk.previous_texts = self._get_sibling_nodes_text(\n                node, prev_next_chunks, False\n            )\n            chunk.next_texts = self._get_sibling_nodes_text(node, prev_next_chunks)\n            retrieved_nodes.append(chunk)\n\n        return retrieved_nodes\n", "private_gpt/server/chunks/chunks_router.py": "from typing import Literal\n\nfrom fastapi import APIRouter, Depends, Request\nfrom pydantic import BaseModel, Field\n\nfrom private_gpt.open_ai.extensions.context_filter import ContextFilter\nfrom private_gpt.server.chunks.chunks_service import Chunk, ChunksService\nfrom private_gpt.server.utils.auth import authenticated\n\nchunks_router = APIRouter(prefix=\"/v1\", dependencies=[Depends(authenticated)])\n\n\nclass ChunksBody(BaseModel):\n    text: str = Field(examples=[\"Q3 2023 sales\"])\n    context_filter: ContextFilter | None = None\n    limit: int = 10\n    prev_next_chunks: int = Field(default=0, examples=[2])\n\n\nclass ChunksResponse(BaseModel):\n    object: Literal[\"list\"]\n    model: Literal[\"private-gpt\"]\n    data: list[Chunk]\n\n\n@chunks_router.post(\"/chunks\", tags=[\"Context Chunks\"])\ndef chunks_retrieval(request: Request, body: ChunksBody) -> ChunksResponse:\n    \"\"\"Given a `text`, returns the most relevant chunks from the ingested documents.\n\n    The returned information can be used to generate prompts that can be\n    passed to `/completions` or `/chat/completions` APIs. Note: it is usually a very\n    fast API, because only the Embeddings model is involved, not the LLM. The\n    returned information contains the relevant chunk `text` together with the source\n    `document` it is coming from. It also contains a score that can be used to\n    compare different results.\n\n    The max number of chunks to be returned is set using the `limit` param.\n\n    Previous and next chunks (pieces of text that appear right before or after in the\n    document) can be fetched by using the `prev_next_chunks` field.\n\n    The documents being used can be filtered using the `context_filter` and passing\n    the document IDs to be used. Ingested documents IDs can be found using\n    `/ingest/list` endpoint. If you want all ingested documents to be used,\n    remove `context_filter` altogether.\n    \"\"\"\n    service = request.state.injector.get(ChunksService)\n    results = service.retrieve_relevant(\n        body.text, body.context_filter, body.limit, body.prev_next_chunks\n    )\n    return ChunksResponse(\n        object=\"list\",\n        model=\"private-gpt\",\n        data=results,\n    )\n", "private_gpt/server/chunks/__init__.py": "", "private_gpt/server/ingest/model.py": "from typing import Any, Literal\n\nfrom llama_index.core.schema import Document\nfrom pydantic import BaseModel, Field\n\n\nclass IngestedDoc(BaseModel):\n    object: Literal[\"ingest.document\"]\n    doc_id: str = Field(examples=[\"c202d5e6-7b69-4869-81cc-dd574ee8ee11\"])\n    doc_metadata: dict[str, Any] | None = Field(\n        examples=[\n            {\n                \"page_label\": \"2\",\n                \"file_name\": \"Sales Report Q3 2023.pdf\",\n            }\n        ]\n    )\n\n    @staticmethod\n    def curate_metadata(metadata: dict[str, Any]) -> dict[str, Any]:\n        \"\"\"Remove unwanted metadata keys.\"\"\"\n        for key in [\"doc_id\", \"window\", \"original_text\"]:\n            metadata.pop(key, None)\n        return metadata\n\n    @staticmethod\n    def from_document(document: Document) -> \"IngestedDoc\":\n        return IngestedDoc(\n            object=\"ingest.document\",\n            doc_id=document.doc_id,\n            doc_metadata=IngestedDoc.curate_metadata(document.metadata),\n        )\n", "private_gpt/server/ingest/ingest_watcher.py": "from collections.abc import Callable\nfrom pathlib import Path\nfrom typing import Any\n\nfrom watchdog.events import (\n    FileCreatedEvent,\n    FileModifiedEvent,\n    FileSystemEvent,\n    FileSystemEventHandler,\n)\nfrom watchdog.observers import Observer\n\n\nclass IngestWatcher:\n    def __init__(\n        self, watch_path: Path, on_file_changed: Callable[[Path], None]\n    ) -> None:\n        self.watch_path = watch_path\n        self.on_file_changed = on_file_changed\n\n        class Handler(FileSystemEventHandler):\n            def on_modified(self, event: FileSystemEvent) -> None:\n                if isinstance(event, FileModifiedEvent):\n                    on_file_changed(Path(event.src_path))\n\n            def on_created(self, event: FileSystemEvent) -> None:\n                if isinstance(event, FileCreatedEvent):\n                    on_file_changed(Path(event.src_path))\n\n        event_handler = Handler()\n        observer: Any = Observer()\n        self._observer = observer\n        self._observer.schedule(event_handler, str(watch_path), recursive=True)\n\n    def start(self) -> None:\n        self._observer.start()\n        while self._observer.is_alive():\n            try:\n                self._observer.join(1)\n            except KeyboardInterrupt:\n                break\n\n    def stop(self) -> None:\n        self._observer.stop()\n        self._observer.join()\n", "private_gpt/server/ingest/ingest_router.py": "from typing import Literal\n\nfrom fastapi import APIRouter, Depends, HTTPException, Request, UploadFile\nfrom pydantic import BaseModel, Field\n\nfrom private_gpt.server.ingest.ingest_service import IngestService\nfrom private_gpt.server.ingest.model import IngestedDoc\nfrom private_gpt.server.utils.auth import authenticated\n\ningest_router = APIRouter(prefix=\"/v1\", dependencies=[Depends(authenticated)])\n\n\nclass IngestTextBody(BaseModel):\n    file_name: str = Field(examples=[\"Avatar: The Last Airbender\"])\n    text: str = Field(\n        examples=[\n            \"Avatar is set in an Asian and Arctic-inspired world in which some \"\n            \"people can telekinetically manipulate one of the four elements\u2014water, \"\n            \"earth, fire or air\u2014through practices known as 'bending', inspired by \"\n            \"Chinese martial arts.\"\n        ]\n    )\n\n\nclass IngestResponse(BaseModel):\n    object: Literal[\"list\"]\n    model: Literal[\"private-gpt\"]\n    data: list[IngestedDoc]\n\n\n@ingest_router.post(\"/ingest\", tags=[\"Ingestion\"], deprecated=True)\ndef ingest(request: Request, file: UploadFile) -> IngestResponse:\n    \"\"\"Ingests and processes a file.\n\n    Deprecated. Use ingest/file instead.\n    \"\"\"\n    return ingest_file(request, file)\n\n\n@ingest_router.post(\"/ingest/file\", tags=[\"Ingestion\"])\ndef ingest_file(request: Request, file: UploadFile) -> IngestResponse:\n    \"\"\"Ingests and processes a file, storing its chunks to be used as context.\n\n    The context obtained from files is later used in\n    `/chat/completions`, `/completions`, and `/chunks` APIs.\n\n    Most common document\n    formats are supported, but you may be prompted to install an extra dependency to\n    manage a specific file type.\n\n    A file can generate different Documents (for example a PDF generates one Document\n    per page). All Documents IDs are returned in the response, together with the\n    extracted Metadata (which is later used to improve context retrieval). Those IDs\n    can be used to filter the context used to create responses in\n    `/chat/completions`, `/completions`, and `/chunks` APIs.\n    \"\"\"\n    service = request.state.injector.get(IngestService)\n    if file.filename is None:\n        raise HTTPException(400, \"No file name provided\")\n    ingested_documents = service.ingest_bin_data(file.filename, file.file)\n    return IngestResponse(object=\"list\", model=\"private-gpt\", data=ingested_documents)\n\n\n@ingest_router.post(\"/ingest/text\", tags=[\"Ingestion\"])\ndef ingest_text(request: Request, body: IngestTextBody) -> IngestResponse:\n    \"\"\"Ingests and processes a text, storing its chunks to be used as context.\n\n    The context obtained from files is later used in\n    `/chat/completions`, `/completions`, and `/chunks` APIs.\n\n    A Document will be generated with the given text. The Document\n    ID is returned in the response, together with the\n    extracted Metadata (which is later used to improve context retrieval). That ID\n    can be used to filter the context used to create responses in\n    `/chat/completions`, `/completions`, and `/chunks` APIs.\n    \"\"\"\n    service = request.state.injector.get(IngestService)\n    if len(body.file_name) == 0:\n        raise HTTPException(400, \"No file name provided\")\n    ingested_documents = service.ingest_text(body.file_name, body.text)\n    return IngestResponse(object=\"list\", model=\"private-gpt\", data=ingested_documents)\n\n\n@ingest_router.get(\"/ingest/list\", tags=[\"Ingestion\"])\ndef list_ingested(request: Request) -> IngestResponse:\n    \"\"\"Lists already ingested Documents including their Document ID and metadata.\n\n    Those IDs can be used to filter the context used to create responses\n    in `/chat/completions`, `/completions`, and `/chunks` APIs.\n    \"\"\"\n    service = request.state.injector.get(IngestService)\n    ingested_documents = service.list_ingested()\n    return IngestResponse(object=\"list\", model=\"private-gpt\", data=ingested_documents)\n\n\n@ingest_router.delete(\"/ingest/{doc_id}\", tags=[\"Ingestion\"])\ndef delete_ingested(request: Request, doc_id: str) -> None:\n    \"\"\"Delete the specified ingested Document.\n\n    The `doc_id` can be obtained from the `GET /ingest/list` endpoint.\n    The document will be effectively deleted from your storage context.\n    \"\"\"\n    service = request.state.injector.get(IngestService)\n    service.delete(doc_id)\n", "private_gpt/server/ingest/__init__.py": "", "private_gpt/server/ingest/ingest_service.py": "import logging\nimport tempfile\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, AnyStr, BinaryIO\n\nfrom injector import inject, singleton\nfrom llama_index.core.node_parser import SentenceWindowNodeParser\nfrom llama_index.core.storage import StorageContext\n\nfrom private_gpt.components.embedding.embedding_component import EmbeddingComponent\nfrom private_gpt.components.ingest.ingest_component import get_ingestion_component\nfrom private_gpt.components.llm.llm_component import LLMComponent\nfrom private_gpt.components.node_store.node_store_component import NodeStoreComponent\nfrom private_gpt.components.vector_store.vector_store_component import (\n    VectorStoreComponent,\n)\nfrom private_gpt.server.ingest.model import IngestedDoc\nfrom private_gpt.settings.settings import settings\n\nif TYPE_CHECKING:\n    from llama_index.core.storage.docstore.types import RefDocInfo\n\nlogger = logging.getLogger(__name__)\n\n\n@singleton\nclass IngestService:\n    @inject\n    def __init__(\n        self,\n        llm_component: LLMComponent,\n        vector_store_component: VectorStoreComponent,\n        embedding_component: EmbeddingComponent,\n        node_store_component: NodeStoreComponent,\n    ) -> None:\n        self.llm_service = llm_component\n        self.storage_context = StorageContext.from_defaults(\n            vector_store=vector_store_component.vector_store,\n            docstore=node_store_component.doc_store,\n            index_store=node_store_component.index_store,\n        )\n        node_parser = SentenceWindowNodeParser.from_defaults()\n\n        self.ingest_component = get_ingestion_component(\n            self.storage_context,\n            embed_model=embedding_component.embedding_model,\n            transformations=[node_parser, embedding_component.embedding_model],\n            settings=settings(),\n        )\n\n    def _ingest_data(self, file_name: str, file_data: AnyStr) -> list[IngestedDoc]:\n        logger.debug(\"Got file data of size=%s to ingest\", len(file_data))\n        # llama-index mainly supports reading from files, so\n        # we have to create a tmp file to read for it to work\n        # delete=False to avoid a Windows 11 permission error.\n        with tempfile.NamedTemporaryFile(delete=False) as tmp:\n            try:\n                path_to_tmp = Path(tmp.name)\n                if isinstance(file_data, bytes):\n                    path_to_tmp.write_bytes(file_data)\n                else:\n                    path_to_tmp.write_text(str(file_data))\n                return self.ingest_file(file_name, path_to_tmp)\n            finally:\n                tmp.close()\n                path_to_tmp.unlink()\n\n    def ingest_file(self, file_name: str, file_data: Path) -> list[IngestedDoc]:\n        logger.info(\"Ingesting file_name=%s\", file_name)\n        documents = self.ingest_component.ingest(file_name, file_data)\n        logger.info(\"Finished ingestion file_name=%s\", file_name)\n        return [IngestedDoc.from_document(document) for document in documents]\n\n    def ingest_text(self, file_name: str, text: str) -> list[IngestedDoc]:\n        logger.debug(\"Ingesting text data with file_name=%s\", file_name)\n        return self._ingest_data(file_name, text)\n\n    def ingest_bin_data(\n        self, file_name: str, raw_file_data: BinaryIO\n    ) -> list[IngestedDoc]:\n        logger.debug(\"Ingesting binary data with file_name=%s\", file_name)\n        file_data = raw_file_data.read()\n        return self._ingest_data(file_name, file_data)\n\n    def bulk_ingest(self, files: list[tuple[str, Path]]) -> list[IngestedDoc]:\n        logger.info(\"Ingesting file_names=%s\", [f[0] for f in files])\n        documents = self.ingest_component.bulk_ingest(files)\n        logger.info(\"Finished ingestion file_name=%s\", [f[0] for f in files])\n        return [IngestedDoc.from_document(document) for document in documents]\n\n    def list_ingested(self) -> list[IngestedDoc]:\n        ingested_docs: list[IngestedDoc] = []\n        try:\n            docstore = self.storage_context.docstore\n            ref_docs: dict[str, RefDocInfo] | None = docstore.get_all_ref_doc_info()\n\n            if not ref_docs:\n                return ingested_docs\n\n            for doc_id, ref_doc_info in ref_docs.items():\n                doc_metadata = None\n                if ref_doc_info is not None and ref_doc_info.metadata is not None:\n                    doc_metadata = IngestedDoc.curate_metadata(ref_doc_info.metadata)\n                ingested_docs.append(\n                    IngestedDoc(\n                        object=\"ingest.document\",\n                        doc_id=doc_id,\n                        doc_metadata=doc_metadata,\n                    )\n                )\n        except ValueError:\n            logger.warning(\"Got an exception when getting list of docs\", exc_info=True)\n            pass\n        logger.debug(\"Found count=%s ingested documents\", len(ingested_docs))\n        return ingested_docs\n\n    def delete(self, doc_id: str) -> None:\n        \"\"\"Delete an ingested document.\n\n        :raises ValueError: if the document does not exist\n        \"\"\"\n        logger.info(\n            \"Deleting the ingested document=%s in the doc and index store\", doc_id\n        )\n        self.ingest_component.delete(doc_id)\n", "private_gpt/server/completions/completions_router.py": "from fastapi import APIRouter, Depends, Request\nfrom pydantic import BaseModel\nfrom starlette.responses import StreamingResponse\n\nfrom private_gpt.open_ai.extensions.context_filter import ContextFilter\nfrom private_gpt.open_ai.openai_models import (\n    OpenAICompletion,\n    OpenAIMessage,\n)\nfrom private_gpt.server.chat.chat_router import ChatBody, chat_completion\nfrom private_gpt.server.utils.auth import authenticated\n\ncompletions_router = APIRouter(prefix=\"/v1\", dependencies=[Depends(authenticated)])\n\n\nclass CompletionsBody(BaseModel):\n    prompt: str\n    system_prompt: str | None = None\n    use_context: bool = False\n    context_filter: ContextFilter | None = None\n    include_sources: bool = True\n    stream: bool = False\n\n    model_config = {\n        \"json_schema_extra\": {\n            \"examples\": [\n                {\n                    \"prompt\": \"How do you fry an egg?\",\n                    \"system_prompt\": \"You are a rapper. Always answer with a rap.\",\n                    \"stream\": False,\n                    \"use_context\": False,\n                    \"include_sources\": False,\n                }\n            ]\n        }\n    }\n\n\n@completions_router.post(\n    \"/completions\",\n    response_model=None,\n    summary=\"Completion\",\n    responses={200: {\"model\": OpenAICompletion}},\n    tags=[\"Contextual Completions\"],\n    openapi_extra={\n        \"x-fern-streaming\": {\n            \"stream-condition\": \"stream\",\n            \"response\": {\"$ref\": \"#/components/schemas/OpenAICompletion\"},\n            \"response-stream\": {\"$ref\": \"#/components/schemas/OpenAICompletion\"},\n        }\n    },\n)\ndef prompt_completion(\n    request: Request, body: CompletionsBody\n) -> OpenAICompletion | StreamingResponse:\n    \"\"\"We recommend most users use our Chat completions API.\n\n    Given a prompt, the model will return one predicted completion.\n\n    Optionally include a `system_prompt` to influence the way the LLM answers.\n\n    If `use_context`\n    is set to `true`, the model will use context coming from the ingested documents\n    to create the response. The documents being used can be filtered using the\n    `context_filter` and passing the document IDs to be used. Ingested documents IDs\n    can be found using `/ingest/list` endpoint. If you want all ingested documents to\n    be used, remove `context_filter` altogether.\n\n    When using `'include_sources': true`, the API will return the source Chunks used\n    to create the response, which come from the context provided.\n\n    When using `'stream': true`, the API will return data chunks following [OpenAI's\n    streaming model](https://platform.openai.com/docs/api-reference/chat/streaming):\n    ```\n    {\"id\":\"12345\",\"object\":\"completion.chunk\",\"created\":1694268190,\n    \"model\":\"private-gpt\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"Hello\"},\n    \"finish_reason\":null}]}\n    ```\n    \"\"\"\n    messages = [OpenAIMessage(content=body.prompt, role=\"user\")]\n    # If system prompt is passed, create a fake message with the system prompt.\n    if body.system_prompt:\n        messages.insert(0, OpenAIMessage(content=body.system_prompt, role=\"system\"))\n\n    chat_body = ChatBody(\n        messages=messages,\n        use_context=body.use_context,\n        stream=body.stream,\n        include_sources=body.include_sources,\n        context_filter=body.context_filter,\n    )\n    return chat_completion(request, chat_body)\n", "private_gpt/server/completions/__init__.py": "\"\"\"Deprecated Openai compatibility endpoint.\"\"\"\n", "private_gpt/server/embeddings/embeddings_service.py": "from typing import Literal\n\nfrom injector import inject, singleton\nfrom pydantic import BaseModel, Field\n\nfrom private_gpt.components.embedding.embedding_component import EmbeddingComponent\n\n\nclass Embedding(BaseModel):\n    index: int\n    object: Literal[\"embedding\"]\n    embedding: list[float] = Field(examples=[[0.0023064255, -0.009327292]])\n\n\n@singleton\nclass EmbeddingsService:\n    @inject\n    def __init__(self, embedding_component: EmbeddingComponent) -> None:\n        self.embedding_model = embedding_component.embedding_model\n\n    def texts_embeddings(self, texts: list[str]) -> list[Embedding]:\n        texts_embeddings = self.embedding_model.get_text_embedding_batch(texts)\n        return [\n            Embedding(\n                index=texts_embeddings.index(embedding),\n                object=\"embedding\",\n                embedding=embedding,\n            )\n            for embedding in texts_embeddings\n        ]\n", "private_gpt/server/embeddings/__init__.py": "", "private_gpt/server/embeddings/embeddings_router.py": "from typing import Literal\n\nfrom fastapi import APIRouter, Depends, Request\nfrom pydantic import BaseModel\n\nfrom private_gpt.server.embeddings.embeddings_service import (\n    Embedding,\n    EmbeddingsService,\n)\nfrom private_gpt.server.utils.auth import authenticated\n\nembeddings_router = APIRouter(prefix=\"/v1\", dependencies=[Depends(authenticated)])\n\n\nclass EmbeddingsBody(BaseModel):\n    input: str | list[str]\n\n\nclass EmbeddingsResponse(BaseModel):\n    object: Literal[\"list\"]\n    model: Literal[\"private-gpt\"]\n    data: list[Embedding]\n\n\n@embeddings_router.post(\"/embeddings\", tags=[\"Embeddings\"])\ndef embeddings_generation(request: Request, body: EmbeddingsBody) -> EmbeddingsResponse:\n    \"\"\"Get a vector representation of a given input.\n\n    That vector representation can be easily consumed\n    by machine learning models and algorithms.\n    \"\"\"\n    service = request.state.injector.get(EmbeddingsService)\n    input_texts = body.input if isinstance(body.input, list) else [body.input]\n    embeddings = service.texts_embeddings(input_texts)\n    return EmbeddingsResponse(object=\"list\", model=\"private-gpt\", data=embeddings)\n", "private_gpt/server/health/health_router.py": "from typing import Literal\n\nfrom fastapi import APIRouter\nfrom pydantic import BaseModel, Field\n\n# Not authentication or authorization required to get the health status.\nhealth_router = APIRouter()\n\n\nclass HealthResponse(BaseModel):\n    status: Literal[\"ok\"] = Field(default=\"ok\")\n\n\n@health_router.get(\"/health\", tags=[\"Health\"])\ndef health() -> HealthResponse:\n    \"\"\"Return ok if the system is up.\"\"\"\n    return HealthResponse(status=\"ok\")\n", "private_gpt/server/health/__init__.py": "", "private_gpt/open_ai/openai_models.py": "import time\nimport uuid\nfrom collections.abc import Iterator\nfrom typing import Literal\n\nfrom llama_index.core.llms import ChatResponse, CompletionResponse\nfrom pydantic import BaseModel, Field\n\nfrom private_gpt.server.chunks.chunks_service import Chunk\n\n\nclass OpenAIDelta(BaseModel):\n    \"\"\"A piece of completion that needs to be concatenated to get the full message.\"\"\"\n\n    content: str | None\n\n\nclass OpenAIMessage(BaseModel):\n    \"\"\"Inference result, with the source of the message.\n\n    Role could be the assistant or system\n    (providing a default response, not AI generated).\n    \"\"\"\n\n    role: Literal[\"assistant\", \"system\", \"user\"] = Field(default=\"user\")\n    content: str | None\n\n\nclass OpenAIChoice(BaseModel):\n    \"\"\"Response from AI.\n\n    Either the delta or the message will be present, but never both.\n    Sources used will be returned in case context retrieval was enabled.\n    \"\"\"\n\n    finish_reason: str | None = Field(examples=[\"stop\"])\n    delta: OpenAIDelta | None = None\n    message: OpenAIMessage | None = None\n    sources: list[Chunk] | None = None\n    index: int = 0\n\n\nclass OpenAICompletion(BaseModel):\n    \"\"\"Clone of OpenAI Completion model.\n\n    For more information see: https://platform.openai.com/docs/api-reference/chat/object\n    \"\"\"\n\n    id: str\n    object: Literal[\"completion\", \"completion.chunk\"] = Field(default=\"completion\")\n    created: int = Field(..., examples=[1623340000])\n    model: Literal[\"private-gpt\"]\n    choices: list[OpenAIChoice]\n\n    @classmethod\n    def from_text(\n        cls,\n        text: str | None,\n        finish_reason: str | None = None,\n        sources: list[Chunk] | None = None,\n    ) -> \"OpenAICompletion\":\n        return OpenAICompletion(\n            id=str(uuid.uuid4()),\n            object=\"completion\",\n            created=int(time.time()),\n            model=\"private-gpt\",\n            choices=[\n                OpenAIChoice(\n                    message=OpenAIMessage(role=\"assistant\", content=text),\n                    finish_reason=finish_reason,\n                    sources=sources,\n                )\n            ],\n        )\n\n    @classmethod\n    def json_from_delta(\n        cls,\n        *,\n        text: str | None,\n        finish_reason: str | None = None,\n        sources: list[Chunk] | None = None,\n    ) -> str:\n        chunk = OpenAICompletion(\n            id=str(uuid.uuid4()),\n            object=\"completion.chunk\",\n            created=int(time.time()),\n            model=\"private-gpt\",\n            choices=[\n                OpenAIChoice(\n                    delta=OpenAIDelta(content=text),\n                    finish_reason=finish_reason,\n                    sources=sources,\n                )\n            ],\n        )\n\n        return chunk.model_dump_json()\n\n\ndef to_openai_response(\n    response: str | ChatResponse, sources: list[Chunk] | None = None\n) -> OpenAICompletion:\n    if isinstance(response, ChatResponse):\n        return OpenAICompletion.from_text(response.delta, finish_reason=\"stop\")\n    else:\n        return OpenAICompletion.from_text(\n            response, finish_reason=\"stop\", sources=sources\n        )\n\n\ndef to_openai_sse_stream(\n    response_generator: Iterator[str | CompletionResponse | ChatResponse],\n    sources: list[Chunk] | None = None,\n) -> Iterator[str]:\n    for response in response_generator:\n        if isinstance(response, CompletionResponse | ChatResponse):\n            yield f\"data: {OpenAICompletion.json_from_delta(text=response.delta)}\\n\\n\"\n        else:\n            yield f\"data: {OpenAICompletion.json_from_delta(text=response, sources=sources)}\\n\\n\"\n    yield f\"data: {OpenAICompletion.json_from_delta(text='', finish_reason='stop')}\\n\\n\"\n    yield \"data: [DONE]\\n\\n\"\n", "private_gpt/open_ai/__init__.py": "\"\"\"OpenAI compatibility utilities.\"\"\"\n", "private_gpt/open_ai/extensions/context_filter.py": "from pydantic import BaseModel, Field\n\n\nclass ContextFilter(BaseModel):\n    docs_ids: list[str] | None = Field(\n        examples=[[\"c202d5e6-7b69-4869-81cc-dd574ee8ee11\"]]\n    )\n", "private_gpt/open_ai/extensions/__init__.py": "\"\"\"OpenAI API extensions.\"\"\"\n"}