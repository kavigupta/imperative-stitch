{"setup.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom setuptools import setup\n\n\nsetup()\n", "__init__.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom .paddleocr import *\nimport importlib.metadata as importlib_metadata\n\ntry:\n    __version__ = importlib_metadata.version(__package__ or __name__)\nexcept importlib_metadata.PackageNotFoundError:\n    __version__ = \"0.0.0\"\n__all__ = [\n    \"PaddleOCR\",\n    \"PPStructure\",\n    \"draw_ocr\",\n    \"draw_structure_result\",\n    \"save_structure_res\",\n    \"download_with_progressbar\",\n    \"sorted_layout_boxes\",\n    \"convert_info_docx\",\n    \"to_excel\",\n]\n", "paddleocr.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport sys\nimport importlib\n\n__dir__ = os.path.dirname(__file__)\n\nimport paddle\nfrom paddle.utils import try_import\n\nsys.path.append(os.path.join(__dir__, \"\"))\n\nimport cv2\nimport logging\nimport numpy as np\nfrom pathlib import Path\nimport base64\nfrom io import BytesIO\nimport pprint\nfrom PIL import Image\nfrom tools.infer import predict_system\n\n\ndef _import_file(module_name, file_path, make_importable=False):\n    spec = importlib.util.spec_from_file_location(module_name, file_path)\n    module = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(module)\n    if make_importable:\n        sys.modules[module_name] = module\n    return module\n\n\ntools = _import_file(\n    \"tools\", os.path.join(__dir__, \"tools/__init__.py\"), make_importable=True\n)\nppocr = importlib.import_module(\"ppocr\", \"paddleocr\")\nppstructure = importlib.import_module(\"ppstructure\", \"paddleocr\")\nfrom ppocr.utils.logging import get_logger\n\nlogger = get_logger()\nfrom ppocr.utils.utility import (\n    check_and_read,\n    get_image_file_list,\n    alpha_to_color,\n    binarize_img,\n)\nfrom ppocr.utils.network import (\n    maybe_download,\n    download_with_progressbar,\n    is_link,\n    confirm_model_dir_url,\n)\nfrom tools.infer.utility import draw_ocr, str2bool, check_gpu\nfrom ppstructure.utility import init_args, draw_structure_result\nfrom ppstructure.predict_system import StructureSystem, save_structure_res, to_excel\n\nlogger = get_logger()\n__all__ = [\n    \"PaddleOCR\",\n    \"PPStructure\",\n    \"draw_ocr\",\n    \"draw_structure_result\",\n    \"save_structure_res\",\n    \"download_with_progressbar\",\n    \"to_excel\",\n]\n\nSUPPORT_DET_MODEL = [\"DB\"]\nSUPPORT_REC_MODEL = [\"CRNN\", \"SVTR_LCNet\"]\nBASE_DIR = os.path.expanduser(\"~/.paddleocr/\")\n\nDEFAULT_OCR_MODEL_VERSION = \"PP-OCRv4\"\nSUPPORT_OCR_MODEL_VERSION = [\"PP-OCR\", \"PP-OCRv2\", \"PP-OCRv3\", \"PP-OCRv4\"]\nDEFAULT_STRUCTURE_MODEL_VERSION = \"PP-StructureV2\"\nSUPPORT_STRUCTURE_MODEL_VERSION = [\"PP-Structure\", \"PP-StructureV2\"]\nMODEL_URLS = {\n    \"OCR\": {\n        \"PP-OCRv4\": {\n            \"det\": {\n                \"ch\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/PP-OCRv4/chinese/ch_PP-OCRv4_det_infer.tar\",\n                },\n                \"en\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_det_infer.tar\",\n                },\n                \"ml\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/PP-OCRv3/multilingual/Multilingual_PP-OCRv3_det_infer.tar\"\n                },\n            },\n            \"rec\": {\n                \"ch\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/PP-OCRv4/chinese/ch_PP-OCRv4_rec_infer.tar\",\n                    \"dict_path\": \"./ppocr/utils/ppocr_keys_v1.txt\",\n                },\n                \"en\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/PP-OCRv4/english/en_PP-OCRv4_rec_infer.tar\",\n                    \"dict_path\": \"./ppocr/utils/en_dict.txt\",\n                },\n                \"korean\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/PP-OCRv4/multilingual/korean_PP-OCRv4_rec_infer.tar\",\n                    \"dict_path\": \"./ppocr/utils/dict/korean_dict.txt\",\n                },\n                \"japan\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/PP-OCRv4/multilingual/japan_PP-OCRv4_rec_infer.tar\",\n                    \"dict_path\": \"./ppocr/utils/dict/japan_dict.txt\",\n                },\n                \"chinese_cht\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/PP-OCRv3/multilingual/chinese_cht_PP-OCRv3_rec_infer.tar\",\n                    \"dict_path\": \"./ppocr/utils/dict/chinese_cht_dict.txt\",\n                },\n                \"ta\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/PP-OCRv4/multilingual/ta_PP-OCRv4_rec_infer.tar\",\n                    \"dict_path\": \"./ppocr/utils/dict/ta_dict.txt\",\n                },\n                \"te\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/PP-OCRv4/multilingual/te_PP-OCRv4_rec_infer.tar\",\n                    \"dict_path\": \"./ppocr/utils/dict/te_dict.txt\",\n                },\n                \"ka\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/PP-OCRv4/multilingual/ka_PP-OCRv4_rec_infer.tar\",\n                    \"dict_path\": \"./ppocr/utils/dict/ka_dict.txt\",\n                },\n                \"latin\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/PP-OCRv3/multilingual/latin_PP-OCRv3_rec_infer.tar\",\n                    \"dict_path\": \"./ppocr/utils/dict/latin_dict.txt\",\n                },\n                \"arabic\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/PP-OCRv4/multilingual/arabic_PP-OCRv4_rec_infer.tar\",\n                    \"dict_path\": \"./ppocr/utils/dict/arabic_dict.txt\",\n                },\n                \"cyrillic\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/PP-OCRv3/multilingual/cyrillic_PP-OCRv3_rec_infer.tar\",\n                    \"dict_path\": \"./ppocr/utils/dict/cyrillic_dict.txt\",\n                },\n                \"devanagari\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/PP-OCRv4/multilingual/devanagari_PP-OCRv4_rec_infer.tar\",\n                    \"dict_path\": \"./ppocr/utils/dict/devanagari_dict.txt\",\n                },\n            },\n            \"cls\": {\n                \"ch\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar\",\n                }\n            },\n        },\n        \"PP-OCRv3\": {\n            \"det\": {\n                \"ch\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_infer.tar\",\n                },\n                \"en\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_det_infer.tar\",\n                },\n                \"ml\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/PP-OCRv3/multilingual/Multilingual_PP-OCRv3_det_infer.tar\"\n                },\n            },\n            \"rec\": {\n                \"ch\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_rec_infer.tar\",\n                    \"dict_path\": \"./ppocr/utils/ppocr_keys_v1.txt\",\n                },\n                \"en\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_rec_infer.tar\",\n                    \"dict_path\": \"./ppocr/utils/en_dict.txt\",\n                },\n                \"korean\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/PP-OCRv3/multilingual/korean_PP-OCRv3_rec_infer.tar\",\n                    \"dict_path\": \"./ppocr/utils/dict/korean_dict.txt\",\n                },\n                \"japan\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/PP-OCRv3/multilingual/japan_PP-OCRv3_rec_infer.tar\",\n                    \"dict_path\": \"./ppocr/utils/dict/japan_dict.txt\",\n                },\n                \"chinese_cht\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/PP-OCRv3/multilingual/chinese_cht_PP-OCRv3_rec_infer.tar\",\n                    \"dict_path\": \"./ppocr/utils/dict/chinese_cht_dict.txt\",\n                },\n                \"ta\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/PP-OCRv3/multilingual/ta_PP-OCRv3_rec_infer.tar\",\n                    \"dict_path\": \"./ppocr/utils/dict/ta_dict.txt\",\n                },\n                \"te\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/PP-OCRv3/multilingual/te_PP-OCRv3_rec_infer.tar\",\n                    \"dict_path\": \"./ppocr/utils/dict/te_dict.txt\",\n                },\n                \"ka\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/PP-OCRv3/multilingual/ka_PP-OCRv3_rec_infer.tar\",\n                    \"dict_path\": \"./ppocr/utils/dict/ka_dict.txt\",\n                },\n                \"latin\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/PP-OCRv3/multilingual/latin_PP-OCRv3_rec_infer.tar\",\n                    \"dict_path\": \"./ppocr/utils/dict/latin_dict.txt\",\n                },\n                \"arabic\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/PP-OCRv3/multilingual/arabic_PP-OCRv3_rec_infer.tar\",\n                    \"dict_path\": \"./ppocr/utils/dict/arabic_dict.txt\",\n                },\n                \"cyrillic\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/PP-OCRv3/multilingual/cyrillic_PP-OCRv3_rec_infer.tar\",\n                    \"dict_path\": \"./ppocr/utils/dict/cyrillic_dict.txt\",\n                },\n                \"devanagari\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/PP-OCRv3/multilingual/devanagari_PP-OCRv3_rec_infer.tar\",\n                    \"dict_path\": \"./ppocr/utils/dict/devanagari_dict.txt\",\n                },\n            },\n            \"cls\": {\n                \"ch\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar\",\n                }\n            },\n        },\n        \"PP-OCRv2\": {\n            \"det\": {\n                \"ch\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/PP-OCRv2/chinese/ch_PP-OCRv2_det_infer.tar\",\n                },\n            },\n            \"rec\": {\n                \"ch\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/PP-OCRv2/chinese/ch_PP-OCRv2_rec_infer.tar\",\n                    \"dict_path\": \"./ppocr/utils/ppocr_keys_v1.txt\",\n                }\n            },\n            \"cls\": {\n                \"ch\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar\",\n                }\n            },\n        },\n        \"PP-OCR\": {\n            \"det\": {\n                \"ch\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_det_infer.tar\",\n                },\n                \"en\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/dygraph_v2.0/multilingual/en_ppocr_mobile_v2.0_det_infer.tar\",\n                },\n                \"structure\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/dygraph_v2.0/table/en_ppocr_mobile_v2.0_table_det_infer.tar\"\n                },\n            },\n            \"rec\": {\n                \"ch\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_rec_infer.tar\",\n                    \"dict_path\": \"./ppocr/utils/ppocr_keys_v1.txt\",\n                },\n                \"en\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/dygraph_v2.0/multilingual/en_number_mobile_v2.0_rec_infer.tar\",\n                    \"dict_path\": \"./ppocr/utils/en_dict.txt\",\n                },\n                \"french\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/dygraph_v2.0/multilingual/french_mobile_v2.0_rec_infer.tar\",\n                    \"dict_path\": \"./ppocr/utils/dict/french_dict.txt\",\n                },\n                \"german\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/dygraph_v2.0/multilingual/german_mobile_v2.0_rec_infer.tar\",\n                    \"dict_path\": \"./ppocr/utils/dict/german_dict.txt\",\n                },\n                \"korean\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/dygraph_v2.0/multilingual/korean_mobile_v2.0_rec_infer.tar\",\n                    \"dict_path\": \"./ppocr/utils/dict/korean_dict.txt\",\n                },\n                \"japan\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/dygraph_v2.0/multilingual/japan_mobile_v2.0_rec_infer.tar\",\n                    \"dict_path\": \"./ppocr/utils/dict/japan_dict.txt\",\n                },\n                \"chinese_cht\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/dygraph_v2.0/multilingual/chinese_cht_mobile_v2.0_rec_infer.tar\",\n                    \"dict_path\": \"./ppocr/utils/dict/chinese_cht_dict.txt\",\n                },\n                \"ta\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/dygraph_v2.0/multilingual/ta_mobile_v2.0_rec_infer.tar\",\n                    \"dict_path\": \"./ppocr/utils/dict/ta_dict.txt\",\n                },\n                \"te\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/dygraph_v2.0/multilingual/te_mobile_v2.0_rec_infer.tar\",\n                    \"dict_path\": \"./ppocr/utils/dict/te_dict.txt\",\n                },\n                \"ka\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/dygraph_v2.0/multilingual/ka_mobile_v2.0_rec_infer.tar\",\n                    \"dict_path\": \"./ppocr/utils/dict/ka_dict.txt\",\n                },\n                \"latin\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/dygraph_v2.0/multilingual/latin_ppocr_mobile_v2.0_rec_infer.tar\",\n                    \"dict_path\": \"./ppocr/utils/dict/latin_dict.txt\",\n                },\n                \"arabic\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/dygraph_v2.0/multilingual/arabic_ppocr_mobile_v2.0_rec_infer.tar\",\n                    \"dict_path\": \"./ppocr/utils/dict/arabic_dict.txt\",\n                },\n                \"cyrillic\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/dygraph_v2.0/multilingual/cyrillic_ppocr_mobile_v2.0_rec_infer.tar\",\n                    \"dict_path\": \"./ppocr/utils/dict/cyrillic_dict.txt\",\n                },\n                \"devanagari\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/dygraph_v2.0/multilingual/devanagari_ppocr_mobile_v2.0_rec_infer.tar\",\n                    \"dict_path\": \"./ppocr/utils/dict/devanagari_dict.txt\",\n                },\n                \"structure\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/dygraph_v2.0/table/en_ppocr_mobile_v2.0_table_rec_infer.tar\",\n                    \"dict_path\": \"ppocr/utils/dict/table_dict.txt\",\n                },\n            },\n            \"cls\": {\n                \"ch\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar\",\n                }\n            },\n        },\n    },\n    \"STRUCTURE\": {\n        \"PP-Structure\": {\n            \"table\": {\n                \"en\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/dygraph_v2.0/table/en_ppocr_mobile_v2.0_table_structure_infer.tar\",\n                    \"dict_path\": \"ppocr/utils/dict/table_structure_dict.txt\",\n                }\n            }\n        },\n        \"PP-StructureV2\": {\n            \"table\": {\n                \"en\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/ppstructure/models/slanet/en_ppstructure_mobile_v2.0_SLANet_infer.tar\",\n                    \"dict_path\": \"ppocr/utils/dict/table_structure_dict.txt\",\n                },\n                \"ch\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/ppstructure/models/slanet/ch_ppstructure_mobile_v2.0_SLANet_infer.tar\",\n                    \"dict_path\": \"ppocr/utils/dict/table_structure_dict_ch.txt\",\n                },\n            },\n            \"layout\": {\n                \"en\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/ppstructure/models/layout/picodet_lcnet_x1_0_fgd_layout_infer.tar\",\n                    \"dict_path\": \"ppocr/utils/dict/layout_dict/layout_publaynet_dict.txt\",\n                },\n                \"ch\": {\n                    \"url\": \"https://paddleocr.bj.bcebos.com/ppstructure/models/layout/picodet_lcnet_x1_0_fgd_layout_cdla_infer.tar\",\n                    \"dict_path\": \"ppocr/utils/dict/layout_dict/layout_cdla_dict.txt\",\n                },\n            },\n        },\n    },\n}\n\n\ndef parse_args(mMain=True):\n    import argparse\n\n    parser = init_args()\n    parser.add_help = mMain\n    parser.add_argument(\"--lang\", type=str, default=\"ch\")\n    parser.add_argument(\"--det\", type=str2bool, default=True)\n    parser.add_argument(\"--rec\", type=str2bool, default=True)\n    parser.add_argument(\"--type\", type=str, default=\"ocr\")\n    parser.add_argument(\"--savefile\", type=str2bool, default=False)\n    parser.add_argument(\n        \"--ocr_version\",\n        type=str,\n        choices=SUPPORT_OCR_MODEL_VERSION,\n        default=\"PP-OCRv4\",\n        help=\"OCR Model version, the current model support list is as follows: \"\n        \"1. PP-OCRv4/v3 Support Chinese and English detection and recognition model, and direction classifier model\"\n        \"2. PP-OCRv2 Support Chinese detection and recognition model. \"\n        \"3. PP-OCR support Chinese detection, recognition and direction classifier and multilingual recognition model.\",\n    )\n    parser.add_argument(\n        \"--structure_version\",\n        type=str,\n        choices=SUPPORT_STRUCTURE_MODEL_VERSION,\n        default=\"PP-StructureV2\",\n        help=\"Model version, the current model support list is as follows:\"\n        \" 1. PP-Structure Support en table structure model.\"\n        \" 2. PP-StructureV2 Support ch and en table structure model.\",\n    )\n\n    for action in parser._actions:\n        if action.dest in [\n            \"rec_char_dict_path\",\n            \"table_char_dict_path\",\n            \"layout_dict_path\",\n        ]:\n            action.default = None\n    if mMain:\n        return parser.parse_args()\n    else:\n        inference_args_dict = {}\n        for action in parser._actions:\n            inference_args_dict[action.dest] = action.default\n        return argparse.Namespace(**inference_args_dict)\n\n\ndef parse_lang(lang):\n    latin_lang = [\n        \"af\",\n        \"az\",\n        \"bs\",\n        \"cs\",\n        \"cy\",\n        \"da\",\n        \"de\",\n        \"es\",\n        \"et\",\n        \"fr\",\n        \"ga\",\n        \"hr\",\n        \"hu\",\n        \"id\",\n        \"is\",\n        \"it\",\n        \"ku\",\n        \"la\",\n        \"lt\",\n        \"lv\",\n        \"mi\",\n        \"ms\",\n        \"mt\",\n        \"nl\",\n        \"no\",\n        \"oc\",\n        \"pi\",\n        \"pl\",\n        \"pt\",\n        \"ro\",\n        \"rs_latin\",\n        \"sk\",\n        \"sl\",\n        \"sq\",\n        \"sv\",\n        \"sw\",\n        \"tl\",\n        \"tr\",\n        \"uz\",\n        \"vi\",\n        \"french\",\n        \"german\",\n    ]\n    arabic_lang = [\"ar\", \"fa\", \"ug\", \"ur\"]\n    cyrillic_lang = [\n        \"ru\",\n        \"rs_cyrillic\",\n        \"be\",\n        \"bg\",\n        \"uk\",\n        \"mn\",\n        \"abq\",\n        \"ady\",\n        \"kbd\",\n        \"ava\",\n        \"dar\",\n        \"inh\",\n        \"che\",\n        \"lbe\",\n        \"lez\",\n        \"tab\",\n    ]\n    devanagari_lang = [\n        \"hi\",\n        \"mr\",\n        \"ne\",\n        \"bh\",\n        \"mai\",\n        \"ang\",\n        \"bho\",\n        \"mah\",\n        \"sck\",\n        \"new\",\n        \"gom\",\n        \"sa\",\n        \"bgc\",\n    ]\n    if lang in latin_lang:\n        lang = \"latin\"\n    elif lang in arabic_lang:\n        lang = \"arabic\"\n    elif lang in cyrillic_lang:\n        lang = \"cyrillic\"\n    elif lang in devanagari_lang:\n        lang = \"devanagari\"\n    assert (\n        lang in MODEL_URLS[\"OCR\"][DEFAULT_OCR_MODEL_VERSION][\"rec\"]\n    ), \"param lang must in {}, but got {}\".format(\n        MODEL_URLS[\"OCR\"][DEFAULT_OCR_MODEL_VERSION][\"rec\"].keys(), lang\n    )\n    if lang == \"ch\":\n        det_lang = \"ch\"\n    elif lang == \"structure\":\n        det_lang = \"structure\"\n    elif lang in [\"en\", \"latin\"]:\n        det_lang = \"en\"\n    else:\n        det_lang = \"ml\"\n    return lang, det_lang\n\n\ndef get_model_config(type, version, model_type, lang):\n    if type == \"OCR\":\n        DEFAULT_MODEL_VERSION = DEFAULT_OCR_MODEL_VERSION\n    elif type == \"STRUCTURE\":\n        DEFAULT_MODEL_VERSION = DEFAULT_STRUCTURE_MODEL_VERSION\n    else:\n        raise NotImplementedError\n\n    model_urls = MODEL_URLS[type]\n    if version not in model_urls:\n        version = DEFAULT_MODEL_VERSION\n    if model_type not in model_urls[version]:\n        if model_type in model_urls[DEFAULT_MODEL_VERSION]:\n            version = DEFAULT_MODEL_VERSION\n        else:\n            logger.error(\n                \"{} models is not support, we only support {}\".format(\n                    model_type, model_urls[DEFAULT_MODEL_VERSION].keys()\n                )\n            )\n            sys.exit(-1)\n\n    if lang not in model_urls[version][model_type]:\n        if lang in model_urls[DEFAULT_MODEL_VERSION][model_type]:\n            version = DEFAULT_MODEL_VERSION\n        else:\n            logger.error(\n                \"lang {} is not support, we only support {} for {} models\".format(\n                    lang,\n                    model_urls[DEFAULT_MODEL_VERSION][model_type].keys(),\n                    model_type,\n                )\n            )\n            sys.exit(-1)\n    return model_urls[version][model_type][lang]\n\n\ndef img_decode(content: bytes):\n    np_arr = np.frombuffer(content, dtype=np.uint8)\n    return cv2.imdecode(np_arr, cv2.IMREAD_UNCHANGED)\n\n\ndef check_img(img, alpha_color=(255, 255, 255)):\n    \"\"\"\n    Check the image data. If it is another type of image file, try to decode it into a numpy array.\n    The inference network requires three-channel images, So the following channel conversions are done\n        single channel image: Gray to RGB R\u2190Y,G\u2190Y,B\u2190Y\n        four channel image: alpha_to_color\n    args:\n        img: image data\n            file format: jpg, png and other image formats that opencv can decode, as well as gif and pdf formats\n            storage type: binary image, net image file, local image file\n        alpha_color: Background color in images in RGBA format\n        return: numpy.array (h, w, 3) or list (p, h, w, 3) (p: page of pdf), boolean, boolean\n    \"\"\"\n    flag_gif, flag_pdf = False, False\n    if isinstance(img, bytes):\n        img = img_decode(img)\n    if isinstance(img, str):\n        # download net image\n        if is_link(img):\n            download_with_progressbar(img, \"tmp.jpg\")\n            img = \"tmp.jpg\"\n        image_file = img\n        img, flag_gif, flag_pdf = check_and_read(image_file)\n        if not flag_gif and not flag_pdf:\n            with open(image_file, \"rb\") as f:\n                img_str = f.read()\n                img = img_decode(img_str)\n            if img is None:\n                try:\n                    buf = BytesIO()\n                    image = BytesIO(img_str)\n                    im = Image.open(image)\n                    rgb = im.convert(\"RGB\")\n                    rgb.save(buf, \"jpeg\")\n                    buf.seek(0)\n                    image_bytes = buf.read()\n                    data_base64 = str(base64.b64encode(image_bytes), encoding=\"utf-8\")\n                    image_decode = base64.b64decode(data_base64)\n                    img_array = np.frombuffer(image_decode, np.uint8)\n                    img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)\n                except:\n                    logger.error(\"error in loading image:{}\".format(image_file))\n                    return None, flag_gif, flag_pdf\n        if img is None:\n            logger.error(\"error in loading image:{}\".format(image_file))\n            return None, flag_gif, flag_pdf\n    # single channel image array.shape:h,w\n    if isinstance(img, np.ndarray) and len(img.shape) == 2:\n        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n    # four channel image array.shape:h,w,c\n    if isinstance(img, np.ndarray) and len(img.shape) == 3 and img.shape[2] == 4:\n        img = alpha_to_color(img, alpha_color)\n    return img, flag_gif, flag_pdf\n\n\nclass PaddleOCR(predict_system.TextSystem):\n    def __init__(self, **kwargs):\n        \"\"\"\n        paddleocr package\n        args:\n            **kwargs: other params show in paddleocr --help\n        \"\"\"\n        params = parse_args(mMain=False)\n        params.__dict__.update(**kwargs)\n        assert (\n            params.ocr_version in SUPPORT_OCR_MODEL_VERSION\n        ), \"ocr_version must in {}, but get {}\".format(\n            SUPPORT_OCR_MODEL_VERSION, params.ocr_version\n        )\n        params.use_gpu = check_gpu(params.use_gpu)\n\n        if not params.show_log:\n            logger.setLevel(logging.INFO)\n        self.use_angle_cls = params.use_angle_cls\n        lang, det_lang = parse_lang(params.lang)\n\n        # init model dir\n        det_model_config = get_model_config(\"OCR\", params.ocr_version, \"det\", det_lang)\n        params.det_model_dir, det_url = confirm_model_dir_url(\n            params.det_model_dir,\n            os.path.join(BASE_DIR, \"whl\", \"det\", det_lang),\n            det_model_config[\"url\"],\n        )\n        rec_model_config = get_model_config(\"OCR\", params.ocr_version, \"rec\", lang)\n        params.rec_model_dir, rec_url = confirm_model_dir_url(\n            params.rec_model_dir,\n            os.path.join(BASE_DIR, \"whl\", \"rec\", lang),\n            rec_model_config[\"url\"],\n        )\n        cls_model_config = get_model_config(\"OCR\", params.ocr_version, \"cls\", \"ch\")\n        params.cls_model_dir, cls_url = confirm_model_dir_url(\n            params.cls_model_dir,\n            os.path.join(BASE_DIR, \"whl\", \"cls\"),\n            cls_model_config[\"url\"],\n        )\n        if params.ocr_version in [\"PP-OCRv3\", \"PP-OCRv4\"]:\n            params.rec_image_shape = \"3, 48, 320\"\n        else:\n            params.rec_image_shape = \"3, 32, 320\"\n        # download model if using paddle infer\n        if not params.use_onnx:\n            maybe_download(params.det_model_dir, det_url)\n            maybe_download(params.rec_model_dir, rec_url)\n            maybe_download(params.cls_model_dir, cls_url)\n\n        if params.det_algorithm not in SUPPORT_DET_MODEL:\n            logger.error(\"det_algorithm must in {}\".format(SUPPORT_DET_MODEL))\n            sys.exit(0)\n        if params.rec_algorithm not in SUPPORT_REC_MODEL:\n            logger.error(\"rec_algorithm must in {}\".format(SUPPORT_REC_MODEL))\n            sys.exit(0)\n\n        if params.rec_char_dict_path is None:\n            params.rec_char_dict_path = str(\n                Path(__file__).parent / rec_model_config[\"dict_path\"]\n            )\n\n        logger.debug(params)\n        # init det_model and rec_model\n        super().__init__(params)\n        self.page_num = params.page_num\n\n    def ocr(\n        self,\n        img,\n        det=True,\n        rec=True,\n        cls=True,\n        bin=False,\n        inv=False,\n        alpha_color=(255, 255, 255),\n        slice={},\n    ):\n        \"\"\"\n        OCR with PaddleOCR\n\n        args:\n            img: img for OCR, support ndarray, img_path and list or ndarray\n            det: use text detection or not. If False, only rec will be exec. Default is True\n            rec: use text recognition or not. If False, only det will be exec. Default is True\n            cls: use angle classifier or not. Default is True. If True, the text with rotation of 180 degrees can be recognized. If no text is rotated by 180 degrees, use cls=False to get better performance. Text with rotation of 90 or 270 degrees can be recognized even if cls=False.\n            bin: binarize image to black and white. Default is False.\n            inv: invert image colors. Default is False.\n            alpha_color: set RGB color Tuple for transparent parts replacement. Default is pure white.\n            slice: use sliding window inference for large images, det and rec must be True. Requires int values for slice[\"horizontal_stride\"], slice[\"vertical_stride\"], slice[\"merge_x_thres\"], slice[\"merge_y_thres] (See doc/doc_en/slice_en.md). Default is {}.\n        \"\"\"\n        assert isinstance(img, (np.ndarray, list, str, bytes))\n        if isinstance(img, list) and det == True:\n            logger.error(\"When input a list of images, det must be false\")\n            exit(0)\n        if cls == True and self.use_angle_cls == False:\n            logger.warning(\n                \"Since the angle classifier is not initialized, it will not be used during the forward process\"\n            )\n\n        img, flag_gif, flag_pdf = check_img(img, alpha_color)\n        # for infer pdf file\n        if isinstance(img, list) and flag_pdf:\n            if self.page_num > len(img) or self.page_num == 0:\n                imgs = img\n            else:\n                imgs = img[: self.page_num]\n        else:\n            imgs = [img]\n\n        def preprocess_image(_image):\n            _image = alpha_to_color(_image, alpha_color)\n            if inv:\n                _image = cv2.bitwise_not(_image)\n            if bin:\n                _image = binarize_img(_image)\n            return _image\n\n        if det and rec:\n            ocr_res = []\n            for idx, img in enumerate(imgs):\n                img = preprocess_image(img)\n                dt_boxes, rec_res, _ = self.__call__(img, cls, slice)\n                if not dt_boxes and not rec_res:\n                    ocr_res.append(None)\n                    continue\n                tmp_res = [[box.tolist(), res] for box, res in zip(dt_boxes, rec_res)]\n                ocr_res.append(tmp_res)\n            return ocr_res\n        elif det and not rec:\n            ocr_res = []\n            for idx, img in enumerate(imgs):\n                img = preprocess_image(img)\n                dt_boxes, elapse = self.text_detector(img)\n                if dt_boxes.size == 0:\n                    ocr_res.append(None)\n                    continue\n                tmp_res = [box.tolist() for box in dt_boxes]\n                ocr_res.append(tmp_res)\n            return ocr_res\n        else:\n            ocr_res = []\n            cls_res = []\n            for idx, img in enumerate(imgs):\n                if not isinstance(img, list):\n                    img = preprocess_image(img)\n                    img = [img]\n                if self.use_angle_cls and cls:\n                    img, cls_res_tmp, elapse = self.text_classifier(img)\n                    if not rec:\n                        cls_res.append(cls_res_tmp)\n                rec_res, elapse = self.text_recognizer(img)\n                ocr_res.append(rec_res)\n            if not rec:\n                return cls_res\n            return ocr_res\n\n\nclass PPStructure(StructureSystem):\n    def __init__(self, **kwargs):\n        params = parse_args(mMain=False)\n        params.__dict__.update(**kwargs)\n        assert (\n            params.structure_version in SUPPORT_STRUCTURE_MODEL_VERSION\n        ), \"structure_version must in {}, but get {}\".format(\n            SUPPORT_STRUCTURE_MODEL_VERSION, params.structure_version\n        )\n        params.use_gpu = check_gpu(params.use_gpu)\n        params.mode = \"structure\"\n\n        if not params.show_log:\n            logger.setLevel(logging.INFO)\n        lang, det_lang = parse_lang(params.lang)\n        if lang == \"ch\":\n            table_lang = \"ch\"\n        else:\n            table_lang = \"en\"\n        if params.structure_version == \"PP-Structure\":\n            params.merge_no_span_structure = False\n\n        # init model dir\n        det_model_config = get_model_config(\"OCR\", params.ocr_version, \"det\", det_lang)\n        params.det_model_dir, det_url = confirm_model_dir_url(\n            params.det_model_dir,\n            os.path.join(BASE_DIR, \"whl\", \"det\", det_lang),\n            det_model_config[\"url\"],\n        )\n        rec_model_config = get_model_config(\"OCR\", params.ocr_version, \"rec\", lang)\n        params.rec_model_dir, rec_url = confirm_model_dir_url(\n            params.rec_model_dir,\n            os.path.join(BASE_DIR, \"whl\", \"rec\", lang),\n            rec_model_config[\"url\"],\n        )\n        table_model_config = get_model_config(\n            \"STRUCTURE\", params.structure_version, \"table\", table_lang\n        )\n        params.table_model_dir, table_url = confirm_model_dir_url(\n            params.table_model_dir,\n            os.path.join(BASE_DIR, \"whl\", \"table\"),\n            table_model_config[\"url\"],\n        )\n        layout_model_config = get_model_config(\n            \"STRUCTURE\", params.structure_version, \"layout\", lang\n        )\n        params.layout_model_dir, layout_url = confirm_model_dir_url(\n            params.layout_model_dir,\n            os.path.join(BASE_DIR, \"whl\", \"layout\"),\n            layout_model_config[\"url\"],\n        )\n        # download model\n        if not params.use_onnx:\n            maybe_download(params.det_model_dir, det_url)\n            maybe_download(params.rec_model_dir, rec_url)\n            maybe_download(params.table_model_dir, table_url)\n            maybe_download(params.layout_model_dir, layout_url)\n\n        if params.rec_char_dict_path is None:\n            params.rec_char_dict_path = str(\n                Path(__file__).parent / rec_model_config[\"dict_path\"]\n            )\n        if params.table_char_dict_path is None:\n            params.table_char_dict_path = str(\n                Path(__file__).parent / table_model_config[\"dict_path\"]\n            )\n        if params.layout_dict_path is None:\n            params.layout_dict_path = str(\n                Path(__file__).parent / layout_model_config[\"dict_path\"]\n            )\n        logger.debug(params)\n        super().__init__(params)\n\n    def __call__(\n        self,\n        img,\n        return_ocr_result_in_table=False,\n        img_idx=0,\n        alpha_color=(255, 255, 255),\n    ):\n        img, flag_gif, flag_pdf = check_img(img, alpha_color)\n        if isinstance(img, list) and flag_pdf:\n            res_list = []\n            for index, pdf_img in enumerate(img):\n                logger.info(\"processing {}/{} page:\".format(index + 1, len(img)))\n                res, _ = super().__call__(\n                    pdf_img, return_ocr_result_in_table, img_idx=index\n                )\n                res_list.append(res)\n            return res_list\n        res, _ = super().__call__(img, return_ocr_result_in_table, img_idx=img_idx)\n        return res\n\n\ndef main():\n    # for cmd\n    args = parse_args(mMain=True)\n    image_dir = args.image_dir\n    if is_link(image_dir):\n        download_with_progressbar(image_dir, \"tmp.jpg\")\n        image_file_list = [\"tmp.jpg\"]\n    else:\n        image_file_list = get_image_file_list(args.image_dir)\n    if len(image_file_list) == 0:\n        logger.error(\"no images find in {}\".format(args.image_dir))\n        return\n    if args.type == \"ocr\":\n        engine = PaddleOCR(**(args.__dict__))\n    elif args.type == \"structure\":\n        engine = PPStructure(**(args.__dict__))\n    else:\n        raise NotImplementedError\n\n    for img_path in image_file_list:\n        img_name = os.path.basename(img_path).split(\".\")[0]\n        logger.info(\"{}{}{}\".format(\"*\" * 10, img_path, \"*\" * 10))\n        if args.type == \"ocr\":\n            result = engine.ocr(\n                img_path,\n                det=args.det,\n                rec=args.rec,\n                cls=args.use_angle_cls,\n                bin=args.binarize,\n                inv=args.invert,\n                alpha_color=args.alphacolor,\n            )\n            if result is not None:\n                lines = []\n                for res in result:\n                    for line in res:\n                        logger.info(line)\n                        lines.append(pprint.pformat(line) + \"\\n\")\n                if args.savefile:\n                    if os.path.exists(args.output) is False:\n                        os.mkdir(args.output)\n                    outfile = args.output + \"/\" + img_name + \".txt\"\n                    with open(outfile, \"w\", encoding=\"utf-8\") as f:\n                        f.writelines(lines)\n\n        elif args.type == \"structure\":\n            img, flag_gif, flag_pdf = check_and_read(img_path)\n            if not flag_gif and not flag_pdf:\n                img = cv2.imread(img_path)\n\n            if args.recovery and args.use_pdf2docx_api and flag_pdf:\n                try_import(\"pdf2docx\")\n                from pdf2docx.converter import Converter\n\n                docx_file = os.path.join(args.output, \"{}.docx\".format(img_name))\n                cv = Converter(img_path)\n                cv.convert(docx_file)\n                cv.close()\n                logger.info(\"docx save to {}\".format(docx_file))\n                continue\n\n            if not flag_pdf:\n                if img is None:\n                    logger.error(\"error in loading image:{}\".format(img_path))\n                    continue\n                img_paths = [[img_path, img]]\n            else:\n                img_paths = []\n                for index, pdf_img in enumerate(img):\n                    os.makedirs(os.path.join(args.output, img_name), exist_ok=True)\n                    pdf_img_path = os.path.join(\n                        args.output, img_name, img_name + \"_\" + str(index) + \".jpg\"\n                    )\n                    cv2.imwrite(pdf_img_path, pdf_img)\n                    img_paths.append([pdf_img_path, pdf_img])\n\n            all_res = []\n            for index, (new_img_path, img) in enumerate(img_paths):\n                logger.info(\"processing {}/{} page:\".format(index + 1, len(img_paths)))\n                result = engine(img, img_idx=index)\n                save_structure_res(result, args.output, img_name, index)\n\n                if args.recovery and result != []:\n                    from copy import deepcopy\n                    from ppstructure.recovery.recovery_to_doc import sorted_layout_boxes\n\n                    h, w, _ = img.shape\n                    result_cp = deepcopy(result)\n                    result_sorted = sorted_layout_boxes(result_cp, w)\n                    all_res += result_sorted\n\n            if args.recovery and all_res != []:\n                try:\n                    from ppstructure.recovery.recovery_to_doc import convert_info_docx\n\n                    convert_info_docx(img, all_res, args.output, img_name)\n                except Exception as ex:\n                    logger.error(\n                        \"error in layout recovery image:{}, err msg: {}\".format(\n                            img_name, ex\n                        )\n                    )\n                    continue\n\n            for item in all_res:\n                item.pop(\"img\")\n                item.pop(\"res\")\n                logger.info(item)\n            logger.info(\"result save to {}\".format(args.output))\n", "ppstructure/utility.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport random\nimport ast\nimport PIL\nfrom PIL import Image, ImageDraw, ImageFont\nimport numpy as np\nfrom tools.infer.utility import (\n    draw_ocr_box_txt,\n    str2bool,\n    str2int_tuple,\n    init_args as infer_args,\n)\nimport math\n\n\ndef init_args():\n    parser = infer_args()\n\n    # params for output\n    parser.add_argument(\"--output\", type=str, default=\"./output\")\n    # params for table structure\n    parser.add_argument(\"--table_max_len\", type=int, default=488)\n    parser.add_argument(\"--table_algorithm\", type=str, default=\"TableAttn\")\n    parser.add_argument(\"--table_model_dir\", type=str)\n    parser.add_argument(\"--merge_no_span_structure\", type=str2bool, default=True)\n    parser.add_argument(\n        \"--table_char_dict_path\",\n        type=str,\n        default=\"../ppocr/utils/dict/table_structure_dict_ch.txt\",\n    )\n    # params for layout\n    parser.add_argument(\"--layout_model_dir\", type=str)\n    parser.add_argument(\n        \"--layout_dict_path\",\n        type=str,\n        default=\"../ppocr/utils/dict/layout_dict/layout_publaynet_dict.txt\",\n    )\n    parser.add_argument(\n        \"--layout_score_threshold\", type=float, default=0.5, help=\"Threshold of score.\"\n    )\n    parser.add_argument(\n        \"--layout_nms_threshold\", type=float, default=0.5, help=\"Threshold of nms.\"\n    )\n    # params for kie\n    parser.add_argument(\"--kie_algorithm\", type=str, default=\"LayoutXLM\")\n    parser.add_argument(\"--ser_model_dir\", type=str)\n    parser.add_argument(\"--re_model_dir\", type=str)\n    parser.add_argument(\"--use_visual_backbone\", type=str2bool, default=True)\n    parser.add_argument(\n        \"--ser_dict_path\", type=str, default=\"../train_data/XFUND/class_list_xfun.txt\"\n    )\n    # need to be None or tb-yx\n    parser.add_argument(\"--ocr_order_method\", type=str, default=None)\n    # params for inference\n    parser.add_argument(\n        \"--mode\",\n        type=str,\n        choices=[\"structure\", \"kie\"],\n        default=\"structure\",\n        help=\"structure and kie is supported\",\n    )\n    parser.add_argument(\n        \"--image_orientation\",\n        type=bool,\n        default=False,\n        help=\"Whether to enable image orientation recognition\",\n    )\n    parser.add_argument(\n        \"--layout\",\n        type=str2bool,\n        default=True,\n        help=\"Whether to enable layout analysis\",\n    )\n    parser.add_argument(\n        \"--table\",\n        type=str2bool,\n        default=True,\n        help=\"In the forward, whether the table area uses table recognition\",\n    )\n    parser.add_argument(\n        \"--ocr\",\n        type=str2bool,\n        default=True,\n        help=\"In the forward, whether the non-table area is recognition by ocr\",\n    )\n    # param for recovery\n    parser.add_argument(\n        \"--recovery\",\n        type=str2bool,\n        default=False,\n        help=\"Whether to enable layout of recovery\",\n    )\n    parser.add_argument(\n        \"--use_pdf2docx_api\",\n        type=str2bool,\n        default=False,\n        help=\"Whether to use pdf2docx api\",\n    )\n    parser.add_argument(\n        \"--invert\",\n        type=str2bool,\n        default=False,\n        help=\"Whether to invert image before processing\",\n    )\n    parser.add_argument(\n        \"--binarize\",\n        type=str2bool,\n        default=False,\n        help=\"Whether to threshold binarize image before processing\",\n    )\n    parser.add_argument(\n        \"--alphacolor\",\n        type=str2int_tuple,\n        default=(255, 255, 255),\n        help=\"Replacement color for the alpha channel, if the latter is present; R,G,B integers\",\n    )\n\n    return parser\n\n\ndef parse_args():\n    parser = init_args()\n    return parser.parse_args()\n\n\ndef draw_structure_result(image, result, font_path):\n    if isinstance(image, np.ndarray):\n        image = Image.fromarray(image)\n    boxes, txts, scores = [], [], []\n\n    img_layout = image.copy()\n    draw_layout = ImageDraw.Draw(img_layout)\n    text_color = (255, 255, 255)\n    text_background_color = (80, 127, 255)\n    catid2color = {}\n    font_size = 15\n    font = ImageFont.truetype(font_path, font_size, encoding=\"utf-8\")\n\n    for region in result:\n        if region[\"type\"] not in catid2color:\n            box_color = (\n                random.randint(0, 255),\n                random.randint(0, 255),\n                random.randint(0, 255),\n            )\n            catid2color[region[\"type\"]] = box_color\n        else:\n            box_color = catid2color[region[\"type\"]]\n        box_layout = region[\"bbox\"]\n        draw_layout.rectangle(\n            [(box_layout[0], box_layout[1]), (box_layout[2], box_layout[3])],\n            outline=box_color,\n            width=3,\n        )\n\n        if int(PIL.__version__.split(\".\")[0]) < 10:\n            text_w, text_h = font.getsize(region[\"type\"])\n        else:\n            left, top, right, bottom = font.getbbox(region[\"type\"])\n            text_w, text_h = right - left, bottom - top\n\n        draw_layout.rectangle(\n            [\n                (box_layout[0], box_layout[1]),\n                (box_layout[0] + text_w, box_layout[1] + text_h),\n            ],\n            fill=text_background_color,\n        )\n        draw_layout.text(\n            (box_layout[0], box_layout[1]), region[\"type\"], fill=text_color, font=font\n        )\n\n        if region[\"type\"] == \"table\":\n            pass\n        else:\n            for text_result in region[\"res\"]:\n                boxes.append(np.array(text_result[\"text_region\"]))\n                txts.append(text_result[\"text\"])\n                scores.append(text_result[\"confidence\"])\n\n                if \"text_word_region\" in text_result:\n                    for word_region in text_result[\"text_word_region\"]:\n                        char_box = word_region\n                        box_height = int(\n                            math.sqrt(\n                                (char_box[0][0] - char_box[3][0]) ** 2\n                                + (char_box[0][1] - char_box[3][1]) ** 2\n                            )\n                        )\n                        box_width = int(\n                            math.sqrt(\n                                (char_box[0][0] - char_box[1][0]) ** 2\n                                + (char_box[0][1] - char_box[1][1]) ** 2\n                            )\n                        )\n                        if box_height == 0 or box_width == 0:\n                            continue\n                        boxes.append(word_region)\n                        txts.append(\"\")\n                        scores.append(1.0)\n\n    im_show = draw_ocr_box_txt(\n        img_layout, boxes, txts, scores, font_path=font_path, drop_score=0\n    )\n    return im_show\n\n\ndef cal_ocr_word_box(rec_str, box, rec_word_info):\n    \"\"\"Calculate the detection frame for each word based on the results of recognition and detection of ocr\"\"\"\n\n    col_num, word_list, word_col_list, state_list = rec_word_info\n    box = box.tolist()\n    bbox_x_start = box[0][0]\n    bbox_x_end = box[1][0]\n    bbox_y_start = box[0][1]\n    bbox_y_end = box[2][1]\n\n    cell_width = (bbox_x_end - bbox_x_start) / col_num\n\n    word_box_list = []\n    word_box_content_list = []\n    cn_width_list = []\n    cn_col_list = []\n    for word, word_col, state in zip(word_list, word_col_list, state_list):\n        if state == \"cn\":\n            if len(word_col) != 1:\n                char_seq_length = (word_col[-1] - word_col[0] + 1) * cell_width\n                char_width = char_seq_length / (len(word_col) - 1)\n                cn_width_list.append(char_width)\n            cn_col_list += word_col\n            word_box_content_list += word\n        else:\n            cell_x_start = bbox_x_start + int(word_col[0] * cell_width)\n            cell_x_end = bbox_x_start + int((word_col[-1] + 1) * cell_width)\n            cell = (\n                (cell_x_start, bbox_y_start),\n                (cell_x_end, bbox_y_start),\n                (cell_x_end, bbox_y_end),\n                (cell_x_start, bbox_y_end),\n            )\n            word_box_list.append(cell)\n            word_box_content_list.append(\"\".join(word))\n    if len(cn_col_list) != 0:\n        if len(cn_width_list) != 0:\n            avg_char_width = np.mean(cn_width_list)\n        else:\n            avg_char_width = (bbox_x_end - bbox_x_start) / len(rec_str)\n        for center_idx in cn_col_list:\n            center_x = (center_idx + 0.5) * cell_width\n            cell_x_start = max(int(center_x - avg_char_width / 2), 0) + bbox_x_start\n            cell_x_end = (\n                min(int(center_x + avg_char_width / 2), bbox_x_end - bbox_x_start)\n                + bbox_x_start\n            )\n            cell = (\n                (cell_x_start, bbox_y_start),\n                (cell_x_end, bbox_y_start),\n                (cell_x_end, bbox_y_end),\n                (cell_x_start, bbox_y_end),\n            )\n            word_box_list.append(cell)\n\n    return word_box_content_list, word_box_list\n", "ppstructure/predict_system.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport sys\nimport subprocess\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(__dir__)\nsys.path.insert(0, os.path.abspath(os.path.join(__dir__, \"../\")))\n\nos.environ[\"FLAGS_allocator_strategy\"] = \"auto_growth\"\nimport cv2\nimport json\nimport numpy as np\nimport time\nimport logging\nfrom copy import deepcopy\n\nfrom paddle.utils import try_import\nfrom ppocr.utils.utility import get_image_file_list, check_and_read\nfrom ppocr.utils.logging import get_logger\nfrom ppocr.utils.visual import draw_ser_results, draw_re_results\nfrom tools.infer.predict_system import TextSystem\nfrom ppstructure.layout.predict_layout import LayoutPredictor\nfrom ppstructure.table.predict_table import TableSystem, to_excel\nfrom ppstructure.utility import parse_args, draw_structure_result, cal_ocr_word_box\n\nlogger = get_logger()\n\n\nclass StructureSystem(object):\n    def __init__(self, args):\n        self.mode = args.mode\n        self.recovery = args.recovery\n\n        self.image_orientation_predictor = None\n        if args.image_orientation:\n            import paddleclas\n\n            self.image_orientation_predictor = paddleclas.PaddleClas(\n                model_name=\"text_image_orientation\"\n            )\n\n        if self.mode == \"structure\":\n            if not args.show_log:\n                logger.setLevel(logging.INFO)\n            if args.layout == False and args.ocr == True:\n                args.ocr = False\n                logger.warning(\n                    \"When args.layout is false, args.ocr is automatically set to false\"\n                )\n            # init model\n            self.layout_predictor = None\n            self.text_system = None\n            self.table_system = None\n            if args.layout:\n                self.layout_predictor = LayoutPredictor(args)\n                if args.ocr:\n                    self.text_system = TextSystem(args)\n            if args.table:\n                if self.text_system is not None:\n                    self.table_system = TableSystem(\n                        args,\n                        self.text_system.text_detector,\n                        self.text_system.text_recognizer,\n                    )\n                else:\n                    self.table_system = TableSystem(args)\n\n        elif self.mode == \"kie\":\n            from ppstructure.kie.predict_kie_token_ser_re import SerRePredictor\n\n            self.kie_predictor = SerRePredictor(args)\n\n        self.return_word_box = args.return_word_box\n\n    def __call__(self, img, return_ocr_result_in_table=False, img_idx=0):\n        time_dict = {\n            \"image_orientation\": 0,\n            \"layout\": 0,\n            \"table\": 0,\n            \"table_match\": 0,\n            \"det\": 0,\n            \"rec\": 0,\n            \"kie\": 0,\n            \"all\": 0,\n        }\n        start = time.time()\n\n        if self.image_orientation_predictor is not None:\n            tic = time.time()\n            cls_result = self.image_orientation_predictor.predict(input_data=img)\n            cls_res = next(cls_result)\n            angle = cls_res[0][\"label_names\"][0]\n            cv_rotate_code = {\n                \"90\": cv2.ROTATE_90_COUNTERCLOCKWISE,\n                \"180\": cv2.ROTATE_180,\n                \"270\": cv2.ROTATE_90_CLOCKWISE,\n            }\n            if angle in cv_rotate_code:\n                img = cv2.rotate(img, cv_rotate_code[angle])\n            toc = time.time()\n            time_dict[\"image_orientation\"] = toc - tic\n\n        if self.mode == \"structure\":\n            ori_im = img.copy()\n            if self.layout_predictor is not None:\n                layout_res, elapse = self.layout_predictor(img)\n                time_dict[\"layout\"] += elapse\n            else:\n                h, w = ori_im.shape[:2]\n                layout_res = [dict(bbox=None, label=\"table\")]\n\n            # As reported in issues such as #10270 and #11665, the old\n            # implementation, which recognizes texts from the layout regions,\n            # has problems with OCR recognition accuracy.\n            #\n            # To enhance the OCR recognition accuracy, we implement a patch fix\n            # that first use text_system to detect and recognize all text information\n            # and then filter out relevant texts according to the layout regions.\n            text_res = None\n            if self.text_system is not None:\n                text_res, ocr_time_dict = self._predict_text(img)\n                time_dict[\"det\"] += ocr_time_dict[\"det\"]\n                time_dict[\"rec\"] += ocr_time_dict[\"rec\"]\n\n            res_list = []\n            for region in layout_res:\n                res = \"\"\n                if region[\"bbox\"] is not None:\n                    x1, y1, x2, y2 = region[\"bbox\"]\n                    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n                    roi_img = ori_im[y1:y2, x1:x2, :]\n                else:\n                    x1, y1, x2, y2 = 0, 0, w, h\n                    roi_img = ori_im\n                bbox = [x1, y1, x2, y2]\n\n                if region[\"label\"] == \"table\":\n                    if self.table_system is not None:\n                        res, table_time_dict = self.table_system(\n                            roi_img, return_ocr_result_in_table\n                        )\n                        time_dict[\"table\"] += table_time_dict[\"table\"]\n                        time_dict[\"table_match\"] += table_time_dict[\"match\"]\n                        time_dict[\"det\"] += table_time_dict[\"det\"]\n                        time_dict[\"rec\"] += table_time_dict[\"rec\"]\n                else:\n                    if text_res is not None:\n                        # Filter the text results whose regions intersect with the current layout bbox.\n                        res = self._filter_text_res(text_res, bbox)\n\n                res_list.append(\n                    {\n                        \"type\": region[\"label\"].lower(),\n                        \"bbox\": bbox,\n                        \"img\": roi_img,\n                        \"res\": res,\n                        \"img_idx\": img_idx,\n                        \"score\": region[\"score\"],\n                    }\n                )\n\n            end = time.time()\n            time_dict[\"all\"] = end - start\n            return res_list, time_dict\n\n        elif self.mode == \"kie\":\n            re_res, elapse = self.kie_predictor(img)\n            time_dict[\"kie\"] = elapse\n            time_dict[\"all\"] = elapse\n            return re_res[0], time_dict\n\n        return None, None\n\n    def _predict_text(self, img):\n        filter_boxes, filter_rec_res, ocr_time_dict = self.text_system(img)\n\n        # remove style char,\n        # when using the recognition model trained on the PubtabNet dataset,\n        # it will recognize the text format in the table, such as <b>\n        style_token = [\n            \"<strike>\",\n            \"<strike>\",\n            \"<sup>\",\n            \"</sub>\",\n            \"<b>\",\n            \"</b>\",\n            \"<sub>\",\n            \"</sup>\",\n            \"<overline>\",\n            \"</overline>\",\n            \"<underline>\",\n            \"</underline>\",\n            \"<i>\",\n            \"</i>\",\n        ]\n        res = []\n        for box, rec_res in zip(filter_boxes, filter_rec_res):\n            rec_str, rec_conf = rec_res[0], rec_res[1]\n            for token in style_token:\n                if token in rec_str:\n                    rec_str = rec_str.replace(token, \"\")\n            if self.return_word_box:\n                word_box_content_list, word_box_list = cal_ocr_word_box(\n                    rec_str, box, rec_res[2]\n                )\n                res.append(\n                    {\n                        \"text\": rec_str,\n                        \"confidence\": float(rec_conf),\n                        \"text_region\": box.tolist(),\n                        \"text_word\": word_box_content_list,\n                        \"text_word_region\": word_box_list,\n                    }\n                )\n            else:\n                res.append(\n                    {\n                        \"text\": rec_str,\n                        \"confidence\": float(rec_conf),\n                        \"text_region\": box.tolist(),\n                    }\n                )\n        return res, ocr_time_dict\n\n    def _filter_text_res(self, text_res, bbox):\n        res = []\n        for r in text_res:\n            box = r[\"text_region\"]\n            rect = box[0][0], box[0][1], box[2][0], box[2][1]\n            if self._has_intersection(bbox, rect):\n                res.append(r)\n        return res\n\n    def _has_intersection(self, rect1, rect2):\n        x_min1, y_min1, x_max1, y_max1 = rect1\n        x_min2, y_min2, x_max2, y_max2 = rect2\n        if x_min1 > x_max2 or x_max1 < x_min2:\n            return False\n        if y_min1 > y_max2 or y_max1 < y_min2:\n            return False\n        return True\n\n\ndef save_structure_res(res, save_folder, img_name, img_idx=0):\n    excel_save_folder = os.path.join(save_folder, img_name)\n    os.makedirs(excel_save_folder, exist_ok=True)\n    res_cp = deepcopy(res)\n    # save res\n    with open(\n        os.path.join(excel_save_folder, \"res_{}.txt\".format(img_idx)),\n        \"w\",\n        encoding=\"utf8\",\n    ) as f:\n        for region in res_cp:\n            roi_img = region.pop(\"img\")\n            f.write(\"{}\\n\".format(json.dumps(region)))\n\n            if (\n                region[\"type\"].lower() == \"table\"\n                and len(region[\"res\"]) > 0\n                and \"html\" in region[\"res\"]\n            ):\n                excel_path = os.path.join(\n                    excel_save_folder, \"{}_{}.xlsx\".format(region[\"bbox\"], img_idx)\n                )\n                to_excel(region[\"res\"][\"html\"], excel_path)\n            elif region[\"type\"].lower() == \"figure\":\n                img_path = os.path.join(\n                    excel_save_folder, \"{}_{}.jpg\".format(region[\"bbox\"], img_idx)\n                )\n                cv2.imwrite(img_path, roi_img)\n\n\ndef main(args):\n    image_file_list = get_image_file_list(args.image_dir)\n    image_file_list = image_file_list\n    image_file_list = image_file_list[args.process_id :: args.total_process_num]\n\n    if not args.use_pdf2docx_api:\n        structure_sys = StructureSystem(args)\n        save_folder = os.path.join(args.output, structure_sys.mode)\n        os.makedirs(save_folder, exist_ok=True)\n    img_num = len(image_file_list)\n\n    for i, image_file in enumerate(image_file_list):\n        logger.info(\"[{}/{}] {}\".format(i, img_num, image_file))\n        img, flag_gif, flag_pdf = check_and_read(image_file)\n        img_name = os.path.basename(image_file).split(\".\")[0]\n\n        if args.recovery and args.use_pdf2docx_api and flag_pdf:\n            try_import(\"pdf2docx\")\n            from pdf2docx.converter import Converter\n\n            os.makedirs(args.output, exist_ok=True)\n            docx_file = os.path.join(args.output, \"{}_api.docx\".format(img_name))\n            cv = Converter(image_file)\n            cv.convert(docx_file)\n            cv.close()\n            logger.info(\"docx save to {}\".format(docx_file))\n            continue\n\n        if not flag_gif and not flag_pdf:\n            img = cv2.imread(image_file)\n\n        if not flag_pdf:\n            if img is None:\n                logger.error(\"error in loading image:{}\".format(image_file))\n                continue\n            imgs = [img]\n        else:\n            imgs = img\n\n        all_res = []\n        for index, img in enumerate(imgs):\n            res, time_dict = structure_sys(img, img_idx=index)\n            img_save_path = os.path.join(\n                save_folder, img_name, \"show_{}.jpg\".format(index)\n            )\n            os.makedirs(os.path.join(save_folder, img_name), exist_ok=True)\n            if structure_sys.mode == \"structure\" and res != []:\n                draw_img = draw_structure_result(img, res, args.vis_font_path)\n                save_structure_res(res, save_folder, img_name, index)\n            elif structure_sys.mode == \"kie\":\n                if structure_sys.kie_predictor.predictor is not None:\n                    draw_img = draw_re_results(img, res, font_path=args.vis_font_path)\n                else:\n                    draw_img = draw_ser_results(img, res, font_path=args.vis_font_path)\n\n                with open(\n                    os.path.join(save_folder, img_name, \"res_{}_kie.txt\".format(index)),\n                    \"w\",\n                    encoding=\"utf8\",\n                ) as f:\n                    res_str = \"{}\\t{}\\n\".format(\n                        image_file, json.dumps({\"ocr_info\": res}, ensure_ascii=False)\n                    )\n                    f.write(res_str)\n            if res != []:\n                cv2.imwrite(img_save_path, draw_img)\n                logger.info(\"result save to {}\".format(img_save_path))\n            if args.recovery and res != []:\n                from ppstructure.recovery.recovery_to_doc import (\n                    sorted_layout_boxes,\n                    convert_info_docx,\n                )\n\n                h, w, _ = img.shape\n                res = sorted_layout_boxes(res, w)\n                all_res += res\n\n        if args.recovery and all_res != []:\n            try:\n                convert_info_docx(img, all_res, save_folder, img_name)\n            except Exception as ex:\n                logger.error(\n                    \"error in layout recovery image:{}, err msg: {}\".format(\n                        image_file, ex\n                    )\n                )\n                continue\n        logger.info(\"Predict time : {:.3f}s\".format(time_dict[\"all\"]))\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    if args.use_mp:\n        p_list = []\n        total_process_num = args.total_process_num\n        for process_id in range(total_process_num):\n            cmd = (\n                [sys.executable, \"-u\"]\n                + sys.argv\n                + [\"--process_id={}\".format(process_id), \"--use_mp={}\".format(False)]\n            )\n            p = subprocess.Popen(cmd, stdout=sys.stdout, stderr=sys.stdout)\n            p_list.append(p)\n        for p in p_list:\n            p.wait()\n    else:\n        main(args)\n", "ppstructure/__init__.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n", "ppstructure/layout/predict_layout.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\nimport sys\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(__dir__)\nsys.path.insert(0, os.path.abspath(os.path.join(__dir__, \"../..\")))\n\nos.environ[\"FLAGS_allocator_strategy\"] = \"auto_growth\"\n\nimport cv2\nimport numpy as np\nimport time\n\nimport tools.infer.utility as utility\nfrom ppocr.data import create_operators, transform\nfrom ppocr.postprocess import build_post_process\nfrom ppocr.utils.logging import get_logger\nfrom ppocr.utils.utility import get_image_file_list, check_and_read\nfrom ppstructure.utility import parse_args\nfrom picodet_postprocess import PicoDetPostProcess\n\nlogger = get_logger()\n\n\nclass LayoutPredictor(object):\n    def __init__(self, args):\n        pre_process_list = [\n            {\"Resize\": {\"size\": [800, 608]}},\n            {\n                \"NormalizeImage\": {\n                    \"std\": [0.229, 0.224, 0.225],\n                    \"mean\": [0.485, 0.456, 0.406],\n                    \"scale\": \"1./255.\",\n                    \"order\": \"hwc\",\n                }\n            },\n            {\"ToCHWImage\": None},\n            {\"KeepKeys\": {\"keep_keys\": [\"image\"]}},\n        ]\n        postprocess_params = {\n            \"name\": \"PicoDetPostProcess\",\n            \"layout_dict_path\": args.layout_dict_path,\n            \"score_threshold\": args.layout_score_threshold,\n            \"nms_threshold\": args.layout_nms_threshold,\n        }\n\n        self.preprocess_op = create_operators(pre_process_list)\n        self.postprocess_op = build_post_process(postprocess_params)\n        (\n            self.predictor,\n            self.input_tensor,\n            self.output_tensors,\n            self.config,\n        ) = utility.create_predictor(args, \"layout\", logger)\n        self.use_onnx = args.use_onnx\n\n    def __call__(self, img):\n        ori_im = img.copy()\n        data = {\"image\": img}\n        data = transform(data, self.preprocess_op)\n        img = data[0]\n\n        if img is None:\n            return None, 0\n\n        img = np.expand_dims(img, axis=0)\n        img = img.copy()\n\n        preds, elapse = 0, 1\n        starttime = time.time()\n\n        np_score_list, np_boxes_list = [], []\n        if self.use_onnx:\n            input_dict = {}\n            input_dict[self.input_tensor.name] = img\n            outputs = self.predictor.run(self.output_tensors, input_dict)\n            num_outs = int(len(outputs) / 2)\n            for out_idx in range(num_outs):\n                np_score_list.append(outputs[out_idx])\n                np_boxes_list.append(outputs[out_idx + num_outs])\n        else:\n            self.input_tensor.copy_from_cpu(img)\n            self.predictor.run()\n            output_names = self.predictor.get_output_names()\n            num_outs = int(len(output_names) / 2)\n            for out_idx in range(num_outs):\n                np_score_list.append(\n                    self.predictor.get_output_handle(\n                        output_names[out_idx]\n                    ).copy_to_cpu()\n                )\n                np_boxes_list.append(\n                    self.predictor.get_output_handle(\n                        output_names[out_idx + num_outs]\n                    ).copy_to_cpu()\n                )\n        preds = dict(boxes=np_score_list, boxes_num=np_boxes_list)\n\n        post_preds = self.postprocess_op(ori_im, img, preds)\n        elapse = time.time() - starttime\n        return post_preds, elapse\n\n\ndef main(args):\n    image_file_list = get_image_file_list(args.image_dir)\n    layout_predictor = LayoutPredictor(args)\n    count = 0\n    total_time = 0\n\n    repeats = 50\n    for image_file in image_file_list:\n        img, flag, _ = check_and_read(image_file)\n        if not flag:\n            img = cv2.imread(image_file)\n        if img is None:\n            logger.info(\"error in loading image:{}\".format(image_file))\n            continue\n\n        layout_res, elapse = layout_predictor(img)\n\n        logger.info(\"result: {}\".format(layout_res))\n\n        if count > 0:\n            total_time += elapse\n        count += 1\n        logger.info(\"Predict time of {}: {}\".format(image_file, elapse))\n\n\nif __name__ == \"__main__\":\n    main(parse_args())\n", "ppstructure/layout/__init__.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n", "ppstructure/recovery/__init__.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n", "ppstructure/recovery/table_process.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from: https://github.com/weizwx/html2docx/blob/master/htmldocx/h2d.py\n\"\"\"\n\nimport re\nimport docx\nfrom docx import Document\nfrom bs4 import BeautifulSoup\nfrom html.parser import HTMLParser\n\n\ndef get_table_rows(table_soup):\n    table_row_selectors = [\n        \"table > tr\",\n        \"table > thead > tr\",\n        \"table > tbody > tr\",\n        \"table > tfoot > tr\",\n    ]\n    # If there's a header, body, footer or direct child tr tags, add row dimensions from there\n    return table_soup.select(\", \".join(table_row_selectors), recursive=False)\n\n\ndef get_table_columns(row):\n    # Get all columns for the specified row tag.\n    return row.find_all([\"th\", \"td\"], recursive=False) if row else []\n\n\ndef get_table_dimensions(table_soup):\n    # Get rows for the table\n    rows = get_table_rows(table_soup)\n    # Table is either empty or has non-direct children between table and tr tags\n    # Thus the row dimensions and column dimensions are assumed to be 0\n\n    cols = get_table_columns(rows[0]) if rows else []\n    # Add colspan calculation column number\n    col_count = 0\n    for col in cols:\n        colspan = col.attrs.get(\"colspan\", 1)\n        col_count += int(colspan)\n\n    return rows, col_count\n\n\ndef get_cell_html(soup):\n    # Returns string of td element with opening and closing <td> tags removed\n    # Cannot use find_all as it only finds element tags and does not find text which\n    # is not inside an element\n    return \" \".join([str(i) for i in soup.contents])\n\n\ndef delete_paragraph(paragraph):\n    # https://github.com/python-openxml/python-docx/issues/33#issuecomment-77661907\n    p = paragraph._element\n    p.getparent().remove(p)\n    p._p = p._element = None\n\n\ndef remove_whitespace(string, leading=False, trailing=False):\n    \"\"\"Remove white space from a string.\n    Args:\n        string(str): The string to remove white space from.\n        leading(bool, optional): Remove leading new lines when True.\n        trailing(bool, optional): Remove trailing new lines when False.\n    Returns:\n        str: The input string with new line characters removed and white space squashed.\n    Examples:\n        Single or multiple new line characters are replaced with space.\n            >>> remove_whitespace(\"abc\\\\ndef\")\n            'abc def'\n            >>> remove_whitespace(\"abc\\\\n\\\\n\\\\ndef\")\n            'abc def'\n        New line characters surrounded by white space are replaced with a single space.\n            >>> remove_whitespace(\"abc \\\\n \\\\n \\\\n def\")\n            'abc def'\n            >>> remove_whitespace(\"abc  \\\\n  \\\\n  \\\\n  def\")\n            'abc def'\n        Leading and trailing new lines are replaced with a single space.\n            >>> remove_whitespace(\"\\\\nabc\")\n            ' abc'\n            >>> remove_whitespace(\"  \\\\n  abc\")\n            ' abc'\n            >>> remove_whitespace(\"abc\\\\n\")\n            'abc '\n            >>> remove_whitespace(\"abc  \\\\n  \")\n            'abc '\n        Use ``leading=True`` to remove leading new line characters, including any surrounding\n        white space:\n            >>> remove_whitespace(\"\\\\nabc\", leading=True)\n            'abc'\n            >>> remove_whitespace(\"  \\\\n  abc\", leading=True)\n            'abc'\n        Use ``trailing=True`` to remove trailing new line characters, including any surrounding\n        white space:\n            >>> remove_whitespace(\"abc  \\\\n  \", trailing=True)\n            'abc'\n    \"\"\"\n    # Remove any leading new line characters along with any surrounding white space\n    if leading:\n        string = re.sub(r\"^\\s*\\n+\\s*\", \"\", string)\n\n    # Remove any trailing new line characters along with any surrounding white space\n    if trailing:\n        string = re.sub(r\"\\s*\\n+\\s*$\", \"\", string)\n\n    # Replace new line characters and absorb any surrounding space.\n    string = re.sub(r\"\\s*\\n\\s*\", \" \", string)\n    # TODO need some way to get rid of extra spaces in e.g. text <span>   </span>  text\n    return re.sub(r\"\\s+\", \" \", string)\n\n\nfont_styles = {\n    \"b\": \"bold\",\n    \"strong\": \"bold\",\n    \"em\": \"italic\",\n    \"i\": \"italic\",\n    \"u\": \"underline\",\n    \"s\": \"strike\",\n    \"sup\": \"superscript\",\n    \"sub\": \"subscript\",\n    \"th\": \"bold\",\n}\n\nfont_names = {\n    \"code\": \"Courier\",\n    \"pre\": \"Courier\",\n}\n\n\nclass HtmlToDocx(HTMLParser):\n    def __init__(self):\n        super().__init__()\n        self.options = {\n            \"fix-html\": True,\n            \"images\": True,\n            \"tables\": True,\n            \"styles\": True,\n        }\n        self.table_row_selectors = [\n            \"table > tr\",\n            \"table > thead > tr\",\n            \"table > tbody > tr\",\n            \"table > tfoot > tr\",\n        ]\n        self.table_style = None\n        self.paragraph_style = None\n\n    def set_initial_attrs(self, document=None):\n        self.tags = {\n            \"span\": [],\n            \"list\": [],\n        }\n        if document:\n            self.doc = document\n        else:\n            self.doc = Document()\n        self.bs = self.options[\"fix-html\"]  # whether or not to clean with BeautifulSoup\n        self.document = self.doc\n        self.include_tables = True  # TODO add this option back in?\n        self.include_images = self.options[\"images\"]\n        self.include_styles = self.options[\"styles\"]\n        self.paragraph = None\n        self.skip = False\n        self.skip_tag = None\n        self.instances_to_skip = 0\n\n    def copy_settings_from(self, other):\n        \"\"\"Copy settings from another instance of HtmlToDocx\"\"\"\n        self.table_style = other.table_style\n        self.paragraph_style = other.paragraph_style\n\n    def ignore_nested_tables(self, tables_soup):\n        \"\"\"\n        Returns array containing only the highest level tables\n        Operates on the assumption that bs4 returns child elements immediately after\n        the parent element in `find_all`. If this changes in the future, this method will need to be updated\n        :return:\n        \"\"\"\n        new_tables = []\n        nest = 0\n        for table in tables_soup:\n            if nest:\n                nest -= 1\n                continue\n            new_tables.append(table)\n            nest = len(table.find_all(\"table\"))\n        return new_tables\n\n    def get_tables(self):\n        if not hasattr(self, \"soup\"):\n            self.include_tables = False\n            return\n            # find other way to do it, or require this dependency?\n        self.tables = self.ignore_nested_tables(self.soup.find_all(\"table\"))\n        self.table_no = 0\n\n    def run_process(self, html):\n        if self.bs and BeautifulSoup:\n            self.soup = BeautifulSoup(html, \"html.parser\")\n            html = str(self.soup)\n        if self.include_tables:\n            self.get_tables()\n        self.feed(html)\n\n    def add_html_to_cell(self, html, cell):\n        if not isinstance(cell, docx.table._Cell):\n            raise ValueError(\"Second argument needs to be a %s\" % docx.table._Cell)\n        unwanted_paragraph = cell.paragraphs[0]\n        if unwanted_paragraph.text == \"\":\n            delete_paragraph(unwanted_paragraph)\n        self.set_initial_attrs(cell)\n        self.run_process(html)\n        # cells must end with a paragraph or will get message about corrupt file\n        # https://stackoverflow.com/a/29287121\n        if not self.doc.paragraphs:\n            self.doc.add_paragraph(\"\")\n\n    def apply_paragraph_style(self, style=None):\n        try:\n            if style:\n                self.paragraph.style = style\n            elif self.paragraph_style:\n                self.paragraph.style = self.paragraph_style\n        except KeyError as e:\n            raise ValueError(f\"Unable to apply style {self.paragraph_style}.\") from e\n\n    def handle_table(self, html, doc):\n        \"\"\"\n        To handle nested tables, we will parse tables manually as follows:\n        Get table soup\n        Create docx table\n        Iterate over soup and fill docx table with new instances of this parser\n        Tell HTMLParser to ignore any tags until the corresponding closing table tag\n        \"\"\"\n        table_soup = BeautifulSoup(html, \"html.parser\")\n        rows, cols_len = get_table_dimensions(table_soup)\n        table = doc.add_table(len(rows), cols_len)\n        table.style = doc.styles[\"Table Grid\"]\n\n        num_rows = len(table.rows)\n        num_cols = len(table.columns)\n\n        cell_row = 0\n        for index, row in enumerate(rows):\n            cols = get_table_columns(row)\n            cell_col = 0\n            for col in cols:\n                colspan = int(col.attrs.get(\"colspan\", 1))\n                rowspan = int(col.attrs.get(\"rowspan\", 1))\n\n                cell_html = get_cell_html(col)\n                if col.name == \"th\":\n                    cell_html = \"<b>%s</b>\" % cell_html\n\n                if cell_row >= num_rows or cell_col >= num_cols:\n                    continue\n\n                docx_cell = table.cell(cell_row, cell_col)\n\n                while docx_cell.text != \"\":  # Skip the merged cell\n                    cell_col += 1\n                    docx_cell = table.cell(cell_row, cell_col)\n\n                cell_to_merge = table.cell(\n                    cell_row + rowspan - 1, cell_col + colspan - 1\n                )\n                if docx_cell != cell_to_merge:\n                    docx_cell.merge(cell_to_merge)\n\n                child_parser = HtmlToDocx()\n                child_parser.copy_settings_from(self)\n                child_parser.add_html_to_cell(cell_html or \" \", docx_cell)\n\n                cell_col += colspan\n            cell_row += 1\n\n    def handle_data(self, data):\n        if self.skip:\n            return\n\n        # Only remove white space if we're not in a pre block.\n        if \"pre\" not in self.tags:\n            # remove leading and trailing whitespace in all instances\n            data = remove_whitespace(data, True, True)\n\n        if not self.paragraph:\n            self.paragraph = self.doc.add_paragraph()\n            self.apply_paragraph_style()\n\n        # There can only be one nested link in a valid html document\n        # You cannot have interactive content in an A tag, this includes links\n        # https://html.spec.whatwg.org/#interactive-content\n        link = self.tags.get(\"a\")\n        if link:\n            self.handle_link(link[\"href\"], data)\n        else:\n            # If there's a link, dont put the data directly in the run\n            self.run = self.paragraph.add_run(data)\n            spans = self.tags[\"span\"]\n            for span in spans:\n                if \"style\" in span:\n                    style = self.parse_dict_string(span[\"style\"])\n                    self.add_styles_to_run(style)\n\n            # add font style and name\n            for tag in self.tags:\n                if tag in font_styles:\n                    font_style = font_styles[tag]\n                    setattr(self.run.font, font_style, True)\n\n                if tag in font_names:\n                    font_name = font_names[tag]\n                    self.run.font.name = font_name\n", "ppstructure/table/predict_structure.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\nimport sys\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(__dir__)\nsys.path.insert(0, os.path.abspath(os.path.join(__dir__, \"../..\")))\n\nos.environ[\"FLAGS_allocator_strategy\"] = \"auto_growth\"\n\nimport cv2\nimport numpy as np\nimport time\nimport json\n\nimport tools.infer.utility as utility\nfrom ppocr.data import create_operators, transform\nfrom ppocr.postprocess import build_post_process\nfrom ppocr.utils.logging import get_logger\nfrom ppocr.utils.utility import get_image_file_list, check_and_read\nfrom ppocr.utils.visual import draw_rectangle\nfrom ppstructure.utility import parse_args\n\nlogger = get_logger()\n\n\ndef build_pre_process_list(args):\n    resize_op = {\n        \"ResizeTableImage\": {\n            \"max_len\": args.table_max_len,\n        }\n    }\n    pad_op = {\"PaddingTableImage\": {\"size\": [args.table_max_len, args.table_max_len]}}\n    normalize_op = {\n        \"NormalizeImage\": {\n            \"std\": (\n                [0.229, 0.224, 0.225]\n                if args.table_algorithm not in [\"TableMaster\"]\n                else [0.5, 0.5, 0.5]\n            ),\n            \"mean\": (\n                [0.485, 0.456, 0.406]\n                if args.table_algorithm not in [\"TableMaster\"]\n                else [0.5, 0.5, 0.5]\n            ),\n            \"scale\": \"1./255.\",\n            \"order\": \"hwc\",\n        }\n    }\n    to_chw_op = {\"ToCHWImage\": None}\n    keep_keys_op = {\"KeepKeys\": {\"keep_keys\": [\"image\", \"shape\"]}}\n    if args.table_algorithm not in [\"TableMaster\"]:\n        pre_process_list = [resize_op, normalize_op, pad_op, to_chw_op, keep_keys_op]\n    else:\n        pre_process_list = [resize_op, pad_op, normalize_op, to_chw_op, keep_keys_op]\n    return pre_process_list\n\n\nclass TableStructurer(object):\n    def __init__(self, args):\n        self.args = args\n        self.use_onnx = args.use_onnx\n        pre_process_list = build_pre_process_list(args)\n        if args.table_algorithm not in [\"TableMaster\"]:\n            postprocess_params = {\n                \"name\": \"TableLabelDecode\",\n                \"character_dict_path\": args.table_char_dict_path,\n                \"merge_no_span_structure\": args.merge_no_span_structure,\n            }\n        else:\n            postprocess_params = {\n                \"name\": \"TableMasterLabelDecode\",\n                \"character_dict_path\": args.table_char_dict_path,\n                \"box_shape\": \"pad\",\n                \"merge_no_span_structure\": args.merge_no_span_structure,\n            }\n\n        self.preprocess_op = create_operators(pre_process_list)\n        self.postprocess_op = build_post_process(postprocess_params)\n        (\n            self.predictor,\n            self.input_tensor,\n            self.output_tensors,\n            self.config,\n        ) = utility.create_predictor(args, \"table\", logger)\n\n        if args.benchmark:\n            import auto_log\n\n            pid = os.getpid()\n            gpu_id = utility.get_infer_gpuid()\n            self.autolog = auto_log.AutoLogger(\n                model_name=\"table\",\n                model_precision=args.precision,\n                batch_size=1,\n                data_shape=\"dynamic\",\n                save_path=None,  # args.save_log_path,\n                inference_config=self.config,\n                pids=pid,\n                process_name=None,\n                gpu_ids=gpu_id if args.use_gpu else None,\n                time_keys=[\"preprocess_time\", \"inference_time\", \"postprocess_time\"],\n                warmup=0,\n                logger=logger,\n            )\n\n    def __call__(self, img):\n        starttime = time.time()\n        if self.args.benchmark:\n            self.autolog.times.start()\n\n        ori_im = img.copy()\n        data = {\"image\": img}\n        data = transform(data, self.preprocess_op)\n        img = data[0]\n        if img is None:\n            return None, 0\n        img = np.expand_dims(img, axis=0)\n        img = img.copy()\n        if self.args.benchmark:\n            self.autolog.times.stamp()\n        if self.use_onnx:\n            input_dict = {}\n            input_dict[self.input_tensor.name] = img\n            outputs = self.predictor.run(self.output_tensors, input_dict)\n        else:\n            self.input_tensor.copy_from_cpu(img)\n            self.predictor.run()\n            outputs = []\n            for output_tensor in self.output_tensors:\n                output = output_tensor.copy_to_cpu()\n                outputs.append(output)\n            if self.args.benchmark:\n                self.autolog.times.stamp()\n\n        preds = {}\n        preds[\"structure_probs\"] = outputs[1]\n        preds[\"loc_preds\"] = outputs[0]\n\n        shape_list = np.expand_dims(data[-1], axis=0)\n        post_result = self.postprocess_op(preds, [shape_list])\n\n        structure_str_list = post_result[\"structure_batch_list\"][0]\n        bbox_list = post_result[\"bbox_batch_list\"][0]\n        structure_str_list = structure_str_list[0]\n        structure_str_list = (\n            [\"<html>\", \"<body>\", \"<table>\"]\n            + structure_str_list\n            + [\"</table>\", \"</body>\", \"</html>\"]\n        )\n        elapse = time.time() - starttime\n        if self.args.benchmark:\n            self.autolog.times.end(stamp=True)\n        return (structure_str_list, bbox_list), elapse\n\n\ndef main(args):\n    image_file_list = get_image_file_list(args.image_dir)\n    table_structurer = TableStructurer(args)\n    count = 0\n    total_time = 0\n    os.makedirs(args.output, exist_ok=True)\n    with open(\n        os.path.join(args.output, \"infer.txt\"), mode=\"w\", encoding=\"utf-8\"\n    ) as f_w:\n        for image_file in image_file_list:\n            img, flag, _ = check_and_read(image_file)\n            if not flag:\n                img = cv2.imread(image_file)\n            if img is None:\n                logger.info(\"error in loading image:{}\".format(image_file))\n                continue\n            structure_res, elapse = table_structurer(img)\n            structure_str_list, bbox_list = structure_res\n            bbox_list_str = json.dumps(bbox_list.tolist())\n            logger.info(\"result: {}, {}\".format(structure_str_list, bbox_list_str))\n            f_w.write(\"result: {}, {}\\n\".format(structure_str_list, bbox_list_str))\n\n            if len(bbox_list) > 0 and len(bbox_list[0]) == 4:\n                img = draw_rectangle(image_file, bbox_list)\n            else:\n                img = utility.draw_boxes(img, bbox_list)\n            img_save_path = os.path.join(args.output, os.path.basename(image_file))\n            cv2.imwrite(img_save_path, img)\n            logger.info(\"save vis result to {}\".format(img_save_path))\n            if count > 0:\n                total_time += elapse\n            count += 1\n            logger.info(\"Predict time of {}: {}\".format(image_file, elapse))\n    if args.benchmark:\n        table_structurer.autolog.report()\n\n\nif __name__ == \"__main__\":\n    main(parse_args())\n", "ppstructure/table/eval_table.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport sys\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(__dir__)\nsys.path.insert(0, os.path.abspath(os.path.join(__dir__, \"../..\")))\n\nimport cv2\nimport pickle\nimport paddle\nfrom tqdm import tqdm\nfrom ppstructure.table.table_metric import TEDS\nfrom ppstructure.table.predict_table import TableSystem\nfrom ppstructure.utility import init_args\nfrom ppocr.utils.logging import get_logger\n\nlogger = get_logger()\n\n\ndef parse_args():\n    parser = init_args()\n    parser.add_argument(\"--gt_path\", type=str)\n    return parser.parse_args()\n\n\ndef load_txt(txt_path):\n    pred_html_dict = {}\n    if not os.path.exists(txt_path):\n        return pred_html_dict\n    with open(txt_path, encoding=\"utf-8\") as f:\n        lines = f.readlines()\n        for line in lines:\n            line = line.strip().split(\"\\t\")\n            img_name, pred_html = line\n            pred_html_dict[img_name] = pred_html\n    return pred_html_dict\n\n\ndef load_result(path):\n    data = {}\n    if os.path.exists(path):\n        data = pickle.load(open(path, \"rb\"))\n    return data\n\n\ndef save_result(path, data):\n    old_data = load_result(path)\n    old_data.update(data)\n    with open(path, \"wb\") as f:\n        pickle.dump(old_data, f)\n\n\ndef main(gt_path, img_root, args):\n    os.makedirs(args.output, exist_ok=True)\n    # init TableSystem\n    text_sys = TableSystem(args)\n    # load gt and preds html result\n    gt_html_dict = load_txt(gt_path)\n\n    ocr_result = load_result(os.path.join(args.output, \"ocr.pickle\"))\n    structure_result = load_result(os.path.join(args.output, \"structure.pickle\"))\n\n    pred_htmls = []\n    gt_htmls = []\n    for img_name, gt_html in tqdm(gt_html_dict.items()):\n        img = cv2.imread(os.path.join(img_root, img_name))\n        # run ocr and save result\n        if img_name not in ocr_result:\n            dt_boxes, rec_res, _, _ = text_sys._ocr(img)\n            ocr_result[img_name] = [dt_boxes, rec_res]\n            save_result(os.path.join(args.output, \"ocr.pickle\"), ocr_result)\n        # run structure and save result\n        if img_name not in structure_result:\n            structure_res, _ = text_sys._structure(img)\n            structure_result[img_name] = structure_res\n            save_result(os.path.join(args.output, \"structure.pickle\"), structure_result)\n        dt_boxes, rec_res = ocr_result[img_name]\n        structure_res = structure_result[img_name]\n        # match ocr and structure\n        pred_html = text_sys.match(structure_res, dt_boxes, rec_res)\n\n        pred_htmls.append(pred_html)\n        gt_htmls.append(gt_html)\n\n    # compute teds\n    teds = TEDS(n_jobs=16)\n    scores = teds.batch_evaluate_html(gt_htmls, pred_htmls)\n    logger.info(\"teds: {}\".format(sum(scores) / len(scores)))\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    main(args.gt_path, args.image_dir, args)\n", "ppstructure/table/table_master_match.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/JiaquanYe/TableMASTER-mmocr/blob/master/table_recognition/match.py\n\"\"\"\n\nimport os\nimport re\nimport cv2\nimport glob\nimport copy\nimport math\nimport pickle\nimport numpy as np\n\nfrom shapely.geometry import Polygon, MultiPoint\n\n\"\"\"\nUseful function in matching.\n\"\"\"\n\n\ndef remove_empty_bboxes(bboxes):\n    \"\"\"\n    remove [0., 0., 0., 0.] in structure master bboxes.\n    len(bboxes.shape) must be 2.\n    :param bboxes:\n    :return:\n    \"\"\"\n    new_bboxes = []\n    for bbox in bboxes:\n        if sum(bbox) == 0.0:\n            continue\n        new_bboxes.append(bbox)\n    return np.array(new_bboxes)\n\n\ndef xywh2xyxy(bboxes):\n    if len(bboxes.shape) == 1:\n        new_bboxes = np.empty_like(bboxes)\n        new_bboxes[0] = bboxes[0] - bboxes[2] / 2\n        new_bboxes[1] = bboxes[1] - bboxes[3] / 2\n        new_bboxes[2] = bboxes[0] + bboxes[2] / 2\n        new_bboxes[3] = bboxes[1] + bboxes[3] / 2\n        return new_bboxes\n    elif len(bboxes.shape) == 2:\n        new_bboxes = np.empty_like(bboxes)\n        new_bboxes[:, 0] = bboxes[:, 0] - bboxes[:, 2] / 2\n        new_bboxes[:, 1] = bboxes[:, 1] - bboxes[:, 3] / 2\n        new_bboxes[:, 2] = bboxes[:, 0] + bboxes[:, 2] / 2\n        new_bboxes[:, 3] = bboxes[:, 1] + bboxes[:, 3] / 2\n        return new_bboxes\n    else:\n        raise ValueError\n\n\ndef xyxy2xywh(bboxes):\n    if len(bboxes.shape) == 1:\n        new_bboxes = np.empty_like(bboxes)\n        new_bboxes[0] = bboxes[0] + (bboxes[2] - bboxes[0]) / 2\n        new_bboxes[1] = bboxes[1] + (bboxes[3] - bboxes[1]) / 2\n        new_bboxes[2] = bboxes[2] - bboxes[0]\n        new_bboxes[3] = bboxes[3] - bboxes[1]\n        return new_bboxes\n    elif len(bboxes.shape) == 2:\n        new_bboxes = np.empty_like(bboxes)\n        new_bboxes[:, 0] = bboxes[:, 0] + (bboxes[:, 2] - bboxes[:, 0]) / 2\n        new_bboxes[:, 1] = bboxes[:, 1] + (bboxes[:, 3] - bboxes[:, 1]) / 2\n        new_bboxes[:, 2] = bboxes[:, 2] - bboxes[:, 0]\n        new_bboxes[:, 3] = bboxes[:, 3] - bboxes[:, 1]\n        return new_bboxes\n    else:\n        raise ValueError\n\n\ndef pickle_load(path, prefix=\"end2end\"):\n    if os.path.isfile(path):\n        data = pickle.load(open(path, \"rb\"))\n    elif os.path.isdir(path):\n        data = dict()\n        search_path = os.path.join(path, \"{}_*.pkl\".format(prefix))\n        pkls = glob.glob(search_path)\n        for pkl in pkls:\n            this_data = pickle.load(open(pkl, \"rb\"))\n            data.update(this_data)\n    else:\n        raise ValueError\n    return data\n\n\ndef convert_coord(xyxy):\n    \"\"\"\n    Convert two points format to four points format.\n    :param xyxy:\n    :return:\n    \"\"\"\n    new_bbox = np.zeros([4, 2], dtype=np.float32)\n    new_bbox[0, 0], new_bbox[0, 1] = xyxy[0], xyxy[1]\n    new_bbox[1, 0], new_bbox[1, 1] = xyxy[2], xyxy[1]\n    new_bbox[2, 0], new_bbox[2, 1] = xyxy[2], xyxy[3]\n    new_bbox[3, 0], new_bbox[3, 1] = xyxy[0], xyxy[3]\n    return new_bbox\n\n\ndef cal_iou(bbox1, bbox2):\n    bbox1_poly = Polygon(bbox1).convex_hull\n    bbox2_poly = Polygon(bbox2).convex_hull\n    union_poly = np.concatenate((bbox1, bbox2))\n\n    if not bbox1_poly.intersects(bbox2_poly):\n        iou = 0\n    else:\n        inter_area = bbox1_poly.intersection(bbox2_poly).area\n        union_area = MultiPoint(union_poly).convex_hull.area\n        if union_area == 0:\n            iou = 0\n        else:\n            iou = float(inter_area) / union_area\n    return iou\n\n\ndef cal_distance(p1, p2):\n    delta_x = p1[0] - p2[0]\n    delta_y = p1[1] - p2[1]\n    d = math.sqrt((delta_x**2) + (delta_y**2))\n    return d\n\n\ndef is_inside(center_point, corner_point):\n    \"\"\"\n    Find if center_point inside the bbox(corner_point) or not.\n    :param center_point: center point (x, y)\n    :param corner_point: corner point ((x1,y1),(x2,y2))\n    :return:\n    \"\"\"\n    x_flag = False\n    y_flag = False\n    if (center_point[0] >= corner_point[0][0]) and (\n        center_point[0] <= corner_point[1][0]\n    ):\n        x_flag = True\n    if (center_point[1] >= corner_point[0][1]) and (\n        center_point[1] <= corner_point[1][1]\n    ):\n        y_flag = True\n    if x_flag and y_flag:\n        return True\n    else:\n        return False\n\n\ndef find_no_match(match_list, all_end2end_nums, type=\"end2end\"):\n    \"\"\"\n    Find out no match end2end bbox in previous match list.\n    :param match_list: matching pairs.\n    :param all_end2end_nums: numbers of end2end_xywh\n    :param type: 'end2end' corresponding to idx 0, 'master' corresponding to idx 1.\n    :return: no match pse bbox index list\n    \"\"\"\n    if type == \"end2end\":\n        idx = 0\n    elif type == \"master\":\n        idx = 1\n    else:\n        raise ValueError\n\n    no_match_indexs = []\n    # m[0] is end2end index m[1] is master index\n    matched_bbox_indexs = [m[idx] for m in match_list]\n    for n in range(all_end2end_nums):\n        if n not in matched_bbox_indexs:\n            no_match_indexs.append(n)\n    return no_match_indexs\n\n\ndef is_abs_lower_than_threshold(this_bbox, target_bbox, threshold=3):\n    # only consider y axis, for grouping in row.\n    delta = abs(this_bbox[1] - target_bbox[1])\n    if delta < threshold:\n        return True\n    else:\n        return False\n\n\ndef sort_line_bbox(g, bg):\n    \"\"\"\n    Sorted the bbox in the same line(group)\n    compare coord 'x' value, where 'y' value is closed in the same group.\n    :param g: index in the same group\n    :param bg: bbox in the same group\n    :return:\n    \"\"\"\n\n    xs = [bg_item[0] for bg_item in bg]\n    xs_sorted = sorted(xs)\n\n    g_sorted = [None] * len(xs_sorted)\n    bg_sorted = [None] * len(xs_sorted)\n    for g_item, bg_item in zip(g, bg):\n        idx = xs_sorted.index(bg_item[0])\n        bg_sorted[idx] = bg_item\n        g_sorted[idx] = g_item\n\n    return g_sorted, bg_sorted\n\n\ndef flatten(sorted_groups, sorted_bbox_groups):\n    idxs = []\n    bboxes = []\n    for group, bbox_group in zip(sorted_groups, sorted_bbox_groups):\n        for g, bg in zip(group, bbox_group):\n            idxs.append(g)\n            bboxes.append(bg)\n    return idxs, bboxes\n\n\ndef sort_bbox(end2end_xywh_bboxes, no_match_end2end_indexes):\n    \"\"\"\n    This function will group the render end2end bboxes in row.\n    :param end2end_xywh_bboxes:\n    :param no_match_end2end_indexes:\n    :return:\n    \"\"\"\n    groups = []\n    bbox_groups = []\n    for index, end2end_xywh_bbox in zip(no_match_end2end_indexes, end2end_xywh_bboxes):\n        this_bbox = end2end_xywh_bbox\n        if len(groups) == 0:\n            groups.append([index])\n            bbox_groups.append([this_bbox])\n        else:\n            flag = False\n            for g, bg in zip(groups, bbox_groups):\n                # this_bbox is belong to bg's row or not\n                if is_abs_lower_than_threshold(this_bbox, bg[0]):\n                    g.append(index)\n                    bg.append(this_bbox)\n                    flag = True\n                    break\n            if not flag:\n                # this_bbox is not belong to bg's row, create a row.\n                groups.append([index])\n                bbox_groups.append([this_bbox])\n\n    # sorted bboxes in a group\n    tmp_groups, tmp_bbox_groups = [], []\n    for g, bg in zip(groups, bbox_groups):\n        g_sorted, bg_sorted = sort_line_bbox(g, bg)\n        tmp_groups.append(g_sorted)\n        tmp_bbox_groups.append(bg_sorted)\n\n    # sorted groups, sort by coord y's value.\n    sorted_groups = [None] * len(tmp_groups)\n    sorted_bbox_groups = [None] * len(tmp_bbox_groups)\n    ys = [bg[0][1] for bg in tmp_bbox_groups]\n    sorted_ys = sorted(ys)\n    for g, bg in zip(tmp_groups, tmp_bbox_groups):\n        idx = sorted_ys.index(bg[0][1])\n        sorted_groups[idx] = g\n        sorted_bbox_groups[idx] = bg\n\n    # flatten, get final result\n    end2end_sorted_idx_list, end2end_sorted_bbox_list = flatten(\n        sorted_groups, sorted_bbox_groups\n    )\n\n    return (\n        end2end_sorted_idx_list,\n        end2end_sorted_bbox_list,\n        sorted_groups,\n        sorted_bbox_groups,\n    )\n\n\ndef get_bboxes_list(end2end_result, structure_master_result):\n    \"\"\"\n    This function is use to convert end2end results and structure master results to\n    List of xyxy bbox format and List of xywh bbox format\n    :param end2end_result: bbox's format is xyxy\n    :param structure_master_result: bbox's format is xywh\n    :return: 4 kind list of bbox ()\n    \"\"\"\n    # end2end\n    end2end_xyxy_list = []\n    end2end_xywh_list = []\n    for end2end_item in end2end_result:\n        src_bbox = end2end_item[\"bbox\"]\n        end2end_xyxy_list.append(src_bbox)\n        xywh_bbox = xyxy2xywh(src_bbox)\n        end2end_xywh_list.append(xywh_bbox)\n    end2end_xyxy_bboxes = np.array(end2end_xyxy_list)\n    end2end_xywh_bboxes = np.array(end2end_xywh_list)\n\n    # structure master\n    src_bboxes = structure_master_result[\"bbox\"]\n    src_bboxes = remove_empty_bboxes(src_bboxes)\n    structure_master_xyxy_bboxes = src_bboxes\n    xywh_bbox = xyxy2xywh(src_bboxes)\n    structure_master_xywh_bboxes = xywh_bbox\n\n    return (\n        end2end_xyxy_bboxes,\n        end2end_xywh_bboxes,\n        structure_master_xywh_bboxes,\n        structure_master_xyxy_bboxes,\n    )\n\n\ndef center_rule_match(end2end_xywh_bboxes, structure_master_xyxy_bboxes):\n    \"\"\"\n    Judge end2end Bbox's center point is inside structure master Bbox or not,\n    if end2end Bbox's center is in structure master Bbox, get matching pair.\n    :param end2end_xywh_bboxes:\n    :param structure_master_xyxy_bboxes:\n    :return: match pairs list, e.g. [[0,1], [1,2], ...]\n    \"\"\"\n    match_pairs_list = []\n    for i, end2end_xywh in enumerate(end2end_xywh_bboxes):\n        for j, master_xyxy in enumerate(structure_master_xyxy_bboxes):\n            x_end2end, y_end2end = end2end_xywh[0], end2end_xywh[1]\n            x_master1, y_master1, x_master2, y_master2 = (\n                master_xyxy[0],\n                master_xyxy[1],\n                master_xyxy[2],\n                master_xyxy[3],\n            )\n            center_point_end2end = (x_end2end, y_end2end)\n            corner_point_master = ((x_master1, y_master1), (x_master2, y_master2))\n            if is_inside(center_point_end2end, corner_point_master):\n                match_pairs_list.append([i, j])\n    return match_pairs_list\n\n\ndef iou_rule_match(\n    end2end_xyxy_bboxes, end2end_xyxy_indexes, structure_master_xyxy_bboxes\n):\n    \"\"\"\n    Use iou to find matching list.\n    choose max iou value bbox as match pair.\n    :param end2end_xyxy_bboxes:\n    :param end2end_xyxy_indexes: original end2end indexes.\n    :param structure_master_xyxy_bboxes:\n    :return: match pairs list, e.g. [[0,1], [1,2], ...]\n    \"\"\"\n    match_pair_list = []\n    for end2end_xyxy_index, end2end_xyxy in zip(\n        end2end_xyxy_indexes, end2end_xyxy_bboxes\n    ):\n        max_iou = 0\n        max_match = [None, None]\n        for j, master_xyxy in enumerate(structure_master_xyxy_bboxes):\n            end2end_4xy = convert_coord(end2end_xyxy)\n            master_4xy = convert_coord(master_xyxy)\n            iou = cal_iou(end2end_4xy, master_4xy)\n            if iou > max_iou:\n                max_match[0], max_match[1] = end2end_xyxy_index, j\n                max_iou = iou\n\n        if max_match[0] is None:\n            # no match\n            continue\n        match_pair_list.append(max_match)\n    return match_pair_list\n\n\ndef distance_rule_match(end2end_indexes, end2end_bboxes, master_indexes, master_bboxes):\n    \"\"\"\n    Get matching between no-match end2end bboxes and no-match master bboxes.\n    Use min distance to match.\n    This rule will only run (no-match end2end nums > 0) and (no-match master nums > 0)\n    It will Return master_bboxes_nums match-pairs.\n    :param end2end_indexes:\n    :param end2end_bboxes:\n    :param master_indexes:\n    :param master_bboxes:\n    :return: match_pairs list, e.g. [[0,1], [1,2], ...]\n    \"\"\"\n    min_match_list = []\n    for j, master_bbox in zip(master_indexes, master_bboxes):\n        min_distance = np.inf\n        min_match = [0, 0]  # i, j\n        for i, end2end_bbox in zip(end2end_indexes, end2end_bboxes):\n            x_end2end, y_end2end = end2end_bbox[0], end2end_bbox[1]\n            x_master, y_master = master_bbox[0], master_bbox[1]\n            end2end_point = (x_end2end, y_end2end)\n            master_point = (x_master, y_master)\n            dist = cal_distance(master_point, end2end_point)\n            if dist < min_distance:\n                min_match[0], min_match[1] = i, j\n                min_distance = dist\n        min_match_list.append(min_match)\n    return min_match_list\n\n\ndef extra_match(no_match_end2end_indexes, master_bbox_nums):\n    \"\"\"\n    This function will create some virtual master bboxes,\n    and get match with the no match end2end indexes.\n    :param no_match_end2end_indexes:\n    :param master_bbox_nums:\n    :return:\n    \"\"\"\n    end_nums = len(no_match_end2end_indexes) + master_bbox_nums\n    extra_match_list = []\n    for i in range(master_bbox_nums, end_nums):\n        end2end_index = no_match_end2end_indexes[i - master_bbox_nums]\n        extra_match_list.append([end2end_index, i])\n    return extra_match_list\n\n\ndef get_match_dict(match_list):\n    \"\"\"\n    Convert match_list to a dict, where key is master bbox's index, value is end2end bbox index.\n    :param match_list:\n    :return:\n    \"\"\"\n    match_dict = dict()\n    for match_pair in match_list:\n        end2end_index, master_index = match_pair[0], match_pair[1]\n        if master_index not in match_dict.keys():\n            match_dict[master_index] = [end2end_index]\n        else:\n            match_dict[master_index].append(end2end_index)\n    return match_dict\n\n\ndef deal_successive_space(text):\n    \"\"\"\n    deal successive space character for text\n    1. Replace ' '*3 with '<space>' which is real space is text\n    2. Remove ' ', which is split token, not true space\n    3. Replace '<space>' with ' ', to get real text\n    :param text:\n    :return:\n    \"\"\"\n    text = text.replace(\" \" * 3, \"<space>\")\n    text = text.replace(\" \", \"\")\n    text = text.replace(\"<space>\", \" \")\n    return text\n\n\ndef reduce_repeat_bb(text_list, break_token):\n    \"\"\"\n    convert ['<b>Local</b>', '<b>government</b>', '<b>unit</b>'] to ['<b>Local government unit</b>']\n    PS: maybe style <i>Local</i> is also exist, too. it can be processed like this.\n    :param text_list:\n    :param break_token:\n    :return:\n    \"\"\"\n    count = 0\n    for text in text_list:\n        if text.startswith(\"<b>\"):\n            count += 1\n    if count == len(text_list):\n        new_text_list = []\n        for text in text_list:\n            text = text.replace(\"<b>\", \"\").replace(\"</b>\", \"\")\n            new_text_list.append(text)\n        return [\"<b>\" + break_token.join(new_text_list) + \"</b>\"]\n    else:\n        return text_list\n\n\ndef get_match_text_dict(match_dict, end2end_info, break_token=\" \"):\n    match_text_dict = dict()\n    for master_index, end2end_index_list in match_dict.items():\n        text_list = [\n            end2end_info[end2end_index][\"text\"] for end2end_index in end2end_index_list\n        ]\n        text_list = reduce_repeat_bb(text_list, break_token)\n        text = break_token.join(text_list)\n        match_text_dict[master_index] = text\n    return match_text_dict\n\n\ndef merge_span_token(master_token_list):\n    \"\"\"\n    Merge the span style token (row span or col span).\n    :param master_token_list:\n    :return:\n    \"\"\"\n    new_master_token_list = []\n    pointer = 0\n    if master_token_list[-1] != \"</tbody>\":\n        master_token_list.append(\"</tbody>\")\n    while master_token_list[pointer] != \"</tbody>\":\n        try:\n            if master_token_list[pointer] == \"<td\":\n                if master_token_list[pointer + 1].startswith(\n                    \" colspan=\"\n                ) or master_token_list[pointer + 1].startswith(\" rowspan=\"):\n                    \"\"\"\n                    example:\n                    pattern <td colspan=\"3\">\n                    '<td' + 'colspan=\" \"' + '>' + '</td>'\n                    \"\"\"\n                    tmp = \"\".join(master_token_list[pointer : pointer + 3 + 1])\n                    pointer += 4\n                    new_master_token_list.append(tmp)\n\n                elif master_token_list[pointer + 2].startswith(\n                    \" colspan=\"\n                ) or master_token_list[pointer + 2].startswith(\" rowspan=\"):\n                    \"\"\"\n                    example:\n                    pattern <td rowspan=\"2\" colspan=\"3\">\n                    '<td' + 'rowspan=\" \"' + 'colspan=\" \"' + '>' + '</td>'\n                    \"\"\"\n                    tmp = \"\".join(master_token_list[pointer : pointer + 4 + 1])\n                    pointer += 5\n                    new_master_token_list.append(tmp)\n\n                else:\n                    new_master_token_list.append(master_token_list[pointer])\n                    pointer += 1\n            else:\n                new_master_token_list.append(master_token_list[pointer])\n                pointer += 1\n        except:\n            print(\"Break in merge...\")\n            break\n    new_master_token_list.append(\"</tbody>\")\n\n    return new_master_token_list\n\n\ndef deal_eb_token(master_token):\n    \"\"\"\n    post process with <eb></eb>, <eb1></eb1>, ...\n    emptyBboxTokenDict = {\n        \"[]\": '<eb></eb>',\n        \"[' ']\": '<eb1></eb1>',\n        \"['<b>', ' ', '</b>']\": '<eb2></eb2>',\n        \"['\\\\u2028', '\\\\u2028']\": '<eb3></eb3>',\n        \"['<sup>', ' ', '</sup>']\": '<eb4></eb4>',\n        \"['<b>', '</b>']\": '<eb5></eb5>',\n        \"['<i>', ' ', '</i>']\": '<eb6></eb6>',\n        \"['<b>', '<i>', '</i>', '</b>']\": '<eb7></eb7>',\n        \"['<b>', '<i>', ' ', '</i>', '</b>']\": '<eb8></eb8>',\n        \"['<i>', '</i>']\": '<eb9></eb9>',\n        \"['<b>', ' ', '\\\\u2028', ' ', '\\\\u2028', ' ', '</b>']\": '<eb10></eb10>',\n    }\n    :param master_token:\n    :return:\n    \"\"\"\n    master_token = master_token.replace(\"<eb></eb>\", \"<td></td>\")\n    master_token = master_token.replace(\"<eb1></eb1>\", \"<td> </td>\")\n    master_token = master_token.replace(\"<eb2></eb2>\", \"<td><b> </b></td>\")\n    master_token = master_token.replace(\"<eb3></eb3>\", \"<td>\\u2028\\u2028</td>\")\n    master_token = master_token.replace(\"<eb4></eb4>\", \"<td><sup> </sup></td>\")\n    master_token = master_token.replace(\"<eb5></eb5>\", \"<td><b></b></td>\")\n    master_token = master_token.replace(\"<eb6></eb6>\", \"<td><i> </i></td>\")\n    master_token = master_token.replace(\"<eb7></eb7>\", \"<td><b><i></i></b></td>\")\n    master_token = master_token.replace(\"<eb8></eb8>\", \"<td><b><i> </i></b></td>\")\n    master_token = master_token.replace(\"<eb9></eb9>\", \"<td><i></i></td>\")\n    master_token = master_token.replace(\n        \"<eb10></eb10>\", \"<td><b> \\u2028 \\u2028 </b></td>\"\n    )\n    return master_token\n\n\ndef insert_text_to_token(master_token_list, match_text_dict):\n    \"\"\"\n    Insert OCR text result to structure token.\n    :param master_token_list:\n    :param match_text_dict:\n    :return:\n    \"\"\"\n    master_token_list = merge_span_token(master_token_list)\n    merged_result_list = []\n    text_count = 0\n    for master_token in master_token_list:\n        if master_token.startswith(\"<td\"):\n            if text_count > len(match_text_dict) - 1:\n                text_count += 1\n                continue\n            elif text_count not in match_text_dict.keys():\n                text_count += 1\n                continue\n            else:\n                master_token = master_token.replace(\n                    \"><\", \">{}<\".format(match_text_dict[text_count])\n                )\n                text_count += 1\n        master_token = deal_eb_token(master_token)\n        merged_result_list.append(master_token)\n\n    return \"\".join(merged_result_list)\n\n\ndef deal_isolate_span(thead_part):\n    \"\"\"\n    Deal with isolate span cases in this function.\n    It causes by wrong prediction in structure recognition model.\n    eg. predict <td rowspan=\"2\"></td> to <td></td> rowspan=\"2\"></b></td>.\n    :param thead_part:\n    :return:\n    \"\"\"\n    # 1. find out isolate span tokens.\n    isolate_pattern = (\n        '<td></td> rowspan=\"(\\d)+\" colspan=\"(\\d)+\"></b></td>|'\n        '<td></td> colspan=\"(\\d)+\" rowspan=\"(\\d)+\"></b></td>|'\n        '<td></td> rowspan=\"(\\d)+\"></b></td>|'\n        '<td></td> colspan=\"(\\d)+\"></b></td>'\n    )\n    isolate_iter = re.finditer(isolate_pattern, thead_part)\n    isolate_list = [i.group() for i in isolate_iter]\n\n    # 2. find out span number, by step 1 results.\n    span_pattern = (\n        ' rowspan=\"(\\d)+\" colspan=\"(\\d)+\"|'\n        ' colspan=\"(\\d)+\" rowspan=\"(\\d)+\"|'\n        ' rowspan=\"(\\d)+\"|'\n        ' colspan=\"(\\d)+\"'\n    )\n    corrected_list = []\n    for isolate_item in isolate_list:\n        span_part = re.search(span_pattern, isolate_item)\n        spanStr_in_isolateItem = span_part.group()\n        # 3. merge the span number into the span token format string.\n        if spanStr_in_isolateItem is not None:\n            corrected_item = \"<td{}></td>\".format(spanStr_in_isolateItem)\n            corrected_list.append(corrected_item)\n        else:\n            corrected_list.append(None)\n\n    # 4. replace original isolated token.\n    for corrected_item, isolate_item in zip(corrected_list, isolate_list):\n        if corrected_item is not None:\n            thead_part = thead_part.replace(isolate_item, corrected_item)\n        else:\n            pass\n    return thead_part\n\n\ndef deal_duplicate_bb(thead_part):\n    \"\"\"\n    Deal duplicate <b> or </b> after replace.\n    Keep one <b></b> in a <td></td> token.\n    :param thead_part:\n    :return:\n    \"\"\"\n    # 1. find out <td></td> in <thead></thead>.\n    td_pattern = (\n        '<td rowspan=\"(\\d)+\" colspan=\"(\\d)+\">(.+?)</td>|'\n        '<td colspan=\"(\\d)+\" rowspan=\"(\\d)+\">(.+?)</td>|'\n        '<td rowspan=\"(\\d)+\">(.+?)</td>|'\n        '<td colspan=\"(\\d)+\">(.+?)</td>|'\n        \"<td>(.*?)</td>\"\n    )\n    td_iter = re.finditer(td_pattern, thead_part)\n    td_list = [t.group() for t in td_iter]\n\n    # 2. is multiply <b></b> in <td></td> or not?\n    new_td_list = []\n    for td_item in td_list:\n        if td_item.count(\"<b>\") > 1 or td_item.count(\"</b>\") > 1:\n            # multiply <b></b> in <td></td> case.\n            # 1. remove all <b></b>\n            td_item = td_item.replace(\"<b>\", \"\").replace(\"</b>\", \"\")\n            # 2. replace <tb> -> <tb><b>, </tb> -> </b></tb>.\n            td_item = td_item.replace(\"<td>\", \"<td><b>\").replace(\"</td>\", \"</b></td>\")\n            new_td_list.append(td_item)\n        else:\n            new_td_list.append(td_item)\n\n    # 3. replace original thead part.\n    for td_item, new_td_item in zip(td_list, new_td_list):\n        thead_part = thead_part.replace(td_item, new_td_item)\n    return thead_part\n\n\ndef deal_bb(result_token):\n    \"\"\"\n    In our opinion, <b></b> always occurs in <thead></thead> text's context.\n    This function will find out all tokens in <thead></thead> and insert <b></b> by manual.\n    :param result_token:\n    :return:\n    \"\"\"\n    # find out <thead></thead> parts.\n    thead_pattern = \"<thead>(.*?)</thead>\"\n    if re.search(thead_pattern, result_token) is None:\n        return result_token\n    thead_part = re.search(thead_pattern, result_token).group()\n    origin_thead_part = copy.deepcopy(thead_part)\n\n    # check \"rowspan\" or \"colspan\" occur in <thead></thead> parts or not .\n    span_pattern = '<td rowspan=\"(\\d)+\" colspan=\"(\\d)+\">|<td colspan=\"(\\d)+\" rowspan=\"(\\d)+\">|<td rowspan=\"(\\d)+\">|<td colspan=\"(\\d)+\">'\n    span_iter = re.finditer(span_pattern, thead_part)\n    span_list = [s.group() for s in span_iter]\n    has_span_in_head = True if len(span_list) > 0 else False\n\n    if not has_span_in_head:\n        # <thead></thead> not include \"rowspan\" or \"colspan\" branch 1.\n        # 1. replace <td> to <td><b>, and </td> to </b></td>\n        # 2. it is possible to predict text include <b> or </b> by Text-line recognition,\n        #    so we replace <b><b> to <b>, and </b></b> to </b>\n        thead_part = (\n            thead_part.replace(\"<td>\", \"<td><b>\")\n            .replace(\"</td>\", \"</b></td>\")\n            .replace(\"<b><b>\", \"<b>\")\n            .replace(\"</b></b>\", \"</b>\")\n        )\n    else:\n        # <thead></thead> include \"rowspan\" or \"colspan\" branch 2.\n        # Firstly, we deal rowspan or colspan cases.\n        # 1. replace > to ><b>\n        # 2. replace </td> to </b></td>\n        # 3. it is possible to predict text include <b> or </b> by Text-line recognition,\n        #    so we replace <b><b> to <b>, and </b><b> to </b>\n\n        # Secondly, deal ordinary cases like branch 1\n\n        # replace \">\" to \"<b>\"\n        replaced_span_list = []\n        for sp in span_list:\n            replaced_span_list.append(sp.replace(\">\", \"><b>\"))\n        for sp, rsp in zip(span_list, replaced_span_list):\n            thead_part = thead_part.replace(sp, rsp)\n\n        # replace \"</td>\" to \"</b></td>\"\n        thead_part = thead_part.replace(\"</td>\", \"</b></td>\")\n\n        # remove duplicated <b> by re.sub\n        mb_pattern = \"(<b>)+\"\n        single_b_string = \"<b>\"\n        thead_part = re.sub(mb_pattern, single_b_string, thead_part)\n\n        mgb_pattern = \"(</b>)+\"\n        single_gb_string = \"</b>\"\n        thead_part = re.sub(mgb_pattern, single_gb_string, thead_part)\n\n        # ordinary cases like branch 1\n        thead_part = thead_part.replace(\"<td>\", \"<td><b>\").replace(\"<b><b>\", \"<b>\")\n\n    # convert <tb><b></b></tb> back to <tb></tb>, empty cell has no <b></b>.\n    # but space cell(<tb> </tb>)  is suitable for <td><b> </b></td>\n    thead_part = thead_part.replace(\"<td><b></b></td>\", \"<td></td>\")\n    # deal with duplicated <b></b>\n    thead_part = deal_duplicate_bb(thead_part)\n    # deal with isolate span tokens, which causes by wrong predict by structure prediction.\n    # eg.PMC5994107_011_00.png\n    thead_part = deal_isolate_span(thead_part)\n    # replace original result with new thead part.\n    result_token = result_token.replace(origin_thead_part, thead_part)\n    return result_token\n\n\nclass Matcher:\n    def __init__(self, end2end_file, structure_master_file):\n        \"\"\"\n        This class process the end2end results and structure recognition results.\n        :param end2end_file: end2end results predict by end2end inference.\n        :param structure_master_file: structure recognition results predict by structure master inference.\n        \"\"\"\n        self.end2end_file = end2end_file\n        self.structure_master_file = structure_master_file\n        self.end2end_results = pickle_load(end2end_file, prefix=\"end2end\")\n        self.structure_master_results = pickle_load(\n            structure_master_file, prefix=\"structure\"\n        )\n\n    def match(self):\n        \"\"\"\n        Match process:\n        pre-process : convert end2end and structure master results to xyxy, xywh ndnarray format.\n        1. Use pseBbox is inside masterBbox judge rule\n        2. Use iou between pseBbox and masterBbox rule\n        3. Use min distance of center point rule\n        :return:\n        \"\"\"\n        match_results = dict()\n        for idx, (file_name, end2end_result) in enumerate(self.end2end_results.items()):\n            match_list = []\n            if file_name not in self.structure_master_results:\n                continue\n            structure_master_result = self.structure_master_results[file_name]\n            (\n                end2end_xyxy_bboxes,\n                end2end_xywh_bboxes,\n                structure_master_xywh_bboxes,\n                structure_master_xyxy_bboxes,\n            ) = get_bboxes_list(end2end_result, structure_master_result)\n\n            # rule 1: center rule\n            center_rule_match_list = center_rule_match(\n                end2end_xywh_bboxes, structure_master_xyxy_bboxes\n            )\n            match_list.extend(center_rule_match_list)\n\n            # rule 2: iou rule\n            # firstly, find not match index in previous step.\n            center_no_match_end2end_indexs = find_no_match(\n                match_list, len(end2end_xywh_bboxes), type=\"end2end\"\n            )\n            if len(center_no_match_end2end_indexs) > 0:\n                center_no_match_end2end_xyxy = end2end_xyxy_bboxes[\n                    center_no_match_end2end_indexs\n                ]\n                # secondly, iou rule match\n                iou_rule_match_list = iou_rule_match(\n                    center_no_match_end2end_xyxy,\n                    center_no_match_end2end_indexs,\n                    structure_master_xyxy_bboxes,\n                )\n                match_list.extend(iou_rule_match_list)\n\n            # rule 3: distance rule\n            # match between no-match end2end bboxes and no-match master bboxes.\n            # it will return master_bboxes_nums match-pairs.\n            # firstly, find not match index in previous step.\n            centerIou_no_match_end2end_indexs = find_no_match(\n                match_list, len(end2end_xywh_bboxes), type=\"end2end\"\n            )\n            centerIou_no_match_master_indexs = find_no_match(\n                match_list, len(structure_master_xywh_bboxes), type=\"master\"\n            )\n            if (\n                len(centerIou_no_match_master_indexs) > 0\n                and len(centerIou_no_match_end2end_indexs) > 0\n            ):\n                centerIou_no_match_end2end_xywh = end2end_xywh_bboxes[\n                    centerIou_no_match_end2end_indexs\n                ]\n                centerIou_no_match_master_xywh = structure_master_xywh_bboxes[\n                    centerIou_no_match_master_indexs\n                ]\n                distance_match_list = distance_rule_match(\n                    centerIou_no_match_end2end_indexs,\n                    centerIou_no_match_end2end_xywh,\n                    centerIou_no_match_master_indexs,\n                    centerIou_no_match_master_xywh,\n                )\n                match_list.extend(distance_match_list)\n\n            # TODO:\n            # The render no-match pseBbox, insert the last\n            # After step3 distance rule, a master bbox at least match one end2end bbox.\n            # But end2end bbox maybe overmuch, because numbers of master bbox will cut by max length.\n            # For these render end2end bboxes, we will make some virtual master bboxes, and get matching.\n            # The above extra insert bboxes will be further processed in \"formatOutput\" function.\n            # After this operation, it will increase TEDS score.\n            no_match_end2end_indexes = find_no_match(\n                match_list, len(end2end_xywh_bboxes), type=\"end2end\"\n            )\n            if len(no_match_end2end_indexes) > 0:\n                no_match_end2end_xywh = end2end_xywh_bboxes[no_match_end2end_indexes]\n                # sort the render no-match end2end bbox in row\n                (\n                    end2end_sorted_indexes_list,\n                    end2end_sorted_bboxes_list,\n                    sorted_groups,\n                    sorted_bboxes_groups,\n                ) = sort_bbox(no_match_end2end_xywh, no_match_end2end_indexes)\n                # make virtual master bboxes, and get matching with the no-match end2end bboxes.\n                extra_match_list = extra_match(\n                    end2end_sorted_indexes_list, len(structure_master_xywh_bboxes)\n                )\n                match_list_add_extra_match = copy.deepcopy(match_list)\n                match_list_add_extra_match.extend(extra_match_list)\n            else:\n                # no no-match end2end bboxes\n                match_list_add_extra_match = copy.deepcopy(match_list)\n                sorted_groups = []\n                sorted_bboxes_groups = []\n\n            match_result_dict = {\n                \"match_list\": match_list,\n                \"match_list_add_extra_match\": match_list_add_extra_match,\n                \"sorted_groups\": sorted_groups,\n                \"sorted_bboxes_groups\": sorted_bboxes_groups,\n            }\n\n            # format output\n            match_result_dict = self._format(match_result_dict, file_name)\n\n            match_results[file_name] = match_result_dict\n\n        return match_results\n\n    def _format(self, match_result, file_name):\n        \"\"\"\n        Extend the master token(insert virtual master token), and format matching result.\n        :param match_result:\n        :param file_name:\n        :return:\n        \"\"\"\n        end2end_info = self.end2end_results[file_name]\n        master_info = self.structure_master_results[file_name]\n        master_token = master_info[\"text\"]\n        sorted_groups = match_result[\"sorted_groups\"]\n\n        # creat virtual master token\n        virtual_master_token_list = []\n        for line_group in sorted_groups:\n            tmp_list = [\"<tr>\"]\n            item_nums = len(line_group)\n            for _ in range(item_nums):\n                tmp_list.append(\"<td></td>\")\n            tmp_list.append(\"</tr>\")\n            virtual_master_token_list.extend(tmp_list)\n\n        # insert virtual master token\n        master_token_list = master_token.split(\",\")\n        if master_token_list[-1] == \"</tbody>\":\n            # complete predict(no cut by max length)\n            # This situation insert virtual master token will drop TEDs score in val set.\n            # So we will not extend virtual token in this situation.\n\n            # fake extend virtual\n            master_token_list[:-1].extend(virtual_master_token_list)\n\n            # real extend virtual\n            # master_token_list = master_token_list[:-1]\n            # master_token_list.extend(virtual_master_token_list)\n            # master_token_list.append('</tbody>')\n\n        elif master_token_list[-1] == \"<td></td>\":\n            master_token_list.append(\"</tr>\")\n            master_token_list.extend(virtual_master_token_list)\n            master_token_list.append(\"</tbody>\")\n        else:\n            master_token_list.extend(virtual_master_token_list)\n            master_token_list.append(\"</tbody>\")\n\n        # format output\n        match_result.setdefault(\"matched_master_token_list\", master_token_list)\n        return match_result\n\n    def get_merge_result(self, match_results):\n        \"\"\"\n        Merge the OCR result into structure token to get final results.\n        :param match_results:\n        :return:\n        \"\"\"\n        merged_results = dict()\n\n        # break_token is linefeed token, when one master bbox has multiply end2end bboxes.\n        break_token = \" \"\n\n        for idx, (file_name, match_info) in enumerate(match_results.items()):\n            end2end_info = self.end2end_results[file_name]\n            master_token_list = match_info[\"matched_master_token_list\"]\n            match_list = match_info[\"match_list_add_extra_match\"]\n\n            match_dict = get_match_dict(match_list)\n            match_text_dict = get_match_text_dict(match_dict, end2end_info, break_token)\n            merged_result = insert_text_to_token(master_token_list, match_text_dict)\n            merged_result = deal_bb(merged_result)\n\n            merged_results[file_name] = merged_result\n\n        return merged_results\n\n\nclass TableMasterMatcher(Matcher):\n    def __init__(self):\n        pass\n\n    def __call__(self, structure_res, dt_boxes, rec_res, img_name=1):\n        end2end_results = {img_name: []}\n        for dt_box, res in zip(dt_boxes, rec_res):\n            d = dict(\n                bbox=np.array(dt_box),\n                text=res[0],\n            )\n            end2end_results[img_name].append(d)\n\n        self.end2end_results = end2end_results\n\n        structure_master_result_dict = {img_name: {}}\n        pred_structures, pred_bboxes = structure_res\n        pred_structures = \",\".join(pred_structures[3:-3])\n        structure_master_result_dict[img_name][\"text\"] = pred_structures\n        structure_master_result_dict[img_name][\"bbox\"] = pred_bboxes\n        self.structure_master_results = structure_master_result_dict\n\n        # match\n        match_results = self.match()\n        merged_results = self.get_merge_result(match_results)\n        pred_html = merged_results[img_name]\n        pred_html = \"<html><body><table>\" + pred_html + \"</table></body></html>\"\n        return pred_html\n", "ppstructure/table/convert_label2html.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nconver table label to html\n\"\"\"\n\nimport json\nimport argparse\nfrom tqdm import tqdm\n\n\ndef save_pred_txt(key, val, tmp_file_path):\n    with open(tmp_file_path, \"a+\", encoding=\"utf-8\") as f:\n        f.write(\"{}\\t{}\\n\".format(key, val))\n\n\ndef skip_char(text, sp_char_list):\n    \"\"\"\n    skip empty cell\n    @param text: text in cell\n    @param sp_char_list: style char and special code\n    @return:\n    \"\"\"\n    for sp_char in sp_char_list:\n        text = text.replace(sp_char, \"\")\n    return text\n\n\ndef gen_html(img):\n    \"\"\"\n    Formats HTML code from tokenized annotation of img\n    \"\"\"\n    html_code = img[\"html\"][\"structure\"][\"tokens\"].copy()\n    to_insert = [i for i, tag in enumerate(html_code) if tag in (\"<td>\", \">\")]\n    for i, cell in zip(to_insert[::-1], img[\"html\"][\"cells\"][::-1]):\n        if cell[\"tokens\"]:\n            text = \"\".join(cell[\"tokens\"])\n            # skip empty text\n            sp_char_list = [\"<b>\", \"</b>\", \"\\u2028\", \" \", \"<i>\", \"</i>\"]\n            text_remove_style = skip_char(text, sp_char_list)\n            if len(text_remove_style) == 0:\n                continue\n            html_code.insert(i + 1, text)\n    html_code = \"\".join(html_code)\n    html_code = \"<html><body><table>{}</table></body></html>\".format(html_code)\n    return html_code\n\n\ndef load_gt_data(gt_path):\n    \"\"\"\n    load gt\n    @param gt_path:\n    @return:\n    \"\"\"\n    data_list = {}\n    with open(gt_path, \"rb\") as f:\n        lines = f.readlines()\n        for line in tqdm(lines):\n            data_line = line.decode(\"utf-8\").strip(\"\\n\")\n            info = json.loads(data_line)\n            data_list[info[\"filename\"]] = info\n    return data_list\n\n\ndef convert(origin_gt_path, save_path):\n    \"\"\"\n    gen html from label file\n    @param origin_gt_path:\n    @param save_path:\n    @return:\n    \"\"\"\n    data_dict = load_gt_data(origin_gt_path)\n    for img_name, gt in tqdm(data_dict.items()):\n        html = gen_html(gt)\n        save_pred_txt(img_name, html, save_path)\n    print(\"conver finish\")\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"args for paddleserving\")\n    parser.add_argument(\"--ori_gt_path\", type=str, required=True, help=\"label gt path\")\n    parser.add_argument(\n        \"--save_path\", type=str, required=True, help=\"path to save file\"\n    )\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    convert(args.ori_gt_path, args.save_path)\n", "ppstructure/table/predict_table.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport sys\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(__dir__)\nsys.path.insert(0, os.path.abspath(os.path.join(__dir__, \"..\")))\nsys.path.insert(0, os.path.abspath(os.path.join(__dir__, \"../..\")))\n\nos.environ[\"FLAGS_allocator_strategy\"] = \"auto_growth\"\nimport cv2\nimport copy\nimport logging\nimport numpy as np\nimport time\nimport tools.infer.predict_rec as predict_rec\nimport tools.infer.predict_det as predict_det\nimport tools.infer.utility as utility\nfrom tools.infer.predict_system import sorted_boxes\nfrom ppocr.utils.utility import get_image_file_list, check_and_read\nfrom ppocr.utils.logging import get_logger\nfrom ppstructure.table.matcher import TableMatch\nfrom ppstructure.table.table_master_match import TableMasterMatcher\nfrom ppstructure.utility import parse_args\nimport ppstructure.table.predict_structure as predict_strture\n\nlogger = get_logger()\n\n\ndef expand(pix, det_box, shape):\n    x0, y0, x1, y1 = det_box\n    #     print(shape)\n    h, w, c = shape\n    tmp_x0 = x0 - pix\n    tmp_x1 = x1 + pix\n    tmp_y0 = y0 - pix\n    tmp_y1 = y1 + pix\n    x0_ = tmp_x0 if tmp_x0 >= 0 else 0\n    x1_ = tmp_x1 if tmp_x1 <= w else w\n    y0_ = tmp_y0 if tmp_y0 >= 0 else 0\n    y1_ = tmp_y1 if tmp_y1 <= h else h\n    return x0_, y0_, x1_, y1_\n\n\nclass TableSystem(object):\n    def __init__(self, args, text_detector=None, text_recognizer=None):\n        self.args = args\n        if not args.show_log:\n            logger.setLevel(logging.INFO)\n        benchmark_tmp = False\n        if args.benchmark:\n            benchmark_tmp = args.benchmark\n            args.benchmark = False\n        self.text_detector = (\n            predict_det.TextDetector(copy.deepcopy(args))\n            if text_detector is None\n            else text_detector\n        )\n        self.text_recognizer = (\n            predict_rec.TextRecognizer(copy.deepcopy(args))\n            if text_recognizer is None\n            else text_recognizer\n        )\n        if benchmark_tmp:\n            args.benchmark = True\n        self.table_structurer = predict_strture.TableStructurer(args)\n        if args.table_algorithm in [\"TableMaster\"]:\n            self.match = TableMasterMatcher()\n        else:\n            self.match = TableMatch(filter_ocr_result=True)\n\n        (\n            self.predictor,\n            self.input_tensor,\n            self.output_tensors,\n            self.config,\n        ) = utility.create_predictor(args, \"table\", logger)\n\n    def __call__(self, img, return_ocr_result_in_table=False):\n        result = dict()\n        time_dict = {\"det\": 0, \"rec\": 0, \"table\": 0, \"all\": 0, \"match\": 0}\n        start = time.time()\n        structure_res, elapse = self._structure(copy.deepcopy(img))\n        result[\"cell_bbox\"] = structure_res[1].tolist()\n        time_dict[\"table\"] = elapse\n\n        dt_boxes, rec_res, det_elapse, rec_elapse = self._ocr(copy.deepcopy(img))\n        time_dict[\"det\"] = det_elapse\n        time_dict[\"rec\"] = rec_elapse\n\n        if return_ocr_result_in_table:\n            result[\"boxes\"] = [x.tolist() for x in dt_boxes]\n            result[\"rec_res\"] = rec_res\n\n        tic = time.time()\n        pred_html = self.match(structure_res, dt_boxes, rec_res)\n        toc = time.time()\n        time_dict[\"match\"] = toc - tic\n        result[\"html\"] = pred_html\n        end = time.time()\n        time_dict[\"all\"] = end - start\n        return result, time_dict\n\n    def _structure(self, img):\n        structure_res, elapse = self.table_structurer(copy.deepcopy(img))\n        return structure_res, elapse\n\n    def _ocr(self, img):\n        h, w = img.shape[:2]\n        dt_boxes, det_elapse = self.text_detector(copy.deepcopy(img))\n        dt_boxes = sorted_boxes(dt_boxes)\n\n        r_boxes = []\n        for box in dt_boxes:\n            x_min = max(0, box[:, 0].min() - 1)\n            x_max = min(w, box[:, 0].max() + 1)\n            y_min = max(0, box[:, 1].min() - 1)\n            y_max = min(h, box[:, 1].max() + 1)\n            box = [x_min, y_min, x_max, y_max]\n            r_boxes.append(box)\n        dt_boxes = np.array(r_boxes)\n        logger.debug(\"dt_boxes num : {}, elapse : {}\".format(len(dt_boxes), det_elapse))\n        if dt_boxes is None:\n            return None, None\n\n        img_crop_list = []\n        for i in range(len(dt_boxes)):\n            det_box = dt_boxes[i]\n            x0, y0, x1, y1 = expand(2, det_box, img.shape)\n            text_rect = img[int(y0) : int(y1), int(x0) : int(x1), :]\n            img_crop_list.append(text_rect)\n        rec_res, rec_elapse = self.text_recognizer(img_crop_list)\n        logger.debug(\"rec_res num  : {}, elapse : {}\".format(len(rec_res), rec_elapse))\n        return dt_boxes, rec_res, det_elapse, rec_elapse\n\n\ndef to_excel(html_table, excel_path):\n    from tablepyxl import tablepyxl\n\n    tablepyxl.document_to_xl(html_table, excel_path)\n\n\ndef main(args):\n    image_file_list = get_image_file_list(args.image_dir)\n    image_file_list = image_file_list[args.process_id :: args.total_process_num]\n    os.makedirs(args.output, exist_ok=True)\n\n    table_sys = TableSystem(args)\n    img_num = len(image_file_list)\n\n    f_html = open(os.path.join(args.output, \"show.html\"), mode=\"w\", encoding=\"utf-8\")\n    f_html.write(\"<html>\\n<body>\\n\")\n    f_html.write('<table border=\"1\">\\n')\n    f_html.write(\n        '<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />'\n    )\n    f_html.write(\"<tr>\\n\")\n    f_html.write(\"<td>img name\\n\")\n    f_html.write(\"<td>ori image</td>\")\n    f_html.write(\"<td>table html</td>\")\n    f_html.write(\"<td>cell box</td>\")\n    f_html.write(\"</tr>\\n\")\n\n    for i, image_file in enumerate(image_file_list):\n        logger.info(\"[{}/{}] {}\".format(i, img_num, image_file))\n        img, flag, _ = check_and_read(image_file)\n        excel_path = os.path.join(\n            args.output, os.path.basename(image_file).split(\".\")[0] + \".xlsx\"\n        )\n        if not flag:\n            img = cv2.imread(image_file)\n        if img is None:\n            logger.error(\"error in loading image:{}\".format(image_file))\n            continue\n        starttime = time.time()\n        pred_res, _ = table_sys(img)\n        pred_html = pred_res[\"html\"]\n        logger.info(pred_html)\n        to_excel(pred_html, excel_path)\n        logger.info(\"excel saved to {}\".format(excel_path))\n        elapse = time.time() - starttime\n        logger.info(\"Predict time : {:.3f}s\".format(elapse))\n\n        if len(pred_res[\"cell_bbox\"]) > 0 and len(pred_res[\"cell_bbox\"][0]) == 4:\n            img = predict_strture.draw_rectangle(image_file, pred_res[\"cell_bbox\"])\n        else:\n            img = utility.draw_boxes(img, pred_res[\"cell_bbox\"])\n        img_save_path = os.path.join(args.output, os.path.basename(image_file))\n        cv2.imwrite(img_save_path, img)\n\n        f_html.write(\"<tr>\\n\")\n        f_html.write(f\"<td> {os.path.basename(image_file)} <br/>\\n\")\n        f_html.write(f'<td><img src=\"{image_file}\" width=640></td>\\n')\n        f_html.write(\n            '<td><table  border=\"1\">'\n            + pred_html.replace(\"<html><body><table>\", \"\").replace(\n                \"</table></body></html>\", \"\"\n            )\n            + \"</table></td>\\n\"\n        )\n        f_html.write(f'<td><img src=\"{os.path.basename(image_file)}\" width=640></td>\\n')\n        f_html.write(\"</tr>\\n\")\n    f_html.write(\"</table>\\n\")\n    f_html.close()\n\n    if args.benchmark:\n        table_sys.table_structurer.autolog.report()\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    if args.use_mp:\n        import subprocess\n\n        p_list = []\n        total_process_num = args.total_process_num\n        for process_id in range(total_process_num):\n            cmd = (\n                [sys.executable, \"-u\"]\n                + sys.argv\n                + [\"--process_id={}\".format(process_id), \"--use_mp={}\".format(False)]\n            )\n            p = subprocess.Popen(cmd, stdout=sys.stdout, stderr=sys.stdout)\n            p_list.append(p)\n        for p in p_list:\n            p.wait()\n    else:\n        main(args)\n", "ppstructure/table/matcher.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nfrom ppstructure.table.table_master_match import deal_eb_token, deal_bb\n\n\ndef distance(box_1, box_2):\n    x1, y1, x2, y2 = box_1\n    x3, y3, x4, y4 = box_2\n    dis = abs(x3 - x1) + abs(y3 - y1) + abs(x4 - x2) + abs(y4 - y2)\n    dis_2 = abs(x3 - x1) + abs(y3 - y1)\n    dis_3 = abs(x4 - x2) + abs(y4 - y2)\n    return dis + min(dis_2, dis_3)\n\n\ndef compute_iou(rec1, rec2):\n    \"\"\"\n    computing IoU\n    :param rec1: (y0, x0, y1, x1), which reflects\n            (top, left, bottom, right)\n    :param rec2: (y0, x0, y1, x1)\n    :return: scala value of IoU\n    \"\"\"\n    # computing area of each rectangles\n    S_rec1 = (rec1[2] - rec1[0]) * (rec1[3] - rec1[1])\n    S_rec2 = (rec2[2] - rec2[0]) * (rec2[3] - rec2[1])\n\n    # computing the sum_area\n    sum_area = S_rec1 + S_rec2\n\n    # find the each edge of intersect rectangle\n    left_line = max(rec1[1], rec2[1])\n    right_line = min(rec1[3], rec2[3])\n    top_line = max(rec1[0], rec2[0])\n    bottom_line = min(rec1[2], rec2[2])\n\n    # judge if there is an intersect\n    if left_line >= right_line or top_line >= bottom_line:\n        return 0.0\n    else:\n        intersect = (right_line - left_line) * (bottom_line - top_line)\n        return (intersect / (sum_area - intersect)) * 1.0\n\n\nclass TableMatch:\n    def __init__(self, filter_ocr_result=False, use_master=False):\n        self.filter_ocr_result = filter_ocr_result\n        self.use_master = use_master\n\n    def __call__(self, structure_res, dt_boxes, rec_res):\n        pred_structures, pred_bboxes = structure_res\n        if self.filter_ocr_result:\n            dt_boxes, rec_res = self._filter_ocr_result(pred_bboxes, dt_boxes, rec_res)\n        matched_index = self.match_result(dt_boxes, pred_bboxes)\n        if self.use_master:\n            pred_html, pred = self.get_pred_html_master(\n                pred_structures, matched_index, rec_res\n            )\n        else:\n            pred_html, pred = self.get_pred_html(\n                pred_structures, matched_index, rec_res\n            )\n        return pred_html\n\n    def match_result(self, dt_boxes, pred_bboxes):\n        matched = {}\n        for i, gt_box in enumerate(dt_boxes):\n            distances = []\n            for j, pred_box in enumerate(pred_bboxes):\n                if len(pred_box) == 8:\n                    pred_box = [\n                        np.min(pred_box[0::2]),\n                        np.min(pred_box[1::2]),\n                        np.max(pred_box[0::2]),\n                        np.max(pred_box[1::2]),\n                    ]\n                distances.append(\n                    (distance(gt_box, pred_box), 1.0 - compute_iou(gt_box, pred_box))\n                )  # compute iou and l1 distance\n            sorted_distances = distances.copy()\n            # select det box by iou and l1 distance\n            sorted_distances = sorted(\n                sorted_distances, key=lambda item: (item[1], item[0])\n            )\n            if distances.index(sorted_distances[0]) not in matched.keys():\n                matched[distances.index(sorted_distances[0])] = [i]\n            else:\n                matched[distances.index(sorted_distances[0])].append(i)\n        return matched\n\n    def get_pred_html(self, pred_structures, matched_index, ocr_contents):\n        end_html = []\n        td_index = 0\n        for tag in pred_structures:\n            if \"</td>\" in tag:\n                if \"<td></td>\" == tag:\n                    end_html.extend(\"<td>\")\n                if td_index in matched_index.keys():\n                    b_with = False\n                    if (\n                        \"<b>\" in ocr_contents[matched_index[td_index][0]]\n                        and len(matched_index[td_index]) > 1\n                    ):\n                        b_with = True\n                        end_html.extend(\"<b>\")\n                    for i, td_index_index in enumerate(matched_index[td_index]):\n                        content = ocr_contents[td_index_index][0]\n                        if len(matched_index[td_index]) > 1:\n                            if len(content) == 0:\n                                continue\n                            if content[0] == \" \":\n                                content = content[1:]\n                            if \"<b>\" in content:\n                                content = content[3:]\n                            if \"</b>\" in content:\n                                content = content[:-4]\n                            if len(content) == 0:\n                                continue\n                            if (\n                                i != len(matched_index[td_index]) - 1\n                                and \" \" != content[-1]\n                            ):\n                                content += \" \"\n                        end_html.extend(content)\n                    if b_with:\n                        end_html.extend(\"</b>\")\n                if \"<td></td>\" == tag:\n                    end_html.append(\"</td>\")\n                else:\n                    end_html.append(tag)\n                td_index += 1\n            else:\n                end_html.append(tag)\n        return \"\".join(end_html), end_html\n\n    def get_pred_html_master(self, pred_structures, matched_index, ocr_contents):\n        end_html = []\n        td_index = 0\n        for token in pred_structures:\n            if \"</td>\" in token:\n                txt = \"\"\n                b_with = False\n                if td_index in matched_index.keys():\n                    if (\n                        \"<b>\" in ocr_contents[matched_index[td_index][0]]\n                        and len(matched_index[td_index]) > 1\n                    ):\n                        b_with = True\n                    for i, td_index_index in enumerate(matched_index[td_index]):\n                        content = ocr_contents[td_index_index][0]\n                        if len(matched_index[td_index]) > 1:\n                            if len(content) == 0:\n                                continue\n                            if content[0] == \" \":\n                                content = content[1:]\n                            if \"<b>\" in content:\n                                content = content[3:]\n                            if \"</b>\" in content:\n                                content = content[:-4]\n                            if len(content) == 0:\n                                continue\n                            if (\n                                i != len(matched_index[td_index]) - 1\n                                and \" \" != content[-1]\n                            ):\n                                content += \" \"\n                        txt += content\n                if b_with:\n                    txt = \"<b>{}</b>\".format(txt)\n                if \"<td></td>\" == token:\n                    token = \"<td>{}</td>\".format(txt)\n                else:\n                    token = \"{}</td>\".format(txt)\n                td_index += 1\n            token = deal_eb_token(token)\n            end_html.append(token)\n        html = \"\".join(end_html)\n        html = deal_bb(html)\n        return html, end_html\n\n    def _filter_ocr_result(self, pred_bboxes, dt_boxes, rec_res):\n        y1 = pred_bboxes[:, 1::2].min()\n        new_dt_boxes = []\n        new_rec_res = []\n\n        for box, rec in zip(dt_boxes, rec_res):\n            if np.max(box[1::2]) < y1:\n                continue\n            new_dt_boxes.append(box)\n            new_rec_res.append(rec)\n        return new_dt_boxes, new_rec_res\n", "ppstructure/table/__init__.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n", "ppstructure/table/table_metric/parallel.py": "from tqdm import tqdm\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\n\n\ndef parallel_process(array, function, n_jobs=16, use_kwargs=False, front_num=0):\n    \"\"\"\n    A parallel version of the map function with a progress bar.\n    Args:\n        array (array-like): An array to iterate over.\n        function (function): A python function to apply to the elements of array\n        n_jobs (int, default=16): The number of cores to use\n        use_kwargs (boolean, default=False): Whether to consider the elements of array as dictionaries of\n            keyword arguments to function\n        front_num (int, default=3): The number of iterations to run serially before kicking off the parallel job.\n            Useful for catching bugs\n    Returns:\n        [function(array[0]), function(array[1]), ...]\n    \"\"\"\n    # We run the first few iterations serially to catch bugs\n    if front_num > 0:\n        front = [\n            function(**a) if use_kwargs else function(a) for a in array[:front_num]\n        ]\n    else:\n        front = []\n    # If we set n_jobs to 1, just run a list comprehension. This is useful for benchmarking and debugging.\n    if n_jobs == 1:\n        return front + [\n            function(**a) if use_kwargs else function(a)\n            for a in tqdm(array[front_num:])\n        ]\n    # Assemble the workers\n    with ProcessPoolExecutor(max_workers=n_jobs) as pool:\n        # Pass the elements of array into function\n        if use_kwargs:\n            futures = [pool.submit(function, **a) for a in array[front_num:]]\n        else:\n            futures = [pool.submit(function, a) for a in array[front_num:]]\n        kwargs = {\n            \"total\": len(futures),\n            \"unit\": \"it\",\n            \"unit_scale\": True,\n            \"leave\": True,\n        }\n        # Print out the progress as tasks complete\n        for f in tqdm(as_completed(futures), **kwargs):\n            pass\n    out = []\n    # Get the results from the futures.\n    for i, future in tqdm(enumerate(futures)):\n        try:\n            out.append(future.result())\n        except Exception as e:\n            out.append(e)\n    return front + out\n", "ppstructure/table/table_metric/table_metric.py": "# Copyright 2020 IBM\n# Author: peter.zhong@au1.ibm.com\n#\n# This is free software; you can redistribute it and/or modify\n# it under the terms of the Apache 2.0 License.\n#\n# This software is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# Apache 2.0 License for more details.\n\nfrom rapidfuzz.distance import Levenshtein\nfrom apted import APTED, Config\nfrom apted.helpers import Tree\nfrom collections import deque\nfrom .parallel import parallel_process\nfrom tqdm import tqdm\nfrom paddle.utils import try_import\n\n\nclass TableTree(Tree):\n    def __init__(self, tag, colspan=None, rowspan=None, content=None, *children):\n        self.tag = tag\n        self.colspan = colspan\n        self.rowspan = rowspan\n        self.content = content\n        self.children = list(children)\n\n    def bracket(self):\n        \"\"\"Show tree using brackets notation\"\"\"\n        if self.tag == \"td\":\n            result = '\"tag\": %s, \"colspan\": %d, \"rowspan\": %d, \"text\": %s' % (\n                self.tag,\n                self.colspan,\n                self.rowspan,\n                self.content,\n            )\n        else:\n            result = '\"tag\": %s' % self.tag\n        for child in self.children:\n            result += child.bracket()\n        return \"{{{}}}\".format(result)\n\n\nclass CustomConfig(Config):\n    def rename(self, node1, node2):\n        \"\"\"Compares attributes of trees\"\"\"\n        # print(node1.tag)\n        if (\n            (node1.tag != node2.tag)\n            or (node1.colspan != node2.colspan)\n            or (node1.rowspan != node2.rowspan)\n        ):\n            return 1.0\n        if node1.tag == \"td\":\n            if node1.content or node2.content:\n                # print(node1.content, )\n                return Levenshtein.normalized_distance(node1.content, node2.content)\n        return 0.0\n\n\nclass CustomConfig_del_short(Config):\n    def rename(self, node1, node2):\n        \"\"\"Compares attributes of trees\"\"\"\n        if (\n            (node1.tag != node2.tag)\n            or (node1.colspan != node2.colspan)\n            or (node1.rowspan != node2.rowspan)\n        ):\n            return 1.0\n        if node1.tag == \"td\":\n            if node1.content or node2.content:\n                # print('before')\n                # print(node1.content, node2.content)\n                # print('after')\n                node1_content = node1.content\n                node2_content = node2.content\n                if len(node1_content) < 3:\n                    node1_content = [\"####\"]\n                if len(node2_content) < 3:\n                    node2_content = [\"####\"]\n                return Levenshtein.normalized_distance(node1_content, node2_content)\n        return 0.0\n\n\nclass CustomConfig_del_block(Config):\n    def rename(self, node1, node2):\n        \"\"\"Compares attributes of trees\"\"\"\n        if (\n            (node1.tag != node2.tag)\n            or (node1.colspan != node2.colspan)\n            or (node1.rowspan != node2.rowspan)\n        ):\n            return 1.0\n        if node1.tag == \"td\":\n            if node1.content or node2.content:\n                node1_content = node1.content\n                node2_content = node2.content\n                while \" \" in node1_content:\n                    print(node1_content.index(\" \"))\n                    node1_content.pop(node1_content.index(\" \"))\n                while \" \" in node2_content:\n                    print(node2_content.index(\" \"))\n                    node2_content.pop(node2_content.index(\" \"))\n                return Levenshtein.normalized_distance(node1_content, node2_content)\n        return 0.0\n\n\nclass TEDS(object):\n    \"\"\"Tree Edit Distance basead Similarity\"\"\"\n\n    def __init__(self, structure_only=False, n_jobs=1, ignore_nodes=None):\n        assert isinstance(n_jobs, int) and (\n            n_jobs >= 1\n        ), \"n_jobs must be an integer greather than 1\"\n        self.structure_only = structure_only\n        self.n_jobs = n_jobs\n        self.ignore_nodes = ignore_nodes\n        self.__tokens__ = []\n\n    def tokenize(self, node):\n        \"\"\"Tokenizes table cells\"\"\"\n        self.__tokens__.append(\"<%s>\" % node.tag)\n        if node.text is not None:\n            self.__tokens__ += list(node.text)\n        for n in node.getchildren():\n            self.tokenize(n)\n        if node.tag != \"unk\":\n            self.__tokens__.append(\"</%s>\" % node.tag)\n        if node.tag != \"td\" and node.tail is not None:\n            self.__tokens__ += list(node.tail)\n\n    def load_html_tree(self, node, parent=None):\n        \"\"\"Converts HTML tree to the format required by apted\"\"\"\n        global __tokens__\n        if node.tag == \"td\":\n            if self.structure_only:\n                cell = []\n            else:\n                self.__tokens__ = []\n                self.tokenize(node)\n                cell = self.__tokens__[1:-1].copy()\n            new_node = TableTree(\n                node.tag,\n                int(node.attrib.get(\"colspan\", \"1\")),\n                int(node.attrib.get(\"rowspan\", \"1\")),\n                cell,\n                *deque(),\n            )\n        else:\n            new_node = TableTree(node.tag, None, None, None, *deque())\n        if parent is not None:\n            parent.children.append(new_node)\n        if node.tag != \"td\":\n            for n in node.getchildren():\n                self.load_html_tree(n, new_node)\n        if parent is None:\n            return new_node\n\n    def evaluate(self, pred, true):\n        \"\"\"Computes TEDS score between the prediction and the ground truth of a\n        given sample\n        \"\"\"\n        try_import(\"lxml\")\n        from lxml import etree, html\n\n        if (not pred) or (not true):\n            return 0.0\n        parser = html.HTMLParser(remove_comments=True, encoding=\"utf-8\")\n        pred = html.fromstring(pred, parser=parser)\n        true = html.fromstring(true, parser=parser)\n        if pred.xpath(\"body/table\") and true.xpath(\"body/table\"):\n            pred = pred.xpath(\"body/table\")[0]\n            true = true.xpath(\"body/table\")[0]\n            if self.ignore_nodes:\n                etree.strip_tags(pred, *self.ignore_nodes)\n                etree.strip_tags(true, *self.ignore_nodes)\n            n_nodes_pred = len(pred.xpath(\".//*\"))\n            n_nodes_true = len(true.xpath(\".//*\"))\n            n_nodes = max(n_nodes_pred, n_nodes_true)\n            tree_pred = self.load_html_tree(pred)\n            tree_true = self.load_html_tree(true)\n            distance = APTED(\n                tree_pred, tree_true, CustomConfig()\n            ).compute_edit_distance()\n            return 1.0 - (float(distance) / n_nodes)\n        else:\n            return 0.0\n\n    def batch_evaluate(self, pred_json, true_json):\n        \"\"\"Computes TEDS score between the prediction and the ground truth of\n        a batch of samples\n        @params pred_json: {'FILENAME': 'HTML CODE', ...}\n        @params true_json: {'FILENAME': {'html': 'HTML CODE'}, ...}\n        @output: {'FILENAME': 'TEDS SCORE', ...}\n        \"\"\"\n        samples = true_json.keys()\n        if self.n_jobs == 1:\n            scores = [\n                self.evaluate(pred_json.get(filename, \"\"), true_json[filename][\"html\"])\n                for filename in tqdm(samples)\n            ]\n        else:\n            inputs = [\n                {\n                    \"pred\": pred_json.get(filename, \"\"),\n                    \"true\": true_json[filename][\"html\"],\n                }\n                for filename in samples\n            ]\n            scores = parallel_process(\n                inputs, self.evaluate, use_kwargs=True, n_jobs=self.n_jobs, front_num=1\n            )\n        scores = dict(zip(samples, scores))\n        return scores\n\n    def batch_evaluate_html(self, pred_htmls, true_htmls):\n        \"\"\"Computes TEDS score between the prediction and the ground truth of\n        a batch of samples\n        \"\"\"\n        if self.n_jobs == 1:\n            scores = [\n                self.evaluate(pred_html, true_html)\n                for (pred_html, true_html) in zip(pred_htmls, true_htmls)\n            ]\n        else:\n            inputs = [\n                {\"pred\": pred_html, \"true\": true_html}\n                for (pred_html, true_html) in zip(pred_htmls, true_htmls)\n            ]\n\n            scores = parallel_process(\n                inputs, self.evaluate, use_kwargs=True, n_jobs=self.n_jobs, front_num=1\n            )\n        return scores\n\n\nif __name__ == \"__main__\":\n    import json\n    import pprint\n\n    with open(\"sample_pred.json\") as fp:\n        pred_json = json.load(fp)\n    with open(\"sample_gt.json\") as fp:\n        true_json = json.load(fp)\n    teds = TEDS(n_jobs=4)\n    scores = teds.batch_evaluate(pred_json, true_json)\n    pp = pprint.PrettyPrinter()\n    pp.pprint(scores)\n", "ppstructure/table/table_metric/__init__.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n__all__ = [\"TEDS\"]\nfrom .table_metric import TEDS\n", "ppstructure/table/tablepyxl/tablepyxl.py": "# Do imports like python3 so our package works for 2 and 3\nfrom __future__ import absolute_import\n\n\nfrom tablepyxl.style import Table\nfrom paddle.utils import try_import\n\n\ndef string_to_int(s):\n    if s.isdigit():\n        return int(s)\n    return 0\n\n\ndef get_Tables(doc):\n    try_import(\"lxml\")\n    from lxml import etree, html\n\n    tree = html.fromstring(doc)\n    comments = tree.xpath(\"//comment()\")\n    for comment in comments:\n        comment.drop_tag()\n    return [Table(table) for table in tree.xpath(\"//table\")]\n\n\ndef write_rows(worksheet, elem, row, column=1):\n    \"\"\"\n    Writes every tr child element of elem to a row in the worksheet\n    returns the next row after all rows are written\n    \"\"\"\n    try_import(\"openpyxl\")\n    from openpyxl.cell.cell import MergedCell\n    from openpyxl.utils import get_column_letter\n\n    initial_column = column\n    for table_row in elem.rows:\n        for table_cell in table_row.cells:\n            cell = worksheet.cell(row=row, column=column)\n            while isinstance(cell, MergedCell):\n                column += 1\n                cell = worksheet.cell(row=row, column=column)\n\n            colspan = string_to_int(table_cell.element.get(\"colspan\", \"1\"))\n            rowspan = string_to_int(table_cell.element.get(\"rowspan\", \"1\"))\n            if rowspan > 1 or colspan > 1:\n                worksheet.merge_cells(\n                    start_row=row,\n                    start_column=column,\n                    end_row=row + rowspan - 1,\n                    end_column=column + colspan - 1,\n                )\n\n            cell.value = table_cell.value\n            table_cell.format(cell)\n            min_width = table_cell.get_dimension(\"min-width\")\n            max_width = table_cell.get_dimension(\"max-width\")\n\n            if colspan == 1:\n                # Initially, when iterating for the first time through the loop, the width of all the cells is None.\n                # As we start filling in contents, the initial width of the cell (which can be retrieved by:\n                # worksheet.column_dimensions[get_column_letter(column)].width) is equal to the width of the previous\n                # cell in the same column (i.e. width of A2 = width of A1)\n                width = max(\n                    worksheet.column_dimensions[get_column_letter(column)].width or 0,\n                    len(table_cell.value) + 2,\n                )\n                if max_width and width > max_width:\n                    width = max_width\n                elif min_width and width < min_width:\n                    width = min_width\n                worksheet.column_dimensions[get_column_letter(column)].width = width\n            column += colspan\n        row += 1\n        column = initial_column\n    return row\n\n\ndef table_to_sheet(table, wb):\n    \"\"\"\n    Takes a table and workbook and writes the table to a new sheet.\n    The sheet title will be the same as the table attribute name.\n    \"\"\"\n    ws = wb.create_sheet(title=table.element.get(\"name\"))\n    insert_table(table, ws, 1, 1)\n\n\ndef document_to_workbook(doc, wb=None, base_url=None):\n    \"\"\"\n    Takes a string representation of an html document and writes one sheet for\n    every table in the document.\n    The workbook is returned\n    \"\"\"\n    try_import(\"premailer\")\n    try_import(\"openpyxl\")\n    from premailer import Premailer\n    from openpyxl import Workbook\n\n    if not wb:\n        wb = Workbook()\n        wb.remove(wb.active)\n\n    inline_styles_doc = Premailer(\n        doc, base_url=base_url, remove_classes=False\n    ).transform()\n    tables = get_Tables(inline_styles_doc)\n\n    for table in tables:\n        table_to_sheet(table, wb)\n\n    return wb\n\n\ndef document_to_xl(doc, filename, base_url=None):\n    \"\"\"\n    Takes a string representation of an html document and writes one sheet for\n    every table in the document. The workbook is written out to a file called filename\n    \"\"\"\n    wb = document_to_workbook(doc, base_url=base_url)\n    wb.save(filename)\n\n\ndef insert_table(table, worksheet, column, row):\n    if table.head:\n        row = write_rows(worksheet, table.head, row, column)\n    if table.body:\n        row = write_rows(worksheet, table.body, row, column)\n\n\ndef insert_table_at_cell(table, cell):\n    \"\"\"\n    Inserts a table at the location of an openpyxl Cell object.\n    \"\"\"\n    ws = cell.parent\n    column, row = cell.column, cell.row\n    insert_table(table, ws, column, row)\n", "ppstructure/table/tablepyxl/style.py": "# This is where we handle translating css styles into openpyxl styles\n# and cascading those from parent to child in the dom.\n\ntry:\n    from openpyxl.cell import cell\n    from openpyxl.styles import (\n        Font,\n        Alignment,\n        PatternFill,\n        NamedStyle,\n        Border,\n        Side,\n        Color,\n    )\n    from openpyxl.styles.fills import FILL_SOLID\n    from openpyxl.styles.numbers import FORMAT_CURRENCY_USD_SIMPLE, FORMAT_PERCENTAGE\n    from openpyxl.styles.colors import BLACK\nexcept:\n    import warnings\n\n    warnings.warn(\n        \"Can not import openpyxl, some functions in the ppstructure may not work. Please manually install openpyxl before using ppstructure.\"\n    )\n\nFORMAT_DATE_MMDDYYYY = \"mm/dd/yyyy\"\n\n\ndef colormap(color):\n    \"\"\"\n    Convenience for looking up known colors\n    \"\"\"\n    cmap = {\"black\": BLACK}\n    return cmap.get(color, color)\n\n\ndef style_string_to_dict(style):\n    \"\"\"\n    Convert css style string to a python dictionary\n    \"\"\"\n\n    def clean_split(string, delim):\n        return (s.strip() for s in string.split(delim))\n\n    styles = [clean_split(s, \":\") for s in style.split(\";\") if \":\" in s]\n    return dict(styles)\n\n\ndef get_side(style, name):\n    return {\n        \"border_style\": style.get(\"border-{}-style\".format(name)),\n        \"color\": colormap(style.get(\"border-{}-color\".format(name))),\n    }\n\n\nknown_styles = {}\n\n\ndef style_dict_to_named_style(style_dict, number_format=None):\n    \"\"\"\n    Change css style (stored in a python dictionary) to openpyxl NamedStyle\n    \"\"\"\n\n    style_and_format_string = str(\n        {\n            \"style_dict\": style_dict,\n            \"parent\": style_dict.parent,\n            \"number_format\": number_format,\n        }\n    )\n\n    if style_and_format_string not in known_styles:\n        # Font\n        font = Font(\n            bold=style_dict.get(\"font-weight\") == \"bold\",\n            color=style_dict.get_color(\"color\", None),\n            size=style_dict.get(\"font-size\"),\n        )\n\n        # Alignment\n        alignment = Alignment(\n            horizontal=style_dict.get(\"text-align\", \"general\"),\n            vertical=style_dict.get(\"vertical-align\"),\n            wrap_text=style_dict.get(\"white-space\", \"nowrap\") == \"normal\",\n        )\n\n        # Fill\n        bg_color = style_dict.get_color(\"background-color\")\n        fg_color = style_dict.get_color(\"foreground-color\", Color())\n        fill_type = style_dict.get(\"fill-type\")\n        if bg_color and bg_color != \"transparent\":\n            fill = PatternFill(\n                fill_type=fill_type or FILL_SOLID,\n                start_color=bg_color,\n                end_color=fg_color,\n            )\n        else:\n            fill = PatternFill()\n\n        # Border\n        border = Border(\n            left=Side(**get_side(style_dict, \"left\")),\n            right=Side(**get_side(style_dict, \"right\")),\n            top=Side(**get_side(style_dict, \"top\")),\n            bottom=Side(**get_side(style_dict, \"bottom\")),\n            diagonal=Side(**get_side(style_dict, \"diagonal\")),\n            diagonal_direction=None,\n            outline=Side(**get_side(style_dict, \"outline\")),\n            vertical=None,\n            horizontal=None,\n        )\n\n        name = \"Style {}\".format(len(known_styles) + 1)\n\n        pyxl_style = NamedStyle(\n            name=name,\n            font=font,\n            fill=fill,\n            alignment=alignment,\n            border=border,\n            number_format=number_format,\n        )\n\n        known_styles[style_and_format_string] = pyxl_style\n\n    return known_styles[style_and_format_string]\n\n\nclass StyleDict(dict):\n    \"\"\"\n    It's like a dictionary, but it looks for items in the parent dictionary\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        self.parent = kwargs.pop(\"parent\", None)\n        super(StyleDict, self).__init__(*args, **kwargs)\n\n    def __getitem__(self, item):\n        if item in self:\n            return super(StyleDict, self).__getitem__(item)\n        elif self.parent:\n            return self.parent[item]\n        else:\n            raise KeyError(\"{} not found\".format(item))\n\n    def __hash__(self):\n        return hash(tuple([(k, self.get(k)) for k in self._keys()]))\n\n    # Yielding the keys avoids creating unnecessary data structures\n    # and happily works with both python2 and python3 where the\n    # .keys() method is a dictionary_view in python3 and a list in python2.\n    def _keys(self):\n        yielded = set()\n        for k in self.keys():\n            yielded.add(k)\n            yield k\n        if self.parent:\n            for k in self.parent._keys():\n                if k not in yielded:\n                    yielded.add(k)\n                    yield k\n\n    def get(self, k, d=None):\n        try:\n            return self[k]\n        except KeyError:\n            return d\n\n    def get_color(self, k, d=None):\n        \"\"\"\n        Strip leading # off colors if necessary\n        \"\"\"\n        color = self.get(k, d)\n        if hasattr(color, \"startswith\") and color.startswith(\"#\"):\n            color = color[1:]\n            if (\n                len(color) == 3\n            ):  # Premailers reduces colors like #00ff00 to #0f0, openpyxl doesn't like that\n                color = \"\".join(2 * c for c in color)\n        return color\n\n\nclass Element(object):\n    \"\"\"\n    Our base class for representing an html element along with a cascading style.\n    The element is created along with a parent so that the StyleDict that we store\n    can point to the parent's StyleDict.\n    \"\"\"\n\n    def __init__(self, element, parent=None):\n        self.element = element\n        self.number_format = None\n        parent_style = parent.style_dict if parent else None\n        self.style_dict = StyleDict(\n            style_string_to_dict(element.get(\"style\", \"\")), parent=parent_style\n        )\n        self._style_cache = None\n\n    def style(self):\n        \"\"\"\n        Turn the css styles for this element into an openpyxl NamedStyle.\n        \"\"\"\n        if not self._style_cache:\n            self._style_cache = style_dict_to_named_style(\n                self.style_dict, number_format=self.number_format\n            )\n        return self._style_cache\n\n    def get_dimension(self, dimension_key):\n        \"\"\"\n        Extracts the dimension from the style dict of the Element and returns it as a float.\n        \"\"\"\n        dimension = self.style_dict.get(dimension_key)\n        if dimension:\n            if dimension[-2:] in [\"px\", \"em\", \"pt\", \"in\", \"cm\"]:\n                dimension = dimension[:-2]\n            dimension = float(dimension)\n        return dimension\n\n\nclass Table(Element):\n    \"\"\"\n    The concrete implementations of Elements are semantically named for the types of elements we are interested in.\n    This defines a very concrete tree structure for html tables that we expect to deal with. I prefer this compared to\n    allowing Element to have an arbitrary number of children and dealing with an abstract element tree.\n    \"\"\"\n\n    def __init__(self, table):\n        \"\"\"\n        takes an html table object (from lxml)\n        \"\"\"\n        super(Table, self).__init__(table)\n        table_head = table.find(\"thead\")\n        self.head = (\n            TableHead(table_head, parent=self) if table_head is not None else None\n        )\n        table_body = table.find(\"tbody\")\n        self.body = TableBody(\n            table_body if table_body is not None else table, parent=self\n        )\n\n\nclass TableHead(Element):\n    \"\"\"\n    This class maps to the `<th>` element of the html table.\n    \"\"\"\n\n    def __init__(self, head, parent=None):\n        super(TableHead, self).__init__(head, parent=parent)\n        self.rows = [TableRow(tr, parent=self) for tr in head.findall(\"tr\")]\n\n\nclass TableBody(Element):\n    \"\"\"\n    This class maps to the `<tbody>` element of the html table.\n    \"\"\"\n\n    def __init__(self, body, parent=None):\n        super(TableBody, self).__init__(body, parent=parent)\n        self.rows = [TableRow(tr, parent=self) for tr in body.findall(\"tr\")]\n\n\nclass TableRow(Element):\n    \"\"\"\n    This class maps to the `<tr>` element of the html table.\n    \"\"\"\n\n    def __init__(self, tr, parent=None):\n        super(TableRow, self).__init__(tr, parent=parent)\n        self.cells = [\n            TableCell(cell, parent=self) for cell in tr.findall(\"th\") + tr.findall(\"td\")\n        ]\n\n\ndef element_to_string(el):\n    return _element_to_string(el).strip()\n\n\ndef _element_to_string(el):\n    string = \"\"\n\n    for x in el.iterchildren():\n        string += \"\\n\" + _element_to_string(x)\n\n    text = el.text.strip() if el.text else \"\"\n    tail = el.tail.strip() if el.tail else \"\"\n\n    return text + string + \"\\n\" + tail\n\n\nclass TableCell(Element):\n    \"\"\"\n    This class maps to the `<td>` element of the html table.\n    \"\"\"\n\n    CELL_TYPES = {\n        \"TYPE_STRING\",\n        \"TYPE_FORMULA\",\n        \"TYPE_NUMERIC\",\n        \"TYPE_BOOL\",\n        \"TYPE_CURRENCY\",\n        \"TYPE_PERCENTAGE\",\n        \"TYPE_NULL\",\n        \"TYPE_INLINE\",\n        \"TYPE_ERROR\",\n        \"TYPE_FORMULA_CACHE_STRING\",\n        \"TYPE_INTEGER\",\n    }\n\n    def __init__(self, cell, parent=None):\n        super(TableCell, self).__init__(cell, parent=parent)\n        self.value = element_to_string(cell)\n        self.number_format = self.get_number_format()\n\n    def data_type(self):\n        cell_types = self.CELL_TYPES & set(self.element.get(\"class\", \"\").split())\n        if cell_types:\n            if \"TYPE_FORMULA\" in cell_types:\n                # Make sure TYPE_FORMULA takes precedence over the other classes in the set.\n                cell_type = \"TYPE_FORMULA\"\n            elif cell_types & {\"TYPE_CURRENCY\", \"TYPE_INTEGER\", \"TYPE_PERCENTAGE\"}:\n                cell_type = \"TYPE_NUMERIC\"\n            else:\n                cell_type = cell_types.pop()\n        else:\n            cell_type = \"TYPE_STRING\"\n        return getattr(cell, cell_type)\n\n    def get_number_format(self):\n        if \"TYPE_CURRENCY\" in self.element.get(\"class\", \"\").split():\n            return FORMAT_CURRENCY_USD_SIMPLE\n        if \"TYPE_INTEGER\" in self.element.get(\"class\", \"\").split():\n            return \"#,##0\"\n        if \"TYPE_PERCENTAGE\" in self.element.get(\"class\", \"\").split():\n            return FORMAT_PERCENTAGE\n        if \"TYPE_DATE\" in self.element.get(\"class\", \"\").split():\n            return FORMAT_DATE_MMDDYYYY\n        if self.data_type() == cell.TYPE_NUMERIC:\n            try:\n                int(self.value)\n            except ValueError:\n                return \"#,##0.##\"\n            else:\n                return \"#,##0\"\n\n    def format(self, cell):\n        cell.style = self.style()\n        data_type = self.data_type()\n        if data_type:\n            cell.data_type = data_type\n", "ppstructure/table/tablepyxl/__init__.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n", "ppstructure/kie/predict_kie_token_ser_re.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\nimport sys\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(__dir__)\nsys.path.insert(0, os.path.abspath(os.path.join(__dir__, \"../..\")))\n\nos.environ[\"FLAGS_allocator_strategy\"] = \"auto_growth\"\n\nimport cv2\nimport json\nimport numpy as np\nimport time\n\nimport tools.infer.utility as utility\nfrom tools.infer_kie_token_ser_re import make_input\nfrom ppocr.postprocess import build_post_process\nfrom ppocr.utils.logging import get_logger\nfrom ppocr.utils.visual import draw_ser_results, draw_re_results\nfrom ppocr.utils.utility import get_image_file_list, check_and_read\nfrom ppstructure.utility import parse_args\nfrom ppstructure.kie.predict_kie_token_ser import SerPredictor\n\nlogger = get_logger()\n\n\nclass SerRePredictor(object):\n    def __init__(self, args):\n        self.use_visual_backbone = args.use_visual_backbone\n        self.ser_engine = SerPredictor(args)\n        if args.re_model_dir is not None:\n            postprocess_params = {\"name\": \"VQAReTokenLayoutLMPostProcess\"}\n            self.postprocess_op = build_post_process(postprocess_params)\n            (\n                self.predictor,\n                self.input_tensor,\n                self.output_tensors,\n                self.config,\n            ) = utility.create_predictor(args, \"re\", logger)\n        else:\n            self.predictor = None\n\n    def __call__(self, img):\n        starttime = time.time()\n        ser_results, ser_inputs, ser_elapse = self.ser_engine(img)\n        if self.predictor is None:\n            return ser_results, ser_elapse\n\n        re_input, entity_idx_dict_batch = make_input(ser_inputs, ser_results)\n        if self.use_visual_backbone == False:\n            re_input.pop(4)\n        for idx in range(len(self.input_tensor)):\n            self.input_tensor[idx].copy_from_cpu(re_input[idx])\n\n        self.predictor.run()\n        outputs = []\n        for output_tensor in self.output_tensors:\n            output = output_tensor.copy_to_cpu()\n            outputs.append(output)\n        preds = dict(\n            loss=outputs[1],\n            pred_relations=outputs[2],\n            hidden_states=outputs[0],\n        )\n\n        post_result = self.postprocess_op(\n            preds, ser_results=ser_results, entity_idx_dict_batch=entity_idx_dict_batch\n        )\n\n        elapse = time.time() - starttime\n        return post_result, elapse\n\n\ndef main(args):\n    image_file_list = get_image_file_list(args.image_dir)\n    ser_re_predictor = SerRePredictor(args)\n    count = 0\n    total_time = 0\n\n    os.makedirs(args.output, exist_ok=True)\n    with open(\n        os.path.join(args.output, \"infer.txt\"), mode=\"w\", encoding=\"utf-8\"\n    ) as f_w:\n        for image_file in image_file_list:\n            img, flag, _ = check_and_read(image_file)\n            if not flag:\n                img = cv2.imread(image_file)\n                img = img[:, :, ::-1]\n            if img is None:\n                logger.info(\"error in loading image:{}\".format(image_file))\n                continue\n            re_res, elapse = ser_re_predictor(img)\n            re_res = re_res[0]\n\n            res_str = \"{}\\t{}\\n\".format(\n                image_file,\n                json.dumps(\n                    {\n                        \"ocr_info\": re_res,\n                    },\n                    ensure_ascii=False,\n                ),\n            )\n            f_w.write(res_str)\n            if ser_re_predictor.predictor is not None:\n                img_res = draw_re_results(\n                    image_file, re_res, font_path=args.vis_font_path\n                )\n                img_save_path = os.path.join(\n                    args.output,\n                    os.path.splitext(os.path.basename(image_file))[0] + \"_ser_re.jpg\",\n                )\n            else:\n                img_res = draw_ser_results(\n                    image_file, re_res, font_path=args.vis_font_path\n                )\n                img_save_path = os.path.join(\n                    args.output,\n                    os.path.splitext(os.path.basename(image_file))[0] + \"_ser.jpg\",\n                )\n\n            cv2.imwrite(img_save_path, img_res)\n            logger.info(\"save vis result to {}\".format(img_save_path))\n            if count > 0:\n                total_time += elapse\n            count += 1\n            logger.info(\"Predict time of {}: {}\".format(image_file, elapse))\n\n\nif __name__ == \"__main__\":\n    main(parse_args())\n", "ppstructure/kie/predict_kie_token_ser.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\nimport sys\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(__dir__)\nsys.path.insert(0, os.path.abspath(os.path.join(__dir__, \"../..\")))\n\nos.environ[\"FLAGS_allocator_strategy\"] = \"auto_growth\"\n\nimport cv2\nimport json\nimport numpy as np\nimport time\n\nimport tools.infer.utility as utility\nfrom ppocr.data import create_operators, transform\nfrom ppocr.postprocess import build_post_process\nfrom ppocr.utils.logging import get_logger\nfrom ppocr.utils.visual import draw_ser_results\nfrom ppocr.utils.utility import get_image_file_list, check_and_read\nfrom ppstructure.utility import parse_args\n\nfrom paddleocr import PaddleOCR\n\nlogger = get_logger()\n\n\nclass SerPredictor(object):\n    def __init__(self, args):\n        self.ocr_engine = PaddleOCR(\n            use_angle_cls=args.use_angle_cls,\n            det_model_dir=args.det_model_dir,\n            rec_model_dir=args.rec_model_dir,\n            show_log=False,\n            use_gpu=args.use_gpu,\n        )\n\n        pre_process_list = [\n            {\n                \"VQATokenLabelEncode\": {\n                    \"algorithm\": args.kie_algorithm,\n                    \"class_path\": args.ser_dict_path,\n                    \"contains_re\": False,\n                    \"ocr_engine\": self.ocr_engine,\n                    \"order_method\": args.ocr_order_method,\n                }\n            },\n            {\"VQATokenPad\": {\"max_seq_len\": 512, \"return_attention_mask\": True}},\n            {\"VQASerTokenChunk\": {\"max_seq_len\": 512, \"return_attention_mask\": True}},\n            {\"Resize\": {\"size\": [224, 224]}},\n            {\n                \"NormalizeImage\": {\n                    \"std\": [58.395, 57.12, 57.375],\n                    \"mean\": [123.675, 116.28, 103.53],\n                    \"scale\": \"1\",\n                    \"order\": \"hwc\",\n                }\n            },\n            {\"ToCHWImage\": None},\n            {\n                \"KeepKeys\": {\n                    \"keep_keys\": [\n                        \"input_ids\",\n                        \"bbox\",\n                        \"attention_mask\",\n                        \"token_type_ids\",\n                        \"image\",\n                        \"labels\",\n                        \"segment_offset_id\",\n                        \"ocr_info\",\n                        \"entities\",\n                    ]\n                }\n            },\n        ]\n        postprocess_params = {\n            \"name\": \"VQASerTokenLayoutLMPostProcess\",\n            \"class_path\": args.ser_dict_path,\n        }\n\n        self.preprocess_op = create_operators(pre_process_list, {\"infer_mode\": True})\n        self.postprocess_op = build_post_process(postprocess_params)\n        (\n            self.predictor,\n            self.input_tensor,\n            self.output_tensors,\n            self.config,\n        ) = utility.create_predictor(args, \"ser\", logger)\n\n    def __call__(self, img):\n        ori_im = img.copy()\n        data = {\"image\": img}\n        data = transform(data, self.preprocess_op)\n        if data[0] is None:\n            return None, 0\n        starttime = time.time()\n\n        for idx in range(len(data)):\n            if isinstance(data[idx], np.ndarray):\n                data[idx] = np.expand_dims(data[idx], axis=0)\n            else:\n                data[idx] = [data[idx]]\n\n        for idx in range(len(self.input_tensor)):\n            self.input_tensor[idx].copy_from_cpu(data[idx])\n\n        self.predictor.run()\n\n        outputs = []\n        for output_tensor in self.output_tensors:\n            output = output_tensor.copy_to_cpu()\n            outputs.append(output)\n        preds = outputs[0]\n\n        post_result = self.postprocess_op(\n            preds, segment_offset_ids=data[6], ocr_infos=data[7]\n        )\n        elapse = time.time() - starttime\n        return post_result, data, elapse\n\n\ndef main(args):\n    image_file_list = get_image_file_list(args.image_dir)\n    ser_predictor = SerPredictor(args)\n    count = 0\n    total_time = 0\n\n    os.makedirs(args.output, exist_ok=True)\n    with open(\n        os.path.join(args.output, \"infer.txt\"), mode=\"w\", encoding=\"utf-8\"\n    ) as f_w:\n        for image_file in image_file_list:\n            img, flag, _ = check_and_read(image_file)\n            if not flag:\n                img = cv2.imread(image_file)\n                img = img[:, :, ::-1]\n            if img is None:\n                logger.info(\"error in loading image:{}\".format(image_file))\n                continue\n            ser_res, _, elapse = ser_predictor(img)\n            ser_res = ser_res[0]\n\n            res_str = \"{}\\t{}\\n\".format(\n                image_file,\n                json.dumps(\n                    {\n                        \"ocr_info\": ser_res,\n                    },\n                    ensure_ascii=False,\n                ),\n            )\n            f_w.write(res_str)\n\n            img_res = draw_ser_results(\n                image_file,\n                ser_res,\n                font_path=args.vis_font_path,\n            )\n\n            img_save_path = os.path.join(args.output, os.path.basename(image_file))\n            cv2.imwrite(img_save_path, img_res)\n            logger.info(\"save vis result to {}\".format(img_save_path))\n            if count > 0:\n                total_time += elapse\n            count += 1\n            logger.info(\"Predict time of {}: {}\".format(image_file, elapse))\n\n\nif __name__ == \"__main__\":\n    main(parse_args())\n", "ppstructure/kie/tools/trans_funsd_label.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport os\nimport sys\nimport cv2\nimport numpy as np\nfrom copy import deepcopy\n\n\ndef trans_poly_to_bbox(poly):\n    x1 = np.min([p[0] for p in poly])\n    x2 = np.max([p[0] for p in poly])\n    y1 = np.min([p[1] for p in poly])\n    y2 = np.max([p[1] for p in poly])\n    return [x1, y1, x2, y2]\n\n\ndef get_outer_poly(bbox_list):\n    x1 = min([bbox[0] for bbox in bbox_list])\n    y1 = min([bbox[1] for bbox in bbox_list])\n    x2 = max([bbox[2] for bbox in bbox_list])\n    y2 = max([bbox[3] for bbox in bbox_list])\n    return [[x1, y1], [x2, y1], [x2, y2], [x1, y2]]\n\n\ndef load_funsd_label(image_dir, anno_dir):\n    imgs = os.listdir(image_dir)\n    annos = os.listdir(anno_dir)\n\n    imgs = [img.replace(\".png\", \"\") for img in imgs]\n    annos = [anno.replace(\".json\", \"\") for anno in annos]\n\n    fn_info_map = dict()\n    for anno_fn in annos:\n        res = []\n        with open(os.path.join(anno_dir, anno_fn + \".json\"), \"r\") as fin:\n            infos = json.load(fin)\n            infos = infos[\"form\"]\n            old_id2new_id_map = dict()\n            global_new_id = 0\n            for info in infos:\n                if info[\"text\"] is None:\n                    continue\n                words = info[\"words\"]\n                if len(words) <= 0:\n                    continue\n                word_idx = 1\n                curr_bboxes = [words[0][\"box\"]]\n                curr_texts = [words[0][\"text\"]]\n                while word_idx < len(words):\n                    # switch to a new link\n                    if words[word_idx][\"box\"][0] + 10 <= words[word_idx - 1][\"box\"][2]:\n                        if len(\"\".join(curr_texts[0])) > 0:\n                            res.append(\n                                {\n                                    \"transcription\": \" \".join(curr_texts),\n                                    \"label\": info[\"label\"],\n                                    \"points\": get_outer_poly(curr_bboxes),\n                                    \"linking\": info[\"linking\"],\n                                    \"id\": global_new_id,\n                                }\n                            )\n                            if info[\"id\"] not in old_id2new_id_map:\n                                old_id2new_id_map[info[\"id\"]] = []\n                            old_id2new_id_map[info[\"id\"]].append(global_new_id)\n                            global_new_id += 1\n                        curr_bboxes = [words[word_idx][\"box\"]]\n                        curr_texts = [words[word_idx][\"text\"]]\n                    else:\n                        curr_bboxes.append(words[word_idx][\"box\"])\n                        curr_texts.append(words[word_idx][\"text\"])\n                    word_idx += 1\n                if len(\"\".join(curr_texts[0])) > 0:\n                    res.append(\n                        {\n                            \"transcription\": \" \".join(curr_texts),\n                            \"label\": info[\"label\"],\n                            \"points\": get_outer_poly(curr_bboxes),\n                            \"linking\": info[\"linking\"],\n                            \"id\": global_new_id,\n                        }\n                    )\n                    if info[\"id\"] not in old_id2new_id_map:\n                        old_id2new_id_map[info[\"id\"]] = []\n                    old_id2new_id_map[info[\"id\"]].append(global_new_id)\n                    global_new_id += 1\n            res = sorted(res, key=lambda r: (r[\"points\"][0][1], r[\"points\"][0][0]))\n            for i in range(len(res) - 1):\n                for j in range(i, 0, -1):\n                    if abs(\n                        res[j + 1][\"points\"][0][1] - res[j][\"points\"][0][1]\n                    ) < 20 and (res[j + 1][\"points\"][0][0] < res[j][\"points\"][0][0]):\n                        tmp = deepcopy(res[j])\n                        res[j] = deepcopy(res[j + 1])\n                        res[j + 1] = deepcopy(tmp)\n                    else:\n                        break\n            # re-generate unique ids\n            for idx, r in enumerate(res):\n                new_links = []\n                for link in r[\"linking\"]:\n                    # illegal links will be removed\n                    if (\n                        link[0] not in old_id2new_id_map\n                        or link[1] not in old_id2new_id_map\n                    ):\n                        continue\n                    for src in old_id2new_id_map[link[0]]:\n                        for dst in old_id2new_id_map[link[1]]:\n                            new_links.append([src, dst])\n                res[idx][\"linking\"] = deepcopy(new_links)\n\n            fn_info_map[anno_fn] = res\n\n    return fn_info_map\n\n\ndef main():\n    test_image_dir = \"train_data/FUNSD/testing_data/images/\"\n    test_anno_dir = \"train_data/FUNSD/testing_data/annotations/\"\n    test_output_dir = \"train_data/FUNSD/test.json\"\n\n    fn_info_map = load_funsd_label(test_image_dir, test_anno_dir)\n    with open(test_output_dir, \"w\") as fout:\n        for fn in fn_info_map:\n            fout.write(\n                fn\n                + \".png\"\n                + \"\\t\"\n                + json.dumps(fn_info_map[fn], ensure_ascii=False)\n                + \"\\n\"\n            )\n\n    train_image_dir = \"train_data/FUNSD/training_data/images/\"\n    train_anno_dir = \"train_data/FUNSD/training_data/annotations/\"\n    train_output_dir = \"train_data/FUNSD/train.json\"\n\n    fn_info_map = load_funsd_label(train_image_dir, train_anno_dir)\n    with open(train_output_dir, \"w\") as fout:\n        for fn in fn_info_map:\n            fout.write(\n                fn\n                + \".png\"\n                + \"\\t\"\n                + json.dumps(fn_info_map[fn], ensure_ascii=False)\n                + \"\\n\"\n            )\n    print(\"====ok====\")\n    return\n\n\nif __name__ == \"__main__\":\n    main()\n", "ppstructure/kie/tools/eval_with_label_end2end.py": "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport re\nimport sys\nimport shapely\nfrom shapely.geometry import Polygon\nimport numpy as np\nfrom collections import defaultdict\nimport operator\nfrom rapidfuzz.distance import Levenshtein\nimport argparse\nimport json\nimport copy\n\n\ndef parse_ser_results_fp(fp, fp_type=\"gt\", ignore_background=True):\n    # img/zh_val_0.jpg        {\n    #     \"height\": 3508,\n    #     \"width\": 2480,\n    #     \"ocr_info\": [\n    #         {\"text\": \"Maribyrnong\", \"label\": \"other\", \"bbox\": [1958, 144, 2184, 198]},\n    #         {\"text\": \"CITYCOUNCIL\", \"label\": \"other\", \"bbox\": [2052, 183, 2171, 214]},\n    #     ]\n    assert fp_type in [\"gt\", \"pred\"]\n    key = \"label\" if fp_type == \"gt\" else \"pred\"\n    res_dict = dict()\n    with open(fp, \"r\", encoding=\"utf-8\") as fin:\n        lines = fin.readlines()\n\n    for _, line in enumerate(lines):\n        img_path, info = line.strip().split(\"\\t\")\n        # get key\n        image_name = os.path.basename(img_path)\n        res_dict[image_name] = []\n        # get infos\n        json_info = json.loads(info)\n        for single_ocr_info in json_info[\"ocr_info\"]:\n            label = single_ocr_info[key].upper()\n            if label in [\"O\", \"OTHERS\", \"OTHER\"]:\n                label = \"O\"\n            if ignore_background and label == \"O\":\n                continue\n            single_ocr_info[\"label\"] = label\n            res_dict[image_name].append(copy.deepcopy(single_ocr_info))\n    return res_dict\n\n\ndef polygon_from_str(polygon_points):\n    \"\"\"\n    Create a shapely polygon object from gt or dt line.\n    \"\"\"\n    polygon_points = np.array(polygon_points).reshape(4, 2)\n    polygon = Polygon(polygon_points).convex_hull\n    return polygon\n\n\ndef polygon_iou(poly1, poly2):\n    \"\"\"\n    Intersection over union between two shapely polygons.\n    \"\"\"\n    if not poly1.intersects(poly2):  # this test is fast and can accelerate calculation\n        iou = 0\n    else:\n        try:\n            inter_area = poly1.intersection(poly2).area\n            union_area = poly1.area + poly2.area - inter_area\n            iou = float(inter_area) / union_area\n        except shapely.geos.TopologicalError:\n            # except Exception as e:\n            #     print(e)\n            print(\"shapely.geos.TopologicalError occurred, iou set to 0\")\n            iou = 0\n    return iou\n\n\ndef ed(args, str1, str2):\n    if args.ignore_space:\n        str1 = str1.replace(\" \", \"\")\n        str2 = str2.replace(\" \", \"\")\n    if args.ignore_case:\n        str1 = str1.lower()\n        str2 = str2.lower()\n    return Levenshtein.distance(str1, str2)\n\n\ndef convert_bbox_to_polygon(bbox):\n    \"\"\"\n    bbox  : [x1, y1, x2, y2]\n    output: [[x1, y1], [x2, y2], [x3, y3], [x4, y4]]\n    \"\"\"\n    xmin, ymin, xmax, ymax = bbox\n    poly = [[xmin, ymin], [xmax, ymin], [xmax, ymax], [xmin, ymax]]\n    return poly\n\n\ndef eval_e2e(args):\n    # gt\n    gt_results = parse_ser_results_fp(args.gt_json_path, \"gt\", args.ignore_background)\n    # pred\n    dt_results = parse_ser_results_fp(\n        args.pred_json_path, \"pred\", args.ignore_background\n    )\n    iou_thresh = args.iou_thres\n    num_gt_chars = 0\n    gt_count = 0\n    dt_count = 0\n    hit = 0\n    ed_sum = 0\n\n    for img_name in dt_results:\n        gt_info = gt_results[img_name]\n        gt_count += len(gt_info)\n\n        dt_info = dt_results[img_name]\n        dt_count += len(dt_info)\n\n        dt_match = [False] * len(dt_info)\n        gt_match = [False] * len(gt_info)\n\n        all_ious = defaultdict(tuple)\n        # gt: {text, label, bbox or poly}\n        for index_gt, gt in enumerate(gt_info):\n            if \"poly\" not in gt:\n                gt[\"poly\"] = convert_bbox_to_polygon(gt[\"bbox\"])\n            gt_poly = polygon_from_str(gt[\"poly\"])\n            for index_dt, dt in enumerate(dt_info):\n                if \"poly\" not in dt:\n                    dt[\"poly\"] = convert_bbox_to_polygon(dt[\"bbox\"])\n                dt_poly = polygon_from_str(dt[\"poly\"])\n                iou = polygon_iou(dt_poly, gt_poly)\n                if iou >= iou_thresh:\n                    all_ious[(index_gt, index_dt)] = iou\n        sorted_ious = sorted(all_ious.items(), key=operator.itemgetter(1), reverse=True)\n        sorted_gt_dt_pairs = [item[0] for item in sorted_ious]\n\n        # matched gt and dt\n        for gt_dt_pair in sorted_gt_dt_pairs:\n            index_gt, index_dt = gt_dt_pair\n            if gt_match[index_gt] == False and dt_match[index_dt] == False:\n                gt_match[index_gt] = True\n                dt_match[index_dt] = True\n                # ocr rec results\n                gt_text = gt_info[index_gt][\"text\"]\n                dt_text = dt_info[index_dt][\"text\"]\n\n                # ser results\n                gt_label = gt_info[index_gt][\"label\"]\n                dt_label = dt_info[index_dt][\"pred\"]\n\n                if True:  # ignore_masks[index_gt] == '0':\n                    ed_sum += ed(args, gt_text, dt_text)\n                    num_gt_chars += len(gt_text)\n                    if gt_text == dt_text:\n                        if args.ignore_ser_prediction or gt_label == dt_label:\n                            hit += 1\n\n        # unmatched dt\n        for tindex, dt_match_flag in enumerate(dt_match):\n            if dt_match_flag == False:\n                dt_text = dt_info[tindex][\"text\"]\n                gt_text = \"\"\n                ed_sum += ed(args, dt_text, gt_text)\n\n        # unmatched gt\n        for tindex, gt_match_flag in enumerate(gt_match):\n            if gt_match_flag == False:\n                dt_text = \"\"\n                gt_text = gt_info[tindex][\"text\"]\n                ed_sum += ed(args, gt_text, dt_text)\n                num_gt_chars += len(gt_text)\n\n    eps = 1e-9\n    print(\"config: \", args)\n    print(\"hit, dt_count, gt_count\", hit, dt_count, gt_count)\n    precision = hit / (dt_count + eps)\n    recall = hit / (gt_count + eps)\n    fmeasure = 2.0 * precision * recall / (precision + recall + eps)\n    avg_edit_dist_img = ed_sum / len(gt_results)\n    avg_edit_dist_field = ed_sum / (gt_count + eps)\n    character_acc = 1 - ed_sum / (num_gt_chars + eps)\n\n    print(\"character_acc: %.2f\" % (character_acc * 100) + \"%\")\n    print(\"avg_edit_dist_field: %.2f\" % (avg_edit_dist_field))\n    print(\"avg_edit_dist_img: %.2f\" % (avg_edit_dist_img))\n    print(\"precision: %.2f\" % (precision * 100) + \"%\")\n    print(\"recall: %.2f\" % (recall * 100) + \"%\")\n    print(\"fmeasure: %.2f\" % (fmeasure * 100) + \"%\")\n\n    return\n\n\ndef parse_args():\n    \"\"\" \"\"\"\n\n    def str2bool(v):\n        return v.lower() in (\"true\", \"t\", \"1\")\n\n    parser = argparse.ArgumentParser()\n    ## Required parameters\n    parser.add_argument(\n        \"--gt_json_path\",\n        default=None,\n        type=str,\n        required=True,\n    )\n    parser.add_argument(\n        \"--pred_json_path\",\n        default=None,\n        type=str,\n        required=True,\n    )\n\n    parser.add_argument(\"--iou_thres\", default=0.5, type=float)\n\n    parser.add_argument(\n        \"--ignore_case\",\n        default=False,\n        type=str2bool,\n        help=\"whether to do lower case for the strs\",\n    )\n\n    parser.add_argument(\n        \"--ignore_space\", default=True, type=str2bool, help=\"whether to ignore space\"\n    )\n\n    parser.add_argument(\n        \"--ignore_background\",\n        default=True,\n        type=str2bool,\n        help=\"whether to ignore other label\",\n    )\n\n    parser.add_argument(\n        \"--ignore_ser_prediction\",\n        default=False,\n        type=str2bool,\n        help=\"whether to ignore ocr pred results\",\n    )\n\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    eval_e2e(args)\n", "ppstructure/pdf2word/pdf2word.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\nimport tarfile\nimport os\nimport time\nimport datetime\nimport functools\nimport cv2\nimport platform\nimport numpy as np\nfrom paddle.utils import try_import\n\nfitz = try_import(\"fitz\")\nfrom PIL import Image\nfrom qtpy.QtWidgets import (\n    QApplication,\n    QWidget,\n    QPushButton,\n    QProgressBar,\n    QGridLayout,\n    QMessageBox,\n    QLabel,\n    QFileDialog,\n    QCheckBox,\n)\nfrom qtpy.QtCore import Signal, QThread, QObject\nfrom qtpy.QtGui import QImage, QPixmap, QIcon\n\nfile = os.path.dirname(os.path.abspath(__file__))\nroot = os.path.abspath(os.path.join(file, \"../../\"))\nsys.path.append(file)\nsys.path.insert(0, root)\n\nfrom ppstructure.predict_system import StructureSystem, save_structure_res\nfrom ppstructure.utility import parse_args, draw_structure_result\nfrom ppocr.utils.network import download_with_progressbar\nfrom ppstructure.recovery.recovery_to_doc import sorted_layout_boxes, convert_info_docx\n\n# from ScreenShotWidget import ScreenShotWidget\n\n__APPNAME__ = \"pdf2word\"\n__VERSION__ = \"0.2.2\"\n\nURLs_EN = {\n    # \u4e0b\u8f7d\u8d85\u82f1\u6587\u8f7b\u91cf\u7ea7PP-OCRv3\u6a21\u578b\u7684\u68c0\u6d4b\u6a21\u578b\u5e76\u89e3\u538b\n    \"en_PP-OCRv3_det_infer\": \"https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_det_infer.tar\",\n    # \u4e0b\u8f7d\u82f1\u6587\u8f7b\u91cf\u7ea7PP-OCRv3\u6a21\u578b\u7684\u8bc6\u522b\u6a21\u578b\u5e76\u89e3\u538b\n    \"en_PP-OCRv3_rec_infer\": \"https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_rec_infer.tar\",\n    # \u4e0b\u8f7d\u8d85\u8f7b\u91cf\u7ea7\u82f1\u6587\u8868\u683c\u82f1\u6587\u6a21\u578b\u5e76\u89e3\u538b\n    \"en_ppstructure_mobile_v2.0_SLANet_infer\": \"https://paddleocr.bj.bcebos.com/ppstructure/models/slanet/en_ppstructure_mobile_v2.0_SLANet_infer.tar\",\n    # \u82f1\u6587\u7248\u9762\u5206\u6790\u6a21\u578b\n    \"picodet_lcnet_x1_0_fgd_layout_infer\": \"https://paddleocr.bj.bcebos.com/ppstructure/models/layout/picodet_lcnet_x1_0_fgd_layout_infer.tar\",\n}\nDICT_EN = {\n    \"rec_char_dict_path\": \"en_dict.txt\",\n    \"layout_dict_path\": \"layout_publaynet_dict.txt\",\n}\n\nURLs_CN = {\n    # \u4e0b\u8f7d\u8d85\u4e2d\u6587\u8f7b\u91cf\u7ea7PP-OCRv3\u6a21\u578b\u7684\u68c0\u6d4b\u6a21\u578b\u5e76\u89e3\u538b\n    \"cn_PP-OCRv3_det_infer\": \"https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_infer.tar\",\n    # \u4e0b\u8f7d\u4e2d\u6587\u8f7b\u91cf\u7ea7PP-OCRv3\u6a21\u578b\u7684\u8bc6\u522b\u6a21\u578b\u5e76\u89e3\u538b\n    \"cn_PP-OCRv3_rec_infer\": \"https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_rec_infer.tar\",\n    # \u4e0b\u8f7d\u8d85\u8f7b\u91cf\u7ea7\u82f1\u6587\u8868\u683c\u82f1\u6587\u6a21\u578b\u5e76\u89e3\u538b\n    \"cn_ppstructure_mobile_v2.0_SLANet_infer\": \"https://paddleocr.bj.bcebos.com/ppstructure/models/slanet/en_ppstructure_mobile_v2.0_SLANet_infer.tar\",\n    # \u4e2d\u6587\u7248\u9762\u5206\u6790\u6a21\u578b\n    \"picodet_lcnet_x1_0_fgd_layout_cdla_infer\": \"https://paddleocr.bj.bcebos.com/ppstructure/models/layout/picodet_lcnet_x1_0_fgd_layout_cdla_infer.tar\",\n}\nDICT_CN = {\n    \"rec_char_dict_path\": \"ppocr_keys_v1.txt\",\n    \"layout_dict_path\": \"layout_cdla_dict.txt\",\n}\n\n\ndef QImageToCvMat(incomingImage) -> np.array:\n    \"\"\"\n    Converts a QImage into an opencv MAT format\n    \"\"\"\n\n    incomingImage = incomingImage.convertToFormat(QImage.Format.Format_RGBA8888)\n\n    width = incomingImage.width()\n    height = incomingImage.height()\n\n    ptr = incomingImage.bits()\n    ptr.setsize(height * width * 4)\n    arr = np.frombuffer(ptr, np.uint8).reshape((height, width, 4))\n    return arr\n\n\ndef readImage(image_file) -> list:\n    if os.path.basename(image_file)[-3:] == \"pdf\":\n        imgs = []\n        with fitz.open(image_file) as pdf:\n            for pg in range(0, pdf.pageCount):\n                page = pdf[pg]\n                mat = fitz.Matrix(2, 2)\n                pm = page.getPixmap(matrix=mat, alpha=False)\n\n                # if width or height > 2000 pixels, don't enlarge the image\n                if pm.width > 2000 or pm.height > 2000:\n                    pm = page.getPixmap(matrix=fitz.Matrix(1, 1), alpha=False)\n\n                img = Image.frombytes(\"RGB\", [pm.width, pm.height], pm.samples)\n                img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n                imgs.append(img)\n    else:\n        img = cv2.imread(image_file, cv2.IMREAD_COLOR)\n        if img is not None:\n            imgs = [img]\n\n    return imgs\n\n\nclass Worker(QThread):\n    progressBarValue = Signal(int)\n    progressBarRange = Signal(int)\n    endsignal = Signal()\n    exceptedsignal = Signal(str)  # \u53d1\u9001\u4e00\u4e2a\u5f02\u5e38\u4fe1\u53f7\n    loopFlag = True\n\n    def __init__(self, predictors, save_pdf, vis_font_path, use_pdf2docx_api):\n        super(Worker, self).__init__()\n        self.predictors = predictors\n        self.save_pdf = save_pdf\n        self.vis_font_path = vis_font_path\n        self.lang = \"EN\"\n        self.imagePaths = []\n        self.use_pdf2docx_api = use_pdf2docx_api\n        self.outputDir = None\n        self.totalPageCnt = 0\n        self.pageCnt = 0\n        self.setStackSize(1024 * 1024)\n\n    def setImagePath(self, imagePaths):\n        self.imagePaths = imagePaths\n\n    def setLang(self, lang):\n        self.lang = lang\n\n    def setOutputDir(self, outputDir):\n        self.outputDir = outputDir\n\n    def setPDFParser(self, enabled):\n        self.use_pdf2docx_api = enabled\n\n    def resetPageCnt(self):\n        self.pageCnt = 0\n\n    def resetTotalPageCnt(self):\n        self.totalPageCnt = 0\n\n    def ppocrPrecitor(self, imgs, img_name):\n        all_res = []\n        # update progress bar ranges\n        self.totalPageCnt += len(imgs)\n        self.progressBarRange.emit(self.totalPageCnt)\n        # processing pages\n        for index, img in enumerate(imgs):\n            res, time_dict = self.predictors[self.lang](img)\n\n            # save output\n            save_structure_res(res, self.outputDir, img_name)\n            # draw_img = draw_structure_result(img, res, self.vis_font_path)\n            # img_save_path = os.path.join(self.outputDir, img_name, 'show_{}.jpg'.format(index))\n            # if res != []:\n            #     cv2.imwrite(img_save_path, draw_img)\n\n            # recovery\n            h, w, _ = img.shape\n            res = sorted_layout_boxes(res, w)\n            all_res += res\n            self.pageCnt += 1\n            self.progressBarValue.emit(self.pageCnt)\n\n        if all_res != []:\n            try:\n                convert_info_docx(imgs, all_res, self.outputDir, img_name)\n            except Exception as ex:\n                print(\n                    \"error in layout recovery image:{}, err msg: {}\".format(\n                        img_name, ex\n                    )\n                )\n        print(\"Predict time : {:.3f}s\".format(time_dict[\"all\"]))\n        print(\"result save to {}\".format(self.outputDir))\n\n    def run(self):\n        self.resetPageCnt()\n        self.resetTotalPageCnt()\n        try:\n            os.makedirs(self.outputDir, exist_ok=True)\n            for i, image_file in enumerate(self.imagePaths):\n                if not self.loopFlag:\n                    break\n                # using use_pdf2docx_api for PDF parsing\n                if self.use_pdf2docx_api and os.path.basename(image_file)[-3:] == \"pdf\":\n                    try_import(\"pdf2docx\")\n                    from pdf2docx.converter import Converter\n\n                    self.totalPageCnt += 1\n                    self.progressBarRange.emit(self.totalPageCnt)\n                    print(\"===============using use_pdf2docx_api===============\")\n                    img_name = os.path.basename(image_file).split(\".\")[0]\n                    docx_file = os.path.join(self.outputDir, \"{}.docx\".format(img_name))\n                    cv = Converter(image_file)\n                    cv.convert(docx_file)\n                    cv.close()\n                    print(\"docx save to {}\".format(docx_file))\n                    self.pageCnt += 1\n                    self.progressBarValue.emit(self.pageCnt)\n                else:\n                    # using PPOCR for PDF/Image parsing\n                    imgs = readImage(image_file)\n                    if len(imgs) == 0:\n                        continue\n                    img_name = os.path.basename(image_file).split(\".\")[0]\n                    os.makedirs(os.path.join(self.outputDir, img_name), exist_ok=True)\n                    self.ppocrPrecitor(imgs, img_name)\n                # file processed\n            self.endsignal.emit()\n            # self.exec()\n        except Exception as e:\n            self.exceptedsignal.emit(str(e))  # \u5c06\u5f02\u5e38\u53d1\u9001\u7ed9UI\u8fdb\u7a0b\n\n\nclass APP_Image2Doc(QWidget):\n    def __init__(self):\n        super().__init__()\n        # self.setFixedHeight(100)\n        # self.setFixedWidth(520)\n\n        # settings\n        self.imagePaths = []\n        # self.screenShotWg = ScreenShotWidget()\n        self.screenShot = None\n        self.save_pdf = False\n        self.output_dir = None\n        self.vis_font_path = os.path.join(root, \"doc\", \"fonts\", \"simfang.ttf\")\n        self.use_pdf2docx_api = False\n\n        # ProgressBar\n        self.pb = QProgressBar()\n        self.pb.setRange(0, 100)\n        self.pb.setValue(0)\n\n        # \u521d\u59cb\u5316\u754c\u9762\n        self.setupUi()\n\n        # \u4e0b\u8f7d\u6a21\u578b\n        self.downloadModels(URLs_EN)\n        self.downloadModels(URLs_CN)\n\n        # \u521d\u59cb\u5316\u6a21\u578b\n        predictors = {\n            \"EN\": self.initPredictor(\"EN\"),\n            \"CN\": self.initPredictor(\"CN\"),\n        }\n\n        # \u8bbe\u7f6e\u5de5\u4f5c\u8fdb\u7a0b\n        self._thread = Worker(\n            predictors, self.save_pdf, self.vis_font_path, self.use_pdf2docx_api\n        )\n        self._thread.progressBarValue.connect(self.handleProgressBarUpdateSingal)\n        self._thread.endsignal.connect(self.handleEndsignalSignal)\n        # self._thread.finished.connect(QObject.deleteLater)\n        self._thread.progressBarRange.connect(self.handleProgressBarRangeSingal)\n        self._thread.exceptedsignal.connect(self.handleThreadException)\n        self.time_start = 0  # save start time\n\n    def setupUi(self):\n        self.setObjectName(\"MainWindow\")\n        self.setWindowTitle(__APPNAME__ + \" \" + __VERSION__)\n\n        layout = QGridLayout()\n\n        self.openFileButton = QPushButton(\"\u6253\u5f00\u6587\u4ef6\")\n        self.openFileButton.setIcon(QIcon(QPixmap(\"./icons/folder-plus.png\")))\n        layout.addWidget(self.openFileButton, 0, 0, 1, 1)\n        self.openFileButton.clicked.connect(self.handleOpenFileSignal)\n\n        # screenShotButton = QPushButton(\"\u622a\u56fe\u8bc6\u522b\")\n        # layout.addWidget(screenShotButton, 0, 1, 1, 1)\n        # screenShotButton.clicked.connect(self.screenShotSlot)\n        # screenShotButton.setEnabled(False) # temporarily disenble\n\n        self.startCNButton = QPushButton(\"\u4e2d\u6587\u8f6c\u6362\")\n        self.startCNButton.setIcon(QIcon(QPixmap(\"./icons/chinese.png\")))\n        layout.addWidget(self.startCNButton, 0, 1, 1, 1)\n        self.startCNButton.clicked.connect(\n            functools.partial(self.handleStartSignal, \"CN\", False)\n        )\n\n        self.startENButton = QPushButton(\"\u82f1\u6587\u8f6c\u6362\")\n        self.startENButton.setIcon(QIcon(QPixmap(\"./icons/english.png\")))\n        layout.addWidget(self.startENButton, 0, 2, 1, 1)\n        self.startENButton.clicked.connect(\n            functools.partial(self.handleStartSignal, \"EN\", False)\n        )\n\n        self.PDFParserButton = QPushButton(\"PDF\u89e3\u6790\", self)\n        layout.addWidget(self.PDFParserButton, 0, 3, 1, 1)\n        self.PDFParserButton.clicked.connect(\n            functools.partial(self.handleStartSignal, \"CN\", True)\n        )\n\n        self.showResultButton = QPushButton(\"\u663e\u793a\u7ed3\u679c\")\n        self.showResultButton.setIcon(QIcon(QPixmap(\"./icons/folder-open.png\")))\n        layout.addWidget(self.showResultButton, 0, 4, 1, 1)\n        self.showResultButton.clicked.connect(self.handleShowResultSignal)\n\n        # ProgressBar\n        layout.addWidget(self.pb, 2, 0, 1, 5)\n        # time estimate label\n        self.timeEstLabel = QLabel((\"Time Left: --\"))\n        layout.addWidget(self.timeEstLabel, 3, 0, 1, 5)\n\n        self.setLayout(layout)\n\n    def downloadModels(self, URLs):\n        # using custom model\n        tar_file_name_list = [\n            \"inference.pdiparams\",\n            \"inference.pdiparams.info\",\n            \"inference.pdmodel\",\n            \"model.pdiparams\",\n            \"model.pdiparams.info\",\n            \"model.pdmodel\",\n        ]\n        model_path = os.path.join(root, \"inference\")\n        os.makedirs(model_path, exist_ok=True)\n\n        # download and unzip models\n        for name in URLs.keys():\n            url = URLs[name]\n            print(\"Try downloading file: {}\".format(url))\n            tarname = url.split(\"/\")[-1]\n            tarpath = os.path.join(model_path, tarname)\n            if os.path.exists(tarpath):\n                print(\"File have already exist. skip\")\n            else:\n                try:\n                    download_with_progressbar(url, tarpath)\n                except Exception as e:\n                    print(\"Error occurred when downloading file, error message:\")\n                    print(e)\n\n            # unzip model tar\n            try:\n                with tarfile.open(tarpath, \"r\") as tarObj:\n                    storage_dir = os.path.join(model_path, name)\n                    os.makedirs(storage_dir, exist_ok=True)\n                    for member in tarObj.getmembers():\n                        filename = None\n                        for tar_file_name in tar_file_name_list:\n                            if tar_file_name in member.name:\n                                filename = tar_file_name\n                        if filename is None:\n                            continue\n                        file = tarObj.extractfile(member)\n                        with open(os.path.join(storage_dir, filename), \"wb\") as f:\n                            f.write(file.read())\n            except Exception as e:\n                print(\"Error occurred when unziping file, error message:\")\n                print(e)\n\n    def initPredictor(self, lang=\"EN\"):\n        # init predictor args\n        args = parse_args()\n        args.table_max_len = 488\n        args.ocr = True\n        args.recovery = True\n        args.save_pdf = self.save_pdf\n        args.table_char_dict_path = os.path.join(\n            root, \"ppocr\", \"utils\", \"dict\", \"table_structure_dict.txt\"\n        )\n        if lang == \"EN\":\n            args.det_model_dir = os.path.join(\n                root, \"inference\", \"en_PP-OCRv3_det_infer\"  # \u6b64\u5904\u4ece\u8fd9\u91cc\u627e\u5230\u6a21\u578b\u5b58\u653e\u4f4d\u7f6e\n            )\n            args.rec_model_dir = os.path.join(\n                root, \"inference\", \"en_PP-OCRv3_rec_infer\"\n            )\n            args.table_model_dir = os.path.join(\n                root, \"inference\", \"en_ppstructure_mobile_v2.0_SLANet_infer\"\n            )\n            args.output = os.path.join(root, \"output\")  # \u7ed3\u679c\u4fdd\u5b58\u8def\u5f84\n            args.layout_model_dir = os.path.join(\n                root, \"inference\", \"picodet_lcnet_x1_0_fgd_layout_infer\"\n            )\n            lang_dict = DICT_EN\n        elif lang == \"CN\":\n            args.det_model_dir = os.path.join(\n                root, \"inference\", \"cn_PP-OCRv3_det_infer\"  # \u6b64\u5904\u4ece\u8fd9\u91cc\u627e\u5230\u6a21\u578b\u5b58\u653e\u4f4d\u7f6e\n            )\n            args.rec_model_dir = os.path.join(\n                root, \"inference\", \"cn_PP-OCRv3_rec_infer\"\n            )\n            args.table_model_dir = os.path.join(\n                root, \"inference\", \"cn_ppstructure_mobile_v2.0_SLANet_infer\"\n            )\n            args.output = os.path.join(root, \"output\")  # \u7ed3\u679c\u4fdd\u5b58\u8def\u5f84\n            args.layout_model_dir = os.path.join(\n                root, \"inference\", \"picodet_lcnet_x1_0_fgd_layout_cdla_infer\"\n            )\n            lang_dict = DICT_CN\n        else:\n            raise ValueError(\"Unsupported language\")\n        args.rec_char_dict_path = os.path.join(\n            root, \"ppocr\", \"utils\", lang_dict[\"rec_char_dict_path\"]\n        )\n        args.layout_dict_path = os.path.join(\n            root, \"ppocr\", \"utils\", \"dict\", \"layout_dict\", lang_dict[\"layout_dict_path\"]\n        )\n        # init predictor\n        return StructureSystem(args)\n\n    def handleOpenFileSignal(self):\n        \"\"\"\n        \u53ef\u4ee5\u591a\u9009\u56fe\u50cf\u6587\u4ef6\n        \"\"\"\n        selectedFiles = QFileDialog.getOpenFileNames(\n            self, \"\u591a\u6587\u4ef6\u9009\u62e9\", \"/\", \"\u56fe\u7247\u6587\u4ef6 (*.png *.jpeg *.jpg *.bmp *.pdf)\"\n        )[0]\n        if len(selectedFiles) > 0:\n            self.imagePaths = selectedFiles\n            self.screenShot = None  # discard screenshot temp image\n            self.pb.setValue(0)\n\n    # def screenShotSlot(self):\n    #     '''\n    #     \u9009\u5b9a\u56fe\u50cf\u6587\u4ef6\u548c\u622a\u56fe\u7684\u8f6c\u6362\u8fc7\u7a0b\u53ea\u80fd\u540c\u65f6\u8fdb\u884c\u4e00\u4e2a\n    #     \u622a\u56fe\u53ea\u80fd\u540c\u65f6\u8f6c\u6362\u4e00\u4e2a\n    #     '''\n    #     self.screenShotWg.start()\n    #     if self.screenShotWg.captureImage:\n    #         self.screenShot = self.screenShotWg.captureImage\n    #         self.imagePaths.clear() # discard openfile temp list\n    #         self.pb.setRange(0, 1)\n    #         self.pb.setValue(0)\n\n    def handleStartSignal(self, lang=\"EN\", pdfParser=False):\n        if self.screenShot:  # for screenShot\n            img_name = \"screenshot_\" + time.strftime(\"%Y%m%d%H%M%S\", time.localtime())\n            image = QImageToCvMat(self.screenShot)\n            self.predictAndSave(image, img_name, lang)\n            # update Progress Bar\n            self.pb.setValue(1)\n            QMessageBox.information(self, \"Information\", \"\u6587\u6863\u63d0\u53d6\u5b8c\u6210\")\n        elif len(self.imagePaths) > 0:  # for image file selection\n            # Must set image path list and language before start\n            self.output_dir = os.path.join(\n                os.path.dirname(self.imagePaths[0]), \"output\"\n            )  # output_dir shold be same as imagepath\n            self._thread.setOutputDir(self.output_dir)\n            self._thread.setImagePath(self.imagePaths)\n            self._thread.setLang(lang)\n            self._thread.setPDFParser(pdfParser)\n            # disenble buttons\n            self.openFileButton.setEnabled(False)\n            self.startCNButton.setEnabled(False)\n            self.startENButton.setEnabled(False)\n            self.PDFParserButton.setEnabled(False)\n            # \u542f\u52a8\u5de5\u4f5c\u8fdb\u7a0b\n            self._thread.start()\n            self.time_start = time.time()  # log start time\n            QMessageBox.information(self, \"Information\", \"\u5f00\u59cb\u8f6c\u6362\")\n        else:\n            QMessageBox.warning(self, \"Information\", \"\u8bf7\u9009\u62e9\u8981\u8bc6\u522b\u7684\u6587\u4ef6\u6216\u622a\u56fe\")\n\n    def handleShowResultSignal(self):\n        if self.output_dir is None:\n            return\n        if os.path.exists(self.output_dir):\n            if platform.system() == \"Windows\":\n                os.startfile(self.output_dir)\n            else:\n                os.system(\"open \" + os.path.normpath(self.output_dir))\n        else:\n            QMessageBox.information(self, \"Information\", \"\u8f93\u51fa\u6587\u4ef6\u4e0d\u5b58\u5728\")\n\n    def handleProgressBarUpdateSingal(self, i):\n        self.pb.setValue(i)\n        # calculate time left of recognition\n        lenbar = self.pb.maximum()\n        avg_time = (\n            time.time() - self.time_start\n        ) / i  # Use average time to prevent time fluctuations\n        time_left = str(datetime.timedelta(seconds=avg_time * (lenbar - i))).split(\".\")[\n            0\n        ]  # Remove microseconds\n        self.timeEstLabel.setText(f\"Time Left: {time_left}\")  # show time left\n\n    def handleProgressBarRangeSingal(self, max):\n        self.pb.setRange(0, max)\n\n    def handleEndsignalSignal(self):\n        # enble buttons\n        self.openFileButton.setEnabled(True)\n        self.startCNButton.setEnabled(True)\n        self.startENButton.setEnabled(True)\n        self.PDFParserButton.setEnabled(True)\n        QMessageBox.information(self, \"Information\", \"\u8f6c\u6362\u7ed3\u675f\")\n\n    def handleCBChangeSignal(self):\n        self._thread.setPDFParser(self.checkBox.isChecked())\n\n    def handleThreadException(self, message):\n        self._thread.quit()\n        QMessageBox.information(self, \"Error\", message)\n\n\ndef main():\n    app = QApplication(sys.argv)\n\n    window = APP_Image2Doc()  # \u521b\u5efa\u5bf9\u8c61\n    window.show()  # \u5168\u5c4f\u663e\u793a\u7a97\u53e3\n\n    QApplication.processEvents()\n    sys.exit(app.exec())\n\n\nif __name__ == \"__main__\":\n    main()\n", "deploy/fastdeploy/cpu-gpu/python/infer_cls.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport fastdeploy as fd\nimport cv2\nimport os\n\n\ndef parse_arguments():\n    import argparse\n    import ast\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--cls_model\", required=True, help=\"Path of Classification model of PPOCR.\"\n    )\n    parser.add_argument(\n        \"--image\", type=str, required=True, help=\"Path of test image file.\"\n    )\n    parser.add_argument(\n        \"--device\",\n        type=str,\n        default=\"cpu\",\n        help=\"Type of inference device, support 'cpu', 'kunlunxin' or 'gpu'.\",\n    )\n    parser.add_argument(\n        \"--device_id\",\n        type=int,\n        default=0,\n        help=\"Define which GPU card used to run model.\",\n    )\n    return parser.parse_args()\n\n\ndef build_option(args):\n    cls_option = fd.RuntimeOption()\n\n    if args.device.lower() == \"gpu\":\n        cls_option.use_gpu(args.device_id)\n\n    return cls_option\n\n\nargs = parse_arguments()\n\ncls_model_file = os.path.join(args.cls_model, \"inference.pdmodel\")\ncls_params_file = os.path.join(args.cls_model, \"inference.pdiparams\")\n\n# Set the runtime option\ncls_option = build_option(args)\n\n# Create the cls_model\ncls_model = fd.vision.ocr.Classifier(\n    cls_model_file, cls_params_file, runtime_option=cls_option\n)\n\n# Set the postprocessing parameters\ncls_model.postprocessor.cls_thresh = 0.9\n\n# Read the image\nim = cv2.imread(args.image)\n\n# Predict and return the results\nresult = cls_model.predict(im)\n\n# User can infer a batch of images by following code.\n# result = cls_model.batch_predict([im])\n\nprint(result)\n", "deploy/fastdeploy/cpu-gpu/python/infer_rec.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport fastdeploy as fd\nimport cv2\nimport os\n\n\ndef parse_arguments():\n    import argparse\n    import ast\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--rec_model\", required=True, help=\"Path of Recognization model of PPOCR.\"\n    )\n    parser.add_argument(\n        \"--rec_label_file\", required=True, help=\"Path of Recognization model of PPOCR.\"\n    )\n    parser.add_argument(\n        \"--image\", type=str, required=True, help=\"Path of test image file.\"\n    )\n    parser.add_argument(\n        \"--device\",\n        type=str,\n        default=\"cpu\",\n        help=\"Type of inference device, support 'cpu', 'kunlunxin' or 'gpu'.\",\n    )\n    parser.add_argument(\n        \"--device_id\",\n        type=int,\n        default=0,\n        help=\"Define which GPU card used to run model.\",\n    )\n    return parser.parse_args()\n\n\ndef build_option(args):\n    rec_option = fd.RuntimeOption()\n\n    if args.device.lower() == \"gpu\":\n        rec_option.use_gpu(args.device_id)\n\n    return rec_option\n\n\nargs = parse_arguments()\n\nrec_model_file = os.path.join(args.rec_model, \"inference.pdmodel\")\nrec_params_file = os.path.join(args.rec_model, \"inference.pdiparams\")\nrec_label_file = args.rec_label_file\n\n# Set the runtime option\nrec_option = build_option(args)\n\n# Create the rec_model\nrec_model = fd.vision.ocr.Recognizer(\n    rec_model_file, rec_params_file, rec_label_file, runtime_option=rec_option\n)\n\n# Read the image\nim = cv2.imread(args.image)\n\n# Predict and return the result\nresult = rec_model.predict(im)\n\n# User can infer a batch of images by following code.\n# result = rec_model.batch_predict([im])\n\nprint(result)\n", "deploy/fastdeploy/cpu-gpu/python/infer_det.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport fastdeploy as fd\nimport cv2\nimport os\n\n\ndef parse_arguments():\n    import argparse\n    import ast\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--det_model\", required=True, help=\"Path of Detection model of PPOCR.\"\n    )\n    parser.add_argument(\n        \"--image\", type=str, required=True, help=\"Path of test image file.\"\n    )\n    parser.add_argument(\n        \"--device\",\n        type=str,\n        default=\"cpu\",\n        help=\"Type of inference device, support 'cpu', 'kunlunxin' or 'gpu'.\",\n    )\n    parser.add_argument(\n        \"--device_id\",\n        type=int,\n        default=0,\n        help=\"Define which GPU card used to run model.\",\n    )\n    return parser.parse_args()\n\n\ndef build_option(args):\n    det_option = fd.RuntimeOption()\n\n    if args.device.lower() == \"gpu\":\n        det_option.use_gpu(args.device_id)\n\n    return det_option\n\n\nargs = parse_arguments()\n\ndet_model_file = os.path.join(args.det_model, \"inference.pdmodel\")\ndet_params_file = os.path.join(args.det_model, \"inference.pdiparams\")\n\n# Set the runtime option\ndet_option = build_option(args)\n\n# Create the det_model\ndet_model = fd.vision.ocr.DBDetector(\n    det_model_file, det_params_file, runtime_option=det_option\n)\n\n# Set the preporcessing parameters\ndet_model.preprocessor.max_side_len = 960\ndet_model.postprocessor.det_db_thresh = 0.3\ndet_model.postprocessor.det_db_box_thresh = 0.6\ndet_model.postprocessor.det_db_unclip_ratio = 1.5\ndet_model.postprocessor.det_db_score_mode = \"slow\"\ndet_model.postprocessor.use_dilation = False\n\n# Read the image\nim = cv2.imread(args.image)\n\n# Predict and return the results\nresult = det_model.predict(im)\n\nprint(result)\n\n# Visualize the results\nvis_im = fd.vision.vis_ppocr(im, result)\ncv2.imwrite(\"visualized_result.jpg\", vis_im)\nprint(\"Visualized result save in ./visualized_result.jpg\")\n", "deploy/fastdeploy/cpu-gpu/python/infer.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport fastdeploy as fd\nimport cv2\nimport os\n\n\ndef parse_arguments():\n    import argparse\n    import ast\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--det_model\", required=True, help=\"Path of Detection model of PPOCR.\"\n    )\n    parser.add_argument(\n        \"--cls_model\", required=True, help=\"Path of Classification model of PPOCR.\"\n    )\n    parser.add_argument(\n        \"--rec_model\", required=True, help=\"Path of Recognization model of PPOCR.\"\n    )\n    parser.add_argument(\n        \"--rec_label_file\", required=True, help=\"Path of Recognization model of PPOCR.\"\n    )\n    parser.add_argument(\n        \"--image\", type=str, required=True, help=\"Path of test image file.\"\n    )\n    parser.add_argument(\n        \"--device\",\n        type=str,\n        default=\"cpu\",\n        help=\"Type of inference device, support 'cpu' or 'gpu'.\",\n    )\n    parser.add_argument(\n        \"--device_id\",\n        type=int,\n        default=0,\n        help=\"Define which GPU card used to run model.\",\n    )\n    parser.add_argument(\n        \"--cls_bs\",\n        type=int,\n        default=1,\n        help=\"Classification model inference batch size.\",\n    )\n    parser.add_argument(\n        \"--rec_bs\", type=int, default=6, help=\"Recognition model inference batch size\"\n    )\n    parser.add_argument(\n        \"--backend\",\n        type=str,\n        default=\"default\",\n        help=\"Type of inference backend, support ort/trt/paddle/openvino, default 'openvino' for cpu, 'tensorrt' for gpu\",\n    )\n\n    return parser.parse_args()\n\n\ndef build_option(args):\n    det_option = fd.RuntimeOption()\n    cls_option = fd.RuntimeOption()\n    rec_option = fd.RuntimeOption()\n\n    if args.device.lower() == \"gpu\":\n        det_option.use_gpu(args.device_id)\n        cls_option.use_gpu(args.device_id)\n        rec_option.use_gpu(args.device_id)\n\n    if args.backend.lower() == \"trt\":\n        assert (\n            args.device.lower() == \"gpu\"\n        ), \"TensorRT backend require inference on device GPU.\"\n        det_option.use_trt_backend()\n        cls_option.use_trt_backend()\n        rec_option.use_trt_backend()\n\n        # If use TRT backend, the dynamic shape will be set as follow.\n        # We recommend that users set the length and height of the detection model to a multiple of 32.\n        # We also recommend that users set the Trt input shape as follow.\n        det_option.set_trt_input_shape(\n            \"x\", [1, 3, 64, 64], [1, 3, 640, 640], [1, 3, 960, 960]\n        )\n        cls_option.set_trt_input_shape(\n            \"x\", [1, 3, 48, 10], [args.cls_bs, 3, 48, 320], [args.cls_bs, 3, 48, 1024]\n        )\n        rec_option.set_trt_input_shape(\n            \"x\", [1, 3, 48, 10], [args.rec_bs, 3, 48, 320], [args.rec_bs, 3, 48, 2304]\n        )\n\n        # Users could save TRT cache file to disk as follow.\n        det_option.set_trt_cache_file(args.det_model + \"/det_trt_cache.trt\")\n        cls_option.set_trt_cache_file(args.cls_model + \"/cls_trt_cache.trt\")\n        rec_option.set_trt_cache_file(args.rec_model + \"/rec_trt_cache.trt\")\n\n    elif args.backend.lower() == \"pptrt\":\n        assert (\n            args.device.lower() == \"gpu\"\n        ), \"Paddle-TensorRT backend require inference on device GPU.\"\n        det_option.use_paddle_infer_backend()\n        det_option.paddle_infer_option.collect_trt_shape = True\n        det_option.paddle_infer_option.enable_trt = True\n\n        cls_option.use_paddle_infer_backend()\n        cls_option.paddle_infer_option.collect_trt_shape = True\n        cls_option.paddle_infer_option.enable_trt = True\n\n        rec_option.use_paddle_infer_backend()\n        rec_option.paddle_infer_option.collect_trt_shape = True\n        rec_option.paddle_infer_option.enable_trt = True\n\n        # If use TRT backend, the dynamic shape will be set as follow.\n        # We recommend that users set the length and height of the detection model to a multiple of 32.\n        # We also recommend that users set the Trt input shape as follow.\n        det_option.set_trt_input_shape(\n            \"x\", [1, 3, 64, 64], [1, 3, 640, 640], [1, 3, 960, 960]\n        )\n        cls_option.set_trt_input_shape(\n            \"x\", [1, 3, 48, 10], [args.cls_bs, 3, 48, 320], [args.cls_bs, 3, 48, 1024]\n        )\n        rec_option.set_trt_input_shape(\n            \"x\", [1, 3, 48, 10], [args.rec_bs, 3, 48, 320], [args.rec_bs, 3, 48, 2304]\n        )\n\n        # Users could save TRT cache file to disk as follow.\n        det_option.set_trt_cache_file(args.det_model)\n        cls_option.set_trt_cache_file(args.cls_model)\n        rec_option.set_trt_cache_file(args.rec_model)\n\n    elif args.backend.lower() == \"ort\":\n        det_option.use_ort_backend()\n        cls_option.use_ort_backend()\n        rec_option.use_ort_backend()\n\n    elif args.backend.lower() == \"paddle\":\n        det_option.use_paddle_infer_backend()\n        cls_option.use_paddle_infer_backend()\n        rec_option.use_paddle_infer_backend()\n\n    elif args.backend.lower() == \"openvino\":\n        assert (\n            args.device.lower() == \"cpu\"\n        ), \"OpenVINO backend require inference on device CPU.\"\n        det_option.use_openvino_backend()\n        cls_option.use_openvino_backend()\n        rec_option.use_openvino_backend()\n\n    elif args.backend.lower() == \"pplite\":\n        assert (\n            args.device.lower() == \"cpu\"\n        ), \"Paddle Lite backend require inference on device CPU.\"\n        det_option.use_lite_backend()\n        cls_option.use_lite_backend()\n        rec_option.use_lite_backend()\n\n    return det_option, cls_option, rec_option\n\n\nargs = parse_arguments()\n\ndet_model_file = os.path.join(args.det_model, \"inference.pdmodel\")\ndet_params_file = os.path.join(args.det_model, \"inference.pdiparams\")\n\ncls_model_file = os.path.join(args.cls_model, \"inference.pdmodel\")\ncls_params_file = os.path.join(args.cls_model, \"inference.pdiparams\")\n\nrec_model_file = os.path.join(args.rec_model, \"inference.pdmodel\")\nrec_params_file = os.path.join(args.rec_model, \"inference.pdiparams\")\nrec_label_file = args.rec_label_file\n\ndet_option, cls_option, rec_option = build_option(args)\n\ndet_model = fd.vision.ocr.DBDetector(\n    det_model_file, det_params_file, runtime_option=det_option\n)\n\ncls_model = fd.vision.ocr.Classifier(\n    cls_model_file, cls_params_file, runtime_option=cls_option\n)\n\nrec_model = fd.vision.ocr.Recognizer(\n    rec_model_file, rec_params_file, rec_label_file, runtime_option=rec_option\n)\n\n# Parameters settings for pre and post processing of Det/Cls/Rec Models.\n# All parameters are set to default values.\ndet_model.preprocessor.max_side_len = 960\ndet_model.postprocessor.det_db_thresh = 0.3\ndet_model.postprocessor.det_db_box_thresh = 0.6\ndet_model.postprocessor.det_db_unclip_ratio = 1.5\ndet_model.postprocessor.det_db_score_mode = \"slow\"\ndet_model.postprocessor.use_dilation = False\ncls_model.postprocessor.cls_thresh = 0.9\n\n# Create PP-OCRv3, if cls_model is not needed, just set cls_model=None .\nppocr_v3 = fd.vision.ocr.PPOCRv3(\n    det_model=det_model, cls_model=cls_model, rec_model=rec_model\n)\n\n# Set inference batch size for cls model and rec model, the value could be -1 and 1 to positive infinity.\n# When inference batch size is set to -1, it means that the inference batch size\n# of the cls and rec models will be the same as the number of boxes detected by the det model.\nppocr_v3.cls_batch_size = args.cls_bs\nppocr_v3.rec_batch_size = args.rec_bs\n\n# Read the input image\nim = cv2.imread(args.image)\n\n# Predict and reutrn the results\nresult = ppocr_v3.predict(im)\n\nprint(result)\n\n# Visuliaze the results.\nvis_im = fd.vision.vis_ppocr(im, result)\ncv2.imwrite(\"visualized_result.jpg\", vis_im)\nprint(\"Visualized result save in ./visualized_result.jpg\")\n", "deploy/fastdeploy/rockchip/python/infer.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport fastdeploy as fd\nimport cv2\nimport os\n\n\ndef parse_arguments():\n    import argparse\n    import ast\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--det_model\", required=True, help=\"Path of Detection model of PPOCR.\"\n    )\n    parser.add_argument(\n        \"--cls_model\", required=True, help=\"Path of Classification model of PPOCR.\"\n    )\n    parser.add_argument(\n        \"--rec_model\", required=True, help=\"Path of Recognization model of PPOCR.\"\n    )\n    parser.add_argument(\n        \"--rec_label_file\", required=True, help=\"Path of Recognization model of PPOCR.\"\n    )\n    parser.add_argument(\n        \"--image\", type=str, required=True, help=\"Path of test image file.\"\n    )\n    parser.add_argument(\n        \"--device\",\n        type=str,\n        default=\"cpu\",\n        help=\"Type of inference device, support 'cpu', 'kunlunxin' or 'gpu'.\",\n    )\n    parser.add_argument(\n        \"--cpu_thread_num\",\n        type=int,\n        default=9,\n        help=\"Number of threads while inference on CPU.\",\n    )\n    return parser.parse_args()\n\n\ndef build_option(args):\n    det_option = fd.RuntimeOption()\n    cls_option = fd.RuntimeOption()\n    rec_option = fd.RuntimeOption()\n    if args.device == \"npu\":\n        det_option.use_rknpu2()\n        cls_option.use_rknpu2()\n        rec_option.use_rknpu2()\n\n    return det_option, cls_option, rec_option\n\n\ndef build_format(args):\n    det_format = fd.ModelFormat.ONNX\n    cls_format = fd.ModelFormat.ONNX\n    rec_format = fd.ModelFormat.ONNX\n    if args.device == \"npu\":\n        det_format = fd.ModelFormat.RKNN\n        cls_format = fd.ModelFormat.RKNN\n        rec_format = fd.ModelFormat.RKNN\n\n    return det_format, cls_format, rec_format\n\n\nargs = parse_arguments()\n\n# Detection\u6a21\u578b, \u68c0\u6d4b\u6587\u5b57\u6846\ndet_model_file = args.det_model\ndet_params_file = \"\"\n# Classification\u6a21\u578b\uff0c\u65b9\u5411\u5206\u7c7b\uff0c\u53ef\u9009\ncls_model_file = args.cls_model\ncls_params_file = \"\"\n# Recognition\u6a21\u578b\uff0c\u6587\u5b57\u8bc6\u522b\u6a21\u578b\nrec_model_file = args.rec_model\nrec_params_file = \"\"\nrec_label_file = args.rec_label_file\n\ndet_option, cls_option, rec_option = build_option(args)\ndet_format, cls_format, rec_format = build_format(args)\n\ndet_model = fd.vision.ocr.DBDetector(\n    det_model_file, det_params_file, runtime_option=det_option, model_format=det_format\n)\n\ncls_model = fd.vision.ocr.Classifier(\n    cls_model_file, cls_params_file, runtime_option=cls_option, model_format=cls_format\n)\n\nrec_model = fd.vision.ocr.Recognizer(\n    rec_model_file,\n    rec_params_file,\n    rec_label_file,\n    runtime_option=rec_option,\n    model_format=rec_format,\n)\n\n# Det,Rec\u6a21\u578b\u542f\u7528\u9759\u6001shape\u63a8\u7406\ndet_model.preprocessor.static_shape_infer = True\nrec_model.preprocessor.static_shape_infer = True\n\nif args.device == \"npu\":\n    det_model.preprocessor.disable_normalize()\n    det_model.preprocessor.disable_permute()\n    cls_model.preprocessor.disable_normalize()\n    cls_model.preprocessor.disable_permute()\n    rec_model.preprocessor.disable_normalize()\n    rec_model.preprocessor.disable_permute()\n\n# \u521b\u5efaPP-OCR\uff0c\u4e32\u80543\u4e2a\u6a21\u578b\uff0c\u5176\u4e2dcls_model\u53ef\u9009\uff0c\u5982\u65e0\u9700\u6c42\uff0c\u53ef\u8bbe\u7f6e\u4e3aNone\nppocr_v3 = fd.vision.ocr.PPOCRv3(\n    det_model=det_model, cls_model=cls_model, rec_model=rec_model\n)\n\n# Cls\u6a21\u578b\u548cRec\u6a21\u578b\u7684batch size \u5fc5\u987b\u8bbe\u7f6e\u4e3a1, \u5f00\u542f\u9759\u6001shape\u63a8\u7406\nppocr_v3.cls_batch_size = 1\nppocr_v3.rec_batch_size = 1\n\n# \u9884\u6d4b\u56fe\u7247\u51c6\u5907\nim = cv2.imread(args.image)\n\n# \u9884\u6d4b\u5e76\u6253\u5370\u7ed3\u679c\nresult = ppocr_v3.predict(im)\n\nprint(result)\n\n# \u53ef\u89c6\u5316\u7ed3\u679c\nvis_im = fd.vision.vis_ppocr(im, result)\ncv2.imwrite(\"visualized_result.jpg\", vis_im)\nprint(\"Visualized result save in ./visualized_result.jpg\")\n", "deploy/fastdeploy/rockchip/rknpu2_tools/export.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\nimport yaml\nimport argparse\nfrom rknn.api import RKNN\n\n\ndef get_config():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--verbose\", default=True, help=\"rknntoolkit verbose\")\n    parser.add_argument(\"--config_path\")\n    parser.add_argument(\"--target_platform\")\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == \"__main__\":\n    config = get_config()\n    with open(config.config_path) as file:\n        file_data = file.read()\n        yaml_config = yaml.safe_load(file_data)\n    print(yaml_config)\n    model = RKNN(config.verbose)\n\n    # Config\n    mean_values = yaml_config[\"mean\"]\n    std_values = yaml_config[\"std\"]\n    model.config(\n        mean_values=mean_values,\n        std_values=std_values,\n        target_platform=config.target_platform,\n    )\n\n    # Load ONNX model\n    if yaml_config[\"outputs_nodes\"] is None:\n        ret = model.load_onnx(model=yaml_config[\"model_path\"])\n    else:\n        ret = model.load_onnx(\n            model=yaml_config[\"model_path\"], outputs=yaml_config[\"outputs_nodes\"]\n        )\n    assert ret == 0, \"Load model failed!\"\n\n    # Build model\n    ret = model.build(\n        do_quantization=yaml_config[\"do_quantization\"], dataset=yaml_config[\"dataset\"]\n    )\n    assert ret == 0, \"Build model failed!\"\n\n    # Init Runtime\n    ret = model.init_runtime()\n    assert ret == 0, \"Init runtime environment failed!\"\n\n    # Export\n    if not os.path.exists(yaml_config[\"output_folder\"]):\n        os.mkdir(yaml_config[\"output_folder\"])\n\n    name_list = os.path.basename(yaml_config[\"model_path\"]).split(\".\")\n    model_base_name = \"\"\n    for name in name_list[0:-1]:\n        model_base_name += name\n    model_device_name = config.target_platform.lower()\n    if yaml_config[\"do_quantization\"]:\n        model_save_name = (\n            model_base_name + \"_\" + model_device_name + \"_quantized\" + \".rknn\"\n        )\n    else:\n        model_save_name = (\n            model_base_name + \"_\" + model_device_name + \"_unquantized\" + \".rknn\"\n        )\n    ret = model.export_rknn(os.path.join(yaml_config[\"output_folder\"], model_save_name))\n    assert ret == 0, \"Export rknn model failed!\"\n    print(\"Export OK!\")\n", "deploy/fastdeploy/serving/fastdeploy_serving/client.py": "import logging\nimport numpy as np\nimport time\nfrom typing import Optional\nimport cv2\nimport json\n\nfrom tritonclient import utils as client_utils\nfrom tritonclient.grpc import (\n    InferenceServerClient,\n    InferInput,\n    InferRequestedOutput,\n    service_pb2_grpc,\n    service_pb2,\n)\n\nLOGGER = logging.getLogger(\"run_inference_on_triton\")\n\n\nclass SyncGRPCTritonRunner:\n    DEFAULT_MAX_RESP_WAIT_S = 120\n\n    def __init__(\n        self,\n        server_url: str,\n        model_name: str,\n        model_version: str,\n        *,\n        verbose=False,\n        resp_wait_s: Optional[float] = None,\n    ):\n        self._server_url = server_url\n        self._model_name = model_name\n        self._model_version = model_version\n        self._verbose = verbose\n        self._response_wait_t = (\n            self.DEFAULT_MAX_RESP_WAIT_S if resp_wait_s is None else resp_wait_s\n        )\n\n        self._client = InferenceServerClient(self._server_url, verbose=self._verbose)\n        error = self._verify_triton_state(self._client)\n        if error:\n            raise RuntimeError(f\"Could not communicate to Triton Server: {error}\")\n\n        LOGGER.debug(\n            f\"Triton server {self._server_url} and model {self._model_name}:{self._model_version} \"\n            f\"are up and ready!\"\n        )\n\n        model_config = self._client.get_model_config(\n            self._model_name, self._model_version\n        )\n        model_metadata = self._client.get_model_metadata(\n            self._model_name, self._model_version\n        )\n        LOGGER.info(f\"Model config {model_config}\")\n        LOGGER.info(f\"Model metadata {model_metadata}\")\n\n        self._inputs = {tm.name: tm for tm in model_metadata.inputs}\n        self._input_names = list(self._inputs)\n        self._outputs = {tm.name: tm for tm in model_metadata.outputs}\n        self._output_names = list(self._outputs)\n        self._outputs_req = [InferRequestedOutput(name) for name in self._outputs]\n\n    def Run(self, inputs):\n        \"\"\"\n        Args:\n            inputs: list, Each value corresponds to an input name of self._input_names\n        Returns:\n            results: dict, {name : numpy.array}\n        \"\"\"\n        infer_inputs = []\n        for idx, data in enumerate(inputs):\n            infer_input = InferInput(self._input_names[idx], data.shape, \"UINT8\")\n            infer_input.set_data_from_numpy(data)\n            infer_inputs.append(infer_input)\n\n        results = self._client.infer(\n            model_name=self._model_name,\n            model_version=self._model_version,\n            inputs=infer_inputs,\n            outputs=self._outputs_req,\n            client_timeout=self._response_wait_t,\n        )\n        results = {name: results.as_numpy(name) for name in self._output_names}\n        return results\n\n    def _verify_triton_state(self, triton_client):\n        if not triton_client.is_server_live():\n            return f\"Triton server {self._server_url} is not live\"\n        elif not triton_client.is_server_ready():\n            return f\"Triton server {self._server_url} is not ready\"\n        elif not triton_client.is_model_ready(self._model_name, self._model_version):\n            return f\"Model {self._model_name}:{self._model_version} is not ready\"\n        return None\n\n\nif __name__ == \"__main__\":\n    model_name = \"pp_ocr\"\n    model_version = \"1\"\n    url = \"localhost:8001\"\n    runner = SyncGRPCTritonRunner(url, model_name, model_version)\n    im = cv2.imread(\"12.jpg\")\n    im = np.array(\n        [\n            im,\n        ]\n    )\n    for i in range(1):\n        result = runner.Run(\n            [\n                im,\n            ]\n        )\n        batch_texts = result[\"rec_texts\"]\n        batch_scores = result[\"rec_scores\"]\n        batch_bboxes = result[\"det_bboxes\"]\n        for i_batch in range(len(batch_texts)):\n            texts = batch_texts[i_batch]\n            scores = batch_scores[i_batch]\n            bboxes = batch_bboxes[i_batch]\n            for i_box in range(len(texts)):\n                print(\n                    \"text=\",\n                    texts[i_box].decode(\"utf-8\"),\n                    \"  score=\",\n                    scores[i_box],\n                    \"  bbox=\",\n                    bboxes[i_box],\n                )\n", "deploy/fastdeploy/serving/fastdeploy_serving/models/det_preprocess/1/model.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport numpy as np\nimport time\n\nimport fastdeploy as fd\n\n# triton_python_backend_utils is available in every Triton Python model. You\n# need to use this module to create inference requests and responses. It also\n# contains some utility functions for extracting information from model_config\n# and converting Triton input/output types to numpy types.\nimport triton_python_backend_utils as pb_utils\n\n\nclass TritonPythonModel:\n    \"\"\"Your Python model must use the same class name. Every Python model\n    that is created must have \"TritonPythonModel\" as the class name.\n    \"\"\"\n\n    def initialize(self, args):\n        \"\"\"`initialize` is called only once when the model is being loaded.\n        Implementing `initialize` function is optional. This function allows\n        the model to intialize any state associated with this model.\n        Parameters\n        ----------\n        args : dict\n          Both keys and values are strings. The dictionary keys and values are:\n          * model_config: A JSON string containing the model configuration\n          * model_instance_kind: A string containing model instance kind\n          * model_instance_device_id: A string containing model instance device ID\n          * model_repository: Model repository path\n          * model_version: Model version\n          * model_name: Model name\n        \"\"\"\n        # You must parse model_config. JSON string is not parsed here\n        self.model_config = json.loads(args[\"model_config\"])\n        print(\"model_config:\", self.model_config)\n\n        self.input_names = []\n        for input_config in self.model_config[\"input\"]:\n            self.input_names.append(input_config[\"name\"])\n        print(\"preprocess input names:\", self.input_names)\n\n        self.output_names = []\n        self.output_dtype = []\n        for output_config in self.model_config[\"output\"]:\n            self.output_names.append(output_config[\"name\"])\n            dtype = pb_utils.triton_string_to_numpy(output_config[\"data_type\"])\n            self.output_dtype.append(dtype)\n        print(\"preprocess output names:\", self.output_names)\n        self.preprocessor = fd.vision.ocr.DBDetectorPreprocessor()\n\n    def execute(self, requests):\n        \"\"\"`execute` must be implemented in every Python model. `execute`\n        function receives a list of pb_utils.InferenceRequest as the only\n        argument. This function is called when an inference is requested\n        for this model. Depending on the batching configuration (e.g. Dynamic\n        Batching) used, `requests` may contain multiple requests. Every\n        Python model, must create one pb_utils.InferenceResponse for every\n        pb_utils.InferenceRequest in `requests`. If there is an error, you can\n        set the error argument when creating a pb_utils.InferenceResponse.\n        Parameters\n        ----------\n        requests : list\n          A list of pb_utils.InferenceRequest\n        Returns\n        -------\n        list\n          A list of pb_utils.InferenceResponse. The length of this list must\n          be the same as `requests`\n        \"\"\"\n        responses = []\n        for request in requests:\n            data = pb_utils.get_input_tensor_by_name(request, self.input_names[0])\n            data = data.as_numpy()\n            outputs, im_infos = self.preprocessor.run(data)\n            dlpack_tensor = outputs[0].to_dlpack()\n            output_tensor_0 = pb_utils.Tensor.from_dlpack(\n                self.output_names[0], dlpack_tensor\n            )\n            output_tensor_1 = pb_utils.Tensor(\n                self.output_names[1], np.array(im_infos, dtype=np.int32)\n            )\n            inference_response = pb_utils.InferenceResponse(\n                output_tensors=[output_tensor_0, output_tensor_1]\n            )\n            responses.append(inference_response)\n        return responses\n\n    def finalize(self):\n        \"\"\"`finalize` is called only once when the model is being unloaded.\n        Implementing `finalize` function is optional. This function allows\n        the model to perform any necessary clean ups before exit.\n        \"\"\"\n        print(\"Cleaning up...\")\n", "deploy/fastdeploy/serving/fastdeploy_serving/models/cls_postprocess/1/model.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport numpy as np\nimport time\n\nimport fastdeploy as fd\n\n# triton_python_backend_utils is available in every Triton Python model. You\n# need to use this module to create inference requests and responses. It also\n# contains some utility functions for extracting information from model_config\n# and converting Triton input/output types to numpy types.\nimport triton_python_backend_utils as pb_utils\n\n\nclass TritonPythonModel:\n    \"\"\"Your Python model must use the same class name. Every Python model\n    that is created must have \"TritonPythonModel\" as the class name.\n    \"\"\"\n\n    def initialize(self, args):\n        \"\"\"`initialize` is called only once when the model is being loaded.\n        Implementing `initialize` function is optional. This function allows\n        the model to intialize any state associated with this model.\n        Parameters\n        ----------\n        args : dict\n          Both keys and values are strings. The dictionary keys and values are:\n          * model_config: A JSON string containing the model configuration\n          * model_instance_kind: A string containing model instance kind\n          * model_instance_device_id: A string containing model instance device ID\n          * model_repository: Model repository path\n          * model_version: Model version\n          * model_name: Model name\n        \"\"\"\n        # You must parse model_config. JSON string is not parsed here\n        self.model_config = json.loads(args[\"model_config\"])\n        print(\"model_config:\", self.model_config)\n\n        self.input_names = []\n        for input_config in self.model_config[\"input\"]:\n            self.input_names.append(input_config[\"name\"])\n        print(\"postprocess input names:\", self.input_names)\n\n        self.output_names = []\n        self.output_dtype = []\n        for output_config in self.model_config[\"output\"]:\n            self.output_names.append(output_config[\"name\"])\n            dtype = pb_utils.triton_string_to_numpy(output_config[\"data_type\"])\n            self.output_dtype.append(dtype)\n        print(\"postprocess output names:\", self.output_names)\n        self.postprocessor = fd.vision.ocr.ClassifierPostprocessor()\n\n    def execute(self, requests):\n        \"\"\"`execute` must be implemented in every Python model. `execute`\n        function receives a list of pb_utils.InferenceRequest as the only\n        argument. This function is called when an inference is requested\n        for this model. Depending on the batching configuration (e.g. Dynamic\n        Batching) used, `requests` may contain multiple requests. Every\n        Python model, must create one pb_utils.InferenceResponse for every\n        pb_utils.InferenceRequest in `requests`. If there is an error, you can\n        set the error argument when creating a pb_utils.InferenceResponse.\n        Parameters\n        ----------\n        requests : list\n          A list of pb_utils.InferenceRequest\n        Returns\n        -------\n        list\n          A list of pb_utils.InferenceResponse. The length of this list must\n          be the same as `requests`\n        \"\"\"\n        responses = []\n        for request in requests:\n            infer_outputs = pb_utils.get_input_tensor_by_name(\n                request, self.input_names[0]\n            )\n            infer_outputs = infer_outputs.as_numpy()\n            results = self.postprocessor.run([infer_outputs])\n            out_tensor_0 = pb_utils.Tensor(self.output_names[0], np.array(results[0]))\n            out_tensor_1 = pb_utils.Tensor(self.output_names[1], np.array(results[1]))\n            inference_response = pb_utils.InferenceResponse(\n                output_tensors=[out_tensor_0, out_tensor_1]\n            )\n            responses.append(inference_response)\n        return responses\n\n    def finalize(self):\n        \"\"\"`finalize` is called only once when the model is being unloaded.\n        Implementing `finalize` function is optional. This function allows\n        the model to perform any necessary clean ups before exit.\n        \"\"\"\n        print(\"Cleaning up...\")\n", "deploy/fastdeploy/serving/fastdeploy_serving/models/det_postprocess/1/model.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport numpy as np\nimport time\nimport math\nimport cv2\nimport fastdeploy as fd\n\n# triton_python_backend_utils is available in every Triton Python model. You\n# need to use this module to create inference requests and responses. It also\n# contains some utility functions for extracting information from model_config\n# and converting Triton input/output types to numpy types.\nimport triton_python_backend_utils as pb_utils\n\n\ndef get_rotate_crop_image(img, box):\n    \"\"\"\n    img_height, img_width = img.shape[0:2]\n    left = int(np.min(points[:, 0]))\n    right = int(np.max(points[:, 0]))\n    top = int(np.min(points[:, 1]))\n    bottom = int(np.max(points[:, 1]))\n    img_crop = img[top:bottom, left:right, :].copy()\n    points[:, 0] = points[:, 0] - left\n    points[:, 1] = points[:, 1] - top\n    \"\"\"\n    points = []\n    for i in range(4):\n        points.append([box[2 * i], box[2 * i + 1]])\n    points = np.array(points, dtype=np.float32)\n    img = img.astype(np.float32)\n    assert len(points) == 4, \"shape of points must be 4*2\"\n    img_crop_width = int(\n        max(\n            np.linalg.norm(points[0] - points[1]), np.linalg.norm(points[2] - points[3])\n        )\n    )\n    img_crop_height = int(\n        max(\n            np.linalg.norm(points[0] - points[3]), np.linalg.norm(points[1] - points[2])\n        )\n    )\n    pts_std = np.float32(\n        [\n            [0, 0],\n            [img_crop_width, 0],\n            [img_crop_width, img_crop_height],\n            [0, img_crop_height],\n        ]\n    )\n    M = cv2.getPerspectiveTransform(points, pts_std)\n    dst_img = cv2.warpPerspective(\n        img,\n        M,\n        (img_crop_width, img_crop_height),\n        borderMode=cv2.BORDER_REPLICATE,\n        flags=cv2.INTER_CUBIC,\n    )\n    dst_img_height, dst_img_width = dst_img.shape[0:2]\n    if dst_img_height * 1.0 / dst_img_width >= 1.5:\n        dst_img = np.rot90(dst_img)\n    return dst_img\n\n\nclass TritonPythonModel:\n    \"\"\"Your Python model must use the same class name. Every Python model\n    that is created must have \"TritonPythonModel\" as the class name.\n    \"\"\"\n\n    def initialize(self, args):\n        \"\"\"`initialize` is called only once when the model is being loaded.\n        Implementing `initialize` function is optional. This function allows\n        the model to intialize any state associated with this model.\n        Parameters\n        ----------\n        args : dict\n          Both keys and values are strings. The dictionary keys and values are:\n          * model_config: A JSON string containing the model configuration\n          * model_instance_kind: A string containing model instance kind\n          * model_instance_device_id: A string containing model instance device ID\n          * model_repository: Model repository path\n          * model_version: Model version\n          * model_name: Model name\n        \"\"\"\n        # You must parse model_config. JSON string is not parsed here\n        self.model_config = json.loads(args[\"model_config\"])\n        print(\"model_config:\", self.model_config)\n\n        self.input_names = []\n        for input_config in self.model_config[\"input\"]:\n            self.input_names.append(input_config[\"name\"])\n        print(\"postprocess input names:\", self.input_names)\n\n        self.output_names = []\n        self.output_dtype = []\n        for output_config in self.model_config[\"output\"]:\n            self.output_names.append(output_config[\"name\"])\n            dtype = pb_utils.triton_string_to_numpy(output_config[\"data_type\"])\n            self.output_dtype.append(dtype)\n        print(\"postprocess output names:\", self.output_names)\n        self.postprocessor = fd.vision.ocr.DBDetectorPostprocessor()\n        self.cls_preprocessor = fd.vision.ocr.ClassifierPreprocessor()\n        self.rec_preprocessor = fd.vision.ocr.RecognizerPreprocessor()\n        self.cls_threshold = 0.9\n\n    def execute(self, requests):\n        \"\"\"`execute` must be implemented in every Python model. `execute`\n        function receives a list of pb_utils.InferenceRequest as the only\n        argument. This function is called when an inference is requested\n        for this model. Depending on the batching configuration (e.g. Dynamic\n        Batching) used, `requests` may contain multiple requests. Every\n        Python model, must create one pb_utils.InferenceResponse for every\n        pb_utils.InferenceRequest in `requests`. If there is an error, you can\n        set the error argument when creating a pb_utils.InferenceResponse.\n        Parameters\n        ----------\n        requests : list\n          A list of pb_utils.InferenceRequest\n        Returns\n        -------\n        list\n          A list of pb_utils.InferenceResponse. The length of this list must\n          be the same as `requests`\n        \"\"\"\n        responses = []\n        for request in requests:\n            infer_outputs = pb_utils.get_input_tensor_by_name(\n                request, self.input_names[0]\n            )\n            im_infos = pb_utils.get_input_tensor_by_name(request, self.input_names[1])\n            ori_imgs = pb_utils.get_input_tensor_by_name(request, self.input_names[2])\n\n            infer_outputs = infer_outputs.as_numpy()\n            im_infos = im_infos.as_numpy()\n            ori_imgs = ori_imgs.as_numpy()\n\n            results = self.postprocessor.run([infer_outputs], im_infos)\n            batch_rec_texts = []\n            batch_rec_scores = []\n            batch_box_list = []\n            for i_batch in range(len(results)):\n                cls_labels = []\n                cls_scores = []\n                rec_texts = []\n                rec_scores = []\n\n                box_list = fd.vision.ocr.sort_boxes(results[i_batch])\n                image_list = []\n                if len(box_list) == 0:\n                    image_list.append(ori_imgs[i_batch])\n                else:\n                    for box in box_list:\n                        crop_img = get_rotate_crop_image(ori_imgs[i_batch], box)\n                        image_list.append(crop_img)\n\n                batch_box_list.append(box_list)\n\n                cls_pre_tensors = self.cls_preprocessor.run(image_list)\n                cls_dlpack_tensor = cls_pre_tensors[0].to_dlpack()\n                cls_input_tensor = pb_utils.Tensor.from_dlpack(\"x\", cls_dlpack_tensor)\n\n                inference_request = pb_utils.InferenceRequest(\n                    model_name=\"cls_pp\",\n                    requested_output_names=[\"cls_labels\", \"cls_scores\"],\n                    inputs=[cls_input_tensor],\n                )\n                inference_response = inference_request.exec()\n                if inference_response.has_error():\n                    raise pb_utils.TritonModelException(\n                        inference_response.error().message()\n                    )\n                else:\n                    # Extract the output tensors from the inference response.\n                    cls_labels = pb_utils.get_output_tensor_by_name(\n                        inference_response, \"cls_labels\"\n                    )\n                    cls_labels = cls_labels.as_numpy()\n\n                    cls_scores = pb_utils.get_output_tensor_by_name(\n                        inference_response, \"cls_scores\"\n                    )\n                    cls_scores = cls_scores.as_numpy()\n\n                for index in range(len(image_list)):\n                    if (\n                        cls_labels[index] == 1\n                        and cls_scores[index] > self.cls_threshold\n                    ):\n                        image_list[index] = cv2.rotate(\n                            image_list[index].astype(np.float32), 1\n                        )\n                        image_list[index] = np.astype(np.uint8)\n\n                rec_pre_tensors = self.rec_preprocessor.run(image_list)\n                rec_dlpack_tensor = rec_pre_tensors[0].to_dlpack()\n                rec_input_tensor = pb_utils.Tensor.from_dlpack(\"x\", rec_dlpack_tensor)\n\n                inference_request = pb_utils.InferenceRequest(\n                    model_name=\"rec_pp\",\n                    requested_output_names=[\"rec_texts\", \"rec_scores\"],\n                    inputs=[rec_input_tensor],\n                )\n                inference_response = inference_request.exec()\n                if inference_response.has_error():\n                    raise pb_utils.TritonModelException(\n                        inference_response.error().message()\n                    )\n                else:\n                    # Extract the output tensors from the inference response.\n                    rec_texts = pb_utils.get_output_tensor_by_name(\n                        inference_response, \"rec_texts\"\n                    )\n                    rec_texts = rec_texts.as_numpy()\n\n                    rec_scores = pb_utils.get_output_tensor_by_name(\n                        inference_response, \"rec_scores\"\n                    )\n                    rec_scores = rec_scores.as_numpy()\n\n                    batch_rec_texts.append(rec_texts)\n                    batch_rec_scores.append(rec_scores)\n\n            out_tensor_0 = pb_utils.Tensor(\n                self.output_names[0], np.array(batch_rec_texts, dtype=np.object_)\n            )\n            out_tensor_1 = pb_utils.Tensor(\n                self.output_names[1], np.array(batch_rec_scores)\n            )\n            out_tensor_2 = pb_utils.Tensor(\n                self.output_names[2], np.array(batch_box_list)\n            )\n            inference_response = pb_utils.InferenceResponse(\n                output_tensors=[out_tensor_0, out_tensor_1, out_tensor_2]\n            )\n            responses.append(inference_response)\n        return responses\n\n    def finalize(self):\n        \"\"\"`finalize` is called only once when the model is being unloaded.\n        Implementing `finalize` function is optional. This function allows\n        the model to perform any necessary clean ups before exit.\n        \"\"\"\n        print(\"Cleaning up...\")\n", "deploy/fastdeploy/serving/fastdeploy_serving/models/rec_postprocess/1/model.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport numpy as np\nimport time\nimport os\nimport sys\nimport codecs\nimport fastdeploy as fd\n\n# triton_python_backend_utils is available in every Triton Python model. You\n# need to use this module to create inference requests and responses. It also\n# contains some utility functions for extracting information from model_config\n# and converting Triton input/output types to numpy types.\nimport triton_python_backend_utils as pb_utils\n\n\nclass TritonPythonModel:\n    \"\"\"Your Python model must use the same class name. Every Python model\n    that is created must have \"TritonPythonModel\" as the class name.\n    \"\"\"\n\n    def initialize(self, args):\n        \"\"\"`initialize` is called only once when the model is being loaded.\n        Implementing `initialize` function is optional. This function allows\n        the model to intialize any state associated with this model.\n        Parameters\n        ----------\n        args : dict\n          Both keys and values are strings. The dictionary keys and values are:\n          * model_config: A JSON string containing the model configuration\n          * model_instance_kind: A string containing model instance kind\n          * model_instance_device_id: A string containing model instance device ID\n          * model_repository: Model repository path\n          * model_version: Model version\n          * model_name: Model name\n        \"\"\"\n        # You must parse model_config. JSON string is not parsed here\n        self.model_config = json.loads(args[\"model_config\"])\n        print(\"model_config:\", self.model_config)\n\n        self.input_names = []\n        for input_config in self.model_config[\"input\"]:\n            self.input_names.append(input_config[\"name\"])\n        print(\"postprocess input names:\", self.input_names)\n\n        self.output_names = []\n        self.output_dtype = []\n        for output_config in self.model_config[\"output\"]:\n            self.output_names.append(output_config[\"name\"])\n            dtype = pb_utils.triton_string_to_numpy(output_config[\"data_type\"])\n            self.output_dtype.append(dtype)\n        print(\"postprocess output names:\", self.output_names)\n\n        dir_name = os.path.dirname(os.path.realpath(__file__)) + \"/\"\n        file_name = dir_name + \"ppocr_keys_v1.txt\"\n        # self.label_list = load_dict()\n        self.postprocessor = fd.vision.ocr.RecognizerPostprocessor(file_name)\n\n    def execute(self, requests):\n        \"\"\"`execute` must be implemented in every Python model. `execute`\n        function receives a list of pb_utils.InferenceRequest as the only\n        argument. This function is called when an inference is requested\n        for this model. Depending on the batching configuration (e.g. Dynamic\n        Batching) used, `requests` may contain multiple requests. Every\n        Python model, must create one pb_utils.InferenceResponse for every\n        pb_utils.InferenceRequest in `requests`. If there is an error, you can\n        set the error argument when creating a pb_utils.InferenceResponse.\n        Parameters\n        ----------\n        requests : list\n          A list of pb_utils.InferenceRequest\n        Returns\n        -------\n        list\n          A list of pb_utils.InferenceResponse. The length of this list must\n          be the same as `requests`\n        \"\"\"\n        responses = []\n        for request in requests:\n            infer_outputs = pb_utils.get_input_tensor_by_name(\n                request, self.input_names[0]\n            )\n            infer_outputs = infer_outputs.as_numpy()\n            results = self.postprocessor.run([infer_outputs])\n            out_tensor_0 = pb_utils.Tensor(\n                self.output_names[0], np.array(results[0], dtype=np.object_)\n            )\n            out_tensor_1 = pb_utils.Tensor(self.output_names[1], np.array(results[1]))\n            inference_response = pb_utils.InferenceResponse(\n                output_tensors=[out_tensor_0, out_tensor_1]\n            )\n            responses.append(inference_response)\n        return responses\n\n    def finalize(self):\n        \"\"\"`finalize` is called only once when the model is being unloaded.\n        Implementing `finalize` function is optional. This function allows\n        the model to perform any necessary clean ups before exit.\n        \"\"\"\n        print(\"Cleaning up...\")\n", "deploy/fastdeploy/serving/simple_serving/client.py": "import requests\nimport json\nimport cv2\nimport fastdeploy as fd\nfrom fastdeploy.serving.utils import cv2_to_base64\n\nif __name__ == \"__main__\":\n    url = \"http://127.0.0.1:8000/fd/ppocrv3\"\n    headers = {\"Content-Type\": \"application/json\"}\n\n    im = cv2.imread(\"12.jpg\")\n    data = {\"data\": {\"image\": cv2_to_base64(im)}, \"parameters\": {}}\n\n    resp = requests.post(url=url, headers=headers, data=json.dumps(data))\n    if resp.status_code == 200:\n        r_json = json.loads(resp.json()[\"result\"])\n        print(r_json)\n        ocr_result = fd.vision.utils.json_to_ocr(r_json)\n        vis_im = fd.vision.vis_ppocr(im, ocr_result)\n        cv2.imwrite(\"visualized_result.jpg\", vis_im)\n        print(\"Visualized result save in ./visualized_result.jpg\")\n    else:\n        print(\"Error code:\", resp.status_code)\n        print(resp.text)\n", "deploy/fastdeploy/serving/simple_serving/server.py": "import fastdeploy as fd\nfrom fastdeploy.serving.server import SimpleServer\nimport os\nimport logging\n\nlogging.getLogger().setLevel(logging.INFO)\n\n# Configurations\ndet_model_dir = \"ch_PP-OCRv3_det_infer\"\ncls_model_dir = \"ch_ppocr_mobile_v2.0_cls_infer\"\nrec_model_dir = \"ch_PP-OCRv3_rec_infer\"\nrec_label_file = \"ppocr_keys_v1.txt\"\ndevice = \"cpu\"\n# backend: ['paddle', 'trt'], you can also use other backends, but need to modify\n# the runtime option below\nbackend = \"paddle\"\n\n# Prepare models\n# Detection model\ndet_model_file = os.path.join(det_model_dir, \"inference.pdmodel\")\ndet_params_file = os.path.join(det_model_dir, \"inference.pdiparams\")\n# Classification model\ncls_model_file = os.path.join(cls_model_dir, \"inference.pdmodel\")\ncls_params_file = os.path.join(cls_model_dir, \"inference.pdiparams\")\n# Recognition model\nrec_model_file = os.path.join(rec_model_dir, \"inference.pdmodel\")\nrec_params_file = os.path.join(rec_model_dir, \"inference.pdiparams\")\n\n# Setup runtime option to select hardware, backend, etc.\noption = fd.RuntimeOption()\nif device.lower() == \"gpu\":\n    option.use_gpu()\nif backend == \"trt\":\n    option.use_trt_backend()\nelse:\n    option.use_paddle_infer_backend()\n\ndet_option = option\ndet_option.set_trt_input_shape(\"x\", [1, 3, 64, 64], [1, 3, 640, 640], [1, 3, 960, 960])\n\n# det_option.set_trt_cache_file(\"det_trt_cache.trt\")\nprint(det_model_file, det_params_file)\ndet_model = fd.vision.ocr.DBDetector(\n    det_model_file, det_params_file, runtime_option=det_option\n)\n\ncls_batch_size = 1\nrec_batch_size = 6\n\ncls_option = option\ncls_option.set_trt_input_shape(\n    \"x\", [1, 3, 48, 10], [cls_batch_size, 3, 48, 320], [cls_batch_size, 3, 48, 1024]\n)\n\n# cls_option.set_trt_cache_file(\"cls_trt_cache.trt\")\ncls_model = fd.vision.ocr.Classifier(\n    cls_model_file, cls_params_file, runtime_option=cls_option\n)\n\nrec_option = option\nrec_option.set_trt_input_shape(\n    \"x\", [1, 3, 48, 10], [rec_batch_size, 3, 48, 320], [rec_batch_size, 3, 48, 2304]\n)\n\n# rec_option.set_trt_cache_file(\"rec_trt_cache.trt\")\nrec_model = fd.vision.ocr.Recognizer(\n    rec_model_file, rec_params_file, rec_label_file, runtime_option=rec_option\n)\n\n# Create PPOCRv3 pipeline\nppocr_v3 = fd.vision.ocr.PPOCRv3(\n    det_model=det_model, cls_model=cls_model, rec_model=rec_model\n)\n\nppocr_v3.cls_batch_size = cls_batch_size\nppocr_v3.rec_batch_size = rec_batch_size\n\n# Create server, setup REST API\napp = SimpleServer()\napp.register(\n    task_name=\"fd/ppocrv3\",\n    model_handler=fd.serving.handler.VisionModelHandler,\n    predictor=ppocr_v3,\n)\n", "deploy/fastdeploy/ascend/python/infer.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport fastdeploy as fd\nimport cv2\nimport os\n\n\ndef parse_arguments():\n    import argparse\n    import ast\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--det_model\", required=True, help=\"Path of Detection model of PPOCR.\"\n    )\n    parser.add_argument(\n        \"--cls_model\", required=True, help=\"Path of Classification model of PPOCR.\"\n    )\n    parser.add_argument(\n        \"--rec_model\", required=True, help=\"Path of Recognization model of PPOCR.\"\n    )\n    parser.add_argument(\n        \"--rec_label_file\", required=True, help=\"Path of Recognization model of PPOCR.\"\n    )\n    parser.add_argument(\n        \"--image\", type=str, required=True, help=\"Path of test image file.\"\n    )\n    return parser.parse_args()\n\n\ndef build_option(args):\n    det_option = fd.RuntimeOption()\n    cls_option = fd.RuntimeOption()\n    rec_option = fd.RuntimeOption()\n\n    det_option.use_ascend()\n    cls_option.use_ascend()\n    rec_option.use_ascend()\n\n    return det_option, cls_option, rec_option\n\n\nargs = parse_arguments()\n\ndet_model_file = os.path.join(args.det_model, \"inference.pdmodel\")\ndet_params_file = os.path.join(args.det_model, \"inference.pdiparams\")\n\ncls_model_file = os.path.join(args.cls_model, \"inference.pdmodel\")\ncls_params_file = os.path.join(args.cls_model, \"inference.pdiparams\")\n\nrec_model_file = os.path.join(args.rec_model, \"inference.pdmodel\")\nrec_params_file = os.path.join(args.rec_model, \"inference.pdiparams\")\nrec_label_file = args.rec_label_file\n\ndet_option, cls_option, rec_option = build_option(args)\n\ndet_model = fd.vision.ocr.DBDetector(\n    det_model_file, det_params_file, runtime_option=det_option\n)\n\ncls_model = fd.vision.ocr.Classifier(\n    cls_model_file, cls_params_file, runtime_option=cls_option\n)\n\nrec_model = fd.vision.ocr.Recognizer(\n    rec_model_file, rec_params_file, rec_label_file, runtime_option=rec_option\n)\n\n# Rec model enable static shape infer.\n# When deploy on Ascend, it must be true.\nrec_model.preprocessor.static_shape_infer = True\n\n# Create PP-OCRv3, if cls_model is not needed,\n# just set cls_model=None .\nppocr_v3 = fd.vision.ocr.PPOCRv3(\n    det_model=det_model, cls_model=cls_model, rec_model=rec_model\n)\n\n# The batch size must be set to 1, when enable static shape infer.\nppocr_v3.cls_batch_size = 1\nppocr_v3.rec_batch_size = 1\n\n# Prepare image.\nim = cv2.imread(args.image)\n\n# Print the results.\nresult = ppocr_v3.predict(im)\n\nprint(result)\n\n# Visuliaze the output.\nvis_im = fd.vision.vis_ppocr(im, result)\ncv2.imwrite(\"visualized_result.jpg\", vis_im)\nprint(\"Visualized result save in ./visualized_result.jpg\")\n", "deploy/fastdeploy/kunlunxin/python/infer.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport fastdeploy as fd\nimport cv2\nimport os\n\n\ndef parse_arguments():\n    import argparse\n    import ast\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--det_model\", required=True, help=\"Path of Detection model of PPOCR.\"\n    )\n    parser.add_argument(\n        \"--cls_model\", required=True, help=\"Path of Classification model of PPOCR.\"\n    )\n    parser.add_argument(\n        \"--rec_model\", required=True, help=\"Path of Recognization model of PPOCR.\"\n    )\n    parser.add_argument(\n        \"--rec_label_file\", required=True, help=\"Path of Recognization model of PPOCR.\"\n    )\n    parser.add_argument(\n        \"--image\", type=str, required=True, help=\"Path of test image file.\"\n    )\n    parser.add_argument(\n        \"--cls_bs\",\n        type=int,\n        default=1,\n        help=\"Classification model inference batch size.\",\n    )\n    parser.add_argument(\n        \"--rec_bs\", type=int, default=6, help=\"Recognition model inference batch size\"\n    )\n    return parser.parse_args()\n\n\ndef build_option(args):\n    det_option = fd.RuntimeOption()\n    cls_option = fd.RuntimeOption()\n    rec_option = fd.RuntimeOption()\n\n    det_option.use_kunlunxin()\n    cls_option.use_kunlunxin()\n    rec_option.use_kunlunxin()\n\n    return det_option, cls_option, rec_option\n\n\nargs = parse_arguments()\n\ndet_model_file = os.path.join(args.det_model, \"inference.pdmodel\")\ndet_params_file = os.path.join(args.det_model, \"inference.pdiparams\")\n\ncls_model_file = os.path.join(args.cls_model, \"inference.pdmodel\")\ncls_params_file = os.path.join(args.cls_model, \"inference.pdiparams\")\n\nrec_model_file = os.path.join(args.rec_model, \"inference.pdmodel\")\nrec_params_file = os.path.join(args.rec_model, \"inference.pdiparams\")\nrec_label_file = args.rec_label_file\n\ndet_option, cls_option, rec_option = build_option(args)\n\ndet_model = fd.vision.ocr.DBDetector(\n    det_model_file, det_params_file, runtime_option=det_option\n)\n\ncls_model = fd.vision.ocr.Classifier(\n    cls_model_file, cls_params_file, runtime_option=cls_option\n)\n\nrec_model = fd.vision.ocr.Recognizer(\n    rec_model_file, rec_params_file, rec_label_file, runtime_option=rec_option\n)\n\n# Create PP-OCRv3, if cls_model is not needed,\n# just set cls_model=None .\nppocr_v3 = fd.vision.ocr.PPOCRv3(\n    det_model=det_model, cls_model=cls_model, rec_model=rec_model\n)\n\n# Set inference batch size for cls model and rec model, the value could be -1 and 1 to positive infinity.\n# When inference batch size is set to -1, it means that the inference batch size\n# of the cls and rec models will be the same as the number of boxes detected by the det model.\nppocr_v3.cls_batch_size = args.cls_bs\nppocr_v3.rec_batch_size = args.rec_bs\n\n# Prepare image.\nim = cv2.imread(args.image)\n\n# Print the results.\nresult = ppocr_v3.predict(im)\n\nprint(result)\n\n# Visuliaze the output.\nvis_im = fd.vision.vis_ppocr(im, result)\ncv2.imwrite(\"visualized_result.jpg\", vis_im)\nprint(\"Visualized result save in ./visualized_result.jpg\")\n", "deploy/fastdeploy/sophgo/python/infer.py": "import fastdeploy as fd\nimport cv2\nimport os\n\n\ndef parse_arguments():\n    import argparse\n    import ast\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--det_model\", required=True, help=\"Path of Detection model of PPOCR.\"\n    )\n    parser.add_argument(\n        \"--cls_model\", required=True, help=\"Path of Classification model of PPOCR.\"\n    )\n    parser.add_argument(\n        \"--rec_model\", required=True, help=\"Path of Recognization model of PPOCR.\"\n    )\n    parser.add_argument(\n        \"--rec_label_file\", required=True, help=\"Path of Recognization label of PPOCR.\"\n    )\n    parser.add_argument(\n        \"--image\", type=str, required=True, help=\"Path of test image file.\"\n    )\n\n    return parser.parse_args()\n\n\nargs = parse_arguments()\n\n# \u914d\u7f6eruntime\uff0c\u52a0\u8f7d\u6a21\u578b\nruntime_option = fd.RuntimeOption()\nruntime_option.use_sophgo()\n\n# Detection\u6a21\u578b, \u68c0\u6d4b\u6587\u5b57\u6846\ndet_model_file = args.det_model\ndet_params_file = \"\"\n# Classification\u6a21\u578b\uff0c\u65b9\u5411\u5206\u7c7b\uff0c\u53ef\u9009\ncls_model_file = args.cls_model\ncls_params_file = \"\"\n# Recognition\u6a21\u578b\uff0c\u6587\u5b57\u8bc6\u522b\u6a21\u578b\nrec_model_file = args.rec_model\nrec_params_file = \"\"\nrec_label_file = args.rec_label_file\n\n# PPOCR\u7684cls\u548crec\u6a21\u578b\u73b0\u5728\u5df2\u7ecf\u652f\u6301\u63a8\u7406\u4e00\u4e2aBatch\u7684\u6570\u636e\n# \u5b9a\u4e49\u4e0b\u9762\u4e24\u4e2a\u53d8\u91cf\u540e, \u53ef\u7528\u4e8e\u8bbe\u7f6etrt\u8f93\u5165shape, \u5e76\u5728PPOCR\u6a21\u578b\u521d\u59cb\u5316\u540e, \u5b8c\u6210Batch\u63a8\u7406\u8bbe\u7f6e\ncls_batch_size = 1\nrec_batch_size = 1\n\n# \u5f53\u4f7f\u7528TRT\u65f6\uff0c\u5206\u522b\u7ed9\u4e09\u4e2a\u6a21\u578b\u7684runtime\u8bbe\u7f6e\u52a8\u6001shape,\u5e76\u5b8c\u6210\u6a21\u578b\u7684\u521b\u5efa.\n# \u6ce8\u610f: \u9700\u8981\u5728\u68c0\u6d4b\u6a21\u578b\u521b\u5efa\u5b8c\u6210\u540e\uff0c\u518d\u8bbe\u7f6e\u5206\u7c7b\u6a21\u578b\u7684\u52a8\u6001\u8f93\u5165\u5e76\u521b\u5efa\u5206\u7c7b\u6a21\u578b, \u8bc6\u522b\u6a21\u578b\u540c\u7406.\n# \u5982\u679c\u7528\u6237\u60f3\u8981\u81ea\u5df1\u6539\u52a8\u68c0\u6d4b\u6a21\u578b\u7684\u8f93\u5165shape, \u6211\u4eec\u5efa\u8bae\u7528\u6237\u628a\u68c0\u6d4b\u6a21\u578b\u7684\u957f\u548c\u9ad8\u8bbe\u7f6e\u4e3a32\u7684\u500d\u6570.\ndet_option = runtime_option\ndet_option.set_trt_input_shape(\"x\", [1, 3, 64, 64], [1, 3, 640, 640], [1, 3, 960, 960])\n# \u7528\u6237\u53ef\u4ee5\u628aTRT\u5f15\u64ce\u6587\u4ef6\u4fdd\u5b58\u81f3\u672c\u5730\n# det_option.set_trt_cache_file(args.det_model  + \"/det_trt_cache.trt\")\ndet_model = fd.vision.ocr.DBDetector(\n    det_model_file,\n    det_params_file,\n    runtime_option=det_option,\n    model_format=fd.ModelFormat.SOPHGO,\n)\n\ncls_option = runtime_option\ncls_option.set_trt_input_shape(\n    \"x\", [1, 3, 48, 10], [cls_batch_size, 3, 48, 320], [cls_batch_size, 3, 48, 1024]\n)\n# \u7528\u6237\u53ef\u4ee5\u628aTRT\u5f15\u64ce\u6587\u4ef6\u4fdd\u5b58\u81f3\u672c\u5730\n# cls_option.set_trt_cache_file(args.cls_model  + \"/cls_trt_cache.trt\")\ncls_model = fd.vision.ocr.Classifier(\n    cls_model_file,\n    cls_params_file,\n    runtime_option=cls_option,\n    model_format=fd.ModelFormat.SOPHGO,\n)\n\nrec_option = runtime_option\nrec_option.set_trt_input_shape(\n    \"x\", [1, 3, 48, 10], [rec_batch_size, 3, 48, 320], [rec_batch_size, 3, 48, 2304]\n)\n# \u7528\u6237\u53ef\u4ee5\u628aTRT\u5f15\u64ce\u6587\u4ef6\u4fdd\u5b58\u81f3\u672c\u5730\n# rec_option.set_trt_cache_file(args.rec_model  + \"/rec_trt_cache.trt\")\nrec_model = fd.vision.ocr.Recognizer(\n    rec_model_file,\n    rec_params_file,\n    rec_label_file,\n    runtime_option=rec_option,\n    model_format=fd.ModelFormat.SOPHGO,\n)\n\n# \u521b\u5efaPP-OCR\uff0c\u4e32\u80543\u4e2a\u6a21\u578b\uff0c\u5176\u4e2dcls_model\u53ef\u9009\uff0c\u5982\u65e0\u9700\u6c42\uff0c\u53ef\u8bbe\u7f6e\u4e3aNone\nppocr_v3 = fd.vision.ocr.PPOCRv3(\n    det_model=det_model, cls_model=cls_model, rec_model=rec_model\n)\n\n# \u9700\u8981\u4f7f\u7528\u4e0b\u884c\u4ee3\u7801, \u6765\u542f\u7528rec\u6a21\u578b\u7684\u9759\u6001shape\u63a8\u7406\uff0c\u8fd9\u91ccrec\u6a21\u578b\u7684\u9759\u6001\u8f93\u5165\u4e3a[3, 48, 584]\nrec_model.preprocessor.static_shape_infer = True\nrec_model.preprocessor.rec_image_shape = [3, 48, 584]\n\n# \u7ed9cls\u548crec\u6a21\u578b\u8bbe\u7f6e\u63a8\u7406\u65f6\u7684batch size\n# \u6b64\u503c\u80fd\u4e3a-1, \u548c1\u5230\u6b63\u65e0\u7a77\n# \u5f53\u6b64\u503c\u4e3a-1\u65f6, cls\u548crec\u6a21\u578b\u7684batch size\u5c06\u9ed8\u8ba4\u548cdet\u6a21\u578b\u68c0\u6d4b\u51fa\u7684\u6846\u7684\u6570\u91cf\u76f8\u540c\nppocr_v3.cls_batch_size = cls_batch_size\nppocr_v3.rec_batch_size = rec_batch_size\n\n# \u9884\u6d4b\u56fe\u7247\u51c6\u5907\nim = cv2.imread(args.image)\n\n# \u9884\u6d4b\u5e76\u6253\u5370\u7ed3\u679c\nresult = ppocr_v3.predict(im)\n\nprint(result)\n\n# \u53ef\u89c6\u5316\u7ed3\u679c\nvis_im = fd.vision.vis_ppocr(im, result)\ncv2.imwrite(\"sophgo_result.jpg\", vis_im)\nprint(\"Visualized result save in ./sophgo_result.jpg\")\n", "deploy/pdserving/web_service_rec.py": "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom paddle_serving_server.web_service import WebService, Op\n\nimport logging\nimport numpy as np\nimport cv2\nimport base64\n\n# from paddle_serving_app.reader import OCRReader\nfrom ocr_reader import OCRReader, DetResizeForTest, ArgsParser\nfrom paddle_serving_app.reader import Sequential, ResizeByFactor\nfrom paddle_serving_app.reader import Div, Normalize, Transpose\n\n_LOGGER = logging.getLogger()\n\n\nclass RecOp(Op):\n    def init_op(self):\n        self.ocr_reader = OCRReader(\n            char_dict_path=\"../../ppocr/utils/ppocr_keys_v1.txt\"\n        )\n\n    def preprocess(self, input_dicts, data_id, log_id):\n        ((_, input_dict),) = input_dicts.items()\n        raw_im = base64.b64decode(input_dict[\"image\"].encode(\"utf8\"))\n        data = np.fromstring(raw_im, np.uint8)\n        im = cv2.imdecode(data, cv2.IMREAD_COLOR)\n        feed_list = []\n        max_wh_ratio = 0\n        ## Many mini-batchs, the type of feed_data is list.\n        max_batch_size = 6  # len(dt_boxes)\n\n        # If max_batch_size is 0, skipping predict stage\n        if max_batch_size == 0:\n            return {}, True, None, \"\"\n        boxes_size = max_batch_size\n        rem = boxes_size % max_batch_size\n\n        h, w = im.shape[0:2]\n        wh_ratio = w * 1.0 / h\n        max_wh_ratio = max(max_wh_ratio, wh_ratio)\n        _, w, h = self.ocr_reader.resize_norm_img(im, max_wh_ratio).shape\n        norm_img = self.ocr_reader.resize_norm_img(im, max_batch_size)\n        norm_img = norm_img[np.newaxis, :]\n        feed = {\"x\": norm_img.copy()}\n        feed_list.append(feed)\n        return feed_list, False, None, \"\"\n\n    def postprocess(self, input_dicts, fetch_data, data_id, log_id):\n        res_list = []\n        if isinstance(fetch_data, dict):\n            if len(fetch_data) > 0:\n                rec_batch_res = self.ocr_reader.postprocess(fetch_data, with_score=True)\n                for res in rec_batch_res:\n                    res_list.append(res[0])\n        elif isinstance(fetch_data, list):\n            for one_batch in fetch_data:\n                one_batch_res = self.ocr_reader.postprocess(one_batch, with_score=True)\n                for res in one_batch_res:\n                    res_list.append(res[0])\n\n        res = {\"res\": str(res_list)}\n        return res, None, \"\"\n\n\nclass OcrService(WebService):\n    def get_pipeline_response(self, read_op):\n        rec_op = RecOp(name=\"rec\", input_ops=[read_op])\n        return rec_op\n\n\nuci_service = OcrService(name=\"ocr\")\nFLAGS = ArgsParser().parse_args()\nuci_service.prepare_pipeline_config(yml_dict=FLAGS.conf_dict)\nuci_service.run_service()\n", "deploy/pdserving/pipeline_rpc_client.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\ntry:\n    from paddle_serving_server_gpu.pipeline import PipelineClient\nexcept ImportError:\n    from paddle_serving_server.pipeline import PipelineClient\nimport numpy as np\nimport requests\nimport json\nimport cv2\nimport base64\nimport os\n\nclient = PipelineClient()\nclient.connect([\"127.0.0.1:18091\"])\n\n\ndef cv2_to_base64(image):\n    return base64.b64encode(image).decode(\"utf8\")\n\n\nimport argparse\n\nparser = argparse.ArgumentParser(description=\"args for paddleserving\")\nparser.add_argument(\"--image_dir\", type=str, default=\"../../doc/imgs/\")\nargs = parser.parse_args()\ntest_img_dir = args.image_dir\n\nfor img_file in os.listdir(test_img_dir):\n    with open(os.path.join(test_img_dir, img_file), \"rb\") as file:\n        image_data = file.read()\n    image = cv2_to_base64(image_data)\n\n    for i in range(1):\n        ret = client.predict(feed_dict={\"image\": image}, fetch=[\"res\"])\n        print(ret)\n", "deploy/pdserving/web_service_det.py": "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom paddle_serving_server.web_service import WebService, Op\n\nimport logging\nimport numpy as np\nimport cv2\nimport base64\n\n# from paddle_serving_app.reader import OCRReader\nfrom ocr_reader import OCRReader, DetResizeForTest, ArgsParser\nfrom paddle_serving_app.reader import Sequential, ResizeByFactor\nfrom paddle_serving_app.reader import Div, Normalize, Transpose\nfrom paddle_serving_app.reader import (\n    DBPostProcess,\n    FilterBoxes,\n    GetRotateCropImage,\n    SortedBoxes,\n)\n\n_LOGGER = logging.getLogger()\n\n\nclass DetOp(Op):\n    def init_op(self):\n        self.det_preprocess = Sequential(\n            [\n                DetResizeForTest(),\n                Div(255),\n                Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n                Transpose((2, 0, 1)),\n            ]\n        )\n        self.filter_func = FilterBoxes(10, 10)\n        self.post_func = DBPostProcess(\n            {\n                \"thresh\": 0.3,\n                \"box_thresh\": 0.5,\n                \"max_candidates\": 1000,\n                \"unclip_ratio\": 1.5,\n                \"min_size\": 3,\n            }\n        )\n\n    def preprocess(self, input_dicts, data_id, log_id):\n        ((_, input_dict),) = input_dicts.items()\n        data = base64.b64decode(input_dict[\"image\"].encode(\"utf8\"))\n        self.raw_im = data\n        data = np.fromstring(data, np.uint8)\n        # Note: class variables(self.var) can only be used in process op mode\n        im = cv2.imdecode(data, cv2.IMREAD_COLOR)\n        self.ori_h, self.ori_w, _ = im.shape\n        det_img = self.det_preprocess(im)\n        _, self.new_h, self.new_w = det_img.shape\n        return {\"x\": det_img[np.newaxis, :].copy()}, False, None, \"\"\n\n    def postprocess(self, input_dicts, fetch_dict, data_id, log_id):\n        det_out = list(fetch_dict.values())[0]\n        ratio_list = [float(self.new_h) / self.ori_h, float(self.new_w) / self.ori_w]\n        dt_boxes_list = self.post_func(det_out, [ratio_list])\n        dt_boxes = self.filter_func(dt_boxes_list[0], [self.ori_h, self.ori_w])\n        out_dict = {\"dt_boxes\": str(dt_boxes)}\n\n        return out_dict, None, \"\"\n\n\nclass OcrService(WebService):\n    def get_pipeline_response(self, read_op):\n        det_op = DetOp(name=\"det\", input_ops=[read_op])\n        return det_op\n\n\nuci_service = OcrService(name=\"ocr\")\nFLAGS = ArgsParser().parse_args()\nuci_service.prepare_pipeline_config(yml_dict=FLAGS.conf_dict)\nuci_service.run_service()\n", "deploy/pdserving/web_service.py": "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom paddle_serving_server.web_service import WebService, Op\n\nimport logging\nimport numpy as np\nimport copy\nimport cv2\nimport base64\n\n# from paddle_serving_app.reader import OCRReader\nfrom ocr_reader import OCRReader, DetResizeForTest, ArgsParser\nfrom paddle_serving_app.reader import Sequential, ResizeByFactor\nfrom paddle_serving_app.reader import Div, Normalize, Transpose\nfrom paddle_serving_app.reader import (\n    DBPostProcess,\n    FilterBoxes,\n    GetRotateCropImage,\n    SortedBoxes,\n)\n\n_LOGGER = logging.getLogger()\n\n\nclass DetOp(Op):\n    def init_op(self):\n        self.det_preprocess = Sequential(\n            [\n                DetResizeForTest(),\n                Div(255),\n                Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n                Transpose((2, 0, 1)),\n            ]\n        )\n        self.filter_func = FilterBoxes(10, 10)\n        self.post_func = DBPostProcess(\n            {\n                \"thresh\": 0.3,\n                \"box_thresh\": 0.6,\n                \"max_candidates\": 1000,\n                \"unclip_ratio\": 1.5,\n                \"min_size\": 3,\n            }\n        )\n\n    def preprocess(self, input_dicts, data_id, log_id):\n        ((_, input_dict),) = input_dicts.items()\n        data = base64.b64decode(input_dict[\"image\"].encode(\"utf8\"))\n        self.raw_im = data\n        data = np.fromstring(data, np.uint8)\n        # Note: class variables(self.var) can only be used in process op mode\n        im = cv2.imdecode(data, cv2.IMREAD_COLOR)\n        self.ori_h, self.ori_w, _ = im.shape\n        det_img = self.det_preprocess(im)\n        _, self.new_h, self.new_w = det_img.shape\n        return {\"x\": det_img[np.newaxis, :].copy()}, False, None, \"\"\n\n    def postprocess(self, input_dicts, fetch_dict, data_id, log_id):\n        det_out = list(fetch_dict.values())[0]\n        ratio_list = [float(self.new_h) / self.ori_h, float(self.new_w) / self.ori_w]\n        dt_boxes_list = self.post_func(det_out, [ratio_list])\n        dt_boxes = self.filter_func(dt_boxes_list[0], [self.ori_h, self.ori_w])\n        out_dict = {\"dt_boxes\": dt_boxes, \"image\": self.raw_im}\n        return out_dict, None, \"\"\n\n\nclass RecOp(Op):\n    def init_op(self):\n        self.ocr_reader = OCRReader(\n            char_dict_path=\"../../ppocr/utils/ppocr_keys_v1.txt\"\n        )\n\n        self.get_rotate_crop_image = GetRotateCropImage()\n        self.sorted_boxes = SortedBoxes()\n\n    def preprocess(self, input_dicts, data_id, log_id):\n        ((_, input_dict),) = input_dicts.items()\n        raw_im = input_dict[\"image\"]\n        data = np.frombuffer(raw_im, np.uint8)\n        im = cv2.imdecode(data, cv2.IMREAD_COLOR)\n        self.dt_list = input_dict[\"dt_boxes\"]\n        self.dt_list = self.sorted_boxes(self.dt_list)\n        # deepcopy to save origin dt_boxes\n        dt_boxes = copy.deepcopy(self.dt_list)\n        feed_list = []\n        img_list = []\n        max_wh_ratio = 320 / 48.0\n        ## Many mini-batchs, the type of feed_data is list.\n        max_batch_size = 6  # len(dt_boxes)\n\n        # If max_batch_size is 0, skipping predict stage\n        if max_batch_size == 0:\n            return {}, True, None, \"\"\n        boxes_size = len(dt_boxes)\n        batch_size = boxes_size // max_batch_size\n        rem = boxes_size % max_batch_size\n        for bt_idx in range(0, batch_size + 1):\n            imgs = None\n            boxes_num_in_one_batch = 0\n            if bt_idx == batch_size:\n                if rem == 0:\n                    continue\n                else:\n                    boxes_num_in_one_batch = rem\n            elif bt_idx < batch_size:\n                boxes_num_in_one_batch = max_batch_size\n            else:\n                _LOGGER.error(\n                    \"batch_size error, bt_idx={}, batch_size={}\".format(\n                        bt_idx, batch_size\n                    )\n                )\n                break\n\n            start = bt_idx * max_batch_size\n            end = start + boxes_num_in_one_batch\n            img_list = []\n            for box_idx in range(start, end):\n                boximg = self.get_rotate_crop_image(im, dt_boxes[box_idx])\n                img_list.append(boximg)\n                h, w = boximg.shape[0:2]\n                wh_ratio = w * 1.0 / h\n                max_wh_ratio = max(max_wh_ratio, wh_ratio)\n            _, w, h = self.ocr_reader.resize_norm_img(img_list[0], max_wh_ratio).shape\n\n            imgs = np.zeros((boxes_num_in_one_batch, 3, w, h)).astype(\"float32\")\n            for id, img in enumerate(img_list):\n                norm_img = self.ocr_reader.resize_norm_img(img, max_wh_ratio)\n                imgs[id] = norm_img\n            feed = {\"x\": imgs.copy()}\n            feed_list.append(feed)\n        return feed_list, False, None, \"\"\n\n    def postprocess(self, input_dicts, fetch_data, data_id, log_id):\n        rec_list = []\n        dt_num = len(self.dt_list)\n        if isinstance(fetch_data, dict):\n            if len(fetch_data) > 0:\n                rec_batch_res = self.ocr_reader.postprocess(fetch_data, with_score=True)\n                for res in rec_batch_res:\n                    rec_list.append(res)\n        elif isinstance(fetch_data, list):\n            for one_batch in fetch_data:\n                one_batch_res = self.ocr_reader.postprocess(one_batch, with_score=True)\n                for res in one_batch_res:\n                    rec_list.append(res)\n        result_list = []\n        for i in range(dt_num):\n            text = rec_list[i]\n            dt_box = self.dt_list[i]\n            if text[1] >= 0.5:\n                result_list.append([text, dt_box.tolist()])\n        res = {\"result\": str(result_list)}\n        return res, None, \"\"\n\n\nclass OcrService(WebService):\n    def get_pipeline_response(self, read_op):\n        det_op = DetOp(name=\"det\", input_ops=[read_op])\n        rec_op = RecOp(name=\"rec\", input_ops=[det_op])\n        return rec_op\n\n\nuci_service = OcrService(name=\"ocr\")\nFLAGS = ArgsParser().parse_args()\nuci_service.prepare_pipeline_config(yml_dict=FLAGS.conf_dict)\nuci_service.run_service()\n", "deploy/pdserving/ocr_reader.py": "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport cv2\nimport copy\nimport numpy as np\nimport math\nimport re\nimport sys\nimport argparse\nimport string\nfrom copy import deepcopy\n\n\nclass DetResizeForTest(object):\n    def __init__(self, **kwargs):\n        super(DetResizeForTest, self).__init__()\n        self.resize_type = 0\n        if \"image_shape\" in kwargs:\n            self.image_shape = kwargs[\"image_shape\"]\n            self.resize_type = 1\n        elif \"limit_side_len\" in kwargs:\n            self.limit_side_len = kwargs[\"limit_side_len\"]\n            self.limit_type = kwargs.get(\"limit_type\", \"min\")\n        elif \"resize_short\" in kwargs:\n            self.limit_side_len = 736\n            self.limit_type = \"min\"\n        else:\n            self.resize_type = 2\n            self.resize_long = kwargs.get(\"resize_long\", 960)\n\n    def __call__(self, data):\n        img = deepcopy(data)\n        src_h, src_w, _ = img.shape\n\n        if self.resize_type == 0:\n            img, [ratio_h, ratio_w] = self.resize_image_type0(img)\n        elif self.resize_type == 2:\n            img, [ratio_h, ratio_w] = self.resize_image_type2(img)\n        else:\n            img, [ratio_h, ratio_w] = self.resize_image_type1(img)\n\n        return img\n\n    def resize_image_type1(self, img):\n        resize_h, resize_w = self.image_shape\n        ori_h, ori_w = img.shape[:2]  # (h, w, c)\n        ratio_h = float(resize_h) / ori_h\n        ratio_w = float(resize_w) / ori_w\n        img = cv2.resize(img, (int(resize_w), int(resize_h)))\n        return img, [ratio_h, ratio_w]\n\n    def resize_image_type0(self, img):\n        \"\"\"\n        resize image to a size multiple of 32 which is required by the network\n        args:\n            img(array): array with shape [h, w, c]\n        return(tuple):\n            img, (ratio_h, ratio_w)\n        \"\"\"\n        limit_side_len = self.limit_side_len\n        h, w, _ = img.shape\n\n        # limit the max side\n        if self.limit_type == \"max\":\n            if max(h, w) > limit_side_len:\n                if h > w:\n                    ratio = float(limit_side_len) / h\n                else:\n                    ratio = float(limit_side_len) / w\n            else:\n                ratio = 1.0\n        else:\n            if min(h, w) < limit_side_len:\n                if h < w:\n                    ratio = float(limit_side_len) / h\n                else:\n                    ratio = float(limit_side_len) / w\n            else:\n                ratio = 1.0\n        resize_h = int(h * ratio)\n        resize_w = int(w * ratio)\n\n        resize_h = int(round(resize_h / 32) * 32)\n        resize_w = int(round(resize_w / 32) * 32)\n\n        try:\n            if int(resize_w) <= 0 or int(resize_h) <= 0:\n                return None, (None, None)\n            img = cv2.resize(img, (int(resize_w), int(resize_h)))\n        except:\n            print(img.shape, resize_w, resize_h)\n            sys.exit(0)\n        ratio_h = resize_h / float(h)\n        ratio_w = resize_w / float(w)\n        # return img, np.array([h, w])\n        return img, [ratio_h, ratio_w]\n\n    def resize_image_type2(self, img):\n        h, w, _ = img.shape\n\n        resize_w = w\n        resize_h = h\n\n        # Fix the longer side\n        if resize_h > resize_w:\n            ratio = float(self.resize_long) / resize_h\n        else:\n            ratio = float(self.resize_long) / resize_w\n\n        resize_h = int(resize_h * ratio)\n        resize_w = int(resize_w * ratio)\n\n        max_stride = 128\n        resize_h = (resize_h + max_stride - 1) // max_stride * max_stride\n        resize_w = (resize_w + max_stride - 1) // max_stride * max_stride\n        img = cv2.resize(img, (int(resize_w), int(resize_h)))\n        ratio_h = resize_h / float(h)\n        ratio_w = resize_w / float(w)\n\n        return img, [ratio_h, ratio_w]\n\n\nclass BaseRecLabelDecode(object):\n    \"\"\"Convert between text-label and text-index\"\"\"\n\n    def __init__(self, config):\n        support_character_type = [\n            \"ch\",\n            \"en\",\n            \"EN_symbol\",\n            \"french\",\n            \"german\",\n            \"japan\",\n            \"korean\",\n            \"it\",\n            \"xi\",\n            \"pu\",\n            \"ru\",\n            \"ar\",\n            \"ta\",\n            \"ug\",\n            \"fa\",\n            \"ur\",\n            \"rs\",\n            \"oc\",\n            \"rsc\",\n            \"bg\",\n            \"uk\",\n            \"be\",\n            \"te\",\n            \"ka\",\n            \"chinese_cht\",\n            \"hi\",\n            \"mr\",\n            \"ne\",\n            \"EN\",\n        ]\n        character_type = config[\"character_type\"]\n        character_dict_path = config[\"character_dict_path\"]\n        use_space_char = True\n        assert (\n            character_type in support_character_type\n        ), \"Only {} are supported now but get {}\".format(\n            support_character_type, character_type\n        )\n\n        self.beg_str = \"sos\"\n        self.end_str = \"eos\"\n\n        if character_type == \"en\":\n            self.character_str = \"0123456789abcdefghijklmnopqrstuvwxyz\"\n            dict_character = list(self.character_str)\n        elif character_type == \"EN_symbol\":\n            # same with ASTER setting (use 94 char).\n            self.character_str = string.printable[:-6]\n            dict_character = list(self.character_str)\n        elif character_type in support_character_type:\n            self.character_str = \"\"\n            assert (\n                character_dict_path is not None\n            ), \"character_dict_path should not be None when character_type is {}\".format(\n                character_type\n            )\n            with open(character_dict_path, \"rb\") as fin:\n                lines = fin.readlines()\n                for line in lines:\n                    line = line.decode(\"utf-8\").strip(\"\\n\").strip(\"\\r\\n\")\n                    self.character_str += line\n            if use_space_char:\n                self.character_str += \" \"\n            dict_character = list(self.character_str)\n\n        else:\n            raise NotImplementedError\n        self.character_type = character_type\n        dict_character = self.add_special_char(dict_character)\n        self.dict = {}\n        for i, char in enumerate(dict_character):\n            self.dict[char] = i\n        self.character = dict_character\n\n    def add_special_char(self, dict_character):\n        return dict_character\n\n    def decode(self, text_index, text_prob=None, is_remove_duplicate=False):\n        \"\"\"convert text-index into text-label.\"\"\"\n        result_list = []\n        ignored_tokens = self.get_ignored_tokens()\n        batch_size = len(text_index)\n        for batch_idx in range(batch_size):\n            char_list = []\n            conf_list = []\n            for idx in range(len(text_index[batch_idx])):\n                if text_index[batch_idx][idx] in ignored_tokens:\n                    continue\n                if is_remove_duplicate:\n                    # only for predict\n                    if (\n                        idx > 0\n                        and text_index[batch_idx][idx - 1] == text_index[batch_idx][idx]\n                    ):\n                        continue\n                char_list.append(self.character[int(text_index[batch_idx][idx])])\n                if text_prob is not None:\n                    conf_list.append(text_prob[batch_idx][idx])\n                else:\n                    conf_list.append(1)\n            text = \"\".join(char_list)\n            result_list.append((text, np.mean(conf_list)))\n        return result_list\n\n    def get_ignored_tokens(self):\n        return [0]  # for ctc blank\n\n\nclass CTCLabelDecode(BaseRecLabelDecode):\n    \"\"\"Convert between text-label and text-index\"\"\"\n\n    def __init__(\n        self,\n        config,\n        # character_dict_path=None,\n        # character_type='ch',\n        # use_space_char=False,\n        **kwargs,\n    ):\n        super(CTCLabelDecode, self).__init__(config)\n\n    def __call__(self, preds, label=None, *args, **kwargs):\n        preds_idx = preds.argmax(axis=2)\n        preds_prob = preds.max(axis=2)\n        text = self.decode(preds_idx, preds_prob, is_remove_duplicate=True)\n        if label is None:\n            return text\n        label = self.decode(label)\n        return text, label\n\n    def add_special_char(self, dict_character):\n        dict_character = [\"blank\"] + dict_character\n        return dict_character\n\n\nclass CharacterOps(object):\n    \"\"\"Convert between text-label and text-index\"\"\"\n\n    def __init__(self, config):\n        self.character_type = config[\"character_type\"]\n        self.loss_type = config[\"loss_type\"]\n        if self.character_type == \"en\":\n            self.character_str = \"0123456789abcdefghijklmnopqrstuvwxyz\"\n            dict_character = list(self.character_str)\n        elif self.character_type == \"ch\":\n            character_dict_path = config[\"character_dict_path\"]\n            self.character_str = \"\"\n            with open(character_dict_path, \"rb\") as fin:\n                lines = fin.readlines()\n                for line in lines:\n                    line = line.decode(\"utf-8\").strip(\"\\n\").strip(\"\\r\\n\")\n                    self.character_str += line\n            dict_character = list(self.character_str)\n        elif self.character_type == \"en_sensitive\":\n            # same with ASTER setting (use 94 char).\n            self.character_str = string.printable[:-6]\n            dict_character = list(self.character_str)\n        else:\n            self.character_str = None\n        assert (\n            self.character_str is not None\n        ), \"Nonsupport type of the character: {}\".format(self.character_str)\n        self.beg_str = \"sos\"\n        self.end_str = \"eos\"\n        if self.loss_type == \"attention\":\n            dict_character = [self.beg_str, self.end_str] + dict_character\n        self.dict = {}\n        for i, char in enumerate(dict_character):\n            self.dict[char] = i\n        self.character = dict_character\n\n    def encode(self, text):\n        \"\"\"convert text-label into text-index.\n        input:\n            text: text labels of each image. [batch_size]\n\n        output:\n            text: concatenated text index for CTCLoss.\n                    [sum(text_lengths)] = [text_index_0 + text_index_1 + ... + text_index_(n - 1)]\n            length: length of each text. [batch_size]\n        \"\"\"\n        if self.character_type == \"en\":\n            text = text.lower()\n\n        text_list = []\n        for char in text:\n            if char not in self.dict:\n                continue\n            text_list.append(self.dict[char])\n        text = np.array(text_list)\n        return text\n\n    def decode(self, text_index, is_remove_duplicate=False):\n        \"\"\"convert text-index into text-label.\"\"\"\n        char_list = []\n        char_num = self.get_char_num()\n\n        if self.loss_type == \"attention\":\n            beg_idx = self.get_beg_end_flag_idx(\"beg\")\n            end_idx = self.get_beg_end_flag_idx(\"end\")\n            ignored_tokens = [beg_idx, end_idx]\n        else:\n            ignored_tokens = [char_num]\n\n        for idx in range(len(text_index)):\n            if text_index[idx] in ignored_tokens:\n                continue\n            if is_remove_duplicate:\n                if idx > 0 and text_index[idx - 1] == text_index[idx]:\n                    continue\n            char_list.append(self.character[text_index[idx]])\n        text = \"\".join(char_list)\n        return text\n\n    def get_char_num(self):\n        return len(self.character)\n\n    def get_beg_end_flag_idx(self, beg_or_end):\n        if self.loss_type == \"attention\":\n            if beg_or_end == \"beg\":\n                idx = np.array(self.dict[self.beg_str])\n            elif beg_or_end == \"end\":\n                idx = np.array(self.dict[self.end_str])\n            else:\n                assert False, \"Unsupport type %s in get_beg_end_flag_idx\" % beg_or_end\n            return idx\n        else:\n            err = \"error in get_beg_end_flag_idx when using the loss %s\" % (\n                self.loss_type\n            )\n            assert False, err\n\n\nclass OCRReader(object):\n    def __init__(\n        self,\n        algorithm=\"CRNN\",\n        image_shape=[3, 48, 320],\n        char_type=\"ch\",\n        batch_num=1,\n        char_dict_path=\"./ppocr_keys_v1.txt\",\n    ):\n        self.rec_image_shape = image_shape\n        self.character_type = char_type\n        self.rec_batch_num = batch_num\n        char_ops_params = {}\n        char_ops_params[\"character_type\"] = char_type\n        char_ops_params[\"character_dict_path\"] = char_dict_path\n        char_ops_params[\"loss_type\"] = \"ctc\"\n        self.char_ops = CharacterOps(char_ops_params)\n        self.label_ops = CTCLabelDecode(char_ops_params)\n\n    def resize_norm_img(self, img, max_wh_ratio):\n        imgC, imgH, imgW = self.rec_image_shape\n        if self.character_type == \"ch\":\n            imgW = int(imgH * max_wh_ratio)\n        h = img.shape[0]\n        w = img.shape[1]\n        ratio = w / float(h)\n        if math.ceil(imgH * ratio) > imgW:\n            resized_w = imgW\n        else:\n            resized_w = int(math.ceil(imgH * ratio))\n        resized_image = cv2.resize(img, (resized_w, imgH))\n        resized_image = resized_image.astype(\"float32\")\n        resized_image = resized_image.transpose((2, 0, 1)) / 255\n        resized_image -= 0.5\n        resized_image /= 0.5\n        padding_im = np.zeros((imgC, imgH, imgW), dtype=np.float32)\n\n        padding_im[:, :, 0:resized_w] = resized_image\n        return padding_im\n\n    def preprocess(self, img_list):\n        img_num = len(img_list)\n        norm_img_batch = []\n        max_wh_ratio = 320 / 48.0\n        for ino in range(img_num):\n            h, w = img_list[ino].shape[0:2]\n            wh_ratio = w * 1.0 / h\n            max_wh_ratio = max(max_wh_ratio, wh_ratio)\n\n        for ino in range(img_num):\n            norm_img = self.resize_norm_img(img_list[ino], max_wh_ratio)\n            norm_img = norm_img[np.newaxis, :]\n            norm_img_batch.append(norm_img)\n        norm_img_batch = np.concatenate(norm_img_batch)\n        norm_img_batch = norm_img_batch.copy()\n\n        return norm_img_batch[0]\n\n    def postprocess(self, outputs, with_score=False):\n        preds = list(outputs.values())[0]\n        try:\n            preds = preds.numpy()\n        except:\n            pass\n        preds_idx = preds.argmax(axis=2)\n        preds_prob = preds.max(axis=2)\n        text = self.label_ops.decode(preds_idx, preds_prob, is_remove_duplicate=True)\n        return text\n\n\nfrom argparse import ArgumentParser, RawDescriptionHelpFormatter\nimport yaml\n\n\nclass ArgsParser(ArgumentParser):\n    def __init__(self):\n        super(ArgsParser, self).__init__(formatter_class=RawDescriptionHelpFormatter)\n        self.add_argument(\"-c\", \"--config\", help=\"configuration file to use\")\n        self.add_argument(\"-o\", \"--opt\", nargs=\"+\", help=\"set configuration options\")\n\n    def parse_args(self, argv=None):\n        args = super(ArgsParser, self).parse_args(argv)\n        assert args.config is not None, \"Please specify --config=configure_file_path.\"\n        args.conf_dict = self._parse_opt(args.opt, args.config)\n        print(\"args config:\", args.conf_dict)\n        return args\n\n    def _parse_helper(self, v):\n        if v.isnumeric():\n            if \".\" in v:\n                v = float(v)\n            else:\n                v = int(v)\n        elif v == \"True\" or v == \"False\":\n            v = v == \"True\"\n        return v\n\n    def _parse_opt(self, opts, conf_path):\n        f = open(conf_path)\n        config = yaml.load(f, Loader=yaml.Loader)\n        if not opts:\n            return config\n        for s in opts:\n            s = s.strip()\n            k, v = s.split(\"=\")\n            v = self._parse_helper(v)\n            print(k, v, type(v))\n            cur = config\n            parent = cur\n            for kk in k.split(\".\"):\n                if kk not in cur:\n                    cur[kk] = {}\n                    parent = cur\n                    cur = cur[kk]\n                else:\n                    parent = cur\n                    cur = cur[kk]\n            parent[k.split(\".\")[-1]] = v\n        return config\n", "deploy/pdserving/ocr_cpp_client.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# pylint: disable=doc-string-missing\n\nfrom paddle_serving_client import Client\nimport sys\nimport numpy as np\nimport base64\nimport os\nimport cv2\nfrom paddle_serving_app.reader import Sequential, URL2Image, ResizeByFactor\nfrom paddle_serving_app.reader import Div, Normalize, Transpose\nfrom ocr_reader import OCRReader\nimport codecs\n\nclient = Client()\n# TODO:load_client need to load more than one client model.\n# this need to figure out some details.\nclient.load_client_config(sys.argv[1:])\nclient.connect([\"127.0.0.1:8181\"])\n\nimport paddle\n\ntest_img_dir = \"../../doc/imgs/1.jpg\"\n\nocr_reader = OCRReader(char_dict_path=\"../../ppocr/utils/ppocr_keys_v1.txt\")\n\n\ndef cv2_to_base64(image):\n    return base64.b64encode(image).decode(\"utf8\")  # data.tostring()).decode('utf8')\n\n\ndef _check_image_file(path):\n    img_end = {\"jpg\", \"bmp\", \"png\", \"jpeg\", \"rgb\", \"tif\", \"tiff\", \"gif\"}\n    return any([path.lower().endswith(e) for e in img_end])\n\n\ntest_img_list = []\nif os.path.isfile(test_img_dir) and _check_image_file(test_img_dir):\n    test_img_list.append(test_img_dir)\nelif os.path.isdir(test_img_dir):\n    for single_file in os.listdir(test_img_dir):\n        file_path = os.path.join(test_img_dir, single_file)\n        if os.path.isfile(file_path) and _check_image_file(file_path):\n            test_img_list.append(file_path)\nif len(test_img_list) == 0:\n    raise Exception(\"not found any img file in {}\".format(test_img_dir))\n\nfor img_file in test_img_list:\n    with open(img_file, \"rb\") as file:\n        image_data = file.read()\n    image = cv2_to_base64(image_data)\n    res_list = []\n    fetch_map = client.predict(feed={\"x\": image}, fetch=[], batch=True)\n    if fetch_map is None:\n        print(\"no results\")\n    else:\n        if \"text\" in fetch_map:\n            for x in fetch_map[\"text\"]:\n                x = codecs.encode(x)\n                words = base64.b64decode(x).decode(\"utf-8\")\n                res_list.append(words)\n        else:\n            try:\n                one_batch_res = ocr_reader.postprocess(fetch_map, with_score=True)\n                for res in one_batch_res:\n                    res_list.append(res[0])\n            except:\n                print(\"no results\")\n        res = {\"res\": str(res_list)}\n        print(res)\n", "deploy/pdserving/pipeline_http_client.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport requests\nimport json\nimport base64\nimport os\n\nimport argparse\n\n\ndef str2bool(v):\n    return v.lower() in (\"true\", \"t\", \"1\")\n\n\nparser = argparse.ArgumentParser(description=\"args for paddleserving\")\nparser.add_argument(\"--image_dir\", type=str, default=\"../../doc/imgs/\")\nparser.add_argument(\"--det\", type=str2bool, default=True)\nparser.add_argument(\"--rec\", type=str2bool, default=True)\nargs = parser.parse_args()\n\n\ndef cv2_to_base64(image):\n    return base64.b64encode(image).decode(\"utf8\")\n\n\ndef _check_image_file(path):\n    img_end = {\"jpg\", \"bmp\", \"png\", \"jpeg\", \"rgb\", \"tif\", \"tiff\", \"gif\"}\n    return any([path.lower().endswith(e) for e in img_end])\n\n\nurl = \"http://127.0.0.1:9998/ocr/prediction\"\ntest_img_dir = args.image_dir\n\ntest_img_list = []\nif os.path.isfile(test_img_dir) and _check_image_file(test_img_dir):\n    test_img_list.append(test_img_dir)\nelif os.path.isdir(test_img_dir):\n    for single_file in os.listdir(test_img_dir):\n        file_path = os.path.join(test_img_dir, single_file)\n        if os.path.isfile(file_path) and _check_image_file(file_path):\n            test_img_list.append(file_path)\nif len(test_img_list) == 0:\n    raise Exception(\"not found any img file in {}\".format(test_img_dir))\n\nfor idx, img_file in enumerate(test_img_list):\n    with open(img_file, \"rb\") as file:\n        image_data1 = file.read()\n    # print file name\n    print(\"{}{}{}\".format(\"*\" * 10, img_file, \"*\" * 10))\n\n    image = cv2_to_base64(image_data1)\n\n    data = {\"key\": [\"image\"], \"value\": [image]}\n    r = requests.post(url=url, data=json.dumps(data))\n    result = r.json()\n    print(\"erro_no:{}, err_msg:{}\".format(result[\"err_no\"], result[\"err_msg\"]))\n    # check success\n    if result[\"err_no\"] == 0:\n        ocr_result = result[\"value\"][0]\n        if not args.det:\n            print(ocr_result)\n        else:\n            try:\n                for item in eval(ocr_result):\n                    # return transcription and points\n                    print(\"{}, {}\".format(item[0], item[1]))\n            except Exception as e:\n                print(ocr_result)\n                print(\"No results\")\n                continue\n\n    else:\n        print(\"For details about error message, see PipelineServingLogs/pipeline.log\")\nprint(\"==> total number of test imgs: \", len(test_img_list))\n", "deploy/pdserving/__init__.py": "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n", "deploy/pdserving/win/ocr_reader.py": "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport cv2\nimport copy\nimport numpy as np\nimport math\nimport re\nimport sys\nimport argparse\nimport string\nfrom copy import deepcopy\n\n\nclass DetResizeForTest(object):\n    def __init__(self, **kwargs):\n        super(DetResizeForTest, self).__init__()\n        self.resize_type = 0\n        if \"image_shape\" in kwargs:\n            self.image_shape = kwargs[\"image_shape\"]\n            self.resize_type = 1\n        elif \"limit_side_len\" in kwargs:\n            self.limit_side_len = kwargs[\"limit_side_len\"]\n            self.limit_type = kwargs.get(\"limit_type\", \"min\")\n        elif \"resize_short\" in kwargs:\n            self.limit_side_len = 736\n            self.limit_type = \"min\"\n        else:\n            self.resize_type = 2\n            self.resize_long = kwargs.get(\"resize_long\", 960)\n\n    def __call__(self, data):\n        img = deepcopy(data)\n        src_h, src_w, _ = img.shape\n\n        if self.resize_type == 0:\n            img, [ratio_h, ratio_w] = self.resize_image_type0(img)\n        elif self.resize_type == 2:\n            img, [ratio_h, ratio_w] = self.resize_image_type2(img)\n        else:\n            img, [ratio_h, ratio_w] = self.resize_image_type1(img)\n\n        return img\n\n    def resize_image_type1(self, img):\n        resize_h, resize_w = self.image_shape\n        ori_h, ori_w = img.shape[:2]  # (h, w, c)\n        ratio_h = float(resize_h) / ori_h\n        ratio_w = float(resize_w) / ori_w\n        img = cv2.resize(img, (int(resize_w), int(resize_h)))\n        return img, [ratio_h, ratio_w]\n\n    def resize_image_type0(self, img):\n        \"\"\"\n        resize image to a size multiple of 32 which is required by the network\n        args:\n            img(array): array with shape [h, w, c]\n        return(tuple):\n            img, (ratio_h, ratio_w)\n        \"\"\"\n        limit_side_len = self.limit_side_len\n        h, w, _ = img.shape\n\n        # limit the max side\n        if self.limit_type == \"max\":\n            if max(h, w) > limit_side_len:\n                if h > w:\n                    ratio = float(limit_side_len) / h\n                else:\n                    ratio = float(limit_side_len) / w\n            else:\n                ratio = 1.0\n        else:\n            if min(h, w) < limit_side_len:\n                if h < w:\n                    ratio = float(limit_side_len) / h\n                else:\n                    ratio = float(limit_side_len) / w\n            else:\n                ratio = 1.0\n        resize_h = int(h * ratio)\n        resize_w = int(w * ratio)\n\n        resize_h = int(round(resize_h / 32) * 32)\n        resize_w = int(round(resize_w / 32) * 32)\n\n        try:\n            if int(resize_w) <= 0 or int(resize_h) <= 0:\n                return None, (None, None)\n            img = cv2.resize(img, (int(resize_w), int(resize_h)))\n        except:\n            print(img.shape, resize_w, resize_h)\n            sys.exit(0)\n        ratio_h = resize_h / float(h)\n        ratio_w = resize_w / float(w)\n        # return img, np.array([h, w])\n        return img, [ratio_h, ratio_w]\n\n    def resize_image_type2(self, img):\n        h, w, _ = img.shape\n\n        resize_w = w\n        resize_h = h\n\n        # Fix the longer side\n        if resize_h > resize_w:\n            ratio = float(self.resize_long) / resize_h\n        else:\n            ratio = float(self.resize_long) / resize_w\n\n        resize_h = int(resize_h * ratio)\n        resize_w = int(resize_w * ratio)\n\n        max_stride = 128\n        resize_h = (resize_h + max_stride - 1) // max_stride * max_stride\n        resize_w = (resize_w + max_stride - 1) // max_stride * max_stride\n        img = cv2.resize(img, (int(resize_w), int(resize_h)))\n        ratio_h = resize_h / float(h)\n        ratio_w = resize_w / float(w)\n\n        return img, [ratio_h, ratio_w]\n\n\nclass BaseRecLabelDecode(object):\n    \"\"\"Convert between text-label and text-index\"\"\"\n\n    def __init__(self, config):\n        support_character_type = [\n            \"ch\",\n            \"en\",\n            \"EN_symbol\",\n            \"french\",\n            \"german\",\n            \"japan\",\n            \"korean\",\n            \"it\",\n            \"xi\",\n            \"pu\",\n            \"ru\",\n            \"ar\",\n            \"ta\",\n            \"ug\",\n            \"fa\",\n            \"ur\",\n            \"rs\",\n            \"oc\",\n            \"rsc\",\n            \"bg\",\n            \"uk\",\n            \"be\",\n            \"te\",\n            \"ka\",\n            \"chinese_cht\",\n            \"hi\",\n            \"mr\",\n            \"ne\",\n            \"EN\",\n        ]\n        character_type = config[\"character_type\"]\n        character_dict_path = config[\"character_dict_path\"]\n        use_space_char = True\n        assert (\n            character_type in support_character_type\n        ), \"Only {} are supported now but get {}\".format(\n            support_character_type, character_type\n        )\n\n        self.beg_str = \"sos\"\n        self.end_str = \"eos\"\n\n        if character_type == \"en\":\n            self.character_str = \"0123456789abcdefghijklmnopqrstuvwxyz\"\n            dict_character = list(self.character_str)\n        elif character_type == \"EN_symbol\":\n            # same with ASTER setting (use 94 char).\n            self.character_str = string.printable[:-6]\n            dict_character = list(self.character_str)\n        elif character_type in support_character_type:\n            self.character_str = \"\"\n            assert (\n                character_dict_path is not None\n            ), \"character_dict_path should not be None when character_type is {}\".format(\n                character_type\n            )\n            with open(character_dict_path, \"rb\") as fin:\n                lines = fin.readlines()\n                for line in lines:\n                    line = line.decode(\"utf-8\").strip(\"\\n\").strip(\"\\r\\n\")\n                    self.character_str += line\n            if use_space_char:\n                self.character_str += \" \"\n            dict_character = list(self.character_str)\n\n        else:\n            raise NotImplementedError\n        self.character_type = character_type\n        dict_character = self.add_special_char(dict_character)\n        self.dict = {}\n        for i, char in enumerate(dict_character):\n            self.dict[char] = i\n        self.character = dict_character\n\n    def add_special_char(self, dict_character):\n        return dict_character\n\n    def decode(self, text_index, text_prob=None, is_remove_duplicate=False):\n        \"\"\"convert text-index into text-label.\"\"\"\n        result_list = []\n        ignored_tokens = self.get_ignored_tokens()\n        batch_size = len(text_index)\n        for batch_idx in range(batch_size):\n            char_list = []\n            conf_list = []\n            for idx in range(len(text_index[batch_idx])):\n                if text_index[batch_idx][idx] in ignored_tokens:\n                    continue\n                if is_remove_duplicate:\n                    # only for predict\n                    if (\n                        idx > 0\n                        and text_index[batch_idx][idx - 1] == text_index[batch_idx][idx]\n                    ):\n                        continue\n                char_list.append(self.character[int(text_index[batch_idx][idx])])\n                if text_prob is not None:\n                    conf_list.append(text_prob[batch_idx][idx])\n                else:\n                    conf_list.append(1)\n            text = \"\".join(char_list)\n            result_list.append((text, np.mean(conf_list)))\n        return result_list\n\n    def get_ignored_tokens(self):\n        return [0]  # for ctc blank\n\n\nclass CTCLabelDecode(BaseRecLabelDecode):\n    \"\"\"Convert between text-label and text-index\"\"\"\n\n    def __init__(\n        self,\n        config,\n        # character_dict_path=None,\n        # character_type='ch',\n        # use_space_char=False,\n        **kwargs,\n    ):\n        super(CTCLabelDecode, self).__init__(config)\n\n    def __call__(self, preds, label=None, *args, **kwargs):\n        preds_idx = preds.argmax(axis=2)\n        preds_prob = preds.max(axis=2)\n        text = self.decode(preds_idx, preds_prob, is_remove_duplicate=True)\n        if label is None:\n            return text\n        label = self.decode(label)\n        return text, label\n\n    def add_special_char(self, dict_character):\n        dict_character = [\"blank\"] + dict_character\n        return dict_character\n\n\nclass CharacterOps(object):\n    \"\"\"Convert between text-label and text-index\"\"\"\n\n    def __init__(self, config):\n        self.character_type = config[\"character_type\"]\n        self.loss_type = config[\"loss_type\"]\n        if self.character_type == \"en\":\n            self.character_str = \"0123456789abcdefghijklmnopqrstuvwxyz\"\n            dict_character = list(self.character_str)\n        elif self.character_type == \"ch\":\n            character_dict_path = config[\"character_dict_path\"]\n            self.character_str = \"\"\n            with open(character_dict_path, \"rb\") as fin:\n                lines = fin.readlines()\n                for line in lines:\n                    line = line.decode(\"utf-8\").strip(\"\\n\").strip(\"\\r\\n\")\n                    self.character_str += line\n            dict_character = list(self.character_str)\n        elif self.character_type == \"en_sensitive\":\n            # same with ASTER setting (use 94 char).\n            self.character_str = string.printable[:-6]\n            dict_character = list(self.character_str)\n        else:\n            self.character_str = None\n        assert (\n            self.character_str is not None\n        ), \"Nonsupport type of the character: {}\".format(self.character_str)\n        self.beg_str = \"sos\"\n        self.end_str = \"eos\"\n        if self.loss_type == \"attention\":\n            dict_character = [self.beg_str, self.end_str] + dict_character\n        self.dict = {}\n        for i, char in enumerate(dict_character):\n            self.dict[char] = i\n        self.character = dict_character\n\n    def encode(self, text):\n        \"\"\"convert text-label into text-index.\n        input:\n            text: text labels of each image. [batch_size]\n\n        output:\n            text: concatenated text index for CTCLoss.\n                    [sum(text_lengths)] = [text_index_0 + text_index_1 + ... + text_index_(n - 1)]\n            length: length of each text. [batch_size]\n        \"\"\"\n        if self.character_type == \"en\":\n            text = text.lower()\n\n        text_list = []\n        for char in text:\n            if char not in self.dict:\n                continue\n            text_list.append(self.dict[char])\n        text = np.array(text_list)\n        return text\n\n    def decode(self, text_index, is_remove_duplicate=False):\n        \"\"\"convert text-index into text-label.\"\"\"\n        char_list = []\n        char_num = self.get_char_num()\n\n        if self.loss_type == \"attention\":\n            beg_idx = self.get_beg_end_flag_idx(\"beg\")\n            end_idx = self.get_beg_end_flag_idx(\"end\")\n            ignored_tokens = [beg_idx, end_idx]\n        else:\n            ignored_tokens = [char_num]\n\n        for idx in range(len(text_index)):\n            if text_index[idx] in ignored_tokens:\n                continue\n            if is_remove_duplicate:\n                if idx > 0 and text_index[idx - 1] == text_index[idx]:\n                    continue\n            char_list.append(self.character[text_index[idx]])\n        text = \"\".join(char_list)\n        return text\n\n    def get_char_num(self):\n        return len(self.character)\n\n    def get_beg_end_flag_idx(self, beg_or_end):\n        if self.loss_type == \"attention\":\n            if beg_or_end == \"beg\":\n                idx = np.array(self.dict[self.beg_str])\n            elif beg_or_end == \"end\":\n                idx = np.array(self.dict[self.end_str])\n            else:\n                assert False, \"Unsupport type %s in get_beg_end_flag_idx\" % beg_or_end\n            return idx\n        else:\n            err = \"error in get_beg_end_flag_idx when using the loss %s\" % (\n                self.loss_type\n            )\n            assert False, err\n\n\nclass OCRReader(object):\n    def __init__(\n        self,\n        algorithm=\"CRNN\",\n        image_shape=[3, 32, 320],\n        char_type=\"ch\",\n        batch_num=1,\n        char_dict_path=\"./ppocr_keys_v1.txt\",\n    ):\n        self.rec_image_shape = image_shape\n        self.character_type = char_type\n        self.rec_batch_num = batch_num\n        char_ops_params = {}\n        char_ops_params[\"character_type\"] = char_type\n        char_ops_params[\"character_dict_path\"] = char_dict_path\n        char_ops_params[\"loss_type\"] = \"ctc\"\n        self.char_ops = CharacterOps(char_ops_params)\n        self.label_ops = CTCLabelDecode(char_ops_params)\n\n    def resize_norm_img(self, img, max_wh_ratio):\n        imgC, imgH, imgW = self.rec_image_shape\n        if self.character_type == \"ch\":\n            imgW = int(32 * max_wh_ratio)\n        h = img.shape[0]\n        w = img.shape[1]\n        ratio = w / float(h)\n        if math.ceil(imgH * ratio) > imgW:\n            resized_w = imgW\n        else:\n            resized_w = int(math.ceil(imgH * ratio))\n        resized_image = cv2.resize(img, (resized_w, imgH))\n        resized_image = resized_image.astype(\"float32\")\n        resized_image = resized_image.transpose((2, 0, 1)) / 255\n        resized_image -= 0.5\n        resized_image /= 0.5\n        padding_im = np.zeros((imgC, imgH, imgW), dtype=np.float32)\n\n        padding_im[:, :, 0:resized_w] = resized_image\n        return padding_im\n\n    def preprocess(self, img_list):\n        img_num = len(img_list)\n        norm_img_batch = []\n        max_wh_ratio = 0\n        for ino in range(img_num):\n            h, w = img_list[ino].shape[0:2]\n            wh_ratio = w * 1.0 / h\n            max_wh_ratio = max(max_wh_ratio, wh_ratio)\n\n        for ino in range(img_num):\n            norm_img = self.resize_norm_img(img_list[ino], max_wh_ratio)\n            norm_img = norm_img[np.newaxis, :]\n            norm_img_batch.append(norm_img)\n        norm_img_batch = np.concatenate(norm_img_batch)\n        norm_img_batch = norm_img_batch.copy()\n\n        return norm_img_batch[0]\n\n    def postprocess(self, outputs, with_score=False):\n        preds = outputs[\"softmax_5.tmp_0\"]\n        try:\n            preds = preds.numpy()\n        except:\n            pass\n        preds_idx = preds.argmax(axis=2)\n        preds_prob = preds.max(axis=2)\n        text = self.label_ops.decode(preds_idx, preds_prob, is_remove_duplicate=True)\n        return text\n", "deploy/pdserving/win/ocr_web_client.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# -*- coding: utf-8 -*-\n\nimport requests\nimport json\nimport cv2\nimport base64\nimport os, sys\nimport time\n\n\ndef cv2_to_base64(image):\n    # data = cv2.imencode('.jpg', image)[1]\n    return base64.b64encode(image).decode(\"utf8\")  # data.tostring()).decode('utf8')\n\n\nheaders = {\"Content-type\": \"application/json\"}\nurl = \"http://127.0.0.1:9292/ocr/prediction\"\n\ntest_img_dir = \"../../../doc/imgs/\"\nfor idx, img_file in enumerate(os.listdir(test_img_dir)):\n    with open(os.path.join(test_img_dir, img_file), \"rb\") as file:\n        image_data1 = file.read()\n\n    image = cv2_to_base64(image_data1)\n    for i in range(1):\n        data = {\"feed\": [{\"image\": image}], \"fetch\": [\"save_infer_model/scale_0.tmp_1\"]}\n        r = requests.post(url=url, headers=headers, data=json.dumps(data))\n        print(r.json())\n\ntest_img_dir = \"../../../doc/imgs/\"\nprint(\"==> total number of test imgs: \", len(os.listdir(test_img_dir)))\n", "deploy/pdserving/win/ocr_web_server.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom paddle_serving_client import Client\nimport cv2\nimport sys\nimport numpy as np\nimport os\nfrom paddle_serving_client import Client\nfrom paddle_serving_app.reader import Sequential, URL2Image, ResizeByFactor\nfrom paddle_serving_app.reader import Div, Normalize, Transpose\nfrom paddle_serving_app.reader import (\n    DBPostProcess,\n    FilterBoxes,\n    GetRotateCropImage,\n    SortedBoxes,\n)\nfrom ocr_reader import OCRReader\n\ntry:\n    from paddle_serving_server_gpu.web_service import WebService\nexcept ImportError:\n    from paddle_serving_server.web_service import WebService\nfrom paddle_serving_app.local_predict import LocalPredictor\nimport time\nimport re\nimport base64\n\n\nclass OCRService(WebService):\n    def init_det_debugger(self, det_model_config):\n        self.det_preprocess = Sequential(\n            [\n                ResizeByFactor(32, 960),\n                Div(255),\n                Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n                Transpose((2, 0, 1)),\n            ]\n        )\n        self.det_client = LocalPredictor()\n        if sys.argv[1] == \"gpu\":\n            self.det_client.load_model_config(det_model_config, use_gpu=True, gpu_id=0)\n        elif sys.argv[1] == \"cpu\":\n            self.det_client.load_model_config(det_model_config)\n        self.ocr_reader = OCRReader(\n            char_dict_path=\"../../../ppocr/utils/ppocr_keys_v1.txt\"\n        )\n\n    def preprocess(self, feed=[], fetch=[]):\n        data = base64.b64decode(feed[0][\"image\"].encode(\"utf8\"))\n        data = np.fromstring(data, np.uint8)\n        im = cv2.imdecode(data, cv2.IMREAD_COLOR)\n        ori_h, ori_w, _ = im.shape\n        det_img = self.det_preprocess(im)\n        _, new_h, new_w = det_img.shape\n        det_img = det_img[np.newaxis, :]\n        det_img = det_img.copy()\n        det_out = self.det_client.predict(\n            feed={\"x\": det_img}, fetch=[\"save_infer_model/scale_0.tmp_1\"], batch=True\n        )\n        filter_func = FilterBoxes(10, 10)\n        post_func = DBPostProcess(\n            {\n                \"thresh\": 0.3,\n                \"box_thresh\": 0.5,\n                \"max_candidates\": 1000,\n                \"unclip_ratio\": 1.5,\n                \"min_size\": 3,\n            }\n        )\n        sorted_boxes = SortedBoxes()\n        ratio_list = [float(new_h) / ori_h, float(new_w) / ori_w]\n        dt_boxes_list = post_func(\n            det_out[\"save_infer_model/scale_0.tmp_1\"], [ratio_list]\n        )\n        dt_boxes = filter_func(dt_boxes_list[0], [ori_h, ori_w])\n        dt_boxes = sorted_boxes(dt_boxes)\n        get_rotate_crop_image = GetRotateCropImage()\n        img_list = []\n        max_wh_ratio = 0\n        for i, dtbox in enumerate(dt_boxes):\n            boximg = get_rotate_crop_image(im, dt_boxes[i])\n            img_list.append(boximg)\n            h, w = boximg.shape[0:2]\n            wh_ratio = w * 1.0 / h\n            max_wh_ratio = max(max_wh_ratio, wh_ratio)\n        if len(img_list) == 0:\n            return [], []\n        _, w, h = self.ocr_reader.resize_norm_img(img_list[0], max_wh_ratio).shape\n        imgs = np.zeros((len(img_list), 3, w, h)).astype(\"float32\")\n        for id, img in enumerate(img_list):\n            norm_img = self.ocr_reader.resize_norm_img(img, max_wh_ratio)\n            imgs[id] = norm_img\n        feed = {\"x\": imgs.copy()}\n        fetch = [\"save_infer_model/scale_0.tmp_1\"]\n        return feed, fetch, True\n\n    def postprocess(self, feed={}, fetch=[], fetch_map=None):\n        rec_res = self.ocr_reader.postprocess(fetch_map, with_score=True)\n        res_lst = []\n        for res in rec_res:\n            res_lst.append(res[0])\n        res = {\"res\": res_lst}\n        return res\n\n\nocr_service = OCRService(name=\"ocr\")\nocr_service.load_model_config(\"../ppocr_rec_mobile_2.0_serving\")\nocr_service.prepare_server(workdir=\"workdir\", port=9292)\nocr_service.init_det_debugger(det_model_config=\"../ppocr_det_mobile_2.0_serving\")\nif sys.argv[1] == \"gpu\":\n    ocr_service.set_gpus(\"0\")\n    ocr_service.run_debugger_service(gpu=True)\nelif sys.argv[1] == \"cpu\":\n    ocr_service.run_debugger_service()\nocr_service.run_web_service()\n", "deploy/hubserving/ocr_system/module.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\n\nsys.path.insert(0, \".\")\nimport copy\n\nimport time\nimport paddlehub\nfrom paddlehub.common.logger import logger\nfrom paddlehub.module.module import moduleinfo, runnable, serving\nimport cv2\nimport numpy as np\nimport paddlehub as hub\n\nfrom tools.infer.utility import base64_to_cv2\nfrom tools.infer.predict_system import TextSystem\nfrom tools.infer.utility import parse_args\nfrom deploy.hubserving.ocr_system.params import read_params\n\n\n@moduleinfo(\n    name=\"ocr_system\",\n    version=\"1.0.0\",\n    summary=\"ocr system service\",\n    author=\"paddle-dev\",\n    author_email=\"paddle-dev@baidu.com\",\n    type=\"cv/PP-OCR_system\",\n)\nclass OCRSystem(hub.Module):\n    def _initialize(self, use_gpu=False, enable_mkldnn=False):\n        \"\"\"\n        initialize with the necessary elements\n        \"\"\"\n        cfg = self.merge_configs()\n\n        cfg.use_gpu = use_gpu\n        if use_gpu:\n            try:\n                _places = os.environ[\"CUDA_VISIBLE_DEVICES\"]\n                int(_places[0])\n                print(\"use gpu: \", use_gpu)\n                print(\"CUDA_VISIBLE_DEVICES: \", _places)\n                cfg.gpu_mem = 8000\n            except:\n                raise RuntimeError(\n                    \"Environment Variable CUDA_VISIBLE_DEVICES is not set correctly. If you wanna use gpu, please set CUDA_VISIBLE_DEVICES via export CUDA_VISIBLE_DEVICES=cuda_device_id.\"\n                )\n        cfg.ir_optim = True\n        cfg.enable_mkldnn = enable_mkldnn\n\n        self.text_sys = TextSystem(cfg)\n\n    def merge_configs(\n        self,\n    ):\n        # deafult cfg\n        backup_argv = copy.deepcopy(sys.argv)\n        sys.argv = sys.argv[:1]\n        cfg = parse_args()\n\n        update_cfg_map = vars(read_params())\n\n        for key in update_cfg_map:\n            cfg.__setattr__(key, update_cfg_map[key])\n\n        sys.argv = copy.deepcopy(backup_argv)\n        return cfg\n\n    def read_images(self, paths=[]):\n        images = []\n        for img_path in paths:\n            assert os.path.isfile(img_path), \"The {} isn't a valid file.\".format(\n                img_path\n            )\n            img = cv2.imread(img_path)\n            if img is None:\n                logger.info(\"error in loading image:{}\".format(img_path))\n                continue\n            images.append(img)\n        return images\n\n    def predict(self, images=[], paths=[]):\n        \"\"\"\n        Get the chinese texts in the predicted images.\n        Args:\n            images (list(numpy.ndarray)): images data, shape of each is [H, W, C]. If images not paths\n            paths (list[str]): The paths of images. If paths not images\n        Returns:\n            res (list): The result of chinese texts and save path of images.\n        \"\"\"\n\n        if images != [] and isinstance(images, list) and paths == []:\n            predicted_data = images\n        elif images == [] and isinstance(paths, list) and paths != []:\n            predicted_data = self.read_images(paths)\n        else:\n            raise TypeError(\"The input data is inconsistent with expectations.\")\n\n        assert (\n            predicted_data != []\n        ), \"There is not any image to be predicted. Please check the input data.\"\n\n        all_results = []\n        for img in predicted_data:\n            if img is None:\n                logger.info(\"error in loading image\")\n                all_results.append([])\n                continue\n            starttime = time.time()\n            dt_boxes, rec_res, _ = self.text_sys(img)\n            elapse = time.time() - starttime\n            logger.info(\"Predict time: {}\".format(elapse))\n\n            dt_num = len(dt_boxes)\n            rec_res_final = []\n\n            for dno in range(dt_num):\n                text, score = rec_res[dno]\n                rec_res_final.append(\n                    {\n                        \"text\": text,\n                        \"confidence\": float(score),\n                        \"text_region\": dt_boxes[dno].astype(np.int32).tolist(),\n                    }\n                )\n            all_results.append(rec_res_final)\n        return all_results\n\n    @serving\n    def serving_method(self, images, **kwargs):\n        \"\"\"\n        Run as a service.\n        \"\"\"\n        images_decode = [base64_to_cv2(image) for image in images]\n        results = self.predict(images_decode, **kwargs)\n        return results\n\n\nif __name__ == \"__main__\":\n    ocr = OCRSystem()\n    ocr._initialize()\n    image_path = [\n        \"./doc/imgs/11.jpg\",\n        \"./doc/imgs/12.jpg\",\n    ]\n    res = ocr.predict(paths=image_path)\n    print(res)\n", "deploy/hubserving/ocr_system/params.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nclass Config(object):\n    pass\n\n\ndef read_params():\n    cfg = Config()\n\n    # params for text detector\n    cfg.det_algorithm = \"DB\"\n    cfg.det_model_dir = \"./inference/ch_PP-OCRv3_det_infer/\"\n    cfg.det_limit_side_len = 960\n    cfg.det_limit_type = \"max\"\n\n    # DB parmas\n    cfg.det_db_thresh = 0.3\n    cfg.det_db_box_thresh = 0.5\n    cfg.det_db_unclip_ratio = 1.6\n    cfg.use_dilation = False\n    cfg.det_db_score_mode = \"fast\"\n\n    # EAST parmas\n    cfg.det_east_score_thresh = 0.8\n    cfg.det_east_cover_thresh = 0.1\n    cfg.det_east_nms_thresh = 0.2\n\n    # params for text recognizer\n    cfg.rec_algorithm = \"CRNN\"\n    cfg.rec_model_dir = \"./inference/ch_PP-OCRv3_rec_infer/\"\n\n    cfg.rec_image_shape = \"3, 48, 320\"\n    cfg.rec_batch_num = 6\n    cfg.max_text_length = 25\n\n    cfg.rec_char_dict_path = \"./ppocr/utils/ppocr_keys_v1.txt\"\n    cfg.use_space_char = True\n\n    # params for text classifier\n    cfg.use_angle_cls = True\n    cfg.cls_model_dir = \"./inference/ch_ppocr_mobile_v2.0_cls_infer/\"\n    cfg.cls_image_shape = \"3, 48, 192\"\n    cfg.label_list = [\"0\", \"180\"]\n    cfg.cls_batch_num = 30\n    cfg.cls_thresh = 0.9\n\n    cfg.use_pdserving = False\n    cfg.use_tensorrt = False\n    cfg.drop_score = 0.5\n\n    return cfg\n", "deploy/hubserving/ocr_system/__init__.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n", "deploy/hubserving/structure_system/module.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\n\nsys.path.insert(0, \".\")\nimport copy\n\nimport time\nimport paddlehub\nfrom paddlehub.common.logger import logger\nfrom paddlehub.module.module import moduleinfo, runnable, serving\nimport cv2\nimport numpy as np\nimport paddlehub as hub\n\nfrom tools.infer.utility import base64_to_cv2\nfrom ppstructure.predict_system import StructureSystem as PPStructureSystem\nfrom ppstructure.predict_system import save_structure_res\nfrom ppstructure.utility import parse_args\nfrom deploy.hubserving.structure_system.params import read_params\n\n\n@moduleinfo(\n    name=\"structure_system\",\n    version=\"1.0.0\",\n    summary=\"PP-Structure system service\",\n    author=\"paddle-dev\",\n    author_email=\"paddle-dev@baidu.com\",\n    type=\"cv/structure_system\",\n)\nclass StructureSystem(hub.Module):\n    def _initialize(self, use_gpu=False, enable_mkldnn=False):\n        \"\"\"\n        initialize with the necessary elements\n        \"\"\"\n        cfg = self.merge_configs()\n\n        cfg.use_gpu = use_gpu\n        if use_gpu:\n            try:\n                _places = os.environ[\"CUDA_VISIBLE_DEVICES\"]\n                int(_places[0])\n                print(\"use gpu: \", use_gpu)\n                print(\"CUDA_VISIBLE_DEVICES: \", _places)\n                cfg.gpu_mem = 8000\n            except:\n                raise RuntimeError(\n                    \"Environment Variable CUDA_VISIBLE_DEVICES is not set correctly. If you wanna use gpu, please set CUDA_VISIBLE_DEVICES via export CUDA_VISIBLE_DEVICES=cuda_device_id.\"\n                )\n        cfg.ir_optim = True\n        cfg.enable_mkldnn = enable_mkldnn\n\n        self.table_sys = PPStructureSystem(cfg)\n\n    def merge_configs(self):\n        # deafult cfg\n        backup_argv = copy.deepcopy(sys.argv)\n        sys.argv = sys.argv[:1]\n        cfg = parse_args()\n\n        update_cfg_map = vars(read_params())\n\n        for key in update_cfg_map:\n            cfg.__setattr__(key, update_cfg_map[key])\n\n        sys.argv = copy.deepcopy(backup_argv)\n        return cfg\n\n    def read_images(self, paths=[]):\n        images = []\n        for img_path in paths:\n            assert os.path.isfile(img_path), \"The {} isn't a valid file.\".format(\n                img_path\n            )\n            img = cv2.imread(img_path)\n            if img is None:\n                logger.info(\"error in loading image:{}\".format(img_path))\n                continue\n            images.append(img)\n        return images\n\n    def predict(self, images=[], paths=[]):\n        \"\"\"\n        Get the chinese texts in the predicted images.\n        Args:\n            images (list(numpy.ndarray)): images data, shape of each is [H, W, C]. If images not paths\n            paths (list[str]): The paths of images. If paths not images\n        Returns:\n            res (list): The result of chinese texts and save path of images.\n        \"\"\"\n\n        if images != [] and isinstance(images, list) and paths == []:\n            predicted_data = images\n        elif images == [] and isinstance(paths, list) and paths != []:\n            predicted_data = self.read_images(paths)\n        else:\n            raise TypeError(\"The input data is inconsistent with expectations.\")\n\n        assert (\n            predicted_data != []\n        ), \"There is not any image to be predicted. Please check the input data.\"\n\n        all_results = []\n        for img in predicted_data:\n            if img is None:\n                logger.info(\"error in loading image\")\n                all_results.append([])\n                continue\n            starttime = time.time()\n            res, _ = self.table_sys(img)\n            elapse = time.time() - starttime\n            logger.info(\"Predict time: {}\".format(elapse))\n\n            # parse result\n            res_final = []\n            for region in res:\n                region.pop(\"img\")\n                res_final.append(region)\n            all_results.append({\"regions\": res_final})\n        return all_results\n\n    @serving\n    def serving_method(self, images, **kwargs):\n        \"\"\"\n        Run as a service.\n        \"\"\"\n        images_decode = [base64_to_cv2(image) for image in images]\n        results = self.predict(images_decode, **kwargs)\n        return results\n\n\nif __name__ == \"__main__\":\n    structure_system = StructureSystem()\n    structure_system._initialize()\n    image_path = [\"./ppstructure/docs/table/1.png\"]\n    res = structure_system.predict(paths=image_path)\n    print(res)\n", "deploy/hubserving/structure_system/params.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom deploy.hubserving.structure_table.params import read_params as table_read_params\n\n\ndef read_params():\n    cfg = table_read_params()\n\n    # params for layout parser model\n    cfg.layout_model_dir = \"\"\n    cfg.layout_dict_path = \"./ppocr/utils/dict/layout_publaynet_dict.txt\"\n    cfg.layout_score_threshold = 0.5\n    cfg.layout_nms_threshold = 0.5\n\n    cfg.mode = \"structure\"\n    cfg.output = \"./output\"\n    return cfg\n", "deploy/hubserving/structure_system/__init__.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n", "deploy/hubserving/ocr_rec/module.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\n\nsys.path.insert(0, \".\")\nimport copy\nimport paddlehub\nfrom paddlehub.common.logger import logger\nfrom paddlehub.module.module import moduleinfo, runnable, serving\nimport cv2\nimport paddlehub as hub\n\nfrom tools.infer.utility import base64_to_cv2\nfrom tools.infer.predict_rec import TextRecognizer\nfrom tools.infer.utility import parse_args\nfrom deploy.hubserving.ocr_rec.params import read_params\n\n\n@moduleinfo(\n    name=\"ocr_rec\",\n    version=\"1.0.0\",\n    summary=\"ocr recognition service\",\n    author=\"paddle-dev\",\n    author_email=\"paddle-dev@baidu.com\",\n    type=\"cv/text_recognition\",\n)\nclass OCRRec(hub.Module):\n    def _initialize(self, use_gpu=False, enable_mkldnn=False):\n        \"\"\"\n        initialize with the necessary elements\n        \"\"\"\n        cfg = self.merge_configs()\n\n        cfg.use_gpu = use_gpu\n        if use_gpu:\n            try:\n                _places = os.environ[\"CUDA_VISIBLE_DEVICES\"]\n                int(_places[0])\n                print(\"use gpu: \", use_gpu)\n                print(\"CUDA_VISIBLE_DEVICES: \", _places)\n                cfg.gpu_mem = 8000\n            except:\n                raise RuntimeError(\n                    \"Environment Variable CUDA_VISIBLE_DEVICES is not set correctly. If you wanna use gpu, please set CUDA_VISIBLE_DEVICES via export CUDA_VISIBLE_DEVICES=cuda_device_id.\"\n                )\n        cfg.ir_optim = True\n        cfg.enable_mkldnn = enable_mkldnn\n\n        self.text_recognizer = TextRecognizer(cfg)\n\n    def merge_configs(\n        self,\n    ):\n        # deafult cfg\n        backup_argv = copy.deepcopy(sys.argv)\n        sys.argv = sys.argv[:1]\n        cfg = parse_args()\n\n        update_cfg_map = vars(read_params())\n\n        for key in update_cfg_map:\n            cfg.__setattr__(key, update_cfg_map[key])\n\n        sys.argv = copy.deepcopy(backup_argv)\n        return cfg\n\n    def read_images(self, paths=[]):\n        images = []\n        for img_path in paths:\n            assert os.path.isfile(img_path), \"The {} isn't a valid file.\".format(\n                img_path\n            )\n            img = cv2.imread(img_path)\n            if img is None:\n                logger.info(\"error in loading image:{}\".format(img_path))\n                continue\n            images.append(img)\n        return images\n\n    def predict(self, images=[], paths=[]):\n        \"\"\"\n        Get the text box in the predicted images.\n        Args:\n            images (list(numpy.ndarray)): images data, shape of each is [H, W, C]. If images not paths\n            paths (list[str]): The paths of images. If paths not images\n        Returns:\n            res (list): The result of text detection box and save path of images.\n        \"\"\"\n\n        if images != [] and isinstance(images, list) and paths == []:\n            predicted_data = images\n        elif images == [] and isinstance(paths, list) and paths != []:\n            predicted_data = self.read_images(paths)\n        else:\n            raise TypeError(\"The input data is inconsistent with expectations.\")\n\n        assert (\n            predicted_data != []\n        ), \"There is not any image to be predicted. Please check the input data.\"\n\n        img_list = []\n        for img in predicted_data:\n            if img is None:\n                continue\n            img_list.append(img)\n\n        rec_res_final = []\n        try:\n            rec_res, predict_time = self.text_recognizer(img_list)\n            for dno in range(len(rec_res)):\n                text, score = rec_res[dno]\n                rec_res_final.append(\n                    {\n                        \"text\": text,\n                        \"confidence\": float(score),\n                    }\n                )\n        except Exception as e:\n            print(e)\n            return [[]]\n\n        return [rec_res_final]\n\n    @serving\n    def serving_method(self, images, **kwargs):\n        \"\"\"\n        Run as a service.\n        \"\"\"\n        images_decode = [base64_to_cv2(image) for image in images]\n        results = self.predict(images_decode, **kwargs)\n        return results\n\n\nif __name__ == \"__main__\":\n    ocr = OCRRec()\n    ocr._initialize()\n    image_path = [\n        \"./doc/imgs_words/ch/word_1.jpg\",\n        \"./doc/imgs_words/ch/word_2.jpg\",\n        \"./doc/imgs_words/ch/word_3.jpg\",\n    ]\n    res = ocr.predict(paths=image_path)\n    print(res)\n", "deploy/hubserving/ocr_rec/params.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nclass Config(object):\n    pass\n\n\ndef read_params():\n    cfg = Config()\n\n    # params for text recognizer\n    cfg.rec_algorithm = \"CRNN\"\n    cfg.rec_model_dir = \"./inference/ch_PP-OCRv3_rec_infer/\"\n\n    cfg.rec_image_shape = \"3, 48, 320\"\n    cfg.rec_batch_num = 6\n    cfg.max_text_length = 25\n\n    cfg.rec_char_dict_path = \"./ppocr/utils/ppocr_keys_v1.txt\"\n    cfg.use_space_char = True\n\n    cfg.use_pdserving = False\n    cfg.use_tensorrt = False\n\n    return cfg\n", "deploy/hubserving/ocr_rec/__init__.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n", "deploy/hubserving/ocr_det/module.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\n\nsys.path.insert(0, \".\")\n\nimport copy\nimport paddlehub\nfrom paddlehub.common.logger import logger\nfrom paddlehub.module.module import moduleinfo, runnable, serving\nimport cv2\nimport numpy as np\nimport paddlehub as hub\n\nfrom tools.infer.utility import base64_to_cv2\nfrom tools.infer.predict_det import TextDetector\nfrom tools.infer.utility import parse_args\nfrom deploy.hubserving.ocr_system.params import read_params\n\n\n@moduleinfo(\n    name=\"ocr_det\",\n    version=\"1.0.0\",\n    summary=\"ocr detection service\",\n    author=\"paddle-dev\",\n    author_email=\"paddle-dev@baidu.com\",\n    type=\"cv/text_detection\",\n)\nclass OCRDet(hub.Module):\n    def _initialize(self, use_gpu=False, enable_mkldnn=False):\n        \"\"\"\n        initialize with the necessary elements\n        \"\"\"\n        cfg = self.merge_configs()\n\n        cfg.use_gpu = use_gpu\n        if use_gpu:\n            try:\n                _places = os.environ[\"CUDA_VISIBLE_DEVICES\"]\n                int(_places[0])\n                print(\"use gpu: \", use_gpu)\n                print(\"CUDA_VISIBLE_DEVICES: \", _places)\n                cfg.gpu_mem = 8000\n            except:\n                raise RuntimeError(\n                    \"Environment Variable CUDA_VISIBLE_DEVICES is not set correctly. If you wanna use gpu, please set CUDA_VISIBLE_DEVICES via export CUDA_VISIBLE_DEVICES=cuda_device_id.\"\n                )\n        cfg.ir_optim = True\n        cfg.enable_mkldnn = enable_mkldnn\n\n        self.text_detector = TextDetector(cfg)\n\n    def merge_configs(\n        self,\n    ):\n        # deafult cfg\n        backup_argv = copy.deepcopy(sys.argv)\n        sys.argv = sys.argv[:1]\n        cfg = parse_args()\n\n        update_cfg_map = vars(read_params())\n\n        for key in update_cfg_map:\n            cfg.__setattr__(key, update_cfg_map[key])\n\n        sys.argv = copy.deepcopy(backup_argv)\n        return cfg\n\n    def read_images(self, paths=[]):\n        images = []\n        for img_path in paths:\n            assert os.path.isfile(img_path), \"The {} isn't a valid file.\".format(\n                img_path\n            )\n            img = cv2.imread(img_path)\n            if img is None:\n                logger.info(\"error in loading image:{}\".format(img_path))\n                continue\n            images.append(img)\n        return images\n\n    def predict(self, images=[], paths=[]):\n        \"\"\"\n        Get the text box in the predicted images.\n        Args:\n            images (list(numpy.ndarray)): images data, shape of each is [H, W, C]. If images not paths\n            paths (list[str]): The paths of images. If paths not images\n        Returns:\n            res (list): The result of text detection box and save path of images.\n        \"\"\"\n\n        if images != [] and isinstance(images, list) and paths == []:\n            predicted_data = images\n        elif images == [] and isinstance(paths, list) and paths != []:\n            predicted_data = self.read_images(paths)\n        else:\n            raise TypeError(\"The input data is inconsistent with expectations.\")\n\n        assert (\n            predicted_data != []\n        ), \"There is not any image to be predicted. Please check the input data.\"\n\n        all_results = []\n        for img in predicted_data:\n            if img is None:\n                logger.info(\"error in loading image\")\n                all_results.append([])\n                continue\n            dt_boxes, elapse = self.text_detector(img)\n            logger.info(\"Predict time : {}\".format(elapse))\n\n            rec_res_final = []\n            for dno in range(len(dt_boxes)):\n                rec_res_final.append(\n                    {\"text_region\": dt_boxes[dno].astype(np.int32).tolist()}\n                )\n            all_results.append(rec_res_final)\n        return all_results\n\n    @serving\n    def serving_method(self, images, **kwargs):\n        \"\"\"\n        Run as a service.\n        \"\"\"\n        images_decode = [base64_to_cv2(image) for image in images]\n        results = self.predict(images_decode, **kwargs)\n        return results\n\n\nif __name__ == \"__main__\":\n    ocr = OCRDet()\n    ocr._initialize()\n    image_path = [\n        \"./doc/imgs/11.jpg\",\n        \"./doc/imgs/12.jpg\",\n    ]\n    res = ocr.predict(paths=image_path)\n    print(res)\n", "deploy/hubserving/ocr_det/params.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nclass Config(object):\n    pass\n\n\ndef read_params():\n    cfg = Config()\n\n    # params for text detector\n    cfg.det_algorithm = \"DB\"\n    cfg.det_model_dir = \"./inference/ch_PP-OCRv3_det_infer/\"\n    cfg.det_limit_side_len = 960\n    cfg.det_limit_type = \"max\"\n\n    # DB parmas\n    cfg.det_db_thresh = 0.3\n    cfg.det_db_box_thresh = 0.6\n    cfg.det_db_unclip_ratio = 1.5\n    cfg.use_dilation = False\n    cfg.det_db_score_mode = \"fast\"\n\n    # #EAST parmas\n    # cfg.det_east_score_thresh = 0.8\n    # cfg.det_east_cover_thresh = 0.1\n    # cfg.det_east_nms_thresh = 0.2\n\n    cfg.use_pdserving = False\n    cfg.use_tensorrt = False\n\n    return cfg\n", "deploy/hubserving/ocr_det/__init__.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n", "deploy/hubserving/structure_layout/module.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\n\nsys.path.insert(0, \".\")\nimport copy\n\nimport time\nimport paddlehub\nfrom paddlehub.common.logger import logger\nfrom paddlehub.module.module import moduleinfo, runnable, serving\nimport cv2\nimport paddlehub as hub\n\nfrom tools.infer.utility import base64_to_cv2\nfrom ppstructure.layout.predict_layout import LayoutPredictor as _LayoutPredictor\nfrom ppstructure.utility import parse_args\nfrom deploy.hubserving.structure_layout.params import read_params\n\n\n@moduleinfo(\n    name=\"structure_layout\",\n    version=\"1.0.0\",\n    summary=\"PP-Structure layout service\",\n    author=\"paddle-dev\",\n    author_email=\"paddle-dev@baidu.com\",\n    type=\"cv/structure_layout\",\n)\nclass LayoutPredictor(hub.Module):\n    def _initialize(self, use_gpu=False, enable_mkldnn=False):\n        \"\"\"\n        initialize with the necessary elements\n        \"\"\"\n        cfg = self.merge_configs()\n        cfg.use_gpu = use_gpu\n        if use_gpu:\n            try:\n                _places = os.environ[\"CUDA_VISIBLE_DEVICES\"]\n                int(_places[0])\n                print(\"use gpu: \", use_gpu)\n                print(\"CUDA_VISIBLE_DEVICES: \", _places)\n                cfg.gpu_mem = 8000\n            except:\n                raise RuntimeError(\n                    \"Environment Variable CUDA_VISIBLE_DEVICES is not set correctly. If you wanna use gpu, please set CUDA_VISIBLE_DEVICES via export CUDA_VISIBLE_DEVICES=cuda_device_id.\"\n                )\n        cfg.ir_optim = True\n        cfg.enable_mkldnn = enable_mkldnn\n\n        self.layout_predictor = _LayoutPredictor(cfg)\n\n    def merge_configs(self):\n        # deafult cfg\n        backup_argv = copy.deepcopy(sys.argv)\n        sys.argv = sys.argv[:1]\n        cfg = parse_args()\n\n        update_cfg_map = vars(read_params())\n\n        for key in update_cfg_map:\n            cfg.__setattr__(key, update_cfg_map[key])\n\n        sys.argv = copy.deepcopy(backup_argv)\n        return cfg\n\n    def read_images(self, paths=[]):\n        images = []\n        for img_path in paths:\n            assert os.path.isfile(img_path), \"The {} isn't a valid file.\".format(\n                img_path\n            )\n            img = cv2.imread(img_path)\n            if img is None:\n                logger.info(\"error in loading image:{}\".format(img_path))\n                continue\n            images.append(img)\n        return images\n\n    def predict(self, images=[], paths=[]):\n        \"\"\"\n        Get the chinese texts in the predicted images.\n        Args:\n            images (list(numpy.ndarray)): images data, shape of each is [H, W, C]. If images not paths\n            paths (list[str]): The paths of images. If paths not images\n        Returns:\n            res (list): The layout results of images.\n        \"\"\"\n\n        if images != [] and isinstance(images, list) and paths == []:\n            predicted_data = images\n        elif images == [] and isinstance(paths, list) and paths != []:\n            predicted_data = self.read_images(paths)\n        else:\n            raise TypeError(\"The input data is inconsistent with expectations.\")\n\n        assert (\n            predicted_data != []\n        ), \"There is not any image to be predicted. Please check the input data.\"\n\n        all_results = []\n        for img in predicted_data:\n            if img is None:\n                logger.info(\"error in loading image\")\n                all_results.append([])\n                continue\n            starttime = time.time()\n            res, _ = self.layout_predictor(img)\n            elapse = time.time() - starttime\n            logger.info(\"Predict time: {}\".format(elapse))\n\n            for item in res:\n                item[\"bbox\"] = item[\"bbox\"].tolist()\n            all_results.append({\"layout\": res})\n        return all_results\n\n    @serving\n    def serving_method(self, images, **kwargs):\n        \"\"\"\n        Run as a service.\n        \"\"\"\n        images_decode = [base64_to_cv2(image) for image in images]\n        results = self.predict(images_decode, **kwargs)\n        return results\n\n\nif __name__ == \"__main__\":\n    layout = LayoutPredictor()\n    layout._initialize()\n    image_path = [\"./ppstructure/docs/table/1.png\"]\n    res = layout.predict(paths=image_path)\n    print(res)\n", "deploy/hubserving/structure_layout/params.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nclass Config(object):\n    pass\n\n\ndef read_params():\n    cfg = Config()\n\n    # params for layout analysis\n    cfg.layout_model_dir = \"./inference/picodet_lcnet_x1_0_fgd_layout_infer/\"\n    cfg.layout_dict_path = \"./ppocr/utils/dict/layout_dict/layout_publaynet_dict.txt\"\n    cfg.layout_score_threshold = 0.5\n    cfg.layout_nms_threshold = 0.5\n    return cfg\n", "deploy/hubserving/structure_layout/__init__.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n", "deploy/hubserving/structure_table/module.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\n\nsys.path.insert(0, \".\")\nimport copy\n\nimport time\nimport paddlehub\nfrom paddlehub.common.logger import logger\nfrom paddlehub.module.module import moduleinfo, runnable, serving\nimport cv2\nimport numpy as np\nimport paddlehub as hub\n\nfrom tools.infer.utility import base64_to_cv2\nfrom ppstructure.table.predict_table import TableSystem as _TableSystem\nfrom ppstructure.predict_system import save_structure_res\nfrom ppstructure.utility import parse_args\nfrom deploy.hubserving.structure_table.params import read_params\n\n\n@moduleinfo(\n    name=\"structure_table\",\n    version=\"1.0.0\",\n    summary=\"PP-Structure table service\",\n    author=\"paddle-dev\",\n    author_email=\"paddle-dev@baidu.com\",\n    type=\"cv/structure_table\",\n)\nclass TableSystem(hub.Module):\n    def _initialize(self, use_gpu=False, enable_mkldnn=False):\n        \"\"\"\n        initialize with the necessary elements\n        \"\"\"\n        cfg = self.merge_configs()\n        cfg.use_gpu = use_gpu\n        if use_gpu:\n            try:\n                _places = os.environ[\"CUDA_VISIBLE_DEVICES\"]\n                int(_places[0])\n                print(\"use gpu: \", use_gpu)\n                print(\"CUDA_VISIBLE_DEVICES: \", _places)\n                cfg.gpu_mem = 8000\n            except:\n                raise RuntimeError(\n                    \"Environment Variable CUDA_VISIBLE_DEVICES is not set correctly. If you wanna use gpu, please set CUDA_VISIBLE_DEVICES via export CUDA_VISIBLE_DEVICES=cuda_device_id.\"\n                )\n        cfg.ir_optim = True\n        cfg.enable_mkldnn = enable_mkldnn\n\n        self.table_sys = _TableSystem(cfg)\n\n    def merge_configs(self):\n        # deafult cfg\n        backup_argv = copy.deepcopy(sys.argv)\n        sys.argv = sys.argv[:1]\n        cfg = parse_args()\n\n        update_cfg_map = vars(read_params())\n\n        for key in update_cfg_map:\n            cfg.__setattr__(key, update_cfg_map[key])\n\n        sys.argv = copy.deepcopy(backup_argv)\n        return cfg\n\n    def read_images(self, paths=[]):\n        images = []\n        for img_path in paths:\n            assert os.path.isfile(img_path), \"The {} isn't a valid file.\".format(\n                img_path\n            )\n            img = cv2.imread(img_path)\n            if img is None:\n                logger.info(\"error in loading image:{}\".format(img_path))\n                continue\n            images.append(img)\n        return images\n\n    def predict(self, images=[], paths=[]):\n        \"\"\"\n        Get the chinese texts in the predicted images.\n        Args:\n            images (list(numpy.ndarray)): images data, shape of each is [H, W, C]. If images not paths\n            paths (list[str]): The paths of images. If paths not images\n        Returns:\n            res (list): The result of chinese texts and save path of images.\n        \"\"\"\n\n        if images != [] and isinstance(images, list) and paths == []:\n            predicted_data = images\n        elif images == [] and isinstance(paths, list) and paths != []:\n            predicted_data = self.read_images(paths)\n        else:\n            raise TypeError(\"The input data is inconsistent with expectations.\")\n\n        assert (\n            predicted_data != []\n        ), \"There is not any image to be predicted. Please check the input data.\"\n\n        all_results = []\n        for img in predicted_data:\n            if img is None:\n                logger.info(\"error in loading image\")\n                all_results.append([])\n                continue\n            starttime = time.time()\n            res, _ = self.table_sys(img)\n            elapse = time.time() - starttime\n            logger.info(\"Predict time: {}\".format(elapse))\n\n            all_results.append({\"html\": res[\"html\"]})\n        return all_results\n\n    @serving\n    def serving_method(self, images, **kwargs):\n        \"\"\"\n        Run as a service.\n        \"\"\"\n        images_decode = [base64_to_cv2(image) for image in images]\n        results = self.predict(images_decode, **kwargs)\n        return results\n\n\nif __name__ == \"__main__\":\n    table_system = TableSystem()\n    table_system._initialize()\n    image_path = [\"./ppstructure/docs/table/table.jpg\"]\n    res = table_system.predict(paths=image_path)\n    print(res)\n", "deploy/hubserving/structure_table/params.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom deploy.hubserving.ocr_system.params import read_params as pp_ocr_read_params\n\n\ndef read_params():\n    cfg = pp_ocr_read_params()\n\n    # params for table structure model\n    cfg.table_max_len = 488\n    cfg.table_model_dir = \"./inference/en_ppocr_mobile_v2.0_table_structure_infer/\"\n    cfg.table_char_dict_path = \"./ppocr/utils/dict/table_structure_dict.txt\"\n    cfg.show_log = False\n    return cfg\n", "deploy/hubserving/structure_table/__init__.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n", "deploy/hubserving/kie_ser/module.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\n\nsys.path.insert(0, \".\")\nimport copy\n\nimport time\nimport paddlehub\nfrom paddlehub.common.logger import logger\nfrom paddlehub.module.module import moduleinfo, runnable, serving\nimport cv2\nimport numpy as np\nimport paddlehub as hub\n\nfrom tools.infer.utility import base64_to_cv2\nfrom ppstructure.kie.predict_kie_token_ser import SerPredictor\nfrom ppstructure.utility import parse_args\n\nfrom deploy.hubserving.kie_ser.params import read_params\n\n\n@moduleinfo(\n    name=\"kie_ser\",\n    version=\"1.0.0\",\n    summary=\"kie ser service\",\n    author=\"paddle-dev\",\n    author_email=\"paddle-dev@baidu.com\",\n    type=\"cv/KIE_SER\",\n)\nclass KIESer(hub.Module):\n    def _initialize(self, use_gpu=False, enable_mkldnn=False):\n        \"\"\"\n        initialize with the necessary elements\n        \"\"\"\n        cfg = self.merge_configs()\n\n        cfg.use_gpu = use_gpu\n        if use_gpu:\n            try:\n                _places = os.environ[\"CUDA_VISIBLE_DEVICES\"]\n                int(_places[0])\n                print(\"use gpu: \", use_gpu)\n                print(\"CUDA_VISIBLE_DEVICES: \", _places)\n                cfg.gpu_mem = 8000\n            except:\n                raise RuntimeError(\n                    \"Environment Variable CUDA_VISIBLE_DEVICES is not set correctly. If you wanna use gpu, please set CUDA_VISIBLE_DEVICES via export CUDA_VISIBLE_DEVICES=cuda_device_id.\"\n                )\n        cfg.ir_optim = True\n        cfg.enable_mkldnn = enable_mkldnn\n\n        self.ser_predictor = SerPredictor(cfg)\n\n    def merge_configs(\n        self,\n    ):\n        # deafult cfg\n        backup_argv = copy.deepcopy(sys.argv)\n        sys.argv = sys.argv[:1]\n        cfg = parse_args()\n\n        update_cfg_map = vars(read_params())\n\n        for key in update_cfg_map:\n            cfg.__setattr__(key, update_cfg_map[key])\n\n        sys.argv = copy.deepcopy(backup_argv)\n        return cfg\n\n    def read_images(self, paths=[]):\n        images = []\n        for img_path in paths:\n            assert os.path.isfile(img_path), \"The {} isn't a valid file.\".format(\n                img_path\n            )\n            img = cv2.imread(img_path)\n            if img is None:\n                logger.info(\"error in loading image:{}\".format(img_path))\n                continue\n            images.append(img)\n        return images\n\n    def predict(self, images=[], paths=[]):\n        \"\"\"\n        Get the chinese texts in the predicted images.\n        Args:\n            images (list(numpy.ndarray)): images data, shape of each is [H, W, C]. If images not paths\n            paths (list[str]): The paths of images. If paths not images\n        Returns:\n            res (list): The result of chinese texts and save path of images.\n        \"\"\"\n        if images != [] and isinstance(images, list) and paths == []:\n            predicted_data = images\n        elif images == [] and isinstance(paths, list) and paths != []:\n            predicted_data = self.read_images(paths)\n        else:\n            raise TypeError(\"The input data is inconsistent with expectations.\")\n\n        assert (\n            predicted_data != []\n        ), \"There is not any image to be predicted. Please check the input data.\"\n\n        all_results = []\n        for img in predicted_data:\n            if img is None:\n                logger.info(\"error in loading image\")\n                all_results.append([])\n                continue\n            starttime = time.time()\n            ser_res, _, elapse = self.ser_predictor(img)\n            elapse = time.time() - starttime\n            logger.info(\"Predict time: {}\".format(elapse))\n            all_results.append(ser_res)\n        return all_results\n\n    @serving\n    def serving_method(self, images, **kwargs):\n        \"\"\"\n        Run as a service.\n        \"\"\"\n        images_decode = [base64_to_cv2(image) for image in images]\n        results = self.predict(images_decode, **kwargs)\n        return results\n\n\nif __name__ == \"__main__\":\n    ocr = KIESer()\n    ocr._initialize()\n    image_path = [\n        \"./doc/imgs/11.jpg\",\n        \"./doc/imgs/12.jpg\",\n    ]\n    res = ocr.predict(paths=image_path)\n    print(res)\n", "deploy/hubserving/kie_ser/params.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom deploy.hubserving.ocr_system.params import read_params as pp_ocr_read_params\n\n\nclass Config(object):\n    pass\n\n\ndef read_params():\n    cfg = pp_ocr_read_params()\n\n    # SER params\n    cfg.kie_algorithm = \"LayoutXLM\"\n    cfg.use_visual_backbone = False\n\n    cfg.ser_model_dir = \"./inference/ser_vi_layoutxlm_xfund_infer\"\n    cfg.ser_dict_path = \"train_data/XFUND/class_list_xfun.txt\"\n    cfg.vis_font_path = \"./doc/fonts/simfang.ttf\"\n    cfg.ocr_order_method = \"tb-yx\"\n\n    return cfg\n", "deploy/hubserving/kie_ser/__init__.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n", "deploy/hubserving/ocr_cls/module.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\n\nsys.path.insert(0, \".\")\nimport copy\nimport paddlehub\nfrom paddlehub.common.logger import logger\nfrom paddlehub.module.module import moduleinfo, runnable, serving\nimport cv2\nimport paddlehub as hub\n\nfrom tools.infer.utility import base64_to_cv2\nfrom tools.infer.predict_cls import TextClassifier\nfrom tools.infer.utility import parse_args\nfrom deploy.hubserving.ocr_cls.params import read_params\n\n\n@moduleinfo(\n    name=\"ocr_cls\",\n    version=\"1.0.0\",\n    summary=\"ocr angle cls service\",\n    author=\"paddle-dev\",\n    author_email=\"paddle-dev@baidu.com\",\n    type=\"cv/text_angle_cls\",\n)\nclass OCRCls(hub.Module):\n    def _initialize(self, use_gpu=False, enable_mkldnn=False):\n        \"\"\"\n        initialize with the necessary elements\n        \"\"\"\n        cfg = self.merge_configs()\n\n        cfg.use_gpu = use_gpu\n        if use_gpu:\n            try:\n                _places = os.environ[\"CUDA_VISIBLE_DEVICES\"]\n                int(_places[0])\n                print(\"use gpu: \", use_gpu)\n                print(\"CUDA_VISIBLE_DEVICES: \", _places)\n                cfg.gpu_mem = 8000\n            except:\n                raise RuntimeError(\n                    \"Environment Variable CUDA_VISIBLE_DEVICES is not set correctly. If you wanna use gpu, please set CUDA_VISIBLE_DEVICES via export CUDA_VISIBLE_DEVICES=cuda_device_id.\"\n                )\n        cfg.ir_optim = True\n        cfg.enable_mkldnn = enable_mkldnn\n\n        self.text_classifier = TextClassifier(cfg)\n\n    def merge_configs(\n        self,\n    ):\n        # deafult cfg\n        backup_argv = copy.deepcopy(sys.argv)\n        sys.argv = sys.argv[:1]\n        cfg = parse_args()\n\n        update_cfg_map = vars(read_params())\n\n        for key in update_cfg_map:\n            cfg.__setattr__(key, update_cfg_map[key])\n\n        sys.argv = copy.deepcopy(backup_argv)\n        return cfg\n\n    def read_images(self, paths=[]):\n        images = []\n        for img_path in paths:\n            assert os.path.isfile(img_path), \"The {} isn't a valid file.\".format(\n                img_path\n            )\n            img = cv2.imread(img_path)\n            if img is None:\n                logger.info(\"error in loading image:{}\".format(img_path))\n                continue\n            images.append(img)\n        return images\n\n    def predict(self, images=[], paths=[]):\n        \"\"\"\n        Get the text angle in the predicted images.\n        Args:\n            images (list(numpy.ndarray)): images data, shape of each is [H, W, C]. If images not paths\n            paths (list[str]): The paths of images. If paths not images\n        Returns:\n            res (list): The result of text detection box and save path of images.\n        \"\"\"\n\n        if images != [] and isinstance(images, list) and paths == []:\n            predicted_data = images\n        elif images == [] and isinstance(paths, list) and paths != []:\n            predicted_data = self.read_images(paths)\n        else:\n            raise TypeError(\"The input data is inconsistent with expectations.\")\n\n        assert (\n            predicted_data != []\n        ), \"There is not any image to be predicted. Please check the input data.\"\n\n        img_list = []\n        for img in predicted_data:\n            if img is None:\n                continue\n            img_list.append(img)\n\n        rec_res_final = []\n        try:\n            img_list, cls_res, predict_time = self.text_classifier(img_list)\n            for dno in range(len(cls_res)):\n                angle, score = cls_res[dno]\n                rec_res_final.append(\n                    {\n                        \"angle\": angle,\n                        \"confidence\": float(score),\n                    }\n                )\n        except Exception as e:\n            print(e)\n            return [[]]\n\n        return [rec_res_final]\n\n    @serving\n    def serving_method(self, images, **kwargs):\n        \"\"\"\n        Run as a service.\n        \"\"\"\n        images_decode = [base64_to_cv2(image) for image in images]\n        results = self.predict(images_decode, **kwargs)\n        return results\n\n\nif __name__ == \"__main__\":\n    ocr = OCRCls()\n    ocr._initialize()\n    image_path = [\n        \"./doc/imgs_words/ch/word_1.jpg\",\n        \"./doc/imgs_words/ch/word_2.jpg\",\n        \"./doc/imgs_words/ch/word_3.jpg\",\n    ]\n    res = ocr.predict(paths=image_path)\n    print(res)\n", "deploy/hubserving/ocr_cls/params.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nclass Config(object):\n    pass\n\n\ndef read_params():\n    cfg = Config()\n\n    # params for text classifier\n    cfg.cls_model_dir = \"./inference/ch_ppocr_mobile_v2.0_cls_infer/\"\n    cfg.cls_image_shape = \"3, 48, 192\"\n    cfg.label_list = [\"0\", \"180\"]\n    cfg.cls_batch_num = 30\n    cfg.cls_thresh = 0.9\n\n    cfg.use_pdserving = False\n    cfg.use_tensorrt = False\n\n    return cfg\n", "deploy/hubserving/ocr_cls/__init__.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n", "deploy/hubserving/kie_ser_re/module.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\n\nsys.path.insert(0, \".\")\nimport copy\n\nimport time\nimport paddlehub\nfrom paddlehub.common.logger import logger\nfrom paddlehub.module.module import moduleinfo, runnable, serving\nimport cv2\nimport numpy as np\nimport paddlehub as hub\n\nfrom tools.infer.utility import base64_to_cv2\nfrom ppstructure.kie.predict_kie_token_ser_re import SerRePredictor\nfrom ppstructure.utility import parse_args\n\nfrom deploy.hubserving.kie_ser_re.params import read_params\n\n\n@moduleinfo(\n    name=\"kie_ser_re\",\n    version=\"1.0.0\",\n    summary=\"kie ser re service\",\n    author=\"paddle-dev\",\n    author_email=\"paddle-dev@baidu.com\",\n    type=\"cv/KIE_SER_RE\",\n)\nclass KIESerRE(hub.Module):\n    def _initialize(self, use_gpu=False, enable_mkldnn=False):\n        \"\"\"\n        initialize with the necessary elements\n        \"\"\"\n        cfg = self.merge_configs()\n\n        cfg.use_gpu = use_gpu\n        if use_gpu:\n            try:\n                _places = os.environ[\"CUDA_VISIBLE_DEVICES\"]\n                int(_places[0])\n                print(\"use gpu: \", use_gpu)\n                print(\"CUDA_VISIBLE_DEVICES: \", _places)\n                cfg.gpu_mem = 8000\n            except:\n                raise RuntimeError(\n                    \"Environment Variable CUDA_VISIBLE_DEVICES is not set correctly. If you wanna use gpu, please set CUDA_VISIBLE_DEVICES via export CUDA_VISIBLE_DEVICES=cuda_device_id.\"\n                )\n        cfg.ir_optim = True\n        cfg.enable_mkldnn = enable_mkldnn\n\n        self.ser_re_predictor = SerRePredictor(cfg)\n\n    def merge_configs(\n        self,\n    ):\n        # deafult cfg\n        backup_argv = copy.deepcopy(sys.argv)\n        sys.argv = sys.argv[:1]\n        cfg = parse_args()\n\n        update_cfg_map = vars(read_params())\n\n        for key in update_cfg_map:\n            cfg.__setattr__(key, update_cfg_map[key])\n\n        sys.argv = copy.deepcopy(backup_argv)\n        return cfg\n\n    def read_images(self, paths=[]):\n        images = []\n        for img_path in paths:\n            assert os.path.isfile(img_path), \"The {} isn't a valid file.\".format(\n                img_path\n            )\n            img = cv2.imread(img_path)\n            if img is None:\n                logger.info(\"error in loading image:{}\".format(img_path))\n                continue\n            images.append(img)\n        return images\n\n    def predict(self, images=[], paths=[]):\n        \"\"\"\n        Get the chinese texts in the predicted images.\n        Args:\n            images (list(numpy.ndarray)): images data, shape of each is [H, W, C]. If images not paths\n            paths (list[str]): The paths of images. If paths not images\n        Returns:\n            res (list): The result of chinese texts and save path of images.\n        \"\"\"\n        if images != [] and isinstance(images, list) and paths == []:\n            predicted_data = images\n        elif images == [] and isinstance(paths, list) and paths != []:\n            predicted_data = self.read_images(paths)\n        else:\n            raise TypeError(\"The input data is inconsistent with expectations.\")\n\n        assert (\n            predicted_data != []\n        ), \"There is not any image to be predicted. Please check the input data.\"\n\n        all_results = []\n        for img in predicted_data:\n            if img is None:\n                logger.info(\"error in loading image\")\n                all_results.append([])\n                continue\n            print(img.shape)\n            starttime = time.time()\n            re_res, _ = self.ser_re_predictor(img)\n            print(re_res)\n            elapse = time.time() - starttime\n            logger.info(\"Predict time: {}\".format(elapse))\n            all_results.append(re_res)\n        return all_results\n\n    @serving\n    def serving_method(self, images, **kwargs):\n        \"\"\"\n        Run as a service.\n        \"\"\"\n        images_decode = [base64_to_cv2(image) for image in images]\n        results = self.predict(images_decode, **kwargs)\n        return results\n\n\nif __name__ == \"__main__\":\n    ocr = KIESerRE()\n    ocr._initialize()\n    image_path = [\n        \"./doc/imgs/11.jpg\",\n        \"./doc/imgs/12.jpg\",\n    ]\n    res = ocr.predict(paths=image_path)\n    print(res)\n", "deploy/hubserving/kie_ser_re/params.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom deploy.hubserving.ocr_system.params import read_params as pp_ocr_read_params\n\n\nclass Config(object):\n    pass\n\n\ndef read_params():\n    cfg = pp_ocr_read_params()\n\n    # SER params\n    cfg.kie_algorithm = \"LayoutXLM\"\n    cfg.use_visual_backbone = False\n\n    cfg.ser_model_dir = \"./inference/ser_vi_layoutxlm_xfund_infer\"\n    cfg.re_model_dir = \"./inference/re_vi_layoutxlm_xfund_infer\"\n\n    cfg.ser_dict_path = \"train_data/XFUND/class_list_xfun.txt\"\n    cfg.vis_font_path = \"./doc/fonts/simfang.ttf\"\n    cfg.ocr_order_method = \"tb-yx\"\n\n    return cfg\n", "deploy/hubserving/kie_ser_re/__init__.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n", "deploy/avh/convert_image.py": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nimport os\nimport pathlib\nimport re\nimport sys\nimport cv2\nimport math\nfrom PIL import Image\nimport numpy as np\n\n\ndef resize_norm_img(img, image_shape, padding=True):\n    imgC, imgH, imgW = image_shape\n    h = img.shape[0]\n    w = img.shape[1]\n    if not padding:\n        resized_image = cv2.resize(img, (imgW, imgH), interpolation=cv2.INTER_LINEAR)\n        resized_w = imgW\n    else:\n        ratio = w / float(h)\n        if math.ceil(imgH * ratio) > imgW:\n            resized_w = imgW\n        else:\n            resized_w = int(math.ceil(imgH * ratio))\n        resized_image = cv2.resize(img, (resized_w, imgH))\n    resized_image = resized_image.astype(\"float32\")\n    if image_shape[0] == 1:\n        resized_image = resized_image / 255\n        resized_image = resized_image[np.newaxis, :]\n    else:\n        resized_image = resized_image.transpose((2, 0, 1)) / 255\n    resized_image -= 0.5\n    resized_image /= 0.5\n    padding_im = np.zeros((imgC, imgH, imgW), dtype=np.float32)\n    padding_im[:, :, 0:resized_w] = resized_image\n    return padding_im\n\n\ndef create_header_file(name, tensor_name, tensor_data, output_path):\n    \"\"\"\n    This function generates a header file containing the data from the numpy array provided.\n    \"\"\"\n    file_path = pathlib.Path(f\"{output_path}/\" + name).resolve()\n    # Create header file with npy_data as a C array\n    raw_path = file_path.with_suffix(\".h\").resolve()\n    with open(raw_path, \"w\") as header_file:\n        header_file.write(\n            \"\\n\"\n            + f\"const size_t {tensor_name}_len = {tensor_data.size};\\n\"\n            + f'__attribute__((section(\".data.tvm\"), aligned(16))) float {tensor_name}[] = '\n        )\n\n        header_file.write(\"{\")\n        for i in np.ndindex(tensor_data.shape):\n            header_file.write(f\"{tensor_data[i]}, \")\n        header_file.write(\"};\\n\\n\")\n\n\ndef create_headers(image_name):\n    \"\"\"\n    This function generates C header files for the input and output arrays required to run inferences\n    \"\"\"\n    img_path = os.path.join(\"./\", f\"{image_name}\")\n\n    # Resize image to 32x320\n    img = cv2.imread(img_path)\n    img = resize_norm_img(img, [3, 32, 320])\n    img_data = img.astype(\"float32\")\n\n    # # Add the batch dimension, as we are expecting 4-dimensional input: NCHW.\n    img_data = np.expand_dims(img_data, axis=0)\n\n    # Create input header file\n    create_header_file(\"inputs\", \"input\", img_data, \"./include\")\n    # Create output header file\n    output_data = np.zeros([7760], np.float32)\n    create_header_file(\n        \"outputs\",\n        \"output\",\n        output_data,\n        \"./include\",\n    )\n\n\nif __name__ == \"__main__\":\n    create_headers(sys.argv[1])\n", "deploy/slim/quantization/quant.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(__dir__)\nsys.path.append(os.path.abspath(os.path.join(__dir__, \"..\", \"..\", \"..\")))\nsys.path.append(os.path.abspath(os.path.join(__dir__, \"..\", \"..\", \"..\", \"tools\")))\n\nimport yaml\nimport paddle\nimport paddle.distributed as dist\n\npaddle.seed(2)\n\nfrom ppocr.data import build_dataloader, set_signal_handlers\nfrom ppocr.modeling.architectures import build_model\nfrom ppocr.losses import build_loss\nfrom ppocr.optimizer import build_optimizer\nfrom ppocr.postprocess import build_post_process\nfrom ppocr.metrics import build_metric\nfrom ppocr.utils.save_load import load_model\nimport tools.program as program\nfrom paddleslim.dygraph.quant import QAT\n\ndist.get_world_size()\n\n\nclass PACT(paddle.nn.Layer):\n    def __init__(self):\n        super(PACT, self).__init__()\n        alpha_attr = paddle.ParamAttr(\n            name=self.full_name() + \".pact\",\n            initializer=paddle.nn.initializer.Constant(value=20),\n            learning_rate=1.0,\n            regularizer=paddle.regularizer.L2Decay(2e-5),\n        )\n\n        self.alpha = self.create_parameter(shape=[1], attr=alpha_attr, dtype=\"float32\")\n\n    def forward(self, x):\n        out_left = paddle.nn.functional.relu(x - self.alpha)\n        out_right = paddle.nn.functional.relu(-self.alpha - x)\n        x = x - out_left + out_right\n        return x\n\n\nquant_config = {\n    # weight preprocess type, default is None and no preprocessing is performed.\n    \"weight_preprocess_type\": None,\n    # activation preprocess type, default is None and no preprocessing is performed.\n    \"activation_preprocess_type\": None,\n    # weight quantize type, default is 'channel_wise_abs_max'\n    \"weight_quantize_type\": \"channel_wise_abs_max\",\n    # activation quantize type, default is 'moving_average_abs_max'\n    \"activation_quantize_type\": \"moving_average_abs_max\",\n    # weight quantize bit num, default is 8\n    \"weight_bits\": 8,\n    # activation quantize bit num, default is 8\n    \"activation_bits\": 8,\n    # data type after quantization, such as 'uint8', 'int8', etc. default is 'int8'\n    \"dtype\": \"int8\",\n    # window size for 'range_abs_max' quantization. default is 10000\n    \"window_size\": 10000,\n    # The decay coefficient of moving average, default is 0.9\n    \"moving_rate\": 0.9,\n    # for dygraph quantization, layers of type in quantizable_layer_type will be quantized\n    \"quantizable_layer_type\": [\"Conv2D\", \"Linear\"],\n}\n\n\ndef main(config, device, logger, vdl_writer):\n    # init dist environment\n    if config[\"Global\"][\"distributed\"]:\n        dist.init_parallel_env()\n\n    global_config = config[\"Global\"]\n\n    # build dataloader\n    set_signal_handlers()\n    train_dataloader = build_dataloader(config, \"Train\", device, logger)\n    if config[\"Eval\"]:\n        valid_dataloader = build_dataloader(config, \"Eval\", device, logger)\n    else:\n        valid_dataloader = None\n\n    # build post process\n    post_process_class = build_post_process(config[\"PostProcess\"], global_config)\n\n    # build model\n    # for rec algorithm\n    if hasattr(post_process_class, \"character\"):\n        char_num = len(getattr(post_process_class, \"character\"))\n        if config[\"Architecture\"][\"algorithm\"] in [\n            \"Distillation\",\n        ]:  # distillation model\n            for key in config[\"Architecture\"][\"Models\"]:\n                if (\n                    config[\"Architecture\"][\"Models\"][key][\"Head\"][\"name\"] == \"MultiHead\"\n                ):  # for multi head\n                    if config[\"PostProcess\"][\"name\"] == \"DistillationSARLabelDecode\":\n                        char_num = char_num - 2\n                    # update SARLoss params\n                    assert (\n                        list(config[\"Loss\"][\"loss_config_list\"][-1].keys())[0]\n                        == \"DistillationSARLoss\"\n                    )\n                    config[\"Loss\"][\"loss_config_list\"][-1][\"DistillationSARLoss\"][\n                        \"ignore_index\"\n                    ] = (char_num + 1)\n                    out_channels_list = {}\n                    out_channels_list[\"CTCLabelDecode\"] = char_num\n                    out_channels_list[\"SARLabelDecode\"] = char_num + 2\n                    config[\"Architecture\"][\"Models\"][key][\"Head\"][\n                        \"out_channels_list\"\n                    ] = out_channels_list\n                else:\n                    config[\"Architecture\"][\"Models\"][key][\"Head\"][\n                        \"out_channels\"\n                    ] = char_num\n        elif config[\"Architecture\"][\"Head\"][\"name\"] == \"MultiHead\":  # for multi head\n            if config[\"PostProcess\"][\"name\"] == \"SARLabelDecode\":\n                char_num = char_num - 2\n            # update SARLoss params\n            assert list(config[\"Loss\"][\"loss_config_list\"][1].keys())[0] == \"SARLoss\"\n            if config[\"Loss\"][\"loss_config_list\"][1][\"SARLoss\"] is None:\n                config[\"Loss\"][\"loss_config_list\"][1][\"SARLoss\"] = {\n                    \"ignore_index\": char_num + 1\n                }\n            else:\n                config[\"Loss\"][\"loss_config_list\"][1][\"SARLoss\"][\"ignore_index\"] = (\n                    char_num + 1\n                )\n            out_channels_list = {}\n            out_channels_list[\"CTCLabelDecode\"] = char_num\n            out_channels_list[\"SARLabelDecode\"] = char_num + 2\n            config[\"Architecture\"][\"Head\"][\"out_channels_list\"] = out_channels_list\n        else:  # base rec model\n            config[\"Architecture\"][\"Head\"][\"out_channels\"] = char_num\n\n        if config[\"PostProcess\"][\"name\"] == \"SARLabelDecode\":  # for SAR model\n            config[\"Loss\"][\"ignore_index\"] = char_num - 1\n    model = build_model(config[\"Architecture\"])\n\n    pre_best_model_dict = dict()\n    # load fp32 model to begin quantization\n    pre_best_model_dict = load_model(\n        config, model, None, config[\"Architecture\"][\"model_type\"]\n    )\n\n    freeze_params = False\n    if config[\"Architecture\"][\"algorithm\"] in [\"Distillation\"]:\n        for key in config[\"Architecture\"][\"Models\"]:\n            freeze_params = freeze_params or config[\"Architecture\"][\"Models\"][key].get(\n                \"freeze_params\", False\n            )\n    act = None if freeze_params else PACT\n    quanter = QAT(config=quant_config, act_preprocess=act)\n    quanter.quantize(model)\n\n    if config[\"Global\"][\"distributed\"]:\n        model = paddle.DataParallel(model)\n\n    # build loss\n    loss_class = build_loss(config[\"Loss\"])\n\n    # build optim\n    optimizer, lr_scheduler = build_optimizer(\n        config[\"Optimizer\"],\n        epochs=config[\"Global\"][\"epoch_num\"],\n        step_each_epoch=len(train_dataloader),\n        model=model,\n    )\n\n    # resume PACT training process\n    pre_best_model_dict = load_model(\n        config, model, optimizer, config[\"Architecture\"][\"model_type\"]\n    )\n\n    # build metric\n    eval_class = build_metric(config[\"Metric\"])\n\n    logger.info(\n        \"train dataloader has {} iters, valid dataloader has {} iters\".format(\n            len(train_dataloader), len(valid_dataloader)\n        )\n    )\n\n    # start train\n    program.train(\n        config,\n        train_dataloader,\n        valid_dataloader,\n        device,\n        model,\n        loss_class,\n        optimizer,\n        lr_scheduler,\n        post_process_class,\n        eval_class,\n        pre_best_model_dict,\n        logger,\n        vdl_writer,\n    )\n\n\nif __name__ == \"__main__\":\n    config, device, logger, vdl_writer = program.preprocess(is_train=True)\n    main(config, device, logger, vdl_writer)\n", "deploy/slim/quantization/export_model.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport sys\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(__dir__)\nsys.path.insert(0, os.path.abspath(os.path.join(__dir__, \"..\", \"..\", \"..\")))\nsys.path.insert(0, os.path.abspath(os.path.join(__dir__, \"..\", \"..\", \"..\", \"tools\")))\n\nimport argparse\n\nimport paddle\nfrom paddle.jit import to_static\n\nfrom ppocr.modeling.architectures import build_model\nfrom ppocr.postprocess import build_post_process\nfrom ppocr.utils.save_load import load_model\nfrom ppocr.utils.logging import get_logger\nfrom tools.program import load_config, merge_config, ArgsParser\nfrom ppocr.metrics import build_metric\nimport tools.program as program\nfrom paddleslim.dygraph.quant import QAT\nfrom ppocr.data import build_dataloader, set_signal_handlers\nfrom tools.export_model import export_single_model\n\n\ndef main():\n    ############################################################################################################\n    # 1. quantization configs\n    ############################################################################################################\n    quant_config = {\n        # weight preprocess type, default is None and no preprocessing is performed.\n        \"weight_preprocess_type\": None,\n        # activation preprocess type, default is None and no preprocessing is performed.\n        \"activation_preprocess_type\": None,\n        # weight quantize type, default is 'channel_wise_abs_max'\n        \"weight_quantize_type\": \"channel_wise_abs_max\",\n        # activation quantize type, default is 'moving_average_abs_max'\n        \"activation_quantize_type\": \"moving_average_abs_max\",\n        # weight quantize bit num, default is 8\n        \"weight_bits\": 8,\n        # activation quantize bit num, default is 8\n        \"activation_bits\": 8,\n        # data type after quantization, such as 'uint8', 'int8', etc. default is 'int8'\n        \"dtype\": \"int8\",\n        # window size for 'range_abs_max' quantization. default is 10000\n        \"window_size\": 10000,\n        # The decay coefficient of moving average, default is 0.9\n        \"moving_rate\": 0.9,\n        # for dygraph quantization, layers of type in quantizable_layer_type will be quantized\n        \"quantizable_layer_type\": [\"Conv2D\", \"Linear\"],\n    }\n    FLAGS = ArgsParser().parse_args()\n    config = load_config(FLAGS.config)\n    config = merge_config(config, FLAGS.opt)\n    logger = get_logger()\n    # build post process\n\n    post_process_class = build_post_process(config[\"PostProcess\"], config[\"Global\"])\n\n    # build model\n    if hasattr(post_process_class, \"character\"):\n        char_num = len(getattr(post_process_class, \"character\"))\n        if config[\"Architecture\"][\"algorithm\"] in [\n            \"Distillation\",\n        ]:  # distillation model\n            for key in config[\"Architecture\"][\"Models\"]:\n                if (\n                    config[\"Architecture\"][\"Models\"][key][\"Head\"][\"name\"] == \"MultiHead\"\n                ):  # for multi head\n                    if config[\"PostProcess\"][\"name\"] == \"DistillationSARLabelDecode\":\n                        char_num = char_num - 2\n                    # update SARLoss params\n                    assert (\n                        list(config[\"Loss\"][\"loss_config_list\"][-1].keys())[0]\n                        == \"DistillationSARLoss\"\n                    )\n                    config[\"Loss\"][\"loss_config_list\"][-1][\"DistillationSARLoss\"][\n                        \"ignore_index\"\n                    ] = (char_num + 1)\n                    out_channels_list = {}\n                    out_channels_list[\"CTCLabelDecode\"] = char_num\n                    out_channels_list[\"SARLabelDecode\"] = char_num + 2\n                    config[\"Architecture\"][\"Models\"][key][\"Head\"][\n                        \"out_channels_list\"\n                    ] = out_channels_list\n                else:\n                    config[\"Architecture\"][\"Models\"][key][\"Head\"][\n                        \"out_channels\"\n                    ] = char_num\n        elif config[\"Architecture\"][\"Head\"][\"name\"] == \"MultiHead\":  # for multi head\n            if config[\"PostProcess\"][\"name\"] == \"SARLabelDecode\":\n                char_num = char_num - 2\n            # update SARLoss params\n            assert list(config[\"Loss\"][\"loss_config_list\"][1].keys())[0] == \"SARLoss\"\n            if config[\"Loss\"][\"loss_config_list\"][1][\"SARLoss\"] is None:\n                config[\"Loss\"][\"loss_config_list\"][1][\"SARLoss\"] = {\n                    \"ignore_index\": char_num + 1\n                }\n            else:\n                config[\"Loss\"][\"loss_config_list\"][1][\"SARLoss\"][\"ignore_index\"] = (\n                    char_num + 1\n                )\n            out_channels_list = {}\n            out_channels_list[\"CTCLabelDecode\"] = char_num\n            out_channels_list[\"SARLabelDecode\"] = char_num + 2\n            config[\"Architecture\"][\"Head\"][\"out_channels_list\"] = out_channels_list\n        else:  # base rec model\n            config[\"Architecture\"][\"Head\"][\"out_channels\"] = char_num\n\n        if config[\"PostProcess\"][\"name\"] == \"SARLabelDecode\":  # for SAR model\n            config[\"Loss\"][\"ignore_index\"] = char_num - 1\n\n    model = build_model(config[\"Architecture\"])\n\n    # get QAT model\n    quanter = QAT(config=quant_config)\n    quanter.quantize(model)\n\n    load_model(config, model)\n\n    # build metric\n    eval_class = build_metric(config[\"Metric\"])\n\n    # build dataloader\n    set_signal_handlers()\n    valid_dataloader = build_dataloader(config, \"Eval\", device, logger)\n\n    use_srn = config[\"Architecture\"][\"algorithm\"] == \"SRN\"\n    model_type = config[\"Architecture\"].get(\"model_type\", None)\n    # start eval\n    metric = program.eval(\n        model, valid_dataloader, post_process_class, eval_class, model_type, use_srn\n    )\n    model.eval()\n\n    logger.info(\"metric eval ***************\")\n    for k, v in metric.items():\n        logger.info(\"{}:{}\".format(k, v))\n\n    save_path = config[\"Global\"][\"save_inference_dir\"]\n\n    arch_config = config[\"Architecture\"]\n\n    if (\n        arch_config[\"algorithm\"] == \"SVTR\"\n        and arch_config[\"Head\"][\"name\"] != \"MultiHead\"\n    ):\n        input_shape = config[\"Eval\"][\"dataset\"][\"transforms\"][-2][\"SVTRRecResizeImg\"][\n            \"image_shape\"\n        ]\n    else:\n        input_shape = None\n\n    if arch_config[\"algorithm\"] in [\n        \"Distillation\",\n    ]:  # distillation model\n        archs = list(arch_config[\"Models\"].values())\n        for idx, name in enumerate(model.model_name_list):\n            sub_model_save_path = os.path.join(save_path, name, \"inference\")\n            export_single_model(\n                model.model_list[idx],\n                archs[idx],\n                sub_model_save_path,\n                logger,\n                input_shape,\n                quanter,\n            )\n    else:\n        save_path = os.path.join(save_path, \"inference\")\n        export_single_model(model, arch_config, save_path, logger, input_shape, quanter)\n\n\nif __name__ == \"__main__\":\n    config, device, logger, vdl_writer = program.preprocess()\n    main()\n", "deploy/slim/quantization/quant_kl.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(__dir__)\nsys.path.append(os.path.abspath(os.path.join(__dir__, \"..\", \"..\", \"..\")))\nsys.path.append(os.path.abspath(os.path.join(__dir__, \"..\", \"..\", \"..\", \"tools\")))\n\nimport yaml\nimport paddle\nimport paddle.distributed as dist\n\npaddle.seed(2)\n\nfrom ppocr.data import build_dataloader, set_signal_handlers\nfrom ppocr.modeling.architectures import build_model\nfrom ppocr.losses import build_loss\nfrom ppocr.optimizer import build_optimizer\nfrom ppocr.postprocess import build_post_process\nfrom ppocr.metrics import build_metric\nfrom ppocr.utils.save_load import load_model\nimport tools.program as program\nimport paddleslim\nfrom paddleslim.dygraph.quant import QAT\nimport numpy as np\n\ndist.get_world_size()\n\n\nclass PACT(paddle.nn.Layer):\n    def __init__(self):\n        super(PACT, self).__init__()\n        alpha_attr = paddle.ParamAttr(\n            name=self.full_name() + \".pact\",\n            initializer=paddle.nn.initializer.Constant(value=20),\n            learning_rate=1.0,\n            regularizer=paddle.regularizer.L2Decay(2e-5),\n        )\n\n        self.alpha = self.create_parameter(shape=[1], attr=alpha_attr, dtype=\"float32\")\n\n    def forward(self, x):\n        out_left = paddle.nn.functional.relu(x - self.alpha)\n        out_right = paddle.nn.functional.relu(-self.alpha - x)\n        x = x - out_left + out_right\n        return x\n\n\nquant_config = {\n    # weight preprocess type, default is None and no preprocessing is performed.\n    \"weight_preprocess_type\": None,\n    # activation preprocess type, default is None and no preprocessing is performed.\n    \"activation_preprocess_type\": None,\n    # weight quantize type, default is 'channel_wise_abs_max'\n    \"weight_quantize_type\": \"channel_wise_abs_max\",\n    # activation quantize type, default is 'moving_average_abs_max'\n    \"activation_quantize_type\": \"moving_average_abs_max\",\n    # weight quantize bit num, default is 8\n    \"weight_bits\": 8,\n    # activation quantize bit num, default is 8\n    \"activation_bits\": 8,\n    # data type after quantization, such as 'uint8', 'int8', etc. default is 'int8'\n    \"dtype\": \"int8\",\n    # window size for 'range_abs_max' quantization. default is 10000\n    \"window_size\": 10000,\n    # The decay coefficient of moving average, default is 0.9\n    \"moving_rate\": 0.9,\n    # for dygraph quantization, layers of type in quantizable_layer_type will be quantized\n    \"quantizable_layer_type\": [\"Conv2D\", \"Linear\"],\n}\n\n\ndef sample_generator(loader):\n    def __reader__():\n        for indx, data in enumerate(loader):\n            images = np.array(data[0])\n            yield images\n\n    return __reader__\n\n\ndef sample_generator_layoutxlm_ser(loader):\n    def __reader__():\n        for indx, data in enumerate(loader):\n            input_ids = np.array(data[0])\n            bbox = np.array(data[1])\n            attention_mask = np.array(data[2])\n            token_type_ids = np.array(data[3])\n            images = np.array(data[4])\n            yield [input_ids, bbox, attention_mask, token_type_ids, images]\n\n    return __reader__\n\n\ndef main(config, device, logger, vdl_writer):\n    # init dist environment\n    if config[\"Global\"][\"distributed\"]:\n        dist.init_parallel_env()\n\n    global_config = config[\"Global\"]\n\n    # build dataloader\n    set_signal_handlers()\n    config[\"Train\"][\"loader\"][\"num_workers\"] = 0\n    is_layoutxlm_ser = (\n        config[\"Architecture\"][\"model_type\"] == \"kie\"\n        and config[\"Architecture\"][\"Backbone\"][\"name\"] == \"LayoutXLMForSer\"\n    )\n    train_dataloader = build_dataloader(config, \"Train\", device, logger)\n    if config[\"Eval\"]:\n        config[\"Eval\"][\"loader\"][\"num_workers\"] = 0\n        valid_dataloader = build_dataloader(config, \"Eval\", device, logger)\n        if is_layoutxlm_ser:\n            train_dataloader = valid_dataloader\n    else:\n        valid_dataloader = None\n\n    paddle.enable_static()\n    exe = paddle.static.Executor(device)\n\n    if \"inference_model\" in global_config.keys():  # , 'inference_model'):\n        inference_model_dir = global_config[\"inference_model\"]\n    else:\n        inference_model_dir = os.path.dirname(global_config[\"pretrained_model\"])\n        if not (\n            os.path.exists(os.path.join(inference_model_dir, \"inference.pdmodel\"))\n            and os.path.exists(os.path.join(inference_model_dir, \"inference.pdiparams\"))\n        ):\n            raise ValueError(\n                \"Please set inference model dir in Global.inference_model or Global.pretrained_model for post-quantization\"\n            )\n\n    if is_layoutxlm_ser:\n        generator = sample_generator_layoutxlm_ser(train_dataloader)\n    else:\n        generator = sample_generator(train_dataloader)\n\n    paddleslim.quant.quant_post_static(\n        executor=exe,\n        model_dir=inference_model_dir,\n        model_filename=\"inference.pdmodel\",\n        params_filename=\"inference.pdiparams\",\n        quantize_model_path=global_config[\"save_inference_dir\"],\n        sample_generator=generator,\n        save_model_filename=\"inference.pdmodel\",\n        save_params_filename=\"inference.pdiparams\",\n        batch_size=1,\n        batch_nums=None,\n    )\n\n\nif __name__ == \"__main__\":\n    config, device, logger, vdl_writer = program.preprocess(is_train=True)\n    main(config, device, logger, vdl_writer)\n", "deploy/slim/auto_compression/run.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport logging\nfrom tqdm import tqdm\nimport numpy as np\nimport argparse\nimport paddle\nfrom paddleslim.common import load_config as load_slim_config\nfrom paddleslim.common import get_logger\nfrom paddleslim.auto_compression import AutoCompression\nfrom paddleslim.common.dataloader import get_feed_vars\n\nimport sys\n\nsys.path.append(\"../../../\")\nfrom ppocr.data import build_dataloader\nfrom ppocr.postprocess import build_post_process\nfrom ppocr.metrics import build_metric\n\nlogger = get_logger(__name__, level=logging.INFO)\n\n\ndef argsparser():\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(\n        \"--config_path\",\n        type=str,\n        default=None,\n        help=\"path of compression strategy config.\",\n        required=True,\n    )\n    parser.add_argument(\n        \"--save_dir\",\n        type=str,\n        default=\"output\",\n        help=\"directory to save compressed model.\",\n    )\n    parser.add_argument(\n        \"--devices\", type=str, default=\"gpu\", help=\"which device used to compress.\"\n    )\n    return parser\n\n\ndef reader_wrapper(reader, input_name):\n    if isinstance(input_name, list) and len(input_name) == 1:\n        input_name = input_name[0]\n\n    def gen():  # \u5f62\u6210\u4e00\u4e2a\u5b57\u5178\u8f93\u5165\n        for i, batch in enumerate(reader()):\n            yield {input_name: batch[0]}\n\n    return gen\n\n\ndef eval_function(exe, compiled_test_program, test_feed_names, test_fetch_list):\n    post_process_class = build_post_process(all_config[\"PostProcess\"], global_config)\n    eval_class = build_metric(all_config[\"Metric\"])\n    model_type = global_config[\"model_type\"]\n\n    with tqdm(\n        total=len(val_loader),\n        bar_format=\"Evaluation stage, Run batch:|{bar}| {n_fmt}/{total_fmt}\",\n        ncols=80,\n    ) as t:\n        for batch_id, batch in enumerate(val_loader):\n            images = batch[0]\n\n            try:\n                (preds,) = exe.run(\n                    compiled_test_program,\n                    feed={test_feed_names[0]: images},\n                    fetch_list=test_fetch_list,\n                )\n            except:\n                preds, _ = exe.run(\n                    compiled_test_program,\n                    feed={test_feed_names[0]: images},\n                    fetch_list=test_fetch_list,\n                )\n\n            batch_numpy = []\n            for item in batch:\n                batch_numpy.append(np.array(item))\n\n            if model_type == \"det\":\n                preds_map = {\"maps\": preds}\n                post_result = post_process_class(preds_map, batch_numpy[1])\n                eval_class(post_result, batch_numpy)\n            elif model_type == \"rec\":\n                post_result = post_process_class(preds, batch_numpy[1])\n                eval_class(post_result, batch_numpy)\n            t.update()\n        metric = eval_class.get_metric()\n    logger.info(\"metric eval ***************\")\n    for k, v in metric.items():\n        logger.info(\"{}:{}\".format(k, v))\n\n    if model_type == \"det\":\n        return metric[\"hmean\"]\n    elif model_type == \"rec\":\n        return metric[\"acc\"]\n    return metric\n\n\ndef main():\n    rank_id = paddle.distributed.get_rank()\n    if args.devices == \"gpu\":\n        place = paddle.CUDAPlace(rank_id)\n        paddle.set_device(\"gpu\")\n    else:\n        place = paddle.CPUPlace()\n        paddle.set_device(\"cpu\")\n\n    global all_config, global_config\n    all_config = load_slim_config(args.config_path)\n\n    if \"Global\" not in all_config:\n        raise KeyError(f\"Key 'Global' not found in config file. \\n{all_config}\")\n    global_config = all_config[\"Global\"]\n\n    gpu_num = paddle.distributed.get_world_size()\n\n    train_dataloader = build_dataloader(all_config, \"Train\", args.devices, logger)\n\n    global val_loader\n    val_loader = build_dataloader(all_config, \"Eval\", args.devices, logger)\n\n    if (\n        isinstance(all_config[\"TrainConfig\"][\"learning_rate\"], dict)\n        and all_config[\"TrainConfig\"][\"learning_rate\"][\"type\"] == \"CosineAnnealingDecay\"\n    ):\n        steps = len(train_dataloader) * all_config[\"TrainConfig\"][\"epochs\"]\n        all_config[\"TrainConfig\"][\"learning_rate\"][\"T_max\"] = steps\n        print(\"total training steps:\", steps)\n\n    global_config[\"input_name\"] = get_feed_vars(\n        global_config[\"model_dir\"],\n        global_config[\"model_filename\"],\n        global_config[\"params_filename\"],\n    )\n\n    ac = AutoCompression(\n        model_dir=global_config[\"model_dir\"],\n        model_filename=global_config[\"model_filename\"],\n        params_filename=global_config[\"params_filename\"],\n        save_dir=args.save_dir,\n        config=all_config,\n        train_dataloader=reader_wrapper(train_dataloader, global_config[\"input_name\"]),\n        eval_callback=eval_function if rank_id == 0 else None,\n        eval_dataloader=reader_wrapper(val_loader, global_config[\"input_name\"]),\n    )\n    ac.compress()\n\n\nif __name__ == \"__main__\":\n    paddle.enable_static()\n    parser = argsparser()\n    args = parser.parse_args()\n    main()\n", "deploy/slim/prune/export_prune_model.py": "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\n\n__dir__ = os.path.dirname(__file__)\nsys.path.append(__dir__)\nsys.path.append(os.path.join(__dir__, \"..\", \"..\", \"..\"))\nsys.path.append(os.path.join(__dir__, \"..\", \"..\", \"..\", \"tools\"))\n\nimport paddle\nfrom ppocr.data import build_dataloader, set_signal_handlers\nfrom ppocr.modeling.architectures import build_model\n\nfrom ppocr.postprocess import build_post_process\nfrom ppocr.metrics import build_metric\nfrom ppocr.utils.save_load import load_model\nimport tools.program as program\n\n\ndef main(config, device, logger, vdl_writer):\n    global_config = config[\"Global\"]\n\n    # build dataloader\n    set_signal_handlers()\n    valid_dataloader = build_dataloader(config, \"Eval\", device, logger)\n\n    # build post process\n    post_process_class = build_post_process(config[\"PostProcess\"], global_config)\n\n    # build model\n    # for rec algorithm\n    if hasattr(post_process_class, \"character\"):\n        char_num = len(getattr(post_process_class, \"character\"))\n        config[\"Architecture\"][\"Head\"][\"out_channels\"] = char_num\n    model = build_model(config[\"Architecture\"])\n\n    if config[\"Architecture\"][\"model_type\"] == \"det\":\n        input_shape = [1, 3, 640, 640]\n    elif config[\"Architecture\"][\"model_type\"] == \"rec\":\n        input_shape = [1, 3, 32, 320]\n\n    flops = paddle.flops(model, input_shape)\n    logger.info(\"FLOPs before pruning: {}\".format(flops))\n\n    from paddleslim.dygraph import FPGMFilterPruner\n\n    model.train()\n    pruner = FPGMFilterPruner(model, input_shape)\n\n    # build metric\n    eval_class = build_metric(config[\"Metric\"])\n\n    def eval_fn():\n        metric = program.eval(model, valid_dataloader, post_process_class, eval_class)\n        if config[\"Architecture\"][\"model_type\"] == \"det\":\n            main_indicator = \"hmean\"\n        else:\n            main_indicator = \"acc\"\n        logger.info(\"metric[{}]: {}\".format(main_indicator, metric[main_indicator]))\n        return metric[main_indicator]\n\n    params_sensitive = pruner.sensitive(\n        eval_func=eval_fn,\n        sen_file=\"./sen.pickle\",\n        skip_vars=[\"conv2d_57.w_0\", \"conv2d_transpose_2.w_0\", \"conv2d_transpose_3.w_0\"],\n    )\n\n    logger.info(\n        \"The sensitivity analysis results of model parameters saved in sen.pickle\"\n    )\n    # calculate pruned params's ratio\n    params_sensitive = pruner._get_ratios_by_loss(params_sensitive, loss=0.02)\n    for key in params_sensitive.keys():\n        logger.info(\"{}, {}\".format(key, params_sensitive[key]))\n\n    plan = pruner.prune_vars(params_sensitive, [0])\n\n    flops = paddle.flops(model, input_shape)\n    logger.info(\"FLOPs after pruning: {}\".format(flops))\n\n    # load pretrain model\n    load_model(config, model)\n    metric = program.eval(model, valid_dataloader, post_process_class, eval_class)\n    if config[\"Architecture\"][\"model_type\"] == \"det\":\n        main_indicator = \"hmean\"\n    else:\n        main_indicator = \"acc\"\n    logger.info(\"metric['']: {}\".format(main_indicator, metric[main_indicator]))\n\n    # start export model\n    from paddle.jit import to_static\n\n    infer_shape = [3, -1, -1]\n    if config[\"Architecture\"][\"model_type\"] == \"rec\":\n        infer_shape = [3, 32, -1]  # for rec model, H must be 32\n\n        if (\n            \"Transform\" in config[\"Architecture\"]\n            and config[\"Architecture\"][\"Transform\"] is not None\n            and config[\"Architecture\"][\"Transform\"][\"name\"] == \"TPS\"\n        ):\n            logger.info(\n                \"When there is tps in the network, variable length input is not supported, and the input size needs to be the same as during training\"\n            )\n            infer_shape[-1] = 100\n    model = to_static(\n        model,\n        input_spec=[\n            paddle.static.InputSpec(shape=[None] + infer_shape, dtype=\"float32\")\n        ],\n    )\n\n    save_path = \"{}/inference\".format(config[\"Global\"][\"save_inference_dir\"])\n    paddle.jit.save(model, save_path)\n    logger.info(\"inference model is saved to {}\".format(save_path))\n\n\nif __name__ == \"__main__\":\n    config, device, logger, vdl_writer = program.preprocess(is_train=True)\n    main(config, device, logger, vdl_writer)\n", "deploy/slim/prune/sensitivity_anal.py": "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\n\n__dir__ = os.path.dirname(__file__)\nsys.path.append(__dir__)\nsys.path.append(os.path.join(__dir__, \"..\", \"..\", \"..\"))\nsys.path.append(os.path.join(__dir__, \"..\", \"..\", \"..\", \"tools\"))\n\nimport paddle\nimport paddle.distributed as dist\nfrom ppocr.data import build_dataloader, set_signal_handlers\nfrom ppocr.modeling.architectures import build_model\nfrom ppocr.losses import build_loss\nfrom ppocr.optimizer import build_optimizer\nfrom ppocr.postprocess import build_post_process\nfrom ppocr.metrics import build_metric\nfrom ppocr.utils.save_load import load_model\nimport tools.program as program\n\ndist.get_world_size()\n\n\ndef get_pruned_params(parameters):\n    params = []\n\n    for param in parameters:\n        if (\n            len(param.shape) == 4\n            and \"depthwise\" not in param.name\n            and \"transpose\" not in param.name\n            and \"conv2d_57\" not in param.name\n            and \"conv2d_56\" not in param.name\n        ):\n            params.append(param.name)\n    return params\n\n\ndef main(config, device, logger, vdl_writer):\n    # init dist environment\n    if config[\"Global\"][\"distributed\"]:\n        dist.init_parallel_env()\n\n    global_config = config[\"Global\"]\n\n    # build dataloader\n    set_signal_handlers()\n    train_dataloader = build_dataloader(config, \"Train\", device, logger)\n    if config[\"Eval\"]:\n        valid_dataloader = build_dataloader(config, \"Eval\", device, logger)\n    else:\n        valid_dataloader = None\n\n    # build post process\n    post_process_class = build_post_process(config[\"PostProcess\"], global_config)\n\n    # build model\n    # for rec algorithm\n    if hasattr(post_process_class, \"character\"):\n        char_num = len(getattr(post_process_class, \"character\"))\n        config[\"Architecture\"][\"Head\"][\"out_channels\"] = char_num\n    model = build_model(config[\"Architecture\"])\n    if config[\"Architecture\"][\"model_type\"] == \"det\":\n        input_shape = [1, 3, 640, 640]\n    elif config[\"Architecture\"][\"model_type\"] == \"rec\":\n        input_shape = [1, 3, 32, 320]\n    flops = paddle.flops(model, input_shape)\n\n    logger.info(\"FLOPs before pruning: {}\".format(flops))\n\n    from paddleslim.dygraph import FPGMFilterPruner\n\n    model.train()\n\n    pruner = FPGMFilterPruner(model, input_shape)\n\n    # build loss\n    loss_class = build_loss(config[\"Loss\"])\n\n    # build optim\n    optimizer, lr_scheduler = build_optimizer(\n        config[\"Optimizer\"],\n        epochs=config[\"Global\"][\"epoch_num\"],\n        step_each_epoch=len(train_dataloader),\n        model=model,\n    )\n\n    # build metric\n    eval_class = build_metric(config[\"Metric\"])\n    # load pretrain model\n    pre_best_model_dict = load_model(config, model, optimizer)\n\n    logger.info(\n        \"train dataloader has {} iters, valid dataloader has {} iters\".format(\n            len(train_dataloader), len(valid_dataloader)\n        )\n    )\n    # build metric\n    eval_class = build_metric(config[\"Metric\"])\n\n    logger.info(\n        \"train dataloader has {} iters, valid dataloader has {} iters\".format(\n            len(train_dataloader), len(valid_dataloader)\n        )\n    )\n\n    def eval_fn():\n        metric = program.eval(\n            model, valid_dataloader, post_process_class, eval_class, False\n        )\n        if config[\"Architecture\"][\"model_type\"] == \"det\":\n            main_indicator = \"hmean\"\n        else:\n            main_indicator = \"acc\"\n\n        logger.info(\"metric[{}]: {}\".format(main_indicator, metric[main_indicator]))\n        return metric[main_indicator]\n\n    run_sensitive_analysis = False\n    \"\"\"\n    run_sensitive_analysis=True:\n        Automatically compute the sensitivities of convolutions in a model.\n        The sensitivity of a convolution is the losses of accuracy on test dataset in\n        different pruned ratios. The sensitivities can be used to get a group of best\n        ratios with some condition.\n\n    run_sensitive_analysis=False:\n        Set prune trim ratio to a fixed value, such as 10%. The larger the value,\n        the more convolution weights will be cropped.\n\n    \"\"\"\n\n    if run_sensitive_analysis:\n        params_sensitive = pruner.sensitive(\n            eval_func=eval_fn,\n            sen_file=\"./deploy/slim/prune/sen.pickle\",\n            skip_vars=[\n                \"conv2d_57.w_0\",\n                \"conv2d_transpose_2.w_0\",\n                \"conv2d_transpose_3.w_0\",\n            ],\n        )\n        logger.info(\n            \"The sensitivity analysis results of model parameters saved in sen.pickle\"\n        )\n        # calculate pruned params's ratio\n        params_sensitive = pruner._get_ratios_by_loss(params_sensitive, loss=0.02)\n        for key in params_sensitive.keys():\n            logger.info(\"{}, {}\".format(key, params_sensitive[key]))\n    else:\n        params_sensitive = {}\n        for param in model.parameters():\n            if \"transpose\" not in param.name and \"linear\" not in param.name:\n                # set prune ratio as 10%. The larger the value, the more convolution weights will be cropped\n                params_sensitive[param.name] = 0.1\n\n    plan = pruner.prune_vars(params_sensitive, [0])\n\n    flops = paddle.flops(model, input_shape)\n    logger.info(\"FLOPs after pruning: {}\".format(flops))\n\n    # start train\n\n    program.train(\n        config,\n        train_dataloader,\n        valid_dataloader,\n        device,\n        model,\n        loss_class,\n        optimizer,\n        lr_scheduler,\n        post_process_class,\n        eval_class,\n        pre_best_model_dict,\n        logger,\n        vdl_writer,\n    )\n\n\nif __name__ == \"__main__\":\n    config, device, logger, vdl_writer = program.preprocess(is_train=True)\n    main(config, device, logger, vdl_writer)\n", "tools/infer_cls.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nimport os\nimport sys\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(__dir__)\nsys.path.insert(0, os.path.abspath(os.path.join(__dir__, \"..\")))\n\nos.environ[\"FLAGS_allocator_strategy\"] = \"auto_growth\"\n\nimport paddle\n\nfrom ppocr.data import create_operators, transform\nfrom ppocr.modeling.architectures import build_model\nfrom ppocr.postprocess import build_post_process\nfrom ppocr.utils.save_load import load_model\nfrom ppocr.utils.utility import get_image_file_list\nimport tools.program as program\n\n\ndef main():\n    global_config = config[\"Global\"]\n\n    # build post process\n    post_process_class = build_post_process(config[\"PostProcess\"], global_config)\n\n    # build model\n    model = build_model(config[\"Architecture\"])\n\n    load_model(config, model)\n\n    # create data ops\n    transforms = []\n    for op in config[\"Eval\"][\"dataset\"][\"transforms\"]:\n        op_name = list(op)[0]\n        if \"Label\" in op_name:\n            continue\n        elif op_name == \"KeepKeys\":\n            op[op_name][\"keep_keys\"] = [\"image\"]\n        elif op_name == \"SSLRotateResize\":\n            op[op_name][\"mode\"] = \"test\"\n        transforms.append(op)\n    global_config[\"infer_mode\"] = True\n    ops = create_operators(transforms, global_config)\n\n    model.eval()\n    for file in get_image_file_list(config[\"Global\"][\"infer_img\"]):\n        logger.info(\"infer_img: {}\".format(file))\n        with open(file, \"rb\") as f:\n            img = f.read()\n            data = {\"image\": img}\n        batch = transform(data, ops)\n\n        images = np.expand_dims(batch[0], axis=0)\n        images = paddle.to_tensor(images)\n        preds = model(images)\n        post_result = post_process_class(preds)\n        for rec_result in post_result:\n            logger.info(\"\\t result: {}\".format(rec_result))\n    logger.info(\"success!\")\n\n\nif __name__ == \"__main__\":\n    config, device, logger, vdl_writer = program.preprocess()\n    main()\n", "tools/train.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(__dir__)\nsys.path.insert(0, os.path.abspath(os.path.join(__dir__, \"..\")))\n\nimport yaml\nimport paddle\nimport paddle.distributed as dist\n\nfrom ppocr.data import build_dataloader, set_signal_handlers\nfrom ppocr.modeling.architectures import build_model\nfrom ppocr.losses import build_loss\nfrom ppocr.optimizer import build_optimizer\nfrom ppocr.postprocess import build_post_process\nfrom ppocr.metrics import build_metric\nfrom ppocr.utils.save_load import load_model\nfrom ppocr.utils.utility import set_seed\nfrom ppocr.modeling.architectures import apply_to_static\nimport tools.program as program\n\ndist.get_world_size()\n\n\ndef main(config, device, logger, vdl_writer, seed):\n    # init dist environment\n    if config[\"Global\"][\"distributed\"]:\n        dist.init_parallel_env()\n\n    global_config = config[\"Global\"]\n\n    # build dataloader\n    set_signal_handlers()\n    train_dataloader = build_dataloader(config, \"Train\", device, logger, seed)\n    if len(train_dataloader) == 0:\n        logger.error(\n            \"No Images in train dataset, please ensure\\n\"\n            + \"\\t1. The images num in the train label_file_list should be larger than or equal with batch size.\\n\"\n            + \"\\t2. The annotation file and path in the configuration file are provided normally.\"\n        )\n        return\n\n    if config[\"Eval\"]:\n        valid_dataloader = build_dataloader(config, \"Eval\", device, logger, seed)\n    else:\n        valid_dataloader = None\n    step_pre_epoch = len(train_dataloader)\n\n    # build post process\n    post_process_class = build_post_process(config[\"PostProcess\"], global_config)\n\n    # build model\n    # for rec algorithm\n    if hasattr(post_process_class, \"character\"):\n        char_num = len(getattr(post_process_class, \"character\"))\n        if config[\"Architecture\"][\"algorithm\"] in [\n            \"Distillation\",\n        ]:  # distillation model\n            for key in config[\"Architecture\"][\"Models\"]:\n                if (\n                    config[\"Architecture\"][\"Models\"][key][\"Head\"][\"name\"] == \"MultiHead\"\n                ):  # for multi head\n                    if config[\"PostProcess\"][\"name\"] == \"DistillationSARLabelDecode\":\n                        char_num = char_num - 2\n                    if config[\"PostProcess\"][\"name\"] == \"DistillationNRTRLabelDecode\":\n                        char_num = char_num - 3\n                    out_channels_list = {}\n                    out_channels_list[\"CTCLabelDecode\"] = char_num\n                    # update SARLoss params\n                    if (\n                        list(config[\"Loss\"][\"loss_config_list\"][-1].keys())[0]\n                        == \"DistillationSARLoss\"\n                    ):\n                        config[\"Loss\"][\"loss_config_list\"][-1][\"DistillationSARLoss\"][\n                            \"ignore_index\"\n                        ] = (char_num + 1)\n                        out_channels_list[\"SARLabelDecode\"] = char_num + 2\n                    elif any(\n                        \"DistillationNRTRLoss\" in d\n                        for d in config[\"Loss\"][\"loss_config_list\"]\n                    ):\n                        out_channels_list[\"NRTRLabelDecode\"] = char_num + 3\n\n                    config[\"Architecture\"][\"Models\"][key][\"Head\"][\n                        \"out_channels_list\"\n                    ] = out_channels_list\n                else:\n                    config[\"Architecture\"][\"Models\"][key][\"Head\"][\n                        \"out_channels\"\n                    ] = char_num\n        elif config[\"Architecture\"][\"Head\"][\"name\"] == \"MultiHead\":  # for multi head\n            if config[\"PostProcess\"][\"name\"] == \"SARLabelDecode\":\n                char_num = char_num - 2\n            if config[\"PostProcess\"][\"name\"] == \"NRTRLabelDecode\":\n                char_num = char_num - 3\n            out_channels_list = {}\n            out_channels_list[\"CTCLabelDecode\"] = char_num\n            # update SARLoss params\n            if list(config[\"Loss\"][\"loss_config_list\"][1].keys())[0] == \"SARLoss\":\n                if config[\"Loss\"][\"loss_config_list\"][1][\"SARLoss\"] is None:\n                    config[\"Loss\"][\"loss_config_list\"][1][\"SARLoss\"] = {\n                        \"ignore_index\": char_num + 1\n                    }\n                else:\n                    config[\"Loss\"][\"loss_config_list\"][1][\"SARLoss\"][\"ignore_index\"] = (\n                        char_num + 1\n                    )\n                out_channels_list[\"SARLabelDecode\"] = char_num + 2\n            elif list(config[\"Loss\"][\"loss_config_list\"][1].keys())[0] == \"NRTRLoss\":\n                out_channels_list[\"NRTRLabelDecode\"] = char_num + 3\n            config[\"Architecture\"][\"Head\"][\"out_channels_list\"] = out_channels_list\n        else:  # base rec model\n            config[\"Architecture\"][\"Head\"][\"out_channels\"] = char_num\n\n        if config[\"PostProcess\"][\"name\"] == \"SARLabelDecode\":  # for SAR model\n            config[\"Loss\"][\"ignore_index\"] = char_num - 1\n\n    model = build_model(config[\"Architecture\"])\n\n    use_sync_bn = config[\"Global\"].get(\"use_sync_bn\", False)\n    if use_sync_bn:\n        model = paddle.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n        logger.info(\"convert_sync_batchnorm\")\n\n    model = apply_to_static(model, config, logger)\n\n    # build loss\n    loss_class = build_loss(config[\"Loss\"])\n\n    # build optim\n    optimizer, lr_scheduler = build_optimizer(\n        config[\"Optimizer\"],\n        epochs=config[\"Global\"][\"epoch_num\"],\n        step_each_epoch=len(train_dataloader),\n        model=model,\n    )\n\n    # build metric\n    eval_class = build_metric(config[\"Metric\"])\n\n    logger.info(\"train dataloader has {} iters\".format(len(train_dataloader)))\n    if valid_dataloader is not None:\n        logger.info(\"valid dataloader has {} iters\".format(len(valid_dataloader)))\n\n    use_amp = config[\"Global\"].get(\"use_amp\", False)\n    amp_level = config[\"Global\"].get(\"amp_level\", \"O2\")\n    amp_dtype = config[\"Global\"].get(\"amp_dtype\", \"float16\")\n    amp_custom_black_list = config[\"Global\"].get(\"amp_custom_black_list\", [])\n    amp_custom_white_list = config[\"Global\"].get(\"amp_custom_white_list\", [])\n    if use_amp:\n        AMP_RELATED_FLAGS_SETTING = {\n            \"FLAGS_max_inplace_grad_add\": 8,\n        }\n        if paddle.is_compiled_with_cuda():\n            AMP_RELATED_FLAGS_SETTING.update(\n                {\n                    \"FLAGS_cudnn_batchnorm_spatial_persistent\": 1,\n                    \"FLAGS_gemm_use_half_precision_compute_type\": 0,\n                }\n            )\n        paddle.set_flags(AMP_RELATED_FLAGS_SETTING)\n        scale_loss = config[\"Global\"].get(\"scale_loss\", 1.0)\n        use_dynamic_loss_scaling = config[\"Global\"].get(\n            \"use_dynamic_loss_scaling\", False\n        )\n        scaler = paddle.amp.GradScaler(\n            init_loss_scaling=scale_loss,\n            use_dynamic_loss_scaling=use_dynamic_loss_scaling,\n        )\n        if amp_level == \"O2\":\n            model, optimizer = paddle.amp.decorate(\n                models=model,\n                optimizers=optimizer,\n                level=amp_level,\n                master_weight=True,\n                dtype=amp_dtype,\n            )\n    else:\n        scaler = None\n\n    # load pretrain model\n    pre_best_model_dict = load_model(\n        config, model, optimizer, config[\"Architecture\"][\"model_type\"]\n    )\n\n    if config[\"Global\"][\"distributed\"]:\n        model = paddle.DataParallel(model)\n    # start train\n    program.train(\n        config,\n        train_dataloader,\n        valid_dataloader,\n        device,\n        model,\n        loss_class,\n        optimizer,\n        lr_scheduler,\n        post_process_class,\n        eval_class,\n        pre_best_model_dict,\n        logger,\n        step_pre_epoch,\n        vdl_writer,\n        scaler,\n        amp_level,\n        amp_custom_black_list,\n        amp_custom_white_list,\n        amp_dtype,\n    )\n\n\ndef test_reader(config, device, logger):\n    loader = build_dataloader(config, \"Train\", device, logger)\n    import time\n\n    starttime = time.time()\n    count = 0\n    try:\n        for data in loader():\n            count += 1\n            if count % 1 == 0:\n                batch_time = time.time() - starttime\n                starttime = time.time()\n                logger.info(\n                    \"reader: {}, {}, {}\".format(count, len(data[0]), batch_time)\n                )\n    except Exception as e:\n        logger.info(e)\n    logger.info(\"finish reader: {}, Success!\".format(count))\n\n\nif __name__ == \"__main__\":\n    config, device, logger, vdl_writer = program.preprocess(is_train=True)\n    seed = config[\"Global\"][\"seed\"] if \"seed\" in config[\"Global\"] else 1024\n    set_seed(seed)\n    main(config, device, logger, vdl_writer, seed)\n    # test_reader(config, device, logger)\n", "tools/program.py": "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport platform\nimport yaml\nimport time\nimport datetime\nimport paddle\nimport paddle.distributed as dist\nfrom tqdm import tqdm\nimport cv2\nimport numpy as np\nfrom argparse import ArgumentParser, RawDescriptionHelpFormatter\n\nfrom ppocr.utils.stats import TrainingStats\nfrom ppocr.utils.save_load import save_model\nfrom ppocr.utils.utility import print_dict, AverageMeter\nfrom ppocr.utils.logging import get_logger\nfrom ppocr.utils.loggers import WandbLogger, Loggers\nfrom ppocr.utils import profiler\nfrom ppocr.data import build_dataloader\n\n\nclass ArgsParser(ArgumentParser):\n    def __init__(self):\n        super(ArgsParser, self).__init__(formatter_class=RawDescriptionHelpFormatter)\n        self.add_argument(\"-c\", \"--config\", help=\"configuration file to use\")\n        self.add_argument(\"-o\", \"--opt\", nargs=\"+\", help=\"set configuration options\")\n        self.add_argument(\n            \"-p\",\n            \"--profiler_options\",\n            type=str,\n            default=None,\n            help=\"The option of profiler, which should be in format \"\n            '\"key1=value1;key2=value2;key3=value3\".',\n        )\n\n    def parse_args(self, argv=None):\n        args = super(ArgsParser, self).parse_args(argv)\n        assert args.config is not None, \"Please specify --config=configure_file_path.\"\n        args.opt = self._parse_opt(args.opt)\n        return args\n\n    def _parse_opt(self, opts):\n        config = {}\n        if not opts:\n            return config\n        for s in opts:\n            s = s.strip()\n            k, v = s.split(\"=\")\n            config[k] = yaml.load(v, Loader=yaml.Loader)\n        return config\n\n\ndef load_config(file_path):\n    \"\"\"\n    Load config from yml/yaml file.\n    Args:\n        file_path (str): Path of the config file to be loaded.\n    Returns: global config\n    \"\"\"\n    _, ext = os.path.splitext(file_path)\n    assert ext in [\".yml\", \".yaml\"], \"only support yaml files for now\"\n    config = yaml.load(open(file_path, \"rb\"), Loader=yaml.Loader)\n    return config\n\n\ndef merge_config(config, opts):\n    \"\"\"\n    Merge config into global config.\n    Args:\n        config (dict): Config to be merged.\n    Returns: global config\n    \"\"\"\n    for key, value in opts.items():\n        if \".\" not in key:\n            if isinstance(value, dict) and key in config:\n                config[key].update(value)\n            else:\n                config[key] = value\n        else:\n            sub_keys = key.split(\".\")\n            assert sub_keys[0] in config, (\n                \"the sub_keys can only be one of global_config: {}, but get: \"\n                \"{}, please check your running command\".format(\n                    config.keys(), sub_keys[0]\n                )\n            )\n            cur = config[sub_keys[0]]\n            for idx, sub_key in enumerate(sub_keys[1:]):\n                if idx == len(sub_keys) - 2:\n                    cur[sub_key] = value\n                else:\n                    cur = cur[sub_key]\n    return config\n\n\ndef check_device(use_gpu, use_xpu=False, use_npu=False, use_mlu=False):\n    \"\"\"\n    Log error and exit when set use_gpu=true in paddlepaddle\n    cpu version.\n    \"\"\"\n    err = (\n        \"Config {} cannot be set as true while your paddle \"\n        \"is not compiled with {} ! \\nPlease try: \\n\"\n        \"\\t1. Install paddlepaddle to run model on {} \\n\"\n        \"\\t2. Set {} as false in config file to run \"\n        \"model on CPU\"\n    )\n\n    try:\n        if use_gpu and use_xpu:\n            print(\"use_xpu and use_gpu can not both be true.\")\n        if use_gpu and not paddle.is_compiled_with_cuda():\n            print(err.format(\"use_gpu\", \"cuda\", \"gpu\", \"use_gpu\"))\n            sys.exit(1)\n        if use_xpu and not paddle.device.is_compiled_with_xpu():\n            print(err.format(\"use_xpu\", \"xpu\", \"xpu\", \"use_xpu\"))\n            sys.exit(1)\n        if use_npu:\n            if (\n                int(paddle.version.major) != 0\n                and int(paddle.version.major) <= 2\n                and int(paddle.version.minor) <= 4\n            ):\n                if not paddle.device.is_compiled_with_npu():\n                    print(err.format(\"use_npu\", \"npu\", \"npu\", \"use_npu\"))\n                    sys.exit(1)\n            # is_compiled_with_npu() has been updated after paddle-2.4\n            else:\n                if not paddle.device.is_compiled_with_custom_device(\"npu\"):\n                    print(err.format(\"use_npu\", \"npu\", \"npu\", \"use_npu\"))\n                    sys.exit(1)\n        if use_mlu and not paddle.device.is_compiled_with_mlu():\n            print(err.format(\"use_mlu\", \"mlu\", \"mlu\", \"use_mlu\"))\n            sys.exit(1)\n    except Exception as e:\n        pass\n\n\ndef to_float32(preds):\n    if isinstance(preds, dict):\n        for k in preds:\n            if isinstance(preds[k], dict) or isinstance(preds[k], list):\n                preds[k] = to_float32(preds[k])\n            elif isinstance(preds[k], paddle.Tensor):\n                preds[k] = preds[k].astype(paddle.float32)\n    elif isinstance(preds, list):\n        for k in range(len(preds)):\n            if isinstance(preds[k], dict):\n                preds[k] = to_float32(preds[k])\n            elif isinstance(preds[k], list):\n                preds[k] = to_float32(preds[k])\n            elif isinstance(preds[k], paddle.Tensor):\n                preds[k] = preds[k].astype(paddle.float32)\n    elif isinstance(preds, paddle.Tensor):\n        preds = preds.astype(paddle.float32)\n    return preds\n\n\ndef train(\n    config,\n    train_dataloader,\n    valid_dataloader,\n    device,\n    model,\n    loss_class,\n    optimizer,\n    lr_scheduler,\n    post_process_class,\n    eval_class,\n    pre_best_model_dict,\n    logger,\n    step_pre_epoch,\n    log_writer=None,\n    scaler=None,\n    amp_level=\"O2\",\n    amp_custom_black_list=[],\n    amp_custom_white_list=[],\n    amp_dtype=\"float16\",\n):\n    cal_metric_during_train = config[\"Global\"].get(\"cal_metric_during_train\", False)\n    calc_epoch_interval = config[\"Global\"].get(\"calc_epoch_interval\", 1)\n    log_smooth_window = config[\"Global\"][\"log_smooth_window\"]\n    epoch_num = config[\"Global\"][\"epoch_num\"]\n    print_batch_step = config[\"Global\"][\"print_batch_step\"]\n    eval_batch_step = config[\"Global\"][\"eval_batch_step\"]\n    eval_batch_epoch = config[\"Global\"].get(\"eval_batch_epoch\", None)\n    profiler_options = config[\"profiler_options\"]\n\n    global_step = 0\n    if \"global_step\" in pre_best_model_dict:\n        global_step = pre_best_model_dict[\"global_step\"]\n    start_eval_step = 0\n    if isinstance(eval_batch_step, list) and len(eval_batch_step) >= 2:\n        start_eval_step = eval_batch_step[0] if not eval_batch_epoch else 0\n        eval_batch_step = (\n            eval_batch_step[1]\n            if not eval_batch_epoch\n            else step_pre_epoch * eval_batch_epoch\n        )\n        if len(valid_dataloader) == 0:\n            logger.info(\n                \"No Images in eval dataset, evaluation during training \"\n                \"will be disabled\"\n            )\n            start_eval_step = 1e111\n        logger.info(\n            \"During the training process, after the {}th iteration, \"\n            \"an evaluation is run every {} iterations\".format(\n                start_eval_step, eval_batch_step\n            )\n        )\n    save_epoch_step = config[\"Global\"][\"save_epoch_step\"]\n    save_model_dir = config[\"Global\"][\"save_model_dir\"]\n    if not os.path.exists(save_model_dir):\n        os.makedirs(save_model_dir)\n    main_indicator = eval_class.main_indicator\n    best_model_dict = {main_indicator: 0}\n    best_model_dict.update(pre_best_model_dict)\n    train_stats = TrainingStats(log_smooth_window, [\"lr\"])\n    model_average = False\n    model.train()\n\n    use_srn = config[\"Architecture\"][\"algorithm\"] == \"SRN\"\n    extra_input_models = [\n        \"SRN\",\n        \"NRTR\",\n        \"SAR\",\n        \"SEED\",\n        \"SVTR\",\n        \"SVTR_LCNet\",\n        \"SPIN\",\n        \"VisionLAN\",\n        \"RobustScanner\",\n        \"RFL\",\n        \"DRRG\",\n        \"SATRN\",\n        \"SVTR_HGNet\",\n        \"ParseQ\",\n        \"CPPD\",\n    ]\n    extra_input = False\n    if config[\"Architecture\"][\"algorithm\"] == \"Distillation\":\n        for key in config[\"Architecture\"][\"Models\"]:\n            extra_input = (\n                extra_input\n                or config[\"Architecture\"][\"Models\"][key][\"algorithm\"]\n                in extra_input_models\n            )\n    else:\n        extra_input = config[\"Architecture\"][\"algorithm\"] in extra_input_models\n    try:\n        model_type = config[\"Architecture\"][\"model_type\"]\n    except:\n        model_type = None\n\n    algorithm = config[\"Architecture\"][\"algorithm\"]\n\n    start_epoch = (\n        best_model_dict[\"start_epoch\"] if \"start_epoch\" in best_model_dict else 1\n    )\n\n    total_samples = 0\n    train_reader_cost = 0.0\n    train_batch_cost = 0.0\n    reader_start = time.time()\n    eta_meter = AverageMeter()\n\n    max_iter = (\n        len(train_dataloader) - 1\n        if platform.system() == \"Windows\"\n        else len(train_dataloader)\n    )\n\n    for epoch in range(start_epoch, epoch_num + 1):\n        if train_dataloader.dataset.need_reset:\n            train_dataloader = build_dataloader(\n                config, \"Train\", device, logger, seed=epoch\n            )\n            max_iter = (\n                len(train_dataloader) - 1\n                if platform.system() == \"Windows\"\n                else len(train_dataloader)\n            )\n\n        for idx, batch in enumerate(train_dataloader):\n            profiler.add_profiler_step(profiler_options)\n            train_reader_cost += time.time() - reader_start\n            if idx >= max_iter:\n                break\n            lr = optimizer.get_lr()\n            images = batch[0]\n            if use_srn:\n                model_average = True\n            # use amp\n            if scaler:\n                with paddle.amp.auto_cast(\n                    level=amp_level,\n                    custom_black_list=amp_custom_black_list,\n                    custom_white_list=amp_custom_white_list,\n                    dtype=amp_dtype,\n                ):\n                    if model_type == \"table\" or extra_input:\n                        preds = model(images, data=batch[1:])\n                    elif model_type in [\"kie\"]:\n                        preds = model(batch)\n                    elif algorithm in [\"CAN\"]:\n                        preds = model(batch[:3])\n                    else:\n                        preds = model(images)\n                preds = to_float32(preds)\n                loss = loss_class(preds, batch)\n                avg_loss = loss[\"loss\"]\n                scaled_avg_loss = scaler.scale(avg_loss)\n                scaled_avg_loss.backward()\n                scaler.minimize(optimizer, scaled_avg_loss)\n            else:\n                if model_type == \"table\" or extra_input:\n                    preds = model(images, data=batch[1:])\n                elif model_type in [\"kie\", \"sr\"]:\n                    preds = model(batch)\n                elif algorithm in [\"CAN\"]:\n                    preds = model(batch[:3])\n                else:\n                    preds = model(images)\n                loss = loss_class(preds, batch)\n                avg_loss = loss[\"loss\"]\n                avg_loss.backward()\n                optimizer.step()\n\n            optimizer.clear_grad()\n\n            if (\n                cal_metric_during_train and epoch % calc_epoch_interval == 0\n            ):  # only rec and cls need\n                batch = [item.numpy() for item in batch]\n                if model_type in [\"kie\", \"sr\"]:\n                    eval_class(preds, batch)\n                elif model_type in [\"table\"]:\n                    post_result = post_process_class(preds, batch)\n                    eval_class(post_result, batch)\n                elif algorithm in [\"CAN\"]:\n                    model_type = \"can\"\n                    eval_class(preds[0], batch[2:], epoch_reset=(idx == 0))\n                else:\n                    if config[\"Loss\"][\"name\"] in [\n                        \"MultiLoss\",\n                        \"MultiLoss_v2\",\n                    ]:  # for multi head loss\n                        post_result = post_process_class(\n                            preds[\"ctc\"], batch[1]\n                        )  # for CTC head out\n                    elif config[\"Loss\"][\"name\"] in [\"VLLoss\"]:\n                        post_result = post_process_class(preds, batch[1], batch[-1])\n                    else:\n                        post_result = post_process_class(preds, batch[1])\n                    eval_class(post_result, batch)\n                metric = eval_class.get_metric()\n                train_stats.update(metric)\n\n            train_batch_time = time.time() - reader_start\n            train_batch_cost += train_batch_time\n            eta_meter.update(train_batch_time)\n            global_step += 1\n            total_samples += len(images)\n\n            if not isinstance(lr_scheduler, float):\n                lr_scheduler.step()\n\n            # logger and visualdl\n            stats = {\n                k: float(v) if v.shape == [] else v.numpy().mean()\n                for k, v in loss.items()\n            }\n            stats[\"lr\"] = lr\n            train_stats.update(stats)\n\n            if log_writer is not None and dist.get_rank() == 0:\n                log_writer.log_metrics(\n                    metrics=train_stats.get(), prefix=\"TRAIN\", step=global_step\n                )\n\n            if dist.get_rank() == 0 and (\n                (global_step > 0 and global_step % print_batch_step == 0)\n                or (idx >= len(train_dataloader) - 1)\n            ):\n                logs = train_stats.log()\n\n                eta_sec = (\n                    (epoch_num + 1 - epoch) * len(train_dataloader) - idx - 1\n                ) * eta_meter.avg\n                eta_sec_format = str(datetime.timedelta(seconds=int(eta_sec)))\n                max_mem_reserved_str = \"\"\n                max_mem_allocated_str = \"\"\n                if paddle.device.is_compiled_with_cuda():\n                    max_mem_reserved_str = f\"max_mem_reserved: {paddle.device.cuda.max_memory_reserved() // (1024 ** 2)} MB,\"\n                    max_mem_allocated_str = f\"max_mem_allocated: {paddle.device.cuda.max_memory_allocated() // (1024 ** 2)} MB\"\n                strs = (\n                    \"epoch: [{}/{}], global_step: {}, {}, avg_reader_cost: \"\n                    \"{:.5f} s, avg_batch_cost: {:.5f} s, avg_samples: {}, \"\n                    \"ips: {:.5f} samples/s, eta: {}, {} {}\".format(\n                        epoch,\n                        epoch_num,\n                        global_step,\n                        logs,\n                        train_reader_cost / print_batch_step,\n                        train_batch_cost / print_batch_step,\n                        total_samples / print_batch_step,\n                        total_samples / train_batch_cost,\n                        eta_sec_format,\n                        max_mem_reserved_str,\n                        max_mem_allocated_str,\n                    )\n                )\n                logger.info(strs)\n\n                total_samples = 0\n                train_reader_cost = 0.0\n                train_batch_cost = 0.0\n            # eval\n            if (\n                global_step > start_eval_step\n                and (global_step - start_eval_step) % eval_batch_step == 0\n                and dist.get_rank() == 0\n            ):\n                if model_average:\n                    Model_Average = paddle.incubate.optimizer.ModelAverage(\n                        0.15,\n                        parameters=model.parameters(),\n                        min_average_window=10000,\n                        max_average_window=15625,\n                    )\n                    Model_Average.apply()\n                cur_metric = eval(\n                    model,\n                    valid_dataloader,\n                    post_process_class,\n                    eval_class,\n                    model_type,\n                    extra_input=extra_input,\n                    scaler=scaler,\n                    amp_level=amp_level,\n                    amp_custom_black_list=amp_custom_black_list,\n                    amp_custom_white_list=amp_custom_white_list,\n                    amp_dtype=amp_dtype,\n                )\n                cur_metric_str = \"cur metric, {}\".format(\n                    \", \".join([\"{}: {}\".format(k, v) for k, v in cur_metric.items()])\n                )\n                logger.info(cur_metric_str)\n\n                # logger metric\n                if log_writer is not None:\n                    log_writer.log_metrics(\n                        metrics=cur_metric, prefix=\"EVAL\", step=global_step\n                    )\n\n                if cur_metric[main_indicator] >= best_model_dict[main_indicator]:\n                    best_model_dict.update(cur_metric)\n                    best_model_dict[\"best_epoch\"] = epoch\n                    save_model(\n                        model,\n                        optimizer,\n                        save_model_dir,\n                        logger,\n                        config,\n                        is_best=True,\n                        prefix=\"best_accuracy\",\n                        best_model_dict=best_model_dict,\n                        epoch=epoch,\n                        global_step=global_step,\n                    )\n                best_str = \"best metric, {}\".format(\n                    \", \".join(\n                        [\"{}: {}\".format(k, v) for k, v in best_model_dict.items()]\n                    )\n                )\n                logger.info(best_str)\n                # logger best metric\n                if log_writer is not None:\n                    log_writer.log_metrics(\n                        metrics={\n                            \"best_{}\".format(main_indicator): best_model_dict[\n                                main_indicator\n                            ]\n                        },\n                        prefix=\"EVAL\",\n                        step=global_step,\n                    )\n\n                    log_writer.log_model(\n                        is_best=True, prefix=\"best_accuracy\", metadata=best_model_dict\n                    )\n\n            reader_start = time.time()\n        if dist.get_rank() == 0:\n            save_model(\n                model,\n                optimizer,\n                save_model_dir,\n                logger,\n                config,\n                is_best=False,\n                prefix=\"latest\",\n                best_model_dict=best_model_dict,\n                epoch=epoch,\n                global_step=global_step,\n            )\n\n            if log_writer is not None:\n                log_writer.log_model(is_best=False, prefix=\"latest\")\n\n        if dist.get_rank() == 0 and epoch > 0 and epoch % save_epoch_step == 0:\n            save_model(\n                model,\n                optimizer,\n                save_model_dir,\n                logger,\n                config,\n                is_best=False,\n                prefix=\"iter_epoch_{}\".format(epoch),\n                best_model_dict=best_model_dict,\n                epoch=epoch,\n                global_step=global_step,\n            )\n            if log_writer is not None:\n                log_writer.log_model(\n                    is_best=False, prefix=\"iter_epoch_{}\".format(epoch)\n                )\n\n    best_str = \"best metric, {}\".format(\n        \", \".join([\"{}: {}\".format(k, v) for k, v in best_model_dict.items()])\n    )\n    logger.info(best_str)\n    if dist.get_rank() == 0 and log_writer is not None:\n        log_writer.close()\n    return\n\n\ndef eval(\n    model,\n    valid_dataloader,\n    post_process_class,\n    eval_class,\n    model_type=None,\n    extra_input=False,\n    scaler=None,\n    amp_level=\"O2\",\n    amp_custom_black_list=[],\n    amp_custom_white_list=[],\n    amp_dtype=\"float16\",\n):\n    model.eval()\n    with paddle.no_grad():\n        total_frame = 0.0\n        total_time = 0.0\n        pbar = tqdm(\n            total=len(valid_dataloader), desc=\"eval model:\", position=0, leave=True\n        )\n        max_iter = (\n            len(valid_dataloader) - 1\n            if platform.system() == \"Windows\"\n            else len(valid_dataloader)\n        )\n        sum_images = 0\n        for idx, batch in enumerate(valid_dataloader):\n            if idx >= max_iter:\n                break\n            images = batch[0]\n            start = time.time()\n\n            # use amp\n            if scaler:\n                with paddle.amp.auto_cast(\n                    level=amp_level,\n                    custom_black_list=amp_custom_black_list,\n                    dtype=amp_dtype,\n                ):\n                    if model_type == \"table\" or extra_input:\n                        preds = model(images, data=batch[1:])\n                    elif model_type in [\"kie\"]:\n                        preds = model(batch)\n                    elif model_type in [\"can\"]:\n                        preds = model(batch[:3])\n                    elif model_type in [\"sr\"]:\n                        preds = model(batch)\n                        sr_img = preds[\"sr_img\"]\n                        lr_img = preds[\"lr_img\"]\n                    else:\n                        preds = model(images)\n                preds = to_float32(preds)\n            else:\n                if model_type == \"table\" or extra_input:\n                    preds = model(images, data=batch[1:])\n                elif model_type in [\"kie\"]:\n                    preds = model(batch)\n                elif model_type in [\"can\"]:\n                    preds = model(batch[:3])\n                elif model_type in [\"sr\"]:\n                    preds = model(batch)\n                    sr_img = preds[\"sr_img\"]\n                    lr_img = preds[\"lr_img\"]\n                else:\n                    preds = model(images)\n\n            batch_numpy = []\n            for item in batch:\n                if isinstance(item, paddle.Tensor):\n                    batch_numpy.append(item.numpy())\n                else:\n                    batch_numpy.append(item)\n            # Obtain usable results from post-processing methods\n            total_time += time.time() - start\n            # Evaluate the results of the current batch\n            if model_type in [\"table\", \"kie\"]:\n                if post_process_class is None:\n                    eval_class(preds, batch_numpy)\n                else:\n                    post_result = post_process_class(preds, batch_numpy)\n                    eval_class(post_result, batch_numpy)\n            elif model_type in [\"sr\"]:\n                eval_class(preds, batch_numpy)\n            elif model_type in [\"can\"]:\n                eval_class(preds[0], batch_numpy[2:], epoch_reset=(idx == 0))\n            else:\n                post_result = post_process_class(preds, batch_numpy[1])\n                eval_class(post_result, batch_numpy)\n\n            pbar.update(1)\n            total_frame += len(images)\n            sum_images += 1\n        # Get final metric\uff0ceg. acc or hmean\n        metric = eval_class.get_metric()\n\n    pbar.close()\n    model.train()\n    metric[\"fps\"] = total_frame / total_time\n    return metric\n\n\ndef update_center(char_center, post_result, preds):\n    result, label = post_result\n    feats, logits = preds\n    logits = paddle.argmax(logits, axis=-1)\n    feats = feats.numpy()\n    logits = logits.numpy()\n\n    for idx_sample in range(len(label)):\n        if result[idx_sample][0] == label[idx_sample][0]:\n            feat = feats[idx_sample]\n            logit = logits[idx_sample]\n            for idx_time in range(len(logit)):\n                index = logit[idx_time]\n                if index in char_center.keys():\n                    char_center[index][0] = (\n                        char_center[index][0] * char_center[index][1] + feat[idx_time]\n                    ) / (char_center[index][1] + 1)\n                    char_center[index][1] += 1\n                else:\n                    char_center[index] = [feat[idx_time], 1]\n    return char_center\n\n\ndef get_center(model, eval_dataloader, post_process_class):\n    pbar = tqdm(total=len(eval_dataloader), desc=\"get center:\")\n    max_iter = (\n        len(eval_dataloader) - 1\n        if platform.system() == \"Windows\"\n        else len(eval_dataloader)\n    )\n    char_center = dict()\n    for idx, batch in enumerate(eval_dataloader):\n        if idx >= max_iter:\n            break\n        images = batch[0]\n        start = time.time()\n        preds = model(images)\n\n        batch = [item.numpy() for item in batch]\n        # Obtain usable results from post-processing methods\n        post_result = post_process_class(preds, batch[1])\n\n        # update char_center\n        char_center = update_center(char_center, post_result, preds)\n        pbar.update(1)\n\n    pbar.close()\n    for key in char_center.keys():\n        char_center[key] = char_center[key][0]\n    return char_center\n\n\ndef preprocess(is_train=False):\n    FLAGS = ArgsParser().parse_args()\n    profiler_options = FLAGS.profiler_options\n    config = load_config(FLAGS.config)\n    config = merge_config(config, FLAGS.opt)\n    profile_dic = {\"profiler_options\": FLAGS.profiler_options}\n    config = merge_config(config, profile_dic)\n\n    if is_train:\n        # save_config\n        save_model_dir = config[\"Global\"][\"save_model_dir\"]\n        os.makedirs(save_model_dir, exist_ok=True)\n        with open(os.path.join(save_model_dir, \"config.yml\"), \"w\") as f:\n            yaml.dump(dict(config), f, default_flow_style=False, sort_keys=False)\n        log_file = \"{}/train.log\".format(save_model_dir)\n    else:\n        log_file = None\n    logger = get_logger(log_file=log_file)\n\n    # check if set use_gpu=True in paddlepaddle cpu version\n    use_gpu = config[\"Global\"].get(\"use_gpu\", False)\n    use_xpu = config[\"Global\"].get(\"use_xpu\", False)\n    use_npu = config[\"Global\"].get(\"use_npu\", False)\n    use_mlu = config[\"Global\"].get(\"use_mlu\", False)\n\n    alg = config[\"Architecture\"][\"algorithm\"]\n    assert alg in [\n        \"EAST\",\n        \"DB\",\n        \"SAST\",\n        \"Rosetta\",\n        \"CRNN\",\n        \"STARNet\",\n        \"RARE\",\n        \"SRN\",\n        \"CLS\",\n        \"PGNet\",\n        \"Distillation\",\n        \"NRTR\",\n        \"TableAttn\",\n        \"SAR\",\n        \"PSE\",\n        \"SEED\",\n        \"SDMGR\",\n        \"LayoutXLM\",\n        \"LayoutLM\",\n        \"LayoutLMv2\",\n        \"PREN\",\n        \"FCE\",\n        \"SVTR\",\n        \"SVTR_LCNet\",\n        \"ViTSTR\",\n        \"ABINet\",\n        \"DB++\",\n        \"TableMaster\",\n        \"SPIN\",\n        \"VisionLAN\",\n        \"Gestalt\",\n        \"SLANet\",\n        \"RobustScanner\",\n        \"CT\",\n        \"RFL\",\n        \"DRRG\",\n        \"CAN\",\n        \"Telescope\",\n        \"SATRN\",\n        \"SVTR_HGNet\",\n        \"ParseQ\",\n        \"CPPD\",\n    ]\n\n    if use_xpu:\n        device = \"xpu:{0}\".format(os.getenv(\"FLAGS_selected_xpus\", 0))\n    elif use_npu:\n        device = \"npu:{0}\".format(os.getenv(\"FLAGS_selected_npus\", 0))\n    elif use_mlu:\n        device = \"mlu:{0}\".format(os.getenv(\"FLAGS_selected_mlus\", 0))\n    else:\n        device = \"gpu:{}\".format(dist.ParallelEnv().dev_id) if use_gpu else \"cpu\"\n    check_device(use_gpu, use_xpu, use_npu, use_mlu)\n\n    device = paddle.set_device(device)\n\n    config[\"Global\"][\"distributed\"] = dist.get_world_size() != 1\n\n    loggers = []\n\n    if \"use_visualdl\" in config[\"Global\"] and config[\"Global\"][\"use_visualdl\"]:\n        logger.warning(\n            \"You are using VisualDL, the VisualDL is deprecated and \"\n            \"removed in ppocr!\"\n        )\n        log_writer = None\n    if (\n        \"use_wandb\" in config[\"Global\"] and config[\"Global\"][\"use_wandb\"]\n    ) or \"wandb\" in config:\n        save_dir = config[\"Global\"][\"save_model_dir\"]\n        wandb_writer_path = \"{}/wandb\".format(save_dir)\n        if \"wandb\" in config:\n            wandb_params = config[\"wandb\"]\n        else:\n            wandb_params = dict()\n        wandb_params.update({\"save_dir\": save_dir})\n        log_writer = WandbLogger(**wandb_params, config=config)\n        loggers.append(log_writer)\n    else:\n        log_writer = None\n    print_dict(config, logger)\n\n    if loggers:\n        log_writer = Loggers(loggers)\n    else:\n        log_writer = None\n\n    logger.info(\"train with paddle {} and device {}\".format(paddle.__version__, device))\n    return config, device, logger, log_writer\n", "tools/eval.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.insert(0, __dir__)\nsys.path.insert(0, os.path.abspath(os.path.join(__dir__, \"..\")))\n\nimport paddle\nfrom ppocr.data import build_dataloader, set_signal_handlers\nfrom ppocr.modeling.architectures import build_model\nfrom ppocr.postprocess import build_post_process\nfrom ppocr.metrics import build_metric\nfrom ppocr.utils.save_load import load_model\nimport tools.program as program\n\n\ndef main():\n    global_config = config[\"Global\"]\n    # build dataloader\n    set_signal_handlers()\n    valid_dataloader = build_dataloader(config, \"Eval\", device, logger)\n\n    # build post process\n    post_process_class = build_post_process(config[\"PostProcess\"], global_config)\n\n    # build model\n    # for rec algorithm\n    if hasattr(post_process_class, \"character\"):\n        char_num = len(getattr(post_process_class, \"character\"))\n        if config[\"Architecture\"][\"algorithm\"] in [\n            \"Distillation\",\n        ]:  # distillation model\n            for key in config[\"Architecture\"][\"Models\"]:\n                if (\n                    config[\"Architecture\"][\"Models\"][key][\"Head\"][\"name\"] == \"MultiHead\"\n                ):  # for multi head\n                    out_channels_list = {}\n                    if config[\"PostProcess\"][\"name\"] == \"DistillationSARLabelDecode\":\n                        char_num = char_num - 2\n                    if config[\"PostProcess\"][\"name\"] == \"DistillationNRTRLabelDecode\":\n                        char_num = char_num - 3\n                    out_channels_list[\"CTCLabelDecode\"] = char_num\n                    out_channels_list[\"SARLabelDecode\"] = char_num + 2\n                    out_channels_list[\"NRTRLabelDecode\"] = char_num + 3\n                    config[\"Architecture\"][\"Models\"][key][\"Head\"][\n                        \"out_channels_list\"\n                    ] = out_channels_list\n                else:\n                    config[\"Architecture\"][\"Models\"][key][\"Head\"][\n                        \"out_channels\"\n                    ] = char_num\n        elif config[\"Architecture\"][\"Head\"][\"name\"] == \"MultiHead\":  # for multi head\n            out_channels_list = {}\n            if config[\"PostProcess\"][\"name\"] == \"SARLabelDecode\":\n                char_num = char_num - 2\n            if config[\"PostProcess\"][\"name\"] == \"NRTRLabelDecode\":\n                char_num = char_num - 3\n            out_channels_list[\"CTCLabelDecode\"] = char_num\n            out_channels_list[\"SARLabelDecode\"] = char_num + 2\n            out_channels_list[\"NRTRLabelDecode\"] = char_num + 3\n            config[\"Architecture\"][\"Head\"][\"out_channels_list\"] = out_channels_list\n        else:  # base rec model\n            config[\"Architecture\"][\"Head\"][\"out_channels\"] = char_num\n\n    model = build_model(config[\"Architecture\"])\n    extra_input_models = [\n        \"SRN\",\n        \"NRTR\",\n        \"SAR\",\n        \"SEED\",\n        \"SVTR\",\n        \"SVTR_LCNet\",\n        \"VisionLAN\",\n        \"RobustScanner\",\n        \"SVTR_HGNet\",\n    ]\n    extra_input = False\n    if config[\"Architecture\"][\"algorithm\"] == \"Distillation\":\n        for key in config[\"Architecture\"][\"Models\"]:\n            extra_input = (\n                extra_input\n                or config[\"Architecture\"][\"Models\"][key][\"algorithm\"]\n                in extra_input_models\n            )\n    else:\n        extra_input = config[\"Architecture\"][\"algorithm\"] in extra_input_models\n    if \"model_type\" in config[\"Architecture\"].keys():\n        if config[\"Architecture\"][\"algorithm\"] == \"CAN\":\n            model_type = \"can\"\n        else:\n            model_type = config[\"Architecture\"][\"model_type\"]\n    else:\n        model_type = None\n\n    # build metric\n    eval_class = build_metric(config[\"Metric\"])\n    # amp\n    use_amp = config[\"Global\"].get(\"use_amp\", False)\n    amp_level = config[\"Global\"].get(\"amp_level\", \"O2\")\n    amp_custom_black_list = config[\"Global\"].get(\"amp_custom_black_list\", [])\n    if use_amp:\n        AMP_RELATED_FLAGS_SETTING = {\n            \"FLAGS_cudnn_batchnorm_spatial_persistent\": 1,\n            \"FLAGS_max_inplace_grad_add\": 8,\n        }\n        paddle.set_flags(AMP_RELATED_FLAGS_SETTING)\n        scale_loss = config[\"Global\"].get(\"scale_loss\", 1.0)\n        use_dynamic_loss_scaling = config[\"Global\"].get(\n            \"use_dynamic_loss_scaling\", False\n        )\n        scaler = paddle.amp.GradScaler(\n            init_loss_scaling=scale_loss,\n            use_dynamic_loss_scaling=use_dynamic_loss_scaling,\n        )\n        if amp_level == \"O2\":\n            model = paddle.amp.decorate(\n                models=model, level=amp_level, master_weight=True\n            )\n    else:\n        scaler = None\n\n    best_model_dict = load_model(\n        config, model, model_type=config[\"Architecture\"][\"model_type\"]\n    )\n    if len(best_model_dict):\n        logger.info(\"metric in ckpt ***************\")\n        for k, v in best_model_dict.items():\n            logger.info(\"{}:{}\".format(k, v))\n\n    # start eval\n    metric = program.eval(\n        model,\n        valid_dataloader,\n        post_process_class,\n        eval_class,\n        model_type,\n        extra_input,\n        scaler,\n        amp_level,\n        amp_custom_black_list,\n    )\n    logger.info(\"metric eval ***************\")\n    for k, v in metric.items():\n        logger.info(\"{}:{}\".format(k, v))\n\n\nif __name__ == \"__main__\":\n    config, device, logger, vdl_writer = program.preprocess()\n    main()\n", "tools/export_model.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport sys\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(__dir__)\nsys.path.insert(0, os.path.abspath(os.path.join(__dir__, \"..\")))\n\nimport argparse\n\nimport paddle\nfrom paddle.jit import to_static\n\nfrom ppocr.modeling.architectures import build_model\nfrom ppocr.postprocess import build_post_process\nfrom ppocr.utils.save_load import load_model\nfrom ppocr.utils.logging import get_logger\nfrom tools.program import load_config, merge_config, ArgsParser\n\n\ndef export_single_model(\n    model, arch_config, save_path, logger, input_shape=None, quanter=None\n):\n    if arch_config[\"algorithm\"] == \"SRN\":\n        max_text_length = arch_config[\"Head\"][\"max_text_length\"]\n        other_shape = [\n            paddle.static.InputSpec(shape=[None, 1, 64, 256], dtype=\"float32\"),\n            [\n                paddle.static.InputSpec(shape=[None, 256, 1], dtype=\"int64\"),\n                paddle.static.InputSpec(\n                    shape=[None, max_text_length, 1], dtype=\"int64\"\n                ),\n                paddle.static.InputSpec(\n                    shape=[None, 8, max_text_length, max_text_length], dtype=\"int64\"\n                ),\n                paddle.static.InputSpec(\n                    shape=[None, 8, max_text_length, max_text_length], dtype=\"int64\"\n                ),\n            ],\n        ]\n        model = to_static(model, input_spec=other_shape)\n    elif arch_config[\"algorithm\"] == \"SAR\":\n        other_shape = [\n            paddle.static.InputSpec(shape=[None, 3, 48, 160], dtype=\"float32\"),\n            [paddle.static.InputSpec(shape=[None], dtype=\"float32\")],\n        ]\n        model = to_static(model, input_spec=other_shape)\n    elif arch_config[\"algorithm\"] in [\"SVTR_LCNet\", \"SVTR_HGNet\"]:\n        other_shape = [\n            paddle.static.InputSpec(shape=[None, 3, 48, -1], dtype=\"float32\"),\n        ]\n        model = to_static(model, input_spec=other_shape)\n    elif arch_config[\"algorithm\"] in [\"SVTR\", \"CPPD\"]:\n        other_shape = [\n            paddle.static.InputSpec(shape=[None] + input_shape, dtype=\"float32\"),\n        ]\n        model = to_static(model, input_spec=other_shape)\n    elif arch_config[\"algorithm\"] == \"PREN\":\n        other_shape = [\n            paddle.static.InputSpec(shape=[None, 3, 64, 256], dtype=\"float32\"),\n        ]\n        model = to_static(model, input_spec=other_shape)\n    elif arch_config[\"model_type\"] == \"sr\":\n        other_shape = [\n            paddle.static.InputSpec(shape=[None, 3, 16, 64], dtype=\"float32\")\n        ]\n        model = to_static(model, input_spec=other_shape)\n    elif arch_config[\"algorithm\"] == \"ViTSTR\":\n        other_shape = [\n            paddle.static.InputSpec(shape=[None, 1, 224, 224], dtype=\"float32\"),\n        ]\n        model = to_static(model, input_spec=other_shape)\n    elif arch_config[\"algorithm\"] == \"ABINet\":\n        if not input_shape:\n            input_shape = [3, 32, 128]\n        other_shape = [\n            paddle.static.InputSpec(shape=[None] + input_shape, dtype=\"float32\"),\n        ]\n        model = to_static(model, input_spec=other_shape)\n    elif arch_config[\"algorithm\"] in [\"NRTR\", \"SPIN\", \"RFL\"]:\n        other_shape = [\n            paddle.static.InputSpec(shape=[None, 1, 32, 100], dtype=\"float32\"),\n        ]\n        model = to_static(model, input_spec=other_shape)\n    elif arch_config[\"algorithm\"] in [\"SATRN\"]:\n        other_shape = [\n            paddle.static.InputSpec(shape=[None, 3, 32, 100], dtype=\"float32\"),\n        ]\n        model = to_static(model, input_spec=other_shape)\n    elif arch_config[\"algorithm\"] == \"VisionLAN\":\n        other_shape = [\n            paddle.static.InputSpec(shape=[None, 3, 64, 256], dtype=\"float32\"),\n        ]\n        model = to_static(model, input_spec=other_shape)\n    elif arch_config[\"algorithm\"] == \"RobustScanner\":\n        max_text_length = arch_config[\"Head\"][\"max_text_length\"]\n        other_shape = [\n            paddle.static.InputSpec(shape=[None, 3, 48, 160], dtype=\"float32\"),\n            [\n                paddle.static.InputSpec(\n                    shape=[\n                        None,\n                    ],\n                    dtype=\"float32\",\n                ),\n                paddle.static.InputSpec(shape=[None, max_text_length], dtype=\"int64\"),\n            ],\n        ]\n        model = to_static(model, input_spec=other_shape)\n    elif arch_config[\"algorithm\"] == \"CAN\":\n        other_shape = [\n            [\n                paddle.static.InputSpec(shape=[None, 1, None, None], dtype=\"float32\"),\n                paddle.static.InputSpec(shape=[None, 1, None, None], dtype=\"float32\"),\n                paddle.static.InputSpec(\n                    shape=[None, arch_config[\"Head\"][\"max_text_length\"]], dtype=\"int64\"\n                ),\n            ]\n        ]\n        model = to_static(model, input_spec=other_shape)\n    elif arch_config[\"algorithm\"] in [\"LayoutLM\", \"LayoutLMv2\", \"LayoutXLM\"]:\n        input_spec = [\n            paddle.static.InputSpec(shape=[None, 512], dtype=\"int64\"),  # input_ids\n            paddle.static.InputSpec(shape=[None, 512, 4], dtype=\"int64\"),  # bbox\n            paddle.static.InputSpec(shape=[None, 512], dtype=\"int64\"),  # attention_mask\n            paddle.static.InputSpec(shape=[None, 512], dtype=\"int64\"),  # token_type_ids\n            paddle.static.InputSpec(shape=[None, 3, 224, 224], dtype=\"int64\"),  # image\n        ]\n        if \"Re\" in arch_config[\"Backbone\"][\"name\"]:\n            input_spec.extend(\n                [\n                    paddle.static.InputSpec(\n                        shape=[None, 512, 3], dtype=\"int64\"\n                    ),  # entities\n                    paddle.static.InputSpec(\n                        shape=[None, None, 2], dtype=\"int64\"\n                    ),  # relations\n                ]\n            )\n        if model.backbone.use_visual_backbone is False:\n            input_spec.pop(4)\n        model = to_static(model, input_spec=[input_spec])\n    else:\n        infer_shape = [3, -1, -1]\n        if arch_config[\"model_type\"] == \"rec\":\n            infer_shape = [3, 32, -1]  # for rec model, H must be 32\n            if (\n                \"Transform\" in arch_config\n                and arch_config[\"Transform\"] is not None\n                and arch_config[\"Transform\"][\"name\"] == \"TPS\"\n            ):\n                logger.info(\n                    \"When there is tps in the network, variable length input is not supported, and the input size needs to be the same as during training\"\n                )\n                infer_shape[-1] = 100\n        elif arch_config[\"model_type\"] == \"table\":\n            infer_shape = [3, 488, 488]\n            if arch_config[\"algorithm\"] == \"TableMaster\":\n                infer_shape = [3, 480, 480]\n            if arch_config[\"algorithm\"] == \"SLANet\":\n                infer_shape = [3, -1, -1]\n        model = to_static(\n            model,\n            input_spec=[\n                paddle.static.InputSpec(shape=[None] + infer_shape, dtype=\"float32\")\n            ],\n        )\n\n    if (\n        arch_config[\"model_type\"] != \"sr\"\n        and arch_config[\"Backbone\"][\"name\"] == \"PPLCNetV3\"\n    ):\n        # for rep lcnetv3\n        for layer in model.sublayers():\n            if hasattr(layer, \"rep\") and not getattr(layer, \"is_repped\"):\n                layer.rep()\n\n    if quanter is None:\n        paddle.jit.save(model, save_path)\n    else:\n        quanter.save_quantized_model(model, save_path)\n    logger.info(\"inference model is saved to {}\".format(save_path))\n    return\n\n\ndef main():\n    FLAGS = ArgsParser().parse_args()\n    config = load_config(FLAGS.config)\n    config = merge_config(config, FLAGS.opt)\n    logger = get_logger()\n    # build post process\n\n    post_process_class = build_post_process(config[\"PostProcess\"], config[\"Global\"])\n\n    # build model\n    # for rec algorithm\n    if hasattr(post_process_class, \"character\"):\n        char_num = len(getattr(post_process_class, \"character\"))\n        if config[\"Architecture\"][\"algorithm\"] in [\n            \"Distillation\",\n        ]:  # distillation model\n            for key in config[\"Architecture\"][\"Models\"]:\n                if (\n                    config[\"Architecture\"][\"Models\"][key][\"Head\"][\"name\"] == \"MultiHead\"\n                ):  # multi head\n                    out_channels_list = {}\n                    if config[\"PostProcess\"][\"name\"] == \"DistillationSARLabelDecode\":\n                        char_num = char_num - 2\n                    if config[\"PostProcess\"][\"name\"] == \"DistillationNRTRLabelDecode\":\n                        char_num = char_num - 3\n                    out_channels_list[\"CTCLabelDecode\"] = char_num\n                    out_channels_list[\"SARLabelDecode\"] = char_num + 2\n                    out_channels_list[\"NRTRLabelDecode\"] = char_num + 3\n                    config[\"Architecture\"][\"Models\"][key][\"Head\"][\n                        \"out_channels_list\"\n                    ] = out_channels_list\n                else:\n                    config[\"Architecture\"][\"Models\"][key][\"Head\"][\n                        \"out_channels\"\n                    ] = char_num\n                # just one final tensor needs to exported for inference\n                config[\"Architecture\"][\"Models\"][key][\"return_all_feats\"] = False\n        elif config[\"Architecture\"][\"Head\"][\"name\"] == \"MultiHead\":  # multi head\n            out_channels_list = {}\n            char_num = len(getattr(post_process_class, \"character\"))\n            if config[\"PostProcess\"][\"name\"] == \"SARLabelDecode\":\n                char_num = char_num - 2\n            if config[\"PostProcess\"][\"name\"] == \"NRTRLabelDecode\":\n                char_num = char_num - 3\n            out_channels_list[\"CTCLabelDecode\"] = char_num\n            out_channels_list[\"SARLabelDecode\"] = char_num + 2\n            out_channels_list[\"NRTRLabelDecode\"] = char_num + 3\n            config[\"Architecture\"][\"Head\"][\"out_channels_list\"] = out_channels_list\n        else:  # base rec model\n            config[\"Architecture\"][\"Head\"][\"out_channels\"] = char_num\n\n    # for sr algorithm\n    if config[\"Architecture\"][\"model_type\"] == \"sr\":\n        config[\"Architecture\"][\"Transform\"][\"infer_mode\"] = True\n    model = build_model(config[\"Architecture\"])\n    load_model(config, model, model_type=config[\"Architecture\"][\"model_type\"])\n    model.eval()\n\n    save_path = config[\"Global\"][\"save_inference_dir\"]\n\n    arch_config = config[\"Architecture\"]\n\n    if (\n        arch_config[\"algorithm\"] in [\"SVTR\", \"CPPD\"]\n        and arch_config[\"Head\"][\"name\"] != \"MultiHead\"\n    ):\n        input_shape = config[\"Eval\"][\"dataset\"][\"transforms\"][-2][\"SVTRRecResizeImg\"][\n            \"image_shape\"\n        ]\n    elif arch_config[\"algorithm\"].lower() == \"ABINet\".lower():\n        rec_rs = [\n            c\n            for c in config[\"Eval\"][\"dataset\"][\"transforms\"]\n            if \"ABINetRecResizeImg\" in c\n        ]\n        input_shape = rec_rs[0][\"ABINetRecResizeImg\"][\"image_shape\"] if rec_rs else None\n    else:\n        input_shape = None\n\n    if arch_config[\"algorithm\"] in [\n        \"Distillation\",\n    ]:  # distillation model\n        archs = list(arch_config[\"Models\"].values())\n        for idx, name in enumerate(model.model_name_list):\n            sub_model_save_path = os.path.join(save_path, name, \"inference\")\n            export_single_model(\n                model.model_list[idx], archs[idx], sub_model_save_path, logger\n            )\n    else:\n        save_path = os.path.join(save_path, \"inference\")\n        export_single_model(\n            model, arch_config, save_path, logger, input_shape=input_shape\n        )\n\n\nif __name__ == \"__main__\":\n    main()\n", "tools/infer_sr.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nimport os\nimport sys\nimport json\nfrom PIL import Image\nimport cv2\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.insert(0, __dir__)\nsys.path.insert(0, os.path.abspath(os.path.join(__dir__, \"..\")))\n\nos.environ[\"FLAGS_allocator_strategy\"] = \"auto_growth\"\n\nimport paddle\n\nfrom ppocr.data import create_operators, transform\nfrom ppocr.modeling.architectures import build_model\nfrom ppocr.postprocess import build_post_process\nfrom ppocr.utils.save_load import load_model\nfrom ppocr.utils.utility import get_image_file_list\nimport tools.program as program\n\n\ndef main():\n    global_config = config[\"Global\"]\n\n    # build post process\n    post_process_class = build_post_process(config[\"PostProcess\"], global_config)\n\n    # sr transform\n    config[\"Architecture\"][\"Transform\"][\"infer_mode\"] = True\n\n    model = build_model(config[\"Architecture\"])\n\n    load_model(config, model)\n\n    # create data ops\n    transforms = []\n    for op in config[\"Eval\"][\"dataset\"][\"transforms\"]:\n        op_name = list(op)[0]\n        if \"Label\" in op_name:\n            continue\n        elif op_name in [\"SRResize\"]:\n            op[op_name][\"infer_mode\"] = True\n        elif op_name == \"KeepKeys\":\n            op[op_name][\"keep_keys\"] = [\"img_lr\"]\n        transforms.append(op)\n    global_config[\"infer_mode\"] = True\n    ops = create_operators(transforms, global_config)\n\n    save_visual_path = config[\"Global\"].get(\"save_visual\", \"infer_result/\")\n    if not os.path.exists(os.path.dirname(save_visual_path)):\n        os.makedirs(os.path.dirname(save_visual_path))\n\n    model.eval()\n    for file in get_image_file_list(config[\"Global\"][\"infer_img\"]):\n        logger.info(\"infer_img: {}\".format(file))\n        img = Image.open(file).convert(\"RGB\")\n        data = {\"image_lr\": img}\n        batch = transform(data, ops)\n        images = np.expand_dims(batch[0], axis=0)\n        images = paddle.to_tensor(images)\n\n        preds = model(images)\n        sr_img = preds[\"sr_img\"][0]\n        lr_img = preds[\"lr_img\"][0]\n        fm_sr = (sr_img.numpy() * 255).transpose(1, 2, 0).astype(np.uint8)\n        fm_lr = (lr_img.numpy() * 255).transpose(1, 2, 0).astype(np.uint8)\n        img_name_pure = os.path.split(file)[-1]\n        cv2.imwrite(\n            \"{}/sr_{}\".format(save_visual_path, img_name_pure), fm_sr[:, :, ::-1]\n        )\n        logger.info(\n            \"The visualized image saved in infer_result/sr_{}\".format(img_name_pure)\n        )\n\n    logger.info(\"success!\")\n\n\nif __name__ == \"__main__\":\n    config, device, logger, vdl_writer = program.preprocess()\n    main()\n", "tools/infer_e2e.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nimport os\nimport sys\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(__dir__)\nsys.path.insert(0, os.path.abspath(os.path.join(__dir__, \"..\")))\n\nos.environ[\"FLAGS_allocator_strategy\"] = \"auto_growth\"\n\nimport cv2\nimport json\nimport paddle\n\nfrom ppocr.data import create_operators, transform\nfrom ppocr.modeling.architectures import build_model\nfrom ppocr.postprocess import build_post_process\nfrom ppocr.utils.save_load import load_model\nfrom ppocr.utils.utility import get_image_file_list\nimport tools.program as program\nfrom PIL import Image, ImageDraw, ImageFont\nimport math\n\n\ndef draw_e2e_res_for_chinese(\n    image, boxes, txts, config, img_name, font_path=\"./doc/simfang.ttf\"\n):\n    h, w = image.height, image.width\n    img_left = image.copy()\n    img_right = Image.new(\"RGB\", (w, h), (255, 255, 255))\n\n    import random\n\n    random.seed(0)\n    draw_left = ImageDraw.Draw(img_left)\n    draw_right = ImageDraw.Draw(img_right)\n    for idx, (box, txt) in enumerate(zip(boxes, txts)):\n        box = np.array(box)\n        box = [tuple(x) for x in box]\n        color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n        draw_left.polygon(box, fill=color)\n        draw_right.polygon(box, outline=color)\n        font = ImageFont.truetype(font_path, 15, encoding=\"utf-8\")\n        draw_right.text([box[0][0], box[0][1]], txt, fill=(0, 0, 0), font=font)\n    img_left = Image.blend(image, img_left, 0.5)\n    img_show = Image.new(\"RGB\", (w * 2, h), (255, 255, 255))\n    img_show.paste(img_left, (0, 0, w, h))\n    img_show.paste(img_right, (w, 0, w * 2, h))\n\n    save_e2e_path = os.path.dirname(config[\"Global\"][\"save_res_path\"]) + \"/e2e_results/\"\n    if not os.path.exists(save_e2e_path):\n        os.makedirs(save_e2e_path)\n    save_path = os.path.join(save_e2e_path, os.path.basename(img_name))\n    cv2.imwrite(save_path, np.array(img_show)[:, :, ::-1])\n    logger.info(\"The e2e Image saved in {}\".format(save_path))\n\n\ndef draw_e2e_res(dt_boxes, strs, config, img, img_name):\n    if len(dt_boxes) > 0:\n        src_im = img\n        for box, str in zip(dt_boxes, strs):\n            box = box.astype(np.int32).reshape((-1, 1, 2))\n            cv2.polylines(src_im, [box], True, color=(255, 255, 0), thickness=2)\n            cv2.putText(\n                src_im,\n                str,\n                org=(int(box[0, 0, 0]), int(box[0, 0, 1])),\n                fontFace=cv2.FONT_HERSHEY_COMPLEX,\n                fontScale=0.7,\n                color=(0, 255, 0),\n                thickness=1,\n            )\n        save_det_path = (\n            os.path.dirname(config[\"Global\"][\"save_res_path\"]) + \"/e2e_results/\"\n        )\n        if not os.path.exists(save_det_path):\n            os.makedirs(save_det_path)\n        save_path = os.path.join(save_det_path, os.path.basename(img_name))\n        cv2.imwrite(save_path, src_im)\n        logger.info(\"The e2e Image saved in {}\".format(save_path))\n\n\ndef main():\n    global_config = config[\"Global\"]\n\n    # build model\n    model = build_model(config[\"Architecture\"])\n\n    load_model(config, model)\n\n    # build post process\n    post_process_class = build_post_process(config[\"PostProcess\"], global_config)\n\n    # create data ops\n    transforms = []\n    for op in config[\"Eval\"][\"dataset\"][\"transforms\"]:\n        op_name = list(op)[0]\n        if \"Label\" in op_name:\n            continue\n        elif op_name == \"KeepKeys\":\n            op[op_name][\"keep_keys\"] = [\"image\", \"shape\"]\n        transforms.append(op)\n\n    ops = create_operators(transforms, global_config)\n\n    save_res_path = config[\"Global\"][\"save_res_path\"]\n    if not os.path.exists(os.path.dirname(save_res_path)):\n        os.makedirs(os.path.dirname(save_res_path))\n\n    model.eval()\n    with open(save_res_path, \"wb\") as fout:\n        for file in get_image_file_list(config[\"Global\"][\"infer_img\"]):\n            logger.info(\"infer_img: {}\".format(file))\n            with open(file, \"rb\") as f:\n                img = f.read()\n                data = {\"image\": img}\n            batch = transform(data, ops)\n            images = np.expand_dims(batch[0], axis=0)\n            shape_list = np.expand_dims(batch[1], axis=0)\n            images = paddle.to_tensor(images)\n            preds = model(images)\n            post_result = post_process_class(preds, shape_list)\n            points, strs = post_result[\"points\"], post_result[\"texts\"]\n            # write result\n            dt_boxes_json = []\n            for poly, str in zip(points, strs):\n                tmp_json = {\"transcription\": str}\n                tmp_json[\"points\"] = poly.tolist()\n                dt_boxes_json.append(tmp_json)\n            otstr = file + \"\\t\" + json.dumps(dt_boxes_json) + \"\\n\"\n            fout.write(otstr.encode())\n            src_img = cv2.imread(file)\n            if global_config[\"infer_visual_type\"] == \"EN\":\n                draw_e2e_res(points, strs, config, src_img, file)\n            elif global_config[\"infer_visual_type\"] == \"CN\":\n                src_img = Image.fromarray(cv2.cvtColor(src_img, cv2.COLOR_BGR2RGB))\n                draw_e2e_res_for_chinese(\n                    src_img,\n                    points,\n                    strs,\n                    config,\n                    file,\n                    font_path=\"./doc/fonts/simfang.ttf\",\n                )\n\n    logger.info(\"success!\")\n\n\nif __name__ == \"__main__\":\n    config, device, logger, vdl_writer = program.preprocess()\n    main()\n", "tools/infer_rec.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nimport os\nimport sys\nimport json\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(__dir__)\nsys.path.insert(0, os.path.abspath(os.path.join(__dir__, \"..\")))\n\nos.environ[\"FLAGS_allocator_strategy\"] = \"auto_growth\"\n\nimport paddle\n\nfrom ppocr.data import create_operators, transform\nfrom ppocr.modeling.architectures import build_model\nfrom ppocr.postprocess import build_post_process\nfrom ppocr.utils.save_load import load_model\nfrom ppocr.utils.utility import get_image_file_list\nimport tools.program as program\n\n\ndef main():\n    global_config = config[\"Global\"]\n\n    # build post process\n    post_process_class = build_post_process(config[\"PostProcess\"], global_config)\n\n    # build model\n    if hasattr(post_process_class, \"character\"):\n        char_num = len(getattr(post_process_class, \"character\"))\n        if config[\"Architecture\"][\"algorithm\"] in [\n            \"Distillation\",\n        ]:  # distillation model\n            for key in config[\"Architecture\"][\"Models\"]:\n                if (\n                    config[\"Architecture\"][\"Models\"][key][\"Head\"][\"name\"] == \"MultiHead\"\n                ):  # multi head\n                    out_channels_list = {}\n                    if config[\"PostProcess\"][\"name\"] == \"DistillationSARLabelDecode\":\n                        char_num = char_num - 2\n                    if config[\"PostProcess\"][\"name\"] == \"DistillationNRTRLabelDecode\":\n                        char_num = char_num - 3\n                    out_channels_list[\"CTCLabelDecode\"] = char_num\n                    out_channels_list[\"SARLabelDecode\"] = char_num + 2\n                    out_channels_list[\"NRTRLabelDecode\"] = char_num + 3\n                    config[\"Architecture\"][\"Models\"][key][\"Head\"][\n                        \"out_channels_list\"\n                    ] = out_channels_list\n                else:\n                    config[\"Architecture\"][\"Models\"][key][\"Head\"][\n                        \"out_channels\"\n                    ] = char_num\n        elif config[\"Architecture\"][\"Head\"][\"name\"] == \"MultiHead\":  # multi head\n            out_channels_list = {}\n            char_num = len(getattr(post_process_class, \"character\"))\n            if config[\"PostProcess\"][\"name\"] == \"SARLabelDecode\":\n                char_num = char_num - 2\n            if config[\"PostProcess\"][\"name\"] == \"NRTRLabelDecode\":\n                char_num = char_num - 3\n            out_channels_list[\"CTCLabelDecode\"] = char_num\n            out_channels_list[\"SARLabelDecode\"] = char_num + 2\n            out_channels_list[\"NRTRLabelDecode\"] = char_num + 3\n            config[\"Architecture\"][\"Head\"][\"out_channels_list\"] = out_channels_list\n        else:  # base rec model\n            config[\"Architecture\"][\"Head\"][\"out_channels\"] = char_num\n    model = build_model(config[\"Architecture\"])\n\n    load_model(config, model)\n\n    # create data ops\n    transforms = []\n    for op in config[\"Eval\"][\"dataset\"][\"transforms\"]:\n        op_name = list(op)[0]\n        if \"Label\" in op_name:\n            continue\n        elif op_name in [\"RecResizeImg\"]:\n            op[op_name][\"infer_mode\"] = True\n        elif op_name == \"KeepKeys\":\n            if config[\"Architecture\"][\"algorithm\"] == \"SRN\":\n                op[op_name][\"keep_keys\"] = [\n                    \"image\",\n                    \"encoder_word_pos\",\n                    \"gsrm_word_pos\",\n                    \"gsrm_slf_attn_bias1\",\n                    \"gsrm_slf_attn_bias2\",\n                ]\n            elif config[\"Architecture\"][\"algorithm\"] == \"SAR\":\n                op[op_name][\"keep_keys\"] = [\"image\", \"valid_ratio\"]\n            elif config[\"Architecture\"][\"algorithm\"] == \"RobustScanner\":\n                op[op_name][\"keep_keys\"] = [\"image\", \"valid_ratio\", \"word_positons\"]\n            else:\n                op[op_name][\"keep_keys\"] = [\"image\"]\n        transforms.append(op)\n    global_config[\"infer_mode\"] = True\n    ops = create_operators(transforms, global_config)\n\n    save_res_path = config[\"Global\"].get(\n        \"save_res_path\", \"./output/rec/predicts_rec.txt\"\n    )\n    if not os.path.exists(os.path.dirname(save_res_path)):\n        os.makedirs(os.path.dirname(save_res_path))\n\n    model.eval()\n\n    infer_imgs = config[\"Global\"][\"infer_img\"]\n    infer_list = config[\"Global\"].get(\"infer_list\", None)\n    with open(save_res_path, \"w\") as fout:\n        for file in get_image_file_list(infer_imgs, infer_list=infer_list):\n            logger.info(\"infer_img: {}\".format(file))\n            with open(file, \"rb\") as f:\n                img = f.read()\n                data = {\"image\": img}\n            batch = transform(data, ops)\n            if config[\"Architecture\"][\"algorithm\"] == \"SRN\":\n                encoder_word_pos_list = np.expand_dims(batch[1], axis=0)\n                gsrm_word_pos_list = np.expand_dims(batch[2], axis=0)\n                gsrm_slf_attn_bias1_list = np.expand_dims(batch[3], axis=0)\n                gsrm_slf_attn_bias2_list = np.expand_dims(batch[4], axis=0)\n\n                others = [\n                    paddle.to_tensor(encoder_word_pos_list),\n                    paddle.to_tensor(gsrm_word_pos_list),\n                    paddle.to_tensor(gsrm_slf_attn_bias1_list),\n                    paddle.to_tensor(gsrm_slf_attn_bias2_list),\n                ]\n            if config[\"Architecture\"][\"algorithm\"] == \"SAR\":\n                valid_ratio = np.expand_dims(batch[-1], axis=0)\n                img_metas = [paddle.to_tensor(valid_ratio)]\n            if config[\"Architecture\"][\"algorithm\"] == \"RobustScanner\":\n                valid_ratio = np.expand_dims(batch[1], axis=0)\n                word_positons = np.expand_dims(batch[2], axis=0)\n                img_metas = [\n                    paddle.to_tensor(valid_ratio),\n                    paddle.to_tensor(word_positons),\n                ]\n            if config[\"Architecture\"][\"algorithm\"] == \"CAN\":\n                image_mask = paddle.ones(\n                    (np.expand_dims(batch[0], axis=0).shape), dtype=\"float32\"\n                )\n                label = paddle.ones((1, 36), dtype=\"int64\")\n            images = np.expand_dims(batch[0], axis=0)\n            images = paddle.to_tensor(images)\n            if config[\"Architecture\"][\"algorithm\"] == \"SRN\":\n                preds = model(images, others)\n            elif config[\"Architecture\"][\"algorithm\"] == \"SAR\":\n                preds = model(images, img_metas)\n            elif config[\"Architecture\"][\"algorithm\"] == \"RobustScanner\":\n                preds = model(images, img_metas)\n            elif config[\"Architecture\"][\"algorithm\"] == \"CAN\":\n                preds = model([images, image_mask, label])\n            else:\n                preds = model(images)\n            post_result = post_process_class(preds)\n            info = None\n            if isinstance(post_result, dict):\n                rec_info = dict()\n                for key in post_result:\n                    if len(post_result[key][0]) >= 2:\n                        rec_info[key] = {\n                            \"label\": post_result[key][0][0],\n                            \"score\": float(post_result[key][0][1]),\n                        }\n                info = json.dumps(rec_info, ensure_ascii=False)\n            elif isinstance(post_result, list) and isinstance(post_result[0], int):\n                # for RFLearning CNT branch\n                info = str(post_result[0])\n            else:\n                if len(post_result[0]) >= 2:\n                    info = post_result[0][0] + \"\\t\" + str(post_result[0][1])\n\n            if info is not None:\n                logger.info(\"\\t result: {}\".format(info))\n                fout.write(file + \"\\t\" + info + \"\\n\")\n    logger.info(\"success!\")\n\n\nif __name__ == \"__main__\":\n    config, device, logger, vdl_writer = program.preprocess()\n    main()\n", "tools/infer_det.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nimport os\nimport sys\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(__dir__)\nsys.path.insert(0, os.path.abspath(os.path.join(__dir__, \"..\")))\n\nos.environ[\"FLAGS_allocator_strategy\"] = \"auto_growth\"\n\nimport cv2\nimport json\nimport paddle\n\nfrom ppocr.data import create_operators, transform\nfrom ppocr.modeling.architectures import build_model\nfrom ppocr.postprocess import build_post_process\nfrom ppocr.utils.save_load import load_model\nfrom ppocr.utils.utility import get_image_file_list\nimport tools.program as program\n\n\ndef draw_det_res(dt_boxes, config, img, img_name, save_path):\n    import cv2\n\n    src_im = img\n    for box in dt_boxes:\n        box = np.array(box).astype(np.int32).reshape((-1, 1, 2))\n        cv2.polylines(src_im, [box], True, color=(255, 255, 0), thickness=2)\n    if not os.path.exists(save_path):\n        os.makedirs(save_path)\n    save_path = os.path.join(save_path, os.path.basename(img_name))\n    cv2.imwrite(save_path, src_im)\n    logger.info(\"The detected Image saved in {}\".format(save_path))\n\n\n@paddle.no_grad()\ndef main():\n    global_config = config[\"Global\"]\n\n    # build model\n    model = build_model(config[\"Architecture\"])\n\n    load_model(config, model)\n    # build post process\n    post_process_class = build_post_process(config[\"PostProcess\"])\n\n    # create data ops\n    transforms = []\n    for op in config[\"Eval\"][\"dataset\"][\"transforms\"]:\n        op_name = list(op)[0]\n        if \"Label\" in op_name:\n            continue\n        elif op_name == \"KeepKeys\":\n            op[op_name][\"keep_keys\"] = [\"image\", \"shape\"]\n        transforms.append(op)\n\n    ops = create_operators(transforms, global_config)\n\n    save_res_path = config[\"Global\"][\"save_res_path\"]\n    if not os.path.exists(os.path.dirname(save_res_path)):\n        os.makedirs(os.path.dirname(save_res_path))\n\n    model.eval()\n    with open(save_res_path, \"wb\") as fout:\n        for file in get_image_file_list(config[\"Global\"][\"infer_img\"]):\n            logger.info(\"infer_img: {}\".format(file))\n            with open(file, \"rb\") as f:\n                img = f.read()\n                data = {\"image\": img}\n            batch = transform(data, ops)\n\n            images = np.expand_dims(batch[0], axis=0)\n            shape_list = np.expand_dims(batch[1], axis=0)\n            images = paddle.to_tensor(images)\n            preds = model(images)\n            post_result = post_process_class(preds, shape_list)\n\n            src_img = cv2.imread(file)\n\n            dt_boxes_json = []\n            # parser boxes if post_result is dict\n            if isinstance(post_result, dict):\n                det_box_json = {}\n                for k in post_result.keys():\n                    boxes = post_result[k][0][\"points\"]\n                    dt_boxes_list = []\n                    for box in boxes:\n                        tmp_json = {\"transcription\": \"\"}\n                        tmp_json[\"points\"] = np.array(box).tolist()\n                        dt_boxes_list.append(tmp_json)\n                    det_box_json[k] = dt_boxes_list\n                    save_det_path = os.path.dirname(\n                        config[\"Global\"][\"save_res_path\"]\n                    ) + \"/det_results_{}/\".format(k)\n                    draw_det_res(boxes, config, src_img, file, save_det_path)\n            else:\n                boxes = post_result[0][\"points\"]\n                dt_boxes_json = []\n                # write result\n                for box in boxes:\n                    tmp_json = {\"transcription\": \"\"}\n                    tmp_json[\"points\"] = np.array(box).tolist()\n                    dt_boxes_json.append(tmp_json)\n                save_det_path = (\n                    os.path.dirname(config[\"Global\"][\"save_res_path\"]) + \"/det_results/\"\n                )\n                draw_det_res(boxes, config, src_img, file, save_det_path)\n            otstr = file + \"\\t\" + json.dumps(dt_boxes_json) + \"\\n\"\n            fout.write(otstr.encode())\n\n    logger.info(\"success!\")\n\n\nif __name__ == \"__main__\":\n    config, device, logger, vdl_writer = program.preprocess()\n    main()\n", "tools/infer_kie.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport paddle.nn.functional as F\n\nimport os\nimport sys\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(__dir__)\nsys.path.insert(0, os.path.abspath(os.path.join(__dir__, \"..\")))\n\nos.environ[\"FLAGS_allocator_strategy\"] = \"auto_growth\"\n\nimport cv2\nimport paddle\n\nfrom ppocr.data import create_operators, transform\nfrom ppocr.modeling.architectures import build_model\nfrom ppocr.utils.save_load import load_model\nimport tools.program as program\nimport time\n\n\ndef read_class_list(filepath):\n    ret = {}\n    with open(filepath, \"r\") as f:\n        lines = f.readlines()\n        for idx, line in enumerate(lines):\n            ret[idx] = line.strip(\"\\n\")\n    return ret\n\n\ndef draw_kie_result(batch, node, idx_to_cls, count):\n    img = batch[6].copy()\n    boxes = batch[7]\n    h, w = img.shape[:2]\n    pred_img = np.ones((h, w * 2, 3), dtype=np.uint8) * 255\n    max_value, max_idx = paddle.max(node, -1), paddle.argmax(node, -1)\n    node_pred_label = max_idx.numpy().tolist()\n    node_pred_score = max_value.numpy().tolist()\n\n    for i, box in enumerate(boxes):\n        if i >= len(node_pred_label):\n            break\n        new_box = [\n            [box[0], box[1]],\n            [box[2], box[1]],\n            [box[2], box[3]],\n            [box[0], box[3]],\n        ]\n        Pts = np.array([new_box], np.int32)\n        cv2.polylines(\n            img, [Pts.reshape((-1, 1, 2))], True, color=(255, 255, 0), thickness=1\n        )\n        x_min = int(min([point[0] for point in new_box]))\n        y_min = int(min([point[1] for point in new_box]))\n\n        pred_label = node_pred_label[i]\n        if pred_label in idx_to_cls:\n            pred_label = idx_to_cls[pred_label]\n        pred_score = \"{:.2f}\".format(node_pred_score[i])\n        text = pred_label + \"(\" + pred_score + \")\"\n        cv2.putText(\n            pred_img,\n            text,\n            (x_min * 2, y_min),\n            cv2.FONT_HERSHEY_SIMPLEX,\n            0.5,\n            (255, 0, 0),\n            1,\n        )\n    vis_img = np.ones((h, w * 3, 3), dtype=np.uint8) * 255\n    vis_img[:, :w] = img\n    vis_img[:, w:] = pred_img\n    save_kie_path = os.path.dirname(config[\"Global\"][\"save_res_path\"]) + \"/kie_results/\"\n    if not os.path.exists(save_kie_path):\n        os.makedirs(save_kie_path)\n    save_path = os.path.join(save_kie_path, str(count) + \".png\")\n    cv2.imwrite(save_path, vis_img)\n    logger.info(\"The Kie Image saved in {}\".format(save_path))\n\n\ndef write_kie_result(fout, node, data):\n    \"\"\"\n    Write infer result to output file, sorted by the predict label of each line.\n    The format keeps the same as the input with additional score attribute.\n    \"\"\"\n    import json\n\n    label = data[\"label\"]\n    annotations = json.loads(label)\n    max_value, max_idx = paddle.max(node, -1), paddle.argmax(node, -1)\n    node_pred_label = max_idx.numpy().tolist()\n    node_pred_score = max_value.numpy().tolist()\n    res = []\n    for i, label in enumerate(node_pred_label):\n        pred_score = \"{:.2f}\".format(node_pred_score[i])\n        pred_res = {\n            \"label\": label,\n            \"transcription\": annotations[i][\"transcription\"],\n            \"score\": pred_score,\n            \"points\": annotations[i][\"points\"],\n        }\n        res.append(pred_res)\n    res.sort(key=lambda x: x[\"label\"])\n    fout.writelines([json.dumps(res, ensure_ascii=False) + \"\\n\"])\n\n\ndef main():\n    global_config = config[\"Global\"]\n\n    # build model\n    model = build_model(config[\"Architecture\"])\n    load_model(config, model)\n\n    # create data ops\n    transforms = []\n    for op in config[\"Eval\"][\"dataset\"][\"transforms\"]:\n        transforms.append(op)\n\n    data_dir = config[\"Eval\"][\"dataset\"][\"data_dir\"]\n\n    ops = create_operators(transforms, global_config)\n\n    save_res_path = config[\"Global\"][\"save_res_path\"]\n    class_path = config[\"Global\"][\"class_path\"]\n    idx_to_cls = read_class_list(class_path)\n    os.makedirs(os.path.dirname(save_res_path), exist_ok=True)\n\n    model.eval()\n\n    warmup_times = 0\n    count_t = []\n    with open(save_res_path, \"w\") as fout:\n        with open(config[\"Global\"][\"infer_img\"], \"rb\") as f:\n            lines = f.readlines()\n            for index, data_line in enumerate(lines):\n                if index == 10:\n                    warmup_t = time.time()\n                data_line = data_line.decode(\"utf-8\")\n                substr = data_line.strip(\"\\n\").split(\"\\t\")\n                img_path, label = data_dir + \"/\" + substr[0], substr[1]\n                data = {\"img_path\": img_path, \"label\": label}\n                with open(data[\"img_path\"], \"rb\") as f:\n                    img = f.read()\n                    data[\"image\"] = img\n                st = time.time()\n                batch = transform(data, ops)\n                batch_pred = [0] * len(batch)\n                for i in range(len(batch)):\n                    batch_pred[i] = paddle.to_tensor(np.expand_dims(batch[i], axis=0))\n                st = time.time()\n                node, edge = model(batch_pred)\n                node = F.softmax(node, -1)\n                count_t.append(time.time() - st)\n                draw_kie_result(batch, node, idx_to_cls, index)\n                write_kie_result(fout, node, data)\n        fout.close()\n    logger.info(\"success!\")\n    logger.info(\n        \"It took {} s for predict {} images.\".format(np.sum(count_t), len(count_t))\n    )\n    ips = len(count_t[warmup_times:]) / np.sum(count_t[warmup_times:])\n    logger.info(\"The ips is {} images/s\".format(ips))\n\n\nif __name__ == \"__main__\":\n    config, device, logger, vdl_writer = program.preprocess()\n    main()\n", "tools/infer_kie_token_ser_re.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nimport os\nimport sys\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(__dir__)\nsys.path.insert(0, os.path.abspath(os.path.join(__dir__, \"..\")))\n\nos.environ[\"FLAGS_allocator_strategy\"] = \"auto_growth\"\nimport cv2\nimport json\nimport paddle\nimport paddle.distributed as dist\n\nfrom ppocr.data import create_operators, transform\nfrom ppocr.modeling.architectures import build_model\nfrom ppocr.postprocess import build_post_process\nfrom ppocr.utils.save_load import load_model\nfrom ppocr.utils.visual import draw_re_results\nfrom ppocr.utils.logging import get_logger\nfrom ppocr.utils.utility import get_image_file_list, load_vqa_bio_label_maps, print_dict\nfrom tools.program import ArgsParser, load_config, merge_config\nfrom tools.infer_kie_token_ser import SerPredictor\n\n\nclass ReArgsParser(ArgsParser):\n    def __init__(self):\n        super(ReArgsParser, self).__init__()\n        self.add_argument(\n            \"-c_ser\", \"--config_ser\", help=\"ser configuration file to use\"\n        )\n        self.add_argument(\n            \"-o_ser\", \"--opt_ser\", nargs=\"+\", help=\"set ser configuration options \"\n        )\n\n    def parse_args(self, argv=None):\n        args = super(ReArgsParser, self).parse_args(argv)\n        assert (\n            args.config_ser is not None\n        ), \"Please specify --config_ser=ser_configure_file_path.\"\n        args.opt_ser = self._parse_opt(args.opt_ser)\n        return args\n\n\ndef make_input(ser_inputs, ser_results):\n    entities_labels = {\"HEADER\": 0, \"QUESTION\": 1, \"ANSWER\": 2}\n    batch_size, max_seq_len = ser_inputs[0].shape[:2]\n    entities = ser_inputs[8][0]\n    ser_results = ser_results[0]\n    assert len(entities) == len(ser_results)\n\n    # entities\n    start = []\n    end = []\n    label = []\n    entity_idx_dict = {}\n    for i, (res, entity) in enumerate(zip(ser_results, entities)):\n        if res[\"pred\"] == \"O\":\n            continue\n        entity_idx_dict[len(start)] = i\n        start.append(entity[\"start\"])\n        end.append(entity[\"end\"])\n        label.append(entities_labels[res[\"pred\"]])\n\n    entities = np.full([max_seq_len + 1, 3], fill_value=-1, dtype=np.int64)\n    entities[0, 0] = len(start)\n    entities[1 : len(start) + 1, 0] = start\n    entities[0, 1] = len(end)\n    entities[1 : len(end) + 1, 1] = end\n    entities[0, 2] = len(label)\n    entities[1 : len(label) + 1, 2] = label\n\n    # relations\n    head = []\n    tail = []\n    for i in range(len(label)):\n        for j in range(len(label)):\n            if label[i] == 1 and label[j] == 2:\n                head.append(i)\n                tail.append(j)\n\n    relations = np.full([len(head) + 1, 2], fill_value=-1, dtype=np.int64)\n    relations[0, 0] = len(head)\n    relations[1 : len(head) + 1, 0] = head\n    relations[0, 1] = len(tail)\n    relations[1 : len(tail) + 1, 1] = tail\n\n    entities = np.expand_dims(entities, axis=0)\n    entities = np.repeat(entities, batch_size, axis=0)\n    relations = np.expand_dims(relations, axis=0)\n    relations = np.repeat(relations, batch_size, axis=0)\n\n    # remove ocr_info segment_offset_id and label in ser input\n    if isinstance(ser_inputs[0], paddle.Tensor):\n        entities = paddle.to_tensor(entities)\n        relations = paddle.to_tensor(relations)\n    ser_inputs = ser_inputs[:5] + [entities, relations]\n\n    entity_idx_dict_batch = []\n    for b in range(batch_size):\n        entity_idx_dict_batch.append(entity_idx_dict)\n    return ser_inputs, entity_idx_dict_batch\n\n\nclass SerRePredictor(object):\n    def __init__(self, config, ser_config):\n        global_config = config[\"Global\"]\n        if \"infer_mode\" in global_config:\n            ser_config[\"Global\"][\"infer_mode\"] = global_config[\"infer_mode\"]\n\n        self.ser_engine = SerPredictor(ser_config)\n\n        #  init re model\n\n        # build post process\n        self.post_process_class = build_post_process(\n            config[\"PostProcess\"], global_config\n        )\n\n        # build model\n        self.model = build_model(config[\"Architecture\"])\n\n        load_model(config, self.model, model_type=config[\"Architecture\"][\"model_type\"])\n\n        self.model.eval()\n\n    def __call__(self, data):\n        ser_results, ser_inputs = self.ser_engine(data)\n        re_input, entity_idx_dict_batch = make_input(ser_inputs, ser_results)\n        if self.model.backbone.use_visual_backbone is False:\n            re_input.pop(4)\n        preds = self.model(re_input)\n        post_result = self.post_process_class(\n            preds, ser_results=ser_results, entity_idx_dict_batch=entity_idx_dict_batch\n        )\n        return post_result\n\n\ndef preprocess():\n    FLAGS = ReArgsParser().parse_args()\n    config = load_config(FLAGS.config)\n    config = merge_config(config, FLAGS.opt)\n\n    ser_config = load_config(FLAGS.config_ser)\n    ser_config = merge_config(ser_config, FLAGS.opt_ser)\n\n    logger = get_logger()\n\n    # check if set use_gpu=True in paddlepaddle cpu version\n    use_gpu = config[\"Global\"][\"use_gpu\"]\n\n    device = \"gpu:{}\".format(dist.ParallelEnv().dev_id) if use_gpu else \"cpu\"\n    device = paddle.set_device(device)\n\n    logger.info(\"{} re config {}\".format(\"*\" * 10, \"*\" * 10))\n    print_dict(config, logger)\n    logger.info(\"\\n\")\n    logger.info(\"{} ser config {}\".format(\"*\" * 10, \"*\" * 10))\n    print_dict(ser_config, logger)\n    logger.info(\"train with paddle {} and device {}\".format(paddle.__version__, device))\n    return config, ser_config, device, logger\n\n\nif __name__ == \"__main__\":\n    config, ser_config, device, logger = preprocess()\n    os.makedirs(config[\"Global\"][\"save_res_path\"], exist_ok=True)\n\n    ser_re_engine = SerRePredictor(config, ser_config)\n\n    if config[\"Global\"].get(\"infer_mode\", None) is False:\n        data_dir = config[\"Eval\"][\"dataset\"][\"data_dir\"]\n        with open(config[\"Global\"][\"infer_img\"], \"rb\") as f:\n            infer_imgs = f.readlines()\n    else:\n        infer_imgs = get_image_file_list(config[\"Global\"][\"infer_img\"])\n\n    with open(\n        os.path.join(config[\"Global\"][\"save_res_path\"], \"infer_results.txt\"),\n        \"w\",\n        encoding=\"utf-8\",\n    ) as fout:\n        for idx, info in enumerate(infer_imgs):\n            if config[\"Global\"].get(\"infer_mode\", None) is False:\n                data_line = info.decode(\"utf-8\")\n                substr = data_line.strip(\"\\n\").split(\"\\t\")\n                img_path = os.path.join(data_dir, substr[0])\n                data = {\"img_path\": img_path, \"label\": substr[1]}\n            else:\n                img_path = info\n                data = {\"img_path\": img_path}\n\n            save_img_path = os.path.join(\n                config[\"Global\"][\"save_res_path\"],\n                os.path.splitext(os.path.basename(img_path))[0] + \"_ser_re.jpg\",\n            )\n\n            result = ser_re_engine(data)\n            result = result[0]\n            fout.write(img_path + \"\\t\" + json.dumps(result, ensure_ascii=False) + \"\\n\")\n            img_res = draw_re_results(img_path, result)\n            cv2.imwrite(save_img_path, img_res)\n\n            logger.info(\n                \"process: [{}/{}], save result to {}\".format(\n                    idx, len(infer_imgs), save_img_path\n                )\n            )\n", "tools/__init__.py": "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n", "tools/infer_kie_token_ser.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nimport os\nimport sys\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(__dir__)\nsys.path.insert(0, os.path.abspath(os.path.join(__dir__, \"..\")))\n\nos.environ[\"FLAGS_allocator_strategy\"] = \"auto_growth\"\nimport cv2\nimport json\nimport paddle\n\nfrom ppocr.data import create_operators, transform\nfrom ppocr.modeling.architectures import build_model\nfrom ppocr.postprocess import build_post_process\nfrom ppocr.utils.save_load import load_model\nfrom ppocr.utils.visual import draw_ser_results\nfrom ppocr.utils.utility import get_image_file_list, load_vqa_bio_label_maps\nimport tools.program as program\n\n\ndef to_tensor(data):\n    import numbers\n    from collections import defaultdict\n\n    data_dict = defaultdict(list)\n    to_tensor_idxs = []\n\n    for idx, v in enumerate(data):\n        if isinstance(v, (np.ndarray, paddle.Tensor, numbers.Number)):\n            if idx not in to_tensor_idxs:\n                to_tensor_idxs.append(idx)\n        data_dict[idx].append(v)\n    for idx in to_tensor_idxs:\n        data_dict[idx] = paddle.to_tensor(data_dict[idx])\n    return list(data_dict.values())\n\n\nclass SerPredictor(object):\n    def __init__(self, config):\n        global_config = config[\"Global\"]\n        self.algorithm = config[\"Architecture\"][\"algorithm\"]\n\n        # build post process\n        self.post_process_class = build_post_process(\n            config[\"PostProcess\"], global_config\n        )\n\n        # build model\n        self.model = build_model(config[\"Architecture\"])\n\n        load_model(config, self.model, model_type=config[\"Architecture\"][\"model_type\"])\n\n        from paddleocr import PaddleOCR\n\n        self.ocr_engine = PaddleOCR(\n            use_angle_cls=False,\n            show_log=False,\n            rec_model_dir=global_config.get(\"kie_rec_model_dir\", None),\n            det_model_dir=global_config.get(\"kie_det_model_dir\", None),\n            use_gpu=global_config[\"use_gpu\"],\n        )\n\n        # create data ops\n        transforms = []\n        for op in config[\"Eval\"][\"dataset\"][\"transforms\"]:\n            op_name = list(op)[0]\n            if \"Label\" in op_name:\n                op[op_name][\"ocr_engine\"] = self.ocr_engine\n            elif op_name == \"KeepKeys\":\n                op[op_name][\"keep_keys\"] = [\n                    \"input_ids\",\n                    \"bbox\",\n                    \"attention_mask\",\n                    \"token_type_ids\",\n                    \"image\",\n                    \"labels\",\n                    \"segment_offset_id\",\n                    \"ocr_info\",\n                    \"entities\",\n                ]\n\n            transforms.append(op)\n        if config[\"Global\"].get(\"infer_mode\", None) is None:\n            global_config[\"infer_mode\"] = True\n        self.ops = create_operators(\n            config[\"Eval\"][\"dataset\"][\"transforms\"], global_config\n        )\n        self.model.eval()\n\n    def __call__(self, data):\n        with open(data[\"img_path\"], \"rb\") as f:\n            img = f.read()\n        data[\"image\"] = img\n        batch = transform(data, self.ops)\n        batch = to_tensor(batch)\n        preds = self.model(batch)\n\n        post_result = self.post_process_class(\n            preds, segment_offset_ids=batch[6], ocr_infos=batch[7]\n        )\n        return post_result, batch\n\n\nif __name__ == \"__main__\":\n    config, device, logger, vdl_writer = program.preprocess()\n    os.makedirs(config[\"Global\"][\"save_res_path\"], exist_ok=True)\n\n    ser_engine = SerPredictor(config)\n\n    if config[\"Global\"].get(\"infer_mode\", None) is False:\n        data_dir = config[\"Eval\"][\"dataset\"][\"data_dir\"]\n        with open(config[\"Global\"][\"infer_img\"], \"rb\") as f:\n            infer_imgs = f.readlines()\n    else:\n        infer_imgs = get_image_file_list(config[\"Global\"][\"infer_img\"])\n\n    with open(\n        os.path.join(config[\"Global\"][\"save_res_path\"], \"infer_results.txt\"),\n        \"w\",\n        encoding=\"utf-8\",\n    ) as fout:\n        for idx, info in enumerate(infer_imgs):\n            if config[\"Global\"].get(\"infer_mode\", None) is False:\n                data_line = info.decode(\"utf-8\")\n                substr = data_line.strip(\"\\n\").split(\"\\t\")\n                img_path = os.path.join(data_dir, substr[0])\n                data = {\"img_path\": img_path, \"label\": substr[1]}\n            else:\n                img_path = info\n                data = {\"img_path\": img_path}\n\n            save_img_path = os.path.join(\n                config[\"Global\"][\"save_res_path\"],\n                os.path.splitext(os.path.basename(img_path))[0] + \"_ser.jpg\",\n            )\n\n            result, _ = ser_engine(data)\n            result = result[0]\n            fout.write(\n                img_path\n                + \"\\t\"\n                + json.dumps(\n                    {\n                        \"ocr_info\": result,\n                    },\n                    ensure_ascii=False,\n                )\n                + \"\\n\"\n            )\n            img_res = draw_ser_results(img_path, result)\n            cv2.imwrite(save_img_path, img_res)\n\n            logger.info(\n                \"process: [{}/{}], save result to {}\".format(\n                    idx, len(infer_imgs), save_img_path\n                )\n            )\n", "tools/infer_table.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nimport os\nimport sys\nimport json\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(__dir__)\nsys.path.insert(0, os.path.abspath(os.path.join(__dir__, \"..\")))\n\nos.environ[\"FLAGS_allocator_strategy\"] = \"auto_growth\"\n\nimport paddle\nfrom paddle.jit import to_static\n\nfrom ppocr.data import create_operators, transform\nfrom ppocr.modeling.architectures import build_model\nfrom ppocr.postprocess import build_post_process\nfrom ppocr.utils.save_load import load_model\nfrom ppocr.utils.utility import get_image_file_list\nfrom ppocr.utils.visual import draw_rectangle\nfrom tools.infer.utility import draw_boxes\nimport tools.program as program\nimport cv2\n\n\n@paddle.no_grad()\ndef main(config, device, logger, vdl_writer):\n    global_config = config[\"Global\"]\n\n    # build post process\n    post_process_class = build_post_process(config[\"PostProcess\"], global_config)\n\n    # build model\n    if hasattr(post_process_class, \"character\"):\n        config[\"Architecture\"][\"Head\"][\"out_channels\"] = len(\n            getattr(post_process_class, \"character\")\n        )\n\n    model = build_model(config[\"Architecture\"])\n    algorithm = config[\"Architecture\"][\"algorithm\"]\n\n    load_model(config, model)\n\n    # create data ops\n    transforms = []\n    for op in config[\"Eval\"][\"dataset\"][\"transforms\"]:\n        op_name = list(op)[0]\n        if \"Encode\" in op_name:\n            continue\n        if op_name == \"KeepKeys\":\n            op[op_name][\"keep_keys\"] = [\"image\", \"shape\"]\n        transforms.append(op)\n\n    global_config[\"infer_mode\"] = True\n    ops = create_operators(transforms, global_config)\n\n    save_res_path = config[\"Global\"][\"save_res_path\"]\n    os.makedirs(save_res_path, exist_ok=True)\n\n    model.eval()\n    with open(\n        os.path.join(save_res_path, \"infer.txt\"), mode=\"w\", encoding=\"utf-8\"\n    ) as f_w:\n        for file in get_image_file_list(config[\"Global\"][\"infer_img\"]):\n            logger.info(\"infer_img: {}\".format(file))\n            with open(file, \"rb\") as f:\n                img = f.read()\n                data = {\"image\": img}\n            batch = transform(data, ops)\n            images = np.expand_dims(batch[0], axis=0)\n            shape_list = np.expand_dims(batch[1], axis=0)\n\n            images = paddle.to_tensor(images)\n            preds = model(images)\n            post_result = post_process_class(preds, [shape_list])\n\n            structure_str_list = post_result[\"structure_batch_list\"][0]\n            bbox_list = post_result[\"bbox_batch_list\"][0]\n            structure_str_list = structure_str_list[0]\n            structure_str_list = (\n                [\"<html>\", \"<body>\", \"<table>\"]\n                + structure_str_list\n                + [\"</table>\", \"</body>\", \"</html>\"]\n            )\n            bbox_list_str = json.dumps(bbox_list.tolist())\n\n            logger.info(\"result: {}, {}\".format(structure_str_list, bbox_list_str))\n            f_w.write(\"result: {}, {}\\n\".format(structure_str_list, bbox_list_str))\n\n            if len(bbox_list) > 0 and len(bbox_list[0]) == 4:\n                img = draw_rectangle(file, bbox_list)\n            else:\n                img = draw_boxes(cv2.imread(file), bbox_list)\n            cv2.imwrite(os.path.join(save_res_path, os.path.basename(file)), img)\n            logger.info(\"save result to {}\".format(save_res_path))\n        logger.info(\"success!\")\n\n\nif __name__ == \"__main__\":\n    config, device, logger, vdl_writer = program.preprocess()\n    main(config, device, logger, vdl_writer)\n", "tools/export_center.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport pickle\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(__dir__)\nsys.path.append(os.path.abspath(os.path.join(__dir__, \"..\")))\n\nfrom ppocr.data import build_dataloader, set_signal_handlers\nfrom ppocr.modeling.architectures import build_model\nfrom ppocr.postprocess import build_post_process\nfrom ppocr.utils.save_load import load_model\nfrom ppocr.utils.utility import print_dict\nimport tools.program as program\n\n\ndef main():\n    global_config = config[\"Global\"]\n    # build dataloader\n    config[\"Eval\"][\"dataset\"][\"name\"] = config[\"Train\"][\"dataset\"][\"name\"]\n    config[\"Eval\"][\"dataset\"][\"data_dir\"] = config[\"Train\"][\"dataset\"][\"data_dir\"]\n    config[\"Eval\"][\"dataset\"][\"label_file_list\"] = config[\"Train\"][\"dataset\"][\n        \"label_file_list\"\n    ]\n    set_signal_handlers()\n    eval_dataloader = build_dataloader(config, \"Eval\", device, logger)\n\n    # build post process\n    post_process_class = build_post_process(config[\"PostProcess\"], global_config)\n\n    # build model\n    # for rec algorithm\n    if hasattr(post_process_class, \"character\"):\n        char_num = len(getattr(post_process_class, \"character\"))\n        config[\"Architecture\"][\"Head\"][\"out_channels\"] = char_num\n\n    # set return_features = True\n    config[\"Architecture\"][\"Head\"][\"return_feats\"] = True\n\n    model = build_model(config[\"Architecture\"])\n\n    best_model_dict = load_model(config, model)\n    if len(best_model_dict):\n        logger.info(\"metric in ckpt ***************\")\n        for k, v in best_model_dict.items():\n            logger.info(\"{}:{}\".format(k, v))\n\n    # get features from train data\n    char_center = program.get_center(model, eval_dataloader, post_process_class)\n\n    # serialize to disk\n    with open(\"train_center.pkl\", \"wb\") as f:\n        pickle.dump(char_center, f)\n    return\n\n\nif __name__ == \"__main__\":\n    config, device, logger, vdl_writer = program.preprocess()\n    main()\n", "tools/end2end/convert_ppocr_label.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport numpy as np\nimport json\nimport os\n\n\ndef poly_to_string(poly):\n    if len(poly.shape) > 1:\n        poly = np.array(poly).flatten()\n\n    string = \"\\t\".join(str(i) for i in poly)\n    return string\n\n\ndef convert_label(label_dir, mode=\"gt\", save_dir=\"./save_results/\"):\n    if not os.path.exists(label_dir):\n        raise ValueError(f\"The file {label_dir} does not exist!\")\n\n    assert label_dir != save_dir, \"hahahhaha\"\n\n    label_file = open(label_dir, \"r\")\n    data = label_file.readlines()\n\n    gt_dict = {}\n\n    for line in data:\n        try:\n            tmp = line.split(\"\\t\")\n            assert len(tmp) == 2, \"\"\n        except:\n            tmp = line.strip().split(\"    \")\n\n        gt_lists = []\n\n        if tmp[0].split(\"/\")[0] is not None:\n            img_path = tmp[0]\n            anno = json.loads(tmp[1])\n            gt_collect = []\n            for dic in anno:\n                # txt = dic['transcription'].replace(' ', '')  # ignore blank\n                txt = dic[\"transcription\"]\n                if \"score\" in dic and float(dic[\"score\"]) < 0.5:\n                    continue\n                if \"\\u3000\" in txt:\n                    txt = txt.replace(\"\\u3000\", \" \")\n                # while ' ' in txt:\n                #    txt = txt.replace(' ', '')\n                poly = np.array(dic[\"points\"]).flatten()\n                if txt == \"###\":\n                    txt_tag = 1  ## ignore 1\n                else:\n                    txt_tag = 0\n                if mode == \"gt\":\n                    gt_label = (\n                        poly_to_string(poly) + \"\\t\" + str(txt_tag) + \"\\t\" + txt + \"\\n\"\n                    )\n                else:\n                    gt_label = poly_to_string(poly) + \"\\t\" + txt + \"\\n\"\n\n                gt_lists.append(gt_label)\n\n            gt_dict[img_path] = gt_lists\n        else:\n            continue\n\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    for img_name in gt_dict.keys():\n        save_name = img_name.split(\"/\")[-1]\n        save_file = os.path.join(save_dir, save_name + \".txt\")\n        with open(save_file, \"w\") as f:\n            f.writelines(gt_dict[img_name])\n\n    print(\"The convert label saved in {}\".format(save_dir))\n\n\ndef parse_args():\n    import argparse\n\n    parser = argparse.ArgumentParser(description=\"args\")\n    parser.add_argument(\"--label_path\", type=str, required=True)\n    parser.add_argument(\"--save_folder\", type=str, required=True)\n    parser.add_argument(\"--mode\", type=str, default=False)\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    convert_label(args.label_path, args.mode, args.save_folder)\n", "tools/end2end/draw_html.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport argparse\n\n\ndef str2bool(v):\n    return v.lower() in (\"true\", \"t\", \"1\")\n\n\ndef init_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--image_dir\", type=str, default=\"\")\n    parser.add_argument(\"--save_html_path\", type=str, default=\"./default.html\")\n    parser.add_argument(\"--width\", type=int, default=640)\n    return parser\n\n\ndef parse_args():\n    parser = init_args()\n    return parser.parse_args()\n\n\ndef draw_debug_img(args):\n    html_path = args.save_html_path\n\n    err_cnt = 0\n    with open(html_path, \"w\") as html:\n        html.write(\"<html>\\n<body>\\n\")\n        html.write('<table border=\"1\">\\n')\n        html.write(\n            '<meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />'\n        )\n        image_list = []\n        path = args.image_dir\n        for i, filename in enumerate(sorted(os.listdir(path))):\n            if filename.endswith(\"txt\"):\n                continue\n            # The image path\n            base = \"{}/{}\".format(path, filename)\n            html.write(\"<tr>\\n\")\n            html.write(f\"<td> {filename}\\n GT\")\n            html.write(f'<td>GT\\n<img src=\"{base}\" width={args.width}></td>')\n\n            html.write(\"</tr>\\n\")\n        html.write(\"<style>\\n\")\n        html.write(\"span {\\n\")\n        html.write(\"    color: red;\\n\")\n        html.write(\"}\\n\")\n        html.write(\"</style>\\n\")\n        html.write(\"</table>\\n\")\n        html.write(\"</html>\\n</body>\\n\")\n    print(f\"The html file saved in {html_path}\")\n    return\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n\n    draw_debug_img(args)\n", "tools/end2end/eval_end2end.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport re\nimport sys\nimport shapely\nfrom shapely.geometry import Polygon\nimport numpy as np\nfrom collections import defaultdict\nimport operator\nimport editdistance\n\n\ndef strQ2B(ustring):\n    rstring = \"\"\n    for uchar in ustring:\n        inside_code = ord(uchar)\n        if inside_code == 12288:\n            inside_code = 32\n        elif inside_code >= 65281 and inside_code <= 65374:\n            inside_code -= 65248\n        rstring += chr(inside_code)\n    return rstring\n\n\ndef polygon_from_str(polygon_points):\n    \"\"\"\n    Create a shapely polygon object from gt or dt line.\n    \"\"\"\n    polygon_points = np.array(polygon_points).reshape(4, 2)\n    polygon = Polygon(polygon_points).convex_hull\n    return polygon\n\n\ndef polygon_iou(poly1, poly2):\n    \"\"\"\n    Intersection over union between two shapely polygons.\n    \"\"\"\n    if not poly1.intersects(poly2):  # this test is fast and can accelerate calculation\n        iou = 0\n    else:\n        try:\n            inter_area = poly1.intersection(poly2).area\n            union_area = poly1.area + poly2.area - inter_area\n            iou = float(inter_area) / union_area\n        except shapely.geos.TopologicalError:\n            # except Exception as e:\n            #     print(e)\n            print(\"shapely.geos.TopologicalError occurred, iou set to 0\")\n            iou = 0\n    return iou\n\n\ndef ed(str1, str2):\n    return editdistance.eval(str1, str2)\n\n\ndef e2e_eval(gt_dir, res_dir, ignore_blank=False):\n    print(\"start testing...\")\n    iou_thresh = 0.5\n    val_names = os.listdir(gt_dir)\n    num_gt_chars = 0\n    gt_count = 0\n    dt_count = 0\n    hit = 0\n    ed_sum = 0\n\n    for i, val_name in enumerate(val_names):\n        with open(os.path.join(gt_dir, val_name), encoding=\"utf-8\") as f:\n            gt_lines = [o.strip() for o in f.readlines()]\n        gts = []\n        ignore_masks = []\n        for line in gt_lines:\n            parts = line.strip().split(\"\\t\")\n            # ignore illegal data\n            if len(parts) < 9:\n                continue\n            assert len(parts) < 11\n            if len(parts) == 9:\n                gts.append(parts[:8] + [\"\"])\n            else:\n                gts.append(parts[:8] + [parts[-1]])\n\n            ignore_masks.append(parts[8])\n\n        val_path = os.path.join(res_dir, val_name)\n        if not os.path.exists(val_path):\n            dt_lines = []\n        else:\n            with open(val_path, encoding=\"utf-8\") as f:\n                dt_lines = [o.strip() for o in f.readlines()]\n        dts = []\n        for line in dt_lines:\n            # print(line)\n            parts = line.strip().split(\"\\t\")\n            assert len(parts) < 10, \"line error: {}\".format(line)\n            if len(parts) == 8:\n                dts.append(parts + [\"\"])\n            else:\n                dts.append(parts)\n\n        dt_match = [False] * len(dts)\n        gt_match = [False] * len(gts)\n        all_ious = defaultdict(tuple)\n        for index_gt, gt in enumerate(gts):\n            gt_coors = [float(gt_coor) for gt_coor in gt[0:8]]\n            gt_poly = polygon_from_str(gt_coors)\n            for index_dt, dt in enumerate(dts):\n                dt_coors = [float(dt_coor) for dt_coor in dt[0:8]]\n                dt_poly = polygon_from_str(dt_coors)\n                iou = polygon_iou(dt_poly, gt_poly)\n                if iou >= iou_thresh:\n                    all_ious[(index_gt, index_dt)] = iou\n        sorted_ious = sorted(all_ious.items(), key=operator.itemgetter(1), reverse=True)\n        sorted_gt_dt_pairs = [item[0] for item in sorted_ious]\n\n        # matched gt and dt\n        for gt_dt_pair in sorted_gt_dt_pairs:\n            index_gt, index_dt = gt_dt_pair\n            if gt_match[index_gt] == False and dt_match[index_dt] == False:\n                gt_match[index_gt] = True\n                dt_match[index_dt] = True\n                if ignore_blank:\n                    gt_str = strQ2B(gts[index_gt][8]).replace(\" \", \"\")\n                    dt_str = strQ2B(dts[index_dt][8]).replace(\" \", \"\")\n                else:\n                    gt_str = strQ2B(gts[index_gt][8])\n                    dt_str = strQ2B(dts[index_dt][8])\n                if ignore_masks[index_gt] == \"0\":\n                    ed_sum += ed(gt_str, dt_str)\n                    num_gt_chars += len(gt_str)\n                    if gt_str == dt_str:\n                        hit += 1\n                    gt_count += 1\n                    dt_count += 1\n\n        # unmatched dt\n        for tindex, dt_match_flag in enumerate(dt_match):\n            if dt_match_flag == False:\n                dt_str = dts[tindex][8]\n                gt_str = \"\"\n                ed_sum += ed(dt_str, gt_str)\n                dt_count += 1\n\n        # unmatched gt\n        for tindex, gt_match_flag in enumerate(gt_match):\n            if gt_match_flag == False and ignore_masks[tindex] == \"0\":\n                dt_str = \"\"\n                gt_str = gts[tindex][8]\n                ed_sum += ed(gt_str, dt_str)\n                num_gt_chars += len(gt_str)\n                gt_count += 1\n\n    eps = 1e-9\n    print(\"hit, dt_count, gt_count\", hit, dt_count, gt_count)\n    precision = hit / (dt_count + eps)\n    recall = hit / (gt_count + eps)\n    fmeasure = 2.0 * precision * recall / (precision + recall + eps)\n    avg_edit_dist_img = ed_sum / len(val_names)\n    avg_edit_dist_field = ed_sum / (gt_count + eps)\n    character_acc = 1 - ed_sum / (num_gt_chars + eps)\n\n    print(\"character_acc: %.2f\" % (character_acc * 100) + \"%\")\n    print(\"avg_edit_dist_field: %.2f\" % (avg_edit_dist_field))\n    print(\"avg_edit_dist_img: %.2f\" % (avg_edit_dist_img))\n    print(\"precision: %.2f\" % (precision * 100) + \"%\")\n    print(\"recall: %.2f\" % (recall * 100) + \"%\")\n    print(\"fmeasure: %.2f\" % (fmeasure * 100) + \"%\")\n\n\nif __name__ == \"__main__\":\n    # if len(sys.argv) != 3:\n    #     print(\"python3 ocr_e2e_eval.py gt_dir res_dir\")\n    #     exit(-1)\n    # gt_folder = sys.argv[1]\n    # pred_folder = sys.argv[2]\n    gt_folder = sys.argv[1]\n    pred_folder = sys.argv[2]\n    e2e_eval(gt_folder, pred_folder)\n", "tools/infer/predict_det.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\nimport sys\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(__dir__)\nsys.path.insert(0, os.path.abspath(os.path.join(__dir__, \"../..\")))\n\nos.environ[\"FLAGS_allocator_strategy\"] = \"auto_growth\"\n\nimport cv2\nimport numpy as np\nimport time\nimport sys\n\nimport tools.infer.utility as utility\nfrom ppocr.utils.logging import get_logger\nfrom ppocr.utils.utility import get_image_file_list, check_and_read\nfrom ppocr.data import create_operators, transform\nfrom ppocr.postprocess import build_post_process\nimport json\n\n\nclass TextDetector(object):\n    def __init__(self, args, logger=None):\n        if logger is None:\n            logger = get_logger()\n        self.args = args\n        self.det_algorithm = args.det_algorithm\n        self.use_onnx = args.use_onnx\n        pre_process_list = [\n            {\n                \"DetResizeForTest\": {\n                    \"limit_side_len\": args.det_limit_side_len,\n                    \"limit_type\": args.det_limit_type,\n                }\n            },\n            {\n                \"NormalizeImage\": {\n                    \"std\": [0.229, 0.224, 0.225],\n                    \"mean\": [0.485, 0.456, 0.406],\n                    \"scale\": \"1./255.\",\n                    \"order\": \"hwc\",\n                }\n            },\n            {\"ToCHWImage\": None},\n            {\"KeepKeys\": {\"keep_keys\": [\"image\", \"shape\"]}},\n        ]\n        postprocess_params = {}\n        if self.det_algorithm == \"DB\":\n            postprocess_params[\"name\"] = \"DBPostProcess\"\n            postprocess_params[\"thresh\"] = args.det_db_thresh\n            postprocess_params[\"box_thresh\"] = args.det_db_box_thresh\n            postprocess_params[\"max_candidates\"] = 1000\n            postprocess_params[\"unclip_ratio\"] = args.det_db_unclip_ratio\n            postprocess_params[\"use_dilation\"] = args.use_dilation\n            postprocess_params[\"score_mode\"] = args.det_db_score_mode\n            postprocess_params[\"box_type\"] = args.det_box_type\n        elif self.det_algorithm == \"DB++\":\n            postprocess_params[\"name\"] = \"DBPostProcess\"\n            postprocess_params[\"thresh\"] = args.det_db_thresh\n            postprocess_params[\"box_thresh\"] = args.det_db_box_thresh\n            postprocess_params[\"max_candidates\"] = 1000\n            postprocess_params[\"unclip_ratio\"] = args.det_db_unclip_ratio\n            postprocess_params[\"use_dilation\"] = args.use_dilation\n            postprocess_params[\"score_mode\"] = args.det_db_score_mode\n            postprocess_params[\"box_type\"] = args.det_box_type\n            pre_process_list[1] = {\n                \"NormalizeImage\": {\n                    \"std\": [1.0, 1.0, 1.0],\n                    \"mean\": [0.48109378172549, 0.45752457890196, 0.40787054090196],\n                    \"scale\": \"1./255.\",\n                    \"order\": \"hwc\",\n                }\n            }\n        elif self.det_algorithm == \"EAST\":\n            postprocess_params[\"name\"] = \"EASTPostProcess\"\n            postprocess_params[\"score_thresh\"] = args.det_east_score_thresh\n            postprocess_params[\"cover_thresh\"] = args.det_east_cover_thresh\n            postprocess_params[\"nms_thresh\"] = args.det_east_nms_thresh\n        elif self.det_algorithm == \"SAST\":\n            pre_process_list[0] = {\n                \"DetResizeForTest\": {\"resize_long\": args.det_limit_side_len}\n            }\n            postprocess_params[\"name\"] = \"SASTPostProcess\"\n            postprocess_params[\"score_thresh\"] = args.det_sast_score_thresh\n            postprocess_params[\"nms_thresh\"] = args.det_sast_nms_thresh\n\n            if args.det_box_type == \"poly\":\n                postprocess_params[\"sample_pts_num\"] = 6\n                postprocess_params[\"expand_scale\"] = 1.2\n                postprocess_params[\"shrink_ratio_of_width\"] = 0.2\n            else:\n                postprocess_params[\"sample_pts_num\"] = 2\n                postprocess_params[\"expand_scale\"] = 1.0\n                postprocess_params[\"shrink_ratio_of_width\"] = 0.3\n\n        elif self.det_algorithm == \"PSE\":\n            postprocess_params[\"name\"] = \"PSEPostProcess\"\n            postprocess_params[\"thresh\"] = args.det_pse_thresh\n            postprocess_params[\"box_thresh\"] = args.det_pse_box_thresh\n            postprocess_params[\"min_area\"] = args.det_pse_min_area\n            postprocess_params[\"box_type\"] = args.det_box_type\n            postprocess_params[\"scale\"] = args.det_pse_scale\n        elif self.det_algorithm == \"FCE\":\n            pre_process_list[0] = {\"DetResizeForTest\": {\"rescale_img\": [1080, 736]}}\n            postprocess_params[\"name\"] = \"FCEPostProcess\"\n            postprocess_params[\"scales\"] = args.scales\n            postprocess_params[\"alpha\"] = args.alpha\n            postprocess_params[\"beta\"] = args.beta\n            postprocess_params[\"fourier_degree\"] = args.fourier_degree\n            postprocess_params[\"box_type\"] = args.det_box_type\n        elif self.det_algorithm == \"CT\":\n            pre_process_list[0] = {\"ScaleAlignedShort\": {\"short_size\": 640}}\n            postprocess_params[\"name\"] = \"CTPostProcess\"\n        else:\n            logger.info(\"unknown det_algorithm:{}\".format(self.det_algorithm))\n            sys.exit(0)\n\n        self.preprocess_op = create_operators(pre_process_list)\n        self.postprocess_op = build_post_process(postprocess_params)\n        (\n            self.predictor,\n            self.input_tensor,\n            self.output_tensors,\n            self.config,\n        ) = utility.create_predictor(args, \"det\", logger)\n\n        if self.use_onnx:\n            img_h, img_w = self.input_tensor.shape[2:]\n            if isinstance(img_h, str) or isinstance(img_w, str):\n                pass\n            elif img_h is not None and img_w is not None and img_h > 0 and img_w > 0:\n                pre_process_list[0] = {\n                    \"DetResizeForTest\": {\"image_shape\": [img_h, img_w]}\n                }\n        self.preprocess_op = create_operators(pre_process_list)\n\n        if args.benchmark:\n            import auto_log\n\n            pid = os.getpid()\n            gpu_id = utility.get_infer_gpuid()\n            self.autolog = auto_log.AutoLogger(\n                model_name=\"det\",\n                model_precision=args.precision,\n                batch_size=1,\n                data_shape=\"dynamic\",\n                save_path=None,  # not used if logger is not None\n                inference_config=self.config,\n                pids=pid,\n                process_name=None,\n                gpu_ids=gpu_id if args.use_gpu else None,\n                time_keys=[\"preprocess_time\", \"inference_time\", \"postprocess_time\"],\n                warmup=2,\n                logger=logger,\n            )\n\n    def order_points_clockwise(self, pts):\n        rect = np.zeros((4, 2), dtype=\"float32\")\n        s = pts.sum(axis=1)\n        rect[0] = pts[np.argmin(s)]\n        rect[2] = pts[np.argmax(s)]\n        tmp = np.delete(pts, (np.argmin(s), np.argmax(s)), axis=0)\n        diff = np.diff(np.array(tmp), axis=1)\n        rect[1] = tmp[np.argmin(diff)]\n        rect[3] = tmp[np.argmax(diff)]\n        return rect\n\n    def pad_polygons(self, polygon, max_points):\n        padding_size = max_points - len(polygon)\n        if padding_size == 0:\n            return polygon\n        last_point = polygon[-1]\n        padding = np.repeat([last_point], padding_size, axis=0)\n        return np.vstack([polygon, padding])\n\n    def clip_det_res(self, points, img_height, img_width):\n        for pno in range(points.shape[0]):\n            points[pno, 0] = int(min(max(points[pno, 0], 0), img_width - 1))\n            points[pno, 1] = int(min(max(points[pno, 1], 0), img_height - 1))\n        return points\n\n    def filter_tag_det_res(self, dt_boxes, image_shape):\n        img_height, img_width = image_shape[0:2]\n        dt_boxes_new = []\n        for box in dt_boxes:\n            if type(box) is list:\n                box = np.array(box)\n            box = self.order_points_clockwise(box)\n            box = self.clip_det_res(box, img_height, img_width)\n            rect_width = int(np.linalg.norm(box[0] - box[1]))\n            rect_height = int(np.linalg.norm(box[0] - box[3]))\n            if rect_width <= 3 or rect_height <= 3:\n                continue\n            dt_boxes_new.append(box)\n        dt_boxes = np.array(dt_boxes_new)\n        return dt_boxes\n\n    def filter_tag_det_res_only_clip(self, dt_boxes, image_shape):\n        img_height, img_width = image_shape[0:2]\n        dt_boxes_new = []\n        for box in dt_boxes:\n            if type(box) is list:\n                box = np.array(box)\n            box = self.clip_det_res(box, img_height, img_width)\n            dt_boxes_new.append(box)\n\n        if len(dt_boxes_new) > 0:\n            max_points = max(len(polygon) for polygon in dt_boxes_new)\n            dt_boxes_new = [\n                self.pad_polygons(polygon, max_points) for polygon in dt_boxes_new\n            ]\n\n        dt_boxes = np.array(dt_boxes_new)\n        return dt_boxes\n\n    def predict(self, img):\n        ori_im = img.copy()\n        data = {\"image\": img}\n\n        st = time.time()\n\n        if self.args.benchmark:\n            self.autolog.times.start()\n\n        data = transform(data, self.preprocess_op)\n        img, shape_list = data\n        if img is None:\n            return None, 0\n        img = np.expand_dims(img, axis=0)\n        shape_list = np.expand_dims(shape_list, axis=0)\n        img = img.copy()\n\n        if self.args.benchmark:\n            self.autolog.times.stamp()\n        if self.use_onnx:\n            input_dict = {}\n            input_dict[self.input_tensor.name] = img\n            outputs = self.predictor.run(self.output_tensors, input_dict)\n        else:\n            self.input_tensor.copy_from_cpu(img)\n            self.predictor.run()\n            outputs = []\n            for output_tensor in self.output_tensors:\n                output = output_tensor.copy_to_cpu()\n                outputs.append(output)\n            if self.args.benchmark:\n                self.autolog.times.stamp()\n\n        preds = {}\n        if self.det_algorithm == \"EAST\":\n            preds[\"f_geo\"] = outputs[0]\n            preds[\"f_score\"] = outputs[1]\n        elif self.det_algorithm == \"SAST\":\n            preds[\"f_border\"] = outputs[0]\n            preds[\"f_score\"] = outputs[1]\n            preds[\"f_tco\"] = outputs[2]\n            preds[\"f_tvo\"] = outputs[3]\n        elif self.det_algorithm in [\"DB\", \"PSE\", \"DB++\"]:\n            preds[\"maps\"] = outputs[0]\n        elif self.det_algorithm == \"FCE\":\n            for i, output in enumerate(outputs):\n                preds[\"level_{}\".format(i)] = output\n        elif self.det_algorithm == \"CT\":\n            preds[\"maps\"] = outputs[0]\n            preds[\"score\"] = outputs[1]\n        else:\n            raise NotImplementedError\n\n        post_result = self.postprocess_op(preds, shape_list)\n        dt_boxes = post_result[0][\"points\"]\n\n        if self.args.det_box_type == \"poly\":\n            dt_boxes = self.filter_tag_det_res_only_clip(dt_boxes, ori_im.shape)\n        else:\n            dt_boxes = self.filter_tag_det_res(dt_boxes, ori_im.shape)\n\n        if self.args.benchmark:\n            self.autolog.times.end(stamp=True)\n        et = time.time()\n        return dt_boxes, et - st\n\n    def __call__(self, img):\n        # For image like poster with one side much greater than the other side,\n        # splitting recursively and processing with overlap to enhance performance.\n        MIN_BOUND_DISTANCE = 50\n        dt_boxes = np.zeros((0, 4, 2), dtype=np.float32)\n        elapse = 0\n        if (\n            img.shape[0] / img.shape[1] > 2\n            and img.shape[0] > self.args.det_limit_side_len\n        ):\n            start_h = 0\n            end_h = 0\n            while end_h <= img.shape[0]:\n                end_h = start_h + img.shape[1] * 3 // 4\n                subimg = img[start_h:end_h, :]\n                if len(subimg) == 0:\n                    break\n                sub_dt_boxes, sub_elapse = self.predict(subimg)\n                offset = start_h\n                # To prevent text blocks from being cut off, roll back a certain buffer area.\n                if (\n                    len(sub_dt_boxes) == 0\n                    or img.shape[1] - max([x[-1][1] for x in sub_dt_boxes])\n                    > MIN_BOUND_DISTANCE\n                ):\n                    start_h = end_h\n                else:\n                    sorted_indices = np.argsort(sub_dt_boxes[:, 2, 1])\n                    sub_dt_boxes = sub_dt_boxes[sorted_indices]\n                    bottom_line = (\n                        0\n                        if len(sub_dt_boxes) <= 1\n                        else int(np.max(sub_dt_boxes[:-1, 2, 1]))\n                    )\n                    if bottom_line > 0:\n                        start_h += bottom_line\n                        sub_dt_boxes = sub_dt_boxes[\n                            sub_dt_boxes[:, 2, 1] <= bottom_line\n                        ]\n                    else:\n                        start_h = end_h\n                if len(sub_dt_boxes) > 0:\n                    if dt_boxes.shape[0] == 0:\n                        dt_boxes = sub_dt_boxes + np.array(\n                            [0, offset], dtype=np.float32\n                        )\n                    else:\n                        dt_boxes = np.append(\n                            dt_boxes,\n                            sub_dt_boxes + np.array([0, offset], dtype=np.float32),\n                            axis=0,\n                        )\n                elapse += sub_elapse\n        elif (\n            img.shape[1] / img.shape[0] > 3\n            and img.shape[1] > self.args.det_limit_side_len * 3\n        ):\n            start_w = 0\n            end_w = 0\n            while end_w <= img.shape[1]:\n                end_w = start_w + img.shape[0] * 3 // 4\n                subimg = img[:, start_w:end_w]\n                if len(subimg) == 0:\n                    break\n                sub_dt_boxes, sub_elapse = self.predict(subimg)\n                offset = start_w\n                if (\n                    len(sub_dt_boxes) == 0\n                    or img.shape[0] - max([x[-1][0] for x in sub_dt_boxes])\n                    > MIN_BOUND_DISTANCE\n                ):\n                    start_w = end_w\n                else:\n                    sorted_indices = np.argsort(sub_dt_boxes[:, 2, 0])\n                    sub_dt_boxes = sub_dt_boxes[sorted_indices]\n                    right_line = (\n                        0\n                        if len(sub_dt_boxes) <= 1\n                        else int(np.max(sub_dt_boxes[:-1, 1, 0]))\n                    )\n                    if right_line > 0:\n                        start_w += right_line\n                        sub_dt_boxes = sub_dt_boxes[sub_dt_boxes[:, 1, 0] <= right_line]\n                    else:\n                        start_w = end_w\n                if len(sub_dt_boxes) > 0:\n                    if dt_boxes.shape[0] == 0:\n                        dt_boxes = sub_dt_boxes + np.array(\n                            [offset, 0], dtype=np.float32\n                        )\n                    else:\n                        dt_boxes = np.append(\n                            dt_boxes,\n                            sub_dt_boxes + np.array([offset, 0], dtype=np.float32),\n                            axis=0,\n                        )\n                elapse += sub_elapse\n        else:\n            dt_boxes, elapse = self.predict(img)\n        return dt_boxes, elapse\n\n\nif __name__ == \"__main__\":\n    args = utility.parse_args()\n    image_file_list = get_image_file_list(args.image_dir)\n    total_time = 0\n    draw_img_save_dir = args.draw_img_save_dir\n    os.makedirs(draw_img_save_dir, exist_ok=True)\n\n    # logger\n    log_file = args.save_log_path\n    if os.path.isdir(args.save_log_path) or (\n        not os.path.exists(args.save_log_path) and args.save_log_path.endswith(\"/\")\n    ):\n        log_file = os.path.join(log_file, \"benchmark_detection.log\")\n    logger = get_logger(log_file=log_file)\n\n    # create text detector\n    text_detector = TextDetector(args, logger)\n\n    if args.warmup:\n        img = np.random.uniform(0, 255, [640, 640, 3]).astype(np.uint8)\n        for i in range(2):\n            res = text_detector(img)\n\n    save_results = []\n    for idx, image_file in enumerate(image_file_list):\n        img, flag_gif, flag_pdf = check_and_read(image_file)\n        if not flag_gif and not flag_pdf:\n            img = cv2.imread(image_file)\n        if not flag_pdf:\n            if img is None:\n                logger.debug(\"error in loading image:{}\".format(image_file))\n                continue\n            imgs = [img]\n        else:\n            page_num = args.page_num\n            if page_num > len(img) or page_num == 0:\n                page_num = len(img)\n            imgs = img[:page_num]\n        for index, img in enumerate(imgs):\n            st = time.time()\n            dt_boxes, _ = text_detector(img)\n            elapse = time.time() - st\n            total_time += elapse\n            if len(imgs) > 1:\n                save_pred = (\n                    os.path.basename(image_file)\n                    + \"_\"\n                    + str(index)\n                    + \"\\t\"\n                    + str(json.dumps([x.tolist() for x in dt_boxes]))\n                    + \"\\n\"\n                )\n            else:\n                save_pred = (\n                    os.path.basename(image_file)\n                    + \"\\t\"\n                    + str(json.dumps([x.tolist() for x in dt_boxes]))\n                    + \"\\n\"\n                )\n            save_results.append(save_pred)\n            logger.info(save_pred)\n            if len(imgs) > 1:\n                logger.info(\n                    \"{}_{} The predict time of {}: {}\".format(\n                        idx, index, image_file, elapse\n                    )\n                )\n            else:\n                logger.info(\n                    \"{} The predict time of {}: {}\".format(idx, image_file, elapse)\n                )\n\n            src_im = utility.draw_text_det_res(dt_boxes, img)\n\n            if flag_gif:\n                save_file = image_file[:-3] + \"png\"\n            elif flag_pdf:\n                save_file = image_file.replace(\".pdf\", \"_\" + str(index) + \".png\")\n            else:\n                save_file = image_file\n            img_path = os.path.join(\n                draw_img_save_dir, \"det_res_{}\".format(os.path.basename(save_file))\n            )\n            cv2.imwrite(img_path, src_im)\n            logger.info(\"The visualized image saved in {}\".format(img_path))\n\n    with open(os.path.join(draw_img_save_dir, \"det_results.txt\"), \"w\") as f:\n        f.writelines(save_results)\n        f.close()\n    if args.benchmark:\n        text_detector.autolog.report()\n", "tools/infer/utility.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\nimport os\nimport sys\nimport platform\nimport cv2\nimport numpy as np\nimport paddle\nimport PIL\nfrom PIL import Image, ImageDraw, ImageFont\nimport math\nfrom paddle import inference\nimport time\nimport random\nfrom ppocr.utils.logging import get_logger\n\n\ndef str2bool(v):\n    return v.lower() in (\"true\", \"yes\", \"t\", \"y\", \"1\")\n\n\ndef str2int_tuple(v):\n    return tuple([int(i.strip()) for i in v.split(\",\")])\n\n\ndef init_args():\n    parser = argparse.ArgumentParser()\n    # params for prediction engine\n    parser.add_argument(\"--use_gpu\", type=str2bool, default=True)\n    parser.add_argument(\"--use_xpu\", type=str2bool, default=False)\n    parser.add_argument(\"--use_npu\", type=str2bool, default=False)\n    parser.add_argument(\"--use_mlu\", type=str2bool, default=False)\n    parser.add_argument(\"--ir_optim\", type=str2bool, default=True)\n    parser.add_argument(\"--use_tensorrt\", type=str2bool, default=False)\n    parser.add_argument(\"--min_subgraph_size\", type=int, default=15)\n    parser.add_argument(\"--precision\", type=str, default=\"fp32\")\n    parser.add_argument(\"--gpu_mem\", type=int, default=500)\n    parser.add_argument(\"--gpu_id\", type=int, default=0)\n\n    # params for text detector\n    parser.add_argument(\"--image_dir\", type=str)\n    parser.add_argument(\"--page_num\", type=int, default=0)\n    parser.add_argument(\"--det_algorithm\", type=str, default=\"DB\")\n    parser.add_argument(\"--det_model_dir\", type=str)\n    parser.add_argument(\"--det_limit_side_len\", type=float, default=960)\n    parser.add_argument(\"--det_limit_type\", type=str, default=\"max\")\n    parser.add_argument(\"--det_box_type\", type=str, default=\"quad\")\n\n    # DB parmas\n    parser.add_argument(\"--det_db_thresh\", type=float, default=0.3)\n    parser.add_argument(\"--det_db_box_thresh\", type=float, default=0.6)\n    parser.add_argument(\"--det_db_unclip_ratio\", type=float, default=1.5)\n    parser.add_argument(\"--max_batch_size\", type=int, default=10)\n    parser.add_argument(\"--use_dilation\", type=str2bool, default=False)\n    parser.add_argument(\"--det_db_score_mode\", type=str, default=\"fast\")\n\n    # EAST parmas\n    parser.add_argument(\"--det_east_score_thresh\", type=float, default=0.8)\n    parser.add_argument(\"--det_east_cover_thresh\", type=float, default=0.1)\n    parser.add_argument(\"--det_east_nms_thresh\", type=float, default=0.2)\n\n    # SAST parmas\n    parser.add_argument(\"--det_sast_score_thresh\", type=float, default=0.5)\n    parser.add_argument(\"--det_sast_nms_thresh\", type=float, default=0.2)\n\n    # PSE parmas\n    parser.add_argument(\"--det_pse_thresh\", type=float, default=0)\n    parser.add_argument(\"--det_pse_box_thresh\", type=float, default=0.85)\n    parser.add_argument(\"--det_pse_min_area\", type=float, default=16)\n    parser.add_argument(\"--det_pse_scale\", type=int, default=1)\n\n    # FCE parmas\n    parser.add_argument(\"--scales\", type=list, default=[8, 16, 32])\n    parser.add_argument(\"--alpha\", type=float, default=1.0)\n    parser.add_argument(\"--beta\", type=float, default=1.0)\n    parser.add_argument(\"--fourier_degree\", type=int, default=5)\n\n    # params for text recognizer\n    parser.add_argument(\"--rec_algorithm\", type=str, default=\"SVTR_LCNet\")\n    parser.add_argument(\"--rec_model_dir\", type=str)\n    parser.add_argument(\"--rec_image_inverse\", type=str2bool, default=True)\n    parser.add_argument(\"--rec_image_shape\", type=str, default=\"3, 48, 320\")\n    parser.add_argument(\"--rec_batch_num\", type=int, default=6)\n    parser.add_argument(\"--max_text_length\", type=int, default=25)\n    parser.add_argument(\n        \"--rec_char_dict_path\", type=str, default=\"./ppocr/utils/ppocr_keys_v1.txt\"\n    )\n    parser.add_argument(\"--use_space_char\", type=str2bool, default=True)\n    parser.add_argument(\"--vis_font_path\", type=str, default=\"./doc/fonts/simfang.ttf\")\n    parser.add_argument(\"--drop_score\", type=float, default=0.5)\n\n    # params for e2e\n    parser.add_argument(\"--e2e_algorithm\", type=str, default=\"PGNet\")\n    parser.add_argument(\"--e2e_model_dir\", type=str)\n    parser.add_argument(\"--e2e_limit_side_len\", type=float, default=768)\n    parser.add_argument(\"--e2e_limit_type\", type=str, default=\"max\")\n\n    # PGNet parmas\n    parser.add_argument(\"--e2e_pgnet_score_thresh\", type=float, default=0.5)\n    parser.add_argument(\n        \"--e2e_char_dict_path\", type=str, default=\"./ppocr/utils/ic15_dict.txt\"\n    )\n    parser.add_argument(\"--e2e_pgnet_valid_set\", type=str, default=\"totaltext\")\n    parser.add_argument(\"--e2e_pgnet_mode\", type=str, default=\"fast\")\n\n    # params for text classifier\n    parser.add_argument(\"--use_angle_cls\", type=str2bool, default=False)\n    parser.add_argument(\"--cls_model_dir\", type=str)\n    parser.add_argument(\"--cls_image_shape\", type=str, default=\"3, 48, 192\")\n    parser.add_argument(\"--label_list\", type=list, default=[\"0\", \"180\"])\n    parser.add_argument(\"--cls_batch_num\", type=int, default=6)\n    parser.add_argument(\"--cls_thresh\", type=float, default=0.9)\n\n    parser.add_argument(\"--enable_mkldnn\", type=str2bool, default=False)\n    parser.add_argument(\"--cpu_threads\", type=int, default=10)\n    parser.add_argument(\"--use_pdserving\", type=str2bool, default=False)\n    parser.add_argument(\"--warmup\", type=str2bool, default=False)\n\n    # SR parmas\n    parser.add_argument(\"--sr_model_dir\", type=str)\n    parser.add_argument(\"--sr_image_shape\", type=str, default=\"3, 32, 128\")\n    parser.add_argument(\"--sr_batch_num\", type=int, default=1)\n\n    #\n    parser.add_argument(\"--draw_img_save_dir\", type=str, default=\"./inference_results\")\n    parser.add_argument(\"--save_crop_res\", type=str2bool, default=False)\n    parser.add_argument(\"--crop_res_save_dir\", type=str, default=\"./output\")\n\n    # multi-process\n    parser.add_argument(\"--use_mp\", type=str2bool, default=False)\n    parser.add_argument(\"--total_process_num\", type=int, default=1)\n    parser.add_argument(\"--process_id\", type=int, default=0)\n\n    parser.add_argument(\"--benchmark\", type=str2bool, default=False)\n    parser.add_argument(\"--save_log_path\", type=str, default=\"./log_output/\")\n\n    parser.add_argument(\"--show_log\", type=str2bool, default=True)\n    parser.add_argument(\"--use_onnx\", type=str2bool, default=False)\n\n    # extended function\n    parser.add_argument(\n        \"--return_word_box\",\n        type=str2bool,\n        default=False,\n        help=\"Whether return the bbox of each word (split by space) or chinese character. Only used in ppstructure for layout recovery\",\n    )\n\n    return parser\n\n\ndef parse_args():\n    parser = init_args()\n    return parser.parse_args()\n\n\ndef create_predictor(args, mode, logger):\n    if mode == \"det\":\n        model_dir = args.det_model_dir\n    elif mode == \"cls\":\n        model_dir = args.cls_model_dir\n    elif mode == \"rec\":\n        model_dir = args.rec_model_dir\n    elif mode == \"table\":\n        model_dir = args.table_model_dir\n    elif mode == \"ser\":\n        model_dir = args.ser_model_dir\n    elif mode == \"re\":\n        model_dir = args.re_model_dir\n    elif mode == \"sr\":\n        model_dir = args.sr_model_dir\n    elif mode == \"layout\":\n        model_dir = args.layout_model_dir\n    else:\n        model_dir = args.e2e_model_dir\n\n    if model_dir is None:\n        logger.info(\"not find {} model file path {}\".format(mode, model_dir))\n        sys.exit(0)\n    if args.use_onnx:\n        import onnxruntime as ort\n\n        model_file_path = model_dir\n        if not os.path.exists(model_file_path):\n            raise ValueError(\"not find model file path {}\".format(model_file_path))\n        if args.use_gpu:\n            sess = ort.InferenceSession(\n                model_file_path,\n                providers=[\n                    (\n                        \"CUDAExecutionProvider\",\n                        {\"device_id\": args.gpu_id, \"cudnn_conv_algo_search\": \"DEFAULT\"},\n                    )\n                ],\n            )\n        else:\n            sess = ort.InferenceSession(\n                model_file_path, providers=[\"CPUExecutionProvider\"]\n            )\n        return sess, sess.get_inputs()[0], None, None\n\n    else:\n        file_names = [\"model\", \"inference\"]\n        for file_name in file_names:\n            model_file_path = \"{}/{}.pdmodel\".format(model_dir, file_name)\n            params_file_path = \"{}/{}.pdiparams\".format(model_dir, file_name)\n            if os.path.exists(model_file_path) and os.path.exists(params_file_path):\n                break\n        if not os.path.exists(model_file_path):\n            raise ValueError(\n                \"not find model.pdmodel or inference.pdmodel in {}\".format(model_dir)\n            )\n        if not os.path.exists(params_file_path):\n            raise ValueError(\n                \"not find model.pdiparams or inference.pdiparams in {}\".format(\n                    model_dir\n                )\n            )\n\n        config = inference.Config(model_file_path, params_file_path)\n\n        if hasattr(args, \"precision\"):\n            if args.precision == \"fp16\" and args.use_tensorrt:\n                precision = inference.PrecisionType.Half\n            elif args.precision == \"int8\":\n                precision = inference.PrecisionType.Int8\n            else:\n                precision = inference.PrecisionType.Float32\n        else:\n            precision = inference.PrecisionType.Float32\n\n        if args.use_gpu:\n            gpu_id = get_infer_gpuid()\n            if gpu_id is None:\n                logger.warning(\n                    \"GPU is not found in current device by nvidia-smi. Please check your device or ignore it if run on jetson.\"\n                )\n            config.enable_use_gpu(args.gpu_mem, args.gpu_id)\n            if args.use_tensorrt:\n                config.enable_tensorrt_engine(\n                    workspace_size=1 << 30,\n                    precision_mode=precision,\n                    max_batch_size=args.max_batch_size,\n                    min_subgraph_size=args.min_subgraph_size,  # skip the minmum trt subgraph\n                    use_calib_mode=False,\n                )\n\n                # collect shape\n                trt_shape_f = os.path.join(model_dir, f\"{mode}_trt_dynamic_shape.txt\")\n\n                if not os.path.exists(trt_shape_f):\n                    config.collect_shape_range_info(trt_shape_f)\n                    logger.info(f\"collect dynamic shape info into : {trt_shape_f}\")\n                try:\n                    config.enable_tuned_tensorrt_dynamic_shape(trt_shape_f, True)\n                except Exception as E:\n                    logger.info(E)\n                    logger.info(\"Please keep your paddlepaddle-gpu >= 2.3.0!\")\n\n        elif args.use_npu:\n            config.enable_custom_device(\"npu\")\n        elif args.use_mlu:\n            config.enable_custom_device(\"mlu\")\n        elif args.use_xpu:\n            config.enable_xpu(10 * 1024 * 1024)\n        else:\n            config.disable_gpu()\n            if args.enable_mkldnn:\n                # cache 10 different shapes for mkldnn to avoid memory leak\n                config.set_mkldnn_cache_capacity(10)\n                config.enable_mkldnn()\n                if args.precision == \"fp16\":\n                    config.enable_mkldnn_bfloat16()\n                if hasattr(args, \"cpu_threads\"):\n                    config.set_cpu_math_library_num_threads(args.cpu_threads)\n                else:\n                    # default cpu threads as 10\n                    config.set_cpu_math_library_num_threads(10)\n        # enable memory optim\n        config.enable_memory_optim()\n        config.disable_glog_info()\n        config.delete_pass(\"conv_transpose_eltwiseadd_bn_fuse_pass\")\n        config.delete_pass(\"matmul_transpose_reshape_fuse_pass\")\n        if mode == \"re\":\n            config.delete_pass(\"simplify_with_basic_ops_pass\")\n        if mode == \"table\":\n            config.delete_pass(\"fc_fuse_pass\")  # not supported for table\n        config.switch_use_feed_fetch_ops(False)\n        config.switch_ir_optim(True)\n\n        # create predictor\n        predictor = inference.create_predictor(config)\n        input_names = predictor.get_input_names()\n        if mode in [\"ser\", \"re\"]:\n            input_tensor = []\n            for name in input_names:\n                input_tensor.append(predictor.get_input_handle(name))\n        else:\n            for name in input_names:\n                input_tensor = predictor.get_input_handle(name)\n        output_tensors = get_output_tensors(args, mode, predictor)\n        return predictor, input_tensor, output_tensors, config\n\n\ndef get_output_tensors(args, mode, predictor):\n    output_names = predictor.get_output_names()\n    output_tensors = []\n    if mode == \"rec\" and args.rec_algorithm in [\"CRNN\", \"SVTR_LCNet\", \"SVTR_HGNet\"]:\n        output_name = \"softmax_0.tmp_0\"\n        if output_name in output_names:\n            return [predictor.get_output_handle(output_name)]\n        else:\n            for output_name in output_names:\n                output_tensor = predictor.get_output_handle(output_name)\n                output_tensors.append(output_tensor)\n    else:\n        for output_name in output_names:\n            output_tensor = predictor.get_output_handle(output_name)\n            output_tensors.append(output_tensor)\n    return output_tensors\n\n\ndef get_infer_gpuid():\n    sysstr = platform.system()\n    if sysstr == \"Windows\":\n        return 0\n\n    if not paddle.device.is_compiled_with_rocm:\n        cmd = \"env | grep CUDA_VISIBLE_DEVICES\"\n    else:\n        cmd = \"env | grep HIP_VISIBLE_DEVICES\"\n    env_cuda = os.popen(cmd).readlines()\n    if len(env_cuda) == 0:\n        return 0\n    else:\n        gpu_id = env_cuda[0].strip().split(\"=\")[1]\n        return int(gpu_id[0])\n\n\ndef draw_e2e_res(dt_boxes, strs, img_path):\n    src_im = cv2.imread(img_path)\n    for box, str in zip(dt_boxes, strs):\n        box = box.astype(np.int32).reshape((-1, 1, 2))\n        cv2.polylines(src_im, [box], True, color=(255, 255, 0), thickness=2)\n        cv2.putText(\n            src_im,\n            str,\n            org=(int(box[0, 0, 0]), int(box[0, 0, 1])),\n            fontFace=cv2.FONT_HERSHEY_COMPLEX,\n            fontScale=0.7,\n            color=(0, 255, 0),\n            thickness=1,\n        )\n    return src_im\n\n\ndef draw_text_det_res(dt_boxes, img):\n    for box in dt_boxes:\n        box = np.array(box).astype(np.int32).reshape(-1, 2)\n        cv2.polylines(img, [box], True, color=(255, 255, 0), thickness=2)\n    return img\n\n\ndef resize_img(img, input_size=600):\n    \"\"\"\n    resize img and limit the longest side of the image to input_size\n    \"\"\"\n    img = np.array(img)\n    im_shape = img.shape\n    im_size_max = np.max(im_shape[0:2])\n    im_scale = float(input_size) / float(im_size_max)\n    img = cv2.resize(img, None, None, fx=im_scale, fy=im_scale)\n    return img\n\n\ndef draw_ocr(\n    image,\n    boxes,\n    txts=None,\n    scores=None,\n    drop_score=0.5,\n    font_path=\"./doc/fonts/simfang.ttf\",\n):\n    \"\"\"\n    Visualize the results of OCR detection and recognition\n    args:\n        image(Image|array): RGB image\n        boxes(list): boxes with shape(N, 4, 2)\n        txts(list): the texts\n        scores(list): txxs corresponding scores\n        drop_score(float): only scores greater than drop_threshold will be visualized\n        font_path: the path of font which is used to draw text\n    return(array):\n        the visualized img\n    \"\"\"\n    if scores is None:\n        scores = [1] * len(boxes)\n    box_num = len(boxes)\n    for i in range(box_num):\n        if scores is not None and (scores[i] < drop_score or math.isnan(scores[i])):\n            continue\n        box = np.reshape(np.array(boxes[i]), [-1, 1, 2]).astype(np.int64)\n        image = cv2.polylines(np.array(image), [box], True, (255, 0, 0), 2)\n    if txts is not None:\n        img = np.array(resize_img(image, input_size=600))\n        txt_img = text_visual(\n            txts,\n            scores,\n            img_h=img.shape[0],\n            img_w=600,\n            threshold=drop_score,\n            font_path=font_path,\n        )\n        img = np.concatenate([np.array(img), np.array(txt_img)], axis=1)\n        return img\n    return image\n\n\ndef draw_ocr_box_txt(\n    image,\n    boxes,\n    txts=None,\n    scores=None,\n    drop_score=0.5,\n    font_path=\"./doc/fonts/simfang.ttf\",\n):\n    h, w = image.height, image.width\n    img_left = image.copy()\n    img_right = np.ones((h, w, 3), dtype=np.uint8) * 255\n    random.seed(0)\n\n    draw_left = ImageDraw.Draw(img_left)\n    if txts is None or len(txts) != len(boxes):\n        txts = [None] * len(boxes)\n    for idx, (box, txt) in enumerate(zip(boxes, txts)):\n        if scores is not None and scores[idx] < drop_score:\n            continue\n        color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n        draw_left.polygon(box, fill=color)\n        img_right_text = draw_box_txt_fine((w, h), box, txt, font_path)\n        pts = np.array(box, np.int32).reshape((-1, 1, 2))\n        cv2.polylines(img_right_text, [pts], True, color, 1)\n        img_right = cv2.bitwise_and(img_right, img_right_text)\n    img_left = Image.blend(image, img_left, 0.5)\n    img_show = Image.new(\"RGB\", (w * 2, h), (255, 255, 255))\n    img_show.paste(img_left, (0, 0, w, h))\n    img_show.paste(Image.fromarray(img_right), (w, 0, w * 2, h))\n    return np.array(img_show)\n\n\ndef draw_box_txt_fine(img_size, box, txt, font_path=\"./doc/fonts/simfang.ttf\"):\n    box_height = int(\n        math.sqrt((box[0][0] - box[3][0]) ** 2 + (box[0][1] - box[3][1]) ** 2)\n    )\n    box_width = int(\n        math.sqrt((box[0][0] - box[1][0]) ** 2 + (box[0][1] - box[1][1]) ** 2)\n    )\n\n    if box_height > 2 * box_width and box_height > 30:\n        img_text = Image.new(\"RGB\", (box_height, box_width), (255, 255, 255))\n        draw_text = ImageDraw.Draw(img_text)\n        if txt:\n            font = create_font(txt, (box_height, box_width), font_path)\n            draw_text.text([0, 0], txt, fill=(0, 0, 0), font=font)\n        img_text = img_text.transpose(Image.ROTATE_270)\n    else:\n        img_text = Image.new(\"RGB\", (box_width, box_height), (255, 255, 255))\n        draw_text = ImageDraw.Draw(img_text)\n        if txt:\n            font = create_font(txt, (box_width, box_height), font_path)\n            draw_text.text([0, 0], txt, fill=(0, 0, 0), font=font)\n\n    pts1 = np.float32(\n        [[0, 0], [box_width, 0], [box_width, box_height], [0, box_height]]\n    )\n    pts2 = np.array(box, dtype=np.float32)\n    M = cv2.getPerspectiveTransform(pts1, pts2)\n\n    img_text = np.array(img_text, dtype=np.uint8)\n    img_right_text = cv2.warpPerspective(\n        img_text,\n        M,\n        img_size,\n        flags=cv2.INTER_NEAREST,\n        borderMode=cv2.BORDER_CONSTANT,\n        borderValue=(255, 255, 255),\n    )\n    return img_right_text\n\n\ndef create_font(txt, sz, font_path=\"./doc/fonts/simfang.ttf\"):\n    font_size = int(sz[1] * 0.99)\n    font = ImageFont.truetype(font_path, font_size, encoding=\"utf-8\")\n    if int(PIL.__version__.split(\".\")[0]) < 10:\n        length = font.getsize(txt)[0]\n    else:\n        length = font.getlength(txt)\n\n    if length > sz[0]:\n        font_size = int(font_size * sz[0] / length)\n        font = ImageFont.truetype(font_path, font_size, encoding=\"utf-8\")\n    return font\n\n\ndef str_count(s):\n    \"\"\"\n    Count the number of Chinese characters,\n    a single English character and a single number\n    equal to half the length of Chinese characters.\n    args:\n        s(string): the input of string\n    return(int):\n        the number of Chinese characters\n    \"\"\"\n    import string\n\n    count_zh = count_pu = 0\n    s_len = len(s)\n    en_dg_count = 0\n    for c in s:\n        if c in string.ascii_letters or c.isdigit() or c.isspace():\n            en_dg_count += 1\n        elif c.isalpha():\n            count_zh += 1\n        else:\n            count_pu += 1\n    return s_len - math.ceil(en_dg_count / 2)\n\n\ndef text_visual(\n    texts, scores, img_h=400, img_w=600, threshold=0.0, font_path=\"./doc/simfang.ttf\"\n):\n    \"\"\"\n    create new blank img and draw txt on it\n    args:\n        texts(list): the text will be draw\n        scores(list|None): corresponding score of each txt\n        img_h(int): the height of blank img\n        img_w(int): the width of blank img\n        font_path: the path of font which is used to draw text\n    return(array):\n    \"\"\"\n    if scores is not None:\n        assert len(texts) == len(\n            scores\n        ), \"The number of txts and corresponding scores must match\"\n\n    def create_blank_img():\n        blank_img = np.ones(shape=[img_h, img_w], dtype=np.int8) * 255\n        blank_img[:, img_w - 1 :] = 0\n        blank_img = Image.fromarray(blank_img).convert(\"RGB\")\n        draw_txt = ImageDraw.Draw(blank_img)\n        return blank_img, draw_txt\n\n    blank_img, draw_txt = create_blank_img()\n\n    font_size = 20\n    txt_color = (0, 0, 0)\n    font = ImageFont.truetype(font_path, font_size, encoding=\"utf-8\")\n\n    gap = font_size + 5\n    txt_img_list = []\n    count, index = 1, 0\n    for idx, txt in enumerate(texts):\n        index += 1\n        if scores[idx] < threshold or math.isnan(scores[idx]):\n            index -= 1\n            continue\n        first_line = True\n        while str_count(txt) >= img_w // font_size - 4:\n            tmp = txt\n            txt = tmp[: img_w // font_size - 4]\n            if first_line:\n                new_txt = str(index) + \": \" + txt\n                first_line = False\n            else:\n                new_txt = \"    \" + txt\n            draw_txt.text((0, gap * count), new_txt, txt_color, font=font)\n            txt = tmp[img_w // font_size - 4 :]\n            if count >= img_h // gap - 1:\n                txt_img_list.append(np.array(blank_img))\n                blank_img, draw_txt = create_blank_img()\n                count = 0\n            count += 1\n        if first_line:\n            new_txt = str(index) + \": \" + txt + \"   \" + \"%.3f\" % (scores[idx])\n        else:\n            new_txt = \"  \" + txt + \"  \" + \"%.3f\" % (scores[idx])\n        draw_txt.text((0, gap * count), new_txt, txt_color, font=font)\n        # whether add new blank img or not\n        if count >= img_h // gap - 1 and idx + 1 < len(texts):\n            txt_img_list.append(np.array(blank_img))\n            blank_img, draw_txt = create_blank_img()\n            count = 0\n        count += 1\n    txt_img_list.append(np.array(blank_img))\n    if len(txt_img_list) == 1:\n        blank_img = np.array(txt_img_list[0])\n    else:\n        blank_img = np.concatenate(txt_img_list, axis=1)\n    return np.array(blank_img)\n\n\ndef base64_to_cv2(b64str):\n    import base64\n\n    data = base64.b64decode(b64str.encode(\"utf8\"))\n    data = np.frombuffer(data, np.uint8)\n    data = cv2.imdecode(data, cv2.IMREAD_COLOR)\n    return data\n\n\ndef draw_boxes(image, boxes, scores=None, drop_score=0.5):\n    if scores is None:\n        scores = [1] * len(boxes)\n    for box, score in zip(boxes, scores):\n        if score < drop_score:\n            continue\n        box = np.reshape(np.array(box), [-1, 1, 2]).astype(np.int64)\n        image = cv2.polylines(np.array(image), [box], True, (255, 0, 0), 2)\n    return image\n\n\ndef get_rotate_crop_image(img, points):\n    \"\"\"\n    img_height, img_width = img.shape[0:2]\n    left = int(np.min(points[:, 0]))\n    right = int(np.max(points[:, 0]))\n    top = int(np.min(points[:, 1]))\n    bottom = int(np.max(points[:, 1]))\n    img_crop = img[top:bottom, left:right, :].copy()\n    points[:, 0] = points[:, 0] - left\n    points[:, 1] = points[:, 1] - top\n    \"\"\"\n    assert len(points) == 4, \"shape of points must be 4*2\"\n    img_crop_width = int(\n        max(\n            np.linalg.norm(points[0] - points[1]), np.linalg.norm(points[2] - points[3])\n        )\n    )\n    img_crop_height = int(\n        max(\n            np.linalg.norm(points[0] - points[3]), np.linalg.norm(points[1] - points[2])\n        )\n    )\n    pts_std = np.float32(\n        [\n            [0, 0],\n            [img_crop_width, 0],\n            [img_crop_width, img_crop_height],\n            [0, img_crop_height],\n        ]\n    )\n    M = cv2.getPerspectiveTransform(points, pts_std)\n    dst_img = cv2.warpPerspective(\n        img,\n        M,\n        (img_crop_width, img_crop_height),\n        borderMode=cv2.BORDER_REPLICATE,\n        flags=cv2.INTER_CUBIC,\n    )\n    dst_img_height, dst_img_width = dst_img.shape[0:2]\n    if dst_img_height * 1.0 / dst_img_width >= 1.5:\n        dst_img = np.rot90(dst_img)\n    return dst_img\n\n\ndef get_minarea_rect_crop(img, points):\n    bounding_box = cv2.minAreaRect(np.array(points).astype(np.int32))\n    points = sorted(list(cv2.boxPoints(bounding_box)), key=lambda x: x[0])\n\n    index_a, index_b, index_c, index_d = 0, 1, 2, 3\n    if points[1][1] > points[0][1]:\n        index_a = 0\n        index_d = 1\n    else:\n        index_a = 1\n        index_d = 0\n    if points[3][1] > points[2][1]:\n        index_b = 2\n        index_c = 3\n    else:\n        index_b = 3\n        index_c = 2\n\n    box = [points[index_a], points[index_b], points[index_c], points[index_d]]\n    crop_img = get_rotate_crop_image(img, np.array(box))\n    return crop_img\n\n\ndef slice_generator(image, horizontal_stride, vertical_stride, maximum_slices=500):\n    if not isinstance(image, np.ndarray):\n        image = np.array(image)\n\n    image_h, image_w = image.shape[:2]\n    vertical_num_slices = (image_h + vertical_stride - 1) // vertical_stride\n    horizontal_num_slices = (image_w + horizontal_stride - 1) // horizontal_stride\n\n    assert (\n        vertical_num_slices > 0\n    ), f\"Invalid number ({vertical_num_slices}) of vertical slices\"\n\n    assert (\n        horizontal_num_slices > 0\n    ), f\"Invalid number ({horizontal_num_slices}) of horizontal slices\"\n\n    if vertical_num_slices >= maximum_slices:\n        recommended_vertical_stride = max(1, image_h // maximum_slices) + 1\n        assert (\n            False\n        ), f\"Too computationally expensive with {vertical_num_slices} slices, try a higher vertical stride (recommended minimum: {recommended_vertical_stride})\"\n\n    if horizontal_num_slices >= maximum_slices:\n        recommended_horizontal_stride = max(1, image_w // maximum_slices) + 1\n        assert (\n            False\n        ), f\"Too computationally expensive with {horizontal_num_slices} slices, try a higher horizontal stride (recommended minimum: {recommended_horizontal_stride})\"\n\n    for v_slice_idx in range(vertical_num_slices):\n        v_start = max(0, (v_slice_idx * vertical_stride))\n        v_end = min(((v_slice_idx + 1) * vertical_stride), image_h)\n        vertical_slice = image[v_start:v_end, :]\n        for h_slice_idx in range(horizontal_num_slices):\n            h_start = max(0, (h_slice_idx * horizontal_stride))\n            h_end = min(((h_slice_idx + 1) * horizontal_stride), image_w)\n            horizontal_slice = vertical_slice[:, h_start:h_end]\n\n            yield (horizontal_slice, v_start, h_start)\n\n\ndef calculate_box_extents(box):\n    min_x = box[0][0]\n    max_x = box[1][0]\n    min_y = box[0][1]\n    max_y = box[2][1]\n    return min_x, max_x, min_y, max_y\n\n\ndef merge_boxes(box1, box2, x_threshold, y_threshold):\n    min_x1, max_x1, min_y1, max_y1 = calculate_box_extents(box1)\n    min_x2, max_x2, min_y2, max_y2 = calculate_box_extents(box2)\n\n    if (\n        abs(min_y1 - min_y2) <= y_threshold\n        and abs(max_y1 - max_y2) <= y_threshold\n        and abs(max_x1 - min_x2) <= x_threshold\n    ):\n        new_xmin = min(min_x1, min_x2)\n        new_xmax = max(max_x1, max_x2)\n        new_ymin = min(min_y1, min_y2)\n        new_ymax = max(max_y1, max_y2)\n        return [\n            [new_xmin, new_ymin],\n            [new_xmax, new_ymin],\n            [new_xmax, new_ymax],\n            [new_xmin, new_ymax],\n        ]\n    else:\n        return None\n\n\ndef merge_fragmented(boxes, x_threshold=10, y_threshold=10):\n    merged_boxes = []\n    visited = set()\n\n    for i, box1 in enumerate(boxes):\n        if i in visited:\n            continue\n\n        merged_box = [point[:] for point in box1]\n\n        for j, box2 in enumerate(boxes[i + 1 :], start=i + 1):\n            if j not in visited:\n                merged_result = merge_boxes(\n                    merged_box, box2, x_threshold=x_threshold, y_threshold=y_threshold\n                )\n                if merged_result:\n                    merged_box = merged_result\n                    visited.add(j)\n\n        merged_boxes.append(merged_box)\n\n    if len(merged_boxes) == len(boxes):\n        return np.array(merged_boxes)\n    else:\n        return merge_fragmented(merged_boxes, x_threshold, y_threshold)\n\n\ndef check_gpu(use_gpu):\n    if use_gpu and (\n        not paddle.is_compiled_with_cuda() or paddle.device.get_device() == \"cpu\"\n    ):\n        use_gpu = False\n    return use_gpu\n\n\nif __name__ == \"__main__\":\n    pass\n", "tools/infer/predict_e2e.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\nimport sys\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(__dir__)\nsys.path.insert(0, os.path.abspath(os.path.join(__dir__, \"../..\")))\n\nos.environ[\"FLAGS_allocator_strategy\"] = \"auto_growth\"\n\nimport cv2\nimport numpy as np\nimport time\nimport sys\n\nimport tools.infer.utility as utility\nfrom ppocr.utils.logging import get_logger\nfrom ppocr.utils.utility import get_image_file_list, check_and_read\nfrom ppocr.data import create_operators, transform\nfrom ppocr.postprocess import build_post_process\n\nlogger = get_logger()\n\n\nclass TextE2E(object):\n    def __init__(self, args):\n        self.args = args\n        self.e2e_algorithm = args.e2e_algorithm\n        self.use_onnx = args.use_onnx\n        pre_process_list = [\n            {\"E2EResizeForTest\": {}},\n            {\n                \"NormalizeImage\": {\n                    \"std\": [0.229, 0.224, 0.225],\n                    \"mean\": [0.485, 0.456, 0.406],\n                    \"scale\": \"1./255.\",\n                    \"order\": \"hwc\",\n                }\n            },\n            {\"ToCHWImage\": None},\n            {\"KeepKeys\": {\"keep_keys\": [\"image\", \"shape\"]}},\n        ]\n        postprocess_params = {}\n        if self.e2e_algorithm == \"PGNet\":\n            pre_process_list[0] = {\n                \"E2EResizeForTest\": {\n                    \"max_side_len\": args.e2e_limit_side_len,\n                    \"valid_set\": \"totaltext\",\n                }\n            }\n            postprocess_params[\"name\"] = \"PGPostProcess\"\n            postprocess_params[\"score_thresh\"] = args.e2e_pgnet_score_thresh\n            postprocess_params[\"character_dict_path\"] = args.e2e_char_dict_path\n            postprocess_params[\"valid_set\"] = args.e2e_pgnet_valid_set\n            postprocess_params[\"mode\"] = args.e2e_pgnet_mode\n        else:\n            logger.info(\"unknown e2e_algorithm:{}\".format(self.e2e_algorithm))\n            sys.exit(0)\n\n        self.preprocess_op = create_operators(pre_process_list)\n        self.postprocess_op = build_post_process(postprocess_params)\n        (\n            self.predictor,\n            self.input_tensor,\n            self.output_tensors,\n            _,\n        ) = utility.create_predictor(\n            args, \"e2e\", logger\n        )  # paddle.jit.load(args.det_model_dir)\n        # self.predictor.eval()\n\n    def clip_det_res(self, points, img_height, img_width):\n        for pno in range(points.shape[0]):\n            points[pno, 0] = int(min(max(points[pno, 0], 0), img_width - 1))\n            points[pno, 1] = int(min(max(points[pno, 1], 0), img_height - 1))\n        return points\n\n    def filter_tag_det_res_only_clip(self, dt_boxes, image_shape):\n        img_height, img_width = image_shape[0:2]\n        dt_boxes_new = []\n        for box in dt_boxes:\n            box = self.clip_det_res(box, img_height, img_width)\n            dt_boxes_new.append(box)\n        dt_boxes = np.array(dt_boxes_new)\n        return dt_boxes\n\n    def __call__(self, img):\n        ori_im = img.copy()\n        data = {\"image\": img}\n        data = transform(data, self.preprocess_op)\n        img, shape_list = data\n        if img is None:\n            return None, 0\n        img = np.expand_dims(img, axis=0)\n        shape_list = np.expand_dims(shape_list, axis=0)\n        img = img.copy()\n        starttime = time.time()\n\n        if self.use_onnx:\n            input_dict = {}\n            input_dict[self.input_tensor.name] = img\n            outputs = self.predictor.run(self.output_tensors, input_dict)\n            preds = {}\n            preds[\"f_border\"] = outputs[0]\n            preds[\"f_char\"] = outputs[1]\n            preds[\"f_direction\"] = outputs[2]\n            preds[\"f_score\"] = outputs[3]\n        else:\n            self.input_tensor.copy_from_cpu(img)\n            self.predictor.run()\n            outputs = []\n            for output_tensor in self.output_tensors:\n                output = output_tensor.copy_to_cpu()\n                outputs.append(output)\n\n            preds = {}\n            if self.e2e_algorithm == \"PGNet\":\n                preds[\"f_border\"] = outputs[0]\n                preds[\"f_char\"] = outputs[1]\n                preds[\"f_direction\"] = outputs[2]\n                preds[\"f_score\"] = outputs[3]\n            else:\n                raise NotImplementedError\n        post_result = self.postprocess_op(preds, shape_list)\n        points, strs = post_result[\"points\"], post_result[\"texts\"]\n        dt_boxes = self.filter_tag_det_res_only_clip(points, ori_im.shape)\n        elapse = time.time() - starttime\n        return dt_boxes, strs, elapse\n\n\nif __name__ == \"__main__\":\n    args = utility.parse_args()\n    image_file_list = get_image_file_list(args.image_dir)\n    text_detector = TextE2E(args)\n    count = 0\n    total_time = 0\n    draw_img_save = \"./inference_results\"\n    if not os.path.exists(draw_img_save):\n        os.makedirs(draw_img_save)\n    for image_file in image_file_list:\n        img, flag, _ = check_and_read(image_file)\n        if not flag:\n            img = cv2.imread(image_file)\n        if img is None:\n            logger.info(\"error in loading image:{}\".format(image_file))\n            continue\n        points, strs, elapse = text_detector(img)\n        if count > 0:\n            total_time += elapse\n        count += 1\n        logger.info(\"Predict time of {}: {}\".format(image_file, elapse))\n        src_im = utility.draw_e2e_res(points, strs, image_file)\n        img_name_pure = os.path.split(image_file)[-1]\n        img_path = os.path.join(draw_img_save, \"e2e_res_{}\".format(img_name_pure))\n        cv2.imwrite(img_path, src_im)\n        logger.info(\"The visualized image saved in {}\".format(img_path))\n    if count > 1:\n        logger.info(\"Avg Time: {}\".format(total_time / (count - 1)))\n", "tools/infer/predict_rec.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\nimport sys\nfrom PIL import Image\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(__dir__)\nsys.path.insert(0, os.path.abspath(os.path.join(__dir__, \"../..\")))\n\nos.environ[\"FLAGS_allocator_strategy\"] = \"auto_growth\"\n\nimport cv2\nimport numpy as np\nimport math\nimport time\nimport traceback\nimport paddle\n\nimport tools.infer.utility as utility\nfrom ppocr.postprocess import build_post_process\nfrom ppocr.utils.logging import get_logger\nfrom ppocr.utils.utility import get_image_file_list, check_and_read\n\nlogger = get_logger()\n\n\nclass TextRecognizer(object):\n    def __init__(self, args, logger=None):\n        if logger is None:\n            logger = get_logger()\n        self.rec_image_shape = [int(v) for v in args.rec_image_shape.split(\",\")]\n        self.rec_batch_num = args.rec_batch_num\n        self.rec_algorithm = args.rec_algorithm\n        postprocess_params = {\n            \"name\": \"CTCLabelDecode\",\n            \"character_dict_path\": args.rec_char_dict_path,\n            \"use_space_char\": args.use_space_char,\n        }\n        if self.rec_algorithm == \"SRN\":\n            postprocess_params = {\n                \"name\": \"SRNLabelDecode\",\n                \"character_dict_path\": args.rec_char_dict_path,\n                \"use_space_char\": args.use_space_char,\n            }\n        elif self.rec_algorithm == \"RARE\":\n            postprocess_params = {\n                \"name\": \"AttnLabelDecode\",\n                \"character_dict_path\": args.rec_char_dict_path,\n                \"use_space_char\": args.use_space_char,\n            }\n        elif self.rec_algorithm == \"NRTR\":\n            postprocess_params = {\n                \"name\": \"NRTRLabelDecode\",\n                \"character_dict_path\": args.rec_char_dict_path,\n                \"use_space_char\": args.use_space_char,\n            }\n        elif self.rec_algorithm == \"SAR\":\n            postprocess_params = {\n                \"name\": \"SARLabelDecode\",\n                \"character_dict_path\": args.rec_char_dict_path,\n                \"use_space_char\": args.use_space_char,\n            }\n        elif self.rec_algorithm == \"VisionLAN\":\n            postprocess_params = {\n                \"name\": \"VLLabelDecode\",\n                \"character_dict_path\": args.rec_char_dict_path,\n                \"use_space_char\": args.use_space_char,\n                \"max_text_length\": args.max_text_length,\n            }\n        elif self.rec_algorithm == \"ViTSTR\":\n            postprocess_params = {\n                \"name\": \"ViTSTRLabelDecode\",\n                \"character_dict_path\": args.rec_char_dict_path,\n                \"use_space_char\": args.use_space_char,\n            }\n        elif self.rec_algorithm == \"ABINet\":\n            postprocess_params = {\n                \"name\": \"ABINetLabelDecode\",\n                \"character_dict_path\": args.rec_char_dict_path,\n                \"use_space_char\": args.use_space_char,\n            }\n        elif self.rec_algorithm == \"SPIN\":\n            postprocess_params = {\n                \"name\": \"SPINLabelDecode\",\n                \"character_dict_path\": args.rec_char_dict_path,\n                \"use_space_char\": args.use_space_char,\n            }\n        elif self.rec_algorithm == \"RobustScanner\":\n            postprocess_params = {\n                \"name\": \"SARLabelDecode\",\n                \"character_dict_path\": args.rec_char_dict_path,\n                \"use_space_char\": args.use_space_char,\n                \"rm_symbol\": True,\n            }\n        elif self.rec_algorithm == \"RFL\":\n            postprocess_params = {\n                \"name\": \"RFLLabelDecode\",\n                \"character_dict_path\": None,\n                \"use_space_char\": args.use_space_char,\n            }\n        elif self.rec_algorithm == \"SATRN\":\n            postprocess_params = {\n                \"name\": \"SATRNLabelDecode\",\n                \"character_dict_path\": args.rec_char_dict_path,\n                \"use_space_char\": args.use_space_char,\n                \"rm_symbol\": True,\n            }\n        elif self.rec_algorithm in [\"CPPD\", \"CPPDPadding\"]:\n            postprocess_params = {\n                \"name\": \"CPPDLabelDecode\",\n                \"character_dict_path\": args.rec_char_dict_path,\n                \"use_space_char\": args.use_space_char,\n                \"rm_symbol\": True,\n            }\n        elif self.rec_algorithm == \"PREN\":\n            postprocess_params = {\"name\": \"PRENLabelDecode\"}\n        elif self.rec_algorithm == \"CAN\":\n            self.inverse = args.rec_image_inverse\n            postprocess_params = {\n                \"name\": \"CANLabelDecode\",\n                \"character_dict_path\": args.rec_char_dict_path,\n                \"use_space_char\": args.use_space_char,\n            }\n        elif self.rec_algorithm == \"ParseQ\":\n            postprocess_params = {\n                \"name\": \"ParseQLabelDecode\",\n                \"character_dict_path\": args.rec_char_dict_path,\n                \"use_space_char\": args.use_space_char,\n            }\n        self.postprocess_op = build_post_process(postprocess_params)\n        self.postprocess_params = postprocess_params\n        (\n            self.predictor,\n            self.input_tensor,\n            self.output_tensors,\n            self.config,\n        ) = utility.create_predictor(args, \"rec\", logger)\n        self.benchmark = args.benchmark\n        self.use_onnx = args.use_onnx\n        if args.benchmark:\n            import auto_log\n\n            pid = os.getpid()\n            gpu_id = utility.get_infer_gpuid()\n            self.autolog = auto_log.AutoLogger(\n                model_name=\"rec\",\n                model_precision=args.precision,\n                batch_size=args.rec_batch_num,\n                data_shape=\"dynamic\",\n                save_path=None,  # not used if logger is not None\n                inference_config=self.config,\n                pids=pid,\n                process_name=None,\n                gpu_ids=gpu_id if args.use_gpu else None,\n                time_keys=[\"preprocess_time\", \"inference_time\", \"postprocess_time\"],\n                warmup=0,\n                logger=logger,\n            )\n        self.return_word_box = args.return_word_box\n\n    def resize_norm_img(self, img, max_wh_ratio):\n        imgC, imgH, imgW = self.rec_image_shape\n        if self.rec_algorithm == \"NRTR\" or self.rec_algorithm == \"ViTSTR\":\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n            # return padding_im\n            image_pil = Image.fromarray(np.uint8(img))\n            if self.rec_algorithm == \"ViTSTR\":\n                img = image_pil.resize([imgW, imgH], Image.BICUBIC)\n            else:\n                img = image_pil.resize([imgW, imgH], Image.Resampling.LANCZOS)\n            img = np.array(img)\n            norm_img = np.expand_dims(img, -1)\n            norm_img = norm_img.transpose((2, 0, 1))\n            if self.rec_algorithm == \"ViTSTR\":\n                norm_img = norm_img.astype(np.float32) / 255.0\n            else:\n                norm_img = norm_img.astype(np.float32) / 128.0 - 1.0\n            return norm_img\n        elif self.rec_algorithm == \"RFL\":\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n            resized_image = cv2.resize(img, (imgW, imgH), interpolation=cv2.INTER_CUBIC)\n            resized_image = resized_image.astype(\"float32\")\n            resized_image = resized_image / 255\n            resized_image = resized_image[np.newaxis, :]\n            resized_image -= 0.5\n            resized_image /= 0.5\n            return resized_image\n\n        assert imgC == img.shape[2]\n        imgW = int((imgH * max_wh_ratio))\n        if self.use_onnx:\n            w = self.input_tensor.shape[3:][0]\n            if isinstance(w, str):\n                pass\n            elif w is not None and w > 0:\n                imgW = w\n        h, w = img.shape[:2]\n        ratio = w / float(h)\n        if math.ceil(imgH * ratio) > imgW:\n            resized_w = imgW\n        else:\n            resized_w = int(math.ceil(imgH * ratio))\n        if self.rec_algorithm == \"RARE\":\n            if resized_w > self.rec_image_shape[2]:\n                resized_w = self.rec_image_shape[2]\n            imgW = self.rec_image_shape[2]\n        resized_image = cv2.resize(img, (resized_w, imgH))\n        resized_image = resized_image.astype(\"float32\")\n        resized_image = resized_image.transpose((2, 0, 1)) / 255\n        resized_image -= 0.5\n        resized_image /= 0.5\n        padding_im = np.zeros((imgC, imgH, imgW), dtype=np.float32)\n        padding_im[:, :, 0:resized_w] = resized_image\n        return padding_im\n\n    def resize_norm_img_vl(self, img, image_shape):\n        imgC, imgH, imgW = image_shape\n        img = img[:, :, ::-1]  # bgr2rgb\n        resized_image = cv2.resize(img, (imgW, imgH), interpolation=cv2.INTER_LINEAR)\n        resized_image = resized_image.astype(\"float32\")\n        resized_image = resized_image.transpose((2, 0, 1)) / 255\n        return resized_image\n\n    def resize_norm_img_srn(self, img, image_shape):\n        imgC, imgH, imgW = image_shape\n\n        img_black = np.zeros((imgH, imgW))\n        im_hei = img.shape[0]\n        im_wid = img.shape[1]\n\n        if im_wid <= im_hei * 1:\n            img_new = cv2.resize(img, (imgH * 1, imgH))\n        elif im_wid <= im_hei * 2:\n            img_new = cv2.resize(img, (imgH * 2, imgH))\n        elif im_wid <= im_hei * 3:\n            img_new = cv2.resize(img, (imgH * 3, imgH))\n        else:\n            img_new = cv2.resize(img, (imgW, imgH))\n\n        img_np = np.asarray(img_new)\n        img_np = cv2.cvtColor(img_np, cv2.COLOR_BGR2GRAY)\n        img_black[:, 0 : img_np.shape[1]] = img_np\n        img_black = img_black[:, :, np.newaxis]\n\n        row, col, c = img_black.shape\n        c = 1\n\n        return np.reshape(img_black, (c, row, col)).astype(np.float32)\n\n    def srn_other_inputs(self, image_shape, num_heads, max_text_length):\n        imgC, imgH, imgW = image_shape\n        feature_dim = int((imgH / 8) * (imgW / 8))\n\n        encoder_word_pos = (\n            np.array(range(0, feature_dim)).reshape((feature_dim, 1)).astype(\"int64\")\n        )\n        gsrm_word_pos = (\n            np.array(range(0, max_text_length))\n            .reshape((max_text_length, 1))\n            .astype(\"int64\")\n        )\n\n        gsrm_attn_bias_data = np.ones((1, max_text_length, max_text_length))\n        gsrm_slf_attn_bias1 = np.triu(gsrm_attn_bias_data, 1).reshape(\n            [-1, 1, max_text_length, max_text_length]\n        )\n        gsrm_slf_attn_bias1 = np.tile(gsrm_slf_attn_bias1, [1, num_heads, 1, 1]).astype(\n            \"float32\"\n        ) * [-1e9]\n\n        gsrm_slf_attn_bias2 = np.tril(gsrm_attn_bias_data, -1).reshape(\n            [-1, 1, max_text_length, max_text_length]\n        )\n        gsrm_slf_attn_bias2 = np.tile(gsrm_slf_attn_bias2, [1, num_heads, 1, 1]).astype(\n            \"float32\"\n        ) * [-1e9]\n\n        encoder_word_pos = encoder_word_pos[np.newaxis, :]\n        gsrm_word_pos = gsrm_word_pos[np.newaxis, :]\n\n        return [\n            encoder_word_pos,\n            gsrm_word_pos,\n            gsrm_slf_attn_bias1,\n            gsrm_slf_attn_bias2,\n        ]\n\n    def process_image_srn(self, img, image_shape, num_heads, max_text_length):\n        norm_img = self.resize_norm_img_srn(img, image_shape)\n        norm_img = norm_img[np.newaxis, :]\n\n        [\n            encoder_word_pos,\n            gsrm_word_pos,\n            gsrm_slf_attn_bias1,\n            gsrm_slf_attn_bias2,\n        ] = self.srn_other_inputs(image_shape, num_heads, max_text_length)\n\n        gsrm_slf_attn_bias1 = gsrm_slf_attn_bias1.astype(np.float32)\n        gsrm_slf_attn_bias2 = gsrm_slf_attn_bias2.astype(np.float32)\n        encoder_word_pos = encoder_word_pos.astype(np.int64)\n        gsrm_word_pos = gsrm_word_pos.astype(np.int64)\n\n        return (\n            norm_img,\n            encoder_word_pos,\n            gsrm_word_pos,\n            gsrm_slf_attn_bias1,\n            gsrm_slf_attn_bias2,\n        )\n\n    def resize_norm_img_sar(self, img, image_shape, width_downsample_ratio=0.25):\n        imgC, imgH, imgW_min, imgW_max = image_shape\n        h = img.shape[0]\n        w = img.shape[1]\n        valid_ratio = 1.0\n        # make sure new_width is an integral multiple of width_divisor.\n        width_divisor = int(1 / width_downsample_ratio)\n        # resize\n        ratio = w / float(h)\n        resize_w = math.ceil(imgH * ratio)\n        if resize_w % width_divisor != 0:\n            resize_w = round(resize_w / width_divisor) * width_divisor\n        if imgW_min is not None:\n            resize_w = max(imgW_min, resize_w)\n        if imgW_max is not None:\n            valid_ratio = min(1.0, 1.0 * resize_w / imgW_max)\n            resize_w = min(imgW_max, resize_w)\n        resized_image = cv2.resize(img, (resize_w, imgH))\n        resized_image = resized_image.astype(\"float32\")\n        # norm\n        if image_shape[0] == 1:\n            resized_image = resized_image / 255\n            resized_image = resized_image[np.newaxis, :]\n        else:\n            resized_image = resized_image.transpose((2, 0, 1)) / 255\n        resized_image -= 0.5\n        resized_image /= 0.5\n        resize_shape = resized_image.shape\n        padding_im = -1.0 * np.ones((imgC, imgH, imgW_max), dtype=np.float32)\n        padding_im[:, :, 0:resize_w] = resized_image\n        pad_shape = padding_im.shape\n\n        return padding_im, resize_shape, pad_shape, valid_ratio\n\n    def resize_norm_img_spin(self, img):\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        # return padding_im\n        img = cv2.resize(img, tuple([100, 32]), cv2.INTER_CUBIC)\n        img = np.array(img, np.float32)\n        img = np.expand_dims(img, -1)\n        img = img.transpose((2, 0, 1))\n        mean = [127.5]\n        std = [127.5]\n        mean = np.array(mean, dtype=np.float32)\n        std = np.array(std, dtype=np.float32)\n        mean = np.float32(mean.reshape(1, -1))\n        stdinv = 1 / np.float32(std.reshape(1, -1))\n        img -= mean\n        img *= stdinv\n        return img\n\n    def resize_norm_img_svtr(self, img, image_shape):\n        imgC, imgH, imgW = image_shape\n        resized_image = cv2.resize(img, (imgW, imgH), interpolation=cv2.INTER_LINEAR)\n        resized_image = resized_image.astype(\"float32\")\n        resized_image = resized_image.transpose((2, 0, 1)) / 255\n        resized_image -= 0.5\n        resized_image /= 0.5\n        return resized_image\n\n    def resize_norm_img_cppd_padding(\n        self, img, image_shape, padding=True, interpolation=cv2.INTER_LINEAR\n    ):\n        imgC, imgH, imgW = image_shape\n        h = img.shape[0]\n        w = img.shape[1]\n        if not padding:\n            resized_image = cv2.resize(img, (imgW, imgH), interpolation=interpolation)\n            resized_w = imgW\n        else:\n            ratio = w / float(h)\n            if math.ceil(imgH * ratio) > imgW:\n                resized_w = imgW\n            else:\n                resized_w = int(math.ceil(imgH * ratio))\n            resized_image = cv2.resize(img, (resized_w, imgH))\n        resized_image = resized_image.astype(\"float32\")\n        if image_shape[0] == 1:\n            resized_image = resized_image / 255\n            resized_image = resized_image[np.newaxis, :]\n        else:\n            resized_image = resized_image.transpose((2, 0, 1)) / 255\n        resized_image -= 0.5\n        resized_image /= 0.5\n        padding_im = np.zeros((imgC, imgH, imgW), dtype=np.float32)\n        padding_im[:, :, 0:resized_w] = resized_image\n\n        return padding_im\n\n    def resize_norm_img_abinet(self, img, image_shape):\n        imgC, imgH, imgW = image_shape\n\n        resized_image = cv2.resize(img, (imgW, imgH), interpolation=cv2.INTER_LINEAR)\n        resized_image = resized_image.astype(\"float32\")\n        resized_image = resized_image / 255.0\n\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225])\n        resized_image = (resized_image - mean[None, None, ...]) / std[None, None, ...]\n        resized_image = resized_image.transpose((2, 0, 1))\n        resized_image = resized_image.astype(\"float32\")\n\n        return resized_image\n\n    def norm_img_can(self, img, image_shape):\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # CAN only predict gray scale image\n\n        if self.inverse:\n            img = 255 - img\n\n        if self.rec_image_shape[0] == 1:\n            h, w = img.shape\n            _, imgH, imgW = self.rec_image_shape\n            if h < imgH or w < imgW:\n                padding_h = max(imgH - h, 0)\n                padding_w = max(imgW - w, 0)\n                img_padded = np.pad(\n                    img,\n                    ((0, padding_h), (0, padding_w)),\n                    \"constant\",\n                    constant_values=(255),\n                )\n                img = img_padded\n\n        img = np.expand_dims(img, 0) / 255.0  # h,w,c -> c,h,w\n        img = img.astype(\"float32\")\n\n        return img\n\n    def __call__(self, img_list):\n        img_num = len(img_list)\n        # Calculate the aspect ratio of all text bars\n        width_list = []\n        for img in img_list:\n            width_list.append(img.shape[1] / float(img.shape[0]))\n        # Sorting can speed up the recognition process\n        indices = np.argsort(np.array(width_list))\n        rec_res = [[\"\", 0.0]] * img_num\n        batch_num = self.rec_batch_num\n        st = time.time()\n        if self.benchmark:\n            self.autolog.times.start()\n        for beg_img_no in range(0, img_num, batch_num):\n            end_img_no = min(img_num, beg_img_no + batch_num)\n            norm_img_batch = []\n            if self.rec_algorithm == \"SRN\":\n                encoder_word_pos_list = []\n                gsrm_word_pos_list = []\n                gsrm_slf_attn_bias1_list = []\n                gsrm_slf_attn_bias2_list = []\n            if self.rec_algorithm == \"SAR\":\n                valid_ratios = []\n            imgC, imgH, imgW = self.rec_image_shape[:3]\n            max_wh_ratio = imgW / imgH\n            wh_ratio_list = []\n            for ino in range(beg_img_no, end_img_no):\n                h, w = img_list[indices[ino]].shape[0:2]\n                wh_ratio = w * 1.0 / h\n                max_wh_ratio = max(max_wh_ratio, wh_ratio)\n                wh_ratio_list.append(wh_ratio)\n            for ino in range(beg_img_no, end_img_no):\n                if self.rec_algorithm == \"SAR\":\n                    norm_img, _, _, valid_ratio = self.resize_norm_img_sar(\n                        img_list[indices[ino]], self.rec_image_shape\n                    )\n                    norm_img = norm_img[np.newaxis, :]\n                    valid_ratio = np.expand_dims(valid_ratio, axis=0)\n                    valid_ratios.append(valid_ratio)\n                    norm_img_batch.append(norm_img)\n                elif self.rec_algorithm == \"SRN\":\n                    norm_img = self.process_image_srn(\n                        img_list[indices[ino]], self.rec_image_shape, 8, 25\n                    )\n                    encoder_word_pos_list.append(norm_img[1])\n                    gsrm_word_pos_list.append(norm_img[2])\n                    gsrm_slf_attn_bias1_list.append(norm_img[3])\n                    gsrm_slf_attn_bias2_list.append(norm_img[4])\n                    norm_img_batch.append(norm_img[0])\n                elif self.rec_algorithm in [\"SVTR\", \"SATRN\", \"ParseQ\", \"CPPD\"]:\n                    norm_img = self.resize_norm_img_svtr(\n                        img_list[indices[ino]], self.rec_image_shape\n                    )\n                    norm_img = norm_img[np.newaxis, :]\n                    norm_img_batch.append(norm_img)\n                elif self.rec_algorithm in [\"CPPDPadding\"]:\n                    norm_img = self.resize_norm_img_cppd_padding(\n                        img_list[indices[ino]], self.rec_image_shape\n                    )\n                    norm_img = norm_img[np.newaxis, :]\n                    norm_img_batch.append(norm_img)\n                elif self.rec_algorithm in [\"VisionLAN\", \"PREN\"]:\n                    norm_img = self.resize_norm_img_vl(\n                        img_list[indices[ino]], self.rec_image_shape\n                    )\n                    norm_img = norm_img[np.newaxis, :]\n                    norm_img_batch.append(norm_img)\n                elif self.rec_algorithm == \"SPIN\":\n                    norm_img = self.resize_norm_img_spin(img_list[indices[ino]])\n                    norm_img = norm_img[np.newaxis, :]\n                    norm_img_batch.append(norm_img)\n                elif self.rec_algorithm == \"ABINet\":\n                    norm_img = self.resize_norm_img_abinet(\n                        img_list[indices[ino]], self.rec_image_shape\n                    )\n                    norm_img = norm_img[np.newaxis, :]\n                    norm_img_batch.append(norm_img)\n                elif self.rec_algorithm == \"RobustScanner\":\n                    norm_img, _, _, valid_ratio = self.resize_norm_img_sar(\n                        img_list[indices[ino]],\n                        self.rec_image_shape,\n                        width_downsample_ratio=0.25,\n                    )\n                    norm_img = norm_img[np.newaxis, :]\n                    valid_ratio = np.expand_dims(valid_ratio, axis=0)\n                    valid_ratios = []\n                    valid_ratios.append(valid_ratio)\n                    norm_img_batch.append(norm_img)\n                    word_positions_list = []\n                    word_positions = np.array(range(0, 40)).astype(\"int64\")\n                    word_positions = np.expand_dims(word_positions, axis=0)\n                    word_positions_list.append(word_positions)\n                elif self.rec_algorithm == \"CAN\":\n                    norm_img = self.norm_img_can(img_list[indices[ino]], max_wh_ratio)\n                    norm_img = norm_img[np.newaxis, :]\n                    norm_img_batch.append(norm_img)\n                    norm_image_mask = np.ones(norm_img.shape, dtype=\"float32\")\n                    word_label = np.ones([1, 36], dtype=\"int64\")\n                    norm_img_mask_batch = []\n                    word_label_list = []\n                    norm_img_mask_batch.append(norm_image_mask)\n                    word_label_list.append(word_label)\n                else:\n                    norm_img = self.resize_norm_img(\n                        img_list[indices[ino]], max_wh_ratio\n                    )\n                    norm_img = norm_img[np.newaxis, :]\n                    norm_img_batch.append(norm_img)\n            norm_img_batch = np.concatenate(norm_img_batch)\n            norm_img_batch = norm_img_batch.copy()\n            if self.benchmark:\n                self.autolog.times.stamp()\n\n            if self.rec_algorithm == \"SRN\":\n                encoder_word_pos_list = np.concatenate(encoder_word_pos_list)\n                gsrm_word_pos_list = np.concatenate(gsrm_word_pos_list)\n                gsrm_slf_attn_bias1_list = np.concatenate(gsrm_slf_attn_bias1_list)\n                gsrm_slf_attn_bias2_list = np.concatenate(gsrm_slf_attn_bias2_list)\n\n                inputs = [\n                    norm_img_batch,\n                    encoder_word_pos_list,\n                    gsrm_word_pos_list,\n                    gsrm_slf_attn_bias1_list,\n                    gsrm_slf_attn_bias2_list,\n                ]\n                if self.use_onnx:\n                    input_dict = {}\n                    input_dict[self.input_tensor.name] = norm_img_batch\n                    outputs = self.predictor.run(self.output_tensors, input_dict)\n                    preds = {\"predict\": outputs[2]}\n                else:\n                    input_names = self.predictor.get_input_names()\n                    for i in range(len(input_names)):\n                        input_tensor = self.predictor.get_input_handle(input_names[i])\n                        input_tensor.copy_from_cpu(inputs[i])\n                    self.predictor.run()\n                    outputs = []\n                    for output_tensor in self.output_tensors:\n                        output = output_tensor.copy_to_cpu()\n                        outputs.append(output)\n                    if self.benchmark:\n                        self.autolog.times.stamp()\n                    preds = {\"predict\": outputs[2]}\n            elif self.rec_algorithm == \"SAR\":\n                valid_ratios = np.concatenate(valid_ratios)\n                inputs = [\n                    norm_img_batch,\n                    np.array([valid_ratios], dtype=np.float32).T,\n                ]\n                if self.use_onnx:\n                    input_dict = {}\n                    input_dict[self.input_tensor.name] = norm_img_batch\n                    outputs = self.predictor.run(self.output_tensors, input_dict)\n                    preds = outputs[0]\n                else:\n                    input_names = self.predictor.get_input_names()\n                    for i in range(len(input_names)):\n                        input_tensor = self.predictor.get_input_handle(input_names[i])\n                        input_tensor.copy_from_cpu(inputs[i])\n                    self.predictor.run()\n                    outputs = []\n                    for output_tensor in self.output_tensors:\n                        output = output_tensor.copy_to_cpu()\n                        outputs.append(output)\n                    if self.benchmark:\n                        self.autolog.times.stamp()\n                    preds = outputs[0]\n            elif self.rec_algorithm == \"RobustScanner\":\n                valid_ratios = np.concatenate(valid_ratios)\n                word_positions_list = np.concatenate(word_positions_list)\n                inputs = [norm_img_batch, valid_ratios, word_positions_list]\n\n                if self.use_onnx:\n                    input_dict = {}\n                    input_dict[self.input_tensor.name] = norm_img_batch\n                    outputs = self.predictor.run(self.output_tensors, input_dict)\n                    preds = outputs[0]\n                else:\n                    input_names = self.predictor.get_input_names()\n                    for i in range(len(input_names)):\n                        input_tensor = self.predictor.get_input_handle(input_names[i])\n                        input_tensor.copy_from_cpu(inputs[i])\n                    self.predictor.run()\n                    outputs = []\n                    for output_tensor in self.output_tensors:\n                        output = output_tensor.copy_to_cpu()\n                        outputs.append(output)\n                    if self.benchmark:\n                        self.autolog.times.stamp()\n                    preds = outputs[0]\n            elif self.rec_algorithm == \"CAN\":\n                norm_img_mask_batch = np.concatenate(norm_img_mask_batch)\n                word_label_list = np.concatenate(word_label_list)\n                inputs = [norm_img_batch, norm_img_mask_batch, word_label_list]\n                if self.use_onnx:\n                    input_dict = {}\n                    input_dict[self.input_tensor.name] = norm_img_batch\n                    outputs = self.predictor.run(self.output_tensors, input_dict)\n                    preds = outputs\n                else:\n                    input_names = self.predictor.get_input_names()\n                    input_tensor = []\n                    for i in range(len(input_names)):\n                        input_tensor_i = self.predictor.get_input_handle(input_names[i])\n                        input_tensor_i.copy_from_cpu(inputs[i])\n                        input_tensor.append(input_tensor_i)\n                    self.input_tensor = input_tensor\n                    self.predictor.run()\n                    outputs = []\n                    for output_tensor in self.output_tensors:\n                        output = output_tensor.copy_to_cpu()\n                        outputs.append(output)\n                    if self.benchmark:\n                        self.autolog.times.stamp()\n                    preds = outputs\n            else:\n                if self.use_onnx:\n                    input_dict = {}\n                    input_dict[self.input_tensor.name] = norm_img_batch\n                    outputs = self.predictor.run(self.output_tensors, input_dict)\n                    preds = outputs[0]\n                else:\n                    self.input_tensor.copy_from_cpu(norm_img_batch)\n                    self.predictor.run()\n                    outputs = []\n                    for output_tensor in self.output_tensors:\n                        output = output_tensor.copy_to_cpu()\n                        outputs.append(output)\n                    if self.benchmark:\n                        self.autolog.times.stamp()\n                    if len(outputs) != 1:\n                        preds = outputs\n                    else:\n                        preds = outputs[0]\n            if self.postprocess_params[\"name\"] == \"CTCLabelDecode\":\n                rec_result = self.postprocess_op(\n                    preds,\n                    return_word_box=self.return_word_box,\n                    wh_ratio_list=wh_ratio_list,\n                    max_wh_ratio=max_wh_ratio,\n                )\n            else:\n                rec_result = self.postprocess_op(preds)\n            for rno in range(len(rec_result)):\n                rec_res[indices[beg_img_no + rno]] = rec_result[rno]\n            if self.benchmark:\n                self.autolog.times.end(stamp=True)\n        return rec_res, time.time() - st\n\n\ndef main(args):\n    image_file_list = get_image_file_list(args.image_dir)\n    valid_image_file_list = []\n    img_list = []\n\n    # logger\n    log_file = args.save_log_path\n    if os.path.isdir(args.save_log_path) or (\n        not os.path.exists(args.save_log_path) and args.save_log_path.endswith(\"/\")\n    ):\n        log_file = os.path.join(log_file, \"benchmark_recognition.log\")\n    logger = get_logger(log_file=log_file)\n\n    # create text recognizer\n    text_recognizer = TextRecognizer(args)\n\n    logger.info(\n        \"In PP-OCRv3, rec_image_shape parameter defaults to '3, 48, 320', \"\n        \"if you are using recognition model with PP-OCRv2 or an older version, please set --rec_image_shape='3,32,320\"\n    )\n\n    # warmup 2 times\n    if args.warmup:\n        img = np.random.uniform(0, 255, [48, 320, 3]).astype(np.uint8)\n        for i in range(2):\n            res = text_recognizer([img] * int(args.rec_batch_num))\n\n    for image_file in image_file_list:\n        img, flag, _ = check_and_read(image_file)\n        if not flag:\n            img = cv2.imread(image_file)\n        if img is None:\n            logger.info(\"error in loading image:{}\".format(image_file))\n            continue\n        valid_image_file_list.append(image_file)\n        img_list.append(img)\n    try:\n        rec_res, _ = text_recognizer(img_list)\n\n    except Exception as E:\n        logger.info(traceback.format_exc())\n        logger.info(E)\n        exit()\n    for ino in range(len(img_list)):\n        logger.info(\n            \"Predicts of {}:{}\".format(valid_image_file_list[ino], rec_res[ino])\n        )\n    if args.benchmark:\n        text_recognizer.autolog.report()\n\n\nif __name__ == \"__main__\":\n    main(utility.parse_args())\n", "tools/infer/predict_cls.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\nimport sys\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(__dir__)\nsys.path.insert(0, os.path.abspath(os.path.join(__dir__, \"../..\")))\n\nos.environ[\"FLAGS_allocator_strategy\"] = \"auto_growth\"\n\nimport cv2\nimport copy\nimport numpy as np\nimport math\nimport time\nimport traceback\n\nimport tools.infer.utility as utility\nfrom ppocr.postprocess import build_post_process\nfrom ppocr.utils.logging import get_logger\nfrom ppocr.utils.utility import get_image_file_list, check_and_read\n\nlogger = get_logger()\n\n\nclass TextClassifier(object):\n    def __init__(self, args):\n        self.cls_image_shape = [int(v) for v in args.cls_image_shape.split(\",\")]\n        self.cls_batch_num = args.cls_batch_num\n        self.cls_thresh = args.cls_thresh\n        postprocess_params = {\n            \"name\": \"ClsPostProcess\",\n            \"label_list\": args.label_list,\n        }\n        self.postprocess_op = build_post_process(postprocess_params)\n        (\n            self.predictor,\n            self.input_tensor,\n            self.output_tensors,\n            _,\n        ) = utility.create_predictor(args, \"cls\", logger)\n        self.use_onnx = args.use_onnx\n\n    def resize_norm_img(self, img):\n        imgC, imgH, imgW = self.cls_image_shape\n        h = img.shape[0]\n        w = img.shape[1]\n        ratio = w / float(h)\n        if math.ceil(imgH * ratio) > imgW:\n            resized_w = imgW\n        else:\n            resized_w = int(math.ceil(imgH * ratio))\n        resized_image = cv2.resize(img, (resized_w, imgH))\n        resized_image = resized_image.astype(\"float32\")\n        if self.cls_image_shape[0] == 1:\n            resized_image = resized_image / 255\n            resized_image = resized_image[np.newaxis, :]\n        else:\n            resized_image = resized_image.transpose((2, 0, 1)) / 255\n        resized_image -= 0.5\n        resized_image /= 0.5\n        padding_im = np.zeros((imgC, imgH, imgW), dtype=np.float32)\n        padding_im[:, :, 0:resized_w] = resized_image\n        return padding_im\n\n    def __call__(self, img_list):\n        img_list = copy.deepcopy(img_list)\n        img_num = len(img_list)\n        # Calculate the aspect ratio of all text bars\n        width_list = []\n        for img in img_list:\n            width_list.append(img.shape[1] / float(img.shape[0]))\n        # Sorting can speed up the cls process\n        indices = np.argsort(np.array(width_list))\n\n        cls_res = [[\"\", 0.0]] * img_num\n        batch_num = self.cls_batch_num\n        elapse = 0\n        for beg_img_no in range(0, img_num, batch_num):\n            end_img_no = min(img_num, beg_img_no + batch_num)\n            norm_img_batch = []\n            max_wh_ratio = 0\n            starttime = time.time()\n            for ino in range(beg_img_no, end_img_no):\n                h, w = img_list[indices[ino]].shape[0:2]\n                wh_ratio = w * 1.0 / h\n                max_wh_ratio = max(max_wh_ratio, wh_ratio)\n            for ino in range(beg_img_no, end_img_no):\n                norm_img = self.resize_norm_img(img_list[indices[ino]])\n                norm_img = norm_img[np.newaxis, :]\n                norm_img_batch.append(norm_img)\n            norm_img_batch = np.concatenate(norm_img_batch)\n            norm_img_batch = norm_img_batch.copy()\n\n            if self.use_onnx:\n                input_dict = {}\n                input_dict[self.input_tensor.name] = norm_img_batch\n                outputs = self.predictor.run(self.output_tensors, input_dict)\n                prob_out = outputs[0]\n            else:\n                self.input_tensor.copy_from_cpu(norm_img_batch)\n                self.predictor.run()\n                prob_out = self.output_tensors[0].copy_to_cpu()\n                self.predictor.try_shrink_memory()\n            cls_result = self.postprocess_op(prob_out)\n            elapse += time.time() - starttime\n            for rno in range(len(cls_result)):\n                label, score = cls_result[rno]\n                cls_res[indices[beg_img_no + rno]] = [label, score]\n                if \"180\" in label and score > self.cls_thresh:\n                    img_list[indices[beg_img_no + rno]] = cv2.rotate(\n                        img_list[indices[beg_img_no + rno]], 1\n                    )\n        return img_list, cls_res, elapse\n\n\ndef main(args):\n    image_file_list = get_image_file_list(args.image_dir)\n    text_classifier = TextClassifier(args)\n    valid_image_file_list = []\n    img_list = []\n    for image_file in image_file_list:\n        img, flag, _ = check_and_read(image_file)\n        if not flag:\n            img = cv2.imread(image_file)\n        if img is None:\n            logger.info(\"error in loading image:{}\".format(image_file))\n            continue\n        valid_image_file_list.append(image_file)\n        img_list.append(img)\n    try:\n        img_list, cls_res, predict_time = text_classifier(img_list)\n    except Exception as E:\n        logger.info(traceback.format_exc())\n        logger.info(E)\n        exit()\n    for ino in range(len(img_list)):\n        logger.info(\n            \"Predicts of {}:{}\".format(valid_image_file_list[ino], cls_res[ino])\n        )\n\n\nif __name__ == \"__main__\":\n    main(utility.parse_args())\n", "tools/infer/predict_system.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\nimport sys\nimport subprocess\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(__dir__)\nsys.path.insert(0, os.path.abspath(os.path.join(__dir__, \"../..\")))\n\nos.environ[\"FLAGS_allocator_strategy\"] = \"auto_growth\"\n\nimport cv2\nimport copy\nimport numpy as np\nimport json\nimport time\nimport logging\nfrom PIL import Image\nimport tools.infer.utility as utility\nimport tools.infer.predict_rec as predict_rec\nimport tools.infer.predict_det as predict_det\nimport tools.infer.predict_cls as predict_cls\nfrom ppocr.utils.utility import get_image_file_list, check_and_read\nfrom ppocr.utils.logging import get_logger\nfrom tools.infer.utility import (\n    draw_ocr_box_txt,\n    get_rotate_crop_image,\n    get_minarea_rect_crop,\n    slice_generator,\n    merge_fragmented,\n)\n\nlogger = get_logger()\n\n\nclass TextSystem(object):\n    def __init__(self, args):\n        if not args.show_log:\n            logger.setLevel(logging.INFO)\n\n        self.text_detector = predict_det.TextDetector(args)\n        self.text_recognizer = predict_rec.TextRecognizer(args)\n        self.use_angle_cls = args.use_angle_cls\n        self.drop_score = args.drop_score\n        if self.use_angle_cls:\n            self.text_classifier = predict_cls.TextClassifier(args)\n\n        self.args = args\n        self.crop_image_res_index = 0\n\n    def draw_crop_rec_res(self, output_dir, img_crop_list, rec_res):\n        os.makedirs(output_dir, exist_ok=True)\n        bbox_num = len(img_crop_list)\n        for bno in range(bbox_num):\n            cv2.imwrite(\n                os.path.join(\n                    output_dir, f\"mg_crop_{bno+self.crop_image_res_index}.jpg\"\n                ),\n                img_crop_list[bno],\n            )\n            logger.debug(f\"{bno}, {rec_res[bno]}\")\n        self.crop_image_res_index += bbox_num\n\n    def __call__(self, img, cls=True, slice={}):\n        time_dict = {\"det\": 0, \"rec\": 0, \"cls\": 0, \"all\": 0}\n\n        if img is None:\n            logger.debug(\"no valid image provided\")\n            return None, None, time_dict\n\n        start = time.time()\n        ori_im = img.copy()\n        if slice:\n            slice_gen = slice_generator(\n                img,\n                horizontal_stride=slice[\"horizontal_stride\"],\n                vertical_stride=slice[\"vertical_stride\"],\n            )\n            elapsed = []\n            dt_slice_boxes = []\n            for slice_crop, v_start, h_start in slice_gen:\n                dt_boxes, elapse = self.text_detector(slice_crop)\n                if dt_boxes.size:\n                    dt_boxes[:, :, 0] += h_start\n                    dt_boxes[:, :, 1] += v_start\n                    dt_slice_boxes.append(dt_boxes)\n                    elapsed.append(elapse)\n            dt_boxes = np.concatenate(dt_slice_boxes)\n\n            dt_boxes = merge_fragmented(\n                boxes=dt_boxes,\n                x_threshold=slice[\"merge_x_thres\"],\n                y_threshold=slice[\"merge_y_thres\"],\n            )\n            elapse = sum(elapsed)\n        else:\n            dt_boxes, elapse = self.text_detector(img)\n\n        time_dict[\"det\"] = elapse\n\n        if dt_boxes is None:\n            logger.debug(\"no dt_boxes found, elapsed : {}\".format(elapse))\n            end = time.time()\n            time_dict[\"all\"] = end - start\n            return None, None, time_dict\n        else:\n            logger.debug(\n                \"dt_boxes num : {}, elapsed : {}\".format(len(dt_boxes), elapse)\n            )\n        img_crop_list = []\n\n        dt_boxes = sorted_boxes(dt_boxes)\n\n        for bno in range(len(dt_boxes)):\n            tmp_box = copy.deepcopy(dt_boxes[bno])\n            if self.args.det_box_type == \"quad\":\n                img_crop = get_rotate_crop_image(ori_im, tmp_box)\n            else:\n                img_crop = get_minarea_rect_crop(ori_im, tmp_box)\n            img_crop_list.append(img_crop)\n        if self.use_angle_cls and cls:\n            img_crop_list, angle_list, elapse = self.text_classifier(img_crop_list)\n            time_dict[\"cls\"] = elapse\n            logger.debug(\n                \"cls num  : {}, elapsed : {}\".format(len(img_crop_list), elapse)\n            )\n        if len(img_crop_list) > 1000:\n            logger.debug(\n                f\"rec crops num: {len(img_crop_list)}, time and memory cost may be large.\"\n            )\n\n        rec_res, elapse = self.text_recognizer(img_crop_list)\n        time_dict[\"rec\"] = elapse\n        logger.debug(\"rec_res num  : {}, elapsed : {}\".format(len(rec_res), elapse))\n        if self.args.save_crop_res:\n            self.draw_crop_rec_res(self.args.crop_res_save_dir, img_crop_list, rec_res)\n        filter_boxes, filter_rec_res = [], []\n        for box, rec_result in zip(dt_boxes, rec_res):\n            text, score = rec_result[0], rec_result[1]\n            if score >= self.drop_score:\n                filter_boxes.append(box)\n                filter_rec_res.append(rec_result)\n        end = time.time()\n        time_dict[\"all\"] = end - start\n        return filter_boxes, filter_rec_res, time_dict\n\n\ndef sorted_boxes(dt_boxes):\n    \"\"\"\n    Sort text boxes in order from top to bottom, left to right\n    args:\n        dt_boxes(array):detected text boxes with shape [4, 2]\n    return:\n        sorted boxes(array) with shape [4, 2]\n    \"\"\"\n    num_boxes = dt_boxes.shape[0]\n    sorted_boxes = sorted(dt_boxes, key=lambda x: (x[0][1], x[0][0]))\n    _boxes = list(sorted_boxes)\n\n    for i in range(num_boxes - 1):\n        for j in range(i, -1, -1):\n            if abs(_boxes[j + 1][0][1] - _boxes[j][0][1]) < 10 and (\n                _boxes[j + 1][0][0] < _boxes[j][0][0]\n            ):\n                tmp = _boxes[j]\n                _boxes[j] = _boxes[j + 1]\n                _boxes[j + 1] = tmp\n            else:\n                break\n    return _boxes\n\n\ndef main(args):\n    image_file_list = get_image_file_list(args.image_dir)\n    image_file_list = image_file_list[args.process_id :: args.total_process_num]\n    text_sys = TextSystem(args)\n    is_visualize = True\n    font_path = args.vis_font_path\n    drop_score = args.drop_score\n    draw_img_save_dir = args.draw_img_save_dir\n    os.makedirs(draw_img_save_dir, exist_ok=True)\n    save_results = []\n\n    logger.info(\n        \"In PP-OCRv3, rec_image_shape parameter defaults to '3, 48, 320', \"\n        \"if you are using recognition model with PP-OCRv2 or an older version, please set --rec_image_shape='3,32,320\"\n    )\n\n    # warm up 10 times\n    if args.warmup:\n        img = np.random.uniform(0, 255, [640, 640, 3]).astype(np.uint8)\n        for i in range(10):\n            res = text_sys(img)\n\n    total_time = 0\n    cpu_mem, gpu_mem, gpu_util = 0, 0, 0\n    _st = time.time()\n    count = 0\n    for idx, image_file in enumerate(image_file_list):\n        img, flag_gif, flag_pdf = check_and_read(image_file)\n        if not flag_gif and not flag_pdf:\n            img = cv2.imread(image_file)\n        if not flag_pdf:\n            if img is None:\n                logger.debug(\"error in loading image:{}\".format(image_file))\n                continue\n            imgs = [img]\n        else:\n            page_num = args.page_num\n            if page_num > len(img) or page_num == 0:\n                page_num = len(img)\n            imgs = img[:page_num]\n        for index, img in enumerate(imgs):\n            starttime = time.time()\n            dt_boxes, rec_res, time_dict = text_sys(img)\n            elapse = time.time() - starttime\n            total_time += elapse\n            if len(imgs) > 1:\n                logger.debug(\n                    str(idx)\n                    + \"_\"\n                    + str(index)\n                    + \"  Predict time of %s: %.3fs\" % (image_file, elapse)\n                )\n            else:\n                logger.debug(\n                    str(idx) + \"  Predict time of %s: %.3fs\" % (image_file, elapse)\n                )\n            for text, score in rec_res:\n                logger.debug(\"{}, {:.3f}\".format(text, score))\n\n            res = [\n                {\n                    \"transcription\": rec_res[i][0],\n                    \"points\": np.array(dt_boxes[i]).astype(np.int32).tolist(),\n                }\n                for i in range(len(dt_boxes))\n            ]\n            if len(imgs) > 1:\n                save_pred = (\n                    os.path.basename(image_file)\n                    + \"_\"\n                    + str(index)\n                    + \"\\t\"\n                    + json.dumps(res, ensure_ascii=False)\n                    + \"\\n\"\n                )\n            else:\n                save_pred = (\n                    os.path.basename(image_file)\n                    + \"\\t\"\n                    + json.dumps(res, ensure_ascii=False)\n                    + \"\\n\"\n                )\n            save_results.append(save_pred)\n\n            if is_visualize:\n                image = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n                boxes = dt_boxes\n                txts = [rec_res[i][0] for i in range(len(rec_res))]\n                scores = [rec_res[i][1] for i in range(len(rec_res))]\n\n                draw_img = draw_ocr_box_txt(\n                    image,\n                    boxes,\n                    txts,\n                    scores,\n                    drop_score=drop_score,\n                    font_path=font_path,\n                )\n                if flag_gif:\n                    save_file = image_file[:-3] + \"png\"\n                elif flag_pdf:\n                    save_file = image_file.replace(\".pdf\", \"_\" + str(index) + \".png\")\n                else:\n                    save_file = image_file\n                cv2.imwrite(\n                    os.path.join(draw_img_save_dir, os.path.basename(save_file)),\n                    draw_img[:, :, ::-1],\n                )\n                logger.debug(\n                    \"The visualized image saved in {}\".format(\n                        os.path.join(draw_img_save_dir, os.path.basename(save_file))\n                    )\n                )\n\n    logger.info(\"The predict total time is {}\".format(time.time() - _st))\n    if args.benchmark:\n        text_sys.text_detector.autolog.report()\n        text_sys.text_recognizer.autolog.report()\n\n    with open(\n        os.path.join(draw_img_save_dir, \"system_results.txt\"), \"w\", encoding=\"utf-8\"\n    ) as f:\n        f.writelines(save_results)\n\n\nif __name__ == \"__main__\":\n    args = utility.parse_args()\n    if args.use_mp:\n        p_list = []\n        total_process_num = args.total_process_num\n        for process_id in range(total_process_num):\n            cmd = (\n                [sys.executable, \"-u\"]\n                + sys.argv\n                + [\"--process_id={}\".format(process_id), \"--use_mp={}\".format(False)]\n            )\n            p = subprocess.Popen(cmd, stdout=sys.stdout, stderr=sys.stdout)\n            p_list.append(p)\n        for p in p_list:\n            p.wait()\n    else:\n        main(args)\n", "tools/infer/predict_sr.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\nimport sys\nfrom PIL import Image\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.insert(0, __dir__)\nsys.path.insert(0, os.path.abspath(os.path.join(__dir__, \"../..\")))\n\nos.environ[\"FLAGS_allocator_strategy\"] = \"auto_growth\"\n\nimport cv2\nimport numpy as np\nimport math\nimport time\nimport traceback\nimport paddle\n\nimport tools.infer.utility as utility\nfrom ppocr.postprocess import build_post_process\nfrom ppocr.utils.logging import get_logger\nfrom ppocr.utils.utility import get_image_file_list, check_and_read\n\nlogger = get_logger()\n\n\nclass TextSR(object):\n    def __init__(self, args):\n        self.sr_image_shape = [int(v) for v in args.sr_image_shape.split(\",\")]\n        self.sr_batch_num = args.sr_batch_num\n\n        (\n            self.predictor,\n            self.input_tensor,\n            self.output_tensors,\n            self.config,\n        ) = utility.create_predictor(args, \"sr\", logger)\n        self.benchmark = args.benchmark\n        if args.benchmark:\n            import auto_log\n\n            pid = os.getpid()\n            gpu_id = utility.get_infer_gpuid()\n            self.autolog = auto_log.AutoLogger(\n                model_name=\"sr\",\n                model_precision=args.precision,\n                batch_size=args.sr_batch_num,\n                data_shape=\"dynamic\",\n                save_path=None,  # args.save_log_path,\n                inference_config=self.config,\n                pids=pid,\n                process_name=None,\n                gpu_ids=gpu_id if args.use_gpu else None,\n                time_keys=[\"preprocess_time\", \"inference_time\", \"postprocess_time\"],\n                warmup=0,\n                logger=logger,\n            )\n\n    def resize_norm_img(self, img):\n        imgC, imgH, imgW = self.sr_image_shape\n        img = img.resize((imgW // 2, imgH // 2), Image.BICUBIC)\n        img_numpy = np.array(img).astype(\"float32\")\n        img_numpy = img_numpy.transpose((2, 0, 1)) / 255\n        return img_numpy\n\n    def __call__(self, img_list):\n        img_num = len(img_list)\n        batch_num = self.sr_batch_num\n        st = time.time()\n        st = time.time()\n        all_result = [] * img_num\n        if self.benchmark:\n            self.autolog.times.start()\n        for beg_img_no in range(0, img_num, batch_num):\n            end_img_no = min(img_num, beg_img_no + batch_num)\n            norm_img_batch = []\n            imgC, imgH, imgW = self.sr_image_shape\n            for ino in range(beg_img_no, end_img_no):\n                norm_img = self.resize_norm_img(img_list[ino])\n                norm_img = norm_img[np.newaxis, :]\n                norm_img_batch.append(norm_img)\n\n            norm_img_batch = np.concatenate(norm_img_batch)\n            norm_img_batch = norm_img_batch.copy()\n            if self.benchmark:\n                self.autolog.times.stamp()\n            self.input_tensor.copy_from_cpu(norm_img_batch)\n            self.predictor.run()\n            outputs = []\n            for output_tensor in self.output_tensors:\n                output = output_tensor.copy_to_cpu()\n                outputs.append(output)\n            if len(outputs) != 1:\n                preds = outputs\n            else:\n                preds = outputs[0]\n            all_result.append(outputs)\n        if self.benchmark:\n            self.autolog.times.end(stamp=True)\n        return all_result, time.time() - st\n\n\ndef main(args):\n    image_file_list = get_image_file_list(args.image_dir)\n    text_recognizer = TextSR(args)\n    valid_image_file_list = []\n    img_list = []\n\n    # warmup 2 times\n    if args.warmup:\n        img = np.random.uniform(0, 255, [16, 64, 3]).astype(np.uint8)\n        for i in range(2):\n            res = text_recognizer([img] * int(args.sr_batch_num))\n\n    for image_file in image_file_list:\n        img, flag, _ = check_and_read(image_file)\n        if not flag:\n            img = Image.open(image_file).convert(\"RGB\")\n        if img is None:\n            logger.info(\"error in loading image:{}\".format(image_file))\n            continue\n        valid_image_file_list.append(image_file)\n        img_list.append(img)\n    try:\n        preds, _ = text_recognizer(img_list)\n        for beg_no in range(len(preds)):\n            sr_img = preds[beg_no][1]\n            lr_img = preds[beg_no][0]\n            for i in range(sr_img.shape[0]):\n                fm_sr = (sr_img[i] * 255).transpose(1, 2, 0).astype(np.uint8)\n                fm_lr = (lr_img[i] * 255).transpose(1, 2, 0).astype(np.uint8)\n                img_name_pure = os.path.split(\n                    valid_image_file_list[beg_no * args.sr_batch_num + i]\n                )[-1]\n                cv2.imwrite(\n                    \"infer_result/sr_{}\".format(img_name_pure), fm_sr[:, :, ::-1]\n                )\n                logger.info(\n                    \"The visualized image saved in infer_result/sr_{}\".format(\n                        img_name_pure\n                    )\n                )\n\n    except Exception as E:\n        logger.info(traceback.format_exc())\n        logger.info(E)\n        exit()\n    if args.benchmark:\n        text_recognizer.autolog.report()\n\n\nif __name__ == \"__main__\":\n    main(utility.parse_args())\n", "ppocr/__init__.py": "# Copyright (c) 2019 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n", "ppocr/optimizer/optimizer.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom paddle import optimizer as optim\n\n\nclass Momentum(object):\n    \"\"\"\n    Simple Momentum optimizer with velocity state.\n    Args:\n        learning_rate (float|Variable) - The learning rate used to update parameters.\n            Can be a float value or a Variable with one float value as data element.\n        momentum (float) - Momentum factor.\n        regularization (WeightDecayRegularizer, optional) - The strategy of regularization.\n    \"\"\"\n\n    def __init__(\n        self, learning_rate, momentum, weight_decay=None, grad_clip=None, **args\n    ):\n        super(Momentum, self).__init__()\n        self.learning_rate = learning_rate\n        self.momentum = momentum\n        self.weight_decay = weight_decay\n        self.grad_clip = grad_clip\n\n    def __call__(self, model):\n        train_params = [\n            param for param in model.parameters() if param.trainable is True\n        ]\n        opt = optim.Momentum(\n            learning_rate=self.learning_rate,\n            momentum=self.momentum,\n            weight_decay=self.weight_decay,\n            grad_clip=self.grad_clip,\n            parameters=train_params,\n        )\n        return opt\n\n\nclass Adam(object):\n    def __init__(\n        self,\n        learning_rate=0.001,\n        beta1=0.9,\n        beta2=0.999,\n        epsilon=1e-08,\n        parameter_list=None,\n        weight_decay=None,\n        grad_clip=None,\n        name=None,\n        lazy_mode=False,\n        **kwargs,\n    ):\n        self.learning_rate = learning_rate\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.epsilon = epsilon\n        self.parameter_list = parameter_list\n        self.learning_rate = learning_rate\n        self.weight_decay = weight_decay\n        self.grad_clip = grad_clip\n        self.name = name\n        self.lazy_mode = lazy_mode\n        self.group_lr = kwargs.get(\"group_lr\", False)\n        self.training_step = kwargs.get(\"training_step\", None)\n\n    def __call__(self, model):\n        if self.group_lr:\n            if self.training_step == \"LF_2\":\n                import paddle\n\n                if isinstance(model, paddle.DataParallel):  # multi gpu\n                    mlm = model._layers.head.MLM_VRM.MLM.parameters()\n                    pre_mlm_pp = (\n                        model._layers.head.MLM_VRM.Prediction.pp_share.parameters()\n                    )\n                    pre_mlm_w = (\n                        model._layers.head.MLM_VRM.Prediction.w_share.parameters()\n                    )\n                else:  # single gpu\n                    mlm = model.head.MLM_VRM.MLM.parameters()\n                    pre_mlm_pp = model.head.MLM_VRM.Prediction.pp_share.parameters()\n                    pre_mlm_w = model.head.MLM_VRM.Prediction.w_share.parameters()\n\n                total = []\n                for param in mlm:\n                    total.append(id(param))\n                for param in pre_mlm_pp:\n                    total.append(id(param))\n                for param in pre_mlm_w:\n                    total.append(id(param))\n\n                group_base_params = [\n                    param for param in model.parameters() if id(param) in total\n                ]\n                group_small_params = [\n                    param for param in model.parameters() if id(param) not in total\n                ]\n                train_params = [\n                    {\"params\": group_base_params},\n                    {\n                        \"params\": group_small_params,\n                        \"learning_rate\": self.learning_rate.values[0] * 0.1,\n                    },\n                ]\n\n            else:\n                print(\"group lr currently only support VisionLAN in LF_2 training step\")\n                train_params = [\n                    param for param in model.parameters() if param.trainable is True\n                ]\n        else:\n            train_params = [\n                param for param in model.parameters() if param.trainable is True\n            ]\n\n        opt = optim.Adam(\n            learning_rate=self.learning_rate,\n            beta1=self.beta1,\n            beta2=self.beta2,\n            epsilon=self.epsilon,\n            weight_decay=self.weight_decay,\n            grad_clip=self.grad_clip,\n            name=self.name,\n            lazy_mode=self.lazy_mode,\n            parameters=train_params,\n        )\n        return opt\n\n\nclass RMSProp(object):\n    \"\"\"\n    Root Mean Squared Propagation (RMSProp) is an unpublished, adaptive learning rate method.\n    Args:\n        learning_rate (float|Variable) - The learning rate used to update parameters.\n            Can be a float value or a Variable with one float value as data element.\n        momentum (float) - Momentum factor.\n        rho (float) - rho value in equation.\n        epsilon (float) - avoid division by zero, default is 1e-6.\n        regularization (WeightDecayRegularizer, optional) - The strategy of regularization.\n    \"\"\"\n\n    def __init__(\n        self,\n        learning_rate,\n        momentum=0.0,\n        rho=0.95,\n        epsilon=1e-6,\n        weight_decay=None,\n        grad_clip=None,\n        **args,\n    ):\n        super(RMSProp, self).__init__()\n        self.learning_rate = learning_rate\n        self.momentum = momentum\n        self.rho = rho\n        self.epsilon = epsilon\n        self.weight_decay = weight_decay\n        self.grad_clip = grad_clip\n\n    def __call__(self, model):\n        train_params = [\n            param for param in model.parameters() if param.trainable is True\n        ]\n        opt = optim.RMSProp(\n            learning_rate=self.learning_rate,\n            momentum=self.momentum,\n            rho=self.rho,\n            epsilon=self.epsilon,\n            weight_decay=self.weight_decay,\n            grad_clip=self.grad_clip,\n            parameters=train_params,\n        )\n        return opt\n\n\nclass Adadelta(object):\n    def __init__(\n        self,\n        learning_rate=0.001,\n        epsilon=1e-08,\n        rho=0.95,\n        parameter_list=None,\n        weight_decay=None,\n        grad_clip=None,\n        name=None,\n        **kwargs,\n    ):\n        self.learning_rate = learning_rate\n        self.epsilon = epsilon\n        self.rho = rho\n        self.parameter_list = parameter_list\n        self.learning_rate = learning_rate\n        self.weight_decay = weight_decay\n        self.grad_clip = grad_clip\n        self.name = name\n\n    def __call__(self, model):\n        train_params = [\n            param for param in model.parameters() if param.trainable is True\n        ]\n        opt = optim.Adadelta(\n            learning_rate=self.learning_rate,\n            epsilon=self.epsilon,\n            rho=self.rho,\n            weight_decay=self.weight_decay,\n            grad_clip=self.grad_clip,\n            name=self.name,\n            parameters=train_params,\n        )\n        return opt\n\n\nclass AdamW(object):\n    def __init__(\n        self,\n        learning_rate=0.001,\n        beta1=0.9,\n        beta2=0.999,\n        epsilon=1e-8,\n        weight_decay=0.01,\n        multi_precision=False,\n        grad_clip=None,\n        no_weight_decay_name=None,\n        one_dim_param_no_weight_decay=False,\n        name=None,\n        lazy_mode=False,\n        **args,\n    ):\n        super().__init__()\n        self.learning_rate = learning_rate\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.epsilon = epsilon\n        self.grad_clip = grad_clip\n        self.weight_decay = 0.01 if weight_decay is None else weight_decay\n        self.grad_clip = grad_clip\n        self.name = name\n        self.lazy_mode = lazy_mode\n        self.multi_precision = multi_precision\n        self.no_weight_decay_name_list = (\n            no_weight_decay_name.split() if no_weight_decay_name else []\n        )\n        self.one_dim_param_no_weight_decay = one_dim_param_no_weight_decay\n\n    def __call__(self, model):\n        parameters = [param for param in model.parameters() if param.trainable is True]\n\n        self.no_weight_decay_param_name_list = [\n            p.name\n            for n, p in model.named_parameters()\n            if any(nd in n for nd in self.no_weight_decay_name_list)\n        ]\n\n        if self.one_dim_param_no_weight_decay:\n            self.no_weight_decay_param_name_list += [\n                p.name for n, p in model.named_parameters() if len(p.shape) == 1\n            ]\n\n        opt = optim.AdamW(\n            learning_rate=self.learning_rate,\n            beta1=self.beta1,\n            beta2=self.beta2,\n            epsilon=self.epsilon,\n            parameters=parameters,\n            weight_decay=self.weight_decay,\n            multi_precision=self.multi_precision,\n            grad_clip=self.grad_clip,\n            name=self.name,\n            lazy_mode=self.lazy_mode,\n            apply_decay_param_fun=self._apply_decay_param_fun,\n        )\n        return opt\n\n    def _apply_decay_param_fun(self, name):\n        return name not in self.no_weight_decay_param_name_list\n", "ppocr/optimizer/learning_rate.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom paddle.optimizer import lr\nfrom .lr_scheduler import CyclicalCosineDecay, OneCycleDecay, TwoStepCosineDecay\n\n\nclass Linear(object):\n    \"\"\"\n    Linear learning rate decay\n    Args:\n        lr (float): The initial learning rate. It is a python float number.\n        epochs(int): The decay step size. It determines the decay cycle.\n        end_lr(float, optional): The minimum final learning rate. Default: 0.0001.\n        power(float, optional): Power of polynomial. Default: 1.0.\n        last_epoch (int, optional):  The index of last epoch. Can be set to restart training. Default: -1, means initial learning rate.\n    \"\"\"\n\n    def __init__(\n        self,\n        learning_rate,\n        epochs,\n        step_each_epoch,\n        end_lr=0.0,\n        power=1.0,\n        warmup_epoch=0,\n        last_epoch=-1,\n        **kwargs,\n    ):\n        super(Linear, self).__init__()\n        self.learning_rate = learning_rate\n        self.epochs = epochs * step_each_epoch\n        self.end_lr = end_lr\n        self.power = power\n        self.last_epoch = last_epoch\n        self.warmup_epoch = round(warmup_epoch * step_each_epoch)\n\n    def __call__(self):\n        learning_rate = lr.PolynomialDecay(\n            learning_rate=self.learning_rate,\n            decay_steps=self.epochs,\n            end_lr=self.end_lr,\n            power=self.power,\n            last_epoch=self.last_epoch,\n        )\n        if self.warmup_epoch > 0:\n            learning_rate = lr.LinearWarmup(\n                learning_rate=learning_rate,\n                warmup_steps=self.warmup_epoch,\n                start_lr=0.0,\n                end_lr=self.learning_rate,\n                last_epoch=self.last_epoch,\n            )\n        return learning_rate\n\n\nclass Cosine(object):\n    \"\"\"\n    Cosine learning rate decay\n    lr = 0.05 * (math.cos(epoch * (math.pi / epochs)) + 1)\n    Args:\n        lr(float): initial learning rate\n        step_each_epoch(int): steps each epoch\n        epochs(int): total training epochs\n        last_epoch (int, optional):  The index of last epoch. Can be set to restart training. Default: -1, means initial learning rate.\n    \"\"\"\n\n    def __init__(\n        self,\n        learning_rate,\n        step_each_epoch,\n        epochs,\n        warmup_epoch=0,\n        last_epoch=-1,\n        **kwargs,\n    ):\n        super(Cosine, self).__init__()\n        self.learning_rate = learning_rate\n        self.T_max = step_each_epoch * epochs\n        self.last_epoch = last_epoch\n        self.warmup_epoch = round(warmup_epoch * step_each_epoch)\n\n    def __call__(self):\n        learning_rate = lr.CosineAnnealingDecay(\n            learning_rate=self.learning_rate,\n            T_max=self.T_max,\n            last_epoch=self.last_epoch,\n        )\n        if self.warmup_epoch > 0:\n            learning_rate = lr.LinearWarmup(\n                learning_rate=learning_rate,\n                warmup_steps=self.warmup_epoch,\n                start_lr=0.0,\n                end_lr=self.learning_rate,\n                last_epoch=self.last_epoch,\n            )\n        return learning_rate\n\n\nclass Step(object):\n    \"\"\"\n    Piecewise learning rate decay\n    Args:\n        step_each_epoch(int): steps each epoch\n        learning_rate (float): The initial learning rate. It is a python float number.\n        step_size (int): the interval to update.\n        gamma (float, optional): The Ratio that the learning rate will be reduced. ``new_lr = origin_lr * gamma`` .\n            It should be less than 1.0. Default: 0.1.\n        last_epoch (int, optional):  The index of last epoch. Can be set to restart training. Default: -1, means initial learning rate.\n    \"\"\"\n\n    def __init__(\n        self,\n        learning_rate,\n        step_size,\n        step_each_epoch,\n        gamma,\n        warmup_epoch=0,\n        last_epoch=-1,\n        **kwargs,\n    ):\n        super(Step, self).__init__()\n        self.step_size = step_each_epoch * step_size\n        self.learning_rate = learning_rate\n        self.gamma = gamma\n        self.last_epoch = last_epoch\n        self.warmup_epoch = round(warmup_epoch * step_each_epoch)\n\n    def __call__(self):\n        learning_rate = lr.StepDecay(\n            learning_rate=self.learning_rate,\n            step_size=self.step_size,\n            gamma=self.gamma,\n            last_epoch=self.last_epoch,\n        )\n        if self.warmup_epoch > 0:\n            learning_rate = lr.LinearWarmup(\n                learning_rate=learning_rate,\n                warmup_steps=self.warmup_epoch,\n                start_lr=0.0,\n                end_lr=self.learning_rate,\n                last_epoch=self.last_epoch,\n            )\n        return learning_rate\n\n\nclass Piecewise(object):\n    \"\"\"\n    Piecewise learning rate decay\n    Args:\n        boundaries(list): A list of steps numbers. The type of element in the list is python int.\n        values(list): A list of learning rate values that will be picked during different epoch boundaries.\n            The type of element in the list is python float.\n        last_epoch (int, optional):  The index of last epoch. Can be set to restart training. Default: -1, means initial learning rate.\n    \"\"\"\n\n    def __init__(\n        self,\n        step_each_epoch,\n        decay_epochs,\n        values,\n        warmup_epoch=0,\n        last_epoch=-1,\n        **kwargs,\n    ):\n        super(Piecewise, self).__init__()\n        self.boundaries = [step_each_epoch * e for e in decay_epochs]\n        self.values = values\n        self.last_epoch = last_epoch\n        self.warmup_epoch = round(warmup_epoch * step_each_epoch)\n\n    def __call__(self):\n        learning_rate = lr.PiecewiseDecay(\n            boundaries=self.boundaries, values=self.values, last_epoch=self.last_epoch\n        )\n        if self.warmup_epoch > 0:\n            learning_rate = lr.LinearWarmup(\n                learning_rate=learning_rate,\n                warmup_steps=self.warmup_epoch,\n                start_lr=0.0,\n                end_lr=self.values[0],\n                last_epoch=self.last_epoch,\n            )\n        return learning_rate\n\n\nclass CyclicalCosine(object):\n    \"\"\"\n    Cyclical cosine learning rate decay\n    Args:\n        learning_rate(float): initial learning rate\n        step_each_epoch(int): steps each epoch\n        epochs(int): total training epochs\n        cycle(int): period of the cosine learning rate\n        last_epoch (int, optional):  The index of last epoch. Can be set to restart training. Default: -1, means initial learning rate.\n    \"\"\"\n\n    def __init__(\n        self,\n        learning_rate,\n        step_each_epoch,\n        epochs,\n        cycle,\n        warmup_epoch=0,\n        last_epoch=-1,\n        **kwargs,\n    ):\n        super(CyclicalCosine, self).__init__()\n        self.learning_rate = learning_rate\n        self.T_max = step_each_epoch * epochs\n        self.last_epoch = last_epoch\n        self.warmup_epoch = round(warmup_epoch * step_each_epoch)\n        self.cycle = round(cycle * step_each_epoch)\n\n    def __call__(self):\n        learning_rate = CyclicalCosineDecay(\n            learning_rate=self.learning_rate,\n            T_max=self.T_max,\n            cycle=self.cycle,\n            last_epoch=self.last_epoch,\n        )\n        if self.warmup_epoch > 0:\n            learning_rate = lr.LinearWarmup(\n                learning_rate=learning_rate,\n                warmup_steps=self.warmup_epoch,\n                start_lr=0.0,\n                end_lr=self.learning_rate,\n                last_epoch=self.last_epoch,\n            )\n        return learning_rate\n\n\nclass OneCycle(object):\n    \"\"\"\n    One Cycle learning rate decay\n    Args:\n        max_lr(float): Upper learning rate boundaries\n        epochs(int): total training epochs\n        step_each_epoch(int): steps each epoch\n        anneal_strategy(str): {\u2018cos\u2019, \u2018linear\u2019} Specifies the annealing strategy: \u201ccos\u201d for cosine annealing, \u201clinear\u201d for linear annealing.\n            Default: \u2018cos\u2019\n        three_phase(bool): If True, use a third phase of the schedule to annihilate the learning rate according to \u2018final_div_factor\u2019\n            instead of modifying the second phase (the first two phases will be symmetrical about the step indicated by \u2018pct_start\u2019).\n        last_epoch (int, optional):  The index of last epoch. Can be set to restart training. Default: -1, means initial learning rate.\n    \"\"\"\n\n    def __init__(\n        self,\n        max_lr,\n        epochs,\n        step_each_epoch,\n        anneal_strategy=\"cos\",\n        three_phase=False,\n        warmup_epoch=0,\n        last_epoch=-1,\n        **kwargs,\n    ):\n        super(OneCycle, self).__init__()\n        self.max_lr = max_lr\n        self.epochs = epochs\n        self.steps_per_epoch = step_each_epoch\n        self.anneal_strategy = anneal_strategy\n        self.three_phase = three_phase\n        self.last_epoch = last_epoch\n        self.warmup_epoch = round(warmup_epoch * step_each_epoch)\n\n    def __call__(self):\n        learning_rate = OneCycleDecay(\n            max_lr=self.max_lr,\n            epochs=self.epochs,\n            steps_per_epoch=self.steps_per_epoch,\n            anneal_strategy=self.anneal_strategy,\n            three_phase=self.three_phase,\n            last_epoch=self.last_epoch,\n        )\n        if self.warmup_epoch > 0:\n            learning_rate = lr.LinearWarmup(\n                learning_rate=learning_rate,\n                warmup_steps=self.warmup_epoch,\n                start_lr=0.0,\n                end_lr=self.max_lr,\n                last_epoch=self.last_epoch,\n            )\n        return learning_rate\n\n\nclass Const(object):\n    \"\"\"\n    Const learning rate decay\n    Args:\n        learning_rate(float): initial learning rate\n        step_each_epoch(int): steps each epoch\n        last_epoch (int, optional):  The index of last epoch. Can be set to restart training. Default: -1, means initial learning rate.\n    \"\"\"\n\n    def __init__(\n        self, learning_rate, step_each_epoch, warmup_epoch=0, last_epoch=-1, **kwargs\n    ):\n        super(Const, self).__init__()\n        self.learning_rate = learning_rate\n        self.last_epoch = last_epoch\n        self.warmup_epoch = round(warmup_epoch * step_each_epoch)\n\n    def __call__(self):\n        learning_rate = self.learning_rate\n        if self.warmup_epoch > 0:\n            learning_rate = lr.LinearWarmup(\n                learning_rate=learning_rate,\n                warmup_steps=self.warmup_epoch,\n                start_lr=0.0,\n                end_lr=self.learning_rate,\n                last_epoch=self.last_epoch,\n            )\n        return learning_rate\n\n\nclass DecayLearningRate(object):\n    \"\"\"\n    DecayLearningRate learning rate decay\n    new_lr = (lr - end_lr) * (1 - epoch/decay_steps)**power + end_lr\n    Args:\n        learning_rate(float): initial learning rate\n        step_each_epoch(int): steps each epoch\n        epochs(int): total training epochs\n        factor(float): Power of polynomial, should greater than 0.0 to get learning rate decay. Default: 0.9\n        end_lr(float): The minimum final learning rate. Default: 0.0.\n    \"\"\"\n\n    def __init__(\n        self, learning_rate, step_each_epoch, epochs, factor=0.9, end_lr=0, **kwargs\n    ):\n        super(DecayLearningRate, self).__init__()\n        self.learning_rate = learning_rate\n        self.epochs = epochs + 1\n        self.factor = factor\n        self.end_lr = 0\n        self.decay_steps = step_each_epoch * epochs\n\n    def __call__(self):\n        learning_rate = lr.PolynomialDecay(\n            learning_rate=self.learning_rate,\n            decay_steps=self.decay_steps,\n            power=self.factor,\n            end_lr=self.end_lr,\n        )\n        return learning_rate\n\n\nclass MultiStepDecay(object):\n    \"\"\"\n    Piecewise learning rate decay\n    Args:\n        step_each_epoch(int): steps each epoch\n        learning_rate (float): The initial learning rate. It is a python float number.\n        step_size (int): the interval to update.\n        gamma (float, optional): The Ratio that the learning rate will be reduced. ``new_lr = origin_lr * gamma`` .\n            It should be less than 1.0. Default: 0.1.\n        last_epoch (int, optional):  The index of last epoch. Can be set to restart training. Default: -1, means initial learning rate.\n    \"\"\"\n\n    def __init__(\n        self,\n        learning_rate,\n        milestones,\n        step_each_epoch,\n        gamma,\n        warmup_epoch=0,\n        last_epoch=-1,\n        **kwargs,\n    ):\n        super(MultiStepDecay, self).__init__()\n        self.milestones = [step_each_epoch * e for e in milestones]\n        self.learning_rate = learning_rate\n        self.gamma = gamma\n        self.last_epoch = last_epoch\n        self.warmup_epoch = round(warmup_epoch * step_each_epoch)\n\n    def __call__(self):\n        learning_rate = lr.MultiStepDecay(\n            learning_rate=self.learning_rate,\n            milestones=self.milestones,\n            gamma=self.gamma,\n            last_epoch=self.last_epoch,\n        )\n        if self.warmup_epoch > 0:\n            learning_rate = lr.LinearWarmup(\n                learning_rate=learning_rate,\n                warmup_steps=self.warmup_epoch,\n                start_lr=0.0,\n                end_lr=self.learning_rate,\n                last_epoch=self.last_epoch,\n            )\n        return learning_rate\n\n\nclass TwoStepCosine(object):\n    \"\"\"\n    Cosine learning rate decay\n    lr = 0.05 * (math.cos(epoch * (math.pi / epochs)) + 1)\n    Args:\n        lr(float): initial learning rate\n        step_each_epoch(int): steps each epoch\n        epochs(int): total training epochs\n        last_epoch (int, optional):  The index of last epoch. Can be set to restart training. Default: -1, means initial learning rate.\n    \"\"\"\n\n    def __init__(\n        self,\n        learning_rate,\n        step_each_epoch,\n        epochs,\n        warmup_epoch=0,\n        last_epoch=-1,\n        **kwargs,\n    ):\n        super(TwoStepCosine, self).__init__()\n        self.learning_rate = learning_rate\n        self.T_max1 = step_each_epoch * 200\n        self.T_max2 = step_each_epoch * epochs\n        self.last_epoch = last_epoch\n        self.warmup_epoch = round(warmup_epoch * step_each_epoch)\n\n    def __call__(self):\n        learning_rate = TwoStepCosineDecay(\n            learning_rate=self.learning_rate,\n            T_max1=self.T_max1,\n            T_max2=self.T_max2,\n            last_epoch=self.last_epoch,\n        )\n        if self.warmup_epoch > 0:\n            learning_rate = lr.LinearWarmup(\n                learning_rate=learning_rate,\n                warmup_steps=self.warmup_epoch,\n                start_lr=0.0,\n                end_lr=self.learning_rate,\n                last_epoch=self.last_epoch,\n            )\n        return learning_rate\n", "ppocr/optimizer/regularizer.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport paddle\n\n\nclass L1Decay(object):\n    \"\"\"\n    L1 Weight Decay Regularization, which encourages the weights to be sparse.\n    Args:\n        factor(float): regularization coeff. Default:0.0.\n    \"\"\"\n\n    def __init__(self, factor=0.0):\n        super(L1Decay, self).__init__()\n        self.coeff = factor\n\n    def __call__(self):\n        reg = paddle.regularizer.L1Decay(self.coeff)\n        return reg\n\n\nclass L2Decay(object):\n    \"\"\"\n    L2 Weight Decay Regularization, which helps to prevent the model over-fitting.\n    Args:\n        factor(float): regularization coeff. Default:0.0.\n    \"\"\"\n\n    def __init__(self, factor=0.0):\n        super(L2Decay, self).__init__()\n        self.coeff = float(factor)\n\n    def __call__(self):\n        return self.coeff\n", "ppocr/optimizer/__init__.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\nimport copy\nimport paddle\n\n__all__ = [\"build_optimizer\"]\n\n\ndef build_lr_scheduler(lr_config, epochs, step_each_epoch):\n    from . import learning_rate\n\n    lr_config.update({\"epochs\": epochs, \"step_each_epoch\": step_each_epoch})\n    lr_name = lr_config.pop(\"name\", \"Const\")\n    lr = getattr(learning_rate, lr_name)(**lr_config)()\n    return lr\n\n\ndef build_optimizer(config, epochs, step_each_epoch, model):\n    from . import regularizer, optimizer\n\n    config = copy.deepcopy(config)\n    # step1 build lr\n    lr = build_lr_scheduler(config.pop(\"lr\"), epochs, step_each_epoch)\n\n    # step2 build regularization\n    if \"regularizer\" in config and config[\"regularizer\"] is not None:\n        reg_config = config.pop(\"regularizer\")\n        reg_name = reg_config.pop(\"name\")\n        if not hasattr(regularizer, reg_name):\n            reg_name += \"Decay\"\n        reg = getattr(regularizer, reg_name)(**reg_config)()\n    elif \"weight_decay\" in config:\n        reg = config.pop(\"weight_decay\")\n    else:\n        reg = None\n\n    # step3 build optimizer\n    optim_name = config.pop(\"name\")\n    if \"clip_norm\" in config:\n        clip_norm = config.pop(\"clip_norm\")\n        grad_clip = paddle.nn.ClipGradByNorm(clip_norm=clip_norm)\n    elif \"clip_norm_global\" in config:\n        clip_norm = config.pop(\"clip_norm_global\")\n        grad_clip = paddle.nn.ClipGradByGlobalNorm(clip_norm=clip_norm)\n    else:\n        grad_clip = None\n    optim = getattr(optimizer, optim_name)(\n        learning_rate=lr, weight_decay=reg, grad_clip=grad_clip, **config\n    )\n    return optim(model), lr\n", "ppocr/optimizer/lr_scheduler.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport math\nfrom paddle.optimizer.lr import LRScheduler\n\n\nclass CyclicalCosineDecay(LRScheduler):\n    def __init__(\n        self, learning_rate, T_max, cycle=1, last_epoch=-1, eta_min=0.0, verbose=False\n    ):\n        \"\"\"\n        Cyclical cosine learning rate decay\n        A learning rate which can be referred in https://arxiv.org/pdf/2012.12645.pdf\n        Args:\n            learning rate(float): learning rate\n            T_max(int): maximum epoch num\n            cycle(int): period of the cosine decay\n            last_epoch (int, optional):  The index of last epoch. Can be set to restart training. Default: -1, means initial learning rate.\n            eta_min(float): minimum learning rate during training\n            verbose(bool): whether to print learning rate for each epoch\n        \"\"\"\n        super(CyclicalCosineDecay, self).__init__(learning_rate, last_epoch, verbose)\n        self.cycle = cycle\n        self.eta_min = eta_min\n\n    def get_lr(self):\n        if self.last_epoch == 0:\n            return self.base_lr\n        reletive_epoch = self.last_epoch % self.cycle\n        lr = self.eta_min + 0.5 * (self.base_lr - self.eta_min) * (\n            1 + math.cos(math.pi * reletive_epoch / self.cycle)\n        )\n        return lr\n\n\nclass OneCycleDecay(LRScheduler):\n    \"\"\"\n    One Cycle learning rate decay\n    A learning rate which can be referred in https://arxiv.org/abs/1708.07120\n    Code refered in https://pytorch.org/docs/stable/_modules/torch/optim/lr_scheduler.html#OneCycleLR\n    \"\"\"\n\n    def __init__(\n        self,\n        max_lr,\n        epochs=None,\n        steps_per_epoch=None,\n        pct_start=0.3,\n        anneal_strategy=\"cos\",\n        div_factor=25.0,\n        final_div_factor=1e4,\n        three_phase=False,\n        last_epoch=-1,\n        verbose=False,\n    ):\n        # Validate total_steps\n        if epochs <= 0 or not isinstance(epochs, int):\n            raise ValueError(\n                \"Expected positive integer epochs, but got {}\".format(epochs)\n            )\n        if steps_per_epoch <= 0 or not isinstance(steps_per_epoch, int):\n            raise ValueError(\n                \"Expected positive integer steps_per_epoch, but got {}\".format(\n                    steps_per_epoch\n                )\n            )\n        self.total_steps = epochs * steps_per_epoch\n\n        self.max_lr = max_lr\n        self.initial_lr = self.max_lr / div_factor\n        self.min_lr = self.initial_lr / final_div_factor\n\n        if three_phase:\n            self._schedule_phases = [\n                {\n                    \"end_step\": float(pct_start * self.total_steps) - 1,\n                    \"start_lr\": self.initial_lr,\n                    \"end_lr\": self.max_lr,\n                },\n                {\n                    \"end_step\": float(2 * pct_start * self.total_steps) - 2,\n                    \"start_lr\": self.max_lr,\n                    \"end_lr\": self.initial_lr,\n                },\n                {\n                    \"end_step\": self.total_steps - 1,\n                    \"start_lr\": self.initial_lr,\n                    \"end_lr\": self.min_lr,\n                },\n            ]\n        else:\n            self._schedule_phases = [\n                {\n                    \"end_step\": float(pct_start * self.total_steps) - 1,\n                    \"start_lr\": self.initial_lr,\n                    \"end_lr\": self.max_lr,\n                },\n                {\n                    \"end_step\": self.total_steps - 1,\n                    \"start_lr\": self.max_lr,\n                    \"end_lr\": self.min_lr,\n                },\n            ]\n\n        # Validate pct_start\n        if pct_start < 0 or pct_start > 1 or not isinstance(pct_start, float):\n            raise ValueError(\n                \"Expected float between 0 and 1 pct_start, but got {}\".format(pct_start)\n            )\n\n        # Validate anneal_strategy\n        if anneal_strategy not in [\"cos\", \"linear\"]:\n            raise ValueError(\n                \"anneal_strategy must by one of 'cos' or 'linear', instead got {}\".format(\n                    anneal_strategy\n                )\n            )\n        elif anneal_strategy == \"cos\":\n            self.anneal_func = self._annealing_cos\n        elif anneal_strategy == \"linear\":\n            self.anneal_func = self._annealing_linear\n\n        super(OneCycleDecay, self).__init__(max_lr, last_epoch, verbose)\n\n    def _annealing_cos(self, start, end, pct):\n        \"Cosine anneal from `start` to `end` as pct goes from 0.0 to 1.0.\"\n        cos_out = math.cos(math.pi * pct) + 1\n        return end + (start - end) / 2.0 * cos_out\n\n    def _annealing_linear(self, start, end, pct):\n        \"Linearly anneal from `start` to `end` as pct goes from 0.0 to 1.0.\"\n        return (end - start) * pct + start\n\n    def get_lr(self):\n        computed_lr = 0.0\n        step_num = self.last_epoch\n\n        if step_num > self.total_steps:\n            raise ValueError(\n                \"Tried to step {} times. The specified number of total steps is {}\".format(\n                    step_num + 1, self.total_steps\n                )\n            )\n        start_step = 0\n        for i, phase in enumerate(self._schedule_phases):\n            end_step = phase[\"end_step\"]\n            if step_num <= end_step or i == len(self._schedule_phases) - 1:\n                pct = (step_num - start_step) / (end_step - start_step)\n                computed_lr = self.anneal_func(phase[\"start_lr\"], phase[\"end_lr\"], pct)\n                break\n            start_step = phase[\"end_step\"]\n\n        return computed_lr\n\n\nclass TwoStepCosineDecay(LRScheduler):\n    def __init__(\n        self, learning_rate, T_max1, T_max2, eta_min=0, last_epoch=-1, verbose=False\n    ):\n        if not isinstance(T_max1, int):\n            raise TypeError(\n                \"The type of 'T_max1' in 'CosineAnnealingDecay' must be 'int', but received %s.\"\n                % type(T_max1)\n            )\n        if not isinstance(T_max2, int):\n            raise TypeError(\n                \"The type of 'T_max2' in 'CosineAnnealingDecay' must be 'int', but received %s.\"\n                % type(T_max2)\n            )\n        if not isinstance(eta_min, (float, int)):\n            raise TypeError(\n                \"The type of 'eta_min' in 'CosineAnnealingDecay' must be 'float, int', but received %s.\"\n                % type(eta_min)\n            )\n        assert T_max1 > 0 and isinstance(\n            T_max1, int\n        ), \" 'T_max1' must be a positive integer.\"\n        assert T_max2 > 0 and isinstance(\n            T_max2, int\n        ), \" 'T_max1' must be a positive integer.\"\n        self.T_max1 = T_max1\n        self.T_max2 = T_max2\n        self.eta_min = float(eta_min)\n        super(TwoStepCosineDecay, self).__init__(learning_rate, last_epoch, verbose)\n\n    def get_lr(self):\n        if self.last_epoch <= self.T_max1:\n            if self.last_epoch == 0:\n                return self.base_lr\n            elif (self.last_epoch - 1 - self.T_max1) % (2 * self.T_max1) == 0:\n                return (\n                    self.last_lr\n                    + (self.base_lr - self.eta_min)\n                    * (1 - math.cos(math.pi / self.T_max1))\n                    / 2\n                )\n\n            return (1 + math.cos(math.pi * self.last_epoch / self.T_max1)) / (\n                1 + math.cos(math.pi * (self.last_epoch - 1) / self.T_max1)\n            ) * (self.last_lr - self.eta_min) + self.eta_min\n        else:\n            if (self.last_epoch - 1 - self.T_max2) % (2 * self.T_max2) == 0:\n                return (\n                    self.last_lr\n                    + (self.base_lr - self.eta_min)\n                    * (1 - math.cos(math.pi / self.T_max2))\n                    / 2\n                )\n\n            return (1 + math.cos(math.pi * self.last_epoch / self.T_max2)) / (\n                1 + math.cos(math.pi * (self.last_epoch - 1) / self.T_max2)\n            ) * (self.last_lr - self.eta_min) + self.eta_min\n\n    def _get_closed_form_lr(self):\n        if self.last_epoch <= self.T_max1:\n            return (\n                self.eta_min\n                + (self.base_lr - self.eta_min)\n                * (1 + math.cos(math.pi * self.last_epoch / self.T_max1))\n                / 2\n            )\n        else:\n            return (\n                self.eta_min\n                + (self.base_lr - self.eta_min)\n                * (1 + math.cos(math.pi * self.last_epoch / self.T_max2))\n                / 2\n            )\n", "ppocr/losses/rec_vl_loss.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/wangyuxin87/VisionLAN\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nfrom paddle import nn\n\n\nclass VLLoss(nn.Layer):\n    def __init__(self, mode=\"LF_1\", weight_res=0.5, weight_mas=0.5, **kwargs):\n        super(VLLoss, self).__init__()\n        self.loss_func = paddle.nn.loss.CrossEntropyLoss(reduction=\"mean\")\n        assert mode in [\"LF_1\", \"LF_2\", \"LA\"]\n        self.mode = mode\n        self.weight_res = weight_res\n        self.weight_mas = weight_mas\n\n    def flatten_label(self, target):\n        label_flatten = []\n        label_length = []\n        for i in range(0, target.shape[0]):\n            cur_label = target[i].tolist()\n            label_flatten += cur_label[: cur_label.index(0) + 1]\n            label_length.append(cur_label.index(0) + 1)\n        label_flatten = paddle.to_tensor(label_flatten, dtype=\"int64\")\n        label_length = paddle.to_tensor(label_length, dtype=\"int32\")\n        return (label_flatten, label_length)\n\n    def _flatten(self, sources, lengths):\n        return paddle.concat([t[:l] for t, l in zip(sources, lengths)])\n\n    def forward(self, predicts, batch):\n        text_pre = predicts[0]\n        target = batch[1].astype(\"int64\")\n        label_flatten, length = self.flatten_label(target)\n        text_pre = self._flatten(text_pre, length)\n        if self.mode == \"LF_1\":\n            loss = self.loss_func(text_pre, label_flatten)\n        else:\n            text_rem = predicts[1]\n            text_mas = predicts[2]\n            target_res = batch[2].astype(\"int64\")\n            target_sub = batch[3].astype(\"int64\")\n            label_flatten_res, length_res = self.flatten_label(target_res)\n            label_flatten_sub, length_sub = self.flatten_label(target_sub)\n            text_rem = self._flatten(text_rem, length_res)\n            text_mas = self._flatten(text_mas, length_sub)\n            loss_ori = self.loss_func(text_pre, label_flatten)\n            loss_res = self.loss_func(text_rem, label_flatten_res)\n            loss_mas = self.loss_func(text_mas, label_flatten_sub)\n            loss = loss_ori + loss_res * self.weight_res + loss_mas * self.weight_mas\n        return {\"loss\": loss}\n", "ppocr/losses/ace_loss.py": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# This code is refer from: https://github.com/viig99/LS-ACELoss\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nimport paddle.nn as nn\n\n\nclass ACELoss(nn.Layer):\n    def __init__(self, **kwargs):\n        super().__init__()\n        self.loss_func = nn.CrossEntropyLoss(\n            weight=None, ignore_index=0, reduction=\"none\", soft_label=True, axis=-1\n        )\n\n    def __call__(self, predicts, batch):\n        if isinstance(predicts, (list, tuple)):\n            predicts = predicts[-1]\n\n        B, N = predicts.shape[:2]\n        div = paddle.to_tensor([N]).astype(\"float32\")\n\n        predicts = nn.functional.softmax(predicts, axis=-1)\n        aggregation_preds = paddle.sum(predicts, axis=1)\n        aggregation_preds = paddle.divide(aggregation_preds, div)\n\n        length = batch[2].astype(\"float32\")\n        batch = batch[3].astype(\"float32\")\n        batch[:, 0] = paddle.subtract(div, length)\n        batch = paddle.divide(batch, div)\n\n        loss = self.loss_func(aggregation_preds, batch)\n        return {\"loss_ace\": loss}\n", "ppocr/losses/rec_att_loss.py": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nfrom paddle import nn\n\n\nclass AttentionLoss(nn.Layer):\n    def __init__(self, **kwargs):\n        super(AttentionLoss, self).__init__()\n        self.loss_func = nn.CrossEntropyLoss(weight=None, reduction=\"none\")\n\n    def forward(self, predicts, batch):\n        targets = batch[1].astype(\"int64\")\n        label_lengths = batch[2].astype(\"int64\")\n        batch_size, num_steps, num_classes = (\n            predicts.shape[0],\n            predicts.shape[1],\n            predicts.shape[2],\n        )\n        assert (\n            len(targets.shape) == len(list(predicts.shape)) - 1\n        ), \"The target's shape and inputs's shape is [N, d] and [N, num_steps]\"\n\n        inputs = paddle.reshape(predicts, [-1, predicts.shape[-1]])\n        targets = paddle.reshape(targets, [-1])\n\n        return {\"loss\": paddle.sum(self.loss_func(inputs, targets))}\n", "ppocr/losses/rec_nrtr_loss.py": "import paddle\nfrom paddle import nn\nimport paddle.nn.functional as F\n\n\nclass NRTRLoss(nn.Layer):\n    def __init__(self, smoothing=True, ignore_index=0, **kwargs):\n        super(NRTRLoss, self).__init__()\n        if ignore_index >= 0 and not smoothing:\n            self.loss_func = nn.CrossEntropyLoss(\n                reduction=\"mean\", ignore_index=ignore_index\n            )\n        self.smoothing = smoothing\n\n    def forward(self, pred, batch):\n        max_len = batch[2].max()\n        tgt = batch[1][:, 1 : 2 + max_len]\n        pred = pred.reshape([-1, pred.shape[2]])\n        tgt = tgt.reshape([-1])\n        if self.smoothing:\n            eps = 0.1\n            n_class = pred.shape[1]\n            one_hot = F.one_hot(tgt, pred.shape[1])\n            one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n            log_prb = F.log_softmax(pred, axis=1)\n            non_pad_mask = paddle.not_equal(\n                tgt, paddle.zeros(tgt.shape, dtype=tgt.dtype)\n            )\n            loss = -(one_hot * log_prb).sum(axis=1)\n            loss = loss.masked_select(non_pad_mask).mean()\n        else:\n            loss = self.loss_func(pred, tgt)\n        return {\"loss\": loss}\n", "ppocr/losses/rec_satrn_loss.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/open-mmlab/mmocr/blob/1.x/mmocr/models/textrecog/module_losses/ce_module_loss.py\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nfrom paddle import nn\n\n\nclass SATRNLoss(nn.Layer):\n    def __init__(self, **kwargs):\n        super(SATRNLoss, self).__init__()\n        ignore_index = kwargs.get(\"ignore_index\", 92)  # 6626\n        self.loss_func = paddle.nn.loss.CrossEntropyLoss(\n            reduction=\"none\", ignore_index=ignore_index\n        )\n\n    def forward(self, predicts, batch):\n        predict = predicts[\n            :, :-1, :\n        ]  # ignore last index of outputs to be in same seq_len with targets\n        label = batch[1].astype(\"int64\")[\n            :, 1:\n        ]  # ignore first index of target in loss calculation\n        batch_size, num_steps, num_classes = (\n            predict.shape[0],\n            predict.shape[1],\n            predict.shape[2],\n        )\n        assert (\n            len(label.shape) == len(list(predict.shape)) - 1\n        ), \"The target's shape and inputs's shape is [N, d] and [N, num_steps]\"\n\n        inputs = paddle.reshape(predict, [-1, num_classes])\n        targets = paddle.reshape(label, [-1])\n        loss = self.loss_func(inputs, targets)\n        return {\"loss\": loss.mean()}\n", "ppocr/losses/det_east_loss.py": "# copyright (c) 2019 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nfrom paddle import nn\nfrom .det_basic_loss import DiceLoss\n\n\nclass EASTLoss(nn.Layer):\n    \"\"\" \"\"\"\n\n    def __init__(self, eps=1e-6, **kwargs):\n        super(EASTLoss, self).__init__()\n        self.dice_loss = DiceLoss(eps=eps)\n\n    def forward(self, predicts, labels):\n        l_score, l_geo, l_mask = labels[1:]\n        f_score = predicts[\"f_score\"]\n        f_geo = predicts[\"f_geo\"]\n\n        dice_loss = self.dice_loss(f_score, l_score, l_mask)\n\n        # smoooth_l1_loss\n        channels = 8\n        l_geo_split = paddle.split(l_geo, num_or_sections=channels + 1, axis=1)\n        f_geo_split = paddle.split(f_geo, num_or_sections=channels, axis=1)\n        smooth_l1 = 0\n        for i in range(0, channels):\n            geo_diff = l_geo_split[i] - f_geo_split[i]\n            abs_geo_diff = paddle.abs(geo_diff)\n            smooth_l1_sign = paddle.less_than(abs_geo_diff, l_score)\n            smooth_l1_sign = paddle.cast(smooth_l1_sign, dtype=\"float32\")\n            in_loss = abs_geo_diff * abs_geo_diff * smooth_l1_sign + (\n                abs_geo_diff - 0.5\n            ) * (1.0 - smooth_l1_sign)\n            out_loss = l_geo_split[-1] / channels * in_loss * l_score\n            smooth_l1 += out_loss\n        smooth_l1_loss = paddle.mean(smooth_l1 * l_score)\n\n        dice_loss = dice_loss * 0.01\n        total_loss = dice_loss + smooth_l1_loss\n        losses = {\n            \"loss\": total_loss,\n            \"dice_loss\": dice_loss,\n            \"smooth_l1_loss\": smooth_l1_loss,\n        }\n        return losses\n", "ppocr/losses/center_loss.py": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# This code is refer from: https://github.com/KaiyangZhou/pytorch-center-loss\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport os\nimport pickle\n\nimport paddle\nimport paddle.nn as nn\nimport paddle.nn.functional as F\n\n\nclass CenterLoss(nn.Layer):\n    \"\"\"\n    Reference: Wen et al. A Discriminative Feature Learning Approach for Deep Face Recognition. ECCV 2016.\n    \"\"\"\n\n    def __init__(self, num_classes=6625, feat_dim=96, center_file_path=None):\n        super().__init__()\n        self.num_classes = num_classes\n        self.feat_dim = feat_dim\n        self.centers = paddle.randn(shape=[self.num_classes, self.feat_dim]).astype(\n            \"float64\"\n        )\n\n        if center_file_path is not None:\n            assert os.path.exists(\n                center_file_path\n            ), f\"center path({center_file_path}) must exist when it is not None.\"\n            with open(center_file_path, \"rb\") as f:\n                char_dict = pickle.load(f)\n                for key in char_dict.keys():\n                    self.centers[key] = paddle.to_tensor(char_dict[key])\n\n    def __call__(self, predicts, batch):\n        assert isinstance(predicts, (list, tuple))\n        features, predicts = predicts\n\n        feats_reshape = paddle.reshape(features, [-1, features.shape[-1]]).astype(\n            \"float64\"\n        )\n        label = paddle.argmax(predicts, axis=2)\n        label = paddle.reshape(label, [label.shape[0] * label.shape[1]])\n\n        batch_size = feats_reshape.shape[0]\n\n        # calc l2 distance between feats and centers\n        square_feat = paddle.sum(paddle.square(feats_reshape), axis=1, keepdim=True)\n        square_feat = paddle.expand(square_feat, [batch_size, self.num_classes])\n\n        square_center = paddle.sum(paddle.square(self.centers), axis=1, keepdim=True)\n        square_center = paddle.expand(\n            square_center, [self.num_classes, batch_size]\n        ).astype(\"float64\")\n        square_center = paddle.transpose(square_center, [1, 0])\n\n        distmat = paddle.add(square_feat, square_center)\n        feat_dot_center = paddle.matmul(\n            feats_reshape, paddle.transpose(self.centers, [1, 0])\n        )\n        distmat = distmat - 2.0 * feat_dot_center\n\n        # generate the mask\n        classes = paddle.arange(self.num_classes).astype(\"int64\")\n        label = paddle.expand(\n            paddle.unsqueeze(label, 1), (batch_size, self.num_classes)\n        )\n        mask = paddle.equal(\n            paddle.expand(classes, [batch_size, self.num_classes]), label\n        ).astype(\"float64\")\n        dist = paddle.multiply(distmat, mask)\n\n        loss = paddle.sum(paddle.clip(dist, min=1e-12, max=1e12)) / batch_size\n        return {\"loss_center\": loss}\n", "ppocr/losses/rec_parseq_loss.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nfrom paddle import nn\n\n\nclass ParseQLoss(nn.Layer):\n    def __init__(self, **kwargs):\n        super(ParseQLoss, self).__init__()\n\n    def forward(self, predicts, targets):\n        label = targets[1]  # label\n        label_len = targets[2]\n        max_step = paddle.max(label_len).cpu().numpy()[0] + 2\n        tgt = label[:, :max_step]\n\n        logits_list = predicts[\"logits_list\"]\n        pad_id = predicts[\"pad_id\"]\n        eos_id = predicts[\"eos_id\"]\n\n        tgt_out = tgt[:, 1:]\n        loss = 0\n        loss_numel = 0\n        n = (tgt_out != pad_id).sum().item()\n\n        for i, logits in enumerate(logits_list):\n            loss += n * paddle.nn.functional.cross_entropy(\n                input=logits, label=tgt_out.flatten(), ignore_index=pad_id\n            )\n            loss_numel += n\n            if i == 1:\n                tgt_out = paddle.where(condition=tgt_out == eos_id, x=pad_id, y=tgt_out)\n                n = (tgt_out != pad_id).sum().item()\n        loss /= loss_numel\n\n        return {\"loss\": loss}\n", "ppocr/losses/rec_ce_loss.py": "import paddle\nfrom paddle import nn\nimport paddle.nn.functional as F\n\n\nclass CELoss(nn.Layer):\n    def __init__(self, smoothing=False, with_all=False, ignore_index=-1, **kwargs):\n        super(CELoss, self).__init__()\n        if ignore_index >= 0:\n            self.loss_func = nn.CrossEntropyLoss(\n                reduction=\"mean\", ignore_index=ignore_index\n            )\n        else:\n            self.loss_func = nn.CrossEntropyLoss(reduction=\"mean\")\n        self.smoothing = smoothing\n        self.with_all = with_all\n\n    def forward(self, pred, batch):\n        if isinstance(pred, dict):  # for ABINet\n            loss = {}\n            loss_sum = []\n            for name, logits in pred.items():\n                if isinstance(logits, list):\n                    logit_num = len(logits)\n                    all_tgt = paddle.concat([batch[1]] * logit_num, 0)\n                    all_logits = paddle.concat(logits, 0)\n                    flt_logtis = all_logits.reshape([-1, all_logits.shape[2]])\n                    flt_tgt = all_tgt.reshape([-1])\n                else:\n                    flt_logtis = logits.reshape([-1, logits.shape[2]])\n                    flt_tgt = batch[1].reshape([-1])\n                loss[name + \"_loss\"] = self.loss_func(flt_logtis, flt_tgt)\n                loss_sum.append(loss[name + \"_loss\"])\n            loss[\"loss\"] = sum(loss_sum)\n            return loss\n        else:\n            if self.with_all:  # for ViTSTR\n                tgt = batch[1]\n                pred = pred.reshape([-1, pred.shape[2]])\n                tgt = tgt.reshape([-1])\n                loss = self.loss_func(pred, tgt)\n                return {\"loss\": loss}\n            else:  # for NRTR\n                max_len = batch[2].max()\n                tgt = batch[1][:, 1 : 2 + max_len]\n                pred = pred.reshape([-1, pred.shape[2]])\n                tgt = tgt.reshape([-1])\n                if self.smoothing:\n                    eps = 0.1\n                    n_class = pred.shape[1]\n                    one_hot = F.one_hot(tgt, pred.shape[1])\n                    one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n                    log_prb = F.log_softmax(pred, axis=1)\n                    non_pad_mask = paddle.not_equal(\n                        tgt, paddle.zeros(tgt.shape, dtype=tgt.dtype)\n                    )\n                    loss = -(one_hot * log_prb).sum(axis=1)\n                    loss = loss.masked_select(non_pad_mask).mean()\n                else:\n                    loss = self.loss_func(pred, tgt)\n                return {\"loss\": loss}\n", "ppocr/losses/det_ct_loss.py": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/shengtao96/CentripetalText/tree/main/models/loss\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nfrom paddle import nn\nimport paddle.nn.functional as F\nimport numpy as np\n\n\ndef ohem_single(score, gt_text, training_mask):\n    # online hard example mining\n\n    pos_num = int(paddle.sum(gt_text > 0.5)) - int(\n        paddle.sum((gt_text > 0.5) & (training_mask <= 0.5))\n    )\n\n    if pos_num == 0:\n        # selected_mask = gt_text.copy() * 0 # may be not good\n        selected_mask = training_mask\n        selected_mask = paddle.cast(\n            selected_mask.reshape((1, selected_mask.shape[0], selected_mask.shape[1])),\n            \"float32\",\n        )\n        return selected_mask\n\n    neg_num = int(paddle.sum((gt_text <= 0.5) & (training_mask > 0.5)))\n    neg_num = int(min(pos_num * 3, neg_num))\n\n    if neg_num == 0:\n        selected_mask = training_mask\n        selected_mask = paddle.cast(\n            selected_mask.reshape((1, selected_mask.shape[0], selected_mask.shape[1])),\n            \"float32\",\n        )\n        return selected_mask\n\n    # hard example\n    neg_score = score[(gt_text <= 0.5) & (training_mask > 0.5)]\n    neg_score_sorted = paddle.sort(-neg_score)\n    threshold = -neg_score_sorted[neg_num - 1]\n\n    selected_mask = ((score >= threshold) | (gt_text > 0.5)) & (training_mask > 0.5)\n    selected_mask = paddle.cast(\n        selected_mask.reshape((1, selected_mask.shape[0], selected_mask.shape[1])),\n        \"float32\",\n    )\n    return selected_mask\n\n\ndef ohem_batch(scores, gt_texts, training_masks):\n    selected_masks = []\n    for i in range(scores.shape[0]):\n        selected_masks.append(\n            ohem_single(scores[i, :, :], gt_texts[i, :, :], training_masks[i, :, :])\n        )\n\n    selected_masks = paddle.cast(paddle.concat(selected_masks, 0), \"float32\")\n    return selected_masks\n\n\ndef iou_single(a, b, mask, n_class):\n    EPS = 1e-6\n    valid = mask == 1\n    a = a[valid]\n    b = b[valid]\n    miou = []\n\n    # iou of each class\n    for i in range(n_class):\n        inter = paddle.cast(((a == i) & (b == i)), \"float32\")\n        union = paddle.cast(((a == i) | (b == i)), \"float32\")\n\n        miou.append(paddle.sum(inter) / (paddle.sum(union) + EPS))\n    miou = sum(miou) / len(miou)\n    return miou\n\n\ndef iou(a, b, mask, n_class=2, reduce=True):\n    batch_size = a.shape[0]\n\n    a = a.reshape((batch_size, -1))\n    b = b.reshape((batch_size, -1))\n    mask = mask.reshape((batch_size, -1))\n\n    iou = paddle.zeros((batch_size,), dtype=\"float32\")\n    for i in range(batch_size):\n        iou[i] = iou_single(a[i], b[i], mask[i], n_class)\n\n    if reduce:\n        iou = paddle.mean(iou)\n    return iou\n\n\nclass DiceLoss(nn.Layer):\n    def __init__(self, loss_weight=1.0):\n        super(DiceLoss, self).__init__()\n        self.loss_weight = loss_weight\n\n    def forward(self, input, target, mask, reduce=True):\n        batch_size = input.shape[0]\n        input = F.sigmoid(input)  # scale to 0-1\n\n        input = input.reshape((batch_size, -1))\n        target = paddle.cast(target.reshape((batch_size, -1)), \"float32\")\n        mask = paddle.cast(mask.reshape((batch_size, -1)), \"float32\")\n\n        input = input * mask\n        target = target * mask\n\n        a = paddle.sum(input * target, axis=1)\n        b = paddle.sum(input * input, axis=1) + 0.001\n        c = paddle.sum(target * target, axis=1) + 0.001\n        d = (2 * a) / (b + c)\n        loss = 1 - d\n\n        loss = self.loss_weight * loss\n\n        if reduce:\n            loss = paddle.mean(loss)\n\n        return loss\n\n\nclass SmoothL1Loss(nn.Layer):\n    def __init__(self, beta=1.0, loss_weight=1.0):\n        super(SmoothL1Loss, self).__init__()\n        self.beta = beta\n        self.loss_weight = loss_weight\n\n        np_coord = np.zeros(shape=[640, 640, 2], dtype=np.int64)\n        for i in range(640):\n            for j in range(640):\n                np_coord[i, j, 0] = j\n                np_coord[i, j, 1] = i\n        np_coord = np_coord.reshape((-1, 2))\n\n        self.coord = self.create_parameter(\n            shape=[640 * 640, 2],\n            dtype=\"int32\",  # NOTE: not support \"int64\" before paddle 2.3.1\n            default_initializer=nn.initializer.Assign(value=np_coord),\n        )\n        self.coord.stop_gradient = True\n\n    def forward_single(self, input, target, mask, beta=1.0, eps=1e-6):\n        batch_size = input.shape[0]\n\n        diff = paddle.abs(input - target) * mask.unsqueeze(1)\n        loss = paddle.where(diff < beta, 0.5 * diff * diff / beta, diff - 0.5 * beta)\n        loss = paddle.cast(loss.reshape((batch_size, -1)), \"float32\")\n        mask = paddle.cast(mask.reshape((batch_size, -1)), \"float32\")\n        loss = paddle.sum(loss, axis=-1)\n        loss = loss / (mask.sum(axis=-1) + eps)\n\n        return loss\n\n    def select_single(self, distance, gt_instance, gt_kernel_instance, training_mask):\n        with paddle.no_grad():\n            # paddle 2.3.1, paddle.slice not support:\n            # distance[:, self.coord[:, 1], self.coord[:, 0]]\n            select_distance_list = []\n            for i in range(2):\n                tmp1 = distance[i, :]\n                tmp2 = tmp1[self.coord[:, 1], self.coord[:, 0]]\n                select_distance_list.append(tmp2.unsqueeze(0))\n            select_distance = paddle.concat(select_distance_list, axis=0)\n\n            off_points = paddle.cast(\n                self.coord, \"float32\"\n            ) + 10 * select_distance.transpose((1, 0))\n\n            off_points = paddle.cast(off_points, \"int64\")\n            off_points = paddle.clip(off_points, 0, distance.shape[-1] - 1)\n\n            selected_mask = (\n                gt_instance[self.coord[:, 1], self.coord[:, 0]]\n                != gt_kernel_instance[off_points[:, 1], off_points[:, 0]]\n            )\n            selected_mask = paddle.cast(\n                selected_mask.reshape((1, -1, distance.shape[-1])), \"int64\"\n            )\n            selected_training_mask = selected_mask * training_mask\n\n            return selected_training_mask\n\n    def forward(\n        self,\n        distances,\n        gt_instances,\n        gt_kernel_instances,\n        training_masks,\n        gt_distances,\n        reduce=True,\n    ):\n        selected_training_masks = []\n        for i in range(distances.shape[0]):\n            selected_training_masks.append(\n                self.select_single(\n                    distances[i, :, :, :],\n                    gt_instances[i, :, :],\n                    gt_kernel_instances[i, :, :],\n                    training_masks[i, :, :],\n                )\n            )\n        selected_training_masks = paddle.cast(\n            paddle.concat(selected_training_masks, 0), \"float32\"\n        )\n\n        loss = self.forward_single(\n            distances, gt_distances, selected_training_masks, self.beta\n        )\n        loss = self.loss_weight * loss\n\n        with paddle.no_grad():\n            batch_size = distances.shape[0]\n            false_num = selected_training_masks.reshape((batch_size, -1))\n            false_num = false_num.sum(axis=-1)\n            total_num = paddle.cast(training_masks.reshape((batch_size, -1)), \"float32\")\n            total_num = total_num.sum(axis=-1)\n            iou_text = (total_num - false_num) / (total_num + 1e-6)\n\n        if reduce:\n            loss = paddle.mean(loss)\n\n        return loss, iou_text\n\n\nclass CTLoss(nn.Layer):\n    def __init__(self):\n        super(CTLoss, self).__init__()\n        self.kernel_loss = DiceLoss()\n        self.loc_loss = SmoothL1Loss(beta=0.1, loss_weight=0.05)\n\n    def forward(self, preds, batch):\n        imgs = batch[0]\n        out = preds[\"maps\"]\n        (\n            gt_kernels,\n            training_masks,\n            gt_instances,\n            gt_kernel_instances,\n            training_mask_distances,\n            gt_distances,\n        ) = batch[1:]\n\n        kernels = out[:, 0, :, :]\n        distances = out[:, 1:, :, :]\n\n        # kernel loss\n        selected_masks = ohem_batch(kernels, gt_kernels, training_masks)\n\n        loss_kernel = self.kernel_loss(\n            kernels, gt_kernels, selected_masks, reduce=False\n        )\n\n        iou_kernel = iou(\n            paddle.cast((kernels > 0), \"int64\"),\n            gt_kernels,\n            training_masks,\n            reduce=False,\n        )\n        losses = dict(\n            loss_kernels=loss_kernel,\n        )\n\n        # loc loss\n        loss_loc, iou_text = self.loc_loss(\n            distances,\n            gt_instances,\n            gt_kernel_instances,\n            training_mask_distances,\n            gt_distances,\n            reduce=False,\n        )\n        losses.update(\n            dict(\n                loss_loc=loss_loc,\n            )\n        )\n\n        loss_all = loss_kernel + loss_loc\n        losses = {\"loss\": loss_all}\n\n        return losses\n", "ppocr/losses/rec_enhanced_ctc_loss.py": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nfrom paddle import nn\nfrom .ace_loss import ACELoss\nfrom .center_loss import CenterLoss\nfrom .rec_ctc_loss import CTCLoss\n\n\nclass EnhancedCTCLoss(nn.Layer):\n    def __init__(\n        self,\n        use_focal_loss=False,\n        use_ace_loss=False,\n        ace_loss_weight=0.1,\n        use_center_loss=False,\n        center_loss_weight=0.05,\n        num_classes=6625,\n        feat_dim=96,\n        init_center=False,\n        center_file_path=None,\n        **kwargs,\n    ):\n        super(EnhancedCTCLoss, self).__init__()\n        self.ctc_loss_func = CTCLoss(use_focal_loss=use_focal_loss)\n\n        self.use_ace_loss = False\n        if use_ace_loss:\n            self.use_ace_loss = use_ace_loss\n            self.ace_loss_func = ACELoss()\n            self.ace_loss_weight = ace_loss_weight\n\n        self.use_center_loss = False\n        if use_center_loss:\n            self.use_center_loss = use_center_loss\n            self.center_loss_func = CenterLoss(\n                num_classes=num_classes,\n                feat_dim=feat_dim,\n                init_center=init_center,\n                center_file_path=center_file_path,\n            )\n            self.center_loss_weight = center_loss_weight\n\n    def __call__(self, predicts, batch):\n        loss = self.ctc_loss_func(predicts, batch)[\"loss\"]\n\n        if self.use_center_loss:\n            center_loss = (\n                self.center_loss_func(predicts, batch)[\"loss_center\"]\n                * self.center_loss_weight\n            )\n            loss = loss + center_loss\n\n        if self.use_ace_loss:\n            ace_loss = (\n                self.ace_loss_func(predicts, batch)[\"loss_ace\"] * self.ace_loss_weight\n            )\n            loss = loss + ace_loss\n\n        return {\"enhanced_ctc_loss\": loss}\n", "ppocr/losses/text_focus_loss.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/FudanVI/FudanOCR/blob/main/scene-text-telescope/loss/text_focus_loss.py\n\"\"\"\n\nimport paddle.nn as nn\nimport paddle\nimport numpy as np\nimport pickle as pkl\n\nstandard_alphebet = \"-0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\nstandard_dict = {}\nfor index in range(len(standard_alphebet)):\n    standard_dict[standard_alphebet[index]] = index\n\n\ndef load_confuse_matrix(confuse_dict_path):\n    f = open(confuse_dict_path, \"rb\")\n    data = pkl.load(f)\n    f.close()\n    number = data[:10]\n    upper = data[10:36]\n    lower = data[36:]\n    end = np.ones((1, 62))\n    pad = np.ones((63, 1))\n    rearrange_data = np.concatenate((end, number, lower, upper), axis=0)\n    rearrange_data = np.concatenate((pad, rearrange_data), axis=1)\n    rearrange_data = 1 / rearrange_data\n    rearrange_data[rearrange_data == np.inf] = 1\n    rearrange_data = paddle.to_tensor(rearrange_data)\n\n    lower_alpha = \"abcdefghijklmnopqrstuvwxyz\"\n    # upper_alpha = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n    for i in range(63):\n        for j in range(63):\n            if i != j and standard_alphebet[j] in lower_alpha:\n                rearrange_data[i][j] = max(\n                    rearrange_data[i][j], rearrange_data[i][j + 26]\n                )\n    rearrange_data = rearrange_data[:37, :37]\n\n    return rearrange_data\n\n\ndef weight_cross_entropy(pred, gt, weight_table):\n    batch = gt.shape[0]\n    weight = weight_table[gt]\n    pred_exp = paddle.exp(pred)\n    pred_exp_weight = weight * pred_exp\n    loss = 0\n    for i in range(len(gt)):\n        loss -= paddle.log(\n            pred_exp_weight[i][gt[i]] / paddle.sum(pred_exp_weight, 1)[i]\n        )\n    return loss / batch\n\n\nclass TelescopeLoss(nn.Layer):\n    def __init__(self, confuse_dict_path):\n        super(TelescopeLoss, self).__init__()\n        self.weight_table = load_confuse_matrix(confuse_dict_path)\n        self.mse_loss = nn.MSELoss()\n        self.ce_loss = nn.CrossEntropyLoss()\n        self.l1_loss = nn.L1Loss()\n\n    def forward(self, pred, data):\n        sr_img = pred[\"sr_img\"]\n        hr_img = pred[\"hr_img\"]\n        sr_pred = pred[\"sr_pred\"]\n        text_gt = pred[\"text_gt\"]\n\n        word_attention_map_gt = pred[\"word_attention_map_gt\"]\n        word_attention_map_pred = pred[\"word_attention_map_pred\"]\n        mse_loss = self.mse_loss(sr_img, hr_img)\n        attention_loss = self.l1_loss(word_attention_map_gt, word_attention_map_pred)\n        recognition_loss = weight_cross_entropy(sr_pred, text_gt, self.weight_table)\n        loss = mse_loss + attention_loss * 10 + recognition_loss * 0.0005\n        return {\"mse_loss\": mse_loss, \"attention_loss\": attention_loss, \"loss\": loss}\n", "ppocr/losses/rec_ctc_loss.py": "# copyright (c) 2019 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nfrom paddle import nn\n\n\nclass CTCLoss(nn.Layer):\n    def __init__(self, use_focal_loss=False, **kwargs):\n        super(CTCLoss, self).__init__()\n        self.loss_func = nn.CTCLoss(blank=0, reduction=\"none\")\n        self.use_focal_loss = use_focal_loss\n\n    def forward(self, predicts, batch):\n        if isinstance(predicts, (list, tuple)):\n            predicts = predicts[-1]\n        predicts = predicts.transpose((1, 0, 2))\n        N, B, _ = predicts.shape\n        preds_lengths = paddle.to_tensor(\n            [N] * B, dtype=\"int64\", place=paddle.CPUPlace()\n        )\n        labels = batch[1].astype(\"int32\")\n        label_lengths = batch[2].astype(\"int64\")\n        loss = self.loss_func(predicts, labels, preds_lengths, label_lengths)\n        if self.use_focal_loss:\n            weight = paddle.exp(-loss)\n            weight = paddle.subtract(paddle.to_tensor([1.0]), weight)\n            weight = paddle.square(weight)\n            loss = paddle.multiply(loss, weight)\n        loss = loss.mean()\n        return {\"loss\": loss}\n", "ppocr/losses/rec_srn_loss.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nfrom paddle import nn\n\n\nclass SRNLoss(nn.Layer):\n    def __init__(self, **kwargs):\n        super(SRNLoss, self).__init__()\n        self.loss_func = paddle.nn.loss.CrossEntropyLoss(reduction=\"sum\")\n\n    def forward(self, predicts, batch):\n        predict = predicts[\"predict\"]\n        word_predict = predicts[\"word_out\"]\n        gsrm_predict = predicts[\"gsrm_out\"]\n        label = batch[1]\n\n        casted_label = paddle.cast(x=label, dtype=\"int64\")\n        casted_label = paddle.reshape(x=casted_label, shape=[-1, 1])\n\n        cost_word = self.loss_func(word_predict, label=casted_label)\n        cost_gsrm = self.loss_func(gsrm_predict, label=casted_label)\n        cost_vsfd = self.loss_func(predict, label=casted_label)\n\n        cost_word = paddle.reshape(x=paddle.sum(cost_word), shape=[1])\n        cost_gsrm = paddle.reshape(x=paddle.sum(cost_gsrm), shape=[1])\n        cost_vsfd = paddle.reshape(x=paddle.sum(cost_vsfd), shape=[1])\n\n        sum_cost = cost_word * 3.0 + cost_vsfd + cost_gsrm * 0.15\n\n        return {\"loss\": sum_cost, \"word_loss\": cost_word, \"img_loss\": cost_vsfd}\n", "ppocr/losses/cls_loss.py": "# copyright (c) 2019 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom paddle import nn\n\n\nclass ClsLoss(nn.Layer):\n    def __init__(self, **kwargs):\n        super(ClsLoss, self).__init__()\n        self.loss_func = nn.CrossEntropyLoss(reduction=\"mean\")\n\n    def forward(self, predicts, batch):\n        label = batch[1].astype(\"int64\")\n        loss = self.loss_func(input=predicts, label=label)\n        return {\"loss\": loss}\n", "ppocr/losses/det_fce_loss.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/open-mmlab/mmocr/blob/main/mmocr/models/textdet/losses/fce_loss.py\n\"\"\"\n\nimport numpy as np\nfrom paddle import nn\nimport paddle\nimport paddle.nn.functional as F\nfrom functools import partial\n\n\ndef multi_apply(func, *args, **kwargs):\n    pfunc = partial(func, **kwargs) if kwargs else func\n    map_results = map(pfunc, *args)\n    return tuple(map(list, zip(*map_results)))\n\n\nclass FCELoss(nn.Layer):\n    \"\"\"The class for implementing FCENet loss\n    FCENet(CVPR2021): Fourier Contour Embedding for Arbitrary-shaped\n        Text Detection\n\n    [https://arxiv.org/abs/2104.10442]\n\n    Args:\n        fourier_degree (int) : The maximum Fourier transform degree k.\n        num_sample (int) : The sampling points number of regression\n            loss. If it is too small, fcenet tends to be overfitting.\n        ohem_ratio (float): the negative/positive ratio in OHEM.\n    \"\"\"\n\n    def __init__(self, fourier_degree, num_sample, ohem_ratio=3.0):\n        super().__init__()\n        self.fourier_degree = fourier_degree\n        self.num_sample = num_sample\n        self.ohem_ratio = ohem_ratio\n\n    def forward(self, preds, labels):\n        assert isinstance(preds, dict)\n        preds = preds[\"levels\"]\n\n        p3_maps, p4_maps, p5_maps = labels[1:]\n        assert (\n            p3_maps[0].shape[0] == 4 * self.fourier_degree + 5\n        ), \"fourier degree not equal in FCEhead and FCEtarget\"\n\n        # to tensor\n        gts = [p3_maps, p4_maps, p5_maps]\n        for idx, maps in enumerate(gts):\n            gts[idx] = paddle.to_tensor(np.stack(maps))\n\n        losses = multi_apply(self.forward_single, preds, gts)\n\n        loss_tr = paddle.to_tensor(0.0).astype(\"float32\")\n        loss_tcl = paddle.to_tensor(0.0).astype(\"float32\")\n        loss_reg_x = paddle.to_tensor(0.0).astype(\"float32\")\n        loss_reg_y = paddle.to_tensor(0.0).astype(\"float32\")\n        loss_all = paddle.to_tensor(0.0).astype(\"float32\")\n\n        for idx, loss in enumerate(losses):\n            loss_all += sum(loss)\n            if idx == 0:\n                loss_tr += sum(loss)\n            elif idx == 1:\n                loss_tcl += sum(loss)\n            elif idx == 2:\n                loss_reg_x += sum(loss)\n            else:\n                loss_reg_y += sum(loss)\n\n        results = dict(\n            loss=loss_all,\n            loss_text=loss_tr,\n            loss_center=loss_tcl,\n            loss_reg_x=loss_reg_x,\n            loss_reg_y=loss_reg_y,\n        )\n        return results\n\n    def forward_single(self, pred, gt):\n        cls_pred = paddle.transpose(pred[0], (0, 2, 3, 1))\n        reg_pred = paddle.transpose(pred[1], (0, 2, 3, 1))\n        gt = paddle.transpose(gt, (0, 2, 3, 1))\n\n        k = 2 * self.fourier_degree + 1\n        tr_pred = paddle.reshape(cls_pred[:, :, :, :2], (-1, 2))\n        tcl_pred = paddle.reshape(cls_pred[:, :, :, 2:], (-1, 2))\n        x_pred = paddle.reshape(reg_pred[:, :, :, 0:k], (-1, k))\n        y_pred = paddle.reshape(reg_pred[:, :, :, k : 2 * k], (-1, k))\n\n        tr_mask = gt[:, :, :, :1].reshape([-1])\n        tcl_mask = gt[:, :, :, 1:2].reshape([-1])\n        train_mask = gt[:, :, :, 2:3].reshape([-1])\n        x_map = paddle.reshape(gt[:, :, :, 3 : 3 + k], (-1, k))\n        y_map = paddle.reshape(gt[:, :, :, 3 + k :], (-1, k))\n\n        tr_train_mask = (train_mask * tr_mask).astype(\"bool\")\n        tr_train_mask2 = paddle.concat(\n            [tr_train_mask.unsqueeze(1), tr_train_mask.unsqueeze(1)], axis=1\n        )\n        # tr loss\n        loss_tr = self.ohem(tr_pred, tr_mask, train_mask)\n        # tcl loss\n        loss_tcl = paddle.to_tensor(0.0).astype(\"float32\")\n        tr_neg_mask = tr_train_mask.logical_not()\n        tr_neg_mask2 = paddle.concat(\n            [tr_neg_mask.unsqueeze(1), tr_neg_mask.unsqueeze(1)], axis=1\n        )\n        if tr_train_mask.sum().item() > 0:\n            loss_tcl_pos = F.cross_entropy(\n                tcl_pred.masked_select(tr_train_mask2).reshape([-1, 2]),\n                tcl_mask.masked_select(tr_train_mask).astype(\"int64\"),\n            )\n            loss_tcl_neg = F.cross_entropy(\n                tcl_pred.masked_select(tr_neg_mask2).reshape([-1, 2]),\n                tcl_mask.masked_select(tr_neg_mask).astype(\"int64\"),\n            )\n            loss_tcl = loss_tcl_pos + 0.5 * loss_tcl_neg\n\n        # regression loss\n        loss_reg_x = paddle.to_tensor(0.0).astype(\"float32\")\n        loss_reg_y = paddle.to_tensor(0.0).astype(\"float32\")\n        if tr_train_mask.sum().item() > 0:\n            weight = (\n                tr_mask.masked_select(tr_train_mask.astype(\"bool\")).astype(\"float32\")\n                + tcl_mask.masked_select(tr_train_mask.astype(\"bool\")).astype(\"float32\")\n            ) / 2\n            weight = weight.reshape([-1, 1])\n\n            ft_x, ft_y = self.fourier2poly(x_map, y_map)\n            ft_x_pre, ft_y_pre = self.fourier2poly(x_pred, y_pred)\n\n            dim = ft_x.shape[1]\n\n            tr_train_mask3 = paddle.concat(\n                [tr_train_mask.unsqueeze(1) for i in range(dim)], axis=1\n            )\n\n            loss_reg_x = paddle.mean(\n                weight\n                * F.smooth_l1_loss(\n                    ft_x_pre.masked_select(tr_train_mask3).reshape([-1, dim]),\n                    ft_x.masked_select(tr_train_mask3).reshape([-1, dim]),\n                    reduction=\"none\",\n                )\n            )\n            loss_reg_y = paddle.mean(\n                weight\n                * F.smooth_l1_loss(\n                    ft_y_pre.masked_select(tr_train_mask3).reshape([-1, dim]),\n                    ft_y.masked_select(tr_train_mask3).reshape([-1, dim]),\n                    reduction=\"none\",\n                )\n            )\n\n        return loss_tr, loss_tcl, loss_reg_x, loss_reg_y\n\n    def ohem(self, predict, target, train_mask):\n        pos = (target * train_mask).astype(\"bool\")\n        neg = ((1 - target) * train_mask).astype(\"bool\")\n\n        pos2 = paddle.concat([pos.unsqueeze(1), pos.unsqueeze(1)], axis=1)\n        neg2 = paddle.concat([neg.unsqueeze(1), neg.unsqueeze(1)], axis=1)\n\n        n_pos = pos.astype(\"float32\").sum()\n\n        if n_pos.item() > 0:\n            loss_pos = F.cross_entropy(\n                predict.masked_select(pos2).reshape([-1, 2]),\n                target.masked_select(pos).astype(\"int64\"),\n                reduction=\"sum\",\n            )\n            loss_neg = F.cross_entropy(\n                predict.masked_select(neg2).reshape([-1, 2]),\n                target.masked_select(neg).astype(\"int64\"),\n                reduction=\"none\",\n            )\n            n_neg = min(\n                int(neg.astype(\"float32\").sum().item()),\n                int(self.ohem_ratio * n_pos.astype(\"float32\")),\n            )\n        else:\n            loss_pos = paddle.to_tensor(0.0)\n            loss_neg = F.cross_entropy(\n                predict.masked_select(neg2).reshape([-1, 2]),\n                target.masked_select(neg).astype(\"int64\"),\n                reduction=\"none\",\n            )\n            n_neg = 100\n        if len(loss_neg) > n_neg:\n            loss_neg, _ = paddle.topk(loss_neg, n_neg)\n\n        return (loss_pos + loss_neg.sum()) / (n_pos + n_neg).astype(\"float32\")\n\n    def fourier2poly(self, real_maps, imag_maps):\n        \"\"\"Transform Fourier coefficient maps to polygon maps.\n\n        Args:\n            real_maps (tensor): A map composed of the real parts of the\n                Fourier coefficients, whose shape is (-1, 2k+1)\n            imag_maps (tensor):A map composed of the imag parts of the\n                Fourier coefficients, whose shape is (-1, 2k+1)\n\n        Returns\n            x_maps (tensor): A map composed of the x value of the polygon\n                represented by n sample points (xn, yn), whose shape is (-1, n)\n            y_maps (tensor): A map composed of the y value of the polygon\n                represented by n sample points (xn, yn), whose shape is (-1, n)\n        \"\"\"\n\n        k_vect = paddle.arange(\n            -self.fourier_degree, self.fourier_degree + 1, dtype=\"float32\"\n        ).reshape([-1, 1])\n        i_vect = paddle.arange(0, self.num_sample, dtype=\"float32\").reshape([1, -1])\n\n        transform_matrix = 2 * np.pi / self.num_sample * paddle.matmul(k_vect, i_vect)\n\n        x1 = paddle.einsum(\"ak, kn-> an\", real_maps, paddle.cos(transform_matrix))\n        x2 = paddle.einsum(\"ak, kn-> an\", imag_maps, paddle.sin(transform_matrix))\n        y1 = paddle.einsum(\"ak, kn-> an\", real_maps, paddle.sin(transform_matrix))\n        y2 = paddle.einsum(\"ak, kn-> an\", imag_maps, paddle.cos(transform_matrix))\n\n        x_maps = x1 - x2\n        y_maps = y1 + y2\n\n        return x_maps, y_maps\n", "ppocr/losses/table_master_loss.py": "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/JiaquanYe/TableMASTER-mmocr/tree/master/mmocr/models/textrecog/losses\n\"\"\"\n\nimport paddle\nfrom paddle import nn\n\n\nclass TableMasterLoss(nn.Layer):\n    def __init__(self, ignore_index=-1):\n        super(TableMasterLoss, self).__init__()\n        self.structure_loss = nn.CrossEntropyLoss(\n            ignore_index=ignore_index, reduction=\"mean\"\n        )\n        self.box_loss = nn.L1Loss(reduction=\"sum\")\n        self.eps = 1e-12\n\n    def forward(self, predicts, batch):\n        # structure_loss\n        structure_probs = predicts[\"structure_probs\"]\n        structure_targets = batch[1]\n        structure_targets = structure_targets[:, 1:]\n        structure_probs = structure_probs.reshape([-1, structure_probs.shape[-1]])\n        structure_targets = structure_targets.reshape([-1])\n\n        structure_loss = self.structure_loss(structure_probs, structure_targets)\n        structure_loss = structure_loss.mean()\n        losses = dict(structure_loss=structure_loss)\n\n        # box loss\n        bboxes_preds = predicts[\"loc_preds\"]\n        bboxes_targets = batch[2][:, 1:, :]\n        bbox_masks = batch[3][:, 1:]\n        # mask empty-bbox or non-bbox structure token's bbox.\n\n        masked_bboxes_preds = bboxes_preds * bbox_masks\n        masked_bboxes_targets = bboxes_targets * bbox_masks\n\n        # horizon loss (x and width)\n        horizon_sum_loss = self.box_loss(\n            masked_bboxes_preds[:, :, 0::2], masked_bboxes_targets[:, :, 0::2]\n        )\n        horizon_loss = horizon_sum_loss / (bbox_masks.sum() + self.eps)\n        # vertical loss (y and height)\n        vertical_sum_loss = self.box_loss(\n            masked_bboxes_preds[:, :, 1::2], masked_bboxes_targets[:, :, 1::2]\n        )\n        vertical_loss = vertical_sum_loss / (bbox_masks.sum() + self.eps)\n\n        horizon_loss = horizon_loss.mean()\n        vertical_loss = vertical_loss.mean()\n        all_loss = structure_loss + horizon_loss + vertical_loss\n        losses.update(\n            {\n                \"loss\": all_loss,\n                \"horizon_bbox_loss\": horizon_loss,\n                \"vertical_bbox_loss\": vertical_loss,\n            }\n        )\n        return losses\n", "ppocr/losses/rec_aster_loss.py": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nfrom paddle import nn\n\n\nclass CosineEmbeddingLoss(nn.Layer):\n    def __init__(self, margin=0.0):\n        super(CosineEmbeddingLoss, self).__init__()\n        self.margin = margin\n        self.epsilon = 1e-12\n\n    def forward(self, x1, x2, target):\n        similarity = paddle.sum(x1 * x2, axis=-1) / (\n            paddle.norm(x1, axis=-1) * paddle.norm(x2, axis=-1) + self.epsilon\n        )\n        one_list = paddle.full_like(target, fill_value=1)\n        out = paddle.mean(\n            paddle.where(\n                paddle.equal(target, one_list),\n                1.0 - similarity,\n                paddle.maximum(paddle.zeros_like(similarity), similarity - self.margin),\n            )\n        )\n\n        return out\n\n\nclass AsterLoss(nn.Layer):\n    def __init__(\n        self,\n        weight=None,\n        size_average=True,\n        ignore_index=-100,\n        sequence_normalize=False,\n        sample_normalize=True,\n        **kwargs,\n    ):\n        super(AsterLoss, self).__init__()\n        self.weight = weight\n        self.size_average = size_average\n        self.ignore_index = ignore_index\n        self.sequence_normalize = sequence_normalize\n        self.sample_normalize = sample_normalize\n        self.loss_sem = CosineEmbeddingLoss()\n        self.is_cosin_loss = True\n        self.loss_func_rec = nn.CrossEntropyLoss(weight=None, reduction=\"none\")\n\n    def forward(self, predicts, batch):\n        targets = batch[1].astype(\"int64\")\n        label_lengths = batch[2].astype(\"int64\")\n        sem_target = batch[3].astype(\"float32\")\n        embedding_vectors = predicts[\"embedding_vectors\"]\n        rec_pred = predicts[\"rec_pred\"]\n\n        if not self.is_cosin_loss:\n            sem_loss = paddle.sum(self.loss_sem(embedding_vectors, sem_target))\n        else:\n            label_target = paddle.ones([embedding_vectors.shape[0]])\n            sem_loss = paddle.sum(\n                self.loss_sem(embedding_vectors, sem_target, label_target)\n            )\n\n        # rec loss\n        batch_size, def_max_length = targets.shape[0], targets.shape[1]\n\n        mask = paddle.zeros([batch_size, def_max_length])\n        for i in range(batch_size):\n            mask[i, : label_lengths[i]] = 1\n        mask = paddle.cast(mask, \"float32\")\n        max_length = max(label_lengths)\n        assert max_length == rec_pred.shape[1]\n        targets = targets[:, :max_length]\n        mask = mask[:, :max_length]\n        rec_pred = paddle.reshape(rec_pred, [-1, rec_pred.shape[2]])\n        input = nn.functional.log_softmax(rec_pred, axis=1)\n        targets = paddle.reshape(targets, [-1, 1])\n        mask = paddle.reshape(mask, [-1, 1])\n        output = -paddle.index_sample(input, index=targets) * mask\n        output = paddle.sum(output)\n        if self.sequence_normalize:\n            output = output / paddle.sum(mask)\n        if self.sample_normalize:\n            output = output / batch_size\n\n        loss = output + sem_loss * 0.1\n        return {\"loss\": loss}\n", "ppocr/losses/kie_sdmgr_loss.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# reference from : https://github.com/open-mmlab/mmocr/blob/main/mmocr/models/kie/losses/sdmgr_loss.py\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom paddle import nn\nimport paddle\n\n\nclass SDMGRLoss(nn.Layer):\n    def __init__(self, node_weight=1.0, edge_weight=1.0, ignore=0):\n        super().__init__()\n        self.loss_node = nn.CrossEntropyLoss(ignore_index=ignore)\n        self.loss_edge = nn.CrossEntropyLoss(ignore_index=-1)\n        self.node_weight = node_weight\n        self.edge_weight = edge_weight\n        self.ignore = ignore\n\n    def pre_process(self, gts, tag):\n        gts, tag = gts.numpy(), tag.numpy().tolist()\n        temp_gts = []\n        batch = len(tag)\n        for i in range(batch):\n            num, recoder_len = tag[i][0], tag[i][1]\n            temp_gts.append(paddle.to_tensor(gts[i, :num, : num + 1], dtype=\"int64\"))\n        return temp_gts\n\n    def accuracy(self, pred, target, topk=1, thresh=None):\n        \"\"\"Calculate accuracy according to the prediction and target.\n\n        Args:\n            pred (torch.Tensor): The model prediction, shape (N, num_class)\n            target (torch.Tensor): The target of each prediction, shape (N, )\n            topk (int | tuple[int], optional): If the predictions in ``topk``\n                matches the target, the predictions will be regarded as\n                correct ones. Defaults to 1.\n            thresh (float, optional): If not None, predictions with scores under\n                this threshold are considered incorrect. Default to None.\n\n        Returns:\n            float | tuple[float]: If the input ``topk`` is a single integer,\n                the function will return a single float as accuracy. If\n                ``topk`` is a tuple containing multiple integers, the\n                function will return a tuple containing accuracies of\n                each ``topk`` number.\n        \"\"\"\n        assert isinstance(topk, (int, tuple))\n        if isinstance(topk, int):\n            topk = (topk,)\n            return_single = True\n        else:\n            return_single = False\n\n        maxk = max(topk)\n        if pred.shape[0] == 0:\n            accu = [pred.new_tensor(0.0) for i in range(len(topk))]\n            return accu[0] if return_single else accu\n        pred_value, pred_label = paddle.topk(pred, maxk, axis=1)\n        pred_label = pred_label.transpose([1, 0])  # transpose to shape (maxk, N)\n        correct = paddle.equal(\n            pred_label, (target.reshape([1, -1]).expand_as(pred_label))\n        )\n        res = []\n        for k in topk:\n            correct_k = paddle.sum(\n                correct[:k].reshape([-1]).astype(\"float32\"), axis=0, keepdim=True\n            )\n            res.append(\n                paddle.multiply(correct_k, paddle.to_tensor(100.0 / pred.shape[0]))\n            )\n        return res[0] if return_single else res\n\n    def forward(self, pred, batch):\n        node_preds, edge_preds = pred\n        gts, tag = batch[4], batch[5]\n        gts = self.pre_process(gts, tag)\n        node_gts, edge_gts = [], []\n        for gt in gts:\n            node_gts.append(gt[:, 0])\n            edge_gts.append(gt[:, 1:].reshape([-1]))\n        node_gts = paddle.concat(node_gts)\n        edge_gts = paddle.concat(edge_gts)\n\n        node_valids = paddle.nonzero(node_gts != self.ignore).reshape([-1])\n        edge_valids = paddle.nonzero(edge_gts != -1).reshape([-1])\n        loss_node = self.loss_node(node_preds, node_gts)\n        loss_edge = self.loss_edge(edge_preds, edge_gts)\n        loss = self.node_weight * loss_node + self.edge_weight * loss_edge\n        return dict(\n            loss=loss,\n            loss_node=loss_node,\n            loss_edge=loss_edge,\n            acc_node=self.accuracy(\n                paddle.gather(node_preds, node_valids),\n                paddle.gather(node_gts, node_valids),\n            ),\n            acc_edge=self.accuracy(\n                paddle.gather(edge_preds, edge_valids),\n                paddle.gather(edge_gts, edge_valids),\n            ),\n        )\n", "ppocr/losses/table_att_loss.py": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nfrom paddle import nn\nfrom paddle.nn import functional as F\n\n\nclass TableAttentionLoss(nn.Layer):\n    def __init__(self, structure_weight, loc_weight, **kwargs):\n        super(TableAttentionLoss, self).__init__()\n        self.loss_func = nn.CrossEntropyLoss(weight=None, reduction=\"none\")\n        self.structure_weight = structure_weight\n        self.loc_weight = loc_weight\n\n    def forward(self, predicts, batch):\n        structure_probs = predicts[\"structure_probs\"]\n        structure_targets = batch[1].astype(\"int64\")\n        structure_targets = structure_targets[:, 1:]\n        structure_probs = paddle.reshape(\n            structure_probs, [-1, structure_probs.shape[-1]]\n        )\n        structure_targets = paddle.reshape(structure_targets, [-1])\n        structure_loss = self.loss_func(structure_probs, structure_targets)\n\n        structure_loss = paddle.mean(structure_loss) * self.structure_weight\n\n        loc_preds = predicts[\"loc_preds\"]\n        loc_targets = batch[2].astype(\"float32\")\n        loc_targets_mask = batch[3].astype(\"float32\")\n        loc_targets = loc_targets[:, 1:, :]\n        loc_targets_mask = loc_targets_mask[:, 1:, :]\n        loc_loss = (\n            F.mse_loss(loc_preds * loc_targets_mask, loc_targets) * self.loc_weight\n        )\n\n        total_loss = structure_loss + loc_loss\n        return {\n            \"loss\": total_loss,\n            \"structure_loss\": structure_loss,\n            \"loc_loss\": loc_loss,\n        }\n\n\nclass SLALoss(nn.Layer):\n    def __init__(self, structure_weight, loc_weight, loc_loss=\"mse\", **kwargs):\n        super(SLALoss, self).__init__()\n        self.loss_func = nn.CrossEntropyLoss(weight=None, reduction=\"mean\")\n        self.structure_weight = structure_weight\n        self.loc_weight = loc_weight\n        self.loc_loss = loc_loss\n        self.eps = 1e-12\n\n    def forward(self, predicts, batch):\n        structure_probs = predicts[\"structure_probs\"]\n        structure_targets = batch[1].astype(\"int64\")\n        max_len = batch[-2].max()\n        structure_targets = structure_targets[:, 1 : max_len + 2]\n\n        structure_loss = self.loss_func(structure_probs, structure_targets)\n\n        structure_loss = paddle.mean(structure_loss) * self.structure_weight\n\n        loc_preds = predicts[\"loc_preds\"]\n        loc_targets = batch[2].astype(\"float32\")\n        loc_targets_mask = batch[3].astype(\"float32\")\n        loc_targets = loc_targets[:, 1 : max_len + 2]\n        loc_targets_mask = loc_targets_mask[:, 1 : max_len + 2]\n\n        loc_loss = (\n            F.smooth_l1_loss(\n                loc_preds * loc_targets_mask,\n                loc_targets * loc_targets_mask,\n                reduction=\"sum\",\n            )\n            * self.loc_weight\n        )\n\n        loc_loss = loc_loss / (loc_targets_mask.sum() + self.eps)\n        total_loss = structure_loss + loc_loss\n        return {\n            \"loss\": total_loss,\n            \"structure_loss\": structure_loss,\n            \"loc_loss\": loc_loss,\n        }\n", "ppocr/losses/distillation_loss.py": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport paddle\nimport paddle.nn as nn\nimport paddle.nn.functional as F\nimport numpy as np\nimport cv2\n\nfrom .rec_ctc_loss import CTCLoss\nfrom .rec_sar_loss import SARLoss\nfrom .rec_ce_loss import CELoss\nfrom .basic_loss import DMLLoss, KLDivLoss, DKDLoss\nfrom .basic_loss import DistanceLoss\nfrom .basic_loss import LossFromOutput\nfrom .det_db_loss import DBLoss\nfrom .det_basic_loss import BalanceLoss, MaskL1Loss, DiceLoss\nfrom .vqa_token_layoutlm_loss import VQASerTokenLayoutLMLoss\n\n\ndef _sum_loss(loss_dict):\n    if \"loss\" in loss_dict.keys():\n        return loss_dict\n    else:\n        loss_dict[\"loss\"] = 0.0\n        for k, value in loss_dict.items():\n            if k == \"loss\":\n                continue\n            else:\n                loss_dict[\"loss\"] += value\n        return loss_dict\n\n\nclass DistillationDMLLoss(DMLLoss):\n    \"\"\" \"\"\"\n\n    def __init__(\n        self,\n        model_name_pairs=[],\n        act=None,\n        use_log=False,\n        key=None,\n        multi_head=False,\n        dis_head=\"ctc\",\n        maps_name=None,\n        name=\"dml\",\n    ):\n        super().__init__(act=act, use_log=use_log)\n        assert isinstance(model_name_pairs, list)\n        self.key = key\n        self.multi_head = multi_head\n        self.dis_head = dis_head\n        self.model_name_pairs = self._check_model_name_pairs(model_name_pairs)\n        self.name = name\n        self.maps_name = self._check_maps_name(maps_name)\n\n    def _check_model_name_pairs(self, model_name_pairs):\n        if not isinstance(model_name_pairs, list):\n            return []\n        elif isinstance(model_name_pairs[0], list) and isinstance(\n            model_name_pairs[0][0], str\n        ):\n            return model_name_pairs\n        else:\n            return [model_name_pairs]\n\n    def _check_maps_name(self, maps_name):\n        if maps_name is None:\n            return None\n        elif isinstance(maps_name, str):\n            return [maps_name]\n        elif isinstance(maps_name, list):\n            return [maps_name]\n        else:\n            return None\n\n    def _slice_out(self, outs):\n        new_outs = {}\n        for k in self.maps_name:\n            if k == \"thrink_maps\":\n                new_outs[k] = outs[:, 0, :, :]\n            elif k == \"threshold_maps\":\n                new_outs[k] = outs[:, 1, :, :]\n            elif k == \"binary_maps\":\n                new_outs[k] = outs[:, 2, :, :]\n            else:\n                continue\n        return new_outs\n\n    def forward(self, predicts, batch):\n        loss_dict = dict()\n        for idx, pair in enumerate(self.model_name_pairs):\n            out1 = predicts[pair[0]]\n            out2 = predicts[pair[1]]\n            if self.key is not None:\n                out1 = out1[self.key]\n                out2 = out2[self.key]\n            if self.maps_name is None:\n                if self.multi_head:\n                    loss = super().forward(out1[self.dis_head], out2[self.dis_head])\n                else:\n                    loss = super().forward(out1, out2)\n                if isinstance(loss, dict):\n                    for key in loss:\n                        loss_dict[\"{}_{}_{}_{}\".format(key, pair[0], pair[1], idx)] = (\n                            loss[key]\n                        )\n                else:\n                    loss_dict[\"{}_{}\".format(self.name, idx)] = loss\n            else:\n                outs1 = self._slice_out(out1)\n                outs2 = self._slice_out(out2)\n                for _c, k in enumerate(outs1.keys()):\n                    loss = super().forward(outs1[k], outs2[k])\n                    if isinstance(loss, dict):\n                        for key in loss:\n                            loss_dict[\n                                \"{}_{}_{}_{}_{}\".format(\n                                    key, pair[0], pair[1], self.maps_name, idx\n                                )\n                            ] = loss[key]\n                    else:\n                        loss_dict[\n                            \"{}_{}_{}\".format(self.name, self.maps_name[_c], idx)\n                        ] = loss\n\n        loss_dict = _sum_loss(loss_dict)\n\n        return loss_dict\n\n\nclass DistillationKLDivLoss(KLDivLoss):\n    \"\"\" \"\"\"\n\n    def __init__(\n        self,\n        model_name_pairs=[],\n        key=None,\n        multi_head=False,\n        dis_head=\"ctc\",\n        maps_name=None,\n        name=\"kl_div\",\n    ):\n        super().__init__()\n        assert isinstance(model_name_pairs, list)\n        self.key = key\n        self.multi_head = multi_head\n        self.dis_head = dis_head\n        self.model_name_pairs = self._check_model_name_pairs(model_name_pairs)\n        self.name = name\n        self.maps_name = self._check_maps_name(maps_name)\n\n    def _check_model_name_pairs(self, model_name_pairs):\n        if not isinstance(model_name_pairs, list):\n            return []\n        elif isinstance(model_name_pairs[0], list) and isinstance(\n            model_name_pairs[0][0], str\n        ):\n            return model_name_pairs\n        else:\n            return [model_name_pairs]\n\n    def _check_maps_name(self, maps_name):\n        if maps_name is None:\n            return None\n        elif isinstance(maps_name, str):\n            return [maps_name]\n        elif isinstance(maps_name, list):\n            return [maps_name]\n        else:\n            return None\n\n    def _slice_out(self, outs):\n        new_outs = {}\n        for k in self.maps_name:\n            if k == \"thrink_maps\":\n                new_outs[k] = outs[:, 0, :, :]\n            elif k == \"threshold_maps\":\n                new_outs[k] = outs[:, 1, :, :]\n            elif k == \"binary_maps\":\n                new_outs[k] = outs[:, 2, :, :]\n            else:\n                continue\n        return new_outs\n\n    def forward(self, predicts, batch):\n        loss_dict = dict()\n        for idx, pair in enumerate(self.model_name_pairs):\n            out1 = predicts[pair[0]]\n            out2 = predicts[pair[1]]\n            if self.key is not None:\n                out1 = out1[self.key]\n                out2 = out2[self.key]\n            if self.maps_name is None:\n                if self.multi_head:\n                    # for nrtr dml loss\n                    max_len = batch[3].max()\n                    tgt = batch[2][:, 1 : 2 + max_len]\n                    tgt = tgt.reshape([-1])\n                    non_pad_mask = paddle.not_equal(\n                        tgt, paddle.zeros(tgt.shape, dtype=tgt.dtype)\n                    )\n                    loss = super().forward(\n                        out1[self.dis_head], out2[self.dis_head], non_pad_mask\n                    )\n                else:\n                    loss = super().forward(out1, out2)\n                if isinstance(loss, dict):\n                    for key in loss:\n                        loss_dict[\"{}_{}_{}_{}\".format(key, pair[0], pair[1], idx)] = (\n                            loss[key]\n                        )\n                else:\n                    loss_dict[\"{}_{}\".format(self.name, idx)] = loss\n            else:\n                outs1 = self._slice_out(out1)\n                outs2 = self._slice_out(out2)\n                for _c, k in enumerate(outs1.keys()):\n                    loss = super().forward(outs1[k], outs2[k])\n                    if isinstance(loss, dict):\n                        for key in loss:\n                            loss_dict[\n                                \"{}_{}_{}_{}_{}\".format(\n                                    key, pair[0], pair[1], self.maps_name, idx\n                                )\n                            ] = loss[key]\n                    else:\n                        loss_dict[\n                            \"{}_{}_{}\".format(self.name, self.maps_name[_c], idx)\n                        ] = loss\n\n        loss_dict = _sum_loss(loss_dict)\n\n        return loss_dict\n\n\nclass DistillationDKDLoss(DKDLoss):\n    \"\"\" \"\"\"\n\n    def __init__(\n        self,\n        model_name_pairs=[],\n        key=None,\n        multi_head=False,\n        dis_head=\"ctc\",\n        maps_name=None,\n        name=\"dkd\",\n        temperature=1.0,\n        alpha=1.0,\n        beta=1.0,\n    ):\n        super().__init__(temperature, alpha, beta)\n        assert isinstance(model_name_pairs, list)\n        self.key = key\n        self.multi_head = multi_head\n        self.dis_head = dis_head\n        self.model_name_pairs = self._check_model_name_pairs(model_name_pairs)\n        self.name = name\n        self.maps_name = self._check_maps_name(maps_name)\n\n    def _check_model_name_pairs(self, model_name_pairs):\n        if not isinstance(model_name_pairs, list):\n            return []\n        elif isinstance(model_name_pairs[0], list) and isinstance(\n            model_name_pairs[0][0], str\n        ):\n            return model_name_pairs\n        else:\n            return [model_name_pairs]\n\n    def _check_maps_name(self, maps_name):\n        if maps_name is None:\n            return None\n        elif isinstance(maps_name, str):\n            return [maps_name]\n        elif isinstance(maps_name, list):\n            return [maps_name]\n        else:\n            return None\n\n    def _slice_out(self, outs):\n        new_outs = {}\n        for k in self.maps_name:\n            if k == \"thrink_maps\":\n                new_outs[k] = outs[:, 0, :, :]\n            elif k == \"threshold_maps\":\n                new_outs[k] = outs[:, 1, :, :]\n            elif k == \"binary_maps\":\n                new_outs[k] = outs[:, 2, :, :]\n            else:\n                continue\n        return new_outs\n\n    def forward(self, predicts, batch):\n        loss_dict = dict()\n\n        for idx, pair in enumerate(self.model_name_pairs):\n            out1 = predicts[pair[0]]\n            out2 = predicts[pair[1]]\n            if self.key is not None:\n                out1 = out1[self.key]\n                out2 = out2[self.key]\n            if self.maps_name is None:\n                if self.multi_head:\n                    # for nrtr dml loss\n                    max_len = batch[3].max()\n                    tgt = batch[2][:, 1 : 2 + max_len]  # [batch_size, max_len + 1]\n\n                    tgt = tgt.reshape([-1])  # batch_size * (max_len + 1)\n                    non_pad_mask = paddle.not_equal(\n                        tgt, paddle.zeros(tgt.shape, dtype=tgt.dtype)\n                    )  # batch_size * (max_len + 1)\n\n                    loss = super().forward(\n                        out1[self.dis_head], out2[self.dis_head], tgt, non_pad_mask\n                    )  # [batch_size, max_len + 1, num_char]\n                else:\n                    loss = super().forward(out1, out2)\n                if isinstance(loss, dict):\n                    for key in loss:\n                        loss_dict[\"{}_{}_{}_{}\".format(key, pair[0], pair[1], idx)] = (\n                            loss[key]\n                        )\n                else:\n                    loss_dict[\"{}_{}\".format(self.name, idx)] = loss\n            else:\n                outs1 = self._slice_out(out1)\n                outs2 = self._slice_out(out2)\n                for _c, k in enumerate(outs1.keys()):\n                    loss = super().forward(outs1[k], outs2[k])\n                    if isinstance(loss, dict):\n                        for key in loss:\n                            loss_dict[\n                                \"{}_{}_{}_{}_{}\".format(\n                                    key, pair[0], pair[1], self.maps_name, idx\n                                )\n                            ] = loss[key]\n                    else:\n                        loss_dict[\n                            \"{}_{}_{}\".format(self.name, self.maps_name[_c], idx)\n                        ] = loss\n\n        loss_dict = _sum_loss(loss_dict)\n\n        return loss_dict\n\n\nclass DistillationNRTRDMLLoss(DistillationDMLLoss):\n    \"\"\" \"\"\"\n\n    def forward(self, predicts, batch):\n        loss_dict = dict()\n        for idx, pair in enumerate(self.model_name_pairs):\n            out1 = predicts[pair[0]]\n            out2 = predicts[pair[1]]\n            if self.key is not None:\n                out1 = out1[self.key]\n                out2 = out2[self.key]\n\n            if self.multi_head:\n                # for nrtr dml loss\n                max_len = batch[3].max()\n                tgt = batch[2][:, 1 : 2 + max_len]\n                tgt = tgt.reshape([-1])\n                non_pad_mask = paddle.not_equal(\n                    tgt, paddle.zeros(tgt.shape, dtype=tgt.dtype)\n                )\n                loss = super().forward(\n                    out1[self.dis_head], out2[self.dis_head], non_pad_mask\n                )\n            else:\n                loss = super().forward(out1, out2)\n            if isinstance(loss, dict):\n                for key in loss:\n                    loss_dict[\"{}_{}_{}_{}\".format(key, pair[0], pair[1], idx)] = loss[\n                        key\n                    ]\n            else:\n                loss_dict[\"{}_{}\".format(self.name, idx)] = loss\n\n        loss_dict = _sum_loss(loss_dict)\n\n        return loss_dict\n\n\nclass DistillationKLDivLoss(KLDivLoss):\n    \"\"\" \"\"\"\n\n    def __init__(\n        self,\n        model_name_pairs=[],\n        key=None,\n        multi_head=False,\n        dis_head=\"ctc\",\n        maps_name=None,\n        name=\"kl_div\",\n    ):\n        super().__init__()\n        assert isinstance(model_name_pairs, list)\n        self.key = key\n        self.multi_head = multi_head\n        self.dis_head = dis_head\n        self.model_name_pairs = self._check_model_name_pairs(model_name_pairs)\n        self.name = name\n        self.maps_name = self._check_maps_name(maps_name)\n\n    def _check_model_name_pairs(self, model_name_pairs):\n        if not isinstance(model_name_pairs, list):\n            return []\n        elif isinstance(model_name_pairs[0], list) and isinstance(\n            model_name_pairs[0][0], str\n        ):\n            return model_name_pairs\n        else:\n            return [model_name_pairs]\n\n    def _check_maps_name(self, maps_name):\n        if maps_name is None:\n            return None\n        elif isinstance(maps_name, str):\n            return [maps_name]\n        elif isinstance(maps_name, list):\n            return [maps_name]\n        else:\n            return None\n\n    def _slice_out(self, outs):\n        new_outs = {}\n        for k in self.maps_name:\n            if k == \"thrink_maps\":\n                new_outs[k] = outs[:, 0, :, :]\n            elif k == \"threshold_maps\":\n                new_outs[k] = outs[:, 1, :, :]\n            elif k == \"binary_maps\":\n                new_outs[k] = outs[:, 2, :, :]\n            else:\n                continue\n        return new_outs\n\n    def forward(self, predicts, batch):\n        loss_dict = dict()\n        for idx, pair in enumerate(self.model_name_pairs):\n            out1 = predicts[pair[0]]\n            out2 = predicts[pair[1]]\n            if self.key is not None:\n                out1 = out1[self.key]\n                out2 = out2[self.key]\n            if self.maps_name is None:\n                if self.multi_head:\n                    # for nrtr dml loss\n                    max_len = batch[3].max()\n                    tgt = batch[2][:, 1 : 2 + max_len]\n                    tgt = tgt.reshape([-1])\n                    non_pad_mask = paddle.not_equal(\n                        tgt, paddle.zeros(tgt.shape, dtype=tgt.dtype)\n                    )\n                    loss = super().forward(\n                        out1[self.dis_head], out2[self.dis_head], non_pad_mask\n                    )\n                else:\n                    loss = super().forward(out1, out2)\n                if isinstance(loss, dict):\n                    for key in loss:\n                        loss_dict[\"{}_{}_{}_{}\".format(key, pair[0], pair[1], idx)] = (\n                            loss[key]\n                        )\n                else:\n                    loss_dict[\"{}_{}\".format(self.name, idx)] = loss\n            else:\n                outs1 = self._slice_out(out1)\n                outs2 = self._slice_out(out2)\n                for _c, k in enumerate(outs1.keys()):\n                    loss = super().forward(outs1[k], outs2[k])\n                    if isinstance(loss, dict):\n                        for key in loss:\n                            loss_dict[\n                                \"{}_{}_{}_{}_{}\".format(\n                                    key, pair[0], pair[1], self.maps_name, idx\n                                )\n                            ] = loss[key]\n                    else:\n                        loss_dict[\n                            \"{}_{}_{}\".format(self.name, self.maps_name[_c], idx)\n                        ] = loss\n\n        loss_dict = _sum_loss(loss_dict)\n\n        return loss_dict\n\n\nclass DistillationDKDLoss(DKDLoss):\n    \"\"\" \"\"\"\n\n    def __init__(\n        self,\n        model_name_pairs=[],\n        key=None,\n        multi_head=False,\n        dis_head=\"ctc\",\n        maps_name=None,\n        name=\"dkd\",\n        temperature=1.0,\n        alpha=1.0,\n        beta=1.0,\n    ):\n        super().__init__(temperature, alpha, beta)\n        assert isinstance(model_name_pairs, list)\n        self.key = key\n        self.multi_head = multi_head\n        self.dis_head = dis_head\n        self.model_name_pairs = self._check_model_name_pairs(model_name_pairs)\n        self.name = name\n        self.maps_name = self._check_maps_name(maps_name)\n\n    def _check_model_name_pairs(self, model_name_pairs):\n        if not isinstance(model_name_pairs, list):\n            return []\n        elif isinstance(model_name_pairs[0], list) and isinstance(\n            model_name_pairs[0][0], str\n        ):\n            return model_name_pairs\n        else:\n            return [model_name_pairs]\n\n    def _check_maps_name(self, maps_name):\n        if maps_name is None:\n            return None\n        elif isinstance(maps_name, str):\n            return [maps_name]\n        elif isinstance(maps_name, list):\n            return [maps_name]\n        else:\n            return None\n\n    def _slice_out(self, outs):\n        new_outs = {}\n        for k in self.maps_name:\n            if k == \"thrink_maps\":\n                new_outs[k] = outs[:, 0, :, :]\n            elif k == \"threshold_maps\":\n                new_outs[k] = outs[:, 1, :, :]\n            elif k == \"binary_maps\":\n                new_outs[k] = outs[:, 2, :, :]\n            else:\n                continue\n        return new_outs\n\n    def forward(self, predicts, batch):\n        loss_dict = dict()\n\n        for idx, pair in enumerate(self.model_name_pairs):\n            out1 = predicts[pair[0]]\n            out2 = predicts[pair[1]]\n            if self.key is not None:\n                out1 = out1[self.key]\n                out2 = out2[self.key]\n            if self.maps_name is None:\n                if self.multi_head:\n                    # for nrtr dml loss\n                    max_len = batch[3].max()\n                    tgt = batch[2][:, 1 : 2 + max_len]  # [batch_size, max_len + 1]\n\n                    tgt = tgt.reshape([-1])  # batch_size * (max_len + 1)\n                    non_pad_mask = paddle.not_equal(\n                        tgt, paddle.zeros(tgt.shape, dtype=tgt.dtype)\n                    )  # batch_size * (max_len + 1)\n\n                    loss = super().forward(\n                        out1[self.dis_head], out2[self.dis_head], tgt, non_pad_mask\n                    )  # [batch_size, max_len + 1, num_char]\n                else:\n                    loss = super().forward(out1, out2)\n                if isinstance(loss, dict):\n                    for key in loss:\n                        loss_dict[\"{}_{}_{}_{}\".format(key, pair[0], pair[1], idx)] = (\n                            loss[key]\n                        )\n                else:\n                    loss_dict[\"{}_{}\".format(self.name, idx)] = loss\n            else:\n                outs1 = self._slice_out(out1)\n                outs2 = self._slice_out(out2)\n                for _c, k in enumerate(outs1.keys()):\n                    loss = super().forward(outs1[k], outs2[k])\n                    if isinstance(loss, dict):\n                        for key in loss:\n                            loss_dict[\n                                \"{}_{}_{}_{}_{}\".format(\n                                    key, pair[0], pair[1], self.maps_name, idx\n                                )\n                            ] = loss[key]\n                    else:\n                        loss_dict[\n                            \"{}_{}_{}\".format(self.name, self.maps_name[_c], idx)\n                        ] = loss\n\n        loss_dict = _sum_loss(loss_dict)\n\n        return loss_dict\n\n\nclass DistillationCTCLoss(CTCLoss):\n    def __init__(self, model_name_list=[], key=None, multi_head=False, name=\"loss_ctc\"):\n        super().__init__()\n        self.model_name_list = model_name_list\n        self.key = key\n        self.name = name\n        self.multi_head = multi_head\n\n    def forward(self, predicts, batch):\n        loss_dict = dict()\n        for idx, model_name in enumerate(self.model_name_list):\n            out = predicts[model_name]\n            if self.key is not None:\n                out = out[self.key]\n            if self.multi_head:\n                assert \"ctc\" in out, \"multi head has multi out\"\n                loss = super().forward(out[\"ctc\"], batch[:2] + batch[3:])\n            else:\n                loss = super().forward(out, batch)\n            if isinstance(loss, dict):\n                for key in loss:\n                    loss_dict[\"{}_{}_{}\".format(self.name, model_name, idx)] = loss[key]\n            else:\n                loss_dict[\"{}_{}\".format(self.name, model_name)] = loss\n        return loss_dict\n\n\nclass DistillationSARLoss(SARLoss):\n    def __init__(\n        self, model_name_list=[], key=None, multi_head=False, name=\"loss_sar\", **kwargs\n    ):\n        ignore_index = kwargs.get(\"ignore_index\", 92)\n        super().__init__(ignore_index=ignore_index)\n        self.model_name_list = model_name_list\n        self.key = key\n        self.name = name\n        self.multi_head = multi_head\n\n    def forward(self, predicts, batch):\n        loss_dict = dict()\n        for idx, model_name in enumerate(self.model_name_list):\n            out = predicts[model_name]\n            if self.key is not None:\n                out = out[self.key]\n            if self.multi_head:\n                assert \"sar\" in out, \"multi head has multi out\"\n                loss = super().forward(out[\"sar\"], batch[:1] + batch[2:])\n            else:\n                loss = super().forward(out, batch)\n            if isinstance(loss, dict):\n                for key in loss:\n                    loss_dict[\"{}_{}_{}\".format(self.name, model_name, idx)] = loss[key]\n            else:\n                loss_dict[\"{}_{}\".format(self.name, model_name)] = loss\n        return loss_dict\n\n\nclass DistillationNRTRLoss(CELoss):\n    def __init__(\n        self,\n        model_name_list=[],\n        key=None,\n        multi_head=False,\n        smoothing=True,\n        name=\"loss_nrtr\",\n        **kwargs,\n    ):\n        super().__init__(smoothing=smoothing)\n        self.model_name_list = model_name_list\n        self.key = key\n        self.name = name\n        self.multi_head = multi_head\n\n    def forward(self, predicts, batch):\n        loss_dict = dict()\n        for idx, model_name in enumerate(self.model_name_list):\n            out = predicts[model_name]\n            if self.key is not None:\n                out = out[self.key]\n            if self.multi_head:\n                assert \"gtc\" in out, \"multi head has multi out\"\n                loss = super().forward(out[\"gtc\"], batch[:1] + batch[2:])\n            else:\n                loss = super().forward(out, batch)\n            if isinstance(loss, dict):\n                for key in loss:\n                    loss_dict[\"{}_{}_{}\".format(self.name, model_name, idx)] = loss[key]\n            else:\n                loss_dict[\"{}_{}\".format(self.name, model_name)] = loss\n        return loss_dict\n\n\nclass DistillationDBLoss(DBLoss):\n    def __init__(\n        self,\n        model_name_list=[],\n        balance_loss=True,\n        main_loss_type=\"DiceLoss\",\n        alpha=5,\n        beta=10,\n        ohem_ratio=3,\n        eps=1e-6,\n        name=\"db\",\n        **kwargs,\n    ):\n        super().__init__()\n        self.model_name_list = model_name_list\n        self.name = name\n        self.key = None\n\n    def forward(self, predicts, batch):\n        loss_dict = {}\n        for idx, model_name in enumerate(self.model_name_list):\n            out = predicts[model_name]\n            if self.key is not None:\n                out = out[self.key]\n            loss = super().forward(out, batch)\n\n            if isinstance(loss, dict):\n                for key in loss.keys():\n                    if key == \"loss\":\n                        continue\n                    name = \"{}_{}_{}\".format(self.name, model_name, key)\n                    loss_dict[name] = loss[key]\n            else:\n                loss_dict[\"{}_{}\".format(self.name, model_name)] = loss\n\n        loss_dict = _sum_loss(loss_dict)\n        return loss_dict\n\n\nclass DistillationDilaDBLoss(DBLoss):\n    def __init__(\n        self,\n        model_name_pairs=[],\n        key=None,\n        balance_loss=True,\n        main_loss_type=\"DiceLoss\",\n        alpha=5,\n        beta=10,\n        ohem_ratio=3,\n        eps=1e-6,\n        name=\"dila_dbloss\",\n    ):\n        super().__init__()\n        self.model_name_pairs = model_name_pairs\n        self.name = name\n        self.key = key\n\n    def forward(self, predicts, batch):\n        loss_dict = dict()\n        for idx, pair in enumerate(self.model_name_pairs):\n            stu_outs = predicts[pair[0]]\n            tch_outs = predicts[pair[1]]\n            if self.key is not None:\n                stu_preds = stu_outs[self.key]\n                tch_preds = tch_outs[self.key]\n\n            stu_shrink_maps = stu_preds[:, 0, :, :]\n            stu_binary_maps = stu_preds[:, 2, :, :]\n\n            # dilation to teacher prediction\n            dilation_w = np.array([[1, 1], [1, 1]])\n            th_shrink_maps = tch_preds[:, 0, :, :]\n            if hasattr(paddle.Tensor, \"contiguous\"):\n                th_shrink_maps = th_shrink_maps.contiguous()\n            th_shrink_maps = th_shrink_maps.numpy() > 0.3  # thresh = 0.3\n            dilate_maps = np.zeros_like(th_shrink_maps).astype(np.float32)\n            for i in range(th_shrink_maps.shape[0]):\n                dilate_maps[i] = cv2.dilate(\n                    th_shrink_maps[i, :, :].astype(np.uint8), dilation_w\n                )\n            th_shrink_maps = paddle.to_tensor(dilate_maps)\n\n            (\n                label_threshold_map,\n                label_threshold_mask,\n                label_shrink_map,\n                label_shrink_mask,\n            ) = batch[1:]\n\n            # calculate the shrink map loss\n            bce_loss = self.alpha * self.bce_loss(\n                stu_shrink_maps, th_shrink_maps, label_shrink_mask\n            )\n            loss_binary_maps = self.dice_loss(\n                stu_binary_maps, th_shrink_maps, label_shrink_mask\n            )\n\n            # k = f\"{self.name}_{pair[0]}_{pair[1]}\"\n            k = \"{}_{}_{}\".format(self.name, pair[0], pair[1])\n            loss_dict[k] = bce_loss + loss_binary_maps\n\n        loss_dict = _sum_loss(loss_dict)\n        return loss_dict\n\n\nclass DistillationDistanceLoss(DistanceLoss):\n    \"\"\" \"\"\"\n\n    def __init__(\n        self, mode=\"l2\", model_name_pairs=[], key=None, name=\"loss_distance\", **kargs\n    ):\n        super().__init__(mode=mode, **kargs)\n        assert isinstance(model_name_pairs, list)\n        self.key = key\n        self.model_name_pairs = model_name_pairs\n        self.name = name + \"_l2\"\n\n    def forward(self, predicts, batch):\n        loss_dict = dict()\n        for idx, pair in enumerate(self.model_name_pairs):\n            out1 = predicts[pair[0]]\n            out2 = predicts[pair[1]]\n            if self.key is not None:\n                out1 = out1[self.key]\n                out2 = out2[self.key]\n            loss = super().forward(out1, out2)\n            if isinstance(loss, dict):\n                for key in loss:\n                    loss_dict[\"{}_{}_{}\".format(self.name, key, idx)] = loss[key]\n            else:\n                loss_dict[\"{}_{}_{}_{}\".format(self.name, pair[0], pair[1], idx)] = loss\n        return loss_dict\n\n\nclass DistillationVQASerTokenLayoutLMLoss(VQASerTokenLayoutLMLoss):\n    def __init__(self, num_classes, model_name_list=[], key=None, name=\"loss_ser\"):\n        super().__init__(num_classes=num_classes)\n        self.model_name_list = model_name_list\n        self.key = key\n        self.name = name\n\n    def forward(self, predicts, batch):\n        loss_dict = dict()\n        for idx, model_name in enumerate(self.model_name_list):\n            out = predicts[model_name]\n            if self.key is not None:\n                out = out[self.key]\n            loss = super().forward(out, batch)\n            loss_dict[\"{}_{}\".format(self.name, model_name)] = loss[\"loss\"]\n        return loss_dict\n\n\nclass DistillationLossFromOutput(LossFromOutput):\n    def __init__(\n        self,\n        reduction=\"none\",\n        model_name_list=[],\n        dist_key=None,\n        key=\"loss\",\n        name=\"loss_re\",\n    ):\n        super().__init__(key=key, reduction=reduction)\n        self.model_name_list = model_name_list\n        self.name = name\n        self.dist_key = dist_key\n\n    def forward(self, predicts, batch):\n        loss_dict = dict()\n        for idx, model_name in enumerate(self.model_name_list):\n            out = predicts[model_name]\n            if self.dist_key is not None:\n                out = out[self.dist_key]\n            loss = super().forward(out, batch)\n            loss_dict[\"{}_{}\".format(self.name, model_name)] = loss[\"loss\"]\n        return loss_dict\n\n\nclass DistillationSERDMLLoss(DMLLoss):\n    \"\"\" \"\"\"\n\n    def __init__(\n        self,\n        act=\"softmax\",\n        use_log=True,\n        num_classes=7,\n        model_name_pairs=[],\n        key=None,\n        name=\"loss_dml_ser\",\n    ):\n        super().__init__(act=act, use_log=use_log)\n        assert isinstance(model_name_pairs, list)\n        self.key = key\n        self.name = name\n        self.num_classes = num_classes\n        self.model_name_pairs = model_name_pairs\n\n    def forward(self, predicts, batch):\n        loss_dict = dict()\n        for idx, pair in enumerate(self.model_name_pairs):\n            out1 = predicts[pair[0]]\n            out2 = predicts[pair[1]]\n            if self.key is not None:\n                out1 = out1[self.key]\n                out2 = out2[self.key]\n            out1 = out1.reshape([-1, out1.shape[-1]])\n            out2 = out2.reshape([-1, out2.shape[-1]])\n\n            attention_mask = batch[2]\n            if attention_mask is not None:\n                active_output = (\n                    attention_mask.reshape(\n                        [\n                            -1,\n                        ]\n                    )\n                    == 1\n                )\n                out1 = out1[active_output]\n                out2 = out2[active_output]\n\n            loss_dict[\"{}_{}\".format(self.name, idx)] = super().forward(out1, out2)\n\n        return loss_dict\n\n\nclass DistillationVQADistanceLoss(DistanceLoss):\n    def __init__(\n        self,\n        mode=\"l2\",\n        model_name_pairs=[],\n        key=None,\n        index=None,\n        name=\"loss_distance\",\n        **kargs,\n    ):\n        super().__init__(mode=mode, **kargs)\n        assert isinstance(model_name_pairs, list)\n        self.key = key\n        self.index = index\n        self.model_name_pairs = model_name_pairs\n        self.name = name + \"_l2\"\n\n    def forward(self, predicts, batch):\n        loss_dict = dict()\n        for idx, pair in enumerate(self.model_name_pairs):\n            out1 = predicts[pair[0]]\n            out2 = predicts[pair[1]]\n            attention_mask = batch[2]\n            if self.key is not None:\n                out1 = out1[self.key]\n                out2 = out2[self.key]\n                if self.index is not None:\n                    out1 = out1[:, self.index, :, :]\n                    out2 = out2[:, self.index, :, :]\n                if attention_mask is not None:\n                    max_len = attention_mask.shape[-1]\n                    out1 = out1[:, :max_len]\n                    out2 = out2[:, :max_len]\n                out1 = out1.reshape([-1, out1.shape[-1]])\n                out2 = out2.reshape([-1, out2.shape[-1]])\n            if attention_mask is not None:\n                active_output = (\n                    attention_mask.reshape(\n                        [\n                            -1,\n                        ]\n                    )\n                    == 1\n                )\n                out1 = out1[active_output]\n                out2 = out2[active_output]\n\n            loss = super().forward(out1, out2)\n            if isinstance(loss, dict):\n                for key in loss:\n                    loss_dict[\"{}_{}nohu_{}\".format(self.name, key, idx)] = loss[key]\n            else:\n                loss_dict[\"{}_{}_{}_{}\".format(self.name, pair[0], pair[1], idx)] = loss\n        return loss_dict\n\n\nclass CTCDKDLoss(nn.Layer):\n    \"\"\"\n    KLDivLoss\n    \"\"\"\n\n    def __init__(self, temperature=0.5, alpha=1.0, beta=1.0):\n        super().__init__()\n        self.temperature = temperature\n        self.alpha = alpha\n        self.beta = beta\n        self.eps = 1e-6\n        self.t = temperature\n        self.act = nn.Softmax(axis=-1)\n        self.use_log = True\n\n    def kl_loss(self, p1, p2):  # predict, label\n        loss = paddle.multiply(\n            p2, paddle.log((p2 + self.eps) / (p1 + self.eps) + self.eps)\n        )\n        bs = loss.shape[0]\n        loss = paddle.sum(loss) / bs\n        return loss\n\n    def _cat_mask(self, t, mask1, mask2):\n        t1 = (t * mask1).sum(axis=1, keepdim=True)\n        t2 = (t * mask2).sum(axis=1, keepdim=True)\n        rt = paddle.concat([t1, t2], axis=1)\n        return rt\n\n    def multi_label_mask(self, targets):\n        targets = targets.astype(\"int32\")\n        res = F.one_hot(targets, num_classes=11465)\n        mask = paddle.clip(paddle.sum(res, axis=1), 0, 1)\n        mask[:, 0] = 0  # ingore ctc blank label\n        return mask\n\n    def forward(self, logits_student, logits_teacher, targets, mask=None):\n        gt_mask = self.multi_label_mask(targets)\n        other_mask = paddle.ones_like(gt_mask) - gt_mask\n\n        pred_student = F.softmax(logits_student / self.temperature, axis=-1)\n        pred_teacher = F.softmax(logits_teacher / self.temperature, axis=-1)\n\n        # differents with dkd\n        pred_student = paddle.mean(pred_student, axis=1)\n        pred_teacher = paddle.mean(pred_teacher, axis=1)\n\n        pred_student = self._cat_mask(pred_student, gt_mask, other_mask)\n        pred_teacher = self._cat_mask(pred_teacher, gt_mask, other_mask)\n\n        # differents with dkd\n        tckd_loss = self.kl_loss(pred_student, pred_teacher)\n\n        gt_mask_ex = paddle.expand_as(gt_mask.unsqueeze(axis=1), logits_teacher)\n        pred_teacher_part2 = F.softmax(\n            logits_teacher / self.temperature - 1000.0 * gt_mask_ex, axis=-1\n        )\n        pred_student_part2 = F.softmax(\n            logits_student / self.temperature - 1000.0 * gt_mask_ex, axis=-1\n        )\n        # differents with dkd\n        pred_teacher_part2 = paddle.mean(pred_teacher_part2, axis=1)\n        pred_student_part2 = paddle.mean(pred_student_part2, axis=1)\n\n        # differents with dkd\n        nckd_loss = self.kl_loss(pred_student_part2, pred_teacher_part2)\n        loss = self.alpha * tckd_loss + self.beta * nckd_loss\n        return loss\n\n\nclass KLCTCLogits(nn.Layer):\n    def __init__(self, weight=1.0, reduction=\"mean\", mode=\"mean\"):\n        super().__init__()\n        self.weight = weight\n        self.reduction = reduction\n        self.eps = 1e-6\n        self.t = 0.5\n        self.act = nn.Softmax(axis=-1)\n        self.use_log = True\n        self.mode = mode\n        self.ctc_dkd_loss = CTCDKDLoss()\n\n    def kl_loss(self, p1, p2):  # predict, label\n        loss = paddle.multiply(\n            p2, paddle.log((p2 + self.eps) / (p1 + self.eps) + self.eps)\n        )\n        bs = loss.shape[0]\n        loss = paddle.sum(loss) / bs\n        return loss\n\n    def forward_meanmax(self, stu_out, tea_out):\n        stu_out = paddle.mean(F.softmax(stu_out / self.t, axis=-1), axis=1)\n        tea_out = paddle.mean(F.softmax(tea_out / self.t, axis=-1), axis=1)\n        loss = self.kl_loss(stu_out, tea_out)\n\n        return loss\n\n    def forward_meanlog(self, stu_out, tea_out):\n        stu_out = paddle.mean(F.softmax(stu_out / self.t, axis=-1), axis=1)\n        tea_out = paddle.mean(F.softmax(tea_out / self.t, axis=-1), axis=1)\n        if self.use_log is True:\n            # for recognition distillation, log is needed for feature map\n            log_out1 = paddle.log(stu_out)\n            log_out2 = paddle.log(tea_out)\n            loss = (\n                self._kldiv(log_out1, tea_out) + self._kldiv(log_out2, stu_out)\n            ) / 2.0\n\n        return loss\n\n    def forward_sum(self, stu_out, tea_out):\n        stu_out = paddle.sum(F.softmax(stu_out / self.t, axis=-1), axis=1)\n        tea_out = paddle.sum(F.softmax(tea_out / self.t, axis=-1), axis=1)\n        stu_out = paddle.log(stu_out)\n        bs = stu_out.shape[0]\n        loss = tea_out * (paddle.log(tea_out + self.eps) - stu_out)\n        loss = paddle.sum(loss, axis=1) / loss.shape[0]\n        return loss\n\n    def _kldiv(self, x, target):\n        eps = 1.0e-10\n        loss = target * (paddle.log(target + eps) - x)\n        loss = paddle.sum(paddle.mean(loss, axis=1)) / loss.shape[0]\n        return loss\n\n    def forward(self, stu_out, tea_out, targets=None):\n        if self.mode == \"log\":\n            return self.forward_log(stu_out, tea_out)\n        elif self.mode == \"mean\":\n            blank_mask = paddle.ones_like(stu_out)\n            blank_mask.stop_gradient = True\n            blank_mask[:, :, 0] = -1\n            stu_out *= blank_mask\n            tea_out *= blank_mask\n            return self.forward_meanmax(stu_out, tea_out)\n        elif self.mode == \"sum\":\n            return self.forward_sum(stu_out, tea_out)\n        elif self.mode == \"meanlog\":\n            blank_mask = paddle.ones_like(stu_out)\n            blank_mask.stop_gradient = True\n            blank_mask[:, :, 0] = -1\n            stu_out *= blank_mask\n            tea_out *= blank_mask\n            return self.forward_meanlog(stu_out, tea_out)\n        elif self.mode == \"ctcdkd\":\n            # ingore ctc blank logits\n            blank_mask = paddle.ones_like(stu_out)\n            blank_mask.stop_gradient = True\n            blank_mask[:, :, 0] = -1\n            stu_out *= blank_mask\n            tea_out *= blank_mask\n            return self.ctc_dkd_loss(stu_out, tea_out, targets)\n        else:\n            raise ValueError(\"error!!!!!!\")\n\n    def forward_log(self, out1, out2):\n        if self.act is not None:\n            out1 = self.act(out1) + 1e-10\n            out2 = self.act(out2) + 1e-10\n        if self.use_log is True:\n            # for recognition distillation, log is needed for feature map\n            log_out1 = paddle.log(out1)\n            log_out2 = paddle.log(out2)\n            loss = (self._kldiv(log_out1, out2) + self._kldiv(log_out2, out1)) / 2.0\n\n        return loss\n\n\nclass DistillCTCLogits(KLCTCLogits):\n    def __init__(\n        self, model_name_pairs=[], key=None, name=\"ctc_logits\", reduction=\"mean\"\n    ):\n        super().__init__(reduction=reduction)\n        self.model_name_pairs = self._check_model_name_pairs(model_name_pairs)\n        self.key = key\n        self.name = name\n\n    def _check_model_name_pairs(self, model_name_pairs):\n        if not isinstance(model_name_pairs, list):\n            return []\n        elif isinstance(model_name_pairs[0], list) and isinstance(\n            model_name_pairs[0][0], str\n        ):\n            return model_name_pairs\n        else:\n            return [model_name_pairs]\n\n    def forward(self, predicts, batch):\n        loss_dict = dict()\n        for idx, pair in enumerate(self.model_name_pairs):\n            out1 = predicts[pair[0]]\n            out2 = predicts[pair[1]]\n\n            if self.key is not None:\n                out1 = out1[self.key][\"ctc\"]\n                out2 = out2[self.key][\"ctc\"]\n\n            ctc_label = batch[1]\n            loss = super().forward(out1, out2, ctc_label)\n            if isinstance(loss, dict):\n                for key in loss:\n                    loss_dict[\n                        \"{}_{}_{}\".format(self.name, self.model_name_pairs, idx)\n                    ] = loss[key]\n            else:\n                loss_dict[\"{}_{}\".format(self.name, idx)] = loss\n        return loss_dict\n", "ppocr/losses/rec_spin_att_loss.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nfrom paddle import nn\n\n\"\"\"This code is refer from:\nhttps://github.com/hikopensource/DAVAR-Lab-OCR\n\"\"\"\n\n\nclass SPINAttentionLoss(nn.Layer):\n    def __init__(self, reduction=\"mean\", ignore_index=-100, **kwargs):\n        super(SPINAttentionLoss, self).__init__()\n        self.loss_func = nn.CrossEntropyLoss(\n            weight=None, reduction=reduction, ignore_index=ignore_index\n        )\n\n    def forward(self, predicts, batch):\n        targets = batch[1].astype(\"int64\")\n        targets = targets[:, 1:]  # remove [eos] in label\n\n        label_lengths = batch[2].astype(\"int64\")\n        batch_size, num_steps, num_classes = (\n            predicts.shape[0],\n            predicts.shape[1],\n            predicts.shape[2],\n        )\n        assert (\n            len(targets.shape) == len(list(predicts.shape)) - 1\n        ), \"The target's shape and inputs's shape is [N, d] and [N, num_steps]\"\n\n        inputs = paddle.reshape(predicts, [-1, predicts.shape[-1]])\n        targets = paddle.reshape(targets, [-1])\n\n        return {\"loss\": self.loss_func(inputs, targets)}\n", "ppocr/losses/det_db_loss.py": "# copyright (c) 2019 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/WenmuZhou/DBNet.pytorch/blob/master/models/losses/DB_loss.py\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nfrom paddle import nn\n\nfrom .det_basic_loss import BalanceLoss, MaskL1Loss, DiceLoss\n\n\nclass DBLoss(nn.Layer):\n    \"\"\"\n    Differentiable Binarization (DB) Loss Function\n    args:\n        param (dict): the super paramter for DB Loss\n    \"\"\"\n\n    def __init__(\n        self,\n        balance_loss=True,\n        main_loss_type=\"DiceLoss\",\n        alpha=5,\n        beta=10,\n        ohem_ratio=3,\n        eps=1e-6,\n        **kwargs,\n    ):\n        super(DBLoss, self).__init__()\n        self.alpha = alpha\n        self.beta = beta\n        self.dice_loss = DiceLoss(eps=eps)\n        self.l1_loss = MaskL1Loss(eps=eps)\n        self.bce_loss = BalanceLoss(\n            balance_loss=balance_loss,\n            main_loss_type=main_loss_type,\n            negative_ratio=ohem_ratio,\n        )\n\n    def forward(self, predicts, labels):\n        predict_maps = predicts[\"maps\"]\n        (\n            label_threshold_map,\n            label_threshold_mask,\n            label_shrink_map,\n            label_shrink_mask,\n        ) = labels[1:]\n        shrink_maps = predict_maps[:, 0, :, :]\n        threshold_maps = predict_maps[:, 1, :, :]\n        binary_maps = predict_maps[:, 2, :, :]\n\n        loss_shrink_maps = self.bce_loss(\n            shrink_maps, label_shrink_map, label_shrink_mask\n        )\n        loss_threshold_maps = self.l1_loss(\n            threshold_maps, label_threshold_map, label_threshold_mask\n        )\n        loss_binary_maps = self.dice_loss(\n            binary_maps, label_shrink_map, label_shrink_mask\n        )\n        loss_shrink_maps = self.alpha * loss_shrink_maps\n        loss_threshold_maps = self.beta * loss_threshold_maps\n        # CBN loss\n        if \"distance_maps\" in predicts.keys():\n            distance_maps = predicts[\"distance_maps\"]\n            cbn_maps = predicts[\"cbn_maps\"]\n            cbn_loss = self.bce_loss(\n                cbn_maps[:, 0, :, :], label_shrink_map, label_shrink_mask\n            )\n        else:\n            dis_loss = paddle.to_tensor([0.0])\n            cbn_loss = paddle.to_tensor([0.0])\n\n        loss_all = loss_shrink_maps + loss_threshold_maps + loss_binary_maps\n        losses = {\n            \"loss\": loss_all + cbn_loss,\n            \"loss_shrink_maps\": loss_shrink_maps,\n            \"loss_threshold_maps\": loss_threshold_maps,\n            \"loss_binary_maps\": loss_binary_maps,\n            \"loss_cbn\": cbn_loss,\n        }\n        return losses\n", "ppocr/losses/e2e_pg_loss.py": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom paddle import nn\nimport paddle\n\nfrom .det_basic_loss import DiceLoss\nfrom ppocr.utils.e2e_utils.extract_batchsize import pre_process\n\n\nclass PGLoss(nn.Layer):\n    def __init__(\n        self, tcl_bs, max_text_length, max_text_nums, pad_num, eps=1e-6, **kwargs\n    ):\n        super(PGLoss, self).__init__()\n        self.tcl_bs = tcl_bs\n        self.max_text_nums = max_text_nums\n        self.max_text_length = max_text_length\n        self.pad_num = pad_num\n        self.dice_loss = DiceLoss(eps=eps)\n\n    def border_loss(self, f_border, l_border, l_score, l_mask):\n        l_border_split, l_border_norm = paddle.tensor.split(\n            l_border, num_or_sections=[4, 1], axis=1\n        )\n        f_border_split = f_border\n        b, c, h, w = l_border_norm.shape\n        l_border_norm_split = paddle.expand(x=l_border_norm, shape=[b, 4 * c, h, w])\n        b, c, h, w = l_score.shape\n        l_border_score = paddle.expand(x=l_score, shape=[b, 4 * c, h, w])\n        b, c, h, w = l_mask.shape\n        l_border_mask = paddle.expand(x=l_mask, shape=[b, 4 * c, h, w])\n        border_diff = l_border_split - f_border_split\n        abs_border_diff = paddle.abs(border_diff)\n        border_sign = abs_border_diff < 1.0\n        border_sign = paddle.cast(border_sign, dtype=\"float32\")\n        border_sign.stop_gradient = True\n        border_in_loss = 0.5 * abs_border_diff * abs_border_diff * border_sign + (\n            abs_border_diff - 0.5\n        ) * (1.0 - border_sign)\n        border_out_loss = l_border_norm_split * border_in_loss\n        border_loss = paddle.sum(border_out_loss * l_border_score * l_border_mask) / (\n            paddle.sum(l_border_score * l_border_mask) + 1e-5\n        )\n        return border_loss\n\n    def direction_loss(self, f_direction, l_direction, l_score, l_mask):\n        l_direction_split, l_direction_norm = paddle.tensor.split(\n            l_direction, num_or_sections=[2, 1], axis=1\n        )\n        f_direction_split = f_direction\n        b, c, h, w = l_direction_norm.shape\n        l_direction_norm_split = paddle.expand(\n            x=l_direction_norm, shape=[b, 2 * c, h, w]\n        )\n        b, c, h, w = l_score.shape\n        l_direction_score = paddle.expand(x=l_score, shape=[b, 2 * c, h, w])\n        b, c, h, w = l_mask.shape\n        l_direction_mask = paddle.expand(x=l_mask, shape=[b, 2 * c, h, w])\n        direction_diff = l_direction_split - f_direction_split\n        abs_direction_diff = paddle.abs(direction_diff)\n        direction_sign = abs_direction_diff < 1.0\n        direction_sign = paddle.cast(direction_sign, dtype=\"float32\")\n        direction_sign.stop_gradient = True\n        direction_in_loss = (\n            0.5 * abs_direction_diff * abs_direction_diff * direction_sign\n            + (abs_direction_diff - 0.5) * (1.0 - direction_sign)\n        )\n        direction_out_loss = l_direction_norm_split * direction_in_loss\n        direction_loss = paddle.sum(\n            direction_out_loss * l_direction_score * l_direction_mask\n        ) / (paddle.sum(l_direction_score * l_direction_mask) + 1e-5)\n        return direction_loss\n\n    def ctcloss(self, f_char, tcl_pos, tcl_mask, tcl_label, label_t):\n        f_char = paddle.transpose(f_char, [0, 2, 3, 1])\n        tcl_pos = paddle.reshape(tcl_pos, [-1, 3])\n        tcl_pos = paddle.cast(tcl_pos, dtype=int)\n        f_tcl_char = paddle.gather_nd(f_char, tcl_pos)\n        f_tcl_char = paddle.reshape(\n            f_tcl_char, [-1, 64, self.pad_num + 1]\n        )  # len(Lexicon_Table)+1\n        f_tcl_char_fg, f_tcl_char_bg = paddle.split(\n            f_tcl_char, [self.pad_num, 1], axis=2\n        )\n        f_tcl_char_bg = f_tcl_char_bg * tcl_mask + (1.0 - tcl_mask) * 20.0\n        b, c, l = tcl_mask.shape\n        tcl_mask_fg = paddle.expand(x=tcl_mask, shape=[b, c, self.pad_num * l])\n        tcl_mask_fg.stop_gradient = True\n        f_tcl_char_fg = f_tcl_char_fg * tcl_mask_fg + (1.0 - tcl_mask_fg) * (-20.0)\n        f_tcl_char_mask = paddle.concat([f_tcl_char_fg, f_tcl_char_bg], axis=2)\n        f_tcl_char_ld = paddle.transpose(f_tcl_char_mask, (1, 0, 2))\n        N, B, _ = f_tcl_char_ld.shape\n        input_lengths = paddle.to_tensor([N] * B, dtype=\"int64\")\n        cost = paddle.nn.functional.ctc_loss(\n            log_probs=f_tcl_char_ld,\n            labels=tcl_label,\n            input_lengths=input_lengths,\n            label_lengths=label_t,\n            blank=self.pad_num,\n            reduction=\"none\",\n        )\n        cost = cost.mean()\n        return cost\n\n    def forward(self, predicts, labels):\n        (\n            images,\n            tcl_maps,\n            tcl_label_maps,\n            border_maps,\n            direction_maps,\n            training_masks,\n            label_list,\n            pos_list,\n            pos_mask,\n        ) = labels\n        # for all the batch_size\n        pos_list, pos_mask, label_list, label_t = pre_process(\n            label_list,\n            pos_list,\n            pos_mask,\n            self.max_text_length,\n            self.max_text_nums,\n            self.pad_num,\n            self.tcl_bs,\n        )\n\n        f_score, f_border, f_direction, f_char = (\n            predicts[\"f_score\"],\n            predicts[\"f_border\"],\n            predicts[\"f_direction\"],\n            predicts[\"f_char\"],\n        )\n        score_loss = self.dice_loss(f_score, tcl_maps, training_masks)\n        border_loss = self.border_loss(f_border, border_maps, tcl_maps, training_masks)\n        direction_loss = self.direction_loss(\n            f_direction, direction_maps, tcl_maps, training_masks\n        )\n        ctc_loss = self.ctcloss(f_char, pos_list, pos_mask, label_list, label_t)\n        loss_all = score_loss + border_loss + direction_loss + 5 * ctc_loss\n\n        losses = {\n            \"loss\": loss_all,\n            \"score_loss\": score_loss,\n            \"border_loss\": border_loss,\n            \"direction_loss\": direction_loss,\n            \"ctc_loss\": ctc_loss,\n        }\n        return losses\n", "ppocr/losses/stroke_focus_loss.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/FudanVI/FudanOCR/blob/main/text-gestalt/loss/stroke_focus_loss.py\n\"\"\"\nimport cv2\nimport sys\nimport time\nimport string\nimport random\nimport numpy as np\nimport paddle.nn as nn\nimport paddle\n\n\nclass StrokeFocusLoss(nn.Layer):\n    def __init__(self, character_dict_path=None, **kwargs):\n        super(StrokeFocusLoss, self).__init__(character_dict_path)\n        self.mse_loss = nn.MSELoss()\n        self.ce_loss = nn.CrossEntropyLoss()\n        self.l1_loss = nn.L1Loss()\n        self.english_stroke_alphabet = \"0123456789\"\n        self.english_stroke_dict = {}\n        for index in range(len(self.english_stroke_alphabet)):\n            self.english_stroke_dict[self.english_stroke_alphabet[index]] = index\n\n        stroke_decompose_lines = open(character_dict_path, \"r\").readlines()\n        self.dic = {}\n        for line in stroke_decompose_lines:\n            line = line.strip()\n            character, sequence = line.split()\n            self.dic[character] = sequence\n\n    def forward(self, pred, data):\n        sr_img = pred[\"sr_img\"]\n        hr_img = pred[\"hr_img\"]\n\n        mse_loss = self.mse_loss(sr_img, hr_img)\n        word_attention_map_gt = pred[\"word_attention_map_gt\"]\n        word_attention_map_pred = pred[\"word_attention_map_pred\"]\n\n        hr_pred = pred[\"hr_pred\"]\n        sr_pred = pred[\"sr_pred\"]\n\n        attention_loss = paddle.nn.functional.l1_loss(\n            word_attention_map_gt, word_attention_map_pred\n        )\n\n        loss = (mse_loss + attention_loss * 50) * 100\n\n        return {\"mse_loss\": mse_loss, \"attention_loss\": attention_loss, \"loss\": loss}\n", "ppocr/losses/det_drrg_loss.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/open-mmlab/mmocr/blob/main/mmocr/models/textdet/losses/drrg_loss.py\n\"\"\"\n\nimport paddle\nimport paddle.nn.functional as F\nfrom paddle import nn\n\n\nclass DRRGLoss(nn.Layer):\n    def __init__(self, ohem_ratio=3.0):\n        super().__init__()\n        self.ohem_ratio = ohem_ratio\n        self.downsample_ratio = 1.0\n\n    def balance_bce_loss(self, pred, gt, mask):\n        \"\"\"Balanced Binary-CrossEntropy Loss.\n\n        Args:\n            pred (Tensor): Shape of :math:`(1, H, W)`.\n            gt (Tensor): Shape of :math:`(1, H, W)`.\n            mask (Tensor): Shape of :math:`(1, H, W)`.\n\n        Returns:\n            Tensor: Balanced bce loss.\n        \"\"\"\n        assert pred.shape == gt.shape == mask.shape\n        assert paddle.all(pred >= 0) and paddle.all(pred <= 1)\n        assert paddle.all(gt >= 0) and paddle.all(gt <= 1)\n        positive = gt * mask\n        negative = (1 - gt) * mask\n        positive_count = int(positive.sum())\n\n        if positive_count > 0:\n            loss = F.binary_cross_entropy(pred, gt, reduction=\"none\")\n            positive_loss = paddle.sum(loss * positive)\n            negative_loss = loss * negative\n            negative_count = min(\n                int(negative.sum()), int(positive_count * self.ohem_ratio)\n            )\n        else:\n            positive_loss = paddle.to_tensor(0.0)\n            loss = F.binary_cross_entropy(pred, gt, reduction=\"none\")\n            negative_loss = loss * negative\n            negative_count = 100\n        negative_loss, _ = paddle.topk(negative_loss.reshape([-1]), negative_count)\n\n        balance_loss = (positive_loss + paddle.sum(negative_loss)) / (\n            float(positive_count + negative_count) + 1e-5\n        )\n\n        return balance_loss\n\n    def gcn_loss(self, gcn_data):\n        \"\"\"CrossEntropy Loss from gcn module.\n\n        Args:\n            gcn_data (tuple(Tensor, Tensor)): The first is the\n                prediction with shape :math:`(N, 2)` and the\n                second is the gt label with shape :math:`(m, n)`\n                where :math:`m * n = N`.\n\n        Returns:\n            Tensor: CrossEntropy loss.\n        \"\"\"\n        gcn_pred, gt_labels = gcn_data\n        gt_labels = gt_labels.reshape([-1])\n        loss = F.cross_entropy(gcn_pred, gt_labels)\n\n        return loss\n\n    def bitmasks2tensor(self, bitmasks, target_sz):\n        \"\"\"Convert Bitmasks to tensor.\n\n        Args:\n            bitmasks (list[BitmapMasks]): The BitmapMasks list. Each item is\n                for one img.\n            target_sz (tuple(int, int)): The target tensor of size\n                :math:`(H, W)`.\n\n        Returns:\n            list[Tensor]: The list of kernel tensors. Each element stands for\n            one kernel level.\n        \"\"\"\n        batch_size = len(bitmasks)\n        results = []\n\n        kernel = []\n        for batch_inx in range(batch_size):\n            mask = bitmasks[batch_inx]\n            # hxw\n            mask_sz = mask.shape\n            # left, right, top, bottom\n            pad = [0, target_sz[1] - mask_sz[1], 0, target_sz[0] - mask_sz[0]]\n            mask = F.pad(mask, pad, mode=\"constant\", value=0)\n            kernel.append(mask)\n        kernel = paddle.stack(kernel)\n        results.append(kernel)\n\n        return results\n\n    def forward(self, preds, labels):\n        \"\"\"Compute Drrg loss.\"\"\"\n\n        assert isinstance(preds, tuple)\n        (\n            gt_text_mask,\n            gt_center_region_mask,\n            gt_mask,\n            gt_top_height_map,\n            gt_bot_height_map,\n            gt_sin_map,\n            gt_cos_map,\n        ) = labels[1:8]\n\n        downsample_ratio = self.downsample_ratio\n\n        pred_maps, gcn_data = preds\n        pred_text_region = pred_maps[:, 0, :, :]\n        pred_center_region = pred_maps[:, 1, :, :]\n        pred_sin_map = pred_maps[:, 2, :, :]\n        pred_cos_map = pred_maps[:, 3, :, :]\n        pred_top_height_map = pred_maps[:, 4, :, :]\n        pred_bot_height_map = pred_maps[:, 5, :, :]\n        feature_sz = pred_maps.shape\n\n        # bitmask 2 tensor\n        mapping = {\n            \"gt_text_mask\": paddle.cast(gt_text_mask, \"float32\"),\n            \"gt_center_region_mask\": paddle.cast(gt_center_region_mask, \"float32\"),\n            \"gt_mask\": paddle.cast(gt_mask, \"float32\"),\n            \"gt_top_height_map\": paddle.cast(gt_top_height_map, \"float32\"),\n            \"gt_bot_height_map\": paddle.cast(gt_bot_height_map, \"float32\"),\n            \"gt_sin_map\": paddle.cast(gt_sin_map, \"float32\"),\n            \"gt_cos_map\": paddle.cast(gt_cos_map, \"float32\"),\n        }\n        gt = {}\n        for key, value in mapping.items():\n            gt[key] = value\n            if abs(downsample_ratio - 1.0) < 1e-2:\n                gt[key] = self.bitmasks2tensor(gt[key], feature_sz[2:])\n            else:\n                gt[key] = [item.rescale(downsample_ratio) for item in gt[key]]\n                gt[key] = self.bitmasks2tensor(gt[key], feature_sz[2:])\n                if key in [\"gt_top_height_map\", \"gt_bot_height_map\"]:\n                    gt[key] = [item * downsample_ratio for item in gt[key]]\n            gt[key] = [item for item in gt[key]]\n\n        scale = paddle.sqrt(1.0 / (pred_sin_map**2 + pred_cos_map**2 + 1e-8))\n        pred_sin_map = pred_sin_map * scale\n        pred_cos_map = pred_cos_map * scale\n\n        loss_text = self.balance_bce_loss(\n            F.sigmoid(pred_text_region), gt[\"gt_text_mask\"][0], gt[\"gt_mask\"][0]\n        )\n\n        text_mask = gt[\"gt_text_mask\"][0] * gt[\"gt_mask\"][0]\n        negative_text_mask = (1 - gt[\"gt_text_mask\"][0]) * gt[\"gt_mask\"][0]\n        loss_center_map = F.binary_cross_entropy(\n            F.sigmoid(pred_center_region),\n            gt[\"gt_center_region_mask\"][0],\n            reduction=\"none\",\n        )\n        if int(text_mask.sum()) > 0:\n            loss_center_positive = paddle.sum(loss_center_map * text_mask) / paddle.sum(\n                text_mask\n            )\n        else:\n            loss_center_positive = paddle.to_tensor(0.0)\n        loss_center_negative = paddle.sum(\n            loss_center_map * negative_text_mask\n        ) / paddle.sum(negative_text_mask)\n        loss_center = loss_center_positive + 0.5 * loss_center_negative\n\n        center_mask = gt[\"gt_center_region_mask\"][0] * gt[\"gt_mask\"][0]\n        if int(center_mask.sum()) > 0:\n            map_sz = pred_top_height_map.shape\n            ones = paddle.ones(map_sz, dtype=\"float32\")\n            loss_top = F.smooth_l1_loss(\n                pred_top_height_map / (gt[\"gt_top_height_map\"][0] + 1e-2),\n                ones,\n                reduction=\"none\",\n            )\n            loss_bot = F.smooth_l1_loss(\n                pred_bot_height_map / (gt[\"gt_bot_height_map\"][0] + 1e-2),\n                ones,\n                reduction=\"none\",\n            )\n            gt_height = gt[\"gt_top_height_map\"][0] + gt[\"gt_bot_height_map\"][0]\n            loss_height = paddle.sum(\n                (paddle.log(gt_height + 1) * (loss_top + loss_bot)) * center_mask\n            ) / paddle.sum(center_mask)\n\n            loss_sin = paddle.sum(\n                F.smooth_l1_loss(pred_sin_map, gt[\"gt_sin_map\"][0], reduction=\"none\")\n                * center_mask\n            ) / paddle.sum(center_mask)\n            loss_cos = paddle.sum(\n                F.smooth_l1_loss(pred_cos_map, gt[\"gt_cos_map\"][0], reduction=\"none\")\n                * center_mask\n            ) / paddle.sum(center_mask)\n        else:\n            loss_height = paddle.to_tensor(0.0)\n            loss_sin = paddle.to_tensor(0.0)\n            loss_cos = paddle.to_tensor(0.0)\n\n        loss_gcn = self.gcn_loss(gcn_data)\n\n        loss = loss_text + loss_center + loss_height + loss_sin + loss_cos + loss_gcn\n        results = dict(\n            loss=loss,\n            loss_text=loss_text,\n            loss_center=loss_center,\n            loss_height=loss_height,\n            loss_sin=loss_sin,\n            loss_cos=loss_cos,\n            loss_gcn=loss_gcn,\n        )\n\n        return results\n", "ppocr/losses/rec_multi_loss.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nfrom paddle import nn\n\nfrom .rec_ctc_loss import CTCLoss\nfrom .rec_sar_loss import SARLoss\nfrom .rec_nrtr_loss import NRTRLoss\n\n\nclass MultiLoss(nn.Layer):\n    def __init__(self, **kwargs):\n        super().__init__()\n        self.loss_funcs = {}\n        self.loss_list = kwargs.pop(\"loss_config_list\")\n        self.weight_1 = kwargs.get(\"weight_1\", 1.0)\n        self.weight_2 = kwargs.get(\"weight_2\", 1.0)\n        for loss_info in self.loss_list:\n            for name, param in loss_info.items():\n                if param is not None:\n                    kwargs.update(param)\n                loss = eval(name)(**kwargs)\n                self.loss_funcs[name] = loss\n\n    def forward(self, predicts, batch):\n        self.total_loss = {}\n        total_loss = 0.0\n        # batch [image, label_ctc, label_sar, length, valid_ratio]\n        for name, loss_func in self.loss_funcs.items():\n            if name == \"CTCLoss\":\n                loss = (\n                    loss_func(predicts[\"ctc\"], batch[:2] + batch[3:])[\"loss\"]\n                    * self.weight_1\n                )\n            elif name == \"SARLoss\":\n                loss = (\n                    loss_func(predicts[\"sar\"], batch[:1] + batch[2:])[\"loss\"]\n                    * self.weight_2\n                )\n            elif name == \"NRTRLoss\":\n                loss = (\n                    loss_func(predicts[\"gtc\"], batch[:1] + batch[2:])[\"loss\"]\n                    * self.weight_2\n                )\n            else:\n                raise NotImplementedError(\n                    \"{} is not supported in MultiLoss yet\".format(name)\n                )\n            self.total_loss[name] = loss\n            total_loss += loss\n        self.total_loss[\"loss\"] = total_loss\n        return self.total_loss\n", "ppocr/losses/combined_loss.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport paddle\nimport paddle.nn as nn\n\nfrom .rec_ctc_loss import CTCLoss\nfrom .center_loss import CenterLoss\nfrom .ace_loss import ACELoss\nfrom .rec_sar_loss import SARLoss\n\nfrom .distillation_loss import DistillationCTCLoss, DistillCTCLogits\nfrom .distillation_loss import DistillationSARLoss, DistillationNRTRLoss\nfrom .distillation_loss import (\n    DistillationDMLLoss,\n    DistillationKLDivLoss,\n    DistillationDKDLoss,\n)\nfrom .distillation_loss import (\n    DistillationDistanceLoss,\n    DistillationDBLoss,\n    DistillationDilaDBLoss,\n)\nfrom .distillation_loss import (\n    DistillationVQASerTokenLayoutLMLoss,\n    DistillationSERDMLLoss,\n)\nfrom .distillation_loss import DistillationLossFromOutput\nfrom .distillation_loss import DistillationVQADistanceLoss\n\n\nclass CombinedLoss(nn.Layer):\n    \"\"\"\n    CombinedLoss:\n        a combionation of loss function\n    \"\"\"\n\n    def __init__(self, loss_config_list=None):\n        super().__init__()\n        self.loss_func = []\n        self.loss_weight = []\n        assert isinstance(loss_config_list, list), \"operator config should be a list\"\n        for config in loss_config_list:\n            assert isinstance(config, dict) and len(config) == 1, \"yaml format error\"\n            name = list(config)[0]\n            param = config[name]\n            assert (\n                \"weight\" in param\n            ), \"weight must be in param, but param just contains {}\".format(\n                param.keys()\n            )\n            self.loss_weight.append(param.pop(\"weight\"))\n            self.loss_func.append(eval(name)(**param))\n\n    def forward(self, input, batch, **kargs):\n        loss_dict = {}\n        loss_all = 0.0\n        for idx, loss_func in enumerate(self.loss_func):\n            loss = loss_func(input, batch, **kargs)\n            if isinstance(loss, paddle.Tensor):\n                loss = {\"loss_{}_{}\".format(str(loss), idx): loss}\n\n            weight = self.loss_weight[idx]\n\n            loss = {key: loss[key] * weight for key in loss}\n\n            if \"loss\" in loss:\n                loss_all += loss[\"loss\"]\n            else:\n                loss_all += paddle.add_n(list(loss.values()))\n            loss_dict.update(loss)\n        loss_dict[\"loss\"] = loss_all\n        return loss_dict\n", "ppocr/losses/det_pse_loss.py": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/whai362/PSENet/blob/python3/models/head/psenet_head.py\n\"\"\"\n\nimport paddle\nfrom paddle import nn\nfrom paddle.nn import functional as F\nimport numpy as np\nfrom ppocr.utils.iou import iou\n\n\nclass PSELoss(nn.Layer):\n    def __init__(\n        self,\n        alpha,\n        ohem_ratio=3,\n        kernel_sample_mask=\"pred\",\n        reduction=\"sum\",\n        eps=1e-6,\n        **kwargs,\n    ):\n        \"\"\"Implement PSE Loss.\"\"\"\n        super(PSELoss, self).__init__()\n        assert reduction in [\"sum\", \"mean\", \"none\"]\n        self.alpha = alpha\n        self.ohem_ratio = ohem_ratio\n        self.kernel_sample_mask = kernel_sample_mask\n        self.reduction = reduction\n        self.eps = eps\n\n    def forward(self, outputs, labels):\n        predicts = outputs[\"maps\"]\n        predicts = F.interpolate(predicts, scale_factor=4)\n\n        texts = predicts[:, 0, :, :]\n        kernels = predicts[:, 1:, :, :]\n        gt_texts, gt_kernels, training_masks = labels[1:]\n\n        # text loss\n        selected_masks = self.ohem_batch(texts, gt_texts, training_masks)\n\n        loss_text = self.dice_loss(texts, gt_texts, selected_masks)\n        iou_text = iou(\n            (texts > 0).astype(\"int64\"), gt_texts, training_masks, reduce=False\n        )\n        losses = dict(loss_text=loss_text, iou_text=iou_text)\n\n        # kernel loss\n        loss_kernels = []\n        if self.kernel_sample_mask == \"gt\":\n            selected_masks = gt_texts * training_masks\n        elif self.kernel_sample_mask == \"pred\":\n            selected_masks = (F.sigmoid(texts) > 0.5).astype(\"float32\") * training_masks\n\n        for i in range(kernels.shape[1]):\n            kernel_i = kernels[:, i, :, :]\n            gt_kernel_i = gt_kernels[:, i, :, :]\n            loss_kernel_i = self.dice_loss(kernel_i, gt_kernel_i, selected_masks)\n            loss_kernels.append(loss_kernel_i)\n        loss_kernels = paddle.mean(paddle.stack(loss_kernels, axis=1), axis=1)\n        iou_kernel = iou(\n            (kernels[:, -1, :, :] > 0).astype(\"int64\"),\n            gt_kernels[:, -1, :, :],\n            training_masks * gt_texts,\n            reduce=False,\n        )\n        losses.update(dict(loss_kernels=loss_kernels, iou_kernel=iou_kernel))\n        loss = self.alpha * loss_text + (1 - self.alpha) * loss_kernels\n        losses[\"loss\"] = loss\n        if self.reduction == \"sum\":\n            losses = {x: paddle.sum(v) for x, v in losses.items()}\n        elif self.reduction == \"mean\":\n            losses = {x: paddle.mean(v) for x, v in losses.items()}\n        return losses\n\n    def dice_loss(self, input, target, mask):\n        input = F.sigmoid(input)\n\n        input = input.reshape([input.shape[0], -1])\n        target = target.reshape([target.shape[0], -1])\n        mask = mask.reshape([mask.shape[0], -1])\n\n        input = input * mask\n        target = target * mask\n\n        a = paddle.sum(input * target, 1)\n        b = paddle.sum(input * input, 1) + self.eps\n        c = paddle.sum(target * target, 1) + self.eps\n        d = (2 * a) / (b + c)\n        return 1 - d\n\n    def ohem_single(self, score, gt_text, training_mask, ohem_ratio=3):\n        pos_num = int(paddle.sum((gt_text > 0.5).astype(\"float32\"))) - int(\n            paddle.sum(\n                paddle.logical_and((gt_text > 0.5), (training_mask <= 0.5)).astype(\n                    \"float32\"\n                )\n            )\n        )\n\n        if pos_num == 0:\n            selected_mask = training_mask\n            selected_mask = selected_mask.reshape(\n                [1, selected_mask.shape[0], selected_mask.shape[1]]\n            ).astype(\"float32\")\n            return selected_mask\n\n        neg_num = int(paddle.sum((gt_text <= 0.5).astype(\"float32\")))\n        neg_num = int(min(pos_num * ohem_ratio, neg_num))\n\n        if neg_num == 0:\n            selected_mask = training_mask\n            selected_mask = selected_mask.reshape(\n                [1, selected_mask.shape[0], selected_mask.shape[1]]\n            ).astype(\"float32\")\n            return selected_mask\n\n        neg_score = paddle.masked_select(score, gt_text <= 0.5)\n        neg_score_sorted = paddle.sort(-neg_score)\n        threshold = -neg_score_sorted[neg_num - 1]\n\n        selected_mask = paddle.logical_and(\n            paddle.logical_or((score >= threshold), (gt_text > 0.5)),\n            (training_mask > 0.5),\n        )\n        selected_mask = selected_mask.reshape(\n            [1, selected_mask.shape[0], selected_mask.shape[1]]\n        ).astype(\"float32\")\n        return selected_mask\n\n    def ohem_batch(self, scores, gt_texts, training_masks, ohem_ratio=3):\n        selected_masks = []\n        for i in range(scores.shape[0]):\n            selected_masks.append(\n                self.ohem_single(\n                    scores[i, :, :],\n                    gt_texts[i, :, :],\n                    training_masks[i, :, :],\n                    ohem_ratio,\n                )\n            )\n\n        selected_masks = paddle.concat(selected_masks, 0).astype(\"float32\")\n        return selected_masks\n", "ppocr/losses/rec_can_loss.py": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/LBH1024/CAN/models/can.py\n\"\"\"\n\nimport paddle\nimport paddle.nn as nn\nimport numpy as np\n\n\nclass CANLoss(nn.Layer):\n    \"\"\"\n    CANLoss is consist of two part:\n        word_average_loss: average accuracy of the symbol\n        counting_loss: counting loss of every symbol\n    \"\"\"\n\n    def __init__(self):\n        super(CANLoss, self).__init__()\n\n        self.use_label_mask = False\n        self.out_channel = 111\n        self.cross = (\n            nn.CrossEntropyLoss(reduction=\"none\")\n            if self.use_label_mask\n            else nn.CrossEntropyLoss()\n        )\n        self.counting_loss = nn.SmoothL1Loss(reduction=\"mean\")\n        self.ratio = 16\n\n    def forward(self, preds, batch):\n        word_probs = preds[0]\n        counting_preds = preds[1]\n        counting_preds1 = preds[2]\n        counting_preds2 = preds[3]\n        labels = batch[2]\n        labels_mask = batch[3]\n        counting_labels = gen_counting_label(labels, self.out_channel, True)\n        counting_loss = (\n            self.counting_loss(counting_preds1, counting_labels)\n            + self.counting_loss(counting_preds2, counting_labels)\n            + self.counting_loss(counting_preds, counting_labels)\n        )\n\n        word_loss = self.cross(\n            paddle.reshape(word_probs, [-1, word_probs.shape[-1]]),\n            paddle.reshape(labels, [-1]),\n        )\n        word_average_loss = (\n            paddle.sum(paddle.reshape(word_loss * labels_mask, [-1]))\n            / (paddle.sum(labels_mask) + 1e-10)\n            if self.use_label_mask\n            else word_loss\n        )\n        loss = word_average_loss + counting_loss\n        return {\"loss\": loss}\n\n\ndef gen_counting_label(labels, channel, tag):\n    b, t = labels.shape\n    counting_labels = np.zeros([b, channel])\n\n    if tag:\n        ignore = [0, 1, 107, 108, 109, 110]\n    else:\n        ignore = []\n    for i in range(b):\n        for j in range(t):\n            k = labels[i][j]\n            if k in ignore:\n                continue\n            else:\n                counting_labels[i][k] += 1\n    counting_labels = paddle.to_tensor(counting_labels, dtype=\"float32\")\n    return counting_labels\n", "ppocr/losses/rec_rfl_loss.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/hikopensource/DAVAR-Lab-OCR/blob/main/davarocr/davar_common/models/loss/cross_entropy_loss.py\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nfrom paddle import nn\n\nfrom .basic_loss import CELoss, DistanceLoss\n\n\nclass RFLLoss(nn.Layer):\n    def __init__(self, ignore_index=-100, **kwargs):\n        super().__init__()\n\n        self.cnt_loss = nn.MSELoss(**kwargs)\n        self.seq_loss = nn.CrossEntropyLoss(ignore_index=ignore_index)\n\n    def forward(self, predicts, batch):\n        self.total_loss = {}\n        total_loss = 0.0\n        if isinstance(predicts, tuple) or isinstance(predicts, list):\n            cnt_outputs, seq_outputs = predicts\n        else:\n            cnt_outputs, seq_outputs = predicts, None\n        # batch [image, label, length, cnt_label]\n        if cnt_outputs is not None:\n            cnt_loss = self.cnt_loss(cnt_outputs, paddle.cast(batch[3], paddle.float32))\n            self.total_loss[\"cnt_loss\"] = cnt_loss\n            total_loss += cnt_loss\n\n        if seq_outputs is not None:\n            targets = batch[1].astype(\"int64\")\n            label_lengths = batch[2].astype(\"int64\")\n            batch_size, num_steps, num_classes = (\n                seq_outputs.shape[0],\n                seq_outputs.shape[1],\n                seq_outputs.shape[2],\n            )\n            assert (\n                len(targets.shape) == len(list(seq_outputs.shape)) - 1\n            ), \"The target's shape and inputs's shape is [N, d] and [N, num_steps]\"\n\n            inputs = seq_outputs[:, :-1, :]\n            targets = targets[:, 1:]\n\n            inputs = paddle.reshape(inputs, [-1, inputs.shape[-1]])\n            targets = paddle.reshape(targets, [-1])\n            seq_loss = self.seq_loss(inputs, targets)\n            self.total_loss[\"seq_loss\"] = seq_loss\n            total_loss += seq_loss\n\n        self.total_loss[\"loss\"] = total_loss\n        return self.total_loss\n", "ppocr/losses/rec_pren_loss.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom paddle import nn\n\n\nclass PRENLoss(nn.Layer):\n    def __init__(self, **kwargs):\n        super(PRENLoss, self).__init__()\n        # note: 0 is padding idx\n        self.loss_func = nn.CrossEntropyLoss(reduction=\"mean\", ignore_index=0)\n\n    def forward(self, predicts, batch):\n        loss = self.loss_func(predicts, batch[1].astype(\"int64\"))\n        return {\"loss\": loss}\n", "ppocr/losses/__init__.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport copy\nimport paddle\nimport paddle.nn as nn\n\n# basic_loss\nfrom .basic_loss import LossFromOutput\n\n# det loss\nfrom .det_db_loss import DBLoss\nfrom .det_east_loss import EASTLoss\nfrom .det_sast_loss import SASTLoss\nfrom .det_pse_loss import PSELoss\nfrom .det_fce_loss import FCELoss\nfrom .det_ct_loss import CTLoss\nfrom .det_drrg_loss import DRRGLoss\n\n# rec loss\nfrom .rec_ctc_loss import CTCLoss\nfrom .rec_att_loss import AttentionLoss\nfrom .rec_srn_loss import SRNLoss\nfrom .rec_ce_loss import CELoss\nfrom .rec_sar_loss import SARLoss\nfrom .rec_aster_loss import AsterLoss\nfrom .rec_pren_loss import PRENLoss\nfrom .rec_multi_loss import MultiLoss\nfrom .rec_vl_loss import VLLoss\nfrom .rec_spin_att_loss import SPINAttentionLoss\nfrom .rec_rfl_loss import RFLLoss\nfrom .rec_can_loss import CANLoss\nfrom .rec_satrn_loss import SATRNLoss\nfrom .rec_nrtr_loss import NRTRLoss\nfrom .rec_parseq_loss import ParseQLoss\nfrom .rec_cppd_loss import CPPDLoss\n\n# cls loss\nfrom .cls_loss import ClsLoss\n\n# e2e loss\nfrom .e2e_pg_loss import PGLoss\nfrom .kie_sdmgr_loss import SDMGRLoss\n\n# basic loss function\nfrom .basic_loss import DistanceLoss\n\n# combined loss function\nfrom .combined_loss import CombinedLoss\n\n# table loss\nfrom .table_att_loss import TableAttentionLoss, SLALoss\nfrom .table_master_loss import TableMasterLoss\n\n# vqa token loss\nfrom .vqa_token_layoutlm_loss import VQASerTokenLayoutLMLoss\n\n# sr loss\nfrom .stroke_focus_loss import StrokeFocusLoss\nfrom .text_focus_loss import TelescopeLoss\n\n\ndef build_loss(config):\n    support_dict = [\n        \"DBLoss\",\n        \"PSELoss\",\n        \"EASTLoss\",\n        \"SASTLoss\",\n        \"FCELoss\",\n        \"CTCLoss\",\n        \"ClsLoss\",\n        \"AttentionLoss\",\n        \"SRNLoss\",\n        \"PGLoss\",\n        \"CombinedLoss\",\n        \"CELoss\",\n        \"TableAttentionLoss\",\n        \"SARLoss\",\n        \"AsterLoss\",\n        \"SDMGRLoss\",\n        \"VQASerTokenLayoutLMLoss\",\n        \"LossFromOutput\",\n        \"PRENLoss\",\n        \"MultiLoss\",\n        \"TableMasterLoss\",\n        \"SPINAttentionLoss\",\n        \"VLLoss\",\n        \"StrokeFocusLoss\",\n        \"SLALoss\",\n        \"CTLoss\",\n        \"RFLLoss\",\n        \"DRRGLoss\",\n        \"CANLoss\",\n        \"TelescopeLoss\",\n        \"SATRNLoss\",\n        \"NRTRLoss\",\n        \"ParseQLoss\",\n        \"CPPDLoss\",\n    ]\n    config = copy.deepcopy(config)\n    module_name = config.pop(\"name\")\n    assert module_name in support_dict, Exception(\n        \"loss only support {}\".format(support_dict)\n    )\n    module_class = eval(module_name)(**config)\n    return module_class\n", "ppocr/losses/det_sast_loss.py": "# copyright (c) 2019 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nfrom paddle import nn\nfrom .det_basic_loss import DiceLoss\nimport numpy as np\n\n\nclass SASTLoss(nn.Layer):\n    \"\"\" \"\"\"\n\n    def __init__(self, eps=1e-6, **kwargs):\n        super(SASTLoss, self).__init__()\n        self.dice_loss = DiceLoss(eps=eps)\n\n    def forward(self, predicts, labels):\n        \"\"\"\n        tcl_pos: N x 128 x 3\n        tcl_mask: N x 128 x 1\n        tcl_label: N x X list or LoDTensor\n        \"\"\"\n\n        f_score = predicts[\"f_score\"]\n        f_border = predicts[\"f_border\"]\n        f_tvo = predicts[\"f_tvo\"]\n        f_tco = predicts[\"f_tco\"]\n\n        l_score, l_border, l_mask, l_tvo, l_tco = labels[1:]\n\n        # score_loss\n        intersection = paddle.sum(f_score * l_score * l_mask)\n        union = paddle.sum(f_score * l_mask) + paddle.sum(l_score * l_mask)\n        score_loss = 1.0 - 2 * intersection / (union + 1e-5)\n\n        # border loss\n        l_border_split, l_border_norm = paddle.split(\n            l_border, num_or_sections=[4, 1], axis=1\n        )\n        f_border_split = f_border\n        border_ex_shape = l_border_norm.shape * np.array([1, 4, 1, 1])\n        l_border_norm_split = paddle.expand(x=l_border_norm, shape=border_ex_shape)\n        l_border_score = paddle.expand(x=l_score, shape=border_ex_shape)\n        l_border_mask = paddle.expand(x=l_mask, shape=border_ex_shape)\n\n        border_diff = l_border_split - f_border_split\n        abs_border_diff = paddle.abs(border_diff)\n        border_sign = abs_border_diff < 1.0\n        border_sign = paddle.cast(border_sign, dtype=\"float32\")\n        border_sign.stop_gradient = True\n        border_in_loss = 0.5 * abs_border_diff * abs_border_diff * border_sign + (\n            abs_border_diff - 0.5\n        ) * (1.0 - border_sign)\n        border_out_loss = l_border_norm_split * border_in_loss\n        border_loss = paddle.sum(border_out_loss * l_border_score * l_border_mask) / (\n            paddle.sum(l_border_score * l_border_mask) + 1e-5\n        )\n\n        # tvo_loss\n        l_tvo_split, l_tvo_norm = paddle.split(l_tvo, num_or_sections=[8, 1], axis=1)\n        f_tvo_split = f_tvo\n        tvo_ex_shape = l_tvo_norm.shape * np.array([1, 8, 1, 1])\n        l_tvo_norm_split = paddle.expand(x=l_tvo_norm, shape=tvo_ex_shape)\n        l_tvo_score = paddle.expand(x=l_score, shape=tvo_ex_shape)\n        l_tvo_mask = paddle.expand(x=l_mask, shape=tvo_ex_shape)\n        #\n        tvo_geo_diff = l_tvo_split - f_tvo_split\n        abs_tvo_geo_diff = paddle.abs(tvo_geo_diff)\n        tvo_sign = abs_tvo_geo_diff < 1.0\n        tvo_sign = paddle.cast(tvo_sign, dtype=\"float32\")\n        tvo_sign.stop_gradient = True\n        tvo_in_loss = 0.5 * abs_tvo_geo_diff * abs_tvo_geo_diff * tvo_sign + (\n            abs_tvo_geo_diff - 0.5\n        ) * (1.0 - tvo_sign)\n        tvo_out_loss = l_tvo_norm_split * tvo_in_loss\n        tvo_loss = paddle.sum(tvo_out_loss * l_tvo_score * l_tvo_mask) / (\n            paddle.sum(l_tvo_score * l_tvo_mask) + 1e-5\n        )\n\n        # tco_loss\n        l_tco_split, l_tco_norm = paddle.split(l_tco, num_or_sections=[2, 1], axis=1)\n        f_tco_split = f_tco\n        tco_ex_shape = l_tco_norm.shape * np.array([1, 2, 1, 1])\n        l_tco_norm_split = paddle.expand(x=l_tco_norm, shape=tco_ex_shape)\n        l_tco_score = paddle.expand(x=l_score, shape=tco_ex_shape)\n        l_tco_mask = paddle.expand(x=l_mask, shape=tco_ex_shape)\n\n        tco_geo_diff = l_tco_split - f_tco_split\n        abs_tco_geo_diff = paddle.abs(tco_geo_diff)\n        tco_sign = abs_tco_geo_diff < 1.0\n        tco_sign = paddle.cast(tco_sign, dtype=\"float32\")\n        tco_sign.stop_gradient = True\n        tco_in_loss = 0.5 * abs_tco_geo_diff * abs_tco_geo_diff * tco_sign + (\n            abs_tco_geo_diff - 0.5\n        ) * (1.0 - tco_sign)\n        tco_out_loss = l_tco_norm_split * tco_in_loss\n        tco_loss = paddle.sum(tco_out_loss * l_tco_score * l_tco_mask) / (\n            paddle.sum(l_tco_score * l_tco_mask) + 1e-5\n        )\n\n        # total loss\n        tvo_lw, tco_lw = 1.5, 1.5\n        score_lw, border_lw = 1.0, 1.0\n        total_loss = (\n            score_loss * score_lw\n            + border_loss * border_lw\n            + tvo_loss * tvo_lw\n            + tco_loss * tco_lw\n        )\n\n        losses = {\n            \"loss\": total_loss,\n            \"score_loss\": score_loss,\n            \"border_loss\": border_loss,\n            \"tvo_loss\": tvo_loss,\n            \"tco_loss\": tco_loss,\n        }\n        return losses\n", "ppocr/losses/det_basic_loss.py": "# copyright (c) 2019 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/WenmuZhou/DBNet.pytorch/blob/master/models/losses/basic_loss.py\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nimport paddle\nfrom paddle import nn\nimport paddle.nn.functional as F\n\n\nclass BalanceLoss(nn.Layer):\n    def __init__(\n        self,\n        balance_loss=True,\n        main_loss_type=\"DiceLoss\",\n        negative_ratio=3,\n        return_origin=False,\n        eps=1e-6,\n        **kwargs,\n    ):\n        \"\"\"\n        The BalanceLoss for Differentiable Binarization text detection\n        args:\n            balance_loss (bool): whether balance loss or not, default is True\n            main_loss_type (str): can only be one of ['CrossEntropy','DiceLoss',\n                'Euclidean','BCELoss', 'MaskL1Loss'], default is  'DiceLoss'.\n            negative_ratio (int|float): float, default is 3.\n            return_origin (bool): whether return unbalanced loss or not, default is False.\n            eps (float): default is 1e-6.\n        \"\"\"\n        super(BalanceLoss, self).__init__()\n        self.balance_loss = balance_loss\n        self.main_loss_type = main_loss_type\n        self.negative_ratio = negative_ratio\n        self.return_origin = return_origin\n        self.eps = eps\n\n        if self.main_loss_type == \"CrossEntropy\":\n            self.loss = nn.CrossEntropyLoss()\n        elif self.main_loss_type == \"Euclidean\":\n            self.loss = nn.MSELoss()\n        elif self.main_loss_type == \"DiceLoss\":\n            self.loss = DiceLoss(self.eps)\n        elif self.main_loss_type == \"BCELoss\":\n            self.loss = BCELoss(reduction=\"none\")\n        elif self.main_loss_type == \"MaskL1Loss\":\n            self.loss = MaskL1Loss(self.eps)\n        else:\n            loss_type = [\n                \"CrossEntropy\",\n                \"DiceLoss\",\n                \"Euclidean\",\n                \"BCELoss\",\n                \"MaskL1Loss\",\n            ]\n            raise Exception(\n                \"main_loss_type in BalanceLoss() can only be one of {}\".format(\n                    loss_type\n                )\n            )\n\n    def forward(self, pred, gt, mask=None):\n        \"\"\"\n        The BalanceLoss for Differentiable Binarization text detection\n        args:\n            pred (variable): predicted feature maps.\n            gt (variable): ground truth feature maps.\n            mask (variable): masked maps.\n        return: (variable) balanced loss\n        \"\"\"\n        positive = gt * mask\n        negative = (1 - gt) * mask\n\n        positive_count = int(positive.sum())\n        negative_count = int(min(negative.sum(), positive_count * self.negative_ratio))\n        loss = self.loss(pred, gt, mask=mask)\n\n        if not self.balance_loss:\n            return loss\n\n        positive_loss = positive * loss\n        negative_loss = negative * loss\n        negative_loss = paddle.reshape(negative_loss, shape=[-1])\n        if negative_count > 0:\n            sort_loss = negative_loss.sort(descending=True)\n            negative_loss = sort_loss[:negative_count]\n            # negative_loss, _ = paddle.topk(negative_loss, k=negative_count_int)\n            balance_loss = (positive_loss.sum() + negative_loss.sum()) / (\n                positive_count + negative_count + self.eps\n            )\n        else:\n            balance_loss = positive_loss.sum() / (positive_count + self.eps)\n        if self.return_origin:\n            return balance_loss, loss\n\n        return balance_loss\n\n\nclass DiceLoss(nn.Layer):\n    def __init__(self, eps=1e-6):\n        super(DiceLoss, self).__init__()\n        self.eps = eps\n\n    def forward(self, pred, gt, mask, weights=None):\n        \"\"\"\n        DiceLoss function.\n        \"\"\"\n\n        assert pred.shape == gt.shape\n        assert pred.shape == mask.shape\n        if weights is not None:\n            assert weights.shape == mask.shape\n            mask = weights * mask\n        intersection = paddle.sum(pred * gt * mask)\n\n        union = paddle.sum(pred * mask) + paddle.sum(gt * mask) + self.eps\n        loss = 1 - 2.0 * intersection / union\n        assert loss <= 1\n        return loss\n\n\nclass MaskL1Loss(nn.Layer):\n    def __init__(self, eps=1e-6):\n        super(MaskL1Loss, self).__init__()\n        self.eps = eps\n\n    def forward(self, pred, gt, mask):\n        \"\"\"\n        Mask L1 Loss\n        \"\"\"\n        loss = (paddle.abs(pred - gt) * mask).sum() / (mask.sum() + self.eps)\n        loss = paddle.mean(loss)\n        return loss\n\n\nclass BCELoss(nn.Layer):\n    def __init__(self, reduction=\"mean\"):\n        super(BCELoss, self).__init__()\n        self.reduction = reduction\n\n    def forward(self, input, label, mask=None, weight=None, name=None):\n        loss = F.binary_cross_entropy(input, label, reduction=self.reduction)\n        return loss\n", "ppocr/losses/rec_sar_loss.py": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nfrom paddle import nn\n\n\nclass SARLoss(nn.Layer):\n    def __init__(self, **kwargs):\n        super(SARLoss, self).__init__()\n        ignore_index = kwargs.get(\"ignore_index\", 92)  # 6626\n        self.loss_func = paddle.nn.loss.CrossEntropyLoss(\n            reduction=\"mean\", ignore_index=ignore_index\n        )\n\n    def forward(self, predicts, batch):\n        predict = predicts[\n            :, :-1, :\n        ]  # ignore last index of outputs to be in same seq_len with targets\n        label = batch[1].astype(\"int64\")[\n            :, 1:\n        ]  # ignore first index of target in loss calculation\n        batch_size, num_steps, num_classes = (\n            predict.shape[0],\n            predict.shape[1],\n            predict.shape[2],\n        )\n        assert (\n            len(label.shape) == len(list(predict.shape)) - 1\n        ), \"The target's shape and inputs's shape is [N, d] and [N, num_steps]\"\n\n        inputs = paddle.reshape(predict, [-1, num_classes])\n        targets = paddle.reshape(label, [-1])\n        loss = self.loss_func(inputs, targets)\n        return {\"loss\": loss}\n", "ppocr/losses/basic_loss.py": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport paddle\nimport paddle.nn as nn\nimport paddle.nn.functional as F\n\nfrom paddle.nn import L1Loss\nfrom paddle.nn import MSELoss as L2Loss\nfrom paddle.nn import SmoothL1Loss\n\n\nclass CELoss(nn.Layer):\n    def __init__(self, epsilon=None):\n        super().__init__()\n        if epsilon is not None and (epsilon <= 0 or epsilon >= 1):\n            epsilon = None\n        self.epsilon = epsilon\n\n    def _labelsmoothing(self, target, class_num):\n        if target.shape[-1] != class_num:\n            one_hot_target = F.one_hot(target, class_num)\n        else:\n            one_hot_target = target\n        soft_target = F.label_smooth(one_hot_target, epsilon=self.epsilon)\n        soft_target = paddle.reshape(soft_target, shape=[-1, class_num])\n        return soft_target\n\n    def forward(self, x, label):\n        loss_dict = {}\n        if self.epsilon is not None:\n            class_num = x.shape[-1]\n            label = self._labelsmoothing(label, class_num)\n            x = -F.log_softmax(x, axis=-1)\n            loss = paddle.sum(x * label, axis=-1)\n        else:\n            if label.shape[-1] == x.shape[-1]:\n                label = F.softmax(label, axis=-1)\n                soft_label = True\n            else:\n                soft_label = False\n            loss = F.cross_entropy(x, label=label, soft_label=soft_label)\n        return loss\n\n\nclass KLJSLoss(object):\n    def __init__(self, mode=\"kl\"):\n        assert mode in [\n            \"kl\",\n            \"js\",\n            \"KL\",\n            \"JS\",\n        ], \"mode can only be one of ['kl', 'KL', 'js', 'JS']\"\n        self.mode = mode\n\n    def __call__(self, p1, p2, reduction=\"mean\", eps=1e-5):\n        if self.mode.lower() == \"kl\":\n            loss = paddle.multiply(p2, paddle.log((p2 + eps) / (p1 + eps) + eps))\n            loss += paddle.multiply(p1, paddle.log((p1 + eps) / (p2 + eps) + eps))\n            loss *= 0.5\n        elif self.mode.lower() == \"js\":\n            loss = paddle.multiply(\n                p2, paddle.log((2 * p2 + eps) / (p1 + p2 + eps) + eps)\n            )\n            loss += paddle.multiply(\n                p1, paddle.log((2 * p1 + eps) / (p1 + p2 + eps) + eps)\n            )\n            loss *= 0.5\n        else:\n            raise ValueError(\n                \"The mode.lower() if KLJSLoss should be one of ['kl', 'js']\"\n            )\n\n        if reduction == \"mean\":\n            loss = paddle.mean(loss, axis=[1, 2])\n        elif reduction == \"none\" or reduction is None:\n            return loss\n        else:\n            loss = paddle.sum(loss, axis=[1, 2])\n\n        return loss\n\n\nclass DMLLoss(nn.Layer):\n    \"\"\"\n    DMLLoss\n    \"\"\"\n\n    def __init__(self, act=None, use_log=False):\n        super().__init__()\n        if act is not None:\n            assert act in [\"softmax\", \"sigmoid\"]\n        if act == \"softmax\":\n            self.act = nn.Softmax(axis=-1)\n        elif act == \"sigmoid\":\n            self.act = nn.Sigmoid()\n        else:\n            self.act = None\n\n        self.use_log = use_log\n        self.jskl_loss = KLJSLoss(mode=\"kl\")\n\n    def _kldiv(self, x, target):\n        eps = 1.0e-10\n        loss = target * (paddle.log(target + eps) - x)\n        # batch mean loss\n        loss = paddle.sum(loss) / loss.shape[0]\n        return loss\n\n    def forward(self, out1, out2):\n        if self.act is not None:\n            out1 = self.act(out1) + 1e-10\n            out2 = self.act(out2) + 1e-10\n        if self.use_log:\n            # for recognition distillation, log is needed for feature map\n            log_out1 = paddle.log(out1)\n            log_out2 = paddle.log(out2)\n            loss = (self._kldiv(log_out1, out2) + self._kldiv(log_out2, out1)) / 2.0\n        else:\n            # for detection distillation log is not needed\n            loss = self.jskl_loss(out1, out2)\n        return loss\n\n\nclass DistanceLoss(nn.Layer):\n    \"\"\"\n    DistanceLoss:\n        mode: loss mode\n    \"\"\"\n\n    def __init__(self, mode=\"l2\", **kargs):\n        super().__init__()\n        assert mode in [\"l1\", \"l2\", \"smooth_l1\"]\n        if mode == \"l1\":\n            self.loss_func = nn.L1Loss(**kargs)\n        elif mode == \"l2\":\n            self.loss_func = nn.MSELoss(**kargs)\n        elif mode == \"smooth_l1\":\n            self.loss_func = nn.SmoothL1Loss(**kargs)\n\n    def forward(self, x, y):\n        return self.loss_func(x, y)\n\n\nclass LossFromOutput(nn.Layer):\n    def __init__(self, key=\"loss\", reduction=\"none\"):\n        super().__init__()\n        self.key = key\n        self.reduction = reduction\n\n    def forward(self, predicts, batch):\n        loss = predicts\n        if self.key is not None and isinstance(predicts, dict):\n            loss = loss[self.key]\n        if self.reduction == \"mean\":\n            loss = paddle.mean(loss)\n        elif self.reduction == \"sum\":\n            loss = paddle.sum(loss)\n        return {\"loss\": loss}\n\n\nclass KLDivLoss(nn.Layer):\n    \"\"\"\n    KLDivLoss\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n    def _kldiv(self, x, target, mask=None):\n        eps = 1.0e-10\n        loss = target * (paddle.log(target + eps) - x)\n        if mask is not None:\n            loss = loss.flatten(0, 1).sum(axis=1)\n            loss = loss.masked_select(mask).mean()\n        else:\n            # batch mean loss\n            loss = paddle.sum(loss) / loss.shape[0]\n        return loss\n\n    def forward(self, logits_s, logits_t, mask=None):\n        log_out_s = F.log_softmax(logits_s, axis=-1)\n        out_t = F.softmax(logits_t, axis=-1)\n        loss = self._kldiv(log_out_s, out_t, mask)\n        return loss\n\n\nclass DKDLoss(nn.Layer):\n    \"\"\"\n    KLDivLoss\n    \"\"\"\n\n    def __init__(self, temperature=1.0, alpha=1.0, beta=1.0):\n        super().__init__()\n        self.temperature = temperature\n        self.alpha = alpha\n        self.beta = beta\n\n    def _cat_mask(self, t, mask1, mask2):\n        t1 = (t * mask1).sum(axis=1, keepdim=True)\n        t2 = (t * mask2).sum(axis=1, keepdim=True)\n        rt = paddle.concat([t1, t2], axis=1)\n        return rt\n\n    def _kl_div(self, x, label, mask=None):\n        y = (label * (paddle.log(label + 1e-10) - x)).sum(axis=1)\n        if mask is not None:\n            y = y.masked_select(mask).mean()\n        else:\n            y = y.mean()\n        return y\n\n    def forward(self, logits_student, logits_teacher, target, mask=None):\n        gt_mask = F.one_hot(target.reshape([-1]), num_classes=logits_student.shape[-1])\n        other_mask = 1 - gt_mask\n        logits_student = logits_student.flatten(0, 1)\n        logits_teacher = logits_teacher.flatten(0, 1)\n        pred_student = F.softmax(logits_student / self.temperature, axis=1)\n        pred_teacher = F.softmax(logits_teacher / self.temperature, axis=1)\n        pred_student = self._cat_mask(pred_student, gt_mask, other_mask)\n        pred_teacher = self._cat_mask(pred_teacher, gt_mask, other_mask)\n        log_pred_student = paddle.log(pred_student)\n        tckd_loss = self._kl_div(log_pred_student, pred_teacher) * (self.temperature**2)\n        pred_teacher_part2 = F.softmax(\n            logits_teacher / self.temperature - 1000.0 * gt_mask, axis=1\n        )\n        log_pred_student_part2 = F.log_softmax(\n            logits_student / self.temperature - 1000.0 * gt_mask, axis=1\n        )\n        nckd_loss = self._kl_div(log_pred_student_part2, pred_teacher_part2) * (\n            self.temperature**2\n        )\n\n        loss = self.alpha * tckd_loss + self.beta * nckd_loss\n\n        return loss\n", "ppocr/losses/rec_cppd_loss.py": "# copyright (c) 2023 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport paddle\nfrom paddle import nn\nimport paddle.nn.functional as F\n\n\nclass CPPDLoss(nn.Layer):\n    def __init__(\n        self, smoothing=False, ignore_index=100, sideloss_weight=1.0, **kwargs\n    ):\n        super(CPPDLoss, self).__init__()\n        self.edge_ce = nn.CrossEntropyLoss(reduction=\"mean\", ignore_index=ignore_index)\n        self.char_node_ce = nn.CrossEntropyLoss(reduction=\"mean\")\n        self.pos_node_ce = nn.BCEWithLogitsLoss(reduction=\"mean\")\n        self.smoothing = smoothing\n        self.ignore_index = ignore_index\n        self.sideloss_weight = sideloss_weight\n\n    def label_smoothing_ce(self, preds, targets):\n        non_pad_mask = paddle.not_equal(\n            targets,\n            paddle.zeros(targets.shape, dtype=targets.dtype) + self.ignore_index,\n        )\n        tgts = paddle.where(\n            targets\n            == (paddle.zeros(targets.shape, dtype=targets.dtype) + self.ignore_index),\n            paddle.zeros(targets.shape, dtype=targets.dtype),\n            targets,\n        )\n        eps = 0.1\n        n_class = preds.shape[1]\n        one_hot = F.one_hot(tgts, preds.shape[1])\n        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n        log_prb = F.log_softmax(preds, axis=1)\n        loss = -(one_hot * log_prb).sum(axis=1)\n        loss = loss.masked_select(non_pad_mask).mean()\n        return loss\n\n    def forward(self, pred, batch):\n        node_feats, edge_feats = pred\n        node_tgt = batch[2]\n        char_tgt = batch[1]\n\n        loss_char_node = self.char_node_ce(\n            node_feats[0].flatten(0, 1), node_tgt[:, :-26].flatten(0, 1)\n        )\n        loss_pos_node = self.pos_node_ce(\n            node_feats[1].flatten(0, 1), node_tgt[:, -26:].flatten(0, 1).cast(\"float32\")\n        )\n        loss_node = loss_char_node + loss_pos_node\n\n        edge_feats = edge_feats.flatten(0, 1)\n        char_tgt = char_tgt.flatten(0, 1)\n        if self.smoothing:\n            loss_edge = self.label_smoothing_ce(edge_feats, char_tgt)\n        else:\n            loss_edge = self.edge_ce(edge_feats, char_tgt)\n\n        return {\n            \"loss\": self.sideloss_weight * loss_node + loss_edge,\n            \"loss_node\": self.sideloss_weight * loss_node,\n            \"loss_edge\": loss_edge,\n        }\n", "ppocr/losses/vqa_token_layoutlm_loss.py": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom paddle import nn\nfrom ppocr.losses.basic_loss import DMLLoss\n\n\nclass VQASerTokenLayoutLMLoss(nn.Layer):\n    def __init__(self, num_classes, key=None):\n        super().__init__()\n        self.loss_class = nn.CrossEntropyLoss()\n        self.num_classes = num_classes\n        self.ignore_index = self.loss_class.ignore_index\n        self.key = key\n\n    def forward(self, predicts, batch):\n        if isinstance(predicts, dict) and self.key is not None:\n            predicts = predicts[self.key]\n        labels = batch[5]\n        attention_mask = batch[2]\n        if attention_mask is not None:\n            active_loss = (\n                attention_mask.reshape(\n                    [\n                        -1,\n                    ]\n                )\n                == 1\n            )\n            active_output = predicts.reshape([-1, self.num_classes])[active_loss]\n            active_label = labels.reshape(\n                [\n                    -1,\n                ]\n            )[active_loss]\n            loss = self.loss_class(active_output, active_label)\n        else:\n            loss = self.loss_class(\n                predicts.reshape([-1, self.num_classes]),\n                labels.reshape(\n                    [\n                        -1,\n                    ]\n                ),\n            )\n        return {\"loss\": loss}\n", "ppocr/utils/logging.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/WenmuZhou/PytorchOCR/blob/master/torchocr/utils/logging.py\n\"\"\"\n\nimport os\nimport sys\nimport logging\nimport functools\nimport paddle.distributed as dist\n\nlogger_initialized = {}\n\n\n@functools.lru_cache()\ndef get_logger(name=\"ppocr\", log_file=None, log_level=logging.DEBUG):\n    \"\"\"Initialize and get a logger by name.\n    If the logger has not been initialized, this method will initialize the\n    logger by adding one or two handlers, otherwise the initialized logger will\n    be directly returned. During initialization, a StreamHandler will always be\n    added. If `log_file` is specified a FileHandler will also be added.\n    Args:\n        name (str): Logger name.\n        log_file (str | None): The log filename. If specified, a FileHandler\n            will be added to the logger.\n        log_level (int): The logger level. Note that only the process of\n            rank 0 is affected, and other processes will set the level to\n            \"Error\" thus be silent most of the time.\n    Returns:\n        logging.Logger: The expected logger.\n    \"\"\"\n    logger = logging.getLogger(name)\n    if name in logger_initialized:\n        return logger\n    for logger_name in logger_initialized:\n        if name.startswith(logger_name):\n            return logger\n\n    formatter = logging.Formatter(\n        \"[%(asctime)s] %(name)s %(levelname)s: %(message)s\", datefmt=\"%Y/%m/%d %H:%M:%S\"\n    )\n\n    stream_handler = logging.StreamHandler(stream=sys.stdout)\n    stream_handler.setFormatter(formatter)\n    logger.addHandler(stream_handler)\n    if log_file is not None and dist.get_rank() == 0:\n        log_file_folder = os.path.split(log_file)[0]\n        os.makedirs(log_file_folder, exist_ok=True)\n        file_handler = logging.FileHandler(log_file, \"a\")\n        file_handler.setFormatter(formatter)\n        logger.addHandler(file_handler)\n    if dist.get_rank() == 0:\n        logger.setLevel(log_level)\n    else:\n        logger.setLevel(logging.ERROR)\n    logger_initialized[name] = True\n    logger.propagate = False\n    return logger\n", "ppocr/utils/iou.py": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/whai362/PSENet/blob/python3/models/loss/iou.py\n\"\"\"\n\nimport paddle\n\nEPS = 1e-6\n\n\ndef iou_single(a, b, mask, n_class):\n    valid = mask == 1\n    a = a.masked_select(valid)\n    b = b.masked_select(valid)\n    miou = []\n    for i in range(n_class):\n        if a.shape == [0] and a.shape == b.shape:\n            inter = paddle.to_tensor(0.0)\n            union = paddle.to_tensor(0.0)\n        else:\n            inter = ((a == i).logical_and(b == i)).astype(\"float32\")\n            union = ((a == i).logical_or(b == i)).astype(\"float32\")\n        miou.append(paddle.sum(inter) / (paddle.sum(union) + EPS))\n    miou = sum(miou) / len(miou)\n    return miou\n\n\ndef iou(a, b, mask, n_class=2, reduce=True):\n    batch_size = a.shape[0]\n\n    a = a.reshape([batch_size, -1])\n    b = b.reshape([batch_size, -1])\n    mask = mask.reshape([batch_size, -1])\n\n    iou = paddle.zeros((batch_size,), dtype=\"float32\")\n    for i in range(batch_size):\n        iou[i] = iou_single(a[i], b[i], mask[i], n_class)\n\n    if reduce:\n        iou = paddle.mean(iou)\n    return iou\n", "ppocr/utils/utility.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport logging\nimport os\nimport cv2\nimport random\nimport numpy as np\nimport paddle\nimport importlib.util\nimport sys\nimport subprocess\n\n\ndef print_dict(d, logger, delimiter=0):\n    \"\"\"\n    Recursively visualize a dict and\n    indenting acrrording by the relationship of keys.\n    \"\"\"\n    for k, v in sorted(d.items()):\n        if isinstance(v, dict):\n            logger.info(\"{}{} : \".format(delimiter * \" \", str(k)))\n            print_dict(v, logger, delimiter + 4)\n        elif isinstance(v, list) and len(v) >= 1 and isinstance(v[0], dict):\n            logger.info(\"{}{} : \".format(delimiter * \" \", str(k)))\n            for value in v:\n                print_dict(value, logger, delimiter + 4)\n        else:\n            logger.info(\"{}{} : {}\".format(delimiter * \" \", k, v))\n\n\ndef get_check_global_params(mode):\n    check_params = [\n        \"use_gpu\",\n        \"max_text_length\",\n        \"image_shape\",\n        \"image_shape\",\n        \"character_type\",\n        \"loss_type\",\n    ]\n    if mode == \"train_eval\":\n        check_params = check_params + [\n            \"train_batch_size_per_card\",\n            \"test_batch_size_per_card\",\n        ]\n    elif mode == \"test\":\n        check_params = check_params + [\"test_batch_size_per_card\"]\n    return check_params\n\n\ndef _check_image_file(path):\n    img_end = {\"jpg\", \"bmp\", \"png\", \"jpeg\", \"rgb\", \"tif\", \"tiff\", \"gif\", \"pdf\"}\n    return any([path.lower().endswith(e) for e in img_end])\n\n\ndef get_image_file_list(img_file, infer_list=None):\n    imgs_lists = []\n    if img_file is None or not os.path.exists(img_file):\n        raise Exception(\"not found any img file in {}\".format(img_file))\n\n    if os.path.isfile(img_file) and _check_image_file(img_file):\n        imgs_lists.append(img_file)\n    elif os.path.isdir(img_file):\n        for single_file in os.listdir(img_file):\n            file_path = os.path.join(img_file, single_file)\n            if os.path.isfile(file_path) and _check_image_file(file_path):\n                imgs_lists.append(file_path)\n\n    if len(imgs_lists) == 0:\n        raise Exception(\"not found any img file in {}\".format(img_file))\n    imgs_lists = sorted(imgs_lists)\n    return imgs_lists\n\n\ndef binarize_img(img):\n    if len(img.shape) == 3 and img.shape[2] == 3:\n        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # conversion to grayscale image\n        # use cv2 threshold binarization\n        _, gray = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n        img = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)\n    return img\n\n\ndef alpha_to_color(img, alpha_color=(255, 255, 255)):\n    if len(img.shape) == 3 and img.shape[2] == 4:\n        B, G, R, A = cv2.split(img)\n        alpha = A / 255\n\n        R = (alpha_color[0] * (1 - alpha) + R * alpha).astype(np.uint8)\n        G = (alpha_color[1] * (1 - alpha) + G * alpha).astype(np.uint8)\n        B = (alpha_color[2] * (1 - alpha) + B * alpha).astype(np.uint8)\n\n        img = cv2.merge((B, G, R))\n    return img\n\n\ndef check_and_read(img_path):\n    if os.path.basename(img_path)[-3:].lower() == \"gif\":\n        gif = cv2.VideoCapture(img_path)\n        ret, frame = gif.read()\n        if not ret:\n            logger = logging.getLogger(\"ppocr\")\n            logger.info(\"Cannot read {}. This gif image maybe corrupted.\")\n            return None, False\n        if len(frame.shape) == 2 or frame.shape[-1] == 1:\n            frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n        imgvalue = frame[:, :, ::-1]\n        return imgvalue, True, False\n    elif os.path.basename(img_path)[-3:].lower() == \"pdf\":\n        from paddle.utils import try_import\n\n        fitz = try_import(\"fitz\")\n        from PIL import Image\n\n        imgs = []\n        with fitz.open(img_path) as pdf:\n            for pg in range(0, pdf.page_count):\n                page = pdf[pg]\n                mat = fitz.Matrix(2, 2)\n                pm = page.get_pixmap(matrix=mat, alpha=False)\n\n                # if width or height > 2000 pixels, don't enlarge the image\n                if pm.width > 2000 or pm.height > 2000:\n                    pm = page.get_pixmap(matrix=fitz.Matrix(1, 1), alpha=False)\n\n                img = Image.frombytes(\"RGB\", [pm.width, pm.height], pm.samples)\n                img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n                imgs.append(img)\n            return imgs, False, True\n    return None, False, False\n\n\ndef load_vqa_bio_label_maps(label_map_path):\n    with open(label_map_path, \"r\", encoding=\"utf-8\") as fin:\n        lines = fin.readlines()\n    old_lines = [line.strip() for line in lines]\n    lines = [\"O\"]\n    for line in old_lines:\n        # \"O\" has already been in lines\n        if line.upper() in [\"OTHER\", \"OTHERS\", \"IGNORE\"]:\n            continue\n        lines.append(line)\n    labels = [\"O\"]\n    for line in lines[1:]:\n        labels.append(\"B-\" + line)\n        labels.append(\"I-\" + line)\n    label2id_map = {label.upper(): idx for idx, label in enumerate(labels)}\n    id2label_map = {idx: label.upper() for idx, label in enumerate(labels)}\n    return label2id_map, id2label_map\n\n\ndef set_seed(seed=1024):\n    random.seed(seed)\n    np.random.seed(seed)\n    paddle.seed(seed)\n\n\ndef check_install(module_name, install_name):\n    spec = importlib.util.find_spec(module_name)\n    if spec is None:\n        print(f\"Warnning! The {module_name} module is NOT installed\")\n        print(\n            f\"Try install {module_name} module automatically. You can also try to install manually by pip install {install_name}.\"\n        )\n        python = sys.executable\n        try:\n            subprocess.check_call(\n                [python, \"-m\", \"pip\", \"install\", install_name],\n                stdout=subprocess.DEVNULL,\n            )\n            print(f\"The {module_name} module is now installed\")\n        except subprocess.CalledProcessError as exc:\n            raise Exception(f\"Install {module_name} failed, please install manually\")\n    else:\n        print(f\"{module_name} has been installed.\")\n\n\nclass AverageMeter:\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        \"\"\"reset\"\"\"\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        \"\"\"update\"\"\"\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n", "ppocr/utils/poly_nms.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nfrom shapely.geometry import Polygon\n\n\ndef points2polygon(points):\n    \"\"\"Convert k points to 1 polygon.\n\n    Args:\n        points (ndarray or list): A ndarray or a list of shape (2k)\n            that indicates k points.\n\n    Returns:\n        polygon (Polygon): A polygon object.\n    \"\"\"\n    if isinstance(points, list):\n        points = np.array(points)\n\n    assert isinstance(points, np.ndarray)\n    assert (points.size % 2 == 0) and (points.size >= 8)\n\n    point_mat = points.reshape([-1, 2])\n    return Polygon(point_mat)\n\n\ndef poly_intersection(poly_det, poly_gt, buffer=0.0001):\n    \"\"\"Calculate the intersection area between two polygon.\n\n    Args:\n        poly_det (Polygon): A polygon predicted by detector.\n        poly_gt (Polygon): A gt polygon.\n\n    Returns:\n        intersection_area (float): The intersection area between two polygons.\n    \"\"\"\n    assert isinstance(poly_det, Polygon)\n    assert isinstance(poly_gt, Polygon)\n\n    if buffer == 0:\n        poly_inter = poly_det & poly_gt\n    else:\n        poly_inter = poly_det.buffer(buffer) & poly_gt.buffer(buffer)\n    return poly_inter.area, poly_inter\n\n\ndef poly_union(poly_det, poly_gt):\n    \"\"\"Calculate the union area between two polygon.\n\n    Args:\n        poly_det (Polygon): A polygon predicted by detector.\n        poly_gt (Polygon): A gt polygon.\n\n    Returns:\n        union_area (float): The union area between two polygons.\n    \"\"\"\n    assert isinstance(poly_det, Polygon)\n    assert isinstance(poly_gt, Polygon)\n\n    area_det = poly_det.area\n    area_gt = poly_gt.area\n    area_inters, _ = poly_intersection(poly_det, poly_gt)\n    return area_det + area_gt - area_inters\n\n\ndef valid_boundary(x, with_score=True):\n    num = len(x)\n    if num < 8:\n        return False\n    if num % 2 == 0 and (not with_score):\n        return True\n    if num % 2 == 1 and with_score:\n        return True\n\n    return False\n\n\ndef boundary_iou(src, target):\n    \"\"\"Calculate the IOU between two boundaries.\n\n    Args:\n       src (list): Source boundary.\n       target (list): Target boundary.\n\n    Returns:\n       iou (float): The iou between two boundaries.\n    \"\"\"\n    assert valid_boundary(src, False)\n    assert valid_boundary(target, False)\n    src_poly = points2polygon(src)\n    target_poly = points2polygon(target)\n\n    return poly_iou(src_poly, target_poly)\n\n\ndef poly_iou(poly_det, poly_gt):\n    \"\"\"Calculate the IOU between two polygons.\n\n    Args:\n        poly_det (Polygon): A polygon predicted by detector.\n        poly_gt (Polygon): A gt polygon.\n\n    Returns:\n        iou (float): The IOU between two polygons.\n    \"\"\"\n    assert isinstance(poly_det, Polygon)\n    assert isinstance(poly_gt, Polygon)\n    area_inters, _ = poly_intersection(poly_det, poly_gt)\n    area_union = poly_union(poly_det, poly_gt)\n    if area_union == 0:\n        return 0.0\n    return area_inters / area_union\n\n\ndef poly_nms(polygons, threshold):\n    assert isinstance(polygons, list)\n\n    polygons = np.array(sorted(polygons, key=lambda x: x[-1]))\n\n    keep_poly = []\n    index = [i for i in range(polygons.shape[0])]\n\n    while len(index) > 0:\n        keep_poly.append(polygons[index[-1]].tolist())\n        A = polygons[index[-1]][:-1]\n        index = np.delete(index, -1)\n        iou_list = np.zeros((len(index),))\n        for i in range(len(index)):\n            B = polygons[index[i]][:-1]\n            iou_list[i] = boundary_iou(A, B)\n        remove_index = np.where(iou_list > threshold)\n        index = np.delete(index, remove_index)\n\n    return keep_poly\n", "ppocr/utils/stats.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport collections\nimport numpy as np\nimport datetime\n\n__all__ = [\"TrainingStats\", \"Time\"]\n\n\nclass SmoothedValue(object):\n    \"\"\"Track a series of values and provide access to smoothed values over a\n    window or the global series average.\n    \"\"\"\n\n    def __init__(self, window_size):\n        self.deque = collections.deque(maxlen=window_size)\n\n    def add_value(self, value):\n        self.deque.append(value)\n\n    def get_median_value(self):\n        return np.median(self.deque)\n\n\ndef Time():\n    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n\n\nclass TrainingStats(object):\n    def __init__(self, window_size, stats_keys):\n        self.window_size = window_size\n        self.smoothed_losses_and_metrics = {\n            key: SmoothedValue(window_size) for key in stats_keys\n        }\n\n    def update(self, stats):\n        for k, v in stats.items():\n            if k not in self.smoothed_losses_and_metrics:\n                self.smoothed_losses_and_metrics[k] = SmoothedValue(self.window_size)\n            self.smoothed_losses_and_metrics[k].add_value(v)\n\n    def get(self, extras=None):\n        stats = collections.OrderedDict()\n        if extras:\n            for k, v in extras.items():\n                stats[k] = v\n        for k, v in self.smoothed_losses_and_metrics.items():\n            stats[k] = round(v.get_median_value(), 6)\n\n        return stats\n\n    def log(self, extras=None):\n        d = self.get(extras)\n        strs = []\n        for k, v in d.items():\n            strs.append(\"{}: {:x<6f}\".format(k, v))\n        strs = \", \".join(strs)\n        return strs\n", "ppocr/utils/profiler.py": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\nimport paddle.profiler as profiler\n\n# A global variable to record the number of calling times for profiler\n# functions. It is used to specify the tracing range of training steps.\n_profiler_step_id = 0\n\n# A global variable to avoid parsing from string every time.\n_profiler_options = None\n_prof = None\n\n\nclass ProfilerOptions(object):\n    \"\"\"\n    Use a string to initialize a ProfilerOptions.\n    The string should be in the format: \"key1=value1;key2=value;key3=value3\".\n    For example:\n      \"profile_path=model.profile\"\n      \"batch_range=[50, 60]; profile_path=model.profile\"\n      \"batch_range=[50, 60]; tracer_option=OpDetail; profile_path=model.profile\"\n\n    ProfilerOptions supports following key-value pair:\n      batch_range      - a integer list, e.g. [100, 110].\n      state            - a string, the optional values are 'CPU', 'GPU' or 'All'.\n      sorted_key       - a string, the optional values are 'calls', 'total',\n                         'max', 'min' or 'ave.\n      tracer_option    - a string, the optional values are 'Default', 'OpDetail',\n                         'AllOpDetail'.\n      profile_path     - a string, the path to save the serialized profile data,\n                         which can be used to generate a timeline.\n      exit_on_finished - a boolean.\n    \"\"\"\n\n    def __init__(self, options_str):\n        assert isinstance(options_str, str)\n\n        self._options = {\n            \"batch_range\": [10, 20],\n            \"state\": \"All\",\n            \"sorted_key\": \"total\",\n            \"tracer_option\": \"Default\",\n            \"profile_path\": \"/tmp/profile\",\n            \"exit_on_finished\": True,\n            \"timer_only\": True,\n        }\n        self._parse_from_string(options_str)\n\n    def _parse_from_string(self, options_str):\n        for kv in options_str.replace(\" \", \"\").split(\";\"):\n            key, value = kv.split(\"=\")\n            if key == \"batch_range\":\n                value_list = value.replace(\"[\", \"\").replace(\"]\", \"\").split(\",\")\n                value_list = list(map(int, value_list))\n                if (\n                    len(value_list) >= 2\n                    and value_list[0] >= 0\n                    and value_list[1] > value_list[0]\n                ):\n                    self._options[key] = value_list\n            elif key == \"exit_on_finished\":\n                self._options[key] = value.lower() in (\"yes\", \"true\", \"t\", \"1\")\n            elif key in [\"state\", \"sorted_key\", \"tracer_option\", \"profile_path\"]:\n                self._options[key] = value\n            elif key == \"timer_only\":\n                self._options[key] = value\n\n    def __getitem__(self, name):\n        if self._options.get(name, None) is None:\n            raise ValueError(\"ProfilerOptions does not have an option named %s.\" % name)\n        return self._options[name]\n\n\ndef add_profiler_step(options_str=None):\n    \"\"\"\n    Enable the operator-level timing using PaddlePaddle's profiler.\n    The profiler uses a independent variable to count the profiler steps.\n    One call of this function is treated as a profiler step.\n    Args:\n      profiler_options - a string to initialize the ProfilerOptions.\n                         Default is None, and the profiler is disabled.\n    \"\"\"\n    if options_str is None:\n        return\n\n    global _prof\n    global _profiler_step_id\n    global _profiler_options\n\n    if _profiler_options is None:\n        _profiler_options = ProfilerOptions(options_str)\n    # profile : https://www.paddlepaddle.org.cn/documentation/docs/zh/guides/performance_improving/profiling_model.html#chakanxingnengshujudetongjibiaodan\n    # timer_only = True  only the model's throughput and time overhead are displayed\n    # timer_only = False calling summary can print a statistical form that presents performance data from different perspectives.\n    # timer_only = False the output Timeline information can be found in the profiler_log directory\n    if _prof is None:\n        _timer_only = str(_profiler_options[\"timer_only\"]) == str(True)\n        _prof = profiler.Profiler(\n            scheduler=(\n                _profiler_options[\"batch_range\"][0],\n                _profiler_options[\"batch_range\"][1],\n            ),\n            on_trace_ready=profiler.export_chrome_tracing(\"./profiler_log\"),\n            timer_only=_timer_only,\n        )\n        _prof.start()\n    else:\n        _prof.step()\n\n    if _profiler_step_id == _profiler_options[\"batch_range\"][1]:\n        _prof.stop()\n        _prof.summary(op_detail=True, thread_sep=False, time_unit=\"ms\")\n        _prof = None\n        if _profiler_options[\"exit_on_finished\"]:\n            sys.exit(0)\n\n    _profiler_step_id += 1\n", "ppocr/utils/__init__.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n", "ppocr/utils/save_load.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport errno\nimport os\nimport pickle\nimport six\n\nimport paddle\n\nfrom ppocr.utils.logging import get_logger\nfrom ppocr.utils.network import maybe_download_params\n\n__all__ = [\"load_model\"]\n\n\ndef _mkdir_if_not_exist(path, logger):\n    \"\"\"\n    mkdir if not exists, ignore the exception when multiprocess mkdir together\n    \"\"\"\n    if not os.path.exists(path):\n        try:\n            os.makedirs(path)\n        except OSError as e:\n            if e.errno == errno.EEXIST and os.path.isdir(path):\n                logger.warning(\n                    \"be happy if some process has already created {}\".format(path)\n                )\n            else:\n                raise OSError(\"Failed to mkdir {}\".format(path))\n\n\ndef load_model(config, model, optimizer=None, model_type=\"det\"):\n    \"\"\"\n    load model from checkpoint or pretrained_model\n    \"\"\"\n    logger = get_logger()\n    global_config = config[\"Global\"]\n    checkpoints = global_config.get(\"checkpoints\")\n    pretrained_model = global_config.get(\"pretrained_model\")\n    best_model_dict = {}\n    is_float16 = False\n    is_nlp_model = model_type == \"kie\" and config[\"Architecture\"][\"algorithm\"] not in [\n        \"SDMGR\"\n    ]\n\n    if is_nlp_model is True:\n        # NOTE: for kie model dsitillation, resume training is not supported now\n        if config[\"Architecture\"][\"algorithm\"] in [\"Distillation\"]:\n            return best_model_dict\n        checkpoints = config[\"Architecture\"][\"Backbone\"][\"checkpoints\"]\n        # load kie method metric\n        if checkpoints:\n            if os.path.exists(os.path.join(checkpoints, \"metric.states\")):\n                with open(os.path.join(checkpoints, \"metric.states\"), \"rb\") as f:\n                    states_dict = (\n                        pickle.load(f) if six.PY2 else pickle.load(f, encoding=\"latin1\")\n                    )\n                best_model_dict = states_dict.get(\"best_model_dict\", {})\n                if \"epoch\" in states_dict:\n                    best_model_dict[\"start_epoch\"] = states_dict[\"epoch\"] + 1\n            logger.info(\"resume from {}\".format(checkpoints))\n\n            if optimizer is not None:\n                if checkpoints[-1] in [\"/\", \"\\\\\"]:\n                    checkpoints = checkpoints[:-1]\n                if os.path.exists(checkpoints + \".pdopt\"):\n                    optim_dict = paddle.load(checkpoints + \".pdopt\")\n                    optimizer.set_state_dict(optim_dict)\n                else:\n                    logger.warning(\n                        \"{}.pdopt is not exists, params of optimizer is not loaded\".format(\n                            checkpoints\n                        )\n                    )\n\n        return best_model_dict\n\n    if checkpoints:\n        if checkpoints.endswith(\".pdparams\"):\n            checkpoints = checkpoints.replace(\".pdparams\", \"\")\n        assert os.path.exists(\n            checkpoints + \".pdparams\"\n        ), \"The {}.pdparams does not exists!\".format(checkpoints)\n\n        # load params from trained model\n        params = paddle.load(checkpoints + \".pdparams\")\n        state_dict = model.state_dict()\n        new_state_dict = {}\n        for key, value in state_dict.items():\n            if key not in params:\n                logger.warning(\n                    \"{} not in loaded params {} !\".format(key, params.keys())\n                )\n                continue\n            pre_value = params[key]\n            if pre_value.dtype == paddle.float16:\n                is_float16 = True\n            if pre_value.dtype != value.dtype:\n                pre_value = pre_value.astype(value.dtype)\n            if list(value.shape) == list(pre_value.shape):\n                new_state_dict[key] = pre_value\n            else:\n                logger.warning(\n                    \"The shape of model params {} {} not matched with loaded params shape {} !\".format(\n                        key, value.shape, pre_value.shape\n                    )\n                )\n        model.set_state_dict(new_state_dict)\n        if is_float16:\n            logger.info(\n                \"The parameter type is float16, which is converted to float32 when loading\"\n            )\n        if optimizer is not None:\n            if os.path.exists(checkpoints + \".pdopt\"):\n                optim_dict = paddle.load(checkpoints + \".pdopt\")\n                optimizer.set_state_dict(optim_dict)\n            else:\n                logger.warning(\n                    \"{}.pdopt is not exists, params of optimizer is not loaded\".format(\n                        checkpoints\n                    )\n                )\n\n        if os.path.exists(checkpoints + \".states\"):\n            with open(checkpoints + \".states\", \"rb\") as f:\n                states_dict = (\n                    pickle.load(f) if six.PY2 else pickle.load(f, encoding=\"latin1\")\n                )\n            best_model_dict = states_dict.get(\"best_model_dict\", {})\n            if \"epoch\" in states_dict:\n                best_model_dict[\"start_epoch\"] = states_dict[\"epoch\"] + 1\n        logger.info(\"resume from {}\".format(checkpoints))\n    elif pretrained_model:\n        is_float16 = load_pretrained_params(model, pretrained_model)\n    else:\n        logger.info(\"train from scratch\")\n    best_model_dict[\"is_float16\"] = is_float16\n    return best_model_dict\n\n\ndef load_pretrained_params(model, path):\n    logger = get_logger()\n    path = maybe_download_params(path)\n    if path.endswith(\".pdparams\"):\n        path = path.replace(\".pdparams\", \"\")\n    assert os.path.exists(\n        path + \".pdparams\"\n    ), \"The {}.pdparams does not exists!\".format(path)\n\n    params = paddle.load(path + \".pdparams\")\n\n    state_dict = model.state_dict()\n\n    new_state_dict = {}\n    is_float16 = False\n\n    for k1 in params.keys():\n        if k1 not in state_dict.keys():\n            logger.warning(\"The pretrained params {} not in model\".format(k1))\n        else:\n            if params[k1].dtype == paddle.float16:\n                is_float16 = True\n            if params[k1].dtype != state_dict[k1].dtype:\n                params[k1] = params[k1].astype(state_dict[k1].dtype)\n            if list(state_dict[k1].shape) == list(params[k1].shape):\n                new_state_dict[k1] = params[k1]\n            else:\n                logger.warning(\n                    \"The shape of model params {} {} not matched with loaded params {} {} !\".format(\n                        k1, state_dict[k1].shape, k1, params[k1].shape\n                    )\n                )\n\n    model.set_state_dict(new_state_dict)\n    if is_float16:\n        logger.info(\n            \"The parameter type is float16, which is converted to float32 when loading\"\n        )\n    logger.info(\"load pretrain successful from {}\".format(path))\n    return is_float16\n\n\ndef save_model(\n    model,\n    optimizer,\n    model_path,\n    logger,\n    config,\n    is_best=False,\n    prefix=\"ppocr\",\n    **kwargs,\n):\n    \"\"\"\n    save model to the target path\n    \"\"\"\n    _mkdir_if_not_exist(model_path, logger)\n    model_prefix = os.path.join(model_path, prefix)\n\n    if prefix == \"best_accuracy\":\n        best_model_path = os.path.join(model_path, \"best_model\")\n        _mkdir_if_not_exist(best_model_path, logger)\n\n    paddle.save(optimizer.state_dict(), model_prefix + \".pdopt\")\n    if prefix == \"best_accuracy\":\n        paddle.save(\n            optimizer.state_dict(), os.path.join(best_model_path, \"model.pdopt\")\n        )\n\n    is_nlp_model = config[\"Architecture\"][\"model_type\"] == \"kie\" and config[\n        \"Architecture\"\n    ][\"algorithm\"] not in [\"SDMGR\"]\n    if is_nlp_model is not True:\n        paddle.save(model.state_dict(), model_prefix + \".pdparams\")\n        metric_prefix = model_prefix\n\n        if prefix == \"best_accuracy\":\n            paddle.save(\n                model.state_dict(), os.path.join(best_model_path, \"model.pdparams\")\n            )\n\n    else:  # for kie system, we follow the save/load rules in NLP\n        if config[\"Global\"][\"distributed\"]:\n            arch = model._layers\n        else:\n            arch = model\n        if config[\"Architecture\"][\"algorithm\"] in [\"Distillation\"]:\n            arch = arch.Student\n        arch.backbone.model.save_pretrained(model_prefix)\n        metric_prefix = os.path.join(model_prefix, \"metric\")\n\n        if prefix == \"best_accuracy\":\n            arch.backbone.model.save_pretrained(best_model_path)\n\n    # save metric and config\n    with open(metric_prefix + \".states\", \"wb\") as f:\n        pickle.dump(kwargs, f, protocol=2)\n    if is_best:\n        logger.info(\"save best model is to {}\".format(model_prefix))\n    else:\n        logger.info(\"save model in {}\".format(model_prefix))\n", "ppocr/utils/network.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport sys\nimport tarfile\nimport requests\nfrom tqdm import tqdm\n\nfrom ppocr.utils.logging import get_logger\n\nMODELS_DIR = os.path.expanduser(\"~/.paddleocr/models/\")\n\n\ndef download_with_progressbar(url, save_path):\n    logger = get_logger()\n    if save_path and os.path.exists(save_path):\n        logger.info(f\"Path {save_path} already exists. Skipping...\")\n        return\n    response = requests.get(url, stream=True)\n    if response.status_code == 200:\n        total_size_in_bytes = int(response.headers.get(\"content-length\", 1))\n        block_size = 1024  # 1 Kibibyte\n        progress_bar = tqdm(total=total_size_in_bytes, unit=\"iB\", unit_scale=True)\n        with open(save_path, \"wb\") as file:\n            for data in response.iter_content(block_size):\n                progress_bar.update(len(data))\n                file.write(data)\n        progress_bar.close()\n    else:\n        logger.error(\"Something went wrong while downloading models\")\n        sys.exit(0)\n\n\ndef maybe_download(model_storage_directory, url):\n    # using custom model\n    tar_file_name_list = [\".pdiparams\", \".pdiparams.info\", \".pdmodel\"]\n    if not os.path.exists(\n        os.path.join(model_storage_directory, \"inference.pdiparams\")\n    ) or not os.path.exists(os.path.join(model_storage_directory, \"inference.pdmodel\")):\n        assert url.endswith(\".tar\"), \"Only supports tar compressed package\"\n        tmp_path = os.path.join(model_storage_directory, url.split(\"/\")[-1])\n        print(\"download {} to {}\".format(url, tmp_path))\n        os.makedirs(model_storage_directory, exist_ok=True)\n        download_with_progressbar(url, tmp_path)\n        with tarfile.open(tmp_path, \"r\") as tarObj:\n            for member in tarObj.getmembers():\n                filename = None\n                for tar_file_name in tar_file_name_list:\n                    if member.name.endswith(tar_file_name):\n                        filename = \"inference\" + tar_file_name\n                if filename is None:\n                    continue\n                file = tarObj.extractfile(member)\n                with open(os.path.join(model_storage_directory, filename), \"wb\") as f:\n                    f.write(file.read())\n        os.remove(tmp_path)\n\n\ndef maybe_download_params(model_path):\n    if os.path.exists(model_path) or not is_link(model_path):\n        return model_path\n    else:\n        url = model_path\n    tmp_path = os.path.join(MODELS_DIR, url.split(\"/\")[-1])\n    print(\"download {} to {}\".format(url, tmp_path))\n    os.makedirs(MODELS_DIR, exist_ok=True)\n    download_with_progressbar(url, tmp_path)\n    return tmp_path\n\n\ndef is_link(s):\n    return s is not None and s.startswith(\"http\")\n\n\ndef confirm_model_dir_url(model_dir, default_model_dir, default_url):\n    url = default_url\n    if model_dir is None or is_link(model_dir):\n        if is_link(model_dir):\n            url = model_dir\n        file_name = url.split(\"/\")[-1][:-4]\n        model_dir = default_model_dir\n        model_dir = os.path.join(model_dir, file_name)\n    return model_dir, url\n", "ppocr/utils/gen_label.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\nimport argparse\nimport json\n\n\ndef gen_rec_label(input_path, out_label):\n    with open(out_label, \"w\") as out_file:\n        with open(input_path, \"r\") as f:\n            for line in f.readlines():\n                tmp = line.strip(\"\\n\").replace(\" \", \"\").split(\",\")\n                img_path, label = tmp[0], tmp[1]\n                label = label.replace('\"', \"\")\n                out_file.write(img_path + \"\\t\" + label + \"\\n\")\n\n\ndef gen_det_label(root_path, input_dir, out_label):\n    with open(out_label, \"w\") as out_file:\n        for label_file in os.listdir(input_dir):\n            img_path = os.path.join(root_path, label_file[3:-4] + \".jpg\")\n            label = []\n            with open(\n                os.path.join(input_dir, label_file), \"r\", encoding=\"utf-8-sig\"\n            ) as f:\n                for line in f.readlines():\n                    tmp = line.strip(\"\\n\\r\").replace(\"\\xef\\xbb\\xbf\", \"\").split(\",\")\n                    points = tmp[:8]\n                    s = []\n                    for i in range(0, len(points), 2):\n                        b = points[i : i + 2]\n                        b = [int(t) for t in b]\n                        s.append(b)\n                    result = {\"transcription\": tmp[8], \"points\": s}\n                    label.append(result)\n\n            out_file.write(\n                img_path + \"\\t\" + json.dumps(label, ensure_ascii=False) + \"\\n\"\n            )\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--mode\",\n        type=str,\n        default=\"rec\",\n        help=\"Generate rec_label or det_label, can be set rec or det\",\n    )\n    parser.add_argument(\n        \"--root_path\",\n        type=str,\n        default=\".\",\n        help=\"The root directory of images.Only takes effect when mode=det \",\n    )\n    parser.add_argument(\n        \"--input_path\",\n        type=str,\n        default=\".\",\n        help=\"Input_label or input path to be converted\",\n    )\n    parser.add_argument(\n        \"--output_label\", type=str, default=\"out_label.txt\", help=\"Output file name\"\n    )\n\n    args = parser.parse_args()\n    if args.mode == \"rec\":\n        print(\"Generate rec label\")\n        gen_rec_label(args.input_path, args.output_label)\n    elif args.mode == \"det\":\n        gen_det_label(args.root_path, args.input_path, args.output_label)\n", "ppocr/utils/visual.py": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport cv2\nimport os\nimport numpy as np\nimport PIL\nfrom PIL import Image, ImageDraw, ImageFont\n\n\ndef draw_ser_results(\n    image, ocr_results, font_path=\"doc/fonts/simfang.ttf\", font_size=14\n):\n    np.random.seed(2021)\n    color = (\n        np.random.permutation(range(255)),\n        np.random.permutation(range(255)),\n        np.random.permutation(range(255)),\n    )\n    color_map = {\n        idx: (color[0][idx], color[1][idx], color[2][idx]) for idx in range(1, 255)\n    }\n    if isinstance(image, np.ndarray):\n        image = Image.fromarray(image)\n    elif isinstance(image, str) and os.path.isfile(image):\n        image = Image.open(image).convert(\"RGB\")\n    img_new = image.copy()\n    draw = ImageDraw.Draw(img_new)\n\n    font = ImageFont.truetype(font_path, font_size, encoding=\"utf-8\")\n    for ocr_info in ocr_results:\n        if ocr_info[\"pred_id\"] not in color_map:\n            continue\n        color = color_map[ocr_info[\"pred_id\"]]\n        text = \"{}: {}\".format(ocr_info[\"pred\"], ocr_info[\"transcription\"])\n\n        if \"bbox\" in ocr_info:\n            # draw with ocr engine\n            bbox = ocr_info[\"bbox\"]\n        else:\n            # draw with ocr groundtruth\n            bbox = trans_poly_to_bbox(ocr_info[\"points\"])\n        draw_box_txt(bbox, text, draw, font, font_size, color)\n\n    img_new = Image.blend(image, img_new, 0.7)\n    return np.array(img_new)\n\n\ndef draw_box_txt(bbox, text, draw, font, font_size, color):\n    # draw ocr results outline\n    bbox = ((bbox[0], bbox[1]), (bbox[2], bbox[3]))\n    draw.rectangle(bbox, fill=color)\n\n    # draw ocr results\n    if int(PIL.__version__.split(\".\")[0]) < 10:\n        tw = font.getsize(text)[0]\n        th = font.getsize(text)[1]\n    else:\n        left, top, right, bottom = font.getbbox(text)\n        tw, th = right - left, bottom - top\n\n    start_y = max(0, bbox[0][1] - th)\n    draw.rectangle(\n        [(bbox[0][0] + 1, start_y), (bbox[0][0] + tw + 1, start_y + th)],\n        fill=(0, 0, 255),\n    )\n    draw.text((bbox[0][0] + 1, start_y), text, fill=(255, 255, 255), font=font)\n\n\ndef trans_poly_to_bbox(poly):\n    x1 = np.min([p[0] for p in poly])\n    x2 = np.max([p[0] for p in poly])\n    y1 = np.min([p[1] for p in poly])\n    y2 = np.max([p[1] for p in poly])\n    return [x1, y1, x2, y2]\n\n\ndef draw_re_results(image, result, font_path=\"doc/fonts/simfang.ttf\", font_size=18):\n    np.random.seed(0)\n    if isinstance(image, np.ndarray):\n        image = Image.fromarray(image)\n    elif isinstance(image, str) and os.path.isfile(image):\n        image = Image.open(image).convert(\"RGB\")\n    img_new = image.copy()\n    draw = ImageDraw.Draw(img_new)\n\n    font = ImageFont.truetype(font_path, font_size, encoding=\"utf-8\")\n    color_head = (0, 0, 255)\n    color_tail = (255, 0, 0)\n    color_line = (0, 255, 0)\n\n    for ocr_info_head, ocr_info_tail in result:\n        draw_box_txt(\n            ocr_info_head[\"bbox\"],\n            ocr_info_head[\"transcription\"],\n            draw,\n            font,\n            font_size,\n            color_head,\n        )\n        draw_box_txt(\n            ocr_info_tail[\"bbox\"],\n            ocr_info_tail[\"transcription\"],\n            draw,\n            font,\n            font_size,\n            color_tail,\n        )\n\n        center_head = (\n            (ocr_info_head[\"bbox\"][0] + ocr_info_head[\"bbox\"][2]) // 2,\n            (ocr_info_head[\"bbox\"][1] + ocr_info_head[\"bbox\"][3]) // 2,\n        )\n        center_tail = (\n            (ocr_info_tail[\"bbox\"][0] + ocr_info_tail[\"bbox\"][2]) // 2,\n            (ocr_info_tail[\"bbox\"][1] + ocr_info_tail[\"bbox\"][3]) // 2,\n        )\n\n        draw.line([center_head, center_tail], fill=color_line, width=5)\n\n    img_new = Image.blend(image, img_new, 0.5)\n    return np.array(img_new)\n\n\ndef draw_rectangle(img_path, boxes):\n    boxes = np.array(boxes)\n    img = cv2.imread(img_path)\n    img_show = img.copy()\n    for box in boxes.astype(int):\n        x1, y1, x2, y2 = box\n        cv2.rectangle(img_show, (x1, y1), (x2, y2), (255, 0, 0), 2)\n    return img_show\n", "ppocr/utils/e2e_metric/polygon_fast.py": "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport numpy as np\nfrom shapely.geometry import Polygon\n\n\"\"\"\n:param det_x: [1, N] Xs of detection's vertices\n:param det_y: [1, N] Ys of detection's vertices\n:param gt_x: [1, N] Xs of groundtruth's vertices\n:param gt_y: [1, N] Ys of groundtruth's vertices\n\n##############\nAll the calculation of 'AREA' in this script is handled by:\n1) First generating a binary mask with the polygon area filled up with 1's\n2) Summing up all the 1's\n\"\"\"\n\n\ndef area(x, y):\n    polygon = Polygon(np.stack([x, y], axis=1))\n    return float(polygon.area)\n\n\ndef approx_area_of_intersection(det_x, det_y, gt_x, gt_y):\n    \"\"\"\n    This helper determine if both polygons are intersecting with each others with an approximation method.\n    Area of intersection represented by the minimum bounding rectangular [xmin, ymin, xmax, ymax]\n    \"\"\"\n    det_ymax = np.max(det_y)\n    det_xmax = np.max(det_x)\n    det_ymin = np.min(det_y)\n    det_xmin = np.min(det_x)\n\n    gt_ymax = np.max(gt_y)\n    gt_xmax = np.max(gt_x)\n    gt_ymin = np.min(gt_y)\n    gt_xmin = np.min(gt_x)\n\n    all_min_ymax = np.minimum(det_ymax, gt_ymax)\n    all_max_ymin = np.maximum(det_ymin, gt_ymin)\n\n    intersect_heights = np.maximum(0.0, (all_min_ymax - all_max_ymin))\n\n    all_min_xmax = np.minimum(det_xmax, gt_xmax)\n    all_max_xmin = np.maximum(det_xmin, gt_xmin)\n    intersect_widths = np.maximum(0.0, (all_min_xmax - all_max_xmin))\n\n    return intersect_heights * intersect_widths\n\n\ndef area_of_intersection(det_x, det_y, gt_x, gt_y):\n    p1 = Polygon(np.stack([det_x, det_y], axis=1)).buffer(0)\n    p2 = Polygon(np.stack([gt_x, gt_y], axis=1)).buffer(0)\n    return float(p1.intersection(p2).area)\n\n\ndef area_of_union(det_x, det_y, gt_x, gt_y):\n    p1 = Polygon(np.stack([det_x, det_y], axis=1)).buffer(0)\n    p2 = Polygon(np.stack([gt_x, gt_y], axis=1)).buffer(0)\n    return float(p1.union(p2).area)\n\n\ndef iou(det_x, det_y, gt_x, gt_y):\n    return area_of_intersection(det_x, det_y, gt_x, gt_y) / (\n        area_of_union(det_x, det_y, gt_x, gt_y) + 1.0\n    )\n\n\ndef iod(det_x, det_y, gt_x, gt_y):\n    \"\"\"\n    This helper determine the fraction of intersection area over detection area\n    \"\"\"\n    return area_of_intersection(det_x, det_y, gt_x, gt_y) / (area(det_x, det_y) + 1.0)\n", "ppocr/utils/e2e_metric/Deteval.py": "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport numpy as np\nimport scipy.io as io\n\nfrom ppocr.utils.utility import check_install\n\nfrom ppocr.utils.e2e_metric.polygon_fast import iod, area_of_intersection, area\n\n\ndef get_socre_A(gt_dir, pred_dict):\n    allInputs = 1\n\n    def input_reading_mod(pred_dict):\n        \"\"\"This helper reads input from txt files\"\"\"\n        det = []\n        n = len(pred_dict)\n        for i in range(n):\n            points = pred_dict[i][\"points\"]\n            text = pred_dict[i][\"texts\"]\n            point = \",\".join(\n                map(\n                    str,\n                    points.reshape(\n                        -1,\n                    ),\n                )\n            )\n            det.append([point, text])\n        return det\n\n    def gt_reading_mod(gt_dict):\n        \"\"\"This helper reads groundtruths from mat files\"\"\"\n        gt = []\n        n = len(gt_dict)\n        for i in range(n):\n            points = gt_dict[i][\"points\"].tolist()\n            h = len(points)\n            text = gt_dict[i][\"text\"]\n            xx = [\n                np.array([\"x:\"], dtype=\"<U2\"),\n                0,\n                np.array([\"y:\"], dtype=\"<U2\"),\n                0,\n                np.array([\"#\"], dtype=\"<U1\"),\n                np.array([\"#\"], dtype=\"<U1\"),\n            ]\n            t_x, t_y = [], []\n            for j in range(h):\n                t_x.append(points[j][0])\n                t_y.append(points[j][1])\n            xx[1] = np.array([t_x], dtype=\"int16\")\n            xx[3] = np.array([t_y], dtype=\"int16\")\n            if text != \"\":\n                xx[4] = np.array([text], dtype=\"U{}\".format(len(text)))\n                xx[5] = np.array([\"c\"], dtype=\"<U1\")\n            gt.append(xx)\n        return gt\n\n    def detection_filtering(detections, groundtruths, threshold=0.5):\n        for gt_id, gt in enumerate(groundtruths):\n            if (gt[5] == \"#\") and (gt[1].shape[1] > 1):\n                gt_x = list(map(int, np.squeeze(gt[1])))\n                gt_y = list(map(int, np.squeeze(gt[3])))\n                for det_id, detection in enumerate(detections):\n                    detection_orig = detection\n                    detection = [float(x) for x in detection[0].split(\",\")]\n                    detection = list(map(int, detection))\n                    det_x = detection[0::2]\n                    det_y = detection[1::2]\n                    det_gt_iou = iod(det_x, det_y, gt_x, gt_y)\n                    if det_gt_iou > threshold:\n                        detections[det_id] = []\n\n                detections[:] = [item for item in detections if item != []]\n        return detections\n\n    def sigma_calculation(det_x, det_y, gt_x, gt_y):\n        \"\"\"\n        sigma = inter_area / gt_area\n        \"\"\"\n        return np.round(\n            (area_of_intersection(det_x, det_y, gt_x, gt_y) / area(gt_x, gt_y)), 2\n        )\n\n    def tau_calculation(det_x, det_y, gt_x, gt_y):\n        if area(det_x, det_y) == 0.0:\n            return 0\n        return np.round(\n            (area_of_intersection(det_x, det_y, gt_x, gt_y) / area(det_x, det_y)), 2\n        )\n\n    ##############################Initialization###################################\n    # global_sigma = []\n    # global_tau = []\n    # global_pred_str = []\n    # global_gt_str = []\n    ###############################################################################\n\n    for input_id in range(allInputs):\n        if (\n            (input_id != \".DS_Store\")\n            and (input_id != \"Pascal_result.txt\")\n            and (input_id != \"Pascal_result_curved.txt\")\n            and (input_id != \"Pascal_result_non_curved.txt\")\n            and (input_id != \"Deteval_result.txt\")\n            and (input_id != \"Deteval_result_curved.txt\")\n            and (input_id != \"Deteval_result_non_curved.txt\")\n        ):\n            detections = input_reading_mod(pred_dict)\n            groundtruths = gt_reading_mod(gt_dir)\n            detections = detection_filtering(\n                detections, groundtruths\n            )  # filters detections overlapping with DC area\n            dc_id = []\n            for i in range(len(groundtruths)):\n                if groundtruths[i][5] == \"#\":\n                    dc_id.append(i)\n            cnt = 0\n            for a in dc_id:\n                num = a - cnt\n                del groundtruths[num]\n                cnt += 1\n\n            local_sigma_table = np.zeros((len(groundtruths), len(detections)))\n            local_tau_table = np.zeros((len(groundtruths), len(detections)))\n            local_pred_str = {}\n            local_gt_str = {}\n\n            for gt_id, gt in enumerate(groundtruths):\n                if len(detections) > 0:\n                    for det_id, detection in enumerate(detections):\n                        detection_orig = detection\n                        detection = [float(x) for x in detection[0].split(\",\")]\n                        detection = list(map(int, detection))\n                        pred_seq_str = detection_orig[1].strip()\n                        det_x = detection[0::2]\n                        det_y = detection[1::2]\n                        gt_x = list(map(int, np.squeeze(gt[1])))\n                        gt_y = list(map(int, np.squeeze(gt[3])))\n                        gt_seq_str = str(gt[4].tolist()[0])\n\n                        local_sigma_table[gt_id, det_id] = sigma_calculation(\n                            det_x, det_y, gt_x, gt_y\n                        )\n                        local_tau_table[gt_id, det_id] = tau_calculation(\n                            det_x, det_y, gt_x, gt_y\n                        )\n                        local_pred_str[det_id] = pred_seq_str\n                        local_gt_str[gt_id] = gt_seq_str\n\n            global_sigma = local_sigma_table\n            global_tau = local_tau_table\n            global_pred_str = local_pred_str\n            global_gt_str = local_gt_str\n\n    single_data = {}\n    single_data[\"sigma\"] = global_sigma\n    single_data[\"global_tau\"] = global_tau\n    single_data[\"global_pred_str\"] = global_pred_str\n    single_data[\"global_gt_str\"] = global_gt_str\n    return single_data\n\n\ndef get_socre_B(gt_dir, img_id, pred_dict):\n    allInputs = 1\n\n    def input_reading_mod(pred_dict):\n        \"\"\"This helper reads input from txt files\"\"\"\n        det = []\n        n = len(pred_dict)\n        for i in range(n):\n            points = pred_dict[i][\"points\"]\n            text = pred_dict[i][\"texts\"]\n            point = \",\".join(\n                map(\n                    str,\n                    points.reshape(\n                        -1,\n                    ),\n                )\n            )\n            det.append([point, text])\n        return det\n\n    def gt_reading_mod(gt_dir, gt_id):\n        gt = io.loadmat(\"%s/poly_gt_img%s.mat\" % (gt_dir, gt_id))\n        gt = gt[\"polygt\"]\n        return gt\n\n    def detection_filtering(detections, groundtruths, threshold=0.5):\n        for gt_id, gt in enumerate(groundtruths):\n            if (gt[5] == \"#\") and (gt[1].shape[1] > 1):\n                gt_x = list(map(int, np.squeeze(gt[1])))\n                gt_y = list(map(int, np.squeeze(gt[3])))\n                for det_id, detection in enumerate(detections):\n                    detection_orig = detection\n                    detection = [float(x) for x in detection[0].split(\",\")]\n                    detection = list(map(int, detection))\n                    det_x = detection[0::2]\n                    det_y = detection[1::2]\n                    det_gt_iou = iod(det_x, det_y, gt_x, gt_y)\n                    if det_gt_iou > threshold:\n                        detections[det_id] = []\n\n                detections[:] = [item for item in detections if item != []]\n        return detections\n\n    def sigma_calculation(det_x, det_y, gt_x, gt_y):\n        \"\"\"\n        sigma = inter_area / gt_area\n        \"\"\"\n        return np.round(\n            (area_of_intersection(det_x, det_y, gt_x, gt_y) / area(gt_x, gt_y)), 2\n        )\n\n    def tau_calculation(det_x, det_y, gt_x, gt_y):\n        if area(det_x, det_y) == 0.0:\n            return 0\n        return np.round(\n            (area_of_intersection(det_x, det_y, gt_x, gt_y) / area(det_x, det_y)), 2\n        )\n\n    ##############################Initialization###################################\n    # global_sigma = []\n    # global_tau = []\n    # global_pred_str = []\n    # global_gt_str = []\n    ###############################################################################\n\n    for input_id in range(allInputs):\n        if (\n            (input_id != \".DS_Store\")\n            and (input_id != \"Pascal_result.txt\")\n            and (input_id != \"Pascal_result_curved.txt\")\n            and (input_id != \"Pascal_result_non_curved.txt\")\n            and (input_id != \"Deteval_result.txt\")\n            and (input_id != \"Deteval_result_curved.txt\")\n            and (input_id != \"Deteval_result_non_curved.txt\")\n        ):\n            detections = input_reading_mod(pred_dict)\n            groundtruths = gt_reading_mod(gt_dir, img_id).tolist()\n            detections = detection_filtering(\n                detections, groundtruths\n            )  # filters detections overlapping with DC area\n            dc_id = []\n            for i in range(len(groundtruths)):\n                if groundtruths[i][5] == \"#\":\n                    dc_id.append(i)\n            cnt = 0\n            for a in dc_id:\n                num = a - cnt\n                del groundtruths[num]\n                cnt += 1\n\n            local_sigma_table = np.zeros((len(groundtruths), len(detections)))\n            local_tau_table = np.zeros((len(groundtruths), len(detections)))\n            local_pred_str = {}\n            local_gt_str = {}\n\n            for gt_id, gt in enumerate(groundtruths):\n                if len(detections) > 0:\n                    for det_id, detection in enumerate(detections):\n                        detection_orig = detection\n                        detection = [float(x) for x in detection[0].split(\",\")]\n                        detection = list(map(int, detection))\n                        pred_seq_str = detection_orig[1].strip()\n                        det_x = detection[0::2]\n                        det_y = detection[1::2]\n                        gt_x = list(map(int, np.squeeze(gt[1])))\n                        gt_y = list(map(int, np.squeeze(gt[3])))\n                        gt_seq_str = str(gt[4].tolist()[0])\n\n                        local_sigma_table[gt_id, det_id] = sigma_calculation(\n                            det_x, det_y, gt_x, gt_y\n                        )\n                        local_tau_table[gt_id, det_id] = tau_calculation(\n                            det_x, det_y, gt_x, gt_y\n                        )\n                        local_pred_str[det_id] = pred_seq_str\n                        local_gt_str[gt_id] = gt_seq_str\n\n            global_sigma = local_sigma_table\n            global_tau = local_tau_table\n            global_pred_str = local_pred_str\n            global_gt_str = local_gt_str\n\n    single_data = {}\n    single_data[\"sigma\"] = global_sigma\n    single_data[\"global_tau\"] = global_tau\n    single_data[\"global_pred_str\"] = global_pred_str\n    single_data[\"global_gt_str\"] = global_gt_str\n    return single_data\n\n\ndef get_score_C(gt_label, text, pred_bboxes):\n    \"\"\"\n    get score for CentripetalText (CT) prediction.\n    \"\"\"\n    check_install(\"Polygon\", \"Polygon3\")\n    import Polygon as plg\n\n    def gt_reading_mod(gt_label, text):\n        \"\"\"This helper reads groundtruths from mat files\"\"\"\n        groundtruths = []\n        nbox = len(gt_label)\n        for i in range(nbox):\n            label = {\"transcription\": text[i][0], \"points\": gt_label[i].numpy()}\n            groundtruths.append(label)\n\n        return groundtruths\n\n    def get_union(pD, pG):\n        areaA = pD.area()\n        areaB = pG.area()\n        return areaA + areaB - get_intersection(pD, pG)\n\n    def get_intersection(pD, pG):\n        pInt = pD & pG\n        if len(pInt) == 0:\n            return 0\n        return pInt.area()\n\n    def detection_filtering(detections, groundtruths, threshold=0.5):\n        for gt in groundtruths:\n            point_num = gt[\"points\"].shape[1] // 2\n            if gt[\"transcription\"] == \"###\" and (point_num > 1):\n                gt_p = np.array(gt[\"points\"]).reshape(point_num, 2).astype(\"int32\")\n                gt_p = plg.Polygon(gt_p)\n\n                for det_id, detection in enumerate(detections):\n                    det_y = detection[0::2]\n                    det_x = detection[1::2]\n\n                    det_p = np.concatenate((np.array(det_x), np.array(det_y)))\n                    det_p = det_p.reshape(2, -1).transpose()\n                    det_p = plg.Polygon(det_p)\n\n                    try:\n                        det_gt_iou = get_intersection(det_p, gt_p) / det_p.area()\n                    except:\n                        print(det_x, det_y, gt_p)\n                    if det_gt_iou > threshold:\n                        detections[det_id] = []\n\n                detections[:] = [item for item in detections if item != []]\n        return detections\n\n    def sigma_calculation(det_p, gt_p):\n        \"\"\"\n        sigma = inter_area / gt_area\n        \"\"\"\n        if gt_p.area() == 0.0:\n            return 0\n        return get_intersection(det_p, gt_p) / gt_p.area()\n\n    def tau_calculation(det_p, gt_p):\n        \"\"\"\n        tau = inter_area / det_area\n        \"\"\"\n        if det_p.area() == 0.0:\n            return 0\n        return get_intersection(det_p, gt_p) / det_p.area()\n\n    detections = []\n\n    for item in pred_bboxes:\n        detections.append(item[:, ::-1].reshape(-1))\n\n    groundtruths = gt_reading_mod(gt_label, text)\n\n    detections = detection_filtering(\n        detections, groundtruths\n    )  # filters detections overlapping with DC area\n\n    for idx in range(len(groundtruths) - 1, -1, -1):\n        # NOTE: source code use 'orin' to indicate '#', here we use 'anno',\n        # which may cause slight drop in fscore, about 0.12\n        if groundtruths[idx][\"transcription\"] == \"###\":\n            groundtruths.pop(idx)\n\n    local_sigma_table = np.zeros((len(groundtruths), len(detections)))\n    local_tau_table = np.zeros((len(groundtruths), len(detections)))\n\n    for gt_id, gt in enumerate(groundtruths):\n        if len(detections) > 0:\n            for det_id, detection in enumerate(detections):\n                point_num = gt[\"points\"].shape[1] // 2\n\n                gt_p = np.array(gt[\"points\"]).reshape(point_num, 2).astype(\"int32\")\n                gt_p = plg.Polygon(gt_p)\n\n                det_y = detection[0::2]\n                det_x = detection[1::2]\n\n                det_p = np.concatenate((np.array(det_x), np.array(det_y)))\n\n                det_p = det_p.reshape(2, -1).transpose()\n                det_p = plg.Polygon(det_p)\n\n                local_sigma_table[gt_id, det_id] = sigma_calculation(det_p, gt_p)\n                local_tau_table[gt_id, det_id] = tau_calculation(det_p, gt_p)\n\n    data = {}\n    data[\"sigma\"] = local_sigma_table\n    data[\"global_tau\"] = local_tau_table\n    data[\"global_pred_str\"] = \"\"\n    data[\"global_gt_str\"] = \"\"\n    return data\n\n\ndef combine_results(all_data, rec_flag=True):\n    tr = 0.7\n    tp = 0.6\n    fsc_k = 0.8\n    k = 2\n    global_sigma = []\n    global_tau = []\n    global_pred_str = []\n    global_gt_str = []\n\n    for data in all_data:\n        global_sigma.append(data[\"sigma\"])\n        global_tau.append(data[\"global_tau\"])\n        global_pred_str.append(data[\"global_pred_str\"])\n        global_gt_str.append(data[\"global_gt_str\"])\n\n    global_accumulative_recall = 0\n    global_accumulative_precision = 0\n    total_num_gt = 0\n    total_num_det = 0\n    hit_str_count = 0\n    hit_count = 0\n\n    def one_to_one(\n        local_sigma_table,\n        local_tau_table,\n        local_accumulative_recall,\n        local_accumulative_precision,\n        global_accumulative_recall,\n        global_accumulative_precision,\n        gt_flag,\n        det_flag,\n        idy,\n        rec_flag,\n    ):\n        hit_str_num = 0\n        for gt_id in range(num_gt):\n            gt_matching_qualified_sigma_candidates = np.where(\n                local_sigma_table[gt_id, :] > tr\n            )\n            gt_matching_num_qualified_sigma_candidates = (\n                gt_matching_qualified_sigma_candidates[0].shape[0]\n            )\n            gt_matching_qualified_tau_candidates = np.where(\n                local_tau_table[gt_id, :] > tp\n            )\n            gt_matching_num_qualified_tau_candidates = (\n                gt_matching_qualified_tau_candidates[0].shape[0]\n            )\n\n            det_matching_qualified_sigma_candidates = np.where(\n                local_sigma_table[:, gt_matching_qualified_sigma_candidates[0]] > tr\n            )\n            det_matching_num_qualified_sigma_candidates = (\n                det_matching_qualified_sigma_candidates[0].shape[0]\n            )\n            det_matching_qualified_tau_candidates = np.where(\n                local_tau_table[:, gt_matching_qualified_tau_candidates[0]] > tp\n            )\n            det_matching_num_qualified_tau_candidates = (\n                det_matching_qualified_tau_candidates[0].shape[0]\n            )\n\n            if (\n                (gt_matching_num_qualified_sigma_candidates == 1)\n                and (gt_matching_num_qualified_tau_candidates == 1)\n                and (det_matching_num_qualified_sigma_candidates == 1)\n                and (det_matching_num_qualified_tau_candidates == 1)\n            ):\n                global_accumulative_recall = global_accumulative_recall + 1.0\n                global_accumulative_precision = global_accumulative_precision + 1.0\n                local_accumulative_recall = local_accumulative_recall + 1.0\n                local_accumulative_precision = local_accumulative_precision + 1.0\n\n                gt_flag[0, gt_id] = 1\n                matched_det_id = np.where(local_sigma_table[gt_id, :] > tr)\n                # recg start\n                if rec_flag:\n                    gt_str_cur = global_gt_str[idy][gt_id]\n                    pred_str_cur = global_pred_str[idy][matched_det_id[0].tolist()[0]]\n                    if pred_str_cur == gt_str_cur:\n                        hit_str_num += 1\n                    else:\n                        if pred_str_cur.lower() == gt_str_cur.lower():\n                            hit_str_num += 1\n                # recg end\n                det_flag[0, matched_det_id] = 1\n        return (\n            local_accumulative_recall,\n            local_accumulative_precision,\n            global_accumulative_recall,\n            global_accumulative_precision,\n            gt_flag,\n            det_flag,\n            hit_str_num,\n        )\n\n    def one_to_many(\n        local_sigma_table,\n        local_tau_table,\n        local_accumulative_recall,\n        local_accumulative_precision,\n        global_accumulative_recall,\n        global_accumulative_precision,\n        gt_flag,\n        det_flag,\n        idy,\n        rec_flag,\n    ):\n        hit_str_num = 0\n        for gt_id in range(num_gt):\n            # skip the following if the groundtruth was matched\n            if gt_flag[0, gt_id] > 0:\n                continue\n\n            non_zero_in_sigma = np.where(local_sigma_table[gt_id, :] > 0)\n            num_non_zero_in_sigma = non_zero_in_sigma[0].shape[0]\n\n            if num_non_zero_in_sigma >= k:\n                ####search for all detections that overlaps with this groundtruth\n                qualified_tau_candidates = np.where(\n                    (local_tau_table[gt_id, :] >= tp) & (det_flag[0, :] == 0)\n                )\n                num_qualified_tau_candidates = qualified_tau_candidates[0].shape[0]\n\n                if num_qualified_tau_candidates == 1:\n                    if (local_tau_table[gt_id, qualified_tau_candidates] >= tp) and (\n                        local_sigma_table[gt_id, qualified_tau_candidates] >= tr\n                    ):\n                        # became an one-to-one case\n                        global_accumulative_recall = global_accumulative_recall + 1.0\n                        global_accumulative_precision = (\n                            global_accumulative_precision + 1.0\n                        )\n                        local_accumulative_recall = local_accumulative_recall + 1.0\n                        local_accumulative_precision = (\n                            local_accumulative_precision + 1.0\n                        )\n\n                        gt_flag[0, gt_id] = 1\n                        det_flag[0, qualified_tau_candidates] = 1\n                        # recg start\n                        if rec_flag:\n                            gt_str_cur = global_gt_str[idy][gt_id]\n                            pred_str_cur = global_pred_str[idy][\n                                qualified_tau_candidates[0].tolist()[0]\n                            ]\n                            if pred_str_cur == gt_str_cur:\n                                hit_str_num += 1\n                            else:\n                                if pred_str_cur.lower() == gt_str_cur.lower():\n                                    hit_str_num += 1\n                        # recg end\n                elif np.sum(local_sigma_table[gt_id, qualified_tau_candidates]) >= tr:\n                    gt_flag[0, gt_id] = 1\n                    det_flag[0, qualified_tau_candidates] = 1\n                    # recg start\n                    if rec_flag:\n                        gt_str_cur = global_gt_str[idy][gt_id]\n                        pred_str_cur = global_pred_str[idy][\n                            qualified_tau_candidates[0].tolist()[0]\n                        ]\n                        if pred_str_cur == gt_str_cur:\n                            hit_str_num += 1\n                        else:\n                            if pred_str_cur.lower() == gt_str_cur.lower():\n                                hit_str_num += 1\n                    # recg end\n\n                    global_accumulative_recall = global_accumulative_recall + fsc_k\n                    global_accumulative_precision = (\n                        global_accumulative_precision\n                        + num_qualified_tau_candidates * fsc_k\n                    )\n\n                    local_accumulative_recall = local_accumulative_recall + fsc_k\n                    local_accumulative_precision = (\n                        local_accumulative_precision\n                        + num_qualified_tau_candidates * fsc_k\n                    )\n\n        return (\n            local_accumulative_recall,\n            local_accumulative_precision,\n            global_accumulative_recall,\n            global_accumulative_precision,\n            gt_flag,\n            det_flag,\n            hit_str_num,\n        )\n\n    def many_to_one(\n        local_sigma_table,\n        local_tau_table,\n        local_accumulative_recall,\n        local_accumulative_precision,\n        global_accumulative_recall,\n        global_accumulative_precision,\n        gt_flag,\n        det_flag,\n        idy,\n        rec_flag,\n    ):\n        hit_str_num = 0\n        for det_id in range(num_det):\n            # skip the following if the detection was matched\n            if det_flag[0, det_id] > 0:\n                continue\n\n            non_zero_in_tau = np.where(local_tau_table[:, det_id] > 0)\n            num_non_zero_in_tau = non_zero_in_tau[0].shape[0]\n\n            if num_non_zero_in_tau >= k:\n                ####search for all detections that overlaps with this groundtruth\n                qualified_sigma_candidates = np.where(\n                    (local_sigma_table[:, det_id] >= tp) & (gt_flag[0, :] == 0)\n                )\n                num_qualified_sigma_candidates = qualified_sigma_candidates[0].shape[0]\n\n                if num_qualified_sigma_candidates == 1:\n                    if (local_tau_table[qualified_sigma_candidates, det_id] >= tp) and (\n                        local_sigma_table[qualified_sigma_candidates, det_id] >= tr\n                    ):\n                        # became an one-to-one case\n                        global_accumulative_recall = global_accumulative_recall + 1.0\n                        global_accumulative_precision = (\n                            global_accumulative_precision + 1.0\n                        )\n                        local_accumulative_recall = local_accumulative_recall + 1.0\n                        local_accumulative_precision = (\n                            local_accumulative_precision + 1.0\n                        )\n\n                        gt_flag[0, qualified_sigma_candidates] = 1\n                        det_flag[0, det_id] = 1\n                        # recg start\n                        if rec_flag:\n                            pred_str_cur = global_pred_str[idy][det_id]\n                            gt_len = len(qualified_sigma_candidates[0])\n                            for idx in range(gt_len):\n                                ele_gt_id = qualified_sigma_candidates[0].tolist()[idx]\n                                if ele_gt_id not in global_gt_str[idy]:\n                                    continue\n                                gt_str_cur = global_gt_str[idy][ele_gt_id]\n                                if pred_str_cur == gt_str_cur:\n                                    hit_str_num += 1\n                                    break\n                                else:\n                                    if pred_str_cur.lower() == gt_str_cur.lower():\n                                        hit_str_num += 1\n                                    break\n                        # recg end\n                elif np.sum(local_tau_table[qualified_sigma_candidates, det_id]) >= tp:\n                    det_flag[0, det_id] = 1\n                    gt_flag[0, qualified_sigma_candidates] = 1\n                    # recg start\n                    if rec_flag:\n                        pred_str_cur = global_pred_str[idy][det_id]\n                        gt_len = len(qualified_sigma_candidates[0])\n                        for idx in range(gt_len):\n                            ele_gt_id = qualified_sigma_candidates[0].tolist()[idx]\n                            if ele_gt_id not in global_gt_str[idy]:\n                                continue\n                            gt_str_cur = global_gt_str[idy][ele_gt_id]\n                            if pred_str_cur == gt_str_cur:\n                                hit_str_num += 1\n                                break\n                            else:\n                                if pred_str_cur.lower() == gt_str_cur.lower():\n                                    hit_str_num += 1\n                                    break\n                    # recg end\n\n                    global_accumulative_recall = (\n                        global_accumulative_recall\n                        + num_qualified_sigma_candidates * fsc_k\n                    )\n                    global_accumulative_precision = (\n                        global_accumulative_precision + fsc_k\n                    )\n\n                    local_accumulative_recall = (\n                        local_accumulative_recall\n                        + num_qualified_sigma_candidates * fsc_k\n                    )\n                    local_accumulative_precision = local_accumulative_precision + fsc_k\n        return (\n            local_accumulative_recall,\n            local_accumulative_precision,\n            global_accumulative_recall,\n            global_accumulative_precision,\n            gt_flag,\n            det_flag,\n            hit_str_num,\n        )\n\n    for idx in range(len(global_sigma)):\n        local_sigma_table = np.array(global_sigma[idx])\n        local_tau_table = global_tau[idx]\n\n        num_gt = local_sigma_table.shape[0]\n        num_det = local_sigma_table.shape[1]\n\n        total_num_gt = total_num_gt + num_gt\n        total_num_det = total_num_det + num_det\n\n        local_accumulative_recall = 0\n        local_accumulative_precision = 0\n        gt_flag = np.zeros((1, num_gt))\n        det_flag = np.zeros((1, num_det))\n\n        #######first check for one-to-one case##########\n        (\n            local_accumulative_recall,\n            local_accumulative_precision,\n            global_accumulative_recall,\n            global_accumulative_precision,\n            gt_flag,\n            det_flag,\n            hit_str_num,\n        ) = one_to_one(\n            local_sigma_table,\n            local_tau_table,\n            local_accumulative_recall,\n            local_accumulative_precision,\n            global_accumulative_recall,\n            global_accumulative_precision,\n            gt_flag,\n            det_flag,\n            idx,\n            rec_flag,\n        )\n\n        hit_str_count += hit_str_num\n        #######then check for one-to-many case##########\n        (\n            local_accumulative_recall,\n            local_accumulative_precision,\n            global_accumulative_recall,\n            global_accumulative_precision,\n            gt_flag,\n            det_flag,\n            hit_str_num,\n        ) = one_to_many(\n            local_sigma_table,\n            local_tau_table,\n            local_accumulative_recall,\n            local_accumulative_precision,\n            global_accumulative_recall,\n            global_accumulative_precision,\n            gt_flag,\n            det_flag,\n            idx,\n            rec_flag,\n        )\n        hit_str_count += hit_str_num\n        #######then check for many-to-one case##########\n        (\n            local_accumulative_recall,\n            local_accumulative_precision,\n            global_accumulative_recall,\n            global_accumulative_precision,\n            gt_flag,\n            det_flag,\n            hit_str_num,\n        ) = many_to_one(\n            local_sigma_table,\n            local_tau_table,\n            local_accumulative_recall,\n            local_accumulative_precision,\n            global_accumulative_recall,\n            global_accumulative_precision,\n            gt_flag,\n            det_flag,\n            idx,\n            rec_flag,\n        )\n        hit_str_count += hit_str_num\n\n    try:\n        recall = global_accumulative_recall / total_num_gt\n    except ZeroDivisionError:\n        recall = 0\n\n    try:\n        precision = global_accumulative_precision / total_num_det\n    except ZeroDivisionError:\n        precision = 0\n\n    try:\n        f_score = 2 * precision * recall / (precision + recall)\n    except ZeroDivisionError:\n        f_score = 0\n\n    try:\n        seqerr = 1 - float(hit_str_count) / global_accumulative_recall\n    except ZeroDivisionError:\n        seqerr = 1\n\n    try:\n        recall_e2e = float(hit_str_count) / total_num_gt\n    except ZeroDivisionError:\n        recall_e2e = 0\n\n    try:\n        precision_e2e = float(hit_str_count) / total_num_det\n    except ZeroDivisionError:\n        precision_e2e = 0\n\n    try:\n        f_score_e2e = 2 * precision_e2e * recall_e2e / (precision_e2e + recall_e2e)\n    except ZeroDivisionError:\n        f_score_e2e = 0\n\n    final = {\n        \"total_num_gt\": total_num_gt,\n        \"total_num_det\": total_num_det,\n        \"global_accumulative_recall\": global_accumulative_recall,\n        \"hit_str_count\": hit_str_count,\n        \"recall\": recall,\n        \"precision\": precision,\n        \"f_score\": f_score,\n        \"seqerr\": seqerr,\n        \"recall_e2e\": recall_e2e,\n        \"precision_e2e\": precision_e2e,\n        \"f_score_e2e\": f_score_e2e,\n    }\n    return final\n", "ppocr/utils/e2e_utils/pgnet_pp_utils.py": "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport paddle\nimport os\nimport sys\n\n__dir__ = os.path.dirname(__file__)\nsys.path.append(__dir__)\nsys.path.append(os.path.join(__dir__, \"..\"))\nfrom extract_textpoint_slow import *\nfrom extract_textpoint_fast import generate_pivot_list_fast, restore_poly\n\n\nclass PGNet_PostProcess(object):\n    # two different post-process\n    def __init__(\n        self,\n        character_dict_path,\n        valid_set,\n        score_thresh,\n        outs_dict,\n        shape_list,\n        point_gather_mode=None,\n    ):\n        self.Lexicon_Table = get_dict(character_dict_path)\n        self.valid_set = valid_set\n        self.score_thresh = score_thresh\n        self.outs_dict = outs_dict\n        self.shape_list = shape_list\n        self.point_gather_mode = point_gather_mode\n\n    def pg_postprocess_fast(self):\n        p_score = self.outs_dict[\"f_score\"]\n        p_border = self.outs_dict[\"f_border\"]\n        p_char = self.outs_dict[\"f_char\"]\n        p_direction = self.outs_dict[\"f_direction\"]\n        if isinstance(p_score, paddle.Tensor):\n            p_score = p_score[0].numpy()\n            p_border = p_border[0].numpy()\n            p_direction = p_direction[0].numpy()\n            p_char = p_char[0].numpy()\n        else:\n            p_score = p_score[0]\n            p_border = p_border[0]\n            p_direction = p_direction[0]\n            p_char = p_char[0]\n\n        src_h, src_w, ratio_h, ratio_w = self.shape_list[0]\n        instance_yxs_list, seq_strs = generate_pivot_list_fast(\n            p_score,\n            p_char,\n            p_direction,\n            self.Lexicon_Table,\n            score_thresh=self.score_thresh,\n            point_gather_mode=self.point_gather_mode,\n        )\n        poly_list, keep_str_list = restore_poly(\n            instance_yxs_list,\n            seq_strs,\n            p_border,\n            ratio_w,\n            ratio_h,\n            src_w,\n            src_h,\n            self.valid_set,\n        )\n        data = {\n            \"points\": poly_list,\n            \"texts\": keep_str_list,\n        }\n        return data\n\n    def pg_postprocess_slow(self):\n        p_score = self.outs_dict[\"f_score\"]\n        p_border = self.outs_dict[\"f_border\"]\n        p_char = self.outs_dict[\"f_char\"]\n        p_direction = self.outs_dict[\"f_direction\"]\n        if isinstance(p_score, paddle.Tensor):\n            p_score = p_score[0].numpy()\n            p_border = p_border[0].numpy()\n            p_direction = p_direction[0].numpy()\n            p_char = p_char[0].numpy()\n        else:\n            p_score = p_score[0]\n            p_border = p_border[0]\n            p_direction = p_direction[0]\n            p_char = p_char[0]\n        src_h, src_w, ratio_h, ratio_w = self.shape_list[0]\n        is_curved = self.valid_set == \"totaltext\"\n        char_seq_idx_set, instance_yxs_list = generate_pivot_list_slow(\n            p_score,\n            p_char,\n            p_direction,\n            score_thresh=self.score_thresh,\n            is_backbone=True,\n            is_curved=is_curved,\n        )\n        seq_strs = []\n        for char_idx_set in char_seq_idx_set:\n            pr_str = \"\".join([self.Lexicon_Table[pos] for pos in char_idx_set])\n            seq_strs.append(pr_str)\n        poly_list = []\n        keep_str_list = []\n        all_point_list = []\n        all_point_pair_list = []\n        for yx_center_line, keep_str in zip(instance_yxs_list, seq_strs):\n            if len(yx_center_line) == 1:\n                yx_center_line.append(yx_center_line[-1])\n\n            offset_expand = 1.0\n            if self.valid_set == \"totaltext\":\n                offset_expand = 1.2\n\n            point_pair_list = []\n            for batch_id, y, x in yx_center_line:\n                offset = p_border[:, y, x].reshape(2, 2)\n                if offset_expand != 1.0:\n                    offset_length = np.linalg.norm(offset, axis=1, keepdims=True)\n                    expand_length = np.clip(\n                        offset_length * (offset_expand - 1), a_min=0.5, a_max=3.0\n                    )\n                    offset_detal = offset / offset_length * expand_length\n                    offset = offset + offset_detal\n                ori_yx = np.array([y, x], dtype=np.float32)\n                point_pair = (\n                    (ori_yx + offset)[:, ::-1]\n                    * 4.0\n                    / np.array([ratio_w, ratio_h]).reshape(-1, 2)\n                )\n                point_pair_list.append(point_pair)\n\n                all_point_list.append(\n                    [int(round(x * 4.0 / ratio_w)), int(round(y * 4.0 / ratio_h))]\n                )\n                all_point_pair_list.append(point_pair.round().astype(np.int32).tolist())\n\n            detected_poly, pair_length_info = point_pair2poly(point_pair_list)\n            detected_poly = expand_poly_along_width(\n                detected_poly, shrink_ratio_of_width=0.2\n            )\n            detected_poly[:, 0] = np.clip(detected_poly[:, 0], a_min=0, a_max=src_w)\n            detected_poly[:, 1] = np.clip(detected_poly[:, 1], a_min=0, a_max=src_h)\n\n            if len(keep_str) < 2:\n                continue\n\n            keep_str_list.append(keep_str)\n            detected_poly = np.round(detected_poly).astype(\"int32\")\n            if self.valid_set == \"partvgg\":\n                middle_point = len(detected_poly) // 2\n                detected_poly = detected_poly[\n                    [0, middle_point - 1, middle_point, -1], :\n                ]\n                poly_list.append(detected_poly)\n            elif self.valid_set == \"totaltext\":\n                poly_list.append(detected_poly)\n            else:\n                print(\"--> Not supported format.\")\n                exit(-1)\n        data = {\n            \"points\": poly_list,\n            \"texts\": keep_str_list,\n        }\n        return data\n", "ppocr/utils/e2e_utils/extract_textpoint_slow.py": "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Contains various CTC decoders.\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport cv2\nimport math\n\nimport numpy as np\nfrom itertools import groupby\nfrom skimage.morphology._skeletonize import thin\n\n\ndef get_dict(character_dict_path):\n    character_str = \"\"\n    with open(character_dict_path, \"rb\") as fin:\n        lines = fin.readlines()\n        for line in lines:\n            line = line.decode(\"utf-8\").strip(\"\\n\").strip(\"\\r\\n\")\n            character_str += line\n        dict_character = list(character_str)\n    return dict_character\n\n\ndef point_pair2poly(point_pair_list):\n    \"\"\"\n    Transfer vertical point_pairs into poly point in clockwise.\n    \"\"\"\n    pair_length_list = []\n    for point_pair in point_pair_list:\n        pair_length = np.linalg.norm(point_pair[0] - point_pair[1])\n        pair_length_list.append(pair_length)\n    pair_length_list = np.array(pair_length_list)\n    pair_info = (\n        pair_length_list.max(),\n        pair_length_list.min(),\n        pair_length_list.mean(),\n    )\n\n    point_num = len(point_pair_list) * 2\n    point_list = [0] * point_num\n    for idx, point_pair in enumerate(point_pair_list):\n        point_list[idx] = point_pair[0]\n        point_list[point_num - 1 - idx] = point_pair[1]\n    return np.array(point_list).reshape(-1, 2), pair_info\n\n\ndef shrink_quad_along_width(quad, begin_width_ratio=0.0, end_width_ratio=1.0):\n    \"\"\"\n    Generate shrink_quad_along_width.\n    \"\"\"\n    ratio_pair = np.array([[begin_width_ratio], [end_width_ratio]], dtype=np.float32)\n    p0_1 = quad[0] + (quad[1] - quad[0]) * ratio_pair\n    p3_2 = quad[3] + (quad[2] - quad[3]) * ratio_pair\n    return np.array([p0_1[0], p0_1[1], p3_2[1], p3_2[0]])\n\n\ndef expand_poly_along_width(poly, shrink_ratio_of_width=0.3):\n    \"\"\"\n    expand poly along width.\n    \"\"\"\n    point_num = poly.shape[0]\n    left_quad = np.array([poly[0], poly[1], poly[-2], poly[-1]], dtype=np.float32)\n    left_ratio = (\n        -shrink_ratio_of_width\n        * np.linalg.norm(left_quad[0] - left_quad[3])\n        / (np.linalg.norm(left_quad[0] - left_quad[1]) + 1e-6)\n    )\n    left_quad_expand = shrink_quad_along_width(left_quad, left_ratio, 1.0)\n    right_quad = np.array(\n        [\n            poly[point_num // 2 - 2],\n            poly[point_num // 2 - 1],\n            poly[point_num // 2],\n            poly[point_num // 2 + 1],\n        ],\n        dtype=np.float32,\n    )\n    right_ratio = 1.0 + shrink_ratio_of_width * np.linalg.norm(\n        right_quad[0] - right_quad[3]\n    ) / (np.linalg.norm(right_quad[0] - right_quad[1]) + 1e-6)\n    right_quad_expand = shrink_quad_along_width(right_quad, 0.0, right_ratio)\n    poly[0] = left_quad_expand[0]\n    poly[-1] = left_quad_expand[-1]\n    poly[point_num // 2 - 1] = right_quad_expand[1]\n    poly[point_num // 2] = right_quad_expand[2]\n    return poly\n\n\ndef softmax(logits):\n    \"\"\"\n    logits: N x d\n    \"\"\"\n    max_value = np.max(logits, axis=1, keepdims=True)\n    exp = np.exp(logits - max_value)\n    exp_sum = np.sum(exp, axis=1, keepdims=True)\n    dist = exp / exp_sum\n    return dist\n\n\ndef get_keep_pos_idxs(labels, remove_blank=None):\n    \"\"\"\n    Remove duplicate and get pos idxs of keep items.\n    The value of keep_blank should be [None, 95].\n    \"\"\"\n    duplicate_len_list = []\n    keep_pos_idx_list = []\n    keep_char_idx_list = []\n    for k, v_ in groupby(labels):\n        current_len = len(list(v_))\n        if k != remove_blank:\n            current_idx = int(sum(duplicate_len_list) + current_len // 2)\n            keep_pos_idx_list.append(current_idx)\n            keep_char_idx_list.append(k)\n        duplicate_len_list.append(current_len)\n    return keep_char_idx_list, keep_pos_idx_list\n\n\ndef remove_blank(labels, blank=0):\n    new_labels = [x for x in labels if x != blank]\n    return new_labels\n\n\ndef insert_blank(labels, blank=0):\n    new_labels = [blank]\n    for l in labels:\n        new_labels += [l, blank]\n    return new_labels\n\n\ndef ctc_greedy_decoder(probs_seq, blank=95, keep_blank_in_idxs=True):\n    \"\"\"\n    CTC greedy (best path) decoder.\n    \"\"\"\n    raw_str = np.argmax(np.array(probs_seq), axis=1)\n    remove_blank_in_pos = None if keep_blank_in_idxs else blank\n    dedup_str, keep_idx_list = get_keep_pos_idxs(\n        raw_str, remove_blank=remove_blank_in_pos\n    )\n    dst_str = remove_blank(dedup_str, blank=blank)\n    return dst_str, keep_idx_list\n\n\ndef instance_ctc_greedy_decoder(gather_info, logits_map, keep_blank_in_idxs=True):\n    \"\"\"\n    gather_info: [[x, y], [x, y] ...]\n    logits_map: H x W X (n_chars + 1)\n    \"\"\"\n    _, _, C = logits_map.shape\n    ys, xs = zip(*gather_info)\n    logits_seq = logits_map[list(ys), list(xs)]  # n x 96\n    probs_seq = softmax(logits_seq)\n    dst_str, keep_idx_list = ctc_greedy_decoder(\n        probs_seq, blank=C - 1, keep_blank_in_idxs=keep_blank_in_idxs\n    )\n    keep_gather_list = [gather_info[idx] for idx in keep_idx_list]\n    return dst_str, keep_gather_list\n\n\ndef ctc_decoder_for_image(gather_info_list, logits_map, keep_blank_in_idxs=True):\n    \"\"\"\n    CTC decoder using multiple processes.\n    \"\"\"\n    decoder_results = []\n    for gather_info in gather_info_list:\n        res = instance_ctc_greedy_decoder(\n            gather_info, logits_map, keep_blank_in_idxs=keep_blank_in_idxs\n        )\n        decoder_results.append(res)\n    return decoder_results\n\n\ndef sort_with_direction(pos_list, f_direction):\n    \"\"\"\n    f_direction: h x w x 2\n    pos_list: [[y, x], [y, x], [y, x] ...]\n    \"\"\"\n\n    def sort_part_with_direction(pos_list, point_direction):\n        pos_list = np.array(pos_list).reshape(-1, 2)\n        point_direction = np.array(point_direction).reshape(-1, 2)\n        average_direction = np.mean(point_direction, axis=0, keepdims=True)\n        pos_proj_leng = np.sum(pos_list * average_direction, axis=1)\n        sorted_list = pos_list[np.argsort(pos_proj_leng)].tolist()\n        sorted_direction = point_direction[np.argsort(pos_proj_leng)].tolist()\n        return sorted_list, sorted_direction\n\n    pos_list = np.array(pos_list).reshape(-1, 2)\n    point_direction = f_direction[pos_list[:, 0], pos_list[:, 1]]  # x, y\n    point_direction = point_direction[:, ::-1]  # x, y -> y, x\n    sorted_point, sorted_direction = sort_part_with_direction(pos_list, point_direction)\n\n    point_num = len(sorted_point)\n    if point_num >= 16:\n        middle_num = point_num // 2\n        first_part_point = sorted_point[:middle_num]\n        first_point_direction = sorted_direction[:middle_num]\n        sorted_fist_part_point, sorted_fist_part_direction = sort_part_with_direction(\n            first_part_point, first_point_direction\n        )\n\n        last_part_point = sorted_point[middle_num:]\n        last_point_direction = sorted_direction[middle_num:]\n        sorted_last_part_point, sorted_last_part_direction = sort_part_with_direction(\n            last_part_point, last_point_direction\n        )\n        sorted_point = sorted_fist_part_point + sorted_last_part_point\n        sorted_direction = sorted_fist_part_direction + sorted_last_part_direction\n\n    return sorted_point, np.array(sorted_direction)\n\n\ndef add_id(pos_list, image_id=0):\n    \"\"\"\n    Add id for gather feature, for inference.\n    \"\"\"\n    new_list = []\n    for item in pos_list:\n        new_list.append((image_id, item[0], item[1]))\n    return new_list\n\n\ndef sort_and_expand_with_direction(pos_list, f_direction):\n    \"\"\"\n    f_direction: h x w x 2\n    pos_list: [[y, x], [y, x], [y, x] ...]\n    \"\"\"\n    h, w, _ = f_direction.shape\n    sorted_list, point_direction = sort_with_direction(pos_list, f_direction)\n\n    # expand along\n    point_num = len(sorted_list)\n    sub_direction_len = max(point_num // 3, 2)\n    left_direction = point_direction[:sub_direction_len, :]\n    right_dirction = point_direction[point_num - sub_direction_len :, :]\n\n    left_average_direction = -np.mean(left_direction, axis=0, keepdims=True)\n    left_average_len = np.linalg.norm(left_average_direction)\n    left_start = np.array(sorted_list[0])\n    left_step = left_average_direction / (left_average_len + 1e-6)\n\n    right_average_direction = np.mean(right_dirction, axis=0, keepdims=True)\n    right_average_len = np.linalg.norm(right_average_direction)\n    right_step = right_average_direction / (right_average_len + 1e-6)\n    right_start = np.array(sorted_list[-1])\n\n    append_num = max(int((left_average_len + right_average_len) / 2.0 * 0.15), 1)\n    left_list = []\n    right_list = []\n    for i in range(append_num):\n        ly, lx = (\n            np.round(left_start + left_step * (i + 1))\n            .flatten()\n            .astype(\"int32\")\n            .tolist()\n        )\n        if ly < h and lx < w and (ly, lx) not in left_list:\n            left_list.append((ly, lx))\n        ry, rx = (\n            np.round(right_start + right_step * (i + 1))\n            .flatten()\n            .astype(\"int32\")\n            .tolist()\n        )\n        if ry < h and rx < w and (ry, rx) not in right_list:\n            right_list.append((ry, rx))\n\n    all_list = left_list[::-1] + sorted_list + right_list\n    return all_list\n\n\ndef sort_and_expand_with_direction_v2(pos_list, f_direction, binary_tcl_map):\n    \"\"\"\n    f_direction: h x w x 2\n    pos_list: [[y, x], [y, x], [y, x] ...]\n    binary_tcl_map: h x w\n    \"\"\"\n    h, w, _ = f_direction.shape\n    sorted_list, point_direction = sort_with_direction(pos_list, f_direction)\n\n    # expand along\n    point_num = len(sorted_list)\n    sub_direction_len = max(point_num // 3, 2)\n    left_direction = point_direction[:sub_direction_len, :]\n    right_dirction = point_direction[point_num - sub_direction_len :, :]\n\n    left_average_direction = -np.mean(left_direction, axis=0, keepdims=True)\n    left_average_len = np.linalg.norm(left_average_direction)\n    left_start = np.array(sorted_list[0])\n    left_step = left_average_direction / (left_average_len + 1e-6)\n\n    right_average_direction = np.mean(right_dirction, axis=0, keepdims=True)\n    right_average_len = np.linalg.norm(right_average_direction)\n    right_step = right_average_direction / (right_average_len + 1e-6)\n    right_start = np.array(sorted_list[-1])\n\n    append_num = max(int((left_average_len + right_average_len) / 2.0 * 0.15), 1)\n    max_append_num = 2 * append_num\n\n    left_list = []\n    right_list = []\n    for i in range(max_append_num):\n        ly, lx = (\n            np.round(left_start + left_step * (i + 1))\n            .flatten()\n            .astype(\"int32\")\n            .tolist()\n        )\n        if ly < h and lx < w and (ly, lx) not in left_list:\n            if binary_tcl_map[ly, lx] > 0.5:\n                left_list.append((ly, lx))\n            else:\n                break\n\n    for i in range(max_append_num):\n        ry, rx = (\n            np.round(right_start + right_step * (i + 1))\n            .flatten()\n            .astype(\"int32\")\n            .tolist()\n        )\n        if ry < h and rx < w and (ry, rx) not in right_list:\n            if binary_tcl_map[ry, rx] > 0.5:\n                right_list.append((ry, rx))\n            else:\n                break\n\n    all_list = left_list[::-1] + sorted_list + right_list\n    return all_list\n\n\ndef generate_pivot_list_curved(\n    p_score,\n    p_char_maps,\n    f_direction,\n    score_thresh=0.5,\n    is_expand=True,\n    is_backbone=False,\n    image_id=0,\n):\n    \"\"\"\n    return center point and end point of TCL instance; filter with the char maps;\n    \"\"\"\n    p_score = p_score[0]\n    f_direction = f_direction.transpose(1, 2, 0)\n    p_tcl_map = (p_score > score_thresh) * 1.0\n    skeleton_map = thin(p_tcl_map)\n    instance_count, instance_label_map = cv2.connectedComponents(\n        skeleton_map.astype(np.uint8), connectivity=8\n    )\n\n    # get TCL Instance\n    all_pos_yxs = []\n    center_pos_yxs = []\n    end_points_yxs = []\n    instance_center_pos_yxs = []\n    pred_strs = []\n    if instance_count > 0:\n        for instance_id in range(1, instance_count):\n            pos_list = []\n            ys, xs = np.where(instance_label_map == instance_id)\n            pos_list = list(zip(ys, xs))\n\n            ### FIX-ME, eliminate outlier\n            if len(pos_list) < 3:\n                continue\n\n            if is_expand:\n                pos_list_sorted = sort_and_expand_with_direction_v2(\n                    pos_list, f_direction, p_tcl_map\n                )\n            else:\n                pos_list_sorted, _ = sort_with_direction(pos_list, f_direction)\n            all_pos_yxs.append(pos_list_sorted)\n\n    # use decoder to filter backgroud points.\n    p_char_maps = p_char_maps.transpose([1, 2, 0])\n    decode_res = ctc_decoder_for_image(\n        all_pos_yxs, logits_map=p_char_maps, keep_blank_in_idxs=True\n    )\n    for decoded_str, keep_yxs_list in decode_res:\n        if is_backbone:\n            keep_yxs_list_with_id = add_id(keep_yxs_list, image_id=image_id)\n            instance_center_pos_yxs.append(keep_yxs_list_with_id)\n            pred_strs.append(decoded_str)\n        else:\n            end_points_yxs.extend((keep_yxs_list[0], keep_yxs_list[-1]))\n            center_pos_yxs.extend(keep_yxs_list)\n\n    if is_backbone:\n        return pred_strs, instance_center_pos_yxs\n    else:\n        return center_pos_yxs, end_points_yxs\n\n\ndef generate_pivot_list_horizontal(\n    p_score, p_char_maps, f_direction, score_thresh=0.5, is_backbone=False, image_id=0\n):\n    \"\"\"\n    return center point and end point of TCL instance; filter with the char maps;\n    \"\"\"\n    p_score = p_score[0]\n    f_direction = f_direction.transpose(1, 2, 0)\n    p_tcl_map_bi = (p_score > score_thresh) * 1.0\n    instance_count, instance_label_map = cv2.connectedComponents(\n        p_tcl_map_bi.astype(np.uint8), connectivity=8\n    )\n\n    # get TCL Instance\n    all_pos_yxs = []\n    center_pos_yxs = []\n    end_points_yxs = []\n    instance_center_pos_yxs = []\n\n    if instance_count > 0:\n        for instance_id in range(1, instance_count):\n            pos_list = []\n            ys, xs = np.where(instance_label_map == instance_id)\n            pos_list = list(zip(ys, xs))\n\n            ### FIX-ME, eliminate outlier\n            if len(pos_list) < 5:\n                continue\n\n            # add rule here\n            main_direction = extract_main_direction(pos_list, f_direction)  # y x\n            reference_directin = np.array([0, 1]).reshape([-1, 2])  # y x\n            is_h_angle = abs(np.sum(main_direction * reference_directin)) < math.cos(\n                math.pi / 180 * 70\n            )\n\n            point_yxs = np.array(pos_list)\n            max_y, max_x = np.max(point_yxs, axis=0)\n            min_y, min_x = np.min(point_yxs, axis=0)\n            is_h_len = (max_y - min_y) < 1.5 * (max_x - min_x)\n\n            pos_list_final = []\n            if is_h_len:\n                xs = np.unique(xs)\n                for x in xs:\n                    ys = instance_label_map[:, x].copy().reshape((-1,))\n                    y = int(np.where(ys == instance_id)[0].mean())\n                    pos_list_final.append((y, x))\n            else:\n                ys = np.unique(ys)\n                for y in ys:\n                    xs = instance_label_map[y, :].copy().reshape((-1,))\n                    x = int(np.where(xs == instance_id)[0].mean())\n                    pos_list_final.append((y, x))\n\n            pos_list_sorted, _ = sort_with_direction(pos_list_final, f_direction)\n            all_pos_yxs.append(pos_list_sorted)\n\n    # use decoder to filter backgroud points.\n    p_char_maps = p_char_maps.transpose([1, 2, 0])\n    decode_res = ctc_decoder_for_image(\n        all_pos_yxs, logits_map=p_char_maps, keep_blank_in_idxs=True\n    )\n    for decoded_str, keep_yxs_list in decode_res:\n        if is_backbone:\n            keep_yxs_list_with_id = add_id(keep_yxs_list, image_id=image_id)\n            instance_center_pos_yxs.append(keep_yxs_list_with_id)\n        else:\n            end_points_yxs.extend((keep_yxs_list[0], keep_yxs_list[-1]))\n            center_pos_yxs.extend(keep_yxs_list)\n\n    if is_backbone:\n        return instance_center_pos_yxs\n    else:\n        return center_pos_yxs, end_points_yxs\n\n\ndef generate_pivot_list_slow(\n    p_score,\n    p_char_maps,\n    f_direction,\n    score_thresh=0.5,\n    is_backbone=False,\n    is_curved=True,\n    image_id=0,\n):\n    \"\"\"\n    Warp all the function together.\n    \"\"\"\n    if is_curved:\n        return generate_pivot_list_curved(\n            p_score,\n            p_char_maps,\n            f_direction,\n            score_thresh=score_thresh,\n            is_expand=True,\n            is_backbone=is_backbone,\n            image_id=image_id,\n        )\n    else:\n        return generate_pivot_list_horizontal(\n            p_score,\n            p_char_maps,\n            f_direction,\n            score_thresh=score_thresh,\n            is_backbone=is_backbone,\n            image_id=image_id,\n        )\n\n\n# for refine module\ndef extract_main_direction(pos_list, f_direction):\n    \"\"\"\n    f_direction: h x w x 2\n    pos_list: [[y, x], [y, x], [y, x] ...]\n    \"\"\"\n    pos_list = np.array(pos_list)\n    point_direction = f_direction[pos_list[:, 0], pos_list[:, 1]]\n    point_direction = point_direction[:, ::-1]  # x, y -> y, x\n    average_direction = np.mean(point_direction, axis=0, keepdims=True)\n    average_direction = average_direction / (np.linalg.norm(average_direction) + 1e-6)\n    return average_direction\n\n\ndef sort_by_direction_with_image_id_deprecated(pos_list, f_direction):\n    \"\"\"\n    f_direction: h x w x 2\n    pos_list: [[id, y, x], [id, y, x], [id, y, x] ...]\n    \"\"\"\n    pos_list_full = np.array(pos_list).reshape(-1, 3)\n    pos_list = pos_list_full[:, 1:]\n    point_direction = f_direction[pos_list[:, 0], pos_list[:, 1]]  # x, y\n    point_direction = point_direction[:, ::-1]  # x, y -> y, x\n    average_direction = np.mean(point_direction, axis=0, keepdims=True)\n    pos_proj_leng = np.sum(pos_list * average_direction, axis=1)\n    sorted_list = pos_list_full[np.argsort(pos_proj_leng)].tolist()\n    return sorted_list\n\n\ndef sort_by_direction_with_image_id(pos_list, f_direction):\n    \"\"\"\n    f_direction: h x w x 2\n    pos_list: [[y, x], [y, x], [y, x] ...]\n    \"\"\"\n\n    def sort_part_with_direction(pos_list_full, point_direction):\n        pos_list_full = np.array(pos_list_full).reshape(-1, 3)\n        pos_list = pos_list_full[:, 1:]\n        point_direction = np.array(point_direction).reshape(-1, 2)\n        average_direction = np.mean(point_direction, axis=0, keepdims=True)\n        pos_proj_leng = np.sum(pos_list * average_direction, axis=1)\n        sorted_list = pos_list_full[np.argsort(pos_proj_leng)].tolist()\n        sorted_direction = point_direction[np.argsort(pos_proj_leng)].tolist()\n        return sorted_list, sorted_direction\n\n    pos_list = np.array(pos_list).reshape(-1, 3)\n    point_direction = f_direction[pos_list[:, 1], pos_list[:, 2]]  # x, y\n    point_direction = point_direction[:, ::-1]  # x, y -> y, x\n    sorted_point, sorted_direction = sort_part_with_direction(pos_list, point_direction)\n\n    point_num = len(sorted_point)\n    if point_num >= 16:\n        middle_num = point_num // 2\n        first_part_point = sorted_point[:middle_num]\n        first_point_direction = sorted_direction[:middle_num]\n        sorted_fist_part_point, sorted_fist_part_direction = sort_part_with_direction(\n            first_part_point, first_point_direction\n        )\n\n        last_part_point = sorted_point[middle_num:]\n        last_point_direction = sorted_direction[middle_num:]\n        sorted_last_part_point, sorted_last_part_direction = sort_part_with_direction(\n            last_part_point, last_point_direction\n        )\n        sorted_point = sorted_fist_part_point + sorted_last_part_point\n        sorted_direction = sorted_fist_part_direction + sorted_last_part_direction\n\n    return sorted_point\n\n\ndef generate_pivot_list_tt_inference(\n    p_score,\n    p_char_maps,\n    f_direction,\n    score_thresh=0.5,\n    is_backbone=False,\n    is_curved=True,\n    image_id=0,\n):\n    \"\"\"\n    return center point and end point of TCL instance; filter with the char maps;\n    \"\"\"\n    p_score = p_score[0]\n    f_direction = f_direction.transpose(1, 2, 0)\n    p_tcl_map = (p_score > score_thresh) * 1.0\n    skeleton_map = thin(p_tcl_map)\n    instance_count, instance_label_map = cv2.connectedComponents(\n        skeleton_map.astype(np.uint8), connectivity=8\n    )\n\n    # get TCL Instance\n    all_pos_yxs = []\n    if instance_count > 0:\n        for instance_id in range(1, instance_count):\n            pos_list = []\n            ys, xs = np.where(instance_label_map == instance_id)\n            pos_list = list(zip(ys, xs))\n            ### FIX-ME, eliminate outlier\n            if len(pos_list) < 3:\n                continue\n            pos_list_sorted = sort_and_expand_with_direction_v2(\n                pos_list, f_direction, p_tcl_map\n            )\n            pos_list_sorted_with_id = add_id(pos_list_sorted, image_id=image_id)\n            all_pos_yxs.append(pos_list_sorted_with_id)\n    return all_pos_yxs\n", "ppocr/utils/e2e_utils/extract_textpoint_fast.py": "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Contains various CTC decoders.\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport cv2\nimport math\n\nimport numpy as np\nfrom itertools import groupby\nfrom skimage.morphology._skeletonize import thin\n\n\ndef get_dict(character_dict_path):\n    character_str = \"\"\n    with open(character_dict_path, \"rb\") as fin:\n        lines = fin.readlines()\n        for line in lines:\n            line = line.decode(\"utf-8\").strip(\"\\n\").strip(\"\\r\\n\")\n            character_str += line\n        dict_character = list(character_str)\n    return dict_character\n\n\ndef softmax(logits):\n    \"\"\"\n    logits: N x d\n    \"\"\"\n    max_value = np.max(logits, axis=1, keepdims=True)\n    exp = np.exp(logits - max_value)\n    exp_sum = np.sum(exp, axis=1, keepdims=True)\n    dist = exp / exp_sum\n    return dist\n\n\ndef get_keep_pos_idxs(labels, remove_blank=None):\n    \"\"\"\n    Remove duplicate and get pos idxs of keep items.\n    The value of keep_blank should be [None, 95].\n    \"\"\"\n    duplicate_len_list = []\n    keep_pos_idx_list = []\n    keep_char_idx_list = []\n    for k, v_ in groupby(labels):\n        current_len = len(list(v_))\n        if k != remove_blank:\n            current_idx = int(sum(duplicate_len_list) + current_len // 2)\n            keep_pos_idx_list.append(current_idx)\n            keep_char_idx_list.append(k)\n        duplicate_len_list.append(current_len)\n    return keep_char_idx_list, keep_pos_idx_list\n\n\ndef remove_blank(labels, blank=0):\n    new_labels = [x for x in labels if x != blank]\n    return new_labels\n\n\ndef insert_blank(labels, blank=0):\n    new_labels = [blank]\n    for l in labels:\n        new_labels += [l, blank]\n    return new_labels\n\n\ndef ctc_greedy_decoder(probs_seq, blank=95, keep_blank_in_idxs=True):\n    \"\"\"\n    CTC greedy (best path) decoder.\n    \"\"\"\n    raw_str = np.argmax(np.array(probs_seq), axis=1)\n    remove_blank_in_pos = None if keep_blank_in_idxs else blank\n    dedup_str, keep_idx_list = get_keep_pos_idxs(\n        raw_str, remove_blank=remove_blank_in_pos\n    )\n    dst_str = remove_blank(dedup_str, blank=blank)\n    return dst_str, keep_idx_list\n\n\ndef instance_ctc_greedy_decoder(\n    gather_info, logits_map, pts_num=4, point_gather_mode=None\n):\n    _, _, C = logits_map.shape\n    if point_gather_mode == \"align\":\n        insert_num = 0\n        gather_info = np.array(gather_info)\n        length = len(gather_info) - 1\n        for index in range(length):\n            stride_y = np.abs(\n                gather_info[index + insert_num][0]\n                - gather_info[index + 1 + insert_num][0]\n            )\n            stride_x = np.abs(\n                gather_info[index + insert_num][1]\n                - gather_info[index + 1 + insert_num][1]\n            )\n            max_points = int(max(stride_x, stride_y))\n            stride = (\n                gather_info[index + insert_num] - gather_info[index + 1 + insert_num]\n            ) / (max_points)\n            insert_num_temp = max_points - 1\n\n            for i in range(int(insert_num_temp)):\n                insert_value = gather_info[index + insert_num] - (i + 1) * stride\n                insert_index = index + i + 1 + insert_num\n                gather_info = np.insert(gather_info, insert_index, insert_value, axis=0)\n            insert_num += insert_num_temp\n        gather_info = gather_info.tolist()\n    else:\n        pass\n    ys, xs = zip(*gather_info)\n    logits_seq = logits_map[list(ys), list(xs)]\n    probs_seq = logits_seq\n    labels = np.argmax(probs_seq, axis=1)\n    dst_str = [k for k, v_ in groupby(labels) if k != C - 1]\n    detal = len(gather_info) // (pts_num - 1)\n    keep_idx_list = [0] + [detal * (i + 1) for i in range(pts_num - 2)] + [-1]\n    keep_gather_list = [gather_info[idx] for idx in keep_idx_list]\n    return dst_str, keep_gather_list\n\n\ndef ctc_decoder_for_image(\n    gather_info_list, logits_map, Lexicon_Table, pts_num=6, point_gather_mode=None\n):\n    \"\"\"\n    CTC decoder using multiple processes.\n    \"\"\"\n    decoder_str = []\n    decoder_xys = []\n    for gather_info in gather_info_list:\n        if len(gather_info) < pts_num:\n            continue\n        dst_str, xys_list = instance_ctc_greedy_decoder(\n            gather_info,\n            logits_map,\n            pts_num=pts_num,\n            point_gather_mode=point_gather_mode,\n        )\n        dst_str_readable = \"\".join([Lexicon_Table[idx] for idx in dst_str])\n        if len(dst_str_readable) < 2:\n            continue\n        decoder_str.append(dst_str_readable)\n        decoder_xys.append(xys_list)\n    return decoder_str, decoder_xys\n\n\ndef sort_with_direction(pos_list, f_direction):\n    \"\"\"\n    f_direction: h x w x 2\n    pos_list: [[y, x], [y, x], [y, x] ...]\n    \"\"\"\n\n    def sort_part_with_direction(pos_list, point_direction):\n        pos_list = np.array(pos_list).reshape(-1, 2)\n        point_direction = np.array(point_direction).reshape(-1, 2)\n        average_direction = np.mean(point_direction, axis=0, keepdims=True)\n        pos_proj_leng = np.sum(pos_list * average_direction, axis=1)\n        sorted_list = pos_list[np.argsort(pos_proj_leng)].tolist()\n        sorted_direction = point_direction[np.argsort(pos_proj_leng)].tolist()\n        return sorted_list, sorted_direction\n\n    pos_list = np.array(pos_list).reshape(-1, 2)\n    point_direction = f_direction[pos_list[:, 0], pos_list[:, 1]]  # x, y\n    point_direction = point_direction[:, ::-1]  # x, y -> y, x\n    sorted_point, sorted_direction = sort_part_with_direction(pos_list, point_direction)\n\n    point_num = len(sorted_point)\n    if point_num >= 16:\n        middle_num = point_num // 2\n        first_part_point = sorted_point[:middle_num]\n        first_point_direction = sorted_direction[:middle_num]\n        sorted_fist_part_point, sorted_fist_part_direction = sort_part_with_direction(\n            first_part_point, first_point_direction\n        )\n\n        last_part_point = sorted_point[middle_num:]\n        last_point_direction = sorted_direction[middle_num:]\n        sorted_last_part_point, sorted_last_part_direction = sort_part_with_direction(\n            last_part_point, last_point_direction\n        )\n        sorted_point = sorted_fist_part_point + sorted_last_part_point\n        sorted_direction = sorted_fist_part_direction + sorted_last_part_direction\n\n    return sorted_point, np.array(sorted_direction)\n\n\ndef add_id(pos_list, image_id=0):\n    \"\"\"\n    Add id for gather feature, for inference.\n    \"\"\"\n    new_list = []\n    for item in pos_list:\n        new_list.append((image_id, item[0], item[1]))\n    return new_list\n\n\ndef sort_and_expand_with_direction(pos_list, f_direction):\n    \"\"\"\n    f_direction: h x w x 2\n    pos_list: [[y, x], [y, x], [y, x] ...]\n    \"\"\"\n    h, w, _ = f_direction.shape\n    sorted_list, point_direction = sort_with_direction(pos_list, f_direction)\n\n    point_num = len(sorted_list)\n    sub_direction_len = max(point_num // 3, 2)\n    left_direction = point_direction[:sub_direction_len, :]\n    right_dirction = point_direction[point_num - sub_direction_len :, :]\n\n    left_average_direction = -np.mean(left_direction, axis=0, keepdims=True)\n    left_average_len = np.linalg.norm(left_average_direction)\n    left_start = np.array(sorted_list[0])\n    left_step = left_average_direction / (left_average_len + 1e-6)\n\n    right_average_direction = np.mean(right_dirction, axis=0, keepdims=True)\n    right_average_len = np.linalg.norm(right_average_direction)\n    right_step = right_average_direction / (right_average_len + 1e-6)\n    right_start = np.array(sorted_list[-1])\n\n    append_num = max(int((left_average_len + right_average_len) / 2.0 * 0.15), 1)\n    left_list = []\n    right_list = []\n    for i in range(append_num):\n        ly, lx = (\n            np.round(left_start + left_step * (i + 1))\n            .flatten()\n            .astype(\"int32\")\n            .tolist()\n        )\n        if ly < h and lx < w and (ly, lx) not in left_list:\n            left_list.append((ly, lx))\n        ry, rx = (\n            np.round(right_start + right_step * (i + 1))\n            .flatten()\n            .astype(\"int32\")\n            .tolist()\n        )\n        if ry < h and rx < w and (ry, rx) not in right_list:\n            right_list.append((ry, rx))\n\n    all_list = left_list[::-1] + sorted_list + right_list\n    return all_list\n\n\ndef sort_and_expand_with_direction_v2(pos_list, f_direction, binary_tcl_map):\n    \"\"\"\n    f_direction: h x w x 2\n    pos_list: [[y, x], [y, x], [y, x] ...]\n    binary_tcl_map: h x w\n    \"\"\"\n    h, w, _ = f_direction.shape\n    sorted_list, point_direction = sort_with_direction(pos_list, f_direction)\n\n    point_num = len(sorted_list)\n    sub_direction_len = max(point_num // 3, 2)\n    left_direction = point_direction[:sub_direction_len, :]\n    right_dirction = point_direction[point_num - sub_direction_len :, :]\n\n    left_average_direction = -np.mean(left_direction, axis=0, keepdims=True)\n    left_average_len = np.linalg.norm(left_average_direction)\n    left_start = np.array(sorted_list[0])\n    left_step = left_average_direction / (left_average_len + 1e-6)\n\n    right_average_direction = np.mean(right_dirction, axis=0, keepdims=True)\n    right_average_len = np.linalg.norm(right_average_direction)\n    right_step = right_average_direction / (right_average_len + 1e-6)\n    right_start = np.array(sorted_list[-1])\n\n    append_num = max(int((left_average_len + right_average_len) / 2.0 * 0.15), 1)\n    max_append_num = 2 * append_num\n\n    left_list = []\n    right_list = []\n    for i in range(max_append_num):\n        ly, lx = (\n            np.round(left_start + left_step * (i + 1))\n            .flatten()\n            .astype(\"int32\")\n            .tolist()\n        )\n        if ly < h and lx < w and (ly, lx) not in left_list:\n            if binary_tcl_map[ly, lx] > 0.5:\n                left_list.append((ly, lx))\n            else:\n                break\n\n    for i in range(max_append_num):\n        ry, rx = (\n            np.round(right_start + right_step * (i + 1))\n            .flatten()\n            .astype(\"int32\")\n            .tolist()\n        )\n        if ry < h and rx < w and (ry, rx) not in right_list:\n            if binary_tcl_map[ry, rx] > 0.5:\n                right_list.append((ry, rx))\n            else:\n                break\n\n    all_list = left_list[::-1] + sorted_list + right_list\n    return all_list\n\n\ndef point_pair2poly(point_pair_list):\n    \"\"\"\n    Transfer vertical point_pairs into poly point in clockwise.\n    \"\"\"\n    point_num = len(point_pair_list) * 2\n    point_list = [0] * point_num\n    for idx, point_pair in enumerate(point_pair_list):\n        point_list[idx] = point_pair[0]\n        point_list[point_num - 1 - idx] = point_pair[1]\n    return np.array(point_list).reshape(-1, 2)\n\n\ndef shrink_quad_along_width(quad, begin_width_ratio=0.0, end_width_ratio=1.0):\n    ratio_pair = np.array([[begin_width_ratio], [end_width_ratio]], dtype=np.float32)\n    p0_1 = quad[0] + (quad[1] - quad[0]) * ratio_pair\n    p3_2 = quad[3] + (quad[2] - quad[3]) * ratio_pair\n    return np.array([p0_1[0], p0_1[1], p3_2[1], p3_2[0]])\n\n\ndef expand_poly_along_width(poly, shrink_ratio_of_width=0.3):\n    \"\"\"\n    expand poly along width.\n    \"\"\"\n    point_num = poly.shape[0]\n    left_quad = np.array([poly[0], poly[1], poly[-2], poly[-1]], dtype=np.float32)\n    left_ratio = (\n        -shrink_ratio_of_width\n        * np.linalg.norm(left_quad[0] - left_quad[3])\n        / (np.linalg.norm(left_quad[0] - left_quad[1]) + 1e-6)\n    )\n    left_quad_expand = shrink_quad_along_width(left_quad, left_ratio, 1.0)\n    right_quad = np.array(\n        [\n            poly[point_num // 2 - 2],\n            poly[point_num // 2 - 1],\n            poly[point_num // 2],\n            poly[point_num // 2 + 1],\n        ],\n        dtype=np.float32,\n    )\n    right_ratio = 1.0 + shrink_ratio_of_width * np.linalg.norm(\n        right_quad[0] - right_quad[3]\n    ) / (np.linalg.norm(right_quad[0] - right_quad[1]) + 1e-6)\n    right_quad_expand = shrink_quad_along_width(right_quad, 0.0, right_ratio)\n    poly[0] = left_quad_expand[0]\n    poly[-1] = left_quad_expand[-1]\n    poly[point_num // 2 - 1] = right_quad_expand[1]\n    poly[point_num // 2] = right_quad_expand[2]\n    return poly\n\n\ndef restore_poly(\n    instance_yxs_list, seq_strs, p_border, ratio_w, ratio_h, src_w, src_h, valid_set\n):\n    poly_list = []\n    keep_str_list = []\n    for yx_center_line, keep_str in zip(instance_yxs_list, seq_strs):\n        if len(keep_str) < 2:\n            print(\"--> too short, {}\".format(keep_str))\n            continue\n\n        offset_expand = 1.0\n        if valid_set == \"totaltext\":\n            offset_expand = 1.2\n\n        point_pair_list = []\n        for y, x in yx_center_line:\n            offset = p_border[:, y, x].reshape(2, 2) * offset_expand\n            ori_yx = np.array([y, x], dtype=np.float32)\n            point_pair = (\n                (ori_yx + offset)[:, ::-1]\n                * 4.0\n                / np.array([ratio_w, ratio_h]).reshape(-1, 2)\n            )\n            point_pair_list.append(point_pair)\n\n        detected_poly = point_pair2poly(point_pair_list)\n        detected_poly = expand_poly_along_width(\n            detected_poly, shrink_ratio_of_width=0.2\n        )\n        detected_poly[:, 0] = np.clip(detected_poly[:, 0], a_min=0, a_max=src_w)\n        detected_poly[:, 1] = np.clip(detected_poly[:, 1], a_min=0, a_max=src_h)\n\n        keep_str_list.append(keep_str)\n        if valid_set == \"partvgg\":\n            middle_point = len(detected_poly) // 2\n            detected_poly = detected_poly[[0, middle_point - 1, middle_point, -1], :]\n            poly_list.append(detected_poly)\n        elif valid_set == \"totaltext\":\n            poly_list.append(detected_poly)\n        else:\n            print(\"--> Not supported format.\")\n            exit(-1)\n    return poly_list, keep_str_list\n\n\ndef generate_pivot_list_fast(\n    p_score,\n    p_char_maps,\n    f_direction,\n    Lexicon_Table,\n    score_thresh=0.5,\n    point_gather_mode=None,\n):\n    \"\"\"\n    return center point and end point of TCL instance; filter with the char maps;\n    \"\"\"\n    p_score = p_score[0]\n    f_direction = f_direction.transpose(1, 2, 0)\n    p_tcl_map = (p_score > score_thresh) * 1.0\n    skeleton_map = thin(p_tcl_map.astype(np.uint8))\n    instance_count, instance_label_map = cv2.connectedComponents(\n        skeleton_map.astype(np.uint8), connectivity=8\n    )\n\n    # get TCL Instance\n    all_pos_yxs = []\n    if instance_count > 0:\n        for instance_id in range(1, instance_count):\n            pos_list = []\n            ys, xs = np.where(instance_label_map == instance_id)\n            pos_list = list(zip(ys, xs))\n\n            if len(pos_list) < 3:\n                continue\n\n            pos_list_sorted = sort_and_expand_with_direction_v2(\n                pos_list, f_direction, p_tcl_map\n            )\n            all_pos_yxs.append(pos_list_sorted)\n\n    p_char_maps = p_char_maps.transpose([1, 2, 0])\n    decoded_str, keep_yxs_list = ctc_decoder_for_image(\n        all_pos_yxs,\n        logits_map=p_char_maps,\n        Lexicon_Table=Lexicon_Table,\n        point_gather_mode=point_gather_mode,\n    )\n    return keep_yxs_list, decoded_str\n\n\ndef extract_main_direction(pos_list, f_direction):\n    \"\"\"\n    f_direction: h x w x 2\n    pos_list: [[y, x], [y, x], [y, x] ...]\n    \"\"\"\n    pos_list = np.array(pos_list)\n    point_direction = f_direction[pos_list[:, 0], pos_list[:, 1]]\n    point_direction = point_direction[:, ::-1]  # x, y -> y, x\n    average_direction = np.mean(point_direction, axis=0, keepdims=True)\n    average_direction = average_direction / (np.linalg.norm(average_direction) + 1e-6)\n    return average_direction\n\n\ndef sort_by_direction_with_image_id_deprecated(pos_list, f_direction):\n    \"\"\"\n    f_direction: h x w x 2\n    pos_list: [[id, y, x], [id, y, x], [id, y, x] ...]\n    \"\"\"\n    pos_list_full = np.array(pos_list).reshape(-1, 3)\n    pos_list = pos_list_full[:, 1:]\n    point_direction = f_direction[pos_list[:, 0], pos_list[:, 1]]  # x, y\n    point_direction = point_direction[:, ::-1]  # x, y -> y, x\n    average_direction = np.mean(point_direction, axis=0, keepdims=True)\n    pos_proj_leng = np.sum(pos_list * average_direction, axis=1)\n    sorted_list = pos_list_full[np.argsort(pos_proj_leng)].tolist()\n    return sorted_list\n\n\ndef sort_by_direction_with_image_id(pos_list, f_direction):\n    \"\"\"\n    f_direction: h x w x 2\n    pos_list: [[y, x], [y, x], [y, x] ...]\n    \"\"\"\n\n    def sort_part_with_direction(pos_list_full, point_direction):\n        pos_list_full = np.array(pos_list_full).reshape(-1, 3)\n        pos_list = pos_list_full[:, 1:]\n        point_direction = np.array(point_direction).reshape(-1, 2)\n        average_direction = np.mean(point_direction, axis=0, keepdims=True)\n        pos_proj_leng = np.sum(pos_list * average_direction, axis=1)\n        sorted_list = pos_list_full[np.argsort(pos_proj_leng)].tolist()\n        sorted_direction = point_direction[np.argsort(pos_proj_leng)].tolist()\n        return sorted_list, sorted_direction\n\n    pos_list = np.array(pos_list).reshape(-1, 3)\n    point_direction = f_direction[pos_list[:, 1], pos_list[:, 2]]  # x, y\n    point_direction = point_direction[:, ::-1]  # x, y -> y, x\n    sorted_point, sorted_direction = sort_part_with_direction(pos_list, point_direction)\n\n    point_num = len(sorted_point)\n    if point_num >= 16:\n        middle_num = point_num // 2\n        first_part_point = sorted_point[:middle_num]\n        first_point_direction = sorted_direction[:middle_num]\n        sorted_fist_part_point, sorted_fist_part_direction = sort_part_with_direction(\n            first_part_point, first_point_direction\n        )\n\n        last_part_point = sorted_point[middle_num:]\n        last_point_direction = sorted_direction[middle_num:]\n        sorted_last_part_point, sorted_last_part_direction = sort_part_with_direction(\n            last_part_point, last_point_direction\n        )\n        sorted_point = sorted_fist_part_point + sorted_last_part_point\n        sorted_direction = sorted_fist_part_direction + sorted_last_part_direction\n\n    return sorted_point\n", "ppocr/utils/e2e_utils/extract_batchsize.py": "import paddle\nimport numpy as np\nimport copy\n\n\ndef org_tcl_rois(batch_size, pos_lists, pos_masks, label_lists, tcl_bs):\n    \"\"\" \"\"\"\n    pos_lists_, pos_masks_, label_lists_ = [], [], []\n    img_bs = batch_size\n    ngpu = int(batch_size / img_bs)\n    img_ids = np.array(pos_lists, dtype=np.int32)[:, 0, 0].copy()\n    pos_lists_split, pos_masks_split, label_lists_split = [], [], []\n    for i in range(ngpu):\n        pos_lists_split.append([])\n        pos_masks_split.append([])\n        label_lists_split.append([])\n\n    for i in range(img_ids.shape[0]):\n        img_id = img_ids[i]\n        gpu_id = int(img_id / img_bs)\n        img_id = img_id % img_bs\n        pos_list = pos_lists[i].copy()\n        pos_list[:, 0] = img_id\n        pos_lists_split[gpu_id].append(pos_list)\n        pos_masks_split[gpu_id].append(pos_masks[i].copy())\n        label_lists_split[gpu_id].append(copy.deepcopy(label_lists[i]))\n    # repeat or delete\n    for i in range(ngpu):\n        vp_len = len(pos_lists_split[i])\n        if vp_len <= tcl_bs:\n            for j in range(0, tcl_bs - vp_len):\n                pos_list = pos_lists_split[i][j].copy()\n                pos_lists_split[i].append(pos_list)\n                pos_mask = pos_masks_split[i][j].copy()\n                pos_masks_split[i].append(pos_mask)\n                label_list = copy.deepcopy(label_lists_split[i][j])\n                label_lists_split[i].append(label_list)\n        else:\n            for j in range(0, vp_len - tcl_bs):\n                c_len = len(pos_lists_split[i])\n                pop_id = np.random.permutation(c_len)[0]\n                pos_lists_split[i].pop(pop_id)\n                pos_masks_split[i].pop(pop_id)\n                label_lists_split[i].pop(pop_id)\n    # merge\n    for i in range(ngpu):\n        pos_lists_.extend(pos_lists_split[i])\n        pos_masks_.extend(pos_masks_split[i])\n        label_lists_.extend(label_lists_split[i])\n    return pos_lists_, pos_masks_, label_lists_\n\n\ndef pre_process(\n    label_list, pos_list, pos_mask, max_text_length, max_text_nums, pad_num, tcl_bs\n):\n    label_list = label_list.numpy()\n    batch, _, _, _ = label_list.shape\n    pos_list = pos_list.numpy()\n    pos_mask = pos_mask.numpy()\n    pos_list_t = []\n    pos_mask_t = []\n    label_list_t = []\n    for i in range(batch):\n        for j in range(max_text_nums):\n            if pos_mask[i, j].any():\n                pos_list_t.append(pos_list[i][j])\n                pos_mask_t.append(pos_mask[i][j])\n                label_list_t.append(label_list[i][j])\n    pos_list, pos_mask, label_list = org_tcl_rois(\n        batch, pos_list_t, pos_mask_t, label_list_t, tcl_bs\n    )\n    label = []\n    tt = [l.tolist() for l in label_list]\n    for i in range(tcl_bs):\n        k = 0\n        for j in range(max_text_length):\n            if tt[i][j][0] != pad_num:\n                k += 1\n            else:\n                break\n        label.append(k)\n    label = paddle.to_tensor(label)\n    label = paddle.cast(label, dtype=\"int64\")\n    pos_list = paddle.to_tensor(pos_list)\n    pos_mask = paddle.to_tensor(pos_mask)\n    label_list = paddle.squeeze(paddle.to_tensor(label_list), axis=2)\n    label_list = paddle.cast(label_list, dtype=\"int32\")\n    return pos_list, pos_mask, label_list, label\n", "ppocr/utils/e2e_utils/visual.py": "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport numpy as np\nimport cv2\nimport time\n\n\ndef resize_image(im, max_side_len=512):\n    \"\"\"\n    resize image to a size multiple of max_stride which is required by the network\n    :param im: the resized image\n    :param max_side_len: limit of max image size to avoid out of memory in gpu\n    :return: the resized image and the resize ratio\n    \"\"\"\n    h, w, _ = im.shape\n\n    resize_w = w\n    resize_h = h\n\n    if resize_h > resize_w:\n        ratio = float(max_side_len) / resize_h\n    else:\n        ratio = float(max_side_len) / resize_w\n\n    resize_h = int(resize_h * ratio)\n    resize_w = int(resize_w * ratio)\n\n    max_stride = 128\n    resize_h = (resize_h + max_stride - 1) // max_stride * max_stride\n    resize_w = (resize_w + max_stride - 1) // max_stride * max_stride\n    im = cv2.resize(im, (int(resize_w), int(resize_h)))\n    ratio_h = resize_h / float(h)\n    ratio_w = resize_w / float(w)\n\n    return im, (ratio_h, ratio_w)\n\n\ndef resize_image_min(im, max_side_len=512):\n    \"\"\" \"\"\"\n    h, w, _ = im.shape\n\n    resize_w = w\n    resize_h = h\n\n    if resize_h < resize_w:\n        ratio = float(max_side_len) / resize_h\n    else:\n        ratio = float(max_side_len) / resize_w\n\n    resize_h = int(resize_h * ratio)\n    resize_w = int(resize_w * ratio)\n\n    max_stride = 128\n    resize_h = (resize_h + max_stride - 1) // max_stride * max_stride\n    resize_w = (resize_w + max_stride - 1) // max_stride * max_stride\n    im = cv2.resize(im, (int(resize_w), int(resize_h)))\n    ratio_h = resize_h / float(h)\n    ratio_w = resize_w / float(w)\n    return im, (ratio_h, ratio_w)\n\n\ndef resize_image_for_totaltext(im, max_side_len=512):\n    \"\"\" \"\"\"\n    h, w, _ = im.shape\n\n    resize_w = w\n    resize_h = h\n    ratio = 1.25\n    if h * ratio > max_side_len:\n        ratio = float(max_side_len) / resize_h\n\n    resize_h = int(resize_h * ratio)\n    resize_w = int(resize_w * ratio)\n\n    max_stride = 128\n    resize_h = (resize_h + max_stride - 1) // max_stride * max_stride\n    resize_w = (resize_w + max_stride - 1) // max_stride * max_stride\n    im = cv2.resize(im, (int(resize_w), int(resize_h)))\n    ratio_h = resize_h / float(h)\n    ratio_w = resize_w / float(w)\n    return im, (ratio_h, ratio_w)\n\n\ndef point_pair2poly(point_pair_list):\n    \"\"\"\n    Transfer vertical point_pairs into poly point in clockwise.\n    \"\"\"\n    pair_length_list = []\n    for point_pair in point_pair_list:\n        pair_length = np.linalg.norm(point_pair[0] - point_pair[1])\n        pair_length_list.append(pair_length)\n    pair_length_list = np.array(pair_length_list)\n    pair_info = (\n        pair_length_list.max(),\n        pair_length_list.min(),\n        pair_length_list.mean(),\n    )\n\n    point_num = len(point_pair_list) * 2\n    point_list = [0] * point_num\n    for idx, point_pair in enumerate(point_pair_list):\n        point_list[idx] = point_pair[0]\n        point_list[point_num - 1 - idx] = point_pair[1]\n    return np.array(point_list).reshape(-1, 2), pair_info\n\n\ndef shrink_quad_along_width(quad, begin_width_ratio=0.0, end_width_ratio=1.0):\n    \"\"\"\n    Generate shrink_quad_along_width.\n    \"\"\"\n    ratio_pair = np.array([[begin_width_ratio], [end_width_ratio]], dtype=np.float32)\n    p0_1 = quad[0] + (quad[1] - quad[0]) * ratio_pair\n    p3_2 = quad[3] + (quad[2] - quad[3]) * ratio_pair\n    return np.array([p0_1[0], p0_1[1], p3_2[1], p3_2[0]])\n\n\ndef expand_poly_along_width(poly, shrink_ratio_of_width=0.3):\n    \"\"\"\n    expand poly along width.\n    \"\"\"\n    point_num = poly.shape[0]\n    left_quad = np.array([poly[0], poly[1], poly[-2], poly[-1]], dtype=np.float32)\n    left_ratio = (\n        -shrink_ratio_of_width\n        * np.linalg.norm(left_quad[0] - left_quad[3])\n        / (np.linalg.norm(left_quad[0] - left_quad[1]) + 1e-6)\n    )\n    left_quad_expand = shrink_quad_along_width(left_quad, left_ratio, 1.0)\n    right_quad = np.array(\n        [\n            poly[point_num // 2 - 2],\n            poly[point_num // 2 - 1],\n            poly[point_num // 2],\n            poly[point_num // 2 + 1],\n        ],\n        dtype=np.float32,\n    )\n    right_ratio = 1.0 + shrink_ratio_of_width * np.linalg.norm(\n        right_quad[0] - right_quad[3]\n    ) / (np.linalg.norm(right_quad[0] - right_quad[1]) + 1e-6)\n    right_quad_expand = shrink_quad_along_width(right_quad, 0.0, right_ratio)\n    poly[0] = left_quad_expand[0]\n    poly[-1] = left_quad_expand[-1]\n    poly[point_num // 2 - 1] = right_quad_expand[1]\n    poly[point_num // 2] = right_quad_expand[2]\n    return poly\n\n\ndef norm2(x, axis=None):\n    if axis:\n        return np.sqrt(np.sum(x**2, axis=axis))\n    return np.sqrt(np.sum(x**2))\n\n\ndef cos(p1, p2):\n    return (p1 * p2).sum() / (norm2(p1) * norm2(p2))\n", "ppocr/utils/loggers/loggers.py": "from .wandb_logger import WandbLogger\n\n\nclass Loggers(object):\n    def __init__(self, loggers):\n        super().__init__()\n        self.loggers = loggers\n\n    def log_metrics(self, metrics, prefix=None, step=None):\n        for logger in self.loggers:\n            logger.log_metrics(metrics, prefix=prefix, step=step)\n\n    def log_model(self, is_best, prefix, metadata=None):\n        for logger in self.loggers:\n            logger.log_model(is_best=is_best, prefix=prefix, metadata=metadata)\n\n    def close(self):\n        for logger in self.loggers:\n            logger.close()\n", "ppocr/utils/loggers/wandb_logger.py": "import os\nfrom .base_logger import BaseLogger\nfrom ppocr.utils.logging import get_logger\n\nlogger = get_logger()\n\n\nclass WandbLogger(BaseLogger):\n    def __init__(\n        self,\n        project=None,\n        name=None,\n        id=None,\n        entity=None,\n        save_dir=None,\n        config=None,\n        **kwargs,\n    ):\n        try:\n            import wandb\n\n            self.wandb = wandb\n        except ModuleNotFoundError:\n            raise ModuleNotFoundError(\"Please install wandb using `pip install wandb`\")\n\n        self.project = project\n        self.name = name\n        self.id = id\n        self.save_dir = save_dir\n        self.config = config\n        self.kwargs = kwargs\n        self.entity = entity\n        self._run = None\n        self._wandb_init = dict(\n            project=self.project,\n            name=self.name,\n            id=self.id,\n            entity=self.entity,\n            dir=self.save_dir,\n            resume=\"allow\",\n        )\n        self._wandb_init.update(**kwargs)\n\n        _ = self.run\n\n        if self.config:\n            self.run.config.update(self.config)\n\n    @property\n    def run(self):\n        if self._run is None:\n            if self.wandb.run is not None:\n                logger.info(\n                    \"There is a wandb run already in progress \"\n                    \"and newly created instances of `WandbLogger` will reuse\"\n                    \" this run. If this is not desired, call `wandb.finish()`\"\n                    \"before instantiating `WandbLogger`.\"\n                )\n                self._run = self.wandb.run\n            else:\n                self._run = self.wandb.init(**self._wandb_init)\n        return self._run\n\n    def log_metrics(self, metrics, prefix=None, step=None):\n        if not prefix:\n            prefix = \"\"\n        updated_metrics = {prefix.lower() + \"/\" + k: v for k, v in metrics.items()}\n\n        self.run.log(updated_metrics, step=step)\n\n    def log_model(self, is_best, prefix, metadata=None):\n        model_path = os.path.join(self.save_dir, prefix + \".pdparams\")\n        artifact = self.wandb.Artifact(\n            \"model-{}\".format(self.run.id), type=\"model\", metadata=metadata\n        )\n        artifact.add_file(model_path, name=\"model_ckpt.pdparams\")\n\n        aliases = [prefix]\n        if is_best:\n            aliases.append(\"best\")\n\n        self.run.log_artifact(artifact, aliases=aliases)\n\n    def close(self):\n        self.run.finish()\n", "ppocr/utils/loggers/base_logger.py": "import os\nfrom abc import ABC, abstractmethod\n\n\nclass BaseLogger(ABC):\n    def __init__(self, save_dir):\n        self.save_dir = save_dir\n        os.makedirs(self.save_dir, exist_ok=True)\n\n    @abstractmethod\n    def log_metrics(self, metrics, prefix=None):\n        pass\n\n    @abstractmethod\n    def close(self):\n        pass\n", "ppocr/utils/loggers/__init__.py": "from .wandb_logger import WandbLogger\nfrom .loggers import Loggers\n", "ppocr/postprocess/sast_postprocess.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\n\n__dir__ = os.path.dirname(__file__)\nsys.path.append(__dir__)\nsys.path.append(os.path.join(__dir__, \"..\"))\n\nimport numpy as np\nfrom .locality_aware_nms import nms_locality\nimport paddle\nimport cv2\nimport time\n\n\nclass SASTPostProcess(object):\n    \"\"\"\n    The post process for SAST.\n    \"\"\"\n\n    def __init__(\n        self,\n        score_thresh=0.5,\n        nms_thresh=0.2,\n        sample_pts_num=2,\n        shrink_ratio_of_width=0.3,\n        expand_scale=1.0,\n        tcl_map_thresh=0.5,\n        **kwargs,\n    ):\n        self.score_thresh = score_thresh\n        self.nms_thresh = nms_thresh\n        self.sample_pts_num = sample_pts_num\n        self.shrink_ratio_of_width = shrink_ratio_of_width\n        self.expand_scale = expand_scale\n        self.tcl_map_thresh = tcl_map_thresh\n\n        # c++ la-nms is faster, but only support python 3.5\n        self.is_python35 = False\n        if sys.version_info.major == 3 and sys.version_info.minor == 5:\n            self.is_python35 = True\n\n    def point_pair2poly(self, point_pair_list):\n        \"\"\"\n        Transfer vertical point_pairs into poly point in clockwise.\n        \"\"\"\n        # constract poly\n        point_num = len(point_pair_list) * 2\n        point_list = [0] * point_num\n        for idx, point_pair in enumerate(point_pair_list):\n            point_list[idx] = point_pair[0]\n            point_list[point_num - 1 - idx] = point_pair[1]\n        return np.array(point_list).reshape(-1, 2)\n\n    def shrink_quad_along_width(self, quad, begin_width_ratio=0.0, end_width_ratio=1.0):\n        \"\"\"\n        Generate shrink_quad_along_width.\n        \"\"\"\n        ratio_pair = np.array(\n            [[begin_width_ratio], [end_width_ratio]], dtype=np.float32\n        )\n        p0_1 = quad[0] + (quad[1] - quad[0]) * ratio_pair\n        p3_2 = quad[3] + (quad[2] - quad[3]) * ratio_pair\n        return np.array([p0_1[0], p0_1[1], p3_2[1], p3_2[0]])\n\n    def expand_poly_along_width(self, poly, shrink_ratio_of_width=0.3):\n        \"\"\"\n        expand poly along width.\n        \"\"\"\n        point_num = poly.shape[0]\n        left_quad = np.array([poly[0], poly[1], poly[-2], poly[-1]], dtype=np.float32)\n        left_ratio = (\n            -shrink_ratio_of_width\n            * np.linalg.norm(left_quad[0] - left_quad[3])\n            / (np.linalg.norm(left_quad[0] - left_quad[1]) + 1e-6)\n        )\n        left_quad_expand = self.shrink_quad_along_width(left_quad, left_ratio, 1.0)\n        right_quad = np.array(\n            [\n                poly[point_num // 2 - 2],\n                poly[point_num // 2 - 1],\n                poly[point_num // 2],\n                poly[point_num // 2 + 1],\n            ],\n            dtype=np.float32,\n        )\n        right_ratio = 1.0 + shrink_ratio_of_width * np.linalg.norm(\n            right_quad[0] - right_quad[3]\n        ) / (np.linalg.norm(right_quad[0] - right_quad[1]) + 1e-6)\n        right_quad_expand = self.shrink_quad_along_width(right_quad, 0.0, right_ratio)\n        poly[0] = left_quad_expand[0]\n        poly[-1] = left_quad_expand[-1]\n        poly[point_num // 2 - 1] = right_quad_expand[1]\n        poly[point_num // 2] = right_quad_expand[2]\n        return poly\n\n    def restore_quad(self, tcl_map, tcl_map_thresh, tvo_map):\n        \"\"\"Restore quad.\"\"\"\n        xy_text = np.argwhere(tcl_map[:, :, 0] > tcl_map_thresh)\n        xy_text = xy_text[:, ::-1]  # (n, 2)\n\n        # Sort the text boxes via the y axis\n        xy_text = xy_text[np.argsort(xy_text[:, 1])]\n\n        scores = tcl_map[xy_text[:, 1], xy_text[:, 0], 0]\n        scores = scores[:, np.newaxis]\n\n        # Restore\n        point_num = int(tvo_map.shape[-1] / 2)\n        assert point_num == 4\n        tvo_map = tvo_map[xy_text[:, 1], xy_text[:, 0], :]\n        xy_text_tile = np.tile(xy_text, (1, point_num))  # (n, point_num * 2)\n        quads = xy_text_tile - tvo_map\n\n        return scores, quads, xy_text\n\n    def quad_area(self, quad):\n        \"\"\"\n        compute area of a quad.\n        \"\"\"\n        edge = [\n            (quad[1][0] - quad[0][0]) * (quad[1][1] + quad[0][1]),\n            (quad[2][0] - quad[1][0]) * (quad[2][1] + quad[1][1]),\n            (quad[3][0] - quad[2][0]) * (quad[3][1] + quad[2][1]),\n            (quad[0][0] - quad[3][0]) * (quad[0][1] + quad[3][1]),\n        ]\n        return np.sum(edge) / 2.0\n\n    def nms(self, dets):\n        if self.is_python35:\n            from ppocr.utils.utility import check_install\n\n            check_install(\"lanms\", \"lanms-nova\")\n            import lanms\n\n            dets = lanms.merge_quadrangle_n9(dets, self.nms_thresh)\n        else:\n            dets = nms_locality(dets, self.nms_thresh)\n        return dets\n\n    def cluster_by_quads_tco(self, tcl_map, tcl_map_thresh, quads, tco_map):\n        \"\"\"\n        Cluster pixels in tcl_map based on quads.\n        \"\"\"\n        instance_count = quads.shape[0] + 1  # contain background\n        instance_label_map = np.zeros(tcl_map.shape[:2], dtype=np.int32)\n        if instance_count == 1:\n            return instance_count, instance_label_map\n\n        # predict text center\n        xy_text = np.argwhere(tcl_map[:, :, 0] > tcl_map_thresh)\n        n = xy_text.shape[0]\n        xy_text = xy_text[:, ::-1]  # (n, 2)\n        tco = tco_map[xy_text[:, 1], xy_text[:, 0], :]  # (n, 2)\n        pred_tc = xy_text - tco\n\n        # get gt text center\n        m = quads.shape[0]\n        gt_tc = np.mean(quads, axis=1)  # (m, 2)\n\n        pred_tc_tile = np.tile(pred_tc[:, np.newaxis, :], (1, m, 1))  # (n, m, 2)\n        gt_tc_tile = np.tile(gt_tc[np.newaxis, :, :], (n, 1, 1))  # (n, m, 2)\n        dist_mat = np.linalg.norm(pred_tc_tile - gt_tc_tile, axis=2)  # (n, m)\n        xy_text_assign = np.argmin(dist_mat, axis=1) + 1  # (n,)\n\n        instance_label_map[xy_text[:, 1], xy_text[:, 0]] = xy_text_assign\n        return instance_count, instance_label_map\n\n    def estimate_sample_pts_num(self, quad, xy_text):\n        \"\"\"\n        Estimate sample points number.\n        \"\"\"\n        eh = (\n            np.linalg.norm(quad[0] - quad[3]) + np.linalg.norm(quad[1] - quad[2])\n        ) / 2.0\n        ew = (\n            np.linalg.norm(quad[0] - quad[1]) + np.linalg.norm(quad[2] - quad[3])\n        ) / 2.0\n\n        dense_sample_pts_num = max(2, int(ew))\n        dense_xy_center_line = xy_text[\n            np.linspace(\n                0,\n                xy_text.shape[0] - 1,\n                dense_sample_pts_num,\n                endpoint=True,\n                dtype=np.float32,\n            ).astype(np.int32)\n        ]\n\n        dense_xy_center_line_diff = dense_xy_center_line[1:] - dense_xy_center_line[:-1]\n        estimate_arc_len = np.sum(np.linalg.norm(dense_xy_center_line_diff, axis=1))\n\n        sample_pts_num = max(2, int(estimate_arc_len / eh))\n        return sample_pts_num\n\n    def detect_sast(\n        self,\n        tcl_map,\n        tvo_map,\n        tbo_map,\n        tco_map,\n        ratio_w,\n        ratio_h,\n        src_w,\n        src_h,\n        shrink_ratio_of_width=0.3,\n        tcl_map_thresh=0.5,\n        offset_expand=1.0,\n        out_strid=4.0,\n    ):\n        \"\"\"\n        first resize the tcl_map, tvo_map and tbo_map to the input_size, then restore the polys\n        \"\"\"\n        # restore quad\n        scores, quads, xy_text = self.restore_quad(tcl_map, tcl_map_thresh, tvo_map)\n        dets = np.hstack((quads, scores)).astype(np.float32, copy=False)\n        dets = self.nms(dets)\n        if dets.shape[0] == 0:\n            return []\n        quads = dets[:, :-1].reshape(-1, 4, 2)\n\n        # Compute quad area\n        quad_areas = []\n        for quad in quads:\n            quad_areas.append(-self.quad_area(quad))\n\n        # instance segmentation\n        # instance_count, instance_label_map = cv2.connectedComponents(tcl_map.astype(np.uint8), connectivity=8)\n        instance_count, instance_label_map = self.cluster_by_quads_tco(\n            tcl_map, tcl_map_thresh, quads, tco_map\n        )\n\n        # restore single poly with tcl instance.\n        poly_list = []\n        for instance_idx in range(1, instance_count):\n            xy_text = np.argwhere(instance_label_map == instance_idx)[:, ::-1]\n            quad = quads[instance_idx - 1]\n            q_area = quad_areas[instance_idx - 1]\n            if q_area < 5:\n                continue\n\n            #\n            len1 = float(np.linalg.norm(quad[0] - quad[1]))\n            len2 = float(np.linalg.norm(quad[1] - quad[2]))\n            min_len = min(len1, len2)\n            if min_len < 3:\n                continue\n\n            # filter small CC\n            if xy_text.shape[0] <= 0:\n                continue\n\n            # filter low confidence instance\n            xy_text_scores = tcl_map[xy_text[:, 1], xy_text[:, 0], 0]\n            if np.sum(xy_text_scores) / quad_areas[instance_idx - 1] < 0.1:\n                # if np.sum(xy_text_scores) / quad_areas[instance_idx - 1] < 0.05:\n                continue\n\n            # sort xy_text\n            left_center_pt = np.array(\n                [[(quad[0, 0] + quad[-1, 0]) / 2.0, (quad[0, 1] + quad[-1, 1]) / 2.0]]\n            )  # (1, 2)\n            right_center_pt = np.array(\n                [[(quad[1, 0] + quad[2, 0]) / 2.0, (quad[1, 1] + quad[2, 1]) / 2.0]]\n            )  # (1, 2)\n            proj_unit_vec = (right_center_pt - left_center_pt) / (\n                np.linalg.norm(right_center_pt - left_center_pt) + 1e-6\n            )\n            proj_value = np.sum(xy_text * proj_unit_vec, axis=1)\n            xy_text = xy_text[np.argsort(proj_value)]\n\n            # Sample pts in tcl map\n            if self.sample_pts_num == 0:\n                sample_pts_num = self.estimate_sample_pts_num(quad, xy_text)\n            else:\n                sample_pts_num = self.sample_pts_num\n            xy_center_line = xy_text[\n                np.linspace(\n                    0,\n                    xy_text.shape[0] - 1,\n                    sample_pts_num,\n                    endpoint=True,\n                    dtype=np.float32,\n                ).astype(np.int32)\n            ]\n\n            point_pair_list = []\n            for x, y in xy_center_line:\n                # get corresponding offset\n                offset = tbo_map[y, x, :].reshape(2, 2)\n                if offset_expand != 1.0:\n                    offset_length = np.linalg.norm(offset, axis=1, keepdims=True)\n                    expand_length = np.clip(\n                        offset_length * (offset_expand - 1), a_min=0.5, a_max=3.0\n                    )\n                    offset_detal = offset / offset_length * expand_length\n                    offset = offset + offset_detal\n                    # original point\n                ori_yx = np.array([y, x], dtype=np.float32)\n                point_pair = (\n                    (ori_yx + offset)[:, ::-1]\n                    * out_strid\n                    / np.array([ratio_w, ratio_h]).reshape(-1, 2)\n                )\n                point_pair_list.append(point_pair)\n\n            # ndarry: (x, 2), expand poly along width\n            detected_poly = self.point_pair2poly(point_pair_list)\n            detected_poly = self.expand_poly_along_width(\n                detected_poly, shrink_ratio_of_width\n            )\n            detected_poly[:, 0] = np.clip(detected_poly[:, 0], a_min=0, a_max=src_w)\n            detected_poly[:, 1] = np.clip(detected_poly[:, 1], a_min=0, a_max=src_h)\n            poly_list.append(detected_poly)\n\n        return poly_list\n\n    def __call__(self, outs_dict, shape_list):\n        score_list = outs_dict[\"f_score\"]\n        border_list = outs_dict[\"f_border\"]\n        tvo_list = outs_dict[\"f_tvo\"]\n        tco_list = outs_dict[\"f_tco\"]\n        if isinstance(score_list, paddle.Tensor):\n            score_list = score_list.numpy()\n            border_list = border_list.numpy()\n            tvo_list = tvo_list.numpy()\n            tco_list = tco_list.numpy()\n\n        img_num = len(shape_list)\n        poly_lists = []\n        for ino in range(img_num):\n            p_score = score_list[ino].transpose((1, 2, 0))\n            p_border = border_list[ino].transpose((1, 2, 0))\n            p_tvo = tvo_list[ino].transpose((1, 2, 0))\n            p_tco = tco_list[ino].transpose((1, 2, 0))\n            src_h, src_w, ratio_h, ratio_w = shape_list[ino]\n\n            poly_list = self.detect_sast(\n                p_score,\n                p_tvo,\n                p_border,\n                p_tco,\n                ratio_w,\n                ratio_h,\n                src_w,\n                src_h,\n                shrink_ratio_of_width=self.shrink_ratio_of_width,\n                tcl_map_thresh=self.tcl_map_thresh,\n                offset_expand=self.expand_scale,\n            )\n            poly_lists.append({\"points\": np.array(poly_list)})\n\n        return poly_lists\n", "ppocr/postprocess/locality_aware_nms.py": "\"\"\"\nLocality aware nms.\nThis code is refered from: https://github.com/songdejia/EAST/blob/master/locality_aware_nms.py\n\"\"\"\n\nimport numpy as np\nfrom shapely.geometry import Polygon\n\n\ndef intersection(g, p):\n    \"\"\"\n    Intersection.\n    \"\"\"\n    g = Polygon(g[:8].reshape((4, 2)))\n    p = Polygon(p[:8].reshape((4, 2)))\n    g = g.buffer(0)\n    p = p.buffer(0)\n    if not g.is_valid or not p.is_valid:\n        return 0\n    inter = Polygon(g).intersection(Polygon(p)).area\n    union = g.area + p.area - inter\n    if union == 0:\n        return 0\n    else:\n        return inter / union\n\n\ndef intersection_iog(g, p):\n    \"\"\"\n    Intersection_iog.\n    \"\"\"\n    g = Polygon(g[:8].reshape((4, 2)))\n    p = Polygon(p[:8].reshape((4, 2)))\n    if not g.is_valid or not p.is_valid:\n        return 0\n    inter = Polygon(g).intersection(Polygon(p)).area\n    # union = g.area + p.area - inter\n    union = p.area\n    if union == 0:\n        print(\"p_area is very small\")\n        return 0\n    else:\n        return inter / union\n\n\ndef weighted_merge(g, p):\n    \"\"\"\n    Weighted merge.\n    \"\"\"\n    g[:8] = (g[8] * g[:8] + p[8] * p[:8]) / (g[8] + p[8])\n    g[8] = g[8] + p[8]\n    return g\n\n\ndef standard_nms(S, thres):\n    \"\"\"\n    Standard nms.\n    \"\"\"\n    order = np.argsort(S[:, 8])[::-1]\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n        ovr = np.array([intersection(S[i], S[t]) for t in order[1:]])\n\n        inds = np.where(ovr <= thres)[0]\n        order = order[inds + 1]\n\n    return S[keep]\n\n\ndef standard_nms_inds(S, thres):\n    \"\"\"\n    Standard nms, retun inds.\n    \"\"\"\n    order = np.argsort(S[:, 8])[::-1]\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n        ovr = np.array([intersection(S[i], S[t]) for t in order[1:]])\n\n        inds = np.where(ovr <= thres)[0]\n        order = order[inds + 1]\n\n    return keep\n\n\ndef nms(S, thres):\n    \"\"\"\n    nms.\n    \"\"\"\n    order = np.argsort(S[:, 8])[::-1]\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n        ovr = np.array([intersection(S[i], S[t]) for t in order[1:]])\n\n        inds = np.where(ovr <= thres)[0]\n        order = order[inds + 1]\n\n    return keep\n\n\ndef soft_nms(boxes_in, Nt_thres=0.3, threshold=0.8, sigma=0.5, method=2):\n    \"\"\"\n    soft_nms\n    :para boxes_in, N x 9 (coords + score)\n    :para threshould, eliminate cases min score(0.001)\n    :para Nt_thres, iou_threshi\n    :para sigma, gaussian weght\n    :method, linear or gaussian\n    \"\"\"\n    boxes = boxes_in.copy()\n    N = boxes.shape[0]\n    if N is None or N < 1:\n        return np.array([])\n    pos, maxpos = 0, 0\n    weight = 0.0\n    inds = np.arange(N)\n    tbox, sbox = boxes[0].copy(), boxes[0].copy()\n    for i in range(N):\n        maxscore = boxes[i, 8]\n        maxpos = i\n        tbox = boxes[i].copy()\n        ti = inds[i]\n        pos = i + 1\n        # get max box\n        while pos < N:\n            if maxscore < boxes[pos, 8]:\n                maxscore = boxes[pos, 8]\n                maxpos = pos\n            pos = pos + 1\n        # add max box as a detection\n        boxes[i, :] = boxes[maxpos, :]\n        inds[i] = inds[maxpos]\n        # swap\n        boxes[maxpos, :] = tbox\n        inds[maxpos] = ti\n        tbox = boxes[i].copy()\n        pos = i + 1\n        # NMS iteration\n        while pos < N:\n            sbox = boxes[pos].copy()\n            ts_iou_val = intersection(tbox, sbox)\n            if ts_iou_val > 0:\n                if method == 1:\n                    if ts_iou_val > Nt_thres:\n                        weight = 1 - ts_iou_val\n                    else:\n                        weight = 1\n                elif method == 2:\n                    weight = np.exp(-1.0 * ts_iou_val**2 / sigma)\n                else:\n                    if ts_iou_val > Nt_thres:\n                        weight = 0\n                    else:\n                        weight = 1\n                boxes[pos, 8] = weight * boxes[pos, 8]\n                # if box score falls below thresold, discard the box by\n                # swaping last box update N\n                if boxes[pos, 8] < threshold:\n                    boxes[pos, :] = boxes[N - 1, :]\n                    inds[pos] = inds[N - 1]\n                    N = N - 1\n                    pos = pos - 1\n            pos = pos + 1\n\n    return boxes[:N]\n\n\ndef nms_locality(polys, thres=0.3):\n    \"\"\"\n    locality aware nms of EAST\n    :param polys: a N*9 numpy array. first 8 coordinates, then prob\n    :return: boxes after nms\n    \"\"\"\n    S = []\n    p = None\n    for g in polys:\n        if p is not None and intersection(g, p) > thres:\n            p = weighted_merge(g, p)\n        else:\n            if p is not None:\n                S.append(p)\n            p = g\n    if p is not None:\n        S.append(p)\n\n    if len(S) == 0:\n        return np.array([])\n    return standard_nms(np.array(S), thres)\n\n\nif __name__ == \"__main__\":\n    # 343,350,448,135,474,143,369,359\n    print(Polygon(np.array([[343, 350], [448, 135], [474, 143], [369, 359]])).area)\n", "ppocr/postprocess/table_postprocess.py": "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport paddle\n\nfrom .rec_postprocess import AttnLabelDecode\n\n\nclass TableLabelDecode(AttnLabelDecode):\n    \"\"\" \"\"\"\n\n    def __init__(self, character_dict_path, merge_no_span_structure=False, **kwargs):\n        dict_character = []\n        with open(character_dict_path, \"rb\") as fin:\n            lines = fin.readlines()\n            for line in lines:\n                line = line.decode(\"utf-8\").strip(\"\\n\").strip(\"\\r\\n\")\n                dict_character.append(line)\n\n        if merge_no_span_structure:\n            if \"<td></td>\" not in dict_character:\n                dict_character.append(\"<td></td>\")\n            if \"<td>\" in dict_character:\n                dict_character.remove(\"<td>\")\n\n        dict_character = self.add_special_char(dict_character)\n        self.dict = {}\n        for i, char in enumerate(dict_character):\n            self.dict[char] = i\n        self.character = dict_character\n        self.td_token = [\"<td>\", \"<td\", \"<td></td>\"]\n\n    def __call__(self, preds, batch=None):\n        structure_probs = preds[\"structure_probs\"]\n        bbox_preds = preds[\"loc_preds\"]\n        if isinstance(structure_probs, paddle.Tensor):\n            structure_probs = structure_probs.numpy()\n        if isinstance(bbox_preds, paddle.Tensor):\n            bbox_preds = bbox_preds.numpy()\n        shape_list = batch[-1]\n        result = self.decode(structure_probs, bbox_preds, shape_list)\n        if len(batch) == 1:  # only contains shape\n            return result\n\n        label_decode_result = self.decode_label(batch)\n        return result, label_decode_result\n\n    def decode(self, structure_probs, bbox_preds, shape_list):\n        \"\"\"convert text-label into text-index.\"\"\"\n        ignored_tokens = self.get_ignored_tokens()\n        end_idx = self.dict[self.end_str]\n\n        structure_idx = structure_probs.argmax(axis=2)\n        structure_probs = structure_probs.max(axis=2)\n\n        structure_batch_list = []\n        bbox_batch_list = []\n        batch_size = len(structure_idx)\n        for batch_idx in range(batch_size):\n            structure_list = []\n            bbox_list = []\n            score_list = []\n            for idx in range(len(structure_idx[batch_idx])):\n                char_idx = int(structure_idx[batch_idx][idx])\n                if idx > 0 and char_idx == end_idx:\n                    break\n                if char_idx in ignored_tokens:\n                    continue\n                text = self.character[char_idx]\n                if text in self.td_token:\n                    bbox = bbox_preds[batch_idx, idx]\n                    bbox = self._bbox_decode(bbox, shape_list[batch_idx])\n                    bbox_list.append(bbox)\n                structure_list.append(text)\n                score_list.append(structure_probs[batch_idx, idx])\n            structure_batch_list.append([structure_list, np.mean(score_list)])\n            bbox_batch_list.append(np.array(bbox_list))\n        result = {\n            \"bbox_batch_list\": bbox_batch_list,\n            \"structure_batch_list\": structure_batch_list,\n        }\n        return result\n\n    def decode_label(self, batch):\n        \"\"\"convert text-label into text-index.\"\"\"\n        structure_idx = batch[1]\n        gt_bbox_list = batch[2]\n        shape_list = batch[-1]\n        ignored_tokens = self.get_ignored_tokens()\n        end_idx = self.dict[self.end_str]\n\n        structure_batch_list = []\n        bbox_batch_list = []\n        batch_size = len(structure_idx)\n        for batch_idx in range(batch_size):\n            structure_list = []\n            bbox_list = []\n            for idx in range(len(structure_idx[batch_idx])):\n                char_idx = int(structure_idx[batch_idx][idx])\n                if idx > 0 and char_idx == end_idx:\n                    break\n                if char_idx in ignored_tokens:\n                    continue\n                structure_list.append(self.character[char_idx])\n\n                bbox = gt_bbox_list[batch_idx][idx]\n                if bbox.sum() != 0:\n                    bbox = self._bbox_decode(bbox, shape_list[batch_idx])\n                    bbox_list.append(bbox)\n            structure_batch_list.append(structure_list)\n            bbox_batch_list.append(bbox_list)\n        result = {\n            \"bbox_batch_list\": bbox_batch_list,\n            \"structure_batch_list\": structure_batch_list,\n        }\n        return result\n\n    def _bbox_decode(self, bbox, shape):\n        h, w, ratio_h, ratio_w, pad_h, pad_w = shape\n        bbox[0::2] *= w\n        bbox[1::2] *= h\n        return bbox\n\n\nclass TableMasterLabelDecode(TableLabelDecode):\n    \"\"\" \"\"\"\n\n    def __init__(\n        self,\n        character_dict_path,\n        box_shape=\"ori\",\n        merge_no_span_structure=True,\n        **kwargs,\n    ):\n        super(TableMasterLabelDecode, self).__init__(\n            character_dict_path, merge_no_span_structure\n        )\n        self.box_shape = box_shape\n        assert box_shape in [\n            \"ori\",\n            \"pad\",\n        ], \"The shape used for box normalization must be ori or pad\"\n\n    def add_special_char(self, dict_character):\n        self.beg_str = \"<SOS>\"\n        self.end_str = \"<EOS>\"\n        self.unknown_str = \"<UKN>\"\n        self.pad_str = \"<PAD>\"\n        dict_character = dict_character\n        dict_character = dict_character + [\n            self.unknown_str,\n            self.beg_str,\n            self.end_str,\n            self.pad_str,\n        ]\n        return dict_character\n\n    def get_ignored_tokens(self):\n        pad_idx = self.dict[self.pad_str]\n        start_idx = self.dict[self.beg_str]\n        end_idx = self.dict[self.end_str]\n        unknown_idx = self.dict[self.unknown_str]\n        return [start_idx, end_idx, pad_idx, unknown_idx]\n\n    def _bbox_decode(self, bbox, shape):\n        h, w, ratio_h, ratio_w, pad_h, pad_w = shape\n        if self.box_shape == \"pad\":\n            h, w = pad_h, pad_w\n        bbox[0::2] *= w\n        bbox[1::2] *= h\n        bbox[0::2] /= ratio_w\n        bbox[1::2] /= ratio_h\n        x, y, w, h = bbox\n        x1, y1, x2, y2 = x - w // 2, y - h // 2, x + w // 2, y + h // 2\n        bbox = np.array([x1, y1, x2, y2])\n        return bbox\n", "ppocr/postprocess/ct_postprocess.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refered from:\nhttps://github.com/shengtao96/CentripetalText/blob/main/test.py\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport os.path as osp\nimport numpy as np\nimport cv2\nimport paddle\nimport pyclipper\n\n\nclass CTPostProcess(object):\n    \"\"\"\n    The post process for Centripetal Text (CT).\n    \"\"\"\n\n    def __init__(self, min_score=0.88, min_area=16, box_type=\"poly\", **kwargs):\n        self.min_score = min_score\n        self.min_area = min_area\n        self.box_type = box_type\n\n        self.coord = np.zeros((2, 300, 300), dtype=np.int32)\n        for i in range(300):\n            for j in range(300):\n                self.coord[0, i, j] = j\n                self.coord[1, i, j] = i\n\n    def __call__(self, preds, batch):\n        outs = preds[\"maps\"]\n        out_scores = preds[\"score\"]\n\n        if isinstance(outs, paddle.Tensor):\n            outs = outs.numpy()\n        if isinstance(out_scores, paddle.Tensor):\n            out_scores = out_scores.numpy()\n\n        batch_size = outs.shape[0]\n        boxes_batch = []\n        for idx in range(batch_size):\n            bboxes = []\n            scores = []\n\n            img_shape = batch[idx]\n\n            org_img_size = img_shape[:3]\n            img_shape = img_shape[3:]\n            img_size = img_shape[:2]\n\n            out = np.expand_dims(outs[idx], axis=0)\n            outputs = dict()\n\n            score = np.expand_dims(out_scores[idx], axis=0)\n\n            kernel = out[:, 0, :, :] > 0.2\n            loc = out[:, 1:, :, :].astype(\"float32\")\n\n            score = score[0].astype(np.float32)\n            kernel = kernel[0].astype(np.uint8)\n            loc = loc[0].astype(np.float32)\n\n            label_num, label_kernel = cv2.connectedComponents(kernel, connectivity=4)\n\n            for i in range(1, label_num):\n                ind = label_kernel == i\n                if ind.sum() < 10:  # pixel number less than 10, treated as background\n                    label_kernel[ind] = 0\n\n            label = np.zeros_like(label_kernel)\n            h, w = label_kernel.shape\n            pixels = self.coord[:, :h, :w].reshape(2, -1)\n            points = pixels.transpose([1, 0]).astype(np.float32)\n\n            off_points = (points + 10.0 / 4.0 * loc[:, pixels[1], pixels[0]].T).astype(\n                np.int32\n            )\n            off_points[:, 0] = np.clip(off_points[:, 0], 0, label.shape[1] - 1)\n            off_points[:, 1] = np.clip(off_points[:, 1], 0, label.shape[0] - 1)\n\n            label[pixels[1], pixels[0]] = label_kernel[\n                off_points[:, 1], off_points[:, 0]\n            ]\n            label[label_kernel > 0] = label_kernel[label_kernel > 0]\n\n            score_pocket = [0.0]\n            for i in range(1, label_num):\n                ind = label_kernel == i\n                if ind.sum() == 0:\n                    score_pocket.append(0.0)\n                    continue\n                score_i = np.mean(score[ind])\n                score_pocket.append(score_i)\n\n            label_num = np.max(label) + 1\n            label = cv2.resize(\n                label, (img_size[1], img_size[0]), interpolation=cv2.INTER_NEAREST\n            )\n\n            scale = (\n                float(org_img_size[1]) / float(img_size[1]),\n                float(org_img_size[0]) / float(img_size[0]),\n            )\n\n            for i in range(1, label_num):\n                ind = label == i\n                points = np.array(np.where(ind)).transpose((1, 0))\n\n                if points.shape[0] < self.min_area:\n                    continue\n\n                score_i = score_pocket[i]\n                if score_i < self.min_score:\n                    continue\n\n                if self.box_type == \"rect\":\n                    rect = cv2.minAreaRect(points[:, ::-1])\n                    bbox = cv2.boxPoints(rect) * scale\n                    z = bbox.mean(0)\n                    bbox = z + (bbox - z) * 0.85\n                elif self.box_type == \"poly\":\n                    binary = np.zeros(label.shape, dtype=\"uint8\")\n                    binary[ind] = 1\n                    try:\n                        _, contours, _ = cv2.findContours(\n                            binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n                        )\n                    except BaseException:\n                        contours, _ = cv2.findContours(\n                            binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n                        )\n\n                    bbox = contours[0] * scale\n\n                bbox = bbox.astype(\"int32\")\n                bboxes.append(bbox.reshape(-1, 2))\n                scores.append(score_i)\n\n            boxes_batch.append({\"points\": bboxes})\n\n        return boxes_batch\n", "ppocr/postprocess/drrg_postprocess.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/open-mmlab/mmocr/blob/main/mmocr/models/textdet/postprocess/drrg_postprocessor.py\n\"\"\"\n\nimport functools\nimport operator\n\nimport numpy as np\nimport paddle\nfrom numpy.linalg import norm\nimport cv2\n\n\nclass Node:\n    def __init__(self, ind):\n        self.__ind = ind\n        self.__links = set()\n\n    @property\n    def ind(self):\n        return self.__ind\n\n    @property\n    def links(self):\n        return set(self.__links)\n\n    def add_link(self, link_node):\n        self.__links.add(link_node)\n        link_node.__links.add(self)\n\n\ndef graph_propagation(edges, scores, text_comps, edge_len_thr=50.0):\n    assert edges.ndim == 2\n    assert edges.shape[1] == 2\n    assert edges.shape[0] == scores.shape[0]\n    assert text_comps.ndim == 2\n    assert isinstance(edge_len_thr, float)\n\n    edges = np.sort(edges, axis=1)\n    score_dict = {}\n    for i, edge in enumerate(edges):\n        if text_comps is not None:\n            box1 = text_comps[edge[0], :8].reshape(4, 2)\n            box2 = text_comps[edge[1], :8].reshape(4, 2)\n            center1 = np.mean(box1, axis=0)\n            center2 = np.mean(box2, axis=0)\n            distance = norm(center1 - center2)\n            if distance > edge_len_thr:\n                scores[i] = 0\n        if (edge[0], edge[1]) in score_dict:\n            score_dict[edge[0], edge[1]] = 0.5 * (\n                score_dict[edge[0], edge[1]] + scores[i]\n            )\n        else:\n            score_dict[edge[0], edge[1]] = scores[i]\n\n    nodes = np.sort(np.unique(edges.flatten()))\n    mapping = -1 * np.ones((np.max(nodes) + 1), dtype=np.int32)\n    mapping[nodes] = np.arange(nodes.shape[0])\n    order_inds = mapping[edges]\n    vertices = [Node(node) for node in nodes]\n    for ind in order_inds:\n        vertices[ind[0]].add_link(vertices[ind[1]])\n\n    return vertices, score_dict\n\n\ndef connected_components(nodes, score_dict, link_thr):\n    assert isinstance(nodes, list)\n    assert all([isinstance(node, Node) for node in nodes])\n    assert isinstance(score_dict, dict)\n    assert isinstance(link_thr, float)\n\n    clusters = []\n    nodes = set(nodes)\n    while nodes:\n        node = nodes.pop()\n        cluster = {node}\n        node_queue = [node]\n        while node_queue:\n            node = node_queue.pop(0)\n            neighbors = set(\n                [\n                    neighbor\n                    for neighbor in node.links\n                    if score_dict[tuple(sorted([node.ind, neighbor.ind]))] >= link_thr\n                ]\n            )\n            neighbors.difference_update(cluster)\n            nodes.difference_update(neighbors)\n            cluster.update(neighbors)\n            node_queue.extend(neighbors)\n        clusters.append(list(cluster))\n    return clusters\n\n\ndef clusters2labels(clusters, num_nodes):\n    assert isinstance(clusters, list)\n    assert all([isinstance(cluster, list) for cluster in clusters])\n    assert all([isinstance(node, Node) for cluster in clusters for node in cluster])\n    assert isinstance(num_nodes, int)\n\n    node_labels = np.zeros(num_nodes)\n    for cluster_ind, cluster in enumerate(clusters):\n        for node in cluster:\n            node_labels[node.ind] = cluster_ind\n    return node_labels\n\n\ndef remove_single(text_comps, comp_pred_labels):\n    assert text_comps.ndim == 2\n    assert text_comps.shape[0] == comp_pred_labels.shape[0]\n\n    single_flags = np.zeros_like(comp_pred_labels)\n    pred_labels = np.unique(comp_pred_labels)\n    for label in pred_labels:\n        current_label_flag = comp_pred_labels == label\n        if np.sum(current_label_flag) == 1:\n            single_flags[np.where(current_label_flag)[0][0]] = 1\n    keep_ind = [i for i in range(len(comp_pred_labels)) if not single_flags[i]]\n    filtered_text_comps = text_comps[keep_ind, :]\n    filtered_labels = comp_pred_labels[keep_ind]\n\n    return filtered_text_comps, filtered_labels\n\n\ndef norm2(point1, point2):\n    return ((point1[0] - point2[0]) ** 2 + (point1[1] - point2[1]) ** 2) ** 0.5\n\n\ndef min_connect_path(points):\n    assert isinstance(points, list)\n    assert all([isinstance(point, list) for point in points])\n    assert all([isinstance(coord, int) for point in points for coord in point])\n\n    points_queue = points.copy()\n    shortest_path = []\n    current_edge = [[], []]\n\n    edge_dict0 = {}\n    edge_dict1 = {}\n    current_edge[0] = points_queue[0]\n    current_edge[1] = points_queue[0]\n    points_queue.remove(points_queue[0])\n    while points_queue:\n        for point in points_queue:\n            length0 = norm2(point, current_edge[0])\n            edge_dict0[length0] = [point, current_edge[0]]\n            length1 = norm2(current_edge[1], point)\n            edge_dict1[length1] = [current_edge[1], point]\n        key0 = min(edge_dict0.keys())\n        key1 = min(edge_dict1.keys())\n\n        if key0 <= key1:\n            start = edge_dict0[key0][0]\n            end = edge_dict0[key0][1]\n            shortest_path.insert(0, [points.index(start), points.index(end)])\n            points_queue.remove(start)\n            current_edge[0] = start\n        else:\n            start = edge_dict1[key1][0]\n            end = edge_dict1[key1][1]\n            shortest_path.append([points.index(start), points.index(end)])\n            points_queue.remove(end)\n            current_edge[1] = end\n\n        edge_dict0 = {}\n        edge_dict1 = {}\n\n    shortest_path = functools.reduce(operator.concat, shortest_path)\n    shortest_path = sorted(set(shortest_path), key=shortest_path.index)\n\n    return shortest_path\n\n\ndef in_contour(cont, point):\n    x, y = point\n    is_inner = cv2.pointPolygonTest(cont, (int(x), int(y)), False) > 0.5\n    return is_inner\n\n\ndef fix_corner(top_line, bot_line, start_box, end_box):\n    assert isinstance(top_line, list)\n    assert all(isinstance(point, list) for point in top_line)\n    assert isinstance(bot_line, list)\n    assert all(isinstance(point, list) for point in bot_line)\n    assert start_box.shape == end_box.shape == (4, 2)\n\n    contour = np.array(top_line + bot_line[::-1])\n    start_left_mid = (start_box[0] + start_box[3]) / 2\n    start_right_mid = (start_box[1] + start_box[2]) / 2\n    end_left_mid = (end_box[0] + end_box[3]) / 2\n    end_right_mid = (end_box[1] + end_box[2]) / 2\n    if not in_contour(contour, start_left_mid):\n        top_line.insert(0, start_box[0].tolist())\n        bot_line.insert(0, start_box[3].tolist())\n    elif not in_contour(contour, start_right_mid):\n        top_line.insert(0, start_box[1].tolist())\n        bot_line.insert(0, start_box[2].tolist())\n    if not in_contour(contour, end_left_mid):\n        top_line.append(end_box[0].tolist())\n        bot_line.append(end_box[3].tolist())\n    elif not in_contour(contour, end_right_mid):\n        top_line.append(end_box[1].tolist())\n        bot_line.append(end_box[2].tolist())\n    return top_line, bot_line\n\n\ndef comps2boundaries(text_comps, comp_pred_labels):\n    assert text_comps.ndim == 2\n    assert len(text_comps) == len(comp_pred_labels)\n    boundaries = []\n    if len(text_comps) < 1:\n        return boundaries\n    for cluster_ind in range(0, int(np.max(comp_pred_labels)) + 1):\n        cluster_comp_inds = np.where(comp_pred_labels == cluster_ind)\n        text_comp_boxes = (\n            text_comps[cluster_comp_inds, :8].reshape((-1, 4, 2)).astype(np.int32)\n        )\n        score = np.mean(text_comps[cluster_comp_inds, -1])\n\n        if text_comp_boxes.shape[0] < 1:\n            continue\n\n        elif text_comp_boxes.shape[0] > 1:\n            centers = np.mean(text_comp_boxes, axis=1).astype(np.int32).tolist()\n            shortest_path = min_connect_path(centers)\n            text_comp_boxes = text_comp_boxes[shortest_path]\n            top_line = (\n                np.mean(text_comp_boxes[:, 0:2, :], axis=1).astype(np.int32).tolist()\n            )\n            bot_line = (\n                np.mean(text_comp_boxes[:, 2:4, :], axis=1).astype(np.int32).tolist()\n            )\n            top_line, bot_line = fix_corner(\n                top_line, bot_line, text_comp_boxes[0], text_comp_boxes[-1]\n            )\n            boundary_points = top_line + bot_line[::-1]\n\n        else:\n            top_line = text_comp_boxes[0, 0:2, :].astype(np.int32).tolist()\n            bot_line = text_comp_boxes[0, 2:4:-1, :].astype(np.int32).tolist()\n            boundary_points = top_line + bot_line\n\n        boundary = [p for coord in boundary_points for p in coord] + [score]\n        boundaries.append(boundary)\n\n    return boundaries\n\n\nclass DRRGPostprocess(object):\n    \"\"\"Merge text components and construct boundaries of text instances.\n\n    Args:\n        link_thr (float): The edge score threshold.\n    \"\"\"\n\n    def __init__(self, link_thr, **kwargs):\n        assert isinstance(link_thr, float)\n        self.link_thr = link_thr\n\n    def __call__(self, preds, shape_list):\n        \"\"\"\n        Args:\n            edges (ndarray): The edge array of shape N * 2, each row is a node\n                index pair that makes up an edge in graph.\n            scores (ndarray): The edge score array of shape (N,).\n            text_comps (ndarray): The text components.\n\n        Returns:\n            List[list[float]]: The predicted boundaries of text instances.\n        \"\"\"\n        edges, scores, text_comps = preds\n        if edges is not None:\n            if isinstance(edges, paddle.Tensor):\n                edges = edges.numpy()\n            if isinstance(scores, paddle.Tensor):\n                scores = scores.numpy()\n            if isinstance(text_comps, paddle.Tensor):\n                text_comps = text_comps.numpy()\n            assert len(edges) == len(scores)\n            assert text_comps.ndim == 2\n            assert text_comps.shape[1] == 9\n\n            vertices, score_dict = graph_propagation(edges, scores, text_comps)\n            clusters = connected_components(vertices, score_dict, self.link_thr)\n            pred_labels = clusters2labels(clusters, text_comps.shape[0])\n            text_comps, pred_labels = remove_single(text_comps, pred_labels)\n            boundaries = comps2boundaries(text_comps, pred_labels)\n        else:\n            boundaries = []\n\n        boundaries, scores = self.resize_boundary(\n            boundaries, (1 / shape_list[0, 2:]).tolist()[::-1]\n        )\n        boxes_batch = [dict(points=boundaries, scores=scores)]\n        return boxes_batch\n\n    def resize_boundary(self, boundaries, scale_factor):\n        \"\"\"Rescale boundaries via scale_factor.\n\n        Args:\n            boundaries (list[list[float]]): The boundary list. Each boundary\n            with size 2k+1 with k>=4.\n            scale_factor(ndarray): The scale factor of size (4,).\n\n        Returns:\n            boundaries (list[list[float]]): The scaled boundaries.\n        \"\"\"\n        boxes = []\n        scores = []\n        for b in boundaries:\n            sz = len(b)\n            scores.append(b[-1])\n            b = (\n                (\n                    np.array(b[: sz - 1])\n                    * (np.tile(scale_factor[:2], int((sz - 1) / 2)).reshape(1, sz - 1))\n                )\n                .flatten()\n                .tolist()\n            )\n            boxes.append(np.array(b).reshape([-1, 2]))\n        return boxes, scores\n", "ppocr/postprocess/db_postprocess.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refered from:\nhttps://github.com/WenmuZhou/DBNet.pytorch/blob/master/post_processing/seg_detector_representer.py\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport cv2\nimport paddle\nfrom shapely.geometry import Polygon\nimport pyclipper\n\n\nclass DBPostProcess(object):\n    \"\"\"\n    The post process for Differentiable Binarization (DB).\n    \"\"\"\n\n    def __init__(\n        self,\n        thresh=0.3,\n        box_thresh=0.7,\n        max_candidates=1000,\n        unclip_ratio=2.0,\n        use_dilation=False,\n        score_mode=\"fast\",\n        box_type=\"quad\",\n        **kwargs,\n    ):\n        self.thresh = thresh\n        self.box_thresh = box_thresh\n        self.max_candidates = max_candidates\n        self.unclip_ratio = unclip_ratio\n        self.min_size = 3\n        self.score_mode = score_mode\n        self.box_type = box_type\n        assert score_mode in [\n            \"slow\",\n            \"fast\",\n        ], \"Score mode must be in [slow, fast] but got: {}\".format(score_mode)\n\n        self.dilation_kernel = None if not use_dilation else np.array([[1, 1], [1, 1]])\n\n    def polygons_from_bitmap(self, pred, _bitmap, dest_width, dest_height):\n        \"\"\"\n        _bitmap: single map with shape (1, H, W),\n            whose values are binarized as {0, 1}\n        \"\"\"\n\n        bitmap = _bitmap\n        height, width = bitmap.shape\n\n        boxes = []\n        scores = []\n\n        contours, _ = cv2.findContours(\n            (bitmap * 255).astype(np.uint8), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE\n        )\n\n        for contour in contours[: self.max_candidates]:\n            epsilon = 0.002 * cv2.arcLength(contour, True)\n            approx = cv2.approxPolyDP(contour, epsilon, True)\n            points = approx.reshape((-1, 2))\n            if points.shape[0] < 4:\n                continue\n\n            score = self.box_score_fast(pred, points.reshape(-1, 2))\n            if self.box_thresh > score:\n                continue\n\n            if points.shape[0] > 2:\n                box = self.unclip(points, self.unclip_ratio)\n                if len(box) > 1:\n                    continue\n            else:\n                continue\n            box = np.array(box).reshape(-1, 2)\n            if len(box) == 0:\n                continue\n\n            _, sside = self.get_mini_boxes(box.reshape((-1, 1, 2)))\n            if sside < self.min_size + 2:\n                continue\n\n            box = np.array(box)\n            box[:, 0] = np.clip(np.round(box[:, 0] / width * dest_width), 0, dest_width)\n            box[:, 1] = np.clip(\n                np.round(box[:, 1] / height * dest_height), 0, dest_height\n            )\n            boxes.append(box.tolist())\n            scores.append(score)\n        return boxes, scores\n\n    def boxes_from_bitmap(self, pred, _bitmap, dest_width, dest_height):\n        \"\"\"\n        _bitmap: single map with shape (1, H, W),\n                whose values are binarized as {0, 1}\n        \"\"\"\n\n        bitmap = _bitmap\n        height, width = bitmap.shape\n\n        outs = cv2.findContours(\n            (bitmap * 255).astype(np.uint8), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE\n        )\n        if len(outs) == 3:\n            img, contours, _ = outs[0], outs[1], outs[2]\n        elif len(outs) == 2:\n            contours, _ = outs[0], outs[1]\n\n        num_contours = min(len(contours), self.max_candidates)\n\n        boxes = []\n        scores = []\n        for index in range(num_contours):\n            contour = contours[index]\n            points, sside = self.get_mini_boxes(contour)\n            if sside < self.min_size:\n                continue\n            points = np.array(points)\n            if self.score_mode == \"fast\":\n                score = self.box_score_fast(pred, points.reshape(-1, 2))\n            else:\n                score = self.box_score_slow(pred, contour)\n            if self.box_thresh > score:\n                continue\n\n            box = self.unclip(points, self.unclip_ratio)\n            if len(box) > 1:\n                continue\n            box = np.array(box).reshape(-1, 1, 2)\n            box, sside = self.get_mini_boxes(box)\n            if sside < self.min_size + 2:\n                continue\n            box = np.array(box)\n\n            box[:, 0] = np.clip(np.round(box[:, 0] / width * dest_width), 0, dest_width)\n            box[:, 1] = np.clip(\n                np.round(box[:, 1] / height * dest_height), 0, dest_height\n            )\n            boxes.append(box.astype(\"int32\"))\n            scores.append(score)\n        return np.array(boxes, dtype=\"int32\"), scores\n\n    def unclip(self, box, unclip_ratio):\n        poly = Polygon(box)\n        distance = poly.area * unclip_ratio / poly.length\n        offset = pyclipper.PyclipperOffset()\n        offset.AddPath(box, pyclipper.JT_ROUND, pyclipper.ET_CLOSEDPOLYGON)\n        expanded = offset.Execute(distance)\n        return expanded\n\n    def get_mini_boxes(self, contour):\n        bounding_box = cv2.minAreaRect(contour)\n        points = sorted(list(cv2.boxPoints(bounding_box)), key=lambda x: x[0])\n\n        index_1, index_2, index_3, index_4 = 0, 1, 2, 3\n        if points[1][1] > points[0][1]:\n            index_1 = 0\n            index_4 = 1\n        else:\n            index_1 = 1\n            index_4 = 0\n        if points[3][1] > points[2][1]:\n            index_2 = 2\n            index_3 = 3\n        else:\n            index_2 = 3\n            index_3 = 2\n\n        box = [points[index_1], points[index_2], points[index_3], points[index_4]]\n        return box, min(bounding_box[1])\n\n    def box_score_fast(self, bitmap, _box):\n        \"\"\"\n        box_score_fast: use bbox mean score as the mean score\n        \"\"\"\n        h, w = bitmap.shape[:2]\n        box = _box.copy()\n        xmin = np.clip(np.floor(box[:, 0].min()).astype(\"int32\"), 0, w - 1)\n        xmax = np.clip(np.ceil(box[:, 0].max()).astype(\"int32\"), 0, w - 1)\n        ymin = np.clip(np.floor(box[:, 1].min()).astype(\"int32\"), 0, h - 1)\n        ymax = np.clip(np.ceil(box[:, 1].max()).astype(\"int32\"), 0, h - 1)\n\n        mask = np.zeros((ymax - ymin + 1, xmax - xmin + 1), dtype=np.uint8)\n        box[:, 0] = box[:, 0] - xmin\n        box[:, 1] = box[:, 1] - ymin\n        cv2.fillPoly(mask, box.reshape(1, -1, 2).astype(\"int32\"), 1)\n        return cv2.mean(bitmap[ymin : ymax + 1, xmin : xmax + 1], mask)[0]\n\n    def box_score_slow(self, bitmap, contour):\n        \"\"\"\n        box_score_slow: use polyon mean score as the mean score\n        \"\"\"\n        h, w = bitmap.shape[:2]\n        contour = contour.copy()\n        contour = np.reshape(contour, (-1, 2))\n\n        xmin = np.clip(np.min(contour[:, 0]), 0, w - 1)\n        xmax = np.clip(np.max(contour[:, 0]), 0, w - 1)\n        ymin = np.clip(np.min(contour[:, 1]), 0, h - 1)\n        ymax = np.clip(np.max(contour[:, 1]), 0, h - 1)\n\n        mask = np.zeros((ymax - ymin + 1, xmax - xmin + 1), dtype=np.uint8)\n\n        contour[:, 0] = contour[:, 0] - xmin\n        contour[:, 1] = contour[:, 1] - ymin\n\n        cv2.fillPoly(mask, contour.reshape(1, -1, 2).astype(\"int32\"), 1)\n        return cv2.mean(bitmap[ymin : ymax + 1, xmin : xmax + 1], mask)[0]\n\n    def __call__(self, outs_dict, shape_list):\n        pred = outs_dict[\"maps\"]\n        if isinstance(pred, paddle.Tensor):\n            pred = pred.numpy()\n        pred = pred[:, 0, :, :]\n        segmentation = pred > self.thresh\n\n        boxes_batch = []\n        for batch_index in range(pred.shape[0]):\n            src_h, src_w, ratio_h, ratio_w = shape_list[batch_index]\n            if self.dilation_kernel is not None:\n                mask = cv2.dilate(\n                    np.array(segmentation[batch_index]).astype(np.uint8),\n                    self.dilation_kernel,\n                )\n            else:\n                mask = segmentation[batch_index]\n            if self.box_type == \"poly\":\n                boxes, scores = self.polygons_from_bitmap(\n                    pred[batch_index], mask, src_w, src_h\n                )\n            elif self.box_type == \"quad\":\n                boxes, scores = self.boxes_from_bitmap(\n                    pred[batch_index], mask, src_w, src_h\n                )\n            else:\n                raise ValueError(\"box_type can only be one of ['quad', 'poly']\")\n\n            boxes_batch.append({\"points\": boxes})\n        return boxes_batch\n\n\nclass DistillationDBPostProcess(object):\n    def __init__(\n        self,\n        model_name=[\"student\"],\n        key=None,\n        thresh=0.3,\n        box_thresh=0.6,\n        max_candidates=1000,\n        unclip_ratio=1.5,\n        use_dilation=False,\n        score_mode=\"fast\",\n        box_type=\"quad\",\n        **kwargs,\n    ):\n        self.model_name = model_name\n        self.key = key\n        self.post_process = DBPostProcess(\n            thresh=thresh,\n            box_thresh=box_thresh,\n            max_candidates=max_candidates,\n            unclip_ratio=unclip_ratio,\n            use_dilation=use_dilation,\n            score_mode=score_mode,\n            box_type=box_type,\n        )\n\n    def __call__(self, predicts, shape_list):\n        results = {}\n        for k in self.model_name:\n            results[k] = self.post_process(predicts[k], shape_list=shape_list)\n        return results\n", "ppocr/postprocess/rec_postprocess.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport paddle\nfrom paddle.nn import functional as F\nimport re\n\n\nclass BaseRecLabelDecode(object):\n    \"\"\"Convert between text-label and text-index\"\"\"\n\n    def __init__(self, character_dict_path=None, use_space_char=False):\n        self.beg_str = \"sos\"\n        self.end_str = \"eos\"\n        self.reverse = False\n        self.character_str = []\n\n        if character_dict_path is None:\n            self.character_str = \"0123456789abcdefghijklmnopqrstuvwxyz\"\n            dict_character = list(self.character_str)\n        else:\n            with open(character_dict_path, \"rb\") as fin:\n                lines = fin.readlines()\n                for line in lines:\n                    line = line.decode(\"utf-8\").strip(\"\\n\").strip(\"\\r\\n\")\n                    self.character_str.append(line)\n            if use_space_char:\n                self.character_str.append(\" \")\n            dict_character = list(self.character_str)\n            if \"arabic\" in character_dict_path:\n                self.reverse = True\n\n        dict_character = self.add_special_char(dict_character)\n        self.dict = {}\n        for i, char in enumerate(dict_character):\n            self.dict[char] = i\n        self.character = dict_character\n\n    def pred_reverse(self, pred):\n        pred_re = []\n        c_current = \"\"\n        for c in pred:\n            if not bool(re.search(\"[a-zA-Z0-9 :*./%+-]\", c)):\n                if c_current != \"\":\n                    pred_re.append(c_current)\n                pred_re.append(c)\n                c_current = \"\"\n            else:\n                c_current += c\n        if c_current != \"\":\n            pred_re.append(c_current)\n\n        return \"\".join(pred_re[::-1])\n\n    def add_special_char(self, dict_character):\n        return dict_character\n\n    def get_word_info(self, text, selection):\n        \"\"\"\n        Group the decoded characters and record the corresponding decoded positions.\n\n        Args:\n            text: the decoded text\n            selection: the bool array that identifies which columns of features are decoded as non-separated characters\n        Returns:\n            word_list: list of the grouped words\n            word_col_list: list of decoding positions corresponding to each character in the grouped word\n            state_list: list of marker to identify the type of grouping words, including two types of grouping words:\n                        - 'cn': continous chinese characters (e.g., \u4f60\u597d\u554a)\n                        - 'en&num': continous english characters (e.g., hello), number (e.g., 123, 1.123), or mixed of them connected by '-' (e.g., VGG-16)\n                        The remaining characters in text are treated as separators between groups (e.g., space, '(', ')', etc.).\n        \"\"\"\n        state = None\n        word_content = []\n        word_col_content = []\n        word_list = []\n        word_col_list = []\n        state_list = []\n        valid_col = np.where(selection == True)[0]\n\n        for c_i, char in enumerate(text):\n            if \"\\u4e00\" <= char <= \"\\u9fff\":\n                c_state = \"cn\"\n            elif bool(re.search(\"[a-zA-Z0-9]\", char)):\n                c_state = \"en&num\"\n            else:\n                c_state = \"splitter\"\n\n            if (\n                char == \".\"\n                and state == \"en&num\"\n                and c_i + 1 < len(text)\n                and bool(re.search(\"[0-9]\", text[c_i + 1]))\n            ):  # grouping floting number\n                c_state = \"en&num\"\n            if (\n                char == \"-\" and state == \"en&num\"\n            ):  # grouping word with '-', such as 'state-of-the-art'\n                c_state = \"en&num\"\n\n            if state == None:\n                state = c_state\n\n            if state != c_state:\n                if len(word_content) != 0:\n                    word_list.append(word_content)\n                    word_col_list.append(word_col_content)\n                    state_list.append(state)\n                    word_content = []\n                    word_col_content = []\n                state = c_state\n\n            if state != \"splitter\":\n                word_content.append(char)\n                word_col_content.append(valid_col[c_i])\n\n        if len(word_content) != 0:\n            word_list.append(word_content)\n            word_col_list.append(word_col_content)\n            state_list.append(state)\n\n        return word_list, word_col_list, state_list\n\n    def decode(\n        self,\n        text_index,\n        text_prob=None,\n        is_remove_duplicate=False,\n        return_word_box=False,\n    ):\n        \"\"\"convert text-index into text-label.\"\"\"\n        result_list = []\n        ignored_tokens = self.get_ignored_tokens()\n        batch_size = len(text_index)\n        for batch_idx in range(batch_size):\n            selection = np.ones(len(text_index[batch_idx]), dtype=bool)\n            if is_remove_duplicate:\n                selection[1:] = text_index[batch_idx][1:] != text_index[batch_idx][:-1]\n            for ignored_token in ignored_tokens:\n                selection &= text_index[batch_idx] != ignored_token\n\n            char_list = [\n                self.character[text_id] for text_id in text_index[batch_idx][selection]\n            ]\n            if text_prob is not None:\n                conf_list = text_prob[batch_idx][selection]\n            else:\n                conf_list = [1] * len(selection)\n            if len(conf_list) == 0:\n                conf_list = [0]\n\n            text = \"\".join(char_list)\n\n            if self.reverse:  # for arabic rec\n                text = self.pred_reverse(text)\n\n            if return_word_box:\n                word_list, word_col_list, state_list = self.get_word_info(\n                    text, selection\n                )\n                result_list.append(\n                    (\n                        text,\n                        np.mean(conf_list).tolist(),\n                        [\n                            len(text_index[batch_idx]),\n                            word_list,\n                            word_col_list,\n                            state_list,\n                        ],\n                    )\n                )\n            else:\n                result_list.append((text, np.mean(conf_list).tolist()))\n        return result_list\n\n    def get_ignored_tokens(self):\n        return [0]  # for ctc blank\n\n\nclass CTCLabelDecode(BaseRecLabelDecode):\n    \"\"\"Convert between text-label and text-index\"\"\"\n\n    def __init__(self, character_dict_path=None, use_space_char=False, **kwargs):\n        super(CTCLabelDecode, self).__init__(character_dict_path, use_space_char)\n\n    def __call__(self, preds, label=None, return_word_box=False, *args, **kwargs):\n        if isinstance(preds, tuple) or isinstance(preds, list):\n            preds = preds[-1]\n        if isinstance(preds, paddle.Tensor):\n            preds = preds.numpy()\n        preds_idx = preds.argmax(axis=2)\n        preds_prob = preds.max(axis=2)\n        text = self.decode(\n            preds_idx,\n            preds_prob,\n            is_remove_duplicate=True,\n            return_word_box=return_word_box,\n        )\n        if return_word_box:\n            for rec_idx, rec in enumerate(text):\n                wh_ratio = kwargs[\"wh_ratio_list\"][rec_idx]\n                max_wh_ratio = kwargs[\"max_wh_ratio\"]\n                rec[2][0] = rec[2][0] * (wh_ratio / max_wh_ratio)\n        if label is None:\n            return text\n        label = self.decode(label)\n        return text, label\n\n    def add_special_char(self, dict_character):\n        dict_character = [\"blank\"] + dict_character\n        return dict_character\n\n\nclass DistillationCTCLabelDecode(CTCLabelDecode):\n    \"\"\"\n    Convert\n    Convert between text-label and text-index\n    \"\"\"\n\n    def __init__(\n        self,\n        character_dict_path=None,\n        use_space_char=False,\n        model_name=[\"student\"],\n        key=None,\n        multi_head=False,\n        **kwargs,\n    ):\n        super(DistillationCTCLabelDecode, self).__init__(\n            character_dict_path, use_space_char\n        )\n        if not isinstance(model_name, list):\n            model_name = [model_name]\n        self.model_name = model_name\n\n        self.key = key\n        self.multi_head = multi_head\n\n    def __call__(self, preds, label=None, *args, **kwargs):\n        output = dict()\n        for name in self.model_name:\n            pred = preds[name]\n            if self.key is not None:\n                pred = pred[self.key]\n            if self.multi_head and isinstance(pred, dict):\n                pred = pred[\"ctc\"]\n            output[name] = super().__call__(pred, label=label, *args, **kwargs)\n        return output\n\n\nclass AttnLabelDecode(BaseRecLabelDecode):\n    \"\"\"Convert between text-label and text-index\"\"\"\n\n    def __init__(self, character_dict_path=None, use_space_char=False, **kwargs):\n        super(AttnLabelDecode, self).__init__(character_dict_path, use_space_char)\n\n    def add_special_char(self, dict_character):\n        self.beg_str = \"sos\"\n        self.end_str = \"eos\"\n        dict_character = dict_character\n        dict_character = [self.beg_str] + dict_character + [self.end_str]\n        return dict_character\n\n    def decode(self, text_index, text_prob=None, is_remove_duplicate=False):\n        \"\"\"convert text-index into text-label.\"\"\"\n        result_list = []\n        ignored_tokens = self.get_ignored_tokens()\n        [beg_idx, end_idx] = self.get_ignored_tokens()\n        batch_size = len(text_index)\n        for batch_idx in range(batch_size):\n            char_list = []\n            conf_list = []\n            for idx in range(len(text_index[batch_idx])):\n                if text_index[batch_idx][idx] in ignored_tokens:\n                    continue\n                if int(text_index[batch_idx][idx]) == int(end_idx):\n                    break\n                if is_remove_duplicate:\n                    # only for predict\n                    if (\n                        idx > 0\n                        and text_index[batch_idx][idx - 1] == text_index[batch_idx][idx]\n                    ):\n                        continue\n                char_list.append(self.character[int(text_index[batch_idx][idx])])\n                if text_prob is not None:\n                    conf_list.append(text_prob[batch_idx][idx])\n                else:\n                    conf_list.append(1)\n            text = \"\".join(char_list)\n            result_list.append((text, np.mean(conf_list).tolist()))\n        return result_list\n\n    def __call__(self, preds, label=None, *args, **kwargs):\n        \"\"\"\n        text = self.decode(text)\n        if label is None:\n            return text\n        else:\n            label = self.decode(label, is_remove_duplicate=False)\n            return text, label\n        \"\"\"\n        if isinstance(preds, paddle.Tensor):\n            preds = preds.numpy()\n\n        preds_idx = preds.argmax(axis=2)\n        preds_prob = preds.max(axis=2)\n        text = self.decode(preds_idx, preds_prob, is_remove_duplicate=False)\n        if label is None:\n            return text\n        label = self.decode(label, is_remove_duplicate=False)\n        return text, label\n\n    def get_ignored_tokens(self):\n        beg_idx = self.get_beg_end_flag_idx(\"beg\")\n        end_idx = self.get_beg_end_flag_idx(\"end\")\n        return [beg_idx, end_idx]\n\n    def get_beg_end_flag_idx(self, beg_or_end):\n        if beg_or_end == \"beg\":\n            idx = np.array(self.dict[self.beg_str])\n        elif beg_or_end == \"end\":\n            idx = np.array(self.dict[self.end_str])\n        else:\n            assert False, \"unsupport type %s in get_beg_end_flag_idx\" % beg_or_end\n        return idx\n\n\nclass RFLLabelDecode(BaseRecLabelDecode):\n    \"\"\"Convert between text-label and text-index\"\"\"\n\n    def __init__(self, character_dict_path=None, use_space_char=False, **kwargs):\n        super(RFLLabelDecode, self).__init__(character_dict_path, use_space_char)\n\n    def add_special_char(self, dict_character):\n        self.beg_str = \"sos\"\n        self.end_str = \"eos\"\n        dict_character = dict_character\n        dict_character = [self.beg_str] + dict_character + [self.end_str]\n        return dict_character\n\n    def decode(self, text_index, text_prob=None, is_remove_duplicate=False):\n        \"\"\"convert text-index into text-label.\"\"\"\n        result_list = []\n        ignored_tokens = self.get_ignored_tokens()\n        [beg_idx, end_idx] = self.get_ignored_tokens()\n        batch_size = len(text_index)\n        for batch_idx in range(batch_size):\n            char_list = []\n            conf_list = []\n            for idx in range(len(text_index[batch_idx])):\n                if text_index[batch_idx][idx] in ignored_tokens:\n                    continue\n                if int(text_index[batch_idx][idx]) == int(end_idx):\n                    break\n                if is_remove_duplicate:\n                    # only for predict\n                    if (\n                        idx > 0\n                        and text_index[batch_idx][idx - 1] == text_index[batch_idx][idx]\n                    ):\n                        continue\n                char_list.append(self.character[int(text_index[batch_idx][idx])])\n                if text_prob is not None:\n                    conf_list.append(text_prob[batch_idx][idx])\n                else:\n                    conf_list.append(1)\n            text = \"\".join(char_list)\n            result_list.append((text, np.mean(conf_list).tolist()))\n        return result_list\n\n    def __call__(self, preds, label=None, *args, **kwargs):\n        # if seq_outputs is not None:\n        if isinstance(preds, tuple) or isinstance(preds, list):\n            cnt_outputs, seq_outputs = preds\n            if isinstance(seq_outputs, paddle.Tensor):\n                seq_outputs = seq_outputs.numpy()\n            preds_idx = seq_outputs.argmax(axis=2)\n            preds_prob = seq_outputs.max(axis=2)\n            text = self.decode(preds_idx, preds_prob, is_remove_duplicate=False)\n\n            if label is None:\n                return text\n            label = self.decode(label, is_remove_duplicate=False)\n            return text, label\n\n        else:\n            cnt_outputs = preds\n            if isinstance(cnt_outputs, paddle.Tensor):\n                cnt_outputs = cnt_outputs.numpy()\n            cnt_length = []\n            for lens in cnt_outputs:\n                length = round(np.sum(lens))\n                cnt_length.append(length)\n            if label is None:\n                return cnt_length\n            label = self.decode(label, is_remove_duplicate=False)\n            length = [len(res[0]) for res in label]\n            return cnt_length, length\n\n    def get_ignored_tokens(self):\n        beg_idx = self.get_beg_end_flag_idx(\"beg\")\n        end_idx = self.get_beg_end_flag_idx(\"end\")\n        return [beg_idx, end_idx]\n\n    def get_beg_end_flag_idx(self, beg_or_end):\n        if beg_or_end == \"beg\":\n            idx = np.array(self.dict[self.beg_str])\n        elif beg_or_end == \"end\":\n            idx = np.array(self.dict[self.end_str])\n        else:\n            assert False, \"unsupport type %s in get_beg_end_flag_idx\" % beg_or_end\n        return idx\n\n\nclass SEEDLabelDecode(BaseRecLabelDecode):\n    \"\"\"Convert between text-label and text-index\"\"\"\n\n    def __init__(self, character_dict_path=None, use_space_char=False, **kwargs):\n        super(SEEDLabelDecode, self).__init__(character_dict_path, use_space_char)\n\n    def add_special_char(self, dict_character):\n        self.padding_str = \"padding\"\n        self.end_str = \"eos\"\n        self.unknown = \"unknown\"\n        dict_character = dict_character + [self.end_str, self.padding_str, self.unknown]\n        return dict_character\n\n    def get_ignored_tokens(self):\n        end_idx = self.get_beg_end_flag_idx(\"eos\")\n        return [end_idx]\n\n    def get_beg_end_flag_idx(self, beg_or_end):\n        if beg_or_end == \"sos\":\n            idx = np.array(self.dict[self.beg_str])\n        elif beg_or_end == \"eos\":\n            idx = np.array(self.dict[self.end_str])\n        else:\n            assert False, \"unsupport type %s in get_beg_end_flag_idx\" % beg_or_end\n        return idx\n\n    def decode(self, text_index, text_prob=None, is_remove_duplicate=False):\n        \"\"\"convert text-index into text-label.\"\"\"\n        result_list = []\n        [end_idx] = self.get_ignored_tokens()\n        batch_size = len(text_index)\n        for batch_idx in range(batch_size):\n            char_list = []\n            conf_list = []\n            for idx in range(len(text_index[batch_idx])):\n                if int(text_index[batch_idx][idx]) == int(end_idx):\n                    break\n                if is_remove_duplicate:\n                    # only for predict\n                    if (\n                        idx > 0\n                        and text_index[batch_idx][idx - 1] == text_index[batch_idx][idx]\n                    ):\n                        continue\n                char_list.append(self.character[int(text_index[batch_idx][idx])])\n                if text_prob is not None:\n                    conf_list.append(text_prob[batch_idx][idx])\n                else:\n                    conf_list.append(1)\n            text = \"\".join(char_list)\n            result_list.append((text, np.mean(conf_list).tolist()))\n        return result_list\n\n    def __call__(self, preds, label=None, *args, **kwargs):\n        \"\"\"\n        text = self.decode(text)\n        if label is None:\n            return text\n        else:\n            label = self.decode(label, is_remove_duplicate=False)\n            return text, label\n        \"\"\"\n        preds_idx = preds[\"rec_pred\"]\n        if isinstance(preds_idx, paddle.Tensor):\n            preds_idx = preds_idx.numpy()\n        if \"rec_pred_scores\" in preds:\n            preds_idx = preds[\"rec_pred\"]\n            preds_prob = preds[\"rec_pred_scores\"]\n        else:\n            preds_idx = preds[\"rec_pred\"].argmax(axis=2)\n            preds_prob = preds[\"rec_pred\"].max(axis=2)\n        text = self.decode(preds_idx, preds_prob, is_remove_duplicate=False)\n        if label is None:\n            return text\n        label = self.decode(label, is_remove_duplicate=False)\n        return text, label\n\n\nclass SRNLabelDecode(BaseRecLabelDecode):\n    \"\"\"Convert between text-label and text-index\"\"\"\n\n    def __init__(self, character_dict_path=None, use_space_char=False, **kwargs):\n        super(SRNLabelDecode, self).__init__(character_dict_path, use_space_char)\n        self.max_text_length = kwargs.get(\"max_text_length\", 25)\n\n    def __call__(self, preds, label=None, *args, **kwargs):\n        pred = preds[\"predict\"]\n        char_num = len(self.character_str) + 2\n        if isinstance(pred, paddle.Tensor):\n            pred = pred.numpy()\n        pred = np.reshape(pred, [-1, char_num])\n\n        preds_idx = np.argmax(pred, axis=1)\n        preds_prob = np.max(pred, axis=1)\n\n        preds_idx = np.reshape(preds_idx, [-1, self.max_text_length])\n\n        preds_prob = np.reshape(preds_prob, [-1, self.max_text_length])\n\n        text = self.decode(preds_idx, preds_prob)\n\n        if label is None:\n            text = self.decode(preds_idx, preds_prob, is_remove_duplicate=False)\n            return text\n        label = self.decode(label)\n        return text, label\n\n    def decode(self, text_index, text_prob=None, is_remove_duplicate=False):\n        \"\"\"convert text-index into text-label.\"\"\"\n        result_list = []\n        ignored_tokens = self.get_ignored_tokens()\n        batch_size = len(text_index)\n\n        for batch_idx in range(batch_size):\n            char_list = []\n            conf_list = []\n            for idx in range(len(text_index[batch_idx])):\n                if text_index[batch_idx][idx] in ignored_tokens:\n                    continue\n                if is_remove_duplicate:\n                    # only for predict\n                    if (\n                        idx > 0\n                        and text_index[batch_idx][idx - 1] == text_index[batch_idx][idx]\n                    ):\n                        continue\n                char_list.append(self.character[int(text_index[batch_idx][idx])])\n                if text_prob is not None:\n                    conf_list.append(text_prob[batch_idx][idx])\n                else:\n                    conf_list.append(1)\n\n            text = \"\".join(char_list)\n            result_list.append((text, np.mean(conf_list).tolist()))\n        return result_list\n\n    def add_special_char(self, dict_character):\n        dict_character = dict_character + [self.beg_str, self.end_str]\n        return dict_character\n\n    def get_ignored_tokens(self):\n        beg_idx = self.get_beg_end_flag_idx(\"beg\")\n        end_idx = self.get_beg_end_flag_idx(\"end\")\n        return [beg_idx, end_idx]\n\n    def get_beg_end_flag_idx(self, beg_or_end):\n        if beg_or_end == \"beg\":\n            idx = np.array(self.dict[self.beg_str])\n        elif beg_or_end == \"end\":\n            idx = np.array(self.dict[self.end_str])\n        else:\n            assert False, \"unsupport type %s in get_beg_end_flag_idx\" % beg_or_end\n        return idx\n\n\nclass ParseQLabelDecode(BaseRecLabelDecode):\n    \"\"\"Convert between text-label and text-index\"\"\"\n\n    BOS = \"[B]\"\n    EOS = \"[E]\"\n    PAD = \"[P]\"\n\n    def __init__(self, character_dict_path=None, use_space_char=False, **kwargs):\n        super(ParseQLabelDecode, self).__init__(character_dict_path, use_space_char)\n        self.max_text_length = kwargs.get(\"max_text_length\", 25)\n\n    def __call__(self, preds, label=None, *args, **kwargs):\n        if isinstance(preds, dict):\n            pred = preds[\"predict\"]\n        else:\n            pred = preds\n\n        char_num = (\n            len(self.character_str) + 1\n        )  # We don't predict <bos> nor <pad>, with only addition <eos>\n        if isinstance(pred, paddle.Tensor):\n            pred = pred.numpy()\n        B, L = pred.shape[:2]\n        pred = np.reshape(pred, [-1, char_num])\n\n        preds_idx = np.argmax(pred, axis=1)\n        preds_prob = np.max(pred, axis=1)\n\n        preds_idx = np.reshape(preds_idx, [B, L])\n        preds_prob = np.reshape(preds_prob, [B, L])\n\n        if label is None:\n            text = self.decode(preds_idx, preds_prob, raw=False)\n            return text\n\n        text = self.decode(preds_idx, preds_prob, raw=False)\n        label = self.decode(label, None, False)\n\n        return text, label\n\n    def decode(self, text_index, text_prob=None, raw=False):\n        \"\"\"convert text-index into text-label.\"\"\"\n        result_list = []\n        ignored_tokens = self.get_ignored_tokens()\n        batch_size = len(text_index)\n\n        for batch_idx in range(batch_size):\n            char_list = []\n            conf_list = []\n\n            index = text_index[batch_idx, :]\n            prob = None\n            if text_prob is not None:\n                prob = text_prob[batch_idx, :]\n\n            if not raw:\n                index, prob = self._filter(index, prob)\n\n            for idx in range(len(index)):\n                if index[idx] in ignored_tokens:\n                    continue\n                char_list.append(self.character[int(index[idx])])\n                if text_prob is not None:\n                    conf_list.append(prob[idx])\n                else:\n                    conf_list.append(1)\n\n            text = \"\".join(char_list)\n            result_list.append((text, np.mean(conf_list).tolist()))\n\n        return result_list\n\n    def add_special_char(self, dict_character):\n        dict_character = [self.EOS] + dict_character + [self.BOS, self.PAD]\n        return dict_character\n\n    def _filter(self, ids, probs=None):\n        ids = ids.tolist()\n        try:\n            eos_idx = ids.index(self.dict[self.EOS])\n        except ValueError:\n            eos_idx = len(ids)  # Nothing to truncate.\n        # Truncate after EOS\n        ids = ids[:eos_idx]\n        if probs is not None:\n            probs = probs[: eos_idx + 1]  # but include prob. for EOS (if it exists)\n        return ids, probs\n\n    def get_ignored_tokens(self):\n        return [self.dict[self.BOS], self.dict[self.EOS], self.dict[self.PAD]]\n\n\nclass SARLabelDecode(BaseRecLabelDecode):\n    \"\"\"Convert between text-label and text-index\"\"\"\n\n    def __init__(self, character_dict_path=None, use_space_char=False, **kwargs):\n        super(SARLabelDecode, self).__init__(character_dict_path, use_space_char)\n\n        self.rm_symbol = kwargs.get(\"rm_symbol\", False)\n\n    def add_special_char(self, dict_character):\n        beg_end_str = \"<BOS/EOS>\"\n        unknown_str = \"<UKN>\"\n        padding_str = \"<PAD>\"\n        dict_character = dict_character + [unknown_str]\n        self.unknown_idx = len(dict_character) - 1\n        dict_character = dict_character + [beg_end_str]\n        self.start_idx = len(dict_character) - 1\n        self.end_idx = len(dict_character) - 1\n        dict_character = dict_character + [padding_str]\n        self.padding_idx = len(dict_character) - 1\n        return dict_character\n\n    def decode(self, text_index, text_prob=None, is_remove_duplicate=False):\n        \"\"\"convert text-index into text-label.\"\"\"\n        result_list = []\n        ignored_tokens = self.get_ignored_tokens()\n\n        batch_size = len(text_index)\n        for batch_idx in range(batch_size):\n            char_list = []\n            conf_list = []\n            for idx in range(len(text_index[batch_idx])):\n                if text_index[batch_idx][idx] in ignored_tokens:\n                    continue\n                if int(text_index[batch_idx][idx]) == int(self.end_idx):\n                    if text_prob is None and idx == 0:\n                        continue\n                    else:\n                        break\n                if is_remove_duplicate:\n                    # only for predict\n                    if (\n                        idx > 0\n                        and text_index[batch_idx][idx - 1] == text_index[batch_idx][idx]\n                    ):\n                        continue\n                char_list.append(self.character[int(text_index[batch_idx][idx])])\n                if text_prob is not None:\n                    conf_list.append(text_prob[batch_idx][idx])\n                else:\n                    conf_list.append(1)\n            text = \"\".join(char_list)\n            if self.rm_symbol:\n                comp = re.compile(\"[^A-Z^a-z^0-9^\\u4e00-\\u9fa5]\")\n                text = text.lower()\n                text = comp.sub(\"\", text)\n            result_list.append((text, np.mean(conf_list).tolist()))\n        return result_list\n\n    def __call__(self, preds, label=None, *args, **kwargs):\n        if isinstance(preds, paddle.Tensor):\n            preds = preds.numpy()\n        preds_idx = preds.argmax(axis=2)\n        preds_prob = preds.max(axis=2)\n\n        text = self.decode(preds_idx, preds_prob, is_remove_duplicate=False)\n\n        if label is None:\n            return text\n        label = self.decode(label, is_remove_duplicate=False)\n        return text, label\n\n    def get_ignored_tokens(self):\n        return [self.padding_idx]\n\n\nclass SATRNLabelDecode(BaseRecLabelDecode):\n    \"\"\"Convert between text-label and text-index\"\"\"\n\n    def __init__(self, character_dict_path=None, use_space_char=False, **kwargs):\n        super(SATRNLabelDecode, self).__init__(character_dict_path, use_space_char)\n\n        self.rm_symbol = kwargs.get(\"rm_symbol\", False)\n\n    def add_special_char(self, dict_character):\n        beg_end_str = \"<BOS/EOS>\"\n        unknown_str = \"<UKN>\"\n        padding_str = \"<PAD>\"\n        dict_character = dict_character + [unknown_str]\n        self.unknown_idx = len(dict_character) - 1\n        dict_character = dict_character + [beg_end_str]\n        self.start_idx = len(dict_character) - 1\n        self.end_idx = len(dict_character) - 1\n        dict_character = dict_character + [padding_str]\n        self.padding_idx = len(dict_character) - 1\n        return dict_character\n\n    def decode(self, text_index, text_prob=None, is_remove_duplicate=False):\n        \"\"\"convert text-index into text-label.\"\"\"\n        result_list = []\n        ignored_tokens = self.get_ignored_tokens()\n\n        batch_size = len(text_index)\n        for batch_idx in range(batch_size):\n            char_list = []\n            conf_list = []\n            for idx in range(len(text_index[batch_idx])):\n                if text_index[batch_idx][idx] in ignored_tokens:\n                    continue\n                if int(text_index[batch_idx][idx]) == int(self.end_idx):\n                    if text_prob is None and idx == 0:\n                        continue\n                    else:\n                        break\n                if is_remove_duplicate:\n                    # only for predict\n                    if (\n                        idx > 0\n                        and text_index[batch_idx][idx - 1] == text_index[batch_idx][idx]\n                    ):\n                        continue\n                char_list.append(self.character[int(text_index[batch_idx][idx])])\n                if text_prob is not None:\n                    conf_list.append(text_prob[batch_idx][idx])\n                else:\n                    conf_list.append(1)\n            text = \"\".join(char_list)\n            if self.rm_symbol:\n                comp = re.compile(\"[^A-Z^a-z^0-9^\\u4e00-\\u9fa5]\")\n                text = text.lower()\n                text = comp.sub(\"\", text)\n            result_list.append((text, np.mean(conf_list).tolist()))\n        return result_list\n\n    def __call__(self, preds, label=None, *args, **kwargs):\n        if isinstance(preds, paddle.Tensor):\n            preds = preds.numpy()\n        preds_idx = preds.argmax(axis=2)\n        preds_prob = preds.max(axis=2)\n\n        text = self.decode(preds_idx, preds_prob, is_remove_duplicate=False)\n\n        if label is None:\n            return text\n        label = self.decode(label, is_remove_duplicate=False)\n        return text, label\n\n    def get_ignored_tokens(self):\n        return [self.padding_idx]\n\n\nclass DistillationSARLabelDecode(SARLabelDecode):\n    \"\"\"\n    Convert\n    Convert between text-label and text-index\n    \"\"\"\n\n    def __init__(\n        self,\n        character_dict_path=None,\n        use_space_char=False,\n        model_name=[\"student\"],\n        key=None,\n        multi_head=False,\n        **kwargs,\n    ):\n        super(DistillationSARLabelDecode, self).__init__(\n            character_dict_path, use_space_char\n        )\n        if not isinstance(model_name, list):\n            model_name = [model_name]\n        self.model_name = model_name\n\n        self.key = key\n        self.multi_head = multi_head\n\n    def __call__(self, preds, label=None, *args, **kwargs):\n        output = dict()\n        for name in self.model_name:\n            pred = preds[name]\n            if self.key is not None:\n                pred = pred[self.key]\n            if self.multi_head and isinstance(pred, dict):\n                pred = pred[\"sar\"]\n            output[name] = super().__call__(pred, label=label, *args, **kwargs)\n        return output\n\n\nclass PRENLabelDecode(BaseRecLabelDecode):\n    \"\"\"Convert between text-label and text-index\"\"\"\n\n    def __init__(self, character_dict_path=None, use_space_char=False, **kwargs):\n        super(PRENLabelDecode, self).__init__(character_dict_path, use_space_char)\n\n    def add_special_char(self, dict_character):\n        padding_str = \"<PAD>\"  # 0\n        end_str = \"<EOS>\"  # 1\n        unknown_str = \"<UNK>\"  # 2\n\n        dict_character = [padding_str, end_str, unknown_str] + dict_character\n        self.padding_idx = 0\n        self.end_idx = 1\n        self.unknown_idx = 2\n\n        return dict_character\n\n    def decode(self, text_index, text_prob=None):\n        \"\"\"convert text-index into text-label.\"\"\"\n        result_list = []\n        batch_size = len(text_index)\n\n        for batch_idx in range(batch_size):\n            char_list = []\n            conf_list = []\n            for idx in range(len(text_index[batch_idx])):\n                if text_index[batch_idx][idx] == self.end_idx:\n                    break\n                if text_index[batch_idx][idx] in [self.padding_idx, self.unknown_idx]:\n                    continue\n                char_list.append(self.character[int(text_index[batch_idx][idx])])\n                if text_prob is not None:\n                    conf_list.append(text_prob[batch_idx][idx])\n                else:\n                    conf_list.append(1)\n\n            text = \"\".join(char_list)\n            if len(text) > 0:\n                result_list.append((text, np.mean(conf_list).tolist()))\n            else:\n                # here confidence of empty recog result is 1\n                result_list.append((\"\", 1))\n        return result_list\n\n    def __call__(self, preds, label=None, *args, **kwargs):\n        if isinstance(preds, paddle.Tensor):\n            preds = preds.numpy()\n        preds_idx = preds.argmax(axis=2)\n        preds_prob = preds.max(axis=2)\n        text = self.decode(preds_idx, preds_prob)\n        if label is None:\n            return text\n        label = self.decode(label)\n        return text, label\n\n\nclass NRTRLabelDecode(BaseRecLabelDecode):\n    \"\"\"Convert between text-label and text-index\"\"\"\n\n    def __init__(self, character_dict_path=None, use_space_char=True, **kwargs):\n        super(NRTRLabelDecode, self).__init__(character_dict_path, use_space_char)\n\n    def __call__(self, preds, label=None, *args, **kwargs):\n        if len(preds) == 2:\n            preds_id = preds[0]\n            preds_prob = preds[1]\n            if isinstance(preds_id, paddle.Tensor):\n                preds_id = preds_id.numpy()\n            if isinstance(preds_prob, paddle.Tensor):\n                preds_prob = preds_prob.numpy()\n            if preds_id[0][0] == 2:\n                preds_idx = preds_id[:, 1:]\n                preds_prob = preds_prob[:, 1:]\n            else:\n                preds_idx = preds_id\n            text = self.decode(preds_idx, preds_prob, is_remove_duplicate=False)\n            if label is None:\n                return text\n            label = self.decode(label[:, 1:])\n        else:\n            if isinstance(preds, paddle.Tensor):\n                preds = preds.numpy()\n            preds_idx = preds.argmax(axis=2)\n            preds_prob = preds.max(axis=2)\n            text = self.decode(preds_idx, preds_prob, is_remove_duplicate=False)\n            if label is None:\n                return text\n            label = self.decode(label[:, 1:])\n        return text, label\n\n    def add_special_char(self, dict_character):\n        dict_character = [\"blank\", \"<unk>\", \"<s>\", \"</s>\"] + dict_character\n        return dict_character\n\n    def decode(self, text_index, text_prob=None, is_remove_duplicate=False):\n        \"\"\"convert text-index into text-label.\"\"\"\n        result_list = []\n        batch_size = len(text_index)\n        for batch_idx in range(batch_size):\n            char_list = []\n            conf_list = []\n            for idx in range(len(text_index[batch_idx])):\n                try:\n                    char_idx = self.character[int(text_index[batch_idx][idx])]\n                except:\n                    continue\n                if char_idx == \"</s>\":  # end\n                    break\n                char_list.append(char_idx)\n                if text_prob is not None:\n                    conf_list.append(text_prob[batch_idx][idx])\n                else:\n                    conf_list.append(1)\n            text = \"\".join(char_list)\n            result_list.append((text, np.mean(conf_list).tolist()))\n        return result_list\n\n\nclass ViTSTRLabelDecode(NRTRLabelDecode):\n    \"\"\"Convert between text-label and text-index\"\"\"\n\n    def __init__(self, character_dict_path=None, use_space_char=False, **kwargs):\n        super(ViTSTRLabelDecode, self).__init__(character_dict_path, use_space_char)\n\n    def __call__(self, preds, label=None, *args, **kwargs):\n        if isinstance(preds, paddle.Tensor):\n            preds = preds[:, 1:].numpy()\n        else:\n            preds = preds[:, 1:]\n        preds_idx = preds.argmax(axis=2)\n        preds_prob = preds.max(axis=2)\n        text = self.decode(preds_idx, preds_prob, is_remove_duplicate=False)\n        if label is None:\n            return text\n        label = self.decode(label[:, 1:])\n        return text, label\n\n    def add_special_char(self, dict_character):\n        dict_character = [\"<s>\", \"</s>\"] + dict_character\n        return dict_character\n\n\nclass ABINetLabelDecode(NRTRLabelDecode):\n    \"\"\"Convert between text-label and text-index\"\"\"\n\n    def __init__(self, character_dict_path=None, use_space_char=False, **kwargs):\n        super(ABINetLabelDecode, self).__init__(character_dict_path, use_space_char)\n\n    def __call__(self, preds, label=None, *args, **kwargs):\n        if isinstance(preds, dict):\n            preds = preds[\"align\"][-1].numpy()\n        elif isinstance(preds, paddle.Tensor):\n            preds = preds.numpy()\n        else:\n            preds = preds\n\n        preds_idx = preds.argmax(axis=2)\n        preds_prob = preds.max(axis=2)\n        text = self.decode(preds_idx, preds_prob, is_remove_duplicate=False)\n        if label is None:\n            return text\n        label = self.decode(label)\n        return text, label\n\n    def add_special_char(self, dict_character):\n        dict_character = [\"</s>\"] + dict_character\n        return dict_character\n\n\nclass SPINLabelDecode(AttnLabelDecode):\n    \"\"\"Convert between text-label and text-index\"\"\"\n\n    def __init__(self, character_dict_path=None, use_space_char=False, **kwargs):\n        super(SPINLabelDecode, self).__init__(character_dict_path, use_space_char)\n\n    def add_special_char(self, dict_character):\n        self.beg_str = \"sos\"\n        self.end_str = \"eos\"\n        dict_character = dict_character\n        dict_character = [self.beg_str] + [self.end_str] + dict_character\n        return dict_character\n\n\nclass VLLabelDecode(BaseRecLabelDecode):\n    \"\"\"Convert between text-label and text-index\"\"\"\n\n    def __init__(self, character_dict_path=None, use_space_char=False, **kwargs):\n        super(VLLabelDecode, self).__init__(character_dict_path, use_space_char)\n        self.max_text_length = kwargs.get(\"max_text_length\", 25)\n        self.nclass = len(self.character) + 1\n\n    def decode(self, text_index, text_prob=None, is_remove_duplicate=False):\n        \"\"\"convert text-index into text-label.\"\"\"\n        result_list = []\n        ignored_tokens = self.get_ignored_tokens()\n        batch_size = len(text_index)\n        for batch_idx in range(batch_size):\n            selection = np.ones(len(text_index[batch_idx]), dtype=bool)\n            if is_remove_duplicate:\n                selection[1:] = text_index[batch_idx][1:] != text_index[batch_idx][:-1]\n            for ignored_token in ignored_tokens:\n                selection &= text_index[batch_idx] != ignored_token\n\n            char_list = [\n                self.character[text_id - 1]\n                for text_id in text_index[batch_idx][selection]\n            ]\n            if text_prob is not None:\n                conf_list = text_prob[batch_idx][selection]\n            else:\n                conf_list = [1] * len(selection)\n            if len(conf_list) == 0:\n                conf_list = [0]\n\n            text = \"\".join(char_list)\n            result_list.append((text, np.mean(conf_list).tolist()))\n        return result_list\n\n    def __call__(self, preds, label=None, length=None, *args, **kwargs):\n        if len(preds) == 2:  # eval mode\n            text_pre, x = preds\n            b = text_pre.shape[1]\n            lenText = self.max_text_length\n            nsteps = self.max_text_length\n\n            if not isinstance(text_pre, paddle.Tensor):\n                text_pre = paddle.to_tensor(text_pre, dtype=\"float32\")\n\n            out_res = paddle.zeros(shape=[lenText, b, self.nclass], dtype=x.dtype)\n            out_length = paddle.zeros(shape=[b], dtype=x.dtype)\n            now_step = 0\n            for _ in range(nsteps):\n                if 0 in out_length and now_step < nsteps:\n                    tmp_result = text_pre[now_step, :, :]\n                    out_res[now_step] = tmp_result\n                    tmp_result = tmp_result.topk(1)[1].squeeze(axis=1)\n                    for j in range(b):\n                        if out_length[j] == 0 and tmp_result[j] == 0:\n                            out_length[j] = now_step + 1\n                    now_step += 1\n            for j in range(0, b):\n                if int(out_length[j]) == 0:\n                    out_length[j] = nsteps\n            start = 0\n            output = paddle.zeros(\n                shape=[int(out_length.sum()), self.nclass], dtype=x.dtype\n            )\n            for i in range(0, b):\n                cur_length = int(out_length[i])\n                output[start : start + cur_length] = out_res[0:cur_length, i, :]\n                start += cur_length\n            net_out = output\n            length = out_length\n\n        else:  # train mode\n            net_out = preds[0]\n            length = length\n            net_out = paddle.concat([t[:l] for t, l in zip(net_out, length)])\n        text = []\n        if not isinstance(net_out, paddle.Tensor):\n            net_out = paddle.to_tensor(net_out, dtype=\"float32\")\n        net_out = F.softmax(net_out, axis=1)\n        for i in range(0, length.shape[0]):\n            if i == 0:\n                start_idx = 0\n                end_idx = int(length[i])\n            else:\n                start_idx = int(length[:i].sum())\n                end_idx = int(length[:i].sum() + length[i])\n            preds_idx = net_out[start_idx:end_idx].topk(1)[1][:, 0].tolist()\n            preds_text = \"\".join(\n                [\n                    (\n                        self.character[idx - 1]\n                        if idx > 0 and idx <= len(self.character)\n                        else \"\"\n                    )\n                    for idx in preds_idx\n                ]\n            )\n            preds_prob = net_out[start_idx:end_idx].topk(1)[0][:, 0]\n            preds_prob = paddle.exp(\n                paddle.log(preds_prob).sum() / (preds_prob.shape[0] + 1e-6)\n            )\n            text.append((preds_text, float(preds_prob)))\n        if label is None:\n            return text\n        label = self.decode(label)\n        return text, label\n\n\nclass CANLabelDecode(BaseRecLabelDecode):\n    \"\"\"Convert between latex-symbol and symbol-index\"\"\"\n\n    def __init__(self, character_dict_path=None, use_space_char=False, **kwargs):\n        super(CANLabelDecode, self).__init__(character_dict_path, use_space_char)\n\n    def decode(self, text_index, preds_prob=None):\n        result_list = []\n        batch_size = len(text_index)\n        for batch_idx in range(batch_size):\n            seq_end = text_index[batch_idx].argmin(0)\n            idx_list = text_index[batch_idx][:seq_end].tolist()\n            symbol_list = [self.character[idx] for idx in idx_list]\n            probs = []\n            if preds_prob is not None:\n                probs = preds_prob[batch_idx][: len(symbol_list)].tolist()\n\n            result_list.append([\" \".join(symbol_list), probs])\n        return result_list\n\n    def __call__(self, preds, label=None, *args, **kwargs):\n        pred_prob, _, _, _ = preds\n        preds_idx = pred_prob.argmax(axis=2)\n\n        text = self.decode(preds_idx)\n        if label is None:\n            return text\n        label = self.decode(label)\n        return text, label\n\n\nclass CPPDLabelDecode(NRTRLabelDecode):\n    \"\"\"Convert between text-label and text-index\"\"\"\n\n    def __init__(self, character_dict_path=None, use_space_char=False, **kwargs):\n        super(CPPDLabelDecode, self).__init__(character_dict_path, use_space_char)\n\n    def __call__(self, preds, label=None, *args, **kwargs):\n        if isinstance(preds, tuple):\n            if isinstance(preds[-1], dict):\n                preds = preds[-1][\"align\"][-1].numpy()\n            else:\n                preds = preds[-1].numpy()\n        if isinstance(preds, paddle.Tensor):\n            preds = preds.numpy()\n        else:\n            preds = preds\n        preds_idx = preds.argmax(axis=2)\n        preds_prob = preds.max(axis=2)\n        text = self.decode(preds_idx, preds_prob, is_remove_duplicate=False)\n        if label is None:\n            return text\n        label = self.decode(label)\n        return text, label\n\n    def add_special_char(self, dict_character):\n        dict_character = [\"</s>\"] + dict_character\n        return dict_character\n", "ppocr/postprocess/picodet_postprocess.py": "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nfrom scipy.special import softmax\n\n\ndef hard_nms(box_scores, iou_threshold, top_k=-1, candidate_size=200):\n    \"\"\"\n    Args:\n        box_scores (N, 5): boxes in corner-form and probabilities.\n        iou_threshold: intersection over union threshold.\n        top_k: keep top_k results. If k <= 0, keep all the results.\n        candidate_size: only consider the candidates with the highest scores.\n    Returns:\n         picked: a list of indexes of the kept boxes\n    \"\"\"\n    scores = box_scores[:, -1]\n    boxes = box_scores[:, :-1]\n    picked = []\n    indexes = np.argsort(scores)\n    indexes = indexes[-candidate_size:]\n    while len(indexes) > 0:\n        current = indexes[-1]\n        picked.append(current)\n        if 0 < top_k == len(picked) or len(indexes) == 1:\n            break\n        current_box = boxes[current, :]\n        indexes = indexes[:-1]\n        rest_boxes = boxes[indexes, :]\n        iou = iou_of(\n            rest_boxes,\n            np.expand_dims(current_box, axis=0),\n        )\n        indexes = indexes[iou <= iou_threshold]\n\n    return box_scores[picked, :]\n\n\ndef iou_of(boxes0, boxes1, eps=1e-5):\n    \"\"\"Return intersection-over-union (Jaccard index) of boxes.\n    Args:\n        boxes0 (N, 4): ground truth boxes.\n        boxes1 (N or 1, 4): predicted boxes.\n        eps: a small number to avoid 0 as denominator.\n    Returns:\n        iou (N): IoU values.\n    \"\"\"\n    overlap_left_top = np.maximum(boxes0[..., :2], boxes1[..., :2])\n    overlap_right_bottom = np.minimum(boxes0[..., 2:], boxes1[..., 2:])\n\n    overlap_area = area_of(overlap_left_top, overlap_right_bottom)\n    area0 = area_of(boxes0[..., :2], boxes0[..., 2:])\n    area1 = area_of(boxes1[..., :2], boxes1[..., 2:])\n    return overlap_area / (area0 + area1 - overlap_area + eps)\n\n\ndef area_of(left_top, right_bottom):\n    \"\"\"Compute the areas of rectangles given two corners.\n    Args:\n        left_top (N, 2): left top corner.\n        right_bottom (N, 2): right bottom corner.\n    Returns:\n        area (N): return the area.\n    \"\"\"\n    hw = np.clip(right_bottom - left_top, 0.0, None)\n    return hw[..., 0] * hw[..., 1]\n\n\ndef calculate_containment(boxes0, boxes1):\n    \"\"\"\n    Calculate the containment of the boxes.\n    Args:\n        boxes0 (N, 4): ground truth boxes.\n        boxes1 (N or 1, 4): predicted boxes.\n    Returns:\n        containment (N): containment values.\n    \"\"\"\n    overlap_left_top = np.maximum(boxes0[..., :2], boxes1[..., :2])\n    overlap_right_bottom = np.minimum(boxes0[..., 2:], boxes1[..., 2:])\n\n    overlap_area = area_of(overlap_left_top, overlap_right_bottom)\n    area0 = area_of(boxes0[..., :2], boxes0[..., 2:])\n    area1 = area_of(boxes1[..., :2], boxes1[..., 2:])\n    return overlap_area / np.minimum(area0, np.expand_dims(area1, axis=0))\n\n\nclass PicoDetPostProcess(object):\n    \"\"\"\n    Args:\n        input_shape (int): network input image size\n        ori_shape (int): ori image shape of before padding\n        scale_factor (float): scale factor of ori image\n        enable_mkldnn (bool): whether to open MKLDNN\n    \"\"\"\n\n    def __init__(\n        self,\n        layout_dict_path,\n        strides=[8, 16, 32, 64],\n        score_threshold=0.4,\n        nms_threshold=0.5,\n        nms_top_k=1000,\n        keep_top_k=100,\n    ):\n        self.labels = self.load_layout_dict(layout_dict_path)\n        self.strides = strides\n        self.score_threshold = score_threshold\n        self.nms_threshold = nms_threshold\n        self.nms_top_k = nms_top_k\n        self.keep_top_k = keep_top_k\n\n    def load_layout_dict(self, layout_dict_path):\n        with open(layout_dict_path, \"r\", encoding=\"utf-8\") as fp:\n            labels = fp.readlines()\n        return [label.strip(\"\\n\") for label in labels]\n\n    def warp_boxes(self, boxes, ori_shape):\n        \"\"\"Apply transform to boxes\"\"\"\n        width, height = ori_shape[1], ori_shape[0]\n        n = len(boxes)\n        if n:\n            # warp points\n            xy = np.ones((n * 4, 3))\n            xy[:, :2] = boxes[:, [0, 1, 2, 3, 0, 3, 2, 1]].reshape(\n                n * 4, 2\n            )  # x1y1, x2y2, x1y2, x2y1\n            # xy = xy @ M.T  # transform\n            xy = (xy[:, :2] / xy[:, 2:3]).reshape(n, 8)  # rescale\n            # create new boxes\n            x = xy[:, [0, 2, 4, 6]]\n            y = xy[:, [1, 3, 5, 7]]\n            xy = (\n                np.concatenate((x.min(1), y.min(1), x.max(1), y.max(1))).reshape(4, n).T\n            )\n            # clip boxes\n            xy[:, [0, 2]] = xy[:, [0, 2]].clip(0, width)\n            xy[:, [1, 3]] = xy[:, [1, 3]].clip(0, height)\n            return xy.astype(np.float32)\n        else:\n            return boxes\n\n    def img_info(self, ori_img, img):\n        origin_shape = ori_img.shape\n        resize_shape = img.shape\n        im_scale_y = resize_shape[2] / float(origin_shape[0])\n        im_scale_x = resize_shape[3] / float(origin_shape[1])\n        scale_factor = np.array([im_scale_y, im_scale_x], dtype=np.float32)\n        img_shape = np.array(img.shape[2:], dtype=np.float32)\n\n        input_shape = np.array(img).astype(\"float32\").shape[2:]\n        ori_shape = np.array((img_shape,)).astype(\"float32\")\n        scale_factor = np.array((scale_factor,)).astype(\"float32\")\n        return ori_shape, input_shape, scale_factor\n\n    def __call__(self, ori_img, img, preds):\n        scores, raw_boxes = preds[\"boxes\"], preds[\"boxes_num\"]\n        batch_size = raw_boxes[0].shape[0]\n        reg_max = int(raw_boxes[0].shape[-1] / 4 - 1)\n        out_boxes_num = []\n        out_boxes_list = []\n        results = []\n        ori_shape, input_shape, scale_factor = self.img_info(ori_img, img)\n\n        for batch_id in range(batch_size):\n            # generate centers\n            decode_boxes = []\n            select_scores = []\n            for stride, box_distribute, score in zip(self.strides, raw_boxes, scores):\n                box_distribute = box_distribute[batch_id]\n                score = score[batch_id]\n                # centers\n                fm_h = input_shape[0] / stride\n                fm_w = input_shape[1] / stride\n                h_range = np.arange(fm_h)\n                w_range = np.arange(fm_w)\n                ww, hh = np.meshgrid(w_range, h_range)\n                ct_row = (hh.flatten() + 0.5) * stride\n                ct_col = (ww.flatten() + 0.5) * stride\n                center = np.stack((ct_col, ct_row, ct_col, ct_row), axis=1)\n\n                # box distribution to distance\n                reg_range = np.arange(reg_max + 1)\n                box_distance = box_distribute.reshape((-1, reg_max + 1))\n                box_distance = softmax(box_distance, axis=1)\n                box_distance = box_distance * np.expand_dims(reg_range, axis=0)\n                box_distance = np.sum(box_distance, axis=1).reshape((-1, 4))\n                box_distance = box_distance * stride\n\n                # top K candidate\n                topk_idx = np.argsort(score.max(axis=1))[::-1]\n                topk_idx = topk_idx[: self.nms_top_k]\n                center = center[topk_idx]\n                score = score[topk_idx]\n                box_distance = box_distance[topk_idx]\n\n                # decode box\n                decode_box = center + [-1, -1, 1, 1] * box_distance\n\n                select_scores.append(score)\n                decode_boxes.append(decode_box)\n\n            # nms\n            bboxes = np.concatenate(decode_boxes, axis=0)\n            confidences = np.concatenate(select_scores, axis=0)\n            picked_box_probs = []\n            picked_labels = []\n            for class_index in range(0, confidences.shape[1]):\n                probs = confidences[:, class_index]\n                mask = probs > self.score_threshold\n                probs = probs[mask]\n                if probs.shape[0] == 0:\n                    continue\n                subset_boxes = bboxes[mask, :]\n                box_probs = np.concatenate([subset_boxes, probs.reshape(-1, 1)], axis=1)\n                box_probs = hard_nms(\n                    box_probs,\n                    iou_threshold=self.nms_threshold,\n                    top_k=self.keep_top_k,\n                )\n                picked_box_probs.append(box_probs)\n                picked_labels.extend([class_index] * box_probs.shape[0])\n\n            if len(picked_box_probs) == 0:\n                out_boxes_list.append(np.empty((0, 4)))\n                out_boxes_num.append(0)\n\n            else:\n                picked_box_probs = np.concatenate(picked_box_probs)\n\n                # resize output boxes\n                picked_box_probs[:, :4] = self.warp_boxes(\n                    picked_box_probs[:, :4], ori_shape[batch_id]\n                )\n                im_scale = np.concatenate(\n                    [scale_factor[batch_id][::-1], scale_factor[batch_id][::-1]]\n                )\n                picked_box_probs[:, :4] /= im_scale\n                # clas score box\n                out_boxes_list.append(\n                    np.concatenate(\n                        [\n                            np.expand_dims(np.array(picked_labels), axis=-1),\n                            np.expand_dims(picked_box_probs[:, 4], axis=-1),\n                            picked_box_probs[:, :4],\n                        ],\n                        axis=1,\n                    )\n                )\n                out_boxes_num.append(len(picked_labels))\n\n        out_boxes_list = np.concatenate(out_boxes_list, axis=0)\n        out_boxes_num = np.asarray(out_boxes_num).astype(np.int32)\n\n        for dt in out_boxes_list:\n            clsid, bbox, score = int(dt[0]), dt[2:], dt[1]\n            label = self.labels[clsid]\n            result = {\"bbox\": bbox, \"label\": label, \"score\": score}\n            results.append(result)\n\n        # Handle conflict where a box is simultaneously recognized as multiple labels.\n        # Use IoU to find similar boxes. Prioritize labels as table, text, and others when deduplicate similar boxes.\n        bboxes = np.array([x[\"bbox\"] for x in results])\n        duplicate_idx = list()\n        for i in range(len(results)):\n            if i in duplicate_idx:\n                continue\n            containments = calculate_containment(bboxes, bboxes[i, ...])\n            overlaps = np.where(containments > 0.5)[0]\n            if len(overlaps) > 1:\n                table_box = [x for x in overlaps if results[x][\"label\"] == \"table\"]\n                if len(table_box) > 0:\n                    keep = sorted(\n                        [(x, results[x]) for x in table_box],\n                        key=lambda x: x[1][\"score\"],\n                        reverse=True,\n                    )[0][0]\n                else:\n                    keep = sorted(\n                        [(x, results[x]) for x in overlaps],\n                        key=lambda x: x[1][\"score\"],\n                        reverse=True,\n                    )[0][0]\n                duplicate_idx.extend([x for x in overlaps if x != keep])\n        results = [x for i, x in enumerate(results) if i not in duplicate_idx]\n        return results\n", "ppocr/postprocess/vqa_token_re_layoutlm_postprocess.py": "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport paddle\n\n\nclass VQAReTokenLayoutLMPostProcess(object):\n    \"\"\"Convert between text-label and text-index\"\"\"\n\n    def __init__(self, **kwargs):\n        super(VQAReTokenLayoutLMPostProcess, self).__init__()\n\n    def __call__(self, preds, label=None, *args, **kwargs):\n        pred_relations = preds[\"pred_relations\"]\n        if isinstance(preds[\"pred_relations\"], paddle.Tensor):\n            pred_relations = pred_relations.numpy()\n        pred_relations = self.decode_pred(pred_relations)\n\n        if label is not None:\n            return self._metric(pred_relations, label)\n        else:\n            return self._infer(pred_relations, *args, **kwargs)\n\n    def _metric(self, pred_relations, label):\n        return pred_relations, label[-1], label[-2]\n\n    def _infer(self, pred_relations, *args, **kwargs):\n        ser_results = kwargs[\"ser_results\"]\n        entity_idx_dict_batch = kwargs[\"entity_idx_dict_batch\"]\n\n        # merge relations and ocr info\n        results = []\n        for pred_relation, ser_result, entity_idx_dict in zip(\n            pred_relations, ser_results, entity_idx_dict_batch\n        ):\n            result = []\n            used_tail_id = []\n            for relation in pred_relation:\n                if relation[\"tail_id\"] in used_tail_id:\n                    continue\n                used_tail_id.append(relation[\"tail_id\"])\n                ocr_info_head = ser_result[entity_idx_dict[relation[\"head_id\"]]]\n                ocr_info_tail = ser_result[entity_idx_dict[relation[\"tail_id\"]]]\n                result.append((ocr_info_head, ocr_info_tail))\n            results.append(result)\n        return results\n\n    def decode_pred(self, pred_relations):\n        pred_relations_new = []\n        for pred_relation in pred_relations:\n            pred_relation_new = []\n            pred_relation = pred_relation[1 : pred_relation[0, 0, 0] + 1]\n            for relation in pred_relation:\n                relation_new = dict()\n                relation_new[\"head_id\"] = relation[0, 0]\n                relation_new[\"head\"] = tuple(relation[1])\n                relation_new[\"head_type\"] = relation[2, 0]\n                relation_new[\"tail_id\"] = relation[3, 0]\n                relation_new[\"tail\"] = tuple(relation[4])\n                relation_new[\"tail_type\"] = relation[5, 0]\n                relation_new[\"type\"] = relation[6, 0]\n                pred_relation_new.append(relation_new)\n            pred_relations_new.append(pred_relation_new)\n        return pred_relations_new\n\n\nclass DistillationRePostProcess(VQAReTokenLayoutLMPostProcess):\n    \"\"\"\n    DistillationRePostProcess\n    \"\"\"\n\n    def __init__(self, model_name=[\"Student\"], key=None, **kwargs):\n        super().__init__(**kwargs)\n        if not isinstance(model_name, list):\n            model_name = [model_name]\n        self.model_name = model_name\n        self.key = key\n\n    def __call__(self, preds, *args, **kwargs):\n        output = dict()\n        for name in self.model_name:\n            pred = preds[name]\n            if self.key is not None:\n                pred = pred[self.key]\n            output[name] = super().__call__(pred, *args, **kwargs)\n        return output\n", "ppocr/postprocess/east_postprocess.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nfrom .locality_aware_nms import nms_locality\nimport cv2\nimport paddle\n\nimport os\nfrom ppocr.utils.utility import check_install\nimport sys\n\n\nclass EASTPostProcess(object):\n    \"\"\"\n    The post process for EAST.\n    \"\"\"\n\n    def __init__(self, score_thresh=0.8, cover_thresh=0.1, nms_thresh=0.2, **kwargs):\n        self.score_thresh = score_thresh\n        self.cover_thresh = cover_thresh\n        self.nms_thresh = nms_thresh\n\n    def restore_rectangle_quad(self, origin, geometry):\n        \"\"\"\n        Restore rectangle from quadrangle.\n        \"\"\"\n        # quad\n        origin_concat = np.concatenate(\n            (origin, origin, origin, origin), axis=1\n        )  # (n, 8)\n        pred_quads = origin_concat - geometry\n        pred_quads = pred_quads.reshape((-1, 4, 2))  # (n, 4, 2)\n        return pred_quads\n\n    def detect(\n        self, score_map, geo_map, score_thresh=0.8, cover_thresh=0.1, nms_thresh=0.2\n    ):\n        \"\"\"\n        restore text boxes from score map and geo map\n        \"\"\"\n\n        score_map = score_map[0]\n        geo_map = np.swapaxes(geo_map, 1, 0)\n        geo_map = np.swapaxes(geo_map, 1, 2)\n        # filter the score map\n        xy_text = np.argwhere(score_map > score_thresh)\n        if len(xy_text) == 0:\n            return []\n        # sort the text boxes via the y axis\n        xy_text = xy_text[np.argsort(xy_text[:, 0])]\n        # restore quad proposals\n        text_box_restored = self.restore_rectangle_quad(\n            xy_text[:, ::-1] * 4, geo_map[xy_text[:, 0], xy_text[:, 1], :]\n        )\n        boxes = np.zeros((text_box_restored.shape[0], 9), dtype=np.float32)\n        boxes[:, :8] = text_box_restored.reshape((-1, 8))\n        boxes[:, 8] = score_map[xy_text[:, 0], xy_text[:, 1]]\n\n        try:\n            check_install(\"lanms\", \"lanms-nova\")\n            import lanms\n\n            boxes = lanms.merge_quadrangle_n9(boxes, nms_thresh)\n        except:\n            print(\n                \"You should install lanms by pip3 install lanms-nova to speed up nms_locality\"\n            )\n            boxes = nms_locality(boxes.astype(np.float64), nms_thresh)\n        if boxes.shape[0] == 0:\n            return []\n        # Here we filter some low score boxes by the average score map,\n        #   this is different from the orginal paper.\n        for i, box in enumerate(boxes):\n            mask = np.zeros_like(score_map, dtype=np.uint8)\n            cv2.fillPoly(mask, box[:8].reshape((-1, 4, 2)).astype(np.int32) // 4, 1)\n            boxes[i, 8] = cv2.mean(score_map, mask)[0]\n        boxes = boxes[boxes[:, 8] > cover_thresh]\n        return boxes\n\n    def sort_poly(self, p):\n        \"\"\"\n        Sort polygons.\n        \"\"\"\n        min_axis = np.argmin(np.sum(p, axis=1))\n        p = p[[min_axis, (min_axis + 1) % 4, (min_axis + 2) % 4, (min_axis + 3) % 4]]\n        if abs(p[0, 0] - p[1, 0]) > abs(p[0, 1] - p[1, 1]):\n            return p\n        else:\n            return p[[0, 3, 2, 1]]\n\n    def __call__(self, outs_dict, shape_list):\n        score_list = outs_dict[\"f_score\"]\n        geo_list = outs_dict[\"f_geo\"]\n        if isinstance(score_list, paddle.Tensor):\n            score_list = score_list.numpy()\n            geo_list = geo_list.numpy()\n        img_num = len(shape_list)\n        dt_boxes_list = []\n        for ino in range(img_num):\n            score = score_list[ino]\n            geo = geo_list[ino]\n            boxes = self.detect(\n                score_map=score,\n                geo_map=geo,\n                score_thresh=self.score_thresh,\n                cover_thresh=self.cover_thresh,\n                nms_thresh=self.nms_thresh,\n            )\n            boxes_norm = []\n            if len(boxes) > 0:\n                h, w = score.shape[1:]\n                src_h, src_w, ratio_h, ratio_w = shape_list[ino]\n                boxes = boxes[:, :8].reshape((-1, 4, 2))\n                boxes[:, :, 0] /= ratio_w\n                boxes[:, :, 1] /= ratio_h\n                for i_box, box in enumerate(boxes):\n                    box = self.sort_poly(box.astype(np.int32))\n                    if (\n                        np.linalg.norm(box[0] - box[1]) < 5\n                        or np.linalg.norm(box[3] - box[0]) < 5\n                    ):\n                        continue\n                    boxes_norm.append(box)\n            dt_boxes_list.append({\"points\": np.array(boxes_norm)})\n        return dt_boxes_list\n", "ppocr/postprocess/__init__.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport copy\n\n__all__ = [\"build_post_process\"]\n\nfrom .db_postprocess import DBPostProcess, DistillationDBPostProcess\nfrom .east_postprocess import EASTPostProcess\nfrom .sast_postprocess import SASTPostProcess\nfrom .fce_postprocess import FCEPostProcess\nfrom .rec_postprocess import (\n    CTCLabelDecode,\n    AttnLabelDecode,\n    SRNLabelDecode,\n    DistillationCTCLabelDecode,\n    NRTRLabelDecode,\n    SARLabelDecode,\n    SEEDLabelDecode,\n    PRENLabelDecode,\n    ViTSTRLabelDecode,\n    ABINetLabelDecode,\n    SPINLabelDecode,\n    VLLabelDecode,\n    RFLLabelDecode,\n    SATRNLabelDecode,\n    ParseQLabelDecode,\n    CPPDLabelDecode,\n)\nfrom .cls_postprocess import ClsPostProcess\nfrom .pg_postprocess import PGPostProcess\nfrom .vqa_token_ser_layoutlm_postprocess import (\n    VQASerTokenLayoutLMPostProcess,\n    DistillationSerPostProcess,\n)\nfrom .vqa_token_re_layoutlm_postprocess import (\n    VQAReTokenLayoutLMPostProcess,\n    DistillationRePostProcess,\n)\nfrom .table_postprocess import TableMasterLabelDecode, TableLabelDecode\nfrom .picodet_postprocess import PicoDetPostProcess\nfrom .ct_postprocess import CTPostProcess\nfrom .drrg_postprocess import DRRGPostprocess\nfrom .rec_postprocess import CANLabelDecode\n\n\ndef build_post_process(config, global_config=None):\n    support_dict = [\n        \"DBPostProcess\",\n        \"EASTPostProcess\",\n        \"SASTPostProcess\",\n        \"FCEPostProcess\",\n        \"CTCLabelDecode\",\n        \"AttnLabelDecode\",\n        \"ClsPostProcess\",\n        \"SRNLabelDecode\",\n        \"PGPostProcess\",\n        \"DistillationCTCLabelDecode\",\n        \"TableLabelDecode\",\n        \"DistillationDBPostProcess\",\n        \"NRTRLabelDecode\",\n        \"SARLabelDecode\",\n        \"SEEDLabelDecode\",\n        \"VQASerTokenLayoutLMPostProcess\",\n        \"VQAReTokenLayoutLMPostProcess\",\n        \"PRENLabelDecode\",\n        \"DistillationSARLabelDecode\",\n        \"ViTSTRLabelDecode\",\n        \"ABINetLabelDecode\",\n        \"TableMasterLabelDecode\",\n        \"SPINLabelDecode\",\n        \"DistillationSerPostProcess\",\n        \"DistillationRePostProcess\",\n        \"VLLabelDecode\",\n        \"PicoDetPostProcess\",\n        \"CTPostProcess\",\n        \"RFLLabelDecode\",\n        \"DRRGPostprocess\",\n        \"CANLabelDecode\",\n        \"SATRNLabelDecode\",\n        \"ParseQLabelDecode\",\n        \"CPPDLabelDecode\",\n    ]\n\n    if config[\"name\"] == \"PSEPostProcess\":\n        from .pse_postprocess import PSEPostProcess\n\n        support_dict.append(\"PSEPostProcess\")\n\n    config = copy.deepcopy(config)\n    module_name = config.pop(\"name\")\n    if module_name == \"None\":\n        return\n    if global_config is not None:\n        config.update(global_config)\n    assert module_name in support_dict, Exception(\n        \"post process only support {}\".format(support_dict)\n    )\n    module_class = eval(module_name)(**config)\n    return module_class\n", "ppocr/postprocess/vqa_token_ser_layoutlm_postprocess.py": "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport numpy as np\nimport paddle\nfrom ppocr.utils.utility import load_vqa_bio_label_maps\n\n\nclass VQASerTokenLayoutLMPostProcess(object):\n    \"\"\"Convert between text-label and text-index\"\"\"\n\n    def __init__(self, class_path, **kwargs):\n        super(VQASerTokenLayoutLMPostProcess, self).__init__()\n        label2id_map, self.id2label_map = load_vqa_bio_label_maps(class_path)\n\n        self.label2id_map_for_draw = dict()\n        for key in label2id_map:\n            if key.startswith(\"I-\"):\n                self.label2id_map_for_draw[key] = label2id_map[\"B\" + key[1:]]\n            else:\n                self.label2id_map_for_draw[key] = label2id_map[key]\n\n        self.id2label_map_for_show = dict()\n        for key in self.label2id_map_for_draw:\n            val = self.label2id_map_for_draw[key]\n            if key == \"O\":\n                self.id2label_map_for_show[val] = key\n            if key.startswith(\"B-\") or key.startswith(\"I-\"):\n                self.id2label_map_for_show[val] = key[2:]\n            else:\n                self.id2label_map_for_show[val] = key\n\n    def __call__(self, preds, batch=None, *args, **kwargs):\n        if isinstance(preds, tuple):\n            preds = preds[0]\n        if isinstance(preds, paddle.Tensor):\n            preds = preds.numpy()\n\n        if batch is not None:\n            return self._metric(preds, batch[5])\n        else:\n            return self._infer(preds, **kwargs)\n\n    def _metric(self, preds, label):\n        pred_idxs = preds.argmax(axis=2)\n        decode_out_list = [[] for _ in range(pred_idxs.shape[0])]\n        label_decode_out_list = [[] for _ in range(pred_idxs.shape[0])]\n\n        for i in range(pred_idxs.shape[0]):\n            for j in range(pred_idxs.shape[1]):\n                if label[i, j] != -100:\n                    label_decode_out_list[i].append(self.id2label_map[label[i, j]])\n                    decode_out_list[i].append(self.id2label_map[pred_idxs[i, j]])\n        return decode_out_list, label_decode_out_list\n\n    def _infer(self, preds, segment_offset_ids, ocr_infos):\n        results = []\n\n        for pred, segment_offset_id, ocr_info in zip(\n            preds, segment_offset_ids, ocr_infos\n        ):\n            pred = np.argmax(pred, axis=1)\n            pred = [self.id2label_map[idx] for idx in pred]\n\n            for idx in range(len(segment_offset_id)):\n                if idx == 0:\n                    start_id = 0\n                else:\n                    start_id = segment_offset_id[idx - 1]\n\n                end_id = segment_offset_id[idx]\n\n                curr_pred = pred[start_id:end_id]\n                curr_pred = [self.label2id_map_for_draw[p] for p in curr_pred]\n\n                if len(curr_pred) <= 0:\n                    pred_id = 0\n                else:\n                    counts = np.bincount(curr_pred)\n                    pred_id = np.argmax(counts)\n                ocr_info[idx][\"pred_id\"] = int(pred_id)\n                ocr_info[idx][\"pred\"] = self.id2label_map_for_show[int(pred_id)]\n            results.append(ocr_info)\n        return results\n\n\nclass DistillationSerPostProcess(VQASerTokenLayoutLMPostProcess):\n    \"\"\"\n    DistillationSerPostProcess\n    \"\"\"\n\n    def __init__(self, class_path, model_name=[\"Student\"], key=None, **kwargs):\n        super().__init__(class_path, **kwargs)\n        if not isinstance(model_name, list):\n            model_name = [model_name]\n        self.model_name = model_name\n        self.key = key\n\n    def __call__(self, preds, batch=None, *args, **kwargs):\n        output = dict()\n        for name in self.model_name:\n            pred = preds[name]\n            if self.key is not None:\n                pred = pred[self.key]\n            output[name] = super().__call__(pred, batch=batch, *args, **kwargs)\n        return output\n", "ppocr/postprocess/pg_postprocess.py": "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\n\n__dir__ = os.path.dirname(__file__)\nsys.path.append(__dir__)\nsys.path.append(os.path.join(__dir__, \"..\"))\nfrom ppocr.utils.e2e_utils.pgnet_pp_utils import PGNet_PostProcess\n\n\nclass PGPostProcess(object):\n    \"\"\"\n    The post process for PGNet.\n    \"\"\"\n\n    def __init__(\n        self,\n        character_dict_path,\n        valid_set,\n        score_thresh,\n        mode,\n        point_gather_mode=None,\n        **kwargs,\n    ):\n        self.character_dict_path = character_dict_path\n        self.valid_set = valid_set\n        self.score_thresh = score_thresh\n        self.mode = mode\n        self.point_gather_mode = point_gather_mode\n\n        # c++ la-nms is faster, but only support python 3.5\n        self.is_python35 = False\n        if sys.version_info.major == 3 and sys.version_info.minor == 5:\n            self.is_python35 = True\n\n    def __call__(self, outs_dict, shape_list):\n        post = PGNet_PostProcess(\n            self.character_dict_path,\n            self.valid_set,\n            self.score_thresh,\n            outs_dict,\n            shape_list,\n            point_gather_mode=self.point_gather_mode,\n        )\n        if self.mode == \"fast\":\n            data = post.pg_postprocess_fast()\n        else:\n            data = post.pg_postprocess_slow()\n        return data\n", "ppocr/postprocess/cls_postprocess.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport paddle\n\n\nclass ClsPostProcess(object):\n    \"\"\"Convert between text-label and text-index\"\"\"\n\n    def __init__(self, label_list=None, key=None, **kwargs):\n        super(ClsPostProcess, self).__init__()\n        self.label_list = label_list\n        self.key = key\n\n    def __call__(self, preds, label=None, *args, **kwargs):\n        if self.key is not None:\n            preds = preds[self.key]\n\n        label_list = self.label_list\n        if label_list is None:\n            label_list = {idx: idx for idx in range(preds.shape[-1])}\n\n        if isinstance(preds, paddle.Tensor):\n            preds = preds.numpy()\n\n        pred_idxs = preds.argmax(axis=1)\n        decode_out = [\n            (label_list[idx], preds[i, idx]) for i, idx in enumerate(pred_idxs)\n        ]\n        if label is None:\n            return decode_out\n        label = [(label_list[idx], 1.0) for idx in label]\n        return decode_out, label\n", "ppocr/postprocess/fce_postprocess.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/open-mmlab/mmocr/blob/v0.3.0/mmocr/models/textdet/postprocess/wrapper.py\n\"\"\"\n\nimport cv2\nimport paddle\nimport numpy as np\nfrom numpy.fft import ifft\nfrom ppocr.utils.poly_nms import poly_nms, valid_boundary\n\n\ndef fill_hole(input_mask):\n    h, w = input_mask.shape\n    canvas = np.zeros((h + 2, w + 2), np.uint8)\n    canvas[1 : h + 1, 1 : w + 1] = input_mask.copy()\n\n    mask = np.zeros((h + 4, w + 4), np.uint8)\n\n    cv2.floodFill(canvas, mask, (0, 0), 1)\n    canvas = canvas[1 : h + 1, 1 : w + 1].astype(np.bool_)\n\n    return ~canvas | input_mask\n\n\ndef fourier2poly(fourier_coeff, num_reconstr_points=50):\n    \"\"\"Inverse Fourier transform\n    Args:\n        fourier_coeff (ndarray): Fourier coefficients shaped (n, 2k+1),\n            with n and k being candidates number and Fourier degree\n            respectively.\n        num_reconstr_points (int): Number of reconstructed polygon points.\n    Returns:\n        Polygons (ndarray): The reconstructed polygons shaped (n, n')\n    \"\"\"\n\n    a = np.zeros((len(fourier_coeff), num_reconstr_points), dtype=\"complex\")\n    k = (len(fourier_coeff[0]) - 1) // 2\n\n    a[:, 0 : k + 1] = fourier_coeff[:, k:]\n    a[:, -k:] = fourier_coeff[:, :k]\n\n    poly_complex = ifft(a) * num_reconstr_points\n    polygon = np.zeros((len(fourier_coeff), num_reconstr_points, 2))\n    polygon[:, :, 0] = poly_complex.real\n    polygon[:, :, 1] = poly_complex.imag\n    return polygon.astype(\"int32\").reshape((len(fourier_coeff), -1))\n\n\nclass FCEPostProcess(object):\n    \"\"\"\n    The post process for FCENet.\n    \"\"\"\n\n    def __init__(\n        self,\n        scales,\n        fourier_degree=5,\n        num_reconstr_points=50,\n        decoding_type=\"fcenet\",\n        score_thr=0.3,\n        nms_thr=0.1,\n        alpha=1.0,\n        beta=1.0,\n        box_type=\"poly\",\n        **kwargs,\n    ):\n        self.scales = scales\n        self.fourier_degree = fourier_degree\n        self.num_reconstr_points = num_reconstr_points\n        self.decoding_type = decoding_type\n        self.score_thr = score_thr\n        self.nms_thr = nms_thr\n        self.alpha = alpha\n        self.beta = beta\n        self.box_type = box_type\n\n    def __call__(self, preds, shape_list):\n        score_maps = []\n        for key, value in preds.items():\n            if isinstance(value, paddle.Tensor):\n                value = value.numpy()\n            cls_res = value[:, :4, :, :]\n            reg_res = value[:, 4:, :, :]\n            score_maps.append([cls_res, reg_res])\n\n        return self.get_boundary(score_maps, shape_list)\n\n    def resize_boundary(self, boundaries, scale_factor):\n        \"\"\"Rescale boundaries via scale_factor.\n\n        Args:\n            boundaries (list[list[float]]): The boundary list. Each boundary\n            with size 2k+1 with k>=4.\n            scale_factor(ndarray): The scale factor of size (4,).\n\n        Returns:\n            boundaries (list[list[float]]): The scaled boundaries.\n        \"\"\"\n        boxes = []\n        scores = []\n        for b in boundaries:\n            sz = len(b)\n            valid_boundary(b, True)\n            scores.append(b[-1])\n            b = (\n                (\n                    np.array(b[: sz - 1])\n                    * (np.tile(scale_factor[:2], int((sz - 1) / 2)).reshape(1, sz - 1))\n                )\n                .flatten()\n                .tolist()\n            )\n            boxes.append(np.array(b).reshape([-1, 2]))\n\n        return np.array(boxes, dtype=np.float32), scores\n\n    def get_boundary(self, score_maps, shape_list):\n        assert len(score_maps) == len(self.scales)\n        boundaries = []\n        for idx, score_map in enumerate(score_maps):\n            scale = self.scales[idx]\n            boundaries = boundaries + self._get_boundary_single(score_map, scale)\n\n        # nms\n        boundaries = poly_nms(boundaries, self.nms_thr)\n        boundaries, scores = self.resize_boundary(\n            boundaries, (1 / shape_list[0, 2:]).tolist()[::-1]\n        )\n\n        boxes_batch = [dict(points=boundaries, scores=scores)]\n        return boxes_batch\n\n    def _get_boundary_single(self, score_map, scale):\n        assert len(score_map) == 2\n        assert score_map[1].shape[1] == 4 * self.fourier_degree + 2\n\n        return self.fcenet_decode(\n            preds=score_map,\n            fourier_degree=self.fourier_degree,\n            num_reconstr_points=self.num_reconstr_points,\n            scale=scale,\n            alpha=self.alpha,\n            beta=self.beta,\n            box_type=self.box_type,\n            score_thr=self.score_thr,\n            nms_thr=self.nms_thr,\n        )\n\n    def fcenet_decode(\n        self,\n        preds,\n        fourier_degree,\n        num_reconstr_points,\n        scale,\n        alpha=1.0,\n        beta=2.0,\n        box_type=\"poly\",\n        score_thr=0.3,\n        nms_thr=0.1,\n    ):\n        \"\"\"Decoding predictions of FCENet to instances.\n\n        Args:\n            preds (list(Tensor)): The head output tensors.\n            fourier_degree (int): The maximum Fourier transform degree k.\n            num_reconstr_points (int): The points number of the polygon\n                reconstructed from predicted Fourier coefficients.\n            scale (int): The down-sample scale of the prediction.\n            alpha (float) : The parameter to calculate final scores. Score_{final}\n                    = (Score_{text region} ^ alpha)\n                    * (Score_{text center region}^ beta)\n            beta (float) : The parameter to calculate final score.\n            box_type (str):  Boundary encoding type 'poly' or 'quad'.\n            score_thr (float) : The threshold used to filter out the final\n                candidates.\n            nms_thr (float) :  The threshold of nms.\n\n        Returns:\n            boundaries (list[list[float]]): The instance boundary and confidence\n                list.\n        \"\"\"\n        assert isinstance(preds, list)\n        assert len(preds) == 2\n        assert box_type in [\"poly\", \"quad\"]\n\n        cls_pred = preds[0][0]\n        tr_pred = cls_pred[0:2]\n        tcl_pred = cls_pred[2:]\n\n        reg_pred = preds[1][0].transpose([1, 2, 0])\n        x_pred = reg_pred[:, :, : 2 * fourier_degree + 1]\n        y_pred = reg_pred[:, :, 2 * fourier_degree + 1 :]\n\n        score_pred = (tr_pred[1] ** alpha) * (tcl_pred[1] ** beta)\n        tr_pred_mask = (score_pred) > score_thr\n        tr_mask = fill_hole(tr_pred_mask)\n\n        tr_contours, _ = cv2.findContours(\n            tr_mask.astype(np.uint8), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE\n        )  # opencv4\n\n        mask = np.zeros_like(tr_mask)\n        boundaries = []\n        for cont in tr_contours:\n            deal_map = mask.copy().astype(np.int8)\n            cv2.drawContours(deal_map, [cont], -1, 1, -1)\n\n            score_map = score_pred * deal_map\n            score_mask = score_map > 0\n            xy_text = np.argwhere(score_mask)\n            dxy = xy_text[:, 1] + xy_text[:, 0] * 1j\n\n            x, y = x_pred[score_mask], y_pred[score_mask]\n            c = x + y * 1j\n            c[:, fourier_degree] = c[:, fourier_degree] + dxy\n            c *= scale\n\n            polygons = fourier2poly(c, num_reconstr_points)\n            score = score_map[score_mask].reshape(-1, 1)\n            polygons = poly_nms(np.hstack((polygons, score)).tolist(), nms_thr)\n\n            boundaries = boundaries + polygons\n\n        boundaries = poly_nms(boundaries, nms_thr)\n\n        if box_type == \"quad\":\n            new_boundaries = []\n            for boundary in boundaries:\n                poly = np.array(boundary[:-1]).reshape(-1, 2).astype(np.float32)\n                score = boundary[-1]\n                points = cv2.boxPoints(cv2.minAreaRect(poly))\n                points = np.int64(points)\n                new_boundaries.append(points.reshape(-1).tolist() + [score])\n                boundaries = new_boundaries\n\n        return boundaries\n", "ppocr/postprocess/pse_postprocess/__init__.py": "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .pse_postprocess import PSEPostProcess\n", "ppocr/postprocess/pse_postprocess/pse_postprocess.py": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/whai362/PSENet/blob/python3/models/head/psenet_head.py\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport cv2\nimport paddle\nfrom paddle.nn import functional as F\n\nfrom ppocr.postprocess.pse_postprocess.pse import pse\n\n\nclass PSEPostProcess(object):\n    \"\"\"\n    The post process for PSE.\n    \"\"\"\n\n    def __init__(\n        self,\n        thresh=0.5,\n        box_thresh=0.85,\n        min_area=16,\n        box_type=\"quad\",\n        scale=4,\n        **kwargs,\n    ):\n        assert box_type in [\"quad\", \"poly\"], \"Only quad and poly is supported\"\n        self.thresh = thresh\n        self.box_thresh = box_thresh\n        self.min_area = min_area\n        self.box_type = box_type\n        self.scale = scale\n\n    def __call__(self, outs_dict, shape_list):\n        pred = outs_dict[\"maps\"]\n        if not isinstance(pred, paddle.Tensor):\n            pred = paddle.to_tensor(pred)\n        pred = F.interpolate(pred, scale_factor=4 // self.scale, mode=\"bilinear\")\n\n        score = F.sigmoid(pred[:, 0, :, :])\n\n        kernels = (pred > self.thresh).astype(\"float32\")\n        text_mask = kernels[:, 0, :, :]\n        text_mask = paddle.unsqueeze(text_mask, axis=1)\n\n        kernels[:, 0:, :, :] = kernels[:, 0:, :, :] * text_mask\n\n        score = score.numpy()\n        kernels = kernels.numpy().astype(np.uint8)\n\n        boxes_batch = []\n        for batch_index in range(pred.shape[0]):\n            boxes, scores = self.boxes_from_bitmap(\n                score[batch_index], kernels[batch_index], shape_list[batch_index]\n            )\n\n            boxes_batch.append({\"points\": boxes, \"scores\": scores})\n        return boxes_batch\n\n    def boxes_from_bitmap(self, score, kernels, shape):\n        label = pse(kernels, self.min_area)\n        return self.generate_box(score, label, shape)\n\n    def generate_box(self, score, label, shape):\n        src_h, src_w, ratio_h, ratio_w = shape\n        label_num = np.max(label) + 1\n\n        boxes = []\n        scores = []\n        for i in range(1, label_num):\n            ind = label == i\n            points = np.array(np.where(ind)).transpose((1, 0))[:, ::-1]\n\n            if points.shape[0] < self.min_area:\n                label[ind] = 0\n                continue\n\n            score_i = np.mean(score[ind])\n            if score_i < self.box_thresh:\n                label[ind] = 0\n                continue\n\n            if self.box_type == \"quad\":\n                rect = cv2.minAreaRect(points)\n                bbox = cv2.boxPoints(rect)\n            elif self.box_type == \"poly\":\n                box_height = np.max(points[:, 1]) + 10\n                box_width = np.max(points[:, 0]) + 10\n\n                mask = np.zeros((box_height, box_width), np.uint8)\n                mask[points[:, 1], points[:, 0]] = 255\n\n                contours, _ = cv2.findContours(\n                    mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n                )\n                bbox = np.squeeze(contours[0], 1)\n            else:\n                raise NotImplementedError\n\n            bbox[:, 0] = np.clip(np.round(bbox[:, 0] / ratio_w), 0, src_w)\n            bbox[:, 1] = np.clip(np.round(bbox[:, 1] / ratio_h), 0, src_h)\n            boxes.append(bbox)\n            scores.append(score_i)\n        return boxes, scores\n", "ppocr/postprocess/pse_postprocess/pse/setup.py": "from distutils.core import setup, Extension\nfrom Cython.Build import cythonize\nimport numpy\n\nsetup(\n    ext_modules=cythonize(\n        Extension(\n            \"pse\",\n            sources=[\"pse.pyx\"],\n            language=\"c++\",\n            include_dirs=[numpy.get_include()],\n            library_dirs=[],\n            libraries=[],\n            extra_compile_args=[\"-O3\"],\n            extra_link_args=[],\n        )\n    )\n)\n", "ppocr/postprocess/pse_postprocess/pse/__init__.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport sys\nimport os\nimport subprocess\n\npython_path = sys.executable\n\nori_path = os.getcwd()\nos.chdir(\"ppocr/postprocess/pse_postprocess/pse\")\nif (\n    subprocess.call(\"{} setup.py build_ext --inplace\".format(python_path), shell=True)\n    != 0\n):\n    raise RuntimeError(\n        \"Cannot compile pse: {}, if your system is windows, you need to install all the default components of `desktop development using C++` in visual studio 2019+\".format(\n            os.path.dirname(os.path.realpath(__file__))\n        )\n    )\nos.chdir(ori_path)\n\nfrom .pse import pse\n", "ppocr/modeling/backbones/rec_resnet_aster.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/ayumiymk/aster.pytorch/blob/master/lib/models/resnet_aster.py\n\"\"\"\nimport paddle\nimport paddle.nn as nn\n\nimport sys\nimport math\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2D(\n        in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias_attr=False\n    )\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2D(\n        in_planes, out_planes, kernel_size=1, stride=stride, bias_attr=False\n    )\n\n\ndef get_sinusoid_encoding(n_position, feat_dim, wave_length=10000):\n    # [n_position]\n    positions = paddle.arange(0, n_position)\n    # [feat_dim]\n    dim_range = paddle.arange(0, feat_dim)\n    dim_range = paddle.pow(wave_length, 2 * (dim_range // 2) / feat_dim)\n    # [n_position, feat_dim]\n    angles = paddle.unsqueeze(positions, axis=1) / paddle.unsqueeze(dim_range, axis=0)\n    angles = paddle.cast(angles, \"float32\")\n    angles[:, 0::2] = paddle.sin(angles[:, 0::2])\n    angles[:, 1::2] = paddle.cos(angles[:, 1::2])\n    return angles\n\n\nclass AsterBlock(nn.Layer):\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(AsterBlock, self).__init__()\n        self.conv1 = conv1x1(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2D(planes)\n        self.relu = nn.ReLU()\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2D(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n        out += residual\n        out = self.relu(out)\n        return out\n\n\nclass ResNet_ASTER(nn.Layer):\n    \"\"\"For aster or crnn\"\"\"\n\n    def __init__(self, with_lstm=True, n_group=1, in_channels=3):\n        super(ResNet_ASTER, self).__init__()\n        self.with_lstm = with_lstm\n        self.n_group = n_group\n\n        self.layer0 = nn.Sequential(\n            nn.Conv2D(\n                in_channels,\n                32,\n                kernel_size=(3, 3),\n                stride=1,\n                padding=1,\n                bias_attr=False,\n            ),\n            nn.BatchNorm2D(32),\n            nn.ReLU(),\n        )\n\n        self.inplanes = 32\n        self.layer1 = self._make_layer(32, 3, [2, 2])  # [16, 50]\n        self.layer2 = self._make_layer(64, 4, [2, 2])  # [8, 25]\n        self.layer3 = self._make_layer(128, 6, [2, 1])  # [4, 25]\n        self.layer4 = self._make_layer(256, 6, [2, 1])  # [2, 25]\n        self.layer5 = self._make_layer(512, 3, [2, 1])  # [1, 25]\n\n        if with_lstm:\n            self.rnn = nn.LSTM(512, 256, direction=\"bidirect\", num_layers=2)\n            self.out_channels = 2 * 256\n        else:\n            self.out_channels = 512\n\n    def _make_layer(self, planes, blocks, stride):\n        downsample = None\n        if stride != [1, 1] or self.inplanes != planes:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes, stride), nn.BatchNorm2D(planes)\n            )\n\n        layers = []\n        layers.append(AsterBlock(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes\n        for _ in range(1, blocks):\n            layers.append(AsterBlock(self.inplanes, planes))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x0 = self.layer0(x)\n        x1 = self.layer1(x0)\n        x2 = self.layer2(x1)\n        x3 = self.layer3(x2)\n        x4 = self.layer4(x3)\n        x5 = self.layer5(x4)\n\n        cnn_feat = x5.squeeze(2)  # [N, c, w]\n        cnn_feat = paddle.transpose(cnn_feat, perm=[0, 2, 1])\n        if self.with_lstm:\n            rnn_feat, _ = self.rnn(cnn_feat)\n            return rnn_feat\n        else:\n            return cnn_feat\n", "ppocr/modeling/backbones/rec_svtrnet.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom paddle import ParamAttr\nfrom paddle.nn.initializer import KaimingNormal\nimport numpy as np\nimport paddle\nimport paddle.nn as nn\nfrom paddle.nn.initializer import TruncatedNormal, Constant, Normal\n\ntrunc_normal_ = TruncatedNormal(std=0.02)\nnormal_ = Normal\nzeros_ = Constant(value=0.0)\nones_ = Constant(value=1.0)\n\n\ndef drop_path(x, drop_prob=0.0, training=False):\n    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ...\n    \"\"\"\n    if drop_prob == 0.0 or not training:\n        return x\n    keep_prob = paddle.to_tensor(1 - drop_prob, dtype=x.dtype)\n    shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n    random_tensor = keep_prob + paddle.rand(shape, dtype=x.dtype)\n    random_tensor = paddle.floor(random_tensor)  # binarize\n    output = x.divide(keep_prob) * random_tensor\n    return output\n\n\nclass ConvBNLayer(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size=3,\n        stride=1,\n        padding=0,\n        bias_attr=False,\n        groups=1,\n        act=nn.GELU,\n    ):\n        super().__init__()\n        self.conv = nn.Conv2D(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            groups=groups,\n            weight_attr=paddle.ParamAttr(initializer=nn.initializer.KaimingUniform()),\n            bias_attr=bias_attr,\n        )\n        self.norm = nn.BatchNorm2D(out_channels)\n        self.act = act()\n\n    def forward(self, inputs):\n        out = self.conv(inputs)\n        out = self.norm(out)\n        out = self.act(out)\n        return out\n\n\nclass DropPath(nn.Layer):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\"\"\"\n\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\n\n\nclass Identity(nn.Layer):\n    def __init__(self):\n        super(Identity, self).__init__()\n\n    def forward(self, input):\n        return input\n\n\nclass Mlp(nn.Layer):\n    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        act_layer=nn.GELU,\n        drop=0.0,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass ConvMixer(nn.Layer):\n    def __init__(\n        self,\n        dim,\n        num_heads=8,\n        HW=[8, 25],\n        local_k=[3, 3],\n    ):\n        super().__init__()\n        self.HW = HW\n        self.dim = dim\n        self.local_mixer = nn.Conv2D(\n            dim,\n            dim,\n            local_k,\n            1,\n            [local_k[0] // 2, local_k[1] // 2],\n            groups=num_heads,\n            weight_attr=ParamAttr(initializer=KaimingNormal()),\n        )\n\n    def forward(self, x):\n        h = self.HW[0]\n        w = self.HW[1]\n        x = x.transpose([0, 2, 1]).reshape([0, self.dim, h, w])\n        x = self.local_mixer(x)\n        x = x.flatten(2).transpose([0, 2, 1])\n        return x\n\n\nclass Attention(nn.Layer):\n    def __init__(\n        self,\n        dim,\n        num_heads=8,\n        mixer=\"Global\",\n        HW=None,\n        local_k=[7, 11],\n        qkv_bias=False,\n        qk_scale=None,\n        attn_drop=0.0,\n        proj_drop=0.0,\n    ):\n        super().__init__()\n        self.num_heads = num_heads\n        self.dim = dim\n        self.head_dim = dim // num_heads\n        self.scale = qk_scale or self.head_dim**-0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias_attr=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n        self.HW = HW\n        if HW is not None:\n            H = HW[0]\n            W = HW[1]\n            self.N = H * W\n            self.C = dim\n        if mixer == \"Local\" and HW is not None:\n            hk = local_k[0]\n            wk = local_k[1]\n            mask = paddle.ones([H * W, H + hk - 1, W + wk - 1], dtype=\"float32\")\n            for h in range(0, H):\n                for w in range(0, W):\n                    mask[h * W + w, h : h + hk, w : w + wk] = 0.0\n            mask_paddle = mask[:, hk // 2 : H + hk // 2, wk // 2 : W + wk // 2].flatten(\n                1\n            )\n            mask_inf = paddle.full([H * W, H * W], \"-inf\", dtype=\"float32\")\n            mask = paddle.where(mask_paddle < 1, mask_paddle, mask_inf)\n            self.mask = mask.unsqueeze([0, 1])\n        self.mixer = mixer\n\n    def forward(self, x):\n        qkv = (\n            self.qkv(x)\n            .reshape((0, -1, 3, self.num_heads, self.head_dim))\n            .transpose((2, 0, 3, 1, 4))\n        )\n        q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n\n        attn = q.matmul(k.transpose((0, 1, 3, 2)))\n        if self.mixer == \"Local\":\n            attn += self.mask\n        attn = nn.functional.softmax(attn, axis=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn.matmul(v)).transpose((0, 2, 1, 3)).reshape((0, -1, self.dim))\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass Block(nn.Layer):\n    def __init__(\n        self,\n        dim,\n        num_heads,\n        mixer=\"Global\",\n        local_mixer=[7, 11],\n        HW=None,\n        mlp_ratio=4.0,\n        qkv_bias=False,\n        qk_scale=None,\n        drop=0.0,\n        attn_drop=0.0,\n        drop_path=0.0,\n        act_layer=nn.GELU,\n        norm_layer=\"nn.LayerNorm\",\n        epsilon=1e-6,\n        prenorm=True,\n    ):\n        super().__init__()\n        if isinstance(norm_layer, str):\n            self.norm1 = eval(norm_layer)(dim, epsilon=epsilon)\n        else:\n            self.norm1 = norm_layer(dim)\n        if mixer == \"Global\" or mixer == \"Local\":\n            self.mixer = Attention(\n                dim,\n                num_heads=num_heads,\n                mixer=mixer,\n                HW=HW,\n                local_k=local_mixer,\n                qkv_bias=qkv_bias,\n                qk_scale=qk_scale,\n                attn_drop=attn_drop,\n                proj_drop=drop,\n            )\n        elif mixer == \"Conv\":\n            self.mixer = ConvMixer(dim, num_heads=num_heads, HW=HW, local_k=local_mixer)\n        else:\n            raise TypeError(\"The mixer must be one of [Global, Local, Conv]\")\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else Identity()\n        if isinstance(norm_layer, str):\n            self.norm2 = eval(norm_layer)(dim, epsilon=epsilon)\n        else:\n            self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp_ratio = mlp_ratio\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=mlp_hidden_dim,\n            act_layer=act_layer,\n            drop=drop,\n        )\n        self.prenorm = prenorm\n\n    def forward(self, x):\n        if self.prenorm:\n            x = self.norm1(x + self.drop_path(self.mixer(x)))\n            x = self.norm2(x + self.drop_path(self.mlp(x)))\n        else:\n            x = x + self.drop_path(self.mixer(self.norm1(x)))\n            x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n\n\nclass PatchEmbed(nn.Layer):\n    \"\"\"Image to Patch Embedding\"\"\"\n\n    def __init__(\n        self,\n        img_size=[32, 100],\n        in_channels=3,\n        embed_dim=768,\n        sub_num=2,\n        patch_size=[4, 4],\n        mode=\"pope\",\n    ):\n        super().__init__()\n        num_patches = (img_size[1] // (2**sub_num)) * (img_size[0] // (2**sub_num))\n        self.img_size = img_size\n        self.num_patches = num_patches\n        self.embed_dim = embed_dim\n        self.norm = None\n        if mode == \"pope\":\n            if sub_num == 2:\n                self.proj = nn.Sequential(\n                    ConvBNLayer(\n                        in_channels=in_channels,\n                        out_channels=embed_dim // 2,\n                        kernel_size=3,\n                        stride=2,\n                        padding=1,\n                        act=nn.GELU,\n                        bias_attr=None,\n                    ),\n                    ConvBNLayer(\n                        in_channels=embed_dim // 2,\n                        out_channels=embed_dim,\n                        kernel_size=3,\n                        stride=2,\n                        padding=1,\n                        act=nn.GELU,\n                        bias_attr=None,\n                    ),\n                )\n            if sub_num == 3:\n                self.proj = nn.Sequential(\n                    ConvBNLayer(\n                        in_channels=in_channels,\n                        out_channels=embed_dim // 4,\n                        kernel_size=3,\n                        stride=2,\n                        padding=1,\n                        act=nn.GELU,\n                        bias_attr=None,\n                    ),\n                    ConvBNLayer(\n                        in_channels=embed_dim // 4,\n                        out_channels=embed_dim // 2,\n                        kernel_size=3,\n                        stride=2,\n                        padding=1,\n                        act=nn.GELU,\n                        bias_attr=None,\n                    ),\n                    ConvBNLayer(\n                        in_channels=embed_dim // 2,\n                        out_channels=embed_dim,\n                        kernel_size=3,\n                        stride=2,\n                        padding=1,\n                        act=nn.GELU,\n                        bias_attr=None,\n                    ),\n                )\n        elif mode == \"linear\":\n            self.proj = nn.Conv2D(\n                1, embed_dim, kernel_size=patch_size, stride=patch_size\n            )\n            self.num_patches = (\n                img_size[0] // patch_size[0] * img_size[1] // patch_size[1]\n            )\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        assert (\n            H == self.img_size[0] and W == self.img_size[1]\n        ), f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x).flatten(2).transpose((0, 2, 1))\n        return x\n\n\nclass SubSample(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        types=\"Pool\",\n        stride=[2, 1],\n        sub_norm=\"nn.LayerNorm\",\n        act=None,\n    ):\n        super().__init__()\n        self.types = types\n        if types == \"Pool\":\n            self.avgpool = nn.AvgPool2D(\n                kernel_size=[3, 5], stride=stride, padding=[1, 2]\n            )\n            self.maxpool = nn.MaxPool2D(\n                kernel_size=[3, 5], stride=stride, padding=[1, 2]\n            )\n            self.proj = nn.Linear(in_channels, out_channels)\n        else:\n            self.conv = nn.Conv2D(\n                in_channels,\n                out_channels,\n                kernel_size=3,\n                stride=stride,\n                padding=1,\n                weight_attr=ParamAttr(initializer=KaimingNormal()),\n            )\n        self.norm = eval(sub_norm)(out_channels)\n        if act is not None:\n            self.act = act()\n        else:\n            self.act = None\n\n    def forward(self, x):\n        if self.types == \"Pool\":\n            x1 = self.avgpool(x)\n            x2 = self.maxpool(x)\n            x = (x1 + x2) * 0.5\n            out = self.proj(x.flatten(2).transpose((0, 2, 1)))\n        else:\n            x = self.conv(x)\n            out = x.flatten(2).transpose((0, 2, 1))\n        out = self.norm(out)\n        if self.act is not None:\n            out = self.act(out)\n\n        return out\n\n\nclass SVTRNet(nn.Layer):\n    def __init__(\n        self,\n        img_size=[32, 100],\n        in_channels=3,\n        embed_dim=[64, 128, 256],\n        depth=[3, 6, 3],\n        num_heads=[2, 4, 8],\n        mixer=[\"Local\"] * 6 + [\"Global\"] * 6,  # Local atten, Global atten, Conv\n        local_mixer=[[7, 11], [7, 11], [7, 11]],\n        patch_merging=\"Conv\",  # Conv, Pool, None\n        mlp_ratio=4,\n        qkv_bias=True,\n        qk_scale=None,\n        drop_rate=0.0,\n        last_drop=0.1,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.1,\n        norm_layer=\"nn.LayerNorm\",\n        sub_norm=\"nn.LayerNorm\",\n        epsilon=1e-6,\n        out_channels=192,\n        out_char_num=25,\n        block_unit=\"Block\",\n        act=\"nn.GELU\",\n        last_stage=True,\n        sub_num=2,\n        prenorm=True,\n        use_lenhead=False,\n        **kwargs,\n    ):\n        super().__init__()\n        self.img_size = img_size\n        self.embed_dim = embed_dim\n        self.out_channels = out_channels\n        self.prenorm = prenorm\n        patch_merging = (\n            None\n            if patch_merging != \"Conv\" and patch_merging != \"Pool\"\n            else patch_merging\n        )\n        self.patch_embed = PatchEmbed(\n            img_size=img_size,\n            in_channels=in_channels,\n            embed_dim=embed_dim[0],\n            sub_num=sub_num,\n        )\n        num_patches = self.patch_embed.num_patches\n        self.HW = [img_size[0] // (2**sub_num), img_size[1] // (2**sub_num)]\n        self.pos_embed = self.create_parameter(\n            shape=[1, num_patches, embed_dim[0]], default_initializer=zeros_\n        )\n        self.add_parameter(\"pos_embed\", self.pos_embed)\n        self.pos_drop = nn.Dropout(p=drop_rate)\n        Block_unit = eval(block_unit)\n\n        dpr = np.linspace(0, drop_path_rate, sum(depth))\n        self.blocks1 = nn.LayerList(\n            [\n                Block_unit(\n                    dim=embed_dim[0],\n                    num_heads=num_heads[0],\n                    mixer=mixer[0 : depth[0]][i],\n                    HW=self.HW,\n                    local_mixer=local_mixer[0],\n                    mlp_ratio=mlp_ratio,\n                    qkv_bias=qkv_bias,\n                    qk_scale=qk_scale,\n                    drop=drop_rate,\n                    act_layer=eval(act),\n                    attn_drop=attn_drop_rate,\n                    drop_path=dpr[0 : depth[0]][i],\n                    norm_layer=norm_layer,\n                    epsilon=epsilon,\n                    prenorm=prenorm,\n                )\n                for i in range(depth[0])\n            ]\n        )\n        if patch_merging is not None:\n            self.sub_sample1 = SubSample(\n                embed_dim[0],\n                embed_dim[1],\n                sub_norm=sub_norm,\n                stride=[2, 1],\n                types=patch_merging,\n            )\n            HW = [self.HW[0] // 2, self.HW[1]]\n        else:\n            HW = self.HW\n        self.patch_merging = patch_merging\n        self.blocks2 = nn.LayerList(\n            [\n                Block_unit(\n                    dim=embed_dim[1],\n                    num_heads=num_heads[1],\n                    mixer=mixer[depth[0] : depth[0] + depth[1]][i],\n                    HW=HW,\n                    local_mixer=local_mixer[1],\n                    mlp_ratio=mlp_ratio,\n                    qkv_bias=qkv_bias,\n                    qk_scale=qk_scale,\n                    drop=drop_rate,\n                    act_layer=eval(act),\n                    attn_drop=attn_drop_rate,\n                    drop_path=dpr[depth[0] : depth[0] + depth[1]][i],\n                    norm_layer=norm_layer,\n                    epsilon=epsilon,\n                    prenorm=prenorm,\n                )\n                for i in range(depth[1])\n            ]\n        )\n        if patch_merging is not None:\n            self.sub_sample2 = SubSample(\n                embed_dim[1],\n                embed_dim[2],\n                sub_norm=sub_norm,\n                stride=[2, 1],\n                types=patch_merging,\n            )\n            HW = [self.HW[0] // 4, self.HW[1]]\n        else:\n            HW = self.HW\n        self.blocks3 = nn.LayerList(\n            [\n                Block_unit(\n                    dim=embed_dim[2],\n                    num_heads=num_heads[2],\n                    mixer=mixer[depth[0] + depth[1] :][i],\n                    HW=HW,\n                    local_mixer=local_mixer[2],\n                    mlp_ratio=mlp_ratio,\n                    qkv_bias=qkv_bias,\n                    qk_scale=qk_scale,\n                    drop=drop_rate,\n                    act_layer=eval(act),\n                    attn_drop=attn_drop_rate,\n                    drop_path=dpr[depth[0] + depth[1] :][i],\n                    norm_layer=norm_layer,\n                    epsilon=epsilon,\n                    prenorm=prenorm,\n                )\n                for i in range(depth[2])\n            ]\n        )\n        self.last_stage = last_stage\n        if last_stage:\n            self.avg_pool = nn.AdaptiveAvgPool2D([1, out_char_num])\n            self.last_conv = nn.Conv2D(\n                in_channels=embed_dim[2],\n                out_channels=self.out_channels,\n                kernel_size=1,\n                stride=1,\n                padding=0,\n                bias_attr=False,\n            )\n            self.hardswish = nn.Hardswish()\n            self.dropout = nn.Dropout(p=last_drop, mode=\"downscale_in_infer\")\n        if not prenorm:\n            self.norm = eval(norm_layer)(embed_dim[-1], epsilon=epsilon)\n        self.use_lenhead = use_lenhead\n        if use_lenhead:\n            self.len_conv = nn.Linear(embed_dim[2], self.out_channels)\n            self.hardswish_len = nn.Hardswish()\n            self.dropout_len = nn.Dropout(p=last_drop, mode=\"downscale_in_infer\")\n\n        trunc_normal_(self.pos_embed)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                zeros_(m.bias)\n        elif isinstance(m, nn.LayerNorm):\n            zeros_(m.bias)\n            ones_(m.weight)\n\n    def forward_features(self, x):\n        x = self.patch_embed(x)\n        x = x + self.pos_embed\n        x = self.pos_drop(x)\n        for blk in self.blocks1:\n            x = blk(x)\n        if self.patch_merging is not None:\n            x = self.sub_sample1(\n                x.transpose([0, 2, 1]).reshape(\n                    [0, self.embed_dim[0], self.HW[0], self.HW[1]]\n                )\n            )\n        for blk in self.blocks2:\n            x = blk(x)\n        if self.patch_merging is not None:\n            x = self.sub_sample2(\n                x.transpose([0, 2, 1]).reshape(\n                    [0, self.embed_dim[1], self.HW[0] // 2, self.HW[1]]\n                )\n            )\n        for blk in self.blocks3:\n            x = blk(x)\n        if not self.prenorm:\n            x = self.norm(x)\n        return x\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        if self.use_lenhead:\n            len_x = self.len_conv(x.mean(1))\n            len_x = self.dropout_len(self.hardswish_len(len_x))\n        if self.last_stage:\n            if self.patch_merging is not None:\n                h = self.HW[0] // 4\n            else:\n                h = self.HW[0]\n            x = self.avg_pool(\n                x.transpose([0, 2, 1]).reshape([0, self.embed_dim[2], h, self.HW[1]])\n            )\n            x = self.last_conv(x)\n            x = self.hardswish(x)\n            x = self.dropout(x)\n        if self.use_lenhead:\n            return x, len_x\n        return x\n", "ppocr/modeling/backbones/rec_densenet.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/LBH1024/CAN/models/densenet.py\n\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport paddle\nimport paddle.nn as nn\nimport paddle.nn.functional as F\n\n\nclass Bottleneck(nn.Layer):\n    def __init__(self, nChannels, growthRate, use_dropout):\n        super(Bottleneck, self).__init__()\n        interChannels = 4 * growthRate\n        self.bn1 = nn.BatchNorm2D(interChannels)\n        self.conv1 = nn.Conv2D(\n            nChannels, interChannels, kernel_size=1, bias_attr=None\n        )  # Xavier initialization\n        self.bn2 = nn.BatchNorm2D(growthRate)\n        self.conv2 = nn.Conv2D(\n            interChannels, growthRate, kernel_size=3, padding=1, bias_attr=None\n        )  # Xavier initialization\n        self.use_dropout = use_dropout\n        self.dropout = nn.Dropout(p=0.2)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        if self.use_dropout:\n            out = self.dropout(out)\n        out = F.relu(self.bn2(self.conv2(out)))\n        if self.use_dropout:\n            out = self.dropout(out)\n        out = paddle.concat([x, out], 1)\n        return out\n\n\nclass SingleLayer(nn.Layer):\n    def __init__(self, nChannels, growthRate, use_dropout):\n        super(SingleLayer, self).__init__()\n        self.bn1 = nn.BatchNorm2D(nChannels)\n        self.conv1 = nn.Conv2D(\n            nChannels, growthRate, kernel_size=3, padding=1, bias_attr=False\n        )\n\n        self.use_dropout = use_dropout\n        self.dropout = nn.Dropout(p=0.2)\n\n    def forward(self, x):\n        out = self.conv1(F.relu(x))\n        if self.use_dropout:\n            out = self.dropout(out)\n\n        out = paddle.concat([x, out], 1)\n        return out\n\n\nclass Transition(nn.Layer):\n    def __init__(self, nChannels, out_channels, use_dropout):\n        super(Transition, self).__init__()\n        self.bn1 = nn.BatchNorm2D(out_channels)\n        self.conv1 = nn.Conv2D(nChannels, out_channels, kernel_size=1, bias_attr=False)\n        self.use_dropout = use_dropout\n        self.dropout = nn.Dropout(p=0.2)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        if self.use_dropout:\n            out = self.dropout(out)\n        out = F.avg_pool2d(out, 2, ceil_mode=True, exclusive=False)\n        return out\n\n\nclass DenseNet(nn.Layer):\n    def __init__(\n        self, growthRate, reduction, bottleneck, use_dropout, input_channel, **kwargs\n    ):\n        super(DenseNet, self).__init__()\n\n        nDenseBlocks = 16\n        nChannels = 2 * growthRate\n\n        self.conv1 = nn.Conv2D(\n            input_channel,\n            nChannels,\n            kernel_size=7,\n            padding=3,\n            stride=2,\n            bias_attr=False,\n        )\n        self.dense1 = self._make_dense(\n            nChannels, growthRate, nDenseBlocks, bottleneck, use_dropout\n        )\n        nChannels += nDenseBlocks * growthRate\n        out_channels = int(math.floor(nChannels * reduction))\n        self.trans1 = Transition(nChannels, out_channels, use_dropout)\n\n        nChannels = out_channels\n        self.dense2 = self._make_dense(\n            nChannels, growthRate, nDenseBlocks, bottleneck, use_dropout\n        )\n        nChannels += nDenseBlocks * growthRate\n        out_channels = int(math.floor(nChannels * reduction))\n        self.trans2 = Transition(nChannels, out_channels, use_dropout)\n\n        nChannels = out_channels\n        self.dense3 = self._make_dense(\n            nChannels, growthRate, nDenseBlocks, bottleneck, use_dropout\n        )\n        self.out_channels = out_channels\n\n    def _make_dense(self, nChannels, growthRate, nDenseBlocks, bottleneck, use_dropout):\n        layers = []\n        for i in range(int(nDenseBlocks)):\n            if bottleneck:\n                layers.append(Bottleneck(nChannels, growthRate, use_dropout))\n            else:\n                layers.append(SingleLayer(nChannels, growthRate, use_dropout))\n            nChannels += growthRate\n        return nn.Sequential(*layers)\n\n    def forward(self, inputs):\n        x, x_m, y = inputs\n        out = self.conv1(x)\n        out = F.relu(out)\n        out = F.max_pool2d(out, 2, ceil_mode=True)\n        out = self.dense1(out)\n        out = self.trans1(out)\n        out = self.dense2(out)\n        out = self.trans2(out)\n        out = self.dense3(out)\n        return out, x_m, y\n", "ppocr/modeling/backbones/rec_resnet_31.py": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/open-mmlab/mmocr/blob/main/mmocr/models/textrecog/layers/conv_layer.py\nhttps://github.com/open-mmlab/mmocr/blob/main/mmocr/models/textrecog/backbones/resnet31_ocr.py\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nfrom paddle import ParamAttr\nimport paddle.nn as nn\nimport paddle.nn.functional as F\nimport numpy as np\n\n__all__ = [\"ResNet31\"]\n\n\ndef conv3x3(in_channel, out_channel, stride=1, conv_weight_attr=None):\n    return nn.Conv2D(\n        in_channel,\n        out_channel,\n        kernel_size=3,\n        stride=stride,\n        padding=1,\n        weight_attr=conv_weight_attr,\n        bias_attr=False,\n    )\n\n\nclass BasicBlock(nn.Layer):\n    expansion = 1\n\n    def __init__(\n        self,\n        in_channels,\n        channels,\n        stride=1,\n        downsample=False,\n        conv_weight_attr=None,\n        bn_weight_attr=None,\n    ):\n        super().__init__()\n        self.conv1 = conv3x3(\n            in_channels, channels, stride, conv_weight_attr=conv_weight_attr\n        )\n        self.bn1 = nn.BatchNorm2D(channels, weight_attr=bn_weight_attr)\n        self.relu = nn.ReLU()\n        self.conv2 = conv3x3(channels, channels, conv_weight_attr=conv_weight_attr)\n        self.bn2 = nn.BatchNorm2D(channels, weight_attr=bn_weight_attr)\n        self.downsample = downsample\n        if downsample:\n            self.downsample = nn.Sequential(\n                nn.Conv2D(\n                    in_channels,\n                    channels * self.expansion,\n                    1,\n                    stride,\n                    weight_attr=conv_weight_attr,\n                    bias_attr=False,\n                ),\n                nn.BatchNorm2D(channels * self.expansion, weight_attr=bn_weight_attr),\n            )\n        else:\n            self.downsample = nn.Sequential()\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet31(nn.Layer):\n    \"\"\"\n    Args:\n        in_channels (int): Number of channels of input image tensor.\n        layers (list[int]): List of BasicBlock number for each stage.\n        channels (list[int]): List of out_channels of Conv2d layer.\n        out_indices (None | Sequence[int]): Indices of output stages.\n        last_stage_pool (bool): If True, add `MaxPool2d` layer to last stage.\n        init_type (None | str): the config to control the initialization.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels=3,\n        layers=[1, 2, 5, 3],\n        channels=[64, 128, 256, 256, 512, 512, 512],\n        out_indices=None,\n        last_stage_pool=False,\n        init_type=None,\n    ):\n        super(ResNet31, self).__init__()\n        assert isinstance(in_channels, int)\n        assert isinstance(last_stage_pool, bool)\n\n        self.out_indices = out_indices\n        self.last_stage_pool = last_stage_pool\n\n        conv_weight_attr = None\n        bn_weight_attr = None\n\n        if init_type is not None:\n            support_dict = [\"KaimingNormal\"]\n            assert init_type in support_dict, Exception(\n                \"resnet31 only support {}\".format(support_dict)\n            )\n            conv_weight_attr = nn.initializer.KaimingNormal()\n            bn_weight_attr = ParamAttr(\n                initializer=nn.initializer.Uniform(), learning_rate=1\n            )\n\n        # conv 1 (Conv Conv)\n        self.conv1_1 = nn.Conv2D(\n            in_channels,\n            channels[0],\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            weight_attr=conv_weight_attr,\n        )\n        self.bn1_1 = nn.BatchNorm2D(channels[0], weight_attr=bn_weight_attr)\n        self.relu1_1 = nn.ReLU()\n\n        self.conv1_2 = nn.Conv2D(\n            channels[0],\n            channels[1],\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            weight_attr=conv_weight_attr,\n        )\n        self.bn1_2 = nn.BatchNorm2D(channels[1], weight_attr=bn_weight_attr)\n        self.relu1_2 = nn.ReLU()\n\n        # conv 2 (Max-pooling, Residual block, Conv)\n        self.pool2 = nn.MaxPool2D(kernel_size=2, stride=2, padding=0, ceil_mode=True)\n        self.block2 = self._make_layer(\n            channels[1],\n            channels[2],\n            layers[0],\n            conv_weight_attr=conv_weight_attr,\n            bn_weight_attr=bn_weight_attr,\n        )\n        self.conv2 = nn.Conv2D(\n            channels[2],\n            channels[2],\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            weight_attr=conv_weight_attr,\n        )\n        self.bn2 = nn.BatchNorm2D(channels[2], weight_attr=bn_weight_attr)\n        self.relu2 = nn.ReLU()\n\n        # conv 3 (Max-pooling, Residual block, Conv)\n        self.pool3 = nn.MaxPool2D(kernel_size=2, stride=2, padding=0, ceil_mode=True)\n        self.block3 = self._make_layer(\n            channels[2],\n            channels[3],\n            layers[1],\n            conv_weight_attr=conv_weight_attr,\n            bn_weight_attr=bn_weight_attr,\n        )\n        self.conv3 = nn.Conv2D(\n            channels[3],\n            channels[3],\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            weight_attr=conv_weight_attr,\n        )\n        self.bn3 = nn.BatchNorm2D(channels[3], weight_attr=bn_weight_attr)\n        self.relu3 = nn.ReLU()\n\n        # conv 4 (Max-pooling, Residual block, Conv)\n        self.pool4 = nn.MaxPool2D(\n            kernel_size=(2, 1), stride=(2, 1), padding=0, ceil_mode=True\n        )\n        self.block4 = self._make_layer(\n            channels[3],\n            channels[4],\n            layers[2],\n            conv_weight_attr=conv_weight_attr,\n            bn_weight_attr=bn_weight_attr,\n        )\n        self.conv4 = nn.Conv2D(\n            channels[4],\n            channels[4],\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            weight_attr=conv_weight_attr,\n        )\n        self.bn4 = nn.BatchNorm2D(channels[4], weight_attr=bn_weight_attr)\n        self.relu4 = nn.ReLU()\n\n        # conv 5 ((Max-pooling), Residual block, Conv)\n        self.pool5 = None\n        if self.last_stage_pool:\n            self.pool5 = nn.MaxPool2D(\n                kernel_size=2, stride=2, padding=0, ceil_mode=True\n            )\n        self.block5 = self._make_layer(\n            channels[4],\n            channels[5],\n            layers[3],\n            conv_weight_attr=conv_weight_attr,\n            bn_weight_attr=bn_weight_attr,\n        )\n        self.conv5 = nn.Conv2D(\n            channels[5],\n            channels[5],\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            weight_attr=conv_weight_attr,\n        )\n        self.bn5 = nn.BatchNorm2D(channels[5], weight_attr=bn_weight_attr)\n        self.relu5 = nn.ReLU()\n\n        self.out_channels = channels[-1]\n\n    def _make_layer(\n        self,\n        input_channels,\n        output_channels,\n        blocks,\n        conv_weight_attr=None,\n        bn_weight_attr=None,\n    ):\n        layers = []\n        for _ in range(blocks):\n            downsample = None\n            if input_channels != output_channels:\n                downsample = nn.Sequential(\n                    nn.Conv2D(\n                        input_channels,\n                        output_channels,\n                        kernel_size=1,\n                        stride=1,\n                        weight_attr=conv_weight_attr,\n                        bias_attr=False,\n                    ),\n                    nn.BatchNorm2D(output_channels, weight_attr=bn_weight_attr),\n                )\n\n            layers.append(\n                BasicBlock(\n                    input_channels,\n                    output_channels,\n                    downsample=downsample,\n                    conv_weight_attr=conv_weight_attr,\n                    bn_weight_attr=bn_weight_attr,\n                )\n            )\n            input_channels = output_channels\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1_1(x)\n        x = self.bn1_1(x)\n        x = self.relu1_1(x)\n\n        x = self.conv1_2(x)\n        x = self.bn1_2(x)\n        x = self.relu1_2(x)\n\n        outs = []\n        for i in range(4):\n            layer_index = i + 2\n            pool_layer = getattr(self, f\"pool{layer_index}\")\n            block_layer = getattr(self, f\"block{layer_index}\")\n            conv_layer = getattr(self, f\"conv{layer_index}\")\n            bn_layer = getattr(self, f\"bn{layer_index}\")\n            relu_layer = getattr(self, f\"relu{layer_index}\")\n\n            if pool_layer is not None:\n                x = pool_layer(x)\n            x = block_layer(x)\n            x = conv_layer(x)\n            x = bn_layer(x)\n            x = relu_layer(x)\n\n            outs.append(x)\n\n        if self.out_indices is not None:\n            return tuple([outs[i] for i in self.out_indices])\n\n        return x\n", "ppocr/modeling/backbones/det_pp_lcnet_v2.py": "# copyright (c) 2024 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import, division, print_function\nimport os\n\nimport paddle\nimport paddle.nn as nn\nimport paddle.nn.functional as F\nfrom paddle import ParamAttr\nfrom paddle.nn import AdaptiveAvgPool2D, BatchNorm2D, Conv2D, Dropout, Linear\nfrom paddle.regularizer import L2Decay\nfrom paddle.nn.initializer import KaimingNormal\nfrom paddle.utils.download import get_path_from_url\n\nMODEL_URLS = {\n    \"PPLCNetV2_small\": \"https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/legendary_models/PPLCNetV2_small_ssld_pretrained.pdparams\",\n    \"PPLCNetV2_base\": \"https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/legendary_models/PPLCNetV2_base_ssld_pretrained.pdparams\",\n    \"PPLCNetV2_large\": \"https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/legendary_models/PPLCNetV2_large_ssld_pretrained.pdparams\",\n}\n\n__all__ = list(MODEL_URLS.keys())\n\nNET_CONFIG = {\n    # in_channels, kernel_size, split_pw, use_rep, use_se, use_shortcut\n    \"stage1\": [64, 3, False, False, False, False],\n    \"stage2\": [128, 3, False, False, False, False],\n    \"stage3\": [256, 5, True, True, True, False],\n    \"stage4\": [512, 5, False, True, False, True],\n}\n\n\ndef make_divisible(v, divisor=8, min_value=None):\n    if min_value is None:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    if new_v < 0.9 * v:\n        new_v += divisor\n    return new_v\n\n\nclass ConvBNLayer(nn.Layer):\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride, groups=1, use_act=True\n    ):\n        super().__init__()\n        self.use_act = use_act\n        self.conv = Conv2D(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=(kernel_size - 1) // 2,\n            groups=groups,\n            weight_attr=ParamAttr(initializer=KaimingNormal()),\n            bias_attr=False,\n        )\n\n        self.bn = BatchNorm2D(\n            out_channels,\n            weight_attr=ParamAttr(regularizer=L2Decay(0.0)),\n            bias_attr=ParamAttr(regularizer=L2Decay(0.0)),\n        )\n        if self.use_act:\n            self.act = nn.ReLU()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        if self.use_act:\n            x = self.act(x)\n        return x\n\n\nclass SEModule(nn.Layer):\n    def __init__(self, channel, reduction=4):\n        super().__init__()\n        self.avg_pool = AdaptiveAvgPool2D(1)\n        self.conv1 = Conv2D(\n            in_channels=channel,\n            out_channels=channel // reduction,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n        )\n        self.relu = nn.ReLU()\n        self.conv2 = Conv2D(\n            in_channels=channel // reduction,\n            out_channels=channel,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n        )\n        self.hardsigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        identity = x\n        x = self.avg_pool(x)\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.hardsigmoid(x)\n        x = paddle.multiply(x=identity, y=x)\n        return x\n\n\nclass RepDepthwiseSeparable(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        stride,\n        dw_size=3,\n        split_pw=False,\n        use_rep=False,\n        use_se=False,\n        use_shortcut=False,\n    ):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.is_repped = False\n\n        self.dw_size = dw_size\n        self.split_pw = split_pw\n        self.use_rep = use_rep\n        self.use_se = use_se\n        self.use_shortcut = (\n            True\n            if use_shortcut and stride == 1 and in_channels == out_channels\n            else False\n        )\n\n        if self.use_rep:\n            self.dw_conv_list = nn.LayerList()\n            for kernel_size in range(self.dw_size, 0, -2):\n                if kernel_size == 1 and stride != 1:\n                    continue\n                dw_conv = ConvBNLayer(\n                    in_channels=in_channels,\n                    out_channels=in_channels,\n                    kernel_size=kernel_size,\n                    stride=stride,\n                    groups=in_channels,\n                    use_act=False,\n                )\n                self.dw_conv_list.append(dw_conv)\n            self.dw_conv = nn.Conv2D(\n                in_channels=in_channels,\n                out_channels=in_channels,\n                kernel_size=dw_size,\n                stride=stride,\n                padding=(dw_size - 1) // 2,\n                groups=in_channels,\n            )\n        else:\n            self.dw_conv = ConvBNLayer(\n                in_channels=in_channels,\n                out_channels=in_channels,\n                kernel_size=dw_size,\n                stride=stride,\n                groups=in_channels,\n            )\n\n        self.act = nn.ReLU()\n\n        if use_se:\n            self.se = SEModule(in_channels)\n\n        if self.split_pw:\n            pw_ratio = 0.5\n            self.pw_conv_1 = ConvBNLayer(\n                in_channels=in_channels,\n                kernel_size=1,\n                out_channels=int(out_channels * pw_ratio),\n                stride=1,\n            )\n            self.pw_conv_2 = ConvBNLayer(\n                in_channels=int(out_channels * pw_ratio),\n                kernel_size=1,\n                out_channels=out_channels,\n                stride=1,\n            )\n        else:\n            self.pw_conv = ConvBNLayer(\n                in_channels=in_channels,\n                kernel_size=1,\n                out_channels=out_channels,\n                stride=1,\n            )\n\n    def forward(self, x):\n        if self.use_rep:\n            input_x = x\n            if self.is_repped:\n                x = self.act(self.dw_conv(x))\n            else:\n                y = self.dw_conv_list[0](x)\n                for dw_conv in self.dw_conv_list[1:]:\n                    y += dw_conv(x)\n                x = self.act(y)\n        else:\n            x = self.dw_conv(x)\n\n        if self.use_se:\n            x = self.se(x)\n        if self.split_pw:\n            x = self.pw_conv_1(x)\n            x = self.pw_conv_2(x)\n        else:\n            x = self.pw_conv(x)\n        if self.use_shortcut:\n            x = x + input_x\n        return x\n\n    def re_parameterize(self):\n        if self.use_rep:\n            self.is_repped = True\n            kernel, bias = self._get_equivalent_kernel_bias()\n            self.dw_conv.weight.set_value(kernel)\n            self.dw_conv.bias.set_value(bias)\n\n    def _get_equivalent_kernel_bias(self):\n        kernel_sum = 0\n        bias_sum = 0\n        for dw_conv in self.dw_conv_list:\n            kernel, bias = self._fuse_bn_tensor(dw_conv)\n            kernel = self._pad_tensor(kernel, to_size=self.dw_size)\n            kernel_sum += kernel\n            bias_sum += bias\n        return kernel_sum, bias_sum\n\n    def _fuse_bn_tensor(self, branch):\n        kernel = branch.conv.weight\n        running_mean = branch.bn._mean\n        running_var = branch.bn._variance\n        gamma = branch.bn.weight\n        beta = branch.bn.bias\n        eps = branch.bn._epsilon\n        std = (running_var + eps).sqrt()\n        t = (gamma / std).reshape((-1, 1, 1, 1))\n        return kernel * t, beta - running_mean * gamma / std\n\n    def _pad_tensor(self, tensor, to_size):\n        from_size = tensor.shape[-1]\n        if from_size == to_size:\n            return tensor\n        pad = (to_size - from_size) // 2\n        return F.pad(tensor, [pad, pad, pad, pad])\n\n\nclass PPLCNetV2(nn.Layer):\n    def __init__(self, scale, depths, out_indx=[1, 2, 3, 4], **kwargs):\n        super().__init__(**kwargs)\n        self.scale = scale\n        self.out_channels = [\n            # int(NET_CONFIG[\"blocks3\"][-1][2] * scale),\n            int(NET_CONFIG[\"stage1\"][0] * scale * 2),\n            int(NET_CONFIG[\"stage2\"][0] * scale * 2),\n            int(NET_CONFIG[\"stage3\"][0] * scale * 2),\n            int(NET_CONFIG[\"stage4\"][0] * scale * 2),\n        ]\n        self.stem = nn.Sequential(\n            *[\n                ConvBNLayer(\n                    in_channels=3,\n                    kernel_size=3,\n                    out_channels=make_divisible(32 * scale),\n                    stride=2,\n                ),\n                RepDepthwiseSeparable(\n                    in_channels=make_divisible(32 * scale),\n                    out_channels=make_divisible(64 * scale),\n                    stride=1,\n                    dw_size=3,\n                ),\n            ]\n        )\n        self.out_indx = out_indx\n        # stages\n        self.stages = nn.LayerList()\n        for depth_idx, k in enumerate(NET_CONFIG):\n            (\n                in_channels,\n                kernel_size,\n                split_pw,\n                use_rep,\n                use_se,\n                use_shortcut,\n            ) = NET_CONFIG[k]\n            self.stages.append(\n                nn.Sequential(\n                    *[\n                        RepDepthwiseSeparable(\n                            in_channels=make_divisible(\n                                (in_channels if i == 0 else in_channels * 2) * scale\n                            ),\n                            out_channels=make_divisible(in_channels * 2 * scale),\n                            stride=2 if i == 0 else 1,\n                            dw_size=kernel_size,\n                            split_pw=split_pw,\n                            use_rep=use_rep,\n                            use_se=use_se,\n                            use_shortcut=use_shortcut,\n                        )\n                        for i in range(depths[depth_idx])\n                    ]\n                )\n            )\n\n        # if pretrained:\n        self._load_pretrained(MODEL_URLS[\"PPLCNetV2_base\"], use_ssld=True)\n\n    def forward(self, x):\n        x = self.stem(x)\n        i = 1\n        outs = []\n        for stage in self.stages:\n            x = stage(x)\n            if i in self.out_indx:\n                outs.append(x)\n            i += 1\n        return outs\n\n    def _load_pretrained(self, pretrained_url, use_ssld=False):\n        print(pretrained_url)\n        local_weight_path = get_path_from_url(\n            pretrained_url, os.path.expanduser(\"~/.paddleclas/weights\")\n        )\n        param_state_dict = paddle.load(local_weight_path)\n        self.set_dict(param_state_dict)\n        print(\"load pretrain ssd success!\")\n        return\n\n\ndef PPLCNetV2_base(in_channels=3, **kwargs):\n    \"\"\"\n    PPLCNetV2_base\n    Args:\n        pretrained: bool=False or str. If `True` load pretrained parameters, `False` otherwise.\n                    If str, means the path of the pretrained model.\n        use_ssld: bool=False. Whether using distillation pretrained model when pretrained=True.\n    Returns:\n        model: nn.Layer. Specific `PPLCNetV2_base` model depends on args.\n    \"\"\"\n    model = PPLCNetV2(scale=1.0, depths=[2, 2, 6, 2], **kwargs)\n    return model\n", "ppocr/modeling/backbones/rec_resnet_rfl.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/hikopensource/DAVAR-Lab-OCR/blob/main/davarocr/davar_rcg/models/backbones/ResNetRFL.py\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nimport paddle.nn as nn\n\nfrom paddle.nn.initializer import TruncatedNormal, Constant, Normal, KaimingNormal\n\nkaiming_init_ = KaimingNormal()\nzeros_ = Constant(value=0.0)\nones_ = Constant(value=1.0)\n\n\nclass BasicBlock(nn.Layer):\n    \"\"\"Res-net Basic Block\"\"\"\n\n    expansion = 1\n\n    def __init__(\n        self, inplanes, planes, stride=1, downsample=None, norm_type=\"BN\", **kwargs\n    ):\n        \"\"\"\n        Args:\n            inplanes (int): input channel\n            planes (int): channels of the middle feature\n            stride (int): stride of the convolution\n            downsample (int): type of the down_sample\n            norm_type (str): type of the normalization\n            **kwargs (None): backup parameter\n        \"\"\"\n        super(BasicBlock, self).__init__()\n        self.conv1 = self._conv3x3(inplanes, planes)\n        self.bn1 = nn.BatchNorm(planes)\n        self.conv2 = self._conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm(planes)\n        self.relu = nn.ReLU()\n        self.downsample = downsample\n        self.stride = stride\n\n    def _conv3x3(self, in_planes, out_planes, stride=1):\n        return nn.Conv2D(\n            in_planes,\n            out_planes,\n            kernel_size=3,\n            stride=stride,\n            padding=1,\n            bias_attr=False,\n        )\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNetRFL(nn.Layer):\n    def __init__(self, in_channels, out_channels=512, use_cnt=True, use_seq=True):\n        \"\"\"\n\n        Args:\n            in_channels (int): input channel\n            out_channels (int): output channel\n        \"\"\"\n        super(ResNetRFL, self).__init__()\n        assert use_cnt or use_seq\n        self.use_cnt, self.use_seq = use_cnt, use_seq\n        self.backbone = RFLBase(in_channels)\n\n        self.out_channels = out_channels\n        self.out_channels_block = [\n            int(self.out_channels / 4),\n            int(self.out_channels / 2),\n            self.out_channels,\n            self.out_channels,\n        ]\n        block = BasicBlock\n        layers = [1, 2, 5, 3]\n        self.inplanes = int(self.out_channels // 2)\n\n        self.relu = nn.ReLU()\n        if self.use_seq:\n            self.maxpool3 = nn.MaxPool2D(kernel_size=2, stride=(2, 1), padding=(0, 1))\n            self.layer3 = self._make_layer(\n                block, self.out_channels_block[2], layers[2], stride=1\n            )\n            self.conv3 = nn.Conv2D(\n                self.out_channels_block[2],\n                self.out_channels_block[2],\n                kernel_size=3,\n                stride=1,\n                padding=1,\n                bias_attr=False,\n            )\n            self.bn3 = nn.BatchNorm(self.out_channels_block[2])\n\n            self.layer4 = self._make_layer(\n                block, self.out_channels_block[3], layers[3], stride=1\n            )\n            self.conv4_1 = nn.Conv2D(\n                self.out_channels_block[3],\n                self.out_channels_block[3],\n                kernel_size=2,\n                stride=(2, 1),\n                padding=(0, 1),\n                bias_attr=False,\n            )\n            self.bn4_1 = nn.BatchNorm(self.out_channels_block[3])\n            self.conv4_2 = nn.Conv2D(\n                self.out_channels_block[3],\n                self.out_channels_block[3],\n                kernel_size=2,\n                stride=1,\n                padding=0,\n                bias_attr=False,\n            )\n            self.bn4_2 = nn.BatchNorm(self.out_channels_block[3])\n\n        if self.use_cnt:\n            self.inplanes = int(self.out_channels // 2)\n            self.v_maxpool3 = nn.MaxPool2D(kernel_size=2, stride=(2, 1), padding=(0, 1))\n            self.v_layer3 = self._make_layer(\n                block, self.out_channels_block[2], layers[2], stride=1\n            )\n            self.v_conv3 = nn.Conv2D(\n                self.out_channels_block[2],\n                self.out_channels_block[2],\n                kernel_size=3,\n                stride=1,\n                padding=1,\n                bias_attr=False,\n            )\n            self.v_bn3 = nn.BatchNorm(self.out_channels_block[2])\n\n            self.v_layer4 = self._make_layer(\n                block, self.out_channels_block[3], layers[3], stride=1\n            )\n            self.v_conv4_1 = nn.Conv2D(\n                self.out_channels_block[3],\n                self.out_channels_block[3],\n                kernel_size=2,\n                stride=(2, 1),\n                padding=(0, 1),\n                bias_attr=False,\n            )\n            self.v_bn4_1 = nn.BatchNorm(self.out_channels_block[3])\n            self.v_conv4_2 = nn.Conv2D(\n                self.out_channels_block[3],\n                self.out_channels_block[3],\n                kernel_size=2,\n                stride=1,\n                padding=0,\n                bias_attr=False,\n            )\n            self.v_bn4_2 = nn.BatchNorm(self.out_channels_block[3])\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2D(\n                    self.inplanes,\n                    planes * block.expansion,\n                    kernel_size=1,\n                    stride=stride,\n                    bias_attr=False,\n                ),\n                nn.BatchNorm(planes * block.expansion),\n            )\n\n        layers = list()\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, inputs):\n        x_1 = self.backbone(inputs)\n\n        if self.use_cnt:\n            v_x = self.v_maxpool3(x_1)\n            v_x = self.v_layer3(v_x)\n            v_x = self.v_conv3(v_x)\n            v_x = self.v_bn3(v_x)\n            visual_feature_2 = self.relu(v_x)\n\n            v_x = self.v_layer4(visual_feature_2)\n            v_x = self.v_conv4_1(v_x)\n            v_x = self.v_bn4_1(v_x)\n            v_x = self.relu(v_x)\n            v_x = self.v_conv4_2(v_x)\n            v_x = self.v_bn4_2(v_x)\n            visual_feature_3 = self.relu(v_x)\n        else:\n            visual_feature_3 = None\n        if self.use_seq:\n            x = self.maxpool3(x_1)\n            x = self.layer3(x)\n            x = self.conv3(x)\n            x = self.bn3(x)\n            x_2 = self.relu(x)\n\n            x = self.layer4(x_2)\n            x = self.conv4_1(x)\n            x = self.bn4_1(x)\n            x = self.relu(x)\n            x = self.conv4_2(x)\n            x = self.bn4_2(x)\n            x_3 = self.relu(x)\n        else:\n            x_3 = None\n\n        return [visual_feature_3, x_3]\n\n\nclass ResNetBase(nn.Layer):\n    def __init__(self, in_channels, out_channels, block, layers):\n        super(ResNetBase, self).__init__()\n\n        self.out_channels_block = [\n            int(out_channels / 4),\n            int(out_channels / 2),\n            out_channels,\n            out_channels,\n        ]\n\n        self.inplanes = int(out_channels / 8)\n        self.conv0_1 = nn.Conv2D(\n            in_channels,\n            int(out_channels / 16),\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            bias_attr=False,\n        )\n        self.bn0_1 = nn.BatchNorm(int(out_channels / 16))\n        self.conv0_2 = nn.Conv2D(\n            int(out_channels / 16),\n            self.inplanes,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            bias_attr=False,\n        )\n        self.bn0_2 = nn.BatchNorm(self.inplanes)\n        self.relu = nn.ReLU()\n\n        self.maxpool1 = nn.MaxPool2D(kernel_size=2, stride=2, padding=0)\n        self.layer1 = self._make_layer(block, self.out_channels_block[0], layers[0])\n        self.conv1 = nn.Conv2D(\n            self.out_channels_block[0],\n            self.out_channels_block[0],\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            bias_attr=False,\n        )\n        self.bn1 = nn.BatchNorm(self.out_channels_block[0])\n\n        self.maxpool2 = nn.MaxPool2D(kernel_size=2, stride=2, padding=0)\n        self.layer2 = self._make_layer(\n            block, self.out_channels_block[1], layers[1], stride=1\n        )\n        self.conv2 = nn.Conv2D(\n            self.out_channels_block[1],\n            self.out_channels_block[1],\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            bias_attr=False,\n        )\n        self.bn2 = nn.BatchNorm(self.out_channels_block[1])\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2D(\n                    self.inplanes,\n                    planes * block.expansion,\n                    kernel_size=1,\n                    stride=stride,\n                    bias_attr=False,\n                ),\n                nn.BatchNorm(planes * block.expansion),\n            )\n\n        layers = list()\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv0_1(x)\n        x = self.bn0_1(x)\n        x = self.relu(x)\n        x = self.conv0_2(x)\n        x = self.bn0_2(x)\n        x = self.relu(x)\n\n        x = self.maxpool1(x)\n        x = self.layer1(x)\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.maxpool2(x)\n        x = self.layer2(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n\n        return x\n\n\nclass RFLBase(nn.Layer):\n    \"\"\"Reciprocal feature learning share backbone network\"\"\"\n\n    def __init__(self, in_channels, out_channels=512):\n        super(RFLBase, self).__init__()\n        self.ConvNet = ResNetBase(in_channels, out_channels, BasicBlock, [1, 2, 5, 3])\n\n    def forward(self, inputs):\n        return self.ConvNet(inputs)\n", "ppocr/modeling/backbones/table_master_resnet.py": "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/JiaquanYe/TableMASTER-mmocr/blob/master/mmocr/models/textrecog/backbones/table_resnet_extra.py\n\"\"\"\n\nimport paddle\nimport paddle.nn as nn\nimport paddle.nn.functional as F\n\n\nclass BasicBlock(nn.Layer):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, gcb_config=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2D(\n            inplanes, planes, kernel_size=3, stride=stride, padding=1, bias_attr=False\n        )\n        self.bn1 = nn.BatchNorm2D(planes, momentum=0.9)\n        self.relu = nn.ReLU()\n        self.conv2 = nn.Conv2D(\n            planes, planes, kernel_size=3, stride=1, padding=1, bias_attr=False\n        )\n        self.bn2 = nn.BatchNorm2D(planes, momentum=0.9)\n        self.downsample = downsample\n        self.stride = stride\n        self.gcb_config = gcb_config\n\n        if self.gcb_config is not None:\n            gcb_ratio = gcb_config[\"ratio\"]\n            gcb_headers = gcb_config[\"headers\"]\n            att_scale = gcb_config[\"att_scale\"]\n            fusion_type = gcb_config[\"fusion_type\"]\n            self.context_block = MultiAspectGCAttention(\n                inplanes=planes,\n                ratio=gcb_ratio,\n                headers=gcb_headers,\n                att_scale=att_scale,\n                fusion_type=fusion_type,\n            )\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.gcb_config is not None:\n            out = self.context_block(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\ndef get_gcb_config(gcb_config, layer):\n    if gcb_config is None or not gcb_config[\"layers\"][layer]:\n        return None\n    else:\n        return gcb_config\n\n\nclass TableResNetExtra(nn.Layer):\n    def __init__(self, layers, in_channels=3, gcb_config=None):\n        assert len(layers) >= 4\n\n        super(TableResNetExtra, self).__init__()\n        self.inplanes = 128\n        self.conv1 = nn.Conv2D(\n            in_channels, 64, kernel_size=3, stride=1, padding=1, bias_attr=False\n        )\n        self.bn1 = nn.BatchNorm2D(64)\n        self.relu1 = nn.ReLU()\n\n        self.conv2 = nn.Conv2D(\n            64, 128, kernel_size=3, stride=1, padding=1, bias_attr=False\n        )\n        self.bn2 = nn.BatchNorm2D(128)\n        self.relu2 = nn.ReLU()\n\n        self.maxpool1 = nn.MaxPool2D(kernel_size=2, stride=2)\n\n        self.layer1 = self._make_layer(\n            BasicBlock,\n            256,\n            layers[0],\n            stride=1,\n            gcb_config=get_gcb_config(gcb_config, 0),\n        )\n\n        self.conv3 = nn.Conv2D(\n            256, 256, kernel_size=3, stride=1, padding=1, bias_attr=False\n        )\n        self.bn3 = nn.BatchNorm2D(256)\n        self.relu3 = nn.ReLU()\n\n        self.maxpool2 = nn.MaxPool2D(kernel_size=2, stride=2)\n\n        self.layer2 = self._make_layer(\n            BasicBlock,\n            256,\n            layers[1],\n            stride=1,\n            gcb_config=get_gcb_config(gcb_config, 1),\n        )\n\n        self.conv4 = nn.Conv2D(\n            256, 256, kernel_size=3, stride=1, padding=1, bias_attr=False\n        )\n        self.bn4 = nn.BatchNorm2D(256)\n        self.relu4 = nn.ReLU()\n\n        self.maxpool3 = nn.MaxPool2D(kernel_size=2, stride=2)\n\n        self.layer3 = self._make_layer(\n            BasicBlock,\n            512,\n            layers[2],\n            stride=1,\n            gcb_config=get_gcb_config(gcb_config, 2),\n        )\n\n        self.conv5 = nn.Conv2D(\n            512, 512, kernel_size=3, stride=1, padding=1, bias_attr=False\n        )\n        self.bn5 = nn.BatchNorm2D(512)\n        self.relu5 = nn.ReLU()\n\n        self.layer4 = self._make_layer(\n            BasicBlock,\n            512,\n            layers[3],\n            stride=1,\n            gcb_config=get_gcb_config(gcb_config, 3),\n        )\n\n        self.conv6 = nn.Conv2D(\n            512, 512, kernel_size=3, stride=1, padding=1, bias_attr=False\n        )\n        self.bn6 = nn.BatchNorm2D(512)\n        self.relu6 = nn.ReLU()\n\n        self.out_channels = [256, 256, 512]\n\n    def _make_layer(self, block, planes, blocks, stride=1, gcb_config=None):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2D(\n                    self.inplanes,\n                    planes * block.expansion,\n                    kernel_size=1,\n                    stride=stride,\n                    bias_attr=False,\n                ),\n                nn.BatchNorm2D(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(\n            block(self.inplanes, planes, stride, downsample, gcb_config=gcb_config)\n        )\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        f = []\n        x = self.conv1(x)\n\n        x = self.bn1(x)\n        x = self.relu1(x)\n\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu2(x)\n\n        x = self.maxpool1(x)\n        x = self.layer1(x)\n\n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = self.relu3(x)\n        f.append(x)\n\n        x = self.maxpool2(x)\n        x = self.layer2(x)\n\n        x = self.conv4(x)\n        x = self.bn4(x)\n        x = self.relu4(x)\n        f.append(x)\n\n        x = self.maxpool3(x)\n\n        x = self.layer3(x)\n        x = self.conv5(x)\n        x = self.bn5(x)\n        x = self.relu5(x)\n\n        x = self.layer4(x)\n        x = self.conv6(x)\n        x = self.bn6(x)\n        x = self.relu6(x)\n        f.append(x)\n        return f\n\n\nclass MultiAspectGCAttention(nn.Layer):\n    def __init__(\n        self,\n        inplanes,\n        ratio,\n        headers,\n        pooling_type=\"att\",\n        att_scale=False,\n        fusion_type=\"channel_add\",\n    ):\n        super(MultiAspectGCAttention, self).__init__()\n        assert pooling_type in [\"avg\", \"att\"]\n\n        assert fusion_type in [\"channel_add\", \"channel_mul\", \"channel_concat\"]\n        assert (\n            inplanes % headers == 0 and inplanes >= 8\n        )  # inplanes must be divided by headers evenly\n\n        self.headers = headers\n        self.inplanes = inplanes\n        self.ratio = ratio\n        self.planes = int(inplanes * ratio)\n        self.pooling_type = pooling_type\n        self.fusion_type = fusion_type\n        self.att_scale = False\n\n        self.single_header_inplanes = int(inplanes / headers)\n\n        if pooling_type == \"att\":\n            self.conv_mask = nn.Conv2D(self.single_header_inplanes, 1, kernel_size=1)\n            self.softmax = nn.Softmax(axis=2)\n        else:\n            self.avg_pool = nn.AdaptiveAvgPool2D(1)\n\n        if fusion_type == \"channel_add\":\n            self.channel_add_conv = nn.Sequential(\n                nn.Conv2D(self.inplanes, self.planes, kernel_size=1),\n                nn.LayerNorm([self.planes, 1, 1]),\n                nn.ReLU(),\n                nn.Conv2D(self.planes, self.inplanes, kernel_size=1),\n            )\n        elif fusion_type == \"channel_concat\":\n            self.channel_concat_conv = nn.Sequential(\n                nn.Conv2D(self.inplanes, self.planes, kernel_size=1),\n                nn.LayerNorm([self.planes, 1, 1]),\n                nn.ReLU(),\n                nn.Conv2D(self.planes, self.inplanes, kernel_size=1),\n            )\n            # for concat\n            self.cat_conv = nn.Conv2D(2 * self.inplanes, self.inplanes, kernel_size=1)\n        elif fusion_type == \"channel_mul\":\n            self.channel_mul_conv = nn.Sequential(\n                nn.Conv2D(self.inplanes, self.planes, kernel_size=1),\n                nn.LayerNorm([self.planes, 1, 1]),\n                nn.ReLU(),\n                nn.Conv2D(self.planes, self.inplanes, kernel_size=1),\n            )\n\n    def spatial_pool(self, x):\n        batch, channel, height, width = x.shape\n        if self.pooling_type == \"att\":\n            # [N*headers, C', H , W] C = headers * C'\n            x = x.reshape(\n                [batch * self.headers, self.single_header_inplanes, height, width]\n            )\n            input_x = x\n\n            # [N*headers, C', H * W] C = headers * C'\n            # input_x = input_x.view(batch, channel, height * width)\n            input_x = input_x.reshape(\n                [batch * self.headers, self.single_header_inplanes, height * width]\n            )\n\n            # [N*headers, 1, C', H * W]\n            input_x = input_x.unsqueeze(1)\n            # [N*headers, 1, H, W]\n            context_mask = self.conv_mask(x)\n            # [N*headers, 1, H * W]\n            context_mask = context_mask.reshape(\n                [batch * self.headers, 1, height * width]\n            )\n\n            # scale variance\n            if self.att_scale and self.headers > 1:\n                context_mask = context_mask / paddle.sqrt(self.single_header_inplanes)\n\n            # [N*headers, 1, H * W]\n            context_mask = self.softmax(context_mask)\n\n            # [N*headers, 1, H * W, 1]\n            context_mask = context_mask.unsqueeze(-1)\n            # [N*headers, 1, C', 1] = [N*headers, 1, C', H * W] * [N*headers, 1, H * W, 1]\n            context = paddle.matmul(input_x, context_mask)\n\n            # [N, headers * C', 1, 1]\n            context = context.reshape(\n                [batch, self.headers * self.single_header_inplanes, 1, 1]\n            )\n        else:\n            # [N, C, 1, 1]\n            context = self.avg_pool(x)\n\n        return context\n\n    def forward(self, x):\n        # [N, C, 1, 1]\n        context = self.spatial_pool(x)\n\n        out = x\n\n        if self.fusion_type == \"channel_mul\":\n            # [N, C, 1, 1]\n            channel_mul_term = F.sigmoid(self.channel_mul_conv(context))\n            out = out * channel_mul_term\n        elif self.fusion_type == \"channel_add\":\n            # [N, C, 1, 1]\n            channel_add_term = self.channel_add_conv(context)\n            out = out + channel_add_term\n        else:\n            # [N, C, 1, 1]\n            channel_concat_term = self.channel_concat_conv(context)\n\n            # use concat\n            _, C1, _, _ = channel_concat_term.shape\n            N, C2, H, W = out.shape\n\n            out = paddle.concat(\n                [out, channel_concat_term.expand([-1, -1, H, W])], axis=1\n            )\n            out = self.cat_conv(out)\n            out = F.layer_norm(out, [self.inplanes, H, W])\n            out = F.relu(out)\n\n        return out\n", "ppocr/modeling/backbones/rec_resnet_45.py": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/FangShancheng/ABINet/tree/main/modules\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nfrom paddle import ParamAttr\nfrom paddle.nn.initializer import KaimingNormal\nimport paddle.nn as nn\nimport paddle.nn.functional as F\nimport numpy as np\nimport math\n\n__all__ = [\"ResNet45\"]\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    return nn.Conv2D(\n        in_planes,\n        out_planes,\n        kernel_size=1,\n        stride=1,\n        weight_attr=ParamAttr(initializer=KaimingNormal()),\n        bias_attr=False,\n    )\n\n\ndef conv3x3(in_channel, out_channel, stride=1):\n    return nn.Conv2D(\n        in_channel,\n        out_channel,\n        kernel_size=3,\n        stride=stride,\n        padding=1,\n        weight_attr=ParamAttr(initializer=KaimingNormal()),\n        bias_attr=False,\n    )\n\n\nclass BasicBlock(nn.Layer):\n    expansion = 1\n\n    def __init__(self, in_channels, channels, stride=1, downsample=None):\n        super().__init__()\n        self.conv1 = conv1x1(in_channels, channels)\n        self.bn1 = nn.BatchNorm2D(channels)\n        self.relu = nn.ReLU()\n        self.conv2 = conv3x3(channels, channels, stride)\n        self.bn2 = nn.BatchNorm2D(channels)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet45(nn.Layer):\n    def __init__(\n        self,\n        in_channels=3,\n        block=BasicBlock,\n        layers=[3, 4, 6, 6, 3],\n        strides=[2, 1, 2, 1, 1],\n    ):\n        self.inplanes = 32\n        super(ResNet45, self).__init__()\n        self.conv1 = nn.Conv2D(\n            in_channels,\n            32,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            weight_attr=ParamAttr(initializer=KaimingNormal()),\n            bias_attr=False,\n        )\n        self.bn1 = nn.BatchNorm2D(32)\n        self.relu = nn.ReLU()\n\n        self.layer1 = self._make_layer(block, 32, layers[0], stride=strides[0])\n        self.layer2 = self._make_layer(block, 64, layers[1], stride=strides[1])\n        self.layer3 = self._make_layer(block, 128, layers[2], stride=strides[2])\n        self.layer4 = self._make_layer(block, 256, layers[3], stride=strides[3])\n        self.layer5 = self._make_layer(block, 512, layers[4], stride=strides[4])\n        self.out_channels = 512\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            # downsample = True\n            downsample = nn.Sequential(\n                nn.Conv2D(\n                    self.inplanes,\n                    planes * block.expansion,\n                    kernel_size=1,\n                    stride=stride,\n                    weight_attr=ParamAttr(initializer=KaimingNormal()),\n                    bias_attr=False,\n                ),\n                nn.BatchNorm2D(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.layer5(x)\n        return x\n", "ppocr/modeling/backbones/rec_svtrv2.py": "# copyright (c) 2024 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom paddle import ParamAttr\nfrom paddle.nn.initializer import KaimingNormal\nimport numpy as np\nimport paddle\nimport paddle.nn as nn\nfrom paddle.nn.initializer import TruncatedNormal, Constant, Normal\n\ntrunc_normal_ = TruncatedNormal(std=0.02)\nnormal_ = Normal\nzeros_ = Constant(value=0.0)\nones_ = Constant(value=1.0)\n\n\ndef drop_path(x, drop_prob=0.0, training=False):\n    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ...\n    \"\"\"\n    if drop_prob == 0.0 or not training:\n        return x\n    keep_prob = paddle.to_tensor(1 - drop_prob, dtype=x.dtype)\n    shape = (paddle.shape(x)[0],) + (1,) * (x.ndim - 1)\n    random_tensor = keep_prob + paddle.rand(shape, dtype=x.dtype)\n    random_tensor = paddle.floor(random_tensor)  # binarize\n    output = x.divide(keep_prob) * random_tensor\n    return output\n\n\nclass DropPath(nn.Layer):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\"\"\"\n\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\n\n\nclass Identity(nn.Layer):\n    def __init__(self):\n        super(Identity, self).__init__()\n\n    def forward(self, input):\n        return input\n\n\nclass Mlp(nn.Layer):\n    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        act_layer=nn.GELU,\n        drop=0.0,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass ConvBNLayer(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size=3,\n        stride=1,\n        padding=0,\n        bias_attr=False,\n        groups=1,\n        act=nn.GELU,\n    ):\n        super().__init__()\n        self.conv = nn.Conv2D(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            groups=groups,\n            weight_attr=paddle.ParamAttr(initializer=nn.initializer.KaimingUniform()),\n            bias_attr=bias_attr,\n        )\n        self.norm = nn.BatchNorm2D(out_channels)\n        self.act = act()\n\n    def forward(self, inputs):\n        out = self.conv(inputs)\n        out = self.norm(out)\n        out = self.act(out)\n        return out\n\n\nclass Attention(nn.Layer):\n    def __init__(\n        self,\n        dim,\n        num_heads=8,\n        qkv_bias=False,\n        qk_scale=None,\n        attn_drop=0.0,\n        proj_drop=0.0,\n    ):\n        super().__init__()\n        self.num_heads = num_heads\n        self.dim = dim\n        self.head_dim = dim // num_heads\n        self.scale = qk_scale or self.head_dim**-0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias_attr=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        qkv = (\n            self.qkv(x)\n            .reshape((0, -1, 3, self.num_heads, self.head_dim))\n            .transpose((2, 0, 3, 1, 4))\n        )\n        q, k, v = qkv[0], qkv[1], qkv[2]\n\n        attn = (q.matmul(k.transpose((0, 1, 3, 2)))) * self.scale\n        attn = nn.functional.softmax(attn, axis=-1)\n        attn = self.attn_drop(attn)\n        x = (attn.matmul(v)).transpose((0, 2, 1, 3)).reshape((0, -1, self.dim))\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass Block(nn.Layer):\n    def __init__(\n        self,\n        dim,\n        num_heads,\n        mlp_ratio=4.0,\n        qkv_bias=False,\n        qk_scale=None,\n        drop=0.0,\n        attn_drop=0.0,\n        drop_path=0.0,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n        epsilon=1e-6,\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(dim, epsilon=epsilon)\n        self.mixer = Attention(\n            dim,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            qk_scale=qk_scale,\n            attn_drop=attn_drop,\n            proj_drop=drop,\n        )\n        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else Identity()\n        self.norm2 = norm_layer(dim, epsilon=epsilon)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp_ratio = mlp_ratio\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=mlp_hidden_dim,\n            act_layer=act_layer,\n            drop=drop,\n        )\n\n    def forward(self, x):\n        x = self.norm1(x + self.drop_path(self.mixer(x)))\n        x = self.norm2(x + self.drop_path(self.mlp(x)))\n        return x\n\n\nclass ConvBlock(nn.Layer):\n    def __init__(\n        self,\n        dim,\n        num_heads,\n        mlp_ratio=4.0,\n        drop=0.0,\n        drop_path=0.0,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n        epsilon=1e-6,\n    ):\n        super().__init__()\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.norm1 = norm_layer(dim, epsilon=epsilon)\n        self.mixer = nn.Conv2D(\n            dim,\n            dim,\n            5,\n            1,\n            2,\n            groups=num_heads,\n            weight_attr=ParamAttr(initializer=KaimingNormal()),\n        )\n        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else Identity()\n        self.norm2 = norm_layer(dim, epsilon=epsilon)\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=mlp_hidden_dim,\n            act_layer=act_layer,\n            drop=drop,\n        )\n\n    def forward(self, x):\n        C, H, W = x.shape[1:]\n        x = x + self.drop_path(self.mixer(x))\n        x = self.norm1(x.flatten(2).transpose([0, 2, 1]))\n        x = self.norm2(x + self.drop_path(self.mlp(x)))\n        x = x.transpose([0, 2, 1]).reshape([0, C, H, W])\n        return x\n\n\nclass FlattenTranspose(nn.Layer):\n    def forward(self, x):\n        return x.flatten(2).transpose([0, 2, 1])\n\n\nclass SubSample2D(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        stride=[2, 1],\n    ):\n        super().__init__()\n        self.conv = nn.Conv2D(\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            stride=stride,\n            padding=1,\n            weight_attr=ParamAttr(initializer=KaimingNormal()),\n        )\n        self.norm = nn.LayerNorm(out_channels)\n\n    def forward(self, x, sz):\n        # print(x.shape)\n        x = self.conv(x)\n        C, H, W = x.shape[1:]\n        x = self.norm(x.flatten(2).transpose([0, 2, 1]))\n        x = x.transpose([0, 2, 1]).reshape([0, C, H, W])\n        return x, [H, W]\n\n\nclass SubSample1D(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        stride=[2, 1],\n    ):\n        super().__init__()\n        self.conv = nn.Conv2D(\n            in_channels,\n            out_channels,\n            kernel_size=3,\n            stride=stride,\n            padding=1,\n            weight_attr=ParamAttr(initializer=KaimingNormal()),\n        )\n        self.norm = nn.LayerNorm(out_channels)\n\n    def forward(self, x, sz):\n        C = x.shape[-1]\n        x = x.transpose([0, 2, 1]).reshape([0, C, sz[0], sz[1]])\n        x = self.conv(x)\n        C, H, W = x.shape[1:]\n        x = self.norm(x.flatten(2).transpose([0, 2, 1]))\n        return x, [H, W]\n\n\nclass IdentitySize(nn.Layer):\n    def forward(self, x, sz):\n        return x, sz\n\n\nclass SVTRStage(nn.Layer):\n    def __init__(\n        self,\n        dim=64,\n        out_dim=256,\n        depth=3,\n        mixer=[\"Local\"] * 3,\n        sub_k=[2, 1],\n        num_heads=2,\n        mlp_ratio=4,\n        qkv_bias=True,\n        qk_scale=None,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path=[0.1] * 3,\n        norm_layer=nn.LayerNorm,\n        act=nn.GELU,\n        eps=1e-6,\n        downsample=None,\n        **kwargs,\n    ):\n        super().__init__()\n        self.dim = dim\n\n        conv_block_num = sum([1 if mix == \"Conv\" else 0 for mix in mixer])\n        blocks = []\n        for i in range(depth):\n            if mixer[i] == \"Conv\":\n                blocks.append(\n                    ConvBlock(\n                        dim=dim,\n                        num_heads=num_heads,\n                        mlp_ratio=mlp_ratio,\n                        drop=drop_rate,\n                        act_layer=act,\n                        drop_path=drop_path[i],\n                        norm_layer=norm_layer,\n                        epsilon=eps,\n                    )\n                )\n            else:\n                blocks.append(\n                    Block(\n                        dim=dim,\n                        num_heads=num_heads,\n                        mlp_ratio=mlp_ratio,\n                        qkv_bias=qkv_bias,\n                        qk_scale=qk_scale,\n                        drop=drop_rate,\n                        act_layer=act,\n                        attn_drop=attn_drop_rate,\n                        drop_path=drop_path[i],\n                        norm_layer=norm_layer,\n                        epsilon=eps,\n                    )\n                )\n            if i == conv_block_num - 1 and mixer[-1] != \"Conv\":\n                blocks.append(FlattenTranspose())\n        self.blocks = nn.Sequential(*blocks)\n        if downsample:\n            if mixer[-1] == \"Conv\":\n                self.downsample = SubSample2D(dim, out_dim, stride=sub_k)\n            elif mixer[-1] == \"Global\":\n                self.downsample = SubSample1D(dim, out_dim, stride=sub_k)\n        else:\n            self.downsample = IdentitySize()\n\n    def forward(self, x, sz):\n        x = self.blocks(x)\n        x, sz = self.downsample(x, sz)\n        return x, sz\n\n\nclass ADDPosEmbed(nn.Layer):\n    def __init__(self, feat_max_size=[8, 32], embed_dim=768):\n        super().__init__()\n        pos_embed = paddle.zeros(\n            [1, feat_max_size[0] * feat_max_size[1], embed_dim], dtype=paddle.float32\n        )\n        trunc_normal_(pos_embed)\n        pos_embed = pos_embed.transpose([0, 2, 1]).reshape(\n            [1, embed_dim, feat_max_size[0], feat_max_size[1]]\n        )\n        self.pos_embed = self.create_parameter(\n            [1, embed_dim, feat_max_size[0], feat_max_size[1]]\n        )\n        self.add_parameter(\"pos_embed\", self.pos_embed)\n        self.pos_embed.set_value(pos_embed)\n\n    def forward(self, x):\n        sz = x.shape[2:]\n        x = x + self.pos_embed[:, :, : sz[0], : sz[1]]\n        return x\n\n\nclass POPatchEmbed(nn.Layer):\n    \"\"\"Image to Patch Embedding\"\"\"\n\n    def __init__(\n        self,\n        in_channels=3,\n        feat_max_size=[8, 32],\n        embed_dim=768,\n        use_pos_embed=False,\n        flatten=False,\n    ):\n        super().__init__()\n        patch_embed = [\n            ConvBNLayer(\n                in_channels=in_channels,\n                out_channels=embed_dim // 2,\n                kernel_size=3,\n                stride=2,\n                padding=1,\n                act=nn.GELU,\n                bias_attr=None,\n            ),\n            ConvBNLayer(\n                in_channels=embed_dim // 2,\n                out_channels=embed_dim,\n                kernel_size=3,\n                stride=2,\n                padding=1,\n                act=nn.GELU,\n                bias_attr=None,\n            ),\n        ]\n        if use_pos_embed:\n            patch_embed.append(ADDPosEmbed(feat_max_size, embed_dim))\n        if flatten:\n            patch_embed.append(FlattenTranspose())\n        self.patch_embed = nn.Sequential(*patch_embed)\n\n    def forward(self, x):\n        sz = x.shape[2:]\n        x = self.patch_embed(x)\n        return x, [sz[0] // 4, sz[1] // 4]\n\n\nclass LastStage(nn.Layer):\n    def __init__(self, in_channels, out_channels, last_drop, out_char_num):\n        super().__init__()\n        self.last_conv = nn.Linear(in_channels, out_channels, bias_attr=False)\n        self.hardswish = nn.Hardswish()\n        self.dropout = nn.Dropout(p=last_drop, mode=\"downscale_in_infer\")\n\n    def forward(self, x, sz):\n        x = x.reshape([0, sz[0], sz[1], x.shape[-1]])\n        x = x.mean(1)\n        x = self.last_conv(x)\n        x = self.hardswish(x)\n        x = self.dropout(x)\n        return x, [1, sz[1]]\n\n\nclass OutPool(nn.Layer):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x, sz):\n        C = x.shape[-1]\n        x = x.transpose([0, 2, 1]).reshape([0, C, sz[0], sz[1]])\n        x = nn.functional.avg_pool2d(x, [sz[0], 2])\n        return x, [1, sz[1] // 2]\n\n\nclass Feat2D(nn.Layer):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x, sz):\n        C = x.shape[-1]\n        x = x.transpose([0, 2, 1]).reshape([0, C, sz[0], sz[1]])\n        return x, sz\n\n\nclass SVTRv2(nn.Layer):\n    def __init__(\n        self,\n        max_sz=[32, 128],\n        in_channels=3,\n        out_channels=192,\n        out_char_num=25,\n        depths=[3, 6, 3],\n        dims=[64, 128, 256],\n        mixer=[[\"Conv\"] * 3, [\"Conv\"] * 3 + [\"Global\"] * 3, [\"Global\"] * 3],\n        use_pos_embed=False,\n        sub_k=[[1, 1], [2, 1], [1, 1]],\n        num_heads=[2, 4, 8],\n        mlp_ratio=4,\n        qkv_bias=True,\n        qk_scale=None,\n        drop_rate=0.0,\n        last_drop=0.1,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.1,\n        norm_layer=nn.LayerNorm,\n        act=nn.GELU,\n        last_stage=False,\n        eps=1e-6,\n        use_pool=False,\n        feat2d=False,\n        **kwargs,\n    ):\n        super().__init__()\n        num_stages = len(depths)\n        self.num_features = dims[-1]\n\n        feat_max_size = [max_sz[0] // 4, max_sz[1] // 4]\n        self.pope = POPatchEmbed(\n            in_channels=in_channels,\n            feat_max_size=feat_max_size,\n            embed_dim=dims[0],\n            use_pos_embed=use_pos_embed,\n            flatten=mixer[0][0] != \"Conv\",\n        )\n\n        dpr = np.linspace(0, drop_path_rate, sum(depths))  # stochastic depth decay rule\n\n        self.stages = nn.LayerList()\n        for i_stage in range(num_stages):\n            stage = SVTRStage(\n                dim=dims[i_stage],\n                out_dim=dims[i_stage + 1] if i_stage < num_stages - 1 else 0,\n                depth=depths[i_stage],\n                mixer=mixer[i_stage],\n                sub_k=sub_k[i_stage],\n                num_heads=num_heads[i_stage],\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                qk_scale=qk_scale,\n                drop=drop_rate,\n                attn_drop=attn_drop_rate,\n                drop_path=dpr[sum(depths[:i_stage]) : sum(depths[: i_stage + 1])],\n                norm_layer=norm_layer,\n                act=act,\n                downsample=False if i_stage == num_stages - 1 else True,\n                eps=eps,\n            )\n            self.stages.append(stage)\n\n        self.out_channels = self.num_features\n        self.last_stage = last_stage\n        if last_stage:\n            self.out_channels = out_channels\n            self.stages.append(\n                LastStage(self.num_features, out_channels, last_drop, out_char_num)\n            )\n        if use_pool:\n            self.stages.append(OutPool())\n\n        if feat2d:\n            self.stages.append(Feat2D())\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                zeros_(m.bias)\n        elif isinstance(m, nn.LayerNorm):\n            zeros_(m.bias)\n            ones_(m.weight)\n\n    def forward(self, x):\n        x, sz = self.pope(x)\n        for stage in self.stages:\n            x, sz = stage(x, sz)\n        return x\n", "ppocr/modeling/backbones/det_resnet.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport paddle\nfrom paddle import ParamAttr\nimport paddle.nn as nn\nimport paddle.nn.functional as F\nfrom paddle.nn import Conv2D, BatchNorm, Linear, Dropout\nfrom paddle.nn import AdaptiveAvgPool2D, MaxPool2D, AvgPool2D\nfrom paddle.nn.initializer import Uniform\n\nimport math\n\nfrom paddle.vision.ops import DeformConv2D\nfrom paddle.regularizer import L2Decay\nfrom paddle.nn.initializer import Normal, Constant, XavierUniform\nfrom .det_resnet_vd import DeformableConvV2, ConvBNLayer\n\n\nclass BottleneckBlock(nn.Layer):\n    def __init__(self, num_channels, num_filters, stride, shortcut=True, is_dcn=False):\n        super(BottleneckBlock, self).__init__()\n\n        self.conv0 = ConvBNLayer(\n            in_channels=num_channels,\n            out_channels=num_filters,\n            kernel_size=1,\n            act=\"relu\",\n        )\n        self.conv1 = ConvBNLayer(\n            in_channels=num_filters,\n            out_channels=num_filters,\n            kernel_size=3,\n            stride=stride,\n            act=\"relu\",\n            is_dcn=is_dcn,\n            dcn_groups=1,\n        )\n        self.conv2 = ConvBNLayer(\n            in_channels=num_filters,\n            out_channels=num_filters * 4,\n            kernel_size=1,\n            act=None,\n        )\n\n        if not shortcut:\n            self.short = ConvBNLayer(\n                in_channels=num_channels,\n                out_channels=num_filters * 4,\n                kernel_size=1,\n                stride=stride,\n            )\n\n        self.shortcut = shortcut\n\n        self._num_channels_out = num_filters * 4\n\n    def forward(self, inputs):\n        y = self.conv0(inputs)\n        conv1 = self.conv1(y)\n        conv2 = self.conv2(conv1)\n\n        if self.shortcut:\n            short = inputs\n        else:\n            short = self.short(inputs)\n\n        y = paddle.add(x=short, y=conv2)\n        y = F.relu(y)\n        return y\n\n\nclass BasicBlock(nn.Layer):\n    def __init__(self, num_channels, num_filters, stride, shortcut=True, name=None):\n        super(BasicBlock, self).__init__()\n        self.stride = stride\n        self.conv0 = ConvBNLayer(\n            in_channels=num_channels,\n            out_channels=num_filters,\n            kernel_size=3,\n            stride=stride,\n            act=\"relu\",\n        )\n        self.conv1 = ConvBNLayer(\n            in_channels=num_filters, out_channels=num_filters, kernel_size=3, act=None\n        )\n\n        if not shortcut:\n            self.short = ConvBNLayer(\n                in_channels=num_channels,\n                out_channels=num_filters,\n                kernel_size=1,\n                stride=stride,\n            )\n\n        self.shortcut = shortcut\n\n    def forward(self, inputs):\n        y = self.conv0(inputs)\n        conv1 = self.conv1(y)\n\n        if self.shortcut:\n            short = inputs\n        else:\n            short = self.short(inputs)\n        y = paddle.add(x=short, y=conv1)\n        y = F.relu(y)\n        return y\n\n\nclass ResNet(nn.Layer):\n    def __init__(self, in_channels=3, layers=50, out_indices=None, dcn_stage=None):\n        super(ResNet, self).__init__()\n\n        self.layers = layers\n        self.input_image_channel = in_channels\n\n        supported_layers = [18, 34, 50, 101, 152]\n        assert (\n            layers in supported_layers\n        ), \"supported layers are {} but input layer is {}\".format(\n            supported_layers, layers\n        )\n\n        if layers == 18:\n            depth = [2, 2, 2, 2]\n        elif layers == 34 or layers == 50:\n            depth = [3, 4, 6, 3]\n        elif layers == 101:\n            depth = [3, 4, 23, 3]\n        elif layers == 152:\n            depth = [3, 8, 36, 3]\n        num_channels = [64, 256, 512, 1024] if layers >= 50 else [64, 64, 128, 256]\n        num_filters = [64, 128, 256, 512]\n\n        self.dcn_stage = (\n            dcn_stage if dcn_stage is not None else [False, False, False, False]\n        )\n        self.out_indices = out_indices if out_indices is not None else [0, 1, 2, 3]\n\n        self.conv = ConvBNLayer(\n            in_channels=self.input_image_channel,\n            out_channels=64,\n            kernel_size=7,\n            stride=2,\n            act=\"relu\",\n        )\n        self.pool2d_max = MaxPool2D(\n            kernel_size=3,\n            stride=2,\n            padding=1,\n        )\n\n        self.stages = []\n        self.out_channels = []\n        if layers >= 50:\n            for block in range(len(depth)):\n                shortcut = False\n                block_list = []\n                is_dcn = self.dcn_stage[block]\n                for i in range(depth[block]):\n                    if layers in [101, 152] and block == 2:\n                        if i == 0:\n                            conv_name = \"res\" + str(block + 2) + \"a\"\n                        else:\n                            conv_name = \"res\" + str(block + 2) + \"b\" + str(i)\n                    else:\n                        conv_name = \"res\" + str(block + 2) + chr(97 + i)\n                    bottleneck_block = self.add_sublayer(\n                        conv_name,\n                        BottleneckBlock(\n                            num_channels=(\n                                num_channels[block]\n                                if i == 0\n                                else num_filters[block] * 4\n                            ),\n                            num_filters=num_filters[block],\n                            stride=2 if i == 0 and block != 0 else 1,\n                            shortcut=shortcut,\n                            is_dcn=is_dcn,\n                        ),\n                    )\n                    block_list.append(bottleneck_block)\n                    shortcut = True\n                if block in self.out_indices:\n                    self.out_channels.append(num_filters[block] * 4)\n                self.stages.append(nn.Sequential(*block_list))\n        else:\n            for block in range(len(depth)):\n                shortcut = False\n                block_list = []\n                for i in range(depth[block]):\n                    conv_name = \"res\" + str(block + 2) + chr(97 + i)\n                    basic_block = self.add_sublayer(\n                        conv_name,\n                        BasicBlock(\n                            num_channels=(\n                                num_channels[block] if i == 0 else num_filters[block]\n                            ),\n                            num_filters=num_filters[block],\n                            stride=2 if i == 0 and block != 0 else 1,\n                            shortcut=shortcut,\n                        ),\n                    )\n                    block_list.append(basic_block)\n                    shortcut = True\n                if block in self.out_indices:\n                    self.out_channels.append(num_filters[block])\n                self.stages.append(nn.Sequential(*block_list))\n\n    def forward(self, inputs):\n        y = self.conv(inputs)\n        y = self.pool2d_max(y)\n        out = []\n        for i, block in enumerate(self.stages):\n            y = block(y)\n            if i in self.out_indices:\n                out.append(y)\n        return out\n", "ppocr/modeling/backbones/e2e_resnet_vd_pg.py": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nfrom paddle import ParamAttr\nimport paddle.nn as nn\nimport paddle.nn.functional as F\n\n__all__ = [\"ResNet\"]\n\n\nclass ConvBNLayer(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride=1,\n        groups=1,\n        is_vd_mode=False,\n        act=None,\n        name=None,\n    ):\n        super(ConvBNLayer, self).__init__()\n\n        self.is_vd_mode = is_vd_mode\n        self._pool2d_avg = nn.AvgPool2D(\n            kernel_size=2, stride=2, padding=0, ceil_mode=True\n        )\n        self._conv = nn.Conv2D(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=(kernel_size - 1) // 2,\n            groups=groups,\n            weight_attr=ParamAttr(name=name + \"_weights\"),\n            bias_attr=False,\n        )\n        if name == \"conv1\":\n            bn_name = \"bn_\" + name\n        else:\n            bn_name = \"bn\" + name[3:]\n        self._batch_norm = nn.BatchNorm(\n            out_channels,\n            act=act,\n            param_attr=ParamAttr(name=bn_name + \"_scale\"),\n            bias_attr=ParamAttr(bn_name + \"_offset\"),\n            moving_mean_name=bn_name + \"_mean\",\n            moving_variance_name=bn_name + \"_variance\",\n        )\n\n    def forward(self, inputs):\n        y = self._conv(inputs)\n        y = self._batch_norm(y)\n        return y\n\n\nclass BottleneckBlock(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        stride,\n        shortcut=True,\n        if_first=False,\n        name=None,\n    ):\n        super(BottleneckBlock, self).__init__()\n\n        self.conv0 = ConvBNLayer(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=1,\n            act=\"relu\",\n            name=name + \"_branch2a\",\n        )\n        self.conv1 = ConvBNLayer(\n            in_channels=out_channels,\n            out_channels=out_channels,\n            kernel_size=3,\n            stride=stride,\n            act=\"relu\",\n            name=name + \"_branch2b\",\n        )\n        self.conv2 = ConvBNLayer(\n            in_channels=out_channels,\n            out_channels=out_channels * 4,\n            kernel_size=1,\n            act=None,\n            name=name + \"_branch2c\",\n        )\n\n        if not shortcut:\n            self.short = ConvBNLayer(\n                in_channels=in_channels,\n                out_channels=out_channels * 4,\n                kernel_size=1,\n                stride=stride,\n                is_vd_mode=False if if_first else True,\n                name=name + \"_branch1\",\n            )\n\n        self.shortcut = shortcut\n\n    def forward(self, inputs):\n        y = self.conv0(inputs)\n        conv1 = self.conv1(y)\n        conv2 = self.conv2(conv1)\n\n        if self.shortcut:\n            short = inputs\n        else:\n            short = self.short(inputs)\n        y = paddle.add(x=short, y=conv2)\n        y = F.relu(y)\n        return y\n\n\nclass BasicBlock(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        stride,\n        shortcut=True,\n        if_first=False,\n        name=None,\n    ):\n        super(BasicBlock, self).__init__()\n        self.stride = stride\n        self.conv0 = ConvBNLayer(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=3,\n            stride=stride,\n            act=\"relu\",\n            name=name + \"_branch2a\",\n        )\n        self.conv1 = ConvBNLayer(\n            in_channels=out_channels,\n            out_channels=out_channels,\n            kernel_size=3,\n            act=None,\n            name=name + \"_branch2b\",\n        )\n\n        if not shortcut:\n            self.short = ConvBNLayer(\n                in_channels=in_channels,\n                out_channels=out_channels,\n                kernel_size=1,\n                stride=1,\n                is_vd_mode=False if if_first else True,\n                name=name + \"_branch1\",\n            )\n\n        self.shortcut = shortcut\n\n    def forward(self, inputs):\n        y = self.conv0(inputs)\n        conv1 = self.conv1(y)\n\n        if self.shortcut:\n            short = inputs\n        else:\n            short = self.short(inputs)\n        y = paddle.add(x=short, y=conv1)\n        y = F.relu(y)\n        return y\n\n\nclass ResNet(nn.Layer):\n    def __init__(self, in_channels=3, layers=50, **kwargs):\n        super(ResNet, self).__init__()\n\n        self.layers = layers\n        supported_layers = [18, 34, 50, 101, 152, 200]\n        assert (\n            layers in supported_layers\n        ), \"supported layers are {} but input layer is {}\".format(\n            supported_layers, layers\n        )\n\n        if layers == 18:\n            depth = [2, 2, 2, 2]\n        elif layers == 34 or layers == 50:\n            # depth = [3, 4, 6, 3]\n            depth = [3, 4, 6, 3, 3]\n        elif layers == 101:\n            depth = [3, 4, 23, 3]\n        elif layers == 152:\n            depth = [3, 8, 36, 3]\n        elif layers == 200:\n            depth = [3, 12, 48, 3]\n        num_channels = (\n            [64, 256, 512, 1024, 2048] if layers >= 50 else [64, 64, 128, 256]\n        )\n        num_filters = [64, 128, 256, 512, 512]\n\n        self.conv1_1 = ConvBNLayer(\n            in_channels=in_channels,\n            out_channels=64,\n            kernel_size=7,\n            stride=2,\n            act=\"relu\",\n            name=\"conv1_1\",\n        )\n        self.pool2d_max = nn.MaxPool2D(kernel_size=3, stride=2, padding=1)\n\n        self.stages = []\n        self.out_channels = [3, 64]\n        # num_filters = [64, 128, 256, 512, 512]\n        if layers >= 50:\n            for block in range(len(depth)):\n                block_list = []\n                shortcut = False\n                for i in range(depth[block]):\n                    if layers in [101, 152] and block == 2:\n                        if i == 0:\n                            conv_name = \"res\" + str(block + 2) + \"a\"\n                        else:\n                            conv_name = \"res\" + str(block + 2) + \"b\" + str(i)\n                    else:\n                        conv_name = \"res\" + str(block + 2) + chr(97 + i)\n                    bottleneck_block = self.add_sublayer(\n                        \"bb_%d_%d\" % (block, i),\n                        BottleneckBlock(\n                            in_channels=(\n                                num_channels[block]\n                                if i == 0\n                                else num_filters[block] * 4\n                            ),\n                            out_channels=num_filters[block],\n                            stride=2 if i == 0 and block != 0 else 1,\n                            shortcut=shortcut,\n                            if_first=block == i == 0,\n                            name=conv_name,\n                        ),\n                    )\n                    shortcut = True\n                    block_list.append(bottleneck_block)\n                self.out_channels.append(num_filters[block] * 4)\n                self.stages.append(nn.Sequential(*block_list))\n        else:\n            for block in range(len(depth)):\n                block_list = []\n                shortcut = False\n                for i in range(depth[block]):\n                    conv_name = \"res\" + str(block + 2) + chr(97 + i)\n                    basic_block = self.add_sublayer(\n                        \"bb_%d_%d\" % (block, i),\n                        BasicBlock(\n                            in_channels=(\n                                num_channels[block] if i == 0 else num_filters[block]\n                            ),\n                            out_channels=num_filters[block],\n                            stride=2 if i == 0 and block != 0 else 1,\n                            shortcut=shortcut,\n                            if_first=block == i == 0,\n                            name=conv_name,\n                        ),\n                    )\n                    shortcut = True\n                    block_list.append(basic_block)\n                self.out_channels.append(num_filters[block])\n                self.stages.append(nn.Sequential(*block_list))\n\n    def forward(self, inputs):\n        out = [inputs]\n        y = self.conv1_1(inputs)\n        out.append(y)\n        y = self.pool2d_max(y)\n        for block in self.stages:\n            y = block(y)\n            out.append(y)\n        return out\n", "ppocr/modeling/backbones/rec_shallow_cnn.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/open-mmlab/mmocr/blob/1.x/mmocr/models/textrecog/backbones/shallow_cnn.py\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport numpy as np\nimport paddle\nfrom paddle import ParamAttr\nimport paddle.nn as nn\nimport paddle.nn.functional as F\nfrom paddle.nn import MaxPool2D\nfrom paddle.nn.initializer import KaimingNormal, Uniform, Constant\n\n\nclass ConvBNLayer(nn.Layer):\n    def __init__(\n        self, num_channels, filter_size, num_filters, stride, padding, num_groups=1\n    ):\n        super(ConvBNLayer, self).__init__()\n\n        self.conv = nn.Conv2D(\n            in_channels=num_channels,\n            out_channels=num_filters,\n            kernel_size=filter_size,\n            stride=stride,\n            padding=padding,\n            groups=num_groups,\n            weight_attr=ParamAttr(initializer=KaimingNormal()),\n            bias_attr=False,\n        )\n\n        self.bn = nn.BatchNorm2D(\n            num_filters,\n            weight_attr=ParamAttr(initializer=Uniform(0, 1)),\n            bias_attr=ParamAttr(initializer=Constant(0)),\n        )\n        self.relu = nn.ReLU()\n\n    def forward(self, inputs):\n        y = self.conv(inputs)\n        y = self.bn(y)\n        y = self.relu(y)\n        return y\n\n\nclass ShallowCNN(nn.Layer):\n    def __init__(self, in_channels=1, hidden_dim=512):\n        super().__init__()\n        assert isinstance(in_channels, int)\n        assert isinstance(hidden_dim, int)\n\n        self.conv1 = ConvBNLayer(in_channels, 3, hidden_dim // 2, stride=1, padding=1)\n        self.conv2 = ConvBNLayer(hidden_dim // 2, 3, hidden_dim, stride=1, padding=1)\n        self.pool = nn.MaxPool2D(kernel_size=2, stride=2, padding=0)\n        self.out_channels = hidden_dim\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.pool(x)\n\n        x = self.conv2(x)\n        x = self.pool(x)\n\n        return x\n", "ppocr/modeling/backbones/det_resnet_vd_sast.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nfrom paddle import ParamAttr\nimport paddle.nn as nn\nimport paddle.nn.functional as F\n\n__all__ = [\"ResNet_SAST\"]\n\n\nclass ConvBNLayer(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride=1,\n        groups=1,\n        is_vd_mode=False,\n        act=None,\n        name=None,\n    ):\n        super(ConvBNLayer, self).__init__()\n\n        self.is_vd_mode = is_vd_mode\n        self._pool2d_avg = nn.AvgPool2D(\n            kernel_size=2, stride=2, padding=0, ceil_mode=True\n        )\n        self._conv = nn.Conv2D(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=(kernel_size - 1) // 2,\n            groups=groups,\n            weight_attr=ParamAttr(name=name + \"_weights\"),\n            bias_attr=False,\n        )\n        if name == \"conv1\":\n            bn_name = \"bn_\" + name\n        else:\n            bn_name = \"bn\" + name[3:]\n        self._batch_norm = nn.BatchNorm(\n            out_channels,\n            act=act,\n            param_attr=ParamAttr(name=bn_name + \"_scale\"),\n            bias_attr=ParamAttr(bn_name + \"_offset\"),\n            moving_mean_name=bn_name + \"_mean\",\n            moving_variance_name=bn_name + \"_variance\",\n        )\n\n    def forward(self, inputs):\n        if self.is_vd_mode:\n            inputs = self._pool2d_avg(inputs)\n        y = self._conv(inputs)\n        y = self._batch_norm(y)\n        return y\n\n\nclass BottleneckBlock(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        stride,\n        shortcut=True,\n        if_first=False,\n        name=None,\n    ):\n        super(BottleneckBlock, self).__init__()\n\n        self.conv0 = ConvBNLayer(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=1,\n            act=\"relu\",\n            name=name + \"_branch2a\",\n        )\n        self.conv1 = ConvBNLayer(\n            in_channels=out_channels,\n            out_channels=out_channels,\n            kernel_size=3,\n            stride=stride,\n            act=\"relu\",\n            name=name + \"_branch2b\",\n        )\n        self.conv2 = ConvBNLayer(\n            in_channels=out_channels,\n            out_channels=out_channels * 4,\n            kernel_size=1,\n            act=None,\n            name=name + \"_branch2c\",\n        )\n\n        if not shortcut:\n            self.short = ConvBNLayer(\n                in_channels=in_channels,\n                out_channels=out_channels * 4,\n                kernel_size=1,\n                stride=1,\n                is_vd_mode=False if if_first else True,\n                name=name + \"_branch1\",\n            )\n\n        self.shortcut = shortcut\n\n    def forward(self, inputs):\n        y = self.conv0(inputs)\n        conv1 = self.conv1(y)\n        conv2 = self.conv2(conv1)\n\n        if self.shortcut:\n            short = inputs\n        else:\n            short = self.short(inputs)\n        y = paddle.add(x=short, y=conv2)\n        y = F.relu(y)\n        return y\n\n\nclass BasicBlock(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        stride,\n        shortcut=True,\n        if_first=False,\n        name=None,\n    ):\n        super(BasicBlock, self).__init__()\n        self.stride = stride\n        self.conv0 = ConvBNLayer(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=3,\n            stride=stride,\n            act=\"relu\",\n            name=name + \"_branch2a\",\n        )\n        self.conv1 = ConvBNLayer(\n            in_channels=out_channels,\n            out_channels=out_channels,\n            kernel_size=3,\n            act=None,\n            name=name + \"_branch2b\",\n        )\n\n        if not shortcut:\n            self.short = ConvBNLayer(\n                in_channels=in_channels,\n                out_channels=out_channels,\n                kernel_size=1,\n                stride=1,\n                is_vd_mode=False if if_first else True,\n                name=name + \"_branch1\",\n            )\n\n        self.shortcut = shortcut\n\n    def forward(self, inputs):\n        y = self.conv0(inputs)\n        conv1 = self.conv1(y)\n\n        if self.shortcut:\n            short = inputs\n        else:\n            short = self.short(inputs)\n        y = paddle.add(x=short, y=conv1)\n        y = F.relu(y)\n        return y\n\n\nclass ResNet_SAST(nn.Layer):\n    def __init__(self, in_channels=3, layers=50, **kwargs):\n        super(ResNet_SAST, self).__init__()\n\n        self.layers = layers\n        supported_layers = [18, 34, 50, 101, 152, 200]\n        assert (\n            layers in supported_layers\n        ), \"supported layers are {} but input layer is {}\".format(\n            supported_layers, layers\n        )\n\n        if layers == 18:\n            depth = [2, 2, 2, 2]\n        elif layers == 34 or layers == 50:\n            # depth = [3, 4, 6, 3]\n            depth = [3, 4, 6, 3, 3]\n        elif layers == 101:\n            depth = [3, 4, 23, 3]\n        elif layers == 152:\n            depth = [3, 8, 36, 3]\n        elif layers == 200:\n            depth = [3, 12, 48, 3]\n        # num_channels = [64, 256, 512,\n        #                 1024] if layers >= 50 else [64, 64, 128, 256]\n        # num_filters = [64, 128, 256, 512]\n        num_channels = (\n            [64, 256, 512, 1024, 2048] if layers >= 50 else [64, 64, 128, 256]\n        )\n        num_filters = [64, 128, 256, 512, 512]\n\n        self.conv1_1 = ConvBNLayer(\n            in_channels=in_channels,\n            out_channels=32,\n            kernel_size=3,\n            stride=2,\n            act=\"relu\",\n            name=\"conv1_1\",\n        )\n        self.conv1_2 = ConvBNLayer(\n            in_channels=32,\n            out_channels=32,\n            kernel_size=3,\n            stride=1,\n            act=\"relu\",\n            name=\"conv1_2\",\n        )\n        self.conv1_3 = ConvBNLayer(\n            in_channels=32,\n            out_channels=64,\n            kernel_size=3,\n            stride=1,\n            act=\"relu\",\n            name=\"conv1_3\",\n        )\n        self.pool2d_max = nn.MaxPool2D(kernel_size=3, stride=2, padding=1)\n\n        self.stages = []\n        self.out_channels = [3, 64]\n        if layers >= 50:\n            for block in range(len(depth)):\n                block_list = []\n                shortcut = False\n                for i in range(depth[block]):\n                    if layers in [101, 152] and block == 2:\n                        if i == 0:\n                            conv_name = \"res\" + str(block + 2) + \"a\"\n                        else:\n                            conv_name = \"res\" + str(block + 2) + \"b\" + str(i)\n                    else:\n                        conv_name = \"res\" + str(block + 2) + chr(97 + i)\n                    bottleneck_block = self.add_sublayer(\n                        \"bb_%d_%d\" % (block, i),\n                        BottleneckBlock(\n                            in_channels=(\n                                num_channels[block]\n                                if i == 0\n                                else num_filters[block] * 4\n                            ),\n                            out_channels=num_filters[block],\n                            stride=2 if i == 0 and block != 0 else 1,\n                            shortcut=shortcut,\n                            if_first=block == i == 0,\n                            name=conv_name,\n                        ),\n                    )\n                    shortcut = True\n                    block_list.append(bottleneck_block)\n                self.out_channels.append(num_filters[block] * 4)\n                self.stages.append(nn.Sequential(*block_list))\n        else:\n            for block in range(len(depth)):\n                block_list = []\n                shortcut = False\n                for i in range(depth[block]):\n                    conv_name = \"res\" + str(block + 2) + chr(97 + i)\n                    basic_block = self.add_sublayer(\n                        \"bb_%d_%d\" % (block, i),\n                        BasicBlock(\n                            in_channels=(\n                                num_channels[block] if i == 0 else num_filters[block]\n                            ),\n                            out_channels=num_filters[block],\n                            stride=2 if i == 0 and block != 0 else 1,\n                            shortcut=shortcut,\n                            if_first=block == i == 0,\n                            name=conv_name,\n                        ),\n                    )\n                    shortcut = True\n                    block_list.append(basic_block)\n                self.out_channels.append(num_filters[block])\n                self.stages.append(nn.Sequential(*block_list))\n\n    def forward(self, inputs):\n        out = [inputs]\n        y = self.conv1_1(inputs)\n        y = self.conv1_2(y)\n        y = self.conv1_3(y)\n        out.append(y)\n        y = self.pool2d_max(y)\n        for block in self.stages:\n            y = block(y)\n            out.append(y)\n        return out\n", "ppocr/modeling/backbones/rec_efficientb3_pren.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nCode is refer from:\nhttps://github.com/RuijieJ/pren/blob/main/Nets/EfficientNet.py\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport re\nimport collections\nimport paddle\nimport paddle.nn as nn\nimport paddle.nn.functional as F\n\n__all__ = [\"EfficientNetb3_PREN\"]\n\nGlobalParams = collections.namedtuple(\n    \"GlobalParams\",\n    [\n        \"batch_norm_momentum\",\n        \"batch_norm_epsilon\",\n        \"dropout_rate\",\n        \"num_classes\",\n        \"width_coefficient\",\n        \"depth_coefficient\",\n        \"depth_divisor\",\n        \"min_depth\",\n        \"drop_connect_rate\",\n        \"image_size\",\n    ],\n)\n\nBlockArgs = collections.namedtuple(\n    \"BlockArgs\",\n    [\n        \"kernel_size\",\n        \"num_repeat\",\n        \"input_filters\",\n        \"output_filters\",\n        \"expand_ratio\",\n        \"id_skip\",\n        \"stride\",\n        \"se_ratio\",\n    ],\n)\n\n\nclass BlockDecoder:\n    @staticmethod\n    def _decode_block_string(block_string):\n        assert isinstance(block_string, str)\n\n        ops = block_string.split(\"_\")\n        options = {}\n        for op in ops:\n            splits = re.split(r\"(\\d.*)\", op)\n            if len(splits) >= 2:\n                key, value = splits[:2]\n                options[key] = value\n\n        assert (\"s\" in options and len(options[\"s\"]) == 1) or (\n            len(options[\"s\"]) == 2 and options[\"s\"][0] == options[\"s\"][1]\n        )\n\n        return BlockArgs(\n            kernel_size=int(options[\"k\"]),\n            num_repeat=int(options[\"r\"]),\n            input_filters=int(options[\"i\"]),\n            output_filters=int(options[\"o\"]),\n            expand_ratio=int(options[\"e\"]),\n            id_skip=(\"noskip\" not in block_string),\n            se_ratio=float(options[\"se\"]) if \"se\" in options else None,\n            stride=[int(options[\"s\"][0])],\n        )\n\n    @staticmethod\n    def decode(string_list):\n        assert isinstance(string_list, list)\n        blocks_args = []\n        for block_string in string_list:\n            blocks_args.append(BlockDecoder._decode_block_string(block_string))\n        return blocks_args\n\n\ndef efficientnet(\n    width_coefficient=None,\n    depth_coefficient=None,\n    dropout_rate=0.2,\n    drop_connect_rate=0.2,\n    image_size=None,\n    num_classes=1000,\n):\n    blocks_args = [\n        \"r1_k3_s11_e1_i32_o16_se0.25\",\n        \"r2_k3_s22_e6_i16_o24_se0.25\",\n        \"r2_k5_s22_e6_i24_o40_se0.25\",\n        \"r3_k3_s22_e6_i40_o80_se0.25\",\n        \"r3_k5_s11_e6_i80_o112_se0.25\",\n        \"r4_k5_s22_e6_i112_o192_se0.25\",\n        \"r1_k3_s11_e6_i192_o320_se0.25\",\n    ]\n    blocks_args = BlockDecoder.decode(blocks_args)\n\n    global_params = GlobalParams(\n        batch_norm_momentum=0.99,\n        batch_norm_epsilon=1e-3,\n        dropout_rate=dropout_rate,\n        drop_connect_rate=drop_connect_rate,\n        num_classes=num_classes,\n        width_coefficient=width_coefficient,\n        depth_coefficient=depth_coefficient,\n        depth_divisor=8,\n        min_depth=None,\n        image_size=image_size,\n    )\n    return blocks_args, global_params\n\n\nclass EffUtils:\n    @staticmethod\n    def round_filters(filters, global_params):\n        \"\"\"Calculate and round number of filters based on depth multiplier.\"\"\"\n        multiplier = global_params.width_coefficient\n        if not multiplier:\n            return filters\n        divisor = global_params.depth_divisor\n        min_depth = global_params.min_depth\n        filters *= multiplier\n        min_depth = min_depth or divisor\n        new_filters = max(min_depth, int(filters + divisor / 2) // divisor * divisor)\n        if new_filters < 0.9 * filters:\n            new_filters += divisor\n        return int(new_filters)\n\n    @staticmethod\n    def round_repeats(repeats, global_params):\n        \"\"\"Round number of filters based on depth multiplier.\"\"\"\n        multiplier = global_params.depth_coefficient\n        if not multiplier:\n            return repeats\n        return int(math.ceil(multiplier * repeats))\n\n\nclass MbConvBlock(nn.Layer):\n    def __init__(self, block_args):\n        super(MbConvBlock, self).__init__()\n        self._block_args = block_args\n        self.has_se = (self._block_args.se_ratio is not None) and (\n            0 < self._block_args.se_ratio <= 1\n        )\n        self.id_skip = block_args.id_skip\n\n        # expansion phase\n        self.inp = self._block_args.input_filters\n        oup = self._block_args.input_filters * self._block_args.expand_ratio\n        if self._block_args.expand_ratio != 1:\n            self._expand_conv = nn.Conv2D(self.inp, oup, 1, bias_attr=False)\n            self._bn0 = nn.BatchNorm(oup)\n\n        # depthwise conv phase\n        k = self._block_args.kernel_size\n        s = self._block_args.stride\n        if isinstance(s, list):\n            s = s[0]\n        self._depthwise_conv = nn.Conv2D(\n            oup,\n            oup,\n            groups=oup,\n            kernel_size=k,\n            stride=s,\n            padding=\"same\",\n            bias_attr=False,\n        )\n        self._bn1 = nn.BatchNorm(oup)\n\n        # squeeze and excitation layer, if desired\n        if self.has_se:\n            num_squeezed_channels = max(\n                1, int(self._block_args.input_filters * self._block_args.se_ratio)\n            )\n            self._se_reduce = nn.Conv2D(oup, num_squeezed_channels, 1)\n            self._se_expand = nn.Conv2D(num_squeezed_channels, oup, 1)\n\n        # output phase and some util class\n        self.final_oup = self._block_args.output_filters\n        self._project_conv = nn.Conv2D(oup, self.final_oup, 1, bias_attr=False)\n        self._bn2 = nn.BatchNorm(self.final_oup)\n        self._swish = nn.Swish()\n\n    def _drop_connect(self, inputs, p, training):\n        if not training:\n            return inputs\n        batch_size = inputs.shape[0]\n        keep_prob = 1 - p\n        random_tensor = keep_prob\n        random_tensor += paddle.rand([batch_size, 1, 1, 1], dtype=inputs.dtype)\n        random_tensor = paddle.to_tensor(random_tensor, place=inputs.place)\n        binary_tensor = paddle.floor(random_tensor)\n        output = inputs / keep_prob * binary_tensor\n        return output\n\n    def forward(self, inputs, drop_connect_rate=None):\n        # expansion and depthwise conv\n        x = inputs\n        if self._block_args.expand_ratio != 1:\n            x = self._swish(self._bn0(self._expand_conv(inputs)))\n        x = self._swish(self._bn1(self._depthwise_conv(x)))\n\n        # squeeze and excitation\n        if self.has_se:\n            x_squeezed = F.adaptive_avg_pool2d(x, 1)\n            x_squeezed = self._se_expand(self._swish(self._se_reduce(x_squeezed)))\n            x = F.sigmoid(x_squeezed) * x\n        x = self._bn2(self._project_conv(x))\n\n        # skip conntection and drop connect\n        if self.id_skip and self._block_args.stride == 1 and self.inp == self.final_oup:\n            if drop_connect_rate:\n                x = self._drop_connect(x, p=drop_connect_rate, training=self.training)\n            x = x + inputs\n        return x\n\n\nclass EfficientNetb3_PREN(nn.Layer):\n    def __init__(self, in_channels):\n        super(EfficientNetb3_PREN, self).__init__()\n        \"\"\"\n        the fllowing are efficientnetb3's superparams,\n        they means efficientnetb3 network's width, depth, resolution and\n        dropout respectively, to fit for text recognition task, the resolution\n        here is changed from 300 to 64.\n        \"\"\"\n        w, d, s, p = 1.2, 1.4, 64, 0.3\n        self._blocks_args, self._global_params = efficientnet(\n            width_coefficient=w, depth_coefficient=d, dropout_rate=p, image_size=s\n        )\n        self.out_channels = []\n        # stem\n        out_channels = EffUtils.round_filters(32, self._global_params)\n        self._conv_stem = nn.Conv2D(\n            in_channels, out_channels, 3, 2, padding=\"same\", bias_attr=False\n        )\n        self._bn0 = nn.BatchNorm(out_channels)\n\n        # build blocks\n        self._blocks = []\n        # to extract three feature maps for fpn based on efficientnetb3 backbone\n        self._concerned_block_idxes = [7, 17, 25]\n        _concerned_idx = 0\n        for i, block_args in enumerate(self._blocks_args):\n            block_args = block_args._replace(\n                input_filters=EffUtils.round_filters(\n                    block_args.input_filters, self._global_params\n                ),\n                output_filters=EffUtils.round_filters(\n                    block_args.output_filters, self._global_params\n                ),\n                num_repeat=EffUtils.round_repeats(\n                    block_args.num_repeat, self._global_params\n                ),\n            )\n            self._blocks.append(self.add_sublayer(f\"{i}-0\", MbConvBlock(block_args)))\n            _concerned_idx += 1\n            if _concerned_idx in self._concerned_block_idxes:\n                self.out_channels.append(block_args.output_filters)\n            if block_args.num_repeat > 1:\n                block_args = block_args._replace(\n                    input_filters=block_args.output_filters, stride=1\n                )\n            for j in range(block_args.num_repeat - 1):\n                self._blocks.append(\n                    self.add_sublayer(f\"{i}-{j+1}\", MbConvBlock(block_args))\n                )\n                _concerned_idx += 1\n                if _concerned_idx in self._concerned_block_idxes:\n                    self.out_channels.append(block_args.output_filters)\n\n        self._swish = nn.Swish()\n\n    def forward(self, inputs):\n        outs = []\n        x = self._swish(self._bn0(self._conv_stem(inputs)))\n        for idx, block in enumerate(self._blocks):\n            drop_connect_rate = self._global_params.drop_connect_rate\n            if drop_connect_rate:\n                drop_connect_rate *= float(idx) / len(self._blocks)\n            x = block(x, drop_connect_rate=drop_connect_rate)\n            if idx in self._concerned_block_idxes:\n                outs.append(x)\n        return outs\n", "ppocr/modeling/backbones/vqa_layoutlm.py": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nfrom paddle import nn\n\nfrom paddlenlp.transformers import (\n    LayoutXLMModel,\n    LayoutXLMForTokenClassification,\n    LayoutXLMForRelationExtraction,\n)\nfrom paddlenlp.transformers import LayoutLMModel, LayoutLMForTokenClassification\nfrom paddlenlp.transformers import (\n    LayoutLMv2Model,\n    LayoutLMv2ForTokenClassification,\n    LayoutLMv2ForRelationExtraction,\n)\nfrom paddlenlp.transformers import AutoModel\n\n__all__ = [\"LayoutXLMForSer\", \"LayoutLMForSer\"]\n\npretrained_model_dict = {\n    LayoutXLMModel: {\n        \"base\": \"layoutxlm-base-uncased\",\n        \"vi\": \"vi-layoutxlm-base-uncased\",\n    },\n    LayoutLMModel: {\n        \"base\": \"layoutlm-base-uncased\",\n    },\n    LayoutLMv2Model: {\n        \"base\": \"layoutlmv2-base-uncased\",\n        \"vi\": \"vi-layoutlmv2-base-uncased\",\n    },\n}\n\n\nclass NLPBaseModel(nn.Layer):\n    def __init__(\n        self,\n        base_model_class,\n        model_class,\n        mode=\"base\",\n        type=\"ser\",\n        pretrained=True,\n        checkpoints=None,\n        **kwargs,\n    ):\n        super(NLPBaseModel, self).__init__()\n        if checkpoints is not None:  # load the trained model\n            self.model = model_class.from_pretrained(checkpoints)\n        else:  # load the pretrained-model\n            pretrained_model_name = pretrained_model_dict[base_model_class][mode]\n            if type == \"ser\":\n                self.model = model_class.from_pretrained(\n                    pretrained_model_name, num_classes=kwargs[\"num_classes\"], dropout=0\n                )\n            else:\n                self.model = model_class.from_pretrained(\n                    pretrained_model_name, dropout=0\n                )\n        self.out_channels = 1\n        self.use_visual_backbone = True\n\n\nclass LayoutLMForSer(NLPBaseModel):\n    def __init__(\n        self, num_classes, pretrained=True, checkpoints=None, mode=\"base\", **kwargs\n    ):\n        super(LayoutLMForSer, self).__init__(\n            LayoutLMModel,\n            LayoutLMForTokenClassification,\n            mode,\n            \"ser\",\n            pretrained,\n            checkpoints,\n            num_classes=num_classes,\n        )\n        self.use_visual_backbone = False\n\n    def forward(self, x):\n        x = self.model(\n            input_ids=x[0],\n            bbox=x[1],\n            attention_mask=x[2],\n            token_type_ids=x[3],\n            position_ids=None,\n            output_hidden_states=False,\n        )\n        return x\n\n\nclass LayoutLMv2ForSer(NLPBaseModel):\n    def __init__(\n        self, num_classes, pretrained=True, checkpoints=None, mode=\"base\", **kwargs\n    ):\n        super(LayoutLMv2ForSer, self).__init__(\n            LayoutLMv2Model,\n            LayoutLMv2ForTokenClassification,\n            mode,\n            \"ser\",\n            pretrained,\n            checkpoints,\n            num_classes=num_classes,\n        )\n        if (\n            hasattr(self.model.layoutlmv2, \"use_visual_backbone\")\n            and self.model.layoutlmv2.use_visual_backbone is False\n        ):\n            self.use_visual_backbone = False\n\n    def forward(self, x):\n        if self.use_visual_backbone is True:\n            image = x[4]\n        else:\n            image = None\n        x = self.model(\n            input_ids=x[0],\n            bbox=x[1],\n            attention_mask=x[2],\n            token_type_ids=x[3],\n            image=image,\n            position_ids=None,\n            head_mask=None,\n            labels=None,\n        )\n        if self.training:\n            res = {\"backbone_out\": x[0]}\n            res.update(x[1])\n            return res\n        else:\n            return x\n\n\nclass LayoutXLMForSer(NLPBaseModel):\n    def __init__(\n        self, num_classes, pretrained=True, checkpoints=None, mode=\"base\", **kwargs\n    ):\n        super(LayoutXLMForSer, self).__init__(\n            LayoutXLMModel,\n            LayoutXLMForTokenClassification,\n            mode,\n            \"ser\",\n            pretrained,\n            checkpoints,\n            num_classes=num_classes,\n        )\n        if (\n            hasattr(self.model.layoutxlm, \"use_visual_backbone\")\n            and self.model.layoutxlm.use_visual_backbone is False\n        ):\n            self.use_visual_backbone = False\n\n    def forward(self, x):\n        if self.use_visual_backbone is True:\n            image = x[4]\n        else:\n            image = None\n        x = self.model(\n            input_ids=x[0],\n            bbox=x[1],\n            attention_mask=x[2],\n            token_type_ids=x[3],\n            image=image,\n            position_ids=None,\n            head_mask=None,\n            labels=None,\n        )\n        if self.training:\n            res = {\"backbone_out\": x[0]}\n            res.update(x[1])\n            return res\n        else:\n            return x\n\n\nclass LayoutLMv2ForRe(NLPBaseModel):\n    def __init__(self, pretrained=True, checkpoints=None, mode=\"base\", **kwargs):\n        super(LayoutLMv2ForRe, self).__init__(\n            LayoutLMv2Model,\n            LayoutLMv2ForRelationExtraction,\n            mode,\n            \"re\",\n            pretrained,\n            checkpoints,\n        )\n        if (\n            hasattr(self.model.layoutlmv2, \"use_visual_backbone\")\n            and self.model.layoutlmv2.use_visual_backbone is False\n        ):\n            self.use_visual_backbone = False\n\n    def forward(self, x):\n        x = self.model(\n            input_ids=x[0],\n            bbox=x[1],\n            attention_mask=x[2],\n            token_type_ids=x[3],\n            image=x[4],\n            position_ids=None,\n            head_mask=None,\n            labels=None,\n            entities=x[5],\n            relations=x[6],\n        )\n        return x\n\n\nclass LayoutXLMForRe(NLPBaseModel):\n    def __init__(self, pretrained=True, checkpoints=None, mode=\"base\", **kwargs):\n        super(LayoutXLMForRe, self).__init__(\n            LayoutXLMModel,\n            LayoutXLMForRelationExtraction,\n            mode,\n            \"re\",\n            pretrained,\n            checkpoints,\n        )\n        if (\n            hasattr(self.model.layoutxlm, \"use_visual_backbone\")\n            and self.model.layoutxlm.use_visual_backbone is False\n        ):\n            self.use_visual_backbone = False\n\n    def forward(self, x):\n        if self.use_visual_backbone is True:\n            image = x[4]\n            entities = x[5]\n            relations = x[6]\n        else:\n            image = None\n            entities = x[4]\n            relations = x[5]\n        x = self.model(\n            input_ids=x[0],\n            bbox=x[1],\n            attention_mask=x[2],\n            token_type_ids=x[3],\n            image=image,\n            position_ids=None,\n            head_mask=None,\n            labels=None,\n            entities=entities,\n            relations=relations,\n        )\n        return x\n", "ppocr/modeling/backbones/rec_vitstr.py": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/roatienza/deep-text-recognition-benchmark/blob/master/modules/vitstr.py\n\"\"\"\n\nimport numpy as np\nimport paddle\nimport paddle.nn as nn\nfrom ppocr.modeling.backbones.rec_svtrnet import (\n    Block,\n    PatchEmbed,\n    zeros_,\n    trunc_normal_,\n    ones_,\n)\n\nscale_dim_heads = {\"tiny\": [192, 3], \"small\": [384, 6], \"base\": [768, 12]}\n\n\nclass ViTSTR(nn.Layer):\n    def __init__(\n        self,\n        img_size=[224, 224],\n        in_channels=1,\n        scale=\"tiny\",\n        seqlen=27,\n        patch_size=[16, 16],\n        embed_dim=None,\n        depth=12,\n        num_heads=None,\n        mlp_ratio=4,\n        qkv_bias=True,\n        qk_scale=None,\n        drop_path_rate=0.0,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        norm_layer=\"nn.LayerNorm\",\n        act_layer=\"nn.GELU\",\n        epsilon=1e-6,\n        out_channels=None,\n        **kwargs,\n    ):\n        super().__init__()\n        self.seqlen = seqlen\n        embed_dim = embed_dim if embed_dim is not None else scale_dim_heads[scale][0]\n        num_heads = num_heads if num_heads is not None else scale_dim_heads[scale][1]\n        out_channels = out_channels if out_channels is not None else embed_dim\n        self.patch_embed = PatchEmbed(\n            img_size=img_size,\n            in_channels=in_channels,\n            embed_dim=embed_dim,\n            patch_size=patch_size,\n            mode=\"linear\",\n        )\n        num_patches = self.patch_embed.num_patches\n\n        self.pos_embed = self.create_parameter(\n            shape=[1, num_patches + 1, embed_dim], default_initializer=zeros_\n        )\n        self.add_parameter(\"pos_embed\", self.pos_embed)\n        self.cls_token = self.create_parameter(\n            shape=[1, 1, embed_dim], default_initializer=zeros_\n        )\n        self.add_parameter(\"cls_token\", self.cls_token)\n\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        dpr = np.linspace(0, drop_path_rate, depth)\n        self.blocks = nn.LayerList(\n            [\n                Block(\n                    dim=embed_dim,\n                    num_heads=num_heads,\n                    mlp_ratio=mlp_ratio,\n                    qkv_bias=qkv_bias,\n                    qk_scale=qk_scale,\n                    drop=drop_rate,\n                    attn_drop=attn_drop_rate,\n                    drop_path=dpr[i],\n                    norm_layer=norm_layer,\n                    act_layer=eval(act_layer),\n                    epsilon=epsilon,\n                    prenorm=False,\n                )\n                for i in range(depth)\n            ]\n        )\n        self.norm = eval(norm_layer)(embed_dim, epsilon=epsilon)\n\n        self.out_channels = out_channels\n\n        trunc_normal_(self.pos_embed)\n        trunc_normal_(self.cls_token)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                zeros_(m.bias)\n        elif isinstance(m, nn.LayerNorm):\n            zeros_(m.bias)\n            ones_(m.weight)\n\n    def forward_features(self, x):\n        B = x.shape[0]\n        x = self.patch_embed(x)\n        cls_tokens = paddle.tile(self.cls_token, repeat_times=[B, 1, 1])\n        x = paddle.concat((cls_tokens, x), axis=1)\n        x = x + self.pos_embed\n        x = self.pos_drop(x)\n        for blk in self.blocks:\n            x = blk(x)\n        x = self.norm(x)\n        return x\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = x[:, : self.seqlen]\n        return x.transpose([0, 2, 1]).unsqueeze(2)\n", "ppocr/modeling/backbones/det_pp_lcnet.py": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\nimport paddle\nimport paddle.nn as nn\nfrom paddle import ParamAttr\nfrom paddle.nn import AdaptiveAvgPool2D, BatchNorm, Conv2D, Dropout, Linear\nfrom paddle.regularizer import L2Decay\nfrom paddle.nn.initializer import KaimingNormal\nfrom paddle.utils.download import get_path_from_url\n\nMODEL_URLS = {\n    \"PPLCNet_x0.25\": \"https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/legendary_models/PPLCNet_x0_25_pretrained.pdparams\",\n    \"PPLCNet_x0.35\": \"https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/legendary_models/PPLCNet_x0_35_pretrained.pdparams\",\n    \"PPLCNet_x0.5\": \"https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/legendary_models/PPLCNet_x0_5_pretrained.pdparams\",\n    \"PPLCNet_x0.75\": \"https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/legendary_models/PPLCNet_x0_75_pretrained.pdparams\",\n    \"PPLCNet_x1.0\": \"https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/legendary_models/PPLCNet_x1_0_pretrained.pdparams\",\n    \"PPLCNet_x1.5\": \"https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/legendary_models/PPLCNet_x1_5_pretrained.pdparams\",\n    \"PPLCNet_x2.0\": \"https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/legendary_models/PPLCNet_x2_0_pretrained.pdparams\",\n    \"PPLCNet_x2.5\": \"https://paddle-imagenet-models-name.bj.bcebos.com/dygraph/legendary_models/PPLCNet_x2_5_pretrained.pdparams\",\n}\n\nMODEL_STAGES_PATTERN = {\n    \"PPLCNet\": [\"blocks2\", \"blocks3\", \"blocks4\", \"blocks5\", \"blocks6\"]\n}\n\n__all__ = list(MODEL_URLS.keys())\n\n# Each element(list) represents a depthwise block, which is composed of k, in_c, out_c, s, use_se.\n# k: kernel_size\n# in_c: input channel number in depthwise block\n# out_c: output channel number in depthwise block\n# s: stride in depthwise block\n# use_se: whether to use SE block\n\nNET_CONFIG = {\n    \"blocks2\":\n    # k, in_c, out_c, s, use_se\n    [[3, 16, 32, 1, False]],\n    \"blocks3\": [[3, 32, 64, 2, False], [3, 64, 64, 1, False]],\n    \"blocks4\": [[3, 64, 128, 2, False], [3, 128, 128, 1, False]],\n    \"blocks5\": [\n        [3, 128, 256, 2, False],\n        [5, 256, 256, 1, False],\n        [5, 256, 256, 1, False],\n        [5, 256, 256, 1, False],\n        [5, 256, 256, 1, False],\n        [5, 256, 256, 1, False],\n    ],\n    \"blocks6\": [[5, 256, 512, 2, True], [5, 512, 512, 1, True]],\n}\n\n\ndef make_divisible(v, divisor=8, min_value=None):\n    if min_value is None:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    if new_v < 0.9 * v:\n        new_v += divisor\n    return new_v\n\n\nclass ConvBNLayer(nn.Layer):\n    def __init__(self, num_channels, filter_size, num_filters, stride, num_groups=1):\n        super().__init__()\n\n        self.conv = Conv2D(\n            in_channels=num_channels,\n            out_channels=num_filters,\n            kernel_size=filter_size,\n            stride=stride,\n            padding=(filter_size - 1) // 2,\n            groups=num_groups,\n            weight_attr=ParamAttr(initializer=KaimingNormal()),\n            bias_attr=False,\n        )\n\n        self.bn = BatchNorm(\n            num_filters,\n            param_attr=ParamAttr(regularizer=L2Decay(0.0)),\n            bias_attr=ParamAttr(regularizer=L2Decay(0.0)),\n        )\n        self.hardswish = nn.Hardswish()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.hardswish(x)\n        return x\n\n\nclass DepthwiseSeparable(nn.Layer):\n    def __init__(self, num_channels, num_filters, stride, dw_size=3, use_se=False):\n        super().__init__()\n        self.use_se = use_se\n        self.dw_conv = ConvBNLayer(\n            num_channels=num_channels,\n            num_filters=num_channels,\n            filter_size=dw_size,\n            stride=stride,\n            num_groups=num_channels,\n        )\n        if use_se:\n            self.se = SEModule(num_channels)\n        self.pw_conv = ConvBNLayer(\n            num_channels=num_channels, filter_size=1, num_filters=num_filters, stride=1\n        )\n\n    def forward(self, x):\n        x = self.dw_conv(x)\n        if self.use_se:\n            x = self.se(x)\n        x = self.pw_conv(x)\n        return x\n\n\nclass SEModule(nn.Layer):\n    def __init__(self, channel, reduction=4):\n        super().__init__()\n        self.avg_pool = AdaptiveAvgPool2D(1)\n        self.conv1 = Conv2D(\n            in_channels=channel,\n            out_channels=channel // reduction,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n        )\n        self.relu = nn.ReLU()\n        self.conv2 = Conv2D(\n            in_channels=channel // reduction,\n            out_channels=channel,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n        )\n        self.hardsigmoid = nn.Hardsigmoid()\n\n    def forward(self, x):\n        identity = x\n        x = self.avg_pool(x)\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.hardsigmoid(x)\n        x = paddle.multiply(x=identity, y=x)\n        return x\n\n\nclass PPLCNet(nn.Layer):\n    def __init__(self, in_channels=3, scale=1.0, pretrained=False, use_ssld=False):\n        super().__init__()\n        self.out_channels = [\n            int(NET_CONFIG[\"blocks3\"][-1][2] * scale),\n            int(NET_CONFIG[\"blocks4\"][-1][2] * scale),\n            int(NET_CONFIG[\"blocks5\"][-1][2] * scale),\n            int(NET_CONFIG[\"blocks6\"][-1][2] * scale),\n        ]\n        self.scale = scale\n\n        self.conv1 = ConvBNLayer(\n            num_channels=in_channels,\n            filter_size=3,\n            num_filters=make_divisible(16 * scale),\n            stride=2,\n        )\n\n        self.blocks2 = nn.Sequential(\n            *[\n                DepthwiseSeparable(\n                    num_channels=make_divisible(in_c * scale),\n                    num_filters=make_divisible(out_c * scale),\n                    dw_size=k,\n                    stride=s,\n                    use_se=se,\n                )\n                for i, (k, in_c, out_c, s, se) in enumerate(NET_CONFIG[\"blocks2\"])\n            ]\n        )\n\n        self.blocks3 = nn.Sequential(\n            *[\n                DepthwiseSeparable(\n                    num_channels=make_divisible(in_c * scale),\n                    num_filters=make_divisible(out_c * scale),\n                    dw_size=k,\n                    stride=s,\n                    use_se=se,\n                )\n                for i, (k, in_c, out_c, s, se) in enumerate(NET_CONFIG[\"blocks3\"])\n            ]\n        )\n\n        self.blocks4 = nn.Sequential(\n            *[\n                DepthwiseSeparable(\n                    num_channels=make_divisible(in_c * scale),\n                    num_filters=make_divisible(out_c * scale),\n                    dw_size=k,\n                    stride=s,\n                    use_se=se,\n                )\n                for i, (k, in_c, out_c, s, se) in enumerate(NET_CONFIG[\"blocks4\"])\n            ]\n        )\n\n        self.blocks5 = nn.Sequential(\n            *[\n                DepthwiseSeparable(\n                    num_channels=make_divisible(in_c * scale),\n                    num_filters=make_divisible(out_c * scale),\n                    dw_size=k,\n                    stride=s,\n                    use_se=se,\n                )\n                for i, (k, in_c, out_c, s, se) in enumerate(NET_CONFIG[\"blocks5\"])\n            ]\n        )\n\n        self.blocks6 = nn.Sequential(\n            *[\n                DepthwiseSeparable(\n                    num_channels=make_divisible(in_c * scale),\n                    num_filters=make_divisible(out_c * scale),\n                    dw_size=k,\n                    stride=s,\n                    use_se=se,\n                )\n                for i, (k, in_c, out_c, s, se) in enumerate(NET_CONFIG[\"blocks6\"])\n            ]\n        )\n\n        if pretrained:\n            self._load_pretrained(\n                MODEL_URLS[\"PPLCNet_x{}\".format(scale)], use_ssld=use_ssld\n            )\n\n    def forward(self, x):\n        outs = []\n        x = self.conv1(x)\n        x = self.blocks2(x)\n        x = self.blocks3(x)\n        outs.append(x)\n        x = self.blocks4(x)\n        outs.append(x)\n        x = self.blocks5(x)\n        outs.append(x)\n        x = self.blocks6(x)\n        outs.append(x)\n        return outs\n\n    def _load_pretrained(self, pretrained_url, use_ssld=False):\n        if use_ssld:\n            pretrained_url = pretrained_url.replace(\"_pretrained\", \"_ssld_pretrained\")\n        print(pretrained_url)\n        local_weight_path = get_path_from_url(\n            pretrained_url, os.path.expanduser(\"~/.paddleclas/weights\")\n        )\n        param_state_dict = paddle.load(local_weight_path)\n        self.set_dict(param_state_dict)\n        return\n", "ppocr/modeling/backbones/rec_mv1_enhance.py": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# This code is refer from: https://github.com/PaddlePaddle/PaddleClas/blob/develop/ppcls/arch/backbone/legendary_models/pp_lcnet.py\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport numpy as np\nimport paddle\nfrom paddle import ParamAttr, reshape, transpose\nimport paddle.nn as nn\nimport paddle.nn.functional as F\nfrom paddle.nn import Conv2D, BatchNorm, Linear, Dropout\nfrom paddle.nn import AdaptiveAvgPool2D, MaxPool2D, AvgPool2D\nfrom paddle.nn.initializer import KaimingNormal\nfrom paddle.regularizer import L2Decay\nfrom paddle.nn.functional import hardswish, hardsigmoid\n\n\nclass ConvBNLayer(nn.Layer):\n    def __init__(\n        self,\n        num_channels,\n        filter_size,\n        num_filters,\n        stride,\n        padding,\n        channels=None,\n        num_groups=1,\n        act=\"hard_swish\",\n    ):\n        super(ConvBNLayer, self).__init__()\n\n        self._conv = Conv2D(\n            in_channels=num_channels,\n            out_channels=num_filters,\n            kernel_size=filter_size,\n            stride=stride,\n            padding=padding,\n            groups=num_groups,\n            weight_attr=ParamAttr(initializer=KaimingNormal()),\n            bias_attr=False,\n        )\n\n        self._batch_norm = BatchNorm(\n            num_filters,\n            act=act,\n            param_attr=ParamAttr(regularizer=L2Decay(0.0)),\n            bias_attr=ParamAttr(regularizer=L2Decay(0.0)),\n        )\n\n    def forward(self, inputs):\n        y = self._conv(inputs)\n        y = self._batch_norm(y)\n        return y\n\n\nclass DepthwiseSeparable(nn.Layer):\n    def __init__(\n        self,\n        num_channels,\n        num_filters1,\n        num_filters2,\n        num_groups,\n        stride,\n        scale,\n        dw_size=3,\n        padding=1,\n        use_se=False,\n    ):\n        super(DepthwiseSeparable, self).__init__()\n        self.use_se = use_se\n        self._depthwise_conv = ConvBNLayer(\n            num_channels=num_channels,\n            num_filters=int(num_filters1 * scale),\n            filter_size=dw_size,\n            stride=stride,\n            padding=padding,\n            num_groups=int(num_groups * scale),\n        )\n        if use_se:\n            self._se = SEModule(int(num_filters1 * scale))\n        self._pointwise_conv = ConvBNLayer(\n            num_channels=int(num_filters1 * scale),\n            filter_size=1,\n            num_filters=int(num_filters2 * scale),\n            stride=1,\n            padding=0,\n        )\n\n    def forward(self, inputs):\n        y = self._depthwise_conv(inputs)\n        if self.use_se:\n            y = self._se(y)\n        y = self._pointwise_conv(y)\n        return y\n\n\nclass MobileNetV1Enhance(nn.Layer):\n    def __init__(\n        self,\n        in_channels=3,\n        scale=0.5,\n        last_conv_stride=1,\n        last_pool_type=\"max\",\n        last_pool_kernel_size=[3, 2],\n        **kwargs,\n    ):\n        super().__init__()\n        self.scale = scale\n        self.block_list = []\n\n        self.conv1 = ConvBNLayer(\n            num_channels=3,\n            filter_size=3,\n            channels=3,\n            num_filters=int(32 * scale),\n            stride=2,\n            padding=1,\n        )\n\n        conv2_1 = DepthwiseSeparable(\n            num_channels=int(32 * scale),\n            num_filters1=32,\n            num_filters2=64,\n            num_groups=32,\n            stride=1,\n            scale=scale,\n        )\n        self.block_list.append(conv2_1)\n\n        conv2_2 = DepthwiseSeparable(\n            num_channels=int(64 * scale),\n            num_filters1=64,\n            num_filters2=128,\n            num_groups=64,\n            stride=1,\n            scale=scale,\n        )\n        self.block_list.append(conv2_2)\n\n        conv3_1 = DepthwiseSeparable(\n            num_channels=int(128 * scale),\n            num_filters1=128,\n            num_filters2=128,\n            num_groups=128,\n            stride=1,\n            scale=scale,\n        )\n        self.block_list.append(conv3_1)\n\n        conv3_2 = DepthwiseSeparable(\n            num_channels=int(128 * scale),\n            num_filters1=128,\n            num_filters2=256,\n            num_groups=128,\n            stride=(2, 1),\n            scale=scale,\n        )\n        self.block_list.append(conv3_2)\n\n        conv4_1 = DepthwiseSeparable(\n            num_channels=int(256 * scale),\n            num_filters1=256,\n            num_filters2=256,\n            num_groups=256,\n            stride=1,\n            scale=scale,\n        )\n        self.block_list.append(conv4_1)\n\n        conv4_2 = DepthwiseSeparable(\n            num_channels=int(256 * scale),\n            num_filters1=256,\n            num_filters2=512,\n            num_groups=256,\n            stride=(2, 1),\n            scale=scale,\n        )\n        self.block_list.append(conv4_2)\n\n        for _ in range(5):\n            conv5 = DepthwiseSeparable(\n                num_channels=int(512 * scale),\n                num_filters1=512,\n                num_filters2=512,\n                num_groups=512,\n                stride=1,\n                dw_size=5,\n                padding=2,\n                scale=scale,\n                use_se=False,\n            )\n            self.block_list.append(conv5)\n\n        conv5_6 = DepthwiseSeparable(\n            num_channels=int(512 * scale),\n            num_filters1=512,\n            num_filters2=1024,\n            num_groups=512,\n            stride=(2, 1),\n            dw_size=5,\n            padding=2,\n            scale=scale,\n            use_se=True,\n        )\n        self.block_list.append(conv5_6)\n\n        conv6 = DepthwiseSeparable(\n            num_channels=int(1024 * scale),\n            num_filters1=1024,\n            num_filters2=1024,\n            num_groups=1024,\n            stride=last_conv_stride,\n            dw_size=5,\n            padding=2,\n            use_se=True,\n            scale=scale,\n        )\n        self.block_list.append(conv6)\n\n        self.block_list = nn.Sequential(*self.block_list)\n        if last_pool_type == \"avg\":\n            self.pool = nn.AvgPool2D(\n                kernel_size=last_pool_kernel_size,\n                stride=last_pool_kernel_size,\n                padding=0,\n            )\n        else:\n            self.pool = nn.MaxPool2D(kernel_size=2, stride=2, padding=0)\n        self.out_channels = int(1024 * scale)\n\n    def forward(self, inputs):\n        y = self.conv1(inputs)\n        y = self.block_list(y)\n        y = self.pool(y)\n        return y\n\n\nclass SEModule(nn.Layer):\n    def __init__(self, channel, reduction=4):\n        super(SEModule, self).__init__()\n        self.avg_pool = AdaptiveAvgPool2D(1)\n        self.conv1 = Conv2D(\n            in_channels=channel,\n            out_channels=channel // reduction,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            weight_attr=ParamAttr(),\n            bias_attr=ParamAttr(),\n        )\n        self.conv2 = Conv2D(\n            in_channels=channel // reduction,\n            out_channels=channel,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            weight_attr=ParamAttr(),\n            bias_attr=ParamAttr(),\n        )\n\n    def forward(self, inputs):\n        outputs = self.avg_pool(inputs)\n        outputs = self.conv1(outputs)\n        outputs = F.relu(outputs)\n        outputs = self.conv2(outputs)\n        outputs = hardsigmoid(outputs)\n        return paddle.multiply(x=inputs, y=outputs)\n", "ppocr/modeling/backbones/rec_micronet.py": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/liyunsheng13/micronet/blob/main/backbone/micronet.py\nhttps://github.com/liyunsheng13/micronet/blob/main/backbone/activation.py\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nimport paddle.nn as nn\n\nfrom ppocr.modeling.backbones.det_mobilenet_v3 import make_divisible\n\nM0_cfgs = [\n    # s, n, c, ks, c1, c2, g1, g2, c3, g3, g4, y1, y2, y3, r\n    [2, 1, 8, 3, 2, 2, 0, 4, 8, 2, 2, 2, 0, 1, 1],\n    [2, 1, 12, 3, 2, 2, 0, 8, 12, 4, 4, 2, 2, 1, 1],\n    [2, 1, 16, 5, 2, 2, 0, 12, 16, 4, 4, 2, 2, 1, 1],\n    [1, 1, 32, 5, 1, 4, 4, 4, 32, 4, 4, 2, 2, 1, 1],\n    [2, 1, 64, 5, 1, 4, 8, 8, 64, 8, 8, 2, 2, 1, 1],\n    [1, 1, 96, 3, 1, 4, 8, 8, 96, 8, 8, 2, 2, 1, 2],\n    [1, 1, 384, 3, 1, 4, 12, 12, 0, 0, 0, 2, 2, 1, 2],\n]\nM1_cfgs = [\n    # s, n, c, ks, c1, c2, g1, g2, c3, g3, g4\n    [2, 1, 8, 3, 2, 2, 0, 6, 8, 2, 2, 2, 0, 1, 1],\n    [2, 1, 16, 3, 2, 2, 0, 8, 16, 4, 4, 2, 2, 1, 1],\n    [2, 1, 16, 5, 2, 2, 0, 16, 16, 4, 4, 2, 2, 1, 1],\n    [1, 1, 32, 5, 1, 6, 4, 4, 32, 4, 4, 2, 2, 1, 1],\n    [2, 1, 64, 5, 1, 6, 8, 8, 64, 8, 8, 2, 2, 1, 1],\n    [1, 1, 96, 3, 1, 6, 8, 8, 96, 8, 8, 2, 2, 1, 2],\n    [1, 1, 576, 3, 1, 6, 12, 12, 0, 0, 0, 2, 2, 1, 2],\n]\nM2_cfgs = [\n    # s, n, c, ks, c1, c2, g1, g2, c3, g3, g4\n    [2, 1, 12, 3, 2, 2, 0, 8, 12, 4, 4, 2, 0, 1, 1],\n    [2, 1, 16, 3, 2, 2, 0, 12, 16, 4, 4, 2, 2, 1, 1],\n    [1, 1, 24, 3, 2, 2, 0, 16, 24, 4, 4, 2, 2, 1, 1],\n    [2, 1, 32, 5, 1, 6, 6, 6, 32, 4, 4, 2, 2, 1, 1],\n    [1, 1, 32, 5, 1, 6, 8, 8, 32, 4, 4, 2, 2, 1, 2],\n    [1, 1, 64, 5, 1, 6, 8, 8, 64, 8, 8, 2, 2, 1, 2],\n    [2, 1, 96, 5, 1, 6, 8, 8, 96, 8, 8, 2, 2, 1, 2],\n    [1, 1, 128, 3, 1, 6, 12, 12, 128, 8, 8, 2, 2, 1, 2],\n    [1, 1, 768, 3, 1, 6, 16, 16, 0, 0, 0, 2, 2, 1, 2],\n]\nM3_cfgs = [\n    # s, n, c, ks, c1, c2, g1, g2, c3, g3, g4\n    [2, 1, 16, 3, 2, 2, 0, 12, 16, 4, 4, 0, 2, 0, 1],\n    [2, 1, 24, 3, 2, 2, 0, 16, 24, 4, 4, 0, 2, 0, 1],\n    [1, 1, 24, 3, 2, 2, 0, 24, 24, 4, 4, 0, 2, 0, 1],\n    [2, 1, 32, 5, 1, 6, 6, 6, 32, 4, 4, 0, 2, 0, 1],\n    [1, 1, 32, 5, 1, 6, 8, 8, 32, 4, 4, 0, 2, 0, 2],\n    [1, 1, 64, 5, 1, 6, 8, 8, 48, 8, 8, 0, 2, 0, 2],\n    [1, 1, 80, 5, 1, 6, 8, 8, 80, 8, 8, 0, 2, 0, 2],\n    [1, 1, 80, 5, 1, 6, 10, 10, 80, 8, 8, 0, 2, 0, 2],\n    [1, 1, 120, 5, 1, 6, 10, 10, 120, 10, 10, 0, 2, 0, 2],\n    [1, 1, 120, 5, 1, 6, 12, 12, 120, 10, 10, 0, 2, 0, 2],\n    [1, 1, 144, 3, 1, 6, 12, 12, 144, 12, 12, 0, 2, 0, 2],\n    [1, 1, 432, 3, 1, 3, 12, 12, 0, 0, 0, 0, 2, 0, 2],\n]\n\n\ndef get_micronet_config(mode):\n    return eval(mode + \"_cfgs\")\n\n\nclass MaxGroupPooling(nn.Layer):\n    def __init__(self, channel_per_group=2):\n        super(MaxGroupPooling, self).__init__()\n        self.channel_per_group = channel_per_group\n\n    def forward(self, x):\n        if self.channel_per_group == 1:\n            return x\n        # max op\n        b, c, h, w = x.shape\n\n        # reshape\n        y = paddle.reshape(x, [b, c // self.channel_per_group, -1, h, w])\n        out = paddle.max(y, axis=2)\n        return out\n\n\nclass SpatialSepConvSF(nn.Layer):\n    def __init__(self, inp, oups, kernel_size, stride):\n        super(SpatialSepConvSF, self).__init__()\n\n        oup1, oup2 = oups\n        self.conv = nn.Sequential(\n            nn.Conv2D(\n                inp,\n                oup1,\n                (kernel_size, 1),\n                (stride, 1),\n                (kernel_size // 2, 0),\n                bias_attr=False,\n                groups=1,\n            ),\n            nn.BatchNorm2D(oup1),\n            nn.Conv2D(\n                oup1,\n                oup1 * oup2,\n                (1, kernel_size),\n                (1, stride),\n                (0, kernel_size // 2),\n                bias_attr=False,\n                groups=oup1,\n            ),\n            nn.BatchNorm2D(oup1 * oup2),\n            ChannelShuffle(oup1),\n        )\n\n    def forward(self, x):\n        out = self.conv(x)\n        return out\n\n\nclass ChannelShuffle(nn.Layer):\n    def __init__(self, groups):\n        super(ChannelShuffle, self).__init__()\n        self.groups = groups\n\n    def forward(self, x):\n        b, c, h, w = x.shape\n\n        channels_per_group = c // self.groups\n\n        # reshape\n        x = paddle.reshape(x, [b, self.groups, channels_per_group, h, w])\n\n        x = paddle.transpose(x, (0, 2, 1, 3, 4))\n        out = paddle.reshape(x, [b, -1, h, w])\n\n        return out\n\n\nclass StemLayer(nn.Layer):\n    def __init__(self, inp, oup, stride, groups=(4, 4)):\n        super(StemLayer, self).__init__()\n\n        g1, g2 = groups\n        self.stem = nn.Sequential(\n            SpatialSepConvSF(inp, groups, 3, stride),\n            MaxGroupPooling(2) if g1 * g2 == 2 * oup else nn.ReLU6(),\n        )\n\n    def forward(self, x):\n        out = self.stem(x)\n        return out\n\n\nclass DepthSpatialSepConv(nn.Layer):\n    def __init__(self, inp, expand, kernel_size, stride):\n        super(DepthSpatialSepConv, self).__init__()\n\n        exp1, exp2 = expand\n\n        hidden_dim = inp * exp1\n        oup = inp * exp1 * exp2\n\n        self.conv = nn.Sequential(\n            nn.Conv2D(\n                inp,\n                inp * exp1,\n                (kernel_size, 1),\n                (stride, 1),\n                (kernel_size // 2, 0),\n                bias_attr=False,\n                groups=inp,\n            ),\n            nn.BatchNorm2D(inp * exp1),\n            nn.Conv2D(\n                hidden_dim,\n                oup,\n                (1, kernel_size),\n                1,\n                (0, kernel_size // 2),\n                bias_attr=False,\n                groups=hidden_dim,\n            ),\n            nn.BatchNorm2D(oup),\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\n\nclass GroupConv(nn.Layer):\n    def __init__(self, inp, oup, groups=2):\n        super(GroupConv, self).__init__()\n        self.inp = inp\n        self.oup = oup\n        self.groups = groups\n        self.conv = nn.Sequential(\n            nn.Conv2D(inp, oup, 1, 1, 0, bias_attr=False, groups=self.groups[0]),\n            nn.BatchNorm2D(oup),\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\n\nclass DepthConv(nn.Layer):\n    def __init__(self, inp, oup, kernel_size, stride):\n        super(DepthConv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2D(\n                inp,\n                oup,\n                kernel_size,\n                stride,\n                kernel_size // 2,\n                bias_attr=False,\n                groups=inp,\n            ),\n            nn.BatchNorm2D(oup),\n        )\n\n    def forward(self, x):\n        out = self.conv(x)\n        return out\n\n\nclass DYShiftMax(nn.Layer):\n    def __init__(\n        self,\n        inp,\n        oup,\n        reduction=4,\n        act_max=1.0,\n        act_relu=True,\n        init_a=[0.0, 0.0],\n        init_b=[0.0, 0.0],\n        relu_before_pool=False,\n        g=None,\n        expansion=False,\n    ):\n        super(DYShiftMax, self).__init__()\n        self.oup = oup\n        self.act_max = act_max * 2\n        self.act_relu = act_relu\n        self.avg_pool = nn.Sequential(\n            nn.ReLU() if relu_before_pool == True else nn.Sequential(),\n            nn.AdaptiveAvgPool2D(1),\n        )\n\n        self.exp = 4 if act_relu else 2\n        self.init_a = init_a\n        self.init_b = init_b\n\n        # determine squeeze\n        squeeze = make_divisible(inp // reduction, 4)\n        if squeeze < 4:\n            squeeze = 4\n\n        self.fc = nn.Sequential(\n            nn.Linear(inp, squeeze),\n            nn.ReLU(),\n            nn.Linear(squeeze, oup * self.exp),\n            nn.Hardsigmoid(),\n        )\n\n        if g is None:\n            g = 1\n        self.g = g[1]\n        if self.g != 1 and expansion:\n            self.g = inp // self.g\n\n        self.gc = inp // self.g\n        index = paddle.to_tensor([range(inp)])\n        index = paddle.reshape(index, [1, inp, 1, 1])\n        index = paddle.reshape(index, [1, self.g, self.gc, 1, 1])\n        indexgs = paddle.split(index, [1, self.g - 1], axis=1)\n        indexgs = paddle.concat((indexgs[1], indexgs[0]), axis=1)\n        indexs = paddle.split(indexgs, [1, self.gc - 1], axis=2)\n        indexs = paddle.concat((indexs[1], indexs[0]), axis=2)\n        self.index = paddle.reshape(indexs, [inp])\n        self.expansion = expansion\n\n    def forward(self, x):\n        x_in = x\n        x_out = x\n\n        b, c, _, _ = x_in.shape\n        y = self.avg_pool(x_in)\n        y = paddle.reshape(y, [b, c])\n        y = self.fc(y)\n        y = paddle.reshape(y, [b, self.oup * self.exp, 1, 1])\n        y = (y - 0.5) * self.act_max\n\n        n2, c2, h2, w2 = x_out.shape\n        x2 = paddle.to_tensor(x_out.numpy()[:, self.index.numpy(), :, :])\n\n        if self.exp == 4:\n            temp = y.shape\n            a1, b1, a2, b2 = paddle.split(y, temp[1] // self.oup, axis=1)\n\n            a1 = a1 + self.init_a[0]\n            a2 = a2 + self.init_a[1]\n\n            b1 = b1 + self.init_b[0]\n            b2 = b2 + self.init_b[1]\n\n            z1 = x_out * a1 + x2 * b1\n            z2 = x_out * a2 + x2 * b2\n\n            out = paddle.maximum(z1, z2)\n\n        elif self.exp == 2:\n            temp = y.shape\n            a1, b1 = paddle.split(y, temp[1] // self.oup, axis=1)\n            a1 = a1 + self.init_a[0]\n            b1 = b1 + self.init_b[0]\n            out = x_out * a1 + x2 * b1\n\n        return out\n\n\nclass DYMicroBlock(nn.Layer):\n    def __init__(\n        self,\n        inp,\n        oup,\n        kernel_size=3,\n        stride=1,\n        ch_exp=(2, 2),\n        ch_per_group=4,\n        groups_1x1=(1, 1),\n        depthsep=True,\n        shuffle=False,\n        activation_cfg=None,\n    ):\n        super(DYMicroBlock, self).__init__()\n\n        self.identity = stride == 1 and inp == oup\n\n        y1, y2, y3 = activation_cfg[\"dy\"]\n        act_reduction = 8 * activation_cfg[\"ratio\"]\n        init_a = activation_cfg[\"init_a\"]\n        init_b = activation_cfg[\"init_b\"]\n\n        t1 = ch_exp\n        gs1 = ch_per_group\n        hidden_fft, g1, g2 = groups_1x1\n        hidden_dim2 = inp * t1[0] * t1[1]\n\n        if gs1[0] == 0:\n            self.layers = nn.Sequential(\n                DepthSpatialSepConv(inp, t1, kernel_size, stride),\n                (\n                    DYShiftMax(\n                        hidden_dim2,\n                        hidden_dim2,\n                        act_max=2.0,\n                        act_relu=True if y2 == 2 else False,\n                        init_a=init_a,\n                        reduction=act_reduction,\n                        init_b=init_b,\n                        g=gs1,\n                        expansion=False,\n                    )\n                    if y2 > 0\n                    else nn.ReLU6()\n                ),\n                ChannelShuffle(gs1[1]) if shuffle else nn.Sequential(),\n                (\n                    ChannelShuffle(hidden_dim2 // 2)\n                    if shuffle and y2 != 0\n                    else nn.Sequential()\n                ),\n                GroupConv(hidden_dim2, oup, (g1, g2)),\n                (\n                    DYShiftMax(\n                        oup,\n                        oup,\n                        act_max=2.0,\n                        act_relu=False,\n                        init_a=[1.0, 0.0],\n                        reduction=act_reduction // 2,\n                        init_b=[0.0, 0.0],\n                        g=(g1, g2),\n                        expansion=False,\n                    )\n                    if y3 > 0\n                    else nn.Sequential()\n                ),\n                ChannelShuffle(g2) if shuffle else nn.Sequential(),\n                (\n                    ChannelShuffle(oup // 2)\n                    if shuffle and oup % 2 == 0 and y3 != 0\n                    else nn.Sequential()\n                ),\n            )\n        elif g2 == 0:\n            self.layers = nn.Sequential(\n                GroupConv(inp, hidden_dim2, gs1),\n                (\n                    DYShiftMax(\n                        hidden_dim2,\n                        hidden_dim2,\n                        act_max=2.0,\n                        act_relu=False,\n                        init_a=[1.0, 0.0],\n                        reduction=act_reduction,\n                        init_b=[0.0, 0.0],\n                        g=gs1,\n                        expansion=False,\n                    )\n                    if y3 > 0\n                    else nn.Sequential()\n                ),\n            )\n        else:\n            self.layers = nn.Sequential(\n                GroupConv(inp, hidden_dim2, gs1),\n                (\n                    DYShiftMax(\n                        hidden_dim2,\n                        hidden_dim2,\n                        act_max=2.0,\n                        act_relu=True if y1 == 2 else False,\n                        init_a=init_a,\n                        reduction=act_reduction,\n                        init_b=init_b,\n                        g=gs1,\n                        expansion=False,\n                    )\n                    if y1 > 0\n                    else nn.ReLU6()\n                ),\n                ChannelShuffle(gs1[1]) if shuffle else nn.Sequential(),\n                (\n                    DepthSpatialSepConv(hidden_dim2, (1, 1), kernel_size, stride)\n                    if depthsep\n                    else DepthConv(hidden_dim2, hidden_dim2, kernel_size, stride)\n                ),\n                nn.Sequential(),\n                (\n                    DYShiftMax(\n                        hidden_dim2,\n                        hidden_dim2,\n                        act_max=2.0,\n                        act_relu=True if y2 == 2 else False,\n                        init_a=init_a,\n                        reduction=act_reduction,\n                        init_b=init_b,\n                        g=gs1,\n                        expansion=True,\n                    )\n                    if y2 > 0\n                    else nn.ReLU6()\n                ),\n                (\n                    ChannelShuffle(hidden_dim2 // 4)\n                    if shuffle and y1 != 0 and y2 != 0\n                    else (\n                        nn.Sequential()\n                        if y1 == 0 and y2 == 0\n                        else ChannelShuffle(hidden_dim2 // 2)\n                    )\n                ),\n                GroupConv(hidden_dim2, oup, (g1, g2)),\n                (\n                    DYShiftMax(\n                        oup,\n                        oup,\n                        act_max=2.0,\n                        act_relu=False,\n                        init_a=[1.0, 0.0],\n                        reduction=(\n                            act_reduction // 2 if oup < hidden_dim2 else act_reduction\n                        ),\n                        init_b=[0.0, 0.0],\n                        g=(g1, g2),\n                        expansion=False,\n                    )\n                    if y3 > 0\n                    else nn.Sequential()\n                ),\n                ChannelShuffle(g2) if shuffle else nn.Sequential(),\n                ChannelShuffle(oup // 2) if shuffle and y3 != 0 else nn.Sequential(),\n            )\n\n    def forward(self, x):\n        identity = x\n        out = self.layers(x)\n\n        if self.identity:\n            out = out + identity\n\n        return out\n\n\nclass MicroNet(nn.Layer):\n    \"\"\"\n    the MicroNet backbone network for recognition module.\n    Args:\n        mode(str): {'M0', 'M1', 'M2', 'M3'}\n            Four models are proposed based on four different computational costs (4M, 6M, 12M, 21M MAdds)\n            Default: 'M3'.\n    \"\"\"\n\n    def __init__(self, mode=\"M3\", **kwargs):\n        super(MicroNet, self).__init__()\n\n        self.cfgs = get_micronet_config(mode)\n\n        activation_cfg = {}\n        if mode == \"M0\":\n            input_channel = 4\n            stem_groups = 2, 2\n            out_ch = 384\n            activation_cfg[\"init_a\"] = 1.0, 1.0\n            activation_cfg[\"init_b\"] = 0.0, 0.0\n        elif mode == \"M1\":\n            input_channel = 6\n            stem_groups = 3, 2\n            out_ch = 576\n            activation_cfg[\"init_a\"] = 1.0, 1.0\n            activation_cfg[\"init_b\"] = 0.0, 0.0\n        elif mode == \"M2\":\n            input_channel = 8\n            stem_groups = 4, 2\n            out_ch = 768\n            activation_cfg[\"init_a\"] = 1.0, 1.0\n            activation_cfg[\"init_b\"] = 0.0, 0.0\n        elif mode == \"M3\":\n            input_channel = 12\n            stem_groups = 4, 3\n            out_ch = 432\n            activation_cfg[\"init_a\"] = 1.0, 0.5\n            activation_cfg[\"init_b\"] = 0.0, 0.5\n        else:\n            raise NotImplementedError(\"mode[\" + mode + \"_model] is not implemented!\")\n\n        layers = [StemLayer(3, input_channel, stride=2, groups=stem_groups)]\n\n        for idx, val in enumerate(self.cfgs):\n            s, n, c, ks, c1, c2, g1, g2, c3, g3, g4, y1, y2, y3, r = val\n\n            t1 = (c1, c2)\n            gs1 = (g1, g2)\n            gs2 = (c3, g3, g4)\n            activation_cfg[\"dy\"] = [y1, y2, y3]\n            activation_cfg[\"ratio\"] = r\n\n            output_channel = c\n            layers.append(\n                DYMicroBlock(\n                    input_channel,\n                    output_channel,\n                    kernel_size=ks,\n                    stride=s,\n                    ch_exp=t1,\n                    ch_per_group=gs1,\n                    groups_1x1=gs2,\n                    depthsep=True,\n                    shuffle=True,\n                    activation_cfg=activation_cfg,\n                )\n            )\n            input_channel = output_channel\n            for i in range(1, n):\n                layers.append(\n                    DYMicroBlock(\n                        input_channel,\n                        output_channel,\n                        kernel_size=ks,\n                        stride=1,\n                        ch_exp=t1,\n                        ch_per_group=gs1,\n                        groups_1x1=gs2,\n                        depthsep=True,\n                        shuffle=True,\n                        activation_cfg=activation_cfg,\n                    )\n                )\n                input_channel = output_channel\n        self.features = nn.Sequential(*layers)\n\n        self.pool = nn.MaxPool2D(kernel_size=2, stride=2, padding=0)\n\n        self.out_channels = make_divisible(out_ch)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.pool(x)\n        return x\n", "ppocr/modeling/backbones/rec_lcnetv3.py": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nimport paddle.nn as nn\nimport paddle.nn.functional as F\nfrom paddle import ParamAttr\nfrom paddle.nn.initializer import Constant, KaimingNormal\nfrom paddle.nn import (\n    AdaptiveAvgPool2D,\n    BatchNorm2D,\n    Conv2D,\n    Dropout,\n    Hardsigmoid,\n    Hardswish,\n    Identity,\n    Linear,\n    ReLU,\n)\nfrom paddle.regularizer import L2Decay\n\nNET_CONFIG_det = {\n    \"blocks2\":\n    # k, in_c, out_c, s, use_se\n    [[3, 16, 32, 1, False]],\n    \"blocks3\": [[3, 32, 64, 2, False], [3, 64, 64, 1, False]],\n    \"blocks4\": [[3, 64, 128, 2, False], [3, 128, 128, 1, False]],\n    \"blocks5\": [\n        [3, 128, 256, 2, False],\n        [5, 256, 256, 1, False],\n        [5, 256, 256, 1, False],\n        [5, 256, 256, 1, False],\n        [5, 256, 256, 1, False],\n    ],\n    \"blocks6\": [\n        [5, 256, 512, 2, True],\n        [5, 512, 512, 1, True],\n        [5, 512, 512, 1, False],\n        [5, 512, 512, 1, False],\n    ],\n}\n\nNET_CONFIG_rec = {\n    \"blocks2\":\n    # k, in_c, out_c, s, use_se\n    [[3, 16, 32, 1, False]],\n    \"blocks3\": [[3, 32, 64, 1, False], [3, 64, 64, 1, False]],\n    \"blocks4\": [[3, 64, 128, (2, 1), False], [3, 128, 128, 1, False]],\n    \"blocks5\": [\n        [3, 128, 256, (1, 2), False],\n        [5, 256, 256, 1, False],\n        [5, 256, 256, 1, False],\n        [5, 256, 256, 1, False],\n        [5, 256, 256, 1, False],\n    ],\n    \"blocks6\": [\n        [5, 256, 512, (2, 1), True],\n        [5, 512, 512, 1, True],\n        [5, 512, 512, (2, 1), False],\n        [5, 512, 512, 1, False],\n    ],\n}\n\n\ndef make_divisible(v, divisor=16, min_value=None):\n    if min_value is None:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    if new_v < 0.9 * v:\n        new_v += divisor\n    return new_v\n\n\nclass LearnableAffineBlock(nn.Layer):\n    def __init__(self, scale_value=1.0, bias_value=0.0, lr_mult=1.0, lab_lr=0.1):\n        super().__init__()\n        self.scale = self.create_parameter(\n            shape=[\n                1,\n            ],\n            default_initializer=Constant(value=scale_value),\n            attr=ParamAttr(learning_rate=lr_mult * lab_lr),\n        )\n        self.add_parameter(\"scale\", self.scale)\n        self.bias = self.create_parameter(\n            shape=[\n                1,\n            ],\n            default_initializer=Constant(value=bias_value),\n            attr=ParamAttr(learning_rate=lr_mult * lab_lr),\n        )\n        self.add_parameter(\"bias\", self.bias)\n\n    def forward(self, x):\n        return self.scale * x + self.bias\n\n\nclass ConvBNLayer(nn.Layer):\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride, groups=1, lr_mult=1.0\n    ):\n        super().__init__()\n        self.conv = Conv2D(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=(kernel_size - 1) // 2,\n            groups=groups,\n            weight_attr=ParamAttr(initializer=KaimingNormal(), learning_rate=lr_mult),\n            bias_attr=False,\n        )\n\n        self.bn = BatchNorm2D(\n            out_channels,\n            weight_attr=ParamAttr(regularizer=L2Decay(0.0), learning_rate=lr_mult),\n            bias_attr=ParamAttr(regularizer=L2Decay(0.0), learning_rate=lr_mult),\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return x\n\n\nclass Act(nn.Layer):\n    def __init__(self, act=\"hswish\", lr_mult=1.0, lab_lr=0.1):\n        super().__init__()\n        if act == \"hswish\":\n            self.act = Hardswish()\n        else:\n            assert act == \"relu\"\n            self.act = ReLU()\n        self.lab = LearnableAffineBlock(lr_mult=lr_mult, lab_lr=lab_lr)\n\n    def forward(self, x):\n        return self.lab(self.act(x))\n\n\nclass LearnableRepLayer(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride=1,\n        groups=1,\n        num_conv_branches=1,\n        lr_mult=1.0,\n        lab_lr=0.1,\n    ):\n        super().__init__()\n        self.is_repped = False\n        self.groups = groups\n        self.stride = stride\n        self.kernel_size = kernel_size\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.num_conv_branches = num_conv_branches\n        self.padding = (kernel_size - 1) // 2\n\n        self.identity = (\n            BatchNorm2D(\n                num_features=in_channels,\n                weight_attr=ParamAttr(learning_rate=lr_mult),\n                bias_attr=ParamAttr(learning_rate=lr_mult),\n            )\n            if out_channels == in_channels and stride == 1\n            else None\n        )\n\n        self.conv_kxk = nn.LayerList(\n            [\n                ConvBNLayer(\n                    in_channels,\n                    out_channels,\n                    kernel_size,\n                    stride,\n                    groups=groups,\n                    lr_mult=lr_mult,\n                )\n                for _ in range(self.num_conv_branches)\n            ]\n        )\n\n        self.conv_1x1 = (\n            ConvBNLayer(\n                in_channels, out_channels, 1, stride, groups=groups, lr_mult=lr_mult\n            )\n            if kernel_size > 1\n            else None\n        )\n\n        self.lab = LearnableAffineBlock(lr_mult=lr_mult, lab_lr=lab_lr)\n        self.act = Act(lr_mult=lr_mult, lab_lr=lab_lr)\n\n    def forward(self, x):\n        # for export\n        if self.is_repped:\n            out = self.lab(self.reparam_conv(x))\n            if self.stride != 2:\n                out = self.act(out)\n            return out\n\n        out = 0\n        if self.identity is not None:\n            out += self.identity(x)\n\n        if self.conv_1x1 is not None:\n            out += self.conv_1x1(x)\n\n        for conv in self.conv_kxk:\n            out += conv(x)\n\n        out = self.lab(out)\n        if self.stride != 2:\n            out = self.act(out)\n        return out\n\n    def rep(self):\n        if self.is_repped:\n            return\n        kernel, bias = self._get_kernel_bias()\n        self.reparam_conv = Conv2D(\n            in_channels=self.in_channels,\n            out_channels=self.out_channels,\n            kernel_size=self.kernel_size,\n            stride=self.stride,\n            padding=self.padding,\n            groups=self.groups,\n        )\n        self.reparam_conv.weight.set_value(kernel)\n        self.reparam_conv.bias.set_value(bias)\n        self.is_repped = True\n\n    def _pad_kernel_1x1_to_kxk(self, kernel1x1, pad):\n        if not isinstance(kernel1x1, paddle.Tensor):\n            return 0\n        else:\n            return nn.functional.pad(kernel1x1, [pad, pad, pad, pad])\n\n    def _get_kernel_bias(self):\n        kernel_conv_1x1, bias_conv_1x1 = self._fuse_bn_tensor(self.conv_1x1)\n        kernel_conv_1x1 = self._pad_kernel_1x1_to_kxk(\n            kernel_conv_1x1, self.kernel_size // 2\n        )\n\n        kernel_identity, bias_identity = self._fuse_bn_tensor(self.identity)\n\n        kernel_conv_kxk = 0\n        bias_conv_kxk = 0\n        for conv in self.conv_kxk:\n            kernel, bias = self._fuse_bn_tensor(conv)\n            kernel_conv_kxk += kernel\n            bias_conv_kxk += bias\n\n        kernel_reparam = kernel_conv_kxk + kernel_conv_1x1 + kernel_identity\n        bias_reparam = bias_conv_kxk + bias_conv_1x1 + bias_identity\n        return kernel_reparam, bias_reparam\n\n    def _fuse_bn_tensor(self, branch):\n        if not branch:\n            return 0, 0\n        elif isinstance(branch, ConvBNLayer):\n            kernel = branch.conv.weight\n            running_mean = branch.bn._mean\n            running_var = branch.bn._variance\n            gamma = branch.bn.weight\n            beta = branch.bn.bias\n            eps = branch.bn._epsilon\n        else:\n            assert isinstance(branch, BatchNorm2D)\n            if not hasattr(self, \"id_tensor\"):\n                input_dim = self.in_channels // self.groups\n                kernel_value = paddle.zeros(\n                    (self.in_channels, input_dim, self.kernel_size, self.kernel_size),\n                    dtype=branch.weight.dtype,\n                )\n                for i in range(self.in_channels):\n                    kernel_value[\n                        i, i % input_dim, self.kernel_size // 2, self.kernel_size // 2\n                    ] = 1\n                self.id_tensor = kernel_value\n            kernel = self.id_tensor\n            running_mean = branch._mean\n            running_var = branch._variance\n            gamma = branch.weight\n            beta = branch.bias\n            eps = branch._epsilon\n        std = (running_var + eps).sqrt()\n        t = (gamma / std).reshape((-1, 1, 1, 1))\n        return kernel * t, beta - running_mean * gamma / std\n\n\nclass SELayer(nn.Layer):\n    def __init__(self, channel, reduction=4, lr_mult=1.0):\n        super().__init__()\n        self.avg_pool = AdaptiveAvgPool2D(1)\n        self.conv1 = Conv2D(\n            in_channels=channel,\n            out_channels=channel // reduction,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            weight_attr=ParamAttr(learning_rate=lr_mult),\n            bias_attr=ParamAttr(learning_rate=lr_mult),\n        )\n        self.relu = ReLU()\n        self.conv2 = Conv2D(\n            in_channels=channel // reduction,\n            out_channels=channel,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            weight_attr=ParamAttr(learning_rate=lr_mult),\n            bias_attr=ParamAttr(learning_rate=lr_mult),\n        )\n        self.hardsigmoid = Hardsigmoid()\n\n    def forward(self, x):\n        identity = x\n        x = self.avg_pool(x)\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.hardsigmoid(x)\n        x = paddle.multiply(x=identity, y=x)\n        return x\n\n\nclass LCNetV3Block(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        stride,\n        dw_size,\n        use_se=False,\n        conv_kxk_num=4,\n        lr_mult=1.0,\n        lab_lr=0.1,\n    ):\n        super().__init__()\n        self.use_se = use_se\n        self.dw_conv = LearnableRepLayer(\n            in_channels=in_channels,\n            out_channels=in_channels,\n            kernel_size=dw_size,\n            stride=stride,\n            groups=in_channels,\n            num_conv_branches=conv_kxk_num,\n            lr_mult=lr_mult,\n            lab_lr=lab_lr,\n        )\n        if use_se:\n            self.se = SELayer(in_channels, lr_mult=lr_mult)\n        self.pw_conv = LearnableRepLayer(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=1,\n            stride=1,\n            num_conv_branches=conv_kxk_num,\n            lr_mult=lr_mult,\n            lab_lr=lab_lr,\n        )\n\n    def forward(self, x):\n        x = self.dw_conv(x)\n        if self.use_se:\n            x = self.se(x)\n        x = self.pw_conv(x)\n        return x\n\n\nclass PPLCNetV3(nn.Layer):\n    def __init__(\n        self,\n        scale=1.0,\n        conv_kxk_num=4,\n        lr_mult_list=[1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n        lab_lr=0.1,\n        det=False,\n        **kwargs,\n    ):\n        super().__init__()\n        self.scale = scale\n        self.lr_mult_list = lr_mult_list\n        self.det = det\n\n        self.net_config = NET_CONFIG_det if self.det else NET_CONFIG_rec\n\n        assert isinstance(\n            self.lr_mult_list, (list, tuple)\n        ), \"lr_mult_list should be in (list, tuple) but got {}\".format(\n            type(self.lr_mult_list)\n        )\n        assert (\n            len(self.lr_mult_list) == 6\n        ), \"lr_mult_list length should be 6 but got {}\".format(len(self.lr_mult_list))\n\n        self.conv1 = ConvBNLayer(\n            in_channels=3,\n            out_channels=make_divisible(16 * scale),\n            kernel_size=3,\n            stride=2,\n            lr_mult=self.lr_mult_list[0],\n        )\n\n        self.blocks2 = nn.Sequential(\n            *[\n                LCNetV3Block(\n                    in_channels=make_divisible(in_c * scale),\n                    out_channels=make_divisible(out_c * scale),\n                    dw_size=k,\n                    stride=s,\n                    use_se=se,\n                    conv_kxk_num=conv_kxk_num,\n                    lr_mult=self.lr_mult_list[1],\n                    lab_lr=lab_lr,\n                )\n                for i, (k, in_c, out_c, s, se) in enumerate(self.net_config[\"blocks2\"])\n            ]\n        )\n\n        self.blocks3 = nn.Sequential(\n            *[\n                LCNetV3Block(\n                    in_channels=make_divisible(in_c * scale),\n                    out_channels=make_divisible(out_c * scale),\n                    dw_size=k,\n                    stride=s,\n                    use_se=se,\n                    conv_kxk_num=conv_kxk_num,\n                    lr_mult=self.lr_mult_list[2],\n                    lab_lr=lab_lr,\n                )\n                for i, (k, in_c, out_c, s, se) in enumerate(self.net_config[\"blocks3\"])\n            ]\n        )\n\n        self.blocks4 = nn.Sequential(\n            *[\n                LCNetV3Block(\n                    in_channels=make_divisible(in_c * scale),\n                    out_channels=make_divisible(out_c * scale),\n                    dw_size=k,\n                    stride=s,\n                    use_se=se,\n                    conv_kxk_num=conv_kxk_num,\n                    lr_mult=self.lr_mult_list[3],\n                    lab_lr=lab_lr,\n                )\n                for i, (k, in_c, out_c, s, se) in enumerate(self.net_config[\"blocks4\"])\n            ]\n        )\n\n        self.blocks5 = nn.Sequential(\n            *[\n                LCNetV3Block(\n                    in_channels=make_divisible(in_c * scale),\n                    out_channels=make_divisible(out_c * scale),\n                    dw_size=k,\n                    stride=s,\n                    use_se=se,\n                    conv_kxk_num=conv_kxk_num,\n                    lr_mult=self.lr_mult_list[4],\n                    lab_lr=lab_lr,\n                )\n                for i, (k, in_c, out_c, s, se) in enumerate(self.net_config[\"blocks5\"])\n            ]\n        )\n\n        self.blocks6 = nn.Sequential(\n            *[\n                LCNetV3Block(\n                    in_channels=make_divisible(in_c * scale),\n                    out_channels=make_divisible(out_c * scale),\n                    dw_size=k,\n                    stride=s,\n                    use_se=se,\n                    conv_kxk_num=conv_kxk_num,\n                    lr_mult=self.lr_mult_list[5],\n                    lab_lr=lab_lr,\n                )\n                for i, (k, in_c, out_c, s, se) in enumerate(self.net_config[\"blocks6\"])\n            ]\n        )\n        self.out_channels = make_divisible(512 * scale)\n\n        if self.det:\n            mv_c = [16, 24, 56, 480]\n            self.out_channels = [\n                make_divisible(self.net_config[\"blocks3\"][-1][2] * scale),\n                make_divisible(self.net_config[\"blocks4\"][-1][2] * scale),\n                make_divisible(self.net_config[\"blocks5\"][-1][2] * scale),\n                make_divisible(self.net_config[\"blocks6\"][-1][2] * scale),\n            ]\n\n            self.layer_list = nn.LayerList(\n                [\n                    nn.Conv2D(self.out_channels[0], int(mv_c[0] * scale), 1, 1, 0),\n                    nn.Conv2D(self.out_channels[1], int(mv_c[1] * scale), 1, 1, 0),\n                    nn.Conv2D(self.out_channels[2], int(mv_c[2] * scale), 1, 1, 0),\n                    nn.Conv2D(self.out_channels[3], int(mv_c[3] * scale), 1, 1, 0),\n                ]\n            )\n            self.out_channels = [\n                int(mv_c[0] * scale),\n                int(mv_c[1] * scale),\n                int(mv_c[2] * scale),\n                int(mv_c[3] * scale),\n            ]\n\n    def forward(self, x):\n        out_list = []\n        x = self.conv1(x)\n\n        x = self.blocks2(x)\n        x = self.blocks3(x)\n        out_list.append(x)\n        x = self.blocks4(x)\n        out_list.append(x)\n        x = self.blocks5(x)\n        out_list.append(x)\n        x = self.blocks6(x)\n        out_list.append(x)\n\n        if self.det:\n            out_list[0] = self.layer_list[0](out_list[0])\n            out_list[1] = self.layer_list[1](out_list[1])\n            out_list[2] = self.layer_list[2](out_list[2])\n            out_list[3] = self.layer_list[3](out_list[3])\n            return out_list\n\n        if self.training:\n            x = F.adaptive_avg_pool2d(x, [1, 40])\n        else:\n            x = F.avg_pool2d(x, [3, 2])\n        return x\n", "ppocr/modeling/backbones/det_mobilenet_v3.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nfrom paddle import nn\nimport paddle.nn.functional as F\nfrom paddle import ParamAttr\n\n__all__ = [\"MobileNetV3\"]\n\n\ndef make_divisible(v, divisor=8, min_value=None):\n    if min_value is None:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    if new_v < 0.9 * v:\n        new_v += divisor\n    return new_v\n\n\nclass MobileNetV3(nn.Layer):\n    def __init__(\n        self, in_channels=3, model_name=\"large\", scale=0.5, disable_se=False, **kwargs\n    ):\n        \"\"\"\n        the MobilenetV3 backbone network for detection module.\n        Args:\n            params(dict): the super parameters for build network\n        \"\"\"\n        super(MobileNetV3, self).__init__()\n\n        self.disable_se = disable_se\n\n        if model_name == \"large\":\n            cfg = [\n                # k, exp, c,  se,     nl,  s,\n                [3, 16, 16, False, \"relu\", 1],\n                [3, 64, 24, False, \"relu\", 2],\n                [3, 72, 24, False, \"relu\", 1],\n                [5, 72, 40, True, \"relu\", 2],\n                [5, 120, 40, True, \"relu\", 1],\n                [5, 120, 40, True, \"relu\", 1],\n                [3, 240, 80, False, \"hardswish\", 2],\n                [3, 200, 80, False, \"hardswish\", 1],\n                [3, 184, 80, False, \"hardswish\", 1],\n                [3, 184, 80, False, \"hardswish\", 1],\n                [3, 480, 112, True, \"hardswish\", 1],\n                [3, 672, 112, True, \"hardswish\", 1],\n                [5, 672, 160, True, \"hardswish\", 2],\n                [5, 960, 160, True, \"hardswish\", 1],\n                [5, 960, 160, True, \"hardswish\", 1],\n            ]\n            cls_ch_squeeze = 960\n        elif model_name == \"small\":\n            cfg = [\n                # k, exp, c,  se,     nl,  s,\n                [3, 16, 16, True, \"relu\", 2],\n                [3, 72, 24, False, \"relu\", 2],\n                [3, 88, 24, False, \"relu\", 1],\n                [5, 96, 40, True, \"hardswish\", 2],\n                [5, 240, 40, True, \"hardswish\", 1],\n                [5, 240, 40, True, \"hardswish\", 1],\n                [5, 120, 48, True, \"hardswish\", 1],\n                [5, 144, 48, True, \"hardswish\", 1],\n                [5, 288, 96, True, \"hardswish\", 2],\n                [5, 576, 96, True, \"hardswish\", 1],\n                [5, 576, 96, True, \"hardswish\", 1],\n            ]\n            cls_ch_squeeze = 576\n        else:\n            raise NotImplementedError(\n                \"mode[\" + model_name + \"_model] is not implemented!\"\n            )\n\n        supported_scale = [0.35, 0.5, 0.75, 1.0, 1.25]\n        assert (\n            scale in supported_scale\n        ), \"supported scale are {} but input scale is {}\".format(supported_scale, scale)\n        inplanes = 16\n        # conv1\n        self.conv = ConvBNLayer(\n            in_channels=in_channels,\n            out_channels=make_divisible(inplanes * scale),\n            kernel_size=3,\n            stride=2,\n            padding=1,\n            groups=1,\n            if_act=True,\n            act=\"hardswish\",\n        )\n\n        self.stages = []\n        self.out_channels = []\n        block_list = []\n        i = 0\n        inplanes = make_divisible(inplanes * scale)\n        for k, exp, c, se, nl, s in cfg:\n            se = se and not self.disable_se\n            start_idx = 2 if model_name == \"large\" else 0\n            if s == 2 and i > start_idx:\n                self.out_channels.append(inplanes)\n                self.stages.append(nn.Sequential(*block_list))\n                block_list = []\n            block_list.append(\n                ResidualUnit(\n                    in_channels=inplanes,\n                    mid_channels=make_divisible(scale * exp),\n                    out_channels=make_divisible(scale * c),\n                    kernel_size=k,\n                    stride=s,\n                    use_se=se,\n                    act=nl,\n                )\n            )\n            inplanes = make_divisible(scale * c)\n            i += 1\n        block_list.append(\n            ConvBNLayer(\n                in_channels=inplanes,\n                out_channels=make_divisible(scale * cls_ch_squeeze),\n                kernel_size=1,\n                stride=1,\n                padding=0,\n                groups=1,\n                if_act=True,\n                act=\"hardswish\",\n            )\n        )\n        self.stages.append(nn.Sequential(*block_list))\n        self.out_channels.append(make_divisible(scale * cls_ch_squeeze))\n        for i, stage in enumerate(self.stages):\n            self.add_sublayer(sublayer=stage, name=\"stage{}\".format(i))\n\n    def forward(self, x):\n        x = self.conv(x)\n        out_list = []\n        for stage in self.stages:\n            x = stage(x)\n            out_list.append(x)\n        return out_list\n\n\nclass ConvBNLayer(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride,\n        padding,\n        groups=1,\n        if_act=True,\n        act=None,\n    ):\n        super(ConvBNLayer, self).__init__()\n        self.if_act = if_act\n        self.act = act\n        self.conv = nn.Conv2D(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            groups=groups,\n            bias_attr=False,\n        )\n\n        self.bn = nn.BatchNorm(num_channels=out_channels, act=None)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        if self.if_act:\n            if self.act == \"relu\":\n                x = F.relu(x)\n            elif self.act == \"hardswish\":\n                x = F.hardswish(x)\n            else:\n                print(\n                    \"The activation function({}) is selected incorrectly.\".format(\n                        self.act\n                    )\n                )\n                exit()\n        return x\n\n\nclass ResidualUnit(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        mid_channels,\n        out_channels,\n        kernel_size,\n        stride,\n        use_se,\n        act=None,\n    ):\n        super(ResidualUnit, self).__init__()\n        self.if_shortcut = stride == 1 and in_channels == out_channels\n        self.if_se = use_se\n\n        self.expand_conv = ConvBNLayer(\n            in_channels=in_channels,\n            out_channels=mid_channels,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            if_act=True,\n            act=act,\n        )\n        self.bottleneck_conv = ConvBNLayer(\n            in_channels=mid_channels,\n            out_channels=mid_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=int((kernel_size - 1) // 2),\n            groups=mid_channels,\n            if_act=True,\n            act=act,\n        )\n        if self.if_se:\n            self.mid_se = SEModule(mid_channels)\n        self.linear_conv = ConvBNLayer(\n            in_channels=mid_channels,\n            out_channels=out_channels,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            if_act=False,\n            act=None,\n        )\n\n    def forward(self, inputs):\n        x = self.expand_conv(inputs)\n        x = self.bottleneck_conv(x)\n        if self.if_se:\n            x = self.mid_se(x)\n        x = self.linear_conv(x)\n        if self.if_shortcut:\n            x = paddle.add(inputs, x)\n        return x\n\n\nclass SEModule(nn.Layer):\n    def __init__(self, in_channels, reduction=4):\n        super(SEModule, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2D(1)\n        self.conv1 = nn.Conv2D(\n            in_channels=in_channels,\n            out_channels=in_channels // reduction,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n        )\n        self.conv2 = nn.Conv2D(\n            in_channels=in_channels // reduction,\n            out_channels=in_channels,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n        )\n\n    def forward(self, inputs):\n        outputs = self.avg_pool(inputs)\n        outputs = self.conv1(outputs)\n        outputs = F.relu(outputs)\n        outputs = self.conv2(outputs)\n        outputs = F.hardsigmoid(outputs, slope=0.2, offset=0.5)\n        return inputs * outputs\n", "ppocr/modeling/backbones/rec_vit_parseq.py": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nThis code is refer from:\nhttps://github.com/PaddlePaddle/PaddleClas/blob/release%2F2.5/ppcls/arch/backbone/model_zoo/vision_transformer.py\n\"\"\"\n\nfrom collections.abc import Callable\n\nimport numpy as np\nimport paddle\nimport paddle.nn as nn\nfrom paddle.nn.initializer import TruncatedNormal, Constant, Normal\n\n\ntrunc_normal_ = TruncatedNormal(std=0.02)\nnormal_ = Normal\nzeros_ = Constant(value=0.0)\nones_ = Constant(value=1.0)\n\n\ndef to_2tuple(x):\n    return tuple([x] * 2)\n\n\ndef drop_path(x, drop_prob=0.0, training=False):\n    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ...\n    \"\"\"\n    if drop_prob == 0.0 or not training:\n        return x\n    keep_prob = paddle.to_tensor(1 - drop_prob, dtype=x.dtype)\n    shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n    random_tensor = keep_prob + paddle.rand(shape).astype(x.dtype)\n    random_tensor = paddle.floor(random_tensor)  # binarize\n    output = x.divide(keep_prob) * random_tensor\n    return output\n\n\nclass DropPath(nn.Layer):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\"\"\"\n\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\n\n\nclass Identity(nn.Layer):\n    def __init__(self):\n        super(Identity, self).__init__()\n\n    def forward(self, input):\n        return input\n\n\nclass Mlp(nn.Layer):\n    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        act_layer=nn.GELU,\n        drop=0.0,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass Attention(nn.Layer):\n    def __init__(\n        self,\n        dim,\n        num_heads=8,\n        qkv_bias=False,\n        qk_scale=None,\n        attn_drop=0.0,\n        proj_drop=0.0,\n    ):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim**-0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias_attr=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        # B= x.shape[0]\n        N, C = x.shape[1:]\n        qkv = (\n            self.qkv(x)\n            .reshape((-1, N, 3, self.num_heads, C // self.num_heads))\n            .transpose((2, 0, 3, 1, 4))\n        )\n        q, k, v = qkv[0], qkv[1], qkv[2]\n\n        attn = (q.matmul(k.transpose((0, 1, 3, 2)))) * self.scale\n        attn = nn.functional.softmax(attn, axis=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn.matmul(v)).transpose((0, 2, 1, 3)).reshape((-1, N, C))\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass Block(nn.Layer):\n    def __init__(\n        self,\n        dim,\n        num_heads,\n        mlp_ratio=4.0,\n        qkv_bias=False,\n        qk_scale=None,\n        drop=0.0,\n        attn_drop=0.0,\n        drop_path=0.0,\n        act_layer=nn.GELU,\n        norm_layer=\"nn.LayerNorm\",\n        epsilon=1e-5,\n    ):\n        super().__init__()\n        if isinstance(norm_layer, str):\n            self.norm1 = eval(norm_layer)(dim, epsilon=epsilon)\n        elif isinstance(norm_layer, Callable):\n            self.norm1 = norm_layer(dim)\n        else:\n            raise TypeError(\"The norm_layer must be str or paddle.nn.layer.Layer class\")\n        self.attn = Attention(\n            dim,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            qk_scale=qk_scale,\n            attn_drop=attn_drop,\n            proj_drop=drop,\n        )\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else Identity()\n        if isinstance(norm_layer, str):\n            self.norm2 = eval(norm_layer)(dim, epsilon=epsilon)\n        elif isinstance(norm_layer, Callable):\n            self.norm2 = norm_layer(dim)\n        else:\n            raise TypeError(\"The norm_layer must be str or paddle.nn.layer.Layer class\")\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=mlp_hidden_dim,\n            act_layer=act_layer,\n            drop=drop,\n        )\n\n    def forward(self, x):\n        x = x + self.drop_path(self.attn(self.norm1(x)))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n\n\nclass PatchEmbed(nn.Layer):\n    \"\"\"Image to Patch Embedding\"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        if isinstance(img_size, int):\n            img_size = to_2tuple(img_size)\n        if isinstance(patch_size, int):\n            patch_size = to_2tuple(patch_size)\n        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n\n        self.proj = nn.Conv2D(\n            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size\n        )\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        assert (\n            H == self.img_size[0] and W == self.img_size[1]\n        ), f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n\n        x = self.proj(x).flatten(2).transpose((0, 2, 1))\n        return x\n\n\nclass VisionTransformer(nn.Layer):\n    \"\"\"Vision Transformer with support for patch input\"\"\"\n\n    def __init__(\n        self,\n        img_size=224,\n        patch_size=16,\n        in_channels=3,\n        class_num=1000,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        mlp_ratio=4,\n        qkv_bias=False,\n        qk_scale=None,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.0,\n        norm_layer=\"nn.LayerNorm\",\n        epsilon=1e-5,\n        **kwargs,\n    ):\n        super().__init__()\n        self.class_num = class_num\n\n        self.num_features = self.embed_dim = embed_dim\n\n        self.patch_embed = PatchEmbed(\n            img_size=img_size,\n            patch_size=patch_size,\n            in_chans=in_channels,\n            embed_dim=embed_dim,\n        )\n        num_patches = self.patch_embed.num_patches\n\n        self.pos_embed = self.create_parameter(\n            shape=(1, num_patches, embed_dim), default_initializer=zeros_\n        )\n        self.add_parameter(\"pos_embed\", self.pos_embed)\n        self.cls_token = self.create_parameter(\n            shape=(1, 1, embed_dim), default_initializer=zeros_\n        )\n        self.add_parameter(\"cls_token\", self.cls_token)\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        dpr = np.linspace(0, drop_path_rate, depth)\n\n        self.blocks = nn.LayerList(\n            [\n                Block(\n                    dim=embed_dim,\n                    num_heads=num_heads,\n                    mlp_ratio=mlp_ratio,\n                    qkv_bias=qkv_bias,\n                    qk_scale=qk_scale,\n                    drop=drop_rate,\n                    attn_drop=attn_drop_rate,\n                    drop_path=dpr[i],\n                    norm_layer=norm_layer,\n                    epsilon=epsilon,\n                )\n                for i in range(depth)\n            ]\n        )\n\n        self.norm = eval(norm_layer)(embed_dim, epsilon=epsilon)\n\n        # Classifier head\n        self.head = nn.Linear(embed_dim, class_num) if class_num > 0 else Identity()\n\n        trunc_normal_(self.pos_embed)\n        self.out_channels = embed_dim\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                zeros_(m.bias)\n        elif isinstance(m, nn.LayerNorm):\n            zeros_(m.bias)\n            ones_(m.weight)\n\n    def forward_features(self, x):\n        B = x.shape[0]\n        x = self.patch_embed(x)\n        x = x + self.pos_embed\n        x = self.pos_drop(x)\n        for blk in self.blocks:\n            x = blk(x)\n        x = self.norm(x)\n        return x\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.head(x)\n        return x\n\n\nclass ViTParseQ(VisionTransformer):\n    def __init__(\n        self,\n        img_size=[224, 224],\n        patch_size=[16, 16],\n        in_channels=3,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        mlp_ratio=4.0,\n        qkv_bias=True,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.0,\n    ):\n        super().__init__(\n            img_size,\n            patch_size,\n            in_channels,\n            embed_dim=embed_dim,\n            depth=depth,\n            num_heads=num_heads,\n            mlp_ratio=mlp_ratio,\n            qkv_bias=qkv_bias,\n            drop_rate=drop_rate,\n            attn_drop_rate=attn_drop_rate,\n            drop_path_rate=drop_path_rate,\n            class_num=0,\n        )\n\n    def forward(self, x):\n        return self.forward_features(x)\n", "ppocr/modeling/backbones/rec_resnet_vd.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nfrom paddle import ParamAttr\nimport paddle.nn as nn\nimport paddle.nn.functional as F\n\n__all__ = [\"ResNet\"]\n\n\nclass ConvBNLayer(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride=1,\n        groups=1,\n        is_vd_mode=False,\n        act=None,\n        name=None,\n    ):\n        super(ConvBNLayer, self).__init__()\n\n        self.is_vd_mode = is_vd_mode\n        self._pool2d_avg = nn.AvgPool2D(\n            kernel_size=stride, stride=stride, padding=0, ceil_mode=True\n        )\n        self._conv = nn.Conv2D(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=1 if is_vd_mode else stride,\n            padding=(kernel_size - 1) // 2,\n            groups=groups,\n            weight_attr=ParamAttr(name=name + \"_weights\"),\n            bias_attr=False,\n        )\n        if name == \"conv1\":\n            bn_name = \"bn_\" + name\n        else:\n            bn_name = \"bn\" + name[3:]\n        self._batch_norm = nn.BatchNorm(\n            out_channels,\n            act=act,\n            param_attr=ParamAttr(name=bn_name + \"_scale\"),\n            bias_attr=ParamAttr(bn_name + \"_offset\"),\n            moving_mean_name=bn_name + \"_mean\",\n            moving_variance_name=bn_name + \"_variance\",\n        )\n\n    def forward(self, inputs):\n        if self.is_vd_mode:\n            inputs = self._pool2d_avg(inputs)\n        y = self._conv(inputs)\n        y = self._batch_norm(y)\n        return y\n\n\nclass BottleneckBlock(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        stride,\n        shortcut=True,\n        if_first=False,\n        name=None,\n    ):\n        super(BottleneckBlock, self).__init__()\n\n        self.conv0 = ConvBNLayer(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=1,\n            act=\"relu\",\n            name=name + \"_branch2a\",\n        )\n        self.conv1 = ConvBNLayer(\n            in_channels=out_channels,\n            out_channels=out_channels,\n            kernel_size=3,\n            stride=stride,\n            act=\"relu\",\n            name=name + \"_branch2b\",\n        )\n        self.conv2 = ConvBNLayer(\n            in_channels=out_channels,\n            out_channels=out_channels * 4,\n            kernel_size=1,\n            act=None,\n            name=name + \"_branch2c\",\n        )\n\n        if not shortcut:\n            self.short = ConvBNLayer(\n                in_channels=in_channels,\n                out_channels=out_channels * 4,\n                kernel_size=1,\n                stride=stride,\n                is_vd_mode=not if_first and stride[0] != 1,\n                name=name + \"_branch1\",\n            )\n\n        self.shortcut = shortcut\n\n    def forward(self, inputs):\n        y = self.conv0(inputs)\n\n        conv1 = self.conv1(y)\n        conv2 = self.conv2(conv1)\n\n        if self.shortcut:\n            short = inputs\n        else:\n            short = self.short(inputs)\n        y = paddle.add(x=short, y=conv2)\n        y = F.relu(y)\n        return y\n\n\nclass BasicBlock(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        stride,\n        shortcut=True,\n        if_first=False,\n        name=None,\n    ):\n        super(BasicBlock, self).__init__()\n        self.stride = stride\n        self.conv0 = ConvBNLayer(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=3,\n            stride=stride,\n            act=\"relu\",\n            name=name + \"_branch2a\",\n        )\n        self.conv1 = ConvBNLayer(\n            in_channels=out_channels,\n            out_channels=out_channels,\n            kernel_size=3,\n            act=None,\n            name=name + \"_branch2b\",\n        )\n\n        if not shortcut:\n            self.short = ConvBNLayer(\n                in_channels=in_channels,\n                out_channels=out_channels,\n                kernel_size=1,\n                stride=stride,\n                is_vd_mode=not if_first and stride[0] != 1,\n                name=name + \"_branch1\",\n            )\n\n        self.shortcut = shortcut\n\n    def forward(self, inputs):\n        y = self.conv0(inputs)\n        conv1 = self.conv1(y)\n\n        if self.shortcut:\n            short = inputs\n        else:\n            short = self.short(inputs)\n        y = paddle.add(x=short, y=conv1)\n        y = F.relu(y)\n        return y\n\n\nclass ResNet(nn.Layer):\n    def __init__(self, in_channels=3, layers=50, **kwargs):\n        super(ResNet, self).__init__()\n\n        self.layers = layers\n        supported_layers = [18, 34, 50, 101, 152, 200]\n        assert (\n            layers in supported_layers\n        ), \"supported layers are {} but input layer is {}\".format(\n            supported_layers, layers\n        )\n\n        if layers == 18:\n            depth = [2, 2, 2, 2]\n        elif layers == 34 or layers == 50:\n            depth = [3, 4, 6, 3]\n        elif layers == 101:\n            depth = [3, 4, 23, 3]\n        elif layers == 152:\n            depth = [3, 8, 36, 3]\n        elif layers == 200:\n            depth = [3, 12, 48, 3]\n        num_channels = [64, 256, 512, 1024] if layers >= 50 else [64, 64, 128, 256]\n        num_filters = [64, 128, 256, 512]\n\n        self.conv1_1 = ConvBNLayer(\n            in_channels=in_channels,\n            out_channels=32,\n            kernel_size=3,\n            stride=1,\n            act=\"relu\",\n            name=\"conv1_1\",\n        )\n        self.conv1_2 = ConvBNLayer(\n            in_channels=32,\n            out_channels=32,\n            kernel_size=3,\n            stride=1,\n            act=\"relu\",\n            name=\"conv1_2\",\n        )\n        self.conv1_3 = ConvBNLayer(\n            in_channels=32,\n            out_channels=64,\n            kernel_size=3,\n            stride=1,\n            act=\"relu\",\n            name=\"conv1_3\",\n        )\n        self.pool2d_max = nn.MaxPool2D(kernel_size=3, stride=2, padding=1)\n\n        self.block_list = []\n        if layers >= 50:\n            for block in range(len(depth)):\n                shortcut = False\n                for i in range(depth[block]):\n                    if layers in [101, 152, 200] and block == 2:\n                        if i == 0:\n                            conv_name = \"res\" + str(block + 2) + \"a\"\n                        else:\n                            conv_name = \"res\" + str(block + 2) + \"b\" + str(i)\n                    else:\n                        conv_name = \"res\" + str(block + 2) + chr(97 + i)\n\n                    if i == 0 and block != 0:\n                        stride = (2, 1)\n                    else:\n                        stride = (1, 1)\n                    bottleneck_block = self.add_sublayer(\n                        \"bb_%d_%d\" % (block, i),\n                        BottleneckBlock(\n                            in_channels=(\n                                num_channels[block]\n                                if i == 0\n                                else num_filters[block] * 4\n                            ),\n                            out_channels=num_filters[block],\n                            stride=stride,\n                            shortcut=shortcut,\n                            if_first=block == i == 0,\n                            name=conv_name,\n                        ),\n                    )\n                    shortcut = True\n                    self.block_list.append(bottleneck_block)\n                self.out_channels = num_filters[block] * 4\n        else:\n            for block in range(len(depth)):\n                shortcut = False\n                for i in range(depth[block]):\n                    conv_name = \"res\" + str(block + 2) + chr(97 + i)\n                    if i == 0 and block != 0:\n                        stride = (2, 1)\n                    else:\n                        stride = (1, 1)\n\n                    basic_block = self.add_sublayer(\n                        \"bb_%d_%d\" % (block, i),\n                        BasicBlock(\n                            in_channels=(\n                                num_channels[block] if i == 0 else num_filters[block]\n                            ),\n                            out_channels=num_filters[block],\n                            stride=stride,\n                            shortcut=shortcut,\n                            if_first=block == i == 0,\n                            name=conv_name,\n                        ),\n                    )\n                    shortcut = True\n                    self.block_list.append(basic_block)\n                self.out_channels = num_filters[block]\n        self.out_pool = nn.MaxPool2D(kernel_size=2, stride=2, padding=0)\n\n    def forward(self, inputs):\n        y = self.conv1_1(inputs)\n        y = self.conv1_2(y)\n        y = self.conv1_3(y)\n        y = self.pool2d_max(y)\n        for block in self.block_list:\n            y = block(y)\n        y = self.out_pool(y)\n        return y\n", "ppocr/modeling/backbones/kie_unet_sdmgr.py": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nfrom paddle import nn\nimport numpy as np\nimport cv2\n\n__all__ = [\"Kie_backbone\"]\n\n\nclass Encoder(nn.Layer):\n    def __init__(self, num_channels, num_filters):\n        super(Encoder, self).__init__()\n        self.conv1 = nn.Conv2D(\n            num_channels,\n            num_filters,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            bias_attr=False,\n        )\n        self.bn1 = nn.BatchNorm(num_filters, act=\"relu\")\n\n        self.conv2 = nn.Conv2D(\n            num_filters,\n            num_filters,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            bias_attr=False,\n        )\n        self.bn2 = nn.BatchNorm(num_filters, act=\"relu\")\n\n        self.pool = nn.MaxPool2D(kernel_size=3, stride=2, padding=1)\n\n    def forward(self, inputs):\n        x = self.conv1(inputs)\n        x = self.bn1(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x_pooled = self.pool(x)\n        return x, x_pooled\n\n\nclass Decoder(nn.Layer):\n    def __init__(self, num_channels, num_filters):\n        super(Decoder, self).__init__()\n\n        self.conv1 = nn.Conv2D(\n            num_channels,\n            num_filters,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            bias_attr=False,\n        )\n        self.bn1 = nn.BatchNorm(num_filters, act=\"relu\")\n\n        self.conv2 = nn.Conv2D(\n            num_filters,\n            num_filters,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            bias_attr=False,\n        )\n        self.bn2 = nn.BatchNorm(num_filters, act=\"relu\")\n\n        self.conv0 = nn.Conv2D(\n            num_channels,\n            num_filters,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias_attr=False,\n        )\n        self.bn0 = nn.BatchNorm(num_filters, act=\"relu\")\n\n    def forward(self, inputs_prev, inputs):\n        x = self.conv0(inputs)\n        x = self.bn0(x)\n        x = paddle.nn.functional.interpolate(\n            x, scale_factor=2, mode=\"bilinear\", align_corners=False\n        )\n        x = paddle.concat([inputs_prev, x], axis=1)\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        return x\n\n\nclass UNet(nn.Layer):\n    def __init__(self):\n        super(UNet, self).__init__()\n        self.down1 = Encoder(num_channels=3, num_filters=16)\n        self.down2 = Encoder(num_channels=16, num_filters=32)\n        self.down3 = Encoder(num_channels=32, num_filters=64)\n        self.down4 = Encoder(num_channels=64, num_filters=128)\n        self.down5 = Encoder(num_channels=128, num_filters=256)\n\n        self.up1 = Decoder(32, 16)\n        self.up2 = Decoder(64, 32)\n        self.up3 = Decoder(128, 64)\n        self.up4 = Decoder(256, 128)\n        self.out_channels = 16\n\n    def forward(self, inputs):\n        x1, _ = self.down1(inputs)\n        _, x2 = self.down2(x1)\n        _, x3 = self.down3(x2)\n        _, x4 = self.down4(x3)\n        _, x5 = self.down5(x4)\n\n        x = self.up4(x4, x5)\n        x = self.up3(x3, x)\n        x = self.up2(x2, x)\n        x = self.up1(x1, x)\n        return x\n\n\nclass Kie_backbone(nn.Layer):\n    def __init__(self, in_channels, **kwargs):\n        super(Kie_backbone, self).__init__()\n        self.out_channels = 16\n        self.img_feat = UNet()\n        self.maxpool = nn.MaxPool2D(kernel_size=7)\n\n    def bbox2roi(self, bbox_list):\n        rois_list = []\n        rois_num = []\n        for img_id, bboxes in enumerate(bbox_list):\n            rois_num.append(bboxes.shape[0])\n            rois_list.append(bboxes)\n        rois = paddle.concat(rois_list, 0)\n        rois_num = paddle.to_tensor(rois_num, dtype=\"int32\")\n        return rois, rois_num\n\n    def pre_process(self, img, relations, texts, gt_bboxes, tag, img_size):\n        img, relations, texts, gt_bboxes, tag, img_size = (\n            img.numpy(),\n            relations.numpy(),\n            texts.numpy(),\n            gt_bboxes.numpy(),\n            tag.numpy().tolist(),\n            img_size.numpy(),\n        )\n        temp_relations, temp_texts, temp_gt_bboxes = [], [], []\n        h, w = int(np.max(img_size[:, 0])), int(np.max(img_size[:, 1]))\n        img = paddle.to_tensor(img[:, :, :h, :w])\n        batch = len(tag)\n        for i in range(batch):\n            num, recoder_len = tag[i][0], tag[i][1]\n            temp_relations.append(\n                paddle.to_tensor(relations[i, :num, :num, :], dtype=\"float32\")\n            )\n            temp_texts.append(\n                paddle.to_tensor(texts[i, :num, :recoder_len], dtype=\"float32\")\n            )\n            temp_gt_bboxes.append(\n                paddle.to_tensor(gt_bboxes[i, :num, ...], dtype=\"float32\")\n            )\n        return img, temp_relations, temp_texts, temp_gt_bboxes\n\n    def forward(self, inputs):\n        img = inputs[0]\n        relations, texts, gt_bboxes, tag, img_size = (\n            inputs[1],\n            inputs[2],\n            inputs[3],\n            inputs[5],\n            inputs[-1],\n        )\n        img, relations, texts, gt_bboxes = self.pre_process(\n            img, relations, texts, gt_bboxes, tag, img_size\n        )\n        x = self.img_feat(img)\n        boxes, rois_num = self.bbox2roi(gt_bboxes)\n        feats = paddle.vision.ops.roi_align(\n            x, boxes, spatial_scale=1.0, output_size=7, boxes_num=rois_num\n        )\n        feats = self.maxpool(feats).squeeze(-1).squeeze(-1)\n        return [relations, texts, feats]\n", "ppocr/modeling/backbones/rec_vit.py": "# copyright (c) 2023 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom paddle import ParamAttr\nfrom paddle.nn.initializer import KaimingNormal\nimport numpy as np\nimport paddle\nimport paddle.nn as nn\nfrom paddle.nn.initializer import TruncatedNormal, Constant, Normal\n\ntrunc_normal_ = TruncatedNormal(std=0.02)\nnormal_ = Normal\nzeros_ = Constant(value=0.0)\nones_ = Constant(value=1.0)\n\n\ndef drop_path(x, drop_prob=0.0, training=False):\n    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ...\n    \"\"\"\n    if drop_prob == 0.0 or not training:\n        return x\n    keep_prob = paddle.to_tensor(1 - drop_prob)\n    shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n    random_tensor = keep_prob + paddle.rand(shape, dtype=x.dtype)\n    random_tensor = paddle.floor(random_tensor)  # binarize\n    output = x.divide(keep_prob) * random_tensor\n    return output\n\n\nclass DropPath(nn.Layer):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\"\"\"\n\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\n\n\nclass Identity(nn.Layer):\n    def __init__(self):\n        super(Identity, self).__init__()\n\n    def forward(self, input):\n        return input\n\n\nclass Mlp(nn.Layer):\n    def __init__(\n        self,\n        in_features,\n        hidden_features=None,\n        out_features=None,\n        act_layer=nn.GELU,\n        drop=0.0,\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass Attention(nn.Layer):\n    def __init__(\n        self,\n        dim,\n        num_heads=8,\n        qkv_bias=False,\n        qk_scale=None,\n        attn_drop=0.0,\n        proj_drop=0.0,\n    ):\n        super().__init__()\n        self.num_heads = num_heads\n        self.dim = dim\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim**-0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias_attr=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        qkv = paddle.reshape(\n            self.qkv(x), (0, -1, 3, self.num_heads, self.dim // self.num_heads)\n        ).transpose((2, 0, 3, 1, 4))\n        q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n\n        attn = q.matmul(k.transpose((0, 1, 3, 2)))\n        attn = nn.functional.softmax(attn, axis=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn.matmul(v)).transpose((0, 2, 1, 3)).reshape((0, -1, self.dim))\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass Block(nn.Layer):\n    def __init__(\n        self,\n        dim,\n        num_heads,\n        mlp_ratio=4.0,\n        qkv_bias=False,\n        qk_scale=None,\n        drop=0.0,\n        attn_drop=0.0,\n        drop_path=0.0,\n        act_layer=nn.GELU,\n        norm_layer=\"nn.LayerNorm\",\n        epsilon=1e-6,\n        prenorm=True,\n    ):\n        super().__init__()\n        if isinstance(norm_layer, str):\n            self.norm1 = eval(norm_layer)(dim, epsilon=epsilon)\n        else:\n            self.norm1 = norm_layer(dim)\n        self.mixer = Attention(\n            dim,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            qk_scale=qk_scale,\n            attn_drop=attn_drop,\n            proj_drop=drop,\n        )\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else Identity()\n        if isinstance(norm_layer, str):\n            self.norm2 = eval(norm_layer)(dim, epsilon=epsilon)\n        else:\n            self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp_ratio = mlp_ratio\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=mlp_hidden_dim,\n            act_layer=act_layer,\n            drop=drop,\n        )\n        self.prenorm = prenorm\n\n    def forward(self, x):\n        if self.prenorm:\n            x = self.norm1(x + self.drop_path(self.mixer(x)))\n            x = self.norm2(x + self.drop_path(self.mlp(x)))\n        else:\n            x = x + self.drop_path(self.mixer(self.norm1(x)))\n            x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n\n\nclass ViT(nn.Layer):\n    def __init__(\n        self,\n        img_size=[32, 128],\n        patch_size=[4, 4],\n        in_channels=3,\n        embed_dim=384,\n        depth=12,\n        num_heads=6,\n        mlp_ratio=4,\n        qkv_bias=False,\n        qk_scale=None,\n        drop_rate=0.0,\n        attn_drop_rate=0.0,\n        drop_path_rate=0.1,\n        norm_layer=\"nn.LayerNorm\",\n        epsilon=1e-6,\n        act=\"nn.GELU\",\n        prenorm=False,\n        **kwargs,\n    ):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.out_channels = embed_dim\n        self.prenorm = prenorm\n        self.patch_embed = nn.Conv2D(\n            in_channels, embed_dim, patch_size, patch_size, padding=(0, 0)\n        )\n        self.pos_embed = self.create_parameter(\n            shape=[1, 257, embed_dim], default_initializer=zeros_\n        )\n        self.add_parameter(\"pos_embed\", self.pos_embed)\n        self.pos_drop = nn.Dropout(p=drop_rate)\n        dpr = np.linspace(0, drop_path_rate, depth)\n        self.blocks1 = nn.LayerList(\n            [\n                Block(\n                    dim=embed_dim,\n                    num_heads=num_heads,\n                    mlp_ratio=mlp_ratio,\n                    qkv_bias=qkv_bias,\n                    qk_scale=qk_scale,\n                    drop=drop_rate,\n                    act_layer=eval(act),\n                    attn_drop=attn_drop_rate,\n                    drop_path=dpr[i],\n                    norm_layer=norm_layer,\n                    epsilon=epsilon,\n                    prenorm=prenorm,\n                )\n                for i in range(depth)\n            ]\n        )\n        if not prenorm:\n            self.norm = eval(norm_layer)(embed_dim, epsilon=epsilon)\n\n        self.avg_pool = nn.AdaptiveAvgPool2D([1, 25])\n        self.last_conv = nn.Conv2D(\n            in_channels=embed_dim,\n            out_channels=self.out_channels,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias_attr=False,\n        )\n        self.hardswish = nn.Hardswish()\n        self.dropout = nn.Dropout(p=0.1, mode=\"downscale_in_infer\")\n\n        trunc_normal_(self.pos_embed)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                zeros_(m.bias)\n        elif isinstance(m, nn.LayerNorm):\n            zeros_(m.bias)\n            ones_(m.weight)\n\n    def forward(self, x):\n        x = self.patch_embed(x).flatten(2).transpose((0, 2, 1))\n        x = x + self.pos_embed[:, 1:, :]  # [:, :x.shape[1], :]\n        x = self.pos_drop(x)\n        for blk in self.blocks1:\n            x = blk(x)\n        if not self.prenorm:\n            x = self.norm(x)\n\n        x = self.avg_pool(x.transpose([0, 2, 1]).reshape([0, self.embed_dim, -1, 25]))\n        x = self.last_conv(x)\n        x = self.hardswish(x)\n        x = self.dropout(x)\n        return x\n", "ppocr/modeling/backbones/rec_nrtr_mtb.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom paddle import nn\nimport paddle\n\n\nclass MTB(nn.Layer):\n    def __init__(self, cnn_num, in_channels):\n        super(MTB, self).__init__()\n        self.block = nn.Sequential()\n        self.out_channels = in_channels\n        self.cnn_num = cnn_num\n        if self.cnn_num == 2:\n            for i in range(self.cnn_num):\n                self.block.add_sublayer(\n                    \"conv_{}\".format(i),\n                    nn.Conv2D(\n                        in_channels=in_channels if i == 0 else 32 * (2 ** (i - 1)),\n                        out_channels=32 * (2**i),\n                        kernel_size=3,\n                        stride=2,\n                        padding=1,\n                    ),\n                )\n                self.block.add_sublayer(\"relu_{}\".format(i), nn.ReLU())\n                self.block.add_sublayer(\"bn_{}\".format(i), nn.BatchNorm2D(32 * (2**i)))\n\n    def forward(self, images):\n        x = self.block(images)\n        if self.cnn_num == 2:\n            # (b, w, h, c)\n            x = paddle.transpose(x, [0, 3, 2, 1])\n            x_shape = x.shape\n            x = paddle.reshape(x, [x_shape[0], x_shape[1], x_shape[2] * x_shape[3]])\n        return x\n", "ppocr/modeling/backbones/rec_repvit.py": "# copyright (c) 2024 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nThis code is refer from:\nhttps://github.com/THU-MIG/RepViT\n\"\"\"\n\nimport paddle.nn as nn\nimport paddle\nfrom paddle.nn.initializer import TruncatedNormal, Constant, Normal\n\ntrunc_normal_ = TruncatedNormal(std=0.02)\nnormal_ = Normal\nzeros_ = Constant(value=0.0)\nones_ = Constant(value=1.0)\n\n\ndef _make_divisible(v, divisor, min_value=None):\n    \"\"\"\n    This function is taken from the original tf repo.\n    It ensures that all layers have a channel number that is divisible by 8\n    It can be seen here:\n    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n    :param v:\n    :param divisor:\n    :param min_value:\n    :return:\n    \"\"\"\n    if min_value is None:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < 0.9 * v:\n        new_v += divisor\n    return new_v\n\n\n# from timm.models.layers import SqueezeExcite\n\n\ndef make_divisible(v, divisor=8, min_value=None, round_limit=0.9):\n    min_value = min_value or divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < round_limit * v:\n        new_v += divisor\n    return new_v\n\n\nclass SEModule(nn.Layer):\n    \"\"\"SE Module as defined in original SE-Nets with a few additions\n    Additions include:\n        * divisor can be specified to keep channels % div == 0 (default: 8)\n        * reduction channels can be specified directly by arg (if rd_channels is set)\n        * reduction channels can be specified by float rd_ratio (default: 1/16)\n        * global max pooling can be added to the squeeze aggregation\n        * customizable activation, normalization, and gate layer\n    \"\"\"\n\n    def __init__(\n        self,\n        channels,\n        rd_ratio=1.0 / 16,\n        rd_channels=None,\n        rd_divisor=8,\n        act_layer=nn.ReLU,\n    ):\n        super(SEModule, self).__init__()\n        if not rd_channels:\n            rd_channels = make_divisible(\n                channels * rd_ratio, rd_divisor, round_limit=0.0\n            )\n        self.fc1 = nn.Conv2D(channels, rd_channels, kernel_size=1, bias_attr=True)\n        self.act = act_layer()\n        self.fc2 = nn.Conv2D(rd_channels, channels, kernel_size=1, bias_attr=True)\n\n    def forward(self, x):\n        x_se = x.mean((2, 3), keepdim=True)\n        x_se = self.fc1(x_se)\n        x_se = self.act(x_se)\n        x_se = self.fc2(x_se)\n        return x * nn.functional.sigmoid(x_se)\n\n\nclass Conv2D_BN(nn.Sequential):\n    def __init__(\n        self,\n        a,\n        b,\n        ks=1,\n        stride=1,\n        pad=0,\n        dilation=1,\n        groups=1,\n        bn_weight_init=1,\n        resolution=-10000,\n    ):\n        super().__init__()\n        self.add_sublayer(\n            \"c\", nn.Conv2D(a, b, ks, stride, pad, dilation, groups, bias_attr=False)\n        )\n        self.add_sublayer(\"bn\", nn.BatchNorm2D(b))\n        if bn_weight_init == 1:\n            ones_(self.bn.weight)\n        else:\n            zeros_(self.bn.weight)\n        zeros_(self.bn.bias)\n\n    @paddle.no_grad()\n    def fuse(self):\n        c, bn = self.c, self.bn\n        w = bn.weight / (bn._variance + bn._epsilon) ** 0.5\n        w = c.weight * w[:, None, None, None]\n        b = bn.bias - bn._mean * bn.weight / (bn._variance + bn._epsilon) ** 0.5\n        m = nn.Conv2D(\n            w.shape[1] * self.c._groups,\n            w.shape[0],\n            w.shape[2:],\n            stride=self.c._stride,\n            padding=self.c._padding,\n            dilation=self.c._dilation,\n            groups=self.c._groups,\n        )\n        m.weight.set_value(w)\n        m.bias.set_value(b)\n        return m\n\n\nclass Residual(nn.Layer):\n    def __init__(self, m, drop=0.0):\n        super().__init__()\n        self.m = m\n        self.drop = drop\n\n    def forward(self, x):\n        if self.training and self.drop > 0:\n            return (\n                x\n                + self.m(x)\n                * paddle.rand(x.size(0), 1, 1, 1)\n                .ge_(self.drop)\n                .div(1 - self.drop)\n                .detach()\n            )\n        else:\n            return x + self.m(x)\n\n    @paddle.no_grad()\n    def fuse(self):\n        if isinstance(self.m, Conv2D_BN):\n            m = self.m.fuse()\n            assert m._groups == m.in_channels\n            identity = paddle.ones([m.weight.shape[0], m.weight.shape[1], 1, 1])\n            identity = nn.functional.pad(identity, [1, 1, 1, 1])\n            m.weight += identity\n            return m\n        elif isinstance(self.m, nn.Conv2D):\n            m = self.m\n            assert m._groups != m.in_channels\n            identity = paddle.ones([m.weight.shape[0], m.weight.shape[1], 1, 1])\n            identity = nn.functional.pad(identity, [1, 1, 1, 1])\n            m.weight += identity\n            return m\n        else:\n            return self\n\n\nclass RepVGGDW(nn.Layer):\n    def __init__(self, ed) -> None:\n        super().__init__()\n        self.conv = Conv2D_BN(ed, ed, 3, 1, 1, groups=ed)\n        self.conv1 = nn.Conv2D(ed, ed, 1, 1, 0, groups=ed)\n        self.dim = ed\n        self.bn = nn.BatchNorm2D(ed)\n\n    def forward(self, x):\n        return self.bn((self.conv(x) + self.conv1(x)) + x)\n\n    @paddle.no_grad()\n    def fuse(self):\n        conv = self.conv.fuse()\n        conv1 = self.conv1\n\n        conv_w = conv.weight\n        conv_b = conv.bias\n        conv1_w = conv1.weight\n        conv1_b = conv1.bias\n\n        conv1_w = nn.functional.pad(conv1_w, [1, 1, 1, 1])\n\n        identity = nn.functional.pad(\n            paddle.ones([conv1_w.shape[0], conv1_w.shape[1], 1, 1]), [1, 1, 1, 1]\n        )\n\n        final_conv_w = conv_w + conv1_w + identity\n        final_conv_b = conv_b + conv1_b\n\n        conv.weight.set_value(final_conv_w)\n        conv.bias.set_value(final_conv_b)\n\n        bn = self.bn\n        w = bn.weight / (bn._variance + bn._epsilon) ** 0.5\n        w = conv.weight * w[:, None, None, None]\n        b = (\n            bn.bias\n            + (conv.bias - bn._mean) * bn.weight / (bn._variance + bn._epsilon) ** 0.5\n        )\n        conv.weight.set_value(w)\n        conv.bias.set_value(b)\n        return conv\n\n\nclass RepViTBlock(nn.Layer):\n    def __init__(self, inp, hidden_dim, oup, kernel_size, stride, use_se, use_hs):\n        super(RepViTBlock, self).__init__()\n\n        self.identity = stride == 1 and inp == oup\n        assert hidden_dim == 2 * inp\n\n        if stride != 1:\n            self.token_mixer = nn.Sequential(\n                Conv2D_BN(\n                    inp, inp, kernel_size, stride, (kernel_size - 1) // 2, groups=inp\n                ),\n                SEModule(inp, 0.25) if use_se else nn.Identity(),\n                Conv2D_BN(inp, oup, ks=1, stride=1, pad=0),\n            )\n            self.channel_mixer = Residual(\n                nn.Sequential(\n                    # pw\n                    Conv2D_BN(oup, 2 * oup, 1, 1, 0),\n                    nn.GELU() if use_hs else nn.GELU(),\n                    # pw-linear\n                    Conv2D_BN(2 * oup, oup, 1, 1, 0, bn_weight_init=0),\n                )\n            )\n        else:\n            assert self.identity\n            self.token_mixer = nn.Sequential(\n                RepVGGDW(inp),\n                SEModule(inp, 0.25) if use_se else nn.Identity(),\n            )\n            self.channel_mixer = Residual(\n                nn.Sequential(\n                    # pw\n                    Conv2D_BN(inp, hidden_dim, 1, 1, 0),\n                    nn.GELU() if use_hs else nn.GELU(),\n                    # pw-linear\n                    Conv2D_BN(hidden_dim, oup, 1, 1, 0, bn_weight_init=0),\n                )\n            )\n\n    def forward(self, x):\n        return self.channel_mixer(self.token_mixer(x))\n\n\nclass RepViT(nn.Layer):\n    def __init__(self, cfgs, in_channels=3, out_indices=None):\n        super(RepViT, self).__init__()\n        # setting of inverted residual blocks\n        self.cfgs = cfgs\n\n        # building first layer\n        input_channel = self.cfgs[0][2]\n        patch_embed = nn.Sequential(\n            Conv2D_BN(in_channels, input_channel // 2, 3, 2, 1),\n            nn.GELU(),\n            Conv2D_BN(input_channel // 2, input_channel, 3, 2, 1),\n        )\n        layers = [patch_embed]\n        # building inverted residual blocks\n        block = RepViTBlock\n        for k, t, c, use_se, use_hs, s in self.cfgs:\n            output_channel = _make_divisible(c, 8)\n            exp_size = _make_divisible(input_channel * t, 8)\n            layers.append(\n                block(input_channel, exp_size, output_channel, k, s, use_se, use_hs)\n            )\n            input_channel = output_channel\n        self.features = nn.LayerList(layers)\n        self.out_indices = out_indices\n        if out_indices is not None:\n            self.out_channels = [self.cfgs[ids - 1][2] for ids in out_indices]\n        else:\n            self.out_channels = self.cfgs[-1][2]\n\n    def forward(self, x):\n        if self.out_indices is not None:\n            return self.forward_det(x)\n        return self.forward_rec(x)\n\n    def forward_det(self, x):\n        outs = []\n        for i, f in enumerate(self.features):\n            x = f(x)\n            if i in self.out_indices:\n                outs.append(x)\n        return outs\n\n    def forward_rec(self, x):\n        for f in self.features:\n            x = f(x)\n        h = x.shape[2]\n        x = nn.functional.avg_pool2d(x, [h, 2])\n        return x\n\n\ndef RepSVTR(in_channels=3):\n    \"\"\"\n    Constructs a MobileNetV3-Large model\n    \"\"\"\n    # k, t, c, SE, HS, s\n    cfgs = [\n        [3, 2, 96, 1, 0, 1],\n        [3, 2, 96, 0, 0, 1],\n        [3, 2, 96, 0, 0, 1],\n        [3, 2, 192, 0, 1, (2, 1)],\n        [3, 2, 192, 1, 1, 1],\n        [3, 2, 192, 0, 1, 1],\n        [3, 2, 192, 1, 1, 1],\n        [3, 2, 192, 0, 1, 1],\n        [3, 2, 192, 1, 1, 1],\n        [3, 2, 192, 0, 1, 1],\n        [3, 2, 384, 0, 1, (2, 1)],\n        [3, 2, 384, 1, 1, 1],\n        [3, 2, 384, 0, 1, 1],\n    ]\n    return RepViT(cfgs, in_channels=in_channels)\n\n\ndef RepSVTR_det(in_channels=3, out_indices=[2, 5, 10, 13]):\n    \"\"\"\n    Constructs a MobileNetV3-Large model\n    \"\"\"\n    # k, t, c, SE, HS, s\n    cfgs = [\n        [3, 2, 48, 1, 0, 1],\n        [3, 2, 48, 0, 0, 1],\n        [3, 2, 96, 0, 0, 2],\n        [3, 2, 96, 1, 0, 1],\n        [3, 2, 96, 0, 0, 1],\n        [3, 2, 192, 0, 1, 2],\n        [3, 2, 192, 1, 1, 1],\n        [3, 2, 192, 0, 1, 1],\n        [3, 2, 192, 1, 1, 1],\n        [3, 2, 192, 0, 1, 1],\n        [3, 2, 384, 0, 1, 2],\n        [3, 2, 384, 1, 1, 1],\n        [3, 2, 384, 0, 1, 1],\n    ]\n    return RepViT(cfgs, in_channels=in_channels, out_indices=out_indices)\n", "ppocr/modeling/backbones/rec_resnet_32.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/hikopensource/DAVAR-Lab-OCR/davarocr/davar_rcg/models/backbones/ResNet32.py\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle.nn as nn\n\n__all__ = [\"ResNet32\"]\n\nconv_weight_attr = nn.initializer.KaimingNormal()\n\n\nclass ResNet32(nn.Layer):\n    \"\"\"\n    Feature Extractor is proposed in  FAN Ref [1]\n\n    Ref [1]: Focusing Attention: Towards Accurate Text Recognition in Neural Images ICCV-2017\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels=512):\n        \"\"\"\n\n        Args:\n            in_channels (int): input channel\n            output_channel (int): output channel\n        \"\"\"\n        super(ResNet32, self).__init__()\n        self.out_channels = out_channels\n        self.ConvNet = ResNet(in_channels, out_channels, BasicBlock, [1, 2, 5, 3])\n\n    def forward(self, inputs):\n        \"\"\"\n        Args:\n            inputs: input feature\n\n        Returns:\n            output feature\n\n        \"\"\"\n        return self.ConvNet(inputs)\n\n\nclass BasicBlock(nn.Layer):\n    \"\"\"Res-net Basic Block\"\"\"\n\n    expansion = 1\n\n    def __init__(\n        self, inplanes, planes, stride=1, downsample=None, norm_type=\"BN\", **kwargs\n    ):\n        \"\"\"\n        Args:\n            inplanes (int): input channel\n            planes (int): channels of the middle feature\n            stride (int): stride of the convolution\n            downsample (int): type of the down_sample\n            norm_type (str): type of the normalization\n            **kwargs (None): backup parameter\n        \"\"\"\n        super(BasicBlock, self).__init__()\n        self.conv1 = self._conv3x3(inplanes, planes)\n        self.bn1 = nn.BatchNorm2D(planes)\n        self.conv2 = self._conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2D(planes)\n        self.relu = nn.ReLU()\n        self.downsample = downsample\n        self.stride = stride\n\n    def _conv3x3(self, in_planes, out_planes, stride=1):\n        \"\"\"\n\n        Args:\n            in_planes (int): input channel\n            out_planes (int): channels of the middle feature\n            stride (int): stride of the convolution\n        Returns:\n            nn.Layer: Conv2D with kernel = 3\n\n        \"\"\"\n\n        return nn.Conv2D(\n            in_planes,\n            out_planes,\n            kernel_size=3,\n            stride=stride,\n            padding=1,\n            weight_attr=conv_weight_attr,\n            bias_attr=False,\n        )\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Layer):\n    \"\"\"Res-Net network structure\"\"\"\n\n    def __init__(self, input_channel, output_channel, block, layers):\n        \"\"\"\n\n        Args:\n            input_channel (int): input channel\n            output_channel (int): output channel\n            block (BasicBlock): convolution block\n            layers (list): layers of the block\n        \"\"\"\n        super(ResNet, self).__init__()\n\n        self.output_channel_block = [\n            int(output_channel / 4),\n            int(output_channel / 2),\n            output_channel,\n            output_channel,\n        ]\n\n        self.inplanes = int(output_channel / 8)\n        self.conv0_1 = nn.Conv2D(\n            input_channel,\n            int(output_channel / 16),\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            weight_attr=conv_weight_attr,\n            bias_attr=False,\n        )\n        self.bn0_1 = nn.BatchNorm2D(int(output_channel / 16))\n        self.conv0_2 = nn.Conv2D(\n            int(output_channel / 16),\n            self.inplanes,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            weight_attr=conv_weight_attr,\n            bias_attr=False,\n        )\n        self.bn0_2 = nn.BatchNorm2D(self.inplanes)\n        self.relu = nn.ReLU()\n\n        self.maxpool1 = nn.MaxPool2D(kernel_size=2, stride=2, padding=0)\n        self.layer1 = self._make_layer(block, self.output_channel_block[0], layers[0])\n        self.conv1 = nn.Conv2D(\n            self.output_channel_block[0],\n            self.output_channel_block[0],\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            weight_attr=conv_weight_attr,\n            bias_attr=False,\n        )\n        self.bn1 = nn.BatchNorm2D(self.output_channel_block[0])\n\n        self.maxpool2 = nn.MaxPool2D(kernel_size=2, stride=2, padding=0)\n        self.layer2 = self._make_layer(\n            block, self.output_channel_block[1], layers[1], stride=1\n        )\n        self.conv2 = nn.Conv2D(\n            self.output_channel_block[1],\n            self.output_channel_block[1],\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            weight_attr=conv_weight_attr,\n            bias_attr=False,\n        )\n        self.bn2 = nn.BatchNorm2D(self.output_channel_block[1])\n\n        self.maxpool3 = nn.MaxPool2D(kernel_size=2, stride=(2, 1), padding=(0, 1))\n        self.layer3 = self._make_layer(\n            block, self.output_channel_block[2], layers[2], stride=1\n        )\n        self.conv3 = nn.Conv2D(\n            self.output_channel_block[2],\n            self.output_channel_block[2],\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            weight_attr=conv_weight_attr,\n            bias_attr=False,\n        )\n        self.bn3 = nn.BatchNorm2D(self.output_channel_block[2])\n\n        self.layer4 = self._make_layer(\n            block, self.output_channel_block[3], layers[3], stride=1\n        )\n        self.conv4_1 = nn.Conv2D(\n            self.output_channel_block[3],\n            self.output_channel_block[3],\n            kernel_size=2,\n            stride=(2, 1),\n            padding=(0, 1),\n            weight_attr=conv_weight_attr,\n            bias_attr=False,\n        )\n        self.bn4_1 = nn.BatchNorm2D(self.output_channel_block[3])\n        self.conv4_2 = nn.Conv2D(\n            self.output_channel_block[3],\n            self.output_channel_block[3],\n            kernel_size=2,\n            stride=1,\n            padding=0,\n            weight_attr=conv_weight_attr,\n            bias_attr=False,\n        )\n        self.bn4_2 = nn.BatchNorm2D(self.output_channel_block[3])\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        \"\"\"\n\n        Args:\n            block (block): convolution block\n            planes (int): input channels\n            blocks (list): layers of the block\n            stride (int): stride of the convolution\n\n        Returns:\n            nn.Sequential: the combination of the convolution block\n\n        \"\"\"\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2D(\n                    self.inplanes,\n                    planes * block.expansion,\n                    kernel_size=1,\n                    stride=stride,\n                    weight_attr=conv_weight_attr,\n                    bias_attr=False,\n                ),\n                nn.BatchNorm2D(planes * block.expansion),\n            )\n\n        layers = list()\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv0_1(x)\n        x = self.bn0_1(x)\n        x = self.relu(x)\n        x = self.conv0_2(x)\n        x = self.bn0_2(x)\n        x = self.relu(x)\n\n        x = self.maxpool1(x)\n        x = self.layer1(x)\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.maxpool2(x)\n        x = self.layer2(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n\n        x = self.maxpool3(x)\n        x = self.layer3(x)\n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = self.relu(x)\n\n        x = self.layer4(x)\n        x = self.conv4_1(x)\n        x = self.bn4_1(x)\n        x = self.relu(x)\n        x = self.conv4_2(x)\n        x = self.bn4_2(x)\n        x = self.relu(x)\n        return x\n", "ppocr/modeling/backbones/rec_resnet_fpn.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom paddle import nn, ParamAttr\nfrom paddle.nn import functional as F\nimport paddle\nimport numpy as np\n\n__all__ = [\"ResNetFPN\"]\n\n\nclass ResNetFPN(nn.Layer):\n    def __init__(self, in_channels=1, layers=50, **kwargs):\n        super(ResNetFPN, self).__init__()\n        supported_layers = {\n            18: {\"depth\": [2, 2, 2, 2], \"block_class\": BasicBlock},\n            34: {\"depth\": [3, 4, 6, 3], \"block_class\": BasicBlock},\n            50: {\"depth\": [3, 4, 6, 3], \"block_class\": BottleneckBlock},\n            101: {\"depth\": [3, 4, 23, 3], \"block_class\": BottleneckBlock},\n            152: {\"depth\": [3, 8, 36, 3], \"block_class\": BottleneckBlock},\n        }\n        stride_list = [(2, 2), (2, 2), (1, 1), (1, 1)]\n        num_filters = [64, 128, 256, 512]\n        self.depth = supported_layers[layers][\"depth\"]\n        self.F = []\n        self.conv = ConvBNLayer(\n            in_channels=in_channels,\n            out_channels=64,\n            kernel_size=7,\n            stride=2,\n            act=\"relu\",\n            name=\"conv1\",\n        )\n        self.block_list = []\n        in_ch = 64\n        if layers >= 50:\n            for block in range(len(self.depth)):\n                for i in range(self.depth[block]):\n                    if layers in [101, 152] and block == 2:\n                        if i == 0:\n                            conv_name = \"res\" + str(block + 2) + \"a\"\n                        else:\n                            conv_name = \"res\" + str(block + 2) + \"b\" + str(i)\n                    else:\n                        conv_name = \"res\" + str(block + 2) + chr(97 + i)\n                    block_list = self.add_sublayer(\n                        \"bottleneckBlock_{}_{}\".format(block, i),\n                        BottleneckBlock(\n                            in_channels=in_ch,\n                            out_channels=num_filters[block],\n                            stride=stride_list[block] if i == 0 else 1,\n                            name=conv_name,\n                        ),\n                    )\n                    in_ch = num_filters[block] * 4\n                    self.block_list.append(block_list)\n                self.F.append(block_list)\n        else:\n            for block in range(len(self.depth)):\n                for i in range(self.depth[block]):\n                    conv_name = \"res\" + str(block + 2) + chr(97 + i)\n                    if i == 0 and block != 0:\n                        stride = (2, 1)\n                    else:\n                        stride = (1, 1)\n                    basic_block = self.add_sublayer(\n                        conv_name,\n                        BasicBlock(\n                            in_channels=in_ch,\n                            out_channels=num_filters[block],\n                            stride=stride_list[block] if i == 0 else 1,\n                            is_first=block == i == 0,\n                            name=conv_name,\n                        ),\n                    )\n                    in_ch = basic_block.out_channels\n                    self.block_list.append(basic_block)\n        out_ch_list = [in_ch // 4, in_ch // 2, in_ch]\n        self.base_block = []\n        self.conv_trans = []\n        self.bn_block = []\n        for i in [-2, -3]:\n            in_channels = out_ch_list[i + 1] + out_ch_list[i]\n\n            self.base_block.append(\n                self.add_sublayer(\n                    \"F_{}_base_block_0\".format(i),\n                    nn.Conv2D(\n                        in_channels=in_channels,\n                        out_channels=out_ch_list[i],\n                        kernel_size=1,\n                        weight_attr=ParamAttr(trainable=True),\n                        bias_attr=ParamAttr(trainable=True),\n                    ),\n                )\n            )\n            self.base_block.append(\n                self.add_sublayer(\n                    \"F_{}_base_block_1\".format(i),\n                    nn.Conv2D(\n                        in_channels=out_ch_list[i],\n                        out_channels=out_ch_list[i],\n                        kernel_size=3,\n                        padding=1,\n                        weight_attr=ParamAttr(trainable=True),\n                        bias_attr=ParamAttr(trainable=True),\n                    ),\n                )\n            )\n            self.base_block.append(\n                self.add_sublayer(\n                    \"F_{}_base_block_2\".format(i),\n                    nn.BatchNorm(\n                        num_channels=out_ch_list[i],\n                        act=\"relu\",\n                        param_attr=ParamAttr(trainable=True),\n                        bias_attr=ParamAttr(trainable=True),\n                    ),\n                )\n            )\n        self.base_block.append(\n            self.add_sublayer(\n                \"F_{}_base_block_3\".format(i),\n                nn.Conv2D(\n                    in_channels=out_ch_list[i],\n                    out_channels=512,\n                    kernel_size=1,\n                    bias_attr=ParamAttr(trainable=True),\n                    weight_attr=ParamAttr(trainable=True),\n                ),\n            )\n        )\n        self.out_channels = 512\n\n    def __call__(self, x):\n        x = self.conv(x)\n        fpn_list = []\n        F = []\n        for i in range(len(self.depth)):\n            fpn_list.append(np.sum(self.depth[: i + 1]))\n\n        for i, block in enumerate(self.block_list):\n            x = block(x)\n            for number in fpn_list:\n                if i + 1 == number:\n                    F.append(x)\n        base = F[-1]\n\n        j = 0\n        for i, block in enumerate(self.base_block):\n            if i % 3 == 0 and i < 6:\n                j = j + 1\n                b, c, w, h = F[-j - 1].shape\n                if [w, h] == list(base.shape[2:]):\n                    base = base\n                else:\n                    base = self.conv_trans[j - 1](base)\n                    base = self.bn_block[j - 1](base)\n                base = paddle.concat([base, F[-j - 1]], axis=1)\n            base = block(base)\n        return base\n\n\nclass ConvBNLayer(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride=1,\n        groups=1,\n        act=None,\n        name=None,\n    ):\n        super(ConvBNLayer, self).__init__()\n        self.conv = nn.Conv2D(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=2 if stride == (1, 1) else kernel_size,\n            dilation=2 if stride == (1, 1) else 1,\n            stride=stride,\n            padding=(kernel_size - 1) // 2,\n            groups=groups,\n            weight_attr=ParamAttr(name=name + \".conv2d.output.1.w_0\"),\n            bias_attr=False,\n        )\n\n        if name == \"conv1\":\n            bn_name = \"bn_\" + name\n        else:\n            bn_name = \"bn\" + name[3:]\n        self.bn = nn.BatchNorm(\n            num_channels=out_channels,\n            act=act,\n            param_attr=ParamAttr(name=name + \".output.1.w_0\"),\n            bias_attr=ParamAttr(name=name + \".output.1.b_0\"),\n            moving_mean_name=bn_name + \"_mean\",\n            moving_variance_name=bn_name + \"_variance\",\n        )\n\n    def __call__(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return x\n\n\nclass ShortCut(nn.Layer):\n    def __init__(self, in_channels, out_channels, stride, name, is_first=False):\n        super(ShortCut, self).__init__()\n        self.use_conv = True\n\n        if in_channels != out_channels or stride != 1 or is_first == True:\n            if stride == (1, 1):\n                self.conv = ConvBNLayer(in_channels, out_channels, 1, 1, name=name)\n            else:  # stride==(2,2)\n                self.conv = ConvBNLayer(in_channels, out_channels, 1, stride, name=name)\n        else:\n            self.use_conv = False\n\n    def forward(self, x):\n        if self.use_conv:\n            x = self.conv(x)\n        return x\n\n\nclass BottleneckBlock(nn.Layer):\n    def __init__(self, in_channels, out_channels, stride, name):\n        super(BottleneckBlock, self).__init__()\n        self.conv0 = ConvBNLayer(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=1,\n            act=\"relu\",\n            name=name + \"_branch2a\",\n        )\n        self.conv1 = ConvBNLayer(\n            in_channels=out_channels,\n            out_channels=out_channels,\n            kernel_size=3,\n            stride=stride,\n            act=\"relu\",\n            name=name + \"_branch2b\",\n        )\n\n        self.conv2 = ConvBNLayer(\n            in_channels=out_channels,\n            out_channels=out_channels * 4,\n            kernel_size=1,\n            act=None,\n            name=name + \"_branch2c\",\n        )\n\n        self.short = ShortCut(\n            in_channels=in_channels,\n            out_channels=out_channels * 4,\n            stride=stride,\n            is_first=False,\n            name=name + \"_branch1\",\n        )\n        self.out_channels = out_channels * 4\n\n    def forward(self, x):\n        y = self.conv0(x)\n        y = self.conv1(y)\n        y = self.conv2(y)\n        y = y + self.short(x)\n        y = F.relu(y)\n        return y\n\n\nclass BasicBlock(nn.Layer):\n    def __init__(self, in_channels, out_channels, stride, name, is_first):\n        super(BasicBlock, self).__init__()\n        self.conv0 = ConvBNLayer(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=3,\n            act=\"relu\",\n            stride=stride,\n            name=name + \"_branch2a\",\n        )\n        self.conv1 = ConvBNLayer(\n            in_channels=out_channels,\n            out_channels=out_channels,\n            kernel_size=3,\n            act=None,\n            name=name + \"_branch2b\",\n        )\n        self.short = ShortCut(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            stride=stride,\n            is_first=is_first,\n            name=name + \"_branch1\",\n        )\n        self.out_channels = out_channels\n\n    def forward(self, x):\n        y = self.conv0(x)\n        y = self.conv1(y)\n        y = y + self.short(x)\n        return F.relu(y)\n", "ppocr/modeling/backbones/rec_mobilenet_v3.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom paddle import nn\n\nfrom ppocr.modeling.backbones.det_mobilenet_v3 import (\n    ResidualUnit,\n    ConvBNLayer,\n    make_divisible,\n)\n\n__all__ = [\"MobileNetV3\"]\n\n\nclass MobileNetV3(nn.Layer):\n    def __init__(\n        self,\n        in_channels=3,\n        model_name=\"small\",\n        scale=0.5,\n        large_stride=None,\n        small_stride=None,\n        disable_se=False,\n        **kwargs,\n    ):\n        super(MobileNetV3, self).__init__()\n        self.disable_se = disable_se\n        if small_stride is None:\n            small_stride = [2, 2, 2, 2]\n        if large_stride is None:\n            large_stride = [1, 2, 2, 2]\n\n        assert isinstance(\n            large_stride, list\n        ), \"large_stride type must \" \"be list but got {}\".format(type(large_stride))\n        assert isinstance(\n            small_stride, list\n        ), \"small_stride type must \" \"be list but got {}\".format(type(small_stride))\n        assert (\n            len(large_stride) == 4\n        ), \"large_stride length must be \" \"4 but got {}\".format(len(large_stride))\n        assert (\n            len(small_stride) == 4\n        ), \"small_stride length must be \" \"4 but got {}\".format(len(small_stride))\n\n        if model_name == \"large\":\n            cfg = [\n                # k, exp, c,  se,     nl,  s,\n                [3, 16, 16, False, \"relu\", large_stride[0]],\n                [3, 64, 24, False, \"relu\", (large_stride[1], 1)],\n                [3, 72, 24, False, \"relu\", 1],\n                [5, 72, 40, True, \"relu\", (large_stride[2], 1)],\n                [5, 120, 40, True, \"relu\", 1],\n                [5, 120, 40, True, \"relu\", 1],\n                [3, 240, 80, False, \"hardswish\", 1],\n                [3, 200, 80, False, \"hardswish\", 1],\n                [3, 184, 80, False, \"hardswish\", 1],\n                [3, 184, 80, False, \"hardswish\", 1],\n                [3, 480, 112, True, \"hardswish\", 1],\n                [3, 672, 112, True, \"hardswish\", 1],\n                [5, 672, 160, True, \"hardswish\", (large_stride[3], 1)],\n                [5, 960, 160, True, \"hardswish\", 1],\n                [5, 960, 160, True, \"hardswish\", 1],\n            ]\n            cls_ch_squeeze = 960\n        elif model_name == \"small\":\n            cfg = [\n                # k, exp, c,  se,     nl,  s,\n                [3, 16, 16, True, \"relu\", (small_stride[0], 1)],\n                [3, 72, 24, False, \"relu\", (small_stride[1], 1)],\n                [3, 88, 24, False, \"relu\", 1],\n                [5, 96, 40, True, \"hardswish\", (small_stride[2], 1)],\n                [5, 240, 40, True, \"hardswish\", 1],\n                [5, 240, 40, True, \"hardswish\", 1],\n                [5, 120, 48, True, \"hardswish\", 1],\n                [5, 144, 48, True, \"hardswish\", 1],\n                [5, 288, 96, True, \"hardswish\", (small_stride[3], 1)],\n                [5, 576, 96, True, \"hardswish\", 1],\n                [5, 576, 96, True, \"hardswish\", 1],\n            ]\n            cls_ch_squeeze = 576\n        else:\n            raise NotImplementedError(\n                \"mode[\" + model_name + \"_model] is not implemented!\"\n            )\n\n        supported_scale = [0.35, 0.5, 0.75, 1.0, 1.25]\n        assert (\n            scale in supported_scale\n        ), \"supported scales are {} but input scale is {}\".format(\n            supported_scale, scale\n        )\n\n        inplanes = 16\n        # conv1\n        self.conv1 = ConvBNLayer(\n            in_channels=in_channels,\n            out_channels=make_divisible(inplanes * scale),\n            kernel_size=3,\n            stride=2,\n            padding=1,\n            groups=1,\n            if_act=True,\n            act=\"hardswish\",\n        )\n        i = 0\n        block_list = []\n        inplanes = make_divisible(inplanes * scale)\n        for k, exp, c, se, nl, s in cfg:\n            se = se and not self.disable_se\n            block_list.append(\n                ResidualUnit(\n                    in_channels=inplanes,\n                    mid_channels=make_divisible(scale * exp),\n                    out_channels=make_divisible(scale * c),\n                    kernel_size=k,\n                    stride=s,\n                    use_se=se,\n                    act=nl,\n                )\n            )\n            inplanes = make_divisible(scale * c)\n            i += 1\n        self.blocks = nn.Sequential(*block_list)\n\n        self.conv2 = ConvBNLayer(\n            in_channels=inplanes,\n            out_channels=make_divisible(scale * cls_ch_squeeze),\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            groups=1,\n            if_act=True,\n            act=\"hardswish\",\n        )\n\n        self.pool = nn.MaxPool2D(kernel_size=2, stride=2, padding=0)\n        self.out_channels = make_divisible(scale * cls_ch_squeeze)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.blocks(x)\n        x = self.conv2(x)\n        x = self.pool(x)\n        return x\n", "ppocr/modeling/backbones/__init__.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n__all__ = [\"build_backbone\"]\n\n\ndef build_backbone(config, model_type):\n    if model_type == \"det\" or model_type == \"table\":\n        from .det_mobilenet_v3 import MobileNetV3\n        from .det_resnet import ResNet\n        from .det_resnet_vd import ResNet_vd\n        from .det_resnet_vd_sast import ResNet_SAST\n        from .det_pp_lcnet import PPLCNet\n        from .rec_lcnetv3 import PPLCNetV3\n        from .rec_hgnet import PPHGNet_small\n        from .rec_vit import ViT\n        from .det_pp_lcnet_v2 import PPLCNetV2_base\n        from .rec_repvit import RepSVTR_det\n\n        support_dict = [\n            \"MobileNetV3\",\n            \"ResNet\",\n            \"ResNet_vd\",\n            \"ResNet_SAST\",\n            \"PPLCNet\",\n            \"PPLCNetV3\",\n            \"PPHGNet_small\",\n            \"PPLCNetV2_base\",\n            \"RepSVTR_det\",\n        ]\n        if model_type == \"table\":\n            from .table_master_resnet import TableResNetExtra\n\n            support_dict.append(\"TableResNetExtra\")\n    elif model_type == \"rec\" or model_type == \"cls\":\n        from .rec_mobilenet_v3 import MobileNetV3\n        from .rec_resnet_vd import ResNet\n        from .rec_resnet_fpn import ResNetFPN\n        from .rec_mv1_enhance import MobileNetV1Enhance\n        from .rec_nrtr_mtb import MTB\n        from .rec_resnet_31 import ResNet31\n        from .rec_resnet_32 import ResNet32\n        from .rec_resnet_45 import ResNet45\n        from .rec_resnet_aster import ResNet_ASTER\n        from .rec_micronet import MicroNet\n        from .rec_efficientb3_pren import EfficientNetb3_PREN\n        from .rec_svtrnet import SVTRNet\n        from .rec_vitstr import ViTSTR\n        from .rec_resnet_rfl import ResNetRFL\n        from .rec_densenet import DenseNet\n        from .rec_shallow_cnn import ShallowCNN\n        from .rec_lcnetv3 import PPLCNetV3\n        from .rec_hgnet import PPHGNet_small\n        from .rec_vit_parseq import ViTParseQ\n        from .rec_repvit import RepSVTR\n        from .rec_svtrv2 import SVTRv2\n\n        support_dict = [\n            \"MobileNetV1Enhance\",\n            \"MobileNetV3\",\n            \"ResNet\",\n            \"ResNetFPN\",\n            \"MTB\",\n            \"ResNet31\",\n            \"ResNet45\",\n            \"ResNet_ASTER\",\n            \"MicroNet\",\n            \"EfficientNetb3_PREN\",\n            \"SVTRNet\",\n            \"ViTSTR\",\n            \"ResNet32\",\n            \"ResNetRFL\",\n            \"DenseNet\",\n            \"ShallowCNN\",\n            \"PPLCNetV3\",\n            \"PPHGNet_small\",\n            \"ViTParseQ\",\n            \"ViT\",\n            \"RepSVTR\",\n            \"SVTRv2\",\n        ]\n    elif model_type == \"e2e\":\n        from .e2e_resnet_vd_pg import ResNet\n\n        support_dict = [\"ResNet\"]\n    elif model_type == \"kie\":\n        from .kie_unet_sdmgr import Kie_backbone\n        from .vqa_layoutlm import (\n            LayoutLMForSer,\n            LayoutLMv2ForSer,\n            LayoutLMv2ForRe,\n            LayoutXLMForSer,\n            LayoutXLMForRe,\n        )\n\n        support_dict = [\n            \"Kie_backbone\",\n            \"LayoutLMForSer\",\n            \"LayoutLMv2ForSer\",\n            \"LayoutLMv2ForRe\",\n            \"LayoutXLMForSer\",\n            \"LayoutXLMForRe\",\n        ]\n    elif model_type == \"table\":\n        from .table_resnet_vd import ResNet\n        from .table_mobilenet_v3 import MobileNetV3\n\n        support_dict = [\"ResNet\", \"MobileNetV3\"]\n    else:\n        raise NotImplementedError\n\n    module_name = config.pop(\"name\")\n    assert module_name in support_dict, Exception(\n        \"when model typs is {}, backbone only support {}\".format(\n            model_type, support_dict\n        )\n    )\n    module_class = eval(module_name)(**config)\n    return module_class\n", "ppocr/modeling/backbones/rec_hgnet.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport paddle\nimport paddle.nn as nn\nimport paddle.nn.functional as F\nfrom paddle.nn.initializer import KaimingNormal, Constant\nfrom paddle.nn import Conv2D, BatchNorm2D, ReLU, AdaptiveAvgPool2D, MaxPool2D\nfrom paddle.regularizer import L2Decay\nfrom paddle import ParamAttr\n\nkaiming_normal_ = KaimingNormal()\nzeros_ = Constant(value=0.0)\nones_ = Constant(value=1.0)\n\n\nclass ConvBNAct(nn.Layer):\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride, groups=1, use_act=True\n    ):\n        super().__init__()\n        self.use_act = use_act\n        self.conv = Conv2D(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            padding=(kernel_size - 1) // 2,\n            groups=groups,\n            bias_attr=False,\n        )\n        self.bn = BatchNorm2D(\n            out_channels,\n            weight_attr=ParamAttr(regularizer=L2Decay(0.0)),\n            bias_attr=ParamAttr(regularizer=L2Decay(0.0)),\n        )\n        if self.use_act:\n            self.act = ReLU()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        if self.use_act:\n            x = self.act(x)\n        return x\n\n\nclass ESEModule(nn.Layer):\n    def __init__(self, channels):\n        super().__init__()\n        self.avg_pool = AdaptiveAvgPool2D(1)\n        self.conv = Conv2D(\n            in_channels=channels,\n            out_channels=channels,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n        )\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        identity = x\n        x = self.avg_pool(x)\n        x = self.conv(x)\n        x = self.sigmoid(x)\n        return paddle.multiply(x=identity, y=x)\n\n\nclass HG_Block(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        mid_channels,\n        out_channels,\n        layer_num,\n        identity=False,\n    ):\n        super().__init__()\n        self.identity = identity\n\n        self.layers = nn.LayerList()\n        self.layers.append(\n            ConvBNAct(\n                in_channels=in_channels,\n                out_channels=mid_channels,\n                kernel_size=3,\n                stride=1,\n            )\n        )\n        for _ in range(layer_num - 1):\n            self.layers.append(\n                ConvBNAct(\n                    in_channels=mid_channels,\n                    out_channels=mid_channels,\n                    kernel_size=3,\n                    stride=1,\n                )\n            )\n\n        # feature aggregation\n        total_channels = in_channels + layer_num * mid_channels\n        self.aggregation_conv = ConvBNAct(\n            in_channels=total_channels,\n            out_channels=out_channels,\n            kernel_size=1,\n            stride=1,\n        )\n        self.att = ESEModule(out_channels)\n\n    def forward(self, x):\n        identity = x\n        output = []\n        output.append(x)\n        for layer in self.layers:\n            x = layer(x)\n            output.append(x)\n        x = paddle.concat(output, axis=1)\n        x = self.aggregation_conv(x)\n        x = self.att(x)\n        if self.identity:\n            x += identity\n        return x\n\n\nclass HG_Stage(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        mid_channels,\n        out_channels,\n        block_num,\n        layer_num,\n        downsample=True,\n        stride=[2, 1],\n    ):\n        super().__init__()\n        self.downsample = downsample\n        if downsample:\n            self.downsample = ConvBNAct(\n                in_channels=in_channels,\n                out_channels=in_channels,\n                kernel_size=3,\n                stride=stride,\n                groups=in_channels,\n                use_act=False,\n            )\n\n        blocks_list = []\n        blocks_list.append(\n            HG_Block(in_channels, mid_channels, out_channels, layer_num, identity=False)\n        )\n        for _ in range(block_num - 1):\n            blocks_list.append(\n                HG_Block(\n                    out_channels, mid_channels, out_channels, layer_num, identity=True\n                )\n            )\n        self.blocks = nn.Sequential(*blocks_list)\n\n    def forward(self, x):\n        if self.downsample:\n            x = self.downsample(x)\n        x = self.blocks(x)\n        return x\n\n\nclass PPHGNet(nn.Layer):\n    \"\"\"\n    PPHGNet\n    Args:\n        stem_channels: list. Stem channel list of PPHGNet.\n        stage_config: dict. The configuration of each stage of PPHGNet. such as the number of channels, stride, etc.\n        layer_num: int. Number of layers of HG_Block.\n        use_last_conv: boolean. Whether to use a 1x1 convolutional layer before the classification layer.\n        class_expand: int=2048. Number of channels for the last 1x1 convolutional layer.\n        dropout_prob: float. Parameters of dropout, 0.0 means dropout is not used.\n        class_num: int=1000. The number of classes.\n    Returns:\n        model: nn.Layer. Specific PPHGNet model depends on args.\n    \"\"\"\n\n    def __init__(\n        self,\n        stem_channels,\n        stage_config,\n        layer_num,\n        in_channels=3,\n        det=False,\n        out_indices=None,\n    ):\n        super().__init__()\n        self.det = det\n        self.out_indices = out_indices if out_indices is not None else [0, 1, 2, 3]\n\n        # stem\n        stem_channels.insert(0, in_channels)\n        self.stem = nn.Sequential(\n            *[\n                ConvBNAct(\n                    in_channels=stem_channels[i],\n                    out_channels=stem_channels[i + 1],\n                    kernel_size=3,\n                    stride=2 if i == 0 else 1,\n                )\n                for i in range(len(stem_channels) - 1)\n            ]\n        )\n\n        if self.det:\n            self.pool = nn.MaxPool2D(kernel_size=3, stride=2, padding=1)\n        # stages\n        self.stages = nn.LayerList()\n        self.out_channels = []\n        for block_id, k in enumerate(stage_config):\n            (\n                in_channels,\n                mid_channels,\n                out_channels,\n                block_num,\n                downsample,\n                stride,\n            ) = stage_config[k]\n            self.stages.append(\n                HG_Stage(\n                    in_channels,\n                    mid_channels,\n                    out_channels,\n                    block_num,\n                    layer_num,\n                    downsample,\n                    stride,\n                )\n            )\n            if block_id in self.out_indices:\n                self.out_channels.append(out_channels)\n\n        if not self.det:\n            self.out_channels = stage_config[\"stage4\"][2]\n\n        self._init_weights()\n\n    def _init_weights(self):\n        for m in self.sublayers():\n            if isinstance(m, nn.Conv2D):\n                kaiming_normal_(m.weight)\n            elif isinstance(m, (nn.BatchNorm2D)):\n                ones_(m.weight)\n                zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                zeros_(m.bias)\n\n    def forward(self, x):\n        x = self.stem(x)\n        if self.det:\n            x = self.pool(x)\n\n        out = []\n        for i, stage in enumerate(self.stages):\n            x = stage(x)\n            if self.det and i in self.out_indices:\n                out.append(x)\n        if self.det:\n            return out\n\n        if self.training:\n            x = F.adaptive_avg_pool2d(x, [1, 40])\n        else:\n            x = F.avg_pool2d(x, [3, 2])\n        return x\n\n\ndef PPHGNet_tiny(pretrained=False, use_ssld=False, **kwargs):\n    \"\"\"\n    PPHGNet_tiny\n    Args:\n        pretrained: bool=False or str. If `True` load pretrained parameters, `False` otherwise.\n                    If str, means the path of the pretrained model.\n        use_ssld: bool=False. Whether using distillation pretrained model when pretrained=True.\n    Returns:\n        model: nn.Layer. Specific `PPHGNet_tiny` model depends on args.\n    \"\"\"\n    stage_config = {\n        # in_channels, mid_channels, out_channels, blocks, downsample\n        \"stage1\": [96, 96, 224, 1, False, [2, 1]],\n        \"stage2\": [224, 128, 448, 1, True, [1, 2]],\n        \"stage3\": [448, 160, 512, 2, True, [2, 1]],\n        \"stage4\": [512, 192, 768, 1, True, [2, 1]],\n    }\n\n    model = PPHGNet(\n        stem_channels=[48, 48, 96], stage_config=stage_config, layer_num=5, **kwargs\n    )\n    return model\n\n\ndef PPHGNet_small(pretrained=False, use_ssld=False, det=False, **kwargs):\n    \"\"\"\n    PPHGNet_small\n    Args:\n        pretrained: bool=False or str. If `True` load pretrained parameters, `False` otherwise.\n                    If str, means the path of the pretrained model.\n        use_ssld: bool=False. Whether using distillation pretrained model when pretrained=True.\n    Returns:\n        model: nn.Layer. Specific `PPHGNet_small` model depends on args.\n    \"\"\"\n    stage_config_det = {\n        # in_channels, mid_channels, out_channels, blocks, downsample\n        \"stage1\": [128, 128, 256, 1, False, 2],\n        \"stage2\": [256, 160, 512, 1, True, 2],\n        \"stage3\": [512, 192, 768, 2, True, 2],\n        \"stage4\": [768, 224, 1024, 1, True, 2],\n    }\n\n    stage_config_rec = {\n        # in_channels, mid_channels, out_channels, blocks, downsample\n        \"stage1\": [128, 128, 256, 1, True, [2, 1]],\n        \"stage2\": [256, 160, 512, 1, True, [1, 2]],\n        \"stage3\": [512, 192, 768, 2, True, [2, 1]],\n        \"stage4\": [768, 224, 1024, 1, True, [2, 1]],\n    }\n\n    model = PPHGNet(\n        stem_channels=[64, 64, 128],\n        stage_config=stage_config_det if det else stage_config_rec,\n        layer_num=6,\n        det=det,\n        **kwargs,\n    )\n    return model\n\n\ndef PPHGNet_base(pretrained=False, use_ssld=True, **kwargs):\n    \"\"\"\n    PPHGNet_base\n    Args:\n        pretrained: bool=False or str. If `True` load pretrained parameters, `False` otherwise.\n                    If str, means the path of the pretrained model.\n        use_ssld: bool=False. Whether using distillation pretrained model when pretrained=True.\n    Returns:\n        model: nn.Layer. Specific `PPHGNet_base` model depends on args.\n    \"\"\"\n    stage_config = {\n        # in_channels, mid_channels, out_channels, blocks, downsample\n        \"stage1\": [160, 192, 320, 1, False, [2, 1]],\n        \"stage2\": [320, 224, 640, 2, True, [1, 2]],\n        \"stage3\": [640, 256, 960, 3, True, [2, 1]],\n        \"stage4\": [960, 288, 1280, 2, True, [2, 1]],\n    }\n\n    model = PPHGNet(\n        stem_channels=[96, 96, 160],\n        stage_config=stage_config,\n        layer_num=7,\n        dropout_prob=0.2,\n        **kwargs,\n    )\n    return model\n", "ppocr/modeling/backbones/det_resnet_vd.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nfrom paddle import ParamAttr\nimport paddle.nn as nn\nimport paddle.nn.functional as F\n\nfrom paddle.vision.ops import DeformConv2D\nfrom paddle.regularizer import L2Decay\nfrom paddle.nn.initializer import Normal, Constant, XavierUniform\n\n__all__ = [\"ResNet_vd\", \"ConvBNLayer\", \"DeformableConvV2\"]\n\n\nclass DeformableConvV2(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride=1,\n        padding=0,\n        dilation=1,\n        groups=1,\n        weight_attr=None,\n        bias_attr=None,\n        lr_scale=1,\n        regularizer=None,\n        skip_quant=False,\n        dcn_bias_regularizer=L2Decay(0.0),\n        dcn_bias_lr_scale=2.0,\n    ):\n        super(DeformableConvV2, self).__init__()\n        self.offset_channel = 2 * kernel_size**2 * groups\n        self.mask_channel = kernel_size**2 * groups\n\n        if bias_attr:\n            # in FCOS-DCN head, specifically need learning_rate and regularizer\n            dcn_bias_attr = ParamAttr(\n                initializer=Constant(value=0),\n                regularizer=dcn_bias_regularizer,\n                learning_rate=dcn_bias_lr_scale,\n            )\n        else:\n            # in ResNet backbone, do not need bias\n            dcn_bias_attr = False\n        self.conv_dcn = DeformConv2D(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride=stride,\n            padding=(kernel_size - 1) // 2 * dilation,\n            dilation=dilation,\n            deformable_groups=groups,\n            weight_attr=weight_attr,\n            bias_attr=dcn_bias_attr,\n        )\n\n        if lr_scale == 1 and regularizer is None:\n            offset_bias_attr = ParamAttr(initializer=Constant(0.0))\n        else:\n            offset_bias_attr = ParamAttr(\n                initializer=Constant(0.0),\n                learning_rate=lr_scale,\n                regularizer=regularizer,\n            )\n        self.conv_offset = nn.Conv2D(\n            in_channels,\n            groups * 3 * kernel_size**2,\n            kernel_size,\n            stride=stride,\n            padding=(kernel_size - 1) // 2,\n            weight_attr=ParamAttr(initializer=Constant(0.0)),\n            bias_attr=offset_bias_attr,\n        )\n        if skip_quant:\n            self.conv_offset.skip_quant = True\n\n    def forward(self, x):\n        offset_mask = self.conv_offset(x)\n        offset, mask = paddle.split(\n            offset_mask,\n            num_or_sections=[self.offset_channel, self.mask_channel],\n            axis=1,\n        )\n        mask = F.sigmoid(mask)\n        y = self.conv_dcn(x, offset, mask=mask)\n        return y\n\n\nclass ConvBNLayer(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride=1,\n        groups=1,\n        dcn_groups=1,\n        is_vd_mode=False,\n        act=None,\n        is_dcn=False,\n    ):\n        super(ConvBNLayer, self).__init__()\n\n        self.is_vd_mode = is_vd_mode\n        self._pool2d_avg = nn.AvgPool2D(\n            kernel_size=2, stride=2, padding=0, ceil_mode=True\n        )\n        if not is_dcn:\n            self._conv = nn.Conv2D(\n                in_channels=in_channels,\n                out_channels=out_channels,\n                kernel_size=kernel_size,\n                stride=stride,\n                padding=(kernel_size - 1) // 2,\n                groups=groups,\n                bias_attr=False,\n            )\n        else:\n            self._conv = DeformableConvV2(\n                in_channels=in_channels,\n                out_channels=out_channels,\n                kernel_size=kernel_size,\n                stride=stride,\n                padding=(kernel_size - 1) // 2,\n                groups=dcn_groups,  # groups,\n                bias_attr=False,\n            )\n        self._batch_norm = nn.BatchNorm(out_channels, act=act)\n\n    def forward(self, inputs):\n        if self.is_vd_mode:\n            inputs = self._pool2d_avg(inputs)\n        y = self._conv(inputs)\n        y = self._batch_norm(y)\n        return y\n\n\nclass BottleneckBlock(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        stride,\n        shortcut=True,\n        if_first=False,\n        is_dcn=False,\n    ):\n        super(BottleneckBlock, self).__init__()\n\n        self.conv0 = ConvBNLayer(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=1,\n            act=\"relu\",\n        )\n        self.conv1 = ConvBNLayer(\n            in_channels=out_channels,\n            out_channels=out_channels,\n            kernel_size=3,\n            stride=stride,\n            act=\"relu\",\n            is_dcn=is_dcn,\n            dcn_groups=2,\n        )\n        self.conv2 = ConvBNLayer(\n            in_channels=out_channels,\n            out_channels=out_channels * 4,\n            kernel_size=1,\n            act=None,\n        )\n\n        if not shortcut:\n            self.short = ConvBNLayer(\n                in_channels=in_channels,\n                out_channels=out_channels * 4,\n                kernel_size=1,\n                stride=1,\n                is_vd_mode=False if if_first else True,\n            )\n\n        self.shortcut = shortcut\n\n    def forward(self, inputs):\n        y = self.conv0(inputs)\n        conv1 = self.conv1(y)\n        conv2 = self.conv2(conv1)\n\n        if self.shortcut:\n            short = inputs\n        else:\n            short = self.short(inputs)\n        y = paddle.add(x=short, y=conv2)\n        y = F.relu(y)\n        return y\n\n\nclass BasicBlock(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        stride,\n        shortcut=True,\n        if_first=False,\n    ):\n        super(BasicBlock, self).__init__()\n        self.stride = stride\n        self.conv0 = ConvBNLayer(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=3,\n            stride=stride,\n            act=\"relu\",\n        )\n        self.conv1 = ConvBNLayer(\n            in_channels=out_channels, out_channels=out_channels, kernel_size=3, act=None\n        )\n\n        if not shortcut:\n            self.short = ConvBNLayer(\n                in_channels=in_channels,\n                out_channels=out_channels,\n                kernel_size=1,\n                stride=1,\n                is_vd_mode=False if if_first else True,\n            )\n\n        self.shortcut = shortcut\n\n    def forward(self, inputs):\n        y = self.conv0(inputs)\n        conv1 = self.conv1(y)\n\n        if self.shortcut:\n            short = inputs\n        else:\n            short = self.short(inputs)\n        y = paddle.add(x=short, y=conv1)\n        y = F.relu(y)\n        return y\n\n\nclass ResNet_vd(nn.Layer):\n    def __init__(\n        self, in_channels=3, layers=50, dcn_stage=None, out_indices=None, **kwargs\n    ):\n        super(ResNet_vd, self).__init__()\n\n        self.layers = layers\n        supported_layers = [18, 34, 50, 101, 152, 200]\n        assert (\n            layers in supported_layers\n        ), \"supported layers are {} but input layer is {}\".format(\n            supported_layers, layers\n        )\n\n        if layers == 18:\n            depth = [2, 2, 2, 2]\n        elif layers == 34 or layers == 50:\n            depth = [3, 4, 6, 3]\n        elif layers == 101:\n            depth = [3, 4, 23, 3]\n        elif layers == 152:\n            depth = [3, 8, 36, 3]\n        elif layers == 200:\n            depth = [3, 12, 48, 3]\n        num_channels = [64, 256, 512, 1024] if layers >= 50 else [64, 64, 128, 256]\n        num_filters = [64, 128, 256, 512]\n\n        self.dcn_stage = (\n            dcn_stage if dcn_stage is not None else [False, False, False, False]\n        )\n        self.out_indices = out_indices if out_indices is not None else [0, 1, 2, 3]\n\n        self.conv1_1 = ConvBNLayer(\n            in_channels=in_channels,\n            out_channels=32,\n            kernel_size=3,\n            stride=2,\n            act=\"relu\",\n        )\n        self.conv1_2 = ConvBNLayer(\n            in_channels=32, out_channels=32, kernel_size=3, stride=1, act=\"relu\"\n        )\n        self.conv1_3 = ConvBNLayer(\n            in_channels=32, out_channels=64, kernel_size=3, stride=1, act=\"relu\"\n        )\n        self.pool2d_max = nn.MaxPool2D(kernel_size=3, stride=2, padding=1)\n\n        self.stages = []\n        self.out_channels = []\n        if layers >= 50:\n            for block in range(len(depth)):\n                block_list = []\n                shortcut = False\n                is_dcn = self.dcn_stage[block]\n                for i in range(depth[block]):\n                    bottleneck_block = self.add_sublayer(\n                        \"bb_%d_%d\" % (block, i),\n                        BottleneckBlock(\n                            in_channels=(\n                                num_channels[block]\n                                if i == 0\n                                else num_filters[block] * 4\n                            ),\n                            out_channels=num_filters[block],\n                            stride=2 if i == 0 and block != 0 else 1,\n                            shortcut=shortcut,\n                            if_first=block == i == 0,\n                            is_dcn=is_dcn,\n                        ),\n                    )\n                    shortcut = True\n                    block_list.append(bottleneck_block)\n                if block in self.out_indices:\n                    self.out_channels.append(num_filters[block] * 4)\n                self.stages.append(nn.Sequential(*block_list))\n        else:\n            for block in range(len(depth)):\n                block_list = []\n                shortcut = False\n                for i in range(depth[block]):\n                    basic_block = self.add_sublayer(\n                        \"bb_%d_%d\" % (block, i),\n                        BasicBlock(\n                            in_channels=(\n                                num_channels[block] if i == 0 else num_filters[block]\n                            ),\n                            out_channels=num_filters[block],\n                            stride=2 if i == 0 and block != 0 else 1,\n                            shortcut=shortcut,\n                            if_first=block == i == 0,\n                        ),\n                    )\n                    shortcut = True\n                    block_list.append(basic_block)\n                if block in self.out_indices:\n                    self.out_channels.append(num_filters[block])\n                self.stages.append(nn.Sequential(*block_list))\n\n    def forward(self, inputs):\n        y = self.conv1_1(inputs)\n        y = self.conv1_2(y)\n        y = self.conv1_3(y)\n        y = self.pool2d_max(y)\n        out = []\n        for i, block in enumerate(self.stages):\n            y = block(y)\n            if i in self.out_indices:\n                out.append(y)\n        return out\n", "ppocr/modeling/heads/det_pse_head.py": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/whai362/PSENet/blob/python3/models/head/psenet_head.py\n\"\"\"\n\nfrom paddle import nn\n\n\nclass PSEHead(nn.Layer):\n    def __init__(self, in_channels, hidden_dim=256, out_channels=7, **kwargs):\n        super(PSEHead, self).__init__()\n        self.conv1 = nn.Conv2D(\n            in_channels, hidden_dim, kernel_size=3, stride=1, padding=1\n        )\n        self.bn1 = nn.BatchNorm2D(hidden_dim)\n        self.relu1 = nn.ReLU()\n\n        self.conv2 = nn.Conv2D(\n            hidden_dim, out_channels, kernel_size=1, stride=1, padding=0\n        )\n\n    def forward(self, x, **kwargs):\n        out = self.conv1(x)\n        out = self.relu1(self.bn1(out))\n        out = self.conv2(out)\n        return {\"maps\": out}\n", "ppocr/modeling/heads/e2e_pg_head.py": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport paddle\nfrom paddle import nn\nimport paddle.nn.functional as F\nfrom paddle import ParamAttr\n\n\nclass ConvBNLayer(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride,\n        padding,\n        groups=1,\n        if_act=True,\n        act=None,\n        name=None,\n    ):\n        super(ConvBNLayer, self).__init__()\n        self.if_act = if_act\n        self.act = act\n        self.conv = nn.Conv2D(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            groups=groups,\n            weight_attr=ParamAttr(name=name + \"_weights\"),\n            bias_attr=False,\n        )\n\n        self.bn = nn.BatchNorm(\n            num_channels=out_channels,\n            act=act,\n            param_attr=ParamAttr(name=\"bn_\" + name + \"_scale\"),\n            bias_attr=ParamAttr(name=\"bn_\" + name + \"_offset\"),\n            moving_mean_name=\"bn_\" + name + \"_mean\",\n            moving_variance_name=\"bn_\" + name + \"_variance\",\n            use_global_stats=False,\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return x\n\n\nclass PGHead(nn.Layer):\n    \"\"\" \"\"\"\n\n    def __init__(\n        self, in_channels, character_dict_path=\"ppocr/utils/ic15_dict.txt\", **kwargs\n    ):\n        super(PGHead, self).__init__()\n\n        # get character_length\n        with open(character_dict_path, \"rb\") as fin:\n            lines = fin.readlines()\n            character_length = len(lines) + 1\n\n        self.conv_f_score1 = ConvBNLayer(\n            in_channels=in_channels,\n            out_channels=64,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            act=\"relu\",\n            name=\"conv_f_score{}\".format(1),\n        )\n        self.conv_f_score2 = ConvBNLayer(\n            in_channels=64,\n            out_channels=64,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            act=\"relu\",\n            name=\"conv_f_score{}\".format(2),\n        )\n        self.conv_f_score3 = ConvBNLayer(\n            in_channels=64,\n            out_channels=128,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            act=\"relu\",\n            name=\"conv_f_score{}\".format(3),\n        )\n\n        self.conv1 = nn.Conv2D(\n            in_channels=128,\n            out_channels=1,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            groups=1,\n            weight_attr=ParamAttr(name=\"conv_f_score{}\".format(4)),\n            bias_attr=False,\n        )\n\n        self.conv_f_boder1 = ConvBNLayer(\n            in_channels=in_channels,\n            out_channels=64,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            act=\"relu\",\n            name=\"conv_f_boder{}\".format(1),\n        )\n        self.conv_f_boder2 = ConvBNLayer(\n            in_channels=64,\n            out_channels=64,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            act=\"relu\",\n            name=\"conv_f_boder{}\".format(2),\n        )\n        self.conv_f_boder3 = ConvBNLayer(\n            in_channels=64,\n            out_channels=128,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            act=\"relu\",\n            name=\"conv_f_boder{}\".format(3),\n        )\n        self.conv2 = nn.Conv2D(\n            in_channels=128,\n            out_channels=4,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            groups=1,\n            weight_attr=ParamAttr(name=\"conv_f_boder{}\".format(4)),\n            bias_attr=False,\n        )\n        self.conv_f_char1 = ConvBNLayer(\n            in_channels=in_channels,\n            out_channels=128,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            act=\"relu\",\n            name=\"conv_f_char{}\".format(1),\n        )\n        self.conv_f_char2 = ConvBNLayer(\n            in_channels=128,\n            out_channels=128,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            act=\"relu\",\n            name=\"conv_f_char{}\".format(2),\n        )\n        self.conv_f_char3 = ConvBNLayer(\n            in_channels=128,\n            out_channels=256,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            act=\"relu\",\n            name=\"conv_f_char{}\".format(3),\n        )\n        self.conv_f_char4 = ConvBNLayer(\n            in_channels=256,\n            out_channels=256,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            act=\"relu\",\n            name=\"conv_f_char{}\".format(4),\n        )\n        self.conv_f_char5 = ConvBNLayer(\n            in_channels=256,\n            out_channels=256,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            act=\"relu\",\n            name=\"conv_f_char{}\".format(5),\n        )\n        self.conv3 = nn.Conv2D(\n            in_channels=256,\n            out_channels=character_length,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            groups=1,\n            weight_attr=ParamAttr(name=\"conv_f_char{}\".format(6)),\n            bias_attr=False,\n        )\n\n        self.conv_f_direc1 = ConvBNLayer(\n            in_channels=in_channels,\n            out_channels=64,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            act=\"relu\",\n            name=\"conv_f_direc{}\".format(1),\n        )\n        self.conv_f_direc2 = ConvBNLayer(\n            in_channels=64,\n            out_channels=64,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            act=\"relu\",\n            name=\"conv_f_direc{}\".format(2),\n        )\n        self.conv_f_direc3 = ConvBNLayer(\n            in_channels=64,\n            out_channels=128,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            act=\"relu\",\n            name=\"conv_f_direc{}\".format(3),\n        )\n        self.conv4 = nn.Conv2D(\n            in_channels=128,\n            out_channels=2,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            groups=1,\n            weight_attr=ParamAttr(name=\"conv_f_direc{}\".format(4)),\n            bias_attr=False,\n        )\n\n    def forward(self, x, targets=None):\n        f_score = self.conv_f_score1(x)\n        f_score = self.conv_f_score2(f_score)\n        f_score = self.conv_f_score3(f_score)\n        f_score = self.conv1(f_score)\n        f_score = F.sigmoid(f_score)\n\n        # f_border\n        f_border = self.conv_f_boder1(x)\n        f_border = self.conv_f_boder2(f_border)\n        f_border = self.conv_f_boder3(f_border)\n        f_border = self.conv2(f_border)\n\n        f_char = self.conv_f_char1(x)\n        f_char = self.conv_f_char2(f_char)\n        f_char = self.conv_f_char3(f_char)\n        f_char = self.conv_f_char4(f_char)\n        f_char = self.conv_f_char5(f_char)\n        f_char = self.conv3(f_char)\n\n        f_direction = self.conv_f_direc1(x)\n        f_direction = self.conv_f_direc2(f_direction)\n        f_direction = self.conv_f_direc3(f_direction)\n        f_direction = self.conv4(f_direction)\n\n        predicts = {}\n        predicts[\"f_score\"] = f_score\n        predicts[\"f_border\"] = f_border\n        predicts[\"f_char\"] = f_char\n        predicts[\"f_direction\"] = f_direction\n        return predicts\n", "ppocr/modeling/heads/rec_abinet_head.py": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/FangShancheng/ABINet/tree/main/modules\n\"\"\"\n\nimport math\nimport paddle\nfrom paddle import nn\nimport paddle.nn.functional as F\nfrom paddle.nn import LayerList\nfrom ppocr.modeling.heads.rec_nrtr_head import TransformerBlock, PositionalEncoding\n\n\nclass BCNLanguage(nn.Layer):\n    def __init__(\n        self,\n        d_model=512,\n        nhead=8,\n        num_layers=4,\n        dim_feedforward=2048,\n        dropout=0.0,\n        max_length=25,\n        detach=True,\n        num_classes=37,\n    ):\n        super().__init__()\n\n        self.d_model = d_model\n        self.detach = detach\n        self.max_length = max_length + 1  # additional stop token\n        self.proj = nn.Linear(num_classes, d_model, bias_attr=False)\n        self.token_encoder = PositionalEncoding(\n            dropout=0.1, dim=d_model, max_len=self.max_length\n        )\n        self.pos_encoder = PositionalEncoding(\n            dropout=0, dim=d_model, max_len=self.max_length\n        )\n\n        self.decoder = nn.LayerList(\n            [\n                TransformerBlock(\n                    d_model=d_model,\n                    nhead=nhead,\n                    dim_feedforward=dim_feedforward,\n                    attention_dropout_rate=dropout,\n                    residual_dropout_rate=dropout,\n                    with_self_attn=False,\n                    with_cross_attn=True,\n                )\n                for i in range(num_layers)\n            ]\n        )\n\n        self.cls = nn.Linear(d_model, num_classes)\n\n    def forward(self, tokens, lengths):\n        \"\"\"\n        Args:\n            tokens: (B, N, C) where N is length, B is batch size and C is classes number\n            lengths: (B,)\n        \"\"\"\n        if self.detach:\n            tokens = tokens.detach()\n        embed = self.proj(tokens)  # (B, N, C)\n        embed = self.token_encoder(embed)  # (B, N, C)\n        padding_mask = _get_mask(lengths, self.max_length)\n        zeros = paddle.zeros_like(embed)  # (B, N, C)\n        qeury = self.pos_encoder(zeros)\n        for decoder_layer in self.decoder:\n            qeury = decoder_layer(qeury, embed, cross_mask=padding_mask)\n        output = qeury  # (B, N, C)\n\n        logits = self.cls(output)  # (B, N, C)\n\n        return output, logits\n\n\ndef encoder_layer(in_c, out_c, k=3, s=2, p=1):\n    return nn.Sequential(\n        nn.Conv2D(in_c, out_c, k, s, p), nn.BatchNorm2D(out_c), nn.ReLU()\n    )\n\n\ndef decoder_layer(\n    in_c, out_c, k=3, s=1, p=1, mode=\"nearest\", scale_factor=None, size=None\n):\n    align_corners = False if mode == \"nearest\" else True\n    return nn.Sequential(\n        nn.Upsample(\n            size=size, scale_factor=scale_factor, mode=mode, align_corners=align_corners\n        ),\n        nn.Conv2D(in_c, out_c, k, s, p),\n        nn.BatchNorm2D(out_c),\n        nn.ReLU(),\n    )\n\n\nclass PositionAttention(nn.Layer):\n    def __init__(\n        self,\n        max_length,\n        in_channels=512,\n        num_channels=64,\n        h=8,\n        w=32,\n        mode=\"nearest\",\n        **kwargs,\n    ):\n        super().__init__()\n        self.max_length = max_length\n        self.k_encoder = nn.Sequential(\n            encoder_layer(in_channels, num_channels, s=(1, 2)),\n            encoder_layer(num_channels, num_channels, s=(2, 2)),\n            encoder_layer(num_channels, num_channels, s=(2, 2)),\n            encoder_layer(num_channels, num_channels, s=(2, 2)),\n        )\n        self.k_decoder = nn.Sequential(\n            decoder_layer(num_channels, num_channels, scale_factor=2, mode=mode),\n            decoder_layer(num_channels, num_channels, scale_factor=2, mode=mode),\n            decoder_layer(num_channels, num_channels, scale_factor=2, mode=mode),\n            decoder_layer(num_channels, in_channels, size=(h, w), mode=mode),\n        )\n\n        self.pos_encoder = PositionalEncoding(\n            dropout=0, dim=in_channels, max_len=max_length\n        )\n        self.project = nn.Linear(in_channels, in_channels)\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        k, v = x, x\n\n        # calculate key vector\n        features = []\n        for i in range(0, len(self.k_encoder)):\n            k = self.k_encoder[i](k)\n            features.append(k)\n        for i in range(0, len(self.k_decoder) - 1):\n            k = self.k_decoder[i](k)\n            # print(k.shape, features[len(self.k_decoder) - 2 - i].shape)\n            k = k + features[len(self.k_decoder) - 2 - i]\n        k = self.k_decoder[-1](k)\n\n        # calculate query vector\n        # TODO q=f(q,k)\n        zeros = paddle.zeros((B, self.max_length, C), dtype=x.dtype)  # (B, N, C)\n        q = self.pos_encoder(zeros)  # (B, N, C)\n        q = self.project(q)  # (B, N, C)\n\n        # calculate attention\n        attn_scores = q @ k.flatten(2)  # (B, N, (H*W))\n        attn_scores = attn_scores / (C**0.5)\n        attn_scores = F.softmax(attn_scores, axis=-1)\n\n        v = v.flatten(2).transpose([0, 2, 1])  # (B, (H*W), C)\n        attn_vecs = attn_scores @ v  # (B, N, C)\n\n        return attn_vecs, attn_scores.reshape([0, self.max_length, H, W])\n\n\nclass ABINetHead(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        d_model=512,\n        nhead=8,\n        num_layers=3,\n        dim_feedforward=2048,\n        dropout=0.1,\n        max_length=25,\n        use_lang=False,\n        iter_size=1,\n        image_size=(32, 128),\n    ):\n        super().__init__()\n        self.max_length = max_length + 1\n        h, w = image_size[0] // 4, image_size[1] // 4\n        self.pos_encoder = PositionalEncoding(dropout=0.1, dim=d_model, max_len=h * w)\n        self.encoder = nn.LayerList(\n            [\n                TransformerBlock(\n                    d_model=d_model,\n                    nhead=nhead,\n                    dim_feedforward=dim_feedforward,\n                    attention_dropout_rate=dropout,\n                    residual_dropout_rate=dropout,\n                    with_self_attn=True,\n                    with_cross_attn=False,\n                )\n                for i in range(num_layers)\n            ]\n        )\n        self.decoder = PositionAttention(\n            max_length=max_length + 1, mode=\"nearest\", h=h, w=w  # additional stop token\n        )\n        self.out_channels = out_channels\n        self.cls = nn.Linear(d_model, self.out_channels)\n        self.use_lang = use_lang\n        if use_lang:\n            self.iter_size = iter_size\n            self.language = BCNLanguage(\n                d_model=d_model,\n                nhead=nhead,\n                num_layers=4,\n                dim_feedforward=dim_feedforward,\n                dropout=dropout,\n                max_length=max_length,\n                num_classes=self.out_channels,\n            )\n            # alignment\n            self.w_att_align = nn.Linear(2 * d_model, d_model)\n            self.cls_align = nn.Linear(d_model, self.out_channels)\n\n    def forward(self, x, targets=None):\n        x = x.transpose([0, 2, 3, 1])\n        _, H, W, C = x.shape\n        feature = x.flatten(1, 2)\n        feature = self.pos_encoder(feature)\n        for encoder_layer in self.encoder:\n            feature = encoder_layer(feature)\n        feature = feature.reshape([0, H, W, C]).transpose([0, 3, 1, 2])\n        v_feature, attn_scores = self.decoder(feature)  # (B, N, C), (B, C, H, W)\n        vis_logits = self.cls(v_feature)  # (B, N, C)\n        logits = vis_logits\n        vis_lengths = _get_length(vis_logits)\n        if self.use_lang:\n            align_logits = vis_logits\n            align_lengths = vis_lengths\n            all_l_res, all_a_res = [], []\n            for i in range(self.iter_size):\n                tokens = F.softmax(align_logits, axis=-1)\n                lengths = align_lengths\n                lengths = paddle.clip(\n                    lengths, 2, self.max_length\n                )  # TODO:move to langauge model\n                l_feature, l_logits = self.language(tokens, lengths)\n\n                # alignment\n                all_l_res.append(l_logits)\n                fuse = paddle.concat((l_feature, v_feature), -1)\n                f_att = F.sigmoid(self.w_att_align(fuse))\n                output = f_att * v_feature + (1 - f_att) * l_feature\n                align_logits = self.cls_align(output)  # (B, N, C)\n\n                align_lengths = _get_length(align_logits)\n                all_a_res.append(align_logits)\n            if self.training:\n                return {\"align\": all_a_res, \"lang\": all_l_res, \"vision\": vis_logits}\n            else:\n                logits = align_logits\n        if self.training:\n            return logits\n        else:\n            return F.softmax(logits, -1)\n\n\ndef _get_length(logit):\n    \"\"\"Greed decoder to obtain length from logit\"\"\"\n    out = logit.argmax(-1) == 0\n    abn = out.any(-1)\n    out_int = out.cast(\"int32\")\n    out = (out_int.cumsum(-1) == 1) & out\n    out = out.cast(\"int32\")\n    out = out.argmax(-1)\n    out = out + 1\n    len_seq = paddle.zeros_like(out) + logit.shape[1]\n    out = paddle.where(abn, out, len_seq)\n    return out\n\n\ndef _get_mask(length, max_length):\n    \"\"\"Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n    Unmasked positions are filled with float(0.0).\n    \"\"\"\n    length = length.unsqueeze(-1)\n    B = length.shape[0]\n    grid = paddle.arange(0, max_length).unsqueeze(0).tile([B, 1])\n    zero_mask = paddle.zeros([B, max_length], dtype=\"float32\")\n    inf_mask = paddle.full([B, max_length], \"-inf\", dtype=\"float32\")\n    diag_mask = paddle.diag(\n        paddle.full([max_length], \"-inf\", dtype=paddle.float32), offset=0, name=None\n    )\n    mask = paddle.where(grid >= length, inf_mask, zero_mask)\n    mask = mask.unsqueeze(1) + diag_mask\n    return mask.unsqueeze(1)\n", "ppocr/modeling/heads/rec_visionlan_head.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/wangyuxin87/VisionLAN\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nfrom paddle import ParamAttr\nimport paddle.nn as nn\nimport paddle.nn.functional as F\nfrom paddle.nn.initializer import Normal, XavierNormal\nimport numpy as np\n\n\nclass PositionalEncoding(nn.Layer):\n    def __init__(self, d_hid, n_position=200):\n        super(PositionalEncoding, self).__init__()\n        self.register_buffer(\n            \"pos_table\", self._get_sinusoid_encoding_table(n_position, d_hid)\n        )\n\n    def _get_sinusoid_encoding_table(self, n_position, d_hid):\n        \"\"\"Sinusoid position encoding table\"\"\"\n\n        def get_position_angle_vec(position):\n            return [\n                position / np.power(10000, 2 * (hid_j // 2) / d_hid)\n                for hid_j in range(d_hid)\n            ]\n\n        sinusoid_table = np.array(\n            [get_position_angle_vec(pos_i) for pos_i in range(n_position)]\n        )\n        sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n        sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n        sinusoid_table = paddle.to_tensor(sinusoid_table, dtype=\"float32\")\n        sinusoid_table = paddle.unsqueeze(sinusoid_table, axis=0)\n        return sinusoid_table\n\n    def forward(self, x):\n        return x + self.pos_table[:, : x.shape[1]].clone().detach()\n\n\nclass ScaledDotProductAttention(nn.Layer):\n    \"Scaled Dot-Product Attention\"\n\n    def __init__(self, temperature, attn_dropout=0.1):\n        super(ScaledDotProductAttention, self).__init__()\n        self.temperature = temperature\n        self.dropout = nn.Dropout(attn_dropout)\n        self.softmax = nn.Softmax(axis=2)\n\n    def forward(self, q, k, v, mask=None):\n        k = paddle.transpose(k, perm=[0, 2, 1])\n        attn = paddle.bmm(q, k)\n        attn = attn / self.temperature\n        if mask is not None:\n            attn = attn.masked_fill(mask, -1e9)\n            if mask.dim() == 3:\n                mask = paddle.unsqueeze(mask, axis=1)\n            elif mask.dim() == 2:\n                mask = paddle.unsqueeze(mask, axis=1)\n                mask = paddle.unsqueeze(mask, axis=1)\n            repeat_times = [\n                attn.shape[1] // mask.shape[1],\n                attn.shape[2] // mask.shape[2],\n            ]\n            mask = paddle.tile(mask, [1, repeat_times[0], repeat_times[1], 1])\n            attn[mask == 0] = -1e9\n        attn = self.softmax(attn)\n        attn = self.dropout(attn)\n        output = paddle.bmm(attn, v)\n        return output\n\n\nclass MultiHeadAttention(nn.Layer):\n    \"Multi-Head Attention module\"\n\n    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n        super(MultiHeadAttention, self).__init__()\n        self.n_head = n_head\n        self.d_k = d_k\n        self.d_v = d_v\n        self.w_qs = nn.Linear(\n            d_model,\n            n_head * d_k,\n            weight_attr=ParamAttr(\n                initializer=Normal(mean=0, std=np.sqrt(2.0 / (d_model + d_k)))\n            ),\n        )\n        self.w_ks = nn.Linear(\n            d_model,\n            n_head * d_k,\n            weight_attr=ParamAttr(\n                initializer=Normal(mean=0, std=np.sqrt(2.0 / (d_model + d_k)))\n            ),\n        )\n        self.w_vs = nn.Linear(\n            d_model,\n            n_head * d_v,\n            weight_attr=ParamAttr(\n                initializer=Normal(mean=0, std=np.sqrt(2.0 / (d_model + d_v)))\n            ),\n        )\n\n        self.attention = ScaledDotProductAttention(temperature=np.power(d_k, 0.5))\n        self.layer_norm = nn.LayerNorm(d_model)\n        self.fc = nn.Linear(\n            n_head * d_v, d_model, weight_attr=ParamAttr(initializer=XavierNormal())\n        )\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, q, k, v, mask=None):\n        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n        sz_b, len_q, _ = q.shape\n        sz_b, len_k, _ = k.shape\n        sz_b, len_v, _ = v.shape\n        residual = q\n\n        q = self.w_qs(q)\n        q = paddle.reshape(q, shape=[-1, len_q, n_head, d_k])  # 4*21*512 ---- 4*21*8*64\n        k = self.w_ks(k)\n        k = paddle.reshape(k, shape=[-1, len_k, n_head, d_k])\n        v = self.w_vs(v)\n        v = paddle.reshape(v, shape=[-1, len_v, n_head, d_v])\n\n        q = paddle.transpose(q, perm=[2, 0, 1, 3])\n        q = paddle.reshape(q, shape=[-1, len_q, d_k])  # (n*b) x lq x dk\n        k = paddle.transpose(k, perm=[2, 0, 1, 3])\n        k = paddle.reshape(k, shape=[-1, len_k, d_k])  # (n*b) x lk x dk\n        v = paddle.transpose(v, perm=[2, 0, 1, 3])\n        v = paddle.reshape(v, shape=[-1, len_v, d_v])  # (n*b) x lv x dv\n\n        mask = (\n            paddle.tile(mask, [n_head, 1, 1]) if mask is not None else None\n        )  # (n*b) x .. x ..\n        output = self.attention(q, k, v, mask=mask)\n        output = paddle.reshape(output, shape=[n_head, -1, len_q, d_v])\n        output = paddle.transpose(output, perm=[1, 2, 0, 3])\n        output = paddle.reshape(\n            output, shape=[-1, len_q, n_head * d_v]\n        )  # b x lq x (n*dv)\n        output = self.dropout(self.fc(output))\n        output = self.layer_norm(output + residual)\n        return output\n\n\nclass PositionwiseFeedForward(nn.Layer):\n    def __init__(self, d_in, d_hid, dropout=0.1):\n        super(PositionwiseFeedForward, self).__init__()\n        self.w_1 = nn.Conv1D(d_in, d_hid, 1)  # position-wise\n        self.w_2 = nn.Conv1D(d_hid, d_in, 1)  # position-wise\n        self.layer_norm = nn.LayerNorm(d_in)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        residual = x\n        x = paddle.transpose(x, perm=[0, 2, 1])\n        x = self.w_2(F.relu(self.w_1(x)))\n        x = paddle.transpose(x, perm=[0, 2, 1])\n        x = self.dropout(x)\n        x = self.layer_norm(x + residual)\n        return x\n\n\nclass EncoderLayer(nn.Layer):\n    \"\"\"Compose with two layers\"\"\"\n\n    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1):\n        super(EncoderLayer, self).__init__()\n        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)\n\n    def forward(self, enc_input, slf_attn_mask=None):\n        enc_output = self.slf_attn(enc_input, enc_input, enc_input, mask=slf_attn_mask)\n        enc_output = self.pos_ffn(enc_output)\n        return enc_output\n\n\nclass Transformer_Encoder(nn.Layer):\n    def __init__(\n        self,\n        n_layers=2,\n        n_head=8,\n        d_word_vec=512,\n        d_k=64,\n        d_v=64,\n        d_model=512,\n        d_inner=2048,\n        dropout=0.1,\n        n_position=256,\n    ):\n        super(Transformer_Encoder, self).__init__()\n        self.position_enc = PositionalEncoding(d_word_vec, n_position=n_position)\n        self.dropout = nn.Dropout(p=dropout)\n        self.layer_stack = nn.LayerList(\n            [\n                EncoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout)\n                for _ in range(n_layers)\n            ]\n        )\n        self.layer_norm = nn.LayerNorm(d_model, epsilon=1e-6)\n\n    def forward(self, enc_output, src_mask, return_attns=False):\n        enc_output = self.dropout(self.position_enc(enc_output))  # position embeding\n        for enc_layer in self.layer_stack:\n            enc_output = enc_layer(enc_output, slf_attn_mask=src_mask)\n        enc_output = self.layer_norm(enc_output)\n        return enc_output\n\n\nclass PP_layer(nn.Layer):\n    def __init__(self, n_dim=512, N_max_character=25, n_position=256):\n        super(PP_layer, self).__init__()\n        self.character_len = N_max_character\n        self.f0_embedding = nn.Embedding(N_max_character, n_dim)\n        self.w0 = nn.Linear(N_max_character, n_position)\n        self.wv = nn.Linear(n_dim, n_dim)\n        self.we = nn.Linear(n_dim, N_max_character)\n        self.active = nn.Tanh()\n        self.softmax = nn.Softmax(axis=2)\n\n    def forward(self, enc_output):\n        # enc_output: b,256,512\n        reading_order = paddle.arange(self.character_len, dtype=\"int64\")\n        reading_order = reading_order.unsqueeze(0).expand(\n            [enc_output.shape[0], self.character_len]\n        )  # (S,) -> (B, S)\n        reading_order = self.f0_embedding(reading_order)  # b,25,512\n\n        # calculate attention\n        reading_order = paddle.transpose(reading_order, perm=[0, 2, 1])\n        t = self.w0(reading_order)  # b,512,256\n        t = self.active(\n            paddle.transpose(t, perm=[0, 2, 1]) + self.wv(enc_output)\n        )  # b,256,512\n        t = self.we(t)  # b,256,25\n        t = self.softmax(paddle.transpose(t, perm=[0, 2, 1]))  # b,25,256\n        g_output = paddle.bmm(t, enc_output)  # b,25,512\n        return g_output\n\n\nclass Prediction(nn.Layer):\n    def __init__(self, n_dim=512, n_position=256, N_max_character=25, n_class=37):\n        super(Prediction, self).__init__()\n        self.pp = PP_layer(\n            n_dim=n_dim, N_max_character=N_max_character, n_position=n_position\n        )\n        self.pp_share = PP_layer(\n            n_dim=n_dim, N_max_character=N_max_character, n_position=n_position\n        )\n        self.w_vrm = nn.Linear(n_dim, n_class)  # output layer\n        self.w_share = nn.Linear(n_dim, n_class)  # output layer\n        self.nclass = n_class\n\n    def forward(self, cnn_feature, f_res, f_sub, train_mode=False, use_mlm=True):\n        if train_mode:\n            if not use_mlm:\n                g_output = self.pp(cnn_feature)  # b,25,512\n                g_output = self.w_vrm(g_output)\n                f_res = 0\n                f_sub = 0\n                return g_output, f_res, f_sub\n            g_output = self.pp(cnn_feature)  # b,25,512\n            f_res = self.pp_share(f_res)\n            f_sub = self.pp_share(f_sub)\n            g_output = self.w_vrm(g_output)\n            f_res = self.w_share(f_res)\n            f_sub = self.w_share(f_sub)\n            return g_output, f_res, f_sub\n        else:\n            g_output = self.pp(cnn_feature)  # b,25,512\n            g_output = self.w_vrm(g_output)\n            return g_output\n\n\nclass MLM(nn.Layer):\n    \"Architecture of MLM\"\n\n    def __init__(self, n_dim=512, n_position=256, max_text_length=25):\n        super(MLM, self).__init__()\n        self.MLM_SequenceModeling_mask = Transformer_Encoder(\n            n_layers=2, n_position=n_position\n        )\n        self.MLM_SequenceModeling_WCL = Transformer_Encoder(\n            n_layers=1, n_position=n_position\n        )\n        self.pos_embedding = nn.Embedding(max_text_length, n_dim)\n        self.w0_linear = nn.Linear(1, n_position)\n        self.wv = nn.Linear(n_dim, n_dim)\n        self.active = nn.Tanh()\n        self.we = nn.Linear(n_dim, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x, label_pos):\n        # transformer unit for generating mask_c\n        feature_v_seq = self.MLM_SequenceModeling_mask(x, src_mask=None)\n        # position embedding layer\n        label_pos = paddle.to_tensor(label_pos, dtype=\"int64\")\n        pos_emb = self.pos_embedding(label_pos)\n        pos_emb = self.w0_linear(paddle.unsqueeze(pos_emb, axis=2))\n        pos_emb = paddle.transpose(pos_emb, perm=[0, 2, 1])\n        # fusion position embedding with features V & generate mask_c\n        att_map_sub = self.active(pos_emb + self.wv(feature_v_seq))\n        att_map_sub = self.we(att_map_sub)  # b,256,1\n        att_map_sub = paddle.transpose(att_map_sub, perm=[0, 2, 1])\n        att_map_sub = self.sigmoid(att_map_sub)  # b,1,256\n        # WCL\n        ## generate inputs for WCL\n        att_map_sub = paddle.transpose(att_map_sub, perm=[0, 2, 1])\n        f_res = x * (1 - att_map_sub)  # second path with remaining string\n        f_sub = x * att_map_sub  # first path with occluded character\n        ## transformer units in WCL\n        f_res = self.MLM_SequenceModeling_WCL(f_res, src_mask=None)\n        f_sub = self.MLM_SequenceModeling_WCL(f_sub, src_mask=None)\n        return f_res, f_sub, att_map_sub\n\n\ndef trans_1d_2d(x):\n    b, w_h, c = x.shape  # b, 256, 512\n    x = paddle.transpose(x, perm=[0, 2, 1])\n    x = paddle.reshape(x, [-1, c, 32, 8])\n    x = paddle.transpose(x, perm=[0, 1, 3, 2])  # [b, c, 8, 32]\n    return x\n\n\nclass MLM_VRM(nn.Layer):\n    \"\"\"\n    MLM+VRM, MLM is only used in training.\n    ratio controls the occluded number in a batch.\n    The pipeline of VisionLAN in testing is very concise with only a backbone + sequence modeling(transformer unit) + prediction layer(pp layer).\n    x: input image\n    label_pos: character index\n    training_step: LF or LA process\n    output\n    text_pre: prediction of VRM\n    test_rem: prediction of remaining string in MLM\n    text_mas: prediction of occluded character in MLM\n    mask_c_show: visualization of Mask_c\n    \"\"\"\n\n    def __init__(\n        self, n_layers=3, n_position=256, n_dim=512, max_text_length=25, nclass=37\n    ):\n        super(MLM_VRM, self).__init__()\n        self.MLM = MLM(\n            n_dim=n_dim, n_position=n_position, max_text_length=max_text_length\n        )\n        self.SequenceModeling = Transformer_Encoder(\n            n_layers=n_layers, n_position=n_position\n        )\n        self.Prediction = Prediction(\n            n_dim=n_dim,\n            n_position=n_position,\n            N_max_character=max_text_length\n            + 1,  # N_max_character = 1 eos + 25 characters\n            n_class=nclass,\n        )\n        self.nclass = nclass\n        self.max_text_length = max_text_length\n\n    def forward(self, x, label_pos, training_step, train_mode=False):\n        b, c, h, w = x.shape\n        nT = self.max_text_length\n        x = paddle.transpose(x, perm=[0, 1, 3, 2])\n        x = paddle.reshape(x, [-1, c, h * w])\n        x = paddle.transpose(x, perm=[0, 2, 1])\n        if train_mode:\n            if training_step == \"LF_1\":\n                f_res = 0\n                f_sub = 0\n                x = self.SequenceModeling(x, src_mask=None)\n                text_pre, test_rem, text_mas = self.Prediction(\n                    x, f_res, f_sub, train_mode=True, use_mlm=False\n                )\n                return text_pre, text_pre, text_pre, text_pre\n            elif training_step == \"LF_2\":\n                # MLM\n                f_res, f_sub, mask_c = self.MLM(x, label_pos)\n                x = self.SequenceModeling(x, src_mask=None)\n                text_pre, test_rem, text_mas = self.Prediction(\n                    x, f_res, f_sub, train_mode=True\n                )\n                mask_c_show = trans_1d_2d(mask_c)\n                return text_pre, test_rem, text_mas, mask_c_show\n            elif training_step == \"LA\":\n                # MLM\n                f_res, f_sub, mask_c = self.MLM(x, label_pos)\n                ## use the mask_c (1 for occluded character and 0 for remaining characters) to occlude input\n                ## ratio controls the occluded number in a batch\n                character_mask = paddle.zeros_like(mask_c)\n\n                ratio = b // 2\n                if ratio >= 1:\n                    with paddle.no_grad():\n                        character_mask[0:ratio, :, :] = mask_c[0:ratio, :, :]\n                else:\n                    character_mask = mask_c\n                x = x * (1 - character_mask)\n                # VRM\n                ## transformer unit for VRM\n                x = self.SequenceModeling(x, src_mask=None)\n                ## prediction layer for MLM and VSR\n                text_pre, test_rem, text_mas = self.Prediction(\n                    x, f_res, f_sub, train_mode=True\n                )\n                mask_c_show = trans_1d_2d(mask_c)\n                return text_pre, test_rem, text_mas, mask_c_show\n            else:\n                raise NotImplementedError\n        else:  # VRM is only used in the testing stage\n            f_res = 0\n            f_sub = 0\n            contextual_feature = self.SequenceModeling(x, src_mask=None)\n            text_pre = self.Prediction(\n                contextual_feature, f_res, f_sub, train_mode=False, use_mlm=False\n            )\n            text_pre = paddle.transpose(text_pre, perm=[1, 0, 2])  # (26, b, 37))\n            return text_pre, x\n\n\nclass VLHead(nn.Layer):\n    \"\"\"\n    Architecture of VisionLAN\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels=36,\n        n_layers=3,\n        n_position=256,\n        n_dim=512,\n        max_text_length=25,\n        training_step=\"LA\",\n    ):\n        super(VLHead, self).__init__()\n        self.MLM_VRM = MLM_VRM(\n            n_layers=n_layers,\n            n_position=n_position,\n            n_dim=n_dim,\n            max_text_length=max_text_length,\n            nclass=out_channels + 1,\n        )\n        self.training_step = training_step\n\n    def forward(self, feat, targets=None):\n        if self.training:\n            label_pos = targets[-2]\n            text_pre, test_rem, text_mas, mask_map = self.MLM_VRM(\n                feat, label_pos, self.training_step, train_mode=True\n            )\n            return text_pre, test_rem, text_mas, mask_map\n        else:\n            text_pre, x = self.MLM_VRM(\n                feat, targets, self.training_step, train_mode=False\n            )\n            return text_pre, x\n", "ppocr/modeling/heads/local_graph.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/open-mmlab/mmocr/blob/main/mmocr/models/textdet/modules/local_graph.py\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport paddle\nimport paddle.nn as nn\nfrom ppocr.ext_op import RoIAlignRotated\n\n\ndef normalize_adjacent_matrix(A):\n    assert A.ndim == 2\n    assert A.shape[0] == A.shape[1]\n\n    A = A + np.eye(A.shape[0])\n    d = np.sum(A, axis=0)\n    d = np.clip(d, 0, None)\n    d_inv = np.power(d, -0.5).flatten()\n    d_inv[np.isinf(d_inv)] = 0.0\n    d_inv = np.diag(d_inv)\n    G = A.dot(d_inv).transpose().dot(d_inv)\n    return G\n\n\ndef euclidean_distance_matrix(A, B):\n    \"\"\"Calculate the Euclidean distance matrix.\n\n    Args:\n        A (ndarray): The point sequence.\n        B (ndarray): The point sequence with the same dimensions as A.\n\n    returns:\n        D (ndarray): The Euclidean distance matrix.\n    \"\"\"\n    assert A.ndim == 2\n    assert B.ndim == 2\n    assert A.shape[1] == B.shape[1]\n\n    m = A.shape[0]\n    n = B.shape[0]\n\n    A_dots = (A * A).sum(axis=1).reshape((m, 1)) * np.ones(shape=(1, n))\n    B_dots = (B * B).sum(axis=1) * np.ones(shape=(m, 1))\n    D_squared = A_dots + B_dots - 2 * A.dot(B.T)\n\n    zero_mask = np.less(D_squared, 0.0)\n    D_squared[zero_mask] = 0.0\n    D = np.sqrt(D_squared)\n    return D\n\n\ndef feature_embedding(input_feats, out_feat_len):\n    \"\"\"Embed features. This code was partially adapted from\n    https://github.com/GXYM/DRRG licensed under the MIT license.\n\n    Args:\n        input_feats (ndarray): The input features of shape (N, d), where N is\n            the number of nodes in graph, d is the input feature vector length.\n        out_feat_len (int): The length of output feature vector.\n\n    Returns:\n        embedded_feats (ndarray): The embedded features.\n    \"\"\"\n    assert input_feats.ndim == 2\n    assert isinstance(out_feat_len, int)\n    assert out_feat_len >= input_feats.shape[1]\n\n    num_nodes = input_feats.shape[0]\n    feat_dim = input_feats.shape[1]\n    feat_repeat_times = out_feat_len // feat_dim\n    residue_dim = out_feat_len % feat_dim\n\n    if residue_dim > 0:\n        embed_wave = np.array(\n            [\n                np.power(1000, 2.0 * (j // 2) / feat_repeat_times + 1)\n                for j in range(feat_repeat_times + 1)\n            ]\n        ).reshape((feat_repeat_times + 1, 1, 1))\n        repeat_feats = np.repeat(\n            np.expand_dims(input_feats, axis=0), feat_repeat_times, axis=0\n        )\n        residue_feats = np.hstack(\n            [\n                input_feats[:, 0:residue_dim],\n                np.zeros((num_nodes, feat_dim - residue_dim)),\n            ]\n        )\n        residue_feats = np.expand_dims(residue_feats, axis=0)\n        repeat_feats = np.concatenate([repeat_feats, residue_feats], axis=0)\n        embedded_feats = repeat_feats / embed_wave\n        embedded_feats[:, 0::2] = np.sin(embedded_feats[:, 0::2])\n        embedded_feats[:, 1::2] = np.cos(embedded_feats[:, 1::2])\n        embedded_feats = np.transpose(embedded_feats, (1, 0, 2)).reshape(\n            (num_nodes, -1)\n        )[:, 0:out_feat_len]\n    else:\n        embed_wave = np.array(\n            [\n                np.power(1000, 2.0 * (j // 2) / feat_repeat_times)\n                for j in range(feat_repeat_times)\n            ]\n        ).reshape((feat_repeat_times, 1, 1))\n        repeat_feats = np.repeat(\n            np.expand_dims(input_feats, axis=0), feat_repeat_times, axis=0\n        )\n        embedded_feats = repeat_feats / embed_wave\n        embedded_feats[:, 0::2] = np.sin(embedded_feats[:, 0::2])\n        embedded_feats[:, 1::2] = np.cos(embedded_feats[:, 1::2])\n        embedded_feats = (\n            np.transpose(embedded_feats, (1, 0, 2))\n            .reshape((num_nodes, -1))\n            .astype(np.float32)\n        )\n\n    return embedded_feats\n\n\nclass LocalGraphs:\n    def __init__(\n        self,\n        k_at_hops,\n        num_adjacent_linkages,\n        node_geo_feat_len,\n        pooling_scale,\n        pooling_output_size,\n        local_graph_thr,\n    ):\n        assert len(k_at_hops) == 2\n        assert all(isinstance(n, int) for n in k_at_hops)\n        assert isinstance(num_adjacent_linkages, int)\n        assert isinstance(node_geo_feat_len, int)\n        assert isinstance(pooling_scale, float)\n        assert all(isinstance(n, int) for n in pooling_output_size)\n        assert isinstance(local_graph_thr, float)\n\n        self.k_at_hops = k_at_hops\n        self.num_adjacent_linkages = num_adjacent_linkages\n        self.node_geo_feat_dim = node_geo_feat_len\n        self.pooling = RoIAlignRotated(pooling_output_size, pooling_scale)\n        self.local_graph_thr = local_graph_thr\n\n    def generate_local_graphs(self, sorted_dist_inds, gt_comp_labels):\n        \"\"\"Generate local graphs for GCN to predict which instance a text\n        component belongs to.\n\n        Args:\n            sorted_dist_inds (ndarray): The complete graph node indices, which\n                is sorted according to the Euclidean distance.\n            gt_comp_labels(ndarray): The ground truth labels define the\n                instance to which the text components (nodes in graphs) belong.\n\n        Returns:\n            pivot_local_graphs(list[list[int]]): The list of local graph\n                neighbor indices of pivots.\n            pivot_knns(list[list[int]]): The list of k-nearest neighbor indices\n                of pivots.\n        \"\"\"\n\n        assert sorted_dist_inds.ndim == 2\n        assert (\n            sorted_dist_inds.shape[0]\n            == sorted_dist_inds.shape[1]\n            == gt_comp_labels.shape[0]\n        )\n\n        knn_graph = sorted_dist_inds[:, 1 : self.k_at_hops[0] + 1]\n        pivot_local_graphs = []\n        pivot_knns = []\n        for pivot_ind, knn in enumerate(knn_graph):\n            local_graph_neighbors = set(knn)\n\n            for neighbor_ind in knn:\n                local_graph_neighbors.update(\n                    set(sorted_dist_inds[neighbor_ind, 1 : self.k_at_hops[1] + 1])\n                )\n\n            local_graph_neighbors.discard(pivot_ind)\n            pivot_local_graph = list(local_graph_neighbors)\n            pivot_local_graph.insert(0, pivot_ind)\n            pivot_knn = [pivot_ind] + list(knn)\n\n            if pivot_ind < 1:\n                pivot_local_graphs.append(pivot_local_graph)\n                pivot_knns.append(pivot_knn)\n            else:\n                add_flag = True\n                for graph_ind, added_knn in enumerate(pivot_knns):\n                    added_pivot_ind = added_knn[0]\n                    added_local_graph = pivot_local_graphs[graph_ind]\n\n                    union = len(\n                        set(pivot_local_graph[1:]).union(set(added_local_graph[1:]))\n                    )\n                    intersect = len(\n                        set(pivot_local_graph[1:]).intersection(\n                            set(added_local_graph[1:])\n                        )\n                    )\n                    local_graph_iou = intersect / (union + 1e-8)\n\n                    if (\n                        local_graph_iou > self.local_graph_thr\n                        and pivot_ind in added_knn\n                        and gt_comp_labels[added_pivot_ind] == gt_comp_labels[pivot_ind]\n                        and gt_comp_labels[pivot_ind] != 0\n                    ):\n                        add_flag = False\n                        break\n                if add_flag:\n                    pivot_local_graphs.append(pivot_local_graph)\n                    pivot_knns.append(pivot_knn)\n\n        return pivot_local_graphs, pivot_knns\n\n    def generate_gcn_input(\n        self,\n        node_feat_batch,\n        node_label_batch,\n        local_graph_batch,\n        knn_batch,\n        sorted_dist_ind_batch,\n    ):\n        \"\"\"Generate graph convolution network input data.\n\n        Args:\n            node_feat_batch (List[Tensor]): The batched graph node features.\n            node_label_batch (List[ndarray]): The batched text component\n                labels.\n            local_graph_batch (List[List[list[int]]]): The local graph node\n                indices of image batch.\n            knn_batch (List[List[list[int]]]): The knn graph node indices of\n                image batch.\n            sorted_dist_ind_batch (list[ndarray]): The node indices sorted\n                according to the Euclidean distance.\n\n        Returns:\n            local_graphs_node_feat (Tensor): The node features of graph.\n            adjacent_matrices (Tensor): The adjacent matrices of local graphs.\n            pivots_knn_inds (Tensor): The k-nearest neighbor indices in\n                local graph.\n            gt_linkage (Tensor): The surpervision signal of GCN for linkage\n                prediction.\n        \"\"\"\n        assert isinstance(node_feat_batch, list)\n        assert isinstance(node_label_batch, list)\n        assert isinstance(local_graph_batch, list)\n        assert isinstance(knn_batch, list)\n        assert isinstance(sorted_dist_ind_batch, list)\n\n        num_max_nodes = max(\n            [\n                len(pivot_local_graph)\n                for pivot_local_graphs in local_graph_batch\n                for pivot_local_graph in pivot_local_graphs\n            ]\n        )\n\n        local_graphs_node_feat = []\n        adjacent_matrices = []\n        pivots_knn_inds = []\n        pivots_gt_linkage = []\n\n        for batch_ind, sorted_dist_inds in enumerate(sorted_dist_ind_batch):\n            node_feats = node_feat_batch[batch_ind]\n            pivot_local_graphs = local_graph_batch[batch_ind]\n            pivot_knns = knn_batch[batch_ind]\n            node_labels = node_label_batch[batch_ind]\n\n            for graph_ind, pivot_knn in enumerate(pivot_knns):\n                pivot_local_graph = pivot_local_graphs[graph_ind]\n                num_nodes = len(pivot_local_graph)\n                pivot_ind = pivot_local_graph[0]\n                node2ind_map = {j: i for i, j in enumerate(pivot_local_graph)}\n\n                knn_inds = paddle.to_tensor([node2ind_map[i] for i in pivot_knn[1:]])\n                pivot_feats = node_feats[pivot_ind]\n                normalized_feats = (\n                    node_feats[paddle.to_tensor(pivot_local_graph)] - pivot_feats\n                )\n\n                adjacent_matrix = np.zeros((num_nodes, num_nodes), dtype=np.float32)\n                for node in pivot_local_graph:\n                    neighbors = sorted_dist_inds[\n                        node, 1 : self.num_adjacent_linkages + 1\n                    ]\n                    for neighbor in neighbors:\n                        if neighbor in pivot_local_graph:\n                            adjacent_matrix[\n                                node2ind_map[node], node2ind_map[neighbor]\n                            ] = 1\n                            adjacent_matrix[\n                                node2ind_map[neighbor], node2ind_map[node]\n                            ] = 1\n\n                adjacent_matrix = normalize_adjacent_matrix(adjacent_matrix)\n                pad_adjacent_matrix = paddle.zeros((num_max_nodes, num_max_nodes))\n                pad_adjacent_matrix[:num_nodes, :num_nodes] = paddle.cast(\n                    paddle.to_tensor(adjacent_matrix), \"float32\"\n                )\n\n                pad_normalized_feats = paddle.concat(\n                    [\n                        normalized_feats,\n                        paddle.zeros(\n                            (num_max_nodes - num_nodes, normalized_feats.shape[1])\n                        ),\n                    ],\n                    axis=0,\n                )\n                local_graph_labels = node_labels[pivot_local_graph]\n                knn_labels = local_graph_labels[knn_inds.numpy()]\n                link_labels = (\n                    (node_labels[pivot_ind] == knn_labels)\n                    & (node_labels[pivot_ind] > 0)\n                ).astype(np.int64)\n                link_labels = paddle.to_tensor(link_labels)\n\n                local_graphs_node_feat.append(pad_normalized_feats)\n                adjacent_matrices.append(pad_adjacent_matrix)\n                pivots_knn_inds.append(knn_inds)\n                pivots_gt_linkage.append(link_labels)\n\n        local_graphs_node_feat = paddle.stack(local_graphs_node_feat, 0)\n        adjacent_matrices = paddle.stack(adjacent_matrices, 0)\n        pivots_knn_inds = paddle.stack(pivots_knn_inds, 0)\n        pivots_gt_linkage = paddle.stack(pivots_gt_linkage, 0)\n\n        return (\n            local_graphs_node_feat,\n            adjacent_matrices,\n            pivots_knn_inds,\n            pivots_gt_linkage,\n        )\n\n    def __call__(self, feat_maps, comp_attribs):\n        \"\"\"Generate local graphs as GCN input.\n\n        Args:\n            feat_maps (Tensor): The feature maps to extract the content\n                features of text components.\n            comp_attribs (ndarray): The text component attributes.\n\n        Returns:\n            local_graphs_node_feat (Tensor): The node features of graph.\n            adjacent_matrices (Tensor): The adjacent matrices of local graphs.\n            pivots_knn_inds (Tensor): The k-nearest neighbor indices in local\n                graph.\n            gt_linkage (Tensor): The surpervision signal of GCN for linkage\n                prediction.\n        \"\"\"\n\n        assert isinstance(feat_maps, paddle.Tensor)\n        assert comp_attribs.ndim == 3\n        assert comp_attribs.shape[2] == 8\n\n        sorted_dist_inds_batch = []\n        local_graph_batch = []\n        knn_batch = []\n        node_feat_batch = []\n        node_label_batch = []\n\n        for batch_ind in range(comp_attribs.shape[0]):\n            num_comps = int(comp_attribs[batch_ind, 0, 0])\n            comp_geo_attribs = comp_attribs[batch_ind, :num_comps, 1:7]\n            node_labels = comp_attribs[batch_ind, :num_comps, 7].astype(np.int32)\n\n            comp_centers = comp_geo_attribs[:, 0:2]\n            distance_matrix = euclidean_distance_matrix(comp_centers, comp_centers)\n\n            batch_id = (\n                np.zeros((comp_geo_attribs.shape[0], 1), dtype=np.float32) * batch_ind\n            )\n            comp_geo_attribs[:, -2] = np.clip(comp_geo_attribs[:, -2], -1, 1)\n            angle = np.arccos(comp_geo_attribs[:, -2]) * np.sign(\n                comp_geo_attribs[:, -1]\n            )\n            angle = angle.reshape((-1, 1))\n            rotated_rois = np.hstack([batch_id, comp_geo_attribs[:, :-2], angle])\n            rois = paddle.to_tensor(rotated_rois)\n            content_feats = self.pooling(feat_maps[batch_ind].unsqueeze(0), rois)\n\n            content_feats = content_feats.reshape([content_feats.shape[0], -1])\n            geo_feats = feature_embedding(comp_geo_attribs, self.node_geo_feat_dim)\n            geo_feats = paddle.to_tensor(geo_feats)\n            node_feats = paddle.concat([content_feats, geo_feats], axis=-1)\n\n            sorted_dist_inds = np.argsort(distance_matrix, axis=1)\n            pivot_local_graphs, pivot_knns = self.generate_local_graphs(\n                sorted_dist_inds, node_labels\n            )\n\n            node_feat_batch.append(node_feats)\n            node_label_batch.append(node_labels)\n            local_graph_batch.append(pivot_local_graphs)\n            knn_batch.append(pivot_knns)\n            sorted_dist_inds_batch.append(sorted_dist_inds)\n\n        (node_feats, adjacent_matrices, knn_inds, gt_linkage) = self.generate_gcn_input(\n            node_feat_batch,\n            node_label_batch,\n            local_graph_batch,\n            knn_batch,\n            sorted_dist_inds_batch,\n        )\n\n        return node_feats, adjacent_matrices, knn_inds, gt_linkage\n", "ppocr/modeling/heads/det_sast_head.py": "# copyright (c) 2019 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport paddle\nfrom paddle import nn\nimport paddle.nn.functional as F\nfrom paddle import ParamAttr\n\n\nclass ConvBNLayer(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride,\n        groups=1,\n        if_act=True,\n        act=None,\n        name=None,\n    ):\n        super(ConvBNLayer, self).__init__()\n        self.if_act = if_act\n        self.act = act\n        self.conv = nn.Conv2D(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=(kernel_size - 1) // 2,\n            groups=groups,\n            weight_attr=ParamAttr(name=name + \"_weights\"),\n            bias_attr=False,\n        )\n\n        self.bn = nn.BatchNorm(\n            num_channels=out_channels,\n            act=act,\n            param_attr=ParamAttr(name=\"bn_\" + name + \"_scale\"),\n            bias_attr=ParamAttr(name=\"bn_\" + name + \"_offset\"),\n            moving_mean_name=\"bn_\" + name + \"_mean\",\n            moving_variance_name=\"bn_\" + name + \"_variance\",\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return x\n\n\nclass SAST_Header1(nn.Layer):\n    def __init__(self, in_channels, **kwargs):\n        super(SAST_Header1, self).__init__()\n        out_channels = [64, 64, 128]\n        self.score_conv = nn.Sequential(\n            ConvBNLayer(\n                in_channels, out_channels[0], 1, 1, act=\"relu\", name=\"f_score1\"\n            ),\n            ConvBNLayer(\n                out_channels[0], out_channels[1], 3, 1, act=\"relu\", name=\"f_score2\"\n            ),\n            ConvBNLayer(\n                out_channels[1], out_channels[2], 1, 1, act=\"relu\", name=\"f_score3\"\n            ),\n            ConvBNLayer(out_channels[2], 1, 3, 1, act=None, name=\"f_score4\"),\n        )\n        self.border_conv = nn.Sequential(\n            ConvBNLayer(\n                in_channels, out_channels[0], 1, 1, act=\"relu\", name=\"f_border1\"\n            ),\n            ConvBNLayer(\n                out_channels[0], out_channels[1], 3, 1, act=\"relu\", name=\"f_border2\"\n            ),\n            ConvBNLayer(\n                out_channels[1], out_channels[2], 1, 1, act=\"relu\", name=\"f_border3\"\n            ),\n            ConvBNLayer(out_channels[2], 4, 3, 1, act=None, name=\"f_border4\"),\n        )\n\n    def forward(self, x):\n        f_score = self.score_conv(x)\n        f_score = F.sigmoid(f_score)\n        f_border = self.border_conv(x)\n        return f_score, f_border\n\n\nclass SAST_Header2(nn.Layer):\n    def __init__(self, in_channels, **kwargs):\n        super(SAST_Header2, self).__init__()\n        out_channels = [64, 64, 128]\n        self.tvo_conv = nn.Sequential(\n            ConvBNLayer(in_channels, out_channels[0], 1, 1, act=\"relu\", name=\"f_tvo1\"),\n            ConvBNLayer(\n                out_channels[0], out_channels[1], 3, 1, act=\"relu\", name=\"f_tvo2\"\n            ),\n            ConvBNLayer(\n                out_channels[1], out_channels[2], 1, 1, act=\"relu\", name=\"f_tvo3\"\n            ),\n            ConvBNLayer(out_channels[2], 8, 3, 1, act=None, name=\"f_tvo4\"),\n        )\n        self.tco_conv = nn.Sequential(\n            ConvBNLayer(in_channels, out_channels[0], 1, 1, act=\"relu\", name=\"f_tco1\"),\n            ConvBNLayer(\n                out_channels[0], out_channels[1], 3, 1, act=\"relu\", name=\"f_tco2\"\n            ),\n            ConvBNLayer(\n                out_channels[1], out_channels[2], 1, 1, act=\"relu\", name=\"f_tco3\"\n            ),\n            ConvBNLayer(out_channels[2], 2, 3, 1, act=None, name=\"f_tco4\"),\n        )\n\n    def forward(self, x):\n        f_tvo = self.tvo_conv(x)\n        f_tco = self.tco_conv(x)\n        return f_tvo, f_tco\n\n\nclass SASTHead(nn.Layer):\n    \"\"\" \"\"\"\n\n    def __init__(self, in_channels, **kwargs):\n        super(SASTHead, self).__init__()\n\n        self.head1 = SAST_Header1(in_channels)\n        self.head2 = SAST_Header2(in_channels)\n\n    def forward(self, x, targets=None):\n        f_score, f_border = self.head1(x)\n        f_tvo, f_tco = self.head2(x)\n\n        predicts = {}\n        predicts[\"f_score\"] = f_score\n        predicts[\"f_border\"] = f_border\n        predicts[\"f_tvo\"] = f_tvo\n        predicts[\"f_tco\"] = f_tco\n        return predicts\n", "ppocr/modeling/heads/rec_can_head.py": "# copyright (c) 2019 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/LBH1024/CAN/models/can.py\nhttps://github.com/LBH1024/CAN/models/counting.py\nhttps://github.com/LBH1024/CAN/models/decoder.py\nhttps://github.com/LBH1024/CAN/models/attention.py\n\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle.nn as nn\nimport paddle\nimport math\n\n\"\"\"\nCounting Module\n\"\"\"\n\n\nclass ChannelAtt(nn.Layer):\n    def __init__(self, channel, reduction):\n        super(ChannelAtt, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2D(1)\n\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction),\n            nn.ReLU(),\n            nn.Linear(channel // reduction, channel),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.shape\n        y = paddle.reshape(self.avg_pool(x), [b, c])\n        y = paddle.reshape(self.fc(y), [b, c, 1, 1])\n        return x * y\n\n\nclass CountingDecoder(nn.Layer):\n    def __init__(self, in_channel, out_channel, kernel_size):\n        super(CountingDecoder, self).__init__()\n        self.in_channel = in_channel\n        self.out_channel = out_channel\n\n        self.trans_layer = nn.Sequential(\n            nn.Conv2D(\n                self.in_channel,\n                512,\n                kernel_size=kernel_size,\n                padding=kernel_size // 2,\n                bias_attr=False,\n            ),\n            nn.BatchNorm2D(512),\n        )\n\n        self.channel_att = ChannelAtt(512, 16)\n\n        self.pred_layer = nn.Sequential(\n            nn.Conv2D(512, self.out_channel, kernel_size=1, bias_attr=False),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x, mask):\n        b, _, h, w = x.shape\n        x = self.trans_layer(x)\n        x = self.channel_att(x)\n        x = self.pred_layer(x)\n\n        if mask is not None:\n            x = x * mask\n        x = paddle.reshape(x, [b, self.out_channel, -1])\n        x1 = paddle.sum(x, axis=-1)\n\n        return x1, paddle.reshape(x, [b, self.out_channel, h, w])\n\n\n\"\"\"\nAttention Decoder\n\"\"\"\n\n\nclass PositionEmbeddingSine(nn.Layer):\n    def __init__(\n        self, num_pos_feats=64, temperature=10000, normalize=False, scale=None\n    ):\n        super().__init__()\n        self.num_pos_feats = num_pos_feats\n        self.temperature = temperature\n        self.normalize = normalize\n        if scale is not None and normalize is False:\n            raise ValueError(\"normalize should be True if scale is passed\")\n        if scale is None:\n            scale = 2 * math.pi\n        self.scale = scale\n\n    def forward(self, x, mask):\n        y_embed = paddle.cumsum(mask, 1, dtype=\"float32\")\n        x_embed = paddle.cumsum(mask, 2, dtype=\"float32\")\n\n        if self.normalize:\n            eps = 1e-6\n            y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale\n            x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale\n        dim_t = paddle.arange(self.num_pos_feats, dtype=\"float32\")\n        dim_d = paddle.expand(paddle.to_tensor(2), dim_t.shape)\n        dim_t = self.temperature ** (\n            2 * (dim_t / dim_d).astype(\"int64\") / self.num_pos_feats\n        )\n\n        pos_x = paddle.unsqueeze(x_embed, [3]) / dim_t\n        pos_y = paddle.unsqueeze(y_embed, [3]) / dim_t\n\n        pos_x = paddle.flatten(\n            paddle.stack(\n                [paddle.sin(pos_x[:, :, :, 0::2]), paddle.cos(pos_x[:, :, :, 1::2])],\n                axis=4,\n            ),\n            3,\n        )\n        pos_y = paddle.flatten(\n            paddle.stack(\n                [paddle.sin(pos_y[:, :, :, 0::2]), paddle.cos(pos_y[:, :, :, 1::2])],\n                axis=4,\n            ),\n            3,\n        )\n\n        pos = paddle.transpose(paddle.concat([pos_y, pos_x], axis=3), [0, 3, 1, 2])\n\n        return pos\n\n\nclass AttDecoder(nn.Layer):\n    def __init__(\n        self,\n        ratio,\n        is_train,\n        input_size,\n        hidden_size,\n        encoder_out_channel,\n        dropout,\n        dropout_ratio,\n        word_num,\n        counting_decoder_out_channel,\n        attention,\n    ):\n        super(AttDecoder, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.out_channel = encoder_out_channel\n        self.attention_dim = attention[\"attention_dim\"]\n        self.dropout_prob = dropout\n        self.ratio = ratio\n        self.word_num = word_num\n\n        self.counting_num = counting_decoder_out_channel\n        self.is_train = is_train\n\n        self.init_weight = nn.Linear(self.out_channel, self.hidden_size)\n        self.embedding = nn.Embedding(self.word_num, self.input_size)\n        self.word_input_gru = nn.GRUCell(self.input_size, self.hidden_size)\n        self.word_attention = Attention(hidden_size, attention[\"attention_dim\"])\n\n        self.encoder_feature_conv = nn.Conv2D(\n            self.out_channel,\n            self.attention_dim,\n            kernel_size=attention[\"word_conv_kernel\"],\n            padding=attention[\"word_conv_kernel\"] // 2,\n        )\n\n        self.word_state_weight = nn.Linear(self.hidden_size, self.hidden_size)\n        self.word_embedding_weight = nn.Linear(self.input_size, self.hidden_size)\n        self.word_context_weight = nn.Linear(self.out_channel, self.hidden_size)\n        self.counting_context_weight = nn.Linear(self.counting_num, self.hidden_size)\n        self.word_convert = nn.Linear(self.hidden_size, self.word_num)\n\n        if dropout:\n            self.dropout = nn.Dropout(dropout_ratio)\n\n    def forward(self, cnn_features, labels, counting_preds, images_mask):\n        if self.is_train:\n            _, num_steps = labels.shape\n        else:\n            num_steps = 36\n\n        batch_size, _, height, width = cnn_features.shape\n        images_mask = images_mask[:, :, :: self.ratio, :: self.ratio]\n\n        word_probs = paddle.zeros((batch_size, num_steps, self.word_num))\n        word_alpha_sum = paddle.zeros((batch_size, 1, height, width))\n\n        hidden = self.init_hidden(cnn_features, images_mask)\n        counting_context_weighted = self.counting_context_weight(counting_preds)\n        cnn_features_trans = self.encoder_feature_conv(cnn_features)\n\n        position_embedding = PositionEmbeddingSine(256, normalize=True)\n        pos = position_embedding(cnn_features_trans, images_mask[:, 0, :, :])\n\n        cnn_features_trans = cnn_features_trans + pos\n\n        word = paddle.ones([batch_size, 1], dtype=\"int64\")  # init word as sos\n        word = word.squeeze(axis=1)\n        for i in range(num_steps):\n            word_embedding = self.embedding(word)\n            _, hidden = self.word_input_gru(word_embedding, hidden)\n            word_context_vec, _, word_alpha_sum = self.word_attention(\n                cnn_features, cnn_features_trans, hidden, word_alpha_sum, images_mask\n            )\n\n            current_state = self.word_state_weight(hidden)\n            word_weighted_embedding = self.word_embedding_weight(word_embedding)\n            word_context_weighted = self.word_context_weight(word_context_vec)\n\n            if self.dropout_prob:\n                word_out_state = self.dropout(\n                    current_state\n                    + word_weighted_embedding\n                    + word_context_weighted\n                    + counting_context_weighted\n                )\n            else:\n                word_out_state = (\n                    current_state\n                    + word_weighted_embedding\n                    + word_context_weighted\n                    + counting_context_weighted\n                )\n\n            word_prob = self.word_convert(word_out_state)\n            word_probs[:, i] = word_prob\n\n            if self.is_train:\n                word = labels[:, i]\n            else:\n                word = word_prob.argmax(1)\n                word = paddle.multiply(\n                    word, labels[:, i]\n                )  # labels are oneslike tensor in infer/predict mode\n\n        return word_probs\n\n    def init_hidden(self, features, feature_mask):\n        average = paddle.sum(\n            paddle.sum(features * feature_mask, axis=-1), axis=-1\n        ) / paddle.sum((paddle.sum(feature_mask, axis=-1)), axis=-1)\n        average = self.init_weight(average)\n        return paddle.tanh(average)\n\n\n\"\"\"\nAttention Module\n\"\"\"\n\n\nclass Attention(nn.Layer):\n    def __init__(self, hidden_size, attention_dim):\n        super(Attention, self).__init__()\n        self.hidden = hidden_size\n        self.attention_dim = attention_dim\n        self.hidden_weight = nn.Linear(self.hidden, self.attention_dim)\n        self.attention_conv = nn.Conv2D(\n            1, 512, kernel_size=11, padding=5, bias_attr=False\n        )\n        self.attention_weight = nn.Linear(512, self.attention_dim, bias_attr=False)\n        self.alpha_convert = nn.Linear(self.attention_dim, 1)\n\n    def forward(\n        self, cnn_features, cnn_features_trans, hidden, alpha_sum, image_mask=None\n    ):\n        query = self.hidden_weight(hidden)\n        alpha_sum_trans = self.attention_conv(alpha_sum)\n        coverage_alpha = self.attention_weight(\n            paddle.transpose(alpha_sum_trans, [0, 2, 3, 1])\n        )\n        alpha_score = paddle.tanh(\n            paddle.unsqueeze(query, [1, 2])\n            + coverage_alpha\n            + paddle.transpose(cnn_features_trans, [0, 2, 3, 1])\n        )\n        energy = self.alpha_convert(alpha_score)\n        energy = energy - energy.max()\n        energy_exp = paddle.exp(paddle.squeeze(energy, -1))\n\n        if image_mask is not None:\n            energy_exp = energy_exp * paddle.squeeze(image_mask, 1)\n        alpha = energy_exp / (\n            paddle.unsqueeze(paddle.sum(paddle.sum(energy_exp, -1), -1), [1, 2]) + 1e-10\n        )\n        alpha_sum = paddle.unsqueeze(alpha, 1) + alpha_sum\n        context_vector = paddle.sum(\n            paddle.sum((paddle.unsqueeze(alpha, 1) * cnn_features), -1), -1\n        )\n\n        return context_vector, alpha, alpha_sum\n\n\nclass CANHead(nn.Layer):\n    def __init__(self, in_channel, out_channel, ratio, attdecoder, **kwargs):\n        super(CANHead, self).__init__()\n\n        self.in_channel = in_channel\n        self.out_channel = out_channel\n\n        self.counting_decoder1 = CountingDecoder(\n            self.in_channel, self.out_channel, 3\n        )  # mscm\n        self.counting_decoder2 = CountingDecoder(self.in_channel, self.out_channel, 5)\n\n        self.decoder = AttDecoder(ratio, **attdecoder)\n\n        self.ratio = ratio\n\n    def forward(self, inputs, targets=None):\n        cnn_features, images_mask, labels = inputs\n\n        counting_mask = images_mask[:, :, :: self.ratio, :: self.ratio]\n        counting_preds1, _ = self.counting_decoder1(cnn_features, counting_mask)\n        counting_preds2, _ = self.counting_decoder2(cnn_features, counting_mask)\n        counting_preds = (counting_preds1 + counting_preds2) / 2\n\n        word_probs = self.decoder(cnn_features, labels, counting_preds, images_mask)\n        return word_probs, counting_preds, counting_preds1, counting_preds2\n", "ppocr/modeling/heads/self_attention.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\n\nimport paddle\nfrom paddle import ParamAttr, nn\nfrom paddle import nn, ParamAttr\nfrom paddle.nn import functional as F\nimport numpy as np\n\ngradient_clip = 10\n\n\nclass WrapEncoderForFeature(nn.Layer):\n    def __init__(\n        self,\n        src_vocab_size,\n        max_length,\n        n_layer,\n        n_head,\n        d_key,\n        d_value,\n        d_model,\n        d_inner_hid,\n        prepostprocess_dropout,\n        attention_dropout,\n        relu_dropout,\n        preprocess_cmd,\n        postprocess_cmd,\n        weight_sharing,\n        bos_idx=0,\n    ):\n        super(WrapEncoderForFeature, self).__init__()\n\n        self.prepare_encoder = PrepareEncoder(\n            src_vocab_size,\n            d_model,\n            max_length,\n            prepostprocess_dropout,\n            bos_idx=bos_idx,\n            word_emb_param_name=\"src_word_emb_table\",\n        )\n        self.encoder = Encoder(\n            n_layer,\n            n_head,\n            d_key,\n            d_value,\n            d_model,\n            d_inner_hid,\n            prepostprocess_dropout,\n            attention_dropout,\n            relu_dropout,\n            preprocess_cmd,\n            postprocess_cmd,\n        )\n\n    def forward(self, enc_inputs):\n        conv_features, src_pos, src_slf_attn_bias = enc_inputs\n        enc_input = self.prepare_encoder(conv_features, src_pos)\n        enc_output = self.encoder(enc_input, src_slf_attn_bias)\n        return enc_output\n\n\nclass WrapEncoder(nn.Layer):\n    \"\"\"\n    embedder + encoder\n    \"\"\"\n\n    def __init__(\n        self,\n        src_vocab_size,\n        max_length,\n        n_layer,\n        n_head,\n        d_key,\n        d_value,\n        d_model,\n        d_inner_hid,\n        prepostprocess_dropout,\n        attention_dropout,\n        relu_dropout,\n        preprocess_cmd,\n        postprocess_cmd,\n        weight_sharing,\n        bos_idx=0,\n    ):\n        super(WrapEncoder, self).__init__()\n\n        self.prepare_decoder = PrepareDecoder(\n            src_vocab_size, d_model, max_length, prepostprocess_dropout, bos_idx=bos_idx\n        )\n        self.encoder = Encoder(\n            n_layer,\n            n_head,\n            d_key,\n            d_value,\n            d_model,\n            d_inner_hid,\n            prepostprocess_dropout,\n            attention_dropout,\n            relu_dropout,\n            preprocess_cmd,\n            postprocess_cmd,\n        )\n\n    def forward(self, enc_inputs):\n        src_word, src_pos, src_slf_attn_bias = enc_inputs\n        enc_input = self.prepare_decoder(src_word, src_pos)\n        enc_output = self.encoder(enc_input, src_slf_attn_bias)\n        return enc_output\n\n\nclass Encoder(nn.Layer):\n    \"\"\"\n    encoder\n    \"\"\"\n\n    def __init__(\n        self,\n        n_layer,\n        n_head,\n        d_key,\n        d_value,\n        d_model,\n        d_inner_hid,\n        prepostprocess_dropout,\n        attention_dropout,\n        relu_dropout,\n        preprocess_cmd=\"n\",\n        postprocess_cmd=\"da\",\n    ):\n        super(Encoder, self).__init__()\n\n        self.encoder_layers = list()\n        for i in range(n_layer):\n            self.encoder_layers.append(\n                self.add_sublayer(\n                    \"layer_%d\" % i,\n                    EncoderLayer(\n                        n_head,\n                        d_key,\n                        d_value,\n                        d_model,\n                        d_inner_hid,\n                        prepostprocess_dropout,\n                        attention_dropout,\n                        relu_dropout,\n                        preprocess_cmd,\n                        postprocess_cmd,\n                    ),\n                )\n            )\n        self.processer = PrePostProcessLayer(\n            preprocess_cmd, d_model, prepostprocess_dropout\n        )\n\n    def forward(self, enc_input, attn_bias):\n        for encoder_layer in self.encoder_layers:\n            enc_output = encoder_layer(enc_input, attn_bias)\n            enc_input = enc_output\n        enc_output = self.processer(enc_output)\n        return enc_output\n\n\nclass EncoderLayer(nn.Layer):\n    \"\"\"\n    EncoderLayer\n    \"\"\"\n\n    def __init__(\n        self,\n        n_head,\n        d_key,\n        d_value,\n        d_model,\n        d_inner_hid,\n        prepostprocess_dropout,\n        attention_dropout,\n        relu_dropout,\n        preprocess_cmd=\"n\",\n        postprocess_cmd=\"da\",\n    ):\n        super(EncoderLayer, self).__init__()\n        self.preprocesser1 = PrePostProcessLayer(\n            preprocess_cmd, d_model, prepostprocess_dropout\n        )\n        self.self_attn = MultiHeadAttention(\n            d_key, d_value, d_model, n_head, attention_dropout\n        )\n        self.postprocesser1 = PrePostProcessLayer(\n            postprocess_cmd, d_model, prepostprocess_dropout\n        )\n\n        self.preprocesser2 = PrePostProcessLayer(\n            preprocess_cmd, d_model, prepostprocess_dropout\n        )\n        self.ffn = FFN(d_inner_hid, d_model, relu_dropout)\n        self.postprocesser2 = PrePostProcessLayer(\n            postprocess_cmd, d_model, prepostprocess_dropout\n        )\n\n    def forward(self, enc_input, attn_bias):\n        attn_output = self.self_attn(\n            self.preprocesser1(enc_input), None, None, attn_bias\n        )\n        attn_output = self.postprocesser1(attn_output, enc_input)\n        ffn_output = self.ffn(self.preprocesser2(attn_output))\n        ffn_output = self.postprocesser2(ffn_output, attn_output)\n        return ffn_output\n\n\nclass MultiHeadAttention(nn.Layer):\n    \"\"\"\n    Multi-Head Attention\n    \"\"\"\n\n    def __init__(self, d_key, d_value, d_model, n_head=1, dropout_rate=0.0):\n        super(MultiHeadAttention, self).__init__()\n        self.n_head = n_head\n        self.d_key = d_key\n        self.d_value = d_value\n        self.d_model = d_model\n        self.dropout_rate = dropout_rate\n        self.q_fc = paddle.nn.Linear(\n            in_features=d_model, out_features=d_key * n_head, bias_attr=False\n        )\n        self.k_fc = paddle.nn.Linear(\n            in_features=d_model, out_features=d_key * n_head, bias_attr=False\n        )\n        self.v_fc = paddle.nn.Linear(\n            in_features=d_model, out_features=d_value * n_head, bias_attr=False\n        )\n        self.proj_fc = paddle.nn.Linear(\n            in_features=d_value * n_head, out_features=d_model, bias_attr=False\n        )\n\n    def _prepare_qkv(self, queries, keys, values, cache=None):\n        if keys is None:  # self-attention\n            keys, values = queries, queries\n            static_kv = False\n        else:  # cross-attention\n            static_kv = True\n\n        q = self.q_fc(queries)\n        q = paddle.reshape(x=q, shape=[0, 0, self.n_head, self.d_key])\n        q = paddle.transpose(x=q, perm=[0, 2, 1, 3])\n\n        if cache is not None and static_kv and \"static_k\" in cache:\n            # for encoder-decoder attention in inference and has cached\n            k = cache[\"static_k\"]\n            v = cache[\"static_v\"]\n        else:\n            k = self.k_fc(keys)\n            v = self.v_fc(values)\n            k = paddle.reshape(x=k, shape=[0, 0, self.n_head, self.d_key])\n            k = paddle.transpose(x=k, perm=[0, 2, 1, 3])\n            v = paddle.reshape(x=v, shape=[0, 0, self.n_head, self.d_value])\n            v = paddle.transpose(x=v, perm=[0, 2, 1, 3])\n\n        if cache is not None:\n            if static_kv and not \"static_k\" in cache:\n                # for encoder-decoder attention in inference and has not cached\n                cache[\"static_k\"], cache[\"static_v\"] = k, v\n            elif not static_kv:\n                # for decoder self-attention in inference\n                cache_k, cache_v = cache[\"k\"], cache[\"v\"]\n                k = paddle.concat([cache_k, k], axis=2)\n                v = paddle.concat([cache_v, v], axis=2)\n                cache[\"k\"], cache[\"v\"] = k, v\n\n        return q, k, v\n\n    def forward(self, queries, keys, values, attn_bias, cache=None):\n        # compute q ,k ,v\n        keys = queries if keys is None else keys\n        values = keys if values is None else values\n        q, k, v = self._prepare_qkv(queries, keys, values, cache)\n\n        # scale dot product attention\n        product = paddle.matmul(x=q, y=k, transpose_y=True)\n        product = product * self.d_model**-0.5\n        if attn_bias is not None:\n            product += attn_bias.astype(product.dtype)\n        weights = F.softmax(product)\n        if self.dropout_rate:\n            weights = F.dropout(weights, p=self.dropout_rate, mode=\"downscale_in_infer\")\n        out = paddle.matmul(weights, v)\n\n        # combine heads\n        out = paddle.transpose(out, perm=[0, 2, 1, 3])\n        out = paddle.reshape(x=out, shape=[0, 0, out.shape[2] * out.shape[3]])\n\n        # project to output\n        out = self.proj_fc(out)\n\n        return out\n\n\nclass PrePostProcessLayer(nn.Layer):\n    \"\"\"\n    PrePostProcessLayer\n    \"\"\"\n\n    def __init__(self, process_cmd, d_model, dropout_rate):\n        super(PrePostProcessLayer, self).__init__()\n        self.process_cmd = process_cmd\n        self.functors = []\n        for cmd in self.process_cmd:\n            if cmd == \"a\":  # add residual connection\n                self.functors.append(lambda x, y: x + y if y is not None else x)\n            elif cmd == \"n\":  # add layer normalization\n                self.functors.append(\n                    self.add_sublayer(\n                        \"layer_norm_%d\" % len(self.sublayers()),\n                        paddle.nn.LayerNorm(\n                            normalized_shape=d_model,\n                            weight_attr=paddle.ParamAttr(\n                                initializer=paddle.nn.initializer.Constant(1.0)\n                            ),\n                            bias_attr=paddle.ParamAttr(\n                                initializer=paddle.nn.initializer.Constant(0.0)\n                            ),\n                        ),\n                    )\n                )\n            elif cmd == \"d\":  # add dropout\n                self.functors.append(\n                    lambda x: (\n                        F.dropout(x, p=dropout_rate, mode=\"downscale_in_infer\")\n                        if dropout_rate\n                        else x\n                    )\n                )\n\n    def forward(self, x, residual=None):\n        for i, cmd in enumerate(self.process_cmd):\n            if cmd == \"a\":\n                x = self.functors[i](x, residual)\n            else:\n                x = self.functors[i](x)\n        return x\n\n\nclass PrepareEncoder(nn.Layer):\n    def __init__(\n        self,\n        src_vocab_size,\n        src_emb_dim,\n        src_max_len,\n        dropout_rate=0,\n        bos_idx=0,\n        word_emb_param_name=None,\n        pos_enc_param_name=None,\n    ):\n        super(PrepareEncoder, self).__init__()\n        self.src_emb_dim = src_emb_dim\n        self.src_max_len = src_max_len\n        self.emb = paddle.nn.Embedding(\n            num_embeddings=self.src_max_len, embedding_dim=self.src_emb_dim\n        )\n        self.dropout_rate = dropout_rate\n\n    def forward(self, src_word, src_pos):\n        src_word_emb = src_word\n        src_word_emb = paddle.cast(src_word_emb, \"float32\")\n        src_word_emb = paddle.scale(x=src_word_emb, scale=self.src_emb_dim**0.5)\n        src_pos = paddle.squeeze(src_pos, axis=-1)\n        src_pos_enc = self.emb(src_pos)\n        src_pos_enc.stop_gradient = True\n        enc_input = src_word_emb + src_pos_enc\n        if self.dropout_rate:\n            out = F.dropout(x=enc_input, p=self.dropout_rate, mode=\"downscale_in_infer\")\n        else:\n            out = enc_input\n        return out\n\n\nclass PrepareDecoder(nn.Layer):\n    def __init__(\n        self,\n        src_vocab_size,\n        src_emb_dim,\n        src_max_len,\n        dropout_rate=0,\n        bos_idx=0,\n        word_emb_param_name=None,\n        pos_enc_param_name=None,\n    ):\n        super(PrepareDecoder, self).__init__()\n        self.src_emb_dim = src_emb_dim\n        \"\"\"\n        self.emb0 = Embedding(num_embeddings=src_vocab_size,\n                              embedding_dim=src_emb_dim)\n        \"\"\"\n        self.emb0 = paddle.nn.Embedding(\n            num_embeddings=src_vocab_size,\n            embedding_dim=self.src_emb_dim,\n            padding_idx=bos_idx,\n            weight_attr=paddle.ParamAttr(\n                name=word_emb_param_name,\n                initializer=nn.initializer.Normal(0.0, src_emb_dim**-0.5),\n            ),\n        )\n        self.emb1 = paddle.nn.Embedding(\n            num_embeddings=src_max_len,\n            embedding_dim=self.src_emb_dim,\n            weight_attr=paddle.ParamAttr(name=pos_enc_param_name),\n        )\n        self.dropout_rate = dropout_rate\n\n    def forward(self, src_word, src_pos):\n        src_word = paddle.cast(src_word, \"int64\")\n        src_word = paddle.squeeze(src_word, axis=-1)\n        src_word_emb = self.emb0(src_word)\n        src_word_emb = paddle.scale(x=src_word_emb, scale=self.src_emb_dim**0.5)\n        src_pos = paddle.squeeze(src_pos, axis=-1)\n        src_pos_enc = self.emb1(src_pos)\n        src_pos_enc.stop_gradient = True\n        enc_input = src_word_emb + src_pos_enc\n        if self.dropout_rate:\n            out = F.dropout(x=enc_input, p=self.dropout_rate, mode=\"downscale_in_infer\")\n        else:\n            out = enc_input\n        return out\n\n\nclass FFN(nn.Layer):\n    \"\"\"\n    Feed-Forward Network\n    \"\"\"\n\n    def __init__(self, d_inner_hid, d_model, dropout_rate):\n        super(FFN, self).__init__()\n        self.dropout_rate = dropout_rate\n        self.fc1 = paddle.nn.Linear(in_features=d_model, out_features=d_inner_hid)\n        self.fc2 = paddle.nn.Linear(in_features=d_inner_hid, out_features=d_model)\n\n    def forward(self, x):\n        hidden = self.fc1(x)\n        hidden = F.relu(hidden)\n        if self.dropout_rate:\n            hidden = F.dropout(hidden, p=self.dropout_rate, mode=\"downscale_in_infer\")\n        out = self.fc2(hidden)\n        return out\n", "ppocr/modeling/heads/det_drrg_head.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/open-mmlab/mmocr/blob/main/mmocr/models/textdet/dense_heads/drrg_head.py\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport warnings\nimport cv2\nimport numpy as np\nimport paddle\nimport paddle.nn as nn\nimport paddle.nn.functional as F\nfrom .gcn import GCN\nfrom .local_graph import LocalGraphs\nfrom .proposal_local_graph import ProposalLocalGraphs\n\n\nclass DRRGHead(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        k_at_hops=(8, 4),\n        num_adjacent_linkages=3,\n        node_geo_feat_len=120,\n        pooling_scale=1.0,\n        pooling_output_size=(4, 3),\n        nms_thr=0.3,\n        min_width=8.0,\n        max_width=24.0,\n        comp_shrink_ratio=1.03,\n        comp_ratio=0.4,\n        comp_score_thr=0.3,\n        text_region_thr=0.2,\n        center_region_thr=0.2,\n        center_region_area_thr=50,\n        local_graph_thr=0.7,\n        **kwargs,\n    ):\n        super().__init__()\n\n        assert isinstance(in_channels, int)\n        assert isinstance(k_at_hops, tuple)\n        assert isinstance(num_adjacent_linkages, int)\n        assert isinstance(node_geo_feat_len, int)\n        assert isinstance(pooling_scale, float)\n        assert isinstance(pooling_output_size, tuple)\n        assert isinstance(comp_shrink_ratio, float)\n        assert isinstance(nms_thr, float)\n        assert isinstance(min_width, float)\n        assert isinstance(max_width, float)\n        assert isinstance(comp_ratio, float)\n        assert isinstance(comp_score_thr, float)\n        assert isinstance(text_region_thr, float)\n        assert isinstance(center_region_thr, float)\n        assert isinstance(center_region_area_thr, int)\n        assert isinstance(local_graph_thr, float)\n\n        self.in_channels = in_channels\n        self.out_channels = 6\n        self.downsample_ratio = 1.0\n        self.k_at_hops = k_at_hops\n        self.num_adjacent_linkages = num_adjacent_linkages\n        self.node_geo_feat_len = node_geo_feat_len\n        self.pooling_scale = pooling_scale\n        self.pooling_output_size = pooling_output_size\n        self.comp_shrink_ratio = comp_shrink_ratio\n        self.nms_thr = nms_thr\n        self.min_width = min_width\n        self.max_width = max_width\n        self.comp_ratio = comp_ratio\n        self.comp_score_thr = comp_score_thr\n        self.text_region_thr = text_region_thr\n        self.center_region_thr = center_region_thr\n        self.center_region_area_thr = center_region_area_thr\n        self.local_graph_thr = local_graph_thr\n\n        self.out_conv = nn.Conv2D(\n            in_channels=self.in_channels,\n            out_channels=self.out_channels,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n        )\n\n        self.graph_train = LocalGraphs(\n            self.k_at_hops,\n            self.num_adjacent_linkages,\n            self.node_geo_feat_len,\n            self.pooling_scale,\n            self.pooling_output_size,\n            self.local_graph_thr,\n        )\n\n        self.graph_test = ProposalLocalGraphs(\n            self.k_at_hops,\n            self.num_adjacent_linkages,\n            self.node_geo_feat_len,\n            self.pooling_scale,\n            self.pooling_output_size,\n            self.nms_thr,\n            self.min_width,\n            self.max_width,\n            self.comp_shrink_ratio,\n            self.comp_ratio,\n            self.comp_score_thr,\n            self.text_region_thr,\n            self.center_region_thr,\n            self.center_region_area_thr,\n        )\n\n        pool_w, pool_h = self.pooling_output_size\n        node_feat_len = (pool_w * pool_h) * (\n            self.in_channels + self.out_channels\n        ) + self.node_geo_feat_len\n        self.gcn = GCN(node_feat_len)\n\n    def forward(self, inputs, targets=None):\n        \"\"\"\n        Args:\n            inputs (Tensor): Shape of :math:`(N, C, H, W)`.\n            gt_comp_attribs (list[ndarray]): The padded text component\n                attributes. Shape: (num_component, 8).\n\n        Returns:\n            tuple: Returns (pred_maps, (gcn_pred, gt_labels)).\n\n                - | pred_maps (Tensor): Prediction map with shape\n                    :math:`(N, C_{out}, H, W)`.\n                - | gcn_pred (Tensor): Prediction from GCN module, with\n                    shape :math:`(N, 2)`.\n                - | gt_labels (Tensor): Ground-truth label with shape\n                    :math:`(N, 8)`.\n        \"\"\"\n        if self.training:\n            assert targets is not None\n            gt_comp_attribs = targets[7]\n            pred_maps = self.out_conv(inputs)\n            feat_maps = paddle.concat([inputs, pred_maps], axis=1)\n            node_feats, adjacent_matrices, knn_inds, gt_labels = self.graph_train(\n                feat_maps, np.stack(gt_comp_attribs)\n            )\n\n            gcn_pred = self.gcn(node_feats, adjacent_matrices, knn_inds)\n\n            return pred_maps, (gcn_pred, gt_labels)\n        else:\n            return self.single_test(inputs)\n\n    def single_test(self, feat_maps):\n        r\"\"\"\n        Args:\n            feat_maps (Tensor): Shape of :math:`(N, C, H, W)`.\n\n        Returns:\n            tuple: Returns (edge, score, text_comps).\n\n                - | edge (ndarray): The edge array of shape :math:`(N, 2)`\n                    where each row is a pair of text component indices\n                    that makes up an edge in graph.\n                - | score (ndarray): The score array of shape :math:`(N,)`,\n                    corresponding to the edge above.\n                - | text_comps (ndarray): The text components of shape\n                    :math:`(N, 9)` where each row corresponds to one box and\n                    its score: (x1, y1, x2, y2, x3, y3, x4, y4, score).\n        \"\"\"\n        pred_maps = self.out_conv(feat_maps)\n        feat_maps = paddle.concat([feat_maps, pred_maps], axis=1)\n\n        none_flag, graph_data = self.graph_test(pred_maps, feat_maps)\n\n        (\n            local_graphs_node_feat,\n            adjacent_matrices,\n            pivots_knn_inds,\n            pivot_local_graphs,\n            text_comps,\n        ) = graph_data\n\n        if none_flag:\n            return None, None, None\n        gcn_pred = self.gcn(local_graphs_node_feat, adjacent_matrices, pivots_knn_inds)\n        pred_labels = F.softmax(gcn_pred, axis=1)\n\n        edges = []\n        scores = []\n        pivot_local_graphs = pivot_local_graphs.squeeze().numpy()\n\n        for pivot_ind, pivot_local_graph in enumerate(pivot_local_graphs):\n            pivot = pivot_local_graph[0]\n            for k_ind, neighbor_ind in enumerate(pivots_knn_inds[pivot_ind]):\n                neighbor = pivot_local_graph[neighbor_ind.item()]\n                edges.append([pivot, neighbor])\n                scores.append(\n                    pred_labels[pivot_ind * pivots_knn_inds.shape[1] + k_ind, 1].item()\n                )\n\n        edges = np.asarray(edges)\n        scores = np.asarray(scores)\n\n        return edges, scores, text_comps\n", "ppocr/modeling/heads/rec_aster_head.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/ayumiymk/aster.pytorch/blob/master/lib/models/attention_recognition_head.py\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\n\nimport paddle\nfrom paddle import nn\nfrom paddle.nn import functional as F\n\n\nclass AsterHead(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        sDim,\n        attDim,\n        max_len_labels,\n        time_step=25,\n        beam_width=5,\n        **kwargs,\n    ):\n        super(AsterHead, self).__init__()\n        self.num_classes = out_channels\n        self.in_planes = in_channels\n        self.sDim = sDim\n        self.attDim = attDim\n        self.max_len_labels = max_len_labels\n        self.decoder = AttentionRecognitionHead(\n            in_channels, out_channels, sDim, attDim, max_len_labels\n        )\n        self.time_step = time_step\n        self.embeder = Embedding(self.time_step, in_channels)\n        self.beam_width = beam_width\n        self.eos = self.num_classes - 3\n\n    def forward(self, x, targets=None, embed=None):\n        return_dict = {}\n        embedding_vectors = self.embeder(x)\n\n        if self.training:\n            rec_targets, rec_lengths, _ = targets\n            rec_pred = self.decoder([x, rec_targets, rec_lengths], embedding_vectors)\n            return_dict[\"rec_pred\"] = rec_pred\n            return_dict[\"embedding_vectors\"] = embedding_vectors\n        else:\n            rec_pred, rec_pred_scores = self.decoder.beam_search(\n                x, self.beam_width, self.eos, embedding_vectors\n            )\n            return_dict[\"rec_pred\"] = rec_pred\n            return_dict[\"rec_pred_scores\"] = rec_pred_scores\n            return_dict[\"embedding_vectors\"] = embedding_vectors\n\n        return return_dict\n\n\nclass Embedding(nn.Layer):\n    def __init__(self, in_timestep, in_planes, mid_dim=4096, embed_dim=300):\n        super(Embedding, self).__init__()\n        self.in_timestep = in_timestep\n        self.in_planes = in_planes\n        self.embed_dim = embed_dim\n        self.mid_dim = mid_dim\n        self.eEmbed = nn.Linear(\n            in_timestep * in_planes, self.embed_dim\n        )  # Embed encoder output to a word-embedding like\n\n    def forward(self, x):\n        x = paddle.reshape(x, [x.shape[0], -1])\n        x = self.eEmbed(x)\n        return x\n\n\nclass AttentionRecognitionHead(nn.Layer):\n    \"\"\"\n    input: [b x 16 x 64 x in_planes]\n    output: probability sequence: [b x T x num_classes]\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, sDim, attDim, max_len_labels):\n        super(AttentionRecognitionHead, self).__init__()\n        self.num_classes = (\n            out_channels  # this is the output classes. So it includes the <EOS>.\n        )\n        self.in_planes = in_channels\n        self.sDim = sDim\n        self.attDim = attDim\n        self.max_len_labels = max_len_labels\n\n        self.decoder = DecoderUnit(\n            sDim=sDim, xDim=in_channels, yDim=self.num_classes, attDim=attDim\n        )\n\n    def forward(self, x, embed):\n        x, targets, lengths = x\n        batch_size = x.shape[0]\n        # Decoder\n        state = self.decoder.get_initial_state(embed)\n        outputs = []\n        for i in range(max(lengths)):\n            if i == 0:\n                y_prev = paddle.full(shape=[batch_size], fill_value=self.num_classes)\n            else:\n                y_prev = targets[:, i - 1]\n            output, state = self.decoder(x, state, y_prev)\n            outputs.append(output)\n        outputs = paddle.concat([_.unsqueeze(1) for _ in outputs], 1)\n        return outputs\n\n    # inference stage.\n    def sample(self, x):\n        x, _, _ = x\n        batch_size = x.size(0)\n        # Decoder\n        state = paddle.zeros([1, batch_size, self.sDim])\n\n        predicted_ids, predicted_scores, predicted = [], [], None\n        for i in range(self.max_len_labels):\n            if i == 0:\n                y_prev = paddle.full(shape=[batch_size], fill_value=self.num_classes)\n            else:\n                y_prev = predicted\n\n            output, state = self.decoder(x, state, y_prev)\n            output = F.softmax(output, axis=1)\n            score, predicted = output.max(1)\n            predicted_ids.append(predicted.unsqueeze(1))\n            predicted_scores.append(score.unsqueeze(1))\n        predicted_ids = paddle.concat([predicted_ids, 1])\n        predicted_scores = paddle.concat([predicted_scores, 1])\n        # return predicted_ids.squeeze(), predicted_scores.squeeze()\n        return predicted_ids, predicted_scores\n\n    def beam_search(self, x, beam_width, eos, embed):\n        def _inflate(tensor, times, dim):\n            repeat_dims = [1] * tensor.dim()\n            repeat_dims[dim] = times\n            output = paddle.tile(tensor, repeat_dims)\n            return output\n\n        # https://github.com/IBM/pytorch-seq2seq/blob/fede87655ddce6c94b38886089e05321dc9802af/seq2seq/models/TopKDecoder.py\n        batch_size, l, d = x.shape\n        x = paddle.tile(\n            paddle.transpose(x.unsqueeze(1), perm=[1, 0, 2, 3]), [beam_width, 1, 1, 1]\n        )\n        inflated_encoder_feats = paddle.reshape(\n            paddle.transpose(x, perm=[1, 0, 2, 3]), [-1, l, d]\n        )\n\n        # Initialize the decoder\n        state = self.decoder.get_initial_state(embed, tile_times=beam_width)\n\n        pos_index = paddle.reshape(\n            paddle.arange(batch_size) * beam_width, shape=[-1, 1]\n        )\n\n        # Initialize the scores\n        sequence_scores = paddle.full(\n            shape=[batch_size * beam_width, 1], fill_value=-float(\"Inf\")\n        )\n        index = [i * beam_width for i in range(0, batch_size)]\n        sequence_scores[index] = 0.0\n\n        # Initialize the input vector\n        y_prev = paddle.full(\n            shape=[batch_size * beam_width], fill_value=self.num_classes\n        )\n\n        # Store decisions for backtracking\n        stored_scores = list()\n        stored_predecessors = list()\n        stored_emitted_symbols = list()\n\n        for i in range(self.max_len_labels):\n            output, state = self.decoder(inflated_encoder_feats, state, y_prev)\n            state = paddle.unsqueeze(state, axis=0)\n            log_softmax_output = paddle.nn.functional.log_softmax(output, axis=1)\n\n            sequence_scores = _inflate(sequence_scores, self.num_classes, 1)\n            sequence_scores += log_softmax_output\n            scores, candidates = paddle.topk(\n                paddle.reshape(sequence_scores, [batch_size, -1]), beam_width, axis=1\n            )\n\n            # Reshape input = (bk, 1) and sequence_scores = (bk, 1)\n            y_prev = paddle.reshape(\n                candidates % self.num_classes, shape=[batch_size * beam_width]\n            )\n            sequence_scores = paddle.reshape(scores, shape=[batch_size * beam_width, 1])\n\n            # Update fields for next timestep\n            pos_index = paddle.expand_as(pos_index, candidates)\n            predecessors = paddle.cast(\n                candidates / self.num_classes + pos_index, dtype=\"int64\"\n            )\n            predecessors = paddle.reshape(\n                predecessors, shape=[batch_size * beam_width, 1]\n            )\n            state = paddle.index_select(state, index=predecessors.squeeze(), axis=1)\n\n            # Update sequence socres and erase scores for <eos> symbol so that they aren't expanded\n            stored_scores.append(sequence_scores.clone())\n            y_prev = paddle.reshape(y_prev, shape=[-1, 1])\n            eos_prev = paddle.full_like(y_prev, fill_value=eos)\n            mask = eos_prev == y_prev\n            mask = paddle.nonzero(mask)\n            if mask.dim() > 0:\n                sequence_scores = sequence_scores.numpy()\n                mask = mask.numpy()\n                sequence_scores[mask] = -float(\"inf\")\n                sequence_scores = paddle.to_tensor(sequence_scores)\n\n            # Cache results for backtracking\n            stored_predecessors.append(predecessors)\n            y_prev = paddle.squeeze(y_prev)\n            stored_emitted_symbols.append(y_prev)\n\n        # Do backtracking to return the optimal values\n        # ====== backtrak ======#\n        # Initialize return variables given different types\n        p = list()\n        l = [\n            [self.max_len_labels] * beam_width for _ in range(batch_size)\n        ]  # Placeholder for lengths of top-k sequences\n\n        # the last step output of the beams are not sorted\n        # thus they are sorted here\n        sorted_score, sorted_idx = paddle.topk(\n            paddle.reshape(stored_scores[-1], shape=[batch_size, beam_width]),\n            beam_width,\n        )\n\n        # initialize the sequence scores with the sorted last step beam scores\n        s = sorted_score.clone()\n\n        batch_eos_found = [0] * batch_size  # the number of EOS found\n        # in the backward loop below for each batch\n        t = self.max_len_labels - 1\n        # initialize the back pointer with the sorted order of the last step beams.\n        # add pos_index for indexing variable with b*k as the first dimension.\n        t_predecessors = paddle.reshape(\n            sorted_idx + pos_index.expand_as(sorted_idx),\n            shape=[batch_size * beam_width],\n        )\n        while t >= 0:\n            # Re-order the variables with the back pointer\n            current_symbol = paddle.index_select(\n                stored_emitted_symbols[t], index=t_predecessors, axis=0\n            )\n            t_predecessors = paddle.index_select(\n                stored_predecessors[t].squeeze(), index=t_predecessors, axis=0\n            )\n            eos_indices = stored_emitted_symbols[t] == eos\n            eos_indices = paddle.nonzero(eos_indices)\n\n            if eos_indices.dim() > 0:\n                for i in range(eos_indices.shape[0] - 1, -1, -1):\n                    # Indices of the EOS symbol for both variables\n                    # with b*k as the first dimension, and b, k for\n                    # the first two dimensions\n                    idx = eos_indices[i]\n                    b_idx = int(idx[0] / beam_width)\n                    # The indices of the replacing position\n                    # according to the replacement strategy noted above\n                    res_k_idx = beam_width - (batch_eos_found[b_idx] % beam_width) - 1\n                    batch_eos_found[b_idx] += 1\n                    res_idx = b_idx * beam_width + res_k_idx\n\n                    # Replace the old information in return variables\n                    # with the new ended sequence information\n                    t_predecessors[res_idx] = stored_predecessors[t][idx[0]]\n                    current_symbol[res_idx] = stored_emitted_symbols[t][idx[0]]\n                    s[b_idx, res_k_idx] = stored_scores[t][idx[0], 0]\n                    l[b_idx][res_k_idx] = t + 1\n\n            # record the back tracked results\n            p.append(current_symbol)\n            t -= 1\n\n        # Sort and re-order again as the added ended sequences may change\n        # the order (very unlikely)\n        s, re_sorted_idx = s.topk(beam_width)\n        for b_idx in range(batch_size):\n            l[b_idx] = [l[b_idx][k_idx.item()] for k_idx in re_sorted_idx[b_idx, :]]\n\n        re_sorted_idx = paddle.reshape(\n            re_sorted_idx + pos_index.expand_as(re_sorted_idx),\n            [batch_size * beam_width],\n        )\n\n        # Reverse the sequences and re-order at the same time\n        # It is reversed because the backtracking happens in reverse time order\n        p = [\n            paddle.reshape(\n                paddle.index_select(step, re_sorted_idx, 0),\n                shape=[batch_size, beam_width, -1],\n            )\n            for step in reversed(p)\n        ]\n        p = paddle.concat(p, -1)[:, 0, :]\n        return p, paddle.ones_like(p)\n\n\nclass AttentionUnit(nn.Layer):\n    def __init__(self, sDim, xDim, attDim):\n        super(AttentionUnit, self).__init__()\n\n        self.sDim = sDim\n        self.xDim = xDim\n        self.attDim = attDim\n\n        self.sEmbed = nn.Linear(sDim, attDim)\n        self.xEmbed = nn.Linear(xDim, attDim)\n        self.wEmbed = nn.Linear(attDim, 1)\n\n    def forward(self, x, sPrev):\n        batch_size, T, _ = x.shape  # [b x T x xDim]\n        x = paddle.reshape(x, [-1, self.xDim])  # [(b x T) x xDim]\n        xProj = self.xEmbed(x)  # [(b x T) x attDim]\n        xProj = paddle.reshape(xProj, [batch_size, T, -1])  # [b x T x attDim]\n\n        sPrev = sPrev.squeeze(0)\n        sProj = self.sEmbed(sPrev)  # [b x attDim]\n        sProj = paddle.unsqueeze(sProj, 1)  # [b x 1 x attDim]\n        sProj = paddle.expand(sProj, [batch_size, T, self.attDim])  # [b x T x attDim]\n\n        sumTanh = paddle.tanh(sProj + xProj)\n        sumTanh = paddle.reshape(sumTanh, [-1, self.attDim])\n\n        vProj = self.wEmbed(sumTanh)  # [(b x T) x 1]\n        vProj = paddle.reshape(vProj, [batch_size, T])\n        alpha = F.softmax(\n            vProj, axis=1\n        )  # attention weights for each sample in the minibatch\n        return alpha\n\n\nclass DecoderUnit(nn.Layer):\n    def __init__(self, sDim, xDim, yDim, attDim):\n        super(DecoderUnit, self).__init__()\n        self.sDim = sDim\n        self.xDim = xDim\n        self.yDim = yDim\n        self.attDim = attDim\n        self.emdDim = attDim\n\n        self.attention_unit = AttentionUnit(sDim, xDim, attDim)\n        self.tgt_embedding = nn.Embedding(\n            yDim + 1, self.emdDim, weight_attr=nn.initializer.Normal(std=0.01)\n        )  # the last is used for <BOS>\n        self.gru = nn.GRUCell(input_size=xDim + self.emdDim, hidden_size=sDim)\n        self.fc = nn.Linear(\n            sDim,\n            yDim,\n            weight_attr=nn.initializer.Normal(std=0.01),\n            bias_attr=nn.initializer.Constant(value=0),\n        )\n        self.embed_fc = nn.Linear(300, self.sDim)\n\n    def get_initial_state(self, embed, tile_times=1):\n        assert embed.shape[1] == 300\n        state = self.embed_fc(embed)  # N * sDim\n        if tile_times != 1:\n            state = state.unsqueeze(1)\n            trans_state = paddle.transpose(state, perm=[1, 0, 2])\n            state = paddle.tile(trans_state, repeat_times=[tile_times, 1, 1])\n            trans_state = paddle.transpose(state, perm=[1, 0, 2])\n            state = paddle.reshape(trans_state, shape=[-1, self.sDim])\n        state = state.unsqueeze(0)  # 1 * N * sDim\n        return state\n\n    def forward(self, x, sPrev, yPrev):\n        # x: feature sequence from the image decoder.\n        batch_size, T, _ = x.shape\n        alpha = self.attention_unit(x, sPrev)\n        context = paddle.squeeze(paddle.matmul(alpha.unsqueeze(1), x), axis=1)\n        yPrev = paddle.cast(yPrev, dtype=\"int64\")\n        yProj = self.tgt_embedding(yPrev)\n\n        concat_context = paddle.concat([yProj, context], 1)\n        concat_context = paddle.squeeze(concat_context, 1)\n        sPrev = paddle.squeeze(sPrev, 0)\n        output, state = self.gru(concat_context, sPrev)\n        output = paddle.squeeze(output, axis=1)\n        output = self.fc(output)\n        return output, state\n", "ppocr/modeling/heads/rec_rfl_head.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/hikopensource/DAVAR-Lab-OCR/blob/main/davarocr/davar_rcg/models/sequence_heads/counting_head.py\n\"\"\"\nimport paddle\nimport paddle.nn as nn\nfrom paddle.nn.initializer import TruncatedNormal, Constant, Normal, KaimingNormal\n\nfrom .rec_att_head import AttentionLSTM\n\nkaiming_init_ = KaimingNormal()\nzeros_ = Constant(value=0.0)\nones_ = Constant(value=1.0)\n\n\nclass CNTHead(nn.Layer):\n    def __init__(self, embed_size=512, encode_length=26, out_channels=38, **kwargs):\n        super(CNTHead, self).__init__()\n\n        self.out_channels = out_channels\n\n        self.Wv_fusion = nn.Linear(embed_size, embed_size, bias_attr=False)\n        self.Prediction_visual = nn.Linear(\n            encode_length * embed_size, self.out_channels\n        )\n\n    def forward(self, visual_feature):\n        b, c, h, w = visual_feature.shape\n        visual_feature = visual_feature.reshape([b, c, h * w]).transpose([0, 2, 1])\n        visual_feature_num = self.Wv_fusion(visual_feature)  # batch * 26 * 512\n        b, n, c = visual_feature_num.shape\n        # using visual feature directly calculate the text length\n        visual_feature_num = visual_feature_num.reshape([b, n * c])\n        prediction_visual = self.Prediction_visual(visual_feature_num)\n\n        return prediction_visual\n\n\nclass RFLHead(nn.Layer):\n    def __init__(\n        self,\n        in_channels=512,\n        hidden_size=256,\n        batch_max_legnth=25,\n        out_channels=38,\n        use_cnt=True,\n        use_seq=True,\n        **kwargs,\n    ):\n        super(RFLHead, self).__init__()\n        assert use_cnt or use_seq\n        self.use_cnt = use_cnt\n        self.use_seq = use_seq\n        if self.use_cnt:\n            self.cnt_head = CNTHead(\n                embed_size=in_channels,\n                encode_length=batch_max_legnth + 1,\n                out_channels=out_channels,\n                **kwargs,\n            )\n        if self.use_seq:\n            self.seq_head = AttentionLSTM(\n                in_channels=in_channels,\n                out_channels=out_channels,\n                hidden_size=hidden_size,\n                **kwargs,\n            )\n        self.batch_max_legnth = batch_max_legnth\n        self.num_class = out_channels\n        self.apply(self.init_weights)\n\n    def init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            kaiming_init_(m.weight)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                zeros_(m.bias)\n\n    def forward(self, x, targets=None):\n        cnt_inputs, seq_inputs = x\n        if self.use_cnt:\n            cnt_outputs = self.cnt_head(cnt_inputs)\n        else:\n            cnt_outputs = None\n        if self.use_seq:\n            if self.training:\n                seq_outputs = self.seq_head(\n                    seq_inputs, targets[0], self.batch_max_legnth\n                )\n            else:\n                seq_outputs = self.seq_head(seq_inputs, None, self.batch_max_legnth)\n            return cnt_outputs, seq_outputs\n        else:\n            return cnt_outputs\n", "ppocr/modeling/heads/rec_satrn_head.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/open-mmlab/mmocr/blob/1.x/mmocr/models/textrecog/encoders/satrn_encoder.py\nhttps://github.com/open-mmlab/mmocr/blob/1.x/mmocr/models/textrecog/decoders/nrtr_decoder.py\n\"\"\"\n\nimport math\nimport numpy as np\nimport paddle\nimport paddle.nn as nn\nimport paddle.nn.functional as F\nfrom paddle import ParamAttr, reshape, transpose\nfrom paddle.nn import Conv2D, BatchNorm, Linear, Dropout\nfrom paddle.nn import AdaptiveAvgPool2D, MaxPool2D, AvgPool2D\nfrom paddle.nn.initializer import KaimingNormal, Uniform, Constant\n\n\nclass ConvBNLayer(nn.Layer):\n    def __init__(\n        self, num_channels, filter_size, num_filters, stride, padding, num_groups=1\n    ):\n        super(ConvBNLayer, self).__init__()\n\n        self.conv = nn.Conv2D(\n            in_channels=num_channels,\n            out_channels=num_filters,\n            kernel_size=filter_size,\n            stride=stride,\n            padding=padding,\n            groups=num_groups,\n            bias_attr=False,\n        )\n\n        self.bn = nn.BatchNorm2D(\n            num_filters,\n            weight_attr=ParamAttr(initializer=Constant(1)),\n            bias_attr=ParamAttr(initializer=Constant(0)),\n        )\n        self.relu = nn.ReLU()\n\n    def forward(self, inputs):\n        y = self.conv(inputs)\n        y = self.bn(y)\n        y = self.relu(y)\n        return y\n\n\nclass SATRNEncoderLayer(nn.Layer):\n    def __init__(\n        self,\n        d_model=512,\n        d_inner=512,\n        n_head=8,\n        d_k=64,\n        d_v=64,\n        dropout=0.1,\n        qkv_bias=False,\n    ):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(d_model)\n        self.attn = MultiHeadAttention(\n            n_head, d_model, d_k, d_v, qkv_bias=qkv_bias, dropout=dropout\n        )\n        self.norm2 = nn.LayerNorm(d_model)\n        self.feed_forward = LocalityAwareFeedforward(d_model, d_inner, dropout=dropout)\n\n    def forward(self, x, h, w, mask=None):\n        n, hw, c = x.shape\n        residual = x\n        x = self.norm1(x)\n        x = residual + self.attn(x, x, x, mask)\n        residual = x\n        x = self.norm2(x)\n        x = x.transpose([0, 2, 1]).reshape([n, c, h, w])\n        x = self.feed_forward(x)\n        x = x.reshape([n, c, hw]).transpose([0, 2, 1])\n        x = residual + x\n        return x\n\n\nclass LocalityAwareFeedforward(nn.Layer):\n    def __init__(\n        self,\n        d_in,\n        d_hid,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.conv1 = ConvBNLayer(d_in, 1, d_hid, stride=1, padding=0)\n\n        self.depthwise_conv = ConvBNLayer(\n            d_hid, 3, d_hid, stride=1, padding=1, num_groups=d_hid\n        )\n\n        self.conv2 = ConvBNLayer(d_hid, 1, d_in, stride=1, padding=0)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.depthwise_conv(x)\n        x = self.conv2(x)\n\n        return x\n\n\nclass Adaptive2DPositionalEncoding(nn.Layer):\n    def __init__(self, d_hid=512, n_height=100, n_width=100, dropout=0.1):\n        super().__init__()\n\n        h_position_encoder = self._get_sinusoid_encoding_table(n_height, d_hid)\n        h_position_encoder = h_position_encoder.transpose([1, 0])\n        h_position_encoder = h_position_encoder.reshape([1, d_hid, n_height, 1])\n\n        w_position_encoder = self._get_sinusoid_encoding_table(n_width, d_hid)\n        w_position_encoder = w_position_encoder.transpose([1, 0])\n        w_position_encoder = w_position_encoder.reshape([1, d_hid, 1, n_width])\n\n        self.register_buffer(\"h_position_encoder\", h_position_encoder)\n        self.register_buffer(\"w_position_encoder\", w_position_encoder)\n\n        self.h_scale = self.scale_factor_generate(d_hid)\n        self.w_scale = self.scale_factor_generate(d_hid)\n        self.pool = nn.AdaptiveAvgPool2D(1)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def _get_sinusoid_encoding_table(self, n_position, d_hid):\n        \"\"\"Sinusoid position encoding table.\"\"\"\n        denominator = paddle.to_tensor(\n            [1.0 / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)]\n        )\n        denominator = denominator.reshape([1, -1])\n        pos_tensor = paddle.cast(paddle.arange(n_position).unsqueeze(-1), \"float32\")\n        sinusoid_table = pos_tensor * denominator\n        sinusoid_table[:, 0::2] = paddle.sin(sinusoid_table[:, 0::2])\n        sinusoid_table[:, 1::2] = paddle.cos(sinusoid_table[:, 1::2])\n\n        return sinusoid_table\n\n    def scale_factor_generate(self, d_hid):\n        scale_factor = nn.Sequential(\n            nn.Conv2D(d_hid, d_hid, 1),\n            nn.ReLU(),\n            nn.Conv2D(d_hid, d_hid, 1),\n            nn.Sigmoid(),\n        )\n\n        return scale_factor\n\n    def forward(self, x):\n        b, c, h, w = x.shape\n\n        avg_pool = self.pool(x)\n\n        h_pos_encoding = self.h_scale(avg_pool) * self.h_position_encoder[:, :, :h, :]\n        w_pos_encoding = self.w_scale(avg_pool) * self.w_position_encoder[:, :, :, :w]\n\n        out = x + h_pos_encoding + w_pos_encoding\n\n        out = self.dropout(out)\n\n        return out\n\n\nclass ScaledDotProductAttention(nn.Layer):\n    def __init__(self, temperature, attn_dropout=0.1):\n        super().__init__()\n        self.temperature = temperature\n        self.dropout = nn.Dropout(attn_dropout)\n\n    def forward(self, q, k, v, mask=None):\n        def masked_fill(x, mask, value):\n            y = paddle.full(x.shape, value, x.dtype)\n            return paddle.where(mask, y, x)\n\n        attn = paddle.matmul(q / self.temperature, k.transpose([0, 1, 3, 2]))\n        if mask is not None:\n            attn = masked_fill(attn, mask == 0, -1e9)\n            # attn = attn.masked_fill(mask == 0, float('-inf'))\n            # attn += mask\n\n        attn = self.dropout(F.softmax(attn, axis=-1))\n        output = paddle.matmul(attn, v)\n\n        return output, attn\n\n\nclass MultiHeadAttention(nn.Layer):\n    def __init__(\n        self, n_head=8, d_model=512, d_k=64, d_v=64, dropout=0.1, qkv_bias=False\n    ):\n        super().__init__()\n        self.n_head = n_head\n        self.d_k = d_k\n        self.d_v = d_v\n\n        self.dim_k = n_head * d_k\n        self.dim_v = n_head * d_v\n\n        self.linear_q = nn.Linear(self.dim_k, self.dim_k, bias_attr=qkv_bias)\n        self.linear_k = nn.Linear(self.dim_k, self.dim_k, bias_attr=qkv_bias)\n        self.linear_v = nn.Linear(self.dim_v, self.dim_v, bias_attr=qkv_bias)\n\n        self.attention = ScaledDotProductAttention(d_k**0.5, dropout)\n\n        self.fc = nn.Linear(self.dim_v, d_model, bias_attr=qkv_bias)\n        self.proj_drop = nn.Dropout(dropout)\n\n    def forward(self, q, k, v, mask=None):\n        batch_size, len_q, _ = q.shape\n        _, len_k, _ = k.shape\n\n        q = self.linear_q(q).reshape([batch_size, len_q, self.n_head, self.d_k])\n        k = self.linear_k(k).reshape([batch_size, len_k, self.n_head, self.d_k])\n        v = self.linear_v(v).reshape([batch_size, len_k, self.n_head, self.d_v])\n\n        q, k, v = (\n            q.transpose([0, 2, 1, 3]),\n            k.transpose([0, 2, 1, 3]),\n            v.transpose([0, 2, 1, 3]),\n        )\n\n        if mask is not None:\n            if mask.dim() == 3:\n                mask = mask.unsqueeze(1)\n            elif mask.dim() == 2:\n                mask = mask.unsqueeze(1).unsqueeze(1)\n\n        attn_out, _ = self.attention(q, k, v, mask=mask)\n\n        attn_out = attn_out.transpose([0, 2, 1, 3]).reshape(\n            [batch_size, len_q, self.dim_v]\n        )\n\n        attn_out = self.fc(attn_out)\n        attn_out = self.proj_drop(attn_out)\n\n        return attn_out\n\n\nclass SATRNEncoder(nn.Layer):\n    def __init__(\n        self,\n        n_layers=12,\n        n_head=8,\n        d_k=64,\n        d_v=64,\n        d_model=512,\n        n_position=100,\n        d_inner=256,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.d_model = d_model\n        self.position_enc = Adaptive2DPositionalEncoding(\n            d_hid=d_model, n_height=n_position, n_width=n_position, dropout=dropout\n        )\n        self.layer_stack = nn.LayerList(\n            [\n                SATRNEncoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout)\n                for _ in range(n_layers)\n            ]\n        )\n        self.layer_norm = nn.LayerNorm(d_model)\n\n    def forward(self, feat, valid_ratios=None):\n        \"\"\"\n        Args:\n            feat (Tensor): Feature tensor of shape :math:`(N, D_m, H, W)`.\n            img_metas (dict): A dict that contains meta information of input\n                images. Preferably with the key ``valid_ratio``.\n\n        Returns:\n            Tensor: A tensor of shape :math:`(N, T, D_m)`.\n        \"\"\"\n        if valid_ratios is None:\n            bs = feat.shape[0]\n            valid_ratios = paddle.full((bs, 1), 1.0, dtype=paddle.float32)\n\n        feat = self.position_enc(feat)\n        n, c, h, w = feat.shape\n\n        mask = paddle.zeros((n, h, w))\n        for i, valid_ratio in enumerate(valid_ratios):\n            valid_width = int(min(w, paddle.ceil(w * valid_ratio)))\n            mask[i, :, :valid_width] = 1\n\n        mask = mask.reshape([n, h * w])\n        feat = feat.reshape([n, c, h * w])\n\n        output = feat.transpose([0, 2, 1])\n        for enc_layer in self.layer_stack:\n            output = enc_layer(output, h, w, mask)\n        output = self.layer_norm(output)\n\n        return output\n\n\nclass PositionwiseFeedForward(nn.Layer):\n    def __init__(self, d_in, d_hid, dropout=0.1):\n        super().__init__()\n        self.w_1 = nn.Linear(d_in, d_hid)\n        self.w_2 = nn.Linear(d_hid, d_in)\n        self.act = nn.GELU()\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.w_1(x)\n        x = self.act(x)\n        x = self.w_2(x)\n        x = self.dropout(x)\n\n        return x\n\n\nclass PositionalEncoding(nn.Layer):\n    def __init__(self, d_hid=512, n_position=200, dropout=0):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        # Not a parameter\n        # Position table of shape (1, n_position, d_hid)\n        self.register_buffer(\n            \"position_table\", self._get_sinusoid_encoding_table(n_position, d_hid)\n        )\n\n    def _get_sinusoid_encoding_table(self, n_position, d_hid):\n        \"\"\"Sinusoid position encoding table.\"\"\"\n        denominator = paddle.to_tensor(\n            [1.0 / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)]\n        )\n        denominator = denominator.reshape([1, -1])\n        pos_tensor = paddle.cast(paddle.arange(n_position).unsqueeze(-1), \"float32\")\n        sinusoid_table = pos_tensor * denominator\n        sinusoid_table[:, 0::2] = paddle.sin(sinusoid_table[:, 0::2])\n        sinusoid_table[:, 1::2] = paddle.cos(sinusoid_table[:, 1::2])\n\n        return sinusoid_table.unsqueeze(0)\n\n    def forward(self, x):\n        x = x + self.position_table[:, : x.shape[1]].clone().detach()\n        return self.dropout(x)\n\n\nclass TFDecoderLayer(nn.Layer):\n    def __init__(\n        self,\n        d_model=512,\n        d_inner=256,\n        n_head=8,\n        d_k=64,\n        d_v=64,\n        dropout=0.1,\n        qkv_bias=False,\n        operation_order=None,\n    ):\n        super().__init__()\n\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n\n        self.self_attn = MultiHeadAttention(\n            n_head, d_model, d_k, d_v, dropout=dropout, qkv_bias=qkv_bias\n        )\n\n        self.enc_attn = MultiHeadAttention(\n            n_head, d_model, d_k, d_v, dropout=dropout, qkv_bias=qkv_bias\n        )\n\n        self.mlp = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)\n\n        self.operation_order = operation_order\n        if self.operation_order is None:\n            self.operation_order = (\n                \"norm\",\n                \"self_attn\",\n                \"norm\",\n                \"enc_dec_attn\",\n                \"norm\",\n                \"ffn\",\n            )\n        assert self.operation_order in [\n            (\"norm\", \"self_attn\", \"norm\", \"enc_dec_attn\", \"norm\", \"ffn\"),\n            (\"self_attn\", \"norm\", \"enc_dec_attn\", \"norm\", \"ffn\", \"norm\"),\n        ]\n\n    def forward(\n        self, dec_input, enc_output, self_attn_mask=None, dec_enc_attn_mask=None\n    ):\n        if self.operation_order == (\n            \"self_attn\",\n            \"norm\",\n            \"enc_dec_attn\",\n            \"norm\",\n            \"ffn\",\n            \"norm\",\n        ):\n            dec_attn_out = self.self_attn(\n                dec_input, dec_input, dec_input, self_attn_mask\n            )\n            dec_attn_out += dec_input\n            dec_attn_out = self.norm1(dec_attn_out)\n\n            enc_dec_attn_out = self.enc_attn(\n                dec_attn_out, enc_output, enc_output, dec_enc_attn_mask\n            )\n            enc_dec_attn_out += dec_attn_out\n            enc_dec_attn_out = self.norm2(enc_dec_attn_out)\n\n            mlp_out = self.mlp(enc_dec_attn_out)\n            mlp_out += enc_dec_attn_out\n            mlp_out = self.norm3(mlp_out)\n        elif self.operation_order == (\n            \"norm\",\n            \"self_attn\",\n            \"norm\",\n            \"enc_dec_attn\",\n            \"norm\",\n            \"ffn\",\n        ):\n            dec_input_norm = self.norm1(dec_input)\n            dec_attn_out = self.self_attn(\n                dec_input_norm, dec_input_norm, dec_input_norm, self_attn_mask\n            )\n            dec_attn_out += dec_input\n\n            enc_dec_attn_in = self.norm2(dec_attn_out)\n            enc_dec_attn_out = self.enc_attn(\n                enc_dec_attn_in, enc_output, enc_output, dec_enc_attn_mask\n            )\n            enc_dec_attn_out += dec_attn_out\n\n            mlp_out = self.mlp(self.norm3(enc_dec_attn_out))\n            mlp_out += enc_dec_attn_out\n\n        return mlp_out\n\n\nclass SATRNDecoder(nn.Layer):\n    def __init__(\n        self,\n        n_layers=6,\n        d_embedding=512,\n        n_head=8,\n        d_k=64,\n        d_v=64,\n        d_model=512,\n        d_inner=256,\n        n_position=200,\n        dropout=0.1,\n        num_classes=93,\n        max_seq_len=40,\n        start_idx=1,\n        padding_idx=92,\n    ):\n        super().__init__()\n\n        self.padding_idx = padding_idx\n        self.start_idx = start_idx\n        self.max_seq_len = max_seq_len\n\n        self.trg_word_emb = nn.Embedding(\n            num_classes, d_embedding, padding_idx=padding_idx\n        )\n\n        self.position_enc = PositionalEncoding(d_embedding, n_position=n_position)\n        self.dropout = nn.Dropout(p=dropout)\n\n        self.layer_stack = nn.LayerList(\n            [\n                TFDecoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout)\n                for _ in range(n_layers)\n            ]\n        )\n        self.layer_norm = nn.LayerNorm(d_model, epsilon=1e-6)\n\n        pred_num_class = num_classes - 1  # ignore padding_idx\n        self.classifier = nn.Linear(d_model, pred_num_class)\n\n    @staticmethod\n    def get_pad_mask(seq, pad_idx):\n        return (seq != pad_idx).unsqueeze(-2)\n\n    @staticmethod\n    def get_subsequent_mask(seq):\n        \"\"\"For masking out the subsequent info.\"\"\"\n        len_s = seq.shape[1]\n        subsequent_mask = 1 - paddle.triu(paddle.ones((len_s, len_s)), diagonal=1)\n        subsequent_mask = paddle.cast(subsequent_mask.unsqueeze(0), \"bool\")\n\n        return subsequent_mask\n\n    def _attention(self, trg_seq, src, src_mask=None):\n        trg_embedding = self.trg_word_emb(trg_seq)\n        trg_pos_encoded = self.position_enc(trg_embedding)\n        tgt = self.dropout(trg_pos_encoded)\n\n        trg_mask = self.get_pad_mask(\n            trg_seq, pad_idx=self.padding_idx\n        ) & self.get_subsequent_mask(trg_seq)\n        output = tgt\n        for dec_layer in self.layer_stack:\n            output = dec_layer(\n                output, src, self_attn_mask=trg_mask, dec_enc_attn_mask=src_mask\n            )\n        output = self.layer_norm(output)\n\n        return output\n\n    def _get_mask(self, logit, valid_ratios):\n        N, T, _ = logit.shape\n        mask = None\n        if valid_ratios is not None:\n            mask = paddle.zeros((N, T))\n            for i, valid_ratio in enumerate(valid_ratios):\n                valid_width = min(T, math.ceil(T * valid_ratio))\n                mask[i, :valid_width] = 1\n\n        return mask\n\n    def forward_train(self, feat, out_enc, targets, valid_ratio):\n        src_mask = self._get_mask(out_enc, valid_ratio)\n        attn_output = self._attention(targets, out_enc, src_mask=src_mask)\n        outputs = self.classifier(attn_output)\n\n        return outputs\n\n    def forward_test(self, feat, out_enc, valid_ratio):\n        src_mask = self._get_mask(out_enc, valid_ratio)\n        N = out_enc.shape[0]\n        init_target_seq = paddle.full(\n            (N, self.max_seq_len + 1), self.padding_idx, dtype=\"int64\"\n        )\n        # bsz * seq_len\n        init_target_seq[:, 0] = self.start_idx\n\n        outputs = []\n        for step in range(0, paddle.to_tensor(self.max_seq_len)):\n            decoder_output = self._attention(\n                init_target_seq, out_enc, src_mask=src_mask\n            )\n            # bsz * seq_len * C\n            step_result = F.softmax(\n                self.classifier(decoder_output[:, step, :]), axis=-1\n            )\n            # bsz * num_classes\n            outputs.append(step_result)\n            step_max_index = paddle.argmax(step_result, axis=-1)\n            init_target_seq[:, step + 1] = step_max_index\n\n        outputs = paddle.stack(outputs, axis=1)\n\n        return outputs\n\n    def forward(self, feat, out_enc, targets=None, valid_ratio=None):\n        if self.training:\n            return self.forward_train(feat, out_enc, targets, valid_ratio)\n        else:\n            return self.forward_test(feat, out_enc, valid_ratio)\n\n\nclass SATRNHead(nn.Layer):\n    def __init__(self, enc_cfg, dec_cfg, **kwargs):\n        super(SATRNHead, self).__init__()\n\n        # encoder module\n        self.encoder = SATRNEncoder(**enc_cfg)\n\n        # decoder module\n        self.decoder = SATRNDecoder(**dec_cfg)\n\n    def forward(self, feat, targets=None):\n        if targets is not None:\n            targets, valid_ratio = targets\n        else:\n            targets, valid_ratio = None, None\n        holistic_feat = self.encoder(feat, valid_ratio)  # bsz c\n        final_out = self.decoder(feat, holistic_feat, targets, valid_ratio)\n\n        return final_out\n", "ppocr/modeling/heads/det_db_head.py": "# copyright (c) 2019 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport paddle\nfrom paddle import nn\nimport paddle.nn.functional as F\nfrom paddle import ParamAttr\nfrom ppocr.modeling.backbones.det_mobilenet_v3 import ConvBNLayer\n\n\ndef get_bias_attr(k):\n    stdv = 1.0 / math.sqrt(k * 1.0)\n    initializer = paddle.nn.initializer.Uniform(-stdv, stdv)\n    bias_attr = ParamAttr(initializer=initializer)\n    return bias_attr\n\n\nclass Head(nn.Layer):\n    def __init__(self, in_channels, kernel_list=[3, 2, 2], **kwargs):\n        super(Head, self).__init__()\n\n        self.conv1 = nn.Conv2D(\n            in_channels=in_channels,\n            out_channels=in_channels // 4,\n            kernel_size=kernel_list[0],\n            padding=int(kernel_list[0] // 2),\n            weight_attr=ParamAttr(),\n            bias_attr=False,\n        )\n        self.conv_bn1 = nn.BatchNorm(\n            num_channels=in_channels // 4,\n            param_attr=ParamAttr(initializer=paddle.nn.initializer.Constant(value=1.0)),\n            bias_attr=ParamAttr(initializer=paddle.nn.initializer.Constant(value=1e-4)),\n            act=\"relu\",\n        )\n\n        self.conv2 = nn.Conv2DTranspose(\n            in_channels=in_channels // 4,\n            out_channels=in_channels // 4,\n            kernel_size=kernel_list[1],\n            stride=2,\n            weight_attr=ParamAttr(initializer=paddle.nn.initializer.KaimingUniform()),\n            bias_attr=get_bias_attr(in_channels // 4),\n        )\n        self.conv_bn2 = nn.BatchNorm(\n            num_channels=in_channels // 4,\n            param_attr=ParamAttr(initializer=paddle.nn.initializer.Constant(value=1.0)),\n            bias_attr=ParamAttr(initializer=paddle.nn.initializer.Constant(value=1e-4)),\n            act=\"relu\",\n        )\n        self.conv3 = nn.Conv2DTranspose(\n            in_channels=in_channels // 4,\n            out_channels=1,\n            kernel_size=kernel_list[2],\n            stride=2,\n            weight_attr=ParamAttr(initializer=paddle.nn.initializer.KaimingUniform()),\n            bias_attr=get_bias_attr(in_channels // 4),\n        )\n\n    def forward(self, x, return_f=False):\n        x = self.conv1(x)\n        x = self.conv_bn1(x)\n        x = self.conv2(x)\n        x = self.conv_bn2(x)\n        if return_f is True:\n            f = x\n        x = self.conv3(x)\n        x = F.sigmoid(x)\n        if return_f is True:\n            return x, f\n        return x\n\n\nclass DBHead(nn.Layer):\n    \"\"\"\n    Differentiable Binarization (DB) for text detection:\n        see https://arxiv.org/abs/1911.08947\n    args:\n        params(dict): super parameters for build DB network\n    \"\"\"\n\n    def __init__(self, in_channels, k=50, **kwargs):\n        super(DBHead, self).__init__()\n        self.k = k\n        self.binarize = Head(in_channels, **kwargs)\n        self.thresh = Head(in_channels, **kwargs)\n\n    def step_function(self, x, y):\n        return paddle.reciprocal(1 + paddle.exp(-self.k * (x - y)))\n\n    def forward(self, x, targets=None):\n        shrink_maps = self.binarize(x)\n        if not self.training:\n            return {\"maps\": shrink_maps}\n\n        threshold_maps = self.thresh(x)\n        binary_maps = self.step_function(shrink_maps, threshold_maps)\n        y = paddle.concat([shrink_maps, threshold_maps, binary_maps], axis=1)\n        return {\"maps\": y}\n\n\nclass LocalModule(nn.Layer):\n    def __init__(self, in_c, mid_c, use_distance=True):\n        super(self.__class__, self).__init__()\n        self.last_3 = ConvBNLayer(in_c + 1, mid_c, 3, 1, 1, act=\"relu\")\n        self.last_1 = nn.Conv2D(mid_c, 1, 1, 1, 0)\n\n    def forward(self, x, init_map, distance_map):\n        outf = paddle.concat([init_map, x], axis=1)\n        # last Conv\n        out = self.last_1(self.last_3(outf))\n        return out\n\n\nclass PFHeadLocal(DBHead):\n    def __init__(self, in_channels, k=50, mode=\"small\", **kwargs):\n        super(PFHeadLocal, self).__init__(in_channels, k, **kwargs)\n        self.mode = mode\n\n        self.up_conv = nn.Upsample(scale_factor=2, mode=\"nearest\", align_mode=1)\n        if self.mode == \"large\":\n            self.cbn_layer = LocalModule(in_channels // 4, in_channels // 4)\n        elif self.mode == \"small\":\n            self.cbn_layer = LocalModule(in_channels // 4, in_channels // 8)\n\n    def forward(self, x, targets=None):\n        shrink_maps, f = self.binarize(x, return_f=True)\n        base_maps = shrink_maps\n        cbn_maps = self.cbn_layer(self.up_conv(f), shrink_maps, None)\n        cbn_maps = F.sigmoid(cbn_maps)\n        if not self.training:\n            return {\"maps\": 0.5 * (base_maps + cbn_maps), \"cbn_maps\": cbn_maps}\n\n        threshold_maps = self.thresh(x)\n        binary_maps = self.step_function(shrink_maps, threshold_maps)\n        y = paddle.concat([cbn_maps, threshold_maps, binary_maps], axis=1)\n        return {\"maps\": y, \"distance_maps\": cbn_maps, \"cbn_maps\": binary_maps}\n", "ppocr/modeling/heads/rec_cppd_head.py": "# copyright (c) 2023 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\ntry:\n    from collections import Callable\nexcept:\n    from collections.abc import Callable\n\nimport numpy as np\nimport paddle\nfrom paddle import nn\nfrom paddle.nn import functional as F\nfrom ppocr.modeling.heads.rec_nrtr_head import Embeddings\nfrom ppocr.modeling.backbones.rec_svtrnet import (\n    DropPath,\n    Identity,\n    trunc_normal_,\n    zeros_,\n    ones_,\n    Mlp,\n)\n\n\nclass Attention(nn.Layer):\n    def __init__(\n        self,\n        dim,\n        num_heads=8,\n        qkv_bias=False,\n        qk_scale=None,\n        attn_drop=0.0,\n        proj_drop=0.0,\n    ):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim**-0.5\n\n        self.q = nn.Linear(dim, dim, bias_attr=qkv_bias)\n        self.kv = nn.Linear(dim, dim * 2, bias_attr=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, q, kv):\n        N, C = kv.shape[1:]\n        QN = q.shape[1]\n        q = (\n            self.q(q)\n            .reshape([-1, QN, self.num_heads, C // self.num_heads])\n            .transpose([0, 2, 1, 3])\n        )\n        k, v = (\n            self.kv(kv)\n            .reshape([-1, N, 2, self.num_heads, C // self.num_heads])\n            .transpose((2, 0, 3, 1, 4))\n        )\n        attn = q.matmul(k.transpose((0, 1, 3, 2))) * self.scale\n        attn = F.softmax(attn, axis=-1)\n        attn = self.attn_drop(attn)\n        x = (attn.matmul(v)).transpose((0, 2, 1, 3)).reshape((-1, QN, C))\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass EdgeDecoderLayer(nn.Layer):\n    def __init__(\n        self,\n        dim,\n        num_heads,\n        mlp_ratio=4.0,\n        qkv_bias=False,\n        qk_scale=None,\n        drop=0.0,\n        attn_drop=0.0,\n        drop_path=[0.0, 0.0],\n        act_layer=nn.GELU,\n        norm_layer=\"nn.LayerNorm\",\n        epsilon=1e-6,\n    ):\n        super().__init__()\n\n        self.head_dim = dim // num_heads\n        self.scale = qk_scale or self.head_dim**-0.5\n\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path1 = DropPath(drop_path[0]) if drop_path[0] > 0.0 else Identity()\n        self.norm1 = eval(norm_layer)(dim, epsilon=epsilon)\n        self.norm2 = eval(norm_layer)(dim, epsilon=epsilon)\n\n        self.p = nn.Linear(dim, dim)\n        self.cv = nn.Linear(dim, dim)\n        self.pv = nn.Linear(dim, dim)\n\n        self.dim = dim\n        self.num_heads = num_heads\n        self.p_proj = nn.Linear(dim, dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp_ratio = mlp_ratio\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=mlp_hidden_dim,\n            act_layer=act_layer,\n            drop=drop,\n        )\n\n    def forward(self, p, cv, pv):\n        pN = p.shape[1]\n        vN = cv.shape[1]\n        p_shortcut = p\n\n        p1 = (\n            self.p(p)\n            .reshape([-1, pN, self.num_heads, self.dim // self.num_heads])\n            .transpose([0, 2, 1, 3])\n        )\n        cv1 = (\n            self.cv(cv)\n            .reshape([-1, vN, self.num_heads, self.dim // self.num_heads])\n            .transpose([0, 2, 1, 3])\n        )\n        pv1 = (\n            self.pv(pv)\n            .reshape([-1, vN, self.num_heads, self.dim // self.num_heads])\n            .transpose([0, 2, 1, 3])\n        )\n\n        edge = F.softmax(p1.matmul(pv1.transpose((0, 1, 3, 2))), -1)  # B h N N\n        p_c = (edge @ cv1).transpose((0, 2, 1, 3)).reshape((-1, pN, self.dim))\n\n        x1 = self.norm1(p_shortcut + self.drop_path1(self.p_proj(p_c)))\n\n        x = self.norm2(x1 + self.drop_path1(self.mlp(x1)))\n        return x\n\n\nclass DecoderLayer(nn.Layer):\n    def __init__(\n        self,\n        dim,\n        num_heads,\n        mlp_ratio=4.0,\n        qkv_bias=False,\n        qk_scale=None,\n        drop=0.0,\n        attn_drop=0.0,\n        drop_path=0.0,\n        act_layer=nn.GELU,\n        norm_layer=\"nn.LayerNorm\",\n        epsilon=1e-6,\n    ):\n        super().__init__()\n        if isinstance(norm_layer, str):\n            self.norm1 = eval(norm_layer)(dim, epsilon=epsilon)\n            self.normkv = eval(norm_layer)(dim, epsilon=epsilon)\n        elif isinstance(norm_layer, Callable):\n            self.norm1 = norm_layer(dim)\n            self.normkv = norm_layer(dim)\n        else:\n            raise TypeError(\"The norm_layer must be str or paddle.nn.LayerNorm class\")\n        self.mixer = Attention(\n            dim,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            qk_scale=qk_scale,\n            attn_drop=attn_drop,\n            proj_drop=drop,\n        )\n\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else Identity()\n        if isinstance(norm_layer, str):\n            self.norm2 = eval(norm_layer)(dim, epsilon=epsilon)\n        elif isinstance(norm_layer, Callable):\n            self.norm2 = norm_layer(dim)\n        else:\n            raise TypeError(\"The norm_layer must be str or paddle.nn.layer.Layer class\")\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp_ratio = mlp_ratio\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=mlp_hidden_dim,\n            act_layer=act_layer,\n            drop=drop,\n        )\n\n    def forward(self, q, kv):\n        x1 = self.norm1(q + self.drop_path(self.mixer(q, kv)))\n        x = self.norm2(x1 + self.drop_path(self.mlp(x1)))\n        return x\n\n\nclass CPPDHead(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        dim,\n        out_channels,\n        num_layer=2,\n        drop_path_rate=0.1,\n        max_len=25,\n        vis_seq=50,\n        ch=False,\n        **kwargs,\n    ):\n        super(CPPDHead, self).__init__()\n\n        self.out_channels = out_channels  # none + 26 + 10\n        self.dim = dim\n        self.ch = ch\n        self.max_len = max_len + 1  # max_len + eos\n        self.char_node_embed = Embeddings(\n            d_model=dim, vocab=self.out_channels, scale_embedding=True\n        )\n        self.pos_node_embed = Embeddings(\n            d_model=dim, vocab=self.max_len, scale_embedding=True\n        )\n        dpr = np.linspace(0, drop_path_rate, num_layer + 1)\n\n        self.char_node_decoder = nn.LayerList(\n            [\n                DecoderLayer(\n                    dim=dim,\n                    num_heads=dim // 32,\n                    mlp_ratio=4.0,\n                    qkv_bias=True,\n                    drop_path=dpr[i],\n                )\n                for i in range(num_layer)\n            ]\n        )\n        self.pos_node_decoder = nn.LayerList(\n            [\n                DecoderLayer(\n                    dim=dim,\n                    num_heads=dim // 32,\n                    mlp_ratio=4.0,\n                    qkv_bias=True,\n                    drop_path=dpr[i],\n                )\n                for i in range(num_layer)\n            ]\n        )\n\n        self.edge_decoder = EdgeDecoderLayer(\n            dim=dim,\n            num_heads=dim // 32,\n            mlp_ratio=4.0,\n            qkv_bias=True,\n            drop_path=dpr[num_layer : num_layer + 1],\n        )\n\n        self.char_pos_embed = self.create_parameter(\n            shape=[1, self.max_len, dim], default_initializer=zeros_\n        )\n        self.add_parameter(\"char_pos_embed\", self.char_pos_embed)\n        self.vis_pos_embed = self.create_parameter(\n            shape=[1, vis_seq, dim], default_initializer=zeros_\n        )\n        self.add_parameter(\"vis_pos_embed\", self.vis_pos_embed)\n\n        self.char_node_fc1 = nn.Linear(dim, max_len)\n        self.pos_node_fc1 = nn.Linear(dim, self.max_len)\n\n        self.edge_fc = nn.Linear(dim, self.out_channels)\n        trunc_normal_(self.char_pos_embed)\n        trunc_normal_(self.vis_pos_embed)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                zeros_(m.bias)\n        elif isinstance(m, nn.LayerNorm):\n            zeros_(m.bias)\n            ones_(m.weight)\n\n    def forward(self, x, targets=None, epoch=0):\n        if self.training:\n            return self.forward_train(x, targets, epoch)\n        else:\n            return self.forward_test(x)\n\n    def forward_test(self, x):\n        visual_feats = x + self.vis_pos_embed\n        bs = visual_feats.shape[0]\n        pos_node_embed = (\n            self.pos_node_embed(paddle.arange(self.max_len)).unsqueeze(0)\n            + self.char_pos_embed\n        )\n        pos_node_embed = paddle.tile(pos_node_embed, [bs, 1, 1])\n        char_vis_node_query = visual_feats\n        pos_vis_node_query = paddle.concat([pos_node_embed, visual_feats], 1)\n\n        for char_decoder_layer, pos_decoder_layer in zip(\n            self.char_node_decoder, self.pos_node_decoder\n        ):\n            char_vis_node_query = char_decoder_layer(\n                char_vis_node_query, char_vis_node_query\n            )\n            pos_vis_node_query = pos_decoder_layer(\n                pos_vis_node_query, pos_vis_node_query[:, self.max_len :, :]\n            )\n        pos_node_query = pos_vis_node_query[:, : self.max_len, :]\n        char_vis_feats = char_vis_node_query\n\n        pos_node_feats = self.edge_decoder(\n            pos_node_query, char_vis_feats, char_vis_feats\n        )  # B, 26, dim\n        edge_feats = self.edge_fc(pos_node_feats)  # B, 26, 37\n        edge_logits = F.softmax(edge_feats, -1)\n\n        return edge_logits\n\n    def forward_train(self, x, targets=None, epoch=0):\n        visual_feats = x + self.vis_pos_embed\n        bs = visual_feats.shape[0]\n\n        if self.ch:\n            char_node_embed = self.char_node_embed(targets[-2])\n        else:\n            char_node_embed = self.char_node_embed(\n                paddle.arange(self.out_channels)\n            ).unsqueeze(0)\n            char_node_embed = paddle.tile(char_node_embed, [bs, 1, 1])\n        counting_char_num = char_node_embed.shape[1]\n        pos_node_embed = (\n            self.pos_node_embed(paddle.arange(self.max_len)).unsqueeze(0)\n            + self.char_pos_embed\n        )\n        pos_node_embed = paddle.tile(pos_node_embed, [bs, 1, 1])\n\n        node_feats = []\n\n        char_vis_node_query = paddle.concat([char_node_embed, visual_feats], 1)\n        pos_vis_node_query = paddle.concat([pos_node_embed, visual_feats], 1)\n\n        for char_decoder_layer, pos_decoder_layer in zip(\n            self.char_node_decoder, self.pos_node_decoder\n        ):\n            char_vis_node_query = char_decoder_layer(\n                char_vis_node_query, char_vis_node_query[:, counting_char_num:, :]\n            )\n            pos_vis_node_query = pos_decoder_layer(\n                pos_vis_node_query, pos_vis_node_query[:, self.max_len :, :]\n            )\n\n        char_node_query = char_vis_node_query[:, :counting_char_num, :]\n        pos_node_query = pos_vis_node_query[:, : self.max_len, :]\n\n        char_vis_feats = char_vis_node_query[:, counting_char_num:, :]\n        char_node_feats1 = self.char_node_fc1(char_node_query)\n\n        pos_node_feats1 = self.pos_node_fc1(pos_node_query)\n        diag_mask = (\n            paddle.eye(pos_node_feats1.shape[1])\n            .unsqueeze(0)\n            .tile([pos_node_feats1.shape[0], 1, 1])\n        )\n        pos_node_feats1 = (pos_node_feats1 * diag_mask).sum(-1)\n\n        node_feats.append(char_node_feats1)\n        node_feats.append(pos_node_feats1)\n\n        pos_node_feats = self.edge_decoder(\n            pos_node_query, char_vis_feats, char_vis_feats\n        )  # B, 26, dim\n        edge_feats = self.edge_fc(pos_node_feats)  # B, 26, 37\n\n        return node_feats, edge_feats\n", "ppocr/modeling/heads/det_ct_head.py": "# copyright (c) 2019 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport paddle\nfrom paddle import nn\nimport paddle.nn.functional as F\nfrom paddle import ParamAttr\n\nimport math\nfrom paddle.nn.initializer import TruncatedNormal, Constant, Normal\n\nones_ = Constant(value=1.0)\nzeros_ = Constant(value=0.0)\n\n\nclass CT_Head(nn.Layer):\n    def __init__(\n        self, in_channels, hidden_dim, num_classes, loss_kernel=None, loss_loc=None\n    ):\n        super(CT_Head, self).__init__()\n        self.conv1 = nn.Conv2D(\n            in_channels, hidden_dim, kernel_size=3, stride=1, padding=1\n        )\n        self.bn1 = nn.BatchNorm2D(hidden_dim)\n        self.relu1 = nn.ReLU()\n\n        self.conv2 = nn.Conv2D(\n            hidden_dim, num_classes, kernel_size=1, stride=1, padding=0\n        )\n\n        for m in self.sublayers():\n            if isinstance(m, nn.Conv2D):\n                n = m._kernel_size[0] * m._kernel_size[1] * m._out_channels\n                normal_ = Normal(mean=0.0, std=math.sqrt(2.0 / n))\n                normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2D):\n                zeros_(m.bias)\n                ones_(m.weight)\n\n    def _upsample(self, x, scale=1):\n        return F.upsample(x, scale_factor=scale, mode=\"bilinear\")\n\n    def forward(self, f, targets=None):\n        out = self.conv1(f)\n        out = self.relu1(self.bn1(out))\n        out = self.conv2(out)\n\n        if self.training:\n            out = self._upsample(out, scale=4)\n            return {\"maps\": out}\n        else:\n            score = F.sigmoid(out[:, 0, :, :])\n            return {\"maps\": out, \"score\": score}\n", "ppocr/modeling/heads/cls_head.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport paddle\nfrom paddle import nn, ParamAttr\nimport paddle.nn.functional as F\n\n\nclass ClsHead(nn.Layer):\n    \"\"\"\n    Class orientation\n\n    Args:\n\n        params(dict): super parameters for build Class network\n    \"\"\"\n\n    def __init__(self, in_channels, class_dim, **kwargs):\n        super(ClsHead, self).__init__()\n        self.pool = nn.AdaptiveAvgPool2D(1)\n        stdv = 1.0 / math.sqrt(in_channels * 1.0)\n        self.fc = nn.Linear(\n            in_channels,\n            class_dim,\n            weight_attr=ParamAttr(\n                name=\"fc_0.w_0\", initializer=nn.initializer.Uniform(-stdv, stdv)\n            ),\n            bias_attr=ParamAttr(name=\"fc_0.b_0\"),\n        )\n\n    def forward(self, x, targets=None):\n        x = self.pool(x)\n        x = paddle.reshape(x, shape=[x.shape[0], x.shape[1]])\n        x = self.fc(x)\n        if not self.training:\n            x = F.softmax(x, axis=1)\n        return x\n", "ppocr/modeling/heads/rec_ctc_head.py": "# copyright (c) 2019 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\n\nimport paddle\nfrom paddle import ParamAttr, nn\nfrom paddle.nn import functional as F\n\n\ndef get_para_bias_attr(l2_decay, k):\n    regularizer = paddle.regularizer.L2Decay(l2_decay)\n    stdv = 1.0 / math.sqrt(k * 1.0)\n    initializer = nn.initializer.Uniform(-stdv, stdv)\n    weight_attr = ParamAttr(regularizer=regularizer, initializer=initializer)\n    bias_attr = ParamAttr(regularizer=regularizer, initializer=initializer)\n    return [weight_attr, bias_attr]\n\n\nclass CTCHead(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        fc_decay=0.0004,\n        mid_channels=None,\n        return_feats=False,\n        **kwargs,\n    ):\n        super(CTCHead, self).__init__()\n        if mid_channels is None:\n            weight_attr, bias_attr = get_para_bias_attr(\n                l2_decay=fc_decay, k=in_channels\n            )\n            self.fc = nn.Linear(\n                in_channels, out_channels, weight_attr=weight_attr, bias_attr=bias_attr\n            )\n        else:\n            weight_attr1, bias_attr1 = get_para_bias_attr(\n                l2_decay=fc_decay, k=in_channels\n            )\n            self.fc1 = nn.Linear(\n                in_channels,\n                mid_channels,\n                weight_attr=weight_attr1,\n                bias_attr=bias_attr1,\n            )\n\n            weight_attr2, bias_attr2 = get_para_bias_attr(\n                l2_decay=fc_decay, k=mid_channels\n            )\n            self.fc2 = nn.Linear(\n                mid_channels,\n                out_channels,\n                weight_attr=weight_attr2,\n                bias_attr=bias_attr2,\n            )\n        self.out_channels = out_channels\n        self.mid_channels = mid_channels\n        self.return_feats = return_feats\n\n    def forward(self, x, targets=None):\n        if self.mid_channels is None:\n            predicts = self.fc(x)\n        else:\n            x = self.fc1(x)\n            predicts = self.fc2(x)\n\n        if self.return_feats:\n            result = (x, predicts)\n        else:\n            result = predicts\n        if not self.training:\n            predicts = F.softmax(predicts, axis=2)\n            result = predicts\n\n        return result\n", "ppocr/modeling/heads/kie_sdmgr_head.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# reference from : https://github.com/open-mmlab/mmocr/blob/main/mmocr/models/kie/heads/sdmgr_head.py\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport paddle\nfrom paddle import nn\nimport paddle.nn.functional as F\nfrom paddle import ParamAttr\n\n\nclass SDMGRHead(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        num_chars=92,\n        visual_dim=16,\n        fusion_dim=1024,\n        node_input=32,\n        node_embed=256,\n        edge_input=5,\n        edge_embed=256,\n        num_gnn=2,\n        num_classes=26,\n        bidirectional=False,\n    ):\n        super().__init__()\n\n        self.fusion = Block([visual_dim, node_embed], node_embed, fusion_dim)\n        self.node_embed = nn.Embedding(num_chars, node_input, 0)\n        hidden = node_embed // 2 if bidirectional else node_embed\n        self.rnn = nn.LSTM(input_size=node_input, hidden_size=hidden, num_layers=1)\n        self.edge_embed = nn.Linear(edge_input, edge_embed)\n        self.gnn_layers = nn.LayerList(\n            [GNNLayer(node_embed, edge_embed) for _ in range(num_gnn)]\n        )\n        self.node_cls = nn.Linear(node_embed, num_classes)\n        self.edge_cls = nn.Linear(edge_embed, 2)\n\n    def forward(self, input, targets):\n        relations, texts, x = input\n        node_nums, char_nums = [], []\n        for text in texts:\n            node_nums.append(text.shape[0])\n            char_nums.append(paddle.sum((text > -1).astype(int), axis=-1))\n\n        max_num = max([char_num.max() for char_num in char_nums])\n        all_nodes = paddle.concat(\n            [\n                paddle.concat(\n                    [text, paddle.zeros((text.shape[0], max_num - text.shape[1]))], -1\n                )\n                for text in texts\n            ]\n        )\n        temp = paddle.clip(all_nodes, min=0).astype(int)\n        embed_nodes = self.node_embed(temp)\n        rnn_nodes, _ = self.rnn(embed_nodes)\n\n        b, h, w = rnn_nodes.shape\n        nodes = paddle.zeros([b, w])\n        all_nums = paddle.concat(char_nums)\n        valid = paddle.nonzero((all_nums > 0).astype(int))\n        temp_all_nums = (paddle.gather(all_nums, valid) - 1).unsqueeze(-1).unsqueeze(-1)\n        temp_all_nums = paddle.expand(\n            temp_all_nums,\n            [temp_all_nums.shape[0], temp_all_nums.shape[1], rnn_nodes.shape[-1]],\n        )\n        temp_all_nodes = paddle.gather(rnn_nodes, valid)\n        N, C, A = temp_all_nodes.shape\n        one_hot = F.one_hot(temp_all_nums[:, 0, :], num_classes=C).transpose([0, 2, 1])\n        one_hot = paddle.multiply(temp_all_nodes, one_hot.astype(\"float32\")).sum(\n            axis=1, keepdim=True\n        )\n        t = one_hot.expand([N, 1, A]).squeeze(1)\n        nodes = paddle.scatter(nodes, valid.squeeze(1), t)\n\n        if x is not None:\n            nodes = self.fusion([x, nodes])\n\n        all_edges = paddle.concat(\n            [rel.reshape([-1, rel.shape[-1]]) for rel in relations]\n        )\n        embed_edges = self.edge_embed(all_edges.astype(\"float32\"))\n        embed_edges = F.normalize(embed_edges)\n\n        for gnn_layer in self.gnn_layers:\n            nodes, cat_nodes = gnn_layer(nodes, embed_edges, node_nums)\n\n        node_cls, edge_cls = self.node_cls(nodes), self.edge_cls(cat_nodes)\n        return node_cls, edge_cls\n\n\nclass GNNLayer(nn.Layer):\n    def __init__(self, node_dim=256, edge_dim=256):\n        super().__init__()\n        self.in_fc = nn.Linear(node_dim * 2 + edge_dim, node_dim)\n        self.coef_fc = nn.Linear(node_dim, 1)\n        self.out_fc = nn.Linear(node_dim, node_dim)\n        self.relu = nn.ReLU()\n\n    def forward(self, nodes, edges, nums):\n        start, cat_nodes = 0, []\n        for num in nums:\n            sample_nodes = nodes[start : start + num]\n            cat_nodes.append(\n                paddle.concat(\n                    [\n                        paddle.expand(sample_nodes.unsqueeze(1), [-1, num, -1]),\n                        paddle.expand(sample_nodes.unsqueeze(0), [num, -1, -1]),\n                    ],\n                    -1,\n                ).reshape([num**2, -1])\n            )\n            start += num\n        cat_nodes = paddle.concat([paddle.concat(cat_nodes), edges], -1)\n        cat_nodes = self.relu(self.in_fc(cat_nodes))\n        coefs = self.coef_fc(cat_nodes)\n\n        start, residuals = 0, []\n        for num in nums:\n            residual = F.softmax(\n                -paddle.eye(num).unsqueeze(-1) * 1e9\n                + coefs[start : start + num**2].reshape([num, num, -1]),\n                1,\n            )\n            residuals.append(\n                (\n                    residual * cat_nodes[start : start + num**2].reshape([num, num, -1])\n                ).sum(1)\n            )\n            start += num**2\n\n        nodes += self.relu(self.out_fc(paddle.concat(residuals)))\n        return [nodes, cat_nodes]\n\n\nclass Block(nn.Layer):\n    def __init__(\n        self,\n        input_dims,\n        output_dim,\n        mm_dim=1600,\n        chunks=20,\n        rank=15,\n        shared=False,\n        dropout_input=0.0,\n        dropout_pre_lin=0.0,\n        dropout_output=0.0,\n        pos_norm=\"before_cat\",\n    ):\n        super().__init__()\n        self.rank = rank\n        self.dropout_input = dropout_input\n        self.dropout_pre_lin = dropout_pre_lin\n        self.dropout_output = dropout_output\n        assert pos_norm in [\"before_cat\", \"after_cat\"]\n        self.pos_norm = pos_norm\n        # Modules\n        self.linear0 = nn.Linear(input_dims[0], mm_dim)\n        self.linear1 = self.linear0 if shared else nn.Linear(input_dims[1], mm_dim)\n        self.merge_linears0 = nn.LayerList()\n        self.merge_linears1 = nn.LayerList()\n        self.chunks = self.chunk_sizes(mm_dim, chunks)\n        for size in self.chunks:\n            ml0 = nn.Linear(size, size * rank)\n            self.merge_linears0.append(ml0)\n            ml1 = ml0 if shared else nn.Linear(size, size * rank)\n            self.merge_linears1.append(ml1)\n        self.linear_out = nn.Linear(mm_dim, output_dim)\n\n    def forward(self, x):\n        x0 = self.linear0(x[0])\n        x1 = self.linear1(x[1])\n        bs = x1.shape[0]\n        if self.dropout_input > 0:\n            x0 = F.dropout(x0, p=self.dropout_input, training=self.training)\n            x1 = F.dropout(x1, p=self.dropout_input, training=self.training)\n        x0_chunks = paddle.split(x0, self.chunks, -1)\n        x1_chunks = paddle.split(x1, self.chunks, -1)\n        zs = []\n        for x0_c, x1_c, m0, m1 in zip(\n            x0_chunks, x1_chunks, self.merge_linears0, self.merge_linears1\n        ):\n            m = m0(x0_c) * m1(x1_c)  # bs x split_size*rank\n            m = m.reshape([bs, self.rank, -1])\n            z = paddle.sum(m, 1)\n            if self.pos_norm == \"before_cat\":\n                z = paddle.sqrt(F.relu(z)) - paddle.sqrt(F.relu(-z))\n                z = F.normalize(z)\n            zs.append(z)\n        z = paddle.concat(zs, 1)\n        if self.pos_norm == \"after_cat\":\n            z = paddle.sqrt(F.relu(z)) - paddle.sqrt(F.relu(-z))\n            z = F.normalize(z)\n\n        if self.dropout_pre_lin > 0:\n            z = F.dropout(z, p=self.dropout_pre_lin, training=self.training)\n        z = self.linear_out(z)\n        if self.dropout_output > 0:\n            z = F.dropout(z, p=self.dropout_output, training=self.training)\n        return z\n\n    def chunk_sizes(self, dim, chunks):\n        split_size = (dim + chunks - 1) // chunks\n        sizes_list = [split_size] * chunks\n        sizes_list[-1] = sizes_list[-1] - (sum(sizes_list) - dim)\n        return sizes_list\n", "ppocr/modeling/heads/det_fce_head.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/open-mmlab/mmocr/blob/main/mmocr/models/textdet/dense_heads/fce_head.py\n\"\"\"\n\nfrom paddle import nn\nfrom paddle import ParamAttr\nimport paddle.nn.functional as F\nfrom paddle.nn.initializer import Normal\nimport paddle\nfrom functools import partial\n\n\ndef multi_apply(func, *args, **kwargs):\n    pfunc = partial(func, **kwargs) if kwargs else func\n    map_results = map(pfunc, *args)\n    return tuple(map(list, zip(*map_results)))\n\n\nclass FCEHead(nn.Layer):\n    \"\"\"The class for implementing FCENet head.\n    FCENet(CVPR2021): Fourier Contour Embedding for Arbitrary-shaped Text\n    Detection.\n\n    [https://arxiv.org/abs/2104.10442]\n\n    Args:\n        in_channels (int): The number of input channels.\n        scales (list[int]) : The scale of each layer.\n        fourier_degree (int) : The maximum Fourier transform degree k.\n    \"\"\"\n\n    def __init__(self, in_channels, fourier_degree=5):\n        super().__init__()\n        assert isinstance(in_channels, int)\n\n        self.downsample_ratio = 1.0\n        self.in_channels = in_channels\n        self.fourier_degree = fourier_degree\n        self.out_channels_cls = 4\n        self.out_channels_reg = (2 * self.fourier_degree + 1) * 2\n\n        self.out_conv_cls = nn.Conv2D(\n            in_channels=self.in_channels,\n            out_channels=self.out_channels_cls,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            groups=1,\n            weight_attr=ParamAttr(\n                name=\"cls_weights\", initializer=Normal(mean=0.0, std=0.01)\n            ),\n            bias_attr=True,\n        )\n        self.out_conv_reg = nn.Conv2D(\n            in_channels=self.in_channels,\n            out_channels=self.out_channels_reg,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            groups=1,\n            weight_attr=ParamAttr(\n                name=\"reg_weights\", initializer=Normal(mean=0.0, std=0.01)\n            ),\n            bias_attr=True,\n        )\n\n    def forward(self, feats, targets=None):\n        cls_res, reg_res = multi_apply(self.forward_single, feats)\n        level_num = len(cls_res)\n        outs = {}\n        if not self.training:\n            for i in range(level_num):\n                tr_pred = F.softmax(cls_res[i][:, 0:2, :, :], axis=1)\n                tcl_pred = F.softmax(cls_res[i][:, 2:, :, :], axis=1)\n                outs[\"level_{}\".format(i)] = paddle.concat(\n                    [tr_pred, tcl_pred, reg_res[i]], axis=1\n                )\n        else:\n            preds = [[cls_res[i], reg_res[i]] for i in range(level_num)]\n            outs[\"levels\"] = preds\n        return outs\n\n    def forward_single(self, x):\n        cls_predict = self.out_conv_cls(x)\n        reg_predict = self.out_conv_reg(x)\n        return cls_predict, reg_predict\n", "ppocr/modeling/heads/rec_nrtr_head.py": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport math\nimport paddle\nfrom paddle import nn\nimport paddle.nn.functional as F\nfrom paddle.nn import Dropout, LayerNorm\nimport numpy as np\nfrom ppocr.modeling.backbones.rec_svtrnet import Mlp, zeros_\nfrom paddle.nn.initializer import XavierNormal as xavier_normal_\n\n\nclass Transformer(nn.Layer):\n    \"\"\"A transformer model. User is able to modify the attributes as needed. The architechture\n    is based on the paper \"Attention Is All You Need\". Ashish Vaswani, Noam Shazeer,\n    Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and\n    Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information\n    Processing Systems, pages 6000-6010.\n\n    Args:\n        d_model: the number of expected features in the encoder/decoder inputs (default=512).\n        nhead: the number of heads in the multiheadattention models (default=8).\n        num_encoder_layers: the number of sub-encoder-layers in the encoder (default=6).\n        num_decoder_layers: the number of sub-decoder-layers in the decoder (default=6).\n        dim_feedforward: the dimension of the feedforward network model (default=2048).\n        dropout: the dropout value (default=0.1).\n        custom_encoder: custom encoder (default=None).\n        custom_decoder: custom decoder (default=None).\n    \"\"\"\n\n    def __init__(\n        self,\n        d_model=512,\n        nhead=8,\n        num_encoder_layers=6,\n        beam_size=0,\n        num_decoder_layers=6,\n        max_len=25,\n        dim_feedforward=1024,\n        attention_dropout_rate=0.0,\n        residual_dropout_rate=0.1,\n        in_channels=0,\n        out_channels=0,\n        scale_embedding=True,\n    ):\n        super(Transformer, self).__init__()\n        self.out_channels = out_channels + 1\n        self.max_len = max_len\n        self.embedding = Embeddings(\n            d_model=d_model,\n            vocab=self.out_channels,\n            padding_idx=0,\n            scale_embedding=scale_embedding,\n        )\n        self.positional_encoding = PositionalEncoding(\n            dropout=residual_dropout_rate, dim=d_model\n        )\n\n        if num_encoder_layers > 0:\n            self.encoder = nn.LayerList(\n                [\n                    TransformerBlock(\n                        d_model,\n                        nhead,\n                        dim_feedforward,\n                        attention_dropout_rate,\n                        residual_dropout_rate,\n                        with_self_attn=True,\n                        with_cross_attn=False,\n                    )\n                    for i in range(num_encoder_layers)\n                ]\n            )\n        else:\n            self.encoder = None\n\n        self.decoder = nn.LayerList(\n            [\n                TransformerBlock(\n                    d_model,\n                    nhead,\n                    dim_feedforward,\n                    attention_dropout_rate,\n                    residual_dropout_rate,\n                    with_self_attn=True,\n                    with_cross_attn=True,\n                )\n                for i in range(num_decoder_layers)\n            ]\n        )\n\n        self.beam_size = beam_size\n        self.d_model = d_model\n        self.nhead = nhead\n        self.tgt_word_prj = nn.Linear(d_model, self.out_channels, bias_attr=False)\n        w0 = np.random.normal(\n            0.0, d_model**-0.5, (d_model, self.out_channels)\n        ).astype(np.float32)\n        self.tgt_word_prj.weight.set_value(w0)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            xavier_normal_(m.weight)\n            if m.bias is not None:\n                zeros_(m.bias)\n\n    def forward_train(self, src, tgt):\n        tgt = tgt[:, :-1]\n\n        tgt = self.embedding(tgt)\n        tgt = self.positional_encoding(tgt)\n        tgt_mask = self.generate_square_subsequent_mask(tgt.shape[1])\n\n        if self.encoder is not None:\n            src = self.positional_encoding(src)\n            for encoder_layer in self.encoder:\n                src = encoder_layer(src)\n            memory = src  # B N C\n        else:\n            memory = src  # B N C\n        for decoder_layer in self.decoder:\n            tgt = decoder_layer(tgt, memory, self_mask=tgt_mask)\n        output = tgt\n        logit = self.tgt_word_prj(output)\n        return logit\n\n    def forward(self, src, targets=None):\n        \"\"\"Take in and process masked source/target sequences.\n        Args:\n            src: the sequence to the encoder (required).\n            tgt: the sequence to the decoder (required).\n        Shape:\n            - src: :math:`(B, sN, C)`.\n            - tgt: :math:`(B, tN, C)`.\n        Examples:\n            >>> output = transformer_model(src, tgt)\n        \"\"\"\n\n        if self.training:\n            max_len = targets[1].max()\n            tgt = targets[0][:, : 2 + max_len]\n            return self.forward_train(src, tgt)\n        else:\n            if self.beam_size > 0:\n                return self.forward_beam(src)\n            else:\n                return self.forward_test(src)\n\n    def forward_test(self, src):\n        bs = src.shape[0]\n        if self.encoder is not None:\n            src = self.positional_encoding(src)\n            for encoder_layer in self.encoder:\n                src = encoder_layer(src)\n            memory = src  # B N C\n        else:\n            memory = src\n        dec_seq = paddle.full((bs, 1), 2, dtype=paddle.int64)\n        dec_prob = paddle.full((bs, 1), 1.0, dtype=paddle.float32)\n        for len_dec_seq in range(1, paddle.to_tensor(self.max_len)):\n            dec_seq_embed = self.embedding(dec_seq)\n            dec_seq_embed = self.positional_encoding(dec_seq_embed)\n            tgt_mask = self.generate_square_subsequent_mask(dec_seq_embed.shape[1])\n            tgt = dec_seq_embed\n            for decoder_layer in self.decoder:\n                tgt = decoder_layer(tgt, memory, self_mask=tgt_mask)\n            dec_output = tgt\n            dec_output = dec_output[:, -1, :]\n            word_prob = F.softmax(self.tgt_word_prj(dec_output), axis=-1)\n            preds_idx = paddle.argmax(word_prob, axis=-1)\n            if paddle.equal_all(\n                preds_idx, paddle.full(preds_idx.shape, 3, dtype=\"int64\")\n            ):\n                break\n            preds_prob = paddle.max(word_prob, axis=-1)\n            dec_seq = paddle.concat(\n                [dec_seq, paddle.reshape(preds_idx, [-1, 1])], axis=1\n            )\n            dec_prob = paddle.concat(\n                [dec_prob, paddle.reshape(preds_prob, [-1, 1])], axis=1\n            )\n        return [dec_seq, dec_prob]\n\n    def forward_beam(self, images):\n        \"\"\"Translation work in one batch\"\"\"\n\n        def get_inst_idx_to_tensor_position_map(inst_idx_list):\n            \"\"\"Indicate the position of an instance in a tensor.\"\"\"\n            return {\n                inst_idx: tensor_position\n                for tensor_position, inst_idx in enumerate(inst_idx_list)\n            }\n\n        def collect_active_part(\n            beamed_tensor, curr_active_inst_idx, n_prev_active_inst, n_bm\n        ):\n            \"\"\"Collect tensor parts associated to active instances.\"\"\"\n\n            beamed_tensor_shape = beamed_tensor.shape\n            n_curr_active_inst = len(curr_active_inst_idx)\n            new_shape = (\n                n_curr_active_inst * n_bm,\n                beamed_tensor_shape[1],\n                beamed_tensor_shape[2],\n            )\n\n            beamed_tensor = beamed_tensor.reshape([n_prev_active_inst, -1])\n            beamed_tensor = beamed_tensor.index_select(curr_active_inst_idx, axis=0)\n            beamed_tensor = beamed_tensor.reshape(new_shape)\n\n            return beamed_tensor\n\n        def collate_active_info(\n            src_enc, inst_idx_to_position_map, active_inst_idx_list\n        ):\n            # Sentences which are still active are collected,\n            # so the decoder will not run on completed sentences.\n\n            n_prev_active_inst = len(inst_idx_to_position_map)\n            active_inst_idx = [\n                inst_idx_to_position_map[k] for k in active_inst_idx_list\n            ]\n            active_inst_idx = paddle.to_tensor(active_inst_idx, dtype=\"int64\")\n            active_src_enc = collect_active_part(\n                src_enc.transpose([1, 0, 2]), active_inst_idx, n_prev_active_inst, n_bm\n            ).transpose([1, 0, 2])\n            active_inst_idx_to_position_map = get_inst_idx_to_tensor_position_map(\n                active_inst_idx_list\n            )\n            return active_src_enc, active_inst_idx_to_position_map\n\n        def beam_decode_step(\n            inst_dec_beams, len_dec_seq, enc_output, inst_idx_to_position_map, n_bm\n        ):\n            \"\"\"Decode and update beam status, and then return active beam idx\"\"\"\n\n            def prepare_beam_dec_seq(inst_dec_beams, len_dec_seq):\n                dec_partial_seq = [\n                    b.get_current_state() for b in inst_dec_beams if not b.done\n                ]\n                dec_partial_seq = paddle.stack(dec_partial_seq)\n                dec_partial_seq = dec_partial_seq.reshape([-1, len_dec_seq])\n                return dec_partial_seq\n\n            def predict_word(dec_seq, enc_output, n_active_inst, n_bm):\n                dec_seq = self.embedding(dec_seq)\n                dec_seq = self.positional_encoding(dec_seq)\n                tgt_mask = self.generate_square_subsequent_mask(dec_seq.shape[1])\n                tgt = dec_seq\n                for decoder_layer in self.decoder:\n                    tgt = decoder_layer(tgt, enc_output, self_mask=tgt_mask)\n                dec_output = tgt\n                dec_output = dec_output[:, -1, :]  # Pick the last step: (bh * bm) * d_h\n                word_prob = F.softmax(self.tgt_word_prj(dec_output), axis=1)\n                word_prob = paddle.reshape(word_prob, [n_active_inst, n_bm, -1])\n                return word_prob\n\n            def collect_active_inst_idx_list(\n                inst_beams, word_prob, inst_idx_to_position_map\n            ):\n                active_inst_idx_list = []\n                for inst_idx, inst_position in inst_idx_to_position_map.items():\n                    is_inst_complete = inst_beams[inst_idx].advance(\n                        word_prob[inst_position]\n                    )\n                    if not is_inst_complete:\n                        active_inst_idx_list += [inst_idx]\n\n                return active_inst_idx_list\n\n            n_active_inst = len(inst_idx_to_position_map)\n            dec_seq = prepare_beam_dec_seq(inst_dec_beams, len_dec_seq)\n            word_prob = predict_word(dec_seq, enc_output, n_active_inst, n_bm)\n            # Update the beam with predicted word prob information and collect incomplete instances\n            active_inst_idx_list = collect_active_inst_idx_list(\n                inst_dec_beams, word_prob, inst_idx_to_position_map\n            )\n            return active_inst_idx_list\n\n        def collect_hypothesis_and_scores(inst_dec_beams, n_best):\n            all_hyp, all_scores = [], []\n            for inst_idx in range(len(inst_dec_beams)):\n                scores, tail_idxs = inst_dec_beams[inst_idx].sort_scores()\n                all_scores += [scores[:n_best]]\n                hyps = [\n                    inst_dec_beams[inst_idx].get_hypothesis(i)\n                    for i in tail_idxs[:n_best]\n                ]\n                all_hyp += [hyps]\n            return all_hyp, all_scores\n\n        with paddle.no_grad():\n            # -- Encode\n            if self.encoder is not None:\n                src = self.positional_encoding(images)\n                src_enc = self.encoder(src)\n            else:\n                src_enc = images\n\n            n_bm = self.beam_size\n            src_shape = src_enc.shape\n            inst_dec_beams = [Beam(n_bm) for _ in range(1)]\n            active_inst_idx_list = list(range(1))\n            # Repeat data for beam search\n            src_enc = paddle.tile(src_enc, [1, n_bm, 1])\n            inst_idx_to_position_map = get_inst_idx_to_tensor_position_map(\n                active_inst_idx_list\n            )\n            # Decode\n            for len_dec_seq in range(1, paddle.to_tensor(self.max_len)):\n                src_enc_copy = src_enc.clone()\n                active_inst_idx_list = beam_decode_step(\n                    inst_dec_beams,\n                    len_dec_seq,\n                    src_enc_copy,\n                    inst_idx_to_position_map,\n                    n_bm,\n                )\n                if not active_inst_idx_list:\n                    break  # all instances have finished their path to <EOS>\n                src_enc, inst_idx_to_position_map = collate_active_info(\n                    src_enc_copy, inst_idx_to_position_map, active_inst_idx_list\n                )\n        batch_hyp, batch_scores = collect_hypothesis_and_scores(inst_dec_beams, 1)\n        result_hyp = []\n        hyp_scores = []\n        for bs_hyp, score in zip(batch_hyp, batch_scores):\n            l = len(bs_hyp[0])\n            bs_hyp_pad = bs_hyp[0] + [3] * (25 - l)\n            result_hyp.append(bs_hyp_pad)\n            score = float(score) / l\n            hyp_score = [score for _ in range(25)]\n            hyp_scores.append(hyp_score)\n        return [\n            paddle.to_tensor(np.array(result_hyp), dtype=paddle.int64),\n            paddle.to_tensor(hyp_scores),\n        ]\n\n    def generate_square_subsequent_mask(self, sz):\n        \"\"\"Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n        Unmasked positions are filled with float(0.0).\n        \"\"\"\n        mask = paddle.zeros([sz, sz], dtype=\"float32\")\n        mask_inf = paddle.triu(\n            paddle.full(shape=[sz, sz], dtype=\"float32\", fill_value=\"-inf\"), diagonal=1\n        )\n        mask = mask + mask_inf\n        return mask.unsqueeze([0, 1])\n\n\nclass MultiheadAttention(nn.Layer):\n    \"\"\"Allows the model to jointly attend to information\n    from different representation subspaces.\n    See reference: Attention Is All You Need\n\n    .. math::\n        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n        \\text{where} head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n\n    Args:\n        embed_dim: total dimension of the model\n        num_heads: parallel attention layers, or heads\n\n    \"\"\"\n\n    def __init__(self, embed_dim, num_heads, dropout=0.0, self_attn=False):\n        super(MultiheadAttention, self).__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        # self.dropout = dropout\n        self.head_dim = embed_dim // num_heads\n        assert (\n            self.head_dim * num_heads == self.embed_dim\n        ), \"embed_dim must be divisible by num_heads\"\n        self.scale = self.head_dim**-0.5\n        self.self_attn = self_attn\n        if self_attn:\n            self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n        else:\n            self.q = nn.Linear(embed_dim, embed_dim)\n            self.kv = nn.Linear(embed_dim, embed_dim * 2)\n        self.attn_drop = nn.Dropout(dropout)\n        self.out_proj = nn.Linear(embed_dim, embed_dim)\n\n    def forward(self, query, key=None, attn_mask=None):\n        qN = query.shape[1]\n\n        if self.self_attn:\n            qkv = (\n                self.qkv(query)\n                .reshape((0, qN, 3, self.num_heads, self.head_dim))\n                .transpose((2, 0, 3, 1, 4))\n            )\n            q, k, v = qkv[0], qkv[1], qkv[2]\n        else:\n            kN = key.shape[1]\n            q = (\n                self.q(query)\n                .reshape([0, qN, self.num_heads, self.head_dim])\n                .transpose([0, 2, 1, 3])\n            )\n            kv = (\n                self.kv(key)\n                .reshape((0, kN, 2, self.num_heads, self.head_dim))\n                .transpose((2, 0, 3, 1, 4))\n            )\n            k, v = kv[0], kv[1]\n\n        attn = (q.matmul(k.transpose((0, 1, 3, 2)))) * self.scale\n\n        if attn_mask is not None:\n            attn += attn_mask\n\n        attn = F.softmax(attn, axis=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn.matmul(v)).transpose((0, 2, 1, 3)).reshape((0, qN, self.embed_dim))\n        x = self.out_proj(x)\n\n        return x\n\n\nclass TransformerBlock(nn.Layer):\n    def __init__(\n        self,\n        d_model,\n        nhead,\n        dim_feedforward=2048,\n        attention_dropout_rate=0.0,\n        residual_dropout_rate=0.1,\n        with_self_attn=True,\n        with_cross_attn=False,\n        epsilon=1e-5,\n    ):\n        super(TransformerBlock, self).__init__()\n        self.with_self_attn = with_self_attn\n        if with_self_attn:\n            self.self_attn = MultiheadAttention(\n                d_model, nhead, dropout=attention_dropout_rate, self_attn=with_self_attn\n            )\n            self.norm1 = LayerNorm(d_model, epsilon=epsilon)\n            self.dropout1 = Dropout(residual_dropout_rate)\n        self.with_cross_attn = with_cross_attn\n        if with_cross_attn:\n            self.cross_attn = (\n                MultiheadAttention(  # for self_attn of encoder or cross_attn of decoder\n                    d_model, nhead, dropout=attention_dropout_rate\n                )\n            )\n            self.norm2 = LayerNorm(d_model, epsilon=epsilon)\n            self.dropout2 = Dropout(residual_dropout_rate)\n\n        self.mlp = Mlp(\n            in_features=d_model,\n            hidden_features=dim_feedforward,\n            act_layer=nn.ReLU,\n            drop=residual_dropout_rate,\n        )\n\n        self.norm3 = LayerNorm(d_model, epsilon=epsilon)\n\n        self.dropout3 = Dropout(residual_dropout_rate)\n\n    def forward(self, tgt, memory=None, self_mask=None, cross_mask=None):\n        if self.with_self_attn:\n            tgt1 = self.self_attn(tgt, attn_mask=self_mask)\n            tgt = self.norm1(tgt + self.dropout1(tgt1))\n\n        if self.with_cross_attn:\n            tgt2 = self.cross_attn(tgt, key=memory, attn_mask=cross_mask)\n            tgt = self.norm2(tgt + self.dropout2(tgt2))\n        tgt = self.norm3(tgt + self.dropout3(self.mlp(tgt)))\n        return tgt\n\n\nclass PositionalEncoding(nn.Layer):\n    \"\"\"Inject some information about the relative or absolute position of the tokens\n        in the sequence. The positional encodings have the same dimension as\n        the embeddings, so that the two can be summed. Here, we use sine and cosine\n        functions of different frequencies.\n    .. math::\n        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n        \\text{where pos is the word position and i is the embed idx)\n    Args:\n        d_model: the embed dim (required).\n        dropout: the dropout value (default=0.1).\n        max_len: the max. length of the incoming sequence (default=5000).\n    Examples:\n        >>> pos_encoder = PositionalEncoding(d_model)\n    \"\"\"\n\n    def __init__(self, dropout, dim, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = paddle.zeros([max_len, dim])\n        position = paddle.arange(0, max_len, dtype=paddle.float32).unsqueeze(1)\n        div_term = paddle.exp(\n            paddle.arange(0, dim, 2).astype(\"float32\") * (-math.log(10000.0) / dim)\n        )\n        pe[:, 0::2] = paddle.sin(position * div_term)\n        pe[:, 1::2] = paddle.cos(position * div_term)\n        pe = paddle.unsqueeze(pe, 0)\n        pe = paddle.transpose(pe, [1, 0, 2])\n        self.register_buffer(\"pe\", pe)\n\n    def forward(self, x):\n        \"\"\"Inputs of forward function\n        Args:\n            x: the sequence fed to the positional encoder model (required).\n        Shape:\n            x: [sequence length, batch size, embed dim]\n            output: [sequence length, batch size, embed dim]\n        Examples:\n            >>> output = pos_encoder(x)\n        \"\"\"\n        x = x.transpose([1, 0, 2])\n        x = x + self.pe[: x.shape[0], :]\n        return self.dropout(x).transpose([1, 0, 2])\n\n\nclass PositionalEncoding_2d(nn.Layer):\n    \"\"\"Inject some information about the relative or absolute position of the tokens\n        in the sequence. The positional encodings have the same dimension as\n        the embeddings, so that the two can be summed. Here, we use sine and cosine\n        functions of different frequencies.\n    .. math::\n        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n        \\text{where pos is the word position and i is the embed idx)\n    Args:\n        d_model: the embed dim (required).\n        dropout: the dropout value (default=0.1).\n        max_len: the max. length of the incoming sequence (default=5000).\n    Examples:\n        >>> pos_encoder = PositionalEncoding(d_model)\n    \"\"\"\n\n    def __init__(self, dropout, dim, max_len=5000):\n        super(PositionalEncoding_2d, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = paddle.zeros([max_len, dim])\n        position = paddle.arange(0, max_len, dtype=paddle.float32).unsqueeze(1)\n        div_term = paddle.exp(\n            paddle.arange(0, dim, 2).astype(\"float32\") * (-math.log(10000.0) / dim)\n        )\n        pe[:, 0::2] = paddle.sin(position * div_term)\n        pe[:, 1::2] = paddle.cos(position * div_term)\n        pe = paddle.transpose(paddle.unsqueeze(pe, 0), [1, 0, 2])\n        self.register_buffer(\"pe\", pe)\n\n        self.avg_pool_1 = nn.AdaptiveAvgPool2D((1, 1))\n        self.linear1 = nn.Linear(dim, dim)\n        self.linear1.weight.data.fill_(1.0)\n        self.avg_pool_2 = nn.AdaptiveAvgPool2D((1, 1))\n        self.linear2 = nn.Linear(dim, dim)\n        self.linear2.weight.data.fill_(1.0)\n\n    def forward(self, x):\n        \"\"\"Inputs of forward function\n        Args:\n            x: the sequence fed to the positional encoder model (required).\n        Shape:\n            x: [sequence length, batch size, embed dim]\n            output: [sequence length, batch size, embed dim]\n        Examples:\n            >>> output = pos_encoder(x)\n        \"\"\"\n        w_pe = self.pe[: x.shape[-1], :]\n        w1 = self.linear1(self.avg_pool_1(x).squeeze()).unsqueeze(0)\n        w_pe = w_pe * w1\n        w_pe = paddle.transpose(w_pe, [1, 2, 0])\n        w_pe = paddle.unsqueeze(w_pe, 2)\n\n        h_pe = self.pe[: x.shape.shape[-2], :]\n        w2 = self.linear2(self.avg_pool_2(x).squeeze()).unsqueeze(0)\n        h_pe = h_pe * w2\n        h_pe = paddle.transpose(h_pe, [1, 2, 0])\n        h_pe = paddle.unsqueeze(h_pe, 3)\n\n        x = x + w_pe + h_pe\n        x = paddle.transpose(\n            paddle.reshape(x, [x.shape[0], x.shape[1], x.shape[2] * x.shape[3]]),\n            [2, 0, 1],\n        )\n\n        return self.dropout(x)\n\n\nclass Embeddings(nn.Layer):\n    def __init__(self, d_model, vocab, padding_idx=None, scale_embedding=True):\n        super(Embeddings, self).__init__()\n        self.embedding = nn.Embedding(vocab, d_model, padding_idx=padding_idx)\n        w0 = np.random.normal(0.0, d_model**-0.5, (vocab, d_model)).astype(np.float32)\n        self.embedding.weight.set_value(w0)\n        self.d_model = d_model\n        self.scale_embedding = scale_embedding\n\n    def forward(self, x):\n        if self.scale_embedding:\n            x = self.embedding(x)\n            return x * math.sqrt(self.d_model)\n        return self.embedding(x)\n\n\nclass Beam:\n    \"\"\"Beam search\"\"\"\n\n    def __init__(self, size, device=False):\n        self.size = size\n        self._done = False\n        # The score for each translation on the beam.\n        self.scores = paddle.zeros((size,), dtype=paddle.float32)\n        self.all_scores = []\n        # The backpointers at each time-step.\n        self.prev_ks = []\n        # The outputs at each time-step.\n        self.next_ys = [paddle.full((size,), 0, dtype=paddle.int64)]\n        self.next_ys[0][0] = 2\n\n    def get_current_state(self):\n        \"Get the outputs for the current timestep.\"\n        return self.get_tentative_hypothesis()\n\n    def get_current_origin(self):\n        \"Get the backpointers for the current timestep.\"\n        return self.prev_ks[-1]\n\n    @property\n    def done(self):\n        return self._done\n\n    def advance(self, word_prob):\n        \"Update beam status and check if finished or not.\"\n        num_words = word_prob.shape[1]\n\n        # Sum the previous scores.\n        if len(self.prev_ks) > 0:\n            beam_lk = word_prob + self.scores.unsqueeze(1).expand_as(word_prob)\n        else:\n            beam_lk = word_prob[0]\n\n        flat_beam_lk = beam_lk.reshape([-1])\n        best_scores, best_scores_id = flat_beam_lk.topk(\n            self.size, 0, True, True\n        )  # 1st sort\n        self.all_scores.append(self.scores)\n        self.scores = best_scores\n        # bestScoresId is flattened as a (beam x word) array,\n        # so we need to calculate which word and beam each score came from\n        prev_k = best_scores_id // num_words\n        self.prev_ks.append(prev_k)\n        self.next_ys.append(best_scores_id - prev_k * num_words)\n        # End condition is when top-of-beam is EOS.\n        if self.next_ys[-1][0] == 3:\n            self._done = True\n            self.all_scores.append(self.scores)\n\n        return self._done\n\n    def sort_scores(self):\n        \"Sort the scores.\"\n        return self.scores, paddle.to_tensor(\n            [i for i in range(int(self.scores.shape[0]))], dtype=\"int32\"\n        )\n\n    def get_the_best_score_and_idx(self):\n        \"Get the score of the best in the beam.\"\n        scores, ids = self.sort_scores()\n        return scores[1], ids[1]\n\n    def get_tentative_hypothesis(self):\n        \"Get the decoded sequence for the current timestep.\"\n        if len(self.next_ys) == 1:\n            dec_seq = self.next_ys[0].unsqueeze(1)\n        else:\n            _, keys = self.sort_scores()\n            hyps = [self.get_hypothesis(k) for k in keys]\n            hyps = [[2] + h for h in hyps]\n            dec_seq = paddle.to_tensor(hyps, dtype=\"int64\")\n        return dec_seq\n\n    def get_hypothesis(self, k):\n        \"\"\"Walk back to construct the full hypothesis.\"\"\"\n        hyp = []\n        for j in range(len(self.prev_ks) - 1, -1, -1):\n            hyp.append(self.next_ys[j + 1][k])\n            k = self.prev_ks[j][k]\n        return list(map(lambda x: x.item(), hyp[::-1]))\n", "ppocr/modeling/heads/rec_parseq_head.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Code was based on https://github.com/baudm/parseq/blob/main/strhub/models/parseq/system.py\n# reference: https://arxiv.org/abs/2207.06966\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport paddle\nfrom paddle import nn, ParamAttr\nfrom paddle.nn import functional as F\nimport numpy as np\nfrom .self_attention import WrapEncoderForFeature\nfrom .self_attention import WrapEncoder\nfrom collections import OrderedDict\nfrom typing import Optional\nimport copy\nfrom itertools import permutations\n\n\nclass DecoderLayer(paddle.nn.Layer):\n    \"\"\"A Transformer decoder layer supporting two-stream attention (XLNet)\n    This implements a pre-LN decoder, as opposed to the post-LN default in PyTorch.\"\"\"\n\n    def __init__(\n        self,\n        d_model,\n        nhead,\n        dim_feedforward=2048,\n        dropout=0.1,\n        activation=\"gelu\",\n        layer_norm_eps=1e-05,\n    ):\n        super().__init__()\n        self.self_attn = paddle.nn.MultiHeadAttention(\n            d_model, nhead, dropout=dropout, need_weights=True\n        )  # paddle.nn.MultiHeadAttention\u9ed8\u8ba4\u4e3abatch_first\u6a21\u5f0f\n        self.cross_attn = paddle.nn.MultiHeadAttention(\n            d_model, nhead, dropout=dropout, need_weights=True\n        )\n        self.linear1 = paddle.nn.Linear(\n            in_features=d_model, out_features=dim_feedforward\n        )\n        self.dropout = paddle.nn.Dropout(p=dropout)\n        self.linear2 = paddle.nn.Linear(\n            in_features=dim_feedforward, out_features=d_model\n        )\n        self.norm1 = paddle.nn.LayerNorm(\n            normalized_shape=d_model, epsilon=layer_norm_eps\n        )\n        self.norm2 = paddle.nn.LayerNorm(\n            normalized_shape=d_model, epsilon=layer_norm_eps\n        )\n        self.norm_q = paddle.nn.LayerNorm(\n            normalized_shape=d_model, epsilon=layer_norm_eps\n        )\n        self.norm_c = paddle.nn.LayerNorm(\n            normalized_shape=d_model, epsilon=layer_norm_eps\n        )\n        self.dropout1 = paddle.nn.Dropout(p=dropout)\n        self.dropout2 = paddle.nn.Dropout(p=dropout)\n        self.dropout3 = paddle.nn.Dropout(p=dropout)\n        if activation == \"gelu\":\n            self.activation = paddle.nn.GELU()\n\n    def __setstate__(self, state):\n        if \"activation\" not in state:\n            state[\"activation\"] = paddle.nn.functional.gelu\n        super().__setstate__(state)\n\n    def forward_stream(\n        self, tgt, tgt_norm, tgt_kv, memory, tgt_mask, tgt_key_padding_mask\n    ):\n        \"\"\"Forward pass for a single stream (i.e. content or query)\n        tgt_norm is just a LayerNorm'd tgt. Added as a separate parameter for efficiency.\n        Both tgt_kv and memory are expected to be LayerNorm'd too.\n        memory is LayerNorm'd by ViT.\n        \"\"\"\n        if tgt_key_padding_mask is not None:\n            tgt_mask1 = (tgt_mask != float(\"-inf\"))[None, None, :, :] & (\n                tgt_key_padding_mask[:, None, None, :] == False\n            )\n            tgt2, sa_weights = self.self_attn(\n                tgt_norm, tgt_kv, tgt_kv, attn_mask=tgt_mask1\n            )\n        else:\n            tgt2, sa_weights = self.self_attn(\n                tgt_norm, tgt_kv, tgt_kv, attn_mask=tgt_mask\n            )\n\n        tgt = tgt + self.dropout1(tgt2)\n        tgt2, ca_weights = self.cross_attn(self.norm1(tgt), memory, memory)\n        tgt = tgt + self.dropout2(tgt2)\n        tgt2 = self.linear2(\n            self.dropout(self.activation(self.linear1(self.norm2(tgt))))\n        )\n        tgt = tgt + self.dropout3(tgt2)\n        return tgt, sa_weights, ca_weights\n\n    def forward(\n        self,\n        query,\n        content,\n        memory,\n        query_mask=None,\n        content_mask=None,\n        content_key_padding_mask=None,\n        update_content=True,\n    ):\n        query_norm = self.norm_q(query)\n        content_norm = self.norm_c(content)\n        query = self.forward_stream(\n            query,\n            query_norm,\n            content_norm,\n            memory,\n            query_mask,\n            content_key_padding_mask,\n        )[0]\n        if update_content:\n            content = self.forward_stream(\n                content,\n                content_norm,\n                content_norm,\n                memory,\n                content_mask,\n                content_key_padding_mask,\n            )[0]\n        return query, content\n\n\ndef get_clones(module, N):\n    return paddle.nn.LayerList([copy.deepcopy(module) for i in range(N)])\n\n\nclass Decoder(paddle.nn.Layer):\n    __constants__ = [\"norm\"]\n\n    def __init__(self, decoder_layer, num_layers, norm):\n        super().__init__()\n        self.layers = get_clones(decoder_layer, num_layers)\n        self.num_layers = num_layers\n        self.norm = norm\n\n    def forward(\n        self,\n        query,\n        content,\n        memory,\n        query_mask: Optional[paddle.Tensor] = None,\n        content_mask: Optional[paddle.Tensor] = None,\n        content_key_padding_mask: Optional[paddle.Tensor] = None,\n    ):\n        for i, mod in enumerate(self.layers):\n            last = i == len(self.layers) - 1\n            query, content = mod(\n                query,\n                content,\n                memory,\n                query_mask,\n                content_mask,\n                content_key_padding_mask,\n                update_content=not last,\n            )\n        query = self.norm(query)\n        return query\n\n\nclass TokenEmbedding(paddle.nn.Layer):\n    def __init__(self, charset_size: int, embed_dim: int):\n        super().__init__()\n        self.embedding = paddle.nn.Embedding(\n            num_embeddings=charset_size, embedding_dim=embed_dim\n        )\n        self.embed_dim = embed_dim\n\n    def forward(self, tokens: paddle.Tensor):\n        return math.sqrt(self.embed_dim) * self.embedding(tokens.astype(paddle.int64))\n\n\ndef trunc_normal_init(param, **kwargs):\n    initializer = nn.initializer.TruncatedNormal(**kwargs)\n    initializer(param, param.block)\n\n\ndef constant_init(param, **kwargs):\n    initializer = nn.initializer.Constant(**kwargs)\n    initializer(param, param.block)\n\n\ndef kaiming_normal_init(param, **kwargs):\n    initializer = nn.initializer.KaimingNormal(**kwargs)\n    initializer(param, param.block)\n\n\nclass ParseQHead(nn.Layer):\n    def __init__(\n        self,\n        out_channels,\n        max_text_length,\n        embed_dim,\n        dec_num_heads,\n        dec_mlp_ratio,\n        dec_depth,\n        perm_num,\n        perm_forward,\n        perm_mirrored,\n        decode_ar,\n        refine_iters,\n        dropout,\n        **kwargs,\n    ):\n        super().__init__()\n\n        self.bos_id = out_channels - 2\n        self.eos_id = 0\n        self.pad_id = out_channels - 1\n\n        self.max_label_length = max_text_length\n        self.decode_ar = decode_ar\n        self.refine_iters = refine_iters\n        decoder_layer = DecoderLayer(\n            embed_dim, dec_num_heads, embed_dim * dec_mlp_ratio, dropout\n        )\n        self.decoder = Decoder(\n            decoder_layer,\n            num_layers=dec_depth,\n            norm=paddle.nn.LayerNorm(normalized_shape=embed_dim),\n        )\n        self.rng = np.random.default_rng()\n        self.max_gen_perms = perm_num // 2 if perm_mirrored else perm_num\n        self.perm_forward = perm_forward\n        self.perm_mirrored = perm_mirrored\n        self.head = paddle.nn.Linear(\n            in_features=embed_dim, out_features=out_channels - 2\n        )\n        self.text_embed = TokenEmbedding(out_channels, embed_dim)\n        self.pos_queries = paddle.create_parameter(\n            shape=paddle.empty(shape=[1, max_text_length + 1, embed_dim]).shape,\n            dtype=paddle.empty(shape=[1, max_text_length + 1, embed_dim]).numpy().dtype,\n            default_initializer=paddle.nn.initializer.Assign(\n                paddle.empty(shape=[1, max_text_length + 1, embed_dim])\n            ),\n        )\n        self.pos_queries.stop_gradient = not True\n        self.dropout = paddle.nn.Dropout(p=dropout)\n        self._device = self.parameters()[0].place\n        trunc_normal_init(self.pos_queries, std=0.02)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, paddle.nn.Linear):\n            trunc_normal_init(m.weight, std=0.02)\n            if m.bias is not None:\n                constant_init(m.bias, value=0.0)\n        elif isinstance(m, paddle.nn.Embedding):\n            trunc_normal_init(m.weight, std=0.02)\n            if m._padding_idx is not None:\n                m.weight.data[m._padding_idx].zero_()\n        elif isinstance(m, paddle.nn.Conv2D):\n            kaiming_normal_init(m.weight, fan_in=None, nonlinearity=\"relu\")\n            if m.bias is not None:\n                constant_init(m.bias, value=0.0)\n        elif isinstance(\n            m, (paddle.nn.LayerNorm, paddle.nn.BatchNorm2D, paddle.nn.GroupNorm)\n        ):\n            constant_init(m.weight, value=1.0)\n            constant_init(m.bias, value=0.0)\n\n    def no_weight_decay(self):\n        param_names = {\"text_embed.embedding.weight\", \"pos_queries\"}\n        enc_param_names = {(\"encoder.\" + n) for n in self.encoder.no_weight_decay()}\n        return param_names.union(enc_param_names)\n\n    def encode(self, img):\n        return self.encoder(img)\n\n    def decode(\n        self,\n        tgt,\n        memory,\n        tgt_mask=None,\n        tgt_padding_mask=None,\n        tgt_query=None,\n        tgt_query_mask=None,\n    ):\n        N, L = tgt.shape\n        null_ctx = self.text_embed(tgt[:, :1])\n        if L != 1:\n            tgt_emb = self.pos_queries[:, : L - 1] + self.text_embed(tgt[:, 1:])\n            tgt_emb = self.dropout(paddle.concat(x=[null_ctx, tgt_emb], axis=1))\n        else:\n            tgt_emb = self.dropout(null_ctx)\n        if tgt_query is None:\n            tgt_query = self.pos_queries[:, :L].expand(shape=[N, -1, -1])\n        tgt_query = self.dropout(tgt_query)\n        return self.decoder(\n            tgt_query, tgt_emb, memory, tgt_query_mask, tgt_mask, tgt_padding_mask\n        )\n\n    def forward_test(self, memory, max_length=None):\n        testing = max_length is None\n        max_length = (\n            self.max_label_length\n            if max_length is None\n            else min(max_length, self.max_label_length)\n        )\n        bs = memory.shape[0]\n        num_steps = max_length + 1\n\n        pos_queries = self.pos_queries[:, :num_steps].expand(shape=[bs, -1, -1])\n        tgt_mask = query_mask = paddle.triu(\n            x=paddle.full(shape=(num_steps, num_steps), fill_value=float(\"-inf\")),\n            diagonal=1,\n        )\n        if self.decode_ar:\n            tgt_in = paddle.full(shape=(bs, num_steps), fill_value=self.pad_id).astype(\n                \"int64\"\n            )\n            tgt_in[:, (0)] = self.bos_id\n\n            logits = []\n            for i in range(paddle.to_tensor(num_steps)):\n                j = i + 1\n                tgt_out = self.decode(\n                    tgt_in[:, :j],\n                    memory,\n                    tgt_mask[:j, :j],\n                    tgt_query=pos_queries[:, i:j],\n                    tgt_query_mask=query_mask[i:j, :j],\n                )\n                p_i = self.head(tgt_out)\n                logits.append(p_i)\n                if j < num_steps:\n                    tgt_in[:, (j)] = p_i.squeeze().argmax(axis=-1)\n                    if (\n                        testing\n                        and (tgt_in == self.eos_id)\n                        .astype(\"bool\")\n                        .any(axis=-1)\n                        .astype(\"bool\")\n                        .all()\n                    ):\n                        break\n            logits = paddle.concat(x=logits, axis=1)\n        else:\n            tgt_in = paddle.full(shape=(bs, 1), fill_value=self.bos_id).astype(\"int64\")\n            tgt_out = self.decode(tgt_in, memory, tgt_query=pos_queries)\n            logits = self.head(tgt_out)\n        if self.refine_iters:\n            temp = paddle.triu(\n                x=paddle.ones(shape=[num_steps, num_steps], dtype=\"bool\"), diagonal=2\n            )\n            posi = np.where(temp.cpu().numpy() == True)\n            query_mask[posi] = 0\n            bos = paddle.full(shape=(bs, 1), fill_value=self.bos_id).astype(\"int64\")\n            for i in range(self.refine_iters):\n                tgt_in = paddle.concat(x=[bos, logits[:, :-1].argmax(axis=-1)], axis=1)\n                tgt_padding_mask = (tgt_in == self.eos_id).astype(dtype=\"int32\")\n                tgt_padding_mask = tgt_padding_mask.cpu()\n                tgt_padding_mask = tgt_padding_mask.cumsum(axis=-1) > 0\n                tgt_padding_mask = (\n                    tgt_padding_mask.cuda().astype(dtype=\"float32\") == 1.0\n                )\n                tgt_out = self.decode(\n                    tgt_in,\n                    memory,\n                    tgt_mask,\n                    tgt_padding_mask,\n                    tgt_query=pos_queries,\n                    tgt_query_mask=query_mask[:, : tgt_in.shape[1]],\n                )\n                logits = self.head(tgt_out)\n\n        # transfer to probility\n        logits = F.softmax(logits, axis=-1)\n\n        final_output = {\"predict\": logits}\n\n        return final_output\n\n    def gen_tgt_perms(self, tgt):\n        \"\"\"Generate shared permutations for the whole batch.\n        This works because the same attention mask can be used for the shorter sequences\n        because of the padding mask.\n        \"\"\"\n        max_num_chars = tgt.shape[1] - 2\n        if max_num_chars == 1:\n            return paddle.arange(end=3).unsqueeze(axis=0)\n        perms = [paddle.arange(end=max_num_chars)] if self.perm_forward else []\n        max_perms = math.factorial(max_num_chars)\n        if self.perm_mirrored:\n            max_perms //= 2\n        num_gen_perms = min(self.max_gen_perms, max_perms)\n        if max_num_chars < 5:\n            if max_num_chars == 4 and self.perm_mirrored:\n                selector = [0, 3, 4, 6, 9, 10, 12, 16, 17, 18, 19, 21]\n            else:\n                selector = list(range(max_perms))\n            perm_pool = paddle.to_tensor(\n                data=list(permutations(range(max_num_chars), max_num_chars)),\n                place=self._device,\n            )[selector]\n            if self.perm_forward:\n                perm_pool = perm_pool[1:]\n            perms = paddle.stack(x=perms)\n            if len(perm_pool):\n                i = self.rng.choice(\n                    len(perm_pool), size=num_gen_perms - len(perms), replace=False\n                )\n                perms = paddle.concat(x=[perms, perm_pool[i]])\n        else:\n            perms.extend(\n                [\n                    paddle.randperm(n=max_num_chars)\n                    for _ in range(num_gen_perms - len(perms))\n                ]\n            )\n            perms = paddle.stack(x=perms)\n        if self.perm_mirrored:\n            comp = perms.flip(axis=-1)\n            x = paddle.stack(x=[perms, comp])\n            perm_2 = list(range(x.ndim))\n            perm_2[0] = 1\n            perm_2[1] = 0\n            perms = x.transpose(perm=perm_2).reshape((-1, max_num_chars))\n        bos_idx = paddle.zeros(shape=(len(perms), 1), dtype=perms.dtype)\n        eos_idx = paddle.full(\n            shape=(len(perms), 1), fill_value=max_num_chars + 1, dtype=perms.dtype\n        )\n        perms = paddle.concat(x=[bos_idx, perms + 1, eos_idx], axis=1)\n        if len(perms) > 1:\n            perms[(1), 1:] = max_num_chars + 1 - paddle.arange(end=max_num_chars + 1)\n        return perms\n\n    def generate_attn_masks(self, perm):\n        \"\"\"Generate attention masks given a sequence permutation (includes pos. for bos and eos tokens)\n        :param perm: the permutation sequence. i = 0 is always the BOS\n        :return: lookahead attention masks\n        \"\"\"\n        sz = perm.shape[0]\n        mask = paddle.zeros(shape=(sz, sz))\n        for i in range(sz):\n            query_idx = perm[i].cpu().numpy().tolist()\n            masked_keys = perm[i + 1 :].cpu().numpy().tolist()\n            if len(masked_keys) == 0:\n                break\n            mask[query_idx, masked_keys] = float(\"-inf\")\n        content_mask = mask[:-1, :-1].clone()\n        mask[paddle.eye(num_rows=sz).astype(\"bool\")] = float(\"-inf\")\n        query_mask = mask[1:, :-1]\n        return content_mask, query_mask\n\n    def forward_train(self, memory, tgt):\n        tgt_perms = self.gen_tgt_perms(tgt)\n        tgt_in = tgt[:, :-1]\n        tgt_padding_mask = (tgt_in == self.pad_id) | (tgt_in == self.eos_id)\n        logits_list = []\n        final_out = {}\n        for i, perm in enumerate(tgt_perms):\n            tgt_mask, query_mask = self.generate_attn_masks(perm)\n            out = self.decode(\n                tgt_in, memory, tgt_mask, tgt_padding_mask, tgt_query_mask=query_mask\n            )\n            logits = self.head(out)\n            if i == 0:\n                final_out[\"predict\"] = logits\n            logits = logits.flatten(stop_axis=1)\n            logits_list.append(logits)\n\n        final_out[\"logits_list\"] = logits_list\n        final_out[\"pad_id\"] = self.pad_id\n        final_out[\"eos_id\"] = self.eos_id\n\n        return final_out\n\n    def forward(self, feat, targets=None):\n        # feat : B, N, C\n        # targets : labels, labels_len\n\n        if self.training:\n            label = targets[0]  # label\n            label_len = targets[1]\n            max_step = paddle.max(label_len).cpu().numpy()[0] + 2\n            crop_label = label[:, :max_step]\n            final_out = self.forward_train(feat, crop_label)\n        else:\n            final_out = self.forward_test(feat)\n\n        return final_out\n", "ppocr/modeling/heads/rec_sar_head.py": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/open-mmlab/mmocr/blob/main/mmocr/models/textrecog/encoders/sar_encoder.py\nhttps://github.com/open-mmlab/mmocr/blob/main/mmocr/models/textrecog/decoders/sar_decoder.py\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport paddle\nfrom paddle import ParamAttr\nimport paddle.nn as nn\nimport paddle.nn.functional as F\n\n\nclass SAREncoder(nn.Layer):\n    \"\"\"\n    Args:\n        enc_bi_rnn (bool): If True, use bidirectional RNN in encoder.\n        enc_drop_rnn (float): Dropout probability of RNN layer in encoder.\n        enc_gru (bool): If True, use GRU, else LSTM in encoder.\n        d_model (int): Dim of channels from backbone.\n        d_enc (int): Dim of encoder RNN layer.\n        mask (bool): If True, mask padding in RNN sequence.\n    \"\"\"\n\n    def __init__(\n        self,\n        enc_bi_rnn=False,\n        enc_drop_rnn=0.1,\n        enc_gru=False,\n        d_model=512,\n        d_enc=512,\n        mask=True,\n        **kwargs,\n    ):\n        super().__init__()\n        assert isinstance(enc_bi_rnn, bool)\n        assert isinstance(enc_drop_rnn, (int, float))\n        assert 0 <= enc_drop_rnn < 1.0\n        assert isinstance(enc_gru, bool)\n        assert isinstance(d_model, int)\n        assert isinstance(d_enc, int)\n        assert isinstance(mask, bool)\n\n        self.enc_bi_rnn = enc_bi_rnn\n        self.enc_drop_rnn = enc_drop_rnn\n        self.mask = mask\n\n        # LSTM Encoder\n        if enc_bi_rnn:\n            direction = \"bidirectional\"\n        else:\n            direction = \"forward\"\n        kwargs = dict(\n            input_size=d_model,\n            hidden_size=d_enc,\n            num_layers=2,\n            time_major=False,\n            dropout=enc_drop_rnn,\n            direction=direction,\n        )\n        if enc_gru:\n            self.rnn_encoder = nn.GRU(**kwargs)\n        else:\n            self.rnn_encoder = nn.LSTM(**kwargs)\n\n        # global feature transformation\n        encoder_rnn_out_size = d_enc * (int(enc_bi_rnn) + 1)\n        self.linear = nn.Linear(encoder_rnn_out_size, encoder_rnn_out_size)\n\n    def forward(self, feat, img_metas=None):\n        if img_metas is not None:\n            assert len(img_metas[0]) == feat.shape[0]\n\n        valid_ratios = None\n        if img_metas is not None and self.mask:\n            valid_ratios = img_metas[-1]\n\n        h_feat = feat.shape[2]  # bsz c h w\n        feat_v = F.max_pool2d(feat, kernel_size=(h_feat, 1), stride=1, padding=0)\n        feat_v = feat_v.squeeze(2)  # bsz * C * W\n        feat_v = paddle.transpose(feat_v, perm=[0, 2, 1])  # bsz * W * C\n        holistic_feat = self.rnn_encoder(feat_v)[0]  # bsz * T * C\n\n        if valid_ratios is not None:\n            valid_hf = []\n            T = paddle.shape(holistic_feat)[1]\n            for i in range(valid_ratios.shape[0]):\n                valid_step = (\n                    paddle.minimum(T, paddle.ceil(valid_ratios[i] * T).astype(T.dtype))\n                    - 1\n                )\n                valid_hf.append(holistic_feat[i, valid_step, :])\n            valid_hf = paddle.stack(valid_hf, axis=0)\n        else:\n            valid_hf = holistic_feat[:, -1, :]  # bsz * C\n        holistic_feat = self.linear(valid_hf)  # bsz * C\n\n        return holistic_feat\n\n\nclass BaseDecoder(nn.Layer):\n    def __init__(self, **kwargs):\n        super().__init__()\n\n    def forward_train(self, feat, out_enc, targets, img_metas):\n        raise NotImplementedError\n\n    def forward_test(self, feat, out_enc, img_metas):\n        raise NotImplementedError\n\n    def forward(self, feat, out_enc, label=None, img_metas=None, train_mode=True):\n        self.train_mode = train_mode\n\n        if train_mode:\n            return self.forward_train(feat, out_enc, label, img_metas)\n        return self.forward_test(feat, out_enc, img_metas)\n\n\nclass ParallelSARDecoder(BaseDecoder):\n    \"\"\"\n    Args:\n        out_channels (int): Output class number.\n        enc_bi_rnn (bool): If True, use bidirectional RNN in encoder.\n        dec_bi_rnn (bool): If True, use bidirectional RNN in decoder.\n        dec_drop_rnn (float): Dropout of RNN layer in decoder.\n        dec_gru (bool): If True, use GRU, else LSTM in decoder.\n        d_model (int): Dim of channels from backbone.\n        d_enc (int): Dim of encoder RNN layer.\n        d_k (int): Dim of channels of attention module.\n        pred_dropout (float): Dropout probability of prediction layer.\n        max_seq_len (int): Maximum sequence length for decoding.\n        mask (bool): If True, mask padding in feature map.\n        start_idx (int): Index of start token.\n        padding_idx (int): Index of padding token.\n        pred_concat (bool): If True, concat glimpse feature from\n            attention with holistic feature and hidden state.\n    \"\"\"\n\n    def __init__(\n        self,\n        out_channels,  # 90 + unknown + start + padding\n        enc_bi_rnn=False,\n        dec_bi_rnn=False,\n        dec_drop_rnn=0.0,\n        dec_gru=False,\n        d_model=512,\n        d_enc=512,\n        d_k=64,\n        pred_dropout=0.1,\n        max_text_length=30,\n        mask=True,\n        pred_concat=True,\n        **kwargs,\n    ):\n        super().__init__()\n\n        self.num_classes = out_channels\n        self.enc_bi_rnn = enc_bi_rnn\n        self.d_k = d_k\n        self.start_idx = out_channels - 2\n        self.padding_idx = out_channels - 1\n        self.max_seq_len = max_text_length\n        self.mask = mask\n        self.pred_concat = pred_concat\n\n        encoder_rnn_out_size = d_enc * (int(enc_bi_rnn) + 1)\n        decoder_rnn_out_size = encoder_rnn_out_size * (int(dec_bi_rnn) + 1)\n\n        # 2D attention layer\n        self.conv1x1_1 = nn.Linear(decoder_rnn_out_size, d_k)\n        self.conv3x3_1 = nn.Conv2D(d_model, d_k, kernel_size=3, stride=1, padding=1)\n        self.conv1x1_2 = nn.Linear(d_k, 1)\n\n        # Decoder RNN layer\n        if dec_bi_rnn:\n            direction = \"bidirectional\"\n        else:\n            direction = \"forward\"\n\n        kwargs = dict(\n            input_size=encoder_rnn_out_size,\n            hidden_size=encoder_rnn_out_size,\n            num_layers=2,\n            time_major=False,\n            dropout=dec_drop_rnn,\n            direction=direction,\n        )\n        if dec_gru:\n            self.rnn_decoder = nn.GRU(**kwargs)\n        else:\n            self.rnn_decoder = nn.LSTM(**kwargs)\n\n        # Decoder input embedding\n        self.embedding = nn.Embedding(\n            self.num_classes, encoder_rnn_out_size, padding_idx=self.padding_idx\n        )\n\n        # Prediction layer\n        self.pred_dropout = nn.Dropout(pred_dropout)\n        pred_num_classes = self.num_classes - 1\n        if pred_concat:\n            fc_in_channel = decoder_rnn_out_size + d_model + encoder_rnn_out_size\n        else:\n            fc_in_channel = d_model\n        self.prediction = nn.Linear(fc_in_channel, pred_num_classes)\n\n    def _2d_attention(self, decoder_input, feat, holistic_feat, valid_ratios=None):\n        y = self.rnn_decoder(decoder_input)[0]\n        # y: bsz * (seq_len + 1) * hidden_size\n\n        attn_query = self.conv1x1_1(y)  # bsz * (seq_len + 1) * attn_size\n        bsz, seq_len, attn_size = attn_query.shape\n        attn_query = paddle.unsqueeze(attn_query, axis=[3, 4])\n        # (bsz, seq_len + 1, attn_size, 1, 1)\n\n        attn_key = self.conv3x3_1(feat)\n        # bsz * attn_size * h * w\n        attn_key = attn_key.unsqueeze(1)\n        # bsz * 1 * attn_size * h * w\n\n        attn_weight = paddle.tanh(paddle.add(attn_key, attn_query))\n\n        # bsz * (seq_len + 1) * attn_size * h * w\n        attn_weight = paddle.transpose(attn_weight, perm=[0, 1, 3, 4, 2])\n        # bsz * (seq_len + 1) * h * w * attn_size\n        attn_weight = self.conv1x1_2(attn_weight)\n        # bsz * (seq_len + 1) * h * w * 1\n        bsz, T, h, w, c = paddle.shape(attn_weight)\n        assert c == 1\n\n        if valid_ratios is not None:\n            # cal mask of attention weight\n            for i in range(valid_ratios.shape[0]):\n                valid_width = paddle.minimum(\n                    w, paddle.ceil(valid_ratios[i] * w).astype(\"int32\")\n                )\n                if valid_width < w:\n                    attn_weight[i, :, :, valid_width:, :] = float(\"-inf\")\n\n        attn_weight = paddle.reshape(attn_weight, [bsz, T, -1])\n        attn_weight = F.softmax(attn_weight, axis=-1)\n\n        attn_weight = paddle.reshape(attn_weight, [bsz, T, h, w, c])\n        attn_weight = paddle.transpose(attn_weight, perm=[0, 1, 4, 2, 3])\n        # attn_weight: bsz * T * c * h * w\n        # feat: bsz * c * h * w\n        attn_feat = paddle.sum(\n            paddle.multiply(feat.unsqueeze(1), attn_weight), (3, 4), keepdim=False\n        )\n        # bsz * (seq_len + 1) * C\n\n        # Linear transformation\n        if self.pred_concat:\n            hf_c = holistic_feat.shape[-1]\n            holistic_feat = paddle.expand(holistic_feat, shape=[bsz, seq_len, hf_c])\n            y = self.prediction(\n                paddle.concat(\n                    (y, attn_feat.astype(y.dtype), holistic_feat.astype(y.dtype)), 2\n                )\n            )\n        else:\n            y = self.prediction(attn_feat)\n        # bsz * (seq_len + 1) * num_classes\n        if self.train_mode:\n            y = self.pred_dropout(y)\n\n        return y\n\n    def forward_train(self, feat, out_enc, label, img_metas):\n        \"\"\"\n        img_metas: [label, valid_ratio]\n        \"\"\"\n        if img_metas is not None:\n            assert img_metas[0].shape[0] == feat.shape[0]\n\n        valid_ratios = None\n        if img_metas is not None and self.mask:\n            valid_ratios = img_metas[-1]\n\n        lab_embedding = self.embedding(label)\n        # bsz * seq_len * emb_dim\n        out_enc = out_enc.unsqueeze(1).astype(lab_embedding.dtype)\n        # bsz * 1 * emb_dim\n        in_dec = paddle.concat((out_enc, lab_embedding), axis=1)\n        # bsz * (seq_len + 1) * C\n        out_dec = self._2d_attention(in_dec, feat, out_enc, valid_ratios=valid_ratios)\n\n        return out_dec[:, 1:, :]  # bsz * seq_len * num_classes\n\n    def forward_test(self, feat, out_enc, img_metas):\n        if img_metas is not None:\n            assert len(img_metas[0]) == feat.shape[0]\n\n        valid_ratios = None\n        if img_metas is not None and self.mask:\n            valid_ratios = img_metas[-1]\n\n        seq_len = self.max_seq_len\n        bsz = feat.shape[0]\n        start_token = paddle.full((bsz,), fill_value=self.start_idx, dtype=\"int64\")\n        # bsz\n        start_token = self.embedding(start_token)\n        # bsz * emb_dim\n        emb_dim = start_token.shape[1]\n        start_token = start_token.unsqueeze(1)\n        start_token = paddle.expand(start_token, shape=[bsz, seq_len, emb_dim])\n        # bsz * seq_len * emb_dim\n        out_enc = out_enc.unsqueeze(1)\n        # bsz * 1 * emb_dim\n        decoder_input = paddle.concat((out_enc, start_token), axis=1)\n        # bsz * (seq_len + 1) * emb_dim\n\n        outputs = []\n        for i in range(1, seq_len + 1):\n            decoder_output = self._2d_attention(\n                decoder_input, feat, out_enc, valid_ratios=valid_ratios\n            )\n            char_output = decoder_output[:, i, :]  # bsz * num_classes\n            char_output = F.softmax(char_output, -1)\n            outputs.append(char_output)\n            max_idx = paddle.argmax(char_output, axis=1, keepdim=False)\n            char_embedding = self.embedding(max_idx)  # bsz * emb_dim\n            if i < seq_len:\n                decoder_input[:, i + 1, :] = char_embedding\n\n        outputs = paddle.stack(outputs, 1)  # bsz * seq_len * num_classes\n\n        return outputs\n\n\nclass SARHead(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        enc_dim=512,\n        max_text_length=30,\n        enc_bi_rnn=False,\n        enc_drop_rnn=0.1,\n        enc_gru=False,\n        dec_bi_rnn=False,\n        dec_drop_rnn=0.0,\n        dec_gru=False,\n        d_k=512,\n        pred_dropout=0.1,\n        pred_concat=True,\n        **kwargs,\n    ):\n        super(SARHead, self).__init__()\n\n        # encoder module\n        self.encoder = SAREncoder(\n            enc_bi_rnn=enc_bi_rnn,\n            enc_drop_rnn=enc_drop_rnn,\n            enc_gru=enc_gru,\n            d_model=in_channels,\n            d_enc=enc_dim,\n        )\n\n        # decoder module\n        self.decoder = ParallelSARDecoder(\n            out_channels=out_channels,\n            enc_bi_rnn=enc_bi_rnn,\n            dec_bi_rnn=dec_bi_rnn,\n            dec_drop_rnn=dec_drop_rnn,\n            dec_gru=dec_gru,\n            d_model=in_channels,\n            d_enc=enc_dim,\n            d_k=d_k,\n            pred_dropout=pred_dropout,\n            max_text_length=max_text_length,\n            pred_concat=pred_concat,\n        )\n\n    def forward(self, feat, targets=None):\n        \"\"\"\n        img_metas: [label, valid_ratio]\n        \"\"\"\n        holistic_feat = self.encoder(feat, targets)  # bsz c\n\n        if self.training:\n            label = targets[0]  # label\n            final_out = self.decoder(feat, holistic_feat, label, img_metas=targets)\n        else:\n            final_out = self.decoder(\n                feat, holistic_feat, label=None, img_metas=targets, train_mode=False\n            )\n            # (bsz, seq_len, num_classes)\n\n        return final_out\n", "ppocr/modeling/heads/rec_robustscanner_head.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nThis code is refer from:\nhttps://github.com/open-mmlab/mmocr/blob/main/mmocr/models/textrecog/encoders/channel_reduction_encoder.py\nhttps://github.com/open-mmlab/mmocr/blob/main/mmocr/models/textrecog/decoders/robust_scanner_decoder.py\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport paddle\nfrom paddle import ParamAttr\nimport paddle.nn as nn\nimport paddle.nn.functional as F\n\n\nclass BaseDecoder(nn.Layer):\n    def __init__(self, **kwargs):\n        super().__init__()\n\n    def forward_train(self, feat, out_enc, targets, img_metas):\n        raise NotImplementedError\n\n    def forward_test(self, feat, out_enc, img_metas):\n        raise NotImplementedError\n\n    def forward(\n        self,\n        feat,\n        out_enc,\n        label=None,\n        valid_ratios=None,\n        word_positions=None,\n        train_mode=True,\n    ):\n        self.train_mode = train_mode\n\n        if train_mode:\n            return self.forward_train(\n                feat, out_enc, label, valid_ratios, word_positions\n            )\n        return self.forward_test(feat, out_enc, valid_ratios, word_positions)\n\n\nclass ChannelReductionEncoder(nn.Layer):\n    \"\"\"Change the channel number with a one by one convoluational layer.\n\n    Args:\n        in_channels (int): Number of input channels.\n        out_channels (int): Number of output channels.\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, **kwargs):\n        super(ChannelReductionEncoder, self).__init__()\n\n        self.layer = nn.Conv2D(\n            in_channels,\n            out_channels,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            weight_attr=nn.initializer.XavierNormal(),\n        )\n\n    def forward(self, feat):\n        \"\"\"\n        Args:\n            feat (Tensor): Image features with the shape of\n                :math:`(N, C_{in}, H, W)`.\n\n        Returns:\n            Tensor: A tensor of shape :math:`(N, C_{out}, H, W)`.\n        \"\"\"\n        return self.layer(feat)\n\n\ndef masked_fill(x, mask, value):\n    y = paddle.full(x.shape, value, x.dtype)\n    return paddle.where(mask, y, x)\n\n\nclass DotProductAttentionLayer(nn.Layer):\n    def __init__(self, dim_model=None):\n        super().__init__()\n\n        self.scale = dim_model**-0.5 if dim_model is not None else 1.0\n\n    def forward(self, query, key, value, h, w, valid_ratios=None):\n        query = paddle.transpose(query, (0, 2, 1))\n        logits = paddle.matmul(query, key) * self.scale\n        n, c, t = logits.shape\n        # reshape to (n, c, h, w)\n        logits = paddle.reshape(logits, [n, c, h, w])\n        if valid_ratios is not None:\n            # cal mask of attention weight\n            with paddle.base.framework._stride_in_no_check_dy2st_diff():\n                for i, valid_ratio in enumerate(valid_ratios):\n                    valid_width = min(w, int(w * valid_ratio + 0.5))\n                    if valid_width < w:\n                        logits[i, :, :, valid_width:] = float(\"-inf\")\n\n        # reshape to (n, c, h, w)\n        logits = paddle.reshape(logits, [n, c, t])\n        weights = F.softmax(logits, axis=2)\n        value = paddle.transpose(value, (0, 2, 1))\n        glimpse = paddle.matmul(weights, value)\n        glimpse = paddle.transpose(glimpse, (0, 2, 1))\n        return glimpse\n\n\nclass SequenceAttentionDecoder(BaseDecoder):\n    \"\"\"Sequence attention decoder for RobustScanner.\n\n    RobustScanner: `RobustScanner: Dynamically Enhancing Positional Clues for\n    Robust Text Recognition <https://arxiv.org/abs/2007.07542>`_\n\n    Args:\n        num_classes (int): Number of output classes :math:`C`.\n        rnn_layers (int): Number of RNN layers.\n        dim_input (int): Dimension :math:`D_i` of input vector ``feat``.\n        dim_model (int): Dimension :math:`D_m` of the model. Should also be the\n            same as encoder output vector ``out_enc``.\n        max_seq_len (int): Maximum output sequence length :math:`T`.\n        start_idx (int): The index of `<SOS>`.\n        mask (bool): Whether to mask input features according to\n            ``img_meta['valid_ratio']``.\n        padding_idx (int): The index of `<PAD>`.\n        dropout (float): Dropout rate.\n        return_feature (bool): Return feature or logits as the result.\n        encode_value (bool): Whether to use the output of encoder ``out_enc``\n            as `value` of attention layer. If False, the original feature\n            ``feat`` will be used.\n\n    Warning:\n        This decoder will not predict the final class which is assumed to be\n        `<PAD>`. Therefore, its output size is always :math:`C - 1`. `<PAD>`\n        is also ignored by loss as specified in\n        :obj:`mmocr.models.textrecog.recognizer.EncodeDecodeRecognizer`.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_classes=None,\n        rnn_layers=2,\n        dim_input=512,\n        dim_model=128,\n        max_seq_len=40,\n        start_idx=0,\n        mask=True,\n        padding_idx=None,\n        dropout=0,\n        return_feature=False,\n        encode_value=False,\n    ):\n        super().__init__()\n\n        self.num_classes = num_classes\n        self.dim_input = dim_input\n        self.dim_model = dim_model\n        self.return_feature = return_feature\n        self.encode_value = encode_value\n        self.max_seq_len = max_seq_len\n        self.start_idx = start_idx\n        self.mask = mask\n\n        self.embedding = nn.Embedding(\n            self.num_classes, self.dim_model, padding_idx=padding_idx\n        )\n\n        self.sequence_layer = nn.LSTM(\n            input_size=dim_model,\n            hidden_size=dim_model,\n            num_layers=rnn_layers,\n            time_major=False,\n            dropout=dropout,\n        )\n\n        self.attention_layer = DotProductAttentionLayer()\n\n        self.prediction = None\n        if not self.return_feature:\n            pred_num_classes = num_classes - 1\n            self.prediction = nn.Linear(\n                dim_model if encode_value else dim_input, pred_num_classes\n            )\n\n    def forward_train(self, feat, out_enc, targets, valid_ratios):\n        \"\"\"\n        Args:\n            feat (Tensor): Tensor of shape :math:`(N, D_i, H, W)`.\n            out_enc (Tensor): Encoder output of shape\n                :math:`(N, D_m, H, W)`.\n            targets (Tensor): a tensor of shape :math:`(N, T)`. Each element is the index of a\n                character.\n            valid_ratios (Tensor): valid length ratio of img.\n        Returns:\n            Tensor: A raw logit tensor of shape :math:`(N, T, C-1)` if\n            ``return_feature=False``. Otherwise it would be the hidden feature\n            before the prediction projection layer, whose shape is\n            :math:`(N, T, D_m)`.\n        \"\"\"\n\n        tgt_embedding = self.embedding(targets)\n\n        n, c_enc, h, w = out_enc.shape\n        assert c_enc == self.dim_model\n        _, c_feat, _, _ = feat.shape\n        assert c_feat == self.dim_input\n        _, len_q, c_q = tgt_embedding.shape\n        assert c_q == self.dim_model\n        assert len_q <= self.max_seq_len\n\n        query, _ = self.sequence_layer(tgt_embedding)\n        query = paddle.transpose(query, (0, 2, 1))\n        key = paddle.reshape(out_enc, [n, c_enc, h * w])\n        if self.encode_value:\n            value = key\n        else:\n            value = paddle.reshape(feat, [n, c_feat, h * w])\n\n        attn_out = self.attention_layer(query, key, value, h, w, valid_ratios)\n        attn_out = paddle.transpose(attn_out, (0, 2, 1))\n\n        if self.return_feature:\n            return attn_out\n\n        out = self.prediction(attn_out)\n\n        return out\n\n    def forward_test(self, feat, out_enc, valid_ratios):\n        \"\"\"\n        Args:\n            feat (Tensor): Tensor of shape :math:`(N, D_i, H, W)`.\n            out_enc (Tensor): Encoder output of shape\n                :math:`(N, D_m, H, W)`.\n            valid_ratios (Tensor): valid length ratio of img.\n\n        Returns:\n            Tensor: The output logit sequence tensor of shape\n            :math:`(N, T, C-1)`.\n        \"\"\"\n        seq_len = self.max_seq_len\n        batch_size = feat.shape[0]\n\n        decode_sequence = (\n            paddle.ones((batch_size, seq_len), dtype=\"int64\") * self.start_idx\n        )\n\n        outputs = []\n        for i in range(seq_len):\n            step_out = self.forward_test_step(\n                feat, out_enc, decode_sequence, i, valid_ratios\n            )\n            outputs.append(step_out)\n            max_idx = paddle.argmax(step_out, axis=1, keepdim=False)\n            if i < seq_len - 1:\n                decode_sequence[:, i + 1] = max_idx\n\n        outputs = paddle.stack(outputs, 1)\n\n        return outputs\n\n    def forward_test_step(\n        self, feat, out_enc, decode_sequence, current_step, valid_ratios\n    ):\n        \"\"\"\n        Args:\n            feat (Tensor): Tensor of shape :math:`(N, D_i, H, W)`.\n            out_enc (Tensor): Encoder output of shape\n                :math:`(N, D_m, H, W)`.\n            decode_sequence (Tensor): Shape :math:`(N, T)`. The tensor that\n                stores history decoding result.\n            current_step (int): Current decoding step.\n            valid_ratios (Tensor): valid length ratio of img\n\n        Returns:\n            Tensor: Shape :math:`(N, C-1)`. The logit tensor of predicted\n            tokens at current time step.\n        \"\"\"\n\n        embed = self.embedding(decode_sequence)\n\n        n, c_enc, h, w = out_enc.shape\n        assert c_enc == self.dim_model\n        _, c_feat, _, _ = feat.shape\n        assert c_feat == self.dim_input\n        _, _, c_q = embed.shape\n        assert c_q == self.dim_model\n\n        query, _ = self.sequence_layer(embed)\n        query = paddle.transpose(query, (0, 2, 1))\n        key = paddle.reshape(out_enc, [n, c_enc, h * w])\n        if self.encode_value:\n            value = key\n        else:\n            value = paddle.reshape(feat, [n, c_feat, h * w])\n\n        # [n, c, l]\n        attn_out = self.attention_layer(query, key, value, h, w, valid_ratios)\n        out = attn_out[:, :, current_step]\n\n        if self.return_feature:\n            return out\n\n        out = self.prediction(out)\n        out = F.softmax(out, dim=-1)\n\n        return out\n\n\nclass PositionAwareLayer(nn.Layer):\n    def __init__(self, dim_model, rnn_layers=2):\n        super().__init__()\n\n        self.dim_model = dim_model\n\n        self.rnn = nn.LSTM(\n            input_size=dim_model,\n            hidden_size=dim_model,\n            num_layers=rnn_layers,\n            time_major=False,\n        )\n\n        self.mixer = nn.Sequential(\n            nn.Conv2D(dim_model, dim_model, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2D(dim_model, dim_model, kernel_size=3, stride=1, padding=1),\n        )\n\n    def forward(self, img_feature):\n        n, c, h, w = img_feature.shape\n        rnn_input = paddle.transpose(img_feature, (0, 2, 3, 1))\n        rnn_input = paddle.reshape(rnn_input, (n * h, w, c))\n        rnn_output, _ = self.rnn(rnn_input)\n        rnn_output = paddle.reshape(rnn_output, (n, h, w, c))\n        rnn_output = paddle.transpose(rnn_output, (0, 3, 1, 2))\n        out = self.mixer(rnn_output)\n        return out\n\n\nclass PositionAttentionDecoder(BaseDecoder):\n    \"\"\"Position attention decoder for RobustScanner.\n\n    RobustScanner: `RobustScanner: Dynamically Enhancing Positional Clues for\n    Robust Text Recognition <https://arxiv.org/abs/2007.07542>`_\n\n    Args:\n        num_classes (int): Number of output classes :math:`C`.\n        rnn_layers (int): Number of RNN layers.\n        dim_input (int): Dimension :math:`D_i` of input vector ``feat``.\n        dim_model (int): Dimension :math:`D_m` of the model. Should also be the\n            same as encoder output vector ``out_enc``.\n        max_seq_len (int): Maximum output sequence length :math:`T`.\n        mask (bool): Whether to mask input features according to\n            ``img_meta['valid_ratio']``.\n        return_feature (bool): Return feature or logits as the result.\n        encode_value (bool): Whether to use the output of encoder ``out_enc``\n            as `value` of attention layer. If False, the original feature\n            ``feat`` will be used.\n\n    Warning:\n        This decoder will not predict the final class which is assumed to be\n        `<PAD>`. Therefore, its output size is always :math:`C - 1`. `<PAD>`\n        is also ignored by loss\n\n    \"\"\"\n\n    def __init__(\n        self,\n        num_classes=None,\n        rnn_layers=2,\n        dim_input=512,\n        dim_model=128,\n        max_seq_len=40,\n        mask=True,\n        return_feature=False,\n        encode_value=False,\n    ):\n        super().__init__()\n\n        self.num_classes = num_classes\n        self.dim_input = dim_input\n        self.dim_model = dim_model\n        self.max_seq_len = max_seq_len\n        self.return_feature = return_feature\n        self.encode_value = encode_value\n        self.mask = mask\n\n        self.embedding = nn.Embedding(self.max_seq_len + 1, self.dim_model)\n\n        self.position_aware_module = PositionAwareLayer(self.dim_model, rnn_layers)\n\n        self.attention_layer = DotProductAttentionLayer()\n\n        self.prediction = None\n        if not self.return_feature:\n            pred_num_classes = num_classes - 1\n            self.prediction = nn.Linear(\n                dim_model if encode_value else dim_input, pred_num_classes\n            )\n\n    def _get_position_index(self, length, batch_size):\n        position_index_list = []\n        for i in range(batch_size):\n            position_index = paddle.arange(0, end=length, step=1, dtype=\"int64\")\n            position_index_list.append(position_index)\n        batch_position_index = paddle.stack(position_index_list, axis=0)\n        return batch_position_index\n\n    def forward_train(self, feat, out_enc, targets, valid_ratios, position_index):\n        \"\"\"\n        Args:\n            feat (Tensor): Tensor of shape :math:`(N, D_i, H, W)`.\n            out_enc (Tensor): Encoder output of shape\n                :math:`(N, D_m, H, W)`.\n            targets (dict): A dict with the key ``padded_targets``, a\n                tensor of shape :math:`(N, T)`. Each element is the index of a\n                character.\n            valid_ratios (Tensor): valid length ratio of img.\n            position_index (Tensor): The position of each word.\n\n        Returns:\n            Tensor: A raw logit tensor of shape :math:`(N, T, C-1)` if\n            ``return_feature=False``. Otherwise it will be the hidden feature\n            before the prediction projection layer, whose shape is\n            :math:`(N, T, D_m)`.\n        \"\"\"\n        n, c_enc, h, w = out_enc.shape\n        assert c_enc == self.dim_model\n        _, c_feat, _, _ = feat.shape\n        assert c_feat == self.dim_input\n        _, len_q = targets.shape\n        assert len_q <= self.max_seq_len\n\n        position_out_enc = self.position_aware_module(out_enc)\n\n        query = self.embedding(position_index)\n        query = paddle.transpose(query, (0, 2, 1))\n        key = paddle.reshape(position_out_enc, (n, c_enc, h * w))\n        if self.encode_value:\n            value = paddle.reshape(out_enc, (n, c_enc, h * w))\n        else:\n            value = paddle.reshape(feat, (n, c_feat, h * w))\n\n        attn_out = self.attention_layer(query, key, value, h, w, valid_ratios)\n        attn_out = paddle.transpose(attn_out, (0, 2, 1))  # [n, len_q, dim_v]\n\n        if self.return_feature:\n            return attn_out\n\n        return self.prediction(attn_out)\n\n    def forward_test(self, feat, out_enc, valid_ratios, position_index):\n        \"\"\"\n        Args:\n            feat (Tensor): Tensor of shape :math:`(N, D_i, H, W)`.\n            out_enc (Tensor): Encoder output of shape\n                :math:`(N, D_m, H, W)`.\n            valid_ratios (Tensor): valid length ratio of img\n            position_index (Tensor): The position of each word.\n\n        Returns:\n            Tensor: A raw logit tensor of shape :math:`(N, T, C-1)` if\n            ``return_feature=False``. Otherwise it would be the hidden feature\n            before the prediction projection layer, whose shape is\n            :math:`(N, T, D_m)`.\n        \"\"\"\n        n, c_enc, h, w = out_enc.shape\n        assert c_enc == self.dim_model\n        _, c_feat, _, _ = feat.shape\n        assert c_feat == self.dim_input\n\n        position_out_enc = self.position_aware_module(out_enc)\n\n        query = self.embedding(position_index)\n        query = paddle.transpose(query, (0, 2, 1))\n        key = paddle.reshape(position_out_enc, (n, c_enc, h * w))\n        if self.encode_value:\n            value = paddle.reshape(out_enc, (n, c_enc, h * w))\n        else:\n            value = paddle.reshape(feat, (n, c_feat, h * w))\n\n        attn_out = self.attention_layer(query, key, value, h, w, valid_ratios)\n        attn_out = paddle.transpose(attn_out, (0, 2, 1))  # [n, len_q, dim_v]\n\n        if self.return_feature:\n            return attn_out\n\n        return self.prediction(attn_out)\n\n\nclass RobustScannerFusionLayer(nn.Layer):\n    def __init__(self, dim_model, dim=-1):\n        super(RobustScannerFusionLayer, self).__init__()\n\n        self.dim_model = dim_model\n        self.dim = dim\n        self.linear_layer = nn.Linear(dim_model * 2, dim_model * 2)\n\n    def forward(self, x0, x1):\n        assert x0.shape == x1.shape\n        fusion_input = paddle.concat([x0, x1], self.dim)\n        output = self.linear_layer(fusion_input)\n        output = F.glu(output, self.dim)\n        return output\n\n\nclass RobustScannerDecoder(BaseDecoder):\n    \"\"\"Decoder for RobustScanner.\n\n    RobustScanner: `RobustScanner: Dynamically Enhancing Positional Clues for\n    Robust Text Recognition <https://arxiv.org/abs/2007.07542>`_\n\n    Args:\n        num_classes (int): Number of output classes :math:`C`.\n        dim_input (int): Dimension :math:`D_i` of input vector ``feat``.\n        dim_model (int): Dimension :math:`D_m` of the model. Should also be the\n            same as encoder output vector ``out_enc``.\n        max_seq_len (int): Maximum output sequence length :math:`T`.\n        start_idx (int): The index of `<SOS>`.\n        mask (bool): Whether to mask input features according to\n            ``img_meta['valid_ratio']``.\n        padding_idx (int): The index of `<PAD>`.\n        encode_value (bool): Whether to use the output of encoder ``out_enc``\n            as `value` of attention layer. If False, the original feature\n            ``feat`` will be used.\n\n    Warning:\n        This decoder will not predict the final class which is assumed to be\n        `<PAD>`. Therefore, its output size is always :math:`C - 1`. `<PAD>`\n        is also ignored by loss as specified in\n        :obj:`mmocr.models.textrecog.recognizer.EncodeDecodeRecognizer`.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_classes=None,\n        dim_input=512,\n        dim_model=128,\n        hybrid_decoder_rnn_layers=2,\n        hybrid_decoder_dropout=0,\n        position_decoder_rnn_layers=2,\n        max_seq_len=40,\n        start_idx=0,\n        mask=True,\n        padding_idx=None,\n        encode_value=False,\n    ):\n        super().__init__()\n        self.num_classes = num_classes\n        self.dim_input = dim_input\n        self.dim_model = dim_model\n        self.max_seq_len = max_seq_len\n        self.encode_value = encode_value\n        self.start_idx = start_idx\n        self.padding_idx = padding_idx\n        self.mask = mask\n\n        # init hybrid decoder\n        self.hybrid_decoder = SequenceAttentionDecoder(\n            num_classes=num_classes,\n            rnn_layers=hybrid_decoder_rnn_layers,\n            dim_input=dim_input,\n            dim_model=dim_model,\n            max_seq_len=max_seq_len,\n            start_idx=start_idx,\n            mask=mask,\n            padding_idx=padding_idx,\n            dropout=hybrid_decoder_dropout,\n            encode_value=encode_value,\n            return_feature=True,\n        )\n\n        # init position decoder\n        self.position_decoder = PositionAttentionDecoder(\n            num_classes=num_classes,\n            rnn_layers=position_decoder_rnn_layers,\n            dim_input=dim_input,\n            dim_model=dim_model,\n            max_seq_len=max_seq_len,\n            mask=mask,\n            encode_value=encode_value,\n            return_feature=True,\n        )\n\n        self.fusion_module = RobustScannerFusionLayer(\n            self.dim_model if encode_value else dim_input\n        )\n\n        pred_num_classes = num_classes - 1\n        self.prediction = nn.Linear(\n            dim_model if encode_value else dim_input, pred_num_classes\n        )\n\n    def forward_train(self, feat, out_enc, target, valid_ratios, word_positions):\n        \"\"\"\n        Args:\n            feat (Tensor): Tensor of shape :math:`(N, D_i, H, W)`.\n            out_enc (Tensor): Encoder output of shape\n                :math:`(N, D_m, H, W)`.\n            target (dict): A dict with the key ``padded_targets``, a\n                tensor of shape :math:`(N, T)`. Each element is the index of a\n                character.\n            valid_ratios (Tensor):\n            word_positions (Tensor): The position of each word.\n\n        Returns:\n            Tensor: A raw logit tensor of shape :math:`(N, T, C-1)`.\n        \"\"\"\n        hybrid_glimpse = self.hybrid_decoder.forward_train(\n            feat, out_enc, target, valid_ratios\n        )\n        position_glimpse = self.position_decoder.forward_train(\n            feat, out_enc, target, valid_ratios, word_positions\n        )\n\n        fusion_out = self.fusion_module(hybrid_glimpse, position_glimpse)\n\n        out = self.prediction(fusion_out)\n\n        return out\n\n    def forward_test(self, feat, out_enc, valid_ratios, word_positions):\n        \"\"\"\n        Args:\n            feat (Tensor): Tensor of shape :math:`(N, D_i, H, W)`.\n            out_enc (Tensor): Encoder output of shape\n                :math:`(N, D_m, H, W)`.\n            valid_ratios (Tensor):\n            word_positions (Tensor): The position of each word.\n        Returns:\n            Tensor: The output logit sequence tensor of shape\n            :math:`(N, T, C-1)`.\n        \"\"\"\n        seq_len = self.max_seq_len\n        batch_size = feat.shape[0]\n\n        decode_sequence = (\n            paddle.ones((batch_size, seq_len), dtype=\"int64\") * self.start_idx\n        )\n\n        position_glimpse = self.position_decoder.forward_test(\n            feat, out_enc, valid_ratios, word_positions\n        )\n\n        outputs = []\n        for i in range(seq_len):\n            hybrid_glimpse_step = self.hybrid_decoder.forward_test_step(\n                feat, out_enc, decode_sequence, i, valid_ratios\n            )\n\n            fusion_out = self.fusion_module(\n                hybrid_glimpse_step, position_glimpse[:, i, :]\n            )\n\n            char_out = self.prediction(fusion_out)\n            char_out = F.softmax(char_out, -1)\n            outputs.append(char_out)\n            max_idx = paddle.argmax(char_out, axis=1, keepdim=False)\n            if i < seq_len - 1:\n                decode_sequence[:, i + 1] = max_idx\n\n        outputs = paddle.stack(outputs, 1)\n\n        return outputs\n\n\nclass RobustScannerHead(nn.Layer):\n    def __init__(\n        self,\n        out_channels,  # 90 + unknown + start + padding\n        in_channels,\n        enc_outchannles=128,\n        hybrid_dec_rnn_layers=2,\n        hybrid_dec_dropout=0,\n        position_dec_rnn_layers=2,\n        start_idx=0,\n        max_text_length=40,\n        mask=True,\n        padding_idx=None,\n        encode_value=False,\n        **kwargs,\n    ):\n        super(RobustScannerHead, self).__init__()\n\n        # encoder module\n        self.encoder = ChannelReductionEncoder(\n            in_channels=in_channels, out_channels=enc_outchannles\n        )\n\n        # decoder module\n        self.decoder = RobustScannerDecoder(\n            num_classes=out_channels,\n            dim_input=in_channels,\n            dim_model=enc_outchannles,\n            hybrid_decoder_rnn_layers=hybrid_dec_rnn_layers,\n            hybrid_decoder_dropout=hybrid_dec_dropout,\n            position_decoder_rnn_layers=position_dec_rnn_layers,\n            max_seq_len=max_text_length,\n            start_idx=start_idx,\n            mask=mask,\n            padding_idx=padding_idx,\n            encode_value=encode_value,\n        )\n\n    def forward(self, inputs, targets=None):\n        \"\"\"\n        targets: [label, valid_ratio, word_positions]\n        \"\"\"\n        out_enc = self.encoder(inputs)\n        valid_ratios = None\n        word_positions = targets[-1]\n\n        if len(targets) > 1:\n            valid_ratios = targets[-2]\n\n        if self.training:\n            label = targets[0]  # label\n            label = paddle.to_tensor(label, dtype=\"int64\")\n            final_out = self.decoder(\n                inputs, out_enc, label, valid_ratios, word_positions\n            )\n        if not self.training:\n            final_out = self.decoder(\n                inputs,\n                out_enc,\n                label=None,\n                valid_ratios=valid_ratios,\n                word_positions=word_positions,\n                train_mode=False,\n            )\n        return final_out\n", "ppocr/modeling/heads/table_att_head.py": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport paddle\nimport paddle.nn as nn\nfrom paddle import ParamAttr\nimport paddle.nn.functional as F\nimport numpy as np\n\nfrom .rec_att_head import AttentionGRUCell\nfrom ppocr.modeling.backbones.rec_svtrnet import DropPath, Identity, Mlp\n\n\ndef get_para_bias_attr(l2_decay, k):\n    if l2_decay > 0:\n        regularizer = paddle.regularizer.L2Decay(l2_decay)\n        stdv = 1.0 / math.sqrt(k * 1.0)\n        initializer = nn.initializer.Uniform(-stdv, stdv)\n    else:\n        regularizer = None\n        initializer = None\n    weight_attr = ParamAttr(regularizer=regularizer, initializer=initializer)\n    bias_attr = ParamAttr(regularizer=regularizer, initializer=initializer)\n    return [weight_attr, bias_attr]\n\n\nclass TableAttentionHead(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        hidden_size,\n        in_max_len=488,\n        max_text_length=800,\n        out_channels=30,\n        loc_reg_num=4,\n        **kwargs,\n    ):\n        super(TableAttentionHead, self).__init__()\n        self.input_size = in_channels[-1]\n        self.hidden_size = hidden_size\n        self.out_channels = out_channels\n        self.max_text_length = max_text_length\n\n        self.structure_attention_cell = AttentionGRUCell(\n            self.input_size, hidden_size, self.out_channels, use_gru=False\n        )\n        self.structure_generator = nn.Linear(hidden_size, self.out_channels)\n        self.in_max_len = in_max_len\n\n        if self.in_max_len == 640:\n            self.loc_fea_trans = nn.Linear(400, self.max_text_length + 1)\n        elif self.in_max_len == 800:\n            self.loc_fea_trans = nn.Linear(625, self.max_text_length + 1)\n        else:\n            self.loc_fea_trans = nn.Linear(256, self.max_text_length + 1)\n        self.loc_generator = nn.Linear(self.input_size + hidden_size, loc_reg_num)\n\n    def _char_to_onehot(self, input_char, onehot_dim):\n        input_ont_hot = F.one_hot(input_char, onehot_dim)\n        return input_ont_hot\n\n    def forward(self, inputs, targets=None):\n        # if and else branch are both needed when you want to assign a variable\n        # if you modify the var in just one branch, then the modification will not work.\n        fea = inputs[-1]\n        last_shape = int(np.prod(fea.shape[2:]))  # gry added\n        fea = paddle.reshape(fea, [fea.shape[0], fea.shape[1], last_shape])\n        fea = fea.transpose([0, 2, 1])  # (NTC)(batch, width, channels)\n        batch_size = fea.shape[0]\n\n        hidden = paddle.zeros((batch_size, self.hidden_size))\n        output_hiddens = paddle.zeros(\n            (batch_size, self.max_text_length + 1, self.hidden_size)\n        )\n        if self.training and targets is not None:\n            structure = targets[0]\n            for i in range(self.max_text_length + 1):\n                elem_onehots = self._char_to_onehot(\n                    structure[:, i], onehot_dim=self.out_channels\n                )\n                (outputs, hidden), alpha = self.structure_attention_cell(\n                    hidden, fea, elem_onehots\n                )\n                output_hiddens[:, i, :] = outputs\n            structure_probs = self.structure_generator(output_hiddens)\n            loc_fea = fea.transpose([0, 2, 1])\n            loc_fea = self.loc_fea_trans(loc_fea)\n            loc_fea = loc_fea.transpose([0, 2, 1])\n            loc_concat = paddle.concat([output_hiddens, loc_fea], axis=2)\n            loc_preds = self.loc_generator(loc_concat)\n            loc_preds = F.sigmoid(loc_preds)\n        else:\n            temp_elem = paddle.zeros(shape=[batch_size], dtype=\"int32\")\n            structure_probs = None\n            loc_preds = None\n            elem_onehots = None\n            outputs = None\n            alpha = None\n            max_text_length = paddle.to_tensor(self.max_text_length)\n            for i in range(max_text_length + 1):\n                elem_onehots = self._char_to_onehot(\n                    temp_elem, onehot_dim=self.out_channels\n                )\n                (outputs, hidden), alpha = self.structure_attention_cell(\n                    hidden, fea, elem_onehots\n                )\n                output_hiddens[:, i, :] = outputs\n                structure_probs_step = self.structure_generator(outputs)\n                temp_elem = structure_probs_step.argmax(axis=1, dtype=\"int32\")\n\n            structure_probs = self.structure_generator(output_hiddens)\n            structure_probs = F.softmax(structure_probs)\n            loc_fea = fea.transpose([0, 2, 1])\n            loc_fea = self.loc_fea_trans(loc_fea)\n            loc_fea = loc_fea.transpose([0, 2, 1])\n            loc_concat = paddle.concat([output_hiddens, loc_fea], axis=2)\n            loc_preds = self.loc_generator(loc_concat)\n            loc_preds = F.sigmoid(loc_preds)\n        return {\"structure_probs\": structure_probs, \"loc_preds\": loc_preds}\n\n\nclass HWAttention(nn.Layer):\n    def __init__(\n        self,\n        head_dim=32,\n        qk_scale=None,\n        attn_drop=0.0,\n    ):\n        super().__init__()\n        self.head_dim = head_dim\n        self.scale = qk_scale or self.head_dim**-0.5\n        self.attn_drop = nn.Dropout(attn_drop)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        C = C // 3\n        qkv = x.reshape([B, N, 3, C // self.head_dim, self.head_dim]).transpose(\n            [2, 0, 3, 1, 4]\n        )\n        q, k, v = qkv.unbind(0)\n        attn = q @ k.transpose([0, 1, 3, 2]) * self.scale\n        attn = F.softmax(attn, -1)\n        attn = self.attn_drop(attn)\n        x = attn @ v\n        x = x.transpose([0, 2, 1]).reshape([B, N, C])\n        return x\n\n\ndef img2windows(img, H_sp, W_sp):\n    \"\"\"\n    img: B C H W\n    \"\"\"\n    B, H, W, C = img.shape\n    img_reshape = img.reshape([B, H // H_sp, H_sp, W // W_sp, W_sp, C])\n    img_perm = img_reshape.transpose([0, 1, 3, 2, 4, 5]).reshape([-1, H_sp * W_sp, C])\n    return img_perm\n\n\ndef windows2img(img_splits_hw, H_sp, W_sp, H, W):\n    \"\"\"\n    img_splits_hw: B' H W C\n    \"\"\"\n    B = int(img_splits_hw.shape[0] / (H * W / H_sp / W_sp))\n\n    img = img_splits_hw.reshape([B, H // H_sp, W // W_sp, H_sp, W_sp, -1])\n    img = img.transpose([0, 1, 3, 2, 4, 5]).flatten(1, 4)\n    return img\n\n\nclass Block(nn.Layer):\n    def __init__(\n        self,\n        dim,\n        num_heads,\n        split_h=4,\n        split_w=4,\n        h_num_heads=None,\n        w_num_heads=None,\n        mlp_ratio=4.0,\n        qkv_bias=False,\n        qk_scale=None,\n        drop=0.0,\n        attn_drop=0.0,\n        drop_path=0.0,\n        act_layer=nn.GELU,\n        norm_layer=nn.LayerNorm,\n        eps=1e-6,\n    ):\n        super().__init__()\n        self.qkv = nn.Linear(dim, dim * 3, bias_attr=qkv_bias)\n        self.proj = nn.Linear(dim, dim)\n        self.split_h = split_h\n        self.split_w = split_w\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.norm1 = norm_layer(dim, epsilon=eps)\n        self.h_num_heads = h_num_heads if h_num_heads is not None else num_heads // 2\n        self.w_num_heads = w_num_heads if w_num_heads is not None else num_heads // 2\n        self.head_dim = dim // num_heads\n        self.mixer = HWAttention(\n            head_dim=dim // num_heads,\n            qk_scale=qk_scale,\n            attn_drop=attn_drop,\n        )\n        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else Identity()\n        self.norm2 = norm_layer(dim, epsilon=eps)\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=mlp_hidden_dim,\n            act_layer=act_layer,\n            drop=drop,\n        )\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        x = x.flatten(2).transpose([0, 2, 1])\n\n        qkv = self.qkv(x).reshape([B, H, W, 3 * C])\n\n        x1 = qkv[:, :, :, : 3 * self.h_num_heads * self.head_dim]  # b, h, w, 3ch\n        x2 = qkv[:, :, :, 3 * self.h_num_heads * self.head_dim :]  # b, h, w, 3cw\n\n        x1 = self.mixer(img2windows(x1, self.split_h, W))  # b*splith, W, 3ch\n        x2 = self.mixer(img2windows(x2, H, self.split_w))  # b*splitw, h, 3ch\n        x1 = windows2img(x1, self.split_h, W, H, W)\n        x2 = windows2img(x2, H, self.split_w, H, W)\n\n        attened_x = paddle.concat([x1, x2], 2)\n        attened_x = self.proj(attened_x)\n\n        x = self.norm1(x + self.drop_path(attened_x))\n        x = self.norm2(x + self.drop_path(self.mlp(x)))\n        x = x.transpose([0, 2, 1]).reshape([-1, C, H, W])\n        return x\n\n\nclass SLAHead(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        hidden_size,\n        out_channels=30,\n        max_text_length=500,\n        loc_reg_num=4,\n        fc_decay=0.0,\n        use_attn=False,\n        **kwargs,\n    ):\n        \"\"\"\n        @param in_channels: input shape\n        @param hidden_size: hidden_size for RNN and Embedding\n        @param out_channels: num_classes to rec\n        @param max_text_length: max text pred\n        \"\"\"\n        super().__init__()\n        in_channels = in_channels[-1]\n        self.hidden_size = hidden_size\n        self.max_text_length = max_text_length\n        self.emb = self._char_to_onehot\n        self.num_embeddings = out_channels\n        self.loc_reg_num = loc_reg_num\n        self.eos = self.num_embeddings - 1\n\n        # structure\n        self.structure_attention_cell = AttentionGRUCell(\n            in_channels, hidden_size, self.num_embeddings\n        )\n        weight_attr, bias_attr = get_para_bias_attr(l2_decay=fc_decay, k=hidden_size)\n        weight_attr1_1, bias_attr1_1 = get_para_bias_attr(\n            l2_decay=fc_decay, k=hidden_size\n        )\n        weight_attr1_2, bias_attr1_2 = get_para_bias_attr(\n            l2_decay=fc_decay, k=hidden_size\n        )\n        self.structure_generator = nn.Sequential(\n            nn.Linear(\n                self.hidden_size,\n                self.hidden_size,\n                weight_attr=weight_attr1_2,\n                bias_attr=bias_attr1_2,\n            ),\n            nn.Linear(\n                hidden_size, out_channels, weight_attr=weight_attr, bias_attr=bias_attr\n            ),\n        )\n        dpr = np.linspace(0, 0.1, 2)\n\n        self.use_attn = use_attn\n        if use_attn:\n            layer_list = [\n                Block(\n                    in_channels,\n                    num_heads=2,\n                    mlp_ratio=4.0,\n                    qkv_bias=True,\n                    drop_path=dpr[i],\n                )\n                for i in range(2)\n            ]\n            self.cross_atten = nn.Sequential(*layer_list)\n        # loc\n        weight_attr1, bias_attr1 = get_para_bias_attr(\n            l2_decay=fc_decay, k=self.hidden_size\n        )\n        weight_attr2, bias_attr2 = get_para_bias_attr(\n            l2_decay=fc_decay, k=self.hidden_size\n        )\n        self.loc_generator = nn.Sequential(\n            nn.Linear(\n                self.hidden_size,\n                self.hidden_size,\n                weight_attr=weight_attr1,\n                bias_attr=bias_attr1,\n            ),\n            nn.Linear(\n                self.hidden_size,\n                loc_reg_num,\n                weight_attr=weight_attr2,\n                bias_attr=bias_attr2,\n            ),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, inputs, targets=None):\n        fea = inputs[-1]\n        batch_size = fea.shape[0]\n        if self.use_attn:\n            fea = fea + self.cross_atten(fea)\n        # reshape\n        fea = paddle.reshape(fea, [fea.shape[0], fea.shape[1], -1])\n        fea = fea.transpose([0, 2, 1])  # (NTC)(batch, width, channels)\n\n        hidden = paddle.zeros((batch_size, self.hidden_size))\n        structure_preds = paddle.zeros(\n            (batch_size, self.max_text_length + 1, self.num_embeddings)\n        )\n        loc_preds = paddle.zeros(\n            (batch_size, self.max_text_length + 1, self.loc_reg_num)\n        )\n        structure_preds.stop_gradient = True\n        loc_preds.stop_gradient = True\n\n        if self.training and targets is not None:\n            structure = targets[0]\n            max_len = targets[-2].max()\n            for i in range(max_len + 1):\n                hidden, structure_step, loc_step = self._decode(\n                    structure[:, i], fea, hidden\n                )\n                structure_preds[:, i, :] = structure_step\n                loc_preds[:, i, :] = loc_step\n            structure_preds = structure_preds[:, : max_len + 1]\n            loc_preds = loc_preds[:, : max_len + 1]\n        else:\n            structure_ids = paddle.zeros(\n                (batch_size, self.max_text_length + 1), dtype=paddle.int64\n            )\n            pre_chars = paddle.zeros(shape=[batch_size], dtype=\"int32\")\n            max_text_length = paddle.to_tensor(self.max_text_length)\n            # for export\n            loc_step, structure_step = None, None\n            for i in range(max_text_length + 1):\n                hidden, structure_step, loc_step = self._decode(pre_chars, fea, hidden)\n                pre_chars = structure_step.argmax(axis=1, dtype=\"int32\")\n                structure_preds[:, i, :] = structure_step\n                loc_preds[:, i, :] = loc_step\n\n                structure_ids[:, i] = pre_chars\n                if (structure_ids == self.eos).any(-1).all():\n                    break\n        if not self.training:\n            structure_preds = F.softmax(structure_preds[:, : i + 1])\n            loc_preds = loc_preds[:, : i + 1]\n        return {\"structure_probs\": structure_preds, \"loc_preds\": loc_preds}\n\n    def _decode(self, pre_chars, features, hidden):\n        \"\"\"\n        Predict table label and coordinates for each step\n        @param pre_chars: Table label in previous step\n        @param features:\n        @param hidden: hidden status in previous step\n        @return:\n        \"\"\"\n        emb_feature = self.emb(pre_chars)\n        # output shape is b * self.hidden_size\n        (output, hidden), alpha = self.structure_attention_cell(\n            hidden, features, emb_feature\n        )\n\n        # structure\n        structure_step = self.structure_generator(output)\n        # loc\n        loc_step = self.loc_generator(output)\n        return hidden, structure_step, loc_step\n\n    def _char_to_onehot(self, input_char):\n        input_ont_hot = F.one_hot(input_char, self.num_embeddings)\n        return input_ont_hot\n", "ppocr/modeling/heads/rec_pren_head.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom paddle import nn\nfrom paddle.nn import functional as F\n\n\nclass PRENHead(nn.Layer):\n    def __init__(self, in_channels, out_channels, **kwargs):\n        super(PRENHead, self).__init__()\n        self.linear = nn.Linear(in_channels, out_channels)\n\n    def forward(self, x, targets=None):\n        predicts = self.linear(x)\n\n        if not self.training:\n            predicts = F.softmax(predicts, axis=2)\n\n        return predicts\n", "ppocr/modeling/heads/sr_rensnet_transformer.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/FudanVI/FudanOCR/blob/main/text-gestalt/loss/transformer_english_decomposition.py\n\"\"\"\nimport copy\nimport math\n\nimport paddle\nimport paddle.nn as nn\nimport paddle.nn.functional as F\n\n\ndef subsequent_mask(size):\n    \"\"\"Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n    Unmasked positions are filled with float(0.0).\n    \"\"\"\n    mask = paddle.ones([1, size, size], dtype=\"float32\")\n    mask_inf = paddle.triu(\n        paddle.full(shape=[1, size, size], dtype=\"float32\", fill_value=\"-inf\"),\n        diagonal=1,\n    )\n    mask = mask + mask_inf\n    padding_mask = paddle.equal(mask, paddle.to_tensor(1, dtype=mask.dtype))\n    return padding_mask\n\n\ndef clones(module, N):\n    return nn.LayerList([copy.deepcopy(module) for _ in range(N)])\n\n\ndef masked_fill(x, mask, value):\n    y = paddle.full(x.shape, value, x.dtype)\n    return paddle.where(mask, y, x)\n\n\ndef attention(query, key, value, mask=None, dropout=None, attention_map=None):\n    d_k = query.shape[-1]\n    scores = paddle.matmul(query, paddle.transpose(key, [0, 1, 3, 2])) / math.sqrt(d_k)\n\n    if mask is not None:\n        scores = masked_fill(scores, mask == 0, float(\"-inf\"))\n    else:\n        pass\n\n    p_attn = F.softmax(scores, axis=-1)\n\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n    return paddle.matmul(p_attn, value), p_attn\n\n\nclass MultiHeadedAttention(nn.Layer):\n    def __init__(self, h, d_model, dropout=0.1, compress_attention=False):\n        super(MultiHeadedAttention, self).__init__()\n        assert d_model % h == 0\n        self.d_k = d_model // h\n        self.h = h\n        self.linears = clones(nn.Linear(d_model, d_model), 4)\n        self.attn = None\n        self.dropout = nn.Dropout(p=dropout, mode=\"downscale_in_infer\")\n        self.compress_attention = compress_attention\n        self.compress_attention_linear = nn.Linear(h, 1)\n\n    def forward(self, query, key, value, mask=None, attention_map=None):\n        if mask is not None:\n            mask = mask.unsqueeze(1)\n        nbatches = query.shape[0]\n\n        query, key, value = [\n            paddle.transpose(\n                l(x).reshape([nbatches, -1, self.h, self.d_k]), [0, 2, 1, 3]\n            )\n            for l, x in zip(self.linears, (query, key, value))\n        ]\n\n        x, attention_map = attention(\n            query,\n            key,\n            value,\n            mask=mask,\n            dropout=self.dropout,\n            attention_map=attention_map,\n        )\n\n        x = paddle.reshape(\n            paddle.transpose(x, [0, 2, 1, 3]), [nbatches, -1, self.h * self.d_k]\n        )\n\n        return self.linears[-1](x), attention_map\n\n\nclass ResNet(nn.Layer):\n    def __init__(self, num_in, block, layers):\n        super(ResNet, self).__init__()\n\n        self.conv1 = nn.Conv2D(num_in, 64, kernel_size=3, stride=1, padding=1)\n        self.bn1 = nn.BatchNorm2D(64, use_global_stats=True)\n        self.relu1 = nn.ReLU()\n        self.pool = nn.MaxPool2D((2, 2), (2, 2))\n\n        self.conv2 = nn.Conv2D(64, 128, kernel_size=3, stride=1, padding=1)\n        self.bn2 = nn.BatchNorm2D(128, use_global_stats=True)\n        self.relu2 = nn.ReLU()\n\n        self.layer1_pool = nn.MaxPool2D((2, 2), (2, 2))\n        self.layer1 = self._make_layer(block, 128, 256, layers[0])\n        self.layer1_conv = nn.Conv2D(256, 256, 3, 1, 1)\n        self.layer1_bn = nn.BatchNorm2D(256, use_global_stats=True)\n        self.layer1_relu = nn.ReLU()\n\n        self.layer2_pool = nn.MaxPool2D((2, 2), (2, 2))\n        self.layer2 = self._make_layer(block, 256, 256, layers[1])\n        self.layer2_conv = nn.Conv2D(256, 256, 3, 1, 1)\n        self.layer2_bn = nn.BatchNorm2D(256, use_global_stats=True)\n        self.layer2_relu = nn.ReLU()\n\n        self.layer3_pool = nn.MaxPool2D((2, 2), (2, 2))\n        self.layer3 = self._make_layer(block, 256, 512, layers[2])\n        self.layer3_conv = nn.Conv2D(512, 512, 3, 1, 1)\n        self.layer3_bn = nn.BatchNorm2D(512, use_global_stats=True)\n        self.layer3_relu = nn.ReLU()\n\n        self.layer4_pool = nn.MaxPool2D((2, 2), (2, 2))\n        self.layer4 = self._make_layer(block, 512, 512, layers[3])\n        self.layer4_conv2 = nn.Conv2D(512, 1024, 3, 1, 1)\n        self.layer4_conv2_bn = nn.BatchNorm2D(1024, use_global_stats=True)\n        self.layer4_conv2_relu = nn.ReLU()\n\n    def _make_layer(self, block, inplanes, planes, blocks):\n        if inplanes != planes:\n            downsample = nn.Sequential(\n                nn.Conv2D(inplanes, planes, 3, 1, 1),\n                nn.BatchNorm2D(planes, use_global_stats=True),\n            )\n        else:\n            downsample = None\n        layers = []\n        layers.append(block(inplanes, planes, downsample))\n        for i in range(1, blocks):\n            layers.append(block(planes, planes, downsample=None))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu1(x)\n        x = self.pool(x)\n\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu2(x)\n\n        x = self.layer1_pool(x)\n        x = self.layer1(x)\n        x = self.layer1_conv(x)\n        x = self.layer1_bn(x)\n        x = self.layer1_relu(x)\n\n        x = self.layer2(x)\n        x = self.layer2_conv(x)\n        x = self.layer2_bn(x)\n        x = self.layer2_relu(x)\n\n        x = self.layer3(x)\n        x = self.layer3_conv(x)\n        x = self.layer3_bn(x)\n        x = self.layer3_relu(x)\n\n        x = self.layer4(x)\n        x = self.layer4_conv2(x)\n        x = self.layer4_conv2_bn(x)\n        x = self.layer4_conv2_relu(x)\n\n        return x\n\n\nclass Bottleneck(nn.Layer):\n    def __init__(self, input_dim):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2D(input_dim, input_dim, 1)\n        self.bn1 = nn.BatchNorm2D(input_dim, use_global_stats=True)\n        self.relu = nn.ReLU()\n\n        self.conv2 = nn.Conv2D(input_dim, input_dim, 3, 1, 1)\n        self.bn2 = nn.BatchNorm2D(input_dim, use_global_stats=True)\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass PositionalEncoding(nn.Layer):\n    \"Implement the PE function.\"\n\n    def __init__(self, dropout, dim, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout, mode=\"downscale_in_infer\")\n\n        pe = paddle.zeros([max_len, dim])\n        position = paddle.arange(0, max_len, dtype=paddle.float32).unsqueeze(1)\n        div_term = paddle.exp(\n            paddle.arange(0, dim, 2).astype(\"float32\") * (-math.log(10000.0) / dim)\n        )\n        pe[:, 0::2] = paddle.sin(position * div_term)\n        pe[:, 1::2] = paddle.cos(position * div_term)\n        pe = paddle.unsqueeze(pe, 0)\n        self.register_buffer(\"pe\", pe)\n\n    def forward(self, x):\n        x = x + self.pe[:, : x.shape[1]]\n        return self.dropout(x)\n\n\nclass PositionwiseFeedForward(nn.Layer):\n    \"Implements FFN equation.\"\n\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super(PositionwiseFeedForward, self).__init__()\n        self.w_1 = nn.Linear(d_model, d_ff)\n        self.w_2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout, mode=\"downscale_in_infer\")\n\n    def forward(self, x):\n        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n\n\nclass Generator(nn.Layer):\n    \"Define standard linear + softmax generation step.\"\n\n    def __init__(self, d_model, vocab):\n        super(Generator, self).__init__()\n        self.proj = nn.Linear(d_model, vocab)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        out = self.proj(x)\n        return out\n\n\nclass Embeddings(nn.Layer):\n    def __init__(self, d_model, vocab):\n        super(Embeddings, self).__init__()\n        self.lut = nn.Embedding(vocab, d_model)\n        self.d_model = d_model\n\n    def forward(self, x):\n        embed = self.lut(x) * math.sqrt(self.d_model)\n        return embed\n\n\nclass LayerNorm(nn.Layer):\n    \"Construct a layernorm module (See citation for details).\"\n\n    def __init__(self, features, eps=1e-6):\n        super(LayerNorm, self).__init__()\n        self.a_2 = self.create_parameter(\n            shape=[features], default_initializer=paddle.nn.initializer.Constant(1.0)\n        )\n        self.b_2 = self.create_parameter(\n            shape=[features], default_initializer=paddle.nn.initializer.Constant(0.0)\n        )\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n\n\nclass Decoder(nn.Layer):\n    def __init__(self):\n        super(Decoder, self).__init__()\n\n        self.mask_multihead = MultiHeadedAttention(h=16, d_model=1024, dropout=0.1)\n        self.mul_layernorm1 = LayerNorm(1024)\n\n        self.multihead = MultiHeadedAttention(h=16, d_model=1024, dropout=0.1)\n        self.mul_layernorm2 = LayerNorm(1024)\n\n        self.pff = PositionwiseFeedForward(1024, 2048)\n        self.mul_layernorm3 = LayerNorm(1024)\n\n    def forward(self, text, conv_feature, attention_map=None):\n        text_max_length = text.shape[1]\n        mask = subsequent_mask(text_max_length)\n        result = text\n        result = self.mul_layernorm1(\n            result + self.mask_multihead(text, text, text, mask=mask)[0]\n        )\n        b, c, h, w = conv_feature.shape\n        conv_feature = paddle.transpose(conv_feature.reshape([b, c, h * w]), [0, 2, 1])\n        word_image_align, attention_map = self.multihead(\n            result, conv_feature, conv_feature, mask=None, attention_map=attention_map\n        )\n        result = self.mul_layernorm2(result + word_image_align)\n        result = self.mul_layernorm3(result + self.pff(result))\n\n        return result, attention_map\n\n\nclass BasicBlock(nn.Layer):\n    def __init__(self, inplanes, planes, downsample):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2D(inplanes, planes, kernel_size=3, stride=1, padding=1)\n        self.bn1 = nn.BatchNorm2D(planes, use_global_stats=True)\n        self.relu = nn.ReLU()\n        self.conv2 = nn.Conv2D(planes, planes, kernel_size=3, stride=1, padding=1)\n        self.bn2 = nn.BatchNorm2D(planes, use_global_stats=True)\n        self.downsample = downsample\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample != None:\n            residual = self.downsample(residual)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Encoder(nn.Layer):\n    def __init__(self):\n        super(Encoder, self).__init__()\n        self.cnn = ResNet(num_in=1, block=BasicBlock, layers=[1, 2, 5, 3])\n\n    def forward(self, input):\n        conv_result = self.cnn(input)\n        return conv_result\n\n\nclass Transformer(nn.Layer):\n    def __init__(self, in_channels=1, alphabet=\"0123456789\"):\n        super(Transformer, self).__init__()\n        self.alphabet = alphabet\n        word_n_class = self.get_alphabet_len()\n        self.embedding_word_with_upperword = Embeddings(512, word_n_class)\n        self.pe = PositionalEncoding(dim=512, dropout=0.1, max_len=5000)\n\n        self.encoder = Encoder()\n        self.decoder = Decoder()\n        self.generator_word_with_upperword = Generator(1024, word_n_class)\n\n        for p in self.parameters():\n            if p.dim() > 1:\n                nn.initializer.XavierNormal(p)\n\n    def get_alphabet_len(self):\n        return len(self.alphabet)\n\n    def forward(self, image, text_length, text_input, attention_map=None):\n        if image.shape[1] == 3:\n            R = image[:, 0:1, :, :]\n            G = image[:, 1:2, :, :]\n            B = image[:, 2:3, :, :]\n            image = 0.299 * R + 0.587 * G + 0.114 * B\n\n        conv_feature = self.encoder(image)  # batch, 1024, 8, 32\n        max_length = max(text_length)\n        text_input = text_input[:, :max_length]\n\n        text_embedding = self.embedding_word_with_upperword(\n            text_input\n        )  # batch, text_max_length, 512\n        postion_embedding = self.pe(\n            paddle.zeros(text_embedding.shape)\n        )  # batch, text_max_length, 512\n        text_input_with_pe = paddle.concat(\n            [text_embedding, postion_embedding], 2\n        )  # batch, text_max_length, 1024\n        batch, seq_len, _ = text_input_with_pe.shape\n\n        text_input_with_pe, word_attention_map = self.decoder(\n            text_input_with_pe, conv_feature\n        )\n\n        word_decoder_result = self.generator_word_with_upperword(text_input_with_pe)\n\n        if self.training:\n            total_length = paddle.sum(text_length)\n            probs_res = paddle.zeros([total_length, self.get_alphabet_len()])\n            start = 0\n\n            for index, length in enumerate(text_length):\n                length = int(length.numpy())\n                probs_res[start : start + length, :] = word_decoder_result[\n                    index, 0 : 0 + length, :\n                ]\n\n                start = start + length\n\n            return probs_res, word_attention_map, None\n        else:\n            return word_decoder_result\n", "ppocr/modeling/heads/table_master_head.py": "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/JiaquanYe/TableMASTER-mmocr/blob/master/mmocr/models/textrecog/decoders/master_decoder.py\n\"\"\"\n\nimport copy\nimport math\nimport paddle\nfrom paddle import nn\nfrom paddle.nn import functional as F\n\n\nclass TableMasterHead(nn.Layer):\n    \"\"\"\n    Split to two transformer header at the last layer.\n    Cls_layer is used to structure token classification.\n    Bbox_layer is used to regress bbox coord.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels=30,\n        headers=8,\n        d_ff=2048,\n        dropout=0,\n        max_text_length=500,\n        loc_reg_num=4,\n        **kwargs,\n    ):\n        super(TableMasterHead, self).__init__()\n        hidden_size = in_channels[-1]\n        self.layers = clones(DecoderLayer(headers, hidden_size, dropout, d_ff), 2)\n        self.cls_layer = clones(DecoderLayer(headers, hidden_size, dropout, d_ff), 1)\n        self.bbox_layer = clones(DecoderLayer(headers, hidden_size, dropout, d_ff), 1)\n        self.cls_fc = nn.Linear(hidden_size, out_channels)\n        self.bbox_fc = nn.Sequential(\n            # nn.Linear(hidden_size, hidden_size),\n            nn.Linear(hidden_size, loc_reg_num),\n            nn.Sigmoid(),\n        )\n        self.norm = nn.LayerNorm(hidden_size)\n        self.embedding = Embeddings(d_model=hidden_size, vocab=out_channels)\n        self.positional_encoding = PositionalEncoding(d_model=hidden_size)\n\n        self.SOS = out_channels - 3\n        self.PAD = out_channels - 1\n        self.out_channels = out_channels\n        self.loc_reg_num = loc_reg_num\n        self.max_text_length = max_text_length\n\n    def make_mask(self, tgt):\n        \"\"\"\n        Make mask for self attention.\n        :param src: [b, c, h, l_src]\n        :param tgt: [b, l_tgt]\n        :return:\n        \"\"\"\n        trg_pad_mask = (tgt != self.PAD).unsqueeze(1).unsqueeze(3)\n\n        tgt_len = tgt.shape[1]\n        trg_sub_mask = paddle.tril(\n            paddle.ones(([tgt_len, tgt_len]), dtype=paddle.float32)\n        )\n\n        tgt_mask = paddle.logical_and(trg_pad_mask.astype(paddle.float32), trg_sub_mask)\n        return tgt_mask.astype(paddle.float32)\n\n    def decode(self, input, feature, src_mask, tgt_mask):\n        # main process of transformer decoder.\n        x = self.embedding(input)  # x: 1*x*512, feature: 1*3600,512\n        x = self.positional_encoding(x)\n\n        # origin transformer layers\n        for i, layer in enumerate(self.layers):\n            x = layer(x, feature, src_mask, tgt_mask)\n\n        # cls head\n        cls_x = x\n        for layer in self.cls_layer:\n            cls_x = layer(x, feature, src_mask, tgt_mask)\n        cls_x = self.norm(cls_x)\n\n        # bbox head\n        bbox_x = x\n        for layer in self.bbox_layer:\n            bbox_x = layer(x, feature, src_mask, tgt_mask)\n        bbox_x = self.norm(bbox_x)\n        return self.cls_fc(cls_x), self.bbox_fc(bbox_x)\n\n    def greedy_forward(self, SOS, feature):\n        input = SOS\n        output = paddle.zeros(\n            [input.shape[0], self.max_text_length + 1, self.out_channels]\n        )\n        bbox_output = paddle.zeros(\n            [input.shape[0], self.max_text_length + 1, self.loc_reg_num]\n        )\n        max_text_length = paddle.to_tensor(self.max_text_length)\n        for i in range(max_text_length + 1):\n            target_mask = self.make_mask(input)\n            out_step, bbox_output_step = self.decode(input, feature, None, target_mask)\n            prob = F.softmax(out_step, axis=-1)\n            next_word = prob.argmax(axis=2, dtype=\"int64\")\n            input = paddle.concat([input, next_word[:, -1].unsqueeze(-1)], axis=1)\n            if i == self.max_text_length:\n                output = out_step\n                bbox_output = bbox_output_step\n        return output, bbox_output\n\n    def forward_train(self, out_enc, targets):\n        # x is token of label\n        # feat is feature after backbone before pe.\n        # out_enc is feature after pe.\n        padded_targets = targets[0]\n        src_mask = None\n        tgt_mask = self.make_mask(padded_targets[:, :-1])\n        output, bbox_output = self.decode(\n            padded_targets[:, :-1], out_enc, src_mask, tgt_mask\n        )\n        return {\"structure_probs\": output, \"loc_preds\": bbox_output}\n\n    def forward_test(self, out_enc):\n        batch_size = out_enc.shape[0]\n        SOS = paddle.zeros([batch_size, 1], dtype=\"int64\") + self.SOS\n        output, bbox_output = self.greedy_forward(SOS, out_enc)\n        output = F.softmax(output)\n        return {\"structure_probs\": output, \"loc_preds\": bbox_output}\n\n    def forward(self, feat, targets=None):\n        feat = feat[-1]\n        b, c, h, w = feat.shape\n        feat = feat.reshape([b, c, h * w])  # flatten 2D feature map\n        feat = feat.transpose((0, 2, 1))\n        out_enc = self.positional_encoding(feat)\n        if self.training:\n            return self.forward_train(out_enc, targets)\n\n        return self.forward_test(out_enc)\n\n\nclass DecoderLayer(nn.Layer):\n    \"\"\"\n    Decoder is made of self attention, srouce attention and feed forward.\n    \"\"\"\n\n    def __init__(self, headers, d_model, dropout, d_ff):\n        super(DecoderLayer, self).__init__()\n        self.self_attn = MultiHeadAttention(headers, d_model, dropout)\n        self.src_attn = MultiHeadAttention(headers, d_model, dropout)\n        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n        self.sublayer = clones(SubLayerConnection(d_model, dropout), 3)\n\n    def forward(self, x, feature, src_mask, tgt_mask):\n        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n        x = self.sublayer[1](x, lambda x: self.src_attn(x, feature, feature, src_mask))\n        return self.sublayer[2](x, self.feed_forward)\n\n\nclass MultiHeadAttention(nn.Layer):\n    def __init__(self, headers, d_model, dropout):\n        super(MultiHeadAttention, self).__init__()\n\n        assert d_model % headers == 0\n        self.d_k = int(d_model / headers)\n        self.headers = headers\n        self.linears = clones(nn.Linear(d_model, d_model), 4)\n        self.attn = None\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, query, key, value, mask=None):\n        B = query.shape[0]\n\n        # 1) Do all the linear projections in batch from d_model => h x d_k\n        query, key, value = [\n            l(x).reshape([B, 0, self.headers, self.d_k]).transpose([0, 2, 1, 3])\n            for l, x in zip(self.linears, (query, key, value))\n        ]\n        # 2) Apply attention on all the projected vectors in batch\n        x, self.attn = self_attention(\n            query, key, value, mask=mask, dropout=self.dropout\n        )\n        x = x.transpose([0, 2, 1, 3]).reshape([B, 0, self.headers * self.d_k])\n        return self.linears[-1](x)\n\n\nclass FeedForward(nn.Layer):\n    def __init__(self, d_model, d_ff, dropout):\n        super(FeedForward, self).__init__()\n        self.w_1 = nn.Linear(d_model, d_ff)\n        self.w_2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n\n\nclass SubLayerConnection(nn.Layer):\n    \"\"\"\n    A residual connection followed by a layer norm.\n    Note for code simplicity the norm is first as opposed to last.\n    \"\"\"\n\n    def __init__(self, size, dropout):\n        super(SubLayerConnection, self).__init__()\n        self.norm = nn.LayerNorm(size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, sublayer):\n        return x + self.dropout(sublayer(self.norm(x)))\n\n\ndef masked_fill(x, mask, value):\n    mask = mask.astype(x.dtype)\n    return x * paddle.logical_not(mask).astype(x.dtype) + mask * value\n\n\ndef self_attention(query, key, value, mask=None, dropout=None):\n    \"\"\"\n    Compute 'Scale Dot Product Attention'\n    \"\"\"\n    d_k = value.shape[-1]\n\n    score = paddle.matmul(query, key.transpose([0, 1, 3, 2]) / math.sqrt(d_k))\n    if mask is not None:\n        # score = score.masked_fill(mask == 0, -1e9) # b, h, L, L\n        score = masked_fill(score, mask == 0, -6.55e4)  # for fp16\n\n    p_attn = F.softmax(score, axis=-1)\n\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n    return paddle.matmul(p_attn, value), p_attn\n\n\ndef clones(module, N):\n    \"\"\"Produce N identical layers\"\"\"\n    return nn.LayerList([copy.deepcopy(module) for _ in range(N)])\n\n\nclass Embeddings(nn.Layer):\n    def __init__(self, d_model, vocab):\n        super(Embeddings, self).__init__()\n        self.lut = nn.Embedding(vocab, d_model)\n        self.d_model = d_model\n\n    def forward(self, *input):\n        x = input[0]\n        return self.lut(x) * math.sqrt(self.d_model)\n\n\nclass PositionalEncoding(nn.Layer):\n    \"\"\"Implement the PE function.\"\"\"\n\n    def __init__(self, d_model, dropout=0.0, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        # Compute the positional encodings once in log space.\n        pe = paddle.zeros([max_len, d_model])\n        position = paddle.arange(0, max_len).unsqueeze(1).astype(\"float32\")\n        div_term = paddle.exp(\n            paddle.arange(0, d_model, 2) * -math.log(10000.0) / d_model\n        )\n        pe[:, 0::2] = paddle.sin(position * div_term)\n        pe[:, 1::2] = paddle.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer(\"pe\", pe)\n\n    def forward(self, feat, **kwargs):\n        feat = feat + self.pe[:, : feat.shape[1]]  # pe 1*5000*512\n        return self.dropout(feat)\n", "ppocr/modeling/heads/rec_srn_head.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport paddle\nfrom paddle import nn, ParamAttr\nfrom paddle.nn import functional as F\nimport numpy as np\nfrom .self_attention import WrapEncoderForFeature\nfrom .self_attention import WrapEncoder\nfrom paddle.static import Program\nfrom ppocr.modeling.backbones.rec_resnet_fpn import ResNetFPN\n\nfrom collections import OrderedDict\n\ngradient_clip = 10\n\n\nclass PVAM(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        char_num,\n        max_text_length,\n        num_heads,\n        num_encoder_tus,\n        hidden_dims,\n    ):\n        super(PVAM, self).__init__()\n        self.char_num = char_num\n        self.max_length = max_text_length\n        self.num_heads = num_heads\n        self.num_encoder_TUs = num_encoder_tus\n        self.hidden_dims = hidden_dims\n        # Transformer encoder\n        t = 256\n        c = 512\n        self.wrap_encoder_for_feature = WrapEncoderForFeature(\n            src_vocab_size=1,\n            max_length=t,\n            n_layer=self.num_encoder_TUs,\n            n_head=self.num_heads,\n            d_key=int(self.hidden_dims / self.num_heads),\n            d_value=int(self.hidden_dims / self.num_heads),\n            d_model=self.hidden_dims,\n            d_inner_hid=self.hidden_dims,\n            prepostprocess_dropout=0.1,\n            attention_dropout=0.1,\n            relu_dropout=0.1,\n            preprocess_cmd=\"n\",\n            postprocess_cmd=\"da\",\n            weight_sharing=True,\n        )\n\n        # PVAM\n        self.flatten0 = paddle.nn.Flatten(start_axis=0, stop_axis=1)\n        self.fc0 = paddle.nn.Linear(\n            in_features=in_channels,\n            out_features=in_channels,\n        )\n        self.emb = paddle.nn.Embedding(\n            num_embeddings=self.max_length, embedding_dim=in_channels\n        )\n        self.flatten1 = paddle.nn.Flatten(start_axis=0, stop_axis=2)\n        self.fc1 = paddle.nn.Linear(\n            in_features=in_channels, out_features=1, bias_attr=False\n        )\n\n    def forward(self, inputs, encoder_word_pos, gsrm_word_pos):\n        b, c, h, w = inputs.shape\n        conv_features = paddle.reshape(inputs, shape=[-1, c, h * w])\n        conv_features = paddle.transpose(conv_features, perm=[0, 2, 1])\n        # transformer encoder\n        b, t, c = conv_features.shape\n\n        enc_inputs = [conv_features, encoder_word_pos, None]\n        word_features = self.wrap_encoder_for_feature(enc_inputs)\n\n        # pvam\n        b, t, c = word_features.shape\n        word_features = self.fc0(word_features)\n        word_features_ = paddle.reshape(word_features, [-1, 1, t, c])\n        word_features_ = paddle.tile(word_features_, [1, self.max_length, 1, 1])\n        word_pos_feature = self.emb(gsrm_word_pos)\n        word_pos_feature_ = paddle.reshape(\n            word_pos_feature, [-1, self.max_length, 1, c]\n        )\n        word_pos_feature_ = paddle.tile(word_pos_feature_, [1, 1, t, 1])\n        y = word_pos_feature_ + word_features_\n        y = F.tanh(y)\n        attention_weight = self.fc1(y)\n        attention_weight = paddle.reshape(\n            attention_weight, shape=[-1, self.max_length, t]\n        )\n        attention_weight = F.softmax(attention_weight, axis=-1)\n        pvam_features = paddle.matmul(\n            attention_weight, word_features\n        )  # [b, max_length, c]\n        return pvam_features\n\n\nclass GSRM(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        char_num,\n        max_text_length,\n        num_heads,\n        num_encoder_tus,\n        num_decoder_tus,\n        hidden_dims,\n    ):\n        super(GSRM, self).__init__()\n        self.char_num = char_num\n        self.max_length = max_text_length\n        self.num_heads = num_heads\n        self.num_encoder_TUs = num_encoder_tus\n        self.num_decoder_TUs = num_decoder_tus\n        self.hidden_dims = hidden_dims\n\n        self.fc0 = paddle.nn.Linear(in_features=in_channels, out_features=self.char_num)\n        self.wrap_encoder0 = WrapEncoder(\n            src_vocab_size=self.char_num + 1,\n            max_length=self.max_length,\n            n_layer=self.num_decoder_TUs,\n            n_head=self.num_heads,\n            d_key=int(self.hidden_dims / self.num_heads),\n            d_value=int(self.hidden_dims / self.num_heads),\n            d_model=self.hidden_dims,\n            d_inner_hid=self.hidden_dims,\n            prepostprocess_dropout=0.1,\n            attention_dropout=0.1,\n            relu_dropout=0.1,\n            preprocess_cmd=\"n\",\n            postprocess_cmd=\"da\",\n            weight_sharing=True,\n        )\n\n        self.wrap_encoder1 = WrapEncoder(\n            src_vocab_size=self.char_num + 1,\n            max_length=self.max_length,\n            n_layer=self.num_decoder_TUs,\n            n_head=self.num_heads,\n            d_key=int(self.hidden_dims / self.num_heads),\n            d_value=int(self.hidden_dims / self.num_heads),\n            d_model=self.hidden_dims,\n            d_inner_hid=self.hidden_dims,\n            prepostprocess_dropout=0.1,\n            attention_dropout=0.1,\n            relu_dropout=0.1,\n            preprocess_cmd=\"n\",\n            postprocess_cmd=\"da\",\n            weight_sharing=True,\n        )\n\n        self.mul = lambda x: paddle.matmul(\n            x=x, y=self.wrap_encoder0.prepare_decoder.emb0.weight, transpose_y=True\n        )\n\n    def forward(self, inputs, gsrm_word_pos, gsrm_slf_attn_bias1, gsrm_slf_attn_bias2):\n        # ===== GSRM Visual-to-semantic embedding block =====\n        b, t, c = inputs.shape\n        pvam_features = paddle.reshape(inputs, [-1, c])\n        word_out = self.fc0(pvam_features)\n        word_ids = paddle.argmax(F.softmax(word_out), axis=1)\n        word_ids = paddle.reshape(x=word_ids, shape=[-1, t, 1])\n\n        # ===== GSRM Semantic reasoning block =====\n        \"\"\"\n        This module is achieved through bi-transformers,\n        ngram_feature1 is the froward one, ngram_fetaure2 is the backward one\n        \"\"\"\n        pad_idx = self.char_num\n\n        word1 = paddle.cast(word_ids, \"float32\")\n        word1 = F.pad(word1, [1, 0], value=1.0 * pad_idx, data_format=\"NLC\")\n        word1 = paddle.cast(word1, \"int64\")\n        word1 = word1[:, :-1, :]\n        word2 = word_ids\n\n        enc_inputs_1 = [word1, gsrm_word_pos, gsrm_slf_attn_bias1]\n        enc_inputs_2 = [word2, gsrm_word_pos, gsrm_slf_attn_bias2]\n\n        gsrm_feature1 = self.wrap_encoder0(enc_inputs_1)\n        gsrm_feature2 = self.wrap_encoder1(enc_inputs_2)\n\n        gsrm_feature2 = F.pad(gsrm_feature2, [0, 1], value=0.0, data_format=\"NLC\")\n        gsrm_feature2 = gsrm_feature2[\n            :,\n            1:,\n        ]\n        gsrm_features = gsrm_feature1 + gsrm_feature2\n\n        gsrm_out = self.mul(gsrm_features)\n\n        b, t, c = gsrm_out.shape\n        gsrm_out = paddle.reshape(gsrm_out, [-1, c])\n\n        return gsrm_features, word_out, gsrm_out\n\n\nclass VSFD(nn.Layer):\n    def __init__(self, in_channels=512, pvam_ch=512, char_num=38):\n        super(VSFD, self).__init__()\n        self.char_num = char_num\n        self.fc0 = paddle.nn.Linear(in_features=in_channels * 2, out_features=pvam_ch)\n        self.fc1 = paddle.nn.Linear(in_features=pvam_ch, out_features=self.char_num)\n\n    def forward(self, pvam_feature, gsrm_feature):\n        b, t, c1 = pvam_feature.shape\n        b, t, c2 = gsrm_feature.shape\n        combine_feature_ = paddle.concat([pvam_feature, gsrm_feature], axis=2)\n        img_comb_feature_ = paddle.reshape(combine_feature_, shape=[-1, c1 + c2])\n        img_comb_feature_map = self.fc0(img_comb_feature_)\n        img_comb_feature_map = F.sigmoid(img_comb_feature_map)\n        img_comb_feature_map = paddle.reshape(img_comb_feature_map, shape=[-1, t, c1])\n        combine_feature = (\n            img_comb_feature_map * pvam_feature\n            + (1.0 - img_comb_feature_map) * gsrm_feature\n        )\n        img_comb_feature = paddle.reshape(combine_feature, shape=[-1, c1])\n\n        out = self.fc1(img_comb_feature)\n        return out\n\n\nclass SRNHead(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        max_text_length,\n        num_heads,\n        num_encoder_TUs,\n        num_decoder_TUs,\n        hidden_dims,\n        **kwargs,\n    ):\n        super(SRNHead, self).__init__()\n        self.char_num = out_channels\n        self.max_length = max_text_length\n        self.num_heads = num_heads\n        self.num_encoder_TUs = num_encoder_TUs\n        self.num_decoder_TUs = num_decoder_TUs\n        self.hidden_dims = hidden_dims\n\n        self.pvam = PVAM(\n            in_channels=in_channels,\n            char_num=self.char_num,\n            max_text_length=self.max_length,\n            num_heads=self.num_heads,\n            num_encoder_tus=self.num_encoder_TUs,\n            hidden_dims=self.hidden_dims,\n        )\n\n        self.gsrm = GSRM(\n            in_channels=in_channels,\n            char_num=self.char_num,\n            max_text_length=self.max_length,\n            num_heads=self.num_heads,\n            num_encoder_tus=self.num_encoder_TUs,\n            num_decoder_tus=self.num_decoder_TUs,\n            hidden_dims=self.hidden_dims,\n        )\n        self.vsfd = VSFD(in_channels=in_channels, char_num=self.char_num)\n\n        self.gsrm.wrap_encoder1.prepare_decoder.emb0 = (\n            self.gsrm.wrap_encoder0.prepare_decoder.emb0\n        )\n\n    def forward(self, inputs, targets=None):\n        others = targets[-4:]\n        encoder_word_pos = others[0]\n        gsrm_word_pos = others[1]\n        gsrm_slf_attn_bias1 = others[2]\n        gsrm_slf_attn_bias2 = others[3]\n\n        pvam_feature = self.pvam(inputs, encoder_word_pos, gsrm_word_pos)\n\n        gsrm_feature, word_out, gsrm_out = self.gsrm(\n            pvam_feature, gsrm_word_pos, gsrm_slf_attn_bias1, gsrm_slf_attn_bias2\n        )\n\n        final_out = self.vsfd(pvam_feature, gsrm_feature)\n        if not self.training:\n            final_out = F.softmax(final_out, axis=1)\n\n        _, decoded_out = paddle.topk(final_out, k=1)\n\n        predicts = OrderedDict(\n            [\n                (\"predict\", final_out),\n                (\"pvam_feature\", pvam_feature),\n                (\"decoded_out\", decoded_out),\n                (\"word_out\", word_out),\n                (\"gsrm_out\", gsrm_out),\n            ]\n        )\n\n        return predicts\n", "ppocr/modeling/heads/proposal_local_graph.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/open-mmlab/mmocr/blob/main/mmocr/models/textdet/modules/proposal_local_graph.py\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport cv2\nimport numpy as np\nimport paddle\nimport paddle.nn as nn\nimport paddle.nn.functional as F\nfrom lanms import merge_quadrangle_n9 as la_nms\n\nfrom ppocr.ext_op import RoIAlignRotated\nfrom .local_graph import (\n    euclidean_distance_matrix,\n    feature_embedding,\n    normalize_adjacent_matrix,\n)\n\n\ndef fill_hole(input_mask):\n    h, w = input_mask.shape\n    canvas = np.zeros((h + 2, w + 2), np.uint8)\n    canvas[1 : h + 1, 1 : w + 1] = input_mask.copy()\n\n    mask = np.zeros((h + 4, w + 4), np.uint8)\n\n    cv2.floodFill(canvas, mask, (0, 0), 1)\n    canvas = canvas[1 : h + 1, 1 : w + 1].astype(np.bool_)\n\n    return ~canvas | input_mask\n\n\nclass ProposalLocalGraphs:\n    def __init__(\n        self,\n        k_at_hops,\n        num_adjacent_linkages,\n        node_geo_feat_len,\n        pooling_scale,\n        pooling_output_size,\n        nms_thr,\n        min_width,\n        max_width,\n        comp_shrink_ratio,\n        comp_w_h_ratio,\n        comp_score_thr,\n        text_region_thr,\n        center_region_thr,\n        center_region_area_thr,\n    ):\n        assert len(k_at_hops) == 2\n        assert isinstance(k_at_hops, tuple)\n        assert isinstance(num_adjacent_linkages, int)\n        assert isinstance(node_geo_feat_len, int)\n        assert isinstance(pooling_scale, float)\n        assert isinstance(pooling_output_size, tuple)\n        assert isinstance(nms_thr, float)\n        assert isinstance(min_width, float)\n        assert isinstance(max_width, float)\n        assert isinstance(comp_shrink_ratio, float)\n        assert isinstance(comp_w_h_ratio, float)\n        assert isinstance(comp_score_thr, float)\n        assert isinstance(text_region_thr, float)\n        assert isinstance(center_region_thr, float)\n        assert isinstance(center_region_area_thr, int)\n\n        self.k_at_hops = k_at_hops\n        self.active_connection = num_adjacent_linkages\n        self.local_graph_depth = len(self.k_at_hops)\n        self.node_geo_feat_dim = node_geo_feat_len\n        self.pooling = RoIAlignRotated(pooling_output_size, pooling_scale)\n        self.nms_thr = nms_thr\n        self.min_width = min_width\n        self.max_width = max_width\n        self.comp_shrink_ratio = comp_shrink_ratio\n        self.comp_w_h_ratio = comp_w_h_ratio\n        self.comp_score_thr = comp_score_thr\n        self.text_region_thr = text_region_thr\n        self.center_region_thr = center_region_thr\n        self.center_region_area_thr = center_region_area_thr\n\n    def propose_comps(\n        self,\n        score_map,\n        top_height_map,\n        bot_height_map,\n        sin_map,\n        cos_map,\n        comp_score_thr,\n        min_width,\n        max_width,\n        comp_shrink_ratio,\n        comp_w_h_ratio,\n    ):\n        \"\"\"Propose text components.\n\n        Args:\n            score_map (ndarray): The score map for NMS.\n            top_height_map (ndarray): The predicted text height map from each\n                pixel in text center region to top sideline.\n            bot_height_map (ndarray): The predicted text height map from each\n                pixel in text center region to bottom sideline.\n            sin_map (ndarray): The predicted sin(theta) map.\n            cos_map (ndarray): The predicted cos(theta) map.\n            comp_score_thr (float): The score threshold of text component.\n            min_width (float): The minimum width of text components.\n            max_width (float): The maximum width of text components.\n            comp_shrink_ratio (float): The shrink ratio of text components.\n            comp_w_h_ratio (float): The width to height ratio of text\n                components.\n\n        Returns:\n            text_comps (ndarray): The text components.\n        \"\"\"\n\n        comp_centers = np.argwhere(score_map > comp_score_thr)\n        comp_centers = comp_centers[np.argsort(comp_centers[:, 0])]\n        y = comp_centers[:, 0]\n        x = comp_centers[:, 1]\n\n        top_height = top_height_map[y, x].reshape((-1, 1)) * comp_shrink_ratio\n        bot_height = bot_height_map[y, x].reshape((-1, 1)) * comp_shrink_ratio\n        sin = sin_map[y, x].reshape((-1, 1))\n        cos = cos_map[y, x].reshape((-1, 1))\n\n        top_mid_pts = comp_centers + np.hstack([top_height * sin, top_height * cos])\n        bot_mid_pts = comp_centers - np.hstack([bot_height * sin, bot_height * cos])\n\n        width = (top_height + bot_height) * comp_w_h_ratio\n        width = np.clip(width, min_width, max_width)\n        r = width / 2\n\n        tl = top_mid_pts[:, ::-1] - np.hstack([-r * sin, r * cos])\n        tr = top_mid_pts[:, ::-1] + np.hstack([-r * sin, r * cos])\n        br = bot_mid_pts[:, ::-1] + np.hstack([-r * sin, r * cos])\n        bl = bot_mid_pts[:, ::-1] - np.hstack([-r * sin, r * cos])\n        text_comps = np.hstack([tl, tr, br, bl]).astype(np.float32)\n\n        score = score_map[y, x].reshape((-1, 1))\n        text_comps = np.hstack([text_comps, score])\n\n        return text_comps\n\n    def propose_comps_and_attribs(\n        self,\n        text_region_map,\n        center_region_map,\n        top_height_map,\n        bot_height_map,\n        sin_map,\n        cos_map,\n    ):\n        \"\"\"Generate text components and attributes.\n\n        Args:\n            text_region_map (ndarray): The predicted text region probability\n                map.\n            center_region_map (ndarray): The predicted text center region\n                probability map.\n            top_height_map (ndarray): The predicted text height map from each\n                pixel in text center region to top sideline.\n            bot_height_map (ndarray): The predicted text height map from each\n                pixel in text center region to bottom sideline.\n            sin_map (ndarray): The predicted sin(theta) map.\n            cos_map (ndarray): The predicted cos(theta) map.\n\n        Returns:\n            comp_attribs (ndarray): The text component attributes.\n            text_comps (ndarray): The text components.\n        \"\"\"\n\n        assert (\n            text_region_map.shape\n            == center_region_map.shape\n            == top_height_map.shape\n            == bot_height_map.shape\n            == sin_map.shape\n            == cos_map.shape\n        )\n        text_mask = text_region_map > self.text_region_thr\n        center_region_mask = (center_region_map > self.center_region_thr) * text_mask\n\n        scale = np.sqrt(1.0 / (sin_map**2 + cos_map**2 + 1e-8))\n        sin_map, cos_map = sin_map * scale, cos_map * scale\n\n        center_region_mask = fill_hole(center_region_mask)\n        center_region_contours, _ = cv2.findContours(\n            center_region_mask.astype(np.uint8), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE\n        )\n\n        mask_sz = center_region_map.shape\n        comp_list = []\n        for contour in center_region_contours:\n            current_center_mask = np.zeros(mask_sz)\n            cv2.drawContours(current_center_mask, [contour], -1, 1, -1)\n            if current_center_mask.sum() <= self.center_region_area_thr:\n                continue\n            score_map = text_region_map * current_center_mask\n\n            text_comps = self.propose_comps(\n                score_map,\n                top_height_map,\n                bot_height_map,\n                sin_map,\n                cos_map,\n                self.comp_score_thr,\n                self.min_width,\n                self.max_width,\n                self.comp_shrink_ratio,\n                self.comp_w_h_ratio,\n            )\n\n            text_comps = la_nms(text_comps, self.nms_thr)\n            text_comp_mask = np.zeros(mask_sz)\n            text_comp_boxes = text_comps[:, :8].reshape((-1, 4, 2)).astype(np.int32)\n\n            cv2.drawContours(text_comp_mask, text_comp_boxes, -1, 1, -1)\n            if (text_comp_mask * text_mask).sum() < text_comp_mask.sum() * 0.5:\n                continue\n            if text_comps.shape[-1] > 0:\n                comp_list.append(text_comps)\n\n        if len(comp_list) <= 0:\n            return None, None\n\n        text_comps = np.vstack(comp_list)\n        text_comp_boxes = text_comps[:, :8].reshape((-1, 4, 2))\n        centers = np.mean(text_comp_boxes, axis=1).astype(np.int32)\n        x = centers[:, 0]\n        y = centers[:, 1]\n\n        scores = []\n        for text_comp_box in text_comp_boxes:\n            text_comp_box[:, 0] = np.clip(text_comp_box[:, 0], 0, mask_sz[1] - 1)\n            text_comp_box[:, 1] = np.clip(text_comp_box[:, 1], 0, mask_sz[0] - 1)\n            min_coord = np.min(text_comp_box, axis=0).astype(np.int32)\n            max_coord = np.max(text_comp_box, axis=0).astype(np.int32)\n            text_comp_box = text_comp_box - min_coord\n            box_sz = max_coord - min_coord + 1\n            temp_comp_mask = np.zeros((box_sz[1], box_sz[0]), dtype=np.uint8)\n            cv2.fillPoly(temp_comp_mask, [text_comp_box.astype(np.int32)], 1)\n            temp_region_patch = text_region_map[\n                min_coord[1] : (max_coord[1] + 1), min_coord[0] : (max_coord[0] + 1)\n            ]\n            score = cv2.mean(temp_region_patch, temp_comp_mask)[0]\n            scores.append(score)\n        scores = np.array(scores).reshape((-1, 1))\n        text_comps = np.hstack([text_comps[:, :-1], scores])\n\n        h = top_height_map[y, x].reshape((-1, 1)) + bot_height_map[y, x].reshape(\n            (-1, 1)\n        )\n        w = np.clip(h * self.comp_w_h_ratio, self.min_width, self.max_width)\n        sin = sin_map[y, x].reshape((-1, 1))\n        cos = cos_map[y, x].reshape((-1, 1))\n\n        x = x.reshape((-1, 1))\n        y = y.reshape((-1, 1))\n        comp_attribs = np.hstack([x, y, h, w, cos, sin])\n\n        return comp_attribs, text_comps\n\n    def generate_local_graphs(self, sorted_dist_inds, node_feats):\n        \"\"\"Generate local graphs and graph convolution network input data.\n\n        Args:\n            sorted_dist_inds (ndarray): The node indices sorted according to\n                the Euclidean distance.\n            node_feats (tensor): The features of nodes in graph.\n\n        Returns:\n            local_graphs_node_feats (tensor): The features of nodes in local\n                graphs.\n            adjacent_matrices (tensor): The adjacent matrices.\n            pivots_knn_inds (tensor): The k-nearest neighbor indices in\n                local graphs.\n            pivots_local_graphs (tensor): The indices of nodes in local\n                graphs.\n        \"\"\"\n\n        assert sorted_dist_inds.ndim == 2\n        assert (\n            sorted_dist_inds.shape[0]\n            == sorted_dist_inds.shape[1]\n            == node_feats.shape[0]\n        )\n\n        knn_graph = sorted_dist_inds[:, 1 : self.k_at_hops[0] + 1]\n        pivot_local_graphs = []\n        pivot_knns = []\n\n        for pivot_ind, knn in enumerate(knn_graph):\n            local_graph_neighbors = set(knn)\n\n            for neighbor_ind in knn:\n                local_graph_neighbors.update(\n                    set(sorted_dist_inds[neighbor_ind, 1 : self.k_at_hops[1] + 1])\n                )\n\n            local_graph_neighbors.discard(pivot_ind)\n            pivot_local_graph = list(local_graph_neighbors)\n            pivot_local_graph.insert(0, pivot_ind)\n            pivot_knn = [pivot_ind] + list(knn)\n\n            pivot_local_graphs.append(pivot_local_graph)\n            pivot_knns.append(pivot_knn)\n\n        num_max_nodes = max(\n            [len(pivot_local_graph) for pivot_local_graph in pivot_local_graphs]\n        )\n\n        local_graphs_node_feat = []\n        adjacent_matrices = []\n        pivots_knn_inds = []\n        pivots_local_graphs = []\n\n        for graph_ind, pivot_knn in enumerate(pivot_knns):\n            pivot_local_graph = pivot_local_graphs[graph_ind]\n            num_nodes = len(pivot_local_graph)\n            pivot_ind = pivot_local_graph[0]\n            node2ind_map = {j: i for i, j in enumerate(pivot_local_graph)}\n\n            knn_inds = paddle.cast(\n                paddle.to_tensor([node2ind_map[i] for i in pivot_knn[1:]]), \"int64\"\n            )\n            pivot_feats = node_feats[pivot_ind]\n            normalized_feats = (\n                node_feats[paddle.to_tensor(pivot_local_graph)] - pivot_feats\n            )\n\n            adjacent_matrix = np.zeros((num_nodes, num_nodes), dtype=np.float32)\n            for node in pivot_local_graph:\n                neighbors = sorted_dist_inds[node, 1 : self.active_connection + 1]\n                for neighbor in neighbors:\n                    if neighbor in pivot_local_graph:\n                        adjacent_matrix[node2ind_map[node], node2ind_map[neighbor]] = 1\n                        adjacent_matrix[node2ind_map[neighbor], node2ind_map[node]] = 1\n\n            adjacent_matrix = normalize_adjacent_matrix(adjacent_matrix)\n            pad_adjacent_matrix = paddle.zeros(\n                (num_max_nodes, num_max_nodes),\n            )\n            pad_adjacent_matrix[:num_nodes, :num_nodes] = paddle.cast(\n                paddle.to_tensor(adjacent_matrix), \"float32\"\n            )\n\n            pad_normalized_feats = paddle.concat(\n                [\n                    normalized_feats,\n                    paddle.zeros(\n                        (num_max_nodes - num_nodes, normalized_feats.shape[1]),\n                    ),\n                ],\n                axis=0,\n            )\n\n            local_graph_nodes = paddle.to_tensor(pivot_local_graph)\n            local_graph_nodes = paddle.concat(\n                [\n                    local_graph_nodes,\n                    paddle.zeros([num_max_nodes - num_nodes], dtype=\"int64\"),\n                ],\n                axis=-1,\n            )\n\n            local_graphs_node_feat.append(pad_normalized_feats)\n            adjacent_matrices.append(pad_adjacent_matrix)\n            pivots_knn_inds.append(knn_inds)\n            pivots_local_graphs.append(local_graph_nodes)\n\n        local_graphs_node_feat = paddle.stack(local_graphs_node_feat, 0)\n        adjacent_matrices = paddle.stack(adjacent_matrices, 0)\n        pivots_knn_inds = paddle.stack(pivots_knn_inds, 0)\n        pivots_local_graphs = paddle.stack(pivots_local_graphs, 0)\n\n        return (\n            local_graphs_node_feat,\n            adjacent_matrices,\n            pivots_knn_inds,\n            pivots_local_graphs,\n        )\n\n    def __call__(self, preds, feat_maps):\n        \"\"\"Generate local graphs and graph convolutional network input data.\n\n        Args:\n            preds (tensor): The predicted maps.\n            feat_maps (tensor): The feature maps to extract content feature of\n                text components.\n\n        Returns:\n            none_flag (bool): The flag showing whether the number of proposed\n                text components is 0.\n            local_graphs_node_feats (tensor): The features of nodes in local\n                graphs.\n            adjacent_matrices (tensor): The adjacent matrices.\n            pivots_knn_inds (tensor): The k-nearest neighbor indices in\n                local graphs.\n            pivots_local_graphs (tensor): The indices of nodes in local\n                graphs.\n            text_comps (ndarray): The predicted text components.\n        \"\"\"\n        if preds.ndim == 4:\n            assert preds.shape[0] == 1\n            preds = paddle.squeeze(preds)\n        pred_text_region = F.sigmoid(preds[0]).numpy()\n        pred_center_region = F.sigmoid(preds[1]).numpy()\n        pred_sin_map = preds[2].numpy()\n        pred_cos_map = preds[3].numpy()\n        pred_top_height_map = preds[4].numpy()\n        pred_bot_height_map = preds[5].numpy()\n\n        comp_attribs, text_comps = self.propose_comps_and_attribs(\n            pred_text_region,\n            pred_center_region,\n            pred_top_height_map,\n            pred_bot_height_map,\n            pred_sin_map,\n            pred_cos_map,\n        )\n\n        if comp_attribs is None or len(comp_attribs) < 2:\n            none_flag = True\n            return none_flag, (0, 0, 0, 0, 0)\n\n        comp_centers = comp_attribs[:, 0:2]\n        distance_matrix = euclidean_distance_matrix(comp_centers, comp_centers)\n\n        geo_feats = feature_embedding(comp_attribs, self.node_geo_feat_dim)\n        geo_feats = paddle.to_tensor(geo_feats)\n\n        batch_id = np.zeros((comp_attribs.shape[0], 1), dtype=np.float32)\n        comp_attribs = comp_attribs.astype(np.float32)\n        angle = np.arccos(comp_attribs[:, -2]) * np.sign(comp_attribs[:, -1])\n        angle = angle.reshape((-1, 1))\n        rotated_rois = np.hstack([batch_id, comp_attribs[:, :-2], angle])\n        rois = paddle.to_tensor(rotated_rois)\n\n        content_feats = self.pooling(feat_maps, rois)\n        content_feats = content_feats.reshape([content_feats.shape[0], -1])\n        node_feats = paddle.concat([content_feats, geo_feats], axis=-1)\n\n        sorted_dist_inds = np.argsort(distance_matrix, axis=1)\n        (\n            local_graphs_node_feat,\n            adjacent_matrices,\n            pivots_knn_inds,\n            pivots_local_graphs,\n        ) = self.generate_local_graphs(sorted_dist_inds, node_feats)\n\n        none_flag = False\n        return none_flag, (\n            local_graphs_node_feat,\n            adjacent_matrices,\n            pivots_knn_inds,\n            pivots_local_graphs,\n            text_comps,\n        )\n", "ppocr/modeling/heads/__init__.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n__all__ = [\"build_head\"]\n\n\ndef build_head(config):\n    # det head\n    from .det_db_head import DBHead, PFHeadLocal\n    from .det_east_head import EASTHead\n    from .det_sast_head import SASTHead\n    from .det_pse_head import PSEHead\n    from .det_fce_head import FCEHead\n    from .e2e_pg_head import PGHead\n    from .det_ct_head import CT_Head\n\n    # rec head\n    from .rec_ctc_head import CTCHead\n    from .rec_att_head import AttentionHead\n    from .rec_srn_head import SRNHead\n    from .rec_nrtr_head import Transformer\n    from .rec_sar_head import SARHead\n    from .rec_aster_head import AsterHead\n    from .rec_pren_head import PRENHead\n    from .rec_multi_head import MultiHead\n    from .rec_spin_att_head import SPINAttentionHead\n    from .rec_abinet_head import ABINetHead\n    from .rec_robustscanner_head import RobustScannerHead\n    from .rec_visionlan_head import VLHead\n    from .rec_rfl_head import RFLHead\n    from .rec_can_head import CANHead\n    from .rec_satrn_head import SATRNHead\n    from .rec_parseq_head import ParseQHead\n    from .rec_cppd_head import CPPDHead\n\n    # cls head\n    from .cls_head import ClsHead\n\n    # kie head\n    from .kie_sdmgr_head import SDMGRHead\n\n    from .table_att_head import TableAttentionHead, SLAHead\n    from .table_master_head import TableMasterHead\n\n    support_dict = [\n        \"DBHead\",\n        \"PSEHead\",\n        \"FCEHead\",\n        \"EASTHead\",\n        \"SASTHead\",\n        \"CTCHead\",\n        \"ClsHead\",\n        \"AttentionHead\",\n        \"SRNHead\",\n        \"PGHead\",\n        \"Transformer\",\n        \"TableAttentionHead\",\n        \"SARHead\",\n        \"AsterHead\",\n        \"SDMGRHead\",\n        \"PRENHead\",\n        \"MultiHead\",\n        \"ABINetHead\",\n        \"TableMasterHead\",\n        \"SPINAttentionHead\",\n        \"VLHead\",\n        \"SLAHead\",\n        \"RobustScannerHead\",\n        \"CT_Head\",\n        \"RFLHead\",\n        \"DRRGHead\",\n        \"CANHead\",\n        \"SATRNHead\",\n        \"PFHeadLocal\",\n        \"ParseQHead\",\n        \"CPPDHead\",\n    ]\n\n    if config[\"name\"] == \"DRRGHead\":\n        from .det_drrg_head import DRRGHead\n\n        support_dict.append(\"DRRGHead\")\n\n    # table head\n\n    module_name = config.pop(\"name\")\n    assert module_name in support_dict, Exception(\n        \"head only support {}\".format(support_dict)\n    )\n    module_class = eval(module_name)(**config)\n    return module_class\n", "ppocr/modeling/heads/rec_att_head.py": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nimport paddle.nn as nn\nimport paddle.nn.functional as F\nimport numpy as np\n\n\nclass AttentionHead(nn.Layer):\n    def __init__(self, in_channels, out_channels, hidden_size, **kwargs):\n        super(AttentionHead, self).__init__()\n        self.input_size = in_channels\n        self.hidden_size = hidden_size\n        self.num_classes = out_channels\n\n        self.attention_cell = AttentionGRUCell(\n            in_channels, hidden_size, out_channels, use_gru=False\n        )\n        self.generator = nn.Linear(hidden_size, out_channels)\n\n    def _char_to_onehot(self, input_char, onehot_dim):\n        input_ont_hot = F.one_hot(input_char, onehot_dim)\n        return input_ont_hot\n\n    def forward(self, inputs, targets=None, batch_max_length=25):\n        batch_size = inputs.shape[0]\n        num_steps = batch_max_length\n\n        hidden = paddle.zeros((batch_size, self.hidden_size))\n        output_hiddens = []\n\n        if targets is not None:\n            for i in range(num_steps):\n                char_onehots = self._char_to_onehot(\n                    targets[:, i], onehot_dim=self.num_classes\n                )\n                (outputs, hidden), alpha = self.attention_cell(\n                    hidden, inputs, char_onehots\n                )\n                output_hiddens.append(paddle.unsqueeze(outputs, axis=1))\n            output = paddle.concat(output_hiddens, axis=1)\n            probs = self.generator(output)\n        else:\n            targets = paddle.zeros(shape=[batch_size], dtype=\"int32\")\n            probs = None\n            char_onehots = None\n            outputs = None\n            alpha = None\n\n            for i in range(num_steps):\n                char_onehots = self._char_to_onehot(\n                    targets, onehot_dim=self.num_classes\n                )\n                (outputs, hidden), alpha = self.attention_cell(\n                    hidden, inputs, char_onehots\n                )\n                probs_step = self.generator(outputs)\n                if probs is None:\n                    probs = paddle.unsqueeze(probs_step, axis=1)\n                else:\n                    probs = paddle.concat(\n                        [probs, paddle.unsqueeze(probs_step, axis=1)], axis=1\n                    )\n                next_input = probs_step.argmax(axis=1)\n                targets = next_input\n        if not self.training:\n            probs = paddle.nn.functional.softmax(probs, axis=2)\n        return probs\n\n\nclass AttentionGRUCell(nn.Layer):\n    def __init__(self, input_size, hidden_size, num_embeddings, use_gru=False):\n        super(AttentionGRUCell, self).__init__()\n        self.i2h = nn.Linear(input_size, hidden_size, bias_attr=False)\n        self.h2h = nn.Linear(hidden_size, hidden_size)\n        self.score = nn.Linear(hidden_size, 1, bias_attr=False)\n\n        self.rnn = nn.GRUCell(\n            input_size=input_size + num_embeddings, hidden_size=hidden_size\n        )\n\n        self.hidden_size = hidden_size\n\n    def forward(self, prev_hidden, batch_H, char_onehots):\n        batch_H_proj = self.i2h(batch_H)\n        prev_hidden_proj = paddle.unsqueeze(self.h2h(prev_hidden), axis=1)\n\n        res = paddle.add(batch_H_proj, prev_hidden_proj)\n        res = paddle.tanh(res)\n        e = self.score(res)\n\n        alpha = F.softmax(e, axis=1)\n        alpha = paddle.transpose(alpha, [0, 2, 1])\n        context = paddle.squeeze(paddle.mm(alpha, batch_H), axis=1)\n        concat_context = paddle.concat([context, char_onehots], 1)\n\n        cur_hidden = self.rnn(concat_context, prev_hidden)\n\n        return cur_hidden, alpha\n\n\nclass AttentionLSTM(nn.Layer):\n    def __init__(self, in_channels, out_channels, hidden_size, **kwargs):\n        super(AttentionLSTM, self).__init__()\n        self.input_size = in_channels\n        self.hidden_size = hidden_size\n        self.num_classes = out_channels\n\n        self.attention_cell = AttentionLSTMCell(\n            in_channels, hidden_size, out_channels, use_gru=False\n        )\n        self.generator = nn.Linear(hidden_size, out_channels)\n\n    def _char_to_onehot(self, input_char, onehot_dim):\n        input_ont_hot = F.one_hot(input_char, onehot_dim)\n        return input_ont_hot\n\n    def forward(self, inputs, targets=None, batch_max_length=25):\n        batch_size = inputs.shape[0]\n        num_steps = batch_max_length\n\n        hidden = (\n            paddle.zeros((batch_size, self.hidden_size)),\n            paddle.zeros((batch_size, self.hidden_size)),\n        )\n        output_hiddens = []\n\n        if targets is not None:\n            for i in range(num_steps):\n                # one-hot vectors for a i-th char\n                char_onehots = self._char_to_onehot(\n                    targets[:, i], onehot_dim=self.num_classes\n                )\n                hidden, alpha = self.attention_cell(hidden, inputs, char_onehots)\n\n                hidden = (hidden[1][0], hidden[1][1])\n                output_hiddens.append(paddle.unsqueeze(hidden[0], axis=1))\n            output = paddle.concat(output_hiddens, axis=1)\n            probs = self.generator(output)\n\n        else:\n            targets = paddle.zeros(shape=[batch_size], dtype=\"int32\")\n            probs = None\n            char_onehots = None\n            alpha = None\n\n            for i in range(num_steps):\n                char_onehots = self._char_to_onehot(\n                    targets, onehot_dim=self.num_classes\n                )\n                hidden, alpha = self.attention_cell(hidden, inputs, char_onehots)\n                probs_step = self.generator(hidden[0])\n                hidden = (hidden[1][0], hidden[1][1])\n                if probs is None:\n                    probs = paddle.unsqueeze(probs_step, axis=1)\n                else:\n                    probs = paddle.concat(\n                        [probs, paddle.unsqueeze(probs_step, axis=1)], axis=1\n                    )\n\n                next_input = probs_step.argmax(axis=1)\n\n                targets = next_input\n        if not self.training:\n            probs = paddle.nn.functional.softmax(probs, axis=2)\n        return probs\n\n\nclass AttentionLSTMCell(nn.Layer):\n    def __init__(self, input_size, hidden_size, num_embeddings, use_gru=False):\n        super(AttentionLSTMCell, self).__init__()\n        self.i2h = nn.Linear(input_size, hidden_size, bias_attr=False)\n        self.h2h = nn.Linear(hidden_size, hidden_size)\n        self.score = nn.Linear(hidden_size, 1, bias_attr=False)\n        if not use_gru:\n            self.rnn = nn.LSTMCell(\n                input_size=input_size + num_embeddings, hidden_size=hidden_size\n            )\n        else:\n            self.rnn = nn.GRUCell(\n                input_size=input_size + num_embeddings, hidden_size=hidden_size\n            )\n\n        self.hidden_size = hidden_size\n\n    def forward(self, prev_hidden, batch_H, char_onehots):\n        batch_H_proj = self.i2h(batch_H)\n        prev_hidden_proj = paddle.unsqueeze(self.h2h(prev_hidden[0]), axis=1)\n        res = paddle.add(batch_H_proj, prev_hidden_proj)\n        res = paddle.tanh(res)\n        e = self.score(res)\n\n        alpha = F.softmax(e, axis=1)\n        alpha = paddle.transpose(alpha, [0, 2, 1])\n        context = paddle.squeeze(paddle.mm(alpha, batch_H), axis=1)\n        concat_context = paddle.concat([context, char_onehots], 1)\n        cur_hidden = self.rnn(concat_context, prev_hidden)\n\n        return cur_hidden, alpha\n", "ppocr/modeling/heads/rec_spin_att_head.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nThis code is refer from:\nhttps://github.com/hikopensource/DAVAR-Lab-OCR/davarocr/davar_rcg/models/sequence_heads/att_head.py\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nimport paddle.nn as nn\nimport paddle.nn.functional as F\n\n\nclass SPINAttentionHead(nn.Layer):\n    def __init__(self, in_channels, out_channels, hidden_size, **kwargs):\n        super(SPINAttentionHead, self).__init__()\n        self.input_size = in_channels\n        self.hidden_size = hidden_size\n        self.num_classes = out_channels\n\n        self.attention_cell = AttentionLSTMCell(\n            in_channels, hidden_size, out_channels, use_gru=False\n        )\n        self.generator = nn.Linear(hidden_size, out_channels)\n\n    def _char_to_onehot(self, input_char, onehot_dim):\n        input_ont_hot = F.one_hot(input_char, onehot_dim)\n        return input_ont_hot\n\n    def forward(self, inputs, targets=None, batch_max_length=25):\n        batch_size = inputs.shape[0]\n        num_steps = batch_max_length + 1  # +1 for [sos] at end of sentence\n\n        hidden = (\n            paddle.zeros((batch_size, self.hidden_size)),\n            paddle.zeros((batch_size, self.hidden_size)),\n        )\n        output_hiddens = []\n        if self.training:  # for train\n            targets = targets[0]\n            for i in range(num_steps):\n                char_onehots = self._char_to_onehot(\n                    targets[:, i], onehot_dim=self.num_classes\n                )\n                (outputs, hidden), alpha = self.attention_cell(\n                    hidden, inputs, char_onehots\n                )\n                output_hiddens.append(paddle.unsqueeze(outputs, axis=1))\n            output = paddle.concat(output_hiddens, axis=1)\n            probs = self.generator(output)\n        else:\n            targets = paddle.zeros(shape=[batch_size], dtype=\"int32\")\n            probs = None\n            char_onehots = None\n            outputs = None\n            alpha = None\n\n            for i in range(num_steps):\n                char_onehots = self._char_to_onehot(\n                    targets, onehot_dim=self.num_classes\n                )\n                (outputs, hidden), alpha = self.attention_cell(\n                    hidden, inputs, char_onehots\n                )\n                probs_step = self.generator(outputs)\n                if probs is None:\n                    probs = paddle.unsqueeze(probs_step, axis=1)\n                else:\n                    probs = paddle.concat(\n                        [probs, paddle.unsqueeze(probs_step, axis=1)], axis=1\n                    )\n                next_input = probs_step.argmax(axis=1)\n                targets = next_input\n        if not self.training:\n            probs = paddle.nn.functional.softmax(probs, axis=2)\n        return probs\n\n\nclass AttentionLSTMCell(nn.Layer):\n    def __init__(self, input_size, hidden_size, num_embeddings, use_gru=False):\n        super(AttentionLSTMCell, self).__init__()\n        self.i2h = nn.Linear(input_size, hidden_size, bias_attr=False)\n        self.h2h = nn.Linear(hidden_size, hidden_size)\n        self.score = nn.Linear(hidden_size, 1, bias_attr=False)\n        if not use_gru:\n            self.rnn = nn.LSTMCell(\n                input_size=input_size + num_embeddings, hidden_size=hidden_size\n            )\n        else:\n            self.rnn = nn.GRUCell(\n                input_size=input_size + num_embeddings, hidden_size=hidden_size\n            )\n\n        self.hidden_size = hidden_size\n\n    def forward(self, prev_hidden, batch_H, char_onehots):\n        batch_H_proj = self.i2h(batch_H)\n        prev_hidden_proj = paddle.unsqueeze(self.h2h(prev_hidden[0]), axis=1)\n        res = paddle.add(batch_H_proj, prev_hidden_proj)\n        res = paddle.tanh(res)\n        e = self.score(res)\n\n        alpha = F.softmax(e, axis=1)\n        alpha = paddle.transpose(alpha, [0, 2, 1])\n        context = paddle.squeeze(paddle.mm(alpha, batch_H), axis=1)\n        concat_context = paddle.concat([context, char_onehots], 1)\n        cur_hidden = self.rnn(concat_context, prev_hidden)\n\n        return cur_hidden, alpha\n", "ppocr/modeling/heads/rec_multi_head.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport paddle\nfrom paddle import ParamAttr\nimport paddle.nn as nn\nimport paddle.nn.functional as F\n\nfrom ppocr.modeling.necks.rnn import (\n    Im2Seq,\n    EncoderWithRNN,\n    EncoderWithFC,\n    SequenceEncoder,\n    EncoderWithSVTR,\n    trunc_normal_,\n    zeros_,\n)\nfrom .rec_ctc_head import CTCHead\nfrom .rec_sar_head import SARHead\nfrom .rec_nrtr_head import Transformer\n\n\nclass FCTranspose(nn.Layer):\n    def __init__(self, in_channels, out_channels, only_transpose=False):\n        super().__init__()\n        self.only_transpose = only_transpose\n        if not self.only_transpose:\n            self.fc = nn.Linear(in_channels, out_channels, bias_attr=False)\n\n    def forward(self, x):\n        if self.only_transpose:\n            return x.transpose([0, 2, 1])\n        else:\n            return self.fc(x.transpose([0, 2, 1]))\n\n\nclass AddPos(nn.Layer):\n    def __init__(self, dim, w):\n        super().__init__()\n        self.dec_pos_embed = self.create_parameter(\n            shape=[1, w, dim], default_initializer=zeros_\n        )\n        self.add_parameter(\"dec_pos_embed\", self.dec_pos_embed)\n        trunc_normal_(self.dec_pos_embed)\n\n    def forward(self, x):\n        x = x + self.dec_pos_embed[:, : x.shape[1], :]\n        return x\n\n\nclass MultiHead(nn.Layer):\n    def __init__(self, in_channels, out_channels_list, **kwargs):\n        super().__init__()\n        self.head_list = kwargs.pop(\"head_list\")\n        self.use_pool = kwargs.get(\"use_pool\", False)\n        self.use_pos = kwargs.get(\"use_pos\", False)\n        self.in_channels = in_channels\n        if self.use_pool:\n            self.pool = nn.AvgPool2D(kernel_size=[3, 2], stride=[3, 2], padding=0)\n        self.gtc_head = \"sar\"\n        assert len(self.head_list) >= 2\n        for idx, head_name in enumerate(self.head_list):\n            name = list(head_name)[0]\n            if name == \"SARHead\":\n                # sar head\n                sar_args = self.head_list[idx][name]\n                self.sar_head = eval(name)(\n                    in_channels=in_channels,\n                    out_channels=out_channels_list[\"SARLabelDecode\"],\n                    **sar_args,\n                )\n            elif name == \"NRTRHead\":\n                gtc_args = self.head_list[idx][name]\n                max_text_length = gtc_args.get(\"max_text_length\", 25)\n                nrtr_dim = gtc_args.get(\"nrtr_dim\", 256)\n                num_decoder_layers = gtc_args.get(\"num_decoder_layers\", 4)\n                if self.use_pos:\n                    self.before_gtc = nn.Sequential(\n                        nn.Flatten(2),\n                        FCTranspose(in_channels, nrtr_dim),\n                        AddPos(nrtr_dim, 80),\n                    )\n                else:\n                    self.before_gtc = nn.Sequential(\n                        nn.Flatten(2), FCTranspose(in_channels, nrtr_dim)\n                    )\n\n                self.gtc_head = Transformer(\n                    d_model=nrtr_dim,\n                    nhead=nrtr_dim // 32,\n                    num_encoder_layers=-1,\n                    beam_size=-1,\n                    num_decoder_layers=num_decoder_layers,\n                    max_len=max_text_length,\n                    dim_feedforward=nrtr_dim * 4,\n                    out_channels=out_channels_list[\"NRTRLabelDecode\"],\n                )\n            elif name == \"CTCHead\":\n                # ctc neck\n                self.encoder_reshape = Im2Seq(in_channels)\n                neck_args = self.head_list[idx][name][\"Neck\"]\n                encoder_type = neck_args.pop(\"name\")\n                self.ctc_encoder = SequenceEncoder(\n                    in_channels=in_channels, encoder_type=encoder_type, **neck_args\n                )\n                # ctc head\n                head_args = self.head_list[idx][name][\"Head\"]\n                self.ctc_head = eval(name)(\n                    in_channels=self.ctc_encoder.out_channels,\n                    out_channels=out_channels_list[\"CTCLabelDecode\"],\n                    **head_args,\n                )\n            else:\n                raise NotImplementedError(\n                    \"{} is not supported in MultiHead yet\".format(name)\n                )\n\n    def forward(self, x, targets=None):\n        if self.use_pool:\n            x = self.pool(\n                x.reshape([0, 3, -1, self.in_channels]).transpose([0, 3, 1, 2])\n            )\n        ctc_encoder = self.ctc_encoder(x)\n        ctc_out = self.ctc_head(ctc_encoder, targets)\n        head_out = dict()\n        head_out[\"ctc\"] = ctc_out\n        head_out[\"ctc_neck\"] = ctc_encoder\n        # eval mode\n        if not self.training:\n            return ctc_out\n        if self.gtc_head == \"sar\":\n            sar_out = self.sar_head(x, targets[1:])\n            head_out[\"sar\"] = sar_out\n        else:\n            gtc_out = self.gtc_head(self.before_gtc(x), targets[1:])\n            head_out[\"gtc\"] = gtc_out\n        return head_out\n", "ppocr/modeling/heads/det_east_head.py": "# copyright (c) 2019 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport paddle\nfrom paddle import nn\nimport paddle.nn.functional as F\nfrom paddle import ParamAttr\n\n\nclass ConvBNLayer(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride,\n        padding,\n        groups=1,\n        if_act=True,\n        act=None,\n        name=None,\n    ):\n        super(ConvBNLayer, self).__init__()\n        self.if_act = if_act\n        self.act = act\n        self.conv = nn.Conv2D(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            groups=groups,\n            weight_attr=ParamAttr(name=name + \"_weights\"),\n            bias_attr=False,\n        )\n\n        self.bn = nn.BatchNorm(\n            num_channels=out_channels,\n            act=act,\n            param_attr=ParamAttr(name=\"bn_\" + name + \"_scale\"),\n            bias_attr=ParamAttr(name=\"bn_\" + name + \"_offset\"),\n            moving_mean_name=\"bn_\" + name + \"_mean\",\n            moving_variance_name=\"bn_\" + name + \"_variance\",\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return x\n\n\nclass EASTHead(nn.Layer):\n    \"\"\" \"\"\"\n\n    def __init__(self, in_channels, model_name, **kwargs):\n        super(EASTHead, self).__init__()\n        self.model_name = model_name\n        if self.model_name == \"large\":\n            num_outputs = [128, 64, 1, 8]\n        else:\n            num_outputs = [64, 32, 1, 8]\n\n        self.det_conv1 = ConvBNLayer(\n            in_channels=in_channels,\n            out_channels=num_outputs[0],\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            if_act=True,\n            act=\"relu\",\n            name=\"det_head1\",\n        )\n        self.det_conv2 = ConvBNLayer(\n            in_channels=num_outputs[0],\n            out_channels=num_outputs[1],\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            if_act=True,\n            act=\"relu\",\n            name=\"det_head2\",\n        )\n        self.score_conv = ConvBNLayer(\n            in_channels=num_outputs[1],\n            out_channels=num_outputs[2],\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            if_act=False,\n            act=None,\n            name=\"f_score\",\n        )\n        self.geo_conv = ConvBNLayer(\n            in_channels=num_outputs[1],\n            out_channels=num_outputs[3],\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            if_act=False,\n            act=None,\n            name=\"f_geo\",\n        )\n\n    def forward(self, x, targets=None):\n        f_det = self.det_conv1(x)\n        f_det = self.det_conv2(f_det)\n        f_score = self.score_conv(f_det)\n        f_score = F.sigmoid(f_score)\n        f_geo = self.geo_conv(f_det)\n        f_geo = (F.sigmoid(f_geo) - 0.5) * 2 * 800\n\n        pred = {\"f_score\": f_score, \"f_geo\": f_geo}\n        return pred\n", "ppocr/modeling/heads/gcn.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/open-mmlab/mmocr/blob/main/mmocr/models/textdet/modules/gcn.py\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nimport paddle.nn as nn\nimport paddle.nn.functional as F\n\n\nclass BatchNorm1D(nn.BatchNorm1D):\n    def __init__(\n        self,\n        num_features,\n        eps=1e-05,\n        momentum=0.1,\n        affine=True,\n        track_running_stats=True,\n    ):\n        momentum = 1 - momentum\n        weight_attr = None\n        bias_attr = None\n        if not affine:\n            weight_attr = paddle.ParamAttr(learning_rate=0.0)\n            bias_attr = paddle.ParamAttr(learning_rate=0.0)\n        super().__init__(\n            num_features,\n            momentum=momentum,\n            epsilon=eps,\n            weight_attr=weight_attr,\n            bias_attr=bias_attr,\n            use_global_stats=track_running_stats,\n        )\n\n\nclass MeanAggregator(nn.Layer):\n    def forward(self, features, A):\n        x = paddle.bmm(A, features)\n        return x\n\n\nclass GraphConv(nn.Layer):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.in_dim = in_dim\n        self.out_dim = out_dim\n        self.weight = self.create_parameter(\n            [in_dim * 2, out_dim], default_initializer=nn.initializer.XavierUniform()\n        )\n        self.bias = self.create_parameter(\n            [out_dim],\n            is_bias=True,\n            default_initializer=nn.initializer.Assign([0] * out_dim),\n        )\n\n        self.aggregator = MeanAggregator()\n\n    def forward(self, features, A):\n        b, n, d = features.shape\n        assert d == self.in_dim\n        agg_feats = self.aggregator(features, A)\n        cat_feats = paddle.concat([features, agg_feats], axis=2)\n        out = paddle.einsum(\"bnd,df->bnf\", cat_feats, self.weight)\n        out = F.relu(out + self.bias)\n        return out\n\n\nclass GCN(nn.Layer):\n    def __init__(self, feat_len):\n        super(GCN, self).__init__()\n        self.bn0 = BatchNorm1D(feat_len, affine=False)\n        self.conv1 = GraphConv(feat_len, 512)\n        self.conv2 = GraphConv(512, 256)\n        self.conv3 = GraphConv(256, 128)\n        self.conv4 = GraphConv(128, 64)\n        self.classifier = nn.Sequential(\n            nn.Linear(64, 32), nn.PReLU(32), nn.Linear(32, 2)\n        )\n\n    def forward(self, x, A, knn_inds):\n        num_local_graphs, num_max_nodes, feat_len = x.shape\n\n        x = x.reshape([-1, feat_len])\n        x = self.bn0(x)\n        x = x.reshape([num_local_graphs, num_max_nodes, feat_len])\n\n        x = self.conv1(x, A)\n        x = self.conv2(x, A)\n        x = self.conv3(x, A)\n        x = self.conv4(x, A)\n        k = knn_inds.shape[-1]\n        mid_feat_len = x.shape[-1]\n        edge_feat = paddle.zeros([num_local_graphs, k, mid_feat_len])\n        for graph_ind in range(num_local_graphs):\n            edge_feat[graph_ind, :, :] = x[graph_ind][\n                paddle.to_tensor(knn_inds[graph_ind])\n            ]\n        edge_feat = edge_feat.reshape([-1, mid_feat_len])\n        pred = self.classifier(edge_feat)\n\n        return pred\n", "ppocr/modeling/transforms/tsrn.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/FudanVI/FudanOCR/blob/main/text-gestalt/model/tsrn.py\n\"\"\"\n\nimport math\nimport paddle\nimport paddle.nn.functional as F\nfrom paddle import nn\nfrom collections import OrderedDict\nimport sys\nimport numpy as np\nimport warnings\nimport math, copy\nimport cv2\n\nwarnings.filterwarnings(\"ignore\")\n\nfrom .tps_spatial_transformer import TPSSpatialTransformer\nfrom .stn import STN as STN_model\nfrom ppocr.modeling.heads.sr_rensnet_transformer import Transformer\n\n\nclass TSRN(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        scale_factor=2,\n        width=128,\n        height=32,\n        STN=False,\n        srb_nums=5,\n        mask=False,\n        hidden_units=32,\n        infer_mode=False,\n        **kwargs,\n    ):\n        super(TSRN, self).__init__()\n        in_planes = 3\n        if mask:\n            in_planes = 4\n        assert math.log(scale_factor, 2) % 1 == 0\n        upsample_block_num = int(math.log(scale_factor, 2))\n        self.block1 = nn.Sequential(\n            nn.Conv2D(in_planes, 2 * hidden_units, kernel_size=9, padding=4), nn.PReLU()\n        )\n        self.srb_nums = srb_nums\n        for i in range(srb_nums):\n            setattr(self, \"block%d\" % (i + 2), RecurrentResidualBlock(2 * hidden_units))\n\n        setattr(\n            self,\n            \"block%d\" % (srb_nums + 2),\n            nn.Sequential(\n                nn.Conv2D(2 * hidden_units, 2 * hidden_units, kernel_size=3, padding=1),\n                nn.BatchNorm2D(2 * hidden_units),\n            ),\n        )\n\n        block_ = [UpsampleBLock(2 * hidden_units, 2) for _ in range(upsample_block_num)]\n        block_.append(nn.Conv2D(2 * hidden_units, in_planes, kernel_size=9, padding=4))\n        setattr(self, \"block%d\" % (srb_nums + 3), nn.Sequential(*block_))\n        self.tps_inputsize = [height // scale_factor, width // scale_factor]\n        tps_outputsize = [height // scale_factor, width // scale_factor]\n        num_control_points = 20\n        tps_margins = [0.05, 0.05]\n        self.stn = STN\n        if self.stn:\n            self.tps = TPSSpatialTransformer(\n                output_image_size=tuple(tps_outputsize),\n                num_control_points=num_control_points,\n                margins=tuple(tps_margins),\n            )\n\n            self.stn_head = STN_model(\n                in_channels=in_planes,\n                num_ctrlpoints=num_control_points,\n                activation=\"none\",\n            )\n        self.out_channels = in_channels\n\n        self.r34_transformer = Transformer()\n        for param in self.r34_transformer.parameters():\n            param.trainable = False\n        self.infer_mode = infer_mode\n\n    def forward(self, x):\n        output = {}\n        if self.infer_mode:\n            output[\"lr_img\"] = x\n            y = x\n        else:\n            output[\"lr_img\"] = x[0]\n            output[\"hr_img\"] = x[1]\n            y = x[0]\n        if self.stn and self.training:\n            _, ctrl_points_x = self.stn_head(y)\n            y, _ = self.tps(y, ctrl_points_x)\n        block = {\"1\": self.block1(y)}\n        for i in range(self.srb_nums + 1):\n            block[str(i + 2)] = getattr(self, \"block%d\" % (i + 2))(block[str(i + 1)])\n\n        block[str(self.srb_nums + 3)] = getattr(self, \"block%d\" % (self.srb_nums + 3))(\n            (block[\"1\"] + block[str(self.srb_nums + 2)])\n        )\n\n        sr_img = paddle.tanh(block[str(self.srb_nums + 3)])\n\n        output[\"sr_img\"] = sr_img\n\n        if self.training:\n            hr_img = x[1]\n            length = x[2]\n            input_tensor = x[3]\n\n            # add transformer\n            sr_pred, word_attention_map_pred, _ = self.r34_transformer(\n                sr_img, length, input_tensor\n            )\n\n            hr_pred, word_attention_map_gt, _ = self.r34_transformer(\n                hr_img, length, input_tensor\n            )\n\n            output[\"hr_img\"] = hr_img\n            output[\"hr_pred\"] = hr_pred\n            output[\"word_attention_map_gt\"] = word_attention_map_gt\n            output[\"sr_pred\"] = sr_pred\n            output[\"word_attention_map_pred\"] = word_attention_map_pred\n\n        return output\n\n\nclass RecurrentResidualBlock(nn.Layer):\n    def __init__(self, channels):\n        super(RecurrentResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2D(channels, channels, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2D(channels)\n        self.gru1 = GruBlock(channels, channels)\n        self.prelu = mish()\n        self.conv2 = nn.Conv2D(channels, channels, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2D(channels)\n        self.gru2 = GruBlock(channels, channels)\n\n    def forward(self, x):\n        residual = self.conv1(x)\n        residual = self.bn1(residual)\n        residual = self.prelu(residual)\n        residual = self.conv2(residual)\n        residual = self.bn2(residual)\n        residual = self.gru1(residual.transpose([0, 1, 3, 2])).transpose([0, 1, 3, 2])\n\n        return self.gru2(x + residual)\n\n\nclass UpsampleBLock(nn.Layer):\n    def __init__(self, in_channels, up_scale):\n        super(UpsampleBLock, self).__init__()\n        self.conv = nn.Conv2D(\n            in_channels, in_channels * up_scale**2, kernel_size=3, padding=1\n        )\n\n        self.pixel_shuffle = nn.PixelShuffle(up_scale)\n        self.prelu = mish()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.pixel_shuffle(x)\n        x = self.prelu(x)\n        return x\n\n\nclass mish(nn.Layer):\n    def __init__(\n        self,\n    ):\n        super(mish, self).__init__()\n        self.activated = True\n\n    def forward(self, x):\n        if self.activated:\n            x = x * (paddle.tanh(F.softplus(x)))\n        return x\n\n\nclass GruBlock(nn.Layer):\n    def __init__(self, in_channels, out_channels):\n        super(GruBlock, self).__init__()\n        assert out_channels % 2 == 0\n        self.conv1 = nn.Conv2D(in_channels, out_channels, kernel_size=1, padding=0)\n        self.gru = nn.GRU(out_channels, out_channels // 2, direction=\"bidirectional\")\n\n    def forward(self, x):\n        # x: b, c, w, h\n        x = self.conv1(x)\n        x = x.transpose([0, 2, 3, 1])  # b, w, h, c\n        batch_size, w, h, c = x.shape\n        x = x.reshape([-1, h, c])  # b*w, h, c\n        x, _ = self.gru(x)\n        x = x.reshape([-1, w, h, c])\n        x = x.transpose([0, 3, 1, 2])\n        return x\n", "ppocr/modeling/transforms/stn.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/ayumiymk/aster.pytorch/blob/master/lib/models/stn_head.py\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport paddle\nfrom paddle import nn, ParamAttr\nfrom paddle.nn import functional as F\nimport numpy as np\n\nfrom .tps_spatial_transformer import TPSSpatialTransformer\n\n\ndef conv3x3_block(in_channels, out_channels, stride=1):\n    n = 3 * 3 * out_channels\n    w = math.sqrt(2.0 / n)\n    conv_layer = nn.Conv2D(\n        in_channels,\n        out_channels,\n        kernel_size=3,\n        stride=stride,\n        padding=1,\n        weight_attr=nn.initializer.Normal(mean=0.0, std=w),\n        bias_attr=nn.initializer.Constant(0),\n    )\n    block = nn.Sequential(conv_layer, nn.BatchNorm2D(out_channels), nn.ReLU())\n    return block\n\n\nclass STN(nn.Layer):\n    def __init__(self, in_channels, num_ctrlpoints, activation=\"none\"):\n        super(STN, self).__init__()\n        self.in_channels = in_channels\n        self.num_ctrlpoints = num_ctrlpoints\n        self.activation = activation\n        self.stn_convnet = nn.Sequential(\n            conv3x3_block(in_channels, 32),  # 32x64\n            nn.MaxPool2D(kernel_size=2, stride=2),\n            conv3x3_block(32, 64),  # 16x32\n            nn.MaxPool2D(kernel_size=2, stride=2),\n            conv3x3_block(64, 128),  # 8*16\n            nn.MaxPool2D(kernel_size=2, stride=2),\n            conv3x3_block(128, 256),  # 4*8\n            nn.MaxPool2D(kernel_size=2, stride=2),\n            conv3x3_block(256, 256),  # 2*4,\n            nn.MaxPool2D(kernel_size=2, stride=2),\n            conv3x3_block(256, 256),\n        )  # 1*2\n        self.stn_fc1 = nn.Sequential(\n            nn.Linear(\n                2 * 256,\n                512,\n                weight_attr=nn.initializer.Normal(0, 0.001),\n                bias_attr=nn.initializer.Constant(0),\n            ),\n            nn.BatchNorm1D(512),\n            nn.ReLU(),\n        )\n        fc2_bias = self.init_stn()\n        self.stn_fc2 = nn.Linear(\n            512,\n            num_ctrlpoints * 2,\n            weight_attr=nn.initializer.Constant(0.0),\n            bias_attr=nn.initializer.Assign(fc2_bias),\n        )\n\n    def init_stn(self):\n        margin = 0.01\n        sampling_num_per_side = int(self.num_ctrlpoints / 2)\n        ctrl_pts_x = np.linspace(margin, 1.0 - margin, sampling_num_per_side)\n        ctrl_pts_y_top = np.ones(sampling_num_per_side) * margin\n        ctrl_pts_y_bottom = np.ones(sampling_num_per_side) * (1 - margin)\n        ctrl_pts_top = np.stack([ctrl_pts_x, ctrl_pts_y_top], axis=1)\n        ctrl_pts_bottom = np.stack([ctrl_pts_x, ctrl_pts_y_bottom], axis=1)\n        ctrl_points = np.concatenate([ctrl_pts_top, ctrl_pts_bottom], axis=0).astype(\n            np.float32\n        )\n        if self.activation == \"none\":\n            pass\n        elif self.activation == \"sigmoid\":\n            ctrl_points = -np.log(1.0 / ctrl_points - 1.0)\n        ctrl_points = paddle.to_tensor(ctrl_points)\n        fc2_bias = paddle.reshape(\n            ctrl_points, shape=[ctrl_points.shape[0] * ctrl_points.shape[1]]\n        )\n        return fc2_bias\n\n    def forward(self, x):\n        x = self.stn_convnet(x)\n        batch_size, _, h, w = x.shape\n        x = paddle.reshape(x, shape=(batch_size, -1))\n        img_feat = self.stn_fc1(x)\n        x = self.stn_fc2(0.1 * img_feat)\n        if self.activation == \"sigmoid\":\n            x = F.sigmoid(x)\n        x = paddle.reshape(x, shape=[-1, self.num_ctrlpoints, 2])\n        return img_feat, x\n\n\nclass STN_ON(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        tps_inputsize,\n        tps_outputsize,\n        num_control_points,\n        tps_margins,\n        stn_activation,\n    ):\n        super(STN_ON, self).__init__()\n        self.tps = TPSSpatialTransformer(\n            output_image_size=tuple(tps_outputsize),\n            num_control_points=num_control_points,\n            margins=tuple(tps_margins),\n        )\n        self.stn_head = STN(\n            in_channels=in_channels,\n            num_ctrlpoints=num_control_points,\n            activation=stn_activation,\n        )\n        self.tps_inputsize = tps_inputsize\n        self.out_channels = in_channels\n\n    def forward(self, image):\n        stn_input = paddle.nn.functional.interpolate(\n            image, self.tps_inputsize, mode=\"bilinear\", align_corners=True\n        )\n        stn_img_feat, ctrl_points = self.stn_head(stn_input)\n        x, _ = self.tps(image, ctrl_points)\n        return x\n", "ppocr/modeling/transforms/tbsrn.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/FudanVI/FudanOCR/blob/main/scene-text-telescope/model/tbsrn.py\n\"\"\"\n\nimport math\nimport warnings\nimport numpy as np\nimport paddle\nfrom paddle import nn\nimport string\n\nwarnings.filterwarnings(\"ignore\")\n\nfrom .tps_spatial_transformer import TPSSpatialTransformer\nfrom .stn import STN as STNHead\nfrom .tsrn import GruBlock, mish, UpsampleBLock\nfrom ppocr.modeling.heads.sr_rensnet_transformer import (\n    Transformer,\n    LayerNorm,\n    PositionwiseFeedForward,\n    MultiHeadedAttention,\n)\n\n\ndef positionalencoding2d(d_model, height, width):\n    \"\"\"\n    :param d_model: dimension of the model\n    :param height: height of the positions\n    :param width: width of the positions\n    :return: d_model*height*width position matrix\n    \"\"\"\n    if d_model % 4 != 0:\n        raise ValueError(\n            \"Cannot use sin/cos positional encoding with \"\n            \"odd dimension (got dim={:d})\".format(d_model)\n        )\n    pe = paddle.zeros([d_model, height, width])\n    # Each dimension use half of d_model\n    d_model = int(d_model / 2)\n    div_term = paddle.exp(\n        paddle.arange(0.0, d_model, 2, dtype=\"int64\") * -(math.log(10000.0) / d_model)\n    )\n    pos_w = paddle.arange(0.0, width, dtype=\"float32\").unsqueeze(1)\n    pos_h = paddle.arange(0.0, height, dtype=\"float32\").unsqueeze(1)\n\n    pe[0:d_model:2, :, :] = (\n        paddle.sin(pos_w * div_term).transpose([1, 0]).unsqueeze(1).tile([1, height, 1])\n    )\n    pe[1:d_model:2, :, :] = (\n        paddle.cos(pos_w * div_term).transpose([1, 0]).unsqueeze(1).tile([1, height, 1])\n    )\n    pe[d_model::2, :, :] = (\n        paddle.sin(pos_h * div_term).transpose([1, 0]).unsqueeze(2).tile([1, 1, width])\n    )\n    pe[d_model + 1 :: 2, :, :] = (\n        paddle.cos(pos_h * div_term).transpose([1, 0]).unsqueeze(2).tile([1, 1, width])\n    )\n\n    return pe\n\n\nclass FeatureEnhancer(nn.Layer):\n    def __init__(self):\n        super(FeatureEnhancer, self).__init__()\n\n        self.multihead = MultiHeadedAttention(h=4, d_model=128, dropout=0.1)\n        self.mul_layernorm1 = LayerNorm(features=128)\n\n        self.pff = PositionwiseFeedForward(128, 128)\n        self.mul_layernorm3 = LayerNorm(features=128)\n\n        self.linear = nn.Linear(128, 64)\n\n    def forward(self, conv_feature):\n        \"\"\"\n        text : (batch, seq_len, embedding_size)\n        global_info: (batch, embedding_size, 1, 1)\n        conv_feature: (batch, channel, H, W)\n        \"\"\"\n        batch = conv_feature.shape[0]\n        position2d = (\n            positionalencoding2d(64, 16, 64)\n            .cast(\"float32\")\n            .unsqueeze(0)\n            .reshape([1, 64, 1024])\n        )\n        position2d = position2d.tile([batch, 1, 1])\n        conv_feature = paddle.concat(\n            [conv_feature, position2d], 1\n        )  # batch, 128(64+64), 32, 128\n        result = conv_feature.transpose([0, 2, 1])\n        origin_result = result\n        result = self.mul_layernorm1(\n            origin_result + self.multihead(result, result, result, mask=None)[0]\n        )\n        origin_result = result\n        result = self.mul_layernorm3(origin_result + self.pff(result))\n        result = self.linear(result)\n        return result.transpose([0, 2, 1])\n\n\ndef str_filt(str_, voc_type):\n    alpha_dict = {\n        \"digit\": string.digits,\n        \"lower\": string.digits + string.ascii_lowercase,\n        \"upper\": string.digits + string.ascii_letters,\n        \"all\": string.digits + string.ascii_letters + string.punctuation,\n    }\n    if voc_type == \"lower\":\n        str_ = str_.lower()\n    for char in str_:\n        if char not in alpha_dict[voc_type]:\n            str_ = str_.replace(char, \"\")\n    str_ = str_.lower()\n    return str_\n\n\nclass TBSRN(nn.Layer):\n    def __init__(\n        self,\n        in_channels=3,\n        scale_factor=2,\n        width=128,\n        height=32,\n        STN=True,\n        srb_nums=5,\n        mask=False,\n        hidden_units=32,\n        infer_mode=False,\n    ):\n        super(TBSRN, self).__init__()\n        in_planes = 3\n        if mask:\n            in_planes = 4\n        assert math.log(scale_factor, 2) % 1 == 0\n        upsample_block_num = int(math.log(scale_factor, 2))\n        self.block1 = nn.Sequential(\n            nn.Conv2D(in_planes, 2 * hidden_units, kernel_size=9, padding=4),\n            nn.PReLU(),\n            # nn.ReLU()\n        )\n        self.srb_nums = srb_nums\n        for i in range(srb_nums):\n            setattr(self, \"block%d\" % (i + 2), RecurrentResidualBlock(2 * hidden_units))\n\n        setattr(\n            self,\n            \"block%d\" % (srb_nums + 2),\n            nn.Sequential(\n                nn.Conv2D(2 * hidden_units, 2 * hidden_units, kernel_size=3, padding=1),\n                nn.BatchNorm2D(2 * hidden_units),\n            ),\n        )\n\n        # self.non_local = NonLocalBlock2D(64, 64)\n        block_ = [UpsampleBLock(2 * hidden_units, 2) for _ in range(upsample_block_num)]\n        block_.append(nn.Conv2D(2 * hidden_units, in_planes, kernel_size=9, padding=4))\n        setattr(self, \"block%d\" % (srb_nums + 3), nn.Sequential(*block_))\n        self.tps_inputsize = [height // scale_factor, width // scale_factor]\n        tps_outputsize = [height // scale_factor, width // scale_factor]\n        num_control_points = 20\n        tps_margins = [0.05, 0.05]\n        self.stn = STN\n        self.out_channels = in_channels\n        if self.stn:\n            self.tps = TPSSpatialTransformer(\n                output_image_size=tuple(tps_outputsize),\n                num_control_points=num_control_points,\n                margins=tuple(tps_margins),\n            )\n\n            self.stn_head = STNHead(\n                in_channels=in_planes,\n                num_ctrlpoints=num_control_points,\n                activation=\"none\",\n            )\n        self.infer_mode = infer_mode\n\n        self.english_alphabet = (\n            \"-0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n        )\n        self.english_dict = {}\n        for index in range(len(self.english_alphabet)):\n            self.english_dict[self.english_alphabet[index]] = index\n        transformer = Transformer(alphabet=\"-0123456789abcdefghijklmnopqrstuvwxyz\")\n        self.transformer = transformer\n        for param in self.transformer.parameters():\n            param.trainable = False\n\n    def label_encoder(self, label):\n        batch = len(label)\n\n        length = [len(i) for i in label]\n        length_tensor = paddle.to_tensor(length, dtype=\"int64\")\n\n        max_length = max(length)\n        input_tensor = np.zeros((batch, max_length))\n        for i in range(batch):\n            for j in range(length[i] - 1):\n                input_tensor[i][j + 1] = self.english_dict[label[i][j]]\n\n        text_gt = []\n        for i in label:\n            for j in i:\n                text_gt.append(self.english_dict[j])\n        text_gt = paddle.to_tensor(text_gt, dtype=\"int64\")\n\n        input_tensor = paddle.to_tensor(input_tensor, dtype=\"int64\")\n        return length_tensor, input_tensor, text_gt\n\n    def forward(self, x):\n        output = {}\n        if self.infer_mode:\n            output[\"lr_img\"] = x\n            y = x\n        else:\n            output[\"lr_img\"] = x[0]\n            output[\"hr_img\"] = x[1]\n            y = x[0]\n        if self.stn and self.training:\n            _, ctrl_points_x = self.stn_head(y)\n            y, _ = self.tps(y, ctrl_points_x)\n        block = {\"1\": self.block1(y)}\n        for i in range(self.srb_nums + 1):\n            block[str(i + 2)] = getattr(self, \"block%d\" % (i + 2))(block[str(i + 1)])\n\n        block[str(self.srb_nums + 3)] = getattr(self, \"block%d\" % (self.srb_nums + 3))(\n            (block[\"1\"] + block[str(self.srb_nums + 2)])\n        )\n\n        sr_img = paddle.tanh(block[str(self.srb_nums + 3)])\n        output[\"sr_img\"] = sr_img\n\n        if self.training:\n            hr_img = x[1]\n\n            # add transformer\n            label = [str_filt(i, \"lower\") + \"-\" for i in x[2]]\n            length_tensor, input_tensor, text_gt = self.label_encoder(label)\n            hr_pred, word_attention_map_gt, hr_correct_list = self.transformer(\n                hr_img, length_tensor, input_tensor\n            )\n            sr_pred, word_attention_map_pred, sr_correct_list = self.transformer(\n                sr_img, length_tensor, input_tensor\n            )\n            output[\"hr_img\"] = hr_img\n            output[\"hr_pred\"] = hr_pred\n            output[\"text_gt\"] = text_gt\n            output[\"word_attention_map_gt\"] = word_attention_map_gt\n            output[\"sr_pred\"] = sr_pred\n            output[\"word_attention_map_pred\"] = word_attention_map_pred\n\n        return output\n\n\nclass RecurrentResidualBlock(nn.Layer):\n    def __init__(self, channels):\n        super(RecurrentResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2D(channels, channels, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2D(channels)\n        self.gru1 = GruBlock(channels, channels)\n        # self.prelu = nn.ReLU()\n        self.prelu = mish()\n        self.conv2 = nn.Conv2D(channels, channels, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2D(channels)\n        self.gru2 = GruBlock(channels, channels)\n        self.feature_enhancer = FeatureEnhancer()\n\n        for p in self.parameters():\n            if p.dim() > 1:\n                paddle.nn.initializer.XavierUniform(p)\n\n    def forward(self, x):\n        residual = self.conv1(x)\n        residual = self.bn1(residual)\n        residual = self.prelu(residual)\n        residual = self.conv2(residual)\n        residual = self.bn2(residual)\n\n        size = residual.shape\n        residual = residual.reshape([size[0], size[1], -1])\n        residual = self.feature_enhancer(residual)\n        residual = residual.reshape([size[0], size[1], size[2], size[3]])\n        return x + residual\n", "ppocr/modeling/transforms/tps_spatial_transformer.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/ayumiymk/aster.pytorch/blob/master/lib/models/tps_spatial_transformer.py\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport paddle\nfrom paddle import nn, ParamAttr\nfrom paddle.nn import functional as F\nimport numpy as np\nimport itertools\n\n\ndef grid_sample(input, grid, canvas=None):\n    input.stop_gradient = False\n\n    is_fp16 = False\n    if grid.dtype != paddle.float32:\n        data_type = grid.dtype\n        input = input.cast(paddle.float32)\n        grid = grid.cast(paddle.float32)\n        is_fp16 = True\n    output = F.grid_sample(input, grid)\n    if is_fp16:\n        output = output.cast(data_type)\n        grid = grid.cast(data_type)\n\n    if canvas is None:\n        return output\n    else:\n        input_mask = paddle.ones(shape=input.shape)\n        if is_fp16:\n            input_mask = input_mask.cast(paddle.float32)\n            grid = grid.cast(paddle.float32)\n        output_mask = F.grid_sample(input_mask, grid)\n        if is_fp16:\n            output_mask = output_mask.cast(data_type)\n        padded_output = output * output_mask + canvas * (1 - output_mask)\n        return padded_output\n\n\n# phi(x1, x2) = r^2 * log(r), where r = ||x1 - x2||_2\ndef compute_partial_repr(input_points, control_points):\n    N = input_points.shape[0]\n    M = control_points.shape[0]\n    pairwise_diff = paddle.reshape(input_points, shape=[N, 1, 2]) - paddle.reshape(\n        control_points, shape=[1, M, 2]\n    )\n    # original implementation, very slow\n    # pairwise_dist = torch.sum(pairwise_diff ** 2, dim = 2) # square of distance\n    pairwise_diff_square = pairwise_diff * pairwise_diff\n    pairwise_dist = pairwise_diff_square[:, :, 0] + pairwise_diff_square[:, :, 1]\n    repr_matrix = 0.5 * pairwise_dist * paddle.log(pairwise_dist)\n    # fix numerical error for 0 * log(0), substitute all nan with 0\n    mask = np.array(repr_matrix != repr_matrix)\n    repr_matrix[mask] = 0\n    return repr_matrix\n\n\n# output_ctrl_pts are specified, according to our task.\ndef build_output_control_points(num_control_points, margins):\n    margin_x, margin_y = margins\n    num_ctrl_pts_per_side = num_control_points // 2\n    ctrl_pts_x = np.linspace(margin_x, 1.0 - margin_x, num_ctrl_pts_per_side)\n    ctrl_pts_y_top = np.ones(num_ctrl_pts_per_side) * margin_y\n    ctrl_pts_y_bottom = np.ones(num_ctrl_pts_per_side) * (1.0 - margin_y)\n    ctrl_pts_top = np.stack([ctrl_pts_x, ctrl_pts_y_top], axis=1)\n    ctrl_pts_bottom = np.stack([ctrl_pts_x, ctrl_pts_y_bottom], axis=1)\n    output_ctrl_pts_arr = np.concatenate([ctrl_pts_top, ctrl_pts_bottom], axis=0)\n    output_ctrl_pts = paddle.to_tensor(output_ctrl_pts_arr)\n    return output_ctrl_pts\n\n\nclass TPSSpatialTransformer(nn.Layer):\n    def __init__(self, output_image_size=None, num_control_points=None, margins=None):\n        super(TPSSpatialTransformer, self).__init__()\n        self.output_image_size = output_image_size\n        self.num_control_points = num_control_points\n        self.margins = margins\n\n        self.target_height, self.target_width = output_image_size\n        target_control_points = build_output_control_points(num_control_points, margins)\n        N = num_control_points\n\n        # create padded kernel matrix\n        forward_kernel = paddle.zeros(shape=[N + 3, N + 3])\n        target_control_partial_repr = compute_partial_repr(\n            target_control_points, target_control_points\n        )\n        target_control_partial_repr = paddle.cast(\n            target_control_partial_repr, forward_kernel.dtype\n        )\n        forward_kernel[:N, :N] = target_control_partial_repr\n        forward_kernel[:N, -3] = 1\n        forward_kernel[-3, :N] = 1\n        target_control_points = paddle.cast(target_control_points, forward_kernel.dtype)\n        forward_kernel[:N, -2:] = target_control_points\n        forward_kernel[-2:, :N] = paddle.transpose(target_control_points, perm=[1, 0])\n        # compute inverse matrix\n        inverse_kernel = paddle.inverse(forward_kernel)\n\n        # create target cordinate matrix\n        HW = self.target_height * self.target_width\n        target_coordinate = list(\n            itertools.product(range(self.target_height), range(self.target_width))\n        )\n        target_coordinate = paddle.to_tensor(target_coordinate)  # HW x 2\n        Y, X = paddle.split(target_coordinate, target_coordinate.shape[1], axis=1)\n        Y = Y / (self.target_height - 1)\n        X = X / (self.target_width - 1)\n        target_coordinate = paddle.concat(\n            [X, Y], axis=1\n        )  # convert from (y, x) to (x, y)\n        target_coordinate_partial_repr = compute_partial_repr(\n            target_coordinate, target_control_points\n        )\n        target_coordinate_repr = paddle.concat(\n            [\n                target_coordinate_partial_repr,\n                paddle.ones(shape=[HW, 1]),\n                target_coordinate,\n            ],\n            axis=1,\n        )\n\n        # register precomputed matrices\n        self.inverse_kernel = inverse_kernel\n        self.padding_matrix = paddle.zeros(shape=[3, 2])\n        self.target_coordinate_repr = target_coordinate_repr\n        self.target_control_points = target_control_points\n\n    def forward(self, input, source_control_points):\n        assert source_control_points.ndimension() == 3\n        assert source_control_points.shape[1] == self.num_control_points\n        assert source_control_points.shape[2] == 2\n        batch_size = source_control_points.shape[0]\n\n        padding_matrix = paddle.expand(self.padding_matrix, shape=[batch_size, 3, 2])\n        Y = paddle.concat(\n            [source_control_points.astype(padding_matrix.dtype), padding_matrix], 1\n        )\n        mapping_matrix = paddle.matmul(self.inverse_kernel, Y)\n        source_coordinate = paddle.matmul(self.target_coordinate_repr, mapping_matrix)\n\n        grid = paddle.reshape(\n            source_coordinate, shape=[-1, self.target_height, self.target_width, 2]\n        )\n        grid = paddle.clip(\n            grid, 0, 1\n        )  # the source_control_points may be out of [0, 1].\n        # the input to grid_sample is normalized [-1, 1], but what we get is [0, 1]\n        grid = 2.0 * grid - 1.0\n        output_maps = grid_sample(input, grid, canvas=None)\n        return output_maps, source_coordinate\n", "ppocr/modeling/transforms/gaspin_transformer.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport paddle\nfrom paddle import nn, ParamAttr\nfrom paddle.nn import functional as F\nimport numpy as np\nimport functools\nfrom .tps import GridGenerator\n\n\"\"\"This code is refer from:\nhttps://github.com/hikopensource/DAVAR-Lab-OCR/davarocr/davar_rcg/models/transformations/gaspin_transformation.py\n\"\"\"\n\n\nclass SP_TransformerNetwork(nn.Layer):\n    \"\"\"\n    Sturture-Preserving Transformation (SPT) as Equa. (2) in Ref. [1]\n    Ref: [1] SPIN: Structure-Preserving Inner Offset Network for Scene Text Recognition. AAAI-2021.\n    \"\"\"\n\n    def __init__(self, nc=1, default_type=5):\n        \"\"\"Based on SPIN\n        Args:\n            nc (int): number of input channels (usually in 1 or 3)\n            default_type (int): the complexity of transformation intensities (by default set to 6 as the paper)\n        \"\"\"\n        super(SP_TransformerNetwork, self).__init__()\n        self.power_list = self.cal_K(default_type)\n        self.sigmoid = nn.Sigmoid()\n        self.bn = nn.InstanceNorm2D(nc)\n\n    def cal_K(self, k=5):\n        \"\"\"\n\n        Args:\n            k (int): the complexity of transformation intensities (by default set to 6 as the paper)\n\n        Returns:\n            List: the normalized intensity of each pixel in [0,1], denoted as \\beta [1x(2K+1)]\n\n        \"\"\"\n        from math import log\n\n        x = []\n        if k != 0:\n            for i in range(1, k + 1):\n                lower = round(\n                    log(1 - (0.5 / (k + 1)) * i) / log((0.5 / (k + 1)) * i), 2\n                )\n                upper = round(1 / lower, 2)\n                x.append(lower)\n                x.append(upper)\n        x.append(1.00)\n        return x\n\n    def forward(self, batch_I, weights, offsets, lambda_color=None):\n        \"\"\"\n\n        Args:\n            batch_I (Tensor): batch of input images [batch_size x nc x I_height x I_width]\n            weights:\n            offsets: the predicted offset by AIN, a scalar\n            lambda_color: the learnable update gate \\alpha in Equa. (5) as\n                          g(x) = (1 - \\alpha) \\odot x + \\alpha \\odot x_{offsets}\n\n        Returns:\n            Tensor: transformed images by SPN as Equa. (4) in Ref. [1]\n                        [batch_size x I_channel_num x I_r_height x I_r_width]\n\n        \"\"\"\n        batch_I = (batch_I + 1) * 0.5\n        if offsets is not None:\n            batch_I = batch_I * (1 - lambda_color) + offsets * lambda_color\n        batch_weight_params = paddle.unsqueeze(paddle.unsqueeze(weights, -1), -1)\n        batch_I_power = paddle.stack([batch_I.pow(p) for p in self.power_list], axis=1)\n\n        batch_weight_sum = paddle.sum(batch_I_power * batch_weight_params, axis=1)\n        batch_weight_sum = self.bn(batch_weight_sum)\n        batch_weight_sum = self.sigmoid(batch_weight_sum)\n        batch_weight_sum = batch_weight_sum * 2 - 1\n        return batch_weight_sum\n\n\nclass GA_SPIN_Transformer(nn.Layer):\n    \"\"\"\n    Geometric-Absorbed SPIN Transformation (GA-SPIN) proposed in Ref. [1]\n\n\n    Ref: [1] SPIN: Structure-Preserving Inner Offset Network for Scene Text Recognition. AAAI-2021.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels=1,\n        I_r_size=(32, 100),\n        offsets=False,\n        norm_type=\"BN\",\n        default_type=6,\n        loc_lr=1,\n        stn=True,\n    ):\n        \"\"\"\n        Args:\n            in_channels (int): channel of input features,\n                                set it to 1 if the grayscale images and 3 if RGB input\n            I_r_size (tuple): size of rectified images (used in STN transformations)\n            offsets (bool): set it to False if use SPN w.o. AIN,\n                            and set it to True if use SPIN (both with SPN and AIN)\n            norm_type (str): the normalization type of the module,\n                            set it to 'BN' by default, 'IN' optionally\n            default_type (int): the K chromatic space,\n                                set it to 3/5/6 depend on the complexity of transformation intensities\n            loc_lr (float): learning rate of location network\n            stn (bool): whther to use stn.\n\n        \"\"\"\n        super(GA_SPIN_Transformer, self).__init__()\n        self.nc = in_channels\n        self.spt = True\n        self.offsets = offsets\n        self.stn = stn  # set to True in GA-SPIN, while set it to False in SPIN\n        self.I_r_size = I_r_size\n        self.out_channels = in_channels\n        if norm_type == \"BN\":\n            norm_layer = functools.partial(nn.BatchNorm2D, use_global_stats=True)\n        elif norm_type == \"IN\":\n            norm_layer = functools.partial(\n                nn.InstanceNorm2D, weight_attr=False, use_global_stats=False\n            )\n        else:\n            raise NotImplementedError(\n                \"normalization layer [%s] is not found\" % norm_type\n            )\n\n        if self.spt:\n            self.sp_net = SP_TransformerNetwork(in_channels, default_type)\n            self.spt_convnet = nn.Sequential(\n                # 32*100\n                nn.Conv2D(in_channels, 32, 3, 1, 1, bias_attr=False),\n                norm_layer(32),\n                nn.ReLU(),\n                nn.MaxPool2D(kernel_size=2, stride=2),\n                # 16*50\n                nn.Conv2D(32, 64, 3, 1, 1, bias_attr=False),\n                norm_layer(64),\n                nn.ReLU(),\n                nn.MaxPool2D(kernel_size=2, stride=2),\n                # 8*25\n                nn.Conv2D(64, 128, 3, 1, 1, bias_attr=False),\n                norm_layer(128),\n                nn.ReLU(),\n                nn.MaxPool2D(kernel_size=2, stride=2),\n                # 4*12\n            )\n            self.stucture_fc1 = nn.Sequential(\n                nn.Conv2D(128, 256, 3, 1, 1, bias_attr=False),\n                norm_layer(256),\n                nn.ReLU(),\n                nn.MaxPool2D(kernel_size=2, stride=2),\n                nn.Conv2D(256, 256, 3, 1, 1, bias_attr=False),\n                norm_layer(256),\n                nn.ReLU(),  # 2*6\n                nn.MaxPool2D(kernel_size=2, stride=2),\n                nn.Conv2D(256, 512, 3, 1, 1, bias_attr=False),\n                norm_layer(512),\n                nn.ReLU(),  # 1*3\n                nn.AdaptiveAvgPool2D(1),\n                nn.Flatten(1, -1),  # batch_size x 512\n                nn.Linear(512, 256, weight_attr=nn.initializer.Normal(0.001)),\n                nn.BatchNorm1D(256),\n                nn.ReLU(),\n            )\n            self.out_weight = 2 * default_type + 1\n            self.spt_length = 2 * default_type + 1\n            if offsets:\n                self.out_weight += 1\n            if self.stn:\n                self.F = 20\n                self.out_weight += self.F * 2\n                self.GridGenerator = GridGenerator(self.F * 2, self.F)\n\n            # self.out_weight*=nc\n            # Init structure_fc2 in LocalizationNetwork\n            initial_bias = self.init_spin(default_type * 2)\n            initial_bias = initial_bias.reshape(-1)\n            param_attr = ParamAttr(\n                learning_rate=loc_lr,\n                initializer=nn.initializer.Assign(np.zeros([256, self.out_weight])),\n            )\n            bias_attr = ParamAttr(\n                learning_rate=loc_lr, initializer=nn.initializer.Assign(initial_bias)\n            )\n            self.stucture_fc2 = nn.Linear(\n                256, self.out_weight, weight_attr=param_attr, bias_attr=bias_attr\n            )\n            self.sigmoid = nn.Sigmoid()\n\n            if offsets:\n                self.offset_fc1 = nn.Sequential(\n                    nn.Conv2D(128, 16, 3, 1, 1, bias_attr=False),\n                    norm_layer(16),\n                    nn.ReLU(),\n                )\n                self.offset_fc2 = nn.Conv2D(16, in_channels, 3, 1, 1)\n                self.pool = nn.MaxPool2D(2, 2)\n\n    def init_spin(self, nz):\n        \"\"\"\n        Args:\n            nz (int): number of paired \\betas exponents, which means the value of K x 2\n\n        \"\"\"\n        init_id = [0.00] * nz + [5.00]\n        if self.offsets:\n            init_id += [-5.00]\n            # init_id *=3\n        init = np.array(init_id)\n\n        if self.stn:\n            F = self.F\n            ctrl_pts_x = np.linspace(-1.0, 1.0, int(F / 2))\n            ctrl_pts_y_top = np.linspace(0.0, -1.0, num=int(F / 2))\n            ctrl_pts_y_bottom = np.linspace(1.0, 0.0, num=int(F / 2))\n            ctrl_pts_top = np.stack([ctrl_pts_x, ctrl_pts_y_top], axis=1)\n            ctrl_pts_bottom = np.stack([ctrl_pts_x, ctrl_pts_y_bottom], axis=1)\n            initial_bias = np.concatenate([ctrl_pts_top, ctrl_pts_bottom], axis=0)\n            initial_bias = initial_bias.reshape(-1)\n            init = np.concatenate([init, initial_bias], axis=0)\n        return init\n\n    def forward(self, x, return_weight=False):\n        \"\"\"\n        Args:\n            x (Tensor): input image batch\n            return_weight (bool): set to False by default,\n                                  if set to True return the predicted offsets of AIN, denoted as x_{offsets}\n\n        Returns:\n            Tensor: rectified image [batch_size x I_channel_num x I_height x I_width], the same as the input size\n        \"\"\"\n\n        if self.spt:\n            feat = self.spt_convnet(x)\n            fc1 = self.stucture_fc1(feat)\n            sp_weight_fusion = self.stucture_fc2(fc1)\n            sp_weight_fusion = sp_weight_fusion.reshape(\n                [x.shape[0], self.out_weight, 1]\n            )\n            if self.offsets:  # SPIN w. AIN\n                lambda_color = sp_weight_fusion[:, self.spt_length, 0]\n                lambda_color = (\n                    self.sigmoid(lambda_color).unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n                )\n                sp_weight = sp_weight_fusion[:, : self.spt_length, :]\n                offsets = self.pool(self.offset_fc2(self.offset_fc1(feat)))\n\n                assert offsets.shape[2] == 2  # 2\n                assert offsets.shape[3] == 6  # 16\n                offsets = self.sigmoid(offsets)  # v12\n\n                if return_weight:\n                    return offsets\n                offsets = nn.functional.upsample(\n                    offsets, size=(x.shape[2], x.shape[3]), mode=\"bilinear\"\n                )\n\n                if self.stn:\n                    batch_C_prime = sp_weight_fusion[\n                        :, (self.spt_length + 1) :, :\n                    ].reshape([x.shape[0], self.F, 2])\n                    build_P_prime = self.GridGenerator(batch_C_prime, self.I_r_size)\n                    build_P_prime_reshape = build_P_prime.reshape(\n                        [build_P_prime.shape[0], self.I_r_size[0], self.I_r_size[1], 2]\n                    )\n\n            else:  # SPIN w.o. AIN\n                sp_weight = sp_weight_fusion[:, : self.spt_length, :]\n                lambda_color, offsets = None, None\n\n                if self.stn:\n                    batch_C_prime = sp_weight_fusion[:, self.spt_length :, :].reshape(\n                        [x.shape[0], self.F, 2]\n                    )\n                    build_P_prime = self.GridGenerator(batch_C_prime, self.I_r_size)\n                    build_P_prime_reshape = build_P_prime.reshape(\n                        [build_P_prime.shape[0], self.I_r_size[0], self.I_r_size[1], 2]\n                    )\n\n            x = self.sp_net(x, sp_weight, offsets, lambda_color)\n            if self.stn:\n                is_fp16 = False\n                if build_P_prime_reshape.dtype != paddle.float32:\n                    data_type = build_P_prime_reshape.dtype\n                    x = x.cast(paddle.float32)\n                    build_P_prime_reshape = build_P_prime_reshape.cast(paddle.float32)\n                    is_fp16 = True\n                x = F.grid_sample(\n                    x=x, grid=build_P_prime_reshape, padding_mode=\"border\"\n                )\n                if is_fp16:\n                    x = x.cast(data_type)\n        return x\n", "ppocr/modeling/transforms/__init__.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n__all__ = [\"build_transform\"]\n\n\ndef build_transform(config):\n    from .tps import TPS\n    from .stn import STN_ON\n    from .tsrn import TSRN\n    from .tbsrn import TBSRN\n    from .gaspin_transformer import GA_SPIN_Transformer as GA_SPIN\n\n    support_dict = [\"TPS\", \"STN_ON\", \"GA_SPIN\", \"TSRN\", \"TBSRN\"]\n\n    module_name = config.pop(\"name\")\n    assert module_name in support_dict, Exception(\n        \"transform only support {}\".format(support_dict)\n    )\n    module_class = eval(module_name)(**config)\n    return module_class\n", "ppocr/modeling/transforms/tps.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/clovaai/deep-text-recognition-benchmark/blob/master/modules/transformation.py\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\nimport paddle\nfrom paddle import nn, ParamAttr\nfrom paddle.nn import functional as F\nimport numpy as np\n\n\nclass ConvBNLayer(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride=1,\n        groups=1,\n        act=None,\n        name=None,\n    ):\n        super(ConvBNLayer, self).__init__()\n        self.conv = nn.Conv2D(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=(kernel_size - 1) // 2,\n            groups=groups,\n            weight_attr=ParamAttr(name=name + \"_weights\"),\n            bias_attr=False,\n        )\n        bn_name = \"bn_\" + name\n        self.bn = nn.BatchNorm(\n            out_channels,\n            act=act,\n            param_attr=ParamAttr(name=bn_name + \"_scale\"),\n            bias_attr=ParamAttr(bn_name + \"_offset\"),\n            moving_mean_name=bn_name + \"_mean\",\n            moving_variance_name=bn_name + \"_variance\",\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return x\n\n\nclass LocalizationNetwork(nn.Layer):\n    def __init__(self, in_channels, num_fiducial, loc_lr, model_name):\n        super(LocalizationNetwork, self).__init__()\n        self.F = num_fiducial\n        F = num_fiducial\n        if model_name == \"large\":\n            num_filters_list = [64, 128, 256, 512]\n            fc_dim = 256\n        else:\n            num_filters_list = [16, 32, 64, 128]\n            fc_dim = 64\n\n        self.block_list = []\n        for fno in range(0, len(num_filters_list)):\n            num_filters = num_filters_list[fno]\n            name = \"loc_conv%d\" % fno\n            conv = self.add_sublayer(\n                name,\n                ConvBNLayer(\n                    in_channels=in_channels,\n                    out_channels=num_filters,\n                    kernel_size=3,\n                    act=\"relu\",\n                    name=name,\n                ),\n            )\n            self.block_list.append(conv)\n            if fno == len(num_filters_list) - 1:\n                pool = nn.AdaptiveAvgPool2D(1)\n            else:\n                pool = nn.MaxPool2D(kernel_size=2, stride=2, padding=0)\n            in_channels = num_filters\n            self.block_list.append(pool)\n        name = \"loc_fc1\"\n        stdv = 1.0 / math.sqrt(num_filters_list[-1] * 1.0)\n        self.fc1 = nn.Linear(\n            in_channels,\n            fc_dim,\n            weight_attr=ParamAttr(\n                learning_rate=loc_lr,\n                name=name + \"_w\",\n                initializer=nn.initializer.Uniform(-stdv, stdv),\n            ),\n            bias_attr=ParamAttr(name=name + \".b_0\"),\n            name=name,\n        )\n\n        # Init fc2 in LocalizationNetwork\n        initial_bias = self.get_initial_fiducials()\n        initial_bias = initial_bias.reshape(-1)\n        name = \"loc_fc2\"\n        param_attr = ParamAttr(\n            learning_rate=loc_lr,\n            initializer=nn.initializer.Assign(np.zeros([fc_dim, F * 2])),\n            name=name + \"_w\",\n        )\n        bias_attr = ParamAttr(\n            learning_rate=loc_lr,\n            initializer=nn.initializer.Assign(initial_bias),\n            name=name + \"_b\",\n        )\n        self.fc2 = nn.Linear(\n            fc_dim, F * 2, weight_attr=param_attr, bias_attr=bias_attr, name=name\n        )\n        self.out_channels = F * 2\n\n    def forward(self, x):\n        \"\"\"\n        Estimating parameters of geometric transformation\n        Args:\n            image: input\n        Return:\n            batch_C_prime: the matrix of the geometric transformation\n        \"\"\"\n        B = x.shape[0]\n        i = 0\n        for block in self.block_list:\n            x = block(x)\n        x = x.squeeze(axis=2).squeeze(axis=2)\n        x = self.fc1(x)\n\n        x = F.relu(x)\n        x = self.fc2(x)\n        x = x.reshape(shape=[-1, self.F, 2])\n        return x\n\n    def get_initial_fiducials(self):\n        \"\"\"see RARE paper Fig. 6 (a)\"\"\"\n        F = self.F\n        ctrl_pts_x = np.linspace(-1.0, 1.0, int(F / 2))\n        ctrl_pts_y_top = np.linspace(0.0, -1.0, num=int(F / 2))\n        ctrl_pts_y_bottom = np.linspace(1.0, 0.0, num=int(F / 2))\n        ctrl_pts_top = np.stack([ctrl_pts_x, ctrl_pts_y_top], axis=1)\n        ctrl_pts_bottom = np.stack([ctrl_pts_x, ctrl_pts_y_bottom], axis=1)\n        initial_bias = np.concatenate([ctrl_pts_top, ctrl_pts_bottom], axis=0)\n        return initial_bias\n\n\nclass GridGenerator(nn.Layer):\n    def __init__(self, in_channels, num_fiducial):\n        super(GridGenerator, self).__init__()\n        self.eps = 1e-6\n        self.F = num_fiducial\n\n        name = \"ex_fc\"\n        initializer = nn.initializer.Constant(value=0.0)\n        param_attr = ParamAttr(\n            learning_rate=0.0, initializer=initializer, name=name + \"_w\"\n        )\n        bias_attr = ParamAttr(\n            learning_rate=0.0, initializer=initializer, name=name + \"_b\"\n        )\n        self.fc = nn.Linear(\n            in_channels, 6, weight_attr=param_attr, bias_attr=bias_attr, name=name\n        )\n\n    def forward(self, batch_C_prime, I_r_size):\n        \"\"\"\n        Generate the grid for the grid_sampler.\n        Args:\n            batch_C_prime: the matrix of the geometric transformation\n            I_r_size: the shape of the input image\n        Return:\n            batch_P_prime: the grid for the grid_sampler\n        \"\"\"\n        C = self.build_C_paddle()\n        P = self.build_P_paddle(I_r_size)\n\n        inv_delta_C_tensor = self.build_inv_delta_C_paddle(C).astype(\"float32\")\n        P_hat_tensor = self.build_P_hat_paddle(C, paddle.to_tensor(P)).astype(\"float32\")\n\n        inv_delta_C_tensor.stop_gradient = True\n        P_hat_tensor.stop_gradient = True\n\n        batch_C_ex_part_tensor = self.get_expand_tensor(batch_C_prime)\n\n        batch_C_ex_part_tensor.stop_gradient = True\n\n        batch_C_prime_with_zeros = paddle.concat(\n            [batch_C_prime, batch_C_ex_part_tensor], axis=1\n        )\n        batch_T = paddle.matmul(inv_delta_C_tensor, batch_C_prime_with_zeros)\n        batch_P_prime = paddle.matmul(P_hat_tensor, batch_T)\n        return batch_P_prime\n\n    def build_C_paddle(self):\n        \"\"\"Return coordinates of fiducial points in I_r; C\"\"\"\n        F = self.F\n        ctrl_pts_x = paddle.linspace(-1.0, 1.0, int(F / 2), dtype=\"float64\")\n        ctrl_pts_y_top = -1 * paddle.ones([int(F / 2)], dtype=\"float64\")\n        ctrl_pts_y_bottom = paddle.ones([int(F / 2)], dtype=\"float64\")\n        ctrl_pts_top = paddle.stack([ctrl_pts_x, ctrl_pts_y_top], axis=1)\n        ctrl_pts_bottom = paddle.stack([ctrl_pts_x, ctrl_pts_y_bottom], axis=1)\n        C = paddle.concat([ctrl_pts_top, ctrl_pts_bottom], axis=0)\n        return C  # F x 2\n\n    def build_P_paddle(self, I_r_size):\n        I_r_height, I_r_width = I_r_size\n        I_r_grid_x = (\n            paddle.arange(-I_r_width, I_r_width, 2, dtype=\"float64\") + 1.0\n        ) / paddle.to_tensor(np.array([I_r_width])).astype(\"float64\")\n\n        I_r_grid_y = (\n            paddle.arange(-I_r_height, I_r_height, 2, dtype=\"float64\") + 1.0\n        ) / paddle.to_tensor(np.array([I_r_height])).astype(\"float64\")\n\n        # P: self.I_r_width x self.I_r_height x 2\n        P = paddle.stack(paddle.meshgrid(I_r_grid_x, I_r_grid_y), axis=2)\n        P = paddle.transpose(P, perm=[1, 0, 2])\n        # n (= self.I_r_width x self.I_r_height) x 2\n        return P.reshape([-1, 2])\n\n    def build_inv_delta_C_paddle(self, C):\n        \"\"\"Return inv_delta_C which is needed to calculate T\"\"\"\n        F = self.F\n        hat_eye = paddle.eye(F, dtype=\"float64\")  # F x F\n        hat_C = (\n            paddle.norm(C.reshape([1, F, 2]) - C.reshape([F, 1, 2]), axis=2) + hat_eye\n        )\n        hat_C = (hat_C**2) * paddle.log(hat_C)\n        delta_C = paddle.concat(  # F+3 x F+3\n            [\n                paddle.concat(\n                    [paddle.ones((F, 1), dtype=\"float64\"), C, hat_C], axis=1\n                ),  # F x F+3\n                paddle.concat(\n                    [\n                        paddle.zeros((2, 3), dtype=\"float64\"),\n                        paddle.transpose(C, perm=[1, 0]),\n                    ],\n                    axis=1,\n                ),  # 2 x F+3\n                paddle.concat(\n                    [\n                        paddle.zeros((1, 3), dtype=\"float64\"),\n                        paddle.ones((1, F), dtype=\"float64\"),\n                    ],\n                    axis=1,\n                ),  # 1 x F+3\n            ],\n            axis=0,\n        )\n        inv_delta_C = paddle.inverse(delta_C)\n        return inv_delta_C  # F+3 x F+3\n\n    def build_P_hat_paddle(self, C, P):\n        F = self.F\n        eps = self.eps\n        n = P.shape[0]  # n (= self.I_r_width x self.I_r_height)\n        # P_tile: n x 2 -> n x 1 x 2 -> n x F x 2\n        P_tile = paddle.tile(paddle.unsqueeze(P, axis=1), (1, F, 1))\n        C_tile = paddle.unsqueeze(C, axis=0)  # 1 x F x 2\n        P_diff = P_tile - C_tile  # n x F x 2\n        # rbf_norm: n x F\n        rbf_norm = paddle.norm(P_diff, p=2, axis=2, keepdim=False)\n\n        # rbf: n x F\n        rbf = paddle.multiply(paddle.square(rbf_norm), paddle.log(rbf_norm + eps))\n        P_hat = paddle.concat([paddle.ones((n, 1), dtype=\"float64\"), P, rbf], axis=1)\n        return P_hat  # n x F+3\n\n    def get_expand_tensor(self, batch_C_prime):\n        B, H, C = batch_C_prime.shape\n        batch_C_prime = batch_C_prime.reshape([B, H * C])\n        batch_C_ex_part_tensor = self.fc(batch_C_prime)\n        batch_C_ex_part_tensor = batch_C_ex_part_tensor.reshape([-1, 3, 2])\n        return batch_C_ex_part_tensor\n\n\nclass TPS(nn.Layer):\n    def __init__(self, in_channels, num_fiducial, loc_lr, model_name):\n        super(TPS, self).__init__()\n        self.loc_net = LocalizationNetwork(\n            in_channels, num_fiducial, loc_lr, model_name\n        )\n        self.grid_generator = GridGenerator(self.loc_net.out_channels, num_fiducial)\n        self.out_channels = in_channels\n\n    def forward(self, image):\n        image.stop_gradient = False\n        batch_C_prime = self.loc_net(image)\n        batch_P_prime = self.grid_generator(batch_C_prime, image.shape[2:])\n        batch_P_prime = batch_P_prime.reshape([-1, image.shape[2], image.shape[3], 2])\n        is_fp16 = False\n        if batch_P_prime.dtype != paddle.float32:\n            data_type = batch_P_prime.dtype\n            image = image.cast(paddle.float32)\n            batch_P_prime = batch_P_prime.cast(paddle.float32)\n            is_fp16 = True\n        batch_I_r = F.grid_sample(x=image, grid=batch_P_prime)\n        if is_fp16:\n            batch_I_r = batch_I_r.cast(data_type)\n\n        return batch_I_r\n", "ppocr/modeling/necks/fpn_unet.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/open-mmlab/mmocr/blob/main/mmocr/models/textdet/necks/fpn_unet.py\n\"\"\"\n\nimport paddle\nimport paddle.nn as nn\nimport paddle.nn.functional as F\n\n\nclass UpBlock(nn.Layer):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n\n        assert isinstance(in_channels, int)\n        assert isinstance(out_channels, int)\n\n        self.conv1x1 = nn.Conv2D(\n            in_channels, in_channels, kernel_size=1, stride=1, padding=0\n        )\n        self.conv3x3 = nn.Conv2D(\n            in_channels, out_channels, kernel_size=3, stride=1, padding=1\n        )\n        self.deconv = nn.Conv2DTranspose(\n            out_channels, out_channels, kernel_size=4, stride=2, padding=1\n        )\n\n    def forward(self, x):\n        x = F.relu(self.conv1x1(x))\n        x = F.relu(self.conv3x3(x))\n        x = self.deconv(x)\n        return x\n\n\nclass FPN_UNet(nn.Layer):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n\n        assert len(in_channels) == 4\n        assert isinstance(out_channels, int)\n        self.out_channels = out_channels\n\n        blocks_out_channels = [out_channels] + [\n            min(out_channels * 2**i, 256) for i in range(4)\n        ]\n        blocks_in_channels = (\n            [blocks_out_channels[1]]\n            + [in_channels[i] + blocks_out_channels[i + 2] for i in range(3)]\n            + [in_channels[3]]\n        )\n\n        self.up4 = nn.Conv2DTranspose(\n            blocks_in_channels[4],\n            blocks_out_channels[4],\n            kernel_size=4,\n            stride=2,\n            padding=1,\n        )\n        self.up_block3 = UpBlock(blocks_in_channels[3], blocks_out_channels[3])\n        self.up_block2 = UpBlock(blocks_in_channels[2], blocks_out_channels[2])\n        self.up_block1 = UpBlock(blocks_in_channels[1], blocks_out_channels[1])\n        self.up_block0 = UpBlock(blocks_in_channels[0], blocks_out_channels[0])\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (list[Tensor] | tuple[Tensor]): A list of four tensors of shape\n                :math:`(N, C_i, H_i, W_i)`, representing C2, C3, C4, C5\n                features respectively. :math:`C_i` should matches the number in\n                ``in_channels``.\n\n        Returns:\n            Tensor: Shape :math:`(N, C, H, W)` where :math:`H=4H_0` and\n            :math:`W=4W_0`.\n        \"\"\"\n        c2, c3, c4, c5 = x\n\n        x = F.relu(self.up4(c5))\n\n        x = paddle.concat([x, c4], axis=1)\n        x = F.relu(self.up_block3(x))\n\n        x = paddle.concat([x, c3], axis=1)\n        x = F.relu(self.up_block2(x))\n\n        x = paddle.concat([x, c2], axis=1)\n        x = F.relu(self.up_block1(x))\n\n        x = self.up_block0(x)\n        return x\n", "ppocr/modeling/necks/ct_fpn.py": "# copyright (c) 2019 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nfrom paddle import nn\nimport paddle.nn.functional as F\nfrom paddle import ParamAttr\nimport os\nimport sys\n\nimport math\nfrom paddle.nn.initializer import TruncatedNormal, Constant, Normal\n\nones_ = Constant(value=1.0)\nzeros_ = Constant(value=0.0)\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(__dir__)\nsys.path.insert(0, os.path.abspath(os.path.join(__dir__, \"../../..\")))\n\n\nclass Conv_BN_ReLU(nn.Layer):\n    def __init__(self, in_planes, out_planes, kernel_size=1, stride=1, padding=0):\n        super(Conv_BN_ReLU, self).__init__()\n        self.conv = nn.Conv2D(\n            in_planes,\n            out_planes,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            bias_attr=False,\n        )\n        self.bn = nn.BatchNorm2D(out_planes)\n        self.relu = nn.ReLU()\n\n        for m in self.sublayers():\n            if isinstance(m, nn.Conv2D):\n                n = m._kernel_size[0] * m._kernel_size[1] * m._out_channels\n                normal_ = Normal(mean=0.0, std=math.sqrt(2.0 / n))\n                normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2D):\n                zeros_(m.bias)\n                ones_(m.weight)\n\n    def forward(self, x):\n        return self.relu(self.bn(self.conv(x)))\n\n\nclass FPEM(nn.Layer):\n    def __init__(self, in_channels, out_channels):\n        super(FPEM, self).__init__()\n        planes = out_channels\n        self.dwconv3_1 = nn.Conv2D(\n            planes,\n            planes,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            groups=planes,\n            bias_attr=False,\n        )\n        self.smooth_layer3_1 = Conv_BN_ReLU(planes, planes)\n\n        self.dwconv2_1 = nn.Conv2D(\n            planes,\n            planes,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            groups=planes,\n            bias_attr=False,\n        )\n        self.smooth_layer2_1 = Conv_BN_ReLU(planes, planes)\n\n        self.dwconv1_1 = nn.Conv2D(\n            planes,\n            planes,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            groups=planes,\n            bias_attr=False,\n        )\n        self.smooth_layer1_1 = Conv_BN_ReLU(planes, planes)\n\n        self.dwconv2_2 = nn.Conv2D(\n            planes,\n            planes,\n            kernel_size=3,\n            stride=2,\n            padding=1,\n            groups=planes,\n            bias_attr=False,\n        )\n        self.smooth_layer2_2 = Conv_BN_ReLU(planes, planes)\n\n        self.dwconv3_2 = nn.Conv2D(\n            planes,\n            planes,\n            kernel_size=3,\n            stride=2,\n            padding=1,\n            groups=planes,\n            bias_attr=False,\n        )\n        self.smooth_layer3_2 = Conv_BN_ReLU(planes, planes)\n\n        self.dwconv4_2 = nn.Conv2D(\n            planes,\n            planes,\n            kernel_size=3,\n            stride=2,\n            padding=1,\n            groups=planes,\n            bias_attr=False,\n        )\n        self.smooth_layer4_2 = Conv_BN_ReLU(planes, planes)\n\n    def _upsample_add(self, x, y):\n        return F.upsample(x, scale_factor=2, mode=\"bilinear\") + y\n\n    def forward(self, f1, f2, f3, f4):\n        # up-down\n        f3 = self.smooth_layer3_1(self.dwconv3_1(self._upsample_add(f4, f3)))\n        f2 = self.smooth_layer2_1(self.dwconv2_1(self._upsample_add(f3, f2)))\n        f1 = self.smooth_layer1_1(self.dwconv1_1(self._upsample_add(f2, f1)))\n\n        # down-up\n        f2 = self.smooth_layer2_2(self.dwconv2_2(self._upsample_add(f2, f1)))\n        f3 = self.smooth_layer3_2(self.dwconv3_2(self._upsample_add(f3, f2)))\n        f4 = self.smooth_layer4_2(self.dwconv4_2(self._upsample_add(f4, f3)))\n\n        return f1, f2, f3, f4\n\n\nclass CTFPN(nn.Layer):\n    def __init__(self, in_channels, out_channel=128):\n        super(CTFPN, self).__init__()\n        self.out_channels = out_channel * 4\n\n        self.reduce_layer1 = Conv_BN_ReLU(in_channels[0], 128)\n        self.reduce_layer2 = Conv_BN_ReLU(in_channels[1], 128)\n        self.reduce_layer3 = Conv_BN_ReLU(in_channels[2], 128)\n        self.reduce_layer4 = Conv_BN_ReLU(in_channels[3], 128)\n\n        self.fpem1 = FPEM(in_channels=(64, 128, 256, 512), out_channels=128)\n        self.fpem2 = FPEM(in_channels=(64, 128, 256, 512), out_channels=128)\n\n    def _upsample(self, x, scale=1):\n        return F.upsample(x, scale_factor=scale, mode=\"bilinear\")\n\n    def forward(self, f):\n        # # reduce channel\n        f1 = self.reduce_layer1(f[0])  # N,64,160,160    --> N, 128, 160, 160\n        f2 = self.reduce_layer2(f[1])  # N, 128, 80, 80  --> N, 128, 80, 80\n        f3 = self.reduce_layer3(f[2])  # N, 256, 40, 40  --> N, 128, 40, 40\n        f4 = self.reduce_layer4(f[3])  # N, 512, 20, 20  --> N, 128, 20, 20\n\n        # FPEM\n        f1_1, f2_1, f3_1, f4_1 = self.fpem1(f1, f2, f3, f4)\n        f1_2, f2_2, f3_2, f4_2 = self.fpem2(f1_1, f2_1, f3_1, f4_1)\n\n        # FFM\n        f1 = f1_1 + f1_2\n        f2 = f2_1 + f2_2\n        f3 = f3_1 + f3_2\n        f4 = f4_1 + f4_2\n\n        f2 = self._upsample(f2, scale=2)\n        f3 = self._upsample(f3, scale=4)\n        f4 = self._upsample(f4, scale=8)\n        ff = paddle.concat((f1, f2, f3, f4), 1)  # N,512, 160,160\n        return ff\n", "ppocr/modeling/necks/table_fpn.py": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nfrom paddle import nn\nimport paddle.nn.functional as F\nfrom paddle import ParamAttr\n\n\nclass TableFPN(nn.Layer):\n    def __init__(self, in_channels, out_channels, **kwargs):\n        super(TableFPN, self).__init__()\n        self.out_channels = 512\n        weight_attr = paddle.nn.initializer.KaimingUniform()\n        self.in2_conv = nn.Conv2D(\n            in_channels=in_channels[0],\n            out_channels=self.out_channels,\n            kernel_size=1,\n            weight_attr=ParamAttr(initializer=weight_attr),\n            bias_attr=False,\n        )\n        self.in3_conv = nn.Conv2D(\n            in_channels=in_channels[1],\n            out_channels=self.out_channels,\n            kernel_size=1,\n            stride=1,\n            weight_attr=ParamAttr(initializer=weight_attr),\n            bias_attr=False,\n        )\n        self.in4_conv = nn.Conv2D(\n            in_channels=in_channels[2],\n            out_channels=self.out_channels,\n            kernel_size=1,\n            weight_attr=ParamAttr(initializer=weight_attr),\n            bias_attr=False,\n        )\n        self.in5_conv = nn.Conv2D(\n            in_channels=in_channels[3],\n            out_channels=self.out_channels,\n            kernel_size=1,\n            weight_attr=ParamAttr(initializer=weight_attr),\n            bias_attr=False,\n        )\n        self.p5_conv = nn.Conv2D(\n            in_channels=self.out_channels,\n            out_channels=self.out_channels // 4,\n            kernel_size=3,\n            padding=1,\n            weight_attr=ParamAttr(initializer=weight_attr),\n            bias_attr=False,\n        )\n        self.p4_conv = nn.Conv2D(\n            in_channels=self.out_channels,\n            out_channels=self.out_channels // 4,\n            kernel_size=3,\n            padding=1,\n            weight_attr=ParamAttr(initializer=weight_attr),\n            bias_attr=False,\n        )\n        self.p3_conv = nn.Conv2D(\n            in_channels=self.out_channels,\n            out_channels=self.out_channels // 4,\n            kernel_size=3,\n            padding=1,\n            weight_attr=ParamAttr(initializer=weight_attr),\n            bias_attr=False,\n        )\n        self.p2_conv = nn.Conv2D(\n            in_channels=self.out_channels,\n            out_channels=self.out_channels // 4,\n            kernel_size=3,\n            padding=1,\n            weight_attr=ParamAttr(initializer=weight_attr),\n            bias_attr=False,\n        )\n        self.fuse_conv = nn.Conv2D(\n            in_channels=self.out_channels * 4,\n            out_channels=512,\n            kernel_size=3,\n            padding=1,\n            weight_attr=ParamAttr(initializer=weight_attr),\n            bias_attr=False,\n        )\n\n    def forward(self, x):\n        c2, c3, c4, c5 = x\n\n        in5 = self.in5_conv(c5)\n        in4 = self.in4_conv(c4)\n        in3 = self.in3_conv(c3)\n        in2 = self.in2_conv(c2)\n\n        out4 = in4 + F.upsample(\n            in5, size=in4.shape[2:4], mode=\"nearest\", align_mode=1\n        )  # 1/16\n        out3 = in3 + F.upsample(\n            out4, size=in3.shape[2:4], mode=\"nearest\", align_mode=1\n        )  # 1/8\n        out2 = in2 + F.upsample(\n            out3, size=in2.shape[2:4], mode=\"nearest\", align_mode=1\n        )  # 1/4\n\n        p4 = F.upsample(out4, size=in5.shape[2:4], mode=\"nearest\", align_mode=1)\n        p3 = F.upsample(out3, size=in5.shape[2:4], mode=\"nearest\", align_mode=1)\n        p2 = F.upsample(out2, size=in5.shape[2:4], mode=\"nearest\", align_mode=1)\n        fuse = paddle.concat([in5, p4, p3, p2], axis=1)\n        fuse_conv = self.fuse_conv(fuse) * 0.005\n        return [c5 + fuse_conv]\n", "ppocr/modeling/necks/rnn.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nfrom paddle import nn\n\nfrom ppocr.modeling.heads.rec_ctc_head import get_para_bias_attr\nfrom ppocr.modeling.backbones.rec_svtrnet import (\n    Block,\n    ConvBNLayer,\n    trunc_normal_,\n    zeros_,\n    ones_,\n)\n\n\nclass Im2Seq(nn.Layer):\n    def __init__(self, in_channels, **kwargs):\n        super().__init__()\n        self.out_channels = in_channels\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        assert H == 1\n        x = x.squeeze(axis=2)\n        x = x.transpose([0, 2, 1])  # (NTC)(batch, width, channels)\n        return x\n\n\nclass EncoderWithRNN(nn.Layer):\n    def __init__(self, in_channels, hidden_size):\n        super(EncoderWithRNN, self).__init__()\n        self.out_channels = hidden_size * 2\n        self.lstm = nn.LSTM(\n            in_channels, hidden_size, direction=\"bidirectional\", num_layers=2\n        )\n\n    def forward(self, x):\n        x, _ = self.lstm(x)\n        return x\n\n\nclass BidirectionalLSTM(nn.Layer):\n    def __init__(\n        self,\n        input_size,\n        hidden_size,\n        output_size=None,\n        num_layers=1,\n        dropout=0,\n        direction=False,\n        time_major=False,\n        with_linear=False,\n    ):\n        super(BidirectionalLSTM, self).__init__()\n        self.with_linear = with_linear\n        self.rnn = nn.LSTM(\n            input_size,\n            hidden_size,\n            num_layers=num_layers,\n            dropout=dropout,\n            direction=direction,\n            time_major=time_major,\n        )\n\n        # text recognition the specified structure LSTM with linear\n        if self.with_linear:\n            self.linear = nn.Linear(hidden_size * 2, output_size)\n\n    def forward(self, input_feature):\n        recurrent, _ = self.rnn(\n            input_feature\n        )  # batch_size x T x input_size -> batch_size x T x (2*hidden_size)\n        if self.with_linear:\n            output = self.linear(recurrent)  # batch_size x T x output_size\n            return output\n        return recurrent\n\n\nclass EncoderWithCascadeRNN(nn.Layer):\n    def __init__(\n        self, in_channels, hidden_size, out_channels, num_layers=2, with_linear=False\n    ):\n        super(EncoderWithCascadeRNN, self).__init__()\n        self.out_channels = out_channels[-1]\n        self.encoder = nn.LayerList(\n            [\n                BidirectionalLSTM(\n                    in_channels if i == 0 else out_channels[i - 1],\n                    hidden_size,\n                    output_size=out_channels[i],\n                    num_layers=1,\n                    direction=\"bidirectional\",\n                    with_linear=with_linear,\n                )\n                for i in range(num_layers)\n            ]\n        )\n\n    def forward(self, x):\n        for i, l in enumerate(self.encoder):\n            x = l(x)\n        return x\n\n\nclass EncoderWithFC(nn.Layer):\n    def __init__(self, in_channels, hidden_size):\n        super(EncoderWithFC, self).__init__()\n        self.out_channels = hidden_size\n        weight_attr, bias_attr = get_para_bias_attr(l2_decay=0.00001, k=in_channels)\n        self.fc = nn.Linear(\n            in_channels,\n            hidden_size,\n            weight_attr=weight_attr,\n            bias_attr=bias_attr,\n            name=\"reduce_encoder_fea\",\n        )\n\n    def forward(self, x):\n        x = self.fc(x)\n        return x\n\n\nclass EncoderWithSVTR(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        dims=64,  # XS\n        depth=2,\n        hidden_dims=120,\n        use_guide=False,\n        num_heads=8,\n        qkv_bias=True,\n        mlp_ratio=2.0,\n        drop_rate=0.1,\n        attn_drop_rate=0.1,\n        drop_path=0.0,\n        kernel_size=[3, 3],\n        qk_scale=None,\n    ):\n        super(EncoderWithSVTR, self).__init__()\n        self.depth = depth\n        self.use_guide = use_guide\n        self.conv1 = ConvBNLayer(\n            in_channels,\n            in_channels // 8,\n            kernel_size=kernel_size,\n            padding=[kernel_size[0] // 2, kernel_size[1] // 2],\n            act=nn.Swish,\n        )\n        self.conv2 = ConvBNLayer(\n            in_channels // 8, hidden_dims, kernel_size=1, act=nn.Swish\n        )\n\n        self.svtr_block = nn.LayerList(\n            [\n                Block(\n                    dim=hidden_dims,\n                    num_heads=num_heads,\n                    mixer=\"Global\",\n                    HW=None,\n                    mlp_ratio=mlp_ratio,\n                    qkv_bias=qkv_bias,\n                    qk_scale=qk_scale,\n                    drop=drop_rate,\n                    act_layer=nn.Swish,\n                    attn_drop=attn_drop_rate,\n                    drop_path=drop_path,\n                    norm_layer=\"nn.LayerNorm\",\n                    epsilon=1e-05,\n                    prenorm=False,\n                )\n                for i in range(depth)\n            ]\n        )\n        self.norm = nn.LayerNorm(hidden_dims, epsilon=1e-6)\n        self.conv3 = ConvBNLayer(hidden_dims, in_channels, kernel_size=1, act=nn.Swish)\n        # last conv-nxn, the input is concat of input tensor and conv3 output tensor\n        self.conv4 = ConvBNLayer(\n            2 * in_channels,\n            in_channels // 8,\n            kernel_size=kernel_size,\n            padding=[kernel_size[0] // 2, kernel_size[1] // 2],\n            act=nn.Swish,\n        )\n\n        self.conv1x1 = ConvBNLayer(in_channels // 8, dims, kernel_size=1, act=nn.Swish)\n        self.out_channels = dims\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                zeros_(m.bias)\n        elif isinstance(m, nn.LayerNorm):\n            zeros_(m.bias)\n            ones_(m.weight)\n\n    def forward(self, x):\n        # for use guide\n        if self.use_guide:\n            z = x.clone()\n            z.stop_gradient = True\n        else:\n            z = x\n        # for short cut\n        h = z\n        # reduce dim\n        z = self.conv1(z)\n        z = self.conv2(z)\n        # SVTR global block\n        B, C, H, W = z.shape\n        z = z.flatten(2).transpose([0, 2, 1])\n        for blk in self.svtr_block:\n            z = blk(z)\n        z = self.norm(z)\n        # last stage\n        z = z.reshape([0, H, W, C]).transpose([0, 3, 1, 2])\n        z = self.conv3(z)\n        z = paddle.concat((h, z), axis=1)\n        z = self.conv1x1(self.conv4(z))\n        return z\n\n\nclass SequenceEncoder(nn.Layer):\n    def __init__(self, in_channels, encoder_type, hidden_size=48, **kwargs):\n        super(SequenceEncoder, self).__init__()\n        self.encoder_reshape = Im2Seq(in_channels)\n        self.out_channels = self.encoder_reshape.out_channels\n        self.encoder_type = encoder_type\n        if encoder_type == \"reshape\":\n            self.only_reshape = True\n        else:\n            support_encoder_dict = {\n                \"reshape\": Im2Seq,\n                \"fc\": EncoderWithFC,\n                \"rnn\": EncoderWithRNN,\n                \"svtr\": EncoderWithSVTR,\n                \"cascadernn\": EncoderWithCascadeRNN,\n            }\n            assert encoder_type in support_encoder_dict, \"{} must in {}\".format(\n                encoder_type, support_encoder_dict.keys()\n            )\n            if encoder_type == \"svtr\":\n                self.encoder = support_encoder_dict[encoder_type](\n                    self.encoder_reshape.out_channels, **kwargs\n                )\n            elif encoder_type == \"cascadernn\":\n                self.encoder = support_encoder_dict[encoder_type](\n                    self.encoder_reshape.out_channels, hidden_size, **kwargs\n                )\n            else:\n                self.encoder = support_encoder_dict[encoder_type](\n                    self.encoder_reshape.out_channels, hidden_size\n                )\n            self.out_channels = self.encoder.out_channels\n            self.only_reshape = False\n\n    def forward(self, x):\n        if self.encoder_type != \"svtr\":\n            x = self.encoder_reshape(x)\n            if not self.only_reshape:\n                x = self.encoder(x)\n            return x\n        else:\n            x = self.encoder(x)\n            x = self.encoder_reshape(x)\n            return x\n", "ppocr/modeling/necks/intracl.py": "import paddle\nfrom paddle import nn\n\n# refer from: https://github.com/ViTAE-Transformer/I3CL/blob/736c80237f66d352d488e83b05f3e33c55201317/mmdet/models/detectors/intra_cl_module.py\n\n\nclass IntraCLBlock(nn.Layer):\n    def __init__(self, in_channels=96, reduce_factor=4):\n        super(IntraCLBlock, self).__init__()\n        self.channels = in_channels\n        self.rf = reduce_factor\n        weight_attr = paddle.nn.initializer.KaimingUniform()\n        self.conv1x1_reduce_channel = nn.Conv2D(\n            self.channels, self.channels // self.rf, kernel_size=1, stride=1, padding=0\n        )\n        self.conv1x1_return_channel = nn.Conv2D(\n            self.channels // self.rf, self.channels, kernel_size=1, stride=1, padding=0\n        )\n\n        self.v_layer_7x1 = nn.Conv2D(\n            self.channels // self.rf,\n            self.channels // self.rf,\n            kernel_size=(7, 1),\n            stride=(1, 1),\n            padding=(3, 0),\n        )\n        self.v_layer_5x1 = nn.Conv2D(\n            self.channels // self.rf,\n            self.channels // self.rf,\n            kernel_size=(5, 1),\n            stride=(1, 1),\n            padding=(2, 0),\n        )\n        self.v_layer_3x1 = nn.Conv2D(\n            self.channels // self.rf,\n            self.channels // self.rf,\n            kernel_size=(3, 1),\n            stride=(1, 1),\n            padding=(1, 0),\n        )\n\n        self.q_layer_1x7 = nn.Conv2D(\n            self.channels // self.rf,\n            self.channels // self.rf,\n            kernel_size=(1, 7),\n            stride=(1, 1),\n            padding=(0, 3),\n        )\n        self.q_layer_1x5 = nn.Conv2D(\n            self.channels // self.rf,\n            self.channels // self.rf,\n            kernel_size=(1, 5),\n            stride=(1, 1),\n            padding=(0, 2),\n        )\n        self.q_layer_1x3 = nn.Conv2D(\n            self.channels // self.rf,\n            self.channels // self.rf,\n            kernel_size=(1, 3),\n            stride=(1, 1),\n            padding=(0, 1),\n        )\n\n        # base\n        self.c_layer_7x7 = nn.Conv2D(\n            self.channels // self.rf,\n            self.channels // self.rf,\n            kernel_size=(7, 7),\n            stride=(1, 1),\n            padding=(3, 3),\n        )\n        self.c_layer_5x5 = nn.Conv2D(\n            self.channels // self.rf,\n            self.channels // self.rf,\n            kernel_size=(5, 5),\n            stride=(1, 1),\n            padding=(2, 2),\n        )\n        self.c_layer_3x3 = nn.Conv2D(\n            self.channels // self.rf,\n            self.channels // self.rf,\n            kernel_size=(3, 3),\n            stride=(1, 1),\n            padding=(1, 1),\n        )\n\n        self.bn = nn.BatchNorm2D(self.channels)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x_new = self.conv1x1_reduce_channel(x)\n\n        x_7_c = self.c_layer_7x7(x_new)\n        x_7_v = self.v_layer_7x1(x_new)\n        x_7_q = self.q_layer_1x7(x_new)\n        x_7 = x_7_c + x_7_v + x_7_q\n\n        x_5_c = self.c_layer_5x5(x_7)\n        x_5_v = self.v_layer_5x1(x_7)\n        x_5_q = self.q_layer_1x5(x_7)\n        x_5 = x_5_c + x_5_v + x_5_q\n\n        x_3_c = self.c_layer_3x3(x_5)\n        x_3_v = self.v_layer_3x1(x_5)\n        x_3_q = self.q_layer_1x3(x_5)\n        x_3 = x_3_c + x_3_v + x_3_q\n\n        x_relation = self.conv1x1_return_channel(x_3)\n\n        x_relation = self.bn(x_relation)\n        x_relation = self.relu(x_relation)\n\n        return x + x_relation\n\n\ndef build_intraclblock_list(num_block):\n    IntraCLBlock_list = nn.LayerList()\n    for i in range(num_block):\n        IntraCLBlock_list.append(IntraCLBlock())\n\n    return IntraCLBlock_list\n", "ppocr/modeling/necks/fce_fpn.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/PaddlePaddle/PaddleDetection/blob/release/2.3/ppdet/modeling/necks/fpn.py\n\"\"\"\n\nimport paddle.nn as nn\nimport paddle.nn.functional as F\nfrom paddle import ParamAttr\nfrom paddle.nn.initializer import XavierUniform\nfrom paddle.nn.initializer import Normal\nfrom paddle.regularizer import L2Decay\n\n__all__ = [\"FCEFPN\"]\n\n\nclass ConvNormLayer(nn.Layer):\n    def __init__(\n        self,\n        ch_in,\n        ch_out,\n        filter_size,\n        stride,\n        groups=1,\n        norm_type=\"bn\",\n        norm_decay=0.0,\n        norm_groups=32,\n        lr_scale=1.0,\n        freeze_norm=False,\n        initializer=Normal(mean=0.0, std=0.01),\n    ):\n        super(ConvNormLayer, self).__init__()\n        assert norm_type in [\"bn\", \"sync_bn\", \"gn\"]\n\n        bias_attr = False\n\n        self.conv = nn.Conv2D(\n            in_channels=ch_in,\n            out_channels=ch_out,\n            kernel_size=filter_size,\n            stride=stride,\n            padding=(filter_size - 1) // 2,\n            groups=groups,\n            weight_attr=ParamAttr(initializer=initializer, learning_rate=1.0),\n            bias_attr=bias_attr,\n        )\n\n        norm_lr = 0.0 if freeze_norm else 1.0\n        param_attr = ParamAttr(\n            learning_rate=norm_lr,\n            regularizer=L2Decay(norm_decay) if norm_decay is not None else None,\n        )\n        bias_attr = ParamAttr(\n            learning_rate=norm_lr,\n            regularizer=L2Decay(norm_decay) if norm_decay is not None else None,\n        )\n        if norm_type == \"bn\":\n            self.norm = nn.BatchNorm2D(\n                ch_out, weight_attr=param_attr, bias_attr=bias_attr\n            )\n        elif norm_type == \"sync_bn\":\n            self.norm = nn.SyncBatchNorm(\n                ch_out, weight_attr=param_attr, bias_attr=bias_attr\n            )\n        elif norm_type == \"gn\":\n            self.norm = nn.GroupNorm(\n                num_groups=norm_groups,\n                num_channels=ch_out,\n                weight_attr=param_attr,\n                bias_attr=bias_attr,\n            )\n\n    def forward(self, inputs):\n        out = self.conv(inputs)\n        out = self.norm(out)\n        return out\n\n\nclass FCEFPN(nn.Layer):\n    \"\"\"\n    Feature Pyramid Network, see https://arxiv.org/abs/1612.03144\n    Args:\n        in_channels (list[int]): input channels of each level which can be\n            derived from the output shape of backbone by from_config\n        out_channels (list[int]): output channel of each level\n        spatial_scales (list[float]): the spatial scales between input feature\n            maps and original input image which can be derived from the output\n            shape of backbone by from_config\n        has_extra_convs (bool): whether to add extra conv to the last level.\n            default False\n        extra_stage (int): the number of extra stages added to the last level.\n            default 1\n        use_c5 (bool): Whether to use c5 as the input of extra stage,\n            otherwise p5 is used. default True\n        norm_type (string|None): The normalization type in FPN module. If\n            norm_type is None, norm will not be used after conv and if\n            norm_type is string, bn, gn, sync_bn are available. default None\n        norm_decay (float): weight decay for normalization layer weights.\n            default 0.\n        freeze_norm (bool): whether to freeze normalization layer.\n            default False\n        relu_before_extra_convs (bool): whether to add relu before extra convs.\n            default False\n\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        spatial_scales=[0.25, 0.125, 0.0625, 0.03125],\n        has_extra_convs=False,\n        extra_stage=1,\n        use_c5=True,\n        norm_type=None,\n        norm_decay=0.0,\n        freeze_norm=False,\n        relu_before_extra_convs=True,\n    ):\n        super(FCEFPN, self).__init__()\n        self.out_channels = out_channels\n        for s in range(extra_stage):\n            spatial_scales = spatial_scales + [spatial_scales[-1] / 2.0]\n        self.spatial_scales = spatial_scales\n        self.has_extra_convs = has_extra_convs\n        self.extra_stage = extra_stage\n        self.use_c5 = use_c5\n        self.relu_before_extra_convs = relu_before_extra_convs\n        self.norm_type = norm_type\n        self.norm_decay = norm_decay\n        self.freeze_norm = freeze_norm\n\n        self.lateral_convs = []\n        self.fpn_convs = []\n        fan = out_channels * 3 * 3\n\n        # stage index 0,1,2,3 stands for res2,res3,res4,res5 on ResNet Backbone\n        # 0 <= st_stage < ed_stage <= 3\n        st_stage = 4 - len(in_channels)\n        ed_stage = st_stage + len(in_channels) - 1\n        for i in range(st_stage, ed_stage + 1):\n            if i == 3:\n                lateral_name = \"fpn_inner_res5_sum\"\n            else:\n                lateral_name = \"fpn_inner_res{}_sum_lateral\".format(i + 2)\n            in_c = in_channels[i - st_stage]\n            if self.norm_type is not None:\n                lateral = self.add_sublayer(\n                    lateral_name,\n                    ConvNormLayer(\n                        ch_in=in_c,\n                        ch_out=out_channels,\n                        filter_size=1,\n                        stride=1,\n                        norm_type=self.norm_type,\n                        norm_decay=self.norm_decay,\n                        freeze_norm=self.freeze_norm,\n                        initializer=XavierUniform(fan_out=in_c),\n                    ),\n                )\n            else:\n                lateral = self.add_sublayer(\n                    lateral_name,\n                    nn.Conv2D(\n                        in_channels=in_c,\n                        out_channels=out_channels,\n                        kernel_size=1,\n                        weight_attr=ParamAttr(initializer=XavierUniform(fan_out=in_c)),\n                    ),\n                )\n            self.lateral_convs.append(lateral)\n\n        for i in range(st_stage, ed_stage + 1):\n            fpn_name = \"fpn_res{}_sum\".format(i + 2)\n            if self.norm_type is not None:\n                fpn_conv = self.add_sublayer(\n                    fpn_name,\n                    ConvNormLayer(\n                        ch_in=out_channels,\n                        ch_out=out_channels,\n                        filter_size=3,\n                        stride=1,\n                        norm_type=self.norm_type,\n                        norm_decay=self.norm_decay,\n                        freeze_norm=self.freeze_norm,\n                        initializer=XavierUniform(fan_out=fan),\n                    ),\n                )\n            else:\n                fpn_conv = self.add_sublayer(\n                    fpn_name,\n                    nn.Conv2D(\n                        in_channels=out_channels,\n                        out_channels=out_channels,\n                        kernel_size=3,\n                        padding=1,\n                        weight_attr=ParamAttr(initializer=XavierUniform(fan_out=fan)),\n                    ),\n                )\n            self.fpn_convs.append(fpn_conv)\n\n        # add extra conv levels for RetinaNet(use_c5)/FCOS(use_p5)\n        if self.has_extra_convs:\n            for i in range(self.extra_stage):\n                lvl = ed_stage + 1 + i\n                if i == 0 and self.use_c5:\n                    in_c = in_channels[-1]\n                else:\n                    in_c = out_channels\n                extra_fpn_name = \"fpn_{}\".format(lvl + 2)\n                if self.norm_type is not None:\n                    extra_fpn_conv = self.add_sublayer(\n                        extra_fpn_name,\n                        ConvNormLayer(\n                            ch_in=in_c,\n                            ch_out=out_channels,\n                            filter_size=3,\n                            stride=2,\n                            norm_type=self.norm_type,\n                            norm_decay=self.norm_decay,\n                            freeze_norm=self.freeze_norm,\n                            initializer=XavierUniform(fan_out=fan),\n                        ),\n                    )\n                else:\n                    extra_fpn_conv = self.add_sublayer(\n                        extra_fpn_name,\n                        nn.Conv2D(\n                            in_channels=in_c,\n                            out_channels=out_channels,\n                            kernel_size=3,\n                            stride=2,\n                            padding=1,\n                            weight_attr=ParamAttr(\n                                initializer=XavierUniform(fan_out=fan)\n                            ),\n                        ),\n                    )\n                self.fpn_convs.append(extra_fpn_conv)\n\n    @classmethod\n    def from_config(cls, cfg, input_shape):\n        return {\n            \"in_channels\": [i.channels for i in input_shape],\n            \"spatial_scales\": [1.0 / i.stride for i in input_shape],\n        }\n\n    def forward(self, body_feats):\n        laterals = []\n        num_levels = len(body_feats)\n\n        for i in range(num_levels):\n            laterals.append(self.lateral_convs[i](body_feats[i]))\n\n        for i in range(1, num_levels):\n            lvl = num_levels - i\n            upsample = F.interpolate(\n                laterals[lvl],\n                scale_factor=2.0,\n                mode=\"nearest\",\n            )\n            laterals[lvl - 1] += upsample\n\n        fpn_output = []\n        for lvl in range(num_levels):\n            fpn_output.append(self.fpn_convs[lvl](laterals[lvl]))\n\n        if self.extra_stage > 0:\n            # use max pool to get more levels on top of outputs (Faster R-CNN, Mask R-CNN)\n            if not self.has_extra_convs:\n                assert (\n                    self.extra_stage == 1\n                ), \"extra_stage should be 1 if FPN has not extra convs\"\n                fpn_output.append(F.max_pool2d(fpn_output[-1], 1, stride=2))\n            # add extra conv levels for RetinaNet(use_c5)/FCOS(use_p5)\n            else:\n                if self.use_c5:\n                    extra_source = body_feats[-1]\n                else:\n                    extra_source = fpn_output[-1]\n                fpn_output.append(self.fpn_convs[num_levels](extra_source))\n\n                for i in range(1, self.extra_stage):\n                    if self.relu_before_extra_convs:\n                        fpn_output.append(\n                            self.fpn_convs[num_levels + i](F.relu(fpn_output[-1]))\n                        )\n                    else:\n                        fpn_output.append(\n                            self.fpn_convs[num_levels + i](fpn_output[-1])\n                        )\n        return fpn_output\n", "ppocr/modeling/necks/fpn.py": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/whai362/PSENet/blob/python3/models/neck/fpn.py\n\"\"\"\n\nimport paddle.nn as nn\nimport paddle\nimport math\nimport paddle.nn.functional as F\n\n\nclass Conv_BN_ReLU(nn.Layer):\n    def __init__(self, in_planes, out_planes, kernel_size=1, stride=1, padding=0):\n        super(Conv_BN_ReLU, self).__init__()\n        self.conv = nn.Conv2D(\n            in_planes,\n            out_planes,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            bias_attr=False,\n        )\n        self.bn = nn.BatchNorm2D(out_planes, momentum=0.1)\n        self.relu = nn.ReLU()\n\n        for m in self.sublayers():\n            if isinstance(m, nn.Conv2D):\n                n = m._kernel_size[0] * m._kernel_size[1] * m._out_channels\n                m.weight = paddle.create_parameter(\n                    shape=m.weight.shape,\n                    dtype=\"float32\",\n                    default_initializer=paddle.nn.initializer.Normal(\n                        0, math.sqrt(2.0 / n)\n                    ),\n                )\n            elif isinstance(m, nn.BatchNorm2D):\n                m.weight = paddle.create_parameter(\n                    shape=m.weight.shape,\n                    dtype=\"float32\",\n                    default_initializer=paddle.nn.initializer.Constant(1.0),\n                )\n                m.bias = paddle.create_parameter(\n                    shape=m.bias.shape,\n                    dtype=\"float32\",\n                    default_initializer=paddle.nn.initializer.Constant(0.0),\n                )\n\n    def forward(self, x):\n        return self.relu(self.bn(self.conv(x)))\n\n\nclass FPN(nn.Layer):\n    def __init__(self, in_channels, out_channels):\n        super(FPN, self).__init__()\n\n        # Top layer\n        self.toplayer_ = Conv_BN_ReLU(\n            in_channels[3], out_channels, kernel_size=1, stride=1, padding=0\n        )\n        # Lateral layers\n        self.latlayer1_ = Conv_BN_ReLU(\n            in_channels[2], out_channels, kernel_size=1, stride=1, padding=0\n        )\n\n        self.latlayer2_ = Conv_BN_ReLU(\n            in_channels[1], out_channels, kernel_size=1, stride=1, padding=0\n        )\n\n        self.latlayer3_ = Conv_BN_ReLU(\n            in_channels[0], out_channels, kernel_size=1, stride=1, padding=0\n        )\n\n        # Smooth layers\n        self.smooth1_ = Conv_BN_ReLU(\n            out_channels, out_channels, kernel_size=3, stride=1, padding=1\n        )\n\n        self.smooth2_ = Conv_BN_ReLU(\n            out_channels, out_channels, kernel_size=3, stride=1, padding=1\n        )\n\n        self.smooth3_ = Conv_BN_ReLU(\n            out_channels, out_channels, kernel_size=3, stride=1, padding=1\n        )\n\n        self.out_channels = out_channels * 4\n        for m in self.sublayers():\n            if isinstance(m, nn.Conv2D):\n                n = m._kernel_size[0] * m._kernel_size[1] * m._out_channels\n                m.weight = paddle.create_parameter(\n                    shape=m.weight.shape,\n                    dtype=\"float32\",\n                    default_initializer=paddle.nn.initializer.Normal(\n                        0, math.sqrt(2.0 / n)\n                    ),\n                )\n            elif isinstance(m, nn.BatchNorm2D):\n                m.weight = paddle.create_parameter(\n                    shape=m.weight.shape,\n                    dtype=\"float32\",\n                    default_initializer=paddle.nn.initializer.Constant(1.0),\n                )\n                m.bias = paddle.create_parameter(\n                    shape=m.bias.shape,\n                    dtype=\"float32\",\n                    default_initializer=paddle.nn.initializer.Constant(0.0),\n                )\n\n    def _upsample(self, x, scale=1):\n        return F.upsample(x, scale_factor=scale, mode=\"bilinear\")\n\n    def _upsample_add(self, x, y, scale=1):\n        return F.upsample(x, scale_factor=scale, mode=\"bilinear\") + y\n\n    def forward(self, x):\n        f2, f3, f4, f5 = x\n        p5 = self.toplayer_(f5)\n\n        f4 = self.latlayer1_(f4)\n        p4 = self._upsample_add(p5, f4, 2)\n        p4 = self.smooth1_(p4)\n\n        f3 = self.latlayer2_(f3)\n        p3 = self._upsample_add(p4, f3, 2)\n        p3 = self.smooth2_(p3)\n\n        f2 = self.latlayer3_(f2)\n        p2 = self._upsample_add(p3, f2, 2)\n        p2 = self.smooth3_(p2)\n\n        p3 = self._upsample(p3, 2)\n        p4 = self._upsample(p4, 4)\n        p5 = self._upsample(p5, 8)\n\n        fuse = paddle.concat([p2, p3, p4, p5], axis=1)\n        return fuse\n", "ppocr/modeling/necks/pren_fpn.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nCode is refer from:\nhttps://github.com/RuijieJ/pren/blob/main/Nets/Aggregation.py\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nfrom paddle import nn\nimport paddle.nn.functional as F\n\n\nclass PoolAggregate(nn.Layer):\n    def __init__(self, n_r, d_in, d_middle=None, d_out=None):\n        super(PoolAggregate, self).__init__()\n        if not d_middle:\n            d_middle = d_in\n        if not d_out:\n            d_out = d_in\n\n        self.d_in = d_in\n        self.d_middle = d_middle\n        self.d_out = d_out\n        self.act = nn.Swish()\n\n        self.n_r = n_r\n        self.aggs = self._build_aggs()\n\n    def _build_aggs(self):\n        aggs = []\n        for i in range(self.n_r):\n            aggs.append(\n                self.add_sublayer(\n                    \"{}\".format(i),\n                    nn.Sequential(\n                        (\n                            \"conv1\",\n                            nn.Conv2D(\n                                self.d_in, self.d_middle, 3, 2, 1, bias_attr=False\n                            ),\n                        ),\n                        (\"bn1\", nn.BatchNorm(self.d_middle)),\n                        (\"act\", self.act),\n                        (\n                            \"conv2\",\n                            nn.Conv2D(\n                                self.d_middle, self.d_out, 3, 2, 1, bias_attr=False\n                            ),\n                        ),\n                        (\"bn2\", nn.BatchNorm(self.d_out)),\n                    ),\n                )\n            )\n        return aggs\n\n    def forward(self, x):\n        b = x.shape[0]\n        outs = []\n        for agg in self.aggs:\n            y = agg(x)\n            p = F.adaptive_avg_pool2d(y, 1)\n            outs.append(p.reshape((b, 1, self.d_out)))\n        out = paddle.concat(outs, 1)\n        return out\n\n\nclass WeightAggregate(nn.Layer):\n    def __init__(self, n_r, d_in, d_middle=None, d_out=None):\n        super(WeightAggregate, self).__init__()\n        if not d_middle:\n            d_middle = d_in\n        if not d_out:\n            d_out = d_in\n\n        self.n_r = n_r\n        self.d_out = d_out\n        self.act = nn.Swish()\n\n        self.conv_n = nn.Sequential(\n            (\"conv1\", nn.Conv2D(d_in, d_in, 3, 1, 1, bias_attr=False)),\n            (\"bn1\", nn.BatchNorm(d_in)),\n            (\"act1\", self.act),\n            (\"conv2\", nn.Conv2D(d_in, n_r, 1, bias_attr=False)),\n            (\"bn2\", nn.BatchNorm(n_r)),\n            (\"act2\", nn.Sigmoid()),\n        )\n        self.conv_d = nn.Sequential(\n            (\"conv1\", nn.Conv2D(d_in, d_middle, 3, 1, 1, bias_attr=False)),\n            (\"bn1\", nn.BatchNorm(d_middle)),\n            (\"act1\", self.act),\n            (\"conv2\", nn.Conv2D(d_middle, d_out, 1, bias_attr=False)),\n            (\"bn2\", nn.BatchNorm(d_out)),\n        )\n\n    def forward(self, x):\n        b, _, h, w = x.shape\n\n        hmaps = self.conv_n(x)\n        fmaps = self.conv_d(x)\n        r = paddle.bmm(\n            hmaps.reshape((b, self.n_r, h * w)),\n            fmaps.reshape((b, self.d_out, h * w)).transpose((0, 2, 1)),\n        )\n        return r\n\n\nclass GCN(nn.Layer):\n    def __init__(self, d_in, n_in, d_out=None, n_out=None, dropout=0.1):\n        super(GCN, self).__init__()\n        if not d_out:\n            d_out = d_in\n        if not n_out:\n            n_out = d_in\n\n        self.conv_n = nn.Conv1D(n_in, n_out, 1)\n        self.linear = nn.Linear(d_in, d_out)\n        self.dropout = nn.Dropout(dropout)\n        self.act = nn.Swish()\n\n    def forward(self, x):\n        x = self.conv_n(x)\n        x = self.dropout(self.linear(x))\n        return self.act(x)\n\n\nclass PRENFPN(nn.Layer):\n    def __init__(self, in_channels, n_r, d_model, max_len, dropout):\n        super(PRENFPN, self).__init__()\n        assert len(in_channels) == 3, \"in_channels' length must be 3.\"\n        c1, c2, c3 = in_channels  # the depths are from big to small\n        # build fpn\n        assert d_model % 3 == 0, \"{} can't be divided by 3.\".format(d_model)\n        self.agg_p1 = PoolAggregate(n_r, c1, d_out=d_model // 3)\n        self.agg_p2 = PoolAggregate(n_r, c2, d_out=d_model // 3)\n        self.agg_p3 = PoolAggregate(n_r, c3, d_out=d_model // 3)\n\n        self.agg_w1 = WeightAggregate(n_r, c1, 4 * c1, d_model // 3)\n        self.agg_w2 = WeightAggregate(n_r, c2, 4 * c2, d_model // 3)\n        self.agg_w3 = WeightAggregate(n_r, c3, 4 * c3, d_model // 3)\n\n        self.gcn_pool = GCN(d_model, n_r, d_model, max_len, dropout)\n        self.gcn_weight = GCN(d_model, n_r, d_model, max_len, dropout)\n\n        self.out_channels = d_model\n\n    def forward(self, inputs):\n        f3, f5, f7 = inputs\n\n        rp1 = self.agg_p1(f3)\n        rp2 = self.agg_p2(f5)\n        rp3 = self.agg_p3(f7)\n        rp = paddle.concat([rp1, rp2, rp3], 2)  # [b,nr,d]\n\n        rw1 = self.agg_w1(f3)\n        rw2 = self.agg_w2(f5)\n        rw3 = self.agg_w3(f7)\n        rw = paddle.concat([rw1, rw2, rw3], 2)  # [b,nr,d]\n\n        y1 = self.gcn_pool(rp)\n        y2 = self.gcn_weight(rw)\n        y = 0.5 * (y1 + y2)\n        return y  # [b,max_len,d]\n", "ppocr/modeling/necks/pg_fpn.py": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nfrom paddle import nn\nimport paddle.nn.functional as F\nfrom paddle import ParamAttr\n\n\nclass ConvBNLayer(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride=1,\n        groups=1,\n        is_vd_mode=False,\n        act=None,\n        name=None,\n    ):\n        super(ConvBNLayer, self).__init__()\n\n        self.is_vd_mode = is_vd_mode\n        self._pool2d_avg = nn.AvgPool2D(\n            kernel_size=2, stride=2, padding=0, ceil_mode=True\n        )\n        self._conv = nn.Conv2D(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=(kernel_size - 1) // 2,\n            groups=groups,\n            weight_attr=ParamAttr(name=name + \"_weights\"),\n            bias_attr=False,\n        )\n        if name == \"conv1\":\n            bn_name = \"bn_\" + name\n        else:\n            bn_name = \"bn\" + name[3:]\n        self._batch_norm = nn.BatchNorm(\n            out_channels,\n            act=act,\n            param_attr=ParamAttr(name=bn_name + \"_scale\"),\n            bias_attr=ParamAttr(bn_name + \"_offset\"),\n            moving_mean_name=bn_name + \"_mean\",\n            moving_variance_name=bn_name + \"_variance\",\n            use_global_stats=False,\n        )\n\n    def forward(self, inputs):\n        y = self._conv(inputs)\n        y = self._batch_norm(y)\n        return y\n\n\nclass DeConvBNLayer(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size=4,\n        stride=2,\n        padding=1,\n        groups=1,\n        if_act=True,\n        act=None,\n        name=None,\n    ):\n        super(DeConvBNLayer, self).__init__()\n\n        self.if_act = if_act\n        self.act = act\n        self.deconv = nn.Conv2DTranspose(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            groups=groups,\n            weight_attr=ParamAttr(name=name + \"_weights\"),\n            bias_attr=False,\n        )\n        self.bn = nn.BatchNorm(\n            num_channels=out_channels,\n            act=act,\n            param_attr=ParamAttr(name=\"bn_\" + name + \"_scale\"),\n            bias_attr=ParamAttr(name=\"bn_\" + name + \"_offset\"),\n            moving_mean_name=\"bn_\" + name + \"_mean\",\n            moving_variance_name=\"bn_\" + name + \"_variance\",\n            use_global_stats=False,\n        )\n\n    def forward(self, x):\n        x = self.deconv(x)\n        x = self.bn(x)\n        return x\n\n\nclass PGFPN(nn.Layer):\n    def __init__(self, in_channels, **kwargs):\n        super(PGFPN, self).__init__()\n        num_inputs = [2048, 2048, 1024, 512, 256]\n        num_outputs = [256, 256, 192, 192, 128]\n        self.out_channels = 128\n        self.conv_bn_layer_1 = ConvBNLayer(\n            in_channels=3,\n            out_channels=32,\n            kernel_size=3,\n            stride=1,\n            act=None,\n            name=\"FPN_d1\",\n        )\n        self.conv_bn_layer_2 = ConvBNLayer(\n            in_channels=64,\n            out_channels=64,\n            kernel_size=3,\n            stride=1,\n            act=None,\n            name=\"FPN_d2\",\n        )\n        self.conv_bn_layer_3 = ConvBNLayer(\n            in_channels=256,\n            out_channels=128,\n            kernel_size=3,\n            stride=1,\n            act=None,\n            name=\"FPN_d3\",\n        )\n        self.conv_bn_layer_4 = ConvBNLayer(\n            in_channels=32,\n            out_channels=64,\n            kernel_size=3,\n            stride=2,\n            act=None,\n            name=\"FPN_d4\",\n        )\n        self.conv_bn_layer_5 = ConvBNLayer(\n            in_channels=64,\n            out_channels=64,\n            kernel_size=3,\n            stride=1,\n            act=\"relu\",\n            name=\"FPN_d5\",\n        )\n        self.conv_bn_layer_6 = ConvBNLayer(\n            in_channels=64,\n            out_channels=128,\n            kernel_size=3,\n            stride=2,\n            act=None,\n            name=\"FPN_d6\",\n        )\n        self.conv_bn_layer_7 = ConvBNLayer(\n            in_channels=128,\n            out_channels=128,\n            kernel_size=3,\n            stride=1,\n            act=\"relu\",\n            name=\"FPN_d7\",\n        )\n        self.conv_bn_layer_8 = ConvBNLayer(\n            in_channels=128,\n            out_channels=128,\n            kernel_size=1,\n            stride=1,\n            act=None,\n            name=\"FPN_d8\",\n        )\n\n        self.conv_h0 = ConvBNLayer(\n            in_channels=num_inputs[0],\n            out_channels=num_outputs[0],\n            kernel_size=1,\n            stride=1,\n            act=None,\n            name=\"conv_h{}\".format(0),\n        )\n        self.conv_h1 = ConvBNLayer(\n            in_channels=num_inputs[1],\n            out_channels=num_outputs[1],\n            kernel_size=1,\n            stride=1,\n            act=None,\n            name=\"conv_h{}\".format(1),\n        )\n        self.conv_h2 = ConvBNLayer(\n            in_channels=num_inputs[2],\n            out_channels=num_outputs[2],\n            kernel_size=1,\n            stride=1,\n            act=None,\n            name=\"conv_h{}\".format(2),\n        )\n        self.conv_h3 = ConvBNLayer(\n            in_channels=num_inputs[3],\n            out_channels=num_outputs[3],\n            kernel_size=1,\n            stride=1,\n            act=None,\n            name=\"conv_h{}\".format(3),\n        )\n        self.conv_h4 = ConvBNLayer(\n            in_channels=num_inputs[4],\n            out_channels=num_outputs[4],\n            kernel_size=1,\n            stride=1,\n            act=None,\n            name=\"conv_h{}\".format(4),\n        )\n\n        self.dconv0 = DeConvBNLayer(\n            in_channels=num_outputs[0],\n            out_channels=num_outputs[0 + 1],\n            name=\"dconv_{}\".format(0),\n        )\n        self.dconv1 = DeConvBNLayer(\n            in_channels=num_outputs[1],\n            out_channels=num_outputs[1 + 1],\n            act=None,\n            name=\"dconv_{}\".format(1),\n        )\n        self.dconv2 = DeConvBNLayer(\n            in_channels=num_outputs[2],\n            out_channels=num_outputs[2 + 1],\n            act=None,\n            name=\"dconv_{}\".format(2),\n        )\n        self.dconv3 = DeConvBNLayer(\n            in_channels=num_outputs[3],\n            out_channels=num_outputs[3 + 1],\n            act=None,\n            name=\"dconv_{}\".format(3),\n        )\n        self.conv_g1 = ConvBNLayer(\n            in_channels=num_outputs[1],\n            out_channels=num_outputs[1],\n            kernel_size=3,\n            stride=1,\n            act=\"relu\",\n            name=\"conv_g{}\".format(1),\n        )\n        self.conv_g2 = ConvBNLayer(\n            in_channels=num_outputs[2],\n            out_channels=num_outputs[2],\n            kernel_size=3,\n            stride=1,\n            act=\"relu\",\n            name=\"conv_g{}\".format(2),\n        )\n        self.conv_g3 = ConvBNLayer(\n            in_channels=num_outputs[3],\n            out_channels=num_outputs[3],\n            kernel_size=3,\n            stride=1,\n            act=\"relu\",\n            name=\"conv_g{}\".format(3),\n        )\n        self.conv_g4 = ConvBNLayer(\n            in_channels=num_outputs[4],\n            out_channels=num_outputs[4],\n            kernel_size=3,\n            stride=1,\n            act=\"relu\",\n            name=\"conv_g{}\".format(4),\n        )\n        self.convf = ConvBNLayer(\n            in_channels=num_outputs[4],\n            out_channels=num_outputs[4],\n            kernel_size=1,\n            stride=1,\n            act=None,\n            name=\"conv_f{}\".format(4),\n        )\n\n    def forward(self, x):\n        c0, c1, c2, c3, c4, c5, c6 = x\n        # FPN_Down_Fusion\n        f = [c0, c1, c2]\n        g = [None, None, None]\n        h = [None, None, None]\n        h[0] = self.conv_bn_layer_1(f[0])\n        h[1] = self.conv_bn_layer_2(f[1])\n        h[2] = self.conv_bn_layer_3(f[2])\n\n        g[0] = self.conv_bn_layer_4(h[0])\n        g[1] = paddle.add(g[0], h[1])\n        g[1] = F.relu(g[1])\n        g[1] = self.conv_bn_layer_5(g[1])\n        g[1] = self.conv_bn_layer_6(g[1])\n\n        g[2] = paddle.add(g[1], h[2])\n        g[2] = F.relu(g[2])\n        g[2] = self.conv_bn_layer_7(g[2])\n        f_down = self.conv_bn_layer_8(g[2])\n\n        # FPN UP Fusion\n        f1 = [c6, c5, c4, c3, c2]\n        g = [None, None, None, None, None]\n        h = [None, None, None, None, None]\n        h[0] = self.conv_h0(f1[0])\n        h[1] = self.conv_h1(f1[1])\n        h[2] = self.conv_h2(f1[2])\n        h[3] = self.conv_h3(f1[3])\n        h[4] = self.conv_h4(f1[4])\n\n        g[0] = self.dconv0(h[0])\n        g[1] = paddle.add(g[0], h[1])\n        g[1] = F.relu(g[1])\n        g[1] = self.conv_g1(g[1])\n        g[1] = self.dconv1(g[1])\n\n        g[2] = paddle.add(g[1], h[2])\n        g[2] = F.relu(g[2])\n        g[2] = self.conv_g2(g[2])\n        g[2] = self.dconv2(g[2])\n\n        g[3] = paddle.add(g[2], h[3])\n        g[3] = F.relu(g[3])\n        g[3] = self.conv_g3(g[3])\n        g[3] = self.dconv3(g[3])\n\n        g[4] = paddle.add(x=g[3], y=h[4])\n        g[4] = F.relu(g[4])\n        g[4] = self.conv_g4(g[4])\n        f_up = self.convf(g[4])\n        f_common = paddle.add(f_down, f_up)\n        f_common = F.relu(f_common)\n        return f_common\n", "ppocr/modeling/necks/csp_pan.py": "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# The code is based on:\n# https://github.com/PaddlePaddle/PaddleDetection/blob/release%2F2.3/ppdet/modeling/necks/csp_pan.py\n\nimport paddle\nimport paddle.nn as nn\nimport paddle.nn.functional as F\nfrom paddle import ParamAttr\n\n__all__ = [\"CSPPAN\"]\n\n\nclass ConvBNLayer(nn.Layer):\n    def __init__(\n        self,\n        in_channel=96,\n        out_channel=96,\n        kernel_size=3,\n        stride=1,\n        groups=1,\n        act=\"leaky_relu\",\n    ):\n        super(ConvBNLayer, self).__init__()\n        initializer = nn.initializer.KaimingUniform()\n        self.act = act\n        assert self.act in [\"leaky_relu\", \"hard_swish\"]\n        self.conv = nn.Conv2D(\n            in_channels=in_channel,\n            out_channels=out_channel,\n            kernel_size=kernel_size,\n            groups=groups,\n            padding=(kernel_size - 1) // 2,\n            stride=stride,\n            weight_attr=ParamAttr(initializer=initializer),\n            bias_attr=False,\n        )\n        self.bn = nn.BatchNorm2D(out_channel)\n\n    def forward(self, x):\n        x = self.bn(self.conv(x))\n        if self.act == \"leaky_relu\":\n            x = F.leaky_relu(x)\n        elif self.act == \"hard_swish\":\n            x = F.hardswish(x)\n        return x\n\n\nclass DPModule(nn.Layer):\n    \"\"\"\n    Depth-wise and point-wise module.\n     Args:\n        in_channel (int): The input channels of this Module.\n        out_channel (int): The output channels of this Module.\n        kernel_size (int): The conv2d kernel size of this Module.\n        stride (int): The conv2d's stride of this Module.\n        act (str): The activation function of this Module,\n                   Now support `leaky_relu` and `hard_swish`.\n    \"\"\"\n\n    def __init__(\n        self, in_channel=96, out_channel=96, kernel_size=3, stride=1, act=\"leaky_relu\"\n    ):\n        super(DPModule, self).__init__()\n        initializer = nn.initializer.KaimingUniform()\n        self.act = act\n        self.dwconv = nn.Conv2D(\n            in_channels=in_channel,\n            out_channels=out_channel,\n            kernel_size=kernel_size,\n            groups=out_channel,\n            padding=(kernel_size - 1) // 2,\n            stride=stride,\n            weight_attr=ParamAttr(initializer=initializer),\n            bias_attr=False,\n        )\n        self.bn1 = nn.BatchNorm2D(out_channel)\n        self.pwconv = nn.Conv2D(\n            in_channels=out_channel,\n            out_channels=out_channel,\n            kernel_size=1,\n            groups=1,\n            padding=0,\n            weight_attr=ParamAttr(initializer=initializer),\n            bias_attr=False,\n        )\n        self.bn2 = nn.BatchNorm2D(out_channel)\n\n    def act_func(self, x):\n        if self.act == \"leaky_relu\":\n            x = F.leaky_relu(x)\n        elif self.act == \"hard_swish\":\n            x = F.hardswish(x)\n        return x\n\n    def forward(self, x):\n        x = self.act_func(self.bn1(self.dwconv(x)))\n        x = self.act_func(self.bn2(self.pwconv(x)))\n        return x\n\n\nclass DarknetBottleneck(nn.Layer):\n    \"\"\"The basic bottleneck block used in Darknet.\n    Each Block consists of two ConvModules and the input is added to the\n    final output. Each ConvModule is composed of Conv, BN, and act.\n    The first convLayer has filter size of 1x1 and the second one has the\n    filter size of 3x3.\n    Args:\n        in_channels (int): The input channels of this Module.\n        out_channels (int): The output channels of this Module.\n        expansion (int): The kernel size of the convolution. Default: 0.5\n        add_identity (bool): Whether to add identity to the out.\n            Default: True\n        use_depthwise (bool): Whether to use depthwise separable convolution.\n            Default: False\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size=3,\n        expansion=0.5,\n        add_identity=True,\n        use_depthwise=False,\n        act=\"leaky_relu\",\n    ):\n        super(DarknetBottleneck, self).__init__()\n        hidden_channels = int(out_channels * expansion)\n        conv_func = DPModule if use_depthwise else ConvBNLayer\n        self.conv1 = ConvBNLayer(\n            in_channel=in_channels, out_channel=hidden_channels, kernel_size=1, act=act\n        )\n        self.conv2 = conv_func(\n            in_channel=hidden_channels,\n            out_channel=out_channels,\n            kernel_size=kernel_size,\n            stride=1,\n            act=act,\n        )\n        self.add_identity = add_identity and in_channels == out_channels\n\n    def forward(self, x):\n        identity = x\n        out = self.conv1(x)\n        out = self.conv2(out)\n\n        if self.add_identity:\n            return out + identity\n        else:\n            return out\n\n\nclass CSPLayer(nn.Layer):\n    \"\"\"Cross Stage Partial Layer.\n    Args:\n        in_channels (int): The input channels of the CSP layer.\n        out_channels (int): The output channels of the CSP layer.\n        expand_ratio (float): Ratio to adjust the number of channels of the\n            hidden layer. Default: 0.5\n        num_blocks (int): Number of blocks. Default: 1\n        add_identity (bool): Whether to add identity in blocks.\n            Default: True\n        use_depthwise (bool): Whether to depthwise separable convolution in\n            blocks. Default: False\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size=3,\n        expand_ratio=0.5,\n        num_blocks=1,\n        add_identity=True,\n        use_depthwise=False,\n        act=\"leaky_relu\",\n    ):\n        super().__init__()\n        mid_channels = int(out_channels * expand_ratio)\n        self.main_conv = ConvBNLayer(in_channels, mid_channels, 1, act=act)\n        self.short_conv = ConvBNLayer(in_channels, mid_channels, 1, act=act)\n        self.final_conv = ConvBNLayer(2 * mid_channels, out_channels, 1, act=act)\n\n        self.blocks = nn.Sequential(\n            *[\n                DarknetBottleneck(\n                    mid_channels,\n                    mid_channels,\n                    kernel_size,\n                    1.0,\n                    add_identity,\n                    use_depthwise,\n                    act=act,\n                )\n                for _ in range(num_blocks)\n            ]\n        )\n\n    def forward(self, x):\n        x_short = self.short_conv(x)\n\n        x_main = self.main_conv(x)\n        x_main = self.blocks(x_main)\n\n        x_final = paddle.concat((x_main, x_short), axis=1)\n        return self.final_conv(x_final)\n\n\nclass Channel_T(nn.Layer):\n    def __init__(self, in_channels=[116, 232, 464], out_channels=96, act=\"leaky_relu\"):\n        super(Channel_T, self).__init__()\n        self.convs = nn.LayerList()\n        for i in range(len(in_channels)):\n            self.convs.append(ConvBNLayer(in_channels[i], out_channels, 1, act=act))\n\n    def forward(self, x):\n        outs = [self.convs[i](x[i]) for i in range(len(x))]\n        return outs\n\n\nclass CSPPAN(nn.Layer):\n    \"\"\"Path Aggregation Network with CSP module.\n    Args:\n        in_channels (List[int]): Number of input channels per scale.\n        out_channels (int): Number of output channels (used at each scale)\n        kernel_size (int): The conv2d kernel size of this Module.\n        num_csp_blocks (int): Number of bottlenecks in CSPLayer. Default: 1\n        use_depthwise (bool): Whether to depthwise separable convolution in\n            blocks. Default: True\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size=5,\n        num_csp_blocks=1,\n        use_depthwise=True,\n        act=\"hard_swish\",\n    ):\n        super(CSPPAN, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = [out_channels] * len(in_channels)\n        conv_func = DPModule if use_depthwise else ConvBNLayer\n\n        self.conv_t = Channel_T(in_channels, out_channels, act=act)\n\n        # build top-down blocks\n        self.upsample = nn.Upsample(scale_factor=2, mode=\"nearest\")\n        self.top_down_blocks = nn.LayerList()\n        for idx in range(len(in_channels) - 1, 0, -1):\n            self.top_down_blocks.append(\n                CSPLayer(\n                    out_channels * 2,\n                    out_channels,\n                    kernel_size=kernel_size,\n                    num_blocks=num_csp_blocks,\n                    add_identity=False,\n                    use_depthwise=use_depthwise,\n                    act=act,\n                )\n            )\n\n        # build bottom-up blocks\n        self.downsamples = nn.LayerList()\n        self.bottom_up_blocks = nn.LayerList()\n        for idx in range(len(in_channels) - 1):\n            self.downsamples.append(\n                conv_func(\n                    out_channels,\n                    out_channels,\n                    kernel_size=kernel_size,\n                    stride=2,\n                    act=act,\n                )\n            )\n            self.bottom_up_blocks.append(\n                CSPLayer(\n                    out_channels * 2,\n                    out_channels,\n                    kernel_size=kernel_size,\n                    num_blocks=num_csp_blocks,\n                    add_identity=False,\n                    use_depthwise=use_depthwise,\n                    act=act,\n                )\n            )\n\n    def forward(self, inputs):\n        \"\"\"\n        Args:\n            inputs (tuple[Tensor]): input features.\n        Returns:\n            tuple[Tensor]: CSPPAN features.\n        \"\"\"\n        assert len(inputs) == len(self.in_channels)\n        inputs = self.conv_t(inputs)\n\n        # top-down path\n        inner_outs = [inputs[-1]]\n        for idx in range(len(self.in_channels) - 1, 0, -1):\n            feat_heigh = inner_outs[0]\n            feat_low = inputs[idx - 1]\n            upsample_feat = F.upsample(\n                feat_heigh, size=feat_low.shape[2:4], mode=\"nearest\"\n            )\n\n            inner_out = self.top_down_blocks[len(self.in_channels) - 1 - idx](\n                paddle.concat([upsample_feat, feat_low], 1)\n            )\n            inner_outs.insert(0, inner_out)\n\n        # bottom-up path\n        outs = [inner_outs[0]]\n        for idx in range(len(self.in_channels) - 1):\n            feat_low = outs[-1]\n            feat_height = inner_outs[idx + 1]\n            downsample_feat = self.downsamples[idx](feat_low)\n            out = self.bottom_up_blocks[idx](\n                paddle.concat([downsample_feat, feat_height], 1)\n            )\n            outs.append(out)\n\n        return tuple(outs)\n", "ppocr/modeling/necks/sast_fpn.py": "# copyright (c) 2019 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nfrom paddle import nn\nimport paddle.nn.functional as F\nfrom paddle import ParamAttr\n\n\nclass ConvBNLayer(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride,\n        groups=1,\n        if_act=True,\n        act=None,\n        name=None,\n    ):\n        super(ConvBNLayer, self).__init__()\n        self.if_act = if_act\n        self.act = act\n        self.conv = nn.Conv2D(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=(kernel_size - 1) // 2,\n            groups=groups,\n            weight_attr=ParamAttr(name=name + \"_weights\"),\n            bias_attr=False,\n        )\n\n        self.bn = nn.BatchNorm(\n            num_channels=out_channels,\n            act=act,\n            param_attr=ParamAttr(name=\"bn_\" + name + \"_scale\"),\n            bias_attr=ParamAttr(name=\"bn_\" + name + \"_offset\"),\n            moving_mean_name=\"bn_\" + name + \"_mean\",\n            moving_variance_name=\"bn_\" + name + \"_variance\",\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return x\n\n\nclass DeConvBNLayer(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride,\n        groups=1,\n        if_act=True,\n        act=None,\n        name=None,\n    ):\n        super(DeConvBNLayer, self).__init__()\n        self.if_act = if_act\n        self.act = act\n        self.deconv = nn.Conv2DTranspose(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=(kernel_size - 1) // 2,\n            groups=groups,\n            weight_attr=ParamAttr(name=name + \"_weights\"),\n            bias_attr=False,\n        )\n        self.bn = nn.BatchNorm(\n            num_channels=out_channels,\n            act=act,\n            param_attr=ParamAttr(name=\"bn_\" + name + \"_scale\"),\n            bias_attr=ParamAttr(name=\"bn_\" + name + \"_offset\"),\n            moving_mean_name=\"bn_\" + name + \"_mean\",\n            moving_variance_name=\"bn_\" + name + \"_variance\",\n        )\n\n    def forward(self, x):\n        x = self.deconv(x)\n        x = self.bn(x)\n        return x\n\n\nclass FPN_Up_Fusion(nn.Layer):\n    def __init__(self, in_channels):\n        super(FPN_Up_Fusion, self).__init__()\n        in_channels = in_channels[::-1]\n        out_channels = [256, 256, 192, 192, 128]\n\n        self.h0_conv = ConvBNLayer(\n            in_channels[0], out_channels[0], 1, 1, act=None, name=\"fpn_up_h0\"\n        )\n        self.h1_conv = ConvBNLayer(\n            in_channels[1], out_channels[1], 1, 1, act=None, name=\"fpn_up_h1\"\n        )\n        self.h2_conv = ConvBNLayer(\n            in_channels[2], out_channels[2], 1, 1, act=None, name=\"fpn_up_h2\"\n        )\n        self.h3_conv = ConvBNLayer(\n            in_channels[3], out_channels[3], 1, 1, act=None, name=\"fpn_up_h3\"\n        )\n        self.h4_conv = ConvBNLayer(\n            in_channels[4], out_channels[4], 1, 1, act=None, name=\"fpn_up_h4\"\n        )\n\n        self.g0_conv = DeConvBNLayer(\n            out_channels[0], out_channels[1], 4, 2, act=None, name=\"fpn_up_g0\"\n        )\n\n        self.g1_conv = nn.Sequential(\n            ConvBNLayer(\n                out_channels[1], out_channels[1], 3, 1, act=\"relu\", name=\"fpn_up_g1_1\"\n            ),\n            DeConvBNLayer(\n                out_channels[1], out_channels[2], 4, 2, act=None, name=\"fpn_up_g1_2\"\n            ),\n        )\n        self.g2_conv = nn.Sequential(\n            ConvBNLayer(\n                out_channels[2], out_channels[2], 3, 1, act=\"relu\", name=\"fpn_up_g2_1\"\n            ),\n            DeConvBNLayer(\n                out_channels[2], out_channels[3], 4, 2, act=None, name=\"fpn_up_g2_2\"\n            ),\n        )\n        self.g3_conv = nn.Sequential(\n            ConvBNLayer(\n                out_channels[3], out_channels[3], 3, 1, act=\"relu\", name=\"fpn_up_g3_1\"\n            ),\n            DeConvBNLayer(\n                out_channels[3], out_channels[4], 4, 2, act=None, name=\"fpn_up_g3_2\"\n            ),\n        )\n\n        self.g4_conv = nn.Sequential(\n            ConvBNLayer(\n                out_channels[4],\n                out_channels[4],\n                3,\n                1,\n                act=\"relu\",\n                name=\"fpn_up_fusion_1\",\n            ),\n            ConvBNLayer(\n                out_channels[4], out_channels[4], 1, 1, act=None, name=\"fpn_up_fusion_2\"\n            ),\n        )\n\n    def _add_relu(self, x1, x2):\n        x = paddle.add(x=x1, y=x2)\n        x = F.relu(x)\n        return x\n\n    def forward(self, x):\n        f = x[2:][::-1]\n        h0 = self.h0_conv(f[0])\n        h1 = self.h1_conv(f[1])\n        h2 = self.h2_conv(f[2])\n        h3 = self.h3_conv(f[3])\n        h4 = self.h4_conv(f[4])\n\n        g0 = self.g0_conv(h0)\n        g1 = self._add_relu(g0, h1)\n        g1 = self.g1_conv(g1)\n        g2 = self.g2_conv(self._add_relu(g1, h2))\n        g3 = self.g3_conv(self._add_relu(g2, h3))\n        g4 = self.g4_conv(self._add_relu(g3, h4))\n\n        return g4\n\n\nclass FPN_Down_Fusion(nn.Layer):\n    def __init__(self, in_channels):\n        super(FPN_Down_Fusion, self).__init__()\n        out_channels = [32, 64, 128]\n\n        self.h0_conv = ConvBNLayer(\n            in_channels[0], out_channels[0], 3, 1, act=None, name=\"fpn_down_h0\"\n        )\n        self.h1_conv = ConvBNLayer(\n            in_channels[1], out_channels[1], 3, 1, act=None, name=\"fpn_down_h1\"\n        )\n        self.h2_conv = ConvBNLayer(\n            in_channels[2], out_channels[2], 3, 1, act=None, name=\"fpn_down_h2\"\n        )\n\n        self.g0_conv = ConvBNLayer(\n            out_channels[0], out_channels[1], 3, 2, act=None, name=\"fpn_down_g0\"\n        )\n\n        self.g1_conv = nn.Sequential(\n            ConvBNLayer(\n                out_channels[1], out_channels[1], 3, 1, act=\"relu\", name=\"fpn_down_g1_1\"\n            ),\n            ConvBNLayer(\n                out_channels[1], out_channels[2], 3, 2, act=None, name=\"fpn_down_g1_2\"\n            ),\n        )\n\n        self.g2_conv = nn.Sequential(\n            ConvBNLayer(\n                out_channels[2],\n                out_channels[2],\n                3,\n                1,\n                act=\"relu\",\n                name=\"fpn_down_fusion_1\",\n            ),\n            ConvBNLayer(\n                out_channels[2],\n                out_channels[2],\n                1,\n                1,\n                act=None,\n                name=\"fpn_down_fusion_2\",\n            ),\n        )\n\n    def forward(self, x):\n        f = x[:3]\n        h0 = self.h0_conv(f[0])\n        h1 = self.h1_conv(f[1])\n        h2 = self.h2_conv(f[2])\n        g0 = self.g0_conv(h0)\n        g1 = paddle.add(x=g0, y=h1)\n        g1 = F.relu(g1)\n        g1 = self.g1_conv(g1)\n        g2 = paddle.add(x=g1, y=h2)\n        g2 = F.relu(g2)\n        g2 = self.g2_conv(g2)\n        return g2\n\n\nclass Cross_Attention(nn.Layer):\n    def __init__(self, in_channels):\n        super(Cross_Attention, self).__init__()\n        self.theta_conv = ConvBNLayer(\n            in_channels, in_channels, 1, 1, act=\"relu\", name=\"f_theta\"\n        )\n        self.phi_conv = ConvBNLayer(\n            in_channels, in_channels, 1, 1, act=\"relu\", name=\"f_phi\"\n        )\n        self.g_conv = ConvBNLayer(\n            in_channels, in_channels, 1, 1, act=\"relu\", name=\"f_g\"\n        )\n\n        self.fh_weight_conv = ConvBNLayer(\n            in_channels, in_channels, 1, 1, act=None, name=\"fh_weight\"\n        )\n        self.fh_sc_conv = ConvBNLayer(\n            in_channels, in_channels, 1, 1, act=None, name=\"fh_sc\"\n        )\n\n        self.fv_weight_conv = ConvBNLayer(\n            in_channels, in_channels, 1, 1, act=None, name=\"fv_weight\"\n        )\n        self.fv_sc_conv = ConvBNLayer(\n            in_channels, in_channels, 1, 1, act=None, name=\"fv_sc\"\n        )\n\n        self.f_attn_conv = ConvBNLayer(\n            in_channels * 2, in_channels, 1, 1, act=\"relu\", name=\"f_attn\"\n        )\n\n    def _cal_fweight(self, f, shape):\n        f_theta, f_phi, f_g = f\n        # flatten\n        f_theta = paddle.transpose(f_theta, [0, 2, 3, 1])\n        f_theta = paddle.reshape(f_theta, [shape[0] * shape[1], shape[2], 128])\n        f_phi = paddle.transpose(f_phi, [0, 2, 3, 1])\n        f_phi = paddle.reshape(f_phi, [shape[0] * shape[1], shape[2], 128])\n        f_g = paddle.transpose(f_g, [0, 2, 3, 1])\n        f_g = paddle.reshape(f_g, [shape[0] * shape[1], shape[2], 128])\n        # correlation\n        f_attn = paddle.matmul(f_theta, paddle.transpose(f_phi, [0, 2, 1]))\n        # scale\n        f_attn = f_attn / (128**0.5)\n        f_attn = F.softmax(f_attn)\n        # weighted sum\n        f_weight = paddle.matmul(f_attn, f_g)\n        f_weight = paddle.reshape(f_weight, [shape[0], shape[1], shape[2], 128])\n        return f_weight\n\n    def forward(self, f_common):\n        f_shape = f_common.shape\n        # print('f_shape: ', f_shape)\n\n        f_theta = self.theta_conv(f_common)\n        f_phi = self.phi_conv(f_common)\n        f_g = self.g_conv(f_common)\n\n        ######## horizon ########\n        fh_weight = self._cal_fweight(\n            [f_theta, f_phi, f_g], [f_shape[0], f_shape[2], f_shape[3]]\n        )\n        fh_weight = paddle.transpose(fh_weight, [0, 3, 1, 2])\n        fh_weight = self.fh_weight_conv(fh_weight)\n        # short cut\n        fh_sc = self.fh_sc_conv(f_common)\n        f_h = F.relu(fh_weight + fh_sc)\n\n        ######## vertical ########\n        fv_theta = paddle.transpose(f_theta, [0, 1, 3, 2])\n        fv_phi = paddle.transpose(f_phi, [0, 1, 3, 2])\n        fv_g = paddle.transpose(f_g, [0, 1, 3, 2])\n        fv_weight = self._cal_fweight(\n            [fv_theta, fv_phi, fv_g], [f_shape[0], f_shape[3], f_shape[2]]\n        )\n        fv_weight = paddle.transpose(fv_weight, [0, 3, 2, 1])\n        fv_weight = self.fv_weight_conv(fv_weight)\n        # short cut\n        fv_sc = self.fv_sc_conv(f_common)\n        f_v = F.relu(fv_weight + fv_sc)\n\n        ######## merge ########\n        f_attn = paddle.concat([f_h, f_v], axis=1)\n        f_attn = self.f_attn_conv(f_attn)\n        return f_attn\n\n\nclass SASTFPN(nn.Layer):\n    def __init__(self, in_channels, with_cab=False, **kwargs):\n        super(SASTFPN, self).__init__()\n        self.in_channels = in_channels\n        self.with_cab = with_cab\n        self.FPN_Down_Fusion = FPN_Down_Fusion(self.in_channels)\n        self.FPN_Up_Fusion = FPN_Up_Fusion(self.in_channels)\n        self.out_channels = 128\n        self.cross_attention = Cross_Attention(self.out_channels)\n\n    def forward(self, x):\n        # down fpn\n        f_down = self.FPN_Down_Fusion(x)\n\n        # up fpn\n        f_up = self.FPN_Up_Fusion(x)\n\n        # fusion\n        f_common = paddle.add(x=f_down, y=f_up)\n        f_common = F.relu(f_common)\n\n        if self.with_cab:\n            # print('enhence f_common with CAB.')\n            f_common = self.cross_attention(f_common)\n\n        return f_common\n", "ppocr/modeling/necks/rf_adaptor.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/hikopensource/DAVAR-Lab-OCR/blob/main/davarocr/davar_rcg/models/connects/single_block/RFAdaptor.py\n\"\"\"\n\nimport paddle\nimport paddle.nn as nn\nfrom paddle.nn.initializer import TruncatedNormal, Constant, Normal, KaimingNormal\n\nkaiming_init_ = KaimingNormal()\nzeros_ = Constant(value=0.0)\nones_ = Constant(value=1.0)\n\n\nclass S2VAdaptor(nn.Layer):\n    \"\"\"Semantic to Visual adaptation module\"\"\"\n\n    def __init__(self, in_channels=512):\n        super(S2VAdaptor, self).__init__()\n\n        self.in_channels = in_channels  # 512\n\n        # feature strengthen module, channel attention\n        self.channel_inter = nn.Linear(\n            self.in_channels, self.in_channels, bias_attr=False\n        )\n        self.channel_bn = nn.BatchNorm1D(self.in_channels)\n        self.channel_act = nn.ReLU()\n        self.apply(self.init_weights)\n\n    def init_weights(self, m):\n        if isinstance(m, nn.Conv2D):\n            kaiming_init_(m.weight)\n            if isinstance(m, nn.Conv2D) and m.bias is not None:\n                zeros_(m.bias)\n        elif isinstance(m, (nn.BatchNorm, nn.BatchNorm2D, nn.BatchNorm1D)):\n            zeros_(m.bias)\n            ones_(m.weight)\n\n    def forward(self, semantic):\n        semantic_source = semantic  # batch, channel, height, width\n\n        # feature transformation\n        semantic = semantic.squeeze(2).transpose([0, 2, 1])  # batch, width, channel\n        channel_att = self.channel_inter(semantic)  # batch, width, channel\n        channel_att = channel_att.transpose([0, 2, 1])  # batch, channel, width\n        channel_bn = self.channel_bn(channel_att)  # batch, channel, width\n        channel_att = self.channel_act(channel_bn)  # batch, channel, width\n\n        # Feature enhancement\n        channel_output = semantic_source * channel_att.unsqueeze(\n            -2\n        )  # batch, channel, 1, width\n\n        return channel_output\n\n\nclass V2SAdaptor(nn.Layer):\n    \"\"\"Visual to Semantic adaptation module\"\"\"\n\n    def __init__(self, in_channels=512, return_mask=False):\n        super(V2SAdaptor, self).__init__()\n\n        # parameter initialization\n        self.in_channels = in_channels\n        self.return_mask = return_mask\n\n        # output transformation\n        self.channel_inter = nn.Linear(\n            self.in_channels, self.in_channels, bias_attr=False\n        )\n        self.channel_bn = nn.BatchNorm1D(self.in_channels)\n        self.channel_act = nn.ReLU()\n\n    def forward(self, visual):\n        # Feature enhancement\n        visual = visual.squeeze(2).transpose([0, 2, 1])  # batch, width, channel\n        channel_att = self.channel_inter(visual)  # batch, width, channel\n        channel_att = channel_att.transpose([0, 2, 1])  # batch, channel, width\n        channel_bn = self.channel_bn(channel_att)  # batch, channel, width\n        channel_att = self.channel_act(channel_bn)  # batch, channel, width\n\n        # size alignment\n        channel_output = channel_att.unsqueeze(-2)  # batch, width, channel\n\n        if self.return_mask:\n            return channel_output, channel_att\n        return channel_output\n\n\nclass RFAdaptor(nn.Layer):\n    def __init__(self, in_channels=512, use_v2s=True, use_s2v=True, **kwargs):\n        super(RFAdaptor, self).__init__()\n        if use_v2s is True:\n            self.neck_v2s = V2SAdaptor(in_channels=in_channels, **kwargs)\n        else:\n            self.neck_v2s = None\n        if use_s2v is True:\n            self.neck_s2v = S2VAdaptor(in_channels=in_channels, **kwargs)\n        else:\n            self.neck_s2v = None\n        self.out_channels = in_channels\n\n    def forward(self, x):\n        visual_feature, rcg_feature = x\n        if visual_feature is not None:\n            (\n                batch,\n                source_channels,\n                v_source_height,\n                v_source_width,\n            ) = visual_feature.shape\n            visual_feature = visual_feature.reshape(\n                [batch, source_channels, 1, v_source_height * v_source_width]\n            )\n\n        if self.neck_v2s is not None:\n            v_rcg_feature = rcg_feature * self.neck_v2s(visual_feature)\n        else:\n            v_rcg_feature = rcg_feature\n\n        if self.neck_s2v is not None:\n            v_visual_feature = visual_feature + self.neck_s2v(rcg_feature)\n        else:\n            v_visual_feature = visual_feature\n        if v_rcg_feature is not None:\n            batch, source_channels, source_height, source_width = v_rcg_feature.shape\n            v_rcg_feature = v_rcg_feature.reshape(\n                [batch, source_channels, 1, source_height * source_width]\n            )\n\n            v_rcg_feature = v_rcg_feature.squeeze(2).transpose([0, 2, 1])\n        return v_visual_feature, v_rcg_feature\n", "ppocr/modeling/necks/__init__.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n__all__ = [\"build_neck\"]\n\n\ndef build_neck(config):\n    from .db_fpn import DBFPN, RSEFPN, LKPAN\n    from .east_fpn import EASTFPN\n    from .sast_fpn import SASTFPN\n    from .rnn import SequenceEncoder\n    from .pg_fpn import PGFPN\n    from .table_fpn import TableFPN\n    from .fpn import FPN\n    from .fce_fpn import FCEFPN\n    from .pren_fpn import PRENFPN\n    from .csp_pan import CSPPAN\n    from .ct_fpn import CTFPN\n    from .fpn_unet import FPN_UNet\n    from .rf_adaptor import RFAdaptor\n\n    support_dict = [\n        \"FPN\",\n        \"FCEFPN\",\n        \"LKPAN\",\n        \"DBFPN\",\n        \"RSEFPN\",\n        \"EASTFPN\",\n        \"SASTFPN\",\n        \"SequenceEncoder\",\n        \"PGFPN\",\n        \"TableFPN\",\n        \"PRENFPN\",\n        \"CSPPAN\",\n        \"CTFPN\",\n        \"RFAdaptor\",\n        \"FPN_UNet\",\n    ]\n\n    module_name = config.pop(\"name\")\n    assert module_name in support_dict, Exception(\n        \"neck only support {}\".format(support_dict)\n    )\n\n    module_class = eval(module_name)(**config)\n    return module_class\n", "ppocr/modeling/necks/east_fpn.py": "# copyright (c) 2019 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nfrom paddle import nn\nimport paddle.nn.functional as F\nfrom paddle import ParamAttr\n\n\nclass ConvBNLayer(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride,\n        padding,\n        groups=1,\n        if_act=True,\n        act=None,\n        name=None,\n    ):\n        super(ConvBNLayer, self).__init__()\n        self.if_act = if_act\n        self.act = act\n        self.conv = nn.Conv2D(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            groups=groups,\n            weight_attr=ParamAttr(name=name + \"_weights\"),\n            bias_attr=False,\n        )\n\n        self.bn = nn.BatchNorm(\n            num_channels=out_channels,\n            act=act,\n            param_attr=ParamAttr(name=\"bn_\" + name + \"_scale\"),\n            bias_attr=ParamAttr(name=\"bn_\" + name + \"_offset\"),\n            moving_mean_name=\"bn_\" + name + \"_mean\",\n            moving_variance_name=\"bn_\" + name + \"_variance\",\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return x\n\n\nclass DeConvBNLayer(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride,\n        padding,\n        groups=1,\n        if_act=True,\n        act=None,\n        name=None,\n    ):\n        super(DeConvBNLayer, self).__init__()\n        self.if_act = if_act\n        self.act = act\n        self.deconv = nn.Conv2DTranspose(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            groups=groups,\n            weight_attr=ParamAttr(name=name + \"_weights\"),\n            bias_attr=False,\n        )\n        self.bn = nn.BatchNorm(\n            num_channels=out_channels,\n            act=act,\n            param_attr=ParamAttr(name=\"bn_\" + name + \"_scale\"),\n            bias_attr=ParamAttr(name=\"bn_\" + name + \"_offset\"),\n            moving_mean_name=\"bn_\" + name + \"_mean\",\n            moving_variance_name=\"bn_\" + name + \"_variance\",\n        )\n\n    def forward(self, x):\n        x = self.deconv(x)\n        x = self.bn(x)\n        return x\n\n\nclass EASTFPN(nn.Layer):\n    def __init__(self, in_channels, model_name, **kwargs):\n        super(EASTFPN, self).__init__()\n        self.model_name = model_name\n        if self.model_name == \"large\":\n            self.out_channels = 128\n        else:\n            self.out_channels = 64\n        self.in_channels = in_channels[::-1]\n        self.h1_conv = ConvBNLayer(\n            in_channels=self.out_channels + self.in_channels[1],\n            out_channels=self.out_channels,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            if_act=True,\n            act=\"relu\",\n            name=\"unet_h_1\",\n        )\n        self.h2_conv = ConvBNLayer(\n            in_channels=self.out_channels + self.in_channels[2],\n            out_channels=self.out_channels,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            if_act=True,\n            act=\"relu\",\n            name=\"unet_h_2\",\n        )\n        self.h3_conv = ConvBNLayer(\n            in_channels=self.out_channels + self.in_channels[3],\n            out_channels=self.out_channels,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            if_act=True,\n            act=\"relu\",\n            name=\"unet_h_3\",\n        )\n        self.g0_deconv = DeConvBNLayer(\n            in_channels=self.in_channels[0],\n            out_channels=self.out_channels,\n            kernel_size=4,\n            stride=2,\n            padding=1,\n            if_act=True,\n            act=\"relu\",\n            name=\"unet_g_0\",\n        )\n        self.g1_deconv = DeConvBNLayer(\n            in_channels=self.out_channels,\n            out_channels=self.out_channels,\n            kernel_size=4,\n            stride=2,\n            padding=1,\n            if_act=True,\n            act=\"relu\",\n            name=\"unet_g_1\",\n        )\n        self.g2_deconv = DeConvBNLayer(\n            in_channels=self.out_channels,\n            out_channels=self.out_channels,\n            kernel_size=4,\n            stride=2,\n            padding=1,\n            if_act=True,\n            act=\"relu\",\n            name=\"unet_g_2\",\n        )\n        self.g3_conv = ConvBNLayer(\n            in_channels=self.out_channels,\n            out_channels=self.out_channels,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            if_act=True,\n            act=\"relu\",\n            name=\"unet_g_3\",\n        )\n\n    def forward(self, x):\n        f = x[::-1]\n\n        h = f[0]\n        g = self.g0_deconv(h)\n        h = paddle.concat([g, f[1]], axis=1)\n        h = self.h1_conv(h)\n        g = self.g1_deconv(h)\n        h = paddle.concat([g, f[2]], axis=1)\n        h = self.h2_conv(h)\n        g = self.g2_deconv(h)\n        h = paddle.concat([g, f[3]], axis=1)\n        h = self.h3_conv(h)\n        g = self.g3_conv(h)\n\n        return g\n", "ppocr/modeling/necks/db_fpn.py": "# copyright (c) 2019 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport paddle\nfrom paddle import nn\nimport paddle.nn.functional as F\nfrom paddle import ParamAttr\nimport os\nimport sys\nfrom ppocr.modeling.necks.intracl import IntraCLBlock\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(__dir__)\nsys.path.insert(0, os.path.abspath(os.path.join(__dir__, \"../../..\")))\n\nfrom ppocr.modeling.backbones.det_mobilenet_v3 import SEModule\n\n\nclass DSConv(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        padding,\n        stride=1,\n        groups=None,\n        if_act=True,\n        act=\"relu\",\n        **kwargs,\n    ):\n        super(DSConv, self).__init__()\n        if groups == None:\n            groups = in_channels\n        self.if_act = if_act\n        self.act = act\n        self.conv1 = nn.Conv2D(\n            in_channels=in_channels,\n            out_channels=in_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            groups=groups,\n            bias_attr=False,\n        )\n\n        self.bn1 = nn.BatchNorm(num_channels=in_channels, act=None)\n\n        self.conv2 = nn.Conv2D(\n            in_channels=in_channels,\n            out_channels=int(in_channels * 4),\n            kernel_size=1,\n            stride=1,\n            bias_attr=False,\n        )\n\n        self.bn2 = nn.BatchNorm(num_channels=int(in_channels * 4), act=None)\n\n        self.conv3 = nn.Conv2D(\n            in_channels=int(in_channels * 4),\n            out_channels=out_channels,\n            kernel_size=1,\n            stride=1,\n            bias_attr=False,\n        )\n        self._c = [in_channels, out_channels]\n        if in_channels != out_channels:\n            self.conv_end = nn.Conv2D(\n                in_channels=in_channels,\n                out_channels=out_channels,\n                kernel_size=1,\n                stride=1,\n                bias_attr=False,\n            )\n\n    def forward(self, inputs):\n        x = self.conv1(inputs)\n        x = self.bn1(x)\n\n        x = self.conv2(x)\n        x = self.bn2(x)\n        if self.if_act:\n            if self.act == \"relu\":\n                x = F.relu(x)\n            elif self.act == \"hardswish\":\n                x = F.hardswish(x)\n            else:\n                print(\n                    \"The activation function({}) is selected incorrectly.\".format(\n                        self.act\n                    )\n                )\n                exit()\n\n        x = self.conv3(x)\n        if self._c[0] != self._c[1]:\n            x = x + self.conv_end(inputs)\n        return x\n\n\nclass DBFPN(nn.Layer):\n    def __init__(self, in_channels, out_channels, use_asf=False, **kwargs):\n        super(DBFPN, self).__init__()\n        self.out_channels = out_channels\n        self.use_asf = use_asf\n        weight_attr = paddle.nn.initializer.KaimingUniform()\n\n        self.in2_conv = nn.Conv2D(\n            in_channels=in_channels[0],\n            out_channels=self.out_channels,\n            kernel_size=1,\n            weight_attr=ParamAttr(initializer=weight_attr),\n            bias_attr=False,\n        )\n        self.in3_conv = nn.Conv2D(\n            in_channels=in_channels[1],\n            out_channels=self.out_channels,\n            kernel_size=1,\n            weight_attr=ParamAttr(initializer=weight_attr),\n            bias_attr=False,\n        )\n        self.in4_conv = nn.Conv2D(\n            in_channels=in_channels[2],\n            out_channels=self.out_channels,\n            kernel_size=1,\n            weight_attr=ParamAttr(initializer=weight_attr),\n            bias_attr=False,\n        )\n        self.in5_conv = nn.Conv2D(\n            in_channels=in_channels[3],\n            out_channels=self.out_channels,\n            kernel_size=1,\n            weight_attr=ParamAttr(initializer=weight_attr),\n            bias_attr=False,\n        )\n        self.p5_conv = nn.Conv2D(\n            in_channels=self.out_channels,\n            out_channels=self.out_channels // 4,\n            kernel_size=3,\n            padding=1,\n            weight_attr=ParamAttr(initializer=weight_attr),\n            bias_attr=False,\n        )\n        self.p4_conv = nn.Conv2D(\n            in_channels=self.out_channels,\n            out_channels=self.out_channels // 4,\n            kernel_size=3,\n            padding=1,\n            weight_attr=ParamAttr(initializer=weight_attr),\n            bias_attr=False,\n        )\n        self.p3_conv = nn.Conv2D(\n            in_channels=self.out_channels,\n            out_channels=self.out_channels // 4,\n            kernel_size=3,\n            padding=1,\n            weight_attr=ParamAttr(initializer=weight_attr),\n            bias_attr=False,\n        )\n        self.p2_conv = nn.Conv2D(\n            in_channels=self.out_channels,\n            out_channels=self.out_channels // 4,\n            kernel_size=3,\n            padding=1,\n            weight_attr=ParamAttr(initializer=weight_attr),\n            bias_attr=False,\n        )\n\n        if self.use_asf is True:\n            self.asf = ASFBlock(self.out_channels, self.out_channels // 4)\n\n    def forward(self, x):\n        c2, c3, c4, c5 = x\n\n        in5 = self.in5_conv(c5)\n        in4 = self.in4_conv(c4)\n        in3 = self.in3_conv(c3)\n        in2 = self.in2_conv(c2)\n\n        out4 = in4 + F.upsample(\n            in5, scale_factor=2, mode=\"nearest\", align_mode=1\n        )  # 1/16\n        out3 = in3 + F.upsample(\n            out4, scale_factor=2, mode=\"nearest\", align_mode=1\n        )  # 1/8\n        out2 = in2 + F.upsample(\n            out3, scale_factor=2, mode=\"nearest\", align_mode=1\n        )  # 1/4\n\n        p5 = self.p5_conv(in5)\n        p4 = self.p4_conv(out4)\n        p3 = self.p3_conv(out3)\n        p2 = self.p2_conv(out2)\n        p5 = F.upsample(p5, scale_factor=8, mode=\"nearest\", align_mode=1)\n        p4 = F.upsample(p4, scale_factor=4, mode=\"nearest\", align_mode=1)\n        p3 = F.upsample(p3, scale_factor=2, mode=\"nearest\", align_mode=1)\n\n        fuse = paddle.concat([p5, p4, p3, p2], axis=1)\n\n        if self.use_asf is True:\n            fuse = self.asf(fuse, [p5, p4, p3, p2])\n\n        return fuse\n\n\nclass RSELayer(nn.Layer):\n    def __init__(self, in_channels, out_channels, kernel_size, shortcut=True):\n        super(RSELayer, self).__init__()\n        weight_attr = paddle.nn.initializer.KaimingUniform()\n        self.out_channels = out_channels\n        self.in_conv = nn.Conv2D(\n            in_channels=in_channels,\n            out_channels=self.out_channels,\n            kernel_size=kernel_size,\n            padding=int(kernel_size // 2),\n            weight_attr=ParamAttr(initializer=weight_attr),\n            bias_attr=False,\n        )\n        self.se_block = SEModule(self.out_channels)\n        self.shortcut = shortcut\n\n    def forward(self, ins):\n        x = self.in_conv(ins)\n        if self.shortcut:\n            out = x + self.se_block(x)\n        else:\n            out = self.se_block(x)\n        return out\n\n\nclass RSEFPN(nn.Layer):\n    def __init__(self, in_channels, out_channels, shortcut=True, **kwargs):\n        super(RSEFPN, self).__init__()\n        self.out_channels = out_channels\n        self.ins_conv = nn.LayerList()\n        self.inp_conv = nn.LayerList()\n        self.intracl = False\n        if \"intracl\" in kwargs.keys() and kwargs[\"intracl\"] is True:\n            self.intracl = kwargs[\"intracl\"]\n            self.incl1 = IntraCLBlock(self.out_channels // 4, reduce_factor=2)\n            self.incl2 = IntraCLBlock(self.out_channels // 4, reduce_factor=2)\n            self.incl3 = IntraCLBlock(self.out_channels // 4, reduce_factor=2)\n            self.incl4 = IntraCLBlock(self.out_channels // 4, reduce_factor=2)\n\n        for i in range(len(in_channels)):\n            self.ins_conv.append(\n                RSELayer(in_channels[i], out_channels, kernel_size=1, shortcut=shortcut)\n            )\n            self.inp_conv.append(\n                RSELayer(\n                    out_channels, out_channels // 4, kernel_size=3, shortcut=shortcut\n                )\n            )\n\n    def forward(self, x):\n        c2, c3, c4, c5 = x\n\n        in5 = self.ins_conv[3](c5)\n        in4 = self.ins_conv[2](c4)\n        in3 = self.ins_conv[1](c3)\n        in2 = self.ins_conv[0](c2)\n\n        out4 = in4 + F.upsample(\n            in5, scale_factor=2, mode=\"nearest\", align_mode=1\n        )  # 1/16\n        out3 = in3 + F.upsample(\n            out4, scale_factor=2, mode=\"nearest\", align_mode=1\n        )  # 1/8\n        out2 = in2 + F.upsample(\n            out3, scale_factor=2, mode=\"nearest\", align_mode=1\n        )  # 1/4\n\n        p5 = self.inp_conv[3](in5)\n        p4 = self.inp_conv[2](out4)\n        p3 = self.inp_conv[1](out3)\n        p2 = self.inp_conv[0](out2)\n\n        if self.intracl is True:\n            p5 = self.incl4(p5)\n            p4 = self.incl3(p4)\n            p3 = self.incl2(p3)\n            p2 = self.incl1(p2)\n\n        p5 = F.upsample(p5, scale_factor=8, mode=\"nearest\", align_mode=1)\n        p4 = F.upsample(p4, scale_factor=4, mode=\"nearest\", align_mode=1)\n        p3 = F.upsample(p3, scale_factor=2, mode=\"nearest\", align_mode=1)\n\n        fuse = paddle.concat([p5, p4, p3, p2], axis=1)\n        return fuse\n\n\nclass LKPAN(nn.Layer):\n    def __init__(self, in_channels, out_channels, mode=\"large\", **kwargs):\n        super(LKPAN, self).__init__()\n        self.out_channels = out_channels\n        weight_attr = paddle.nn.initializer.KaimingUniform()\n\n        self.ins_conv = nn.LayerList()\n        self.inp_conv = nn.LayerList()\n        # pan head\n        self.pan_head_conv = nn.LayerList()\n        self.pan_lat_conv = nn.LayerList()\n\n        if mode.lower() == \"lite\":\n            p_layer = DSConv\n        elif mode.lower() == \"large\":\n            p_layer = nn.Conv2D\n        else:\n            raise ValueError(\n                \"mode can only be one of ['lite', 'large'], but received {}\".format(\n                    mode\n                )\n            )\n\n        for i in range(len(in_channels)):\n            self.ins_conv.append(\n                nn.Conv2D(\n                    in_channels=in_channels[i],\n                    out_channels=self.out_channels,\n                    kernel_size=1,\n                    weight_attr=ParamAttr(initializer=weight_attr),\n                    bias_attr=False,\n                )\n            )\n\n            self.inp_conv.append(\n                p_layer(\n                    in_channels=self.out_channels,\n                    out_channels=self.out_channels // 4,\n                    kernel_size=9,\n                    padding=4,\n                    weight_attr=ParamAttr(initializer=weight_attr),\n                    bias_attr=False,\n                )\n            )\n\n            if i > 0:\n                self.pan_head_conv.append(\n                    nn.Conv2D(\n                        in_channels=self.out_channels // 4,\n                        out_channels=self.out_channels // 4,\n                        kernel_size=3,\n                        padding=1,\n                        stride=2,\n                        weight_attr=ParamAttr(initializer=weight_attr),\n                        bias_attr=False,\n                    )\n                )\n            self.pan_lat_conv.append(\n                p_layer(\n                    in_channels=self.out_channels // 4,\n                    out_channels=self.out_channels // 4,\n                    kernel_size=9,\n                    padding=4,\n                    weight_attr=ParamAttr(initializer=weight_attr),\n                    bias_attr=False,\n                )\n            )\n\n        self.intracl = False\n        if \"intracl\" in kwargs.keys() and kwargs[\"intracl\"] is True:\n            self.intracl = kwargs[\"intracl\"]\n            self.incl1 = IntraCLBlock(self.out_channels // 4, reduce_factor=2)\n            self.incl2 = IntraCLBlock(self.out_channels // 4, reduce_factor=2)\n            self.incl3 = IntraCLBlock(self.out_channels // 4, reduce_factor=2)\n            self.incl4 = IntraCLBlock(self.out_channels // 4, reduce_factor=2)\n\n    def forward(self, x):\n        c2, c3, c4, c5 = x\n\n        in5 = self.ins_conv[3](c5)\n        in4 = self.ins_conv[2](c4)\n        in3 = self.ins_conv[1](c3)\n        in2 = self.ins_conv[0](c2)\n\n        out4 = in4 + F.upsample(\n            in5, scale_factor=2, mode=\"nearest\", align_mode=1\n        )  # 1/16\n        out3 = in3 + F.upsample(\n            out4, scale_factor=2, mode=\"nearest\", align_mode=1\n        )  # 1/8\n        out2 = in2 + F.upsample(\n            out3, scale_factor=2, mode=\"nearest\", align_mode=1\n        )  # 1/4\n\n        f5 = self.inp_conv[3](in5)\n        f4 = self.inp_conv[2](out4)\n        f3 = self.inp_conv[1](out3)\n        f2 = self.inp_conv[0](out2)\n\n        pan3 = f3 + self.pan_head_conv[0](f2)\n        pan4 = f4 + self.pan_head_conv[1](pan3)\n        pan5 = f5 + self.pan_head_conv[2](pan4)\n\n        p2 = self.pan_lat_conv[0](f2)\n        p3 = self.pan_lat_conv[1](pan3)\n        p4 = self.pan_lat_conv[2](pan4)\n        p5 = self.pan_lat_conv[3](pan5)\n\n        if self.intracl is True:\n            p5 = self.incl4(p5)\n            p4 = self.incl3(p4)\n            p3 = self.incl2(p3)\n            p2 = self.incl1(p2)\n\n        p5 = F.upsample(p5, scale_factor=8, mode=\"nearest\", align_mode=1)\n        p4 = F.upsample(p4, scale_factor=4, mode=\"nearest\", align_mode=1)\n        p3 = F.upsample(p3, scale_factor=2, mode=\"nearest\", align_mode=1)\n\n        fuse = paddle.concat([p5, p4, p3, p2], axis=1)\n        return fuse\n\n\nclass ASFBlock(nn.Layer):\n    \"\"\"\n    This code is refered from:\n        https://github.com/MhLiao/DB/blob/master/decoders/feature_attention.py\n    \"\"\"\n\n    def __init__(self, in_channels, inter_channels, out_features_num=4):\n        \"\"\"\n        Adaptive Scale Fusion (ASF) block of DBNet++\n        Args:\n            in_channels: the number of channels in the input data\n            inter_channels: the number of middle channels\n            out_features_num: the number of fused stages\n        \"\"\"\n        super(ASFBlock, self).__init__()\n        weight_attr = paddle.nn.initializer.KaimingUniform()\n        self.in_channels = in_channels\n        self.inter_channels = inter_channels\n        self.out_features_num = out_features_num\n        self.conv = nn.Conv2D(in_channels, inter_channels, 3, padding=1)\n\n        self.spatial_scale = nn.Sequential(\n            # Nx1xHxW\n            nn.Conv2D(\n                in_channels=1,\n                out_channels=1,\n                kernel_size=3,\n                bias_attr=False,\n                padding=1,\n                weight_attr=ParamAttr(initializer=weight_attr),\n            ),\n            nn.ReLU(),\n            nn.Conv2D(\n                in_channels=1,\n                out_channels=1,\n                kernel_size=1,\n                bias_attr=False,\n                weight_attr=ParamAttr(initializer=weight_attr),\n            ),\n            nn.Sigmoid(),\n        )\n\n        self.channel_scale = nn.Sequential(\n            nn.Conv2D(\n                in_channels=inter_channels,\n                out_channels=out_features_num,\n                kernel_size=1,\n                bias_attr=False,\n                weight_attr=ParamAttr(initializer=weight_attr),\n            ),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, fuse_features, features_list):\n        fuse_features = self.conv(fuse_features)\n        spatial_x = paddle.mean(fuse_features, axis=1, keepdim=True)\n        attention_scores = self.spatial_scale(spatial_x) + fuse_features\n        attention_scores = self.channel_scale(attention_scores)\n        assert len(features_list) == self.out_features_num\n\n        out_list = []\n        for i in range(self.out_features_num):\n            out_list.append(attention_scores[:, i : i + 1] * features_list[i])\n        return paddle.concat(out_list, axis=1)\n", "ppocr/modeling/architectures/distillation_model.py": "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom paddle import nn\nfrom ppocr.modeling.transforms import build_transform\nfrom ppocr.modeling.backbones import build_backbone\nfrom ppocr.modeling.necks import build_neck\nfrom ppocr.modeling.heads import build_head\nfrom .base_model import BaseModel\nfrom ppocr.utils.save_load import load_pretrained_params\n\n__all__ = [\"DistillationModel\"]\n\n\nclass DistillationModel(nn.Layer):\n    def __init__(self, config):\n        \"\"\"\n        the module for OCR distillation.\n        args:\n            config (dict): the super parameters for module.\n        \"\"\"\n        super().__init__()\n        self.model_list = []\n        self.model_name_list = []\n        for key in config[\"Models\"]:\n            model_config = config[\"Models\"][key]\n            freeze_params = False\n            pretrained = None\n            if \"freeze_params\" in model_config:\n                freeze_params = model_config.pop(\"freeze_params\")\n            if \"pretrained\" in model_config:\n                pretrained = model_config.pop(\"pretrained\")\n            model = BaseModel(model_config)\n            if pretrained is not None:\n                load_pretrained_params(model, pretrained)\n            if freeze_params:\n                for param in model.parameters():\n                    param.trainable = False\n            self.model_list.append(self.add_sublayer(key, model))\n            self.model_name_list.append(key)\n\n    def forward(self, x, data=None):\n        result_dict = dict()\n        for idx, model_name in enumerate(self.model_name_list):\n            result_dict[model_name] = self.model_list[idx](x, data)\n        return result_dict\n", "ppocr/modeling/architectures/base_model.py": "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom paddle import nn\nfrom ppocr.modeling.transforms import build_transform\nfrom ppocr.modeling.backbones import build_backbone\nfrom ppocr.modeling.necks import build_neck\nfrom ppocr.modeling.heads import build_head\n\n__all__ = [\"BaseModel\"]\n\n\nclass BaseModel(nn.Layer):\n    def __init__(self, config):\n        \"\"\"\n        the module for OCR.\n        args:\n            config (dict): the super parameters for module.\n        \"\"\"\n        super(BaseModel, self).__init__()\n        in_channels = config.get(\"in_channels\", 3)\n        model_type = config[\"model_type\"]\n        # build transfrom,\n        # for rec, transfrom can be TPS,None\n        # for det and cls, transfrom shoule to be None,\n        # if you make model differently, you can use transfrom in det and cls\n        if \"Transform\" not in config or config[\"Transform\"] is None:\n            self.use_transform = False\n        else:\n            self.use_transform = True\n            config[\"Transform\"][\"in_channels\"] = in_channels\n            self.transform = build_transform(config[\"Transform\"])\n            in_channels = self.transform.out_channels\n\n        # build backbone, backbone is need for del, rec and cls\n        if \"Backbone\" not in config or config[\"Backbone\"] is None:\n            self.use_backbone = False\n        else:\n            self.use_backbone = True\n            config[\"Backbone\"][\"in_channels\"] = in_channels\n            self.backbone = build_backbone(config[\"Backbone\"], model_type)\n            in_channels = self.backbone.out_channels\n\n        # build neck\n        # for rec, neck can be cnn,rnn or reshape(None)\n        # for det, neck can be FPN, BIFPN and so on.\n        # for cls, neck should be none\n        if \"Neck\" not in config or config[\"Neck\"] is None:\n            self.use_neck = False\n        else:\n            self.use_neck = True\n            config[\"Neck\"][\"in_channels\"] = in_channels\n            self.neck = build_neck(config[\"Neck\"])\n            in_channels = self.neck.out_channels\n\n        # # build head, head is need for det, rec and cls\n        if \"Head\" not in config or config[\"Head\"] is None:\n            self.use_head = False\n        else:\n            self.use_head = True\n            config[\"Head\"][\"in_channels\"] = in_channels\n            self.head = build_head(config[\"Head\"])\n\n        self.return_all_feats = config.get(\"return_all_feats\", False)\n\n    def forward(self, x, data=None):\n        y = dict()\n        if self.use_transform:\n            x = self.transform(x)\n        if self.use_backbone:\n            x = self.backbone(x)\n        if isinstance(x, dict):\n            y.update(x)\n        else:\n            y[\"backbone_out\"] = x\n        final_name = \"backbone_out\"\n        if self.use_neck:\n            x = self.neck(x)\n            if isinstance(x, dict):\n                y.update(x)\n            else:\n                y[\"neck_out\"] = x\n            final_name = \"neck_out\"\n        if self.use_head:\n            x = self.head(x, targets=data)\n            # for multi head, save ctc neck out for udml\n            if isinstance(x, dict) and \"ctc_neck\" in x.keys():\n                y[\"neck_out\"] = x[\"ctc_neck\"]\n                y[\"head_out\"] = x\n            elif isinstance(x, dict):\n                y.update(x)\n            else:\n                y[\"head_out\"] = x\n            final_name = \"head_out\"\n        if self.return_all_feats:\n            if self.training:\n                return y\n            elif isinstance(x, dict):\n                return x\n            else:\n                return {final_name: x}\n        else:\n            return x\n", "ppocr/modeling/architectures/__init__.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport copy\nimport importlib\n\nfrom paddle.jit import to_static\nfrom paddle.static import InputSpec\n\nfrom .base_model import BaseModel\nfrom .distillation_model import DistillationModel\n\n__all__ = [\"build_model\", \"apply_to_static\"]\n\n\ndef build_model(config):\n    config = copy.deepcopy(config)\n    if not \"name\" in config:\n        arch = BaseModel(config)\n    else:\n        name = config.pop(\"name\")\n        mod = importlib.import_module(__name__)\n        arch = getattr(mod, name)(config)\n    return arch\n\n\ndef apply_to_static(model, config, logger):\n    if config[\"Global\"].get(\"to_static\", False) is not True:\n        return model\n    assert (\n        \"d2s_train_image_shape\" in config[\"Global\"]\n    ), \"d2s_train_image_shape must be assigned for static training mode...\"\n    supported_list = [\"DB\", \"SVTR_LCNet\", \"TableMaster\", \"LayoutXLM\", \"SLANet\", \"SVTR\"]\n    if config[\"Architecture\"][\"algorithm\"] in [\"Distillation\"]:\n        algo = list(config[\"Architecture\"][\"Models\"].values())[0][\"algorithm\"]\n    else:\n        algo = config[\"Architecture\"][\"algorithm\"]\n    assert (\n        algo in supported_list\n    ), f\"algorithms that supports static training must in in {supported_list} but got {algo}\"\n\n    specs = [\n        InputSpec([None] + config[\"Global\"][\"d2s_train_image_shape\"], dtype=\"float32\")\n    ]\n\n    if algo == \"SVTR_LCNet\":\n        specs.append(\n            [\n                InputSpec([None, config[\"Global\"][\"max_text_length\"]], dtype=\"int64\"),\n                InputSpec([None, config[\"Global\"][\"max_text_length\"]], dtype=\"int64\"),\n                InputSpec([None], dtype=\"int64\"),\n                InputSpec([None], dtype=\"float64\"),\n            ]\n        )\n    elif algo == \"TableMaster\":\n        specs.append(\n            [\n                InputSpec([None, config[\"Global\"][\"max_text_length\"]], dtype=\"int64\"),\n                InputSpec(\n                    [None, config[\"Global\"][\"max_text_length\"], 4], dtype=\"float32\"\n                ),\n                InputSpec(\n                    [None, config[\"Global\"][\"max_text_length\"], 1], dtype=\"float32\"\n                ),\n                InputSpec([None, 6], dtype=\"float32\"),\n            ]\n        )\n    elif algo == \"LayoutXLM\":\n        specs = [\n            [\n                InputSpec(shape=[None, 512], dtype=\"int64\"),  # input_ids\n                InputSpec(shape=[None, 512, 4], dtype=\"int64\"),  # bbox\n                InputSpec(shape=[None, 512], dtype=\"int64\"),  # attention_mask\n                InputSpec(shape=[None, 512], dtype=\"int64\"),  # token_type_ids\n                InputSpec(shape=[None, 3, 224, 224], dtype=\"float32\"),  # image\n                InputSpec(shape=[None, 512], dtype=\"int64\"),  # label\n            ]\n        ]\n    elif algo == \"SLANet\":\n        specs.append(\n            [\n                InputSpec(\n                    [None, config[\"Global\"][\"max_text_length\"] + 2], dtype=\"int64\"\n                ),\n                InputSpec(\n                    [None, config[\"Global\"][\"max_text_length\"] + 2, 4], dtype=\"float32\"\n                ),\n                InputSpec(\n                    [None, config[\"Global\"][\"max_text_length\"] + 2, 1], dtype=\"float32\"\n                ),\n                InputSpec([None], dtype=\"int64\"),\n                InputSpec([None, 6], dtype=\"float64\"),\n            ]\n        )\n    elif algo == \"SVTR\":\n        specs.append(\n            [\n                InputSpec([None, config[\"Global\"][\"max_text_length\"]], dtype=\"int64\"),\n                InputSpec([None], dtype=\"int64\"),\n            ]\n        )\n    model = to_static(model, input_spec=specs)\n    logger.info(\"Successfully to apply @to_static with specs: {}\".format(specs))\n    return model\n", "ppocr/ext_op/__init__.py": "from .roi_align_rotated.roi_align_rotated import RoIAlignRotated\n", "ppocr/ext_op/roi_align_rotated/roi_align_rotated.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/open-mmlab/mmcv/blob/master/mmcv/ops/roi_align_rotated.py\n\"\"\"\n\nimport paddle\nimport paddle.nn as nn\nfrom paddle.utils.cpp_extension import load\n\ncustom_ops = load(\n    name=\"custom_jit_ops\",\n    sources=[\n        \"ppocr/ext_op/roi_align_rotated/roi_align_rotated.cc\",\n        \"ppocr/ext_op/roi_align_rotated/roi_align_rotated.cu\",\n    ],\n)\n\nroi_align_rotated = custom_ops.roi_align_rotated\n\n\nclass RoIAlignRotated(nn.Layer):\n    \"\"\"RoI align pooling layer for rotated proposals.\"\"\"\n\n    def __init__(\n        self, out_size, spatial_scale, sample_num=0, aligned=True, clockwise=False\n    ):\n        super(RoIAlignRotated, self).__init__()\n\n        if isinstance(out_size, int):\n            self.out_h = out_size\n            self.out_w = out_size\n        elif isinstance(out_size, tuple):\n            assert len(out_size) == 2\n            assert isinstance(out_size[0], int)\n            assert isinstance(out_size[1], int)\n            self.out_h, self.out_w = out_size\n        else:\n            raise TypeError('\"out_size\" must be an integer or tuple of integers')\n\n        self.spatial_scale = float(spatial_scale)\n        self.sample_num = int(sample_num)\n        self.aligned = aligned\n        self.clockwise = clockwise\n\n    def forward(self, feats, rois):\n        output = roi_align_rotated(\n            feats,\n            rois,\n            self.out_h,\n            self.out_w,\n            self.spatial_scale,\n            self.sample_num,\n            self.aligned,\n            self.clockwise,\n        )\n        return output\n", "ppocr/metrics/vqa_token_ser_metric.py": "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport paddle\n\n__all__ = [\"VQASerTokenMetric\"]\n\n\nclass VQASerTokenMetric(object):\n    def __init__(self, main_indicator=\"hmean\", **kwargs):\n        self.main_indicator = main_indicator\n        self.reset()\n\n    def __call__(self, preds, batch, **kwargs):\n        preds, labels = preds\n        self.pred_list.extend(preds)\n        self.gt_list.extend(labels)\n\n    def get_metric(self):\n        from seqeval.metrics import f1_score, precision_score, recall_score\n\n        metrics = {\n            \"precision\": precision_score(self.gt_list, self.pred_list),\n            \"recall\": recall_score(self.gt_list, self.pred_list),\n            \"hmean\": f1_score(self.gt_list, self.pred_list),\n        }\n        self.reset()\n        return metrics\n\n    def reset(self):\n        self.pred_list = []\n        self.gt_list = []\n", "ppocr/metrics/vqa_token_re_metric.py": "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport paddle\n\n__all__ = [\"VQAReTokenMetric\"]\n\n\nclass VQAReTokenMetric(object):\n    def __init__(self, main_indicator=\"hmean\", **kwargs):\n        self.main_indicator = main_indicator\n        self.reset()\n\n    def __call__(self, preds, batch, **kwargs):\n        pred_relations, relations, entities = preds\n        self.pred_relations_list.extend(pred_relations)\n        self.relations_list.extend(relations)\n        self.entities_list.extend(entities)\n\n    def get_metric(self):\n        gt_relations = []\n        for b in range(len(self.relations_list)):\n            rel_sent = []\n            relation_list = self.relations_list[b]\n            entitie_list = self.entities_list[b]\n            head_len = relation_list[0, 0]\n            if head_len > 0:\n                entitie_start_list = entitie_list[1 : entitie_list[0, 0] + 1, 0]\n                entitie_end_list = entitie_list[1 : entitie_list[0, 1] + 1, 1]\n                entitie_label_list = entitie_list[1 : entitie_list[0, 2] + 1, 2]\n                for head, tail in zip(\n                    relation_list[1 : head_len + 1, 0],\n                    relation_list[1 : head_len + 1, 1],\n                ):\n                    rel = {}\n                    rel[\"head_id\"] = head\n                    rel[\"head\"] = (entitie_start_list[head], entitie_end_list[head])\n                    rel[\"head_type\"] = entitie_label_list[head]\n\n                    rel[\"tail_id\"] = tail\n                    rel[\"tail\"] = (entitie_start_list[tail], entitie_end_list[tail])\n                    rel[\"tail_type\"] = entitie_label_list[tail]\n\n                    rel[\"type\"] = 1\n                    rel_sent.append(rel)\n            gt_relations.append(rel_sent)\n        re_metrics = self.re_score(\n            self.pred_relations_list, gt_relations, mode=\"boundaries\"\n        )\n        metrics = {\n            \"precision\": re_metrics[\"ALL\"][\"p\"],\n            \"recall\": re_metrics[\"ALL\"][\"r\"],\n            \"hmean\": re_metrics[\"ALL\"][\"f1\"],\n        }\n        self.reset()\n        return metrics\n\n    def reset(self):\n        self.pred_relations_list = []\n        self.relations_list = []\n        self.entities_list = []\n\n    def re_score(self, pred_relations, gt_relations, mode=\"strict\"):\n        \"\"\"Evaluate RE predictions\n\n        Args:\n            pred_relations (list) :  list of list of predicted relations (several relations in each sentence)\n            gt_relations (list) :    list of list of ground truth relations\n\n                rel = { \"head\": (start_idx (inclusive), end_idx (exclusive)),\n                        \"tail\": (start_idx (inclusive), end_idx (exclusive)),\n                        \"head_type\": ent_type,\n                        \"tail_type\": ent_type,\n                        \"type\": rel_type}\n\n            vocab (Vocab) :         dataset vocabulary\n            mode (str) :            in 'strict' or 'boundaries'\"\"\"\n\n        assert mode in [\"strict\", \"boundaries\"]\n\n        relation_types = [v for v in [0, 1] if not v == 0]\n        scores = {rel: {\"tp\": 0, \"fp\": 0, \"fn\": 0} for rel in relation_types + [\"ALL\"]}\n\n        # Count GT relations and Predicted relations\n        n_sents = len(gt_relations)\n        n_rels = sum([len([rel for rel in sent]) for sent in gt_relations])\n        n_found = sum([len([rel for rel in sent]) for sent in pred_relations])\n\n        # Count TP, FP and FN per type\n        for pred_sent, gt_sent in zip(pred_relations, gt_relations):\n            for rel_type in relation_types:\n                # strict mode takes argument types into account\n                if mode == \"strict\":\n                    pred_rels = {\n                        (rel[\"head\"], rel[\"head_type\"], rel[\"tail\"], rel[\"tail_type\"])\n                        for rel in pred_sent\n                        if rel[\"type\"] == rel_type\n                    }\n                    gt_rels = {\n                        (rel[\"head\"], rel[\"head_type\"], rel[\"tail\"], rel[\"tail_type\"])\n                        for rel in gt_sent\n                        if rel[\"type\"] == rel_type\n                    }\n\n                # boundaries mode only takes argument spans into account\n                elif mode == \"boundaries\":\n                    pred_rels = {\n                        (rel[\"head\"], rel[\"tail\"])\n                        for rel in pred_sent\n                        if rel[\"type\"] == rel_type\n                    }\n                    gt_rels = {\n                        (rel[\"head\"], rel[\"tail\"])\n                        for rel in gt_sent\n                        if rel[\"type\"] == rel_type\n                    }\n\n                scores[rel_type][\"tp\"] += len(pred_rels & gt_rels)\n                scores[rel_type][\"fp\"] += len(pred_rels - gt_rels)\n                scores[rel_type][\"fn\"] += len(gt_rels - pred_rels)\n\n        # Compute per entity Precision / Recall / F1\n        for rel_type in scores.keys():\n            if scores[rel_type][\"tp\"]:\n                scores[rel_type][\"p\"] = scores[rel_type][\"tp\"] / (\n                    scores[rel_type][\"fp\"] + scores[rel_type][\"tp\"]\n                )\n                scores[rel_type][\"r\"] = scores[rel_type][\"tp\"] / (\n                    scores[rel_type][\"fn\"] + scores[rel_type][\"tp\"]\n                )\n            else:\n                scores[rel_type][\"p\"], scores[rel_type][\"r\"] = 0, 0\n\n            if not scores[rel_type][\"p\"] + scores[rel_type][\"r\"] == 0:\n                scores[rel_type][\"f1\"] = (\n                    2\n                    * scores[rel_type][\"p\"]\n                    * scores[rel_type][\"r\"]\n                    / (scores[rel_type][\"p\"] + scores[rel_type][\"r\"])\n                )\n            else:\n                scores[rel_type][\"f1\"] = 0\n\n        # Compute micro F1 Scores\n        tp = sum([scores[rel_type][\"tp\"] for rel_type in relation_types])\n        fp = sum([scores[rel_type][\"fp\"] for rel_type in relation_types])\n        fn = sum([scores[rel_type][\"fn\"] for rel_type in relation_types])\n\n        if tp:\n            precision = tp / (tp + fp)\n            recall = tp / (tp + fn)\n            f1 = 2 * precision * recall / (precision + recall)\n\n        else:\n            precision, recall, f1 = 0, 0, 0\n\n        scores[\"ALL\"][\"p\"] = precision\n        scores[\"ALL\"][\"r\"] = recall\n        scores[\"ALL\"][\"f1\"] = f1\n        scores[\"ALL\"][\"tp\"] = tp\n        scores[\"ALL\"][\"fp\"] = fp\n        scores[\"ALL\"][\"fn\"] = fn\n\n        # Compute Macro F1 Scores\n        scores[\"ALL\"][\"Macro_f1\"] = np.mean(\n            [scores[ent_type][\"f1\"] for ent_type in relation_types]\n        )\n        scores[\"ALL\"][\"Macro_p\"] = np.mean(\n            [scores[ent_type][\"p\"] for ent_type in relation_types]\n        )\n        scores[\"ALL\"][\"Macro_r\"] = np.mean(\n            [scores[ent_type][\"r\"] for ent_type in relation_types]\n        )\n\n        return scores\n", "ppocr/metrics/sr_metric.py": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nhttps://github.com/FudanVI/FudanOCR/blob/main/text-gestalt/utils/ssim_psnr.py\n\"\"\"\n\nfrom math import exp\n\nimport paddle\nimport paddle.nn.functional as F\nimport paddle.nn as nn\nimport string\n\n\nclass SSIM(nn.Layer):\n    def __init__(self, window_size=11, size_average=True):\n        super(SSIM, self).__init__()\n        self.window_size = window_size\n        self.size_average = size_average\n        self.channel = 1\n        self.window = self.create_window(window_size, self.channel)\n\n    def gaussian(self, window_size, sigma):\n        gauss = paddle.to_tensor(\n            [\n                exp(-((x - window_size // 2) ** 2) / float(2 * sigma**2))\n                for x in range(window_size)\n            ]\n        )\n        return gauss / gauss.sum()\n\n    def create_window(self, window_size, channel):\n        _1D_window = self.gaussian(window_size, 1.5).unsqueeze(1)\n        _2D_window = _1D_window.mm(_1D_window.t()).unsqueeze(0).unsqueeze(0)\n        window = _2D_window.expand([channel, 1, window_size, window_size])\n        return window\n\n    def _ssim(self, img1, img2, window, window_size, channel, size_average=True):\n        mu1 = F.conv2d(img1, window, padding=window_size // 2, groups=channel)\n        mu2 = F.conv2d(img2, window, padding=window_size // 2, groups=channel)\n\n        mu1_sq = mu1.pow(2)\n        mu2_sq = mu2.pow(2)\n        mu1_mu2 = mu1 * mu2\n\n        sigma1_sq = (\n            F.conv2d(img1 * img1, window, padding=window_size // 2, groups=channel)\n            - mu1_sq\n        )\n        sigma2_sq = (\n            F.conv2d(img2 * img2, window, padding=window_size // 2, groups=channel)\n            - mu2_sq\n        )\n        sigma12 = (\n            F.conv2d(img1 * img2, window, padding=window_size // 2, groups=channel)\n            - mu1_mu2\n        )\n\n        C1 = 0.01**2\n        C2 = 0.03**2\n\n        ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / (\n            (mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2)\n        )\n\n        if size_average:\n            return ssim_map.mean()\n        else:\n            return ssim_map.mean([1, 2, 3])\n\n    def ssim(self, img1, img2, window_size=11, size_average=True):\n        (_, channel, _, _) = img1.shape\n        window = self.create_window(window_size, channel)\n\n        return self._ssim(img1, img2, window, window_size, channel, size_average)\n\n    def forward(self, img1, img2):\n        (_, channel, _, _) = img1.shape\n\n        if channel == self.channel and self.window.dtype == img1.dtype:\n            window = self.window\n        else:\n            window = self.create_window(self.window_size, channel)\n\n            self.window = window\n            self.channel = channel\n\n        return self._ssim(\n            img1, img2, window, self.window_size, channel, self.size_average\n        )\n\n\nclass SRMetric(object):\n    def __init__(self, main_indicator=\"all\", **kwargs):\n        self.main_indicator = main_indicator\n        self.eps = 1e-5\n        self.psnr_result = []\n        self.ssim_result = []\n        self.calculate_ssim = SSIM()\n        self.reset()\n\n    def reset(self):\n        self.correct_num = 0\n        self.all_num = 0\n        self.norm_edit_dis = 0\n        self.psnr_result = []\n        self.ssim_result = []\n\n    def calculate_psnr(self, img1, img2):\n        # img1 and img2 have range [0, 1]\n        mse = ((img1 * 255 - img2 * 255) ** 2).mean()\n        if mse == 0:\n            return float(\"inf\")\n        return 20 * paddle.log10(255.0 / paddle.sqrt(mse))\n\n    def _normalize_text(self, text):\n        text = \"\".join(\n            filter(lambda x: x in (string.digits + string.ascii_letters), text)\n        )\n        return text.lower()\n\n    def __call__(self, pred_label, *args, **kwargs):\n        metric = {}\n        images_sr = pred_label[\"sr_img\"]\n        images_hr = pred_label[\"hr_img\"]\n        psnr = self.calculate_psnr(images_sr, images_hr)\n        ssim = self.calculate_ssim(images_sr, images_hr)\n        self.psnr_result.append(psnr)\n        self.ssim_result.append(ssim)\n\n    def get_metric(self):\n        \"\"\"\n        return metrics {\n                 'acc': 0,\n                 'norm_edit_dis': 0,\n            }\n        \"\"\"\n        self.psnr_avg = sum(self.psnr_result) / len(self.psnr_result)\n        self.psnr_avg = round(self.psnr_avg.item(), 6)\n        self.ssim_avg = sum(self.ssim_result) / len(self.ssim_result)\n        self.ssim_avg = round(self.ssim_avg.item(), 6)\n\n        self.all_avg = self.psnr_avg + self.ssim_avg\n\n        self.reset()\n        return {\n            \"psnr_avg\": self.psnr_avg,\n            \"ssim_avg\": self.ssim_avg,\n            \"all\": self.all_avg,\n        }\n", "ppocr/metrics/e2e_metric.py": "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n__all__ = [\"E2EMetric\"]\n\nfrom ppocr.utils.e2e_metric.Deteval import get_socre_A, get_socre_B, combine_results\nfrom ppocr.utils.e2e_utils.extract_textpoint_slow import get_dict\n\n\nclass E2EMetric(object):\n    def __init__(\n        self,\n        mode,\n        gt_mat_dir,\n        character_dict_path,\n        main_indicator=\"f_score_e2e\",\n        **kwargs,\n    ):\n        self.mode = mode\n        self.gt_mat_dir = gt_mat_dir\n        self.label_list = get_dict(character_dict_path)\n        self.max_index = len(self.label_list)\n        self.main_indicator = main_indicator\n        self.reset()\n\n    def __call__(self, preds, batch, **kwargs):\n        if self.mode == \"A\":\n            gt_polyons_batch = batch[2]\n            temp_gt_strs_batch = batch[3][0]\n            ignore_tags_batch = batch[4]\n            gt_strs_batch = []\n\n            for temp_list in temp_gt_strs_batch:\n                t = \"\"\n                for index in temp_list:\n                    if index < self.max_index:\n                        t += self.label_list[index]\n                gt_strs_batch.append(t)\n\n            for pred, gt_polyons, gt_strs, ignore_tags in zip(\n                [preds], gt_polyons_batch, [gt_strs_batch], ignore_tags_batch\n            ):\n                # prepare gt\n                gt_info_list = [\n                    {\"points\": gt_polyon, \"text\": gt_str, \"ignore\": ignore_tag}\n                    for gt_polyon, gt_str, ignore_tag in zip(\n                        gt_polyons, gt_strs, ignore_tags\n                    )\n                ]\n                # prepare det\n                e2e_info_list = [\n                    {\"points\": det_polyon, \"texts\": pred_str}\n                    for det_polyon, pred_str in zip(pred[\"points\"], pred[\"texts\"])\n                ]\n\n                result = get_socre_A(gt_info_list, e2e_info_list)\n                self.results.append(result)\n        else:\n            img_id = batch[5][0]\n            e2e_info_list = [\n                {\"points\": det_polyon, \"texts\": pred_str}\n                for det_polyon, pred_str in zip(preds[\"points\"], preds[\"texts\"])\n            ]\n            result = get_socre_B(self.gt_mat_dir, img_id, e2e_info_list)\n            self.results.append(result)\n\n    def get_metric(self):\n        metrics = combine_results(self.results)\n        self.reset()\n        return metrics\n\n    def reset(self):\n        self.results = []  # clear results\n", "ppocr/metrics/kie_metric.py": "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# The code is refer from: https://github.com/open-mmlab/mmocr/blob/main/mmocr/core/evaluation/kie_metric.py\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport paddle\n\n__all__ = [\"KIEMetric\"]\n\n\nclass KIEMetric(object):\n    def __init__(self, main_indicator=\"hmean\", **kwargs):\n        self.main_indicator = main_indicator\n        self.reset()\n        self.node = []\n        self.gt = []\n\n    def __call__(self, preds, batch, **kwargs):\n        nodes, _ = preds\n        gts, tag = batch[4].squeeze(0), batch[5].tolist()[0]\n        gts = gts[: tag[0], :1].reshape([-1])\n        self.node.append(nodes.numpy())\n        self.gt.append(gts)\n        # result = self.compute_f1_score(nodes, gts)\n        # self.results.append(result)\n\n    def compute_f1_score(self, preds, gts):\n        ignores = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 25]\n        C = preds.shape[1]\n        classes = np.array(sorted(set(range(C)) - set(ignores)))\n        hist = (\n            np.bincount((gts * C).astype(\"int64\") + preds.argmax(1), minlength=C**2)\n            .reshape([C, C])\n            .astype(\"float32\")\n        )\n        diag = np.diag(hist)\n        recalls = diag / hist.sum(1).clip(min=1)\n        precisions = diag / hist.sum(0).clip(min=1)\n        f1 = 2 * recalls * precisions / (recalls + precisions).clip(min=1e-8)\n        return f1[classes]\n\n    def combine_results(self, results):\n        node = np.concatenate(self.node, 0)\n        gts = np.concatenate(self.gt, 0)\n        results = self.compute_f1_score(node, gts)\n        data = {\"hmean\": results.mean()}\n        return data\n\n    def get_metric(self):\n        metrics = self.combine_results(self.results)\n        self.reset()\n        return metrics\n\n    def reset(self):\n        self.results = []  # clear results\n        self.node = []\n        self.gt = []\n", "ppocr/metrics/ct_metric.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nfrom scipy import io\nimport numpy as np\n\nfrom ppocr.utils.e2e_metric.Deteval import combine_results, get_score_C\n\n\nclass CTMetric(object):\n    def __init__(self, main_indicator, delimiter=\"\\t\", **kwargs):\n        self.delimiter = delimiter\n        self.main_indicator = main_indicator\n        self.reset()\n\n    def reset(self):\n        self.results = []  # clear results\n\n    def __call__(self, preds, batch, **kwargs):\n        # NOTE: only support bs=1 now, as the label length of different sample is Unequal\n        assert len(preds) == 1, \"CentripetalText test now only suuport batch_size=1.\"\n        label = batch[2]\n        text = batch[3]\n        pred = preds[0][\"points\"]\n        result = get_score_C(label, text, pred)\n\n        self.results.append(result)\n\n    def get_metric(self):\n        \"\"\"\n        Input format: y0,x0, ..... yn,xn. Each detection is separated by the end of line token ('\\n')'\n        \"\"\"\n        metrics = combine_results(self.results, rec_flag=False)\n        self.reset()\n        return metrics\n", "ppocr/metrics/distillation_metric.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport importlib\nimport copy\n\nfrom .rec_metric import RecMetric\nfrom .det_metric import DetMetric\nfrom .e2e_metric import E2EMetric\nfrom .cls_metric import ClsMetric\nfrom .vqa_token_ser_metric import VQASerTokenMetric\nfrom .vqa_token_re_metric import VQAReTokenMetric\n\n\nclass DistillationMetric(object):\n    def __init__(self, key=None, base_metric_name=None, main_indicator=None, **kwargs):\n        self.main_indicator = main_indicator\n        self.key = key\n        self.main_indicator = main_indicator\n        self.base_metric_name = base_metric_name\n        self.kwargs = kwargs\n        self.metrics = None\n\n    def _init_metrcis(self, preds):\n        self.metrics = dict()\n        mod = importlib.import_module(__name__)\n        for key in preds:\n            self.metrics[key] = getattr(mod, self.base_metric_name)(\n                main_indicator=self.main_indicator, **self.kwargs\n            )\n            self.metrics[key].reset()\n\n    def __call__(self, preds, batch, **kwargs):\n        assert isinstance(preds, dict)\n        if self.metrics is None:\n            self._init_metrcis(preds)\n        output = dict()\n        for key in preds:\n            self.metrics[key].__call__(preds[key], batch, **kwargs)\n\n    def get_metric(self):\n        \"\"\"\n        return metrics {\n                 'acc': 0,\n                 'norm_edit_dis': 0,\n            }\n        \"\"\"\n        output = dict()\n        for key in self.metrics:\n            metric = self.metrics[key].get_metric()\n            # main indicator\n            if key == self.key:\n                output.update(metric)\n            else:\n                for sub_key in metric:\n                    output[\"{}_{}\".format(key, sub_key)] = metric[sub_key]\n        return output\n\n    def reset(self):\n        for key in self.metrics:\n            self.metrics[key].reset()\n", "ppocr/metrics/table_metric.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport numpy as np\nfrom ppocr.metrics.det_metric import DetMetric\n\n\nclass TableStructureMetric(object):\n    def __init__(self, main_indicator=\"acc\", eps=1e-6, del_thead_tbody=False, **kwargs):\n        self.main_indicator = main_indicator\n        self.eps = eps\n        self.del_thead_tbody = del_thead_tbody\n        self.reset()\n\n    def __call__(self, pred_label, batch=None, *args, **kwargs):\n        preds, labels = pred_label\n        pred_structure_batch_list = preds[\"structure_batch_list\"]\n        gt_structure_batch_list = labels[\"structure_batch_list\"]\n        correct_num = 0\n        all_num = 0\n        for (pred, pred_conf), target in zip(\n            pred_structure_batch_list, gt_structure_batch_list\n        ):\n            pred_str = \"\".join(pred)\n            target_str = \"\".join(target)\n            if self.del_thead_tbody:\n                pred_str = (\n                    pred_str.replace(\"<thead>\", \"\")\n                    .replace(\"</thead>\", \"\")\n                    .replace(\"<tbody>\", \"\")\n                    .replace(\"</tbody>\", \"\")\n                )\n                target_str = (\n                    target_str.replace(\"<thead>\", \"\")\n                    .replace(\"</thead>\", \"\")\n                    .replace(\"<tbody>\", \"\")\n                    .replace(\"</tbody>\", \"\")\n                )\n            if pred_str == target_str:\n                correct_num += 1\n            all_num += 1\n        self.correct_num += correct_num\n        self.all_num += all_num\n\n    def get_metric(self):\n        \"\"\"\n        return metrics {\n                 'acc': 0,\n            }\n        \"\"\"\n        acc = 1.0 * self.correct_num / (self.all_num + self.eps)\n        self.reset()\n        return {\"acc\": acc}\n\n    def reset(self):\n        self.correct_num = 0\n        self.all_num = 0\n        self.len_acc_num = 0\n        self.token_nums = 0\n        self.anys_dict = dict()\n\n\nclass TableMetric(object):\n    def __init__(\n        self,\n        main_indicator=\"acc\",\n        compute_bbox_metric=False,\n        box_format=\"xyxy\",\n        del_thead_tbody=False,\n        **kwargs,\n    ):\n        \"\"\"\n\n        @param sub_metrics: configs of sub_metric\n        @param main_matric: main_matric for save best_model\n        @param kwargs:\n        \"\"\"\n        self.structure_metric = TableStructureMetric(del_thead_tbody=del_thead_tbody)\n        self.bbox_metric = DetMetric() if compute_bbox_metric else None\n        self.main_indicator = main_indicator\n        self.box_format = box_format\n        self.reset()\n\n    def __call__(self, pred_label, batch=None, *args, **kwargs):\n        self.structure_metric(pred_label)\n        if self.bbox_metric is not None:\n            self.bbox_metric(*self.prepare_bbox_metric_input(pred_label))\n\n    def prepare_bbox_metric_input(self, pred_label):\n        pred_bbox_batch_list = []\n        gt_ignore_tags_batch_list = []\n        gt_bbox_batch_list = []\n        preds, labels = pred_label\n\n        batch_num = len(preds[\"bbox_batch_list\"])\n        for batch_idx in range(batch_num):\n            # pred\n            pred_bbox_list = [\n                self.format_box(pred_box)\n                for pred_box in preds[\"bbox_batch_list\"][batch_idx]\n            ]\n            pred_bbox_batch_list.append({\"points\": pred_bbox_list})\n\n            # gt\n            gt_bbox_list = []\n            gt_ignore_tags_list = []\n            for gt_box in labels[\"bbox_batch_list\"][batch_idx]:\n                gt_bbox_list.append(self.format_box(gt_box))\n                gt_ignore_tags_list.append(0)\n            gt_bbox_batch_list.append(gt_bbox_list)\n            gt_ignore_tags_batch_list.append(gt_ignore_tags_list)\n\n        return [\n            pred_bbox_batch_list,\n            [0, 0, gt_bbox_batch_list, gt_ignore_tags_batch_list],\n        ]\n\n    def get_metric(self):\n        structure_metric = self.structure_metric.get_metric()\n        if self.bbox_metric is None:\n            return structure_metric\n        bbox_metric = self.bbox_metric.get_metric()\n        if self.main_indicator == self.bbox_metric.main_indicator:\n            output = bbox_metric\n            for sub_key in structure_metric:\n                output[\"structure_metric_{}\".format(sub_key)] = structure_metric[\n                    sub_key\n                ]\n        else:\n            output = structure_metric\n            for sub_key in bbox_metric:\n                output[\"bbox_metric_{}\".format(sub_key)] = bbox_metric[sub_key]\n        return output\n\n    def reset(self):\n        self.structure_metric.reset()\n        if self.bbox_metric is not None:\n            self.bbox_metric.reset()\n\n    def format_box(self, box):\n        if self.box_format == \"xyxy\":\n            x1, y1, x2, y2 = box\n            box = [[x1, y1], [x2, y1], [x2, y2], [x1, y2]]\n        elif self.box_format == \"xywh\":\n            x, y, w, h = box\n            x1, y1, x2, y2 = x - w // 2, y - h // 2, x + w // 2, y + h // 2\n            box = [[x1, y1], [x2, y1], [x2, y2], [x1, y2]]\n        elif self.box_format == \"xyxyxyxy\":\n            x1, y1, x2, y2, x3, y3, x4, y4 = box\n            box = [[x1, y1], [x2, y2], [x3, y3], [x4, y4]]\n        return box\n", "ppocr/metrics/rec_metric.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom rapidfuzz.distance import Levenshtein\nfrom difflib import SequenceMatcher\n\nimport numpy as np\nimport string\n\n\nclass RecMetric(object):\n    def __init__(\n        self, main_indicator=\"acc\", is_filter=False, ignore_space=True, **kwargs\n    ):\n        self.main_indicator = main_indicator\n        self.is_filter = is_filter\n        self.ignore_space = ignore_space\n        self.eps = 1e-5\n        self.reset()\n\n    def _normalize_text(self, text):\n        text = \"\".join(\n            filter(lambda x: x in (string.digits + string.ascii_letters), text)\n        )\n        return text.lower()\n\n    def __call__(self, pred_label, *args, **kwargs):\n        preds, labels = pred_label\n        correct_num = 0\n        all_num = 0\n        norm_edit_dis = 0.0\n        for (pred, pred_conf), (target, _) in zip(preds, labels):\n            if self.ignore_space:\n                pred = pred.replace(\" \", \"\")\n                target = target.replace(\" \", \"\")\n            if self.is_filter:\n                pred = self._normalize_text(pred)\n                target = self._normalize_text(target)\n            norm_edit_dis += Levenshtein.normalized_distance(pred, target)\n            if pred == target:\n                correct_num += 1\n            all_num += 1\n        self.correct_num += correct_num\n        self.all_num += all_num\n        self.norm_edit_dis += norm_edit_dis\n        return {\n            \"acc\": correct_num / (all_num + self.eps),\n            \"norm_edit_dis\": 1 - norm_edit_dis / (all_num + self.eps),\n        }\n\n    def get_metric(self):\n        \"\"\"\n        return metrics {\n                 'acc': 0,\n                 'norm_edit_dis': 0,\n            }\n        \"\"\"\n        acc = 1.0 * self.correct_num / (self.all_num + self.eps)\n        norm_edit_dis = 1 - self.norm_edit_dis / (self.all_num + self.eps)\n        self.reset()\n        return {\"acc\": acc, \"norm_edit_dis\": norm_edit_dis}\n\n    def reset(self):\n        self.correct_num = 0\n        self.all_num = 0\n        self.norm_edit_dis = 0\n\n\nclass CNTMetric(object):\n    def __init__(self, main_indicator=\"acc\", **kwargs):\n        self.main_indicator = main_indicator\n        self.eps = 1e-5\n        self.reset()\n\n    def __call__(self, pred_label, *args, **kwargs):\n        preds, labels = pred_label\n        correct_num = 0\n        all_num = 0\n        for pred, target in zip(preds, labels):\n            if pred == target:\n                correct_num += 1\n            all_num += 1\n        self.correct_num += correct_num\n        self.all_num += all_num\n        return {\n            \"acc\": correct_num / (all_num + self.eps),\n        }\n\n    def get_metric(self):\n        \"\"\"\n        return metrics {\n                 'acc': 0,\n            }\n        \"\"\"\n        acc = 1.0 * self.correct_num / (self.all_num + self.eps)\n        self.reset()\n        return {\"acc\": acc}\n\n    def reset(self):\n        self.correct_num = 0\n        self.all_num = 0\n\n\nclass CANMetric(object):\n    def __init__(self, main_indicator=\"exp_rate\", **kwargs):\n        self.main_indicator = main_indicator\n        self.word_right = []\n        self.exp_right = []\n        self.word_total_length = 0\n        self.exp_total_num = 0\n        self.word_rate = 0\n        self.exp_rate = 0\n        self.reset()\n        self.epoch_reset()\n\n    def __call__(self, preds, batch, **kwargs):\n        for k, v in kwargs.items():\n            epoch_reset = v\n            if epoch_reset:\n                self.epoch_reset()\n        word_probs = preds\n        word_label, word_label_mask = batch\n        line_right = 0\n        if word_probs is not None:\n            word_pred = word_probs.argmax(2)\n        word_pred = word_pred.cpu().detach().numpy()\n        word_scores = [\n            SequenceMatcher(\n                None, s1[: int(np.sum(s3))], s2[: int(np.sum(s3))], autojunk=False\n            ).ratio()\n            * (len(s1[: int(np.sum(s3))]) + len(s2[: int(np.sum(s3))]))\n            / len(s1[: int(np.sum(s3))])\n            / 2\n            for s1, s2, s3 in zip(word_label, word_pred, word_label_mask)\n        ]\n        batch_size = len(word_scores)\n        for i in range(batch_size):\n            if word_scores[i] == 1:\n                line_right += 1\n        self.word_rate = np.mean(word_scores)  # float\n        self.exp_rate = line_right / batch_size  # float\n        exp_length, word_length = word_label.shape[:2]\n        self.word_right.append(self.word_rate * word_length)\n        self.exp_right.append(self.exp_rate * exp_length)\n        self.word_total_length = self.word_total_length + word_length\n        self.exp_total_num = self.exp_total_num + exp_length\n\n    def get_metric(self):\n        \"\"\"\n        return {\n            'word_rate': 0,\n            \"exp_rate\": 0,\n        }\n        \"\"\"\n        cur_word_rate = sum(self.word_right) / self.word_total_length\n        cur_exp_rate = sum(self.exp_right) / self.exp_total_num\n        self.reset()\n        return {\"word_rate\": cur_word_rate, \"exp_rate\": cur_exp_rate}\n\n    def reset(self):\n        self.word_rate = 0\n        self.exp_rate = 0\n\n    def epoch_reset(self):\n        self.word_right = []\n        self.exp_right = []\n        self.word_total_length = 0\n        self.exp_total_num = 0\n", "ppocr/metrics/cls_metric.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nclass ClsMetric(object):\n    def __init__(self, main_indicator=\"acc\", **kwargs):\n        self.main_indicator = main_indicator\n        self.eps = 1e-5\n        self.reset()\n\n    def __call__(self, pred_label, *args, **kwargs):\n        preds, labels = pred_label\n        correct_num = 0\n        all_num = 0\n        for (pred, pred_conf), (target, _) in zip(preds, labels):\n            if pred == target:\n                correct_num += 1\n            all_num += 1\n        self.correct_num += correct_num\n        self.all_num += all_num\n        return {\n            \"acc\": correct_num / (all_num + self.eps),\n        }\n\n    def get_metric(self):\n        \"\"\"\n        return metrics {\n                 'acc': 0\n            }\n        \"\"\"\n        acc = self.correct_num / (self.all_num + self.eps)\n        self.reset()\n        return {\"acc\": acc}\n\n    def reset(self):\n        self.correct_num = 0\n        self.all_num = 0\n", "ppocr/metrics/eval_det_iou.py": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nfrom collections import namedtuple\nimport numpy as np\nfrom shapely.geometry import Polygon\n\n\"\"\"\nreference from :\nhttps://github.com/MhLiao/DB/blob/3c32b808d4412680310d3d28eeb6a2d5bf1566c5/concern/icdar2015_eval/detection/iou.py#L8\n\"\"\"\n\n\nclass DetectionIoUEvaluator(object):\n    def __init__(self, iou_constraint=0.5, area_precision_constraint=0.5):\n        self.iou_constraint = iou_constraint\n        self.area_precision_constraint = area_precision_constraint\n\n    def evaluate_image(self, gt, pred):\n        def get_union(pD, pG):\n            return Polygon(pD).union(Polygon(pG)).area\n\n        def get_intersection_over_union(pD, pG):\n            return get_intersection(pD, pG) / get_union(pD, pG)\n\n        def get_intersection(pD, pG):\n            return Polygon(pD).intersection(Polygon(pG)).area\n\n        def compute_ap(confList, matchList, numGtCare):\n            correct = 0\n            AP = 0\n            if len(confList) > 0:\n                confList = np.array(confList)\n                matchList = np.array(matchList)\n                sorted_ind = np.argsort(-confList)\n                confList = confList[sorted_ind]\n                matchList = matchList[sorted_ind]\n                for n in range(len(confList)):\n                    match = matchList[n]\n                    if match:\n                        correct += 1\n                        AP += float(correct) / (n + 1)\n\n                if numGtCare > 0:\n                    AP /= numGtCare\n\n            return AP\n\n        perSampleMetrics = {}\n\n        matchedSum = 0\n\n        Rectangle = namedtuple(\"Rectangle\", \"xmin ymin xmax ymax\")\n\n        numGlobalCareGt = 0\n        numGlobalCareDet = 0\n\n        arrGlobalConfidences = []\n        arrGlobalMatches = []\n\n        recall = 0\n        precision = 0\n        hmean = 0\n\n        detMatched = 0\n\n        iouMat = np.empty([1, 1])\n\n        gtPols = []\n        detPols = []\n\n        gtPolPoints = []\n        detPolPoints = []\n\n        # Array of Ground Truth Polygons' keys marked as don't Care\n        gtDontCarePolsNum = []\n        # Array of Detected Polygons' matched with a don't Care GT\n        detDontCarePolsNum = []\n\n        pairs = []\n        detMatchedNums = []\n\n        arrSampleConfidences = []\n        arrSampleMatch = []\n\n        evaluationLog = \"\"\n\n        for n in range(len(gt)):\n            points = gt[n][\"points\"]\n            dontCare = gt[n][\"ignore\"]\n            if not Polygon(points).is_valid:\n                continue\n\n            gtPol = points\n            gtPols.append(gtPol)\n            gtPolPoints.append(points)\n            if dontCare:\n                gtDontCarePolsNum.append(len(gtPols) - 1)\n\n        evaluationLog += (\n            \"GT polygons: \"\n            + str(len(gtPols))\n            + (\n                \" (\" + str(len(gtDontCarePolsNum)) + \" don't care)\\n\"\n                if len(gtDontCarePolsNum) > 0\n                else \"\\n\"\n            )\n        )\n\n        for n in range(len(pred)):\n            points = pred[n][\"points\"]\n            if not Polygon(points).is_valid:\n                continue\n\n            detPol = points\n            detPols.append(detPol)\n            detPolPoints.append(points)\n            if len(gtDontCarePolsNum) > 0:\n                for dontCarePol in gtDontCarePolsNum:\n                    dontCarePol = gtPols[dontCarePol]\n                    intersected_area = get_intersection(dontCarePol, detPol)\n                    pdDimensions = Polygon(detPol).area\n                    precision = (\n                        0 if pdDimensions == 0 else intersected_area / pdDimensions\n                    )\n                    if precision > self.area_precision_constraint:\n                        detDontCarePolsNum.append(len(detPols) - 1)\n                        break\n\n        evaluationLog += (\n            \"DET polygons: \"\n            + str(len(detPols))\n            + (\n                \" (\" + str(len(detDontCarePolsNum)) + \" don't care)\\n\"\n                if len(detDontCarePolsNum) > 0\n                else \"\\n\"\n            )\n        )\n\n        if len(gtPols) > 0 and len(detPols) > 0:\n            # Calculate IoU and precision matrixs\n            outputShape = [len(gtPols), len(detPols)]\n            iouMat = np.empty(outputShape)\n            gtRectMat = np.zeros(len(gtPols), np.int8)\n            detRectMat = np.zeros(len(detPols), np.int8)\n            for gtNum in range(len(gtPols)):\n                for detNum in range(len(detPols)):\n                    pG = gtPols[gtNum]\n                    pD = detPols[detNum]\n                    iouMat[gtNum, detNum] = get_intersection_over_union(pD, pG)\n\n            for gtNum in range(len(gtPols)):\n                for detNum in range(len(detPols)):\n                    if (\n                        gtRectMat[gtNum] == 0\n                        and detRectMat[detNum] == 0\n                        and gtNum not in gtDontCarePolsNum\n                        and detNum not in detDontCarePolsNum\n                    ):\n                        if iouMat[gtNum, detNum] > self.iou_constraint:\n                            gtRectMat[gtNum] = 1\n                            detRectMat[detNum] = 1\n                            detMatched += 1\n                            pairs.append({\"gt\": gtNum, \"det\": detNum})\n                            detMatchedNums.append(detNum)\n                            evaluationLog += (\n                                \"Match GT #\"\n                                + str(gtNum)\n                                + \" with Det #\"\n                                + str(detNum)\n                                + \"\\n\"\n                            )\n\n        numGtCare = len(gtPols) - len(gtDontCarePolsNum)\n        numDetCare = len(detPols) - len(detDontCarePolsNum)\n        if numGtCare == 0:\n            recall = float(1)\n            precision = float(0) if numDetCare > 0 else float(1)\n        else:\n            recall = float(detMatched) / numGtCare\n            precision = 0 if numDetCare == 0 else float(detMatched) / numDetCare\n\n        hmean = (\n            0\n            if (precision + recall) == 0\n            else 2.0 * precision * recall / (precision + recall)\n        )\n\n        matchedSum += detMatched\n        numGlobalCareGt += numGtCare\n        numGlobalCareDet += numDetCare\n\n        perSampleMetrics = {\n            \"gtCare\": numGtCare,\n            \"detCare\": numDetCare,\n            \"detMatched\": detMatched,\n        }\n        return perSampleMetrics\n\n    def combine_results(self, results):\n        numGlobalCareGt = 0\n        numGlobalCareDet = 0\n        matchedSum = 0\n        for result in results:\n            numGlobalCareGt += result[\"gtCare\"]\n            numGlobalCareDet += result[\"detCare\"]\n            matchedSum += result[\"detMatched\"]\n\n        methodRecall = (\n            0 if numGlobalCareGt == 0 else float(matchedSum) / numGlobalCareGt\n        )\n        methodPrecision = (\n            0 if numGlobalCareDet == 0 else float(matchedSum) / numGlobalCareDet\n        )\n        methodHmean = (\n            0\n            if methodRecall + methodPrecision == 0\n            else 2 * methodRecall * methodPrecision / (methodRecall + methodPrecision)\n        )\n        methodMetrics = {\n            \"precision\": methodPrecision,\n            \"recall\": methodRecall,\n            \"hmean\": methodHmean,\n        }\n\n        return methodMetrics\n\n\nif __name__ == \"__main__\":\n    evaluator = DetectionIoUEvaluator()\n    gts = [\n        [\n            {\n                \"points\": [(0, 0), (1, 0), (1, 1), (0, 1)],\n                \"text\": 1234,\n                \"ignore\": False,\n            },\n            {\n                \"points\": [(2, 2), (3, 2), (3, 3), (2, 3)],\n                \"text\": 5678,\n                \"ignore\": False,\n            },\n        ]\n    ]\n    preds = [\n        [\n            {\n                \"points\": [(0.1, 0.1), (1, 0), (1, 1), (0, 1)],\n                \"text\": 123,\n                \"ignore\": False,\n            }\n        ]\n    ]\n    results = []\n    for gt, pred in zip(gts, preds):\n        results.append(evaluator.evaluate_image(gt, pred))\n    metrics = evaluator.combine_results(results)\n    print(metrics)\n", "ppocr/metrics/det_metric.py": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n__all__ = [\"DetMetric\", \"DetFCEMetric\"]\n\nfrom .eval_det_iou import DetectionIoUEvaluator\n\n\nclass DetMetric(object):\n    def __init__(self, main_indicator=\"hmean\", **kwargs):\n        self.evaluator = DetectionIoUEvaluator()\n        self.main_indicator = main_indicator\n        self.reset()\n\n    def __call__(self, preds, batch, **kwargs):\n        \"\"\"\n        batch: a list produced by dataloaders.\n            image: np.ndarray  of shape (N, C, H, W).\n            ratio_list: np.ndarray  of shape(N,2)\n            polygons: np.ndarray  of shape (N, K, 4, 2), the polygons of objective regions.\n            ignore_tags: np.ndarray  of shape (N, K), indicates whether a region is ignorable or not.\n        preds: a list of dict produced by post process\n             points: np.ndarray of shape (N, K, 4, 2), the polygons of objective regions.\n        \"\"\"\n        gt_polyons_batch = batch[2]\n        ignore_tags_batch = batch[3]\n        for pred, gt_polyons, ignore_tags in zip(\n            preds, gt_polyons_batch, ignore_tags_batch\n        ):\n            # prepare gt\n            gt_info_list = [\n                {\"points\": gt_polyon, \"text\": \"\", \"ignore\": ignore_tag}\n                for gt_polyon, ignore_tag in zip(gt_polyons, ignore_tags)\n            ]\n            # prepare det\n            det_info_list = [\n                {\"points\": det_polyon, \"text\": \"\"} for det_polyon in pred[\"points\"]\n            ]\n            result = self.evaluator.evaluate_image(gt_info_list, det_info_list)\n            self.results.append(result)\n\n    def get_metric(self):\n        \"\"\"\n        return metrics {\n                 'precision': 0,\n                 'recall': 0,\n                 'hmean': 0\n            }\n        \"\"\"\n\n        metrics = self.evaluator.combine_results(self.results)\n        self.reset()\n        return metrics\n\n    def reset(self):\n        self.results = []  # clear results\n\n\nclass DetFCEMetric(object):\n    def __init__(self, main_indicator=\"hmean\", **kwargs):\n        self.evaluator = DetectionIoUEvaluator()\n        self.main_indicator = main_indicator\n        self.reset()\n\n    def __call__(self, preds, batch, **kwargs):\n        \"\"\"\n        batch: a list produced by dataloaders.\n            image: np.ndarray  of shape (N, C, H, W).\n            ratio_list: np.ndarray  of shape(N,2)\n            polygons: np.ndarray  of shape (N, K, 4, 2), the polygons of objective regions.\n            ignore_tags: np.ndarray  of shape (N, K), indicates whether a region is ignorable or not.\n        preds: a list of dict produced by post process\n             points: np.ndarray of shape (N, K, 4, 2), the polygons of objective regions.\n        \"\"\"\n        gt_polyons_batch = batch[2]\n        ignore_tags_batch = batch[3]\n\n        for pred, gt_polyons, ignore_tags in zip(\n            preds, gt_polyons_batch, ignore_tags_batch\n        ):\n            # prepare gt\n            gt_info_list = [\n                {\"points\": gt_polyon, \"text\": \"\", \"ignore\": ignore_tag}\n                for gt_polyon, ignore_tag in zip(gt_polyons, ignore_tags)\n            ]\n            # prepare det\n            det_info_list = [\n                {\"points\": det_polyon, \"text\": \"\", \"score\": score}\n                for det_polyon, score in zip(pred[\"points\"], pred[\"scores\"])\n            ]\n\n            for score_thr in self.results.keys():\n                det_info_list_thr = [\n                    det_info\n                    for det_info in det_info_list\n                    if det_info[\"score\"] >= score_thr\n                ]\n                result = self.evaluator.evaluate_image(gt_info_list, det_info_list_thr)\n                self.results[score_thr].append(result)\n\n    def get_metric(self):\n        \"\"\"\n        return metrics {'heman':0,\n            'thr 0.3':'precision: 0 recall: 0 hmean: 0',\n            'thr 0.4':'precision: 0 recall: 0 hmean: 0',\n            'thr 0.5':'precision: 0 recall: 0 hmean: 0',\n            'thr 0.6':'precision: 0 recall: 0 hmean: 0',\n            'thr 0.7':'precision: 0 recall: 0 hmean: 0',\n            'thr 0.8':'precision: 0 recall: 0 hmean: 0',\n            'thr 0.9':'precision: 0 recall: 0 hmean: 0',\n            }\n        \"\"\"\n        metrics = {}\n        hmean = 0\n        for score_thr in self.results.keys():\n            metric = self.evaluator.combine_results(self.results[score_thr])\n            # for key, value in metric.items():\n            #     metrics['{}_{}'.format(key, score_thr)] = value\n            metric_str = \"precision:{:.5f} recall:{:.5f} hmean:{:.5f}\".format(\n                metric[\"precision\"], metric[\"recall\"], metric[\"hmean\"]\n            )\n            metrics[\"thr {}\".format(score_thr)] = metric_str\n            hmean = max(hmean, metric[\"hmean\"])\n        metrics[\"hmean\"] = hmean\n\n        self.reset()\n        return metrics\n\n    def reset(self):\n        self.results = {\n            0.3: [],\n            0.4: [],\n            0.5: [],\n            0.6: [],\n            0.7: [],\n            0.8: [],\n            0.9: [],\n        }  # clear results\n", "ppocr/metrics/__init__.py": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport copy\n\n__all__ = [\"build_metric\"]\n\nfrom .det_metric import DetMetric, DetFCEMetric\nfrom .rec_metric import RecMetric, CNTMetric, CANMetric\nfrom .cls_metric import ClsMetric\nfrom .e2e_metric import E2EMetric\nfrom .distillation_metric import DistillationMetric\nfrom .table_metric import TableMetric\nfrom .kie_metric import KIEMetric\nfrom .vqa_token_ser_metric import VQASerTokenMetric\nfrom .vqa_token_re_metric import VQAReTokenMetric\nfrom .sr_metric import SRMetric\nfrom .ct_metric import CTMetric\n\n\ndef build_metric(config):\n    support_dict = [\n        \"DetMetric\",\n        \"DetFCEMetric\",\n        \"RecMetric\",\n        \"ClsMetric\",\n        \"E2EMetric\",\n        \"DistillationMetric\",\n        \"TableMetric\",\n        \"KIEMetric\",\n        \"VQASerTokenMetric\",\n        \"VQAReTokenMetric\",\n        \"SRMetric\",\n        \"CTMetric\",\n        \"CNTMetric\",\n        \"CANMetric\",\n    ]\n\n    config = copy.deepcopy(config)\n    module_name = config.pop(\"name\")\n    assert module_name in support_dict, Exception(\n        \"metric only support {}\".format(support_dict)\n    )\n    module_class = eval(module_name)(**config)\n    return module_class\n", "benchmark/analysis.py": "# copyright (c) 2019 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import print_function\n\nimport argparse\nimport json\nimport os\nimport re\nimport traceback\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(\n        \"--filename\", type=str, help=\"The name of log which need to analysis.\"\n    )\n    parser.add_argument(\n        \"--log_with_profiler\", type=str, help=\"The path of train log with profiler\"\n    )\n    parser.add_argument(\n        \"--profiler_path\", type=str, help=\"The path of profiler timeline log.\"\n    )\n    parser.add_argument(\"--keyword\", type=str, help=\"Keyword to specify analysis data\")\n    parser.add_argument(\n        \"--separator\",\n        type=str,\n        default=None,\n        help=\"Separator of different field in log\",\n    )\n    parser.add_argument(\n        \"--position\", type=int, default=None, help=\"The position of data field\"\n    )\n    parser.add_argument(\n        \"--range\", type=str, default=\"\", help=\"The range of data field to intercept\"\n    )\n    parser.add_argument(\"--base_batch_size\", type=int, help=\"base_batch size on gpu\")\n    parser.add_argument(\n        \"--skip_steps\", type=int, default=0, help=\"The number of steps to be skipped\"\n    )\n    parser.add_argument(\n        \"--model_mode\", type=int, default=-1, help=\"Analysis mode, default value is -1\"\n    )\n    parser.add_argument(\"--ips_unit\", type=str, default=None, help=\"IPS unit\")\n    parser.add_argument(\n        \"--model_name\",\n        type=str,\n        default=0,\n        help=\"training model_name, transformer_base\",\n    )\n    parser.add_argument(\n        \"--mission_name\", type=str, default=0, help=\"training mission name\"\n    )\n    parser.add_argument(\n        \"--direction_id\", type=int, default=0, help=\"training direction_id\"\n    )\n    parser.add_argument(\n        \"--run_mode\", type=str, default=\"sp\", help=\"multi process or single process\"\n    )\n    parser.add_argument(\n        \"--index\",\n        type=int,\n        default=1,\n        help=\"{1: speed, 2:mem, 3:profiler, 6:max_batch_size}\",\n    )\n    parser.add_argument(\"--gpu_num\", type=int, default=1, help=\"nums of training gpus\")\n    args = parser.parse_args()\n    args.separator = None if args.separator == \"None\" else args.separator\n    return args\n\n\ndef _is_number(num):\n    pattern = re.compile(r\"^[-+]?[-0-9]\\d*\\.\\d*|[-+]?\\.?[0-9]\\d*$\")\n    result = pattern.match(num)\n    if result:\n        return True\n    else:\n        return False\n\n\nclass TimeAnalyzer(object):\n    def __init__(\n        self, filename, keyword=None, separator=None, position=None, range=\"-1\"\n    ):\n        if filename is None:\n            raise Exception(\"Please specify the filename!\")\n\n        if keyword is None:\n            raise Exception(\"Please specify the keyword!\")\n\n        self.filename = filename\n        self.keyword = keyword\n        self.separator = separator\n        self.position = position\n        self.range = range\n        self.records = None\n        self._distil()\n\n    def _distil(self):\n        self.records = []\n        with open(self.filename, \"r\") as f_object:\n            lines = f_object.readlines()\n            for line in lines:\n                if self.keyword not in line:\n                    continue\n                try:\n                    result = None\n\n                    # Distil the string from a line.\n                    line = line.strip()\n                    line_words = (\n                        line.split(self.separator) if self.separator else line.split()\n                    )\n                    if args.position:\n                        result = line_words[self.position]\n                    else:\n                        # Distil the string following the keyword.\n                        for i in range(len(line_words) - 1):\n                            if line_words[i] == self.keyword:\n                                result = line_words[i + 1]\n                                break\n\n                    # Distil the result from the picked string.\n                    if not self.range:\n                        result = result[0:]\n                    elif _is_number(self.range):\n                        result = result[0 : int(self.range)]\n                    else:\n                        result = result[\n                            int(self.range.split(\":\")[0]) : int(\n                                self.range.split(\":\")[1]\n                            )\n                        ]\n                    self.records.append(float(result))\n                except Exception as exc:\n                    print(\n                        \"line is: {}; separator={}; position={}\".format(\n                            line, self.separator, self.position\n                        )\n                    )\n\n        print(\n            \"Extract {} records: separator={}; position={}\".format(\n                len(self.records), self.separator, self.position\n            )\n        )\n\n    def _get_fps(self, mode, batch_size, gpu_num, avg_of_records, run_mode, unit=None):\n        if mode == -1 and run_mode == \"sp\":\n            assert unit, \"Please set the unit when mode is -1.\"\n            fps = gpu_num * avg_of_records\n        elif mode == -1 and run_mode == \"mp\":\n            assert unit, \"Please set the unit when mode is -1.\"\n            fps = gpu_num * avg_of_records  # temporarily, not used now\n            print(\"------------this is mp\")\n        elif mode == 0:\n            # s/step -> samples/s\n            fps = (batch_size * gpu_num) / avg_of_records\n            unit = \"samples/s\"\n        elif mode == 1:\n            # steps/s -> steps/s\n            fps = avg_of_records\n            unit = \"steps/s\"\n        elif mode == 2:\n            # s/step -> steps/s\n            fps = 1 / avg_of_records\n            unit = \"steps/s\"\n        elif mode == 3:\n            # steps/s -> samples/s\n            fps = batch_size * gpu_num * avg_of_records\n            unit = \"samples/s\"\n        elif mode == 4:\n            # s/epoch -> s/epoch\n            fps = avg_of_records\n            unit = \"s/epoch\"\n        else:\n            ValueError(\"Unsupported analysis mode.\")\n\n        return fps, unit\n\n    def analysis(\n        self, batch_size, gpu_num=1, skip_steps=0, mode=-1, run_mode=\"sp\", unit=None\n    ):\n        if batch_size <= 0:\n            print(\"base_batch_size should larger than 0.\")\n            return 0, \"\"\n\n        if (\n            len(self.records) <= skip_steps\n        ):  # to address the condition which item of log equals to skip_steps\n            print(\"no records\")\n            return 0, \"\"\n\n        sum_of_records = 0\n        sum_of_records_skipped = 0\n        skip_min = self.records[skip_steps]\n        skip_max = self.records[skip_steps]\n\n        count = len(self.records)\n        for i in range(count):\n            sum_of_records += self.records[i]\n            if i >= skip_steps:\n                sum_of_records_skipped += self.records[i]\n                if self.records[i] < skip_min:\n                    skip_min = self.records[i]\n                if self.records[i] > skip_max:\n                    skip_max = self.records[i]\n\n        avg_of_records = sum_of_records / float(count)\n        avg_of_records_skipped = sum_of_records_skipped / float(count - skip_steps)\n\n        fps, fps_unit = self._get_fps(\n            mode, batch_size, gpu_num, avg_of_records, run_mode, unit\n        )\n        fps_skipped, _ = self._get_fps(\n            mode, batch_size, gpu_num, avg_of_records_skipped, run_mode, unit\n        )\n        if mode == -1:\n            print(\"average ips of %d steps, skip 0 step:\" % count)\n            print(\"\\tAvg: %.3f %s\" % (avg_of_records, fps_unit))\n            print(\"\\tFPS: %.3f %s\" % (fps, fps_unit))\n            if skip_steps > 0:\n                print(\"average ips of %d steps, skip %d steps:\" % (count, skip_steps))\n                print(\"\\tAvg: %.3f %s\" % (avg_of_records_skipped, fps_unit))\n                print(\"\\tMin: %.3f %s\" % (skip_min, fps_unit))\n                print(\"\\tMax: %.3f %s\" % (skip_max, fps_unit))\n                print(\"\\tFPS: %.3f %s\" % (fps_skipped, fps_unit))\n        elif mode == 1 or mode == 3:\n            print(\"average latency of %d steps, skip 0 step:\" % count)\n            print(\"\\tAvg: %.3f steps/s\" % avg_of_records)\n            print(\"\\tFPS: %.3f %s\" % (fps, fps_unit))\n            if skip_steps > 0:\n                print(\n                    \"average latency of %d steps, skip %d steps:\" % (count, skip_steps)\n                )\n                print(\"\\tAvg: %.3f steps/s\" % avg_of_records_skipped)\n                print(\"\\tMin: %.3f steps/s\" % skip_min)\n                print(\"\\tMax: %.3f steps/s\" % skip_max)\n                print(\"\\tFPS: %.3f %s\" % (fps_skipped, fps_unit))\n        elif mode == 0 or mode == 2:\n            print(\"average latency of %d steps, skip 0 step:\" % count)\n            print(\"\\tAvg: %.3f s/step\" % avg_of_records)\n            print(\"\\tFPS: %.3f %s\" % (fps, fps_unit))\n            if skip_steps > 0:\n                print(\n                    \"average latency of %d steps, skip %d steps:\" % (count, skip_steps)\n                )\n                print(\"\\tAvg: %.3f s/step\" % avg_of_records_skipped)\n                print(\"\\tMin: %.3f s/step\" % skip_min)\n                print(\"\\tMax: %.3f s/step\" % skip_max)\n                print(\"\\tFPS: %.3f %s\" % (fps_skipped, fps_unit))\n\n        return round(fps_skipped, 3), fps_unit\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    run_info = dict()\n    run_info[\"log_file\"] = args.filename\n    run_info[\"model_name\"] = args.model_name\n    run_info[\"mission_name\"] = args.mission_name\n    run_info[\"direction_id\"] = args.direction_id\n    run_info[\"run_mode\"] = args.run_mode\n    run_info[\"index\"] = args.index\n    run_info[\"gpu_num\"] = args.gpu_num\n    run_info[\"FINAL_RESULT\"] = 0\n    run_info[\"JOB_FAIL_FLAG\"] = 0\n\n    try:\n        if args.index == 1:\n            if args.gpu_num == 1:\n                run_info[\"log_with_profiler\"] = args.log_with_profiler\n                run_info[\"profiler_path\"] = args.profiler_path\n            analyzer = TimeAnalyzer(\n                args.filename, args.keyword, args.separator, args.position, args.range\n            )\n            run_info[\"FINAL_RESULT\"], run_info[\"UNIT\"] = analyzer.analysis(\n                batch_size=args.base_batch_size,\n                gpu_num=args.gpu_num,\n                skip_steps=args.skip_steps,\n                mode=args.model_mode,\n                run_mode=args.run_mode,\n                unit=args.ips_unit,\n            )\n            try:\n                if (\n                    int(os.getenv(\"job_fail_flag\")) == 1\n                    or int(run_info[\"FINAL_RESULT\"]) == 0\n                ):\n                    run_info[\"JOB_FAIL_FLAG\"] = 1\n            except:\n                pass\n        elif args.index == 3:\n            run_info[\"FINAL_RESULT\"] = {}\n            records_fo_total = TimeAnalyzer(\n                args.filename, \"Framework overhead\", None, 3, \"\"\n            ).records\n            records_fo_ratio = TimeAnalyzer(\n                args.filename, \"Framework overhead\", None, 5\n            ).records\n            records_ct_total = TimeAnalyzer(\n                args.filename, \"Computation time\", None, 3, \"\"\n            ).records\n            records_gm_total = TimeAnalyzer(\n                args.filename, \"GpuMemcpy                Calls\", None, 4, \"\"\n            ).records\n            records_gm_ratio = TimeAnalyzer(\n                args.filename, \"GpuMemcpy                Calls\", None, 6\n            ).records\n            records_gmas_total = TimeAnalyzer(\n                args.filename, \"GpuMemcpyAsync         Calls\", None, 4, \"\"\n            ).records\n            records_gms_total = TimeAnalyzer(\n                args.filename, \"GpuMemcpySync          Calls\", None, 4, \"\"\n            ).records\n            run_info[\"FINAL_RESULT\"][\"Framework_Total\"] = (\n                records_fo_total[0] if records_fo_total else 0\n            )\n            run_info[\"FINAL_RESULT\"][\"Framework_Ratio\"] = (\n                records_fo_ratio[0] if records_fo_ratio else 0\n            )\n            run_info[\"FINAL_RESULT\"][\"ComputationTime_Total\"] = (\n                records_ct_total[0] if records_ct_total else 0\n            )\n            run_info[\"FINAL_RESULT\"][\"GpuMemcpy_Total\"] = (\n                records_gm_total[0] if records_gm_total else 0\n            )\n            run_info[\"FINAL_RESULT\"][\"GpuMemcpy_Ratio\"] = (\n                records_gm_ratio[0] if records_gm_ratio else 0\n            )\n            run_info[\"FINAL_RESULT\"][\"GpuMemcpyAsync_Total\"] = (\n                records_gmas_total[0] if records_gmas_total else 0\n            )\n            run_info[\"FINAL_RESULT\"][\"GpuMemcpySync_Total\"] = (\n                records_gms_total[0] if records_gms_total else 0\n            )\n        else:\n            print(\"Not support!\")\n    except Exception:\n        traceback.print_exc()\n    print(\n        \"{}\".format(json.dumps(run_info))\n    )  # it's required, for the log file path  insert to the database\n", "benchmark/PaddleOCR_DBNet/models/model.py": "# -*- coding: utf-8 -*-\n# @Time    : 2019/8/23 21:57\n# @Author  : zhoujun\nfrom addict import Dict\nfrom paddle import nn\nimport paddle.nn.functional as F\n\nfrom models.backbone import build_backbone\nfrom models.neck import build_neck\nfrom models.head import build_head\n\n\nclass Model(nn.Layer):\n    def __init__(self, model_config: dict):\n        \"\"\"\n        PANnet\n        :param model_config: \u6a21\u578b\u914d\u7f6e\n        \"\"\"\n        super().__init__()\n        model_config = Dict(model_config)\n        backbone_type = model_config.backbone.pop(\"type\")\n        neck_type = model_config.neck.pop(\"type\")\n        head_type = model_config.head.pop(\"type\")\n        self.backbone = build_backbone(backbone_type, **model_config.backbone)\n        self.neck = build_neck(\n            neck_type, in_channels=self.backbone.out_channels, **model_config.neck\n        )\n        self.head = build_head(\n            head_type, in_channels=self.neck.out_channels, **model_config.head\n        )\n        self.name = f\"{backbone_type}_{neck_type}_{head_type}\"\n\n    def forward(self, x):\n        _, _, H, W = x.shape\n        backbone_out = self.backbone(x)\n        neck_out = self.neck(backbone_out)\n        y = self.head(neck_out)\n        y = F.interpolate(y, size=(H, W), mode=\"bilinear\", align_corners=True)\n        return y\n", "benchmark/PaddleOCR_DBNet/models/basic.py": "# -*- coding: utf-8 -*-\n# @Time    : 2019/12/6 11:19\n# @Author  : zhoujun\nfrom paddle import nn\n\n\nclass ConvBnRelu(nn.Layer):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride=1,\n        padding=0,\n        dilation=1,\n        groups=1,\n        bias=True,\n        padding_mode=\"zeros\",\n        inplace=True,\n    ):\n        super().__init__()\n        self.conv = nn.Conv2D(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            groups=groups,\n            bias_attr=bias,\n            padding_mode=padding_mode,\n        )\n        self.bn = nn.BatchNorm2D(out_channels)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n", "benchmark/PaddleOCR_DBNet/models/__init__.py": "# -*- coding: utf-8 -*-\n# @Time    : 2019/8/23 21:55\n# @Author  : zhoujun\nimport copy\nfrom .model import Model\nfrom .losses import build_loss\n\n__all__ = [\"build_loss\", \"build_model\"]\nsupport_model = [\"Model\"]\n\n\ndef build_model(config):\n    \"\"\"\n    get architecture model class\n    \"\"\"\n    copy_config = copy.deepcopy(config)\n    arch_type = copy_config.pop(\"type\")\n    assert (\n        arch_type in support_model\n    ), f\"{arch_type} is not developed yet!, only {support_model} are support now\"\n    arch_model = eval(arch_type)(copy_config)\n    return arch_model\n", "benchmark/PaddleOCR_DBNet/models/backbone/resnet.py": "import math\nimport paddle\nfrom paddle import nn\n\nBatchNorm2d = nn.BatchNorm2D\n\n__all__ = [\n    \"ResNet\",\n    \"resnet18\",\n    \"resnet34\",\n    \"resnet50\",\n    \"resnet101\",\n    \"deformable_resnet18\",\n    \"deformable_resnet50\",\n    \"resnet152\",\n]\n\nmodel_urls = {\n    \"resnet18\": \"https://download.pytorch.org/models/resnet18-5c106cde.pth\",\n    \"resnet34\": \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\",\n    \"resnet50\": \"https://download.pytorch.org/models/resnet50-19c8e357.pth\",\n    \"resnet101\": \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\",\n    \"resnet152\": \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\",\n}\n\n\ndef constant_init(module, constant, bias=0):\n    module.weight = paddle.create_parameter(\n        shape=module.weight.shape,\n        dtype=\"float32\",\n        default_initializer=paddle.nn.initializer.Constant(constant),\n    )\n    if hasattr(module, \"bias\"):\n        module.bias = paddle.create_parameter(\n            shape=module.bias.shape,\n            dtype=\"float32\",\n            default_initializer=paddle.nn.initializer.Constant(bias),\n        )\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2D(\n        in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias_attr=False\n    )\n\n\nclass BasicBlock(nn.Layer):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, dcn=None):\n        super(BasicBlock, self).__init__()\n        self.with_dcn = dcn is not None\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = BatchNorm2d(planes, momentum=0.1)\n        self.relu = nn.ReLU()\n        self.with_modulated_dcn = False\n        if not self.with_dcn:\n            self.conv2 = nn.Conv2D(\n                planes, planes, kernel_size=3, padding=1, bias_attr=False\n            )\n        else:\n            from paddle.version.ops import DeformConv2D\n\n            deformable_groups = dcn.get(\"deformable_groups\", 1)\n            offset_channels = 18\n            self.conv2_offset = nn.Conv2D(\n                planes, deformable_groups * offset_channels, kernel_size=3, padding=1\n            )\n            self.conv2 = DeformConv2D(\n                planes, planes, kernel_size=3, padding=1, bias_attr=False\n            )\n        self.bn2 = BatchNorm2d(planes, momentum=0.1)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        # out = self.conv2(out)\n        if not self.with_dcn:\n            out = self.conv2(out)\n        else:\n            offset = self.conv2_offset(out)\n            out = self.conv2(out, offset)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Layer):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, dcn=None):\n        super(Bottleneck, self).__init__()\n        self.with_dcn = dcn is not None\n        self.conv1 = nn.Conv2D(inplanes, planes, kernel_size=1, bias_attr=False)\n        self.bn1 = BatchNorm2d(planes, momentum=0.1)\n        self.with_modulated_dcn = False\n        if not self.with_dcn:\n            self.conv2 = nn.Conv2D(\n                planes, planes, kernel_size=3, stride=stride, padding=1, bias_attr=False\n            )\n        else:\n            deformable_groups = dcn.get(\"deformable_groups\", 1)\n            from paddle.vision.ops import DeformConv2D\n\n            offset_channels = 18\n            self.conv2_offset = nn.Conv2D(\n                planes,\n                deformable_groups * offset_channels,\n                stride=stride,\n                kernel_size=3,\n                padding=1,\n            )\n            self.conv2 = DeformConv2D(\n                planes, planes, kernel_size=3, padding=1, stride=stride, bias_attr=False\n            )\n        self.bn2 = BatchNorm2d(planes, momentum=0.1)\n        self.conv3 = nn.Conv2D(planes, planes * 4, kernel_size=1, bias_attr=False)\n        self.bn3 = BatchNorm2d(planes * 4, momentum=0.1)\n        self.relu = nn.ReLU()\n        self.downsample = downsample\n        self.stride = stride\n        self.dcn = dcn\n        self.with_dcn = dcn is not None\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        # out = self.conv2(out)\n        if not self.with_dcn:\n            out = self.conv2(out)\n        else:\n            offset = self.conv2_offset(out)\n            out = self.conv2(out, offset)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Layer):\n    def __init__(self, block, layers, in_channels=3, dcn=None):\n        self.dcn = dcn\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.out_channels = []\n        self.conv1 = nn.Conv2D(\n            in_channels, 64, kernel_size=7, stride=2, padding=3, bias_attr=False\n        )\n        self.bn1 = BatchNorm2d(64, momentum=0.1)\n        self.relu = nn.ReLU()\n        self.maxpool = nn.MaxPool2D(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dcn=dcn)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dcn=dcn)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dcn=dcn)\n\n        if self.dcn is not None:\n            for m in self.modules():\n                if isinstance(m, Bottleneck) or isinstance(m, BasicBlock):\n                    if hasattr(m, \"conv2_offset\"):\n                        constant_init(m.conv2_offset, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1, dcn=None):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2D(\n                    self.inplanes,\n                    planes * block.expansion,\n                    kernel_size=1,\n                    stride=stride,\n                    bias_attr=False,\n                ),\n                BatchNorm2d(planes * block.expansion, momentum=0.1),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, dcn=dcn))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, dcn=dcn))\n        self.out_channels.append(planes * block.expansion)\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x2 = self.layer1(x)\n        x3 = self.layer2(x2)\n        x4 = self.layer3(x3)\n        x5 = self.layer4(x4)\n\n        return x2, x3, x4, x5\n\n\ndef load_torch_params(paddle_model, torch_patams):\n    paddle_params = paddle_model.state_dict()\n\n    fc_names = [\"classifier\"]\n    for key, torch_value in torch_patams.items():\n        if \"num_batches_tracked\" in key:\n            continue\n        key = (\n            key.replace(\"running_var\", \"_variance\")\n            .replace(\"running_mean\", \"_mean\")\n            .replace(\"module.\", \"\")\n        )\n        torch_value = torch_value.detach().cpu().numpy()\n        if key in paddle_params:\n            flag = [i in key for i in fc_names]\n            if any(flag) and \"weight\" in key:  # ignore bias\n                new_shape = [1, 0] + list(range(2, torch_value.ndim))\n                print(\n                    f\"name: {key}, ori shape: {torch_value.shape}, new shape: {torch_value.transpose(new_shape).shape}\"\n                )\n                torch_value = torch_value.transpose(new_shape)\n            paddle_params[key] = torch_value\n        else:\n            print(f\"{key} not in paddle\")\n    paddle_model.set_state_dict(paddle_params)\n\n\ndef load_models(model, model_name):\n    import torch.utils.model_zoo as model_zoo\n\n    torch_patams = model_zoo.load_url(model_urls[model_name])\n    load_torch_params(model, torch_patams)\n\n\ndef resnet18(pretrained=True, **kwargs):\n    \"\"\"Constructs a ResNet-18 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        assert (\n            kwargs.get(\"in_channels\", 3) == 3\n        ), \"in_channels must be 3 whem pretrained is True\"\n        print(\"load from imagenet\")\n        load_models(model, \"resnet18\")\n    return model\n\n\ndef deformable_resnet18(pretrained=True, **kwargs):\n    \"\"\"Constructs a ResNet-18 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(BasicBlock, [2, 2, 2, 2], dcn=dict(deformable_groups=1), **kwargs)\n    if pretrained:\n        assert (\n            kwargs.get(\"in_channels\", 3) == 3\n        ), \"in_channels must be 3 whem pretrained is True\"\n        print(\"load from imagenet\")\n        model.load_state_dict(model_zoo.load_url(model_urls[\"resnet18\"]), strict=False)\n    return model\n\n\ndef resnet34(pretrained=True, **kwargs):\n    \"\"\"Constructs a ResNet-34 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        assert (\n            kwargs.get(\"in_channels\", 3) == 3\n        ), \"in_channels must be 3 whem pretrained is True\"\n        model.load_state_dict(model_zoo.load_url(model_urls[\"resnet34\"]), strict=False)\n    return model\n\n\ndef resnet50(pretrained=True, **kwargs):\n    \"\"\"Constructs a ResNet-50 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        assert (\n            kwargs.get(\"in_channels\", 3) == 3\n        ), \"in_channels must be 3 whem pretrained is True\"\n        load_models(model, \"resnet50\")\n    return model\n\n\ndef deformable_resnet50(pretrained=True, **kwargs):\n    \"\"\"Constructs a ResNet-50 model with deformable conv.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(Bottleneck, [3, 4, 6, 3], dcn=dict(deformable_groups=1), **kwargs)\n    if pretrained:\n        assert (\n            kwargs.get(\"in_channels\", 3) == 3\n        ), \"in_channels must be 3 whem pretrained is True\"\n        model.load_state_dict(model_zoo.load_url(model_urls[\"resnet50\"]), strict=False)\n    return model\n\n\ndef resnet101(pretrained=True, **kwargs):\n    \"\"\"Constructs a ResNet-101 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        assert (\n            kwargs.get(\"in_channels\", 3) == 3\n        ), \"in_channels must be 3 whem pretrained is True\"\n        model.load_state_dict(model_zoo.load_url(model_urls[\"resnet101\"]), strict=False)\n    return model\n\n\ndef resnet152(pretrained=True, **kwargs):\n    \"\"\"Constructs a ResNet-152 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    if pretrained:\n        assert (\n            kwargs.get(\"in_channels\", 3) == 3\n        ), \"in_channels must be 3 whem pretrained is True\"\n        model.load_state_dict(model_zoo.load_url(model_urls[\"resnet152\"]), strict=False)\n    return model\n\n\nif __name__ == \"__main__\":\n    x = paddle.zeros([2, 3, 640, 640])\n    net = resnet50(pretrained=True)\n    y = net(x)\n    for u in y:\n        print(u.shape)\n\n    print(net.out_channels)\n", "benchmark/PaddleOCR_DBNet/models/backbone/__init__.py": "# -*- coding: utf-8 -*-\n# @Time    : 2019/8/23 21:54\n# @Author  : zhoujun\n\nfrom .resnet import *\n\n__all__ = [\"build_backbone\"]\n\nsupport_backbone = [\n    \"resnet18\",\n    \"deformable_resnet18\",\n    \"deformable_resnet50\",\n    \"resnet50\",\n    \"resnet34\",\n    \"resnet101\",\n    \"resnet152\",\n]\n\n\ndef build_backbone(backbone_name, **kwargs):\n    assert (\n        backbone_name in support_backbone\n    ), f\"all support backbone is {support_backbone}\"\n    backbone = eval(backbone_name)(**kwargs)\n    return backbone\n", "benchmark/PaddleOCR_DBNet/models/losses/DB_loss.py": "import paddle\nfrom models.losses.basic_loss import BalanceCrossEntropyLoss, MaskL1Loss, DiceLoss\n\n\nclass DBLoss(paddle.nn.Layer):\n    def __init__(self, alpha=1.0, beta=10, ohem_ratio=3, reduction=\"mean\", eps=1e-06):\n        \"\"\"\n        Implement PSE Loss.\n        :param alpha: binary_map loss \u524d\u9762\u7684\u7cfb\u6570\n        :param beta: threshold_map loss \u524d\u9762\u7684\u7cfb\u6570\n        :param ohem_ratio: OHEM\u7684\u6bd4\u4f8b\n        :param reduction: 'mean' or 'sum'\u5bf9 batch\u91cc\u7684loss \u7b97\u5747\u503c\u6216\u6c42\u548c\n        \"\"\"\n        super().__init__()\n        assert reduction in [\"mean\", \"sum\"], \" reduction must in ['mean','sum']\"\n        self.alpha = alpha\n        self.beta = beta\n        self.bce_loss = BalanceCrossEntropyLoss(negative_ratio=ohem_ratio)\n        self.dice_loss = DiceLoss(eps=eps)\n        self.l1_loss = MaskL1Loss(eps=eps)\n        self.ohem_ratio = ohem_ratio\n        self.reduction = reduction\n\n    def forward(self, pred, batch):\n        shrink_maps = pred[:, 0, :, :]\n        threshold_maps = pred[:, 1, :, :]\n        binary_maps = pred[:, 2, :, :]\n        loss_shrink_maps = self.bce_loss(\n            shrink_maps, batch[\"shrink_map\"], batch[\"shrink_mask\"]\n        )\n        loss_threshold_maps = self.l1_loss(\n            threshold_maps, batch[\"threshold_map\"], batch[\"threshold_mask\"]\n        )\n        metrics = dict(\n            loss_shrink_maps=loss_shrink_maps, loss_threshold_maps=loss_threshold_maps\n        )\n        if pred.shape[1] > 2:\n            loss_binary_maps = self.dice_loss(\n                binary_maps, batch[\"shrink_map\"], batch[\"shrink_mask\"]\n            )\n            metrics[\"loss_binary_maps\"] = loss_binary_maps\n            loss_all = (\n                self.alpha * loss_shrink_maps\n                + self.beta * loss_threshold_maps\n                + loss_binary_maps\n            )\n            metrics[\"loss\"] = loss_all\n        else:\n            metrics[\"loss\"] = loss_shrink_maps\n        return metrics\n", "benchmark/PaddleOCR_DBNet/models/losses/__init__.py": "# -*- coding: utf-8 -*-\n# @Time    : 2020/6/5 11:36\n# @Author  : zhoujun\nimport copy\nfrom .DB_loss import DBLoss\n\n__all__ = [\"build_loss\"]\nsupport_loss = [\"DBLoss\"]\n\n\ndef build_loss(config):\n    copy_config = copy.deepcopy(config)\n    loss_type = copy_config.pop(\"type\")\n    assert loss_type in support_loss, f\"all support loss is {support_loss}\"\n    criterion = eval(loss_type)(**copy_config)\n    return criterion\n", "benchmark/PaddleOCR_DBNet/models/losses/basic_loss.py": "# -*- coding: utf-8 -*-\n# @Time    : 2019/12/4 14:39\n# @Author  : zhoujun\nimport paddle\nimport paddle.nn as nn\n\n\nclass BalanceCrossEntropyLoss(nn.Layer):\n    \"\"\"\n    Balanced cross entropy loss.\n    Shape:\n        - Input: :math:`(N, 1, H, W)`\n        - GT: :math:`(N, 1, H, W)`, same shape as the input\n        - Mask: :math:`(N, H, W)`, same spatial shape as the input\n        - Output: scalar.\n\n    \"\"\"\n\n    def __init__(self, negative_ratio=3.0, eps=1e-6):\n        super(BalanceCrossEntropyLoss, self).__init__()\n        self.negative_ratio = negative_ratio\n        self.eps = eps\n\n    def forward(\n        self,\n        pred: paddle.Tensor,\n        gt: paddle.Tensor,\n        mask: paddle.Tensor,\n        return_origin=False,\n    ):\n        \"\"\"\n        Args:\n            pred: shape :math:`(N, 1, H, W)`, the prediction of network\n            gt: shape :math:`(N, 1, H, W)`, the target\n            mask: shape :math:`(N, H, W)`, the mask indicates positive regions\n        \"\"\"\n        positive = gt * mask\n        negative = (1 - gt) * mask\n        positive_count = int(positive.sum())\n        negative_count = min(\n            int(negative.sum()), int(positive_count * self.negative_ratio)\n        )\n        loss = nn.functional.binary_cross_entropy(pred, gt, reduction=\"none\")\n        positive_loss = loss * positive\n        negative_loss = loss * negative\n        negative_loss, _ = negative_loss.reshape([-1]).topk(negative_count)\n\n        balance_loss = (positive_loss.sum() + negative_loss.sum()) / (\n            positive_count + negative_count + self.eps\n        )\n\n        if return_origin:\n            return balance_loss, loss\n        return balance_loss\n\n\nclass DiceLoss(nn.Layer):\n    \"\"\"\n    Loss function from https://arxiv.org/abs/1707.03237,\n    where iou computation is introduced heatmap manner to measure the\n    diversity bwtween tow heatmaps.\n    \"\"\"\n\n    def __init__(self, eps=1e-6):\n        super(DiceLoss, self).__init__()\n        self.eps = eps\n\n    def forward(self, pred: paddle.Tensor, gt, mask, weights=None):\n        \"\"\"\n        pred: one or two heatmaps of shape (N, 1, H, W),\n            the losses of tow heatmaps are added together.\n        gt: (N, 1, H, W)\n        mask: (N, H, W)\n        \"\"\"\n        return self._compute(pred, gt, mask, weights)\n\n    def _compute(self, pred, gt, mask, weights):\n        if len(pred.shape) == 4:\n            pred = pred[:, 0, :, :]\n            gt = gt[:, 0, :, :]\n        assert pred.shape == gt.shape\n        assert pred.shape == mask.shape\n        if weights is not None:\n            assert weights.shape == mask.shape\n            mask = weights * mask\n        intersection = (pred * gt * mask).sum()\n\n        union = (pred * mask).sum() + (gt * mask).sum() + self.eps\n        loss = 1 - 2.0 * intersection / union\n        assert loss <= 1\n        return loss\n\n\nclass MaskL1Loss(nn.Layer):\n    def __init__(self, eps=1e-6):\n        super(MaskL1Loss, self).__init__()\n        self.eps = eps\n\n    def forward(self, pred: paddle.Tensor, gt, mask):\n        loss = (paddle.abs(pred - gt) * mask).sum() / (mask.sum() + self.eps)\n        return loss\n", "benchmark/PaddleOCR_DBNet/models/head/DBHead.py": "# -*- coding: utf-8 -*-\n# @Time    : 2019/12/4 14:54\n# @Author  : zhoujun\nimport paddle\nfrom paddle import nn, ParamAttr\n\n\nclass DBHead(nn.Layer):\n    def __init__(self, in_channels, out_channels, k=50):\n        super().__init__()\n        self.k = k\n        self.binarize = nn.Sequential(\n            nn.Conv2D(\n                in_channels,\n                in_channels // 4,\n                3,\n                padding=1,\n                weight_attr=ParamAttr(initializer=nn.initializer.KaimingNormal()),\n            ),\n            nn.BatchNorm2D(\n                in_channels // 4,\n                weight_attr=ParamAttr(initializer=nn.initializer.Constant(1)),\n                bias_attr=ParamAttr(initializer=nn.initializer.Constant(1e-4)),\n            ),\n            nn.ReLU(),\n            nn.Conv2DTranspose(\n                in_channels // 4,\n                in_channels // 4,\n                2,\n                2,\n                weight_attr=ParamAttr(initializer=nn.initializer.KaimingNormal()),\n            ),\n            nn.BatchNorm2D(\n                in_channels // 4,\n                weight_attr=ParamAttr(initializer=nn.initializer.Constant(1)),\n                bias_attr=ParamAttr(initializer=nn.initializer.Constant(1e-4)),\n            ),\n            nn.ReLU(),\n            nn.Conv2DTranspose(\n                in_channels // 4, 1, 2, 2, weight_attr=nn.initializer.KaimingNormal()\n            ),\n            nn.Sigmoid(),\n        )\n\n        self.thresh = self._init_thresh(in_channels)\n\n    def forward(self, x):\n        shrink_maps = self.binarize(x)\n        threshold_maps = self.thresh(x)\n        if self.training:\n            binary_maps = self.step_function(shrink_maps, threshold_maps)\n            y = paddle.concat((shrink_maps, threshold_maps, binary_maps), axis=1)\n        else:\n            y = paddle.concat((shrink_maps, threshold_maps), axis=1)\n        return y\n\n    def _init_thresh(self, inner_channels, serial=False, smooth=False, bias=False):\n        in_channels = inner_channels\n        if serial:\n            in_channels += 1\n        self.thresh = nn.Sequential(\n            nn.Conv2D(\n                in_channels,\n                inner_channels // 4,\n                3,\n                padding=1,\n                bias_attr=bias,\n                weight_attr=ParamAttr(initializer=nn.initializer.KaimingNormal()),\n            ),\n            nn.BatchNorm2D(\n                inner_channels // 4,\n                weight_attr=ParamAttr(initializer=nn.initializer.Constant(1)),\n                bias_attr=ParamAttr(initializer=nn.initializer.Constant(1e-4)),\n            ),\n            nn.ReLU(),\n            self._init_upsample(\n                inner_channels // 4, inner_channels // 4, smooth=smooth, bias=bias\n            ),\n            nn.BatchNorm2D(\n                inner_channels // 4,\n                weight_attr=ParamAttr(initializer=nn.initializer.Constant(1)),\n                bias_attr=ParamAttr(initializer=nn.initializer.Constant(1e-4)),\n            ),\n            nn.ReLU(),\n            self._init_upsample(inner_channels // 4, 1, smooth=smooth, bias=bias),\n            nn.Sigmoid(),\n        )\n        return self.thresh\n\n    def _init_upsample(self, in_channels, out_channels, smooth=False, bias=False):\n        if smooth:\n            inter_out_channels = out_channels\n            if out_channels == 1:\n                inter_out_channels = in_channels\n            module_list = [\n                nn.Upsample(scale_factor=2, mode=\"nearest\"),\n                nn.Conv2D(\n                    in_channels,\n                    inter_out_channels,\n                    3,\n                    1,\n                    1,\n                    bias_attr=bias,\n                    weight_attr=ParamAttr(initializer=nn.initializer.KaimingNormal()),\n                ),\n            ]\n            if out_channels == 1:\n                module_list.append(\n                    nn.Conv2D(\n                        in_channels,\n                        out_channels,\n                        kernel_size=1,\n                        stride=1,\n                        padding=1,\n                        bias_attr=True,\n                        weight_attr=ParamAttr(\n                            initializer=nn.initializer.KaimingNormal()\n                        ),\n                    )\n                )\n            return nn.Sequential(module_list)\n        else:\n            return nn.Conv2DTranspose(\n                in_channels,\n                out_channels,\n                2,\n                2,\n                weight_attr=ParamAttr(initializer=nn.initializer.KaimingNormal()),\n            )\n\n    def step_function(self, x, y):\n        return paddle.reciprocal(1 + paddle.exp(-self.k * (x - y)))\n", "benchmark/PaddleOCR_DBNet/models/head/__init__.py": "# -*- coding: utf-8 -*-\n# @Time    : 2020/6/5 11:35\n# @Author  : zhoujun\nfrom .DBHead import DBHead\n\n__all__ = [\"build_head\"]\nsupport_head = [\"DBHead\"]\n\n\ndef build_head(head_name, **kwargs):\n    assert head_name in support_head, f\"all support head is {support_head}\"\n    head = eval(head_name)(**kwargs)\n    return head\n", "benchmark/PaddleOCR_DBNet/models/neck/FPN.py": "# -*- coding: utf-8 -*-\n# @Time    : 2019/9/13 10:29\n# @Author  : zhoujun\nimport paddle\nimport paddle.nn.functional as F\nfrom paddle import nn\n\nfrom models.basic import ConvBnRelu\n\n\nclass FPN(nn.Layer):\n    def __init__(self, in_channels, inner_channels=256, **kwargs):\n        \"\"\"\n        :param in_channels: \u57fa\u7840\u7f51\u7edc\u8f93\u51fa\u7684\u7ef4\u5ea6\n        :param kwargs:\n        \"\"\"\n        super().__init__()\n        inplace = True\n        self.conv_out = inner_channels\n        inner_channels = inner_channels // 4\n        # reduce layers\n        self.reduce_conv_c2 = ConvBnRelu(\n            in_channels[0], inner_channels, kernel_size=1, inplace=inplace\n        )\n        self.reduce_conv_c3 = ConvBnRelu(\n            in_channels[1], inner_channels, kernel_size=1, inplace=inplace\n        )\n        self.reduce_conv_c4 = ConvBnRelu(\n            in_channels[2], inner_channels, kernel_size=1, inplace=inplace\n        )\n        self.reduce_conv_c5 = ConvBnRelu(\n            in_channels[3], inner_channels, kernel_size=1, inplace=inplace\n        )\n        # Smooth layers\n        self.smooth_p4 = ConvBnRelu(\n            inner_channels, inner_channels, kernel_size=3, padding=1, inplace=inplace\n        )\n        self.smooth_p3 = ConvBnRelu(\n            inner_channels, inner_channels, kernel_size=3, padding=1, inplace=inplace\n        )\n        self.smooth_p2 = ConvBnRelu(\n            inner_channels, inner_channels, kernel_size=3, padding=1, inplace=inplace\n        )\n\n        self.conv = nn.Sequential(\n            nn.Conv2D(self.conv_out, self.conv_out, kernel_size=3, padding=1, stride=1),\n            nn.BatchNorm2D(self.conv_out),\n            nn.ReLU(),\n        )\n        self.out_channels = self.conv_out\n\n    def forward(self, x):\n        c2, c3, c4, c5 = x\n        # Top-down\n        p5 = self.reduce_conv_c5(c5)\n        p4 = self._upsample_add(p5, self.reduce_conv_c4(c4))\n        p4 = self.smooth_p4(p4)\n        p3 = self._upsample_add(p4, self.reduce_conv_c3(c3))\n        p3 = self.smooth_p3(p3)\n        p2 = self._upsample_add(p3, self.reduce_conv_c2(c2))\n        p2 = self.smooth_p2(p2)\n\n        x = self._upsample_cat(p2, p3, p4, p5)\n        x = self.conv(x)\n        return x\n\n    def _upsample_add(self, x, y):\n        return F.interpolate(x, size=y.shape[2:]) + y\n\n    def _upsample_cat(self, p2, p3, p4, p5):\n        h, w = p2.shape[2:]\n        p3 = F.interpolate(p3, size=(h, w))\n        p4 = F.interpolate(p4, size=(h, w))\n        p5 = F.interpolate(p5, size=(h, w))\n        return paddle.concat([p2, p3, p4, p5], axis=1)\n", "benchmark/PaddleOCR_DBNet/models/neck/__init__.py": "# -*- coding: utf-8 -*-\n# @Time    : 2020/6/5 11:34\n# @Author  : zhoujun\nfrom .FPN import FPN\n\n__all__ = [\"build_neck\"]\nsupport_neck = [\"FPN\"]\n\n\ndef build_neck(neck_name, **kwargs):\n    assert neck_name in support_neck, f\"all support neck is {support_neck}\"\n    neck = eval(neck_name)(**kwargs)\n    return neck\n", "benchmark/PaddleOCR_DBNet/tools/train.py": "import os\nimport sys\nimport pathlib\n\n__dir__ = pathlib.Path(os.path.abspath(__file__))\nsys.path.append(str(__dir__))\nsys.path.append(str(__dir__.parent.parent))\n\nimport paddle\nimport paddle.distributed as dist\nfrom utils import Config, ArgsParser\n\n\ndef init_args():\n    parser = ArgsParser()\n    args = parser.parse_args()\n    return args\n\n\ndef main(config, profiler_options):\n    from models import build_model, build_loss\n    from data_loader import get_dataloader\n    from trainer import Trainer\n    from post_processing import get_post_processing\n    from utils import get_metric\n\n    if paddle.device.cuda.device_count() > 1:\n        dist.init_parallel_env()\n        config[\"distributed\"] = True\n    else:\n        config[\"distributed\"] = False\n    train_loader = get_dataloader(config[\"dataset\"][\"train\"], config[\"distributed\"])\n    assert train_loader is not None\n    if \"validate\" in config[\"dataset\"]:\n        validate_loader = get_dataloader(config[\"dataset\"][\"validate\"], False)\n    else:\n        validate_loader = None\n    criterion = build_loss(config[\"loss\"])\n    config[\"arch\"][\"backbone\"][\"in_channels\"] = (\n        3 if config[\"dataset\"][\"train\"][\"dataset\"][\"args\"][\"img_mode\"] != \"GRAY\" else 1\n    )\n    model = build_model(config[\"arch\"])\n    # set @to_static for benchmark, skip this by default.\n    post_p = get_post_processing(config[\"post_processing\"])\n    metric = get_metric(config[\"metric\"])\n    trainer = Trainer(\n        config=config,\n        model=model,\n        criterion=criterion,\n        train_loader=train_loader,\n        post_process=post_p,\n        metric_cls=metric,\n        validate_loader=validate_loader,\n        profiler_options=profiler_options,\n    )\n    trainer.train()\n\n\nif __name__ == \"__main__\":\n    args = init_args()\n    assert os.path.exists(args.config_file)\n    config = Config(args.config_file)\n    config.merge_dict(args.opt)\n    main(config.cfg, args.profiler_options)\n", "benchmark/PaddleOCR_DBNet/tools/eval.py": "# -*- coding: utf-8 -*-\n# @Time    : 2018/6/11 15:54\n# @Author  : zhoujun\nimport os\nimport sys\nimport pathlib\n\n__dir__ = pathlib.Path(os.path.abspath(__file__))\nsys.path.append(str(__dir__))\nsys.path.append(str(__dir__.parent.parent))\n\nimport argparse\nimport time\nimport paddle\nfrom tqdm.auto import tqdm\n\n\nclass EVAL:\n    def __init__(self, model_path, gpu_id=0):\n        from models import build_model\n        from data_loader import get_dataloader\n        from post_processing import get_post_processing\n        from utils import get_metric\n\n        self.gpu_id = gpu_id\n        if (\n            self.gpu_id is not None\n            and isinstance(self.gpu_id, int)\n            and paddle.device.is_compiled_with_cuda()\n        ):\n            paddle.device.set_device(\"gpu:{}\".format(self.gpu_id))\n        else:\n            paddle.device.set_device(\"cpu\")\n        checkpoint = paddle.load(model_path)\n        config = checkpoint[\"config\"]\n        config[\"arch\"][\"backbone\"][\"pretrained\"] = False\n\n        self.validate_loader = get_dataloader(\n            config[\"dataset\"][\"validate\"], config[\"distributed\"]\n        )\n\n        self.model = build_model(config[\"arch\"])\n        self.model.set_state_dict(checkpoint[\"state_dict\"])\n\n        self.post_process = get_post_processing(config[\"post_processing\"])\n        self.metric_cls = get_metric(config[\"metric\"])\n\n    def eval(self):\n        self.model.eval()\n        raw_metrics = []\n        total_frame = 0.0\n        total_time = 0.0\n        for i, batch in tqdm(\n            enumerate(self.validate_loader),\n            total=len(self.validate_loader),\n            desc=\"test model\",\n        ):\n            with paddle.no_grad():\n                start = time.time()\n                preds = self.model(batch[\"img\"])\n                boxes, scores = self.post_process(\n                    batch, preds, is_output_polygon=self.metric_cls.is_output_polygon\n                )\n                total_frame += batch[\"img\"].shape[0]\n                total_time += time.time() - start\n                raw_metric = self.metric_cls.validate_measure(batch, (boxes, scores))\n                raw_metrics.append(raw_metric)\n        metrics = self.metric_cls.gather_measure(raw_metrics)\n        print(\"FPS:{}\".format(total_frame / total_time))\n        return {\n            \"recall\": metrics[\"recall\"].avg,\n            \"precision\": metrics[\"precision\"].avg,\n            \"fmeasure\": metrics[\"fmeasure\"].avg,\n        }\n\n\ndef init_args():\n    parser = argparse.ArgumentParser(description=\"DBNet.paddle\")\n    parser.add_argument(\n        \"--model_path\",\n        required=False,\n        default=\"output/DBNet_resnet18_FPN_DBHead/checkpoint/1.pth\",\n        type=str,\n    )\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == \"__main__\":\n    args = init_args()\n    eval = EVAL(args.model_path)\n    result = eval.eval()\n    print(result)\n", "benchmark/PaddleOCR_DBNet/tools/export_model.py": "import os\nimport sys\n\n__dir__ = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(__dir__)\nsys.path.insert(0, os.path.abspath(os.path.join(__dir__, \"..\")))\n\nimport argparse\n\nimport paddle\nfrom paddle.jit import to_static\n\nfrom models import build_model\nfrom utils import Config, ArgsParser\n\n\ndef init_args():\n    parser = ArgsParser()\n    args = parser.parse_args()\n    return args\n\n\ndef load_checkpoint(model, checkpoint_path):\n    \"\"\"\n    load checkpoints\n    :param checkpoint_path: Checkpoint path to be loaded\n    \"\"\"\n    checkpoint = paddle.load(checkpoint_path)\n    model.set_state_dict(checkpoint[\"state_dict\"])\n    print(\"load checkpoint from {}\".format(checkpoint_path))\n\n\ndef main(config):\n    model = build_model(config[\"arch\"])\n    load_checkpoint(model, config[\"trainer\"][\"resume_checkpoint\"])\n    model.eval()\n\n    save_path = config[\"trainer\"][\"output_dir\"]\n    save_path = os.path.join(save_path, \"inference\")\n    infer_shape = [3, -1, -1]\n    model = to_static(\n        model,\n        input_spec=[\n            paddle.static.InputSpec(shape=[None] + infer_shape, dtype=\"float32\")\n        ],\n    )\n\n    paddle.jit.save(model, save_path)\n    print(\"inference model is saved to {}\".format(save_path))\n\n\nif __name__ == \"__main__\":\n    args = init_args()\n    assert os.path.exists(args.config_file)\n    config = Config(args.config_file)\n    config.merge_dict(args.opt)\n    main(config.cfg)\n", "benchmark/PaddleOCR_DBNet/tools/predict.py": "# -*- coding: utf-8 -*-\n# @Time    : 2019/8/24 12:06\n# @Author  : zhoujun\n\nimport os\nimport sys\nimport pathlib\n\n__dir__ = pathlib.Path(os.path.abspath(__file__))\nsys.path.append(str(__dir__))\nsys.path.append(str(__dir__.parent.parent))\n\nimport time\nimport cv2\nimport paddle\n\nfrom data_loader import get_transforms\nfrom models import build_model\nfrom post_processing import get_post_processing\n\n\ndef resize_image(img, short_size):\n    height, width, _ = img.shape\n    if height < width:\n        new_height = short_size\n        new_width = new_height / height * width\n    else:\n        new_width = short_size\n        new_height = new_width / width * height\n    new_height = int(round(new_height / 32) * 32)\n    new_width = int(round(new_width / 32) * 32)\n    resized_img = cv2.resize(img, (new_width, new_height))\n    return resized_img\n\n\nclass PaddleModel:\n    def __init__(self, model_path, post_p_thre=0.7, gpu_id=None):\n        \"\"\"\n        \u521d\u59cb\u5316\u6a21\u578b\n        :param model_path: \u6a21\u578b\u5730\u5740(\u53ef\u4ee5\u662f\u6a21\u578b\u7684\u53c2\u6570\u6216\u8005\u53c2\u6570\u548c\u8ba1\u7b97\u56fe\u4e00\u8d77\u4fdd\u5b58\u7684\u6587\u4ef6)\n        :param gpu_id: \u5728\u54ea\u4e00\u5757gpu\u4e0a\u8fd0\u884c\n        \"\"\"\n        self.gpu_id = gpu_id\n\n        if (\n            self.gpu_id is not None\n            and isinstance(self.gpu_id, int)\n            and paddle.device.is_compiled_with_cuda()\n        ):\n            paddle.device.set_device(\"gpu:{}\".format(self.gpu_id))\n        else:\n            paddle.device.set_device(\"cpu\")\n        checkpoint = paddle.load(model_path)\n\n        config = checkpoint[\"config\"]\n        config[\"arch\"][\"backbone\"][\"pretrained\"] = False\n        self.model = build_model(config[\"arch\"])\n        self.post_process = get_post_processing(config[\"post_processing\"])\n        self.post_process.box_thresh = post_p_thre\n        self.img_mode = config[\"dataset\"][\"train\"][\"dataset\"][\"args\"][\"img_mode\"]\n        self.model.set_state_dict(checkpoint[\"state_dict\"])\n        self.model.eval()\n\n        self.transform = []\n        for t in config[\"dataset\"][\"train\"][\"dataset\"][\"args\"][\"transforms\"]:\n            if t[\"type\"] in [\"ToTensor\", \"Normalize\"]:\n                self.transform.append(t)\n        self.transform = get_transforms(self.transform)\n\n    def predict(self, img_path: str, is_output_polygon=False, short_size: int = 1024):\n        \"\"\"\n        \u5bf9\u4f20\u5165\u7684\u56fe\u50cf\u8fdb\u884c\u9884\u6d4b\uff0c\u652f\u6301\u56fe\u50cf\u5730\u5740,opecv \u8bfb\u53d6\u56fe\u7247\uff0c\u504f\u6162\n        :param img_path: \u56fe\u50cf\u5730\u5740\n        :param is_numpy:\n        :return:\n        \"\"\"\n        assert os.path.exists(img_path), \"file is not exists\"\n        img = cv2.imread(img_path, 1 if self.img_mode != \"GRAY\" else 0)\n        if self.img_mode == \"RGB\":\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        h, w = img.shape[:2]\n        img = resize_image(img, short_size)\n        # \u5c06\u56fe\u7247\u7531(w,h)\u53d8\u4e3a(1,img_channel,h,w)\n        tensor = self.transform(img)\n        tensor = tensor.unsqueeze_(0)\n\n        batch = {\"shape\": [(h, w)]}\n        with paddle.no_grad():\n            start = time.time()\n            preds = self.model(tensor)\n            box_list, score_list = self.post_process(\n                batch, preds, is_output_polygon=is_output_polygon\n            )\n            box_list, score_list = box_list[0], score_list[0]\n            if len(box_list) > 0:\n                if is_output_polygon:\n                    idx = [x.sum() > 0 for x in box_list]\n                    box_list = [box_list[i] for i, v in enumerate(idx) if v]\n                    score_list = [score_list[i] for i, v in enumerate(idx) if v]\n                else:\n                    idx = (\n                        box_list.reshape(box_list.shape[0], -1).sum(axis=1) > 0\n                    )  # \u53bb\u6389\u5168\u4e3a0\u7684\u6846\n                    box_list, score_list = box_list[idx], score_list[idx]\n            else:\n                box_list, score_list = [], []\n            t = time.time() - start\n        return preds[0, 0, :, :].detach().cpu().numpy(), box_list, score_list, t\n\n\ndef save_depoly(net, input, save_path):\n    input_spec = [paddle.static.InputSpec(shape=[None, 3, None, None], dtype=\"float32\")]\n    net = paddle.jit.to_static(net, input_spec=input_spec)\n\n    # save static model for inference directly\n    paddle.jit.save(net, save_path)\n\n\ndef init_args():\n    import argparse\n\n    parser = argparse.ArgumentParser(description=\"DBNet.paddle\")\n    parser.add_argument(\"--model_path\", default=r\"model_best.pth\", type=str)\n    parser.add_argument(\n        \"--input_folder\", default=\"./test/input\", type=str, help=\"img path for predict\"\n    )\n    parser.add_argument(\n        \"--output_folder\", default=\"./test/output\", type=str, help=\"img path for output\"\n    )\n    parser.add_argument(\"--gpu\", default=0, type=int, help=\"gpu for inference\")\n    parser.add_argument(\n        \"--thre\", default=0.3, type=float, help=\"the thresh of post_processing\"\n    )\n    parser.add_argument(\"--polygon\", action=\"store_true\", help=\"output polygon or box\")\n    parser.add_argument(\"--show\", action=\"store_true\", help=\"show result\")\n    parser.add_argument(\n        \"--save_result\", action=\"store_true\", help=\"save box and score to txt file\"\n    )\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == \"__main__\":\n    import pathlib\n    from tqdm import tqdm\n    import matplotlib.pyplot as plt\n    from utils.util import show_img, draw_bbox, save_result, get_image_file_list\n\n    args = init_args()\n    print(args)\n    # \u521d\u59cb\u5316\u7f51\u7edc\n    model = PaddleModel(args.model_path, post_p_thre=args.thre, gpu_id=args.gpu)\n    img_folder = pathlib.Path(args.input_folder)\n    for img_path in tqdm(get_image_file_list(args.input_folder)):\n        preds, boxes_list, score_list, t = model.predict(\n            img_path, is_output_polygon=args.polygon\n        )\n        img = draw_bbox(cv2.imread(img_path)[:, :, ::-1], boxes_list)\n        if args.show:\n            show_img(preds)\n            show_img(img, title=os.path.basename(img_path))\n            plt.show()\n        # \u4fdd\u5b58\u7ed3\u679c\u5230\u8def\u5f84\n        os.makedirs(args.output_folder, exist_ok=True)\n        img_path = pathlib.Path(img_path)\n        output_path = os.path.join(args.output_folder, img_path.stem + \"_result.jpg\")\n        pred_path = os.path.join(args.output_folder, img_path.stem + \"_pred.jpg\")\n        cv2.imwrite(output_path, img[:, :, ::-1])\n        cv2.imwrite(pred_path, preds * 255)\n        save_result(\n            output_path.replace(\"_result.jpg\", \".txt\"),\n            boxes_list,\n            score_list,\n            args.polygon,\n        )\n", "benchmark/PaddleOCR_DBNet/tools/infer.py": "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport sys\nimport pathlib\n\n__dir__ = pathlib.Path(os.path.abspath(__file__))\nsys.path.append(str(__dir__))\nsys.path.append(str(__dir__.parent.parent))\n\nimport cv2\nimport paddle\nfrom paddle import inference\nimport numpy as np\nfrom PIL import Image\n\nfrom paddle.vision import transforms\nfrom tools.predict import resize_image\nfrom post_processing import get_post_processing\nfrom utils.util import draw_bbox, save_result\n\n\nclass InferenceEngine(object):\n    \"\"\"InferenceEngine\n\n    Inference engina class which contains preprocess, run, postprocess\n    \"\"\"\n\n    def __init__(self, args):\n        \"\"\"\n        Args:\n            args: Parameters generated using argparser.\n        Returns: None\n        \"\"\"\n        super().__init__()\n        self.args = args\n\n        # init inference engine\n        (\n            self.predictor,\n            self.config,\n            self.input_tensor,\n            self.output_tensor,\n        ) = self.load_predictor(\n            os.path.join(args.model_dir, \"inference.pdmodel\"),\n            os.path.join(args.model_dir, \"inference.pdiparams\"),\n        )\n\n        # build transforms\n        self.transforms = transforms.Compose(\n            [\n                transforms.ToTensor(),\n                transforms.Normalize(\n                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n                ),\n            ]\n        )\n\n        # wamrup\n        if self.args.warmup > 0:\n            for idx in range(args.warmup):\n                print(idx)\n                x = np.random.rand(\n                    1, 3, self.args.crop_size, self.args.crop_size\n                ).astype(\"float32\")\n                self.input_tensor.copy_from_cpu(x)\n                self.predictor.run()\n                self.output_tensor.copy_to_cpu()\n\n        self.post_process = get_post_processing(\n            {\n                \"type\": \"SegDetectorRepresenter\",\n                \"args\": {\n                    \"thresh\": 0.3,\n                    \"box_thresh\": 0.7,\n                    \"max_candidates\": 1000,\n                    \"unclip_ratio\": 1.5,\n                },\n            }\n        )\n\n    def load_predictor(self, model_file_path, params_file_path):\n        \"\"\"load_predictor\n        initialize the inference engine\n        Args:\n            model_file_path: inference model path (*.pdmodel)\n            model_file_path: inference parmaeter path (*.pdiparams)\n        Return:\n            predictor: Predictor created using Paddle Inference.\n            config: Configuration of the predictor.\n            input_tensor: Input tensor of the predictor.\n            output_tensor: Output tensor of the predictor.\n        \"\"\"\n        args = self.args\n        config = inference.Config(model_file_path, params_file_path)\n        if args.use_gpu:\n            config.enable_use_gpu(1000, 0)\n            if args.use_tensorrt:\n                config.enable_tensorrt_engine(\n                    workspace_size=1 << 30,\n                    precision_mode=precision,\n                    max_batch_size=args.max_batch_size,\n                    min_subgraph_size=args.min_subgraph_size,  # skip the minmum trt subgraph\n                    use_calib_mode=False,\n                )\n\n                # collect shape\n                trt_shape_f = os.path.join(model_dir, \"_trt_dynamic_shape.txt\")\n\n                if not os.path.exists(trt_shape_f):\n                    config.collect_shape_range_info(trt_shape_f)\n                    logger.info(f\"collect dynamic shape info into : {trt_shape_f}\")\n                try:\n                    config.enable_tuned_tensorrt_dynamic_shape(trt_shape_f, True)\n                except Exception as E:\n                    logger.info(E)\n                    logger.info(\"Please keep your paddlepaddle-gpu >= 2.3.0!\")\n        else:\n            config.disable_gpu()\n            # The thread num should not be greater than the number of cores in the CPU.\n            if args.enable_mkldnn:\n                # cache 10 different shapes for mkldnn to avoid memory leak\n                config.set_mkldnn_cache_capacity(10)\n                config.enable_mkldnn()\n                if args.precision == \"fp16\":\n                    config.enable_mkldnn_bfloat16()\n                if hasattr(args, \"cpu_threads\"):\n                    config.set_cpu_math_library_num_threads(args.cpu_threads)\n                else:\n                    # default cpu threads as 10\n                    config.set_cpu_math_library_num_threads(10)\n\n        # enable memory optim\n        config.enable_memory_optim()\n        config.disable_glog_info()\n\n        config.switch_use_feed_fetch_ops(False)\n        config.switch_ir_optim(True)\n\n        # create predictor\n        predictor = inference.create_predictor(config)\n\n        # get input and output tensor property\n        input_names = predictor.get_input_names()\n        input_tensor = predictor.get_input_handle(input_names[0])\n\n        output_names = predictor.get_output_names()\n        output_tensor = predictor.get_output_handle(output_names[0])\n\n        return predictor, config, input_tensor, output_tensor\n\n    def preprocess(self, img_path, short_size):\n        \"\"\"preprocess\n        Preprocess to the input.\n        Args:\n            img_path: Image path.\n        Returns: Input data after preprocess.\n        \"\"\"\n        img = cv2.imread(img_path, 1)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        h, w = img.shape[:2]\n        img = resize_image(img, short_size)\n        img = self.transforms(img)\n        img = np.expand_dims(img, axis=0)\n        shape_info = {\"shape\": [(h, w)]}\n        return img, shape_info\n\n    def postprocess(self, x, shape_info, is_output_polygon):\n        \"\"\"postprocess\n        Postprocess to the inference engine output.\n        Args:\n            x: Inference engine output.\n        Returns: Output data after argmax.\n        \"\"\"\n        box_list, score_list = self.post_process(\n            shape_info, x, is_output_polygon=is_output_polygon\n        )\n        box_list, score_list = box_list[0], score_list[0]\n        if len(box_list) > 0:\n            if is_output_polygon:\n                idx = [x.sum() > 0 for x in box_list]\n                box_list = [box_list[i] for i, v in enumerate(idx) if v]\n                score_list = [score_list[i] for i, v in enumerate(idx) if v]\n            else:\n                idx = (\n                    box_list.reshape(box_list.shape[0], -1).sum(axis=1) > 0\n                )  # \u53bb\u6389\u5168\u4e3a0\u7684\u6846\n                box_list, score_list = box_list[idx], score_list[idx]\n        else:\n            box_list, score_list = [], []\n        return box_list, score_list\n\n    def run(self, x):\n        \"\"\"run\n        Inference process using inference engine.\n        Args:\n            x: Input data after preprocess.\n        Returns: Inference engine output\n        \"\"\"\n        self.input_tensor.copy_from_cpu(x)\n        self.predictor.run()\n        output = self.output_tensor.copy_to_cpu()\n        return output\n\n\ndef get_args(add_help=True):\n    \"\"\"\n    parse args\n    \"\"\"\n    import argparse\n\n    def str2bool(v):\n        return v.lower() in (\"true\", \"t\", \"1\")\n\n    parser = argparse.ArgumentParser(\n        description=\"PaddlePaddle Classification Training\", add_help=add_help\n    )\n\n    parser.add_argument(\"--model_dir\", default=None, help=\"inference model dir\")\n    parser.add_argument(\"--batch_size\", type=int, default=1)\n    parser.add_argument(\"--short_size\", default=1024, type=int, help=\"short size\")\n    parser.add_argument(\"--img_path\", default=\"./images/demo.jpg\")\n\n    parser.add_argument(\"--benchmark\", default=False, type=str2bool, help=\"benchmark\")\n    parser.add_argument(\"--warmup\", default=0, type=int, help=\"warmup iter\")\n    parser.add_argument(\"--polygon\", action=\"store_true\", help=\"output polygon or box\")\n\n    parser.add_argument(\"--use_gpu\", type=str2bool, default=True)\n    parser.add_argument(\"--use_tensorrt\", type=str2bool, default=False)\n    parser.add_argument(\"--precision\", type=str, default=\"fp32\")\n    parser.add_argument(\"--gpu_mem\", type=int, default=500)\n    parser.add_argument(\"--gpu_id\", type=int, default=0)\n    parser.add_argument(\"--enable_mkldnn\", type=str2bool, default=False)\n    parser.add_argument(\"--cpu_threads\", type=int, default=10)\n\n    args = parser.parse_args()\n    return args\n\n\ndef main(args):\n    \"\"\"\n    Main inference function.\n    Args:\n        args: Parameters generated using argparser.\n    Returns:\n        class_id: Class index of the input.\n        prob: : Probability of the input.\n    \"\"\"\n    inference_engine = InferenceEngine(args)\n\n    # init benchmark\n    if args.benchmark:\n        import auto_log\n\n        autolog = auto_log.AutoLogger(\n            model_name=\"db\",\n            batch_size=args.batch_size,\n            inference_config=inference_engine.config,\n            gpu_ids=\"auto\" if args.use_gpu else None,\n        )\n\n    # enable benchmark\n    if args.benchmark:\n        autolog.times.start()\n\n    # preprocess\n    img, shape_info = inference_engine.preprocess(args.img_path, args.short_size)\n\n    if args.benchmark:\n        autolog.times.stamp()\n\n    output = inference_engine.run(img)\n\n    if args.benchmark:\n        autolog.times.stamp()\n\n    # postprocess\n    box_list, score_list = inference_engine.postprocess(\n        output, shape_info, args.polygon\n    )\n\n    if args.benchmark:\n        autolog.times.stamp()\n        autolog.times.end(stamp=True)\n        autolog.report()\n\n    img = draw_bbox(cv2.imread(args.img_path)[:, :, ::-1], box_list)\n    # \u4fdd\u5b58\u7ed3\u679c\u5230\u8def\u5f84\n    os.makedirs(\"output\", exist_ok=True)\n    img_path = pathlib.Path(args.img_path)\n    output_path = os.path.join(\"output\", img_path.stem + \"_infer_result.jpg\")\n    cv2.imwrite(output_path, img[:, :, ::-1])\n    save_result(\n        output_path.replace(\"_infer_result.jpg\", \".txt\"),\n        box_list,\n        score_list,\n        args.polygon,\n    )\n\n\nif __name__ == \"__main__\":\n    args = get_args()\n    main(args)\n", "benchmark/PaddleOCR_DBNet/tools/__init__.py": "# -*- coding: utf-8 -*-\n# @Time    : 2019/12/8 13:14\n# @Author  : zhoujun\n", "benchmark/PaddleOCR_DBNet/utils/compute_mean_std.py": "# -*- coding: utf-8 -*-\n# @Time    : 2019/12/7 14:46\n# @Author  : zhoujun\n\nimport numpy as np\nimport cv2\nimport os\nimport random\nfrom tqdm import tqdm\n\n# calculate means and std\ntrain_txt_path = \"./train_val_list.txt\"\n\nCNum = 10000  # \u6311\u9009\u591a\u5c11\u56fe\u7247\u8fdb\u884c\u8ba1\u7b97\n\nimg_h, img_w = 640, 640\nimgs = np.zeros([img_w, img_h, 3, 1])\nmeans, stdevs = [], []\n\nwith open(train_txt_path, \"r\") as f:\n    lines = f.readlines()\n    random.shuffle(lines)  # shuffle , \u968f\u673a\u6311\u9009\u56fe\u7247\n\n    for i in tqdm(range(CNum)):\n        img_path = lines[i].split(\"\\t\")[0]\n\n        img = cv2.imread(img_path)\n        img = cv2.resize(img, (img_h, img_w))\n        img = img[:, :, :, np.newaxis]\n\n        imgs = np.concatenate((imgs, img), axis=3)\n#         print(i)\n\nimgs = imgs.astype(np.float32) / 255.0\n\nfor i in tqdm(range(3)):\n    pixels = imgs[:, :, i, :].ravel()  # \u62c9\u6210\u4e00\u884c\n    means.append(np.mean(pixels))\n    stdevs.append(np.std(pixels))\n\n# cv2 \u8bfb\u53d6\u7684\u56fe\u50cf\u683c\u5f0f\u4e3aBGR\uff0cPIL/Skimage\u8bfb\u53d6\u5230\u7684\u90fd\u662fRGB\u4e0d\u7528\u8f6c\nmeans.reverse()  # BGR --> RGB\nstdevs.reverse()\n\nprint(\"normMean = {}\".format(means))\nprint(\"normStd = {}\".format(stdevs))\nprint(\"transforms.Normalize(normMean = {}, normStd = {})\".format(means, stdevs))\n", "benchmark/PaddleOCR_DBNet/utils/util.py": "# -*- coding: utf-8 -*-\n# @Time    : 2019/8/23 21:59\n# @Author  : zhoujun\nimport json\nimport pathlib\nimport time\nimport os\nimport glob\nimport cv2\nimport yaml\nfrom typing import Mapping\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom argparse import ArgumentParser, RawDescriptionHelpFormatter\n\n\ndef _check_image_file(path):\n    img_end = {\"jpg\", \"bmp\", \"png\", \"jpeg\", \"rgb\", \"tif\", \"tiff\", \"gif\", \"pdf\"}\n    return any([path.lower().endswith(e) for e in img_end])\n\n\ndef get_image_file_list(img_file):\n    imgs_lists = []\n    if img_file is None or not os.path.exists(img_file):\n        raise Exception(\"not found any img file in {}\".format(img_file))\n\n    img_end = {\"jpg\", \"bmp\", \"png\", \"jpeg\", \"rgb\", \"tif\", \"tiff\", \"gif\", \"pdf\"}\n    if os.path.isfile(img_file) and _check_image_file(img_file):\n        imgs_lists.append(img_file)\n    elif os.path.isdir(img_file):\n        for single_file in os.listdir(img_file):\n            file_path = os.path.join(img_file, single_file)\n            if os.path.isfile(file_path) and _check_image_file(file_path):\n                imgs_lists.append(file_path)\n    if len(imgs_lists) == 0:\n        raise Exception(\"not found any img file in {}\".format(img_file))\n    imgs_lists = sorted(imgs_lists)\n    return imgs_lists\n\n\ndef setup_logger(log_file_path: str = None):\n    import logging\n\n    logging._warn_preinit_stderr = 0\n    logger = logging.getLogger(\"DBNet.paddle\")\n    formatter = logging.Formatter(\"%(asctime)s %(name)s %(levelname)s: %(message)s\")\n    ch = logging.StreamHandler()\n    ch.setFormatter(formatter)\n    logger.addHandler(ch)\n    if log_file_path is not None:\n        file_handle = logging.FileHandler(log_file_path)\n        file_handle.setFormatter(formatter)\n        logger.addHandler(file_handle)\n    logger.setLevel(logging.DEBUG)\n    return logger\n\n\n# --exeTime\ndef exe_time(func):\n    def newFunc(*args, **args2):\n        t0 = time.time()\n        back = func(*args, **args2)\n        print(\"{} cost {:.3f}s\".format(func.__name__, time.time() - t0))\n        return back\n\n    return newFunc\n\n\ndef load(file_path: str):\n    file_path = pathlib.Path(file_path)\n    func_dict = {\".txt\": _load_txt, \".json\": _load_json, \".list\": _load_txt}\n    assert file_path.suffix in func_dict\n    return func_dict[file_path.suffix](file_path)\n\n\ndef _load_txt(file_path: str):\n    with open(file_path, \"r\", encoding=\"utf8\") as f:\n        content = [\n            x.strip().strip(\"\\ufeff\").strip(\"\\xef\\xbb\\xbf\") for x in f.readlines()\n        ]\n    return content\n\n\ndef _load_json(file_path: str):\n    with open(file_path, \"r\", encoding=\"utf8\") as f:\n        content = json.load(f)\n    return content\n\n\ndef save(data, file_path):\n    file_path = pathlib.Path(file_path)\n    func_dict = {\".txt\": _save_txt, \".json\": _save_json}\n    assert file_path.suffix in func_dict\n    return func_dict[file_path.suffix](data, file_path)\n\n\ndef _save_txt(data, file_path):\n    \"\"\"\n    \u5c06\u4e00\u4e2alist\u7684\u6570\u7ec4\u5199\u5165txt\u6587\u4ef6\u91cc\n    :param data:\n    :param file_path:\n    :return:\n    \"\"\"\n    if not isinstance(data, list):\n        data = [data]\n    with open(file_path, mode=\"w\", encoding=\"utf8\") as f:\n        f.write(\"\\n\".join(data))\n\n\ndef _save_json(data, file_path):\n    with open(file_path, \"w\", encoding=\"utf-8\") as json_file:\n        json.dump(data, json_file, ensure_ascii=False, indent=4)\n\n\ndef show_img(imgs: np.ndarray, title=\"img\"):\n    color = len(imgs.shape) == 3 and imgs.shape[-1] == 3\n    imgs = np.expand_dims(imgs, axis=0)\n    for i, img in enumerate(imgs):\n        plt.figure()\n        plt.title(\"{}_{}\".format(title, i))\n        plt.imshow(img, cmap=None if color else \"gray\")\n    plt.show()\n\n\ndef draw_bbox(img_path, result, color=(255, 0, 0), thickness=2):\n    if isinstance(img_path, str):\n        img_path = cv2.imread(img_path)\n        # img_path = cv2.cvtColor(img_path, cv2.COLOR_BGR2RGB)\n    img_path = img_path.copy()\n    for point in result:\n        point = point.astype(int)\n        cv2.polylines(img_path, [point], True, color, thickness)\n    return img_path\n\n\ndef cal_text_score(texts, gt_texts, training_masks, running_metric_text, thred=0.5):\n    training_masks = training_masks.numpy()\n    pred_text = texts.numpy() * training_masks\n    pred_text[pred_text <= thred] = 0\n    pred_text[pred_text > thred] = 1\n    pred_text = pred_text.astype(np.int32)\n    gt_text = gt_texts.numpy() * training_masks\n    gt_text = gt_text.astype(np.int32)\n    running_metric_text.update(gt_text, pred_text)\n    score_text, _ = running_metric_text.get_scores()\n    return score_text\n\n\ndef order_points_clockwise(pts):\n    rect = np.zeros((4, 2), dtype=\"float32\")\n    s = pts.sum(axis=1)\n    rect[0] = pts[np.argmin(s)]\n    rect[2] = pts[np.argmax(s)]\n    diff = np.diff(pts, axis=1)\n    rect[1] = pts[np.argmin(diff)]\n    rect[3] = pts[np.argmax(diff)]\n    return rect\n\n\ndef order_points_clockwise_list(pts):\n    pts = pts.tolist()\n    pts.sort(key=lambda x: (x[1], x[0]))\n    pts[:2] = sorted(pts[:2], key=lambda x: x[0])\n    pts[2:] = sorted(pts[2:], key=lambda x: -x[0])\n    pts = np.array(pts)\n    return pts\n\n\ndef get_datalist(train_data_path):\n    \"\"\"\n    \u83b7\u53d6\u8bad\u7ec3\u548c\u9a8c\u8bc1\u7684\u6570\u636elist\n    :param train_data_path: \u8bad\u7ec3\u7684dataset\u6587\u4ef6\u5217\u8868\uff0c\u6bcf\u4e2a\u6587\u4ef6\u5185\u4ee5\u5982\u4e0b\u683c\u5f0f\u5b58\u50a8 \u2018path/to/img\\tlabel\u2019\n    :return:\n    \"\"\"\n    train_data = []\n    for p in train_data_path:\n        with open(p, \"r\", encoding=\"utf-8\") as f:\n            for line in f.readlines():\n                line = line.strip(\"\\n\").replace(\".jpg \", \".jpg\\t\").split(\"\\t\")\n                if len(line) > 1:\n                    img_path = pathlib.Path(line[0].strip(\" \"))\n                    label_path = pathlib.Path(line[1].strip(\" \"))\n                    if (\n                        img_path.exists()\n                        and img_path.stat().st_size > 0\n                        and label_path.exists()\n                        and label_path.stat().st_size > 0\n                    ):\n                        train_data.append((str(img_path), str(label_path)))\n    return train_data\n\n\ndef save_result(result_path, box_list, score_list, is_output_polygon):\n    if is_output_polygon:\n        with open(result_path, \"wt\") as res:\n            for i, box in enumerate(box_list):\n                box = box.reshape(-1).tolist()\n                result = \",\".join([str(int(x)) for x in box])\n                score = score_list[i]\n                res.write(result + \",\" + str(score) + \"\\n\")\n    else:\n        with open(result_path, \"wt\") as res:\n            for i, box in enumerate(box_list):\n                score = score_list[i]\n                box = box.reshape(-1).tolist()\n                result = \",\".join([str(int(x)) for x in box])\n                res.write(result + \",\" + str(score) + \"\\n\")\n\n\ndef expand_polygon(polygon):\n    \"\"\"\n    \u5bf9\u53ea\u6709\u4e00\u4e2a\u5b57\u7b26\u7684\u6846\u8fdb\u884c\u6269\u5145\n    \"\"\"\n    (x, y), (w, h), angle = cv2.minAreaRect(np.float32(polygon))\n    if angle < -45:\n        w, h = h, w\n        angle += 90\n    new_w = w + h\n    box = ((x, y), (new_w, h), angle)\n    points = cv2.boxPoints(box)\n    return order_points_clockwise(points)\n\n\ndef _merge_dict(config, merge_dct):\n    \"\"\"Recursive dict merge. Inspired by :meth:``dict.update()``, instead of\n    updating only top-level keys, dict_merge recurses down into dicts nested\n    to an arbitrary depth, updating keys. The ``merge_dct`` is merged into\n    ``dct``.\n    Args:\n        config: dict onto which the merge is executed\n        merge_dct: dct merged into config\n    Returns: dct\n    \"\"\"\n    for key, value in merge_dct.items():\n        sub_keys = key.split(\".\")\n        key = sub_keys[0]\n        if key in config and len(sub_keys) > 1:\n            _merge_dict(config[key], {\".\".join(sub_keys[1:]): value})\n        elif (\n            key in config\n            and isinstance(config[key], dict)\n            and isinstance(value, Mapping)\n        ):\n            _merge_dict(config[key], value)\n        else:\n            config[key] = value\n    return config\n\n\ndef print_dict(cfg, print_func=print, delimiter=0):\n    \"\"\"\n    Recursively visualize a dict and\n    indenting acrrording by the relationship of keys.\n    \"\"\"\n    for k, v in sorted(cfg.items()):\n        if isinstance(v, dict):\n            print_func(\"{}{} : \".format(delimiter * \" \", str(k)))\n            print_dict(v, print_func, delimiter + 4)\n        elif isinstance(v, list) and len(v) >= 1 and isinstance(v[0], dict):\n            print_func(\"{}{} : \".format(delimiter * \" \", str(k)))\n            for value in v:\n                print_dict(value, print_func, delimiter + 4)\n        else:\n            print_func(\"{}{} : {}\".format(delimiter * \" \", k, v))\n\n\nclass Config(object):\n    def __init__(self, config_path, BASE_KEY=\"base\"):\n        self.BASE_KEY = BASE_KEY\n        self.cfg = self._load_config_with_base(config_path)\n\n    def _load_config_with_base(self, file_path):\n        \"\"\"\n        Load config from file.\n        Args:\n            file_path (str): Path of the config file to be loaded.\n        Returns: global config\n        \"\"\"\n        _, ext = os.path.splitext(file_path)\n        assert ext in [\".yml\", \".yaml\"], \"only support yaml files for now\"\n\n        with open(file_path) as f:\n            file_cfg = yaml.load(f, Loader=yaml.Loader)\n\n        # NOTE: cfgs outside have higher priority than cfgs in _BASE_\n        if self.BASE_KEY in file_cfg:\n            all_base_cfg = dict()\n            base_ymls = list(file_cfg[self.BASE_KEY])\n            for base_yml in base_ymls:\n                with open(base_yml) as f:\n                    base_cfg = self._load_config_with_base(base_yml)\n                    all_base_cfg = _merge_dict(all_base_cfg, base_cfg)\n\n            del file_cfg[self.BASE_KEY]\n            file_cfg = _merge_dict(all_base_cfg, file_cfg)\n        file_cfg[\"filename\"] = os.path.splitext(os.path.split(file_path)[-1])[0]\n        return file_cfg\n\n    def merge_dict(self, args):\n        self.cfg = _merge_dict(self.cfg, args)\n\n    def print_cfg(self, print_func=print):\n        \"\"\"\n        Recursively visualize a dict and\n        indenting acrrording by the relationship of keys.\n        \"\"\"\n        print_func(\"----------- Config -----------\")\n        print_dict(self.cfg, print_func)\n        print_func(\"---------------------------------------------\")\n\n    def save(self, p):\n        with open(p, \"w\") as f:\n            yaml.dump(dict(self.cfg), f, default_flow_style=False, sort_keys=False)\n\n\nclass ArgsParser(ArgumentParser):\n    def __init__(self):\n        super(ArgsParser, self).__init__(formatter_class=RawDescriptionHelpFormatter)\n        self.add_argument(\"-c\", \"--config_file\", help=\"configuration file to use\")\n        self.add_argument(\"-o\", \"--opt\", nargs=\"*\", help=\"set configuration options\")\n        self.add_argument(\n            \"-p\",\n            \"--profiler_options\",\n            type=str,\n            default=None,\n            help=\"The option of profiler, which should be in format \"\n            '\"key1=value1;key2=value2;key3=value3\".',\n        )\n\n    def parse_args(self, argv=None):\n        args = super(ArgsParser, self).parse_args(argv)\n        assert (\n            args.config_file is not None\n        ), \"Please specify --config_file=configure_file_path.\"\n        args.opt = self._parse_opt(args.opt)\n        return args\n\n    def _parse_opt(self, opts):\n        config = {}\n        if not opts:\n            return config\n        for s in opts:\n            s = s.strip()\n            k, v = s.split(\"=\", 1)\n            if \".\" not in k:\n                config[k] = yaml.load(v, Loader=yaml.Loader)\n            else:\n                keys = k.split(\".\")\n                if keys[0] not in config:\n                    config[keys[0]] = {}\n                cur = config[keys[0]]\n                for idx, key in enumerate(keys[1:]):\n                    if idx == len(keys) - 2:\n                        cur[key] = yaml.load(v, Loader=yaml.Loader)\n                    else:\n                        cur[key] = {}\n                        cur = cur[key]\n        return config\n\n\nif __name__ == \"__main__\":\n    img = np.zeros((1, 3, 640, 640))\n    show_img(img[0][0])\n    plt.show()\n", "benchmark/PaddleOCR_DBNet/utils/profiler.py": "# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\nimport paddle\n\n# A global variable to record the number of calling times for profiler\n# functions. It is used to specify the tracing range of training steps.\n_profiler_step_id = 0\n\n# A global variable to avoid parsing from string every time.\n_profiler_options = None\n\n\nclass ProfilerOptions(object):\n    \"\"\"\n    Use a string to initialize a ProfilerOptions.\n    The string should be in the format: \"key1=value1;key2=value;key3=value3\".\n    For example:\n      \"profile_path=model.profile\"\n      \"batch_range=[50, 60]; profile_path=model.profile\"\n      \"batch_range=[50, 60]; tracer_option=OpDetail; profile_path=model.profile\"\n    ProfilerOptions supports following key-value pair:\n      batch_range      - a integer list, e.g. [100, 110].\n      state            - a string, the optional values are 'CPU', 'GPU' or 'All'.\n      sorted_key       - a string, the optional values are 'calls', 'total',\n                         'max', 'min' or 'ave.\n      tracer_option    - a string, the optional values are 'Default', 'OpDetail',\n                         'AllOpDetail'.\n      profile_path     - a string, the path to save the serialized profile data,\n                         which can be used to generate a timeline.\n      exit_on_finished - a boolean.\n    \"\"\"\n\n    def __init__(self, options_str):\n        assert isinstance(options_str, str)\n\n        self._options = {\n            \"batch_range\": [10, 20],\n            \"state\": \"All\",\n            \"sorted_key\": \"total\",\n            \"tracer_option\": \"Default\",\n            \"profile_path\": \"/tmp/profile\",\n            \"exit_on_finished\": True,\n        }\n        self._parse_from_string(options_str)\n\n    def _parse_from_string(self, options_str):\n        for kv in options_str.replace(\" \", \"\").split(\";\"):\n            key, value = kv.split(\"=\")\n            if key == \"batch_range\":\n                value_list = value.replace(\"[\", \"\").replace(\"]\", \"\").split(\",\")\n                value_list = list(map(int, value_list))\n                if (\n                    len(value_list) >= 2\n                    and value_list[0] >= 0\n                    and value_list[1] > value_list[0]\n                ):\n                    self._options[key] = value_list\n            elif key == \"exit_on_finished\":\n                self._options[key] = value.lower() in (\"yes\", \"true\", \"t\", \"1\")\n            elif key in [\"state\", \"sorted_key\", \"tracer_option\", \"profile_path\"]:\n                self._options[key] = value\n\n    def __getitem__(self, name):\n        if self._options.get(name, None) is None:\n            raise ValueError(\"ProfilerOptions does not have an option named %s.\" % name)\n        return self._options[name]\n\n\ndef add_profiler_step(options_str=None):\n    \"\"\"\n    Enable the operator-level timing using PaddlePaddle's profiler.\n    The profiler uses a independent variable to count the profiler steps.\n    One call of this function is treated as a profiler step.\n\n    Args:\n      profiler_options - a string to initialize the ProfilerOptions.\n                         Default is None, and the profiler is disabled.\n    \"\"\"\n    if options_str is None:\n        return\n\n    global _profiler_step_id\n    global _profiler_options\n\n    if _profiler_options is None:\n        _profiler_options = ProfilerOptions(options_str)\n\n    if _profiler_step_id == _profiler_options[\"batch_range\"][0]:\n        paddle.utils.profiler.start_profiler(\n            _profiler_options[\"state\"], _profiler_options[\"tracer_option\"]\n        )\n    elif _profiler_step_id == _profiler_options[\"batch_range\"][1]:\n        paddle.utils.profiler.stop_profiler(\n            _profiler_options[\"sorted_key\"], _profiler_options[\"profile_path\"]\n        )\n        if _profiler_options[\"exit_on_finished\"]:\n            sys.exit(0)\n\n    _profiler_step_id += 1\n", "benchmark/PaddleOCR_DBNet/utils/metrics.py": "# Adapted from score written by wkentaro\n# https://github.com/wkentaro/pytorch-fcn/blob/master/torchfcn/utils.py\n\nimport numpy as np\n\n\nclass runningScore(object):\n    def __init__(self, n_classes):\n        self.n_classes = n_classes\n        self.confusion_matrix = np.zeros((n_classes, n_classes))\n\n    def _fast_hist(self, label_true, label_pred, n_class):\n        mask = (label_true >= 0) & (label_true < n_class)\n\n        if np.sum((label_pred[mask] < 0)) > 0:\n            print(label_pred[label_pred < 0])\n        hist = np.bincount(\n            n_class * label_true[mask].astype(int) + label_pred[mask],\n            minlength=n_class**2,\n        ).reshape(n_class, n_class)\n        return hist\n\n    def update(self, label_trues, label_preds):\n        # print label_trues.dtype, label_preds.dtype\n        for lt, lp in zip(label_trues, label_preds):\n            try:\n                self.confusion_matrix += self._fast_hist(\n                    lt.flatten(), lp.flatten(), self.n_classes\n                )\n            except:\n                pass\n\n    def get_scores(self):\n        \"\"\"Returns accuracy score evaluation result.\n        - overall accuracy\n        - mean accuracy\n        - mean IU\n        - fwavacc\n        \"\"\"\n        hist = self.confusion_matrix\n        acc = np.diag(hist).sum() / (hist.sum() + 0.0001)\n        acc_cls = np.diag(hist) / (hist.sum(axis=1) + 0.0001)\n        acc_cls = np.nanmean(acc_cls)\n        iu = np.diag(hist) / (\n            hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist) + 0.0001\n        )\n        mean_iu = np.nanmean(iu)\n        freq = hist.sum(axis=1) / (hist.sum() + 0.0001)\n        fwavacc = (freq[freq > 0] * iu[freq > 0]).sum()\n        cls_iu = dict(zip(range(self.n_classes), iu))\n\n        return {\n            \"Overall Acc\": acc,\n            \"Mean Acc\": acc_cls,\n            \"FreqW Acc\": fwavacc,\n            \"Mean IoU\": mean_iu,\n        }, cls_iu\n\n    def reset(self):\n        self.confusion_matrix = np.zeros((self.n_classes, self.n_classes))\n", "benchmark/PaddleOCR_DBNet/utils/__init__.py": "# -*- coding: utf-8 -*-\n# @Time    : 2019/8/23 21:58\n# @Author  : zhoujun\nfrom .util import *\nfrom .metrics import *\nfrom .schedulers import *\nfrom .cal_recall.script import cal_recall_precison_f1\nfrom .ocr_metric import get_metric\n", "benchmark/PaddleOCR_DBNet/utils/schedulers.py": "from paddle.optimizer import lr\nimport logging\n\n__all__ = [\"Polynomial\"]\n\n\nclass Polynomial(object):\n    \"\"\"\n    Polynomial learning rate decay\n    Args:\n        learning_rate (float): The initial learning rate. It is a python float number.\n        epochs(int): The decay epoch size. It determines the decay cycle, when by_epoch is set to true, it will change to epochs=epochs*step_each_epoch.\n        step_each_epoch: all steps in each epoch.\n        end_lr(float, optional): The minimum final learning rate. Default: 0.0001.\n        power(float, optional): Power of polynomial. Default: 1.0.\n        warmup_epoch(int): The epoch numbers for LinearWarmup. Default: 0, , when by_epoch is set to true, it will change to warmup_epoch=warmup_epoch*step_each_epoch.\n        warmup_start_lr(float): Initial learning rate of warm up. Default: 0.0.\n        last_epoch (int, optional):  The index of last epoch. Can be set to restart training. Default: -1, means initial learning rate.\n        by_epoch: Whether the set parameter is based on epoch or iter, when set to true,, epochs and warmup_epoch will be automatically multiplied by step_each_epoch. Default: True\n    \"\"\"\n\n    def __init__(\n        self,\n        learning_rate,\n        epochs,\n        step_each_epoch,\n        end_lr=0.0,\n        power=1.0,\n        warmup_epoch=0,\n        warmup_start_lr=0.0,\n        last_epoch=-1,\n        by_epoch=True,\n        **kwargs,\n    ):\n        super().__init__()\n        if warmup_epoch >= epochs:\n            msg = f'When using warm up, the value of \"epochs\" must be greater than value of \"Optimizer.lr.warmup_epoch\". The value of \"Optimizer.lr.warmup_epoch\" has been set to {epochs}.'\n            logging.warning(msg)\n            warmup_epoch = epochs\n        self.learning_rate = learning_rate\n        self.epochs = epochs\n        self.end_lr = end_lr\n        self.power = power\n        self.last_epoch = last_epoch\n        self.warmup_epoch = warmup_epoch\n        self.warmup_start_lr = warmup_start_lr\n\n        if by_epoch:\n            self.epochs *= step_each_epoch\n            self.warmup_epoch = int(self.warmup_epoch * step_each_epoch)\n\n    def __call__(self):\n        learning_rate = (\n            lr.PolynomialDecay(\n                learning_rate=self.learning_rate,\n                decay_steps=self.epochs,\n                end_lr=self.end_lr,\n                power=self.power,\n                last_epoch=self.last_epoch,\n            )\n            if self.epochs > 0\n            else self.learning_rate\n        )\n        if self.warmup_epoch > 0:\n            learning_rate = lr.LinearWarmup(\n                learning_rate=learning_rate,\n                warmup_steps=self.warmup_epoch,\n                start_lr=self.warmup_start_lr,\n                end_lr=self.learning_rate,\n                last_epoch=self.last_epoch,\n            )\n        return learning_rate\n", "benchmark/PaddleOCR_DBNet/utils/make_trainfile.py": "# -*- coding: utf-8 -*-\n# @Time    : 2019/8/24 12:06\n# @Author  : zhoujun\nimport os\nimport glob\nimport pathlib\n\ndata_path = r\"test\"\n# data_path/img \u5b58\u653e\u56fe\u7247\n# data_path/gt \u5b58\u653e\u6807\u7b7e\u6587\u4ef6\n\nf_w = open(os.path.join(data_path, \"test.txt\"), \"w\", encoding=\"utf8\")\nfor img_path in glob.glob(data_path + \"/img/*.jpg\", recursive=True):\n    d = pathlib.Path(img_path)\n    label_path = os.path.join(data_path, \"gt\", (\"gt_\" + str(d.stem) + \".txt\"))\n    if os.path.exists(img_path) and os.path.exists(label_path):\n        print(img_path, label_path)\n    else:\n        print(\"\u4e0d\u5b58\u5728\", img_path, label_path)\n    f_w.write(\"{}\\t{}\\n\".format(img_path, label_path))\nf_w.close()\n", "benchmark/PaddleOCR_DBNet/utils/ocr_metric/__init__.py": "# -*- coding: utf-8 -*-\n# @Time    : 2019/12/5 15:36\n# @Author  : zhoujun\nfrom .icdar2015 import QuadMetric\n\n\ndef get_metric(config):\n    try:\n        if \"args\" not in config:\n            args = {}\n        else:\n            args = config[\"args\"]\n        if isinstance(args, dict):\n            cls = eval(config[\"type\"])(**args)\n        else:\n            cls = eval(config[\"type\"])(args)\n        return cls\n    except:\n        return None\n", "benchmark/PaddleOCR_DBNet/utils/ocr_metric/icdar2015/__init__.py": "# -*- coding: utf-8 -*-\n# @Time    : 2019/12/5 15:36\n# @Author  : zhoujun\n\nfrom .quad_metric import QuadMetric\n", "benchmark/PaddleOCR_DBNet/utils/ocr_metric/icdar2015/quad_metric.py": "import numpy as np\n\nfrom .detection.iou import DetectionIoUEvaluator\n\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n        return self\n\n\nclass QuadMetric:\n    def __init__(self, is_output_polygon=False):\n        self.is_output_polygon = is_output_polygon\n        self.evaluator = DetectionIoUEvaluator(is_output_polygon=is_output_polygon)\n\n    def measure(self, batch, output, box_thresh=0.6):\n        \"\"\"\n        batch: (image, polygons, ignore_tags\n        batch: a dict produced by dataloaders.\n            image: tensor of shape (N, C, H, W).\n            polygons: tensor of shape (N, K, 4, 2), the polygons of objective regions.\n            ignore_tags: tensor of shape (N, K), indicates whether a region is ignorable or not.\n            shape: the original shape of images.\n            filename: the original filenames of images.\n        output: (polygons, ...)\n        \"\"\"\n        results = []\n        gt_polyons_batch = batch[\"text_polys\"]\n        ignore_tags_batch = batch[\"ignore_tags\"]\n        pred_polygons_batch = np.array(output[0])\n        pred_scores_batch = np.array(output[1])\n        for polygons, pred_polygons, pred_scores, ignore_tags in zip(\n            gt_polyons_batch, pred_polygons_batch, pred_scores_batch, ignore_tags_batch\n        ):\n            gt = [\n                dict(points=np.int64(polygons[i]), ignore=ignore_tags[i])\n                for i in range(len(polygons))\n            ]\n            if self.is_output_polygon:\n                pred = [\n                    dict(points=pred_polygons[i]) for i in range(len(pred_polygons))\n                ]\n            else:\n                pred = []\n                # print(pred_polygons.shape)\n                for i in range(pred_polygons.shape[0]):\n                    if pred_scores[i] >= box_thresh:\n                        # print(pred_polygons[i,:,:].tolist())\n                        pred.append(\n                            dict(points=pred_polygons[i, :, :].astype(np.int32))\n                        )\n                # pred = [dict(points=pred_polygons[i,:,:].tolist()) if pred_scores[i] >= box_thresh for i in range(pred_polygons.shape[0])]\n            results.append(self.evaluator.evaluate_image(gt, pred))\n        return results\n\n    def validate_measure(self, batch, output, box_thresh=0.6):\n        return self.measure(batch, output, box_thresh)\n\n    def evaluate_measure(self, batch, output):\n        return (\n            self.measure(batch, output),\n            np.linspace(0, batch[\"image\"].shape[0]).tolist(),\n        )\n\n    def gather_measure(self, raw_metrics):\n        raw_metrics = [\n            image_metrics\n            for batch_metrics in raw_metrics\n            for image_metrics in batch_metrics\n        ]\n\n        result = self.evaluator.combine_results(raw_metrics)\n\n        precision = AverageMeter()\n        recall = AverageMeter()\n        fmeasure = AverageMeter()\n\n        precision.update(result[\"precision\"], n=len(raw_metrics))\n        recall.update(result[\"recall\"], n=len(raw_metrics))\n        fmeasure_score = (\n            2 * precision.val * recall.val / (precision.val + recall.val + 1e-8)\n        )\n        fmeasure.update(fmeasure_score)\n\n        return {\"precision\": precision, \"recall\": recall, \"fmeasure\": fmeasure}\n", "benchmark/PaddleOCR_DBNet/utils/ocr_metric/icdar2015/detection/deteval.py": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport math\nfrom collections import namedtuple\nimport numpy as np\nfrom shapely.geometry import Polygon\n\n\nclass DetectionDetEvalEvaluator(object):\n    def __init__(\n        self,\n        area_recall_constraint=0.8,\n        area_precision_constraint=0.4,\n        ev_param_ind_center_diff_thr=1,\n        mtype_oo_o=1.0,\n        mtype_om_o=0.8,\n        mtype_om_m=1.0,\n    ):\n        self.area_recall_constraint = area_recall_constraint\n        self.area_precision_constraint = area_precision_constraint\n        self.ev_param_ind_center_diff_thr = ev_param_ind_center_diff_thr\n        self.mtype_oo_o = mtype_oo_o\n        self.mtype_om_o = mtype_om_o\n        self.mtype_om_m = mtype_om_m\n\n    def evaluate_image(self, gt, pred):\n        def get_union(pD, pG):\n            return Polygon(pD).union(Polygon(pG)).area\n\n        def get_intersection_over_union(pD, pG):\n            return get_intersection(pD, pG) / get_union(pD, pG)\n\n        def get_intersection(pD, pG):\n            return Polygon(pD).intersection(Polygon(pG)).area\n\n        def one_to_one_match(row, col):\n            cont = 0\n            for j in range(len(recallMat[0])):\n                if (\n                    recallMat[row, j] >= self.area_recall_constraint\n                    and precisionMat[row, j] >= self.area_precision_constraint\n                ):\n                    cont = cont + 1\n            if cont != 1:\n                return False\n            cont = 0\n            for i in range(len(recallMat)):\n                if (\n                    recallMat[i, col] >= self.area_recall_constraint\n                    and precisionMat[i, col] >= self.area_precision_constraint\n                ):\n                    cont = cont + 1\n            if cont != 1:\n                return False\n\n            if (\n                recallMat[row, col] >= self.area_recall_constraint\n                and precisionMat[row, col] >= self.area_precision_constraint\n            ):\n                return True\n            return False\n\n        def num_overlaps_gt(gtNum):\n            cont = 0\n            for detNum in range(len(detRects)):\n                if detNum not in detDontCareRectsNum:\n                    if recallMat[gtNum, detNum] > 0:\n                        cont = cont + 1\n            return cont\n\n        def num_overlaps_det(detNum):\n            cont = 0\n            for gtNum in range(len(recallMat)):\n                if gtNum not in gtDontCareRectsNum:\n                    if recallMat[gtNum, detNum] > 0:\n                        cont = cont + 1\n            return cont\n\n        def is_single_overlap(row, col):\n            if num_overlaps_gt(row) == 1 and num_overlaps_det(col) == 1:\n                return True\n            else:\n                return False\n\n        def one_to_many_match(gtNum):\n            many_sum = 0\n            detRects = []\n            for detNum in range(len(recallMat[0])):\n                if (\n                    gtRectMat[gtNum] == 0\n                    and detRectMat[detNum] == 0\n                    and detNum not in detDontCareRectsNum\n                ):\n                    if precisionMat[gtNum, detNum] >= self.area_precision_constraint:\n                        many_sum += recallMat[gtNum, detNum]\n                        detRects.append(detNum)\n            if round(many_sum, 4) >= self.area_recall_constraint:\n                return True, detRects\n            else:\n                return False, []\n\n        def many_to_one_match(detNum):\n            many_sum = 0\n            gtRects = []\n            for gtNum in range(len(recallMat)):\n                if (\n                    gtRectMat[gtNum] == 0\n                    and detRectMat[detNum] == 0\n                    and gtNum not in gtDontCareRectsNum\n                ):\n                    if recallMat[gtNum, detNum] >= self.area_recall_constraint:\n                        many_sum += precisionMat[gtNum, detNum]\n                        gtRects.append(gtNum)\n            if round(many_sum, 4) >= self.area_precision_constraint:\n                return True, gtRects\n            else:\n                return False, []\n\n        def center_distance(r1, r2):\n            return ((np.mean(r1, axis=0) - np.mean(r2, axis=0)) ** 2).sum() ** 0.5\n\n        def diag(r):\n            r = np.array(r)\n            return (\n                (r[:, 0].max() - r[:, 0].min()) ** 2\n                + (r[:, 1].max() - r[:, 1].min()) ** 2\n            ) ** 0.5\n\n        perSampleMetrics = {}\n\n        recall = 0\n        precision = 0\n        hmean = 0\n        recallAccum = 0.0\n        precisionAccum = 0.0\n        gtRects = []\n        detRects = []\n        gtPolPoints = []\n        detPolPoints = []\n        gtDontCareRectsNum = (\n            []\n        )  # Array of Ground Truth Rectangles' keys marked as don't Care\n        detDontCareRectsNum = (\n            []\n        )  # Array of Detected Rectangles' matched with a don't Care GT\n        pairs = []\n        evaluationLog = \"\"\n\n        recallMat = np.empty([1, 1])\n        precisionMat = np.empty([1, 1])\n\n        for n in range(len(gt)):\n            points = gt[n][\"points\"]\n            # transcription = gt[n]['text']\n            dontCare = gt[n][\"ignore\"]\n\n            if not Polygon(points).is_valid or not Polygon(points).is_simple:\n                continue\n\n            gtRects.append(points)\n            gtPolPoints.append(points)\n            if dontCare:\n                gtDontCareRectsNum.append(len(gtRects) - 1)\n\n        evaluationLog += (\n            \"GT rectangles: \"\n            + str(len(gtRects))\n            + (\n                \" (\" + str(len(gtDontCareRectsNum)) + \" don't care)\\n\"\n                if len(gtDontCareRectsNum) > 0\n                else \"\\n\"\n            )\n        )\n\n        for n in range(len(pred)):\n            points = pred[n][\"points\"]\n\n            if not Polygon(points).is_valid or not Polygon(points).is_simple:\n                continue\n\n            detRect = points\n            detRects.append(detRect)\n            detPolPoints.append(points)\n            if len(gtDontCareRectsNum) > 0:\n                for dontCareRectNum in gtDontCareRectsNum:\n                    dontCareRect = gtRects[dontCareRectNum]\n                    intersected_area = get_intersection(dontCareRect, detRect)\n                    rdDimensions = Polygon(detRect).area\n                    if rdDimensions == 0:\n                        precision = 0\n                    else:\n                        precision = intersected_area / rdDimensions\n                    if precision > self.area_precision_constraint:\n                        detDontCareRectsNum.append(len(detRects) - 1)\n                        break\n\n        evaluationLog += (\n            \"DET rectangles: \"\n            + str(len(detRects))\n            + (\n                \" (\" + str(len(detDontCareRectsNum)) + \" don't care)\\n\"\n                if len(detDontCareRectsNum) > 0\n                else \"\\n\"\n            )\n        )\n\n        if len(gtRects) == 0:\n            recall = 1\n            precision = 0 if len(detRects) > 0 else 1\n\n        if len(detRects) > 0:\n            # Calculate recall and precision matrixs\n            outputShape = [len(gtRects), len(detRects)]\n            recallMat = np.empty(outputShape)\n            precisionMat = np.empty(outputShape)\n            gtRectMat = np.zeros(len(gtRects), np.int8)\n            detRectMat = np.zeros(len(detRects), np.int8)\n            for gtNum in range(len(gtRects)):\n                for detNum in range(len(detRects)):\n                    rG = gtRects[gtNum]\n                    rD = detRects[detNum]\n                    intersected_area = get_intersection(rG, rD)\n                    rgDimensions = Polygon(rG).area\n                    rdDimensions = Polygon(rD).area\n                    recallMat[gtNum, detNum] = (\n                        0 if rgDimensions == 0 else intersected_area / rgDimensions\n                    )\n                    precisionMat[gtNum, detNum] = (\n                        0 if rdDimensions == 0 else intersected_area / rdDimensions\n                    )\n\n            # Find one-to-one matches\n            evaluationLog += \"Find one-to-one matches\\n\"\n            for gtNum in range(len(gtRects)):\n                for detNum in range(len(detRects)):\n                    if (\n                        gtRectMat[gtNum] == 0\n                        and detRectMat[detNum] == 0\n                        and gtNum not in gtDontCareRectsNum\n                        and detNum not in detDontCareRectsNum\n                    ):\n                        match = one_to_one_match(gtNum, detNum)\n                        if match is True:\n                            # in deteval we have to make other validation before mark as one-to-one\n                            if is_single_overlap(gtNum, detNum) is True:\n                                rG = gtRects[gtNum]\n                                rD = detRects[detNum]\n                                normDist = center_distance(rG, rD)\n                                normDist /= diag(rG) + diag(rD)\n                                normDist *= 2.0\n                                if normDist < self.ev_param_ind_center_diff_thr:\n                                    gtRectMat[gtNum] = 1\n                                    detRectMat[detNum] = 1\n                                    recallAccum += self.mtype_oo_o\n                                    precisionAccum += self.mtype_oo_o\n                                    pairs.append(\n                                        {\"gt\": gtNum, \"det\": detNum, \"type\": \"OO\"}\n                                    )\n                                    evaluationLog += (\n                                        \"Match GT #\"\n                                        + str(gtNum)\n                                        + \" with Det #\"\n                                        + str(detNum)\n                                        + \"\\n\"\n                                    )\n                                else:\n                                    evaluationLog += (\n                                        \"Match Discarded GT #\"\n                                        + str(gtNum)\n                                        + \" with Det #\"\n                                        + str(detNum)\n                                        + \" normDist: \"\n                                        + str(normDist)\n                                        + \" \\n\"\n                                    )\n                            else:\n                                evaluationLog += (\n                                    \"Match Discarded GT #\"\n                                    + str(gtNum)\n                                    + \" with Det #\"\n                                    + str(detNum)\n                                    + \" not single overlap\\n\"\n                                )\n            # Find one-to-many matches\n            evaluationLog += \"Find one-to-many matches\\n\"\n            for gtNum in range(len(gtRects)):\n                if gtNum not in gtDontCareRectsNum:\n                    match, matchesDet = one_to_many_match(gtNum)\n                    if match is True:\n                        evaluationLog += \"num_overlaps_gt=\" + str(\n                            num_overlaps_gt(gtNum)\n                        )\n                        # in deteval we have to make other validation before mark as one-to-one\n                        if num_overlaps_gt(gtNum) >= 2:\n                            gtRectMat[gtNum] = 1\n                            recallAccum += (\n                                self.mtype_oo_o\n                                if len(matchesDet) == 1\n                                else self.mtype_om_o\n                            )\n                            precisionAccum += (\n                                self.mtype_oo_o\n                                if len(matchesDet) == 1\n                                else self.mtype_om_o * len(matchesDet)\n                            )\n                            pairs.append(\n                                {\n                                    \"gt\": gtNum,\n                                    \"det\": matchesDet,\n                                    \"type\": \"OO\" if len(matchesDet) == 1 else \"OM\",\n                                }\n                            )\n                            for detNum in matchesDet:\n                                detRectMat[detNum] = 1\n                            evaluationLog += (\n                                \"Match GT #\"\n                                + str(gtNum)\n                                + \" with Det #\"\n                                + str(matchesDet)\n                                + \"\\n\"\n                            )\n                        else:\n                            evaluationLog += (\n                                \"Match Discarded GT #\"\n                                + str(gtNum)\n                                + \" with Det #\"\n                                + str(matchesDet)\n                                + \" not single overlap\\n\"\n                            )\n\n            # Find many-to-one matches\n            evaluationLog += \"Find many-to-one matches\\n\"\n            for detNum in range(len(detRects)):\n                if detNum not in detDontCareRectsNum:\n                    match, matchesGt = many_to_one_match(detNum)\n                    if match is True:\n                        # in deteval we have to make other validation before mark as one-to-one\n                        if num_overlaps_det(detNum) >= 2:\n                            detRectMat[detNum] = 1\n                            recallAccum += (\n                                self.mtype_oo_o\n                                if len(matchesGt) == 1\n                                else self.mtype_om_m * len(matchesGt)\n                            )\n                            precisionAccum += (\n                                self.mtype_oo_o\n                                if len(matchesGt) == 1\n                                else self.mtype_om_m\n                            )\n                            pairs.append(\n                                {\n                                    \"gt\": matchesGt,\n                                    \"det\": detNum,\n                                    \"type\": \"OO\" if len(matchesGt) == 1 else \"MO\",\n                                }\n                            )\n                            for gtNum in matchesGt:\n                                gtRectMat[gtNum] = 1\n                            evaluationLog += (\n                                \"Match GT #\"\n                                + str(matchesGt)\n                                + \" with Det #\"\n                                + str(detNum)\n                                + \"\\n\"\n                            )\n                        else:\n                            evaluationLog += (\n                                \"Match Discarded GT #\"\n                                + str(matchesGt)\n                                + \" with Det #\"\n                                + str(detNum)\n                                + \" not single overlap\\n\"\n                            )\n\n            numGtCare = len(gtRects) - len(gtDontCareRectsNum)\n            if numGtCare == 0:\n                recall = float(1)\n                precision = float(0) if len(detRects) > 0 else float(1)\n            else:\n                recall = float(recallAccum) / numGtCare\n                precision = (\n                    float(0)\n                    if (len(detRects) - len(detDontCareRectsNum)) == 0\n                    else float(precisionAccum)\n                    / (len(detRects) - len(detDontCareRectsNum))\n                )\n            hmean = (\n                0\n                if (precision + recall) == 0\n                else 2.0 * precision * recall / (precision + recall)\n            )\n\n        numGtCare = len(gtRects) - len(gtDontCareRectsNum)\n        numDetCare = len(detRects) - len(detDontCareRectsNum)\n\n        perSampleMetrics = {\n            \"precision\": precision,\n            \"recall\": recall,\n            \"hmean\": hmean,\n            \"pairs\": pairs,\n            \"recallMat\": [] if len(detRects) > 100 else recallMat.tolist(),\n            \"precisionMat\": [] if len(detRects) > 100 else precisionMat.tolist(),\n            \"gtPolPoints\": gtPolPoints,\n            \"detPolPoints\": detPolPoints,\n            \"gtCare\": numGtCare,\n            \"detCare\": numDetCare,\n            \"gtDontCare\": gtDontCareRectsNum,\n            \"detDontCare\": detDontCareRectsNum,\n            \"recallAccum\": recallAccum,\n            \"precisionAccum\": precisionAccum,\n            \"evaluationLog\": evaluationLog,\n        }\n\n        return perSampleMetrics\n\n    def combine_results(self, results):\n        numGt = 0\n        numDet = 0\n        methodRecallSum = 0\n        methodPrecisionSum = 0\n\n        for result in results:\n            numGt += result[\"gtCare\"]\n            numDet += result[\"detCare\"]\n            methodRecallSum += result[\"recallAccum\"]\n            methodPrecisionSum += result[\"precisionAccum\"]\n\n        methodRecall = 0 if numGt == 0 else methodRecallSum / numGt\n        methodPrecision = 0 if numDet == 0 else methodPrecisionSum / numDet\n        methodHmean = (\n            0\n            if methodRecall + methodPrecision == 0\n            else 2 * methodRecall * methodPrecision / (methodRecall + methodPrecision)\n        )\n\n        methodMetrics = {\n            \"precision\": methodPrecision,\n            \"recall\": methodRecall,\n            \"hmean\": methodHmean,\n        }\n\n        return methodMetrics\n\n\nif __name__ == \"__main__\":\n    evaluator = DetectionDetEvalEvaluator()\n    gts = [\n        [\n            {\n                \"points\": [(0, 0), (1, 0), (1, 1), (0, 1)],\n                \"text\": 1234,\n                \"ignore\": False,\n            },\n            {\n                \"points\": [(2, 2), (3, 2), (3, 3), (2, 3)],\n                \"text\": 5678,\n                \"ignore\": True,\n            },\n        ]\n    ]\n    preds = [\n        [\n            {\n                \"points\": [(0.1, 0.1), (1, 0), (1, 1), (0, 1)],\n                \"text\": 123,\n                \"ignore\": False,\n            }\n        ]\n    ]\n    results = []\n    for gt, pred in zip(gts, preds):\n        results.append(evaluator.evaluate_image(gt, pred))\n    metrics = evaluator.combine_results(results)\n    print(metrics)\n", "benchmark/PaddleOCR_DBNet/utils/ocr_metric/icdar2015/detection/iou.py": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nfrom collections import namedtuple\nimport numpy as np\nfrom shapely.geometry import Polygon\nimport cv2\n\n\ndef iou_rotate(box_a, box_b, method=\"union\"):\n    rect_a = cv2.minAreaRect(box_a)\n    rect_b = cv2.minAreaRect(box_b)\n    r1 = cv2.rotatedRectangleIntersection(rect_a, rect_b)\n    if r1[0] == 0:\n        return 0\n    else:\n        inter_area = cv2.contourArea(r1[1])\n        area_a = cv2.contourArea(box_a)\n        area_b = cv2.contourArea(box_b)\n        union_area = area_a + area_b - inter_area\n        if union_area == 0 or inter_area == 0:\n            return 0\n        if method == \"union\":\n            iou = inter_area / union_area\n        elif method == \"intersection\":\n            iou = inter_area / min(area_a, area_b)\n        else:\n            raise NotImplementedError\n        return iou\n\n\nclass DetectionIoUEvaluator(object):\n    def __init__(\n        self, is_output_polygon=False, iou_constraint=0.5, area_precision_constraint=0.5\n    ):\n        self.is_output_polygon = is_output_polygon\n        self.iou_constraint = iou_constraint\n        self.area_precision_constraint = area_precision_constraint\n\n    def evaluate_image(self, gt, pred):\n        def get_union(pD, pG):\n            return Polygon(pD).union(Polygon(pG)).area\n\n        def get_intersection_over_union(pD, pG):\n            return get_intersection(pD, pG) / get_union(pD, pG)\n\n        def get_intersection(pD, pG):\n            return Polygon(pD).intersection(Polygon(pG)).area\n\n        def compute_ap(confList, matchList, numGtCare):\n            correct = 0\n            AP = 0\n            if len(confList) > 0:\n                confList = np.array(confList)\n                matchList = np.array(matchList)\n                sorted_ind = np.argsort(-confList)\n                confList = confList[sorted_ind]\n                matchList = matchList[sorted_ind]\n                for n in range(len(confList)):\n                    match = matchList[n]\n                    if match:\n                        correct += 1\n                        AP += float(correct) / (n + 1)\n\n                if numGtCare > 0:\n                    AP /= numGtCare\n\n            return AP\n\n        perSampleMetrics = {}\n\n        matchedSum = 0\n\n        Rectangle = namedtuple(\"Rectangle\", \"xmin ymin xmax ymax\")\n\n        numGlobalCareGt = 0\n        numGlobalCareDet = 0\n\n        arrGlobalConfidences = []\n        arrGlobalMatches = []\n\n        recall = 0\n        precision = 0\n        hmean = 0\n\n        detMatched = 0\n\n        iouMat = np.empty([1, 1])\n\n        gtPols = []\n        detPols = []\n\n        gtPolPoints = []\n        detPolPoints = []\n\n        # Array of Ground Truth Polygons' keys marked as don't Care\n        gtDontCarePolsNum = []\n        # Array of Detected Polygons' matched with a don't Care GT\n        detDontCarePolsNum = []\n\n        pairs = []\n        detMatchedNums = []\n\n        arrSampleConfidences = []\n        arrSampleMatch = []\n\n        evaluationLog = \"\"\n\n        for n in range(len(gt)):\n            points = gt[n][\"points\"]\n            # transcription = gt[n]['text']\n            dontCare = gt[n][\"ignore\"]\n\n            if not Polygon(points).is_valid or not Polygon(points).is_simple:\n                continue\n\n            gtPol = points\n            gtPols.append(gtPol)\n            gtPolPoints.append(points)\n            if dontCare:\n                gtDontCarePolsNum.append(len(gtPols) - 1)\n\n        evaluationLog += (\n            \"GT polygons: \"\n            + str(len(gtPols))\n            + (\n                \" (\" + str(len(gtDontCarePolsNum)) + \" don't care)\\n\"\n                if len(gtDontCarePolsNum) > 0\n                else \"\\n\"\n            )\n        )\n\n        for n in range(len(pred)):\n            points = pred[n][\"points\"]\n            if not Polygon(points).is_valid or not Polygon(points).is_simple:\n                continue\n\n            detPol = points\n            detPols.append(detPol)\n            detPolPoints.append(points)\n            if len(gtDontCarePolsNum) > 0:\n                for dontCarePol in gtDontCarePolsNum:\n                    dontCarePol = gtPols[dontCarePol]\n                    intersected_area = get_intersection(dontCarePol, detPol)\n                    pdDimensions = Polygon(detPol).area\n                    precision = (\n                        0 if pdDimensions == 0 else intersected_area / pdDimensions\n                    )\n                    if precision > self.area_precision_constraint:\n                        detDontCarePolsNum.append(len(detPols) - 1)\n                        break\n\n        evaluationLog += (\n            \"DET polygons: \"\n            + str(len(detPols))\n            + (\n                \" (\" + str(len(detDontCarePolsNum)) + \" don't care)\\n\"\n                if len(detDontCarePolsNum) > 0\n                else \"\\n\"\n            )\n        )\n\n        if len(gtPols) > 0 and len(detPols) > 0:\n            # Calculate IoU and precision matrixs\n            outputShape = [len(gtPols), len(detPols)]\n            iouMat = np.empty(outputShape)\n            gtRectMat = np.zeros(len(gtPols), np.int8)\n            detRectMat = np.zeros(len(detPols), np.int8)\n            if self.is_output_polygon:\n                for gtNum in range(len(gtPols)):\n                    for detNum in range(len(detPols)):\n                        pG = gtPols[gtNum]\n                        pD = detPols[detNum]\n                        iouMat[gtNum, detNum] = get_intersection_over_union(pD, pG)\n            else:\n                # gtPols = np.float32(gtPols)\n                # detPols = np.float32(detPols)\n                for gtNum in range(len(gtPols)):\n                    for detNum in range(len(detPols)):\n                        pG = np.float32(gtPols[gtNum])\n                        pD = np.float32(detPols[detNum])\n                        iouMat[gtNum, detNum] = iou_rotate(pD, pG)\n            for gtNum in range(len(gtPols)):\n                for detNum in range(len(detPols)):\n                    if (\n                        gtRectMat[gtNum] == 0\n                        and detRectMat[detNum] == 0\n                        and gtNum not in gtDontCarePolsNum\n                        and detNum not in detDontCarePolsNum\n                    ):\n                        if iouMat[gtNum, detNum] > self.iou_constraint:\n                            gtRectMat[gtNum] = 1\n                            detRectMat[detNum] = 1\n                            detMatched += 1\n                            pairs.append({\"gt\": gtNum, \"det\": detNum})\n                            detMatchedNums.append(detNum)\n                            evaluationLog += (\n                                \"Match GT #\"\n                                + str(gtNum)\n                                + \" with Det #\"\n                                + str(detNum)\n                                + \"\\n\"\n                            )\n\n        numGtCare = len(gtPols) - len(gtDontCarePolsNum)\n        numDetCare = len(detPols) - len(detDontCarePolsNum)\n        if numGtCare == 0:\n            recall = float(1)\n            precision = float(0) if numDetCare > 0 else float(1)\n        else:\n            recall = float(detMatched) / numGtCare\n            precision = 0 if numDetCare == 0 else float(detMatched) / numDetCare\n\n        hmean = (\n            0\n            if (precision + recall) == 0\n            else 2.0 * precision * recall / (precision + recall)\n        )\n\n        matchedSum += detMatched\n        numGlobalCareGt += numGtCare\n        numGlobalCareDet += numDetCare\n\n        perSampleMetrics = {\n            \"precision\": precision,\n            \"recall\": recall,\n            \"hmean\": hmean,\n            \"pairs\": pairs,\n            \"iouMat\": [] if len(detPols) > 100 else iouMat.tolist(),\n            \"gtPolPoints\": gtPolPoints,\n            \"detPolPoints\": detPolPoints,\n            \"gtCare\": numGtCare,\n            \"detCare\": numDetCare,\n            \"gtDontCare\": gtDontCarePolsNum,\n            \"detDontCare\": detDontCarePolsNum,\n            \"detMatched\": detMatched,\n            \"evaluationLog\": evaluationLog,\n        }\n\n        return perSampleMetrics\n\n    def combine_results(self, results):\n        numGlobalCareGt = 0\n        numGlobalCareDet = 0\n        matchedSum = 0\n        for result in results:\n            numGlobalCareGt += result[\"gtCare\"]\n            numGlobalCareDet += result[\"detCare\"]\n            matchedSum += result[\"detMatched\"]\n\n        methodRecall = (\n            0 if numGlobalCareGt == 0 else float(matchedSum) / numGlobalCareGt\n        )\n        methodPrecision = (\n            0 if numGlobalCareDet == 0 else float(matchedSum) / numGlobalCareDet\n        )\n        methodHmean = (\n            0\n            if methodRecall + methodPrecision == 0\n            else 2 * methodRecall * methodPrecision / (methodRecall + methodPrecision)\n        )\n\n        methodMetrics = {\n            \"precision\": methodPrecision,\n            \"recall\": methodRecall,\n            \"hmean\": methodHmean,\n        }\n\n        return methodMetrics\n\n\nif __name__ == \"__main__\":\n    evaluator = DetectionIoUEvaluator()\n    preds = [\n        [\n            {\n                \"points\": [(0.1, 0.1), (0.5, 0), (0.5, 1), (0, 1)],\n                \"text\": 1234,\n                \"ignore\": False,\n            },\n            {\n                \"points\": [(0.5, 0.1), (1, 0), (1, 1), (0.5, 1)],\n                \"text\": 5678,\n                \"ignore\": False,\n            },\n        ]\n    ]\n    gts = [\n        [\n            {\n                \"points\": [(0.1, 0.1), (1, 0), (1, 1), (0, 1)],\n                \"text\": 123,\n                \"ignore\": False,\n            }\n        ]\n    ]\n    results = []\n    for gt, pred in zip(gts, preds):\n        results.append(evaluator.evaluate_image(gt, pred))\n    metrics = evaluator.combine_results(results)\n    print(metrics)\n", "benchmark/PaddleOCR_DBNet/utils/ocr_metric/icdar2015/detection/mtwi2018.py": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport math\nfrom collections import namedtuple\nimport numpy as np\nfrom shapely.geometry import Polygon\n\n\nclass DetectionMTWI2018Evaluator(object):\n    def __init__(\n        self,\n        area_recall_constraint=0.7,\n        area_precision_constraint=0.7,\n        ev_param_ind_center_diff_thr=1,\n    ):\n        self.area_recall_constraint = area_recall_constraint\n        self.area_precision_constraint = area_precision_constraint\n        self.ev_param_ind_center_diff_thr = ev_param_ind_center_diff_thr\n\n    def evaluate_image(self, gt, pred):\n        def get_union(pD, pG):\n            return Polygon(pD).union(Polygon(pG)).area\n\n        def get_intersection_over_union(pD, pG):\n            return get_intersection(pD, pG) / get_union(pD, pG)\n\n        def get_intersection(pD, pG):\n            return Polygon(pD).intersection(Polygon(pG)).area\n\n        def one_to_one_match(row, col):\n            cont = 0\n            for j in range(len(recallMat[0])):\n                if (\n                    recallMat[row, j] >= self.area_recall_constraint\n                    and precisionMat[row, j] >= self.area_precision_constraint\n                ):\n                    cont = cont + 1\n            if cont != 1:\n                return False\n            cont = 0\n            for i in range(len(recallMat)):\n                if (\n                    recallMat[i, col] >= self.area_recall_constraint\n                    and precisionMat[i, col] >= self.area_precision_constraint\n                ):\n                    cont = cont + 1\n            if cont != 1:\n                return False\n\n            if (\n                recallMat[row, col] >= self.area_recall_constraint\n                and precisionMat[row, col] >= self.area_precision_constraint\n            ):\n                return True\n            return False\n\n        def one_to_many_match(gtNum):\n            many_sum = 0\n            detRects = []\n            for detNum in range(len(recallMat[0])):\n                if (\n                    gtRectMat[gtNum] == 0\n                    and detRectMat[detNum] == 0\n                    and detNum not in detDontCareRectsNum\n                ):\n                    if precisionMat[gtNum, detNum] >= self.area_precision_constraint:\n                        many_sum += recallMat[gtNum, detNum]\n                        detRects.append(detNum)\n            if round(many_sum, 4) >= self.area_recall_constraint:\n                return True, detRects\n            else:\n                return False, []\n\n        def many_to_one_match(detNum):\n            many_sum = 0\n            gtRects = []\n            for gtNum in range(len(recallMat)):\n                if (\n                    gtRectMat[gtNum] == 0\n                    and detRectMat[detNum] == 0\n                    and gtNum not in gtDontCareRectsNum\n                ):\n                    if recallMat[gtNum, detNum] >= self.area_recall_constraint:\n                        many_sum += precisionMat[gtNum, detNum]\n                        gtRects.append(gtNum)\n            if round(many_sum, 4) >= self.area_precision_constraint:\n                return True, gtRects\n            else:\n                return False, []\n\n        def center_distance(r1, r2):\n            return ((np.mean(r1, axis=0) - np.mean(r2, axis=0)) ** 2).sum() ** 0.5\n\n        def diag(r):\n            r = np.array(r)\n            return (\n                (r[:, 0].max() - r[:, 0].min()) ** 2\n                + (r[:, 1].max() - r[:, 1].min()) ** 2\n            ) ** 0.5\n\n        perSampleMetrics = {}\n\n        recall = 0\n        precision = 0\n        hmean = 0\n        recallAccum = 0.0\n        precisionAccum = 0.0\n        gtRects = []\n        detRects = []\n        gtPolPoints = []\n        detPolPoints = []\n        gtDontCareRectsNum = (\n            []\n        )  # Array of Ground Truth Rectangles' keys marked as don't Care\n        detDontCareRectsNum = (\n            []\n        )  # Array of Detected Rectangles' matched with a don't Care GT\n        pairs = []\n        evaluationLog = \"\"\n\n        recallMat = np.empty([1, 1])\n        precisionMat = np.empty([1, 1])\n\n        for n in range(len(gt)):\n            points = gt[n][\"points\"]\n            # transcription = gt[n]['text']\n            dontCare = gt[n][\"ignore\"]\n\n            if not Polygon(points).is_valid or not Polygon(points).is_simple:\n                continue\n\n            gtRects.append(points)\n            gtPolPoints.append(points)\n            if dontCare:\n                gtDontCareRectsNum.append(len(gtRects) - 1)\n\n        evaluationLog += (\n            \"GT rectangles: \"\n            + str(len(gtRects))\n            + (\n                \" (\" + str(len(gtDontCareRectsNum)) + \" don't care)\\n\"\n                if len(gtDontCareRectsNum) > 0\n                else \"\\n\"\n            )\n        )\n\n        for n in range(len(pred)):\n            points = pred[n][\"points\"]\n\n            if not Polygon(points).is_valid or not Polygon(points).is_simple:\n                continue\n\n            detRect = points\n            detRects.append(detRect)\n            detPolPoints.append(points)\n            if len(gtDontCareRectsNum) > 0:\n                for dontCareRectNum in gtDontCareRectsNum:\n                    dontCareRect = gtRects[dontCareRectNum]\n                    intersected_area = get_intersection(dontCareRect, detRect)\n                    rdDimensions = Polygon(detRect).area\n                    if rdDimensions == 0:\n                        precision = 0\n                    else:\n                        precision = intersected_area / rdDimensions\n                    if precision > 0.5:\n                        detDontCareRectsNum.append(len(detRects) - 1)\n                        break\n\n        evaluationLog += (\n            \"DET rectangles: \"\n            + str(len(detRects))\n            + (\n                \" (\" + str(len(detDontCareRectsNum)) + \" don't care)\\n\"\n                if len(detDontCareRectsNum) > 0\n                else \"\\n\"\n            )\n        )\n\n        if len(gtRects) == 0:\n            recall = 1\n            precision = 0 if len(detRects) > 0 else 1\n\n        if len(detRects) > 0:\n            # Calculate recall and precision matrixs\n            outputShape = [len(gtRects), len(detRects)]\n            recallMat = np.empty(outputShape)\n            precisionMat = np.empty(outputShape)\n            gtRectMat = np.zeros(len(gtRects), np.int8)\n            detRectMat = np.zeros(len(detRects), np.int8)\n            for gtNum in range(len(gtRects)):\n                for detNum in range(len(detRects)):\n                    rG = gtRects[gtNum]\n                    rD = detRects[detNum]\n                    intersected_area = get_intersection(rG, rD)\n                    rgDimensions = Polygon(rG).area\n                    rdDimensions = Polygon(rD).area\n                    recallMat[gtNum, detNum] = (\n                        0 if rgDimensions == 0 else intersected_area / rgDimensions\n                    )\n                    precisionMat[gtNum, detNum] = (\n                        0 if rdDimensions == 0 else intersected_area / rdDimensions\n                    )\n\n            # Find one-to-one matches\n            evaluationLog += \"Find one-to-one matches\\n\"\n            for gtNum in range(len(gtRects)):\n                for detNum in range(len(detRects)):\n                    if (\n                        gtRectMat[gtNum] == 0\n                        and detRectMat[detNum] == 0\n                        and gtNum not in gtDontCareRectsNum\n                        and detNum not in detDontCareRectsNum\n                    ):\n                        match = one_to_one_match(gtNum, detNum)\n                        if match is True:\n                            # in deteval we have to make other validation before mark as one-to-one\n                            rG = gtRects[gtNum]\n                            rD = detRects[detNum]\n                            normDist = center_distance(rG, rD)\n                            normDist /= diag(rG) + diag(rD)\n                            normDist *= 2.0\n                            if normDist < self.ev_param_ind_center_diff_thr:\n                                gtRectMat[gtNum] = 1\n                                detRectMat[detNum] = 1\n                                recallAccum += 1.0\n                                precisionAccum += 1.0\n                                pairs.append({\"gt\": gtNum, \"det\": detNum, \"type\": \"OO\"})\n                                evaluationLog += (\n                                    \"Match GT #\"\n                                    + str(gtNum)\n                                    + \" with Det #\"\n                                    + str(detNum)\n                                    + \"\\n\"\n                                )\n                            else:\n                                evaluationLog += (\n                                    \"Match Discarded GT #\"\n                                    + str(gtNum)\n                                    + \" with Det #\"\n                                    + str(detNum)\n                                    + \" normDist: \"\n                                    + str(normDist)\n                                    + \" \\n\"\n                                )\n            # Find one-to-many matches\n            evaluationLog += \"Find one-to-many matches\\n\"\n            for gtNum in range(len(gtRects)):\n                if gtNum not in gtDontCareRectsNum:\n                    match, matchesDet = one_to_many_match(gtNum)\n                    if match is True:\n                        gtRectMat[gtNum] = 1\n                        recallAccum += 1.0\n                        precisionAccum += len(matchesDet) / (\n                            1 + math.log(len(matchesDet))\n                        )\n                        pairs.append(\n                            {\n                                \"gt\": gtNum,\n                                \"det\": matchesDet,\n                                \"type\": \"OO\" if len(matchesDet) == 1 else \"OM\",\n                            }\n                        )\n                        for detNum in matchesDet:\n                            detRectMat[detNum] = 1\n                        evaluationLog += (\n                            \"Match GT #\"\n                            + str(gtNum)\n                            + \" with Det #\"\n                            + str(matchesDet)\n                            + \"\\n\"\n                        )\n\n            # Find many-to-one matches\n            evaluationLog += \"Find many-to-one matches\\n\"\n            for detNum in range(len(detRects)):\n                if detNum not in detDontCareRectsNum:\n                    match, matchesGt = many_to_one_match(detNum)\n                    if match is True:\n                        detRectMat[detNum] = 1\n                        recallAccum += len(matchesGt) / (1 + math.log(len(matchesGt)))\n                        precisionAccum += 1.0\n                        pairs.append(\n                            {\n                                \"gt\": matchesGt,\n                                \"det\": detNum,\n                                \"type\": \"OO\" if len(matchesGt) == 1 else \"MO\",\n                            }\n                        )\n                        for gtNum in matchesGt:\n                            gtRectMat[gtNum] = 1\n                        evaluationLog += (\n                            \"Match GT #\"\n                            + str(matchesGt)\n                            + \" with Det #\"\n                            + str(detNum)\n                            + \"\\n\"\n                        )\n\n            numGtCare = len(gtRects) - len(gtDontCareRectsNum)\n            if numGtCare == 0:\n                recall = float(1)\n                precision = float(0) if len(detRects) > 0 else float(1)\n            else:\n                recall = float(recallAccum) / numGtCare\n                precision = (\n                    float(0)\n                    if (len(detRects) - len(detDontCareRectsNum)) == 0\n                    else float(precisionAccum)\n                    / (len(detRects) - len(detDontCareRectsNum))\n                )\n            hmean = (\n                0\n                if (precision + recall) == 0\n                else 2.0 * precision * recall / (precision + recall)\n            )\n\n        numGtCare = len(gtRects) - len(gtDontCareRectsNum)\n        numDetCare = len(detRects) - len(detDontCareRectsNum)\n\n        perSampleMetrics = {\n            \"precision\": precision,\n            \"recall\": recall,\n            \"hmean\": hmean,\n            \"pairs\": pairs,\n            \"recallMat\": [] if len(detRects) > 100 else recallMat.tolist(),\n            \"precisionMat\": [] if len(detRects) > 100 else precisionMat.tolist(),\n            \"gtPolPoints\": gtPolPoints,\n            \"detPolPoints\": detPolPoints,\n            \"gtCare\": numGtCare,\n            \"detCare\": numDetCare,\n            \"gtDontCare\": gtDontCareRectsNum,\n            \"detDontCare\": detDontCareRectsNum,\n            \"recallAccum\": recallAccum,\n            \"precisionAccum\": precisionAccum,\n            \"evaluationLog\": evaluationLog,\n        }\n\n        return perSampleMetrics\n\n    def combine_results(self, results):\n        numGt = 0\n        numDet = 0\n        methodRecallSum = 0\n        methodPrecisionSum = 0\n\n        for result in results:\n            numGt += result[\"gtCare\"]\n            numDet += result[\"detCare\"]\n            methodRecallSum += result[\"recallAccum\"]\n            methodPrecisionSum += result[\"precisionAccum\"]\n\n        methodRecall = 0 if numGt == 0 else methodRecallSum / numGt\n        methodPrecision = 0 if numDet == 0 else methodPrecisionSum / numDet\n        methodHmean = (\n            0\n            if methodRecall + methodPrecision == 0\n            else 2 * methodRecall * methodPrecision / (methodRecall + methodPrecision)\n        )\n\n        methodMetrics = {\n            \"precision\": methodPrecision,\n            \"recall\": methodRecall,\n            \"hmean\": methodHmean,\n        }\n\n        return methodMetrics\n\n\nif __name__ == \"__main__\":\n    evaluator = DetectionICDAR2013Evaluator()\n    gts = [\n        [\n            {\n                \"points\": [(0, 0), (1, 0), (1, 1), (0, 1)],\n                \"text\": 1234,\n                \"ignore\": False,\n            },\n            {\n                \"points\": [(2, 2), (3, 2), (3, 3), (2, 3)],\n                \"text\": 5678,\n                \"ignore\": True,\n            },\n        ]\n    ]\n    preds = [\n        [\n            {\n                \"points\": [(0.1, 0.1), (1, 0), (1, 1), (0, 1)],\n                \"text\": 123,\n                \"ignore\": False,\n            }\n        ]\n    ]\n    results = []\n    for gt, pred in zip(gts, preds):\n        results.append(evaluator.evaluate_image(gt, pred))\n    metrics = evaluator.combine_results(results)\n    print(metrics)\n", "benchmark/PaddleOCR_DBNet/utils/ocr_metric/icdar2015/detection/__init__.py": "", "benchmark/PaddleOCR_DBNet/utils/ocr_metric/icdar2015/detection/icdar2013.py": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport math\nfrom collections import namedtuple\nimport numpy as np\nfrom shapely.geometry import Polygon\n\n\nclass DetectionICDAR2013Evaluator(object):\n    def __init__(\n        self,\n        area_recall_constraint=0.8,\n        area_precision_constraint=0.4,\n        ev_param_ind_center_diff_thr=1,\n        mtype_oo_o=1.0,\n        mtype_om_o=0.8,\n        mtype_om_m=1.0,\n    ):\n        self.area_recall_constraint = area_recall_constraint\n        self.area_precision_constraint = area_precision_constraint\n        self.ev_param_ind_center_diff_thr = ev_param_ind_center_diff_thr\n        self.mtype_oo_o = mtype_oo_o\n        self.mtype_om_o = mtype_om_o\n        self.mtype_om_m = mtype_om_m\n\n    def evaluate_image(self, gt, pred):\n        def get_union(pD, pG):\n            return Polygon(pD).union(Polygon(pG)).area\n\n        def get_intersection_over_union(pD, pG):\n            return get_intersection(pD, pG) / get_union(pD, pG)\n\n        def get_intersection(pD, pG):\n            return Polygon(pD).intersection(Polygon(pG)).area\n\n        def one_to_one_match(row, col):\n            cont = 0\n            for j in range(len(recallMat[0])):\n                if (\n                    recallMat[row, j] >= self.area_recall_constraint\n                    and precisionMat[row, j] >= self.area_precision_constraint\n                ):\n                    cont = cont + 1\n            if cont != 1:\n                return False\n            cont = 0\n            for i in range(len(recallMat)):\n                if (\n                    recallMat[i, col] >= self.area_recall_constraint\n                    and precisionMat[i, col] >= self.area_precision_constraint\n                ):\n                    cont = cont + 1\n            if cont != 1:\n                return False\n\n            if (\n                recallMat[row, col] >= self.area_recall_constraint\n                and precisionMat[row, col] >= self.area_precision_constraint\n            ):\n                return True\n            return False\n\n        def one_to_many_match(gtNum):\n            many_sum = 0\n            detRects = []\n            for detNum in range(len(recallMat[0])):\n                if (\n                    gtRectMat[gtNum] == 0\n                    and detRectMat[detNum] == 0\n                    and detNum not in detDontCareRectsNum\n                ):\n                    if precisionMat[gtNum, detNum] >= self.area_precision_constraint:\n                        many_sum += recallMat[gtNum, detNum]\n                        detRects.append(detNum)\n            if round(many_sum, 4) >= self.area_recall_constraint:\n                return True, detRects\n            else:\n                return False, []\n\n        def many_to_one_match(detNum):\n            many_sum = 0\n            gtRects = []\n            for gtNum in range(len(recallMat)):\n                if (\n                    gtRectMat[gtNum] == 0\n                    and detRectMat[detNum] == 0\n                    and gtNum not in gtDontCareRectsNum\n                ):\n                    if recallMat[gtNum, detNum] >= self.area_recall_constraint:\n                        many_sum += precisionMat[gtNum, detNum]\n                        gtRects.append(gtNum)\n            if round(many_sum, 4) >= self.area_precision_constraint:\n                return True, gtRects\n            else:\n                return False, []\n\n        def center_distance(r1, r2):\n            return ((np.mean(r1, axis=0) - np.mean(r2, axis=0)) ** 2).sum() ** 0.5\n\n        def diag(r):\n            r = np.array(r)\n            return (\n                (r[:, 0].max() - r[:, 0].min()) ** 2\n                + (r[:, 1].max() - r[:, 1].min()) ** 2\n            ) ** 0.5\n\n        perSampleMetrics = {}\n\n        recall = 0\n        precision = 0\n        hmean = 0\n        recallAccum = 0.0\n        precisionAccum = 0.0\n        gtRects = []\n        detRects = []\n        gtPolPoints = []\n        detPolPoints = []\n        gtDontCareRectsNum = (\n            []\n        )  # Array of Ground Truth Rectangles' keys marked as don't Care\n        detDontCareRectsNum = (\n            []\n        )  # Array of Detected Rectangles' matched with a don't Care GT\n        pairs = []\n        evaluationLog = \"\"\n\n        recallMat = np.empty([1, 1])\n        precisionMat = np.empty([1, 1])\n\n        for n in range(len(gt)):\n            points = gt[n][\"points\"]\n            # transcription = gt[n]['text']\n            dontCare = gt[n][\"ignore\"]\n\n            if not Polygon(points).is_valid or not Polygon(points).is_simple:\n                continue\n\n            gtRects.append(points)\n            gtPolPoints.append(points)\n            if dontCare:\n                gtDontCareRectsNum.append(len(gtRects) - 1)\n\n        evaluationLog += (\n            \"GT rectangles: \"\n            + str(len(gtRects))\n            + (\n                \" (\" + str(len(gtDontCareRectsNum)) + \" don't care)\\n\"\n                if len(gtDontCareRectsNum) > 0\n                else \"\\n\"\n            )\n        )\n\n        for n in range(len(pred)):\n            points = pred[n][\"points\"]\n\n            if not Polygon(points).is_valid or not Polygon(points).is_simple:\n                continue\n\n            detRect = points\n            detRects.append(detRect)\n            detPolPoints.append(points)\n            if len(gtDontCareRectsNum) > 0:\n                for dontCareRectNum in gtDontCareRectsNum:\n                    dontCareRect = gtRects[dontCareRectNum]\n                    intersected_area = get_intersection(dontCareRect, detRect)\n                    rdDimensions = Polygon(detRect).area\n                    if rdDimensions == 0:\n                        precision = 0\n                    else:\n                        precision = intersected_area / rdDimensions\n                    if precision > self.area_precision_constraint:\n                        detDontCareRectsNum.append(len(detRects) - 1)\n                        break\n\n        evaluationLog += (\n            \"DET rectangles: \"\n            + str(len(detRects))\n            + (\n                \" (\" + str(len(detDontCareRectsNum)) + \" don't care)\\n\"\n                if len(detDontCareRectsNum) > 0\n                else \"\\n\"\n            )\n        )\n\n        if len(gtRects) == 0:\n            recall = 1\n            precision = 0 if len(detRects) > 0 else 1\n\n        if len(detRects) > 0:\n            # Calculate recall and precision matrixs\n            outputShape = [len(gtRects), len(detRects)]\n            recallMat = np.empty(outputShape)\n            precisionMat = np.empty(outputShape)\n            gtRectMat = np.zeros(len(gtRects), np.int8)\n            detRectMat = np.zeros(len(detRects), np.int8)\n            for gtNum in range(len(gtRects)):\n                for detNum in range(len(detRects)):\n                    rG = gtRects[gtNum]\n                    rD = detRects[detNum]\n                    intersected_area = get_intersection(rG, rD)\n                    rgDimensions = Polygon(rG).area\n                    rdDimensions = Polygon(rD).area\n                    recallMat[gtNum, detNum] = (\n                        0 if rgDimensions == 0 else intersected_area / rgDimensions\n                    )\n                    precisionMat[gtNum, detNum] = (\n                        0 if rdDimensions == 0 else intersected_area / rdDimensions\n                    )\n\n            # Find one-to-one matches\n            evaluationLog += \"Find one-to-one matches\\n\"\n            for gtNum in range(len(gtRects)):\n                for detNum in range(len(detRects)):\n                    if (\n                        gtRectMat[gtNum] == 0\n                        and detRectMat[detNum] == 0\n                        and gtNum not in gtDontCareRectsNum\n                        and detNum not in detDontCareRectsNum\n                    ):\n                        match = one_to_one_match(gtNum, detNum)\n                        if match is True:\n                            # in deteval we have to make other validation before mark as one-to-one\n                            rG = gtRects[gtNum]\n                            rD = detRects[detNum]\n                            normDist = center_distance(rG, rD)\n                            normDist /= diag(rG) + diag(rD)\n                            normDist *= 2.0\n                            if normDist < self.ev_param_ind_center_diff_thr:\n                                gtRectMat[gtNum] = 1\n                                detRectMat[detNum] = 1\n                                recallAccum += self.mtype_oo_o\n                                precisionAccum += self.mtype_oo_o\n                                pairs.append({\"gt\": gtNum, \"det\": detNum, \"type\": \"OO\"})\n                                evaluationLog += (\n                                    \"Match GT #\"\n                                    + str(gtNum)\n                                    + \" with Det #\"\n                                    + str(detNum)\n                                    + \"\\n\"\n                                )\n                            else:\n                                evaluationLog += (\n                                    \"Match Discarded GT #\"\n                                    + str(gtNum)\n                                    + \" with Det #\"\n                                    + str(detNum)\n                                    + \" normDist: \"\n                                    + str(normDist)\n                                    + \" \\n\"\n                                )\n            # Find one-to-many matches\n            evaluationLog += \"Find one-to-many matches\\n\"\n            for gtNum in range(len(gtRects)):\n                if gtNum not in gtDontCareRectsNum:\n                    match, matchesDet = one_to_many_match(gtNum)\n                    if match is True:\n                        evaluationLog += \"num_overlaps_gt=\" + str(\n                            num_overlaps_gt(gtNum)\n                        )\n                        gtRectMat[gtNum] = 1\n                        recallAccum += (\n                            self.mtype_oo_o if len(matchesDet) == 1 else self.mtype_om_o\n                        )\n                        precisionAccum += (\n                            self.mtype_oo_o\n                            if len(matchesDet) == 1\n                            else self.mtype_om_o * len(matchesDet)\n                        )\n                        pairs.append(\n                            {\n                                \"gt\": gtNum,\n                                \"det\": matchesDet,\n                                \"type\": \"OO\" if len(matchesDet) == 1 else \"OM\",\n                            }\n                        )\n                        for detNum in matchesDet:\n                            detRectMat[detNum] = 1\n                        evaluationLog += (\n                            \"Match GT #\"\n                            + str(gtNum)\n                            + \" with Det #\"\n                            + str(matchesDet)\n                            + \"\\n\"\n                        )\n\n            # Find many-to-one matches\n            evaluationLog += \"Find many-to-one matches\\n\"\n            for detNum in range(len(detRects)):\n                if detNum not in detDontCareRectsNum:\n                    match, matchesGt = many_to_one_match(detNum)\n                    if match is True:\n                        detRectMat[detNum] = 1\n                        recallAccum += (\n                            self.mtype_oo_o\n                            if len(matchesGt) == 1\n                            else self.mtype_om_m * len(matchesGt)\n                        )\n                        precisionAccum += (\n                            self.mtype_oo_o if len(matchesGt) == 1 else self.mtype_om_m\n                        )\n                        pairs.append(\n                            {\n                                \"gt\": matchesGt,\n                                \"det\": detNum,\n                                \"type\": \"OO\" if len(matchesGt) == 1 else \"MO\",\n                            }\n                        )\n                        for gtNum in matchesGt:\n                            gtRectMat[gtNum] = 1\n                        evaluationLog += (\n                            \"Match GT #\"\n                            + str(matchesGt)\n                            + \" with Det #\"\n                            + str(detNum)\n                            + \"\\n\"\n                        )\n\n            numGtCare = len(gtRects) - len(gtDontCareRectsNum)\n            if numGtCare == 0:\n                recall = float(1)\n                precision = float(0) if len(detRects) > 0 else float(1)\n            else:\n                recall = float(recallAccum) / numGtCare\n                precision = (\n                    float(0)\n                    if (len(detRects) - len(detDontCareRectsNum)) == 0\n                    else float(precisionAccum)\n                    / (len(detRects) - len(detDontCareRectsNum))\n                )\n            hmean = (\n                0\n                if (precision + recall) == 0\n                else 2.0 * precision * recall / (precision + recall)\n            )\n\n        numGtCare = len(gtRects) - len(gtDontCareRectsNum)\n        numDetCare = len(detRects) - len(detDontCareRectsNum)\n\n        perSampleMetrics = {\n            \"precision\": precision,\n            \"recall\": recall,\n            \"hmean\": hmean,\n            \"pairs\": pairs,\n            \"recallMat\": [] if len(detRects) > 100 else recallMat.tolist(),\n            \"precisionMat\": [] if len(detRects) > 100 else precisionMat.tolist(),\n            \"gtPolPoints\": gtPolPoints,\n            \"detPolPoints\": detPolPoints,\n            \"gtCare\": numGtCare,\n            \"detCare\": numDetCare,\n            \"gtDontCare\": gtDontCareRectsNum,\n            \"detDontCare\": detDontCareRectsNum,\n            \"recallAccum\": recallAccum,\n            \"precisionAccum\": precisionAccum,\n            \"evaluationLog\": evaluationLog,\n        }\n\n        return perSampleMetrics\n\n    def combine_results(self, results):\n        numGt = 0\n        numDet = 0\n        methodRecallSum = 0\n        methodPrecisionSum = 0\n\n        for result in results:\n            numGt += result[\"gtCare\"]\n            numDet += result[\"detCare\"]\n            methodRecallSum += result[\"recallAccum\"]\n            methodPrecisionSum += result[\"precisionAccum\"]\n\n        methodRecall = 0 if numGt == 0 else methodRecallSum / numGt\n        methodPrecision = 0 if numDet == 0 else methodPrecisionSum / numDet\n        methodHmean = (\n            0\n            if methodRecall + methodPrecision == 0\n            else 2 * methodRecall * methodPrecision / (methodRecall + methodPrecision)\n        )\n\n        methodMetrics = {\n            \"precision\": methodPrecision,\n            \"recall\": methodRecall,\n            \"hmean\": methodHmean,\n        }\n\n        return methodMetrics\n\n\nif __name__ == \"__main__\":\n    evaluator = DetectionICDAR2013Evaluator()\n    gts = [\n        [\n            {\n                \"points\": [(0, 0), (1, 0), (1, 1), (0, 1)],\n                \"text\": 1234,\n                \"ignore\": False,\n            },\n            {\n                \"points\": [(2, 2), (3, 2), (3, 3), (2, 3)],\n                \"text\": 5678,\n                \"ignore\": True,\n            },\n        ]\n    ]\n    preds = [\n        [\n            {\n                \"points\": [(0.1, 0.1), (1, 0), (1, 1), (0, 1)],\n                \"text\": 123,\n                \"ignore\": False,\n            }\n        ]\n    ]\n    results = []\n    for gt, pred in zip(gts, preds):\n        results.append(evaluator.evaluate_image(gt, pred))\n    metrics = evaluator.combine_results(results)\n    print(metrics)\n", "benchmark/PaddleOCR_DBNet/utils/cal_recall/rrc_evaluation_funcs.py": "#!/usr/bin/env python2\n# encoding: UTF-8\nimport json\nimport sys\n\nsys.path.append(\"./\")\nimport zipfile\nimport re\nimport sys\nimport os\nimport codecs\nimport traceback\nimport numpy as np\nfrom utils import order_points_clockwise\n\n\ndef print_help():\n    sys.stdout.write(\n        \"Usage: python %s.py -g=<gtFile> -s=<submFile> [-o=<outputFolder> -p=<jsonParams>]\"\n        % sys.argv[0]\n    )\n    sys.exit(2)\n\n\ndef load_zip_file_keys(file, fileNameRegExp=\"\"):\n    \"\"\"\n    Returns an array with the entries of the ZIP file that match with the regular expression.\n    The key's are the names or the file or the capturing group definied in the fileNameRegExp\n    \"\"\"\n    try:\n        archive = zipfile.ZipFile(file, mode=\"r\", allowZip64=True)\n    except:\n        raise Exception(\"Error loading the ZIP archive.\")\n\n    pairs = []\n\n    for name in archive.namelist():\n        addFile = True\n        keyName = name\n        if fileNameRegExp != \"\":\n            m = re.match(fileNameRegExp, name)\n            if m == None:\n                addFile = False\n            else:\n                if len(m.groups()) > 0:\n                    keyName = m.group(1)\n\n        if addFile:\n            pairs.append(keyName)\n\n    return pairs\n\n\ndef load_zip_file(file, fileNameRegExp=\"\", allEntries=False):\n    \"\"\"\n    Returns an array with the contents (filtered by fileNameRegExp) of a ZIP file.\n    The key's are the names or the file or the capturing group definied in the fileNameRegExp\n    allEntries validates that all entries in the ZIP file pass the fileNameRegExp\n    \"\"\"\n    try:\n        archive = zipfile.ZipFile(file, mode=\"r\", allowZip64=True)\n    except:\n        raise Exception(\"Error loading the ZIP archive\")\n\n    pairs = []\n    for name in archive.namelist():\n        addFile = True\n        keyName = name\n        if fileNameRegExp != \"\":\n            m = re.match(fileNameRegExp, name)\n            if m == None:\n                addFile = False\n            else:\n                if len(m.groups()) > 0:\n                    keyName = m.group(1)\n\n        if addFile:\n            pairs.append([keyName, archive.read(name)])\n        else:\n            if allEntries:\n                raise Exception(\"ZIP entry not valid: %s\" % name)\n\n    return dict(pairs)\n\n\ndef load_folder_file(file, fileNameRegExp=\"\", allEntries=False):\n    \"\"\"\n    Returns an array with the contents (filtered by fileNameRegExp) of a ZIP file.\n    The key's are the names or the file or the capturing group definied in the fileNameRegExp\n    allEntries validates that all entries in the ZIP file pass the fileNameRegExp\n    \"\"\"\n    pairs = []\n    for name in os.listdir(file):\n        addFile = True\n        keyName = name\n        if fileNameRegExp != \"\":\n            m = re.match(fileNameRegExp, name)\n            if m == None:\n                addFile = False\n            else:\n                if len(m.groups()) > 0:\n                    keyName = m.group(1)\n\n        if addFile:\n            pairs.append([keyName, open(os.path.join(file, name)).read()])\n        else:\n            if allEntries:\n                raise Exception(\"ZIP entry not valid: %s\" % name)\n\n    return dict(pairs)\n\n\ndef decode_utf8(raw):\n    \"\"\"\n    Returns a Unicode object on success, or None on failure\n    \"\"\"\n    try:\n        raw = codecs.decode(raw, \"utf-8\", \"replace\")\n        # extracts BOM if exists\n        raw = raw.encode(\"utf8\")\n        if raw.startswith(codecs.BOM_UTF8):\n            raw = raw.replace(codecs.BOM_UTF8, \"\", 1)\n        return raw.decode(\"utf-8\")\n    except:\n        return None\n\n\ndef validate_lines_in_file(\n    fileName,\n    file_contents,\n    CRLF=True,\n    LTRB=True,\n    withTranscription=False,\n    withConfidence=False,\n    imWidth=0,\n    imHeight=0,\n):\n    \"\"\"\n    This function validates that all lines of the file calling the Line validation function for each line\n    \"\"\"\n    utf8File = decode_utf8(file_contents)\n    if utf8File is None:\n        raise Exception(\"The file %s is not UTF-8\" % fileName)\n\n    lines = utf8File.split(\"\\r\\n\" if CRLF else \"\\n\")\n    for line in lines:\n        line = line.replace(\"\\r\", \"\").replace(\"\\n\", \"\")\n        if line != \"\":\n            try:\n                validate_tl_line(\n                    line, LTRB, withTranscription, withConfidence, imWidth, imHeight\n                )\n            except Exception as e:\n                raise Exception(\n                    (\n                        \"Line in sample not valid. Sample: %s Line: %s Error: %s\"\n                        % (fileName, line, str(e))\n                    ).encode(\"utf-8\", \"replace\")\n                )\n\n\ndef validate_tl_line(\n    line, LTRB=True, withTranscription=True, withConfidence=True, imWidth=0, imHeight=0\n):\n    \"\"\"\n    Validate the format of the line. If the line is not valid an exception will be raised.\n    If maxWidth and maxHeight are specified, all points must be inside the imgage bounds.\n    Posible values are:\n    LTRB=True: xmin,ymin,xmax,ymax[,confidence][,transcription]\n    LTRB=False: x1,y1,x2,y2,x3,y3,x4,y4[,confidence][,transcription]\n    \"\"\"\n    get_tl_line_values(line, LTRB, withTranscription, withConfidence, imWidth, imHeight)\n\n\ndef get_tl_line_values(\n    line,\n    LTRB=True,\n    withTranscription=False,\n    withConfidence=False,\n    imWidth=0,\n    imHeight=0,\n):\n    \"\"\"\n    Validate the format of the line. If the line is not valid an exception will be raised.\n    If maxWidth and maxHeight are specified, all points must be inside the imgage bounds.\n    Posible values are:\n    LTRB=True: xmin,ymin,xmax,ymax[,confidence][,transcription]\n    LTRB=False: x1,y1,x2,y2,x3,y3,x4,y4[,confidence][,transcription]\n    Returns values from a textline. Points , [Confidences], [Transcriptions]\n    \"\"\"\n    confidence = 0.0\n    transcription = \"\"\n    points = []\n\n    numPoints = 4\n\n    if LTRB:\n        numPoints = 4\n\n        if withTranscription and withConfidence:\n            m = re.match(\n                r\"^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-1].?[0-9]*)\\s*,(.*)$\",\n                line,\n            )\n            if m == None:\n                m = re.match(\n                    r\"^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-1].?[0-9]*)\\s*,(.*)$\",\n                    line,\n                )\n                raise Exception(\n                    \"Format incorrect. Should be: xmin,ymin,xmax,ymax,confidence,transcription\"\n                )\n        elif withConfidence:\n            m = re.match(\n                r\"^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-1].?[0-9]*)\\s*$\",\n                line,\n            )\n            if m == None:\n                raise Exception(\n                    \"Format incorrect. Should be: xmin,ymin,xmax,ymax,confidence\"\n                )\n        elif withTranscription:\n            m = re.match(\n                r\"^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-9]+)\\s*,(.*)$\",\n                line,\n            )\n            if m == None:\n                raise Exception(\n                    \"Format incorrect. Should be: xmin,ymin,xmax,ymax,transcription\"\n                )\n        else:\n            m = re.match(\n                r\"^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-9]+)\\s*,?\\s*$\",\n                line,\n            )\n            if m == None:\n                raise Exception(\"Format incorrect. Should be: xmin,ymin,xmax,ymax\")\n\n        xmin = int(m.group(1))\n        ymin = int(m.group(2))\n        xmax = int(m.group(3))\n        ymax = int(m.group(4))\n        if xmax < xmin:\n            raise Exception(\"Xmax value (%s) not valid (Xmax < Xmin).\" % (xmax))\n        if ymax < ymin:\n            raise Exception(\"Ymax value (%s)  not valid (Ymax < Ymin).\" % (ymax))\n\n        points = [float(m.group(i)) for i in range(1, (numPoints + 1))]\n\n        if imWidth > 0 and imHeight > 0:\n            validate_point_inside_bounds(xmin, ymin, imWidth, imHeight)\n            validate_point_inside_bounds(xmax, ymax, imWidth, imHeight)\n\n    else:\n        numPoints = 8\n\n        if withTranscription and withConfidence:\n            m = re.match(\n                r\"^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*([0-1].?[0-9]*)\\s*,(.*)$\",\n                line,\n            )\n            if m == None:\n                raise Exception(\n                    \"Format incorrect. Should be: x1,y1,x2,y2,x3,y3,x4,y4,confidence,transcription\"\n                )\n        elif withConfidence:\n            m = re.match(\n                r\"^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*([0-1].?[0-9]*)\\s*$\",\n                line,\n            )\n            if m == None:\n                raise Exception(\n                    \"Format incorrect. Should be: x1,y1,x2,y2,x3,y3,x4,y4,confidence\"\n                )\n        elif withTranscription:\n            m = re.match(\n                r\"^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,(.*)$\",\n                line,\n            )\n            if m == None:\n                raise Exception(\n                    \"Format incorrect. Should be: x1,y1,x2,y2,x3,y3,x4,y4,transcription\"\n                )\n        else:\n            m = re.match(\n                r\"^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*$\",\n                line,\n            )\n            if m == None:\n                raise Exception(\"Format incorrect. Should be: x1,y1,x2,y2,x3,y3,x4,y4\")\n\n        points = [float(m.group(i)) for i in range(1, (numPoints + 1))]\n\n        points = order_points_clockwise(np.array(points).reshape(-1, 2)).reshape(-1)\n        validate_clockwise_points(points)\n\n        if imWidth > 0 and imHeight > 0:\n            validate_point_inside_bounds(points[0], points[1], imWidth, imHeight)\n            validate_point_inside_bounds(points[2], points[3], imWidth, imHeight)\n            validate_point_inside_bounds(points[4], points[5], imWidth, imHeight)\n            validate_point_inside_bounds(points[6], points[7], imWidth, imHeight)\n\n    if withConfidence:\n        try:\n            confidence = float(m.group(numPoints + 1))\n        except ValueError:\n            raise Exception(\"Confidence value must be a float\")\n\n    if withTranscription:\n        posTranscription = numPoints + (2 if withConfidence else 1)\n        transcription = m.group(posTranscription)\n        m2 = re.match(r\"^\\s*\\\"(.*)\\\"\\s*$\", transcription)\n        if (\n            m2 != None\n        ):  # Transcription with double quotes, we extract the value and replace escaped characters\n            transcription = m2.group(1).replace(\"\\\\\\\\\", \"\\\\\").replace('\\\\\"', '\"')\n\n    return points, confidence, transcription\n\n\ndef validate_point_inside_bounds(x, y, imWidth, imHeight):\n    if x < 0 or x > imWidth:\n        raise Exception(\n            \"X value (%s) not valid. Image dimensions: (%s,%s)\"\n            % (xmin, imWidth, imHeight)\n        )\n    if y < 0 or y > imHeight:\n        raise Exception(\n            \"Y value (%s)  not valid. Image dimensions: (%s,%s) Sample: %s Line:%s\"\n            % (ymin, imWidth, imHeight)\n        )\n\n\ndef validate_clockwise_points(points):\n    \"\"\"\n    Validates that the points that the 4 points that dlimite a polygon are in clockwise order.\n    \"\"\"\n\n    if len(points) != 8:\n        raise Exception(\"Points list not valid.\" + str(len(points)))\n\n    point = [\n        [int(points[0]), int(points[1])],\n        [int(points[2]), int(points[3])],\n        [int(points[4]), int(points[5])],\n        [int(points[6]), int(points[7])],\n    ]\n    edge = [\n        (point[1][0] - point[0][0]) * (point[1][1] + point[0][1]),\n        (point[2][0] - point[1][0]) * (point[2][1] + point[1][1]),\n        (point[3][0] - point[2][0]) * (point[3][1] + point[2][1]),\n        (point[0][0] - point[3][0]) * (point[0][1] + point[3][1]),\n    ]\n\n    summatory = edge[0] + edge[1] + edge[2] + edge[3]\n    if summatory > 0:\n        raise Exception(\n            \"Points are not clockwise. The coordinates of bounding quadrilaterals have to be given in clockwise order. Regarding the correct interpretation of 'clockwise' remember that the image coordinate system used is the standard one, with the image origin at the upper left, the X axis extending to the right and Y axis extending downwards.\"\n        )\n\n\ndef get_tl_line_values_from_file_contents(\n    content,\n    CRLF=True,\n    LTRB=True,\n    withTranscription=False,\n    withConfidence=False,\n    imWidth=0,\n    imHeight=0,\n    sort_by_confidences=True,\n):\n    \"\"\"\n    Returns all points, confindences and transcriptions of a file in lists. Valid line formats:\n    xmin,ymin,xmax,ymax,[confidence],[transcription]\n    x1,y1,x2,y2,x3,y3,x4,y4,[confidence],[transcription]\n    \"\"\"\n    pointsList = []\n    transcriptionsList = []\n    confidencesList = []\n\n    lines = content.split(\"\\r\\n\" if CRLF else \"\\n\")\n    for line in lines:\n        line = line.replace(\"\\r\", \"\").replace(\"\\n\", \"\")\n        if line != \"\":\n            points, confidence, transcription = get_tl_line_values(\n                line, LTRB, withTranscription, withConfidence, imWidth, imHeight\n            )\n            pointsList.append(points)\n            transcriptionsList.append(transcription)\n            confidencesList.append(confidence)\n\n    if withConfidence and len(confidencesList) > 0 and sort_by_confidences:\n        import numpy as np\n\n        sorted_ind = np.argsort(-np.array(confidencesList))\n        confidencesList = [confidencesList[i] for i in sorted_ind]\n        pointsList = [pointsList[i] for i in sorted_ind]\n        transcriptionsList = [transcriptionsList[i] for i in sorted_ind]\n\n    return pointsList, confidencesList, transcriptionsList\n\n\ndef main_evaluation(\n    p,\n    default_evaluation_params_fn,\n    validate_data_fn,\n    evaluate_method_fn,\n    show_result=True,\n    per_sample=True,\n):\n    \"\"\"\n    This process validates a method, evaluates it and if it succed generates a ZIP file with a JSON entry for each sample.\n    Params:\n    p: Dictionary of parmeters with the GT/submission locations. If None is passed, the parameters send by the system are used.\n    default_evaluation_params_fn: points to a function that returns a dictionary with the default parameters used for the evaluation\n    validate_data_fn: points to a method that validates the corrct format of the submission\n    evaluate_method_fn: points to a function that evaluated the submission and return a Dictionary with the results\n    \"\"\"\n    evalParams = default_evaluation_params_fn()\n    if \"p\" in p.keys():\n        evalParams.update(\n            p[\"p\"] if isinstance(p[\"p\"], dict) else json.loads(p[\"p\"][1:-1])\n        )\n\n    resDict = {\"calculated\": True, \"Message\": \"\", \"method\": \"{}\", \"per_sample\": \"{}\"}\n    try:\n        # validate_data_fn(p['g'], p['s'], evalParams)\n        evalData = evaluate_method_fn(p[\"g\"], p[\"s\"], evalParams)\n        resDict.update(evalData)\n\n    except Exception as e:\n        traceback.print_exc()\n        resDict[\"Message\"] = str(e)\n        resDict[\"calculated\"] = False\n\n    if \"o\" in p:\n        if not os.path.exists(p[\"o\"]):\n            os.makedirs(p[\"o\"])\n\n        resultsOutputname = p[\"o\"] + \"/results.zip\"\n        outZip = zipfile.ZipFile(resultsOutputname, mode=\"w\", allowZip64=True)\n\n        del resDict[\"per_sample\"]\n        if \"output_items\" in resDict.keys():\n            del resDict[\"output_items\"]\n\n        outZip.writestr(\"method.json\", json.dumps(resDict))\n\n    if not resDict[\"calculated\"]:\n        if show_result:\n            sys.stderr.write(\"Error!\\n\" + resDict[\"Message\"] + \"\\n\\n\")\n        if \"o\" in p:\n            outZip.close()\n        return resDict\n\n    if \"o\" in p:\n        if per_sample == True:\n            for k, v in evalData[\"per_sample\"].iteritems():\n                outZip.writestr(k + \".json\", json.dumps(v))\n\n            if \"output_items\" in evalData.keys():\n                for k, v in evalData[\"output_items\"].iteritems():\n                    outZip.writestr(k, v)\n\n        outZip.close()\n\n    if show_result:\n        sys.stdout.write(\"Calculated!\")\n        sys.stdout.write(json.dumps(resDict[\"method\"]))\n\n    return resDict\n\n\ndef main_validation(default_evaluation_params_fn, validate_data_fn):\n    \"\"\"\n    This process validates a method\n    Params:\n    default_evaluation_params_fn: points to a function that returns a dictionary with the default parameters used for the evaluation\n    validate_data_fn: points to a method that validates the corrct format of the submission\n    \"\"\"\n    try:\n        p = dict([s[1:].split(\"=\") for s in sys.argv[1:]])\n        evalParams = default_evaluation_params_fn()\n        if \"p\" in p.keys():\n            evalParams.update(\n                p[\"p\"] if isinstance(p[\"p\"], dict) else json.loads(p[\"p\"][1:-1])\n            )\n\n        validate_data_fn(p[\"g\"], p[\"s\"], evalParams)\n        print(\"SUCCESS\")\n        sys.exit(0)\n    except Exception as e:\n        print(str(e))\n        sys.exit(101)\n", "benchmark/PaddleOCR_DBNet/utils/cal_recall/__init__.py": "# -*- coding: utf-8 -*-\n# @Time    : 1/16/19 6:40 AM\n# @Author  : zhoujun\nfrom .script import cal_recall_precison_f1\n\n__all__ = [\"cal_recall_precison_f1\"]\n", "benchmark/PaddleOCR_DBNet/utils/cal_recall/script.py": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nfrom collections import namedtuple\nfrom . import rrc_evaluation_funcs\nimport Polygon as plg\nimport numpy as np\n\n\ndef default_evaluation_params():\n    \"\"\"\n    default_evaluation_params: Default parameters to use for the validation and evaluation.\n    \"\"\"\n    return {\n        \"IOU_CONSTRAINT\": 0.5,\n        \"AREA_PRECISION_CONSTRAINT\": 0.5,\n        \"GT_SAMPLE_NAME_2_ID\": \"gt_img_([0-9]+).txt\",\n        \"DET_SAMPLE_NAME_2_ID\": \"res_img_([0-9]+).txt\",\n        \"LTRB\": False,  # LTRB:2points(left,top,right,bottom) or 4 points(x1,y1,x2,y2,x3,y3,x4,y4)\n        \"CRLF\": False,  # Lines are delimited by Windows CRLF format\n        \"CONFIDENCES\": False,  # Detections must include confidence value. AP will be calculated\n        \"PER_SAMPLE_RESULTS\": True,  # Generate per sample results and produce data for visualization\n    }\n\n\ndef validate_data(gtFilePath, submFilePath, evaluationParams):\n    \"\"\"\n    Method validate_data: validates that all files in the results folder are correct (have the correct name contents).\n                            Validates also that there are no missing files in the folder.\n                            If some error detected, the method raises the error\n    \"\"\"\n    gt = rrc_evaluation_funcs.load_folder_file(\n        gtFilePath, evaluationParams[\"GT_SAMPLE_NAME_2_ID\"]\n    )\n\n    subm = rrc_evaluation_funcs.load_folder_file(\n        submFilePath, evaluationParams[\"DET_SAMPLE_NAME_2_ID\"], True\n    )\n\n    # Validate format of GroundTruth\n    for k in gt:\n        rrc_evaluation_funcs.validate_lines_in_file(\n            k, gt[k], evaluationParams[\"CRLF\"], evaluationParams[\"LTRB\"], True\n        )\n\n    # Validate format of results\n    for k in subm:\n        if (k in gt) == False:\n            raise Exception(\"The sample %s not present in GT\" % k)\n\n        rrc_evaluation_funcs.validate_lines_in_file(\n            k,\n            subm[k],\n            evaluationParams[\"CRLF\"],\n            evaluationParams[\"LTRB\"],\n            False,\n            evaluationParams[\"CONFIDENCES\"],\n        )\n\n\ndef evaluate_method(gtFilePath, submFilePath, evaluationParams):\n    \"\"\"\n    Method evaluate_method: evaluate method and returns the results\n        Results. Dictionary with the following values:\n        - method (required)  Global method metrics. Ex: { 'Precision':0.8,'Recall':0.9 }\n        - samples (optional) Per sample metrics. Ex: {'sample1' : { 'Precision':0.8,'Recall':0.9 } , 'sample2' : { 'Precision':0.8,'Recall':0.9 }\n    \"\"\"\n\n    def polygon_from_points(points):\n        \"\"\"\n        Returns a Polygon object to use with the Polygon2 class from a list of 8 points: x1,y1,x2,y2,x3,y3,x4,y4\n        \"\"\"\n        resBoxes = np.empty([1, 8], dtype=\"int32\")\n        resBoxes[0, 0] = int(points[0])\n        resBoxes[0, 4] = int(points[1])\n        resBoxes[0, 1] = int(points[2])\n        resBoxes[0, 5] = int(points[3])\n        resBoxes[0, 2] = int(points[4])\n        resBoxes[0, 6] = int(points[5])\n        resBoxes[0, 3] = int(points[6])\n        resBoxes[0, 7] = int(points[7])\n        pointMat = resBoxes[0].reshape([2, 4]).T\n        return plg.Polygon(pointMat)\n\n    def rectangle_to_polygon(rect):\n        resBoxes = np.empty([1, 8], dtype=\"int32\")\n        resBoxes[0, 0] = int(rect.xmin)\n        resBoxes[0, 4] = int(rect.ymax)\n        resBoxes[0, 1] = int(rect.xmin)\n        resBoxes[0, 5] = int(rect.ymin)\n        resBoxes[0, 2] = int(rect.xmax)\n        resBoxes[0, 6] = int(rect.ymin)\n        resBoxes[0, 3] = int(rect.xmax)\n        resBoxes[0, 7] = int(rect.ymax)\n\n        pointMat = resBoxes[0].reshape([2, 4]).T\n\n        return plg.Polygon(pointMat)\n\n    def rectangle_to_points(rect):\n        points = [\n            int(rect.xmin),\n            int(rect.ymax),\n            int(rect.xmax),\n            int(rect.ymax),\n            int(rect.xmax),\n            int(rect.ymin),\n            int(rect.xmin),\n            int(rect.ymin),\n        ]\n        return points\n\n    def get_union(pD, pG):\n        areaA = pD.area()\n        areaB = pG.area()\n        return areaA + areaB - get_intersection(pD, pG)\n\n    def get_intersection_over_union(pD, pG):\n        try:\n            return get_intersection(pD, pG) / get_union(pD, pG)\n        except:\n            return 0\n\n    def get_intersection(pD, pG):\n        pInt = pD & pG\n        if len(pInt) == 0:\n            return 0\n        return pInt.area()\n\n    def compute_ap(confList, matchList, numGtCare):\n        correct = 0\n        AP = 0\n        if len(confList) > 0:\n            confList = np.array(confList)\n            matchList = np.array(matchList)\n            sorted_ind = np.argsort(-confList)\n            confList = confList[sorted_ind]\n            matchList = matchList[sorted_ind]\n            for n in range(len(confList)):\n                match = matchList[n]\n                if match:\n                    correct += 1\n                    AP += float(correct) / (n + 1)\n\n            if numGtCare > 0:\n                AP /= numGtCare\n\n        return AP\n\n    perSampleMetrics = {}\n\n    matchedSum = 0\n\n    Rectangle = namedtuple(\"Rectangle\", \"xmin ymin xmax ymax\")\n\n    gt = rrc_evaluation_funcs.load_folder_file(\n        gtFilePath, evaluationParams[\"GT_SAMPLE_NAME_2_ID\"]\n    )\n    subm = rrc_evaluation_funcs.load_folder_file(\n        submFilePath, evaluationParams[\"DET_SAMPLE_NAME_2_ID\"], True\n    )\n\n    numGlobalCareGt = 0\n    numGlobalCareDet = 0\n\n    arrGlobalConfidences = []\n    arrGlobalMatches = []\n\n    for resFile in gt:\n        gtFile = gt[resFile]  # rrc_evaluation_funcs.decode_utf8(gt[resFile])\n        recall = 0\n        precision = 0\n        hmean = 0\n\n        detMatched = 0\n\n        iouMat = np.empty([1, 1])\n\n        gtPols = []\n        detPols = []\n\n        gtPolPoints = []\n        detPolPoints = []\n\n        # Array of Ground Truth Polygons' keys marked as don't Care\n        gtDontCarePolsNum = []\n        # Array of Detected Polygons' matched with a don't Care GT\n        detDontCarePolsNum = []\n\n        pairs = []\n        detMatchedNums = []\n\n        arrSampleConfidences = []\n        arrSampleMatch = []\n        sampleAP = 0\n\n        evaluationLog = \"\"\n\n        (\n            pointsList,\n            _,\n            transcriptionsList,\n        ) = rrc_evaluation_funcs.get_tl_line_values_from_file_contents(\n            gtFile, evaluationParams[\"CRLF\"], evaluationParams[\"LTRB\"], True, False\n        )\n        for n in range(len(pointsList)):\n            points = pointsList[n]\n            transcription = transcriptionsList[n]\n            dontCare = transcription == \"###\"\n            if evaluationParams[\"LTRB\"]:\n                gtRect = Rectangle(*points)\n                gtPol = rectangle_to_polygon(gtRect)\n            else:\n                gtPol = polygon_from_points(points)\n            gtPols.append(gtPol)\n            gtPolPoints.append(points)\n            if dontCare:\n                gtDontCarePolsNum.append(len(gtPols) - 1)\n\n        evaluationLog += (\n            \"GT polygons: \"\n            + str(len(gtPols))\n            + (\n                \" (\" + str(len(gtDontCarePolsNum)) + \" don't care)\\n\"\n                if len(gtDontCarePolsNum) > 0\n                else \"\\n\"\n            )\n        )\n\n        if resFile in subm:\n            detFile = subm[resFile]  # rrc_evaluation_funcs.decode_utf8(subm[resFile])\n\n            (\n                pointsList,\n                confidencesList,\n                _,\n            ) = rrc_evaluation_funcs.get_tl_line_values_from_file_contents(\n                detFile,\n                evaluationParams[\"CRLF\"],\n                evaluationParams[\"LTRB\"],\n                False,\n                evaluationParams[\"CONFIDENCES\"],\n            )\n            for n in range(len(pointsList)):\n                points = pointsList[n]\n\n                if evaluationParams[\"LTRB\"]:\n                    detRect = Rectangle(*points)\n                    detPol = rectangle_to_polygon(detRect)\n                else:\n                    detPol = polygon_from_points(points)\n                detPols.append(detPol)\n                detPolPoints.append(points)\n                if len(gtDontCarePolsNum) > 0:\n                    for dontCarePol in gtDontCarePolsNum:\n                        dontCarePol = gtPols[dontCarePol]\n                        intersected_area = get_intersection(dontCarePol, detPol)\n                        pdDimensions = detPol.area()\n                        precision = (\n                            0 if pdDimensions == 0 else intersected_area / pdDimensions\n                        )\n                        if precision > evaluationParams[\"AREA_PRECISION_CONSTRAINT\"]:\n                            detDontCarePolsNum.append(len(detPols) - 1)\n                            break\n\n            evaluationLog += (\n                \"DET polygons: \"\n                + str(len(detPols))\n                + (\n                    \" (\" + str(len(detDontCarePolsNum)) + \" don't care)\\n\"\n                    if len(detDontCarePolsNum) > 0\n                    else \"\\n\"\n                )\n            )\n\n            if len(gtPols) > 0 and len(detPols) > 0:\n                # Calculate IoU and precision matrixs\n                outputShape = [len(gtPols), len(detPols)]\n                iouMat = np.empty(outputShape)\n                gtRectMat = np.zeros(len(gtPols), np.int8)\n                detRectMat = np.zeros(len(detPols), np.int8)\n                for gtNum in range(len(gtPols)):\n                    for detNum in range(len(detPols)):\n                        pG = gtPols[gtNum]\n                        pD = detPols[detNum]\n                        iouMat[gtNum, detNum] = get_intersection_over_union(pD, pG)\n\n                for gtNum in range(len(gtPols)):\n                    for detNum in range(len(detPols)):\n                        if (\n                            gtRectMat[gtNum] == 0\n                            and detRectMat[detNum] == 0\n                            and gtNum not in gtDontCarePolsNum\n                            and detNum not in detDontCarePolsNum\n                        ):\n                            if (\n                                iouMat[gtNum, detNum]\n                                > evaluationParams[\"IOU_CONSTRAINT\"]\n                            ):\n                                gtRectMat[gtNum] = 1\n                                detRectMat[detNum] = 1\n                                detMatched += 1\n                                pairs.append({\"gt\": gtNum, \"det\": detNum})\n                                detMatchedNums.append(detNum)\n                                evaluationLog += (\n                                    \"Match GT #\"\n                                    + str(gtNum)\n                                    + \" with Det #\"\n                                    + str(detNum)\n                                    + \"\\n\"\n                                )\n\n            if evaluationParams[\"CONFIDENCES\"]:\n                for detNum in range(len(detPols)):\n                    if detNum not in detDontCarePolsNum:\n                        # we exclude the don't care detections\n                        match = detNum in detMatchedNums\n\n                        arrSampleConfidences.append(confidencesList[detNum])\n                        arrSampleMatch.append(match)\n\n                        arrGlobalConfidences.append(confidencesList[detNum])\n                        arrGlobalMatches.append(match)\n\n        numGtCare = len(gtPols) - len(gtDontCarePolsNum)\n        numDetCare = len(detPols) - len(detDontCarePolsNum)\n        if numGtCare == 0:\n            recall = float(1)\n            precision = float(0) if numDetCare > 0 else float(1)\n            sampleAP = precision\n        else:\n            recall = float(detMatched) / numGtCare\n            precision = 0 if numDetCare == 0 else float(detMatched) / numDetCare\n            if (\n                evaluationParams[\"CONFIDENCES\"]\n                and evaluationParams[\"PER_SAMPLE_RESULTS\"]\n            ):\n                sampleAP = compute_ap(arrSampleConfidences, arrSampleMatch, numGtCare)\n\n        hmean = (\n            0\n            if (precision + recall) == 0\n            else 2.0 * precision * recall / (precision + recall)\n        )\n\n        matchedSum += detMatched\n        numGlobalCareGt += numGtCare\n        numGlobalCareDet += numDetCare\n\n        if evaluationParams[\"PER_SAMPLE_RESULTS\"]:\n            perSampleMetrics[resFile] = {\n                \"precision\": precision,\n                \"recall\": recall,\n                \"hmean\": hmean,\n                \"pairs\": pairs,\n                \"AP\": sampleAP,\n                \"iouMat\": [] if len(detPols) > 100 else iouMat.tolist(),\n                \"gtPolPoints\": gtPolPoints,\n                \"detPolPoints\": detPolPoints,\n                \"gtDontCare\": gtDontCarePolsNum,\n                \"detDontCare\": detDontCarePolsNum,\n                \"evaluationParams\": evaluationParams,\n                \"evaluationLog\": evaluationLog,\n            }\n\n    # Compute MAP and MAR\n    AP = 0\n    if evaluationParams[\"CONFIDENCES\"]:\n        AP = compute_ap(arrGlobalConfidences, arrGlobalMatches, numGlobalCareGt)\n\n    methodRecall = 0 if numGlobalCareGt == 0 else float(matchedSum) / numGlobalCareGt\n    methodPrecision = (\n        0 if numGlobalCareDet == 0 else float(matchedSum) / numGlobalCareDet\n    )\n    methodHmean = (\n        0\n        if methodRecall + methodPrecision == 0\n        else 2 * methodRecall * methodPrecision / (methodRecall + methodPrecision)\n    )\n\n    methodMetrics = {\n        \"precision\": methodPrecision,\n        \"recall\": methodRecall,\n        \"hmean\": methodHmean,\n        \"AP\": AP,\n    }\n\n    resDict = {\n        \"calculated\": True,\n        \"Message\": \"\",\n        \"method\": methodMetrics,\n        \"per_sample\": perSampleMetrics,\n    }\n\n    return resDict\n\n\ndef cal_recall_precison_f1(gt_path, result_path, show_result=False):\n    p = {\"g\": gt_path, \"s\": result_path}\n    result = rrc_evaluation_funcs.main_evaluation(\n        p, default_evaluation_params, validate_data, evaluate_method, show_result\n    )\n    return result[\"method\"]\n", "benchmark/PaddleOCR_DBNet/trainer/trainer.py": "# -*- coding: utf-8 -*-\n# @Time    : 2019/8/23 21:58\n# @Author  : zhoujun\nimport time\n\nimport paddle\nfrom tqdm import tqdm\n\nfrom base import BaseTrainer\nfrom utils import runningScore, cal_text_score, Polynomial, profiler\n\n\nclass Trainer(BaseTrainer):\n    def __init__(\n        self,\n        config,\n        model,\n        criterion,\n        train_loader,\n        validate_loader,\n        metric_cls,\n        post_process=None,\n        profiler_options=None,\n    ):\n        super(Trainer, self).__init__(\n            config,\n            model,\n            criterion,\n            train_loader,\n            validate_loader,\n            metric_cls,\n            post_process,\n        )\n        self.profiler_options = profiler_options\n        self.enable_eval = config[\"trainer\"].get(\"enable_eval\", True)\n\n    def _train_epoch(self, epoch):\n        self.model.train()\n        total_samples = 0\n        train_reader_cost = 0.0\n        train_batch_cost = 0.0\n        reader_start = time.time()\n        epoch_start = time.time()\n        train_loss = 0.0\n        running_metric_text = runningScore(2)\n\n        for i, batch in enumerate(self.train_loader):\n            profiler.add_profiler_step(self.profiler_options)\n            if i >= self.train_loader_len:\n                break\n            self.global_step += 1\n            lr = self.optimizer.get_lr()\n\n            cur_batch_size = batch[\"img\"].shape[0]\n\n            train_reader_cost += time.time() - reader_start\n            if self.amp:\n                with paddle.amp.auto_cast(\n                    enable=\"gpu\" in paddle.device.get_device(),\n                    custom_white_list=self.amp.get(\"custom_white_list\", []),\n                    custom_black_list=self.amp.get(\"custom_black_list\", []),\n                    level=self.amp.get(\"level\", \"O2\"),\n                ):\n                    preds = self.model(batch[\"img\"])\n                loss_dict = self.criterion(preds.astype(paddle.float32), batch)\n                scaled_loss = self.amp[\"scaler\"].scale(loss_dict[\"loss\"])\n                scaled_loss.backward()\n                self.amp[\"scaler\"].minimize(self.optimizer, scaled_loss)\n            else:\n                preds = self.model(batch[\"img\"])\n                loss_dict = self.criterion(preds, batch)\n                # backward\n                loss_dict[\"loss\"].backward()\n                self.optimizer.step()\n            self.lr_scheduler.step()\n            self.optimizer.clear_grad()\n\n            train_batch_time = time.time() - reader_start\n            train_batch_cost += train_batch_time\n            total_samples += cur_batch_size\n\n            # acc iou\n            score_shrink_map = cal_text_score(\n                preds[:, 0, :, :],\n                batch[\"shrink_map\"],\n                batch[\"shrink_mask\"],\n                running_metric_text,\n                thred=self.config[\"post_processing\"][\"args\"][\"thresh\"],\n            )\n\n            # loss \u548c acc \u8bb0\u5f55\u5230\u65e5\u5fd7\n            loss_str = \"loss: {:.4f}, \".format(loss_dict[\"loss\"].item())\n            for idx, (key, value) in enumerate(loss_dict.items()):\n                loss_dict[key] = value.item()\n                if key == \"loss\":\n                    continue\n                loss_str += \"{}: {:.4f}\".format(key, loss_dict[key])\n                if idx < len(loss_dict) - 1:\n                    loss_str += \", \"\n\n            train_loss += loss_dict[\"loss\"]\n            acc = score_shrink_map[\"Mean Acc\"]\n            iou_shrink_map = score_shrink_map[\"Mean IoU\"]\n\n            if self.global_step % self.log_iter == 0:\n                self.logger_info(\n                    \"[{}/{}], [{}/{}], global_step: {}, ips: {:.1f} samples/sec, avg_reader_cost: {:.5f} s, avg_batch_cost: {:.5f} s, avg_samples: {}, acc: {:.4f}, iou_shrink_map: {:.4f}, {}lr:{:.6}, time:{:.2f}\".format(\n                        epoch,\n                        self.epochs,\n                        i + 1,\n                        self.train_loader_len,\n                        self.global_step,\n                        total_samples / train_batch_cost,\n                        train_reader_cost / self.log_iter,\n                        train_batch_cost / self.log_iter,\n                        total_samples / self.log_iter,\n                        acc,\n                        iou_shrink_map,\n                        loss_str,\n                        lr,\n                        train_batch_cost,\n                    )\n                )\n                total_samples = 0\n                train_reader_cost = 0.0\n                train_batch_cost = 0.0\n\n            if self.visualdl_enable and paddle.distributed.get_rank() == 0:\n                # write tensorboard\n                for key, value in loss_dict.items():\n                    self.writer.add_scalar(\n                        \"TRAIN/LOSS/{}\".format(key), value, self.global_step\n                    )\n                self.writer.add_scalar(\"TRAIN/ACC_IOU/acc\", acc, self.global_step)\n                self.writer.add_scalar(\n                    \"TRAIN/ACC_IOU/iou_shrink_map\", iou_shrink_map, self.global_step\n                )\n                self.writer.add_scalar(\"TRAIN/lr\", lr, self.global_step)\n            reader_start = time.time()\n        return {\n            \"train_loss\": train_loss / self.train_loader_len,\n            \"lr\": lr,\n            \"time\": time.time() - epoch_start,\n            \"epoch\": epoch,\n        }\n\n    def _eval(self, epoch):\n        self.model.eval()\n        raw_metrics = []\n        total_frame = 0.0\n        total_time = 0.0\n        for i, batch in tqdm(\n            enumerate(self.validate_loader),\n            total=len(self.validate_loader),\n            desc=\"test model\",\n        ):\n            with paddle.no_grad():\n                start = time.time()\n                if self.amp:\n                    with paddle.amp.auto_cast(\n                        enable=\"gpu\" in paddle.device.get_device(),\n                        custom_white_list=self.amp.get(\"custom_white_list\", []),\n                        custom_black_list=self.amp.get(\"custom_black_list\", []),\n                        level=self.amp.get(\"level\", \"O2\"),\n                    ):\n                        preds = self.model(batch[\"img\"])\n                    preds = preds.astype(paddle.float32)\n                else:\n                    preds = self.model(batch[\"img\"])\n                boxes, scores = self.post_process(\n                    batch, preds, is_output_polygon=self.metric_cls.is_output_polygon\n                )\n                total_frame += batch[\"img\"].shape[0]\n                total_time += time.time() - start\n                raw_metric = self.metric_cls.validate_measure(batch, (boxes, scores))\n                raw_metrics.append(raw_metric)\n        metrics = self.metric_cls.gather_measure(raw_metrics)\n        self.logger_info(\"FPS:{}\".format(total_frame / total_time))\n        return metrics[\"recall\"].avg, metrics[\"precision\"].avg, metrics[\"fmeasure\"].avg\n\n    def _on_epoch_finish(self):\n        self.logger_info(\n            \"[{}/{}], train_loss: {:.4f}, time: {:.4f}, lr: {}\".format(\n                self.epoch_result[\"epoch\"],\n                self.epochs,\n                self.epoch_result[\"train_loss\"],\n                self.epoch_result[\"time\"],\n                self.epoch_result[\"lr\"],\n            )\n        )\n        net_save_path = \"{}/model_latest.pth\".format(self.checkpoint_dir)\n        net_save_path_best = \"{}/model_best.pth\".format(self.checkpoint_dir)\n\n        if paddle.distributed.get_rank() == 0:\n            self._save_checkpoint(self.epoch_result[\"epoch\"], net_save_path)\n            save_best = False\n            if (\n                self.validate_loader is not None\n                and self.metric_cls is not None\n                and self.enable_eval\n            ):  # \u4f7f\u7528f1\u4f5c\u4e3a\u6700\u4f18\u6a21\u578b\u6307\u6807\n                recall, precision, hmean = self._eval(self.epoch_result[\"epoch\"])\n\n                if self.visualdl_enable:\n                    self.writer.add_scalar(\"EVAL/recall\", recall, self.global_step)\n                    self.writer.add_scalar(\n                        \"EVAL/precision\", precision, self.global_step\n                    )\n                    self.writer.add_scalar(\"EVAL/hmean\", hmean, self.global_step)\n                self.logger_info(\n                    \"test: recall: {:.6f}, precision: {:.6f}, hmean: {:.6f}\".format(\n                        recall, precision, hmean\n                    )\n                )\n\n                if hmean >= self.metrics[\"hmean\"]:\n                    save_best = True\n                    self.metrics[\"train_loss\"] = self.epoch_result[\"train_loss\"]\n                    self.metrics[\"hmean\"] = hmean\n                    self.metrics[\"precision\"] = precision\n                    self.metrics[\"recall\"] = recall\n                    self.metrics[\"best_model_epoch\"] = self.epoch_result[\"epoch\"]\n            else:\n                if self.epoch_result[\"train_loss\"] <= self.metrics[\"train_loss\"]:\n                    save_best = True\n                    self.metrics[\"train_loss\"] = self.epoch_result[\"train_loss\"]\n                    self.metrics[\"best_model_epoch\"] = self.epoch_result[\"epoch\"]\n            best_str = \"current best, \"\n            for k, v in self.metrics.items():\n                best_str += \"{}: {:.6f}, \".format(k, v)\n            self.logger_info(best_str)\n            if save_best:\n                import shutil\n\n                shutil.copy(net_save_path, net_save_path_best)\n                self.logger_info(\"Saving current best: {}\".format(net_save_path_best))\n            else:\n                self.logger_info(\"Saving checkpoint: {}\".format(net_save_path))\n\n    def _on_train_finish(self):\n        if self.enable_eval:\n            for k, v in self.metrics.items():\n                self.logger_info(\"{}:{}\".format(k, v))\n        self.logger_info(\"finish train\")\n\n    def _initialize_scheduler(self):\n        if self.config[\"lr_scheduler\"][\"type\"] == \"Polynomial\":\n            self.config[\"lr_scheduler\"][\"args\"][\"epochs\"] = self.config[\"trainer\"][\n                \"epochs\"\n            ]\n            self.config[\"lr_scheduler\"][\"args\"][\"step_each_epoch\"] = len(\n                self.train_loader\n            )\n            self.lr_scheduler = Polynomial(**self.config[\"lr_scheduler\"][\"args\"])()\n        else:\n            self.lr_scheduler = self._initialize(\"lr_scheduler\", paddle.optimizer.lr)\n", "benchmark/PaddleOCR_DBNet/trainer/__init__.py": "# -*- coding: utf-8 -*-\n# @Time    : 2019/8/23 21:58\n# @Author  : zhoujun\nfrom .trainer import Trainer\n", "benchmark/PaddleOCR_DBNet/post_processing/seg_detector_representer.py": "import cv2\nimport numpy as np\nimport pyclipper\nimport paddle\nfrom shapely.geometry import Polygon\n\n\nclass SegDetectorRepresenter:\n    def __init__(\n        self, thresh=0.3, box_thresh=0.7, max_candidates=1000, unclip_ratio=1.5\n    ):\n        self.min_size = 3\n        self.thresh = thresh\n        self.box_thresh = box_thresh\n        self.max_candidates = max_candidates\n        self.unclip_ratio = unclip_ratio\n\n    def __call__(self, batch, pred, is_output_polygon=False):\n        \"\"\"\n        batch: (image, polygons, ignore_tags\n        batch: a dict produced by dataloaders.\n            image: tensor of shape (N, C, H, W).\n            polygons: tensor of shape (N, K, 4, 2), the polygons of objective regions.\n            ignore_tags: tensor of shape (N, K), indicates whether a region is ignorable or not.\n            shape: the original shape of images.\n            filename: the original filenames of images.\n        pred:\n            binary: text region segmentation map, with shape (N, H, W)\n            thresh: [if exists] thresh hold prediction with shape (N, H, W)\n            thresh_binary: [if exists] binarized with threshhold, (N, H, W)\n        \"\"\"\n        if isinstance(pred, paddle.Tensor):\n            pred = pred.numpy()\n        pred = pred[:, 0, :, :]\n        segmentation = self.binarize(pred)\n        boxes_batch = []\n        scores_batch = []\n        for batch_index in range(pred.shape[0]):\n            height, width = batch[\"shape\"][batch_index]\n            if is_output_polygon:\n                boxes, scores = self.polygons_from_bitmap(\n                    pred[batch_index], segmentation[batch_index], width, height\n                )\n            else:\n                boxes, scores = self.boxes_from_bitmap(\n                    pred[batch_index], segmentation[batch_index], width, height\n                )\n            boxes_batch.append(boxes)\n            scores_batch.append(scores)\n        return boxes_batch, scores_batch\n\n    def binarize(self, pred):\n        return pred > self.thresh\n\n    def polygons_from_bitmap(self, pred, _bitmap, dest_width, dest_height):\n        \"\"\"\n        _bitmap: single map with shape (H, W),\n            whose values are binarized as {0, 1}\n        \"\"\"\n\n        assert len(_bitmap.shape) == 2\n        bitmap = _bitmap  # The first channel\n        height, width = bitmap.shape\n        boxes = []\n        scores = []\n\n        contours, _ = cv2.findContours(\n            (bitmap * 255).astype(np.uint8), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE\n        )\n\n        for contour in contours[: self.max_candidates]:\n            epsilon = 0.005 * cv2.arcLength(contour, True)\n            approx = cv2.approxPolyDP(contour, epsilon, True)\n            points = approx.reshape((-1, 2))\n            if points.shape[0] < 4:\n                continue\n            # _, sside = self.get_mini_boxes(contour)\n            # if sside < self.min_size:\n            #     continue\n            score = self.box_score_fast(pred, contour.squeeze(1))\n            if self.box_thresh > score:\n                continue\n\n            if points.shape[0] > 2:\n                box = self.unclip(points, unclip_ratio=self.unclip_ratio)\n                if len(box) > 1:\n                    continue\n            else:\n                continue\n            box = box.reshape(-1, 2)\n            _, sside = self.get_mini_boxes(box.reshape((-1, 1, 2)))\n            if sside < self.min_size + 2:\n                continue\n\n            if not isinstance(dest_width, int):\n                dest_width = dest_width.item()\n                dest_height = dest_height.item()\n\n            box[:, 0] = np.clip(np.round(box[:, 0] / width * dest_width), 0, dest_width)\n            box[:, 1] = np.clip(\n                np.round(box[:, 1] / height * dest_height), 0, dest_height\n            )\n            boxes.append(box)\n            scores.append(score)\n        return boxes, scores\n\n    def boxes_from_bitmap(self, pred, _bitmap, dest_width, dest_height):\n        \"\"\"\n        _bitmap: single map with shape (H, W),\n            whose values are binarized as {0, 1}\n        \"\"\"\n\n        assert len(_bitmap.shape) == 2\n        bitmap = _bitmap  # The first channel\n        height, width = bitmap.shape\n        contours, _ = cv2.findContours(\n            (bitmap * 255).astype(np.uint8), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE\n        )\n        num_contours = min(len(contours), self.max_candidates)\n        boxes = np.zeros((num_contours, 4, 2), dtype=np.int16)\n        scores = np.zeros((num_contours,), dtype=np.float32)\n\n        for index in range(num_contours):\n            contour = contours[index].squeeze(1)\n            points, sside = self.get_mini_boxes(contour)\n            if sside < self.min_size:\n                continue\n            points = np.array(points)\n            score = self.box_score_fast(pred, contour)\n            if self.box_thresh > score:\n                continue\n\n            box = self.unclip(points, unclip_ratio=self.unclip_ratio).reshape(-1, 1, 2)\n            box, sside = self.get_mini_boxes(box)\n            if sside < self.min_size + 2:\n                continue\n            box = np.array(box)\n            if not isinstance(dest_width, int):\n                dest_width = dest_width.item()\n                dest_height = dest_height.item()\n\n            box[:, 0] = np.clip(np.round(box[:, 0] / width * dest_width), 0, dest_width)\n            box[:, 1] = np.clip(\n                np.round(box[:, 1] / height * dest_height), 0, dest_height\n            )\n            boxes[index, :, :] = box.astype(np.int16)\n            scores[index] = score\n        return boxes, scores\n\n    def unclip(self, box, unclip_ratio=1.5):\n        poly = Polygon(box)\n        distance = poly.area * unclip_ratio / poly.length\n        offset = pyclipper.PyclipperOffset()\n        offset.AddPath(box, pyclipper.JT_ROUND, pyclipper.ET_CLOSEDPOLYGON)\n        expanded = np.array(offset.Execute(distance))\n        return expanded\n\n    def get_mini_boxes(self, contour):\n        bounding_box = cv2.minAreaRect(contour)\n        points = sorted(list(cv2.boxPoints(bounding_box)), key=lambda x: x[0])\n\n        index_1, index_2, index_3, index_4 = 0, 1, 2, 3\n        if points[1][1] > points[0][1]:\n            index_1 = 0\n            index_4 = 1\n        else:\n            index_1 = 1\n            index_4 = 0\n        if points[3][1] > points[2][1]:\n            index_2 = 2\n            index_3 = 3\n        else:\n            index_2 = 3\n            index_3 = 2\n\n        box = [points[index_1], points[index_2], points[index_3], points[index_4]]\n        return box, min(bounding_box[1])\n\n    def box_score_fast(self, bitmap, _box):\n        h, w = bitmap.shape[:2]\n        box = _box.copy()\n        xmin = np.clip(np.floor(box[:, 0].min()).astype(np.int32), 0, w - 1)\n        xmax = np.clip(np.ceil(box[:, 0].max()).astype(np.int32), 0, w - 1)\n        ymin = np.clip(np.floor(box[:, 1].min()).astype(np.int32), 0, h - 1)\n        ymax = np.clip(np.ceil(box[:, 1].max()).astype(np.int32), 0, h - 1)\n\n        mask = np.zeros((ymax - ymin + 1, xmax - xmin + 1), dtype=np.uint8)\n        box[:, 0] = box[:, 0] - xmin\n        box[:, 1] = box[:, 1] - ymin\n        cv2.fillPoly(mask, box.reshape(1, -1, 2).astype(np.int32), 1)\n        return cv2.mean(bitmap[ymin : ymax + 1, xmin : xmax + 1], mask)[0]\n", "benchmark/PaddleOCR_DBNet/post_processing/__init__.py": "# -*- coding: utf-8 -*-\n# @Time    : 2019/12/5 15:17\n# @Author  : zhoujun\n\nfrom .seg_detector_representer import SegDetectorRepresenter\n\n\ndef get_post_processing(config):\n    try:\n        cls = eval(config[\"type\"])(**config[\"args\"])\n        return cls\n    except:\n        return None\n", "benchmark/PaddleOCR_DBNet/base/base_trainer.py": "# -*- coding: utf-8 -*-\n# @Time    : 2019/8/23 21:50\n# @Author  : zhoujun\n\nimport os\nimport pathlib\nimport shutil\nfrom pprint import pformat\n\nimport anyconfig\nimport paddle\nimport numpy as np\nimport random\nfrom paddle.jit import to_static\nfrom paddle.static import InputSpec\n\nfrom utils import setup_logger\n\n\nclass BaseTrainer:\n    def __init__(\n        self,\n        config,\n        model,\n        criterion,\n        train_loader,\n        validate_loader,\n        metric_cls,\n        post_process=None,\n    ):\n        config[\"trainer\"][\"output_dir\"] = os.path.join(\n            str(pathlib.Path(os.path.abspath(__name__)).parent),\n            config[\"trainer\"][\"output_dir\"],\n        )\n        config[\"name\"] = config[\"name\"] + \"_\" + model.name\n        self.save_dir = config[\"trainer\"][\"output_dir\"]\n        self.checkpoint_dir = os.path.join(self.save_dir, \"checkpoint\")\n\n        os.makedirs(self.checkpoint_dir, exist_ok=True)\n\n        self.global_step = 0\n        self.start_epoch = 0\n        self.config = config\n        self.criterion = criterion\n        # logger and tensorboard\n        self.visualdl_enable = self.config[\"trainer\"].get(\"visual_dl\", False)\n        self.epochs = self.config[\"trainer\"][\"epochs\"]\n        self.log_iter = self.config[\"trainer\"][\"log_iter\"]\n        if paddle.distributed.get_rank() == 0:\n            anyconfig.dump(config, os.path.join(self.save_dir, \"config.yaml\"))\n            self.logger = setup_logger(os.path.join(self.save_dir, \"train.log\"))\n            self.logger_info(pformat(self.config))\n\n        self.model = self.apply_to_static(model)\n\n        # device\n        if (\n            paddle.device.cuda.device_count() > 0\n            and paddle.device.is_compiled_with_cuda()\n        ):\n            self.with_cuda = True\n            random.seed(self.config[\"trainer\"][\"seed\"])\n            np.random.seed(self.config[\"trainer\"][\"seed\"])\n            paddle.seed(self.config[\"trainer\"][\"seed\"])\n        else:\n            self.with_cuda = False\n        self.logger_info(\"train with and paddle {}\".format(paddle.__version__))\n        # metrics\n        self.metrics = {\n            \"recall\": 0,\n            \"precision\": 0,\n            \"hmean\": 0,\n            \"train_loss\": float(\"inf\"),\n            \"best_model_epoch\": 0,\n        }\n\n        self.train_loader = train_loader\n        if validate_loader is not None:\n            assert post_process is not None and metric_cls is not None\n        self.validate_loader = validate_loader\n        self.post_process = post_process\n        self.metric_cls = metric_cls\n        self.train_loader_len = len(train_loader)\n\n        if self.validate_loader is not None:\n            self.logger_info(\n                \"train dataset has {} samples,{} in dataloader, validate dataset has {} samples,{} in dataloader\".format(\n                    len(self.train_loader.dataset),\n                    self.train_loader_len,\n                    len(self.validate_loader.dataset),\n                    len(self.validate_loader),\n                )\n            )\n        else:\n            self.logger_info(\n                \"train dataset has {} samples,{} in dataloader\".format(\n                    len(self.train_loader.dataset), self.train_loader_len\n                )\n            )\n\n        self._initialize_scheduler()\n\n        self._initialize_optimizer()\n\n        # resume or finetune\n        if self.config[\"trainer\"][\"resume_checkpoint\"] != \"\":\n            self._load_checkpoint(\n                self.config[\"trainer\"][\"resume_checkpoint\"], resume=True\n            )\n        elif self.config[\"trainer\"][\"finetune_checkpoint\"] != \"\":\n            self._load_checkpoint(\n                self.config[\"trainer\"][\"finetune_checkpoint\"], resume=False\n            )\n\n        if self.visualdl_enable and paddle.distributed.get_rank() == 0:\n            from visualdl import LogWriter\n\n            self.writer = LogWriter(self.save_dir)\n\n        # \u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\n        self.amp = self.config.get(\"amp\", None)\n        if self.amp == \"None\":\n            self.amp = None\n        if self.amp:\n            self.amp[\"scaler\"] = paddle.amp.GradScaler(\n                init_loss_scaling=self.amp.get(\"scale_loss\", 1024),\n                use_dynamic_loss_scaling=self.amp.get(\"use_dynamic_loss_scaling\", True),\n            )\n            self.model, self.optimizer = paddle.amp.decorate(\n                models=self.model,\n                optimizers=self.optimizer,\n                level=self.amp.get(\"amp_level\", \"O2\"),\n            )\n\n        # \u5206\u5e03\u5f0f\u8bad\u7ec3\n        if paddle.device.cuda.device_count() > 1:\n            self.model = paddle.DataParallel(self.model)\n        # make inverse Normalize\n        self.UN_Normalize = False\n        for t in self.config[\"dataset\"][\"train\"][\"dataset\"][\"args\"][\"transforms\"]:\n            if t[\"type\"] == \"Normalize\":\n                self.normalize_mean = t[\"args\"][\"mean\"]\n                self.normalize_std = t[\"args\"][\"std\"]\n                self.UN_Normalize = True\n\n    def apply_to_static(self, model):\n        support_to_static = self.config[\"trainer\"].get(\"to_static\", False)\n        if support_to_static:\n            specs = None\n            print(\"static\")\n            specs = [InputSpec([None, 3, -1, -1])]\n            model = to_static(model, input_spec=specs)\n            self.logger_info(\n                \"Successfully to apply @to_static with specs: {}\".format(specs)\n            )\n        return model\n\n    def train(self):\n        \"\"\"\n        Full training logic\n        \"\"\"\n        for epoch in range(self.start_epoch + 1, self.epochs + 1):\n            self.epoch_result = self._train_epoch(epoch)\n            self._on_epoch_finish()\n        if paddle.distributed.get_rank() == 0 and self.visualdl_enable:\n            self.writer.close()\n        self._on_train_finish()\n\n    def _train_epoch(self, epoch):\n        \"\"\"\n        Training logic for an epoch\n\n        :param epoch: Current epoch number\n        \"\"\"\n        raise NotImplementedError\n\n    def _eval(self, epoch):\n        \"\"\"\n        eval logic for an epoch\n\n        :param epoch: Current epoch number\n        \"\"\"\n        raise NotImplementedError\n\n    def _on_epoch_finish(self):\n        raise NotImplementedError\n\n    def _on_train_finish(self):\n        raise NotImplementedError\n\n    def _save_checkpoint(self, epoch, file_name):\n        \"\"\"\n        Saving checkpoints\n\n        :param epoch: current epoch number\n        :param log: logging information of the epoch\n        :param save_best: if True, rename the saved checkpoint to 'model_best.pth.tar'\n        \"\"\"\n        state_dict = self.model.state_dict()\n        state = {\n            \"epoch\": epoch,\n            \"global_step\": self.global_step,\n            \"state_dict\": state_dict,\n            \"optimizer\": self.optimizer.state_dict(),\n            \"config\": self.config,\n            \"metrics\": self.metrics,\n        }\n        filename = os.path.join(self.checkpoint_dir, file_name)\n        paddle.save(state, filename)\n\n    def _load_checkpoint(self, checkpoint_path, resume):\n        \"\"\"\n        Resume from saved checkpoints\n        :param checkpoint_path: Checkpoint path to be resumed\n        \"\"\"\n        self.logger_info(\"Loading checkpoint: {} ...\".format(checkpoint_path))\n        checkpoint = paddle.load(checkpoint_path)\n        self.model.set_state_dict(checkpoint[\"state_dict\"])\n        if resume:\n            self.global_step = checkpoint[\"global_step\"]\n            self.start_epoch = checkpoint[\"epoch\"]\n            self.config[\"lr_scheduler\"][\"args\"][\"last_epoch\"] = self.start_epoch\n            # self.scheduler.load_state_dict(checkpoint['scheduler'])\n            self.optimizer.set_state_dict(checkpoint[\"optimizer\"])\n            if \"metrics\" in checkpoint:\n                self.metrics = checkpoint[\"metrics\"]\n            self.logger_info(\n                \"resume from checkpoint {} (epoch {})\".format(\n                    checkpoint_path, self.start_epoch\n                )\n            )\n        else:\n            self.logger_info(\"finetune from checkpoint {}\".format(checkpoint_path))\n\n    def _initialize(self, name, module, *args, **kwargs):\n        module_name = self.config[name][\"type\"]\n        module_args = self.config[name].get(\"args\", {})\n        assert all(\n            [k not in module_args for k in kwargs]\n        ), \"Overwriting kwargs given in config file is not allowed\"\n        module_args.update(kwargs)\n        return getattr(module, module_name)(*args, **module_args)\n\n    def _initialize_scheduler(self):\n        self.lr_scheduler = self._initialize(\"lr_scheduler\", paddle.optimizer.lr)\n\n    def _initialize_optimizer(self):\n        self.optimizer = self._initialize(\n            \"optimizer\",\n            paddle.optimizer,\n            parameters=self.model.parameters(),\n            learning_rate=self.lr_scheduler,\n        )\n\n    def inverse_normalize(self, batch_img):\n        if self.UN_Normalize:\n            batch_img[:, 0, :, :] = (\n                batch_img[:, 0, :, :] * self.normalize_std[0] + self.normalize_mean[0]\n            )\n            batch_img[:, 1, :, :] = (\n                batch_img[:, 1, :, :] * self.normalize_std[1] + self.normalize_mean[1]\n            )\n            batch_img[:, 2, :, :] = (\n                batch_img[:, 2, :, :] * self.normalize_std[2] + self.normalize_mean[2]\n            )\n\n    def logger_info(self, s):\n        if paddle.distributed.get_rank() == 0:\n            self.logger.info(s)\n", "benchmark/PaddleOCR_DBNet/base/__init__.py": "from .base_trainer import BaseTrainer\nfrom .base_dataset import BaseDataSet\n", "configs/rec/multi_language/generate_multi_language_configs.py": "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport yaml\nfrom argparse import ArgumentParser, RawDescriptionHelpFormatter\nimport os.path\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\n\nsupport_list = {\n    \"it\": \"italian\",\n    \"xi\": \"spanish\",\n    \"pu\": \"portuguese\",\n    \"ru\": \"russian\",\n    \"ar\": \"arabic\",\n    \"ta\": \"tamil\",\n    \"ug\": \"uyghur\",\n    \"fa\": \"persian\",\n    \"ur\": \"urdu\",\n    \"rs\": \"serbian latin\",\n    \"oc\": \"occitan\",\n    \"rsc\": \"serbian cyrillic\",\n    \"bg\": \"bulgarian\",\n    \"uk\": \"ukranian\",\n    \"be\": \"belarusian\",\n    \"te\": \"telugu\",\n    \"ka\": \"kannada\",\n    \"chinese_cht\": \"chinese tradition\",\n    \"hi\": \"hindi\",\n    \"mr\": \"marathi\",\n    \"ne\": \"nepali\",\n}\n\nlatin_lang = [\n    \"af\",\n    \"az\",\n    \"bs\",\n    \"cs\",\n    \"cy\",\n    \"da\",\n    \"de\",\n    \"es\",\n    \"et\",\n    \"fr\",\n    \"ga\",\n    \"hr\",\n    \"hu\",\n    \"id\",\n    \"is\",\n    \"it\",\n    \"ku\",\n    \"la\",\n    \"lt\",\n    \"lv\",\n    \"mi\",\n    \"ms\",\n    \"mt\",\n    \"nl\",\n    \"no\",\n    \"oc\",\n    \"pi\",\n    \"pl\",\n    \"pt\",\n    \"ro\",\n    \"rs_latin\",\n    \"sk\",\n    \"sl\",\n    \"sq\",\n    \"sv\",\n    \"sw\",\n    \"tl\",\n    \"tr\",\n    \"uz\",\n    \"vi\",\n    \"latin\",\n]\narabic_lang = [\"ar\", \"fa\", \"ug\", \"ur\"]\ncyrillic_lang = [\n    \"ru\",\n    \"rs_cyrillic\",\n    \"be\",\n    \"bg\",\n    \"uk\",\n    \"mn\",\n    \"abq\",\n    \"ady\",\n    \"kbd\",\n    \"ava\",\n    \"dar\",\n    \"inh\",\n    \"che\",\n    \"lbe\",\n    \"lez\",\n    \"tab\",\n    \"cyrillic\",\n]\ndevanagari_lang = [\n    \"hi\",\n    \"mr\",\n    \"ne\",\n    \"bh\",\n    \"mai\",\n    \"ang\",\n    \"bho\",\n    \"mah\",\n    \"sck\",\n    \"new\",\n    \"gom\",\n    \"sa\",\n    \"bgc\",\n    \"devanagari\",\n]\nmulti_lang = latin_lang + arabic_lang + cyrillic_lang + devanagari_lang\n\nassert os.path.isfile(\n    \"./rec_multi_language_lite_train.yml\"\n), \"Loss basic configuration file rec_multi_language_lite_train.yml.\\\nYou can download it from \\\nhttps://github.com/PaddlePaddle/PaddleOCR/tree/dygraph/configs/rec/multi_language/\"\n\nglobal_config = yaml.load(\n    open(\"./rec_multi_language_lite_train.yml\", \"rb\"), Loader=yaml.Loader\n)\nproject_path = os.path.abspath(os.path.join(os.getcwd(), \"../../../\"))\n\n\nclass ArgsParser(ArgumentParser):\n    def __init__(self):\n        super(ArgsParser, self).__init__(formatter_class=RawDescriptionHelpFormatter)\n        self.add_argument(\"-o\", \"--opt\", nargs=\"+\", help=\"set configuration options\")\n        self.add_argument(\n            \"-l\",\n            \"--language\",\n            nargs=\"+\",\n            help=\"set language type, support {}\".format(support_list),\n        )\n        self.add_argument(\n            \"--train\",\n            type=str,\n            help=\"you can use this command to change the train dataset default path\",\n        )\n        self.add_argument(\n            \"--val\",\n            type=str,\n            help=\"you can use this command to change the eval dataset default path\",\n        )\n        self.add_argument(\n            \"--dict\",\n            type=str,\n            help=\"you can use this command to change the dictionary default path\",\n        )\n        self.add_argument(\n            \"--data_dir\",\n            type=str,\n            help=\"you can use this command to change the dataset default root path\",\n        )\n\n    def parse_args(self, argv=None):\n        args = super(ArgsParser, self).parse_args(argv)\n        args.opt = self._parse_opt(args.opt)\n        args.language = self._set_language(args.language)\n        return args\n\n    def _parse_opt(self, opts):\n        config = {}\n        if not opts:\n            return config\n        for s in opts:\n            s = s.strip()\n            k, v = s.split(\"=\")\n            config[k] = yaml.load(v, Loader=yaml.Loader)\n        return config\n\n    def _set_language(self, type):\n        lang = type[0]\n        assert type, \"please use -l or --language to choose language type\"\n        assert lang in support_list.keys() or lang in multi_lang, (\n            \"the sub_keys(-l or --language) can only be one of support list: \\n{},\\nbut get: {}, \"\n            \"please check your running command\".format(multi_lang, type)\n        )\n        if lang in latin_lang:\n            lang = \"latin\"\n        elif lang in arabic_lang:\n            lang = \"arabic\"\n        elif lang in cyrillic_lang:\n            lang = \"cyrillic\"\n        elif lang in devanagari_lang:\n            lang = \"devanagari\"\n        global_config[\"Global\"][\"character_dict_path\"] = (\n            \"ppocr/utils/dict/{}_dict.txt\".format(lang)\n        )\n        global_config[\"Global\"][\"save_model_dir\"] = \"./output/rec_{}_lite\".format(lang)\n        global_config[\"Train\"][\"dataset\"][\"label_file_list\"] = [\n            \"train_data/{}_train.txt\".format(lang)\n        ]\n        global_config[\"Eval\"][\"dataset\"][\"label_file_list\"] = [\n            \"train_data/{}_val.txt\".format(lang)\n        ]\n        global_config[\"Global\"][\"character_type\"] = lang\n        assert os.path.isfile(\n            os.path.join(project_path, global_config[\"Global\"][\"character_dict_path\"])\n        ), \"Loss default dictionary file {}_dict.txt.You can download it from \\\nhttps://github.com/PaddlePaddle/PaddleOCR/tree/dygraph/ppocr/utils/dict/\".format(\n            lang\n        )\n        return lang\n\n\ndef merge_config(config):\n    \"\"\"\n    Merge config into global config.\n    Args:\n        config (dict): Config to be merged.\n    Returns: global config\n    \"\"\"\n    for key, value in config.items():\n        if \".\" not in key:\n            if isinstance(value, dict) and key in global_config:\n                global_config[key].update(value)\n            else:\n                global_config[key] = value\n        else:\n            sub_keys = key.split(\".\")\n            assert (\n                sub_keys[0] in global_config\n            ), \"the sub_keys can only be one of global_config: {}, but get: {}, please check your running command\".format(\n                global_config.keys(), sub_keys[0]\n            )\n            cur = global_config[sub_keys[0]]\n            for idx, sub_key in enumerate(sub_keys[1:]):\n                if idx == len(sub_keys) - 2:\n                    cur[sub_key] = value\n                else:\n                    cur = cur[sub_key]\n\n\ndef loss_file(path):\n    assert os.path.exists(\n        path\n    ), \"There is no such file:{},Please do not forget to put in the specified file\".format(\n        path\n    )\n\n\nif __name__ == \"__main__\":\n    FLAGS = ArgsParser().parse_args()\n    merge_config(FLAGS.opt)\n    save_file_path = \"rec_{}_lite_train.yml\".format(FLAGS.language)\n    if os.path.isfile(save_file_path):\n        os.remove(save_file_path)\n\n    if FLAGS.train:\n        global_config[\"Train\"][\"dataset\"][\"label_file_list\"] = [FLAGS.train]\n        train_label_path = os.path.join(project_path, FLAGS.train)\n        loss_file(train_label_path)\n    if FLAGS.val:\n        global_config[\"Eval\"][\"dataset\"][\"label_file_list\"] = [FLAGS.val]\n        eval_label_path = os.path.join(project_path, FLAGS.val)\n        loss_file(eval_label_path)\n    if FLAGS.dict:\n        global_config[\"Global\"][\"character_dict_path\"] = FLAGS.dict\n        dict_path = os.path.join(project_path, FLAGS.dict)\n        loss_file(dict_path)\n    if FLAGS.data_dir:\n        global_config[\"Eval\"][\"dataset\"][\"data_dir\"] = FLAGS.data_dir\n        global_config[\"Train\"][\"dataset\"][\"data_dir\"] = FLAGS.data_dir\n        data_dir = os.path.join(project_path, FLAGS.data_dir)\n        loss_file(data_dir)\n\n    with open(save_file_path, \"w\") as f:\n        yaml.dump(dict(global_config), f, default_flow_style=False, sort_keys=False)\n    logging.info(\"Project path is          :{}\".format(project_path))\n    logging.info(\n        \"Train list path set to   :{}\".format(\n            global_config[\"Train\"][\"dataset\"][\"label_file_list\"][0]\n        )\n    )\n    logging.info(\n        \"Eval list path set to    :{}\".format(\n            global_config[\"Eval\"][\"dataset\"][\"label_file_list\"][0]\n        )\n    )\n    logging.info(\n        \"Dataset root path set to :{}\".format(\n            global_config[\"Eval\"][\"dataset\"][\"data_dir\"]\n        )\n    )\n    logging.info(\n        \"Dict path set to         :{}\".format(\n            global_config[\"Global\"][\"character_dict_path\"]\n        )\n    )\n    logging.info(\n        \"Config file set to       :configs/rec/multi_language/{}\".format(save_file_path)\n    )\n"}