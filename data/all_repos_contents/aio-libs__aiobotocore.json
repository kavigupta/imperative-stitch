{"setup.py": "import os\nimport re\n\nfrom setuptools import find_packages, setup\n\n# NOTE: If updating requirements make sure to also check Pipfile for any locks\n# NOTE: When updating botocore make sure to update awscli/boto3 versions below\ninstall_requires = [\n    # pegged to also match items in `extras_require`\n    'botocore>=1.34.70,<1.34.132',\n    'aiohttp>=3.9.2,<4.0.0',\n    'wrapt>=1.10.10, <2.0.0',\n    'aioitertools>=0.5.1,<1.0.0',\n]\n\nextras_require = {\n    'awscli': ['awscli>=1.32.70,<1.33.14'],\n    'boto3': ['boto3>=1.34.70,<1.34.132'],\n}\n\n\ndef read(f):\n    return open(os.path.join(os.path.dirname(__file__), f)).read().strip()\n\n\ndef read_version():\n    regexp = re.compile(r\"^__version__\\W*=\\W*'([\\d.abrc]+)'\")\n    init_py = os.path.join(\n        os.path.dirname(__file__), 'aiobotocore', '__init__.py'\n    )\n    with open(init_py) as f:\n        for line in f:\n            match = regexp.match(line)\n            if match is not None:\n                return match.group(1)\n        raise RuntimeError('Cannot find version in aiobotocore/__init__.py')\n\n\nsetup(\n    name='aiobotocore',\n    version=read_version(),\n    description='Async client for aws services using botocore and aiohttp',\n    long_description='\\n\\n'.join((read('README.rst'), read('CHANGES.rst'))),\n    long_description_content_type='text/x-rst',\n    classifiers=[\n        'Development Status :: 4 - Beta',\n        'Intended Audience :: Developers',\n        'Intended Audience :: System Administrators',\n        'Natural Language :: English',\n        'License :: OSI Approved :: Apache Software License',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.8',\n        'Programming Language :: Python :: 3.9',\n        'Programming Language :: Python :: 3.10',\n        'Programming Language :: Python :: 3.11',\n        'Programming Language :: Python :: 3.12',\n        'Environment :: Web Environment',\n        'Framework :: AsyncIO',\n    ],\n    author=\"Nikolay Novik\",\n    author_email=\"nickolainovik@gmail.com\",\n    url='https://github.com/aio-libs/aiobotocore',\n    download_url='https://pypi.python.org/pypi/aiobotocore',\n    license='Apache License 2.0',\n    packages=find_packages(include=['aiobotocore']),\n    python_requires='>=3.8',\n    install_requires=install_requires,\n    extras_require=extras_require,\n    include_package_data=True,\n)\n", "docs/conf.py": "#!/usr/bin/env python3\n#\n# aiobotocore documentation build configuration file, created by\n# sphinx-quickstart on Sun Dec 11 17:08:38 2016.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, os.path.abspath('.'))\n\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = '1.0'\n\nimport pathlib\nimport re\n\n_docs_path = pathlib.Path(__file__).parent\n_version_path = _docs_path / '../aiobotocore/__init__.py'\n\n\nwith _version_path.open() as fp:\n    try:\n        _version_info = re.search(\n            r\"^__version__ = '\"\n            r\"(?P<major>\\d+)\"\n            r\"\\.(?P<minor>\\d+)\"\n            r\"\\.(?P<patch>\\d+)\"\n            r\"(?P<tag>.*)?'$\",\n            fp.read(),\n            re.M,\n        ).groupdict()\n    except IndexError:\n        raise RuntimeError('Unable to determine version.')\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\n    'sphinx.ext.autodoc',\n    'sphinx.ext.intersphinx',\n    'sphinx.ext.todo',\n    'sphinx.ext.coverage',\n    'sphinx.ext.viewcode',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = ['_templates']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = ['.rst', '.md']\nsource_suffix = '.rst'\n\n# The master toctree document.\nmaster_doc = 'index'\n\n# General information about the project.\nproject = 'aiobotocore'\ncopyright = '2021, Nikolay Novik and aio-libs contributors'\nauthor = 'Nikolay Novik'\n\n# The version info for the project you're documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = '{major}.{minor}'.format(**_version_info)\n# The full version, including alpha/beta/rc tags.\nrelease = '{major}.{minor}.{patch}-{tag}'.format(**_version_info)\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set \"language\" from the command line for these cases.\nlanguage = \"en\"\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = 'sphinx'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = 'alabaster'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\naiobotocore_desc = 'Async client for aws services using botocore and aiohttp'\nhtml_theme_options = {\n    'description': aiobotocore_desc,\n    'github_user': 'aio-libs',\n    'github_repo': 'aiobotocore',\n    'github_button': True,\n    'github_type': 'star',\n    'github_banner': True,\n}\n\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = 'aiobotocoredoc'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size ('letterpaper' or 'a4paper').\n    #\n    # 'papersize': 'letterpaper',\n    # The font size ('10pt', '11pt' or '12pt').\n    #\n    # 'pointsize': '10pt',\n    # Additional stuff for the LaTeX preamble.\n    #\n    # 'preamble': '',\n    # Latex figure (float) alignment\n    #\n    # 'figure_align': 'htbp',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (\n        master_doc,\n        'aiobotocore.tex',\n        'aiobotocore Documentation',\n        'Nikolay Novik',\n        'manual',\n    ),\n]\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, 'aiobotocore', 'aiobotocore Documentation', [author], 1)\n]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (\n        master_doc,\n        'aiobotocore',\n        'aiobotocore Documentation',\n        author,\n        'aiobotocore',\n        aiobotocore_desc,\n        'Miscellaneous',\n    ),\n]\n\n\n# Example configuration for intersphinx: refer to the Python standard library.\nintersphinx_mapping = {'https://docs.python.org/': None}\n", "tests/test_waiter.py": "import pytest\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_sqs(cloudformation_client):\n    cloudformation_template = \"\"\"{\n      \"AWSTemplateFormatVersion\": \"2010-09-09\",\n      \"Resources\": {\n        \"queue1\": {\n          \"Type\": \"AWS::SQS::Queue\",\n          \"Properties\": {\n            \"QueueName\": \"my-queue\"\n          }\n        }\n      }\n    }\"\"\"\n\n    # Create stack\n    resp = await cloudformation_client.create_stack(\n        StackName='my-stack', TemplateBody=cloudformation_template\n    )\n\n    assert resp['ResponseMetadata']['HTTPStatusCode'] == 200\n\n    # wait for complete\n    waiter = cloudformation_client.get_waiter('stack_create_complete')\n    await waiter.wait(StackName='my-stack')\n", "tests/test_mturk.py": "import pytest\nfrom botocore.stub import ANY, Stubber\n\n_mturk_list_hits_response = {\n    'NumResults': 0,\n    'HITs': [],\n    'ResponseMetadata': {\n        'RequestId': '00000000-4989-4ffc-85cd-aaaaaaaaaaaa',\n        'HTTPStatusCode': 200,\n        'HTTPHeaders': {\n            'x-amzn-requestid': '00000000-4989-4ffc-85cd-aaaaaaaaaaaa',\n            'content-type': 'application/x-amz-json-1.1',\n            'content-length': '26',\n            'date': 'Thu, 04 Jun 2020 00:48:16 GMT',\n        },\n        'RetryAttempts': 0,\n    },\n}\n\n\n# Unfortunately moto does not support mturk yet\n# Also looks like we won't be able to support this (see notes from 1.0.6 release)\n# @pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_mturk_stubber(session):\n    async with session.create_client(\n        'mturk', region_name='us-east-1'\n    ) as client:\n        with Stubber(client) as stubber:\n            stubber.add_response(\n                'list_hits_for_qualification_type',\n                _mturk_list_hits_response,\n                {'QualificationTypeId': ANY},\n            )\n\n            response = await client.list_hits_for_qualification_type(\n                QualificationTypeId='string'\n            )\n            assert response == _mturk_list_hits_response\n", "tests/test_sqs.py": "import time\n\nimport pytest\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_list_queues(sqs_client, sqs_queue_url):\n    response = await sqs_client.list_queues()\n    pytest.aio.assert_status_code(response, 200)\n\n    assert sqs_queue_url in response['QueueUrls']\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_get_queue_name(sqs_client, sqs_queue_url):\n    queue_name = sqs_queue_url.rsplit('/', 1)[-1]\n\n    response = await sqs_client.get_queue_url(QueueName=queue_name)\n    pytest.aio.assert_status_code(response, 200)\n\n    assert sqs_queue_url == response['QueueUrl']\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_put_pull_delete_test(sqs_client, sqs_queue_url):\n    response = await sqs_client.send_message(\n        QueueUrl=sqs_queue_url,\n        MessageBody='test_message_1',\n        MessageAttributes={\n            'attr1': {'DataType': 'String', 'StringValue': 'value1'}\n        },\n    )\n    pytest.aio.assert_status_code(response, 200)\n\n    response = await sqs_client.receive_message(\n        QueueUrl=sqs_queue_url, MessageAttributeNames=['attr1']\n    )\n    pytest.aio.assert_status_code(response, 200)\n\n    # Messages wont be a key if its empty\n    assert len(response.get('Messages', [])) == 1\n    msg = response['Messages'][0]\n    assert msg['Body'] == 'test_message_1'\n    assert msg['MessageAttributes']['attr1']['StringValue'] == 'value1'\n\n    receipt_handle = response['Messages'][0]['ReceiptHandle']\n    response = await sqs_client.delete_message(\n        QueueUrl=sqs_queue_url, ReceiptHandle=receipt_handle\n    )\n    pytest.aio.assert_status_code(response, 200)\n    response = await sqs_client.receive_message(\n        QueueUrl=sqs_queue_url,\n    )\n    pytest.aio.assert_status_code(response, 200)\n    assert len(response.get('Messages', [])) == 0\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_put_pull_wait(sqs_client, sqs_queue_url):\n    start = time.perf_counter()\n    response = await sqs_client.receive_message(\n        QueueUrl=sqs_queue_url, WaitTimeSeconds=2\n    )\n    end = time.perf_counter()\n    pytest.aio.assert_status_code(response, 200)\n\n    assert 'Messages' not in response\n    assert end - start > 1.5\n", "tests/test_sns.py": "import json\n\nimport botocore\nimport pytest\n\n\ndef _get_topic_policy(topic_arn: str):\n    return {\n        \"Version\": \"2008-10-17\",\n        \"Id\": \"__default_policy_ID\",\n        \"Statement\": [\n            {\n                \"Effect\": \"Allow\",\n                \"Sid\": \"__default_statement_ID\",\n                \"Principal\": {\"AWS\": \"*\"},\n                \"Action\": [\n                    \"SNS:GetTopicAttributes\",\n                    \"SNS:SetTopicAttributes\",\n                    \"SNS:AddPermission\",\n                    \"SNS:RemovePermission\",\n                    \"SNS:DeleteTopic\",\n                    \"SNS:Subscribe\",\n                    \"SNS:ListSubscriptionsByTopic\",\n                    \"SNS:Publish\",\n                    \"SNS:Receive\",\n                ],\n                'Resource': topic_arn,\n                'Condition': {\n                    'StringEquals': {'AWS:SourceOwner': '123456789012'}\n                },\n            }\n        ],\n    }\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_topic_attributes(sns_client, topic_arn):\n    response = await sns_client.list_topics()\n    pytest.aio.assert_status_code(response, 200)\n    arn1 = response['Topics'][0]['TopicArn']\n    topic_properties = await sns_client.get_topic_attributes(TopicArn=arn1)\n    attributes = topic_properties['Attributes']\n\n    assert arn1 == topic_arn\n    assert json.loads(attributes['Policy']) == _get_topic_policy(topic_arn)\n    assert attributes['DisplayName'] == ''\n\n    display_name = 'My display name'\n    await sns_client.set_topic_attributes(\n        TopicArn=arn1, AttributeName='DisplayName', AttributeValue=display_name\n    )\n\n    topic_properties = await sns_client.get_topic_attributes(TopicArn=arn1)\n    attributes = topic_properties['Attributes']\n    assert attributes['DisplayName'] == display_name\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_creating_subscription(sns_client, topic_arn):\n    response = await sns_client.subscribe(\n        TopicArn=topic_arn, Protocol=\"http\", Endpoint=\"http://httpbin.org/\"\n    )\n    subscription_arn = response['SubscriptionArn']\n    subscriptions = (await sns_client.list_subscriptions())[\"Subscriptions\"]\n    assert len([s for s in subscriptions if s['Protocol'] == 'http']) == 1\n\n    await sns_client.unsubscribe(SubscriptionArn=subscription_arn)\n    subscriptions = (await sns_client.list_subscriptions())[\"Subscriptions\"]\n    assert len([s for s in subscriptions if s['Protocol'] == 'http']) == 0\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_publish_to_http(sns_client, topic_arn):\n    response = await sns_client.subscribe(\n        TopicArn=topic_arn,\n        Protocol='http',\n        Endpoint=\"http://httpbin.org/endpoint\",\n    )\n    subscription_arn = response['SubscriptionArn']\n\n    response = await sns_client.publish(\n        TopicArn=topic_arn,\n        Message=\"Test msg\",\n        Subject=\"my subject\",\n    )\n    pytest.aio.assert_status_code(response, 200)\n    await sns_client.unsubscribe(SubscriptionArn=subscription_arn)\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_get_missing_endpoint_attributes(sns_client):\n    with pytest.raises(botocore.exceptions.ClientError):\n        await sns_client.get_endpoint_attributes(EndpointArn=\"arn1\")\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_platform_applications(sns_client):\n    await sns_client.create_platform_application(\n        Name=\"app1\",\n        Platform=\"APNS\",\n        Attributes={},\n    )\n    await sns_client.create_platform_application(\n        Name=\"app2\",\n        Platform=\"APNS\",\n        Attributes={},\n    )\n\n    repsonse = await sns_client.list_platform_applications()\n    apps = repsonse['PlatformApplications']\n    assert len(apps) == 2\n", "tests/test_config.py": "import asyncio\n\nimport aiohttp.resolver\nimport pytest\nfrom botocore.config import Config\nfrom botocore.exceptions import ParamValidationError, ReadTimeoutError\n\nfrom aiobotocore.config import AioConfig\nfrom aiobotocore.httpsession import AIOHTTPSession\nfrom aiobotocore.session import AioSession, get_session\nfrom tests.mock_server import AIOServer\n\n\n# NOTE: this doesn't require moto but needs to be marked to run with coverage\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_connector_args():\n    with pytest.raises(ParamValidationError):\n        # wrong type\n        connector_args = dict(use_dns_cache=1)\n        AioConfig(connector_args)\n\n    with pytest.raises(ParamValidationError):\n        # wrong type\n        connector_args = dict(keepalive_timeout=\"1\")\n        AioConfig(connector_args)\n\n    with pytest.raises(ParamValidationError):\n        # wrong type\n        connector_args = dict(force_close=\"1\")\n        AioConfig(connector_args)\n\n    with pytest.raises(ParamValidationError):\n        # wrong type\n        connector_args = dict(keepalive_timeout=\"1\")\n        AioConfig(connector_args)\n\n    with pytest.raises(ParamValidationError):\n        # wrong type\n        connector_args = dict(ssl_context=\"1\")\n        AioConfig(connector_args)\n\n    with pytest.raises(ParamValidationError):\n        # invalid DNS resolver\n        connector_args = dict(resolver=\"1\")\n        AioConfig(connector_args)\n\n    with pytest.raises(ParamValidationError):\n        # invalid key\n        connector_args = dict(foo=\"1\")\n        AioConfig(connector_args)\n\n    # Test valid configs:\n    AioConfig({\"resolver\": aiohttp.resolver.DefaultResolver()})\n    AioConfig({'keepalive_timeout': None})\n\n    # test merge\n    cfg = Config(read_timeout=75)\n    aio_cfg = AioConfig({'keepalive_timeout': 75})\n    aio_cfg.merge(cfg)\n\n    assert cfg.read_timeout == 75\n    assert aio_cfg.connector_args['keepalive_timeout'] == 75\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_connector_timeout():\n    session = AioSession()\n    config = AioConfig(\n        max_pool_connections=1, connect_timeout=1, retries={'max_attempts': 0}\n    )\n    async with AIOServer() as server, session.create_client(\n        's3',\n        config=config,\n        endpoint_url=server.endpoint_url,\n        aws_secret_access_key='xxx',\n        aws_access_key_id='xxx',\n    ) as s3_client:\n\n        async def get_and_wait():\n            await s3_client.get_object(Bucket='foo', Key='bar')\n            await asyncio.sleep(100)\n\n        task1 = asyncio.Task(get_and_wait())\n        task2 = asyncio.Task(get_and_wait())\n\n        try:\n            done, pending = await asyncio.wait([task1, task2], timeout=3)\n\n            # second request should not timeout just because there isn't a\n            # connector available\n            assert len(pending) == 2\n        finally:\n            task1.cancel()\n            task2.cancel()\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_connector_timeout2():\n    session = AioSession()\n    config = AioConfig(\n        max_pool_connections=1,\n        connect_timeout=1,\n        read_timeout=1,\n        retries={'max_attempts': 0},\n    )\n    async with AIOServer() as server, session.create_client(\n        's3',\n        config=config,\n        endpoint_url=server.endpoint_url,\n        aws_secret_access_key='xxx',\n        aws_access_key_id='xxx',\n    ) as s3_client:\n        with pytest.raises(ReadTimeoutError):\n            resp = await s3_client.get_object(Bucket='foo', Key='bar')\n            await resp[\"Body\"].read()\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_get_session():\n    session = get_session()\n    assert isinstance(session, AioSession)\n\n\n@pytest.mark.moto\ndef test_merge():\n    config = AioConfig()\n    other_config = AioConfig()\n    new_config = config.merge(other_config)\n    assert isinstance(new_config, AioConfig)\n    assert new_config is not config\n    assert new_config is not other_config\n\n\n# Check that it's possible to specify custom http_session_cls\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_config_http_session_cls():\n    class SuccessExc(Exception):\n        ...\n\n    class MyHttpSession(AIOHTTPSession):\n        async def send(self, request):\n            raise SuccessExc\n\n    config = AioConfig(http_session_cls=MyHttpSession)\n    session = AioSession()\n    async with AIOServer() as server, session.create_client(\n        's3',\n        config=config,\n        endpoint_url=server.endpoint_url,\n        aws_secret_access_key='xxx',\n        aws_access_key_id='xxx',\n    ) as s3_client:\n        with pytest.raises(SuccessExc):\n            await s3_client.get_object(Bucket='foo', Key='bar')\n", "tests/test_eventstreams.py": "import botocore.parsers\nimport pytest\n\nfrom aiobotocore.eventstream import AioEventStream\n\n# TODO once Moto supports either S3 Select or Kinesis SubscribeToShard then\n# this can be tested against a real AWS API\n\n\nTEST_STREAM_DATA = (\n    b'\\x00\\x00\\x00w\\x00\\x00\\x00U5\\xd1F\\xcd\\r:message-type\\x07\\x00\\x05event\\x0b:event-'\n    b'type\\x07\\x00\\x07Records\\r:content-type\\x07\\x00\\x18application/octet-stream{\"hel'\n    b'lo\":\"world\"}\\nF\\x0e\\x9a2',\n    b'\\x00\\x00\\x00\\xce\\x00\\x00\\x00C\\xdc\\xd2\\x99\\xf9\\r:message-type\\x07\\x00\\x05event'\n    b'\\x0b:event-type\\x07\\x00\\x05Stats\\r:content-type\\x07\\x00\\x08text/xml<Stats xml'\n    b'ns=\"\"><BytesScanned>19</BytesScanned><BytesProcessed>19</BytesProcessed><Byte'\n    b'sReturned>18</BytesReturned></Stats>\\x92\\xd0?\\xa5\\x00\\x00\\x008\\x00\\x00\\x00(\\xc1'\n    b'\\xc6\\x84\\xd4\\r:message-type\\x07\\x00\\x05event\\x0b:event-type\\x07\\x00\\x03End\\xcf'\n    b'\\x97\\xd3\\x92',\n)\n\n\nclass FakeStreamReader:\n    class ChunkedIterator:\n        def __init__(self, chunks):\n            self.iter = iter(chunks)\n\n        def __aiter__(self):\n            return self\n\n        async def __anext__(self):\n            try:\n                result = next(self.iter)\n                return result, True\n            except StopIteration:\n                raise StopAsyncIteration()\n\n    def __init__(self, chunks):\n        self.chunks = chunks\n        self.content = self\n\n    def iter_chunks(self):\n        return self.ChunkedIterator(self.chunks)\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_eventstream_chunking(s3_client):\n    # These are the options passed to the EventStream class\n    # during a normal run with botocore.\n    operation_name = 'SelectObjectContent'\n    outputshape = s3_client._service_model.operation_model(\n        operation_name\n    ).output_shape.members['Payload']\n    parser = botocore.parsers.EventStreamXMLParser()\n    sr = FakeStreamReader(TEST_STREAM_DATA)\n\n    event_stream = AioEventStream(sr, outputshape, parser, operation_name)\n\n    events = []\n    # {'Records': {'Payload': b'{\"hello\":\"world\"}\\n'}}\n    # {'Stats': {'Details': {'BytesScanned': 19,\n    #                        'BytesProcessed': 19,\n    #                        'BytesReturned': 18}}}\n    # {'End': {}}\n    async for event in event_stream:\n        events.append(event)\n\n    assert len(events) == 3\n    event1, event2, event3 = events\n\n    assert 'Records' in event1\n    assert 'Stats' in event2\n    assert 'End' in event3\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_eventstream_no_iter(s3_client):\n    # These are the options passed to the EventStream class\n    # during a normal run with botocore.\n    operation_name = 'SelectObjectContent'\n    outputshape = s3_client._service_model.operation_model(\n        operation_name\n    ).output_shape.members['Payload']\n    parser = botocore.parsers.EventStreamXMLParser()\n    sr = FakeStreamReader(TEST_STREAM_DATA)\n\n    event_stream = AioEventStream(sr, outputshape, parser, operation_name)\n\n    with pytest.raises(NotImplementedError):\n        for _ in event_stream:\n            pass\n", "tests/test_patches.py": "import hashlib\n\nimport botocore\nimport pytest\nfrom botocore import retryhandler, stub\nfrom botocore.args import ClientArgsCreator\nfrom botocore.awsrequest import AWSResponse\nfrom botocore.client import BaseClient, ClientCreator, Config\nfrom botocore.configprovider import SmartDefaultsConfigStoreFactory\nfrom botocore.credentials import (\n    AssumeRoleCredentialFetcher,\n    AssumeRoleProvider,\n    AssumeRoleWithWebIdentityCredentialFetcher,\n    AssumeRoleWithWebIdentityProvider,\n    BotoProvider,\n    CachedCredentialFetcher,\n    CanonicalNameCredentialSourcer,\n    ConfigProvider,\n    ContainerProvider,\n    CredentialResolver,\n    Credentials,\n    EnvProvider,\n    InstanceMetadataProvider,\n    OriginalEC2Provider,\n    ProcessProvider,\n    ProfileProviderBuilder,\n    RefreshableCredentials,\n    SharedCredentialProvider,\n    SSOCredentialFetcher,\n    SSOProvider,\n    create_credential_resolver,\n    create_mfa_serial_refresher,\n    get_credentials,\n)\nfrom botocore.discovery import (\n    EndpointDiscoveryHandler,\n    EndpointDiscoveryManager,\n)\nfrom botocore.endpoint import (\n    Endpoint,\n    EndpointCreator,\n    convert_to_response_dict,\n)\nfrom botocore.eventstream import EventStream\nfrom botocore.handlers import (\n    _looks_like_special_case_error,\n    check_for_200_error,\n    inject_presigned_url_ec2,\n    inject_presigned_url_rds,\n    parse_get_bucket_location,\n)\nfrom botocore.hooks import EventAliaser, HierarchicalEmitter\nfrom botocore.httpchecksum import (\n    AwsChunkedWrapper,\n    _apply_request_trailer_checksum,\n    _handle_bytes_response,\n    apply_request_checksum,\n    handle_checksum_body,\n)\nfrom botocore.httpsession import URLLib3Session\nfrom botocore.paginate import PageIterator, ResultKeyIterator\nfrom botocore.parsers import (\n    PROTOCOL_PARSERS,\n    EC2QueryParser,\n    JSONParser,\n    QueryParser,\n    ResponseParserFactory,\n    RestJSONParser,\n    RestXMLParser,\n    create_parser,\n)\nfrom botocore.regions import EndpointRulesetResolver\nfrom botocore.response import StreamingBody, get_response\nfrom botocore.retries import adaptive, special, standard\nfrom botocore.retries.bucket import TokenBucket\nfrom botocore.session import Session, get_session\nfrom botocore.signers import (\n    RequestSigner,\n    S3PostPresigner,\n    add_generate_db_auth_token,\n    add_generate_presigned_post,\n    add_generate_presigned_url,\n    generate_db_auth_token,\n    generate_presigned_post,\n    generate_presigned_url,\n)\nfrom botocore.tokens import (\n    DeferredRefreshableToken,\n    SSOTokenProvider,\n    create_token_resolver,\n)\nfrom botocore.utils import (\n    ContainerMetadataFetcher,\n    IdentityCache,\n    IMDSFetcher,\n    IMDSRegionProvider,\n    InstanceMetadataFetcher,\n    InstanceMetadataRegionFetcher,\n    S3ExpressIdentityCache,\n    S3ExpressIdentityResolver,\n    S3RegionRedirector,\n    S3RegionRedirectorv2,\n)\nfrom botocore.waiter import (\n    NormalizedOperationMethod,\n    Waiter,\n    create_waiter_with_client,\n)\nfrom dill.source import getsource\n\n# This file ensures that our private patches will work going forward.  If a\n# method gets updated this will assert and someone will need to validate:\n# 1) If our code needs to be updated\n# 2) If our minimum botocore version needs to be updated\n# 3) If we need to replace the below hash (not backwards compatible) or add\n#    to the set\n\n# These are guards to our main patches\n\n# !!! README: HOW TO UPDATE THESE !!!\n# -----------------------------------\n# (tests break with new version of aiohttp/botocore)\n#\n# 1) Adding support for more versions of aiohttp/botocore\n#    In this scenario you need to ensure that aiobotocore supports the changes\n#    that broke these tests along with the old versions of the libraries\n#    and APPEND to the set of hashes that we support for each object you\n#    validated.\n# 2) Bumping up the base version of aiohttp/botocore that we support\n#    In this scenario ensure aiobotocore supports the new version of the libs\n#    and REPLACE all entries with the current hashes with the new libs.\n\n# REPLACE = backwards incompatible change\n# APPEND = officially supporting more versions of botocore/aiohttp\n\n# If you're changing these, most likely need to update setup.py as well.\n_API_DIGESTS = {\n    # args.py\n    ClientArgsCreator.get_client_args: {\n        '2dc13a6f32c470bc415a2cfc1f82cf569b1a5196'\n    },\n    ClientArgsCreator._build_endpoint_resolver: {\n        '0f80192233321ae4a55d95b68f5b8a68f3ad18e6',\n    },\n    # client.py\n    ClientCreator.create_client: {\n        'eeb7c4730ac86aec37de53b2be0779490b05f50b',\n    },\n    ClientCreator._create_client_class: {\n        'fcecaf8d4f2c1ac3c5d0eb50c573233ef86d641d'\n    },\n    ClientCreator._register_endpoint_discovery: {\n        '483c6c8e035810d1b76110fc1956de76943c2f18'\n    },\n    ClientCreator._get_client_args: {\n        'd5e19b1e62f64a745de842963c2472825a66e854'\n    },\n    ClientCreator._register_s3express_events: {\n        '716c1549989eef6bbd048bf4f134c1b4659e124a',\n    },\n    ClientCreator._register_s3_events: {\n        '4ab15da7cb36fa795f64154581a970b0966fdf50',\n    },\n    ClientCreator._register_retries: {\n        '16d3064142e5f9e45b0094bbfabf7be30183f255'\n    },\n    ClientCreator._register_v2_adaptive_retries: {\n        '665ecd77d36a5abedffb746d83a44bb0a64c660a'\n    },\n    ClientCreator._register_v2_standard_retries: {\n        '9ec4ff68599544b4f46067b3783287862d38fb50'\n    },\n    ClientCreator._register_legacy_retries: {\n        '000b2f2a122602e2e741ec2e89308dc2e2b67329'\n    },\n    BaseClient._make_api_call: {\n        '2cb11088d36a89cf9f5c41508bce908acbde24c4',\n    },\n    BaseClient._make_request: {'cfd8bbf19ea132134717cdf9c460694ddacdbf58'},\n    BaseClient._convert_to_request_dict: {\n        '5e0a374926b6ee1a8715963ab551e126506e7fc9'\n    },\n    BaseClient._emit_api_params: {'abd67874dae8d5cd2788412e4699398cb915a119'},\n    BaseClient._resolve_endpoint_ruleset: {\n        'f09731451ff6ba0645dc82e5c7948dfbf781e025',\n    },\n    BaseClient.get_paginator: {\n        '1c38079de68ccd43a5a06e36b1a47ec62233a7c2',\n    },\n    BaseClient.get_waiter: {\n        '4a4aeabe53af25d3737204187a31f930230864b4',\n    },\n    BaseClient.__getattr__: {'3ec17f468f50789fa633d6041f40b66a2f593e77'},\n    # config.py\n    Config.merge: {'c3dd8c3ffe0da86953ceba4a35267dfb79c6a2c8'},\n    Config: {\n        '9154efda503c21ab9aa048214ddca7aa637e2ff9',\n    },\n    # credentials.py\n    create_mfa_serial_refresher: {'9b5e98782fcacdcea5899a6d0d29d1b9de348bb0'},\n    Credentials.get_frozen_credentials: {\n        'eb247f2884aee311bdabba3435e749c3b8589100'\n    },\n    RefreshableCredentials.__init__: {\n        '25ee814f47e5ce617f57e893ae158e5fd6d358ea',\n    },\n    # We've overridden some properties\n    RefreshableCredentials.__dict__['access_key'].fset: {\n        'edc4a25baef877a9662f68cd9ccefcd33a81bab7'\n    },\n    RefreshableCredentials.__dict__['access_key'].fget: {\n        'f6c823210099db99dd343d9e1fae6d4eb5aa5fce'\n    },\n    RefreshableCredentials.__dict__['secret_key'].fset: {\n        'b19fe41d66822c72bd6ae2e60de5c5d27367868a'\n    },\n    RefreshableCredentials.__dict__['secret_key'].fget: {\n        '3e27331a037549104b8669e225bbbb2c465a16d4'\n    },\n    RefreshableCredentials.__dict__['token'].fset: {\n        '1f8a308d4bf21e666f8054a0546e91541661da7b'\n    },\n    RefreshableCredentials.__dict__['token'].fget: {\n        '005c1b44b616f37739ce9276352e4e83644d8220'\n    },\n    RefreshableCredentials._refresh: {\n        'd5731d01db2812d498df19b4bd5d7c17519241fe'\n    },\n    RefreshableCredentials._protected_refresh: {\n        '9f8fdb76f41c3b1c64fd4d03d0701504626939e5'\n    },\n    RefreshableCredentials.get_frozen_credentials: {\n        'f661c84a8b759786e011f0b1e8a468a0c6294e36'\n    },\n    SSOCredentialFetcher: {'fa2a1dd73e0ec37e250c97f55a7b2c341a7f836a'},\n    SSOProvider.load: {'67aba81dd1def437f2035f5e20b0720b328d970a'},\n    CachedCredentialFetcher._get_credentials: {\n        '02a7d13599d972e3f258d2b53f87eeda4cc3e3a4'\n    },\n    CachedCredentialFetcher.fetch_credentials: {\n        '0dd2986a4cbb38764ec747075306a33117e86c3d'\n    },\n    CachedCredentialFetcher._get_cached_credentials: {\n        'a9f8c348d226e62122972da9ccc025365b6803d6'\n    },\n    AssumeRoleCredentialFetcher._get_credentials: {\n        '5c575634bc0a713c10e5668f28fbfa8779d5a1da'\n    },\n    AssumeRoleCredentialFetcher._create_client: {\n        '27c76f07bd43e665899ca8d21b6ba2038b276fbb'\n    },\n    # Referenced by AioAssumeRoleWithWebIdentityCredentialFetcher\n    AssumeRoleWithWebIdentityCredentialFetcher.__init__: {\n        'ab270375dfe425c5e21276590dea690fdbfe40a5'\n    },\n    AssumeRoleWithWebIdentityCredentialFetcher._get_credentials: {\n        '02eba9d4e846474910cb076710070348e395a819'\n    },\n    AssumeRoleWithWebIdentityCredentialFetcher._assume_role_kwargs: {\n        '8fb4fefe8664b7d82a67e0fd6d6812c1c8d92285'\n    },\n    # Ensure that the load method doesn't do anything we should asyncify\n    EnvProvider.load: {'39871a6ec3b3f5d51bc967122793e86b7ca6ed3c'},\n    ContainerProvider.__init__: {'ea6aafb2e12730066af930fb5a27f7659c1736a1'},\n    ContainerProvider.load: {'57c35569050b45c1e9e33fcdb3b49da9e342fdcf'},\n    ContainerProvider._retrieve_or_fail: {\n        'c99153a4c68927810a3edde09ee98c5ba33d3697'\n    },\n    ContainerProvider._create_fetcher: {\n        'a921ee40b9b4779f238adcf369a3757b19857fc7'\n    },\n    InstanceMetadataProvider.load: {\n        '15becfc0373ccfbc1bb200bd6a34731e61561d06'\n    },\n    ProfileProviderBuilder._create_process_provider: {\n        'c5eea47bcfc449a6d73a9892bd0e1897f6be0c20'\n    },\n    ProfileProviderBuilder._create_shared_credential_provider: {\n        '33f99c6a0ef71a92b0c52ccc59c8ca7e33fa0890'\n    },\n    ProfileProviderBuilder._create_config_provider: {\n        'f9a40d4211f6e663ba2ae9682fba5306152178c5'\n    },\n    ProfileProviderBuilder._create_web_identity_provider: {\n        '478745fa6779a7c69fe9441d89d3e921438e3a59'\n    },\n    ProfileProviderBuilder._create_sso_provider: {\n        'e463160179add7a1a513e46ee848447a216504aa'\n    },\n    ConfigProvider.load: {'d0714da9f1f54cebc555df82f181c4913ce97258'},\n    SharedCredentialProvider.load: {\n        '8a17d992e2a90ebc0e07ba5a5dfef2b725367496'\n    },\n    ProcessProvider.__init__: {'2e870ec0c6b0bc8483fa9b1159ef68bbd7a12c56'},\n    ProcessProvider.load: {'6866e1d3abbde7a14e83aea28cc49377faaca84b'},\n    ProcessProvider._retrieve_credentials_using: {\n        'c12acda42ddc5dfd73946adce8c155295f8c6b88'\n    },\n    CredentialResolver.load_credentials: {\n        'ef31ba8817f84c1f61f36259da1cc6e597b8625a'\n    },\n    AssumeRoleWithWebIdentityProvider.load: {\n        '8f48f6cadf08a09cf5a22b1cc668e60bc4ea389d'\n    },\n    AssumeRoleWithWebIdentityProvider._assume_role_with_web_identity: {\n        '32c9d720ab5f12054583758b5cd5d287f652ccd3'\n    },\n    AssumeRoleProvider.load: {'ee9ddb43e25eb1105185253c0963a2f5add49a95'},\n    AssumeRoleProvider._load_creds_via_assume_role: {\n        '85116d63561c9a8bfdfffdbf837b8a7e61b47ea3'\n    },\n    AssumeRoleProvider._resolve_source_credentials: {\n        '105c0c011e23d76a3b8bd3d9b91b6d945c8307a1'\n    },\n    AssumeRoleProvider._resolve_credentials_from_profile: {\n        'a87ece979f8c94c1afd5801156e2b39f0d6d45ab'\n    },\n    AssumeRoleProvider._resolve_static_credentials_from_profile: {\n        'a470795f6ba451cf99ce7456fef24777f8087654'\n    },\n    AssumeRoleProvider._resolve_credentials_from_source: {\n        'de41138b36bfc74d7f8a21f6002b55279d3de017'\n    },\n    CanonicalNameCredentialSourcer.source_credentials: {\n        '602930a78e0e64e3b313a046aab5edc3bcf5c2d9'\n    },\n    CanonicalNameCredentialSourcer._get_provider: {\n        'c028b9776383cc566be10999745b6082f458d902'\n    },\n    BotoProvider.load: {'e84ebfe3d6698dc4683f0f6699d4a9907c87bebb'},\n    OriginalEC2Provider.load: {'dc58cd1c79d177105b183a2d20e1154e6f8f0733'},\n    create_credential_resolver: {\n        'fe797afd33126be87f86e44ab20475d50d727a4e',\n    },\n    get_credentials: {'ff0c735a388ac8dd7fe300a32c1e36cdf33c0f56'},\n    # configprovider.py\n    SmartDefaultsConfigStoreFactory.merge_smart_defaults: {\n        'e320299bb739694fefe2f5df6be62cc5321d3dc5'\n    },\n    SmartDefaultsConfigStoreFactory.resolve_auto_mode: {\n        '013fa8904b42931c69e3d8623025a1582379ba2a'\n    },\n    # endpoint.py\n    convert_to_response_dict: {'5b7701c1f5b3cb2daa6eb307cdbdbbb2e9d33e5f'},\n    Endpoint.create_request: {'37d0fbd02f91aef6c0499a2d0a725bf067c3ce8b'},\n    Endpoint._send_request: {'5d40748a95c3005728e6548b402b90cb57d6f575'},\n    Endpoint._get_response: {'bbf10e6e07147d50e09d7205bf0883bd673a8bf3'},\n    Endpoint._do_get_response: {'5afcfe76196406903afb24e05e3dd0feeac1a23d'},\n    Endpoint._needs_retry: {'f718e2ff874763a677648fe6f87cc65e4cec2792'},\n    Endpoint._send: {'644c7e5bb88fecaa0b2a204411f8c7e69cc90bf1'},\n    Endpoint._add_modeled_error_fields: {\n        'd0390647f2d7a4a325be048dcda4dcc7f42fdd17'\n    },\n    EndpointCreator.create_endpoint: {\n        '863e17b1299f9fda2cef5be3297d470d1bfa86ae'\n    },\n    # eventstream.py\n    EventStream._create_raw_event_generator: {\n        '1764be20b3abe19b60381756a989794de298ffbb'\n    },\n    EventStream.__iter__: {'8a9b454943f8ef6e81f5794d641adddd1fdd5248'},\n    EventStream.get_initial_response: {\n        'aed648305970c90bb5d1e31f6fe5ff12cf6a2a06'\n    },\n    # hooks.py\n    HierarchicalEmitter._emit: {'5d9a6b1aea1323667a9310e707a9f0a006f8f6e8'},\n    HierarchicalEmitter.emit_until_response: {\n        '23670e04e0b09a9575c7533442bca1b2972ade82'\n    },\n    HierarchicalEmitter._verify_and_register: {\n        '41eda968127e35e02e7120ec621240b61639e3dd'\n    },\n    EventAliaser.emit_until_response: {\n        '0d635bf7ae5022b1fdde891cd9a91cd4c449fd49'\n    },\n    # paginate.py\n    PageIterator.__iter__: {'a7e83728338e61ff2ca0a26c6f03c67cbabffc32'},\n    PageIterator.result_key_iters: {\n        'e8cd36fdc4960e08c9aa50196c4e5d1ee4e39756'\n    },\n    PageIterator.build_full_result: {\n        '9051327d350ed5a4843c74d34be74ba2f1732e30'\n    },\n    ResultKeyIterator: {'3028dde4c4de6029f628f4a9d1fff36986b41591'},\n    # parsers.py\n    ResponseParserFactory.create_parser: {\n        '5cf11c9acecd1f60a013f6facbe0f294daa3f390'\n    },\n    RestXMLParser._create_event_stream: {\n        '0564ba55383a71cc1ba3e5be7110549d7e9992f5'\n    },\n    EC2QueryParser._create_event_stream: {\n        '0564ba55383a71cc1ba3e5be7110549d7e9992f5'\n    },\n    QueryParser._create_event_stream: {\n        '0564ba55383a71cc1ba3e5be7110549d7e9992f5'\n    },\n    JSONParser._create_event_stream: {\n        '0564ba55383a71cc1ba3e5be7110549d7e9992f5'\n    },\n    JSONParser._do_parse: {'9c3d5832e6c55a87630128cc8b9121579ef4a708'},\n    JSONParser._handle_event_stream: {\n        '3cf7bb1ecff0d72bafd7e7fd6625595b4060abd6'\n    },\n    # NOTE: if this hits we need to change our ResponseParser impl in JSONParser\n    JSONParser.parse: {'c2153eac3789855f4fc6a816a1f30a6afe0cf969'},\n    RestJSONParser._create_event_stream: {\n        '0564ba55383a71cc1ba3e5be7110549d7e9992f5'\n    },\n    create_parser: {'37e9f1c3b60de17f477a9b79eae8e1acaa7c89d7'},\n    # regions.py\n    EndpointRulesetResolver.construct_endpoint: {\n        'ccbed61e316a0e92e1d0f67c554ee15efa4ee6b8'\n    },\n    EndpointRulesetResolver._get_provider_params: {\n        'e17f8fce4a5d8adba932cb85e588f369845ce534'\n    },\n    EndpointRulesetResolver._get_customized_builtins: {\n        '41085e0e1ac19915c24339f25b8d966708905fd0'\n    },\n    # response.py\n    StreamingBody: {'73cb1276dfb509331b964d3d5ed69e5efa008de5'},\n    get_response: {'6515f43730b546419695c26d4bc0d198fde54b10'},\n    # session.py\n    Session.__init__: {'c796153d589ea6fe46a3a1afa2c460f06a1c37a2'},\n    Session._register_response_parser_factory: {\n        'bb8f7f3cc4d9ff9551f0875604747c4bb5030ff6'\n    },\n    Session.create_client: {'a821ae3870f33b65b1ea7cd347ca0497ed306ccd'},\n    Session._create_token_resolver: {\n        '142df7a219db0dd9c96fd81dc9e84a764a2fe5fb'\n    },\n    Session._create_credential_resolver: {\n        '87e98d201c72d06f7fbdb4ebee2dce1c09de0fb2'\n    },\n    Session.get_credentials: {'718da08b630569e631f93aedd65f1d9215bfc30b'},\n    get_session: {'c47d588f5da9b8bde81ccc26eaef3aee19ddd901'},\n    Session.get_service_data: {'3879b969c0c2b1d5b454006a1025deb4322ae804'},\n    Session.get_service_model: {'1c8f93e6fb9913e859e43aea9bc2546edbea8365'},\n    Session.get_available_regions: {\n        '9fb4df0b7d082a74d524a4a15aaf92a2717e0358'\n    },\n    Session._register_smart_defaults_factory: {\n        'af5fc9cf6837ed119284603ca1086e4113febec0'\n    },\n    # signers.py\n    RequestSigner.handler: {'371909df136a0964ef7469a63d25149176c2b442'},\n    RequestSigner.sign: {\n        '2f1f45a6fcfcca1be2c5e292c9e1b80453e7fa57',\n    },\n    RequestSigner.get_auth: {'4f8099bef30f9a72fa3bcaa1bd3d22c4fbd224a8'},\n    RequestSigner.get_auth_instance: {\n        'b78756b9d4f7a5bce9630195d761bab557677225',\n    },\n    RequestSigner._choose_signer: {'bd0e9784029b8aa182b5aec73910d94cb67c36b0'},\n    RequestSigner.generate_presigned_url: {\n        '417682868eacc10bf4c65f3dfbdba7d20d9250db'\n    },\n    add_generate_presigned_url: {'5820f74ac46b004eb79e00eea1adc467bcf4defe'},\n    generate_presigned_url: {\n        'd03631d6810e2453b8874bc76619927b694a4207',\n    },\n    S3PostPresigner.generate_presigned_post: {\n        '269efc9af054a2fd2728d5b0a27db82c48053d7f'\n    },\n    add_generate_presigned_post: {'e30360f2bd893fabf47f5cdb04b0de420ccd414d'},\n    generate_presigned_post: {\n        'a3a834a08be2cf76c20ea137ba6b28e7a12f58ed',\n    },\n    add_generate_db_auth_token: {'f61014e6fac4b5c7ee7ac2d2bec15fb16fa9fbe5'},\n    generate_db_auth_token: {'1f37e1e5982d8528841ce6b79f229b3e23a18959'},\n    # tokens.py\n    create_token_resolver: {'b287f4879235a4292592a49b201d2b0bc2dbf401'},\n    DeferredRefreshableToken.__init__: {\n        '199254ed7e211119bdebf285c5d9a9789f6dc540'\n    },\n    DeferredRefreshableToken.get_frozen_token: {\n        '846a689a25550c63d2a460555dc27148abdcc992'\n    },\n    DeferredRefreshableToken._refresh: {\n        '92af1e549b5719caa246a81493823a37a684d017'\n    },\n    DeferredRefreshableToken._protected_refresh: {\n        'bd5c1911626e420005e0e60d583a73c68925f4b6'\n    },\n    SSOTokenProvider._attempt_create_token: {\n        '9cf7b75618a253d585819485e5da641cef129d46'\n    },\n    SSOTokenProvider._refresh_access_token: {\n        'cb179d1f262e41cc03a7c218e624e8c7fbeeaf19'\n    },\n    SSOTokenProvider._refresher: {'824d41775dbb8a05184f6e9c7b2ea7202b72f2a9'},\n    SSOTokenProvider.load_token: {'aea8584ef3fb83948ed82f2a2518eec40fb537a0'},\n    # utils.py\n    ContainerMetadataFetcher.__init__: {\n        '46d90a7249ba8389feb487779b0a02e6faa98e57'\n    },\n    ContainerMetadataFetcher.retrieve_full_uri: {\n        '2c7080f7d6ee5a3dacc1b690945c045dba1b1d21'\n    },\n    ContainerMetadataFetcher.retrieve_uri: {\n        '9067ffe122e6fcff4a7f4dd2b7f5b3de5e1ea4ec',\n    },\n    ContainerMetadataFetcher._retrieve_credentials: {\n        'b00694931af86ef1a9305ad29328030ee366cea9'\n    },\n    ContainerMetadataFetcher._get_response: {\n        'b2c2fe2d74ce1894168e8e052c4e97cc70539b1a',\n    },\n    IMDSFetcher.__init__: {\n        '50ea982d3b94d7301d39480f827cfc1502800cb4',\n    },\n    IMDSFetcher._get_request: {\n        '118354ef768da5a7402d5d2bf6f55b4fbb2525e4',\n    },\n    IMDSFetcher._fetch_metadata_token: {\n        '12225b35a73130632038785a8c2e6fbaaf9de1f4'\n    },\n    IMDSFetcher._default_retry: {'362ce5eff50bfb74e58fbdd3f44146a87958318a'},\n    IMDSFetcher._is_non_ok_response: {\n        '448b80545b1946ec44ff19ebca8d4993872a6281'\n    },\n    IMDSFetcher._is_empty: {'241b141c9c352a4ef72964f8399d46cbe9a5aebc'},\n    IMDSFetcher._log_imds_response: {\n        'dcbe619ce2ddb8b5015f128612d86dd8a5dd31e8'\n    },\n    InstanceMetadataFetcher.retrieve_iam_role_credentials: {\n        '40f31ba06abb9853c2e6fea68846742bd3eda919'\n    },\n    InstanceMetadataFetcher._get_iam_role: {\n        '80073d7adc9fb604bc6235af87241f5efc296ad7'\n    },\n    InstanceMetadataFetcher._get_credentials: {\n        '1a64f59a3ca70b83700bd14deeac25af14100d58'\n    },\n    InstanceMetadataFetcher._is_invalid_json: {\n        '97818b51182a2507c99876a40155adda0451dd82'\n    },\n    InstanceMetadataFetcher._needs_retry_for_role_name: {\n        'ca9557fb8e58d03e09d77f9fb63d21afb4689b58'\n    },\n    InstanceMetadataFetcher._needs_retry_for_credentials: {\n        'e7e5a8ce541110eb79bf98414171d3a1c137e32b'\n    },\n    S3RegionRedirectorv2.redirect_from_error: {\n        'ac37ca2ca48f7bde42d9659c01d5bd5bc08a78f9'\n    },\n    S3RegionRedirectorv2.get_bucket_region: {\n        'b5bbc8b010576668dc2812d657c4b48af79e8f99'\n    },\n    S3RegionRedirector.redirect_from_error: {\n        '3863b2c6472513b7896bfccc9dfd2567c472f441'\n    },\n    S3RegionRedirector.get_bucket_region: {\n        'b5bbc8b010576668dc2812d657c4b48af79e8f99'\n    },\n    InstanceMetadataRegionFetcher.retrieve_region: {\n        '0134024f0aa2d2b49ec436ea8058c1eca8fac4af'\n    },\n    InstanceMetadataRegionFetcher._get_region: {\n        '16e8fc546958471650eef233b0fd287758293019'\n    },\n    IMDSRegionProvider.provide: {'09d1b70bc1dd7a37cb9ffd437acd71283b9142e9'},\n    IMDSRegionProvider._get_instance_metadata_region: {\n        '4631ced79cff143de5d3fdf03cd69720778f141b'\n    },\n    IMDSRegionProvider._create_fetcher: {\n        '18da52c786a20d91615258a8127b566688ecbb39',\n    },\n    IdentityCache.get_credentials: {\n        'baf98c4caaddfa0594745eb490c327c65cff8920',\n    },\n    S3ExpressIdentityCache.get_credentials.__wrapped__: {\n        '71f2ae5e0ea32e9bbac6f318cba963700e23b9a0',\n    },\n    S3ExpressIdentityCache.build_refresh_callback: {\n        '0e833cc5e30b76fa13e8caf5c024fe2a21c10f22',\n    },\n    S3ExpressIdentityResolver.__init__: {\n        '148a10274d3268dd42df05d3bcfb98c668f01086',\n    },\n    # waiter.py\n    NormalizedOperationMethod.__call__: {\n        '79723632d023739aa19c8a899bc2b814b8ab12ff'\n    },\n    Waiter.wait: {'735608297a2a3d4572e6705daafcf4fc8556fc03'},\n    create_waiter_with_client: {'e6ea06674b6fdf9157c95757a12b3c9c35af531c'},\n    # handlers.py\n    inject_presigned_url_rds: {'b5d45b339686346e81b255d4e8c36e76d3fe6a78'},\n    inject_presigned_url_ec2: {'48e09a5e4e95577e716be30f2d2706949261a07f'},\n    parse_get_bucket_location: {'64ffbf5c6aa6ebd083f49371000fa046d0de1fc6'},\n    check_for_200_error: {'ded7f3aaef7b1a5d047c4dac86692ab55cbd7a13'},\n    _looks_like_special_case_error: {\n        '86946722d10a72b593483fca0abf30100c609178'\n    },\n    # httpsession.py\n    URLLib3Session: {\n        '1c418944abceb3a3d76c2c22348b4a39280d27ef',\n    },\n    EndpointDiscoveryHandler.discover_endpoint: {\n        'd87eff9008356a6aaa9b7078f23ba7a9ff0c7a60'\n    },\n    EndpointDiscoveryManager.describe_endpoint: {\n        'b2f1b29177cf30f299e61b85ddec09eaa070e54e'\n    },\n    EndpointDiscoveryManager._refresh_current_endpoints: {\n        'f8a51047c8f395d9458a904e778a3ac156a11911'\n    },\n    # retries/adaptive.py\n    # See comments in AsyncTokenBucket: we completely replace the ClientRateLimiter\n    # implementation from botocore.\n    adaptive.ClientRateLimiter: {'9dbf36d36614a4a2e2719ca7e4382aa4694caae3'},\n    adaptive.register_retry_handler: {\n        '96c073719a3d5d41d1ca7ae5f7e31bbb431c75b3'\n    },\n    # retries/standard.py\n    standard.register_retry_handler: {\n        'da0ae35712211bc38938e93c4af8b7aeb999084e'\n    },\n    standard.RetryHandler.needs_retry: {\n        '89a4148d7f4af9d2795d1d0189293528aa668b59'\n    },\n    standard.RetryPolicy.should_retry: {\n        'b30eadcb94dadcdb90a5810cdeb2e3a0bc0c74c9'\n    },\n    standard.StandardRetryConditions.__init__: {\n        'e17de49a447769160964a2da926b7d72544efd48'\n    },\n    standard.StandardRetryConditions.is_retryable: {\n        '558a0f0b4d30f996e046779fe233f587611ca5c7'\n    },\n    standard.OrRetryChecker.is_retryable: {\n        '5ef0b84b1ef3a49bc193d76a359dbd314682856b'\n    },\n    # retries/special.py\n    special.RetryDDBChecksumError.is_retryable: {\n        '6c6e0945b0989b13fd8e7d78dbfcde307a131eae'\n    },\n    # retries/bucket.py\n    # See comments in AsyncTokenBucket: we completely replace the TokenBucket\n    # implementation from botocore.\n    TokenBucket: {'ce932001b13e256d1a2cc625094989fff087d484'},\n    # awsresponse.py\n    AWSResponse.content: {'307a4eb1d46360ef808a876d7d00cbbde6198eb1'},\n    AWSResponse.text: {'a724100ba9f6d51b333b8fe470fac46376d5044a'},\n    # httpchecksum.py\n    handle_checksum_body: {\n        '898cee7a7a5e5a02af7e0e65dcbb8122257b85df',\n    },\n    _handle_bytes_response: {'0761c4590c6addbe8c674e40fca9f7dd375a184b'},\n    AwsChunkedWrapper._make_chunk: {\n        '097361692f0fd6c863a17dd695739629982ef7e4'\n    },\n    AwsChunkedWrapper.__iter__: {'261e26d1061655555fe3dcb2689d963e43f80fb0'},\n    apply_request_checksum: {'bcc044f0655f30769994efab72b29e76d73f7e39'},\n    _apply_request_trailer_checksum: {\n        '28cdf19282be7cd2c99a734831ec4f489648bcc7'\n    },\n    # retryhandler.py\n    retryhandler.create_retry_handler: {\n        '8fee36ed89d789194585f56b8dd4f525985a5811'\n    },\n    retryhandler.create_checker_from_retry_config: {\n        'bc43996b75ab9ffc7a4e8f20fc62805857867109'\n    },\n    retryhandler._create_single_checker: {\n        'da29339040ab1faeaf2d80752504e4f8116686f2'\n    },\n    retryhandler._create_single_response_checker: {\n        'dda92bb44f295a1f61750c7e1fbc176f66cb8b44'\n    },\n    retryhandler.RetryHandler.__call__: {\n        'e599399167b1f278e4cd839170f887d60eea5bfa'\n    },\n    retryhandler.MaxAttemptsDecorator.__call__: {\n        '24b442126f0ff730be0ae64dc7158929d4d2fca7'\n    },\n    retryhandler.MaxAttemptsDecorator._should_retry: {\n        '581273f875bb779a9ff796df8c8597ec551abf97',\n    },\n    retryhandler.MultiChecker.__call__: {\n        'e8302c52e1bbbb129b6f505633a4bc4ae1e5a34f'\n    },\n    retryhandler.CRC32Checker.__call__: {\n        '882a731eaf6b0ddca68ab4032a169a0fa09a4d43'\n    },\n    retryhandler.CRC32Checker._check_response: {\n        '3ee7afd0bb1a3bf53934d77e44f619962c52b0c9'\n    },\n    stub.Stubber: {'bccf23c3733cc656b909f5130cba80dbc9540b05'},\n}\n\n\n_PROTOCOL_PARSER_CONTENT = {'ec2', 'query', 'json', 'rest-json', 'rest-xml'}\n\n\n@pytest.mark.moto\ndef test_protocol_parsers():\n    # Check that no new parsers have been added\n    current_parsers = set(PROTOCOL_PARSERS.keys())\n    assert current_parsers == _PROTOCOL_PARSER_CONTENT\n\n\n# NOTE: this doesn't require moto but needs to be marked to run with coverage\n@pytest.mark.moto\ndef test_patches():\n    print(f\"Botocore version: {botocore.__version__}\")\n\n    success = True\n    for obj, digests in _API_DIGESTS.items():\n        try:\n            source = getsource(obj)\n        except TypeError:\n            obj = obj.fget\n            source = getsource(obj)\n\n        digest = hashlib.sha1(source.encode('utf-8')).hexdigest()\n\n        if digest not in digests:\n            print(\n                \"Digest of {}:{} not found in: {}\".format(\n                    obj.__qualname__, digest, digests\n                )\n            )\n            success = False\n\n    assert success\n", "tests/test_response.py": "import io\n\nimport pytest\nfrom botocore.exceptions import IncompleteReadError\n\nfrom aiobotocore import response\n\n\n# https://github.com/boto/botocore/blob/develop/tests/unit/test_response.py\nasync def assert_lines(line_iterator, expected_lines):\n    for expected_line in expected_lines:\n        line = await line_iterator.__anext__()\n        assert line == expected_line\n\n    # We should have exhausted the iterator.\n    with pytest.raises(StopAsyncIteration):\n        await line_iterator.__anext__()\n\n\nclass AsyncBytesIO(io.BytesIO):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.content = self\n\n    async def read(self, amt=-1):\n        if amt == -1:  # aiohttp to regular response\n            amt = None\n        return super().read(amt)\n\n\nasync def _tolist(aiter):\n    results = []\n    async for item in aiter:\n        results.append(item)\n    return results\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_streaming_wrapper_validates_content_length():\n    body = AsyncBytesIO(b'1234567890')\n    stream = response.StreamingBody(body, content_length=10)\n    assert await stream.read() == b'1234567890'\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_streaming_body_with_invalid_length():\n    body = AsyncBytesIO(b'123456789')\n    stream = response.StreamingBody(body, content_length=10)\n    with pytest.raises(IncompleteReadError):\n        assert await stream.read(9) == b'123456789'\n        # The next read will have nothing returned and raise\n        # an IncompleteReadError because we were expectd 10 bytes, not 9.\n        await stream.read()\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_streaming_body_with_zero_read():\n    body = AsyncBytesIO(b'1234567890')\n    stream = response.StreamingBody(body, content_length=10)\n    chunk = await stream.read(0)\n    assert chunk == b''\n    assert await stream.read() == b'1234567890'\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_streaming_body_with_single_read():\n    body = AsyncBytesIO(b'123456789')\n    stream = response.StreamingBody(body, content_length=10)\n    with pytest.raises(IncompleteReadError):\n        await stream.read()\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_streaming_body_closes():\n    body = AsyncBytesIO(b'1234567890')\n    stream = response.StreamingBody(body, content_length=10)\n    assert body.closed is False\n    stream.close()\n    assert body.closed is True\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_default_iter_behavior():\n    body = AsyncBytesIO(b'a' * 2048)\n    stream = response.StreamingBody(body, content_length=2048)\n    chunks = await _tolist(stream)\n    assert len(chunks) == 2\n    assert chunks, [b'a' * 1024 == b'a' * 1024]\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_iter_chunks_single_byte():\n    body = AsyncBytesIO(b'abcde')\n    stream = response.StreamingBody(body, content_length=5)\n    chunks = await _tolist(stream.iter_chunks(chunk_size=1))\n    assert chunks, [b'a', b'b', b'c', b'd' == b'e']\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_iter_chunks_with_leftover():\n    body = AsyncBytesIO(b'abcde')\n    stream = response.StreamingBody(body, content_length=5)\n    chunks = await _tolist(stream.iter_chunks(chunk_size=2))\n    assert chunks, [b'ab', b'cd' == b'e']\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_iter_chunks_single_chunk():\n    body = AsyncBytesIO(b'abcde')\n    stream = response.StreamingBody(body, content_length=5)\n    chunks = await _tolist(stream.iter_chunks(chunk_size=1024))\n    assert chunks == [b'abcde']\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_streaming_line_iterator():\n    body = AsyncBytesIO(b'1234567890\\n1234567890\\n12345')\n    stream = response.StreamingBody(body, content_length=27)\n    await assert_lines(\n        stream.iter_lines(),\n        [b'1234567890', b'1234567890', b'12345'],\n    )\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_streaming_line_iterator_ends_newline():\n    body = AsyncBytesIO(b'1234567890\\n1234567890\\n12345\\n')\n    stream = response.StreamingBody(body, content_length=28)\n    await assert_lines(\n        stream.iter_lines(),\n        [b'1234567890', b'1234567890', b'12345'],\n    )\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_streaming_line_iter_chunk_sizes():\n    for chunk_size in range(1, 30):\n        body = AsyncBytesIO(b'1234567890\\n1234567890\\n12345')\n        stream = response.StreamingBody(body, content_length=27)\n        await assert_lines(\n            stream.iter_lines(chunk_size),\n            [b'1234567890', b'1234567890', b'12345'],\n        )\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_streaming_body_is_an_iterator():\n    body = AsyncBytesIO(b'a' * 1024 + b'b' * 1024 + b'c' * 2)\n    stream = response.StreamingBody(body, content_length=2050)\n    assert b'a' * 1024 == await stream.__anext__()\n    assert b'b' * 1024 == await stream.__anext__()\n    assert b'c' * 2 == await stream.__anext__()\n    with pytest.raises(StopAsyncIteration):\n        await stream.__anext__()\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_streaming_line_abstruse_newline_standard():\n    for chunk_size in range(1, 30):\n        body = AsyncBytesIO(b'1234567890\\r\\n1234567890\\r\\n12345\\r\\n')\n        stream = response.StreamingBody(body, content_length=31)\n        await assert_lines(\n            stream.iter_lines(chunk_size),\n            [b'1234567890', b'1234567890', b'12345'],\n        )\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_streaming_line_empty_body():\n    stream = response.StreamingBody(\n        AsyncBytesIO(b''),\n        content_length=0,\n    )\n    await assert_lines(stream.iter_lines(), [])\n", "tests/test_lambda.py": "import base64\nimport io\nimport json\nimport zipfile\n\n# Third Party\nimport botocore.client\nimport pytest\n\n\nasync def _get_role_arn(iam_client, role_name: str):\n    try:\n        response = await iam_client.get_role(RoleName=role_name)\n        return response[\"Role\"][\"Arn\"]\n    except botocore.client.ClientError:\n        response = await iam_client.create_role(\n            RoleName=role_name,\n            AssumeRolePolicyDocument=\"some policy\",\n            Path=\"/my-path/\",\n        )\n        return response[\"Role\"][\"Arn\"]\n\n\ndef _process_lambda(func_str) -> bytes:\n    zip_output = io.BytesIO()\n    zip_file = zipfile.ZipFile(zip_output, \"w\", zipfile.ZIP_DEFLATED)\n    zip_file.writestr(\"lambda_function.py\", func_str)\n    zip_file.close()\n    zip_output.seek(0)\n    return zip_output.read()\n\n\n@pytest.fixture\ndef aws_lambda_zip() -> bytes:\n    lambda_src = \"\"\"\nimport json\ndef lambda_handler(event, context):\n    print(event)\n    return {\"statusCode\": 200, \"body\": event}\n\"\"\"\n    return _process_lambda(lambda_src)\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_run_lambda(iam_client, lambda_client, aws_lambda_zip):\n    role_arn = await _get_role_arn(iam_client, 'test-iam-role')\n    lambda_response = await lambda_client.create_function(\n        FunctionName='test-function',\n        Runtime='python3.8',\n        Role=role_arn,\n        Handler='lambda_function.lambda_handler',\n        Timeout=10,\n        MemorySize=128,\n        Publish=True,\n        Code={'ZipFile': aws_lambda_zip},\n    )\n    assert lambda_response['FunctionName'] == 'test-function'\n\n    invoke_response = await lambda_client.invoke(\n        FunctionName=\"test-function\",\n        InvocationType=\"RequestResponse\",\n        LogType='Tail',\n        Payload=json.dumps({\"hello\": \"world\"}),\n    )\n\n    async with invoke_response['Payload'] as stream:\n        data = await stream.read()\n\n    log_result = base64.b64decode(invoke_response[\"LogResult\"])\n\n    assert json.loads(data) == {'statusCode': 200, \"body\": {\"hello\": \"world\"}}\n    assert b\"{'hello': 'world'}\" in log_result\n", "tests/test_version.py": "import ast\nimport operator\nimport re\nfrom datetime import datetime\nfrom itertools import chain\nfrom pathlib import Path\nfrom typing import NamedTuple, Optional\n\nimport docutils.frontend\nimport docutils.nodes\nimport docutils.parsers.rst\nimport docutils.utils\nimport pytest\nimport requests\nfrom packaging import version\nfrom pip._internal.req import InstallRequirement\nfrom pip._internal.req.constructors import install_req_from_line\nfrom pip._vendor.packaging.specifiers import SpecifierSet\n\nimport aiobotocore\n\n_root_path = Path(__file__).absolute().parent.parent\n\n\n# date can be YYYY-MM-DD or \"TBD\"\n_rst_ver_date_str_re = re.compile(\n    r'(?P<version>\\d+\\.\\d+\\.\\d+(\\.dev\\d+)?) \\((?P<date>\\d{4}-\\d{2}-\\d{2}|TBD)\\)'\n)\n\n\n# from: https://stackoverflow.com/a/48719723/1241593\ndef _parse_rst(text: str) -> docutils.nodes.document:\n    parser = docutils.parsers.rst.Parser()\n    components = (docutils.parsers.rst.Parser,)\n    settings = docutils.frontend.OptionParser(\n        components=components\n    ).get_default_values()\n    document = docutils.utils.new_document('<rst-doc>', settings=settings)\n    parser.parse(text, document)\n    return document\n\n\ndef _get_assign_target_name(node: ast.Assign):\n    assert len(node.targets) == 1\n    target = node.targets[0]\n    assert isinstance(target, ast.Name)\n    return target.id\n\n\nclass VersionInfo(NamedTuple):\n    least_version: str\n    specifier_set: SpecifierSet\n\n\ndef _get_boto_module_versions(\n    setup_content: str, ensure_plus_one_patch_range: bool = False\n):\n    parsed = ast.parse(setup_content)\n    top_level_vars = {\"install_requires\", \"requires\", \"extras_require\"}\n    assignments = dict()\n    for node in parsed.body:\n        if isinstance(node, ast.Assign):\n            target_name = _get_assign_target_name(node)\n            if target_name not in top_level_vars:\n                continue\n\n            value = ast.literal_eval(node.value)\n            assignments[target_name] = value\n\n    module_versions = dict()\n\n    for ver in chain(\n        assignments.get(\"install_requires\", []),\n        assignments.get(\"requires\", []),\n        assignments.get(\"extras_require\", {}).values(),\n    ):\n        if isinstance(ver, str):\n            ver: InstallRequirement = install_req_from_line(ver)\n        elif isinstance(ver, list):\n            assert len(ver) == 1\n            ver: InstallRequirement = install_req_from_line(ver[0])\n        else:\n            assert False, f'Unsupported ver: {ver}'\n\n        module = ver.req.name\n        if module not in {'botocore', 'awscli', 'boto3'}:\n            continue\n\n        # NOTE: don't support complex versioning yet as requirements are unknown\n        gte: Optional[version.Version] = None\n        lt: Optional[version.Version] = None\n        eq: Optional[version.Version] = None\n        for spec in ver.req.specifier:\n            if spec.operator == '>=':\n                assert gte is None\n                gte = version.parse(spec.version)\n            elif spec.operator == '<':\n                assert lt is None\n                lt = version.parse(spec.version)\n            elif spec.operator == '==':\n                assert eq is None\n                eq = version.parse(spec.version)\n            else:\n                assert False, f'unsupported operator: {spec.operator}'\n\n        if ensure_plus_one_patch_range:\n            assert (\n                len(gte.release) == len(lt.release) == 3\n            ), f'{module} gte: {gte} diff len than {lt}'\n            assert lt.release == tuple(\n                map(operator.add, gte.release, (0, 0, 1))\n            ), f'{module} gte: {gte} not one patch off from {lt}'\n\n        module_versions[module] = VersionInfo(\n            gte.public if gte else None, ver.req.specifier\n        )\n\n    return module_versions\n\n\n@pytest.mark.moto\ndef test_release_versions():\n    # ensures versions in CHANGES.rst + __init__.py match\n    init_version = version.parse(aiobotocore.__version__)\n\n    changes_path = _root_path / 'CHANGES.rst'\n\n    with changes_path.open('r') as f:\n        changes_doc = _parse_rst(f.read())\n\n    rst_ver_str = changes_doc[0][1][0][0]  # ex: 0.11.1 (2020-01-03)\n    rst_prev_ver_str = changes_doc[0][2][0][0]\n\n    rst_ver_groups = _rst_ver_date_str_re.match(rst_ver_str)\n    rst_prev_ver_groups = _rst_ver_date_str_re.match(rst_prev_ver_str)\n\n    rst_ver = version.parse(rst_ver_groups['version'])\n    rst_prev_ver = version.parse(rst_prev_ver_groups['version'])\n\n    # first the init version should match the rst version\n    assert init_version == rst_ver\n\n    # the current version must be greater than the previous version\n    assert rst_ver > rst_prev_ver\n\n    rst_date = rst_ver_groups['date']\n    rst_prev_date = rst_prev_ver_groups['date']\n\n    if rst_date == 'TBD':\n        # TODO: we can now lock if we're a prerelease version\n        pass\n        # assert (\n        #     rst_ver.is_prerelease\n        # ), 'Version must be prerelease if final release date not set'\n    else:\n        assert (\n            not rst_ver.is_prerelease\n        ), 'Version must not be prerelease if release date set'\n\n        rst_date = datetime.strptime(rst_date, '%Y-%m-%d').date()\n        rst_prev_date = datetime.strptime(rst_prev_date, '%Y-%m-%d').date()\n\n        assert (\n            rst_date >= rst_prev_date\n        ), 'Current release must be after last release'\n\n    # get aioboto reqs\n    with (_root_path / 'setup.py').open() as f:\n        content = f.read()\n        aioboto_reqs = _get_boto_module_versions(content, False)\n\n    # get awscli reqs\n    awscli_resp = requests.get(\n        f\"https://raw.githubusercontent.com/aws/aws-cli/\"\n        f\"{aioboto_reqs['awscli'].least_version}/setup.py\"\n    )\n    awscli_reqs = _get_boto_module_versions(awscli_resp.text)\n    assert awscli_reqs['botocore'].specifier_set.contains(\n        aioboto_reqs['botocore'].least_version\n    )\n\n    # get boto3 reqs\n    boto3_resp = requests.get(\n        f\"https://raw.githubusercontent.com/boto/boto3/\"\n        f\"{aioboto_reqs['boto3'].least_version}/setup.py\"\n    )\n    boto3_reqs = _get_boto_module_versions(boto3_resp.text)\n    assert boto3_reqs['botocore'].specifier_set.contains(\n        aioboto_reqs['botocore'].least_version\n    )\n\n    print()\n", "tests/test_dynamodb.py": "import asyncio\nimport uuid\n\nimport pytest\n\nfrom aiobotocore.waiter import WaiterError\n\n\n@pytest.fixture\ndef dynamodb_table_def():\n    table_name = str(uuid.uuid4())\n    return dict(\n        TableName=table_name,\n        AttributeDefinitions=[\n            {'AttributeName': 'testKey', 'AttributeType': 'N'},\n        ],\n        KeySchema=[\n            {'AttributeName': 'testKey', 'KeyType': 'HASH'},\n        ],\n        ProvisionedThroughput={\n            'ReadCapacityUnits': 1,\n            'WriteCapacityUnits': 1,\n        },\n    )\n\n\n@pytest.mark.moto\n@pytest.mark.parametrize('signature_version', ['v4'])\n@pytest.mark.asyncio\nasync def test_get_item(dynamodb_client, table_name, dynamodb_put_item):\n    test_value = 'testValue'\n    await dynamodb_put_item(test_value)\n    response = await dynamodb_client.get_item(\n        TableName=table_name, Key={'testKey': {'S': test_value}}\n    )\n    pytest.aio.assert_status_code(response, 200)\n    assert response['Item']['testKey'] == {'S': test_value}\n\n\n@pytest.mark.moto\n@pytest.mark.parametrize('signature_version', ['v4'])\n@pytest.mark.asyncio\nasync def test_create_waiter(dynamodb_client, dynamodb_table_def):\n    table_name = dynamodb_table_def['TableName']\n\n    response = await dynamodb_client.create_table(**dynamodb_table_def)\n\n    pytest.aio.assert_status_code(response, 200)\n\n    waiter = dynamodb_client.get_waiter('table_exists')\n    await waiter.wait(TableName=table_name)\n\n    response = await dynamodb_client.describe_table(TableName=table_name)\n    assert response['Table']['TableStatus'] == 'ACTIVE'\n\n\n@pytest.mark.moto\n@pytest.mark.parametrize('signature_version', ['v4'])\n@pytest.mark.asyncio\nasync def test_batch_write_scan(dynamodb_client, table_name):\n    response = await dynamodb_client.batch_write_item(\n        RequestItems={\n            table_name: [\n                {\n                    'PutRequest': {\n                        'Item': {\n                            'testKey': {'S': 'key1'},\n                            'testKey2': {'S': 'key2'},\n                        }\n                    }\n                },\n                {\n                    'PutRequest': {\n                        'Item': {\n                            'testKey': {'S': 'key3'},\n                            'testKey2': {'S': 'key4'},\n                        }\n                    }\n                },\n            ]\n        }\n    )\n    pytest.aio.assert_status_code(response, 200)\n\n    response = await dynamodb_client.scan(TableName=table_name)\n    test_keys = sorted(item['testKey']['S'] for item in response['Items'])\n\n    assert response['Count'] == 2\n    assert test_keys == ['key1', 'key3']\n\n\n@pytest.mark.moto\n@pytest.mark.parametrize('signature_version', ['v4'])\n@pytest.mark.asyncio\nasync def test_delete_table(dynamodb_client, dynamodb_table_def):\n    table_name = dynamodb_table_def['TableName']\n\n    await dynamodb_client.create_table(**dynamodb_table_def)\n\n    response = await dynamodb_client.describe_table(TableName=table_name)\n    assert response['Table']['TableStatus'] == 'ACTIVE'\n\n    response = await dynamodb_client.delete_table(TableName=table_name)\n    pytest.aio.assert_status_code(response, 200)\n\n    response = await dynamodb_client.list_tables()\n    assert table_name not in response['TableNames']\n\n\n@pytest.mark.moto\n@pytest.mark.parametrize('signature_version', ['v4'])\n@pytest.mark.asyncio\nasync def test_waiter_table_exists_failure(dynamodb_client):\n    waiter = dynamodb_client.get_waiter('table_exists')\n    with pytest.raises(\n        WaiterError, match='Waiter TableExists failed: Max attempts exceeded'\n    ):\n        await waiter.wait(\n            TableName='unknown', WaiterConfig=dict(Delay=1, MaxAttempts=1)\n        )\n\n\n@pytest.mark.moto\n@pytest.mark.parametrize('signature_version', ['v4'])\n@pytest.mark.asyncio\nasync def test_waiter_table_exists(\n    event_loop, dynamodb_client, dynamodb_table_def\n):\n    table_name = dynamodb_table_def['TableName']\n\n    async def _create_table():\n        await asyncio.sleep(2)\n        await dynamodb_client.create_table(**dynamodb_table_def)\n\n    task = event_loop.create_task(_create_table())\n    assert not task.done()\n\n    waiter = dynamodb_client.get_waiter('table_exists')\n    await waiter.wait(\n        TableName=table_name, WaiterConfig=dict(Delay=1, MaxAttempts=5)\n    )\n\n    assert task.done()\n", "tests/test_monitor.py": "import pytest\n\nfrom aiobotocore.session import AioSession\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_monitor_response_received(session: AioSession, s3_client):\n    # Basic smoke test to ensure we can talk to s3.\n    handler_kwargs = {}\n\n    def handler(**kwargs):\n        nonlocal handler_kwargs\n        handler_kwargs = kwargs\n\n    s3_client.meta.events.register('response-received.s3.ListBuckets', handler)\n    result = await s3_client.list_buckets()\n    # Can't really assume anything about whether or not they have buckets,\n    # but we can assume something about the structure of the response.\n    actual_keys = sorted(list(result.keys()))\n    assert actual_keys == ['Buckets', 'Owner', 'ResponseMetadata']\n\n    assert handler_kwargs['response_dict']['status_code'] == 200\n", "tests/test_adaptive.py": "# The following tests are the adaptation of the unit tests for the original (sync)\n# ClientRateLimiter and TokenBucket in botocore.\n# see: https://github.com/boto/botocore:\n# `/tests/unit/retries/test_bucket.py` and `/tests/unit/retries/test_adaptive.py`.\n\nfrom unittest import mock\n\nimport pytest\nfrom botocore.exceptions import CapacityNotAvailableError\nfrom botocore.retries import standard, throttling\n\nfrom aiobotocore.retries import adaptive, bucket\n\n\nclass _SleepMethodCalled(Exception):\n    \"\"\"Raised to explicitly fail a test for calling the blocking `sleep` method.\"\"\"\n\n    pass\n\n\nclass FakeClock(bucket.Clock):\n    def __init__(self, timestamp_sequences):\n        self.timestamp_sequences = timestamp_sequences\n        self.sleep_call_amounts = []\n\n    def sleep(self, amount):\n        raise _SleepMethodCalled(\n            \"sleep method should never be called, non-blocking behavior expected\"\n        )\n\n    def current_time(self):\n        return self.timestamp_sequences.pop(0)\n\n\nclass TestAsyncClientRateLimiter:\n    @pytest.fixture(autouse=True)\n    def _setup(self):\n        self.timestamp_sequences = [0]\n        self.clock = FakeClock(self.timestamp_sequences)\n        self.token_bucket = mock.Mock(spec=bucket.AsyncTokenBucket)\n        self.rate_adjustor = mock.Mock(spec=throttling.CubicCalculator)\n        self.rate_clocker = mock.Mock(spec=adaptive.RateClocker)\n        self.throttling_detector = mock.Mock(\n            spec=standard.ThrottlingErrorDetector\n        )\n\n    def create_client_limiter(self):\n        rate_limiter = adaptive.AsyncClientRateLimiter(\n            rate_adjustor=self.rate_adjustor,\n            rate_clocker=self.rate_clocker,\n            token_bucket=self.token_bucket,\n            throttling_detector=self.throttling_detector,\n            clock=self.clock,\n        )\n        return rate_limiter\n\n    @pytest.mark.asyncio\n    async def test_bucket_bucket_acquisition_only_if_enabled(self):\n        rate_limiter = self.create_client_limiter()\n        await rate_limiter.on_sending_request(request=mock.sentinel.request)\n        assert not self.token_bucket.acquire.called\n\n    @pytest.mark.asyncio\n    async def test_token_bucket_enabled_on_throttling_error(self):\n        rate_limiter = self.create_client_limiter()\n        self.throttling_detector.is_throttling_error.return_value = True\n        self.rate_clocker.record.return_value = 21\n        self.rate_adjustor.error_received.return_value = 17\n        await rate_limiter.on_receiving_response()\n        # Now if we call on_receiving_response we should try to acquire\n        # token.\n        self.timestamp_sequences.append(1)\n        await rate_limiter.on_sending_request(request=mock.sentinel.request)\n        assert self.token_bucket.acquire.called\n\n    @pytest.mark.asyncio\n    async def test_max_rate_updated_on_success_response(self):\n        rate_limiter = self.create_client_limiter()\n        self.throttling_detector.is_throttling_error.return_value = False\n        self.rate_adjustor.success_received.return_value = 20\n        self.rate_clocker.record.return_value = 21\n        await rate_limiter.on_receiving_response()\n        assert await self.token_bucket.set_max_rate.called_with(20)\n\n    @pytest.mark.asyncio\n    async def test_max_rate_cant_exceed_20_percent_max(self):\n        rate_limiter = self.create_client_limiter()\n        self.throttling_detector.is_throttling_error.return_value = False\n        # So if our actual measured sending rate is 20 TPS\n        self.rate_clocker.record.return_value = 20\n        # But the rate adjustor is telling us to go up to 100 TPS\n        self.rate_adjustor.success_received.return_value = 100\n\n        # The most we should go up is 2.0 * 20\n        await rate_limiter.on_receiving_response()\n        assert await self.token_bucket.set_max_rate.called_with(2.0 * 20)\n\n\nclass TestAsyncTokenBucket:\n    @pytest.fixture(autouse=True)\n    def _setup(self):\n        self.timestamp_sequences = [0]\n        self.clock = FakeClock(self.timestamp_sequences)\n\n    def create_token_bucket(self, max_rate=10, min_rate=0.1):\n        return bucket.AsyncTokenBucket(\n            max_rate=max_rate, clock=self.clock, min_rate=min_rate\n        )\n\n    @pytest.mark.asyncio\n    async def test_can_acquire_amount(self):\n        self.timestamp_sequences.extend(\n            [\n                # Requests tokens every second, which is well below our\n                # 10 TPS fill rate.\n                1,\n                2,\n                3,\n                4,\n                5,\n            ]\n        )\n        token_bucket = self.create_token_bucket(max_rate=10)\n        for _ in range(5):\n            assert await token_bucket.acquire(1, block=False)\n\n    @pytest.mark.asyncio\n    async def test_can_change_max_capacity_lower(self):\n        # Requests at 1 TPS.\n        self.timestamp_sequences.extend([1, 2, 3, 4, 5])\n        token_bucket = self.create_token_bucket(max_rate=10)\n        # Request the first 5 tokens with max_rate=10\n        for _ in range(5):\n            assert await token_bucket.acquire(1, block=False)\n        # Now scale the max_rate down to 1 on the 5th second.\n        self.timestamp_sequences.append(5)\n        await token_bucket.set_max_rate(1)\n        # And then from seconds 6-10 we request at one per second.\n        self.timestamp_sequences.extend([6, 7, 8, 9, 10])\n        for _ in range(5):\n            assert await token_bucket.acquire(1, block=False)\n\n    @pytest.mark.asyncio\n    async def test_max_capacity_is_at_least_one(self):\n        token_bucket = self.create_token_bucket()\n        self.timestamp_sequences.append(1)\n        await token_bucket.set_max_rate(0.5)\n        assert token_bucket._fill_rate == 0.5\n        assert token_bucket._max_capacity == 1\n\n    @pytest.mark.asyncio\n    async def test_acquire_fails_on_non_block_mode_returns_false(self):\n        self.timestamp_sequences.extend(\n            [\n                # Initial creation time.\n                0,\n                # Requests a token 1 second later.\n                1,\n            ]\n        )\n        token_bucket = self.create_token_bucket(max_rate=10)\n        with pytest.raises(CapacityNotAvailableError):\n            await token_bucket.acquire(100, block=False)\n\n    @pytest.mark.asyncio\n    async def test_can_retrieve_at_max_send_rate(self):\n        self.timestamp_sequences.extend(\n            [\n                # Request a new token every 100ms (10 TPS) for 2 seconds.\n                1 + 0.1 * i\n                for i in range(20)\n            ]\n        )\n        token_bucket = self.create_token_bucket(max_rate=10)\n        for _ in range(20):\n            assert await token_bucket.acquire(1, block=False)\n\n    @pytest.mark.asyncio\n    async def test_acquiring_blocks_when_capacity_reached(self):\n        # This is 1 token every 0.1 seconds.\n        token_bucket = self.create_token_bucket(max_rate=10)\n        self.timestamp_sequences.extend(\n            [\n                # The first acquire() happens after .1 seconds.\n                0.1,\n                # The second acquire() will fail because we get tokens at\n                # 1 per 0.1 seconds.  We will then sleep for 0.05 seconds until we\n                # get a new token.\n                0.15,\n                # And at 0.2 seconds we get our token.\n                0.2,\n                # And at 0.3 seconds we have no issues getting a token.\n                # Because we're using such small units (to avoid bloating the\n                # test run time), we have to go slightly over 0.3 seconds here.\n                0.300001,\n            ]\n        )\n        assert await token_bucket.acquire(1, block=False)\n        assert token_bucket._current_capacity == 0\n        assert await token_bucket.acquire(1, block=True)\n        assert token_bucket._current_capacity == 0\n        assert await token_bucket.acquire(1, block=False)\n\n    @pytest.mark.asyncio\n    async def test_rate_cant_go_below_min(self):\n        token_bucket = self.create_token_bucket(max_rate=1, min_rate=0.2)\n        self.timestamp_sequences.append(1)\n        await token_bucket.set_max_rate(0.1)\n        assert token_bucket._fill_rate == 0.2\n        assert token_bucket._current_capacity == 1\n", "tests/moto_server.py": "import asyncio\nimport functools\nimport logging\nimport os\nimport socket\nimport threading\nimport time\n\n# Third Party\nimport aiohttp\nimport moto.server\nimport werkzeug.serving\n\nhost = '127.0.0.1'\n\n_PYCHARM_HOSTED = os.environ.get('PYCHARM_HOSTED') == '1'\n_CONNECT_TIMEOUT = 90 if _PYCHARM_HOSTED else 10\n\n\ndef get_free_tcp_port(release_socket: bool = False):\n    sckt = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    sckt.bind((host, 0))\n    addr, port = sckt.getsockname()\n    if release_socket:\n        sckt.close()\n        return port\n\n    return sckt, port\n\n\nclass MotoService:\n    \"\"\"Will Create MotoService.\n    Service is ref-counted so there will only be one per process. Real Service will\n    be returned by `__aenter__`.\"\"\"\n\n    _services = dict()  # {name: instance}\n\n    def __init__(self, service_name: str, port: int = None, ssl: bool = False):\n        self._service_name = service_name\n\n        if port:\n            self._socket = None\n            self._port = port\n        else:\n            self._socket, self._port = get_free_tcp_port()\n\n        self._thread = None\n        self._logger = logging.getLogger('MotoService')\n        self._refcount = None\n        self._ip_address = host\n        self._server = None\n        self._ssl_ctx = (\n            werkzeug.serving.generate_adhoc_ssl_context() if ssl else None\n        )\n        self._schema = 'http' if not self._ssl_ctx else 'https'\n\n    @property\n    def endpoint_url(self):\n        return f'{self._schema}://{self._ip_address}:{self._port}'\n\n    def __call__(self, func):\n        async def wrapper(*args, **kwargs):\n            await self._start()\n            try:\n                result = await func(*args, **kwargs)\n            finally:\n                await self._stop()\n            return result\n\n        functools.update_wrapper(wrapper, func)\n        wrapper.__wrapped__ = func\n        return wrapper\n\n    async def __aenter__(self):\n        svc = self._services.get(self._service_name)\n        if svc is None:\n            self._services[self._service_name] = self\n            self._refcount = 1\n            await self._start()\n            return self\n        else:\n            svc._refcount += 1\n            return svc\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        self._refcount -= 1\n\n        if self._socket:\n            self._socket.close()\n            self._socket = None\n\n        if self._refcount == 0:\n            del self._services[self._service_name]\n            await self._stop()\n\n    def _server_entry(self):\n        self._main_app = moto.server.DomainDispatcherApplication(\n            moto.server.create_backend_app, service=self._service_name\n        )\n        self._main_app.debug = True\n\n        if self._socket:\n            self._socket.close()  # release right before we use it\n            self._socket = None\n\n        self._server = werkzeug.serving.make_server(\n            self._ip_address,\n            self._port,\n            self._main_app,\n            True,\n            ssl_context=self._ssl_ctx,\n        )\n        self._server.serve_forever()\n\n    async def _start(self):\n        self._thread = threading.Thread(target=self._server_entry, daemon=True)\n        self._thread.start()\n\n        async with aiohttp.ClientSession() as session:\n            start = time.time()\n\n            while time.time() - start < 20:\n                if not self._thread.is_alive():\n                    break\n\n                try:\n                    # we need to bypass the proxies due to monkeypatches\n                    async with session.get(\n                        self.endpoint_url + '/static',\n                        timeout=_CONNECT_TIMEOUT,\n                        verify_ssl=False,\n                    ):\n                        pass\n                    break\n                except (asyncio.TimeoutError, aiohttp.ClientConnectionError):\n                    await asyncio.sleep(0.5)\n            else:\n                await self._stop()  # pytest.fail doesn't call stop_process\n                raise Exception(f\"Can not start service: {self._service_name}\")\n\n    async def _stop(self):\n        if self._server:\n            self._server.shutdown()\n\n        self._thread.join()\n", "tests/test_ec2.py": "import pytest\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_ec2_snapshot(ec2_client):\n    # TODO: this needs to somehow validate the presigned url sent because moto is not\n    volume_response = await ec2_client.create_volume(\n        AvailabilityZone=\"us-east-1\", Size=10\n    )\n    tag_spec = [\n        {\n            \"ResourceType\": \"snapshot\",\n            \"Tags\": [{\"Key\": \"key\", \"Value\": \"value\"}],\n        }\n    ]\n\n    create_snapshot_response = await ec2_client.create_snapshot(\n        VolumeId=volume_response[\"VolumeId\"], TagSpecifications=tag_spec\n    )\n\n    copy_snapshot_response = await ec2_client.copy_snapshot(\n        SourceSnapshotId=create_snapshot_response[\"SnapshotId\"],\n        SourceRegion=\"us-east-1\",\n        DestinationRegion=\"us-east-1\",\n        Encrypted=True,\n        TagSpecifications=tag_spec,\n        KmsKeyId=\"key-1234\",\n    )\n\n    assert copy_snapshot_response['SnapshotId']\n", "tests/test_batch.py": "import pytest\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_batch(batch_client):\n    job_queues = await batch_client.describe_job_queues()\n    assert job_queues['jobQueues'] == []\n", "tests/conftest.py": "import asyncio\nimport os\nimport random\nimport string\nimport tempfile\nfrom contextlib import ExitStack\nfrom itertools import chain\nfrom unittest.mock import patch\n\nimport aiohttp\n\n# Third Party\nimport pytest\n\nimport aiobotocore.session\nfrom aiobotocore.config import AioConfig\nfrom tests._helpers import AsyncExitStack\n\nhost = '127.0.0.1'\n\n_PYCHARM_HOSTED = os.environ.get('PYCHARM_HOSTED') == '1'\n\n\n@pytest.fixture(\n    scope=\"session\", params=[True, False], ids=['debug[true]', 'debug[false]']\n)\ndef debug(request):\n    return request.param\n\n\ndef random_bucketname():\n    # 63 is the max bucket length.\n    return random_name()\n\n\ndef random_tablename():\n    return random_name()\n\n\ndef random_name():\n    \"\"\"Return a string with presumably unique contents\n\n    The string contains only symbols allowed for s3 buckets\n    (alphanumeric, dot and hyphen).\n    \"\"\"\n    return ''.join(random.sample(string.ascii_lowercase, k=26))\n\n\ndef assert_status_code(response, status_code):\n    assert response['ResponseMetadata']['HTTPStatusCode'] == status_code\n\n\nasync def assert_num_uploads_found(\n    s3_client,\n    bucket_name,\n    operation,\n    num_uploads,\n    *,\n    max_items=None,\n    num_attempts=5,\n):\n    paginator = s3_client.get_paginator(operation)\n    for _ in range(num_attempts):\n        pages = paginator.paginate(\n            Bucket=bucket_name, PaginationConfig={'MaxItems': max_items}\n        )\n        responses = []\n        async for page in pages:\n            responses.append(page)\n\n        # It sometimes takes a while for all the uploads to show up,\n        # especially if the upload was just created.  If we don't\n        # see the expected amount, we retry up to num_attempts time\n        # before failing.\n        amount_seen = len(responses[0]['Uploads'])\n        if amount_seen == num_uploads:\n            # Test passed.\n            return\n        else:\n            # Sleep and try again.\n            await asyncio.sleep(2)\n\n        pytest.fail(\n            \"Expected to see {} uploads, instead saw: {}\".format(\n                num_uploads, amount_seen\n            )\n        )\n\n\n@pytest.fixture\ndef aa_fail_proxy_config(monkeypatch):\n    # NOTE: name of this fixture must be alphabetically first to run first\n    monkeypatch.setenv('HTTP_PROXY', f'http://{host}:54321')\n    monkeypatch.setenv('HTTPS_PROXY', f'http://{host}:54321')\n\n\n@pytest.fixture\ndef aa_succeed_proxy_config(monkeypatch):\n    # NOTE: name of this fixture must be alphabetically first to run first\n    monkeypatch.setenv('HTTP_PROXY', f'http://{host}:54321')\n    monkeypatch.setenv('HTTPS_PROXY', f'http://{host}:54321')\n\n    # this will cause us to skip proxying\n    monkeypatch.setenv('NO_PROXY', 'amazonaws.com')\n\n\n@pytest.fixture\ndef session():\n    session = aiobotocore.session.AioSession()\n    return session\n\n\n@pytest.fixture\ndef region():\n    return 'us-east-1'\n\n\n@pytest.fixture\ndef alternative_region():\n    return 'us-west-2'\n\n\n@pytest.fixture\ndef signature_version():\n    return 's3'\n\n\n@pytest.fixture\ndef server_scheme():\n    return 'http'\n\n\n@pytest.fixture\ndef s3_verify():\n    return None\n\n\n@pytest.fixture\ndef config(request, region, signature_version):\n    config_kwargs = request.node.get_closest_marker(\"config_kwargs\") or {}\n    if config_kwargs:\n        assert not config_kwargs.kwargs, config_kwargs\n        assert len(config_kwargs.args) == 1\n        config_kwargs = config_kwargs.args[0]\n\n    connect_timeout = read_timout = 5\n    if _PYCHARM_HOSTED:\n        connect_timeout = read_timout = 180\n\n    return AioConfig(\n        region_name=region,\n        signature_version=signature_version,\n        read_timeout=read_timout,\n        connect_timeout=connect_timeout,\n        **config_kwargs,\n    )\n\n\n@pytest.fixture\ndef mocking_test():\n    # change this flag for test with real aws\n    # TODO: this should be merged with pytest.mark.moto\n    return True\n\n\ndef moto_config(endpoint_url):\n    kw = dict(\n        endpoint_url=endpoint_url,\n        aws_secret_access_key=\"xxx\",\n        aws_access_key_id=\"xxx\",\n    )\n\n    return kw\n\n\n@pytest.fixture\ndef patch_attributes(request):\n    \"\"\"Call unittest.mock.patch on arguments passed through a pytest mark.\n\n    This fixture looks at the @pytest.mark.patch_attributes mark. This mark is a list\n    of arguments to be passed to unittest.mock.patch (see example below). This fixture\n    returns the list of mock objects, one per element in the input list.\n\n    Why do we need this? In some cases, we want to perform the patching before other\n    fixtures are run. For instance, the `s3_client` fixture creates an aiobotocore\n    client. During the client creation process, some event listeners are registered.\n    When we want to patch the target of these event listeners, we must do so before\n    the `s3_client` fixture is executed.  Otherwise, the aiobotocore client will store\n    references to the unpatched targets.\n\n    In such situations, make sure that subsequent fixtures explicitly depends on\n    `patch_attribute` to enforce the ordering between fixtures.\n\n    Example:\n\n    @pytest.mark.patch_attributes([\n        dict(\n            target=\"aiobotocore.retries.adaptive.AsyncClientRateLimiter.on_sending_request\",\n            side_effect=aiobotocore.retries.adaptive.AsyncClientRateLimiter.on_sending_request,\n            autospec=True\n        )\n    ])\n    async def test_client_rate_limiter_called(s3_client, patch_attributes):\n        await s3_client.get_object(Bucket=\"bucket\", Key=\"key\")\n        # Just for illustration (this test doesn't pass).\n        # mock_attributes is a list of 1 element, since we passed a list of 1 element\n        # to the patch_attributes marker.\n        mock_attributes[0].assert_called_once()\n    \"\"\"\n    marker = request.node.get_closest_marker(\"patch_attributes\")\n    if marker is None:\n        yield\n    else:\n        with ExitStack() as stack:\n            yield [\n                stack.enter_context(patch(**kwargs))\n                for kwargs in marker.args[0]\n            ]\n\n\n@pytest.fixture\nasync def s3_client(\n    session,\n    region,\n    config,\n    s3_server,\n    mocking_test,\n    s3_verify,\n    patch_attributes,\n):\n    # This depends on mock_attributes because we may want to test event listeners.\n    # See the documentation of `mock_attributes` for details.\n    kw = moto_config(s3_server) if mocking_test else {}\n\n    async with session.create_client(\n        's3', region_name=region, config=config, verify=s3_verify, **kw\n    ) as client:\n        yield client\n\n\n@pytest.fixture\nasync def alternative_s3_client(\n    session, alternative_region, signature_version, s3_server, mocking_test\n):\n    kw = moto_config(s3_server) if mocking_test else {}\n\n    config = AioConfig(\n        region_name=alternative_region,\n        signature_version=signature_version,\n        read_timeout=5,\n        connect_timeout=5,\n    )\n\n    async with session.create_client(\n        's3', region_name=alternative_region, config=config, **kw\n    ) as client:\n        yield client\n\n\n@pytest.fixture\nasync def dynamodb_client(\n    session, region, config, dynamodb2_server, mocking_test\n):\n    kw = moto_config(dynamodb2_server) if mocking_test else {}\n    async with session.create_client(\n        'dynamodb', region_name=region, config=config, **kw\n    ) as client:\n        yield client\n\n\n@pytest.fixture\nasync def cloudformation_client(\n    session, region, config, cloudformation_server, mocking_test\n):\n    kw = moto_config(cloudformation_server) if mocking_test else {}\n    async with session.create_client(\n        'cloudformation', region_name=region, config=config, **kw\n    ) as client:\n        yield client\n\n\n@pytest.fixture\nasync def sns_client(session, region, config, sns_server, mocking_test):\n    kw = moto_config(sns_server) if mocking_test else {}\n    async with session.create_client(\n        'sns', region_name=region, config=config, **kw\n    ) as client:\n        yield client\n\n\n@pytest.fixture\nasync def sqs_client(session, region, config, sqs_server, mocking_test):\n    kw = moto_config(sqs_server) if mocking_test else {}\n    async with session.create_client(\n        'sqs', region_name=region, config=config, **kw\n    ) as client:\n        yield client\n\n\n@pytest.fixture\nasync def batch_client(session, region, config, batch_server, mocking_test):\n    kw = moto_config(batch_server) if mocking_test else {}\n    async with session.create_client(\n        'batch', region_name=region, config=config, **kw\n    ) as client:\n        yield client\n\n\n@pytest.fixture\nasync def lambda_client(session, region, config, lambda_server, mocking_test):\n    kw = moto_config(lambda_server) if mocking_test else {}\n    async with session.create_client(\n        'lambda', region_name=region, config=config, **kw\n    ) as client:\n        yield client\n\n\n@pytest.fixture\nasync def iam_client(session, region, config, iam_server, mocking_test):\n    kw = moto_config(iam_server) if mocking_test else {}\n    async with session.create_client(\n        'iam', region_name=region, config=config, **kw\n    ) as client:\n        yield client\n\n\n@pytest.fixture\nasync def rds_client(session, region, config, rds_server, mocking_test):\n    kw = moto_config(rds_server) if mocking_test else {}\n    async with session.create_client(\n        'rds', region_name=region, config=config, **kw\n    ) as client:\n        yield client\n\n\n@pytest.fixture\nasync def ec2_client(session, region, config, ec2_server, mocking_test):\n    kw = moto_config(ec2_server) if mocking_test else {}\n    async with session.create_client(\n        'ec2', region_name=region, config=config, **kw\n    ) as client:\n        yield client\n\n\n@pytest.fixture\nasync def kinesis_client(\n    session, region, config, kinesis_server, mocking_test\n):\n    kw = moto_config(kinesis_server) if mocking_test else {}\n    async with session.create_client(\n        'kinesis', region_name=region, config=config, **kw\n    ) as client:\n        yield client\n\n\nasync def recursive_delete(s3_client, bucket_name):\n    # Recursively deletes a bucket and all of its contents.\n    paginator = s3_client.get_paginator('list_object_versions')\n    async for n in paginator.paginate(Bucket=bucket_name, Prefix=''):\n        for obj in chain(\n            n.get('Versions', []),\n            n.get('DeleteMarkers', []),\n            n.get('Contents', []),\n            n.get('CommonPrefixes', []),\n        ):\n            kwargs = dict(Bucket=bucket_name, Key=obj['Key'])\n            if 'VersionId' in obj:\n                kwargs['VersionId'] = obj['VersionId']\n            resp = await s3_client.delete_object(**kwargs)\n            assert_status_code(resp, 204)\n\n    resp = await s3_client.delete_bucket(Bucket=bucket_name)\n    assert_status_code(resp, 204)\n\n\n@pytest.fixture\nasync def bucket_name(region, create_bucket):\n    name = await create_bucket(region)\n    yield name\n\n\n@pytest.fixture\nasync def table_name(create_table):\n    name = await create_table()\n    yield name\n\n\n@pytest.fixture\nasync def create_bucket(s3_client):\n    _bucket_name = None\n\n    async def _f(region_name, bucket_name=None):\n        nonlocal _bucket_name\n        if bucket_name is None:\n            bucket_name = random_bucketname()\n        _bucket_name = bucket_name\n        bucket_kwargs = {'Bucket': bucket_name}\n        if region_name != 'us-east-1':\n            bucket_kwargs['CreateBucketConfiguration'] = {\n                'LocationConstraint': region_name,\n            }\n        response = await s3_client.create_bucket(**bucket_kwargs)\n        assert_status_code(response, 200)\n        await s3_client.put_bucket_versioning(\n            Bucket=bucket_name, VersioningConfiguration={'Status': 'Enabled'}\n        )\n        return bucket_name\n\n    try:\n        yield _f\n    finally:\n        await recursive_delete(s3_client, _bucket_name)\n\n\n@pytest.fixture\nasync def create_table(dynamodb_client):\n    _table_name = None\n\n    async def _is_table_ready(table_name):\n        response = await dynamodb_client.describe_table(TableName=table_name)\n        return response['Table']['TableStatus'] == 'ACTIVE'\n\n    async def _f(table_name=None):\n        nonlocal _table_name\n        if table_name is None:\n            table_name = random_tablename()\n        _table_name = table_name\n        table_kwargs = {\n            'TableName': table_name,\n            'AttributeDefinitions': [\n                {'AttributeName': 'testKey', 'AttributeType': 'S'},\n            ],\n            'KeySchema': [\n                {'AttributeName': 'testKey', 'KeyType': 'HASH'},\n            ],\n            'ProvisionedThroughput': {\n                'ReadCapacityUnits': 1,\n                'WriteCapacityUnits': 1,\n            },\n        }\n\n        response = await dynamodb_client.create_table(**table_kwargs)\n        while not (await _is_table_ready(table_name)):\n            pass\n\n        assert_status_code(response, 200)\n        return table_name\n\n    try:\n        yield _f\n    finally:\n        await delete_table(dynamodb_client, _table_name)\n\n\nasync def delete_table(dynamodb_client, table_name):\n    response = await dynamodb_client.delete_table(TableName=table_name)\n    assert_status_code(response, 200)\n\n\n@pytest.fixture\ndef tempdir():\n    with tempfile.TemporaryDirectory() as td:\n        yield td\n\n\n@pytest.fixture\ndef create_object(s3_client, bucket_name):\n    async def _f(key_name, body='foo'):\n        r = await s3_client.put_object(\n            Bucket=bucket_name, Key=key_name, Body=body\n        )\n        assert_status_code(r, 200)\n        return r\n\n    return _f\n\n\n@pytest.fixture\ndef create_multipart_upload(request, s3_client, bucket_name, event_loop):\n    _key_name = None\n    upload_id = None\n\n    async def _f(key_name):\n        nonlocal _key_name\n        nonlocal upload_id\n        _key_name = key_name\n\n        parsed = await s3_client.create_multipart_upload(\n            Bucket=bucket_name, Key=key_name\n        )\n        upload_id = parsed['UploadId']\n        return upload_id\n\n    def fin():\n        event_loop.run_until_complete(\n            s3_client.abort_multipart_upload(\n                UploadId=upload_id, Bucket=bucket_name, Key=_key_name\n            )\n        )\n\n    request.addfinalizer(fin)\n    return _f\n\n\n@pytest.fixture\nasync def aio_session():\n    async with aiohttp.ClientSession() as session:\n        yield session\n\n\ndef pytest_configure():\n    class AIOUtils:\n        def __init__(self):\n            self.assert_status_code = assert_status_code\n            self.assert_num_uploads_found = assert_num_uploads_found\n\n    pytest.aio = AIOUtils()\n\n\n@pytest.fixture\ndef dynamodb_put_item(dynamodb_client, table_name):\n    async def _f(key_string_value):\n        response = await dynamodb_client.put_item(\n            TableName=table_name,\n            Item={'testKey': {'S': key_string_value}},\n        )\n        assert_status_code(response, 200)\n\n    return _f\n\n\n@pytest.fixture\ndef topic_arn(region, create_topic, sns_client, event_loop):\n    arn = event_loop.run_until_complete(create_topic())\n    return arn\n\n\nasync def delete_topic(sns_client, topic_arn):\n    response = await sns_client.delete_topic(TopicArn=topic_arn)\n    assert_status_code(response, 200)\n\n\n@pytest.fixture\ndef create_topic(request, sns_client, event_loop):\n    _topic_arn = None\n\n    async def _f():\n        nonlocal _topic_arn\n        response = await sns_client.create_topic(Name=random_name())\n        _topic_arn = response['TopicArn']\n        assert_status_code(response, 200)\n        return _topic_arn\n\n    def fin():\n        event_loop.run_until_complete(delete_topic(sns_client, _topic_arn))\n\n    request.addfinalizer(fin)\n    return _f\n\n\n@pytest.fixture\nasync def sqs_queue_url(sqs_client):\n    response = await sqs_client.create_queue(QueueName=random_name())\n    queue_url = response['QueueUrl']\n    assert_status_code(response, 200)\n\n    try:\n        yield queue_url\n    finally:\n        response = await sqs_client.delete_queue(QueueUrl=queue_url)\n        assert_status_code(response, 200)\n\n\n@pytest.fixture\nasync def exit_stack():\n    async with AsyncExitStack() as es:\n        yield es\n\n\npytest_plugins = ['tests.mock_server']\n", "tests/test_basic_s3.py": "import asyncio\nimport base64\nimport hashlib\nfrom collections import defaultdict\n\nimport aioitertools\nimport botocore.retries.adaptive\nimport pytest\n\nimport aiobotocore.retries.adaptive\nfrom aiobotocore import httpsession\n\n\nasync def fetch_all(pages):\n    responses = []\n    async for n in pages:\n        responses.append(n)\n    return responses\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_can_make_request(s3_client):\n    # Basic smoke test to ensure we can talk to s3.\n    result = await s3_client.list_buckets()\n    # Can't really assume anything about whether or not they have buckets,\n    # but we can assume something about the structure of the response.\n    actual_keys = sorted(list(result.keys()))\n    assert actual_keys == ['Buckets', 'Owner', 'ResponseMetadata']\n\n\n@pytest.mark.moto\n@pytest.mark.parametrize('s3_verify', [False])\n@pytest.mark.asyncio\nasync def test_can_make_request_no_verify(s3_client):\n    # Basic smoke test to ensure we can talk to s3.\n    result = await s3_client.list_buckets()\n    # Can't really assume anything about whether or not they have buckets,\n    # but we can assume something about the structure of the response.\n    actual_keys = sorted(list(result.keys()))\n    assert actual_keys == ['Buckets', 'Owner', 'ResponseMetadata']\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_fail_proxy_request(\n    aa_fail_proxy_config, s3_client, monkeypatch\n):\n    # based on test_can_make_request\n    with pytest.raises(httpsession.ProxyConnectionError):\n        await s3_client.list_buckets()\n\n\n@pytest.mark.asyncio\n@pytest.mark.parametrize('mocking_test', [False])\nasync def test_succeed_proxy_request(aa_succeed_proxy_config, s3_client):\n    result = await s3_client.list_buckets()\n    actual_keys = sorted(list(result.keys()))\n    assert actual_keys == ['Buckets', 'Owner', 'ResponseMetadata']\n\n\n@pytest.mark.asyncio\n@pytest.mark.moto\nasync def test_can_get_bucket_location(s3_client, bucket_name):\n    result = await s3_client.get_bucket_location(Bucket=bucket_name)\n    assert 'LocationConstraint' in result\n    # For buckets in us-east-1 (US Classic Region) this will be None\n    # TODO fix this\n    assert result['LocationConstraint'] in [None, 'us-west-2', 'us-east-1']\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_can_delete_urlencoded_object(\n    s3_client, bucket_name, create_object\n):\n    key_name = 'a+b/foo'\n    await create_object(key_name=key_name)\n    resp = await s3_client.list_objects(Bucket=bucket_name)\n    bucket_contents = resp['Contents']\n    assert len(bucket_contents) == 1\n    assert bucket_contents[-1]['Key'] == 'a+b/foo'\n\n    # TODO: unfortunately this is broken now: https://github.com/spulec/moto/issues/5030\n    # resp = await s3_client.list_objects(Bucket=bucket_name, Prefix='a+b')\n    # subdir_contents = resp['Contents']\n    # assert len(subdir_contents) == 1\n    # assert subdir_contents[0]['Key'] == 'a+b/foo'\n\n    response = await s3_client.delete_object(Bucket=bucket_name, Key=key_name)\n    pytest.aio.assert_status_code(response, 204)\n\n\n@pytest.mark.asyncio\n@pytest.mark.moto\nasync def test_can_paginate(s3_client, bucket_name, create_object):\n    for i in range(5):\n        key_name = 'key%s' % i\n        await create_object(key_name)\n\n    paginator = s3_client.get_paginator('list_objects')\n    pages = paginator.paginate(MaxKeys=1, Bucket=bucket_name)\n    responses = await fetch_all(pages)\n\n    assert len(responses) == 5, responses\n    key_names = [el['Contents'][0]['Key'] for el in responses]\n    assert key_names == ['key0', 'key1', 'key2', 'key3', 'key4']\n\n\n@pytest.mark.asyncio\n@pytest.mark.moto\nasync def test_can_paginate_with_page_size(\n    s3_client, bucket_name, create_object\n):\n    for i in range(5):\n        key_name = 'key%s' % i\n        await create_object(key_name)\n\n    paginator = s3_client.get_paginator('list_objects')\n    pages = paginator.paginate(\n        PaginationConfig={'PageSize': 1}, Bucket=bucket_name\n    )\n\n    responses = await fetch_all(pages)\n    assert len(responses) == 5, responses\n    data = [r for r in responses]\n    key_names = [el['Contents'][0]['Key'] for el in data]\n    assert key_names == ['key0', 'key1', 'key2', 'key3', 'key4']\n\n\n@pytest.mark.asyncio\n@pytest.mark.moto\nasync def test_can_search_paginate(s3_client, bucket_name, create_object):\n    keys = []\n    for i in range(5):\n        key_name = 'key%s' % i\n        keys.append(key_name)\n        await create_object(key_name)\n\n    paginator = s3_client.get_paginator('list_objects')\n    page_iter = paginator.paginate(Bucket=bucket_name)\n    async for key_name in page_iter.search('Contents[*].Key'):\n        assert key_name in keys\n\n\n@pytest.mark.asyncio\n@pytest.mark.moto\nasync def test_can_paginate_iterator(s3_client, bucket_name, create_object):\n    for i in range(5):\n        key_name = 'key%s' % i\n        await create_object(key_name)\n\n    paginator = s3_client.get_paginator('list_objects')\n    responses = []\n    async for page in paginator.paginate(\n        PaginationConfig={'PageSize': 1}, Bucket=bucket_name\n    ):\n        assert not asyncio.iscoroutine(page)\n        responses.append(page)\n    assert len(responses) == 5, responses\n    data = [r for r in responses]\n    key_names = [el['Contents'][0]['Key'] for el in data]\n    assert key_names == ['key0', 'key1', 'key2', 'key3', 'key4']\n\n\n@pytest.mark.asyncio\n@pytest.mark.moto\nasync def test_result_key_iters(s3_client, bucket_name, create_object):\n    for i in range(5):\n        key_name = f'key/{i}/{i}'\n        await create_object(key_name)\n        key_name2 = 'key/%s' % i\n        await create_object(key_name2)\n\n    paginator = s3_client.get_paginator('list_objects')\n    generator = paginator.paginate(\n        MaxKeys=2, Prefix='key/', Delimiter='/', Bucket=bucket_name\n    )\n    iterators = generator.result_key_iters()\n    response = defaultdict(list)\n    key_names = [i.result_key for i in iterators]\n\n    # adapt to aioitertools ideas\n    iterators = [itr.__aiter__() for itr in iterators]\n\n    async for vals in aioitertools.zip_longest(*iterators):\n        pass\n\n        for k, val in zip(key_names, vals):\n            response.setdefault(k.expression, [])\n            response[k.expression].append(val)\n\n    assert 'Contents' in response\n    assert 'CommonPrefixes' in response\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_can_get_and_put_object(s3_client, create_object, bucket_name):\n    await create_object('foobarbaz', body='body contents')\n    resp = await s3_client.get_object(Bucket=bucket_name, Key='foobarbaz')\n    data = await resp['Body'].read()\n    # TODO: think about better api and make behavior like in aiohttp\n    resp['Body'].close()\n    assert data == b'body contents'\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\n@pytest.mark.patch_attributes(\n    [\n        dict(\n            target=\"aiobotocore.retries.adaptive.AsyncClientRateLimiter.on_sending_request\",\n            side_effect=aiobotocore.retries.adaptive.AsyncClientRateLimiter.on_sending_request,\n            autospec=True,\n        ),\n        dict(\n            target=\"aiobotocore.retries.adaptive.AsyncClientRateLimiter.on_receiving_response\",\n            side_effect=aiobotocore.retries.adaptive.AsyncClientRateLimiter.on_receiving_response,\n            autospec=True,\n        ),\n        dict(\n            target=\"botocore.retries.adaptive.ClientRateLimiter.on_sending_request\",\n            side_effect=botocore.retries.adaptive.ClientRateLimiter.on_sending_request,\n            autospec=True,\n        ),\n        dict(\n            target=\"botocore.retries.adaptive.ClientRateLimiter.on_receiving_response\",\n            side_effect=botocore.retries.adaptive.ClientRateLimiter.on_receiving_response,\n            autospec=True,\n        ),\n    ]\n)\n@pytest.mark.config_kwargs(\n    dict(retries={\"max_attempts\": 5, \"mode\": \"adaptive\"})\n)\nasync def test_adaptive_retry(\n    s3_client, config, create_object, bucket_name, patch_attributes\n):\n    await create_object('foobarbaz', body='body contents')\n\n    # Check that our async implementations were correctly called.\n    # We need to patch event listeners before the S3 client is created (see\n    # documentation for `patch_attributes`), but as a result, other calls may be\n    # performed during the setup of other fixtures. Thus, we can't rely on the total\n    # number of calls, we just inspect the last one.\n    assert len(patch_attributes[0].mock_calls) > 0  # on_sending_request\n    _, _, call_args = patch_attributes[0].mock_calls[-1]\n    assert call_args[\"event_name\"] == \"before-send.s3.PutObject\"\n    assert call_args[\"request\"].url.endswith(\"foobarbaz\")\n\n    assert len(patch_attributes[1].mock_calls) > 0  # on_receiving_response\n    _, _, call_args = patch_attributes[1].mock_calls[-1]\n    assert call_args[\"event_name\"] == \"needs-retry.s3.PutObject\"\n\n    # Check that we did not call any blocking method.\n    # Unfortunately can't directly patch threading.Lock.__enter__.\n    patch_attributes[2].assert_not_called()\n    patch_attributes[3].assert_not_called()\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_get_object_stream_wrapper(\n    s3_client, create_object, bucket_name\n):\n    await create_object('foobarbaz', body='body contents')\n    response = await s3_client.get_object(Bucket=bucket_name, Key='foobarbaz')\n    body = response['Body']\n    chunk1 = await body.read(1)\n    chunk2 = await body.read()\n    assert chunk1 == b'b'\n    assert chunk2 == b'ody contents'\n    response['Body'].close()\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_get_object_stream_context(\n    s3_client, create_object, bucket_name\n):\n    await create_object('foobarbaz', body='body contents')\n    response = await s3_client.get_object(Bucket=bucket_name, Key='foobarbaz')\n    async with response['Body'] as stream:\n        await stream.read()\n\n\n@pytest.mark.asyncio\n@pytest.mark.moto\nasync def test_paginate_max_items(\n    s3_client, create_multipart_upload, bucket_name\n):\n    await create_multipart_upload('foo/key1')\n    await create_multipart_upload('foo/key1')\n    await create_multipart_upload('foo/key1')\n    await create_multipart_upload('foo/key2')\n    await create_multipart_upload('foobar/key1')\n    await create_multipart_upload('foobar/key2')\n    await create_multipart_upload('bar/key1')\n    await create_multipart_upload('bar/key2')\n\n    # Verify when we have MaxItems=None, we get back all 8 uploads.\n    await pytest.aio.assert_num_uploads_found(\n        s3_client,\n        bucket_name,\n        'list_multipart_uploads',\n        max_items=None,\n        num_uploads=8,\n    )\n\n    # Verify when we have MaxItems=1, we get back 1 upload.\n    await pytest.aio.assert_num_uploads_found(\n        s3_client,\n        bucket_name,\n        'list_multipart_uploads',\n        max_items=1,\n        num_uploads=1,\n    )\n\n    paginator = s3_client.get_paginator('list_multipart_uploads')\n    # Works similar with build_full_result()\n    pages = paginator.paginate(\n        PaginationConfig={'MaxItems': 1}, Bucket=bucket_name\n    )\n    full_result = await pages.build_full_result()\n    assert len(full_result['Uploads']) == 1\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_paginate_within_page_boundaries(\n    s3_client, create_object, bucket_name\n):\n    await create_object('a')\n    await create_object('b')\n    await create_object('c')\n    await create_object('d')\n    paginator = s3_client.get_paginator('list_objects')\n    # First do it without a max keys so we're operating on a single page of\n    # results.\n    pages = paginator.paginate(\n        PaginationConfig={'MaxItems': 1}, Bucket=bucket_name\n    )\n    first = await pages.build_full_result()\n    t1 = first['NextToken']\n\n    pages = paginator.paginate(\n        PaginationConfig={'MaxItems': 1, 'StartingToken': t1},\n        Bucket=bucket_name,\n    )\n    second = await pages.build_full_result()\n    t2 = second['NextToken']\n\n    pages = paginator.paginate(\n        PaginationConfig={'MaxItems': 1, 'StartingToken': t2},\n        Bucket=bucket_name,\n    )\n    third = await pages.build_full_result()\n    t3 = third['NextToken']\n\n    pages = paginator.paginate(\n        PaginationConfig={'MaxItems': 1, 'StartingToken': t3},\n        Bucket=bucket_name,\n    )\n    fourth = await pages.build_full_result()\n\n    assert first['Contents'][-1]['Key'] == 'a'\n    assert second['Contents'][-1]['Key'] == 'b'\n    assert third['Contents'][-1]['Key'] == 'c'\n    assert fourth['Contents'][-1]['Key'] == 'd'\n\n\n@pytest.mark.asyncio\n@pytest.mark.parametrize('mocking_test', [False])\nasync def test_unicode_key_put_list(s3_client, bucket_name, create_object):\n    # Verify we can upload a key with a unicode char and list it as well.\n    key_name = '\\u2713'\n    await create_object(key_name)\n    parsed = await s3_client.list_objects(Bucket=bucket_name)\n    assert len(parsed['Contents']) == 1\n    assert parsed['Contents'][0]['Key'] == key_name\n    parsed = await s3_client.get_object(Bucket=bucket_name, Key=key_name)\n    data = await parsed['Body'].read()\n    parsed['Body'].close()\n    assert data == b'foo'\n\n\n@pytest.mark.asyncio\n@pytest.mark.parametrize('mocking_test', [False])\nasync def test_unicode_system_character(s3_client, bucket_name, create_object):\n    # Verify we can use a unicode system character which would normally\n    # break the xml parser\n    key_name = 'foo\\x08'\n    await create_object(key_name)\n    parsed = await s3_client.list_objects(Bucket=bucket_name)\n    assert len(parsed['Contents']) == 1\n    assert parsed['Contents'][0]['Key'] == key_name\n\n    parsed = await s3_client.list_objects(\n        Bucket=bucket_name, EncodingType='url'\n    )\n    assert len(parsed['Contents']) == 1\n    assert parsed['Contents'][0]['Key'] == 'foo%08'\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_non_normalized_key_paths(s3_client, bucket_name, create_object):\n    # The create_object method has assertEqual checks for 200 status.\n    await create_object('key./././name')\n    bucket = await s3_client.list_objects(Bucket=bucket_name)\n    bucket_contents = bucket['Contents']\n    assert len(bucket_contents) == 1\n    assert bucket_contents[0]['Key'] == 'key./././name'\n\n\n@pytest.mark.skipif(True, reason='Not supported')\n@pytest.mark.asyncio\nasync def test_reset_stream_on_redirects(region, create_bucket):\n    # Create a bucket in a non classic region.\n    bucket_name = await create_bucket(region)\n    # Then try to put a file like object to this location.\n    assert bucket_name\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_copy_with_quoted_char(s3_client, create_object, bucket_name):\n    key_name = 'a+b/foo'\n    await create_object(key_name=key_name)\n\n    key_name2 = key_name + 'bar'\n    source = f'{bucket_name}/{key_name}'\n    await s3_client.copy_object(\n        Bucket=bucket_name, Key=key_name2, CopySource=source\n    )\n\n    # Now verify we can retrieve the copied object.\n    resp = await s3_client.get_object(Bucket=bucket_name, Key=key_name2)\n    data = await resp['Body'].read()\n    resp['Body'].close()\n    assert data == b'foo'\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_copy_with_query_string(s3_client, create_object, bucket_name):\n    key_name = 'a+b/foo?notVersionid=bar'\n    await create_object(key_name=key_name)\n\n    key_name2 = key_name + 'bar'\n    await s3_client.copy_object(\n        Bucket=bucket_name,\n        Key=key_name2,\n        CopySource=f'{bucket_name}/{key_name}',\n    )\n\n    # Now verify we can retrieve the copied object.\n    resp = await s3_client.get_object(Bucket=bucket_name, Key=key_name2)\n    data = await resp['Body'].read()\n    resp['Body'].close()\n    assert data == b'foo'\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_can_copy_with_dict_form(s3_client, create_object, bucket_name):\n    key_name = 'a+b/foo?versionId=abcd'\n    await create_object(key_name=key_name)\n\n    key_name2 = key_name + 'bar'\n    await s3_client.copy_object(\n        Bucket=bucket_name,\n        Key=key_name2,\n        CopySource={'Bucket': bucket_name, 'Key': key_name},\n    )\n\n    # Now verify we can retrieve the copied object.\n    resp = await s3_client.get_object(Bucket=bucket_name, Key=key_name2)\n    data = await resp['Body'].read()\n    resp['Body'].close()\n    assert data == b'foo'\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_can_copy_with_dict_form_with_version(\n    s3_client, create_object, bucket_name\n):\n    key_name = 'a+b/foo?versionId=abcd'\n    response = await create_object(key_name=key_name)\n    key_name2 = key_name + 'bar'\n    await s3_client.copy_object(\n        Bucket=bucket_name,\n        Key=key_name2,\n        CopySource={\n            'Bucket': bucket_name,\n            'Key': key_name,\n            'VersionId': response[\"VersionId\"],\n        },\n    )\n\n    # Now verify we can retrieve the copied object.\n    resp = await s3_client.get_object(Bucket=bucket_name, Key=key_name2)\n    data = await resp['Body'].read()\n    resp['Body'].close()\n    assert data == b'foo'\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_copy_with_s3_metadata(s3_client, create_object, bucket_name):\n    key_name = 'foo.txt'\n    await create_object(key_name=key_name)\n    copied_key = 'copied.txt'\n    parsed = await s3_client.copy_object(\n        Bucket=bucket_name,\n        Key=copied_key,\n        CopySource=f'{bucket_name}/{key_name}',\n        MetadataDirective='REPLACE',\n        Metadata={\"mykey\": \"myvalue\", \"mykey2\": \"myvalue2\"},\n    )\n    pytest.aio.assert_status_code(parsed, 200)\n\n\n@pytest.mark.parametrize('region', ['us-east-1'])\n@pytest.mark.parametrize('signature_version', ['s3'])\n# 'Content-Disposition' not supported by moto yet\n@pytest.mark.parametrize('mocking_test', [False])\n@pytest.mark.asyncio\nasync def test_presign_with_existing_query_string_values(\n    s3_client, bucket_name, aio_session, create_object\n):\n    key_name = 'foo.txt'\n    await create_object(key_name=key_name)\n    content_disposition = 'attachment; filename=foo.txt;'\n    params = {\n        'Bucket': bucket_name,\n        'Key': key_name,\n        'ResponseContentDisposition': content_disposition,\n    }\n    presigned_url = await s3_client.generate_presigned_url(\n        'get_object', Params=params\n    )\n    # Try to retrieve the object using the presigned url.\n\n    async with aio_session.get(presigned_url) as resp:\n        data = await resp.read()\n        assert resp.headers['Content-Disposition'] == content_disposition\n        assert data == b'foo'\n\n\n@pytest.mark.parametrize('region', ['us-east-1'])\n@pytest.mark.parametrize('signature_version', ['s3v4'])\n# moto host will be localhost\n@pytest.mark.parametrize('mocking_test', [False])\n@pytest.mark.asyncio\nasync def test_presign_sigv4(\n    s3_client, bucket_name, aio_session, create_object\n):\n    key = 'myobject'\n    await create_object(key_name=key)\n    presigned_url = await s3_client.generate_presigned_url(\n        'get_object', Params={'Bucket': bucket_name, 'Key': key}\n    )\n    msg = (\n        \"Host was suppose to be the us-east-1 endpoint, \"\n        \"instead got: %s\" % presigned_url\n    )\n    assert presigned_url.startswith(\n        f'https://{bucket_name}.s3.amazonaws.com/{key}'\n    ), msg\n\n    # Try to retrieve the object using the presigned url.\n    async with aio_session.get(presigned_url) as resp:\n        data = await resp.read()\n        assert data == b'foo'\n\n\n@pytest.mark.parametrize('signature_version', ['s3v4'])\n@pytest.mark.parametrize('mocking_test', [False])\n@pytest.mark.asyncio\nasync def test_can_follow_signed_url_redirect(\n    alternative_s3_client, create_object, bucket_name\n):\n    await create_object('foobarbaz')\n\n    # Simulate redirection by provide wrong endpoint intentionally\n    resp = await alternative_s3_client.get_object(\n        Bucket=bucket_name, Key='foobarbaz'\n    )\n    data = await resp['Body'].read()\n    resp['Body'].close()\n    assert data == b'foo'\n\n\n@pytest.mark.parametrize('region', ['eu-west-1'])\n@pytest.mark.parametrize('alternative_region', ['us-west-2'])\n@pytest.mark.parametrize('mocking_test', [False])\n@pytest.mark.asyncio\nasync def test_bucket_redirect(\n    s3_client, alternative_s3_client, region, create_bucket\n):\n    key = 'foobarbaz'\n\n    # create bucket in alternative region\n    bucket_name = await create_bucket(region)\n\n    await s3_client.put_object(Bucket=bucket_name, Key=key, Body=b'')\n    await s3_client.get_object(Bucket=bucket_name, Key=key)\n\n    # This should not raise\n    await alternative_s3_client.put_object(\n        Bucket=bucket_name, Key=key, Body=b''\n    )\n    await alternative_s3_client.get_object(Bucket=bucket_name, Key=key)\n\n\n@pytest.mark.parametrize('signature_version', ['s3v4'])\n@pytest.mark.asyncio\n@pytest.mark.moto\nasync def test_head_object_keys(s3_client, create_object, bucket_name):\n    await create_object('foobarbaz')\n\n    resp = await s3_client.head_object(Bucket=bucket_name, Key='foobarbaz')\n\n    # this is to ensure things like:\n    # https://github.com/aio-libs/aiobotocore/issues/131 don't happen again\n    assert set(resp.keys()) == {\n        'AcceptRanges',\n        'ETag',\n        'ContentType',\n        'Metadata',\n        'LastModified',\n        'ResponseMetadata',\n        'ContentLength',\n        'VersionId',\n    }\n\n\n@pytest.mark.xfail(\n    reason=\"moto does not yet support Checksum: https://github.com/spulec/moto/issues/5719\"\n)\n@pytest.mark.parametrize('server_scheme', ['https'])\n@pytest.mark.parametrize('s3_verify', [False])\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_put_object_sha256(s3_client, bucket_name):\n    data = b'test1234'\n    digest = hashlib.sha256(data).digest().hex()\n\n    resp = await s3_client.put_object(\n        Bucket=bucket_name,\n        Key='foobarbaz',\n        Body=data,\n        ChecksumAlgorithm='SHA256',\n    )\n    sha256_trailer_checksum = base64.b64decode(resp['ChecksumSHA256'])\n\n    assert digest == sha256_trailer_checksum\n", "tests/__init__.py": "", "tests/test_stubber.py": "import pytest\n\nfrom aiobotocore.awsrequest import AioAWSResponse\nfrom aiobotocore.session import AioSession\nfrom aiobotocore.stub import AioStubber\n\nfrom .mock_server import AIOServer\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_add_response():\n    session = AioSession()\n\n    async with AIOServer() as server, session.create_client(\n        's3',\n        endpoint_url=server.endpoint_url,\n        aws_secret_access_key='xxx',\n        aws_access_key_id='xxx',\n    ) as s3_client:\n        stubber = AioStubber(s3_client)\n        operation_name = 'put_object'\n        service_response = dict(\n            ETag=\"6805f2cfc46c0f04559748bb039d69ae\",\n            VersionId=\"psM2sYY4.o1501dSx8wMvnkOzSBB.V4a\",\n        )\n        expected_params = dict()\n        stubber.add_response(operation_name, service_response, expected_params)\n\n        assert len(stubber._queue) == 1\n        assert stubber._queue[0][\n            'operation_name'\n        ] == s3_client.meta.method_to_api_mapping.get(operation_name)\n        assert isinstance(stubber._queue[0]['response'][0], AioAWSResponse)\n        assert stubber._queue[0]['response'][1] == service_response\n        assert stubber._queue[0]['expected_params'] == expected_params\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_add_client_error():\n    session = AioSession()\n\n    async with AIOServer() as server, session.create_client(\n        's3',\n        endpoint_url=server.endpoint_url,\n        aws_secret_access_key='xxx',\n        aws_access_key_id='xxx',\n    ) as s3_client:\n        stubber = AioStubber(s3_client)\n        operation_name = 'put_object'\n        service_error_code = 'InvalidObjectState'\n        service_message = 'Object is in invalid state'\n        http_status_code = 400\n        service_error_meta = {\"AdditionalInfo\": \"value\"}\n        response_meta = {\"AdditionalResponseInfo\": \"value\"}\n        modeled_fields = {'StorageClass': 'foo', 'AccessTier': 'bar'}\n\n        stubber.add_client_error(\n            operation_name,\n            service_error_code,\n            service_message,\n            http_status_code,\n            service_error_meta,\n            response_meta=response_meta,\n            modeled_fields=modeled_fields,\n        )\n\n        assert len(stubber._queue) == 1\n        assert stubber._queue[0][\n            'operation_name'\n        ] == s3_client.meta.method_to_api_mapping.get(operation_name)\n        assert isinstance(stubber._queue[0]['response'][0], AioAWSResponse)\n        assert stubber._queue[0]['response'][1]\n", "tests/mock_server.py": "import asyncio\nimport multiprocessing\n\n# Third Party\nimport aiohttp\nimport aiohttp.web\nimport pytest\nfrom aiohttp.web import StreamResponse\n\n# aiobotocore\nfrom tests.moto_server import MotoService, get_free_tcp_port, host\n\n_proxy_bypass = {\n    \"http\": None,\n    \"https\": None,\n}\n\n\n# This runs in a subprocess for a variety of reasons\n# 1) early versions of python 3.5 did not correctly set one thread per run loop\n# 2) aiohttp uses get_event_loop instead of using the passed in run loop\n# 3) aiohttp shutdown can be hairy\nclass AIOServer(multiprocessing.Process):\n    \"\"\"\n    This is a mock AWS service which will 5 seconds before returning\n    a response to test socket timeouts.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(target=self._run)\n        self._loop = None\n        self._port = get_free_tcp_port(True)\n        self.endpoint_url = f'http://{host}:{self._port}'\n        self.daemon = True  # die when parent dies\n\n    def _run(self):\n        asyncio.set_event_loop(asyncio.new_event_loop())\n        app = aiohttp.web.Application()\n        app.router.add_route('*', '/ok', self.ok)\n        app.router.add_route('*', '/{anything:.*}', self.stream_handler)\n\n        try:\n            aiohttp.web.run_app(\n                app, host=host, port=self._port, handle_signals=False\n            )\n        except BaseException:\n            pytest.fail('unable to start and connect to aiohttp server')\n            raise\n\n    async def __aenter__(self):\n        self.start()\n        await self._wait_until_up()\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        try:\n            self.terminate()\n        except BaseException:\n            pytest.fail(\"Unable to shut down server\")\n            raise\n\n    @staticmethod\n    async def ok(request):\n        return aiohttp.web.Response()\n\n    async def stream_handler(self, request):\n        # Without the Content-Type, most (all?) browsers will not render\n        # partially downloaded content. Note, the response type is\n        # StreamResponse not Response.\n        resp = StreamResponse(\n            status=200, reason='OK', headers={'Content-Type': 'text/html'}\n        )\n\n        await resp.prepare(request)\n        await asyncio.sleep(5)\n        await resp.drain()\n        return resp\n\n    async def _wait_until_up(self):\n        async with aiohttp.ClientSession() as session:\n            for i in range(0, 30):\n                if self.exitcode is not None:\n                    pytest.fail('unable to start/connect to aiohttp server')\n                    return\n\n                try:\n                    # we need to bypass the proxies due to monkey patches\n                    await session.get(self.endpoint_url + '/ok', timeout=0.5)\n                    return\n                except (aiohttp.ClientConnectionError, asyncio.TimeoutError):\n                    await asyncio.sleep(0.5)\n                except BaseException:\n                    pytest.fail('unable to start/connect to aiohttp server')\n                    raise\n\n        pytest.fail('unable to start and connect to aiohttp server')\n\n\n@pytest.fixture\nasync def s3_server(server_scheme):\n    async with MotoService('s3', ssl=server_scheme == 'https') as svc:\n        yield svc.endpoint_url\n\n\n@pytest.fixture\nasync def dynamodb2_server(server_scheme):\n    async with MotoService('dynamodb', ssl=server_scheme == 'https') as svc:\n        yield svc.endpoint_url\n\n\n@pytest.fixture\nasync def cloudformation_server(server_scheme):\n    async with MotoService(\n        'cloudformation', ssl=server_scheme == 'https'\n    ) as svc:\n        yield svc.endpoint_url\n\n\n@pytest.fixture\nasync def sns_server(server_scheme):\n    async with MotoService('sns', ssl=server_scheme == 'https') as svc:\n        yield svc.endpoint_url\n\n\n@pytest.fixture\nasync def sqs_server(server_scheme):\n    async with MotoService('sqs', ssl=server_scheme == 'https') as svc:\n        yield svc.endpoint_url\n\n\n@pytest.fixture\nasync def batch_server(server_scheme):\n    async with MotoService('batch', ssl=server_scheme == 'https') as svc:\n        yield svc.endpoint_url\n\n\n@pytest.fixture\nasync def lambda_server(server_scheme):\n    async with MotoService('lambda', ssl=server_scheme == 'https') as svc:\n        yield svc.endpoint_url\n\n\n@pytest.fixture\nasync def iam_server(server_scheme):\n    async with MotoService('iam', ssl=server_scheme == 'https') as svc:\n        yield svc.endpoint_url\n\n\n@pytest.fixture\nasync def rds_server(server_scheme):\n    async with MotoService('rds', ssl=server_scheme == 'https') as svc:\n        yield svc.endpoint_url\n\n\n@pytest.fixture\nasync def ec2_server(server_scheme):\n    async with MotoService('ec2', ssl=server_scheme == 'https') as svc:\n        yield svc.endpoint_url\n\n\n@pytest.fixture\nasync def kinesis_server(server_scheme):\n    async with MotoService('kinesis', ssl=server_scheme == 'https') as svc:\n        yield svc.endpoint_url\n", "tests/test_session.py": "import logging\n\nimport pytest\nfrom _pytest.logging import LogCaptureFixture\n\nfrom aiobotocore import __version__, httpsession\nfrom aiobotocore.config import AioConfig\nfrom aiobotocore.session import AioSession\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_get_service_data(session):\n    handler_called = False\n\n    def handler(**kwargs):\n        nonlocal handler_called\n        handler_called = True\n\n    session.register('service-data-loaded.s3', handler)\n    await session.get_service_data('s3')\n\n    assert handler_called\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_retry(\n    session: AioSession, caplog: LogCaptureFixture, monkeypatch\n):\n    caplog.set_level(logging.DEBUG)\n\n    config = AioConfig(\n        connect_timeout=1,\n        read_timeout=1,\n        # this goes through a slightly different codepath than regular retries\n        retries={\n            \"mode\": \"standard\",\n            \"total_max_attempts\": 3,\n        },\n    )\n\n    async with session.create_client(\n        's3',\n        config=config,\n        aws_secret_access_key=\"xxx\",\n        aws_access_key_id=\"xxx\",\n        endpoint_url='http://localhost:7878',\n    ) as client:\n        # this needs the new style exceptions to work\n        with pytest.raises(httpsession.EndpointConnectionError):\n            await client.get_object(Bucket='foo', Key='bar')\n\n        assert 'sleeping for' in caplog.text\n\n\n@pytest.mark.moto\nasync def test_set_user_agent_for_session(session: AioSession):\n    assert session.user_agent_name == \"aiobotocore\"\n    assert session.user_agent_version == __version__\n    assert session.user_agent_extra.startswith(\"botocore/\")\n", "tests/_helpers.py": "try:\n    from contextlib import AsyncExitStack  # noqa: F401 lgtm[py/unused-import]\nexcept ImportError:\n    from async_exit_stack import (  # noqa: F401 lgtm[py/unused-import]\n        AsyncExitStack,\n    )\n", "tests/python3.8/test_eventstreams.py": "import asyncio\n\nimport pytest\n\nimport aiobotocore.session\nfrom tests._helpers import AsyncExitStack\n\n\n@pytest.mark.asyncio\nasync def test_kinesis_stream_json_parser(exit_stack: AsyncExitStack):\n    # unfortunately moto doesn't support kinesis register_stream_consumer +\n    # subscribe_to_shard yet\n    stream_name = \"my_stream\"\n    stream_arn = consumer_arn = None\n    consumer_name = 'consumer'\n\n    session = aiobotocore.session.AioSession()\n\n    kinesis_client = await exit_stack.enter_async_context(\n        session.create_client('kinesis')\n    )\n    await kinesis_client.create_stream(StreamName=stream_name, ShardCount=1)\n\n    while (\n        describe_response := (\n            await kinesis_client.describe_stream(  # noqa: E231, E999, E251, E501\n                StreamName=stream_name\n            )\n        )\n    ) and describe_response['StreamDescription']['StreamStatus'] == 'CREATING':\n        print(\"Waiting for stream creation\")\n        await asyncio.sleep(1)\n\n    shard_id = describe_response[\"StreamDescription\"][\"Shards\"][0][\"ShardId\"]\n    stream_arn = describe_response[\"StreamDescription\"][\"StreamARN\"]\n\n    try:\n        # Create some data\n        keys = [str(i) for i in range(1, 5)]\n        for k in keys:\n            await kinesis_client.put_record(\n                StreamName=stream_name, Data=k, PartitionKey=k\n            )\n\n        register_response = await kinesis_client.register_stream_consumer(\n            StreamARN=stream_arn, ConsumerName=consumer_name\n        )\n        consumer_arn = register_response['Consumer']['ConsumerARN']\n\n        while (\n            describe_response := (\n                await kinesis_client.describe_stream_consumer(  # noqa: E231, E999, E251, E501\n                    StreamARN=stream_arn,\n                    ConsumerName=consumer_name,\n                    ConsumerARN=consumer_arn,\n                )\n            )\n        ) and describe_response['ConsumerDescription'][\n            'ConsumerStatus'\n        ] == 'CREATING':\n            print(\"Waiting for stream consumer creation\")\n            await asyncio.sleep(1)\n\n        starting_position = {'Type': 'LATEST'}\n        subscribe_response = await kinesis_client.subscribe_to_shard(\n            ConsumerARN=consumer_arn,\n            ShardId=shard_id,\n            StartingPosition=starting_position,\n        )\n        async for event in subscribe_response['EventStream']:\n            assert event['SubscribeToShardEvent']['Records'] == []\n            break\n    finally:\n        if consumer_arn:\n            await kinesis_client.deregister_stream_consumer(\n                StreamARN=stream_arn,\n                ConsumerName=consumer_name,\n                ConsumerARN=consumer_arn,\n            )\n\n        await kinesis_client.delete_stream(StreamName=stream_name)\n", "tests/python3.8/__init__.py": "", "tests/python3.8/boto_tests/test_signers.py": "from unittest import mock\n\nimport botocore.auth\nimport pytest\nfrom botocore.awsrequest import AWSRequest\nfrom botocore.exceptions import (\n    NoRegionError,\n    UnknownClientMethodError,\n    UnknownSignatureVersionError,\n)\nfrom botocore.model import ServiceId\n\nimport aiobotocore.credentials\nimport aiobotocore.signers\n\n\n# From class TestSigner\n@pytest.fixture\nasync def base_signer_setup() -> dict:\n    emitter = mock.AsyncMock()\n    emitter.emit_until_response.return_value = (None, None)\n    credentials = aiobotocore.credentials.AioCredentials('key', 'secret')\n\n    signer = aiobotocore.signers.AioRequestSigner(\n        ServiceId('service_name'),\n        'region_name',\n        'signing_name',\n        'v4',\n        credentials,\n        emitter,\n    )\n    return {\n        'credentials': credentials,\n        'emitter': emitter,\n        'signer': signer,\n        'fixed_credentials': await credentials.get_frozen_credentials(),\n        'request': AWSRequest(),\n    }\n\n\n@pytest.fixture\nasync def base_signer_setup_s3v4() -> dict:\n    emitter = mock.AsyncMock()\n    emitter.emit_until_response.return_value = (None, None)\n    credentials = aiobotocore.credentials.AioCredentials('key', 'secret')\n\n    request_signer = aiobotocore.signers.AioRequestSigner(\n        ServiceId('service_name'),\n        'region_name',\n        'signing_name',\n        's3v4',\n        credentials,\n        emitter,\n    )\n    signer = aiobotocore.signers.AioS3PostPresigner(request_signer)\n\n    return {\n        'credentials': credentials,\n        'emitter': emitter,\n        'signer': signer,\n        'fixed_credentials': await credentials.get_frozen_credentials(),\n        'request': AWSRequest(),\n    }\n\n\n# From class TestGenerateUrl\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_signers_generate_presigned_urls():\n    with mock.patch(\n        'aiobotocore.signers.AioRequestSigner.generate_presigned_url'\n    ) as cls_gen_presigned_url_mock:\n        session = aiobotocore.session.get_session()\n        async with session.create_client(\n            's3',\n            region_name='us-east-1',\n            aws_access_key_id='lalala',\n            aws_secret_access_key='lalala',\n            aws_session_token='lalala',\n        ) as client:\n            # Uses HEAD as it covers more lines :)\n            await client.generate_presigned_url(\n                'get_object',\n                Params={'Bucket': 'mybucket', 'Key': 'mykey'},\n                HttpMethod='HEAD',\n            )\n\n            ref_request_dict = {\n                'body': b'',\n                'url': 'https://mybucket.s3.amazonaws.com/mykey',\n                'headers': {},\n                'query_string': {},\n                'url_path': '/mykey',\n                'method': 'HEAD',\n                'context': mock.ANY,\n                'auth_path': '/mybucket/mykey',\n            }\n\n            cls_gen_presigned_url_mock.assert_called_with(\n                request_dict=ref_request_dict,\n                expires_in=3600,\n                operation_name='GetObject',\n            )\n\n            cls_gen_presigned_url_mock.reset_mock()\n\n            with pytest.raises(UnknownClientMethodError):\n                await client.generate_presigned_url('lalala')\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_signers_generate_presigned_post():\n    with mock.patch(\n        'aiobotocore.signers.AioS3PostPresigner.generate_presigned_post'\n    ) as cls_gen_presigned_url_mock:\n        session = aiobotocore.session.get_session()\n        async with session.create_client(\n            's3',\n            region_name='us-east-1',\n            aws_access_key_id='lalala',\n            aws_secret_access_key='lalala',\n            aws_session_token='lalala',\n        ) as client:\n            await client.generate_presigned_post(\n                'somebucket', 'someprefix/key'\n            )\n\n            cls_gen_presigned_url_mock.assert_called_once()\n\n            cls_gen_presigned_url_mock.reset_mock()\n\n            await client.generate_presigned_post(\n                'somebucket',\n                'someprefix/${filename}',\n                {'some': 'fields'},\n                [{'acl': 'public-read'}],\n            )\n\n            cls_gen_presigned_url_mock.assert_called_once()\n\n            cls_gen_presigned_url_mock.reset_mock()\n\n            with pytest.raises(UnknownClientMethodError):\n                await client.generate_presigned_url('lalala')\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_testsigner_get_auth(base_signer_setup: dict):\n    auth_cls = mock.Mock()\n    with mock.patch.dict(botocore.auth.AUTH_TYPE_MAPS, {'v4': auth_cls}):\n        signer = base_signer_setup['signer']\n        auth = await signer.get_auth('service_name', 'region_name')\n\n        assert auth_cls.return_value is auth\n        auth_cls.assert_called_with(\n            credentials=base_signer_setup['fixed_credentials'],\n            service_name='service_name',\n            region_name='region_name',\n        )\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_testsigner_region_required_for_sig4(base_signer_setup: dict):\n    signer = aiobotocore.signers.AioRequestSigner(\n        ServiceId('service_name'),\n        None,\n        'signing_name',\n        'v4',\n        base_signer_setup['credentials'],\n        base_signer_setup['emitter'],\n    )\n\n    with pytest.raises(NoRegionError):\n        await signer.sign('operation_name', base_signer_setup['request'])\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_testsigner_custom_sign_version(base_signer_setup: dict):\n    signer = base_signer_setup['signer']\n    with pytest.raises(UnknownSignatureVersionError):\n        await signer.get_auth(\n            'service_name', 'region_name', signature_version='bad'\n        )\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_testsigner_choose_signer_override(base_signer_setup: dict):\n    auth_cls = mock.Mock()\n    auth_cls.REQUIRES_REGION = False\n    base_signer_setup['emitter'].emit_until_response.return_value = (\n        None,\n        'custom',\n    )\n\n    with mock.patch.dict(botocore.auth.AUTH_TYPE_MAPS, {'custom': auth_cls}):\n        signer = base_signer_setup['signer']\n        request = base_signer_setup['request']\n        await signer.sign('operation_name', request)\n\n        fixed_credentials = base_signer_setup['fixed_credentials']\n        auth_cls.assert_called_with(credentials=fixed_credentials)\n        auth_cls.return_value.add_auth.assert_called_with(request)\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_testsigner_generate_presigned_url(base_signer_setup: dict):\n    auth_cls = mock.Mock()\n    auth_cls.REQUIRES_REGION = True\n\n    request_dict = {\n        'headers': {},\n        'url': 'https://foo.com',\n        'body': b'',\n        'url_path': '/',\n        'method': 'GET',\n        'context': {},\n    }\n\n    with mock.patch.dict(botocore.auth.AUTH_TYPE_MAPS, {'v4-query': auth_cls}):\n        signer = base_signer_setup['signer']\n        presigned_url = await signer.generate_presigned_url(\n            request_dict, operation_name='operation_name'\n        )\n\n    auth_cls.assert_called_with(\n        credentials=base_signer_setup['fixed_credentials'],\n        region_name='region_name',\n        service_name='signing_name',\n        expires=3600,\n    )\n    assert presigned_url == 'https://foo.com'\n\n\n# From class TestGeneratePresignedPost\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_testsigner_generate_presigned_post(\n    base_signer_setup_s3v4: dict,\n):\n    auth_cls = mock.Mock()\n    auth_cls.REQUIRES_REGION = True\n\n    request_dict = {\n        'headers': {},\n        'url': 'https://s3.amazonaws.com/mybucket',\n        'body': b'',\n        'url_path': '/',\n        'method': 'POST',\n        'context': {},\n    }\n\n    with mock.patch.dict(\n        botocore.auth.AUTH_TYPE_MAPS, {'s3v4-presign-post': auth_cls}\n    ):\n        signer = base_signer_setup_s3v4['signer']\n        presigned_url = await signer.generate_presigned_post(\n            request_dict, conditions=[{'acl': 'public-read'}]\n        )\n\n    auth_cls.assert_called_with(\n        credentials=base_signer_setup_s3v4['fixed_credentials'],\n        region_name='region_name',\n        service_name='signing_name',\n    )\n    assert presigned_url['url'] == 'https://s3.amazonaws.com/mybucket'\n", "tests/python3.8/boto_tests/test_credentials.py": "import asyncio\nimport json\nimport subprocess\nfrom datetime import datetime, timedelta\nfrom unittest import mock\n\nimport botocore.exceptions\nimport pytest\nfrom dateutil.tz import tzlocal\n\nfrom aiobotocore import credentials\nfrom aiobotocore.session import AioSession\nfrom tests.boto_tests.test_credentials import full_url\n\n\n# From class TestRefreshableCredentials(TestCredentials):\n@pytest.fixture\ndef refreshable_creds():\n    def _f(mock_time_return_value=None, refresher_return_value='METADATA'):\n        refresher = mock.AsyncMock()\n        future_time = datetime.now(tzlocal()) + timedelta(hours=24)\n        expiry_time = datetime.now(tzlocal()) - timedelta(minutes=30)\n        metadata = {\n            'access_key': 'NEW-ACCESS',\n            'secret_key': 'NEW-SECRET',\n            'token': 'NEW-TOKEN',\n            'expiry_time': future_time.isoformat(),\n            'role_name': 'rolename',\n        }\n        refresher.return_value = (\n            metadata\n            if refresher_return_value == 'METADATA'\n            else refresher_return_value\n        )\n        mock_time = mock.Mock()\n        mock_time.return_value = mock_time_return_value\n        creds = credentials.AioRefreshableCredentials(\n            'ORIGINAL-ACCESS',\n            'ORIGINAL-SECRET',\n            'ORIGINAL-TOKEN',\n            expiry_time,\n            refresher,\n            'iam-role',\n            time_fetcher=mock_time,\n        )\n        return creds\n\n    return _f\n\n\n# From class TestDeferredRefreshableCredentials(unittest.TestCase):\n@pytest.fixture\ndef deferrable_creds():\n    def _f(mock_time_return_value=None, refresher_return_value='METADATA'):\n        refresher = mock.AsyncMock()\n        future_time = datetime.now(tzlocal()) + timedelta(hours=24)\n        metadata = {\n            'access_key': 'NEW-ACCESS',\n            'secret_key': 'NEW-SECRET',\n            'token': 'NEW-TOKEN',\n            'expiry_time': future_time.isoformat(),\n            'role_name': 'rolename',\n        }\n        refresher.return_value = (\n            metadata\n            if refresher_return_value == 'METADATA'\n            else refresher_return_value\n        )\n        mock_time = mock.Mock()\n        mock_time.return_value = mock_time_return_value or datetime.now(\n            tzlocal()\n        )\n        creds = credentials.AioDeferredRefreshableCredentials(\n            refresher, 'iam-role', time_fetcher=mock_time\n        )\n        return creds\n\n    return _f\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_refreshablecredentials_get_credentials_set(refreshable_creds):\n    creds = refreshable_creds(\n        mock_time_return_value=(\n            datetime.now(tzlocal()) - timedelta(minutes=60)\n        )\n    )\n\n    assert not creds.refresh_needed()\n\n    credentials_set = await creds.get_frozen_credentials()\n    assert isinstance(credentials_set, credentials.ReadOnlyCredentials)\n    assert credentials_set.access_key == 'ORIGINAL-ACCESS'\n    assert credentials_set.secret_key == 'ORIGINAL-SECRET'\n    assert credentials_set.token == 'ORIGINAL-TOKEN'\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_refreshablecredentials_refresh_returns_empty_dict(\n    refreshable_creds,\n):\n    creds = refreshable_creds(\n        mock_time_return_value=datetime.now(tzlocal()),\n        refresher_return_value={},\n    )\n\n    assert creds.refresh_needed()\n\n    with pytest.raises(botocore.exceptions.CredentialRetrievalError):\n        await creds.get_frozen_credentials()\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_refreshablecredentials_refresh_returns_none(refreshable_creds):\n    creds = refreshable_creds(\n        mock_time_return_value=datetime.now(tzlocal()),\n        refresher_return_value=None,\n    )\n\n    assert creds.refresh_needed()\n\n    with pytest.raises(botocore.exceptions.CredentialRetrievalError):\n        await creds.get_frozen_credentials()\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_refreshablecredentials_refresh_returns_partial(\n    refreshable_creds,\n):\n    creds = refreshable_creds(\n        mock_time_return_value=datetime.now(tzlocal()),\n        refresher_return_value={'access_key': 'akid'},\n    )\n\n    assert creds.refresh_needed()\n\n    with pytest.raises(botocore.exceptions.CredentialRetrievalError):\n        await creds.get_frozen_credentials()\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_deferrablecredentials_get_credentials_set(deferrable_creds):\n    creds = deferrable_creds()\n\n    creds._refresh_using.assert_not_called()\n\n    await creds.get_frozen_credentials()\n    assert creds._refresh_using.call_count == 1\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_deferrablecredentials_refresh_only_called_once(\n    deferrable_creds,\n):\n    creds = deferrable_creds()\n\n    creds._refresh_using.assert_not_called()\n\n    for _ in range(5):\n        await creds.get_frozen_credentials()\n\n    assert creds._refresh_using.call_count == 1\n\n\n# From class TestInstanceMetadataProvider(BaseEnvVar):\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_instancemetadata_load():\n    timeobj = datetime.now(tzlocal())\n    timestamp = (timeobj + timedelta(hours=24)).isoformat()\n\n    fetcher = mock.AsyncMock()\n    fetcher.retrieve_iam_role_credentials = mock.AsyncMock(\n        return_value={\n            'access_key': 'a',\n            'secret_key': 'b',\n            'token': 'c',\n            'expiry_time': timestamp,\n            'role_name': 'myrole',\n        }\n    )\n\n    provider = credentials.AioInstanceMetadataProvider(\n        iam_role_fetcher=fetcher\n    )\n    creds = await provider.load()\n    assert creds is not None\n    assert creds.method == 'iam-role'\n\n    creds = await creds.get_frozen_credentials()\n    assert creds.access_key == 'a'\n    assert creds.secret_key == 'b'\n    assert creds.token == 'c'\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_containerprovider_assume_role_no_cache():\n    environ = {\n        'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/latest/credentials?id=foo'\n    }\n    fetcher = mock.AsyncMock()\n    fetcher.full_url = full_url\n\n    timeobj = datetime.now(tzlocal())\n    timestamp = (timeobj + timedelta(hours=24)).isoformat()\n    fetcher.retrieve_full_uri.return_value = {\n        \"AccessKeyId\": \"access_key\",\n        \"SecretAccessKey\": \"secret_key\",\n        \"Token\": \"token\",\n        \"Expiration\": timestamp,\n    }\n    provider = credentials.AioContainerProvider(environ, fetcher)\n    # Will return refreshable credentials\n    creds = await provider.load()\n\n    url = full_url('/latest/credentials?id=foo')\n    fetcher.retrieve_full_uri.assert_called_with(url, headers=None)\n\n    assert creds.method == 'container-role'\n\n    creds = await creds.get_frozen_credentials()\n    assert creds.access_key == 'access_key'\n    assert creds.secret_key == 'secret_key'\n    assert creds.token == 'token'\n\n\n# From class TestProcessProvider\n@pytest.fixture()\ndef process_provider():\n    def _f(profile_name='default', loaded_config=None, invoked_process=None):\n        load_config = mock.Mock(return_value=loaded_config)\n        popen_mock = mock.Mock(\n            return_value=invoked_process or mock.Mock(),\n            spec=asyncio.create_subprocess_exec,\n        )\n        return popen_mock, credentials.AioProcessProvider(\n            profile_name, load_config, popen=popen_mock\n        )\n\n    return _f\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_processprovider_retrieve_refereshable_creds(process_provider):\n    config = {\n        'profiles': {'default': {'credential_process': 'my-process /somefile'}}\n    }\n    invoked_process = mock.AsyncMock()\n    stdout = json.dumps(\n        {\n            'Version': 1,\n            'AccessKeyId': 'foo',\n            'SecretAccessKey': 'bar',\n            'SessionToken': 'baz',\n            'Expiration': '2999-01-01T00:00:00Z',\n        }\n    )\n    invoked_process.communicate.return_value = (stdout.encode('utf-8'), b'')\n    invoked_process.returncode = 0\n\n    popen_mock, provider = process_provider(\n        loaded_config=config, invoked_process=invoked_process\n    )\n    creds = await provider.load()\n    assert isinstance(creds, credentials.AioRefreshableCredentials)\n    assert creds is not None\n    assert creds.method == 'custom-process'\n\n    creds = await creds.get_frozen_credentials()\n    assert creds.access_key == 'foo'\n    assert creds.secret_key == 'bar'\n    assert creds.token == 'baz'\n    popen_mock.assert_called_with(\n        'my-process',\n        '/somefile',\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n    )\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_processprovider_retrieve_creds(process_provider):\n    config = {'profiles': {'default': {'credential_process': 'my-process'}}}\n    invoked_process = mock.AsyncMock()\n    stdout = json.dumps(\n        {\n            'Version': 1,\n            'AccessKeyId': 'foo',\n            'SecretAccessKey': 'bar',\n            'SessionToken': 'baz',\n        }\n    )\n    invoked_process.communicate.return_value = (stdout.encode('utf-8'), b'')\n    invoked_process.returncode = 0\n\n    popen_mock, provider = process_provider(\n        loaded_config=config, invoked_process=invoked_process\n    )\n    creds = await provider.load()\n    assert isinstance(creds, credentials.AioCredentials)\n    assert creds is not None\n    assert creds.access_key == 'foo'\n    assert creds.secret_key == 'bar'\n    assert creds.token == 'baz'\n    assert creds.method == 'custom-process'\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_processprovider_bad_version(process_provider):\n    config = {'profiles': {'default': {'credential_process': 'my-process'}}}\n    invoked_process = mock.AsyncMock()\n    stdout = json.dumps(\n        {\n            'Version': 2,\n            'AccessKeyId': 'foo',\n            'SecretAccessKey': 'bar',\n            'SessionToken': 'baz',\n            'Expiration': '2999-01-01T00:00:00Z',\n        }\n    )\n    invoked_process.communicate.return_value = (stdout.encode('utf-8'), b'')\n    invoked_process.returncode = 0\n\n    popen_mock, provider = process_provider(\n        loaded_config=config, invoked_process=invoked_process\n    )\n    with pytest.raises(botocore.exceptions.CredentialRetrievalError):\n        await provider.load()\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_processprovider_missing_field(process_provider):\n    config = {'profiles': {'default': {'credential_process': 'my-process'}}}\n    invoked_process = mock.AsyncMock()\n    stdout = json.dumps(\n        {\n            'Version': 1,\n            'SecretAccessKey': 'bar',\n            'SessionToken': 'baz',\n            'Expiration': '2999-01-01T00:00:00Z',\n        }\n    )\n    invoked_process.communicate.return_value = (stdout.encode('utf-8'), b'')\n    invoked_process.returncode = 0\n\n    popen_mock, provider = process_provider(\n        loaded_config=config, invoked_process=invoked_process\n    )\n    with pytest.raises(botocore.exceptions.CredentialRetrievalError):\n        await provider.load()\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_processprovider_bad_exitcode(process_provider):\n    config = {'profiles': {'default': {'credential_process': 'my-process'}}}\n    invoked_process = mock.AsyncMock()\n    stdout = 'lah'\n    invoked_process.communicate.return_value = (stdout.encode('utf-8'), b'')\n    invoked_process.returncode = 1\n\n    popen_mock, provider = process_provider(\n        loaded_config=config, invoked_process=invoked_process\n    )\n    with pytest.raises(botocore.exceptions.CredentialRetrievalError):\n        await provider.load()\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_processprovider_bad_config(process_provider):\n    config = {'profiles': {'default': {'credential_process': None}}}\n    invoked_process = mock.AsyncMock()\n    stdout = json.dumps(\n        {\n            'Version': 2,\n            'AccessKeyId': 'foo',\n            'SecretAccessKey': 'bar',\n            'SessionToken': 'baz',\n            'Expiration': '2999-01-01T00:00:00Z',\n        }\n    )\n    invoked_process.communicate.return_value = (stdout.encode('utf-8'), b'')\n    invoked_process.returncode = 0\n\n    popen_mock, provider = process_provider(\n        loaded_config=config, invoked_process=invoked_process\n    )\n    creds = await provider.load()\n    assert creds is None\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_session_credentials():\n    with mock.patch(\n        'aiobotocore.credentials.AioCredential' 'Resolver.load_credentials'\n    ) as mock_obj:\n        mock_obj.return_value = 'somecreds'\n\n        session = AioSession()\n        creds = await session.get_credentials()\n        assert creds == 'somecreds'\n", "tests/python3.8/boto_tests/__init__.py": "", "tests/python3.8/boto_tests/test_tokens.py": "# Copyright 2022 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom unittest import mock\n\nimport dateutil.parser\nimport pytest\nfrom botocore.exceptions import (\n    InvalidConfigError,\n    SSOTokenLoadError,\n    TokenRetrievalError,\n)\n\nfrom aiobotocore.session import AioSession\nfrom aiobotocore.tokens import AioSSOTokenProvider\n\n\ndef parametrize(cases):\n    return pytest.mark.parametrize(\n        \"test_case\",\n        cases,\n        ids=[c[\"documentation\"] for c in cases],\n    )\n\n\nsso_provider_resolution_cases = [\n    {\n        \"documentation\": \"Full valid profile\",\n        \"config\": {\n            \"profiles\": {\"test\": {\"sso_session\": \"admin\"}},\n            \"sso_sessions\": {\n                \"admin\": {\n                    \"sso_region\": \"us-east-1\",\n                    \"sso_start_url\": \"https://d-abc123.awsapps.com/start\",\n                }\n            },\n        },\n        \"resolves\": True,\n    },\n    {\n        \"documentation\": \"Non-SSO profiles are skipped\",\n        \"config\": {\"profiles\": {\"test\": {\"region\": \"us-west-2\"}}},\n        \"resolves\": False,\n    },\n    {\n        \"documentation\": \"Only start URL is invalid\",\n        \"config\": {\n            \"profiles\": {\"test\": {\"sso_session\": \"admin\"}},\n            \"sso_sessions\": {\n                \"admin\": {\n                    \"sso_start_url\": \"https://d-abc123.awsapps.com/start\"\n                }\n            },\n        },\n        \"resolves\": False,\n        \"expectedException\": InvalidConfigError,\n    },\n    {\n        \"documentation\": \"Only sso_region is invalid\",\n        \"config\": {\n            \"profiles\": {\"test\": {\"sso_session\": \"admin\"}},\n            \"sso_sessions\": {\"admin\": {\"sso_region\": \"us-east-1\"}},\n        },\n        \"resolves\": False,\n        \"expectedException\": InvalidConfigError,\n    },\n    {\n        \"documentation\": \"Specified sso-session must exist\",\n        \"config\": {\n            \"profiles\": {\"test\": {\"sso_session\": \"dev\"}},\n            \"sso_sessions\": {\"admin\": {\"sso_region\": \"us-east-1\"}},\n        },\n        \"resolves\": False,\n        \"expectedException\": InvalidConfigError,\n    },\n    {\n        \"documentation\": \"The sso_session must be specified\",\n        \"config\": {\n            \"profiles\": {\"test\": {\"region\": \"us-west-2\"}},\n            \"sso_sessions\": {\n                \"admin\": {\n                    \"sso_region\": \"us-east-1\",\n                    \"sso_start_url\": \"https://d-abc123.awsapps.com/start\",\n                }\n            },\n        },\n        \"resolves\": False,\n    },\n]\n\n\ndef _create_mock_session(config):\n    mock_session = mock.Mock(spec=AioSession)\n    mock_session.get_config_variable.return_value = \"test\"\n    mock_session.full_config = config\n    return mock_session\n\n\ndef _run_token_provider_test_case(provider, test_case):\n    expected_exception = test_case.get(\"expectedException\")\n    if expected_exception is not None:\n        with pytest.raises(expected_exception):\n            auth_token = provider.load_token()\n        return\n\n    auth_token = provider.load_token()\n    if test_case[\"resolves\"]:\n        assert auth_token is not None\n    else:\n        assert auth_token is None\n\n\n@pytest.mark.moto\n@parametrize(sso_provider_resolution_cases)\ndef test_sso_token_provider_resolution(test_case):\n    mock_session = _create_mock_session(test_case[\"config\"])\n    resolver = AioSSOTokenProvider(mock_session)\n\n    _run_token_provider_test_case(resolver, test_case)\n\n\n@pytest.mark.moto\n@parametrize(sso_provider_resolution_cases)\ndef test_sso_token_provider_profile_name_overrides_session_profile(test_case):\n    mock_session = _create_mock_session(test_case[\"config\"])\n    mock_session.get_config_variable.return_value = \"default\"\n    resolver = AioSSOTokenProvider(mock_session, profile_name='test')\n\n    _run_token_provider_test_case(resolver, test_case)\n\n\nsso_provider_refresh_cases = [\n    {\n        \"documentation\": \"Valid token with all fields\",\n        \"currentTime\": \"2021-12-25T13:30:00Z\",\n        \"cachedToken\": {\n            \"startUrl\": \"https://d-123.awsapps.com/start\",\n            \"region\": \"us-west-2\",\n            \"accessToken\": \"cachedtoken\",\n            \"expiresAt\": \"2021-12-25T21:30:00Z\",\n            \"clientId\": \"clientid\",\n            \"clientSecret\": \"YSBzZWNyZXQ=\",\n            \"registrationExpiresAt\": \"2022-12-25T13:30:00Z\",\n            \"refreshToken\": \"cachedrefreshtoken\",\n        },\n        \"expectedToken\": {\n            \"token\": \"cachedtoken\",\n            \"expiration\": \"2021-12-25T21:30:00Z\",\n        },\n    },\n    {\n        \"documentation\": \"Minimal valid cached token\",\n        \"currentTime\": \"2021-12-25T13:30:00Z\",\n        \"cachedToken\": {\n            \"accessToken\": \"cachedtoken\",\n            \"expiresAt\": \"2021-12-25T21:30:00Z\",\n        },\n        \"expectedToken\": {\n            \"token\": \"cachedtoken\",\n            \"expiration\": \"2021-12-25T21:30:00Z\",\n        },\n    },\n    {\n        \"documentation\": \"Minimal expired cached token\",\n        \"currentTime\": \"2021-12-25T13:30:00Z\",\n        \"cachedToken\": {\n            \"accessToken\": \"cachedtoken\",\n            \"expiresAt\": \"2021-12-25T13:00:00Z\",\n        },\n        \"expectedException\": TokenRetrievalError,\n    },\n    {\n        \"documentation\": \"Token missing the expiresAt field\",\n        \"currentTime\": \"2021-12-25T13:30:00Z\",\n        \"cachedToken\": {\"accessToken\": \"cachedtoken\"},\n        \"expectedException\": SSOTokenLoadError,\n    },\n    {\n        \"documentation\": \"Token missing the accessToken field\",\n        \"currentTime\": \"2021-12-25T13:30:00Z\",\n        \"cachedToken\": {\"expiresAt\": \"2021-12-25T13:00:00Z\"},\n        \"expectedException\": SSOTokenLoadError,\n    },\n    {\n        \"documentation\": \"Expired token refresh with refresh token\",\n        \"currentTime\": \"2021-12-25T13:30:00Z\",\n        \"cachedToken\": {\n            \"startUrl\": \"https://d-123.awsapps.com/start\",\n            \"region\": \"us-west-2\",\n            \"accessToken\": \"cachedtoken\",\n            \"expiresAt\": \"2021-12-25T13:00:00Z\",\n            \"clientId\": \"clientid\",\n            \"clientSecret\": \"YSBzZWNyZXQ=\",\n            \"registrationExpiresAt\": \"2022-12-25T13:30:00Z\",\n            \"refreshToken\": \"cachedrefreshtoken\",\n        },\n        \"refreshResponse\": {\n            \"tokenType\": \"Bearer\",\n            \"accessToken\": \"newtoken\",\n            \"expiresIn\": 28800,\n            \"refreshToken\": \"newrefreshtoken\",\n        },\n        \"expectedTokenWriteback\": {\n            \"startUrl\": \"https://d-123.awsapps.com/start\",\n            \"region\": \"us-west-2\",\n            \"accessToken\": \"newtoken\",\n            \"expiresAt\": \"2021-12-25T21:30:00Z\",\n            \"clientId\": \"clientid\",\n            \"clientSecret\": \"YSBzZWNyZXQ=\",\n            \"registrationExpiresAt\": \"2022-12-25T13:30:00Z\",\n            \"refreshToken\": \"newrefreshtoken\",\n        },\n        \"expectedToken\": {\n            \"token\": \"newtoken\",\n            \"expiration\": \"2021-12-25T21:30:00Z\",\n        },\n    },\n    {\n        \"documentation\": \"Expired token refresh without new refresh token\",\n        \"currentTime\": \"2021-12-25T13:30:00Z\",\n        \"cachedToken\": {\n            \"startUrl\": \"https://d-123.awsapps.com/start\",\n            \"region\": \"us-west-2\",\n            \"accessToken\": \"cachedtoken\",\n            \"expiresAt\": \"2021-12-25T13:00:00Z\",\n            \"clientId\": \"clientid\",\n            \"clientSecret\": \"YSBzZWNyZXQ=\",\n            \"registrationExpiresAt\": \"2022-12-25T13:30:00Z\",\n            \"refreshToken\": \"cachedrefreshtoken\",\n        },\n        \"refreshResponse\": {\n            \"tokenType\": \"Bearer\",\n            \"accessToken\": \"newtoken\",\n            \"expiresIn\": 28800,\n        },\n        \"expectedTokenWriteback\": {\n            \"startUrl\": \"https://d-123.awsapps.com/start\",\n            \"region\": \"us-west-2\",\n            \"accessToken\": \"newtoken\",\n            \"expiresAt\": \"2021-12-25T21:30:00Z\",\n            \"clientId\": \"clientid\",\n            \"clientSecret\": \"YSBzZWNyZXQ=\",\n            \"registrationExpiresAt\": \"2022-12-25T13:30:00Z\",\n        },\n        \"expectedToken\": {\n            \"token\": \"newtoken\",\n            \"expiration\": \"2021-12-25T21:30:00Z\",\n        },\n    },\n    {\n        \"documentation\": \"Expired token and expired client registration\",\n        \"currentTime\": \"2021-12-25T13:30:00Z\",\n        \"cachedToken\": {\n            \"startUrl\": \"https://d-123.awsapps.com/start\",\n            \"region\": \"us-west-2\",\n            \"accessToken\": \"cachedtoken\",\n            \"expiresAt\": \"2021-10-25T13:00:00Z\",\n            \"clientId\": \"clientid\",\n            \"clientSecret\": \"YSBzZWNyZXQ=\",\n            \"registrationExpiresAt\": \"2021-11-25T13:30:00Z\",\n            \"refreshToken\": \"cachedrefreshtoken\",\n        },\n        \"expectedException\": TokenRetrievalError,\n    },\n]\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\n@parametrize(sso_provider_refresh_cases)\nasync def test_sso_token_provider_refresh(test_case):\n    config = {\n        \"profiles\": {\"test\": {\"sso_session\": \"admin\"}},\n        \"sso_sessions\": {\n            \"admin\": {\n                \"sso_region\": \"us-west-2\",\n                \"sso_start_url\": \"https://d-123.awsapps.com/start\",\n            }\n        },\n    }\n    cache_key = \"d033e22ae348aeb5660fc2140aec35850c4da997\"\n    token_cache = {}\n\n    # Prepopulate the token cache\n    cached_token = test_case.pop(\"cachedToken\", None)\n    if cached_token:\n        token_cache[cache_key] = cached_token\n\n    mock_session = _create_mock_session(config)\n    mock_sso_oidc = mock.AsyncMock()\n    mock_sso_oidc.__aenter__.return_value = mock_sso_oidc\n    mock_sso_oidc.__aexit__.return_value = None\n    mock_session.create_client.return_value = mock_sso_oidc\n\n    refresh_response = test_case.pop(\"refreshResponse\", None)\n    mock_sso_oidc.create_token = mock.AsyncMock(return_value=refresh_response)\n\n    current_time = dateutil.parser.parse(test_case.pop(\"currentTime\"))\n\n    def _time_fetcher():\n        return current_time\n\n    resolver = AioSSOTokenProvider(\n        mock_session,\n        token_cache,\n        time_fetcher=_time_fetcher,\n    )\n\n    auth_token = resolver.load_token()\n\n    actual_exception = None\n    try:\n        actual_token = await auth_token.get_frozen_token()\n    except Exception as e:\n        actual_exception = e\n\n    expected_exception = test_case.pop(\"expectedException\", None)\n    if expected_exception is not None:\n        assert isinstance(actual_exception, expected_exception)\n    elif actual_exception is not None:\n        raise actual_exception\n\n    expected_token = test_case.pop(\"expectedToken\", {})\n    raw_token = expected_token.get(\"token\")\n    if raw_token is not None:\n        assert actual_token.token == raw_token\n\n    raw_expiration = expected_token.get(\"expiration\")\n    if raw_expiration is not None:\n        expected_expiration = dateutil.parser.parse(raw_expiration)\n        assert actual_token.expiration == expected_expiration\n\n    expected_token_write_back = test_case.pop(\"expectedTokenWriteback\", None)\n    if expected_token_write_back:\n        mock_sso_oidc.create_token.assert_called_with(\n            grantType=\"refresh_token\",\n            clientId=cached_token[\"clientId\"],\n            clientSecret=cached_token[\"clientSecret\"],\n            refreshToken=cached_token[\"refreshToken\"],\n        )\n        raw_expiration = expected_token_write_back[\"expiresAt\"]\n        # The in-memory cache doesn't serialize to JSON so expect a datetime\n        expected_expiration = dateutil.parser.parse(raw_expiration)\n        expected_token_write_back[\"expiresAt\"] = expected_expiration\n        assert expected_token_write_back == token_cache[cache_key]\n\n    # Pop the documentation to ensure all test fields are handled\n    test_case.pop(\"documentation\")\n    assert not test_case.keys(), \"All fields of test case should be handled\"\n", "tests/python3.8/boto_tests/test_utils.py": "import json\nimport unittest\nfrom unittest import mock\n\nimport pytest\nfrom botocore.exceptions import (\n    ConnectionClosedError,\n    ConnectTimeoutError,\n    ReadTimeoutError,\n)\nfrom botocore.utils import MetadataRetrievalError\n\nfrom aiobotocore import utils\nfrom aiobotocore.awsrequest import AioAWSResponse\nfrom aiobotocore.utils import AioInstanceMetadataFetcher\nfrom tests.boto_tests.test_utils import fake_aiohttp_session\nfrom tests.test_response import AsyncBytesIO\n\n\nclass TestInstanceMetadataFetcher(unittest.IsolatedAsyncioTestCase):\n    async def asyncSetUp(self):\n        urllib3_session_send = 'aiobotocore.httpsession.AIOHTTPSession.send'\n        self._urllib3_patch = mock.patch(urllib3_session_send)\n        self._send = self._urllib3_patch.start()\n        self._imds_responses = []\n        self._send.side_effect = self.get_imds_response\n        self._role_name = 'role-name'\n        self._creds = {\n            'AccessKeyId': 'spam',\n            'SecretAccessKey': 'eggs',\n            'Token': 'spam-token',\n            'Expiration': 'something',\n        }\n        self._expected_creds = {\n            'access_key': self._creds['AccessKeyId'],\n            'secret_key': self._creds['SecretAccessKey'],\n            'token': self._creds['Token'],\n            'expiry_time': self._creds['Expiration'],\n            'role_name': self._role_name,\n        }\n\n    async def asyncTearDown(self):\n        self._urllib3_patch.stop()\n\n    def add_imds_response(self, body, status_code=200):\n        response = AioAWSResponse(\n            url='http://169.254.169.254/',\n            status_code=status_code,\n            headers={},\n            raw=AsyncBytesIO(body),\n        )\n\n        self._imds_responses.append(response)\n\n    def add_get_role_name_imds_response(self, role_name=None):\n        if role_name is None:\n            role_name = self._role_name\n        self.add_imds_response(body=role_name.encode('utf-8'))\n\n    def add_get_credentials_imds_response(self, creds=None):\n        if creds is None:\n            creds = self._creds\n        self.add_imds_response(body=json.dumps(creds).encode('utf-8'))\n\n    def add_get_token_imds_response(self, token, status_code=200):\n        self.add_imds_response(\n            body=token.encode('utf-8'), status_code=status_code\n        )\n\n    def add_metadata_token_not_supported_response(self):\n        self.add_imds_response(b'', status_code=404)\n\n    def add_imds_connection_error(self, exception):\n        self._imds_responses.append(exception)\n\n    def get_imds_response(self, *args, **kwargs):\n        response = self._imds_responses.pop(0)\n        if isinstance(response, Exception):\n            raise response\n        return response\n\n    @pytest.mark.moto\n    @pytest.mark.asyncio\n    async def test_disabled_by_environment(self):\n        env = {'AWS_EC2_METADATA_DISABLED': 'true'}\n        fetcher = AioInstanceMetadataFetcher(env=env)\n        result = await fetcher.retrieve_iam_role_credentials()\n        self.assertEqual(result, {})\n        self._send.assert_not_called()\n\n    @pytest.mark.moto\n    @pytest.mark.asyncio\n    async def test_disabled_by_environment_mixed_case(self):\n        env = {'AWS_EC2_METADATA_DISABLED': 'tRuE'}\n        fetcher = AioInstanceMetadataFetcher(env=env)\n        result = await fetcher.retrieve_iam_role_credentials()\n        self.assertEqual(result, {})\n        self._send.assert_not_called()\n\n    @pytest.mark.moto\n    @pytest.mark.asyncio\n    async def test_disabling_env_var_not_true(self):\n        url = 'https://example.com/'\n        env = {'AWS_EC2_METADATA_DISABLED': 'false'}\n\n        self.add_get_token_imds_response(token='token')\n        self.add_get_role_name_imds_response()\n        self.add_get_credentials_imds_response()\n\n        fetcher = AioInstanceMetadataFetcher(base_url=url, env=env)\n        result = await fetcher.retrieve_iam_role_credentials()\n\n        self.assertEqual(result, self._expected_creds)\n\n    @pytest.mark.moto\n    @pytest.mark.asyncio\n    async def test_includes_user_agent_header(self):\n        user_agent = 'my-user-agent'\n        self.add_get_token_imds_response(token='token')\n        self.add_get_role_name_imds_response()\n        self.add_get_credentials_imds_response()\n\n        await AioInstanceMetadataFetcher(\n            user_agent=user_agent\n        ).retrieve_iam_role_credentials()\n\n        self.assertEqual(self._send.call_count, 3)\n        for call in self._send.calls:\n            self.assertTrue(call[0][0].headers['User-Agent'], user_agent)\n\n    @pytest.mark.moto\n    @pytest.mark.asyncio\n    async def test_non_200_response_for_role_name_is_retried(self):\n        # Response for role name that have a non 200 status code should\n        # be retried.\n        self.add_get_token_imds_response(token='token')\n        self.add_imds_response(\n            status_code=429, body=b'{\"message\": \"Slow down\"}'\n        )\n        self.add_get_role_name_imds_response()\n        self.add_get_credentials_imds_response()\n        result = await AioInstanceMetadataFetcher(\n            num_attempts=2\n        ).retrieve_iam_role_credentials()\n        self.assertEqual(result, self._expected_creds)\n\n    @pytest.mark.moto\n    @pytest.mark.asyncio\n    async def test_http_connection_error_for_role_name_is_retried(self):\n        # Connection related errors should be retried\n        self.add_get_token_imds_response(token='token')\n        self.add_imds_connection_error(ConnectionClosedError(endpoint_url=''))\n        self.add_get_role_name_imds_response()\n        self.add_get_credentials_imds_response()\n        result = await AioInstanceMetadataFetcher(\n            num_attempts=2\n        ).retrieve_iam_role_credentials()\n        self.assertEqual(result, self._expected_creds)\n\n    @pytest.mark.moto\n    @pytest.mark.asyncio\n    async def test_empty_response_for_role_name_is_retried(self):\n        # Response for role name that have a non 200 status code should\n        # be retried.\n        self.add_get_token_imds_response(token='token')\n        self.add_imds_response(body=b'')\n        self.add_get_role_name_imds_response()\n        self.add_get_credentials_imds_response()\n        result = await AioInstanceMetadataFetcher(\n            num_attempts=2\n        ).retrieve_iam_role_credentials()\n        self.assertEqual(result, self._expected_creds)\n\n    @pytest.mark.moto\n    @pytest.mark.asyncio\n    async def test_non_200_response_is_retried(self):\n        self.add_get_token_imds_response(token='token')\n        self.add_get_role_name_imds_response()\n        # Response for creds that has a 200 status code but has an empty\n        # body should be retried.\n        self.add_imds_response(\n            status_code=429, body=b'{\"message\": \"Slow down\"}'\n        )\n        self.add_get_credentials_imds_response()\n        result = await AioInstanceMetadataFetcher(\n            num_attempts=2\n        ).retrieve_iam_role_credentials()\n        self.assertEqual(result, self._expected_creds)\n\n    @pytest.mark.moto\n    @pytest.mark.asyncio\n    async def test_http_connection_errors_is_retried(self):\n        self.add_get_token_imds_response(token='token')\n        self.add_get_role_name_imds_response()\n        # Connection related errors should be retried\n        self.add_imds_connection_error(ConnectionClosedError(endpoint_url=''))\n        self.add_get_credentials_imds_response()\n        result = await AioInstanceMetadataFetcher(\n            num_attempts=2\n        ).retrieve_iam_role_credentials()\n        self.assertEqual(result, self._expected_creds)\n\n    @pytest.mark.moto\n    @pytest.mark.asyncio\n    async def test_empty_response_is_retried(self):\n        self.add_get_token_imds_response(token='token')\n        self.add_get_role_name_imds_response()\n        # Response for creds that has a 200 status code but is empty.\n        # This should be retried.\n        self.add_imds_response(body=b'')\n        self.add_get_credentials_imds_response()\n        result = await AioInstanceMetadataFetcher(\n            num_attempts=2\n        ).retrieve_iam_role_credentials()\n        self.assertEqual(result, self._expected_creds)\n\n    @pytest.mark.moto\n    @pytest.mark.asyncio\n    async def test_invalid_json_is_retried(self):\n        self.add_get_token_imds_response(token='token')\n        self.add_get_role_name_imds_response()\n        # Response for creds that has a 200 status code but is invalid JSON.\n        # This should be retried.\n        self.add_imds_response(body=b'{\"AccessKey\":')\n        self.add_get_credentials_imds_response()\n        result = await AioInstanceMetadataFetcher(\n            num_attempts=2\n        ).retrieve_iam_role_credentials()\n        self.assertEqual(result, self._expected_creds)\n\n    @pytest.mark.moto\n    @pytest.mark.asyncio\n    async def test_exhaust_retries_on_role_name_request(self):\n        self.add_get_token_imds_response(token='token')\n        self.add_imds_response(status_code=400, body=b'')\n        result = await AioInstanceMetadataFetcher(\n            num_attempts=1\n        ).retrieve_iam_role_credentials()\n        self.assertEqual(result, {})\n\n    @pytest.mark.moto\n    @pytest.mark.asyncio\n    async def test_exhaust_retries_on_credentials_request(self):\n        self.add_get_token_imds_response(token='token')\n        self.add_get_role_name_imds_response()\n        self.add_imds_response(status_code=400, body=b'')\n        result = await AioInstanceMetadataFetcher(\n            num_attempts=1\n        ).retrieve_iam_role_credentials()\n        self.assertEqual(result, {})\n\n    @pytest.mark.moto\n    @pytest.mark.asyncio\n    async def test_missing_fields_in_credentials_response(self):\n        self.add_get_token_imds_response(token='token')\n        self.add_get_role_name_imds_response()\n        # Response for creds that has a 200 status code and a JSON body\n        # representing an error. We do not necessarily want to retry this.\n        self.add_imds_response(\n            body=b'{\"Code\":\"AssumeRoleUnauthorizedAccess\",\"Message\":\"error\"}'\n        )\n        result = (\n            await AioInstanceMetadataFetcher().retrieve_iam_role_credentials()\n        )\n        self.assertEqual(result, {})\n\n    @pytest.mark.moto\n    @pytest.mark.asyncio\n    async def test_token_is_included(self):\n        user_agent = 'my-user-agent'\n        self.add_get_token_imds_response(token='token')\n        self.add_get_role_name_imds_response()\n        self.add_get_credentials_imds_response()\n\n        result = await AioInstanceMetadataFetcher(\n            user_agent=user_agent\n        ).retrieve_iam_role_credentials()\n\n        # Check that subsequent calls after getting the token include the token.\n        self.assertEqual(self._send.call_count, 3)\n        for call in self._send.call_args_list[1:]:\n            self.assertEqual(\n                call[0][0].headers['x-aws-ec2-metadata-token'], 'token'\n            )\n        self.assertEqual(result, self._expected_creds)\n\n    @pytest.mark.moto\n    @pytest.mark.asyncio\n    async def test_metadata_token_not_supported_404(self):\n        user_agent = 'my-user-agent'\n        self.add_imds_response(b'', status_code=404)\n        self.add_get_role_name_imds_response()\n        self.add_get_credentials_imds_response()\n\n        result = await AioInstanceMetadataFetcher(\n            user_agent=user_agent\n        ).retrieve_iam_role_credentials()\n\n        for call in self._send.call_args_list[1:]:\n            self.assertNotIn('x-aws-ec2-metadata-token', call[0][0].headers)\n        self.assertEqual(result, self._expected_creds)\n\n    @pytest.mark.moto\n    @pytest.mark.asyncio\n    async def test_metadata_token_not_supported_403(self):\n        user_agent = 'my-user-agent'\n        self.add_imds_response(b'', status_code=403)\n        self.add_get_role_name_imds_response()\n        self.add_get_credentials_imds_response()\n\n        result = await AioInstanceMetadataFetcher(\n            user_agent=user_agent\n        ).retrieve_iam_role_credentials()\n\n        for call in self._send.call_args_list[1:]:\n            self.assertNotIn('x-aws-ec2-metadata-token', call[0][0].headers)\n        self.assertEqual(result, self._expected_creds)\n\n    @pytest.mark.moto\n    @pytest.mark.asyncio\n    async def test_metadata_token_not_supported_405(self):\n        user_agent = 'my-user-agent'\n        self.add_imds_response(b'', status_code=405)\n        self.add_get_role_name_imds_response()\n        self.add_get_credentials_imds_response()\n\n        result = await AioInstanceMetadataFetcher(\n            user_agent=user_agent\n        ).retrieve_iam_role_credentials()\n\n        for call in self._send.call_args_list[1:]:\n            self.assertNotIn('x-aws-ec2-metadata-token', call[0][0].headers)\n        self.assertEqual(result, self._expected_creds)\n\n    @pytest.mark.moto\n    @pytest.mark.asyncio\n    async def test_metadata_token_not_supported_timeout(self):\n        user_agent = 'my-user-agent'\n        self.add_imds_connection_error(ReadTimeoutError(endpoint_url='url'))\n        self.add_get_role_name_imds_response()\n        self.add_get_credentials_imds_response()\n\n        result = await AioInstanceMetadataFetcher(\n            user_agent=user_agent\n        ).retrieve_iam_role_credentials()\n\n        for call in self._send.call_args_list[1:]:\n            self.assertNotIn('x-aws-ec2-metadata-token', call[0][0].headers)\n        self.assertEqual(result, self._expected_creds)\n\n    @pytest.mark.moto\n    @pytest.mark.asyncio\n    async def test_token_not_supported_exhaust_retries(self):\n        user_agent = 'my-user-agent'\n        self.add_imds_connection_error(ConnectTimeoutError(endpoint_url='url'))\n        self.add_get_role_name_imds_response()\n        self.add_get_credentials_imds_response()\n\n        result = await AioInstanceMetadataFetcher(\n            user_agent=user_agent\n        ).retrieve_iam_role_credentials()\n\n        for call in self._send.call_args_list[1:]:\n            self.assertNotIn('x-aws-ec2-metadata-token', call[0][0].headers)\n        self.assertEqual(result, self._expected_creds)\n\n    @pytest.mark.moto\n    @pytest.mark.asyncio\n    async def test_metadata_token_bad_request_yields_no_credentials(self):\n        user_agent = 'my-user-agent'\n        self.add_imds_response(b'', status_code=400)\n        result = await AioInstanceMetadataFetcher(\n            user_agent=user_agent\n        ).retrieve_iam_role_credentials()\n        self.assertEqual(result, {})\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_containermetadatafetcher_retrieve_url():\n    json_body = json.dumps(\n        {\n            \"AccessKeyId\": \"a\",\n            \"SecretAccessKey\": \"b\",\n            \"Token\": \"c\",\n            \"Expiration\": \"d\",\n        }\n    )\n\n    sleep = mock.AsyncMock()\n    http = fake_aiohttp_session((json_body, 200))\n\n    fetcher = utils.AioContainerMetadataFetcher(http, sleep)\n    resp = await fetcher.retrieve_uri('/foo?id=1')\n    assert resp['AccessKeyId'] == 'a'\n    assert resp['SecretAccessKey'] == 'b'\n    assert resp['Token'] == 'c'\n    assert resp['Expiration'] == 'd'\n\n    resp = await fetcher.retrieve_full_uri(\n        'http://localhost/foo?id=1', {'extra': 'header'}\n    )\n    assert resp['AccessKeyId'] == 'a'\n    assert resp['SecretAccessKey'] == 'b'\n    assert resp['Token'] == 'c'\n    assert resp['Expiration'] == 'd'\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_containermetadatafetcher_retrieve_url_bad_status():\n    json_body = \"not json\"\n\n    sleep = mock.AsyncMock()\n    http = fake_aiohttp_session((json_body, 500))\n\n    fetcher = utils.AioContainerMetadataFetcher(http, sleep)\n    with pytest.raises(MetadataRetrievalError):\n        await fetcher.retrieve_uri('/foo?id=1')\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_containermetadatafetcher_retrieve_url_not_json():\n    json_body = \"not json\"\n\n    sleep = mock.AsyncMock()\n    http = fake_aiohttp_session((json_body, 200))\n\n    fetcher = utils.AioContainerMetadataFetcher(http, sleep)\n    with pytest.raises(MetadataRetrievalError):\n        await fetcher.retrieve_uri('/foo?id=1')\n", "tests/boto_tests/test_signers.py": "import datetime\nfrom datetime import timezone\nfrom unittest import mock\n\nimport pytest\n\nimport aiobotocore.credentials\nimport aiobotocore.session\nimport aiobotocore.signers\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_signers_generate_db_auth_token(rds_client):\n    hostname = 'prod-instance.us-east-1.rds.amazonaws.com'\n    port = 3306\n    username = 'someusername'\n    clock = datetime.datetime(2016, 11, 7, 17, 39, 33, tzinfo=timezone.utc)\n\n    with mock.patch('datetime.datetime') as dt:\n        dt.utcnow.return_value = clock\n        result = await aiobotocore.signers.generate_db_auth_token(\n            rds_client, hostname, port, username\n        )\n\n        result2 = await rds_client.generate_db_auth_token(\n            hostname, port, username\n        )\n\n    # A scheme needs to be appended to the beginning or urlsplit may fail\n    # on certain systems.\n    assert result.startswith(\n        'prod-instance.us-east-1.rds.amazonaws.com:3306/?AWSAccessKeyId=xxx&'\n    )\n    assert result2.startswith(\n        'prod-instance.us-east-1.rds.amazonaws.com:3306/?AWSAccessKeyId=xxx&'\n    )\n", "tests/boto_tests/test_credentials.py": "\"\"\"\nThese tests have been taken from\nhttps://github.com/boto/botocore/blob/develop/tests/unit/test_credentials.py\nand adapted to work with asyncio and pytest\n\"\"\"\nimport binascii\nimport os\nimport sys\nimport tempfile\nimport uuid\nfrom datetime import datetime, timedelta\nfrom functools import partial\nfrom typing import Optional\nfrom unittest import TestCase, mock\n\nimport botocore.exceptions\nimport pytest\nimport wrapt\nfrom botocore.configprovider import ConfigValueStore\nfrom botocore.credentials import (\n    Credentials,\n    JSONFileCache,\n    ReadOnlyCredentials,\n)\nfrom botocore.stub import Stubber\nfrom botocore.utils import (\n    FileWebIdentityTokenLoader,\n    SSOTokenLoader,\n    datetime2timestamp,\n)\nfrom dateutil.tz import tzlocal, tzutc\n\nfrom aiobotocore import credentials\nfrom aiobotocore._helpers import asynccontextmanager\nfrom aiobotocore.credentials import (\n    AioAssumeRoleProvider,\n    AioCanonicalNameCredentialSourcer,\n    AioContainerProvider,\n    AioEnvProvider,\n    AioInstanceMetadataProvider,\n    AioProfileProviderBuilder,\n    AioSSOCredentialFetcher,\n    AioSSOProvider,\n)\nfrom aiobotocore.session import AioSession\n\nfrom .helpers import StubbedSession\n\n\ndef random_chars(num_chars):\n    return binascii.hexlify(os.urandom(int(num_chars / 2))).decode('ascii')\n\n\n# From class TestCredentials(BaseEnvVar):\n@pytest.mark.moto\n@pytest.mark.parametrize(\n    \"access,secret\", [('foo\\xe2\\x80\\x99', 'bar\\xe2\\x80\\x99'), ('foo', 'bar')]\n)\ndef test_credentials_normalization(access, secret):\n    c = credentials.AioCredentials(access, secret)\n    assert isinstance(c.access_key, str)\n    assert isinstance(c.secret_key, str)\n\n\n# From class TestAssumeRoleCredentialFetcher(BaseEnvVar):\ndef assume_role_client_creator(with_response):\n    class _Client:\n        def __init__(self, resp):\n            self._resp = resp\n\n            self._called = []\n            self._call_count = 0\n\n        async def assume_role(self, *args, **kwargs):\n            self._call_count += 1\n            self._called.append((args, kwargs))\n\n            if isinstance(self._resp, list):\n                return self._resp.pop(0)\n            return self._resp\n\n        async def __aenter__(self):\n            return self\n\n        async def __aexit__(self, exc_type, exc_val, exc_tb):\n            pass\n\n    return mock.Mock(return_value=_Client(with_response))\n\n\ndef some_future_time():\n    timeobj = datetime.now(tzlocal())\n    return timeobj + timedelta(hours=24)\n\n\ndef get_expected_creds_from_response(response):\n    expiration = response['Credentials']['Expiration']\n    if isinstance(expiration, datetime):\n        expiration = expiration.isoformat()\n    return {\n        'access_key': response['Credentials']['AccessKeyId'],\n        'secret_key': response['Credentials']['SecretAccessKey'],\n        'token': response['Credentials']['SessionToken'],\n        'expiry_time': expiration,\n    }\n\n\n# From class CredentialResolverTest(BaseEnvVar):\n@pytest.fixture\ndef credential_provider():\n    def _f(method, canonical_name, creds='None'):\n        # 'None' so that we can differentiate from None\n        provider = mock.Mock()\n        provider.METHOD = method\n        provider.CANONICAL_NAME = canonical_name\n\n        async def load():\n            if creds != 'None':\n                return creds\n\n            return mock.Mock()\n\n        provider.load = load\n        return provider\n\n    return _f\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_assumerolefetcher_no_cache():\n    response = {\n        'Credentials': {\n            'AccessKeyId': 'foo',\n            'SecretAccessKey': 'bar',\n            'SessionToken': 'baz',\n            'Expiration': some_future_time().isoformat(),\n        },\n    }\n    refresher = credentials.AioAssumeRoleCredentialFetcher(\n        assume_role_client_creator(response),\n        credentials.AioCredentials('a', 'b', 'c'),\n        'myrole',\n    )\n\n    expected_response = get_expected_creds_from_response(response)\n    response = await refresher.fetch_credentials()\n\n    assert response == expected_response\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_assumerolefetcher_cache_key_with_role_session_name():\n    response = {\n        'Credentials': {\n            'AccessKeyId': 'foo',\n            'SecretAccessKey': 'bar',\n            'SessionToken': 'baz',\n            'Expiration': some_future_time().isoformat(),\n        },\n    }\n    cache = {}\n    client_creator = assume_role_client_creator(response)\n    role_session_name = 'my_session_name'\n\n    refresher = credentials.AioAssumeRoleCredentialFetcher(\n        client_creator,\n        credentials.AioCredentials('a', 'b', 'c'),\n        'myrole',\n        cache=cache,\n        extra_args={'RoleSessionName': role_session_name},\n    )\n    await refresher.fetch_credentials()\n\n    # This is the sha256 hex digest of the expected assume role args.\n    cache_key = '2964201f5648c8be5b9460a9cf842d73a266daf2'\n    assert cache_key in cache\n    assert cache[cache_key] == response\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_assumerolefetcher_cache_in_cache_but_expired():\n    response = {\n        'Credentials': {\n            'AccessKeyId': 'foo',\n            'SecretAccessKey': 'bar',\n            'SessionToken': 'baz',\n            'Expiration': some_future_time().isoformat(),\n        },\n    }\n    client_creator = assume_role_client_creator(response)\n    cache = {\n        'development--myrole': {\n            'Credentials': {\n                'AccessKeyId': 'foo-cached',\n                'SecretAccessKey': 'bar-cached',\n                'SessionToken': 'baz-cached',\n                'Expiration': datetime.now(tzlocal()),\n            }\n        }\n    }\n\n    refresher = credentials.AioAssumeRoleCredentialFetcher(\n        client_creator,\n        credentials.AioCredentials('a', 'b', 'c'),\n        'myrole',\n        cache=cache,\n    )\n    expected = get_expected_creds_from_response(response)\n    response = await refresher.fetch_credentials()\n\n    assert response == expected\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_assumerolefetcher_mfa():\n    response = {\n        'Credentials': {\n            'AccessKeyId': 'foo',\n            'SecretAccessKey': 'bar',\n            'SessionToken': 'baz',\n            'Expiration': some_future_time().isoformat(),\n        },\n    }\n    client_creator = assume_role_client_creator(response)\n    prompter = mock.Mock(return_value='token-code')\n    mfa_serial = 'mfa'\n\n    refresher = credentials.AioAssumeRoleCredentialFetcher(\n        client_creator,\n        credentials.AioCredentials('a', 'b', 'c'),\n        'myrole',\n        extra_args={'SerialNumber': mfa_serial},\n        mfa_prompter=prompter,\n    )\n    await refresher.fetch_credentials()\n\n    # Slighly different to the botocore mock\n    client = client_creator.return_value\n    assert client._call_count == 1\n    call_kwargs = client._called[0][1]\n    assert call_kwargs['SerialNumber'] == 'mfa'\n    assert call_kwargs['RoleArn'] == 'myrole'\n    assert call_kwargs['TokenCode'] == 'token-code'\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_recursive_assume_role(assume_role_setup):\n    self = assume_role_setup\n\n    config = (\n        '[profile A]\\n'\n        'role_arn = arn:aws:iam::123456789:role/RoleA\\n'\n        'source_profile = B\\n\\n'\n        '[profile B]\\n'\n        'role_arn = arn:aws:iam::123456789:role/RoleB\\n'\n        'source_profile = C\\n\\n'\n        '[profile C]\\n'\n        'aws_access_key_id = abc123\\n'\n        'aws_secret_access_key = def456\\n'\n    )\n    self.write_config(config)\n\n    profile_b_creds = self.create_random_credentials()\n    profile_b_response = self.create_assume_role_response(profile_b_creds)\n    profile_a_creds = self.create_random_credentials()\n    profile_a_response = self.create_assume_role_response(profile_a_creds)\n\n    async with self.create_session(profile='A') as (session, stubber):\n        stubber.add_response('assume_role', profile_b_response)\n        stubber.add_response('assume_role', profile_a_response)\n\n        actual_creds = await session.get_credentials()\n        await self.assert_creds_equal(actual_creds, profile_a_creds)\n        stubber.assert_no_pending_responses()\n\n\n# From class TestAssumeRoleWithWebIdentityCredentialFetcher(BaseEnvVar):\ndef assume_role_web_identity_client_creator(with_response):\n    class _Client:\n        def __init__(self, resp):\n            self._resp = resp\n\n            self._called = []\n            self._call_count = 0\n\n        async def assume_role_with_web_identity(self, *args, **kwargs):\n            self._call_count += 1\n            self._called.append((args, kwargs))\n\n            if isinstance(self._resp, list):\n                return self._resp.pop(0)\n            return self._resp\n\n        async def __aenter__(self):\n            return self\n\n        async def __aexit__(self, exc_type, exc_val, exc_tb):\n            pass\n\n    return mock.Mock(return_value=_Client(with_response))\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_webidentfetcher_no_cache():\n    response = {\n        'Credentials': {\n            'AccessKeyId': 'foo',\n            'SecretAccessKey': 'bar',\n            'SessionToken': 'baz',\n            'Expiration': some_future_time().isoformat(),\n        },\n    }\n    refresher = credentials.AioAssumeRoleWithWebIdentityCredentialFetcher(\n        assume_role_web_identity_client_creator(response),\n        lambda: 'totally.a.token',\n        'myrole',\n    )\n\n    expected_response = get_expected_creds_from_response(response)\n    response = await refresher.fetch_credentials()\n\n    assert response == expected_response\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_credresolver_load_credentials_single_provider(\n    credential_provider,\n):\n    provider1 = credential_provider(\n        'provider1',\n        'CustomProvider1',\n        credentials.AioCredentials('a', 'b', 'c'),\n    )\n    resolver = credentials.AioCredentialResolver(providers=[provider1])\n\n    creds = await resolver.load_credentials()\n    assert creds.access_key == 'a'\n    assert creds.secret_key == 'b'\n    assert creds.token == 'c'\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_credresolver_no_providers(credential_provider):\n    provider1 = credential_provider('provider1', 'CustomProvider1', None)\n    resolver = credentials.AioCredentialResolver(providers=[provider1])\n\n    creds = await resolver.load_credentials()\n    assert creds is None\n\n\n# From class TestCanonicalNameSourceProvider(BaseEnvVar):\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_canonicalsourceprovider_source_creds(credential_provider):\n    creds = credentials.AioCredentials('a', 'b', 'c')\n    provider1 = credential_provider('provider1', 'CustomProvider1', creds)\n    provider2 = credential_provider('provider2', 'CustomProvider2')\n    provider = credentials.AioCanonicalNameCredentialSourcer(\n        providers=[provider1, provider2]\n    )\n\n    result = await provider.source_credentials('CustomProvider1')\n    assert result is creds\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_canonicalsourceprovider_source_creds_case_insensitive(\n    credential_provider,\n):\n    creds = credentials.AioCredentials('a', 'b', 'c')\n    provider1 = credential_provider('provider1', 'CustomProvider1', creds)\n    provider2 = credential_provider('provider2', 'CustomProvider2')\n    provider = credentials.AioCanonicalNameCredentialSourcer(\n        providers=[provider1, provider2]\n    )\n\n    result = await provider.source_credentials('cUsToMpRoViDeR1')\n    assert result is creds\n\n\n# From class TestAssumeRoleCredentialProvider(unittest.TestCase):\n@pytest.fixture\ndef assumerolecredprovider_config_loader():\n    fake_config = {\n        'profiles': {\n            'development': {\n                'role_arn': 'myrole',\n                'source_profile': 'longterm',\n            },\n            'longterm': {\n                'aws_access_key_id': 'akid',\n                'aws_secret_access_key': 'skid',\n            },\n            'non-static': {\n                'role_arn': 'myrole',\n                'credential_source': 'Environment',\n            },\n            'chained': {\n                'role_arn': 'chained-role',\n                'source_profile': 'development',\n            },\n        }\n    }\n\n    def _f(config=None):\n        return lambda: (config or fake_config)\n\n    return _f\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_assumerolecredprovider_assume_role_no_cache(\n    credential_provider, assumerolecredprovider_config_loader\n):\n    creds = credentials.AioCredentials('a', 'b', 'c')\n    provider1 = credential_provider('provider1', 'CustomProvider1', creds)\n    provider2 = credential_provider('provider2', 'CustomProvider2')\n    provider = credentials.AioCanonicalNameCredentialSourcer(\n        providers=[provider1, provider2]\n    )\n\n    result = await provider.source_credentials('cUsToMpRoViDeR1')\n    assert result is creds\n\n    response = {\n        'Credentials': {\n            'AccessKeyId': 'foo',\n            'SecretAccessKey': 'bar',\n            'SessionToken': 'baz',\n            'Expiration': some_future_time().isoformat(),\n        },\n    }\n    client_creator = assume_role_client_creator(response)\n    provider = credentials.AioAssumeRoleProvider(\n        assumerolecredprovider_config_loader(),\n        client_creator,\n        cache={},\n        profile_name='development',\n    )\n\n    creds = await provider.load()\n\n    # So calling .access_key would cause deferred credentials to be loaded,\n    # according to the source, you're supposed to call get_frozen_credentials\n    # so will do that.\n    creds = await creds.get_frozen_credentials()\n    assert creds.access_key == 'foo'\n    assert creds.secret_key == 'bar'\n    assert creds.token == 'baz'\n\n\n# MFA\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_assumerolecredprovider_mfa(\n    credential_provider, assumerolecredprovider_config_loader\n):\n    fake_config = {\n        'profiles': {\n            'development': {\n                'role_arn': 'myrole',\n                'source_profile': 'longterm',\n                'mfa_serial': 'mfa',\n            },\n            'longterm': {\n                'aws_access_key_id': 'akid',\n                'aws_secret_access_key': 'skid',\n            },\n            'non-static': {\n                'role_arn': 'myrole',\n                'credential_source': 'Environment',\n            },\n            'chained': {\n                'role_arn': 'chained-role',\n                'source_profile': 'development',\n            },\n        }\n    }\n\n    response = {\n        'Credentials': {\n            'AccessKeyId': 'foo',\n            'SecretAccessKey': 'bar',\n            'SessionToken': 'baz',\n            'Expiration': some_future_time().isoformat(),\n        },\n    }\n    client_creator = assume_role_client_creator(response)\n    prompter = mock.Mock(return_value='token-code')\n    provider = credentials.AioAssumeRoleProvider(\n        assumerolecredprovider_config_loader(fake_config),\n        client_creator,\n        cache={},\n        profile_name='development',\n        prompter=prompter,\n    )\n\n    creds = await provider.load()\n    # So calling .access_key would cause deferred credentials to be loaded,\n    # according to the source, you're supposed to call get_frozen_credentials\n    # so will do that.\n    await creds.get_frozen_credentials()\n\n    client = client_creator.return_value\n    assert client._call_count == 1\n    call_kwargs = client._called[0][1]\n    assert call_kwargs['SerialNumber'] == 'mfa'\n    assert call_kwargs['RoleArn'] == 'myrole'\n    assert call_kwargs['TokenCode'] == 'token-code'\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_assumerolecredprovider_mfa_cannot_refresh_credentials(\n    credential_provider, assumerolecredprovider_config_loader\n):\n    fake_config = {\n        'profiles': {\n            'development': {\n                'role_arn': 'myrole',\n                'source_profile': 'longterm',\n                'mfa_serial': 'mfa',\n            },\n            'longterm': {\n                'aws_access_key_id': 'akid',\n                'aws_secret_access_key': 'skid',\n            },\n            'non-static': {\n                'role_arn': 'myrole',\n                'credential_source': 'Environment',\n            },\n            'chained': {\n                'role_arn': 'chained-role',\n                'source_profile': 'development',\n            },\n        }\n    }\n\n    expiration_time = some_future_time()\n    response = {\n        'Credentials': {\n            'AccessKeyId': 'foo',\n            'SecretAccessKey': 'bar',\n            'SessionToken': 'baz',\n            'Expiration': expiration_time.isoformat(),\n        },\n    }\n    client_creator = assume_role_client_creator(response)\n    prompter = mock.Mock(return_value='token-code')\n    provider = credentials.AioAssumeRoleProvider(\n        assumerolecredprovider_config_loader(fake_config),\n        client_creator,\n        cache={},\n        profile_name='development',\n        prompter=prompter,\n    )\n\n    local_now = mock.Mock(return_value=datetime.now(tzlocal()))\n    with mock.patch('aiobotocore.credentials._local_now', local_now):\n        creds = await provider.load()\n        await creds.get_frozen_credentials()\n\n        local_now.return_value = expiration_time\n        with pytest.raises(credentials.RefreshWithMFAUnsupportedError):\n            await creds.get_frozen_credentials()\n\n\n# From class TestAssumeRoleWithWebIdentityCredentialProvider\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_assumerolewebidentprovider_no_cache():\n    future = datetime.now(tzlocal()) + timedelta(hours=24)\n\n    response = {\n        'Credentials': {\n            'AccessKeyId': 'foo',\n            'SecretAccessKey': 'bar',\n            'SessionToken': 'baz',\n            'Expiration': future.isoformat(),\n        },\n    }\n\n    # client\n    client_creator = assume_role_web_identity_client_creator(response)\n\n    mock_loader = mock.Mock(spec=FileWebIdentityTokenLoader)\n    mock_loader.return_value = 'totally.a.token'\n    mock_loader_cls = mock.Mock(return_value=mock_loader)\n\n    config = {\n        'profiles': {\n            'some-profile': {\n                'role_arn': 'arn:aws:iam::123:role/role-name',\n                'web_identity_token_file': '/some/path/token.jwt',\n            }\n        }\n    }\n\n    provider = credentials.AioAssumeRoleWithWebIdentityProvider(\n        load_config=lambda: config,\n        client_creator=client_creator,\n        cache={},\n        profile_name='some-profile',\n        token_loader_cls=mock_loader_cls,\n    )\n\n    creds = await provider.load()\n    creds = await creds.get_frozen_credentials()\n    assert creds.access_key == 'foo'\n    assert creds.secret_key == 'bar'\n    assert creds.token == 'baz'\n\n    mock_loader_cls.assert_called_with('/some/path/token.jwt')\n\n\n# From class TestContainerProvider(BaseEnvVar):\ndef full_url(url):\n    return 'http://{}{}'.format(\n        credentials.AioContainerMetadataFetcher.IP_ADDRESS, url\n    )\n\n\n# From class TestEnvVar(BaseEnvVar):\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_envvarprovider_env_var_present():\n    environ = {\n        'AWS_ACCESS_KEY_ID': 'foo',\n        'AWS_SECRET_ACCESS_KEY': 'bar',\n    }\n    provider = credentials.AioEnvProvider(environ)\n    creds = await provider.load()\n    assert isinstance(creds, credentials.AioCredentials)\n\n    assert creds.access_key == 'foo'\n    assert creds.secret_key == 'bar'\n    assert creds.method == 'env'\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_envvarprovider_env_var_absent():\n    environ = {}\n    provider = credentials.AioEnvProvider(environ)\n    creds = await provider.load()\n    assert creds is None\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_envvarprovider_env_var_expiry():\n    expiry_time = datetime.now(tzlocal()) - timedelta(hours=1)\n    environ = {\n        'AWS_ACCESS_KEY_ID': 'foo',\n        'AWS_SECRET_ACCESS_KEY': 'bar',\n        'AWS_CREDENTIAL_EXPIRATION': expiry_time.isoformat(),\n    }\n    provider = credentials.AioEnvProvider(environ)\n    creds = await provider.load()\n    assert isinstance(creds, credentials.AioRefreshableCredentials)\n\n    del environ['AWS_CREDENTIAL_EXPIRATION']\n\n    with pytest.raises(botocore.exceptions.PartialCredentialsError):\n        await creds.get_frozen_credentials()\n\n\n# From class TestConfigFileProvider(BaseEnvVar):\n@pytest.fixture\ndef profile_config():\n    parser = mock.Mock()\n    profile_config = {\n        'aws_access_key_id': 'a',\n        'aws_secret_access_key': 'b',\n        'aws_session_token': 'c',\n        # Non creds related configs can be in a session's # config.\n        'region': 'us-west-2',\n        'output': 'json',\n    }\n    parsed = {'profiles': {'default': profile_config}}\n    parser.return_value = parsed\n    return parser\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_configprovider_file_exists(profile_config):\n    provider = credentials.AioConfigProvider(\n        'cli.cfg', 'default', profile_config\n    )\n    creds = await provider.load()\n    assert isinstance(creds, credentials.AioCredentials)\n\n    assert creds.access_key == 'a'\n    assert creds.secret_key == 'b'\n    assert creds.method == 'config-file'\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_configprovider_file_missing_profile(profile_config):\n    provider = credentials.AioConfigProvider(\n        'cli.cfg', 'NOT-default', profile_config\n    )\n    creds = await provider.load()\n    assert creds is None\n\n\n# From class TestSharedCredentialsProvider(BaseEnvVar):\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_sharedcredentials_file_exists():\n    parser = mock.Mock()\n    parser.return_value = {\n        'default': {\n            'aws_access_key_id': 'foo',\n            'aws_secret_access_key': 'bar',\n        }\n    }\n\n    provider = credentials.AioSharedCredentialProvider(\n        creds_filename='~/.aws/creds',\n        profile_name='default',\n        ini_parser=parser,\n    )\n    creds = await provider.load()\n    assert isinstance(creds, credentials.AioCredentials)\n\n    assert creds.access_key == 'foo'\n    assert creds.secret_key == 'bar'\n    assert creds.method == 'shared-credentials-file'\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_sharedcredentials_file_missing():\n    parser = mock.Mock()\n    parser.side_effect = botocore.exceptions.ConfigNotFound(path='foo')\n\n    provider = credentials.AioSharedCredentialProvider(\n        creds_filename='~/.aws/creds', profile_name='dev', ini_parser=parser\n    )\n    creds = await provider.load()\n    assert creds is None\n\n\n# From class TestBotoProvider(BaseEnvVar):\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_botoprovider_file_exists():\n    parser = mock.Mock()\n    parser.return_value = {\n        'Credentials': {\n            'aws_access_key_id': 'a',\n            'aws_secret_access_key': 'b',\n        }\n    }\n\n    provider = credentials.AioBotoProvider(environ={}, ini_parser=parser)\n    creds = await provider.load()\n    assert isinstance(creds, credentials.AioCredentials)\n\n    assert creds.access_key == 'a'\n    assert creds.secret_key == 'b'\n    assert creds.method == 'boto-config'\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_botoprovider_file_missing():\n    parser = mock.Mock()\n    parser.side_effect = botocore.exceptions.ConfigNotFound(path='foo')\n\n    provider = credentials.AioBotoProvider(environ={}, ini_parser=parser)\n    creds = await provider.load()\n    assert creds is None\n\n\n# From class TestOriginalEC2Provider(BaseEnvVar):\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_originalec2provider_file_exists():\n    envrion = {'AWS_CREDENTIAL_FILE': 'foo.cfg'}\n    parser = mock.Mock()\n    parser.return_value = {\n        'AWSAccessKeyId': 'a',\n        'AWSSecretKey': 'b',\n    }\n\n    provider = credentials.AioOriginalEC2Provider(\n        environ=envrion, parser=parser\n    )\n    creds = await provider.load()\n    assert isinstance(creds, credentials.AioCredentials)\n\n    assert creds.access_key == 'a'\n    assert creds.secret_key == 'b'\n    assert creds.method == 'ec2-credentials-file'\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_originalec2provider_file_missing():\n    provider = credentials.AioOriginalEC2Provider(environ={})\n    creds = await provider.load()\n    assert creds is None\n\n\n# From class TestCreateCredentialResolver\n@pytest.fixture\ndef mock_session():\n    def _f(config_loader: Optional[ConfigValueStore] = None) -> AioSession:\n        if not config_loader:\n            config_loader = ConfigValueStore()\n\n        fake_instance_variables = {\n            'credentials_file': 'a',\n            'legacy_config_file': 'b',\n            'config_file': 'c',\n            'metadata_service_timeout': 1,\n            'metadata_service_num_attempts': 1,\n            'imds_use_ipv6': 'false',\n        }\n\n        def fake_get_component(self, key):\n            if key == 'config_provider':\n                return config_loader\n            return None\n\n        def fake_set_config_variable(self, logical_name, value):\n            fake_instance_variables[logical_name] = value\n\n        session = mock.Mock(spec=AioSession)\n        session.get_component = fake_get_component\n        session.full_config = {}\n\n        for name, value in fake_instance_variables.items():\n            config_loader.set_config_variable(name, value)\n\n        session.get_config_variable = config_loader.get_config_variable\n        session.set_config_variable = fake_set_config_variable\n\n        return session\n\n    return _f\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_createcredentialresolver(mock_session):\n    session = mock_session()\n\n    resolver = credentials.create_credential_resolver(session)\n    assert isinstance(resolver, credentials.AioCredentialResolver)\n\n\n# Disabled on travis as we cant easily disable the tests properly and\n#  travis has an IAM role which can't be applied to the mock session\n# @pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_get_credentials(mock_session):\n    session = mock_session()\n\n    creds = await credentials.get_credentials(session)\n\n    assert creds is None\n\n\nclass Self:\n    pass\n\n\nclass _AsyncCtx:\n    def __init__(self, value):\n        self._value = value\n\n    async def __aenter__(self):\n        return self._value\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        pass\n\n\n# From class TestSSOCredentialFetcher:\n@pytest.fixture\nasync def ssl_credential_fetcher_setup():\n    async with AioSession().create_client(\n        'sso', region_name='us-east-1'\n    ) as sso:\n        self = Self()\n        self.sso = sso\n        self.stubber = Stubber(self.sso)\n        self.mock_session = mock.Mock(spec=AioSession)\n        self.mock_session.create_client.return_value = _AsyncCtx(sso)\n\n        self.cache = {}\n        self.sso_region = 'us-east-1'\n        self.start_url = 'https://d-92671207e4.awsapps.com/start'\n        self.role_name = 'test-role'\n        self.account_id = '1234567890'\n        self.access_token = 'some.sso.token'\n        # This is just an arbitrary point in time we can pin to\n        self.now = datetime(2008, 9, 23, 12, 26, 40, tzinfo=tzutc())\n        # The SSO endpoint uses ms whereas the OIDC endpoint uses seconds\n        self.now_timestamp = 1222172800000\n\n        self.loader = mock.Mock(spec=SSOTokenLoader)\n        self.loader.return_value = {'accessToken': self.access_token}\n        self.fetcher = AioSSOCredentialFetcher(\n            self.start_url,\n            self.sso_region,\n            self.role_name,\n            self.account_id,\n            self.mock_session.create_client,\n            token_loader=self.loader,\n            cache=self.cache,\n        )\n\n        tc = TestCase()\n        self.assertEqual = tc.assertEqual\n        self.assertRaises = tc.assertRaises\n        yield self\n\n\n@pytest.fixture\ndef base_env_var_setup():\n    self = Self()\n    self.environ = {}\n    with mock.patch('os.environ', self.environ):\n        yield self\n\n\ndef _some_future_time():\n    timeobj = datetime.now(tzlocal())\n    return timeobj + timedelta(hours=24)\n\n\ndef _create_assume_role_response(credentials, expiration=None):\n    if expiration is None:\n        expiration = _some_future_time()\n\n    response = {\n        'Credentials': {\n            'AccessKeyId': credentials.access_key,\n            'SecretAccessKey': credentials.secret_key,\n            'SessionToken': credentials.token,\n            'Expiration': expiration,\n        },\n        'AssumedRoleUser': {\n            'AssumedRoleId': 'myroleid',\n            'Arn': 'arn:aws:iam::1234567890:user/myuser',\n        },\n    }\n\n    return response\n\n\ndef _create_random_credentials():\n    return Credentials(\n        'fake-%s' % random_chars(15),\n        'fake-%s' % random_chars(35),\n        'fake-%s' % random_chars(45),\n    )\n\n\nasync def _assert_creds_equal(c1, c2):\n    c1_frozen = c1\n    if not isinstance(c1_frozen, ReadOnlyCredentials):\n        c1_frozen = await c1.get_frozen_credentials()\n    c2_frozen = c2\n    if not isinstance(c2_frozen, ReadOnlyCredentials):\n        c2_frozen = c2.get_frozen_credentials()\n    assert c1_frozen == c2_frozen\n\n\ndef _write_config(self, config):\n    with open(self.config_file, 'w') as f:\n        f.write(config)\n\n\n@pytest.fixture\ndef base_assume_role_test_setup(base_env_var_setup):\n    self = base_env_var_setup\n    with tempfile.TemporaryDirectory() as td_name:\n        self.tempdir = td_name\n        self.config_file = os.path.join(self.tempdir, 'config')\n        self.environ['AWS_CONFIG_FILE'] = self.config_file\n        self.environ['AWS_SHARED_CREDENTIALS_FILE'] = str(uuid.uuid4())\n\n        self.some_future_time = _some_future_time\n        self.create_assume_role_response = _create_assume_role_response\n        self.create_random_credentials = _create_random_credentials\n        self.assert_creds_equal = _assert_creds_equal\n        self.write_config = partial(_write_config, self)\n\n        yield self\n\n\ndef _mock_provider(provider_cls):\n    mock_instance = mock.Mock(spec=provider_cls)\n    mock_instance.load.return_value = None\n    mock_instance.METHOD = provider_cls.METHOD\n    mock_instance.CANONICAL_NAME = provider_cls.CANONICAL_NAME\n    return mock_instance\n\n\nclass DummyContextWrapper(wrapt.ObjectProxy):\n    async def __aenter__(self):\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        pass\n\n\n@asynccontextmanager\nasync def _create_session(self, profile=None):\n    session = StubbedSession(profile=profile)\n\n    # We have to set bogus credentials here or otherwise we'll trigger\n    # an early credential chain resolution.\n    async with session.create_client(\n        'sts',\n        aws_access_key_id='spam',\n        aws_secret_access_key='eggs',\n    ) as sts:\n        self.mock_client_creator.return_value = DummyContextWrapper(sts)\n        assume_role_provider = AioAssumeRoleProvider(\n            load_config=lambda: session.full_config,\n            client_creator=self.mock_client_creator,\n            cache={},\n            profile_name=profile,\n            credential_sourcer=AioCanonicalNameCredentialSourcer(\n                [\n                    self.env_provider,\n                    self.container_provider,\n                    self.metadata_provider,\n                ]\n            ),\n            profile_provider_builder=AioProfileProviderBuilder(\n                session,\n                sso_token_cache=JSONFileCache(self.tempdir),\n            ),\n        )\n        async with session.stub('sts') as stubber:\n            stubber.activate()\n\n            component_name = 'credential_provider'\n            resolver = session.get_component(component_name)\n            available_methods = [p.METHOD for p in resolver.providers]\n            replacements = {\n                'env': self.env_provider,\n                'iam-role': self.metadata_provider,\n                'container-role': self.container_provider,\n                'assume-role': assume_role_provider,\n            }\n            for name, provider in replacements.items():\n                try:\n                    index = available_methods.index(name)\n                except ValueError:\n                    # The provider isn't in the session\n                    continue\n\n                resolver.providers[index] = provider\n\n            session.register_component('credential_provider', resolver)\n            yield session, stubber\n\n\n@pytest.fixture\ndef assume_role_setup(base_assume_role_test_setup):\n    self = base_assume_role_test_setup\n\n    self.environ['AWS_ACCESS_KEY_ID'] = 'access_key'\n    self.environ['AWS_SECRET_ACCESS_KEY'] = 'secret_key'\n\n    self.mock_provider = _mock_provider\n    self.create_session = partial(_create_session, self)\n\n    self.metadata_provider = self.mock_provider(AioInstanceMetadataProvider)\n    self.env_provider = self.mock_provider(AioEnvProvider)\n    self.container_provider = self.mock_provider(AioContainerProvider)\n    self.mock_client_creator = mock.Mock(spec=AioSession.create_client)\n    self.actual_client_region = None\n\n    current_dir = os.path.dirname(os.path.abspath(__file__))\n    credential_process = os.path.join(\n        current_dir, 'utils', 'credentialprocess.py'\n    )\n    self.credential_process = '{} {}'.format(\n        sys.executable, credential_process\n    )\n\n    yield self\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_sso_credential_fetcher_can_fetch_credentials(\n    ssl_credential_fetcher_setup,\n):\n    self = ssl_credential_fetcher_setup\n    expected_params = {\n        'roleName': self.role_name,\n        'accountId': self.account_id,\n        'accessToken': self.access_token,\n    }\n    expected_response = {\n        'roleCredentials': {\n            'accessKeyId': 'foo',\n            'secretAccessKey': 'bar',\n            'sessionToken': 'baz',\n            'expiration': self.now_timestamp + 1000000,\n        }\n    }\n    self.stubber.add_response(\n        'get_role_credentials',\n        expected_response,\n        expected_params=expected_params,\n    )\n    with self.stubber:\n        credentials = await self.fetcher.fetch_credentials()\n    self.assertEqual(credentials['access_key'], 'foo')\n    self.assertEqual(credentials['secret_key'], 'bar')\n    self.assertEqual(credentials['token'], 'baz')\n    self.assertEqual(credentials['expiry_time'], '2008-09-23T12:43:20Z')\n    cache_key = '048db75bbe50955c16af7aba6ff9c41a3131bb7e'\n    expected_cached_credentials = {\n        'ProviderType': 'sso',\n        'Credentials': {\n            'AccessKeyId': 'foo',\n            'SecretAccessKey': 'bar',\n            'SessionToken': 'baz',\n            'Expiration': '2008-09-23T12:43:20Z',\n        },\n    }\n    self.assertEqual(self.cache[cache_key], expected_cached_credentials)\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_sso_cred_fetcher_raises_helpful_message_on_unauthorized_exception(\n    ssl_credential_fetcher_setup,\n):\n    self = ssl_credential_fetcher_setup\n    expected_params = {\n        'roleName': self.role_name,\n        'accountId': self.account_id,\n        'accessToken': self.access_token,\n    }\n    self.stubber.add_client_error(\n        'get_role_credentials',\n        service_error_code='UnauthorizedException',\n        expected_params=expected_params,\n    )\n    with self.assertRaises(botocore.exceptions.UnauthorizedSSOTokenError):\n        with self.stubber:\n            await self.fetcher.fetch_credentials()\n\n\n# from TestSSOProvider\n@pytest.fixture\nasync def sso_provider_setup():\n    self = Self()\n    async with AioSession().create_client(\n        'sso', region_name='us-east-1'\n    ) as sso:\n        self.sso = sso\n        self.stubber = Stubber(self.sso)\n        self.mock_session = mock.Mock(spec=AioSession)\n        self.mock_session.create_client.return_value = _AsyncCtx(sso)\n\n        self.sso_region = 'us-east-1'\n        self.start_url = 'https://d-92671207e4.awsapps.com/start'\n        self.role_name = 'test-role'\n        self.account_id = '1234567890'\n        self.access_token = 'some.sso.token'\n\n        self.profile_name = 'sso-profile'\n        self.config = {\n            'sso_region': self.sso_region,\n            'sso_start_url': self.start_url,\n            'sso_role_name': self.role_name,\n            'sso_account_id': self.account_id,\n        }\n        self.expires_at = datetime.now(tzlocal()) + timedelta(hours=24)\n        self.cached_creds_key = '048db75bbe50955c16af7aba6ff9c41a3131bb7e'\n        self.cached_token_key = '13f9d35043871d073ab260e020f0ffde092cb14b'\n        self.cache = {\n            self.cached_token_key: {\n                'accessToken': self.access_token,\n                'expiresAt': self.expires_at.strftime('%Y-%m-%dT%H:%M:%S%Z'),\n            }\n        }\n        self._mock_load_config = partial(_mock_load_config, self)\n        self._add_get_role_credentials_response = partial(\n            _add_get_role_credentials_response, self\n        )\n        self.provider = AioSSOProvider(\n            load_config=self._mock_load_config,\n            client_creator=self.mock_session.create_client,\n            profile_name=self.profile_name,\n            cache=self.cache,\n            token_cache=self.cache,\n        )\n\n        self.expected_get_role_credentials_params = {\n            'roleName': self.role_name,\n            'accountId': self.account_id,\n            'accessToken': self.access_token,\n        }\n        expiration = datetime2timestamp(self.expires_at)\n        self.expected_get_role_credentials_response = {\n            'roleCredentials': {\n                'accessKeyId': 'foo',\n                'secretAccessKey': 'bar',\n                'sessionToken': 'baz',\n                'expiration': int(expiration * 1000),\n            }\n        }\n\n        tc = TestCase()\n        self.assertEqual = tc.assertEqual\n        self.assertRaises = tc.assertRaises\n\n        yield self\n\n\ndef _mock_load_config(self):\n    return {\n        'profiles': {\n            self.profile_name: self.config,\n        }\n    }\n\n\ndef _add_get_role_credentials_response(self):\n    self.stubber.add_response(\n        'get_role_credentials',\n        self.expected_get_role_credentials_response,\n        self.expected_get_role_credentials_params,\n    )\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_load_sso_credentials_without_cache(sso_provider_setup):\n    self = sso_provider_setup\n    _add_get_role_credentials_response(self)\n    with self.stubber:\n        credentials = await self.provider.load()\n        credentials = await credentials.get_frozen_credentials()\n        self.assertEqual(credentials.access_key, 'foo')\n        self.assertEqual(credentials.secret_key, 'bar')\n        self.assertEqual(credentials.token, 'baz')\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_load_sso_credentials_with_cache(sso_provider_setup):\n    self = sso_provider_setup\n\n    cached_creds = {\n        'Credentials': {\n            'AccessKeyId': 'cached-akid',\n            'SecretAccessKey': 'cached-sak',\n            'SessionToken': 'cached-st',\n            'Expiration': self.expires_at.strftime('%Y-%m-%dT%H:%M:%S%Z'),\n        }\n    }\n    self.cache[self.cached_creds_key] = cached_creds\n    credentials = await self.provider.load()\n    credentials = await credentials.get_frozen_credentials()\n    self.assertEqual(credentials.access_key, 'cached-akid')\n    self.assertEqual(credentials.secret_key, 'cached-sak')\n    self.assertEqual(credentials.token, 'cached-st')\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_load_sso_credentials_with_cache_expired(sso_provider_setup):\n    self = sso_provider_setup\n    cached_creds = {\n        'Credentials': {\n            'AccessKeyId': 'expired-akid',\n            'SecretAccessKey': 'expired-sak',\n            'SessionToken': 'expired-st',\n            'Expiration': '2002-10-22T20:52:11UTC',\n        }\n    }\n    self.cache[self.cached_creds_key] = cached_creds\n\n    self._add_get_role_credentials_response()\n    with self.stubber:\n        credentials = await self.provider.load()\n        credentials = await credentials.get_frozen_credentials()\n\n        self.assertEqual(credentials.access_key, 'foo')\n        self.assertEqual(credentials.secret_key, 'bar')\n        self.assertEqual(credentials.token, 'baz')\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_required_config_not_set(sso_provider_setup):\n    self = sso_provider_setup\n    del self.config['sso_start_url']\n    # If any required configuration is missing we should get an error\n    with self.assertRaises(botocore.exceptions.InvalidConfigError):\n        await self.provider.load()\n", "tests/boto_tests/helpers.py": "from botocore.stub import Stubber\n\nimport aiobotocore.session\nfrom aiobotocore._helpers import asynccontextmanager\nfrom tests._helpers import AsyncExitStack\n\n\nclass StubbedSession(aiobotocore.session.AioSession):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._cached_clients = {}\n        self._client_stubs = {}\n\n    @asynccontextmanager\n    async def create_client(self, service_name, *args, **kwargs):\n        async with AsyncExitStack() as es:\n            es: AsyncExitStack\n            if service_name not in self._cached_clients:\n                client = await es.enter_async_context(\n                    self._create_stubbed_client(service_name, *args, **kwargs)\n                )\n                self._cached_clients[service_name] = client\n            yield self._cached_clients[service_name]\n\n    @asynccontextmanager\n    async def _create_stubbed_client(self, service_name, *args, **kwargs):\n        async with AsyncExitStack() as es:\n            es: AsyncExitStack\n            client = await es.enter_async_context(\n                super().create_client(service_name, *args, **kwargs)\n            )\n            stubber = Stubber(client)\n            self._client_stubs[service_name] = stubber\n            yield client\n\n    @asynccontextmanager\n    async def stub(self, service_name, *args, **kwargs):\n        async with AsyncExitStack() as es:\n            es: AsyncExitStack\n            if service_name not in self._client_stubs:\n                await es.enter_async_context(\n                    self.create_client(service_name, *args, **kwargs)\n                )\n            yield self._client_stubs[service_name]\n\n    def activate_stubs(self):\n        for stub in self._client_stubs.values():\n            stub.activate()\n\n    def verify_stubs(self):\n        for stub in self._client_stubs.values():\n            stub.assert_no_pending_responses()\n", "tests/boto_tests/__init__.py": "", "tests/boto_tests/test_utils.py": "from __future__ import annotations\n\nimport itertools\nimport json\nfrom typing import Iterator, Tuple, Union\n\nimport pytest\nfrom botocore.exceptions import ReadTimeoutError\nfrom botocore.utils import BadIMDSRequestError\n\nfrom aiobotocore import utils\nfrom aiobotocore._helpers import asynccontextmanager\n\n# TypeAlias (requires typing_extensions or >=3.10 to annotate)\nResponse = Tuple[Union[str, object], int]\n\n\n# From class TestContainerMetadataFetcher\ndef fake_aiohttp_session(responses: list[Response] | Response):\n    \"\"\"\n    Dodgy shim class\n    \"\"\"\n    if isinstance(responses, tuple):\n        data: Iterator[Response] = itertools.cycle([responses])\n    else:\n        data = iter(responses)\n\n    class FakeAioHttpSession:\n        @asynccontextmanager\n        async def acquire(self):\n            yield self\n\n        class FakeResponse:\n            def __init__(self, request, *args, **kwargs):\n                self.request = request\n                self.url = request.url\n                self._body, self.status_code = next(data)\n                self.content = self._content()\n                self.text = self._text()\n                if not isinstance(self._body, str):\n                    raise self._body\n\n            async def _content(self):\n                return self._body.encode('utf-8')\n\n            async def __aenter__(self):\n                return self\n\n            async def __aexit__(self, exc_type, exc_val, exc_tb):\n                pass\n\n            async def _text(self):\n                return self._body\n\n            async def json(self):\n                return json.loads(self._body)\n\n        def __init__(self, *args, **kwargs):\n            pass\n\n        async def __aenter__(self):\n            return self\n\n        async def __aexit__(self, exc_type, exc_val, exc_tb):\n            pass\n\n        async def send(self, request):\n            return self.FakeResponse(request)\n\n    return FakeAioHttpSession()\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_idmsfetcher_disabled():\n    env = {'AWS_EC2_METADATA_DISABLED': 'true'}\n    fetcher = utils.AioIMDSFetcher(env=env)\n\n    with pytest.raises(fetcher._RETRIES_EXCEEDED_ERROR_CLS):\n        await fetcher._get_request('path', None)\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_idmsfetcher_get_token_success():\n    session = fake_aiohttp_session(\n        ('blah', 200),\n    )\n\n    fetcher = utils.AioIMDSFetcher(\n        num_attempts=2, session=session, user_agent='test'\n    )\n    response = await fetcher._fetch_metadata_token()\n    assert response == 'blah'\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_idmsfetcher_get_token_not_found():\n    session = fake_aiohttp_session(\n        ('blah', 404),\n    )\n\n    fetcher = utils.AioIMDSFetcher(\n        num_attempts=2, session=session, user_agent='test'\n    )\n    response = await fetcher._fetch_metadata_token()\n    assert response is None\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_idmsfetcher_get_token_bad_request():\n    session = fake_aiohttp_session(\n        ('blah', 400),\n    )\n\n    fetcher = utils.AioIMDSFetcher(\n        num_attempts=2, session=session, user_agent='test'\n    )\n    with pytest.raises(BadIMDSRequestError):\n        await fetcher._fetch_metadata_token()\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_idmsfetcher_get_token_timeout():\n    session = fake_aiohttp_session(\n        [\n            (ReadTimeoutError(endpoint_url='aaa'), 500),\n        ]\n    )\n\n    fetcher = utils.AioIMDSFetcher(num_attempts=2, session=session)\n\n    response = await fetcher._fetch_metadata_token()\n    assert response is None\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_idmsfetcher_get_token_retry():\n    session = fake_aiohttp_session(\n        [\n            ('blah', 500),\n            ('blah', 500),\n            ('token', 200),\n        ]\n    )\n\n    fetcher = utils.AioIMDSFetcher(num_attempts=3, session=session)\n\n    response = await fetcher._fetch_metadata_token()\n    assert response == 'token'\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_idmsfetcher_retry():\n    session = fake_aiohttp_session(\n        [\n            ('blah', 500),\n            ('data', 200),\n        ]\n    )\n\n    fetcher = utils.AioIMDSFetcher(\n        num_attempts=2, session=session, user_agent='test'\n    )\n    response = await fetcher._get_request('path', None, 'some_token')\n\n    assert await response.text == 'data'\n\n    session = fake_aiohttp_session(\n        [\n            ('blah', 500),\n            ('data', 200),\n        ]\n    )\n\n    fetcher = utils.AioIMDSFetcher(num_attempts=1, session=session)\n    with pytest.raises(fetcher._RETRIES_EXCEEDED_ERROR_CLS):\n        await fetcher._get_request('path', None)\n\n\n@pytest.mark.moto\n@pytest.mark.asyncio\nasync def test_idmsfetcher_timeout():\n    session = fake_aiohttp_session(\n        [\n            (ReadTimeoutError(endpoint_url='url'), 500),\n        ]\n    )\n\n    fetcher = utils.AioIMDSFetcher(num_attempts=1, session=session)\n\n    with pytest.raises(fetcher._RETRIES_EXCEEDED_ERROR_CLS):\n        await fetcher._get_request('path', None)\n", "aiobotocore/response.py": "import asyncio\n\nimport aiohttp\nimport aiohttp.client_exceptions\nimport wrapt\nfrom botocore.response import (\n    IncompleteReadError,\n    ReadTimeoutError,\n    ResponseStreamingError,\n)\n\nfrom aiobotocore import parsers\n\n\nclass AioReadTimeoutError(ReadTimeoutError, asyncio.TimeoutError):\n    pass\n\n\nclass StreamingBody(wrapt.ObjectProxy):\n    \"\"\"Wrapper class for an http response body.\n\n    This provides a few additional conveniences that do not exist\n    in the urllib3 model:\n\n        * Auto validation of content length, if the amount of bytes\n          we read does not match the content length, an exception\n          is raised.\n    \"\"\"\n\n    _DEFAULT_CHUNK_SIZE = 1024\n\n    def __init__(self, raw_stream: aiohttp.StreamReader, content_length: str):\n        super().__init__(raw_stream)\n        self._self_content_length = content_length\n        self._self_amount_read = 0\n\n    # https://github.com/GrahamDumpleton/wrapt/issues/73\n    async def __aenter__(self):\n        return await self.__wrapped__.__aenter__()\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        return await self.__wrapped__.__aexit__(exc_type, exc_val, exc_tb)\n\n    # NOTE: set_socket_timeout was only for when requests didn't support\n    #       read timeouts, so not needed\n    def readable(self):\n        return not self.at_eof()\n\n    async def read(self, amt=None):\n        \"\"\"Read at most amt bytes from the stream.\n\n        If the amt argument is omitted, read all data.\n        \"\"\"\n        # botocore to aiohttp mapping\n        try:\n            chunk = await self.__wrapped__.content.read(\n                amt if amt is not None else -1\n            )\n        except asyncio.TimeoutError as e:\n            raise AioReadTimeoutError(\n                endpoint_url=self.__wrapped__.url, error=e\n            )\n        except aiohttp.client_exceptions.ClientConnectionError as e:\n            raise ResponseStreamingError(error=e)\n\n        self._self_amount_read += len(chunk)\n        if amt is None or (not chunk and amt > 0):\n            # If the server sends empty contents or\n            # we ask to read all of the contents, then we know\n            # we need to verify the content length.\n            self._verify_content_length()\n        return chunk\n\n    async def readlines(self):\n        # assuming this is not an iterator\n        lines = [line async for line in self.iter_lines()]\n        return lines\n\n    def __aiter__(self):\n        \"\"\"Return an iterator to yield 1k chunks from the raw stream.\"\"\"\n        return self.iter_chunks(self._DEFAULT_CHUNK_SIZE)\n\n    async def __anext__(self):\n        \"\"\"Return the next 1k chunk from the raw stream.\"\"\"\n        current_chunk = await self.read(self._DEFAULT_CHUNK_SIZE)\n        if current_chunk:\n            return current_chunk\n        raise StopAsyncIteration\n\n    anext = __anext__\n\n    async def iter_lines(self, chunk_size=_DEFAULT_CHUNK_SIZE, keepends=False):\n        \"\"\"Return an iterator to yield lines from the raw stream.\n\n        This is achieved by reading chunk of bytes (of size chunk_size) at a\n        time from the raw stream, and then yielding lines from there.\n        \"\"\"\n        pending = b''\n        async for chunk in self.iter_chunks(chunk_size):\n            lines = (pending + chunk).splitlines(True)\n            for line in lines[:-1]:\n                yield line.splitlines(keepends)[0]\n            pending = lines[-1]\n        if pending:\n            yield pending.splitlines(keepends)[0]\n\n    async def iter_chunks(self, chunk_size=_DEFAULT_CHUNK_SIZE):\n        \"\"\"Return an iterator to yield chunks of chunk_size bytes from the raw\n        stream.\n        \"\"\"\n        while True:\n            current_chunk = await self.read(chunk_size)\n            if current_chunk == b\"\":\n                break\n            yield current_chunk\n\n    def _verify_content_length(self):\n        # See: https://github.com/kennethreitz/requests/issues/1855\n        # Basically, our http library doesn't do this for us, so we have\n        # to do this our self.\n        if (\n            self._self_content_length is not None\n            and self._self_amount_read != int(self._self_content_length)\n        ):\n            raise IncompleteReadError(\n                actual_bytes=self._self_amount_read,\n                expected_bytes=int(self._self_content_length),\n            )\n\n    def tell(self):\n        return self._self_amount_read\n\n\nasync def get_response(operation_model, http_response):\n    protocol = operation_model.metadata['protocol']\n    response_dict = {\n        'headers': http_response.headers,\n        'status_code': http_response.status_code,\n    }\n    # TODO: Unfortunately, we have to have error logic here.\n    # If it looks like an error, in the streaming response case we\n    # need to actually grab the contents.\n    if response_dict['status_code'] >= 300:\n        response_dict['body'] = await http_response.content\n    elif operation_model.has_streaming_output:\n        response_dict['body'] = StreamingBody(\n            http_response.raw, response_dict['headers'].get('content-length')\n        )\n    else:\n        response_dict['body'] = await http_response.content\n\n    parser = parsers.create_parser(protocol)\n    if asyncio.iscoroutinefunction(parser.parse):\n        parsed = await parser.parse(\n            response_dict, operation_model.output_shape\n        )\n    else:\n        parsed = parser.parse(response_dict, operation_model.output_shape)\n    return http_response, parsed\n", "aiobotocore/tokens.py": "import asyncio\nimport logging\nfrom datetime import timedelta\n\nimport dateutil.parser\nfrom botocore.compat import total_seconds\nfrom botocore.exceptions import ClientError, TokenRetrievalError\nfrom botocore.tokens import (\n    DeferredRefreshableToken,\n    FrozenAuthToken,\n    SSOTokenProvider,\n    TokenProviderChain,\n    _utc_now,\n)\n\nlogger = logging.getLogger(__name__)\n\n\ndef create_token_resolver(session):\n    providers = [\n        AioSSOTokenProvider(session),\n    ]\n    return TokenProviderChain(providers=providers)\n\n\nclass AioDeferredRefreshableToken(DeferredRefreshableToken):\n    def __init__(\n        self, method, refresh_using, time_fetcher=_utc_now\n    ):  # noqa: E501, lgtm [py/missing-call-to-init]\n        self._time_fetcher = time_fetcher\n        self._refresh_using = refresh_using\n        self.method = method\n\n        # The frozen token is protected by this lock\n        self._refresh_lock = asyncio.Lock()\n        self._frozen_token = None\n        self._next_refresh = None\n\n    async def get_frozen_token(self):\n        await self._refresh()\n        return self._frozen_token\n\n    async def _refresh(self):\n        # If we don't need to refresh just return\n        refresh_type = self._should_refresh()\n        if not refresh_type:\n            return None\n\n        # Block for refresh if we're in the mandatory refresh window\n        block_for_refresh = refresh_type == \"mandatory\"\n        if block_for_refresh or not self._refresh_lock.locked():\n            async with self._refresh_lock:\n                await self._protected_refresh()\n\n    async def _protected_refresh(self):\n        # This should only be called after acquiring the refresh lock\n        # Another task may have already refreshed, double check refresh\n        refresh_type = self._should_refresh()\n        if not refresh_type:\n            return None\n\n        try:\n            now = self._time_fetcher()\n            self._next_refresh = now + timedelta(seconds=self._attempt_timeout)\n            self._frozen_token = await self._refresh_using()\n        except Exception:\n            logger.warning(\n                \"Refreshing token failed during the %s refresh period.\",\n                refresh_type,\n                exc_info=True,\n            )\n            if refresh_type == \"mandatory\":\n                # This refresh was mandatory, error must be propagated back\n                raise\n\n        if self._is_expired():\n            # Fresh credentials should never be expired\n            raise TokenRetrievalError(\n                provider=self.method,\n                error_msg=\"Token has expired and refresh failed\",\n            )\n\n\nclass AioSSOTokenProvider(SSOTokenProvider):\n    async def _attempt_create_token(self, token):\n        async with self._client as client:\n            response = await client.create_token(\n                grantType=self._GRANT_TYPE,\n                clientId=token[\"clientId\"],\n                clientSecret=token[\"clientSecret\"],\n                refreshToken=token[\"refreshToken\"],\n            )\n        expires_in = timedelta(seconds=response[\"expiresIn\"])\n        new_token = {\n            \"startUrl\": self._sso_config[\"sso_start_url\"],\n            \"region\": self._sso_config[\"sso_region\"],\n            \"accessToken\": response[\"accessToken\"],\n            \"expiresAt\": self._now() + expires_in,\n            # Cache the registration alongside the token\n            \"clientId\": token[\"clientId\"],\n            \"clientSecret\": token[\"clientSecret\"],\n            \"registrationExpiresAt\": token[\"registrationExpiresAt\"],\n        }\n        if \"refreshToken\" in response:\n            new_token[\"refreshToken\"] = response[\"refreshToken\"]\n        logger.info(\"SSO Token refresh succeeded\")\n        return new_token\n\n    async def _refresh_access_token(self, token):\n        keys = (\n            \"refreshToken\",\n            \"clientId\",\n            \"clientSecret\",\n            \"registrationExpiresAt\",\n        )\n        missing_keys = [k for k in keys if k not in token]\n        if missing_keys:\n            msg = f\"Unable to refresh SSO token: missing keys: {missing_keys}\"\n            logger.info(msg)\n            return None\n\n        expiry = dateutil.parser.parse(token[\"registrationExpiresAt\"])\n        if total_seconds(expiry - self._now()) <= 0:\n            logger.info(f\"SSO token registration expired at {expiry}\")\n            return None\n\n        try:\n            return await self._attempt_create_token(token)\n        except ClientError:\n            logger.warning(\"SSO token refresh attempt failed\", exc_info=True)\n            return None\n\n    async def _refresher(self):\n        start_url = self._sso_config[\"sso_start_url\"]\n        session_name = self._sso_config[\"session_name\"]\n        logger.info(f\"Loading cached SSO token for {session_name}\")\n        token_dict = self._token_loader(start_url, session_name=session_name)\n        expiration = dateutil.parser.parse(token_dict[\"expiresAt\"])\n        logger.debug(f\"Cached SSO token expires at {expiration}\")\n\n        remaining = total_seconds(expiration - self._now())\n        if remaining < self._REFRESH_WINDOW:\n            new_token_dict = await self._refresh_access_token(token_dict)\n            if new_token_dict is not None:\n                token_dict = new_token_dict\n                expiration = token_dict[\"expiresAt\"]\n                self._token_loader.save_token(\n                    start_url, token_dict, session_name=session_name\n                )\n\n        return FrozenAuthToken(\n            token_dict[\"accessToken\"], expiration=expiration\n        )\n\n    def load_token(self):\n        if self._sso_config is None:\n            return None\n\n        return AioDeferredRefreshableToken(\n            self.METHOD, self._refresher, time_fetcher=self._now\n        )\n", "aiobotocore/parsers.py": "from botocore.parsers import (\n    LOG,\n    EC2QueryParser,\n    JSONParser,\n    NoInitialResponseError,\n    QueryParser,\n    ResponseParserError,\n    ResponseParserFactory,\n    RestJSONParser,\n    RestXMLParser,\n    lowercase_dict,\n)\n\nfrom .eventstream import AioEventStream\n\n\nclass AioResponseParserFactory(ResponseParserFactory):\n    def create_parser(self, protocol_name):\n        parser_cls = PROTOCOL_PARSERS[protocol_name]\n        return parser_cls(**self._defaults)\n\n\ndef create_parser(protocol):\n    return AioResponseParserFactory().create_parser(protocol)\n\n\nclass AioQueryParser(QueryParser):\n    def _create_event_stream(self, response, shape):\n        parser = self._event_stream_parser\n        name = response['context'].get('operation_name')\n        return AioEventStream(response['body'], shape, parser, name)\n\n\nclass AioEC2QueryParser(EC2QueryParser):\n    def _create_event_stream(self, response, shape):\n        parser = self._event_stream_parser\n        name = response['context'].get('operation_name')\n        return AioEventStream(response['body'], shape, parser, name)\n\n\nclass AioJSONParser(JSONParser):\n    async def _do_parse(self, response, shape):\n        parsed = {}\n        if shape is not None:\n            event_name = shape.event_stream_name\n            if event_name:\n                parsed = await self._handle_event_stream(\n                    response, shape, event_name\n                )\n            else:\n                parsed = self._handle_json_body(response['body'], shape)\n        self._inject_response_metadata(parsed, response['headers'])\n        return parsed\n\n    def _create_event_stream(self, response, shape):\n        parser = self._event_stream_parser\n        name = response['context'].get('operation_name')\n        return AioEventStream(response['body'], shape, parser, name)\n\n    async def _handle_event_stream(self, response, shape, event_name):\n        event_stream_shape = shape.members[event_name]\n        event_stream = self._create_event_stream(response, event_stream_shape)\n        try:\n            event = await event_stream.get_initial_response()\n        except NoInitialResponseError:\n            error_msg = 'First event was not of type initial-response'\n            raise ResponseParserError(error_msg)\n        parsed = self._handle_json_body(event.payload, shape)\n        parsed[event_name] = event_stream\n        return parsed\n\n    # this is actually from ResponseParser however for now JSONParser is the\n    # only class that needs this async\n    async def parse(self, response, shape):\n        LOG.debug('Response headers: %s', response['headers'])\n        LOG.debug('Response body:\\n%s', response['body'])\n        if response['status_code'] >= 301:\n            if self._is_generic_error_response(response):\n                parsed = self._do_generic_error_parse(response)\n            elif self._is_modeled_error_shape(shape):\n                parsed = self._do_modeled_error_parse(response, shape)\n                # We don't want to decorate the modeled fields with metadata\n                return parsed\n            else:\n                parsed = self._do_error_parse(response, shape)\n        else:\n            parsed = await self._do_parse(response, shape)\n\n        # We don't want to decorate event stream responses with metadata\n        if shape and shape.serialization.get('eventstream'):\n            return parsed\n\n        # Add ResponseMetadata if it doesn't exist and inject the HTTP\n        # status code and headers from the response.\n        if isinstance(parsed, dict):\n            response_metadata = parsed.get('ResponseMetadata', {})\n            response_metadata['HTTPStatusCode'] = response['status_code']\n            # Ensure that the http header keys are all lower cased. Older\n            # versions of urllib3 (< 1.11) would unintentionally do this for us\n            # (see urllib3#633). We need to do this conversion manually now.\n            headers = response['headers']\n            response_metadata['HTTPHeaders'] = lowercase_dict(headers)\n            parsed['ResponseMetadata'] = response_metadata\n            self._add_checksum_response_metadata(response, response_metadata)\n        return parsed\n\n\nclass AioRestJSONParser(RestJSONParser):\n    def _create_event_stream(self, response, shape):\n        parser = self._event_stream_parser\n        name = response['context'].get('operation_name')\n        return AioEventStream(response['body'], shape, parser, name)\n\n\nclass AioRestXMLParser(RestXMLParser):\n    def _create_event_stream(self, response, shape):\n        parser = self._event_stream_parser\n        name = response['context'].get('operation_name')\n        return AioEventStream(response['body'], shape, parser, name)\n\n\nPROTOCOL_PARSERS = {\n    'ec2': AioEC2QueryParser,\n    'query': AioQueryParser,\n    'json': AioJSONParser,\n    'rest-json': AioRestJSONParser,\n    'rest-xml': AioRestXMLParser,\n}\n", "aiobotocore/_endpoint_helpers.py": "import asyncio\n\nimport aiohttp.http_exceptions\nimport botocore.retryhandler\nimport wrapt\n\n# Monkey patching: We need to insert the aiohttp exception equivalents\n# The only other way to do this would be to have another config file :(\n_aiohttp_retryable_exceptions = [\n    aiohttp.ClientConnectionError,\n    aiohttp.ClientPayloadError,\n    aiohttp.ServerDisconnectedError,\n    aiohttp.http_exceptions.HttpProcessingError,\n    asyncio.TimeoutError,\n]\n\nbotocore.retryhandler.EXCEPTION_MAP['GENERAL_CONNECTION_ERROR'].extend(\n    _aiohttp_retryable_exceptions\n)\n\n\ndef _text(s, encoding='utf-8', errors='strict'):\n    if isinstance(s, bytes):\n        return s.decode(encoding, errors)\n    return s  # pragma: no cover\n\n\n# Unfortunately aiohttp changed the behavior of streams:\n#   github.com/aio-libs/aiohttp/issues/1907\n# We need this wrapper until we have a final resolution\nclass _IOBaseWrapper(wrapt.ObjectProxy):\n    def close(self):\n        # this stream should not be closed by aiohttp, like 1.x\n        pass\n", "aiobotocore/discovery.py": "import inspect\n\nfrom botocore.discovery import (\n    EndpointDiscoveryHandler,\n    EndpointDiscoveryManager,\n    EndpointDiscoveryRefreshFailed,\n    HTTPClientError,\n    logger,\n)\n\n\nclass AioEndpointDiscoveryManager(EndpointDiscoveryManager):\n    async def _refresh_current_endpoints(self, **kwargs):\n        cache_key = self._create_cache_key(**kwargs)\n        try:\n            response = self._describe_endpoints(**kwargs)\n\n            if inspect.isawaitable(response):\n                response = await response\n\n            endpoints = self._parse_endpoints(response)\n            self._cache[cache_key] = endpoints\n            self._failed_attempts.pop(cache_key, None)\n            return endpoints\n        except (ConnectionError, HTTPClientError):\n            self._failed_attempts[cache_key] = self._time() + 60\n            return None\n\n    async def describe_endpoint(self, **kwargs):\n        operation = kwargs['Operation']\n        discovery_required = self._model.discovery_required_for(operation)\n\n        if not self._always_discover and not discovery_required:\n            # Discovery set to only run on required operations\n            logger.debug(\n                'Optional discovery disabled. Skipping discovery for Operation: %s'\n                % operation\n            )\n            return None\n\n        # Get the endpoint for the provided operation and identifiers\n        cache_key = self._create_cache_key(**kwargs)\n        endpoints = self._get_current_endpoints(cache_key)\n        if endpoints:\n            return self._select_endpoint(endpoints)\n        # All known endpoints are stale\n        recently_failed = self._recently_failed(cache_key)\n        if not recently_failed:\n            # We haven't failed to discover recently, go ahead and refresh\n            endpoints = await self._refresh_current_endpoints(**kwargs)\n            if endpoints:\n                return self._select_endpoint(endpoints)\n        # Discovery has failed recently, do our best to get an endpoint\n        logger.debug('Endpoint Discovery has failed for: %s', kwargs)\n        stale_entries = self._cache.get(cache_key, None)\n        if stale_entries:\n            # We have stale entries, use those while discovery is failing\n            return self._select_endpoint(stale_entries)\n        if discovery_required:\n            # It looks strange to be checking recently_failed again but,\n            # this informs us as to whether or not we tried to refresh earlier\n            if recently_failed:\n                # Discovery is required and we haven't already refreshed\n                endpoints = await self._refresh_current_endpoints(**kwargs)\n                if endpoints:\n                    return self._select_endpoint(endpoints)\n            # No endpoints even refresh, raise hard error\n            raise EndpointDiscoveryRefreshFailed()\n        # Discovery is optional, just use the default endpoint for now\n        return None\n\n\nclass AioEndpointDiscoveryHandler(EndpointDiscoveryHandler):\n    async def discover_endpoint(self, request, operation_name, **kwargs):\n        ids = request.context.get('discovery', {}).get('identifiers')\n        if ids is None:\n            return\n        endpoint = await self._manager.describe_endpoint(\n            Operation=operation_name, Identifiers=ids\n        )\n        if endpoint is None:\n            logger.debug('Failed to discover and inject endpoint')\n            return\n        if not endpoint.startswith('http'):\n            endpoint = 'https://' + endpoint\n        logger.debug('Injecting discovered endpoint: %s', endpoint)\n        request.url = endpoint\n", "aiobotocore/awsrequest.py": "import botocore.utils\nfrom botocore.awsrequest import AWSResponse\n\n\nclass AioAWSResponse(AWSResponse):\n    # Unlike AWSResponse, these return awaitables\n\n    async def _content_prop(self):\n        \"\"\"Content of the response as bytes.\"\"\"\n\n        if self._content is None:\n            # NOTE: this will cache the data in self.raw\n            self._content = await self.raw.read() or b''\n\n        return self._content\n\n    @property\n    def content(self):\n        return self._content_prop()\n\n    async def _text_prop(self):\n        encoding = botocore.utils.get_encoding_from_headers(self.headers)\n        if encoding:\n            return (await self.content).decode(encoding)\n        else:\n            return (await self.content).decode('utf-8')\n\n    @property\n    def text(self):\n        return self._text_prop()\n", "aiobotocore/config.py": "import copy\n\nimport botocore.client\nfrom botocore.exceptions import ParamValidationError\n\nfrom aiobotocore.endpoint import DEFAULT_HTTP_SESSION_CLS\n\n\nclass AioConfig(botocore.client.Config):\n    def __init__(\n        self,\n        connector_args=None,\n        http_session_cls=DEFAULT_HTTP_SESSION_CLS,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n\n        self._validate_connector_args(connector_args)\n        self.connector_args = copy.copy(connector_args)\n        self.http_session_cls = http_session_cls\n        if not self.connector_args:\n            self.connector_args = dict()\n\n        if 'keepalive_timeout' not in self.connector_args:\n            # AWS has a 20 second idle timeout:\n            #   https://web.archive.org/web/20150926192339/https://forums.aws.amazon.com/message.jspa?messageID=215367\n            # and aiohttp default timeout is 30s so we set it to something\n            # reasonable here\n            self.connector_args['keepalive_timeout'] = 12\n\n    def merge(self, other_config):\n        # Adapted from parent class\n        config_options = copy.copy(self._user_provided_options)\n        config_options.update(other_config._user_provided_options)\n        return AioConfig(self.connector_args, **config_options)\n\n    @staticmethod\n    def _validate_connector_args(connector_args):\n        if connector_args is None:\n            return\n\n        for k, v in connector_args.items():\n            # verify_ssl is handled by verify parameter to create_client\n            if k == 'use_dns_cache':\n                if not isinstance(v, bool):\n                    raise ParamValidationError(\n                        report=f'{k} value must be a boolean'\n                    )\n            elif k == 'keepalive_timeout':\n                if v is not None and not isinstance(v, (float, int)):\n                    raise ParamValidationError(\n                        report=f'{k} value must be a float/int or None'\n                    )\n            elif k == 'force_close':\n                if not isinstance(v, bool):\n                    raise ParamValidationError(\n                        report=f'{k} value must be a boolean'\n                    )\n            # limit is handled by max_pool_connections\n            elif k == 'ssl_context':\n                import ssl\n\n                if not isinstance(v, ssl.SSLContext):\n                    raise ParamValidationError(\n                        report=f'{k} must be an SSLContext instance'\n                    )\n            elif k == \"resolver\":\n                from aiohttp.abc import AbstractResolver\n\n                if not isinstance(v, AbstractResolver):\n                    raise ParamValidationError(\n                        report=f'{k} must be an instance of a AbstractResolver'\n                    )\n            else:\n                raise ParamValidationError(report=f'invalid connector_arg:{k}')\n", "aiobotocore/endpoint.py": "import asyncio\n\nfrom botocore.endpoint import (\n    DEFAULT_TIMEOUT,\n    MAX_POOL_CONNECTIONS,\n    Endpoint,\n    EndpointCreator,\n    HTTPClientError,\n    create_request_object,\n    history_recorder,\n    is_valid_endpoint_url,\n    is_valid_ipv6_endpoint_url,\n    logger,\n)\nfrom botocore.hooks import first_non_none_response\nfrom urllib3.response import HTTPHeaderDict\n\nfrom aiobotocore.httpchecksum import handle_checksum_body\nfrom aiobotocore.httpsession import AIOHTTPSession\nfrom aiobotocore.response import StreamingBody\n\nDEFAULT_HTTP_SESSION_CLS = AIOHTTPSession\n\n\nasync def convert_to_response_dict(http_response, operation_model):\n    \"\"\"Convert an HTTP response object to a request dict.\n\n    This converts the requests library's HTTP response object to\n    a dictionary.\n\n    :type http_response: botocore.vendored.requests.model.Response\n    :param http_response: The HTTP response from an AWS service request.\n\n    :rtype: dict\n    :return: A response dictionary which will contain the following keys:\n        * headers (dict)\n        * status_code (int)\n        * body (string or file-like object)\n\n    \"\"\"\n    response_dict = {\n        # botocore converts keys to str, so make sure that they are in\n        # the expected case. See detailed discussion here:\n        # https://github.com/aio-libs/aiobotocore/pull/116\n        # aiohttp's CIMultiDict camel cases the headers :(\n        'headers': HTTPHeaderDict(\n            {\n                k.decode('utf-8').lower(): v.decode('utf-8')\n                for k, v in http_response.raw.raw_headers\n            }\n        ),\n        'status_code': http_response.status_code,\n        'context': {\n            'operation_name': operation_model.name,\n        },\n    }\n    if response_dict['status_code'] >= 300:\n        response_dict['body'] = await http_response.content\n    elif operation_model.has_event_stream_output:\n        response_dict['body'] = http_response.raw\n    elif operation_model.has_streaming_output:\n        length = response_dict['headers'].get('content-length')\n        response_dict['body'] = StreamingBody(http_response.raw, length)\n    else:\n        response_dict['body'] = await http_response.content\n    return response_dict\n\n\nclass AioEndpoint(Endpoint):\n    async def close(self):\n        await self.http_session.close()\n\n    async def create_request(self, params, operation_model=None):\n        request = create_request_object(params)\n        if operation_model:\n            request.stream_output = any(\n                [\n                    operation_model.has_streaming_output,\n                    operation_model.has_event_stream_output,\n                ]\n            )\n            service_id = operation_model.service_model.service_id.hyphenize()\n            event_name = 'request-created.{service_id}.{op_name}'.format(\n                service_id=service_id, op_name=operation_model.name\n            )\n            await self._event_emitter.emit(\n                event_name,\n                request=request,\n                operation_name=operation_model.name,\n            )\n        prepared_request = self.prepare_request(request)\n        return prepared_request\n\n    async def _send_request(self, request_dict, operation_model):\n        attempts = 1\n        context = request_dict['context']\n        self._update_retries_context(context, attempts)\n        request = await self.create_request(request_dict, operation_model)\n        success_response, exception = await self._get_response(\n            request, operation_model, context\n        )\n        while await self._needs_retry(\n            attempts,\n            operation_model,\n            request_dict,\n            success_response,\n            exception,\n        ):\n            attempts += 1\n            self._update_retries_context(context, attempts, success_response)\n            # If there is a stream associated with the request, we need\n            # to reset it before attempting to send the request again.\n            # This will ensure that we resend the entire contents of the\n            # body.\n            request.reset_stream()\n            # Create a new request when retried (including a new signature).\n            request = await self.create_request(request_dict, operation_model)\n            success_response, exception = await self._get_response(\n                request, operation_model, context\n            )\n        if (\n            success_response is not None\n            and 'ResponseMetadata' in success_response[1]\n        ):\n            # We want to share num retries, not num attempts.\n            total_retries = attempts - 1\n            success_response[1]['ResponseMetadata'][\n                'RetryAttempts'\n            ] = total_retries\n        if exception is not None:\n            raise exception\n        else:\n            return success_response\n\n    async def _get_response(self, request, operation_model, context):\n        # This will return a tuple of (success_response, exception)\n        # and success_response is itself a tuple of\n        # (http_response, parsed_dict).\n        # If an exception occurs then the success_response is None.\n        # If no exception occurs then exception is None.\n        success_response, exception = await self._do_get_response(\n            request, operation_model, context\n        )\n        kwargs_to_emit = {\n            'response_dict': None,\n            'parsed_response': None,\n            'context': context,\n            'exception': exception,\n        }\n        if success_response is not None:\n            http_response, parsed_response = success_response\n            kwargs_to_emit['parsed_response'] = parsed_response\n            kwargs_to_emit['response_dict'] = await convert_to_response_dict(\n                http_response, operation_model\n            )\n        service_id = operation_model.service_model.service_id.hyphenize()\n        await self._event_emitter.emit(\n            f\"response-received.{service_id}.{operation_model.name}\",\n            **kwargs_to_emit,\n        )\n        return success_response, exception\n\n    async def _do_get_response(self, request, operation_model, context):\n        try:\n            logger.debug(\"Sending http request: %s\", request)\n            history_recorder.record(\n                'HTTP_REQUEST',\n                {\n                    'method': request.method,\n                    'headers': request.headers,\n                    'streaming': operation_model.has_streaming_input,\n                    'url': request.url,\n                    'body': request.body,\n                },\n            )\n            service_id = operation_model.service_model.service_id.hyphenize()\n            event_name = f\"before-send.{service_id}.{operation_model.name}\"\n            responses = await self._event_emitter.emit(\n                event_name, request=request\n            )\n            http_response = first_non_none_response(responses)\n            if http_response is None:\n                http_response = await self._send(request)\n        except HTTPClientError as e:\n            return (None, e)\n        except Exception as e:\n            logger.debug(\n                \"Exception received when sending HTTP request.\", exc_info=True\n            )\n            return (None, e)\n\n        # This returns the http_response and the parsed_data.\n        response_dict = await convert_to_response_dict(\n            http_response, operation_model\n        )\n        await handle_checksum_body(\n            http_response,\n            response_dict,\n            context,\n            operation_model,\n        )\n\n        http_response_record_dict = response_dict.copy()\n        http_response_record_dict[\n            'streaming'\n        ] = operation_model.has_streaming_output\n        history_recorder.record('HTTP_RESPONSE', http_response_record_dict)\n\n        protocol = operation_model.metadata['protocol']\n        parser = self._response_parser_factory.create_parser(protocol)\n\n        if asyncio.iscoroutinefunction(parser.parse):\n            parsed_response = await parser.parse(\n                response_dict, operation_model.output_shape\n            )\n        else:\n            parsed_response = parser.parse(\n                response_dict, operation_model.output_shape\n            )\n\n        if http_response.status_code >= 300:\n            await self._add_modeled_error_fields(\n                response_dict,\n                parsed_response,\n                operation_model,\n                parser,\n            )\n        history_recorder.record('PARSED_RESPONSE', parsed_response)\n        return (http_response, parsed_response), None\n\n    async def _add_modeled_error_fields(\n        self,\n        response_dict,\n        parsed_response,\n        operation_model,\n        parser,\n    ):\n        error_code = parsed_response.get(\"Error\", {}).get(\"Code\")\n        if error_code is None:\n            return\n        service_model = operation_model.service_model\n        error_shape = service_model.shape_for_error_code(error_code)\n        if error_shape is None:\n            return\n\n        if asyncio.iscoroutinefunction(parser.parse):\n            modeled_parse = await parser.parse(response_dict, error_shape)\n        else:\n            modeled_parse = parser.parse(response_dict, error_shape)\n        # TODO: avoid naming conflicts with ResponseMetadata and Error\n        parsed_response.update(modeled_parse)\n\n    # NOTE: The only line changed here changing time.sleep to asyncio.sleep\n    async def _needs_retry(\n        self,\n        attempts,\n        operation_model,\n        request_dict,\n        response=None,\n        caught_exception=None,\n    ):\n        service_id = operation_model.service_model.service_id.hyphenize()\n        event_name = f\"needs-retry.{service_id}.{operation_model.name}\"\n        responses = await self._event_emitter.emit(\n            event_name,\n            response=response,\n            endpoint=self,\n            operation=operation_model,\n            attempts=attempts,\n            caught_exception=caught_exception,\n            request_dict=request_dict,\n        )\n        handler_response = first_non_none_response(responses)\n        if handler_response is None:\n            return False\n        else:\n            # Request needs to be retried, and we need to sleep\n            # for the specified number of times.\n            logger.debug(\n                \"Response received to retry, sleeping for %s seconds\",\n                handler_response,\n            )\n            await asyncio.sleep(handler_response)\n            return True\n\n    async def _send(self, request):\n        return await self.http_session.send(request)\n\n\nclass AioEndpointCreator(EndpointCreator):\n    def create_endpoint(\n        self,\n        service_model,\n        region_name,\n        endpoint_url,\n        verify=None,\n        response_parser_factory=None,\n        timeout=DEFAULT_TIMEOUT,\n        max_pool_connections=MAX_POOL_CONNECTIONS,\n        http_session_cls=DEFAULT_HTTP_SESSION_CLS,\n        proxies=None,\n        socket_options=None,\n        client_cert=None,\n        proxies_config=None,\n        connector_args=None,\n    ):\n        if not is_valid_endpoint_url(\n            endpoint_url\n        ) and not is_valid_ipv6_endpoint_url(endpoint_url):\n            raise ValueError(\"Invalid endpoint: %s\" % endpoint_url)\n\n        if proxies is None:\n            proxies = self._get_proxies(endpoint_url)\n        endpoint_prefix = service_model.endpoint_prefix\n\n        logger.debug('Setting %s timeout as %s', endpoint_prefix, timeout)\n        http_session = http_session_cls(\n            timeout=timeout,\n            proxies=proxies,\n            verify=self._get_verify_value(verify),\n            max_pool_connections=max_pool_connections,\n            socket_options=socket_options,\n            client_cert=client_cert,\n            proxies_config=proxies_config,\n            connector_args=connector_args,\n        )\n\n        return AioEndpoint(\n            endpoint_url,\n            endpoint_prefix=endpoint_prefix,\n            event_emitter=self._event_emitter,\n            response_parser_factory=response_parser_factory,\n            http_session=http_session,\n        )\n", "aiobotocore/regions.py": "import copy\nimport logging\n\nfrom botocore.exceptions import EndpointProviderError\nfrom botocore.regions import EndpointRulesetResolver\n\nLOG = logging.getLogger(__name__)\n\n\nclass AioEndpointRulesetResolver(EndpointRulesetResolver):\n    async def construct_endpoint(\n        self,\n        operation_model,\n        call_args,\n        request_context,\n    ):\n        \"\"\"Invokes the provider with params defined in the service's ruleset\"\"\"\n        if call_args is None:\n            call_args = {}\n\n        if request_context is None:\n            request_context = {}\n\n        provider_params = await self._get_provider_params(\n            operation_model, call_args, request_context\n        )\n        LOG.debug(\n            'Calling endpoint provider with parameters: %s' % provider_params\n        )\n        try:\n            provider_result = self._provider.resolve_endpoint(\n                **provider_params\n            )\n        except EndpointProviderError as ex:\n            botocore_exception = self.ruleset_error_to_botocore_exception(\n                ex, provider_params\n            )\n            if botocore_exception is None:\n                raise\n            else:\n                raise botocore_exception from ex\n        LOG.debug('Endpoint provider result: %s' % provider_result.url)\n\n        # The endpoint provider does not support non-secure transport.\n        if not self._use_ssl and provider_result.url.startswith('https://'):\n            provider_result = provider_result._replace(\n                url=f'http://{provider_result.url[8:]}'\n            )\n\n        # Multi-valued headers are not supported in botocore. Replace the list\n        # of values returned for each header with just its first entry,\n        # dropping any additionally entries.\n        provider_result = provider_result._replace(\n            headers={\n                key: val[0] for key, val in provider_result.headers.items()\n            }\n        )\n\n        return provider_result\n\n    async def _get_provider_params(\n        self, operation_model, call_args, request_context\n    ):\n        \"\"\"Resolve a value for each parameter defined in the service's ruleset\n\n        The resolution order for parameter values is:\n        1. Operation-specific static context values from the service definition\n        2. Operation-specific dynamic context values from API parameters\n        3. Client-specific context parameters\n        4. Built-in values such as region, FIPS usage, ...\n        \"\"\"\n        provider_params = {}\n        # Builtin values can be customized for each operation by hooks\n        # subscribing to the ``before-endpoint-resolution.*`` event.\n        customized_builtins = await self._get_customized_builtins(\n            operation_model, call_args, request_context\n        )\n        for param_name, param_def in self._param_definitions.items():\n            param_val = self._resolve_param_from_context(\n                param_name=param_name,\n                operation_model=operation_model,\n                call_args=call_args,\n            )\n            if param_val is None and param_def.builtin is not None:\n                param_val = self._resolve_param_as_builtin(\n                    builtin_name=param_def.builtin,\n                    builtins=customized_builtins,\n                )\n            if param_val is not None:\n                provider_params[param_name] = param_val\n\n        return provider_params\n\n    async def _get_customized_builtins(\n        self, operation_model, call_args, request_context\n    ):\n        service_id = self._service_model.service_id.hyphenize()\n        customized_builtins = copy.copy(self._builtins)\n        # Handlers are expected to modify the builtins dict in place.\n        await self._event_emitter.emit(\n            'before-endpoint-resolution.%s' % service_id,\n            builtins=customized_builtins,\n            model=operation_model,\n            params=call_args,\n            context=request_context,\n        )\n        return customized_builtins\n", "aiobotocore/retryhandler.py": "from botocore.retryhandler import (\n    ChecksumError,\n    CRC32Checker,\n    ExceptionRaiser,\n    HTTPStatusCodeChecker,\n    MaxAttemptsDecorator,\n    MultiChecker,\n    RetryHandler,\n    ServiceErrorCodeChecker,\n    _extract_retryable_exception,\n    crc32,\n    create_retry_action_from_config,\n    logger,\n)\n\nfrom ._helpers import resolve_awaitable\n\n\ndef create_retry_handler(config, operation_name=None):\n    checker = create_checker_from_retry_config(\n        config, operation_name=operation_name\n    )\n    action = create_retry_action_from_config(\n        config, operation_name=operation_name\n    )\n    return AioRetryHandler(checker=checker, action=action)\n\n\ndef create_checker_from_retry_config(config, operation_name=None):\n    checkers = []\n    max_attempts = None\n    retryable_exceptions = []\n    if '__default__' in config:\n        policies = config['__default__'].get('policies', [])\n        max_attempts = config['__default__']['max_attempts']\n        for key in policies:\n            current_config = policies[key]\n            checkers.append(_create_single_checker(current_config))\n            retry_exception = _extract_retryable_exception(current_config)\n            if retry_exception is not None:\n                retryable_exceptions.extend(retry_exception)\n    if operation_name is not None and config.get(operation_name) is not None:\n        operation_policies = config[operation_name]['policies']\n        for key in operation_policies:\n            checkers.append(_create_single_checker(operation_policies[key]))\n            retry_exception = _extract_retryable_exception(\n                operation_policies[key]\n            )\n            if retry_exception is not None:\n                retryable_exceptions.extend(retry_exception)\n    if len(checkers) == 1:\n        # Don't need to use a MultiChecker\n        return AioMaxAttemptsDecorator(checkers[0], max_attempts=max_attempts)\n    else:\n        multi_checker = AioMultiChecker(checkers)\n        return AioMaxAttemptsDecorator(\n            multi_checker,\n            max_attempts=max_attempts,\n            retryable_exceptions=tuple(retryable_exceptions),\n        )\n\n\ndef _create_single_checker(config):\n    if 'response' in config['applies_when']:\n        return _create_single_response_checker(\n            config['applies_when']['response']\n        )\n    elif 'socket_errors' in config['applies_when']:\n        return ExceptionRaiser()\n\n\ndef _create_single_response_checker(response):\n    if 'service_error_code' in response:\n        checker = ServiceErrorCodeChecker(\n            status_code=response['http_status_code'],\n            error_code=response['service_error_code'],\n        )\n    elif 'http_status_code' in response:\n        checker = HTTPStatusCodeChecker(\n            status_code=response['http_status_code']\n        )\n    elif 'crc32body' in response:\n        checker = AioCRC32Checker(header=response['crc32body'])\n    else:\n        # TODO: send a signal.\n        raise ValueError(\"Unknown retry policy\")\n    return checker\n\n\nclass AioRetryHandler(RetryHandler):\n    async def _call(self, attempts, response, caught_exception, **kwargs):\n        \"\"\"Handler for a retry.\n\n        Intended to be hooked up to an event handler (hence the **kwargs),\n        this will process retries appropriately.\n\n        \"\"\"\n        checker_kwargs = {\n            'attempt_number': attempts,\n            'response': response,\n            'caught_exception': caught_exception,\n        }\n        if isinstance(self._checker, MaxAttemptsDecorator):\n            retries_context = kwargs['request_dict']['context'].get('retries')\n            checker_kwargs.update({'retries_context': retries_context})\n\n        if await resolve_awaitable(self._checker(**checker_kwargs)):\n            result = self._action(attempts=attempts)\n            logger.debug(\"Retry needed, action of: %s\", result)\n            return result\n        logger.debug(\"No retry needed.\")\n\n    def __call__(self, *args, **kwargs):\n        return self._call(*args, **kwargs)  # return awaitable\n\n\nclass AioMaxAttemptsDecorator(MaxAttemptsDecorator):\n    async def _call(\n        self, attempt_number, response, caught_exception, retries_context\n    ):\n        if retries_context:\n            retries_context['max'] = max(\n                retries_context.get('max', 0), self._max_attempts\n            )\n\n        should_retry = await self._should_retry(\n            attempt_number, response, caught_exception\n        )\n        if should_retry:\n            if attempt_number >= self._max_attempts:\n                # explicitly set MaxAttemptsReached\n                if response is not None and 'ResponseMetadata' in response[1]:\n                    response[1]['ResponseMetadata'][\n                        'MaxAttemptsReached'\n                    ] = True\n                logger.debug(\n                    \"Reached the maximum number of retry attempts: %s\",\n                    attempt_number,\n                )\n                return False\n            else:\n                return should_retry\n        else:\n            return False\n\n    def __call__(self, *args, **kwargs):\n        return self._call(*args, **kwargs)\n\n    async def _should_retry(self, attempt_number, response, caught_exception):\n        if self._retryable_exceptions and attempt_number < self._max_attempts:\n            try:\n                return await resolve_awaitable(\n                    self._checker(attempt_number, response, caught_exception)\n                )\n            except self._retryable_exceptions as e:\n                logger.debug(\n                    \"retry needed, retryable exception caught: %s\",\n                    e,\n                    exc_info=True,\n                )\n                return True\n        else:\n            # If we've exceeded the max attempts we just let the exception\n            # propagate if one has occurred.\n            return await resolve_awaitable(\n                self._checker(attempt_number, response, caught_exception)\n            )\n\n\nclass AioMultiChecker(MultiChecker):\n    async def _call(self, attempt_number, response, caught_exception):\n        for checker in self._checkers:\n            checker_response = await resolve_awaitable(\n                checker(attempt_number, response, caught_exception)\n            )\n            if checker_response:\n                return checker_response\n        return False\n\n    def __call__(self, *args, **kwargs):\n        return self._call(*args, **kwargs)\n\n\nclass AioCRC32Checker(CRC32Checker):\n    async def _call(self, attempt_number, response, caught_exception):\n        if response is not None:\n            return await self._check_response(attempt_number, response)\n        elif caught_exception is not None:\n            return self._check_caught_exception(\n                attempt_number, caught_exception\n            )\n        else:\n            raise ValueError(\"Both response and caught_exception are None.\")\n\n    def __call__(self, *args, **kwargs):\n        return self._call(*args, **kwargs)\n\n    async def _check_response(self, attempt_number, response):\n        http_response = response[0]\n        expected_crc = http_response.headers.get(self._header_name)\n        if expected_crc is None:\n            logger.debug(\n                \"crc32 check skipped, the %s header is not \"\n                \"in the http response.\",\n                self._header_name,\n            )\n        else:\n            actual_crc32 = crc32(await response[0].content) & 0xFFFFFFFF\n            if not actual_crc32 == int(expected_crc):\n                logger.debug(\n                    \"retry needed: crc32 check failed, expected != actual: \"\n                    \"%s != %s\",\n                    int(expected_crc),\n                    actual_crc32,\n                )\n                raise ChecksumError(\n                    checksum_type='crc32',\n                    expected_checksum=int(expected_crc),\n                    actual_checksum=actual_crc32,\n                )\n", "aiobotocore/httpchecksum.py": "import io\n\nfrom botocore.httpchecksum import (\n    _CHECKSUM_CLS,\n    AwsChunkedWrapper,\n    FlexibleChecksumError,\n    _apply_request_header_checksum,\n    _handle_streaming_response,\n    base64,\n    conditionally_calculate_md5,\n    determine_content_length,\n    logger,\n)\n\nfrom aiobotocore._helpers import resolve_awaitable\n\n\nclass AioAwsChunkedWrapper(AwsChunkedWrapper):\n    async def _make_chunk(self):\n        # NOTE: Chunk size is not deterministic as read could return less. This\n        # means we cannot know the content length of the encoded aws-chunked\n        # stream ahead of time without ensuring a consistent chunk size\n\n        raw_chunk = await resolve_awaitable(self._raw.read(self._chunk_size))\n        hex_len = hex(len(raw_chunk))[2:].encode(\"ascii\")\n        self._complete = not raw_chunk\n\n        if self._checksum:\n            self._checksum.update(raw_chunk)\n\n        if self._checksum and self._complete:\n            name = self._checksum_name.encode(\"ascii\")\n            checksum = self._checksum.b64digest().encode(\"ascii\")\n            return b\"0\\r\\n%s:%s\\r\\n\\r\\n\" % (name, checksum)\n\n        return b\"%s\\r\\n%s\\r\\n\" % (hex_len, raw_chunk)\n\n    def __aiter__(self):\n        return self\n\n    async def __anext__(self):\n        while not self._complete:\n            return await self._make_chunk()\n        raise StopAsyncIteration()\n\n\nasync def handle_checksum_body(\n    http_response, response, context, operation_model\n):\n    headers = response[\"headers\"]\n    checksum_context = context.get(\"checksum\", {})\n    algorithms = checksum_context.get(\"response_algorithms\")\n\n    if not algorithms:\n        return\n\n    for algorithm in algorithms:\n        header_name = \"x-amz-checksum-%s\" % algorithm\n        # If the header is not found, check the next algorithm\n        if header_name not in headers:\n            continue\n\n        # If a - is in the checksum this is not valid Base64. S3 returns\n        # checksums that include a -# suffix to indicate a checksum derived\n        # from the hash of all part checksums. We cannot wrap this response\n        if \"-\" in headers[header_name]:\n            continue\n\n        if operation_model.has_streaming_output:\n            response[\"body\"] = _handle_streaming_response(\n                http_response, response, algorithm\n            )\n        else:\n            response[\"body\"] = await _handle_bytes_response(\n                http_response, response, algorithm\n            )\n\n        # Expose metadata that the checksum check actually occurred\n        checksum_context = response[\"context\"].get(\"checksum\", {})\n        checksum_context[\"response_algorithm\"] = algorithm\n        response[\"context\"][\"checksum\"] = checksum_context\n        return\n\n    logger.info(\n        f'Skipping checksum validation. Response did not contain one of the '\n        f'following algorithms: {algorithms}.'\n    )\n\n\nasync def _handle_bytes_response(http_response, response, algorithm):\n    body = await http_response.content\n    header_name = \"x-amz-checksum-%s\" % algorithm\n    checksum_cls = _CHECKSUM_CLS.get(algorithm)\n    checksum = checksum_cls()\n    checksum.update(body)\n    expected = response[\"headers\"][header_name]\n    if checksum.digest() != base64.b64decode(expected):\n        error_msg = (\n            \"Expected checksum %s did not match calculated checksum: %s\"\n            % (\n                expected,\n                checksum.b64digest(),\n            )\n        )\n        raise FlexibleChecksumError(error_msg=error_msg)\n    return body\n\n\ndef apply_request_checksum(request):\n    checksum_context = request.get(\"context\", {}).get(\"checksum\", {})\n    algorithm = checksum_context.get(\"request_algorithm\")\n\n    if not algorithm:\n        return\n\n    if algorithm == \"conditional-md5\":\n        # Special case to handle the http checksum required trait\n        conditionally_calculate_md5(request)\n    elif algorithm[\"in\"] == \"header\":\n        _apply_request_header_checksum(request)\n    elif algorithm[\"in\"] == \"trailer\":\n        _apply_request_trailer_checksum(request)\n    else:\n        raise FlexibleChecksumError(\n            error_msg=\"Unknown checksum variant: %s\" % algorithm[\"in\"]\n        )\n\n\ndef _apply_request_trailer_checksum(request):\n    checksum_context = request.get(\"context\", {}).get(\"checksum\", {})\n    algorithm = checksum_context.get(\"request_algorithm\")\n    location_name = algorithm[\"name\"]\n    checksum_cls = _CHECKSUM_CLS.get(algorithm[\"algorithm\"])\n\n    headers = request[\"headers\"]\n    body = request[\"body\"]\n\n    if location_name in headers:\n        # If the header is already set by the customer, skip calculation\n        return\n\n    # Cannot set this as aiohttp complains\n    headers[\"Transfer-Encoding\"] = \"chunked\"\n    if \"Content-Encoding\" in headers:\n        # We need to preserve the existing content encoding and add\n        # aws-chunked as a new content encoding.\n        headers[\"Content-Encoding\"] += \",aws-chunked\"\n    else:\n        headers[\"Content-Encoding\"] = \"aws-chunked\"\n    headers[\"X-Amz-Trailer\"] = location_name\n\n    content_length = determine_content_length(body)\n    if content_length is not None:\n        # Send the decoded content length if we can determine it. Some\n        # services such as S3 may require the decoded content length\n        headers[\"X-Amz-Decoded-Content-Length\"] = str(content_length)\n\n    if isinstance(body, (bytes, bytearray)):\n        body = io.BytesIO(body)\n\n    request[\"body\"] = AioAwsChunkedWrapper(\n        body,\n        checksum_cls=checksum_cls,\n        checksum_name=location_name,\n    )\n", "aiobotocore/utils.py": "import asyncio\nimport functools\nimport inspect\nimport json\nimport logging\n\nimport botocore.awsrequest\nfrom botocore.exceptions import (\n    InvalidIMDSEndpointError,\n    MetadataRetrievalError,\n)\nfrom botocore.utils import (\n    DEFAULT_METADATA_SERVICE_TIMEOUT,\n    METADATA_BASE_URL,\n    RETRYABLE_HTTP_ERRORS,\n    ArnParser,\n    BadIMDSRequestError,\n    ClientError,\n    ContainerMetadataFetcher,\n    HTTPClientError,\n    IdentityCache,\n    IMDSFetcher,\n    IMDSRegionProvider,\n    InstanceMetadataFetcher,\n    InstanceMetadataRegionFetcher,\n    ReadTimeoutError,\n    S3ExpressIdentityCache,\n    S3ExpressIdentityResolver,\n    S3RegionRedirector,\n    S3RegionRedirectorv2,\n    get_environ_proxies,\n    os,\n    resolve_imds_endpoint_mode,\n)\n\nimport aiobotocore.httpsession\nfrom aiobotocore._helpers import asynccontextmanager\n\nlogger = logging.getLogger(__name__)\n\n\nclass _RefCountedSession(aiobotocore.httpsession.AIOHTTPSession):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.__ref_count = 0\n        self.__lock = None\n\n    @asynccontextmanager\n    async def acquire(self):\n        if not self.__lock:\n            self.__lock = asyncio.Lock()\n\n        # ensure we have a session\n        async with self.__lock:\n            self.__ref_count += 1\n\n            try:\n                if self.__ref_count == 1:\n                    await self.__aenter__()\n            except BaseException:\n                self.__ref_count -= 1\n                raise\n\n        try:\n            yield self\n        finally:\n            async with self.__lock:\n                if self.__ref_count == 1:\n                    await self.__aexit__(None, None, None)\n\n                self.__ref_count -= 1\n\n\nclass AioIMDSFetcher(IMDSFetcher):\n    def __init__(\n        self,\n        timeout=DEFAULT_METADATA_SERVICE_TIMEOUT,  # noqa: E501, lgtm [py/missing-call-to-init]\n        num_attempts=1,\n        base_url=METADATA_BASE_URL,\n        env=None,\n        user_agent=None,\n        config=None,\n        session=None,\n    ):\n        self._timeout = timeout\n        self._num_attempts = num_attempts\n        if config is None:\n            config = {}\n        self._base_url = self._select_base_url(base_url, config)\n        self._config = config\n\n        if env is None:\n            env = os.environ.copy()\n        self._disabled = (\n            env.get('AWS_EC2_METADATA_DISABLED', 'false').lower() == 'true'\n        )\n        self._imds_v1_disabled = config.get('ec2_metadata_v1_disabled')\n        self._user_agent = user_agent\n\n        self._session = session or _RefCountedSession(\n            timeout=self._timeout,\n            proxies=get_environ_proxies(self._base_url),\n        )\n\n    async def _fetch_metadata_token(self):\n        self._assert_enabled()\n        url = self._construct_url(self._TOKEN_PATH)\n        headers = {\n            'x-aws-ec2-metadata-token-ttl-seconds': self._TOKEN_TTL,\n        }\n        self._add_user_agent(headers)\n\n        request = botocore.awsrequest.AWSRequest(\n            method='PUT', url=url, headers=headers\n        )\n\n        async with self._session.acquire() as session:\n            for i in range(self._num_attempts):\n                try:\n                    response = await session.send(request.prepare())\n                    if response.status_code == 200:\n                        return await response.text\n                    elif response.status_code in (404, 403, 405):\n                        return None\n                    elif response.status_code in (400,):\n                        raise BadIMDSRequestError(request)\n                except ReadTimeoutError:\n                    return None\n                except RETRYABLE_HTTP_ERRORS as e:\n                    logger.debug(\n                        \"Caught retryable HTTP exception while making metadata \"\n                        \"service request to %s: %s\",\n                        url,\n                        e,\n                        exc_info=True,\n                    )\n                except HTTPClientError as e:\n                    error = e.kwargs.get('error')\n                    if (\n                        error\n                        and getattr(error, 'errno', None) == 8\n                        or str(getattr(error, 'os_error', None))\n                        == 'Domain name not found'\n                    ):  # threaded vs async resolver\n                        raise InvalidIMDSEndpointError(endpoint=url, error=e)\n                    else:\n                        raise\n\n        return None\n\n    async def _get_request(self, url_path, retry_func, token=None):\n        self._assert_enabled()\n        if not token:\n            self._assert_v1_enabled()\n        if retry_func is None:\n            retry_func = self._default_retry\n        url = self._construct_url(url_path)\n        headers = {}\n        if token is not None:\n            headers['x-aws-ec2-metadata-token'] = token\n        self._add_user_agent(headers)\n\n        async with self._session.acquire() as session:\n            for i in range(self._num_attempts):\n                try:\n                    request = botocore.awsrequest.AWSRequest(\n                        method='GET', url=url, headers=headers\n                    )\n                    response = await session.send(request.prepare())\n                    should_retry = retry_func(response)\n                    if inspect.isawaitable(should_retry):\n                        should_retry = await should_retry\n\n                    if not should_retry:\n                        return response\n                except RETRYABLE_HTTP_ERRORS as e:\n                    logger.debug(\n                        \"Caught retryable HTTP exception while making metadata \"\n                        \"service request to %s: %s\",\n                        url,\n                        e,\n                        exc_info=True,\n                    )\n        raise self._RETRIES_EXCEEDED_ERROR_CLS()\n\n    async def _default_retry(self, response):\n        return await self._is_non_ok_response(\n            response\n        ) or await self._is_empty(response)\n\n    async def _is_non_ok_response(self, response):\n        if response.status_code != 200:\n            await self._log_imds_response(response, 'non-200', log_body=True)\n            return True\n        return False\n\n    async def _is_empty(self, response):\n        if not await response.content:\n            await self._log_imds_response(response, 'no body', log_body=True)\n            return True\n        return False\n\n    async def _log_imds_response(\n        self, response, reason_to_log, log_body=False\n    ):\n        statement = (\n            \"Metadata service returned %s response \"\n            \"with status code of %s for url: %s\"\n        )\n        logger_args = [reason_to_log, response.status_code, response.url]\n        if log_body:\n            statement += \", content body: %s\"\n            logger_args.append(await response.content)\n        logger.debug(statement, *logger_args)\n\n\nclass AioInstanceMetadataFetcher(AioIMDSFetcher, InstanceMetadataFetcher):\n    async def retrieve_iam_role_credentials(self):\n        try:\n            token = await self._fetch_metadata_token()\n            role_name = await self._get_iam_role(token)\n            credentials = await self._get_credentials(role_name, token)\n            if self._contains_all_credential_fields(credentials):\n                credentials = {\n                    'role_name': role_name,\n                    'access_key': credentials['AccessKeyId'],\n                    'secret_key': credentials['SecretAccessKey'],\n                    'token': credentials['Token'],\n                    'expiry_time': credentials['Expiration'],\n                }\n                self._evaluate_expiration(credentials)\n                return credentials\n            else:\n                if 'Code' in credentials and 'Message' in credentials:\n                    logger.debug(\n                        'Error response received when retrieving'\n                        'credentials: %s.',\n                        credentials,\n                    )\n                return {}\n        except self._RETRIES_EXCEEDED_ERROR_CLS:\n            logger.debug(\n                \"Max number of attempts exceeded (%s) when \"\n                \"attempting to retrieve data from metadata service.\",\n                self._num_attempts,\n            )\n        except BadIMDSRequestError as e:\n            logger.debug(\"Bad IMDS request: %s\", e.request)\n        return {}\n\n    async def _get_iam_role(self, token=None):\n        return await (\n            await self._get_request(\n                url_path=self._URL_PATH,\n                retry_func=self._needs_retry_for_role_name,\n                token=token,\n            )\n        ).text\n\n    async def _get_credentials(self, role_name, token=None):\n        r = await self._get_request(\n            url_path=self._URL_PATH + role_name,\n            retry_func=self._needs_retry_for_credentials,\n            token=token,\n        )\n        return json.loads(await r.text)\n\n    async def _is_invalid_json(self, response):\n        try:\n            json.loads(await response.text)\n            return False\n        except ValueError:\n            await self._log_imds_response(response, 'invalid json')\n            return True\n\n    async def _needs_retry_for_role_name(self, response):\n        return await self._is_non_ok_response(\n            response\n        ) or await self._is_empty(response)\n\n    async def _needs_retry_for_credentials(self, response):\n        return (\n            await self._is_non_ok_response(response)\n            or await self._is_empty(response)\n            or await self._is_invalid_json(response)\n        )\n\n\nclass AioIMDSRegionProvider(IMDSRegionProvider):\n    async def provide(self):\n        \"\"\"Provide the region value from IMDS.\"\"\"\n        instance_region = await self._get_instance_metadata_region()\n        return instance_region\n\n    async def _get_instance_metadata_region(self):\n        fetcher = self._get_fetcher()\n        region = await fetcher.retrieve_region()\n        return region\n\n    def _create_fetcher(self):\n        metadata_timeout = self._session.get_config_variable(\n            'metadata_service_timeout'\n        )\n        metadata_num_attempts = self._session.get_config_variable(\n            'metadata_service_num_attempts'\n        )\n        imds_config = {\n            'ec2_metadata_service_endpoint': self._session.get_config_variable(\n                'ec2_metadata_service_endpoint'\n            ),\n            'ec2_metadata_service_endpoint_mode': resolve_imds_endpoint_mode(\n                self._session\n            ),\n            'ec2_metadata_v1_disabled': self._session.get_config_variable(\n                'ec2_metadata_v1_disabled'\n            ),\n        }\n        fetcher = AioInstanceMetadataRegionFetcher(\n            timeout=metadata_timeout,\n            num_attempts=metadata_num_attempts,\n            env=self._environ,\n            user_agent=self._session.user_agent(),\n            config=imds_config,\n        )\n        return fetcher\n\n\nclass AioInstanceMetadataRegionFetcher(\n    AioIMDSFetcher, InstanceMetadataRegionFetcher\n):\n    async def retrieve_region(self):\n        try:\n            region = await self._get_region()\n            return region\n        except self._RETRIES_EXCEEDED_ERROR_CLS:\n            logger.debug(\n                \"Max number of attempts exceeded (%s) when \"\n                \"attempting to retrieve data from metadata service.\",\n                self._num_attempts,\n            )\n        return None\n\n    async def _get_region(self):\n        token = await self._fetch_metadata_token()\n        response = await self._get_request(\n            url_path=self._URL_PATH,\n            retry_func=self._default_retry,\n            token=token,\n        )\n        availability_zone = await response.text\n        region = availability_zone[:-1]\n        return region\n\n\nclass AioIdentityCache(IdentityCache):\n    async def get_credentials(self, **kwargs):\n        callback = self.build_refresh_callback(**kwargs)\n        metadata = await callback()\n        credential_entry = self._credential_cls.create_from_metadata(\n            metadata=metadata,\n            refresh_using=callback,\n            method=self.METHOD,\n            advisory_timeout=45,\n            mandatory_timeout=10,\n        )\n        return credential_entry\n\n\nclass AioS3ExpressIdentityCache(AioIdentityCache, S3ExpressIdentityCache):\n    @functools.lru_cache(maxsize=100)\n    def _get_credentials(self, bucket):\n        return asyncio.create_task(super().get_credentials(bucket=bucket))\n\n    async def get_credentials(self, bucket):\n        # upstream uses `@functools.lru_cache(maxsize=100)` to cache credentials.\n        # This is incompatible with async code.\n        # We need to implement custom caching logic.\n\n        return await self._get_credentials(bucket=bucket)\n\n    def build_refresh_callback(self, bucket):\n        async def refresher():\n            response = await self._client.create_session(Bucket=bucket)\n            creds = response['Credentials']\n            expiration = self._serialize_if_needed(\n                creds['Expiration'], iso=True\n            )\n            return {\n                \"access_key\": creds['AccessKeyId'],\n                \"secret_key\": creds['SecretAccessKey'],\n                \"token\": creds['SessionToken'],\n                \"expiry_time\": expiration,\n            }\n\n        return refresher\n\n\nclass AioS3ExpressIdentityResolver(S3ExpressIdentityResolver):\n    def __init__(self, client, credential_cls, cache=None):\n        super().__init__(client, credential_cls, cache)\n\n        if cache is None:\n            cache = AioS3ExpressIdentityCache(self._client, credential_cls)\n        self._cache = cache\n\n\nclass AioS3RegionRedirectorv2(S3RegionRedirectorv2):\n    async def redirect_from_error(\n        self,\n        request_dict,\n        response,\n        operation,\n        **kwargs,\n    ):\n        \"\"\"\n        An S3 request sent to the wrong region will return an error that\n        contains the endpoint the request should be sent to. This handler\n        will add the redirect information to the signing context and then\n        redirect the request.\n        \"\"\"\n        if response is None:\n            # This could be none if there was a ConnectionError or other\n            # transport error.\n            return\n\n        redirect_ctx = request_dict.get('context', {}).get('s3_redirect', {})\n        if ArnParser.is_arn(redirect_ctx.get('bucket')):\n            logger.debug(\n                'S3 request was previously for an Accesspoint ARN, not '\n                'redirecting.'\n            )\n            return\n\n        if redirect_ctx.get('redirected'):\n            logger.debug(\n                'S3 request was previously redirected, not redirecting.'\n            )\n            return\n\n        error = response[1].get('Error', {})\n        error_code = error.get('Code')\n        response_metadata = response[1].get('ResponseMetadata', {})\n\n        # We have to account for 400 responses because\n        # if we sign a Head* request with the wrong region,\n        # we'll get a 400 Bad Request but we won't get a\n        # body saying it's an \"AuthorizationHeaderMalformed\".\n        is_special_head_object = (\n            error_code in ('301', '400') and operation.name == 'HeadObject'\n        )\n        is_special_head_bucket = (\n            error_code in ('301', '400')\n            and operation.name == 'HeadBucket'\n            and 'x-amz-bucket-region'\n            in response_metadata.get('HTTPHeaders', {})\n        )\n        is_wrong_signing_region = (\n            error_code == 'AuthorizationHeaderMalformed' and 'Region' in error\n        )\n        is_redirect_status = response[0] is not None and response[\n            0\n        ].status_code in (301, 302, 307)\n        is_permanent_redirect = error_code == 'PermanentRedirect'\n        if not any(\n            [\n                is_special_head_object,\n                is_wrong_signing_region,\n                is_permanent_redirect,\n                is_special_head_bucket,\n                is_redirect_status,\n            ]\n        ):\n            return\n\n        bucket = request_dict['context']['s3_redirect']['bucket']\n        client_region = request_dict['context'].get('client_region')\n        new_region = await self.get_bucket_region(bucket, response)\n\n        if new_region is None:\n            logger.debug(\n                \"S3 client configured for region %s but the bucket %s is not \"\n                \"in that region and the proper region could not be \"\n                \"automatically determined.\" % (client_region, bucket)\n            )\n            return\n\n        logger.debug(\n            \"S3 client configured for region %s but the bucket %s is in region\"\n            \" %s; Please configure the proper region to avoid multiple \"\n            \"unnecessary redirects and signing attempts.\"\n            % (client_region, bucket, new_region)\n        )\n        # Adding the new region to _cache will make construct_endpoint() to\n        # use the new region as value for the AWS::Region builtin parameter.\n        self._cache[bucket] = new_region\n\n        # Re-resolve endpoint with new region and modify request_dict with\n        # the new URL, auth scheme, and signing context.\n        ep_resolver = self._client._ruleset_resolver\n        ep_info = await ep_resolver.construct_endpoint(\n            operation_model=operation,\n            call_args=request_dict['context']['s3_redirect']['params'],\n            request_context=request_dict['context'],\n        )\n        request_dict['url'] = self.set_request_url(\n            request_dict['url'], ep_info.url\n        )\n        request_dict['context']['s3_redirect']['redirected'] = True\n        auth_schemes = ep_info.properties.get('authSchemes')\n        if auth_schemes is not None:\n            auth_info = ep_resolver.auth_schemes_to_signing_ctx(auth_schemes)\n            auth_type, signing_context = auth_info\n            request_dict['context']['auth_type'] = auth_type\n            request_dict['context']['signing'] = {\n                **request_dict['context'].get('signing', {}),\n                **signing_context,\n            }\n\n        # Return 0 so it doesn't wait to retry\n        return 0\n\n    async def get_bucket_region(self, bucket, response):\n        \"\"\"\n        There are multiple potential sources for the new region to redirect to,\n        but they aren't all universally available for use. This will try to\n        find region from response elements, but will fall back to calling\n        HEAD on the bucket if all else fails.\n        :param bucket: The bucket to find the region for. This is necessary if\n            the region is not available in the error response.\n        :param response: A response representing a service request that failed\n            due to incorrect region configuration.\n        \"\"\"\n        # First try to source the region from the headers.\n        service_response = response[1]\n        response_headers = service_response['ResponseMetadata']['HTTPHeaders']\n        if 'x-amz-bucket-region' in response_headers:\n            return response_headers['x-amz-bucket-region']\n\n        # Next, check the error body\n        region = service_response.get('Error', {}).get('Region', None)\n        if region is not None:\n            return region\n\n        # Finally, HEAD the bucket. No other choice sadly.\n        try:\n            # NOTE: we don't need to aenter/aexit as we have a ref to the base client\n            response = await self._client.head_bucket(Bucket=bucket)\n            headers = response['ResponseMetadata']['HTTPHeaders']\n        except ClientError as e:\n            headers = e.response['ResponseMetadata']['HTTPHeaders']\n\n        region = headers.get('x-amz-bucket-region', None)\n        return region\n\n\nclass AioS3RegionRedirector(S3RegionRedirector):\n    async def redirect_from_error(\n        self, request_dict, response, operation, **kwargs\n    ):\n        if response is None:\n            # This could be none if there was a ConnectionError or other\n            # transport error.\n            return\n\n        if self._is_s3_accesspoint(request_dict.get('context', {})):\n            logger.debug(\n                'S3 request was previously to an accesspoint, not redirecting.'\n            )\n            return\n\n        if request_dict.get('context', {}).get('s3_redirected'):\n            logger.debug(\n                'S3 request was previously redirected, not redirecting.'\n            )\n            return\n\n        error = response[1].get('Error', {})\n        error_code = error.get('Code')\n        response_metadata = response[1].get('ResponseMetadata', {})\n\n        # We have to account for 400 responses because\n        # if we sign a Head* request with the wrong region,\n        # we'll get a 400 Bad Request but we won't get a\n        # body saying it's an \"AuthorizationHeaderMalformed\".\n        is_special_head_object = (\n            error_code in ('301', '400') and operation.name == 'HeadObject'\n        )\n        is_special_head_bucket = (\n            error_code in ('301', '400')\n            and operation.name == 'HeadBucket'\n            and 'x-amz-bucket-region'\n            in response_metadata.get('HTTPHeaders', {})\n        )\n        is_wrong_signing_region = (\n            error_code == 'AuthorizationHeaderMalformed' and 'Region' in error\n        )\n        is_redirect_status = response[0] is not None and response[\n            0\n        ].status_code in (301, 302, 307)\n        is_permanent_redirect = error_code == 'PermanentRedirect'\n        if not any(\n            [\n                is_special_head_object,\n                is_wrong_signing_region,\n                is_permanent_redirect,\n                is_special_head_bucket,\n                is_redirect_status,\n            ]\n        ):\n            return\n\n        bucket = request_dict['context']['signing']['bucket']\n        client_region = request_dict['context'].get('client_region')\n        new_region = await self.get_bucket_region(bucket, response)\n\n        if new_region is None:\n            logger.debug(\n                \"S3 client configured for region %s but the bucket %s is not \"\n                \"in that region and the proper region could not be \"\n                \"automatically determined.\" % (client_region, bucket)\n            )\n            return\n\n        logger.debug(\n            \"S3 client configured for region %s but the bucket %s is in region\"\n            \" %s; Please configure the proper region to avoid multiple \"\n            \"unnecessary redirects and signing attempts.\"\n            % (client_region, bucket, new_region)\n        )\n        endpoint = self._endpoint_resolver.resolve('s3', new_region)\n        endpoint = endpoint['endpoint_url']\n\n        signing_context = {\n            'region': new_region,\n            'bucket': bucket,\n            'endpoint': endpoint,\n        }\n        request_dict['context']['signing'] = signing_context\n\n        self._cache[bucket] = signing_context\n        self.set_request_url(request_dict, request_dict['context'])\n\n        request_dict['context']['s3_redirected'] = True\n\n        # Return 0 so it doesn't wait to retry\n        return 0\n\n    async def get_bucket_region(self, bucket, response):\n        # First try to source the region from the headers.\n        service_response = response[1]\n        response_headers = service_response['ResponseMetadata']['HTTPHeaders']\n        if 'x-amz-bucket-region' in response_headers:\n            return response_headers['x-amz-bucket-region']\n\n        # Next, check the error body\n        region = service_response.get('Error', {}).get('Region', None)\n        if region is not None:\n            return region\n\n        # Finally, HEAD the bucket. No other choice sadly.\n        try:\n            # NOTE: we don't need to aenter/aexit as we have a ref to the base client\n            response = await self._client.head_bucket(Bucket=bucket)\n            headers = response['ResponseMetadata']['HTTPHeaders']\n        except ClientError as e:\n            headers = e.response['ResponseMetadata']['HTTPHeaders']\n\n        region = headers.get('x-amz-bucket-region', None)\n        return region\n\n\nclass AioContainerMetadataFetcher(ContainerMetadataFetcher):\n    def __init__(\n        self, session=None, sleep=asyncio.sleep\n    ):  # noqa: E501, lgtm [py/missing-call-to-init]\n        if session is None:\n            session = _RefCountedSession(timeout=self.TIMEOUT_SECONDS)\n        self._session = session\n        self._sleep = sleep\n\n    async def retrieve_full_uri(self, full_url, headers=None):\n        self._validate_allowed_url(full_url)\n        return await self._retrieve_credentials(full_url, headers)\n\n    async def retrieve_uri(self, relative_uri):\n        \"\"\"Retrieve JSON metadata from container metadata.\n\n        :type relative_uri: str\n        :param relative_uri: A relative URI, e.g \"/foo/bar?id=123\"\n\n        :return: The parsed JSON response.\n\n        \"\"\"\n        full_url = self.full_url(relative_uri)\n        return await self._retrieve_credentials(full_url)\n\n    async def _retrieve_credentials(self, full_url, extra_headers=None):\n        headers = {'Accept': 'application/json'}\n        if extra_headers is not None:\n            headers.update(extra_headers)\n        attempts = 0\n        while True:\n            try:\n                return await self._get_response(\n                    full_url, headers, self.TIMEOUT_SECONDS\n                )\n            except MetadataRetrievalError as e:\n                logger.debug(\n                    \"Received error when attempting to retrieve \"\n                    \"container metadata: %s\",\n                    e,\n                    exc_info=True,\n                )\n                await self._sleep(self.SLEEP_TIME)\n                attempts += 1\n                if attempts >= self.RETRY_ATTEMPTS:\n                    raise\n\n    async def _get_response(self, full_url, headers, timeout):\n        try:\n            async with self._session.acquire() as session:\n                AWSRequest = botocore.awsrequest.AWSRequest\n                request = AWSRequest(\n                    method='GET', url=full_url, headers=headers\n                )\n                response = await session.send(request.prepare())\n                response_text = (await response.content).decode('utf-8')\n\n                if response.status_code != 200:\n                    raise MetadataRetrievalError(\n                        error_msg=(\n                            f\"Received non 200 response {response.status_code} \"\n                            f\"from container metadata: {response_text}\"\n                        )\n                    )\n                try:\n                    return json.loads(response_text)\n                except ValueError:\n                    error_msg = \"Unable to parse JSON returned from container metadata services\"\n                    logger.debug('%s:%s', error_msg, response_text)\n                    raise MetadataRetrievalError(error_msg=error_msg)\n\n        except RETRYABLE_HTTP_ERRORS as e:\n            error_msg = (\n                \"Received error when attempting to retrieve \"\n                f\"container metadata: {e}\"\n            )\n            raise MetadataRetrievalError(error_msg=error_msg)\n", "aiobotocore/signers.py": "import datetime\n\nimport botocore\nimport botocore.auth\nfrom botocore.exceptions import UnknownClientMethodError\nfrom botocore.signers import (\n    RequestSigner,\n    S3PostPresigner,\n    UnknownSignatureVersionError,\n    UnsupportedSignatureVersionError,\n    _should_use_global_endpoint,\n    create_request_object,\n    prepare_request_dict,\n)\nfrom botocore.utils import ArnParser\n\n\nclass AioRequestSigner(RequestSigner):\n    async def handler(self, operation_name=None, request=None, **kwargs):\n        # This is typically hooked up to the \"request-created\" event\n        # from a client's event emitter.  When a new request is created\n        # this method is invoked to sign the request.\n        # Don't call this method directly.\n        return await self.sign(operation_name, request)\n\n    async def sign(\n        self,\n        operation_name,\n        request,\n        region_name=None,\n        signing_type='standard',\n        expires_in=None,\n        signing_name=None,\n    ):\n        explicit_region_name = region_name\n        if region_name is None:\n            region_name = self._region_name\n\n        if signing_name is None:\n            signing_name = self._signing_name\n\n        signature_version = await self._choose_signer(\n            operation_name, signing_type, request.context\n        )\n\n        # Allow mutating request before signing\n        await self._event_emitter.emit(\n            'before-sign.{}.{}'.format(\n                self._service_id.hyphenize(), operation_name\n            ),\n            request=request,\n            signing_name=signing_name,\n            region_name=self._region_name,\n            signature_version=signature_version,\n            request_signer=self,\n            operation_name=operation_name,\n        )\n\n        if signature_version != botocore.UNSIGNED:\n            kwargs = {\n                'signing_name': signing_name,\n                'region_name': region_name,\n                'signature_version': signature_version,\n            }\n            if expires_in is not None:\n                kwargs['expires'] = expires_in\n            signing_context = request.context.get('signing', {})\n            if not explicit_region_name and signing_context.get('region'):\n                kwargs['region_name'] = signing_context['region']\n            if signing_context.get('signing_name'):\n                kwargs['signing_name'] = signing_context['signing_name']\n            if signing_context.get('request_credentials'):\n                kwargs['request_credentials'] = signing_context[\n                    'request_credentials'\n                ]\n            if signing_context.get('identity_cache') is not None:\n                self._resolve_identity_cache(\n                    kwargs,\n                    signing_context['identity_cache'],\n                    signing_context['cache_key'],\n                )\n            try:\n                auth = await self.get_auth_instance(**kwargs)\n            except UnknownSignatureVersionError as e:\n                if signing_type != 'standard':\n                    raise UnsupportedSignatureVersionError(\n                        signature_version=signature_version\n                    )\n                else:\n                    raise e\n\n            auth.add_auth(request)\n\n    async def _choose_signer(self, operation_name, signing_type, context):\n        signing_type_suffix_map = {\n            'presign-post': '-presign-post',\n            'presign-url': '-query',\n        }\n        suffix = signing_type_suffix_map.get(signing_type, '')\n\n        # operation specific signing context takes precedent over client-level\n        # defaults\n        signature_version = context.get('auth_type') or self._signature_version\n        signing = context.get('signing', {})\n        signing_name = signing.get('signing_name', self._signing_name)\n        region_name = signing.get('region', self._region_name)\n        if (\n            signature_version is not botocore.UNSIGNED\n            and not signature_version.endswith(suffix)\n        ):\n            signature_version += suffix\n\n        handler, response = await self._event_emitter.emit_until_response(\n            'choose-signer.{}.{}'.format(\n                self._service_id.hyphenize(), operation_name\n            ),\n            signing_name=signing_name,\n            region_name=region_name,\n            signature_version=signature_version,\n            context=context,\n        )\n\n        if response is not None:\n            signature_version = response\n            # The suffix needs to be checked again in case we get an improper\n            # signature version from choose-signer.\n            if (\n                signature_version is not botocore.UNSIGNED\n                and not signature_version.endswith(suffix)\n            ):\n                signature_version += suffix\n\n        return signature_version\n\n    async def get_auth_instance(\n        self,\n        signing_name,\n        region_name,\n        signature_version=None,\n        request_credentials=None,\n        **kwargs,\n    ):\n        if signature_version is None:\n            signature_version = self._signature_version\n\n        cls = botocore.auth.AUTH_TYPE_MAPS.get(signature_version)\n        if cls is None:\n            raise UnknownSignatureVersionError(\n                signature_version=signature_version\n            )\n\n        if cls.REQUIRES_TOKEN is True:\n            frozen_token = None\n            if self._auth_token is not None:\n                frozen_token = await self._auth_token.get_frozen_token()\n            auth = cls(frozen_token)\n            return auth\n\n        credentials = request_credentials or self._credentials\n        if getattr(cls, \"REQUIRES_IDENTITY_CACHE\", None) is True:\n            cache = kwargs[\"identity_cache\"]\n            key = kwargs[\"cache_key\"]\n            credentials = await cache.get_credentials(key)\n            del kwargs[\"cache_key\"]\n\n        frozen_credentials = None\n        if credentials is not None:\n            frozen_credentials = await credentials.get_frozen_credentials()\n        kwargs['credentials'] = frozen_credentials\n        if cls.REQUIRES_REGION:\n            if self._region_name is None:\n                raise botocore.exceptions.NoRegionError()\n            kwargs['region_name'] = region_name\n            kwargs['service_name'] = signing_name\n        auth = cls(**kwargs)\n        return auth\n\n    # Alias get_auth for backwards compatibility.\n    get_auth = get_auth_instance\n\n    async def generate_presigned_url(\n        self,\n        request_dict,\n        operation_name,\n        expires_in=3600,\n        region_name=None,\n        signing_name=None,\n    ):\n        request = create_request_object(request_dict)\n        await self.sign(\n            operation_name,\n            request,\n            region_name,\n            'presign-url',\n            expires_in,\n            signing_name,\n        )\n\n        request.prepare()\n        return request.url\n\n\ndef add_generate_db_auth_token(class_attributes, **kwargs):\n    class_attributes['generate_db_auth_token'] = generate_db_auth_token\n\n\nasync def generate_db_auth_token(\n    self, DBHostname, Port, DBUsername, Region=None\n):\n    \"\"\"Generates an auth token used to connect to a db with IAM credentials.\n\n    :type DBHostname: str\n    :param DBHostname: The hostname of the database to connect to.\n\n    :type Port: int\n    :param Port: The port number the database is listening on.\n\n    :type DBUsername: str\n    :param DBUsername: The username to log in as.\n\n    :type Region: str\n    :param Region: The region the database is in. If None, the client\n        region will be used.\n\n    :return: A presigned url which can be used as an auth token.\n    \"\"\"\n    region = Region\n    if region is None:\n        region = self.meta.region_name\n\n    params = {\n        'Action': 'connect',\n        'DBUser': DBUsername,\n    }\n\n    request_dict = {\n        'url_path': '/',\n        'query_string': '',\n        'headers': {},\n        'body': params,\n        'method': 'GET',\n    }\n\n    # RDS requires that the scheme not be set when sent over. This can cause\n    # issues when signing because the Python url parsing libraries follow\n    # RFC 1808 closely, which states that a netloc must be introduced by `//`.\n    # Otherwise the url is presumed to be relative, and thus the whole\n    # netloc would be treated as a path component. To work around this we\n    # introduce https here and remove it once we're done processing it.\n    scheme = 'https://'\n    endpoint_url = f'{scheme}{DBHostname}:{Port}'\n    prepare_request_dict(request_dict, endpoint_url)\n    presigned_url = await self._request_signer.generate_presigned_url(\n        operation_name='connect',\n        request_dict=request_dict,\n        region_name=region,\n        expires_in=900,\n        signing_name='rds-db',\n    )\n    return presigned_url[len(scheme) :]\n\n\nclass AioS3PostPresigner(S3PostPresigner):\n    async def generate_presigned_post(\n        self,\n        request_dict,\n        fields=None,\n        conditions=None,\n        expires_in=3600,\n        region_name=None,\n    ):\n        if fields is None:\n            fields = {}\n\n        if conditions is None:\n            conditions = []\n\n        # Create the policy for the post.\n        policy = {}\n\n        # Create an expiration date for the policy\n        datetime_now = datetime.datetime.utcnow()\n        expire_date = datetime_now + datetime.timedelta(seconds=expires_in)\n        policy['expiration'] = expire_date.strftime(botocore.auth.ISO8601)\n\n        # Append all of the conditions that the user supplied.\n        policy['conditions'] = []\n        for condition in conditions:\n            policy['conditions'].append(condition)\n\n        # Store the policy and the fields in the request for signing\n        request = create_request_object(request_dict)\n        request.context['s3-presign-post-fields'] = fields\n        request.context['s3-presign-post-policy'] = policy\n\n        await self._request_signer.sign(\n            'PutObject', request, region_name, 'presign-post'\n        )\n        # Return the url and the fields for th form to post.\n        return {'url': request.url, 'fields': fields}\n\n\ndef add_generate_presigned_url(class_attributes, **kwargs):\n    class_attributes['generate_presigned_url'] = generate_presigned_url\n\n\nasync def generate_presigned_url(\n    self, ClientMethod, Params=None, ExpiresIn=3600, HttpMethod=None\n):\n    \"\"\"Generate a presigned url given a client, its method, and arguments\n\n    :type ClientMethod: string\n    :param ClientMethod: The client method to presign for\n\n    :type Params: dict\n    :param Params: The parameters normally passed to\n        ``ClientMethod``.\n\n    :type ExpiresIn: int\n    :param ExpiresIn: The number of seconds the presigned url is valid\n        for. By default it expires in an hour (3600 seconds)\n\n    :type HttpMethod: string\n    :param HttpMethod: The http method to use on the generated url. By\n        default, the http method is whatever is used in the method's model.\n\n    :returns: The presigned url\n    \"\"\"\n    client_method = ClientMethod\n    params = Params\n    if params is None:\n        params = {}\n    expires_in = ExpiresIn\n    http_method = HttpMethod\n    context = {\n        'is_presign_request': True,\n        'use_global_endpoint': _should_use_global_endpoint(self),\n    }\n\n    request_signer = self._request_signer\n\n    try:\n        operation_name = self._PY_TO_OP_NAME[client_method]\n    except KeyError:\n        raise UnknownClientMethodError(method_name=client_method)\n\n    operation_model = self.meta.service_model.operation_model(operation_name)\n    params = await self._emit_api_params(\n        api_params=params,\n        operation_model=operation_model,\n        context=context,\n    )\n    bucket_is_arn = ArnParser.is_arn(params.get('Bucket', ''))\n    (\n        endpoint_url,\n        additional_headers,\n        properties,\n    ) = await self._resolve_endpoint_ruleset(\n        operation_model,\n        params,\n        context,\n        ignore_signing_region=(not bucket_is_arn),\n    )\n\n    request_dict = await self._convert_to_request_dict(\n        api_params=params,\n        operation_model=operation_model,\n        endpoint_url=endpoint_url,\n        context=context,\n        headers=additional_headers,\n        set_user_agent_header=False,\n    )\n\n    # Switch out the http method if user specified it.\n    if http_method is not None:\n        request_dict['method'] = http_method\n\n    # Generate the presigned url.\n    return await request_signer.generate_presigned_url(\n        request_dict=request_dict,\n        expires_in=expires_in,\n        operation_name=operation_name,\n    )\n\n\ndef add_generate_presigned_post(class_attributes, **kwargs):\n    class_attributes['generate_presigned_post'] = generate_presigned_post\n\n\nasync def generate_presigned_post(\n    self, Bucket, Key, Fields=None, Conditions=None, ExpiresIn=3600\n):\n    bucket = Bucket\n    key = Key\n    fields = Fields\n    conditions = Conditions\n    expires_in = ExpiresIn\n\n    if fields is None:\n        fields = {}\n    else:\n        fields = fields.copy()\n\n    if conditions is None:\n        conditions = []\n\n    context = {\n        'is_presign_request': True,\n        'use_global_endpoint': _should_use_global_endpoint(self),\n    }\n\n    post_presigner = AioS3PostPresigner(self._request_signer)\n\n    # We choose the CreateBucket operation model because its url gets\n    # serialized to what a presign post requires.\n    operation_model = self.meta.service_model.operation_model('CreateBucket')\n    params = await self._emit_api_params(\n        api_params={'Bucket': bucket},\n        operation_model=operation_model,\n        context=context,\n    )\n    bucket_is_arn = ArnParser.is_arn(params.get('Bucket', ''))\n    (\n        endpoint_url,\n        additional_headers,\n        properties,\n    ) = await self._resolve_endpoint_ruleset(\n        operation_model,\n        params,\n        context,\n        ignore_signing_region=(not bucket_is_arn),\n    )\n\n    request_dict = await self._convert_to_request_dict(\n        api_params=params,\n        operation_model=operation_model,\n        endpoint_url=endpoint_url,\n        context=context,\n        headers=additional_headers,\n        set_user_agent_header=False,\n    )\n\n    # Append that the bucket name to the list of conditions.\n    conditions.append({'bucket': bucket})\n\n    # If the key ends with filename, the only constraint that can be\n    # imposed is if it starts with the specified prefix.\n    if key.endswith('${filename}'):\n        conditions.append([\"starts-with\", '$key', key[: -len('${filename}')]])\n    else:\n        conditions.append({'key': key})\n\n    # Add the key to the fields.\n    fields['key'] = key\n\n    return await post_presigner.generate_presigned_post(\n        request_dict=request_dict,\n        fields=fields,\n        conditions=conditions,\n        expires_in=expires_in,\n    )\n", "aiobotocore/httpsession.py": "import asyncio\nimport contextlib\nimport io\nimport os\nimport socket\nfrom typing import Dict, Optional\n\nimport aiohttp  # lgtm [py/import-and-import-from]\nfrom aiohttp import (\n    ClientConnectionError,\n    ClientConnectorError,\n    ClientHttpProxyError,\n    ClientProxyConnectionError,\n    ClientSSLError,\n    ServerDisconnectedError,\n    ServerTimeoutError,\n)\nfrom aiohttp.client import URL\nfrom botocore.httpsession import (\n    MAX_POOL_CONNECTIONS,\n    ConnectionClosedError,\n    ConnectTimeoutError,\n    EndpointConnectionError,\n    HTTPClientError,\n    InvalidProxiesConfigError,\n    LocationParseError,\n    ProxyConfiguration,\n    ProxyConnectionError,\n    ReadTimeoutError,\n    SSLError,\n    _is_ipaddress,\n    create_urllib3_context,\n    ensure_boolean,\n    get_cert_path,\n    logger,\n    mask_proxy_url,\n    parse_url,\n    urlparse,\n)\nfrom multidict import CIMultiDict\n\nimport aiobotocore.awsrequest\nfrom aiobotocore._endpoint_helpers import _IOBaseWrapper, _text\n\n\nclass AIOHTTPSession:\n    def __init__(\n        self,\n        verify: bool = True,\n        proxies: Dict[str, str] = None,  # {scheme: url}\n        timeout: float = None,\n        max_pool_connections: int = MAX_POOL_CONNECTIONS,\n        socket_options=None,\n        client_cert=None,\n        proxies_config=None,\n        connector_args=None,\n    ):\n        self._exit_stack = contextlib.AsyncExitStack()\n\n        # TODO: handle socket_options\n        # keep track of sessions by proxy url (if any)\n        self._sessions: Dict[Optional[str], aiohttp.ClientSession] = {}\n        self._verify = verify\n        self._proxy_config = ProxyConfiguration(\n            proxies=proxies, proxies_settings=proxies_config\n        )\n        if isinstance(timeout, (list, tuple)):\n            conn_timeout, read_timeout = timeout\n        else:\n            conn_timeout = read_timeout = timeout\n\n        timeout = aiohttp.ClientTimeout(\n            sock_connect=conn_timeout, sock_read=read_timeout\n        )\n\n        self._cert_file = None\n        self._key_file = None\n        if isinstance(client_cert, str):\n            self._cert_file = client_cert\n        elif isinstance(client_cert, tuple):\n            self._cert_file, self._key_file = client_cert\n\n        self._timeout = timeout\n        self._connector_args = connector_args\n        if self._connector_args is None:\n            # AWS has a 20 second idle timeout:\n            #   https://web.archive.org/web/20150926192339/https://forums.aws.amazon.com/message.jspa?messageID=215367\n            # aiohttp default timeout is 30s so set something reasonable here\n            self._connector_args = dict(keepalive_timeout=12)\n\n        self._max_pool_connections = max_pool_connections\n        self._socket_options = socket_options\n        if socket_options is None:\n            self._socket_options = []\n\n        # aiohttp handles 100 continue so we shouldn't need AWSHTTP[S]ConnectionPool\n        # it also pools by host so we don't need a manager, and can pass proxy via\n        # request so don't need proxy manager\n\n    async def __aenter__(self):\n        assert not self._sessions\n\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        self._sessions.clear()\n        await self._exit_stack.aclose()\n\n    def _get_ssl_context(self):\n        return create_urllib3_context()\n\n    def _setup_proxy_ssl_context(self, proxy_url):\n        proxies_settings = self._proxy_config.settings\n        proxy_ca_bundle = proxies_settings.get('proxy_ca_bundle')\n        proxy_cert = proxies_settings.get('proxy_client_cert')\n        if proxy_ca_bundle is None and proxy_cert is None:\n            return None\n\n        context = self._get_ssl_context()\n        try:\n            url = parse_url(proxy_url)\n            # urllib3 disables this by default but we need it for proper\n            # proxy tls negotiation when proxy_url is not an IP Address\n            if not _is_ipaddress(url.host):\n                context.check_hostname = True\n            if proxy_ca_bundle is not None:\n                context.load_verify_locations(cafile=proxy_ca_bundle)\n\n            if isinstance(proxy_cert, tuple):\n                context.load_cert_chain(proxy_cert[0], keyfile=proxy_cert[1])\n            elif isinstance(proxy_cert, str):\n                context.load_cert_chain(proxy_cert)\n\n            return context\n        except (OSError, LocationParseError) as e:\n            raise InvalidProxiesConfigError(error=e)\n\n    def _chunked(self, headers):\n        transfer_encoding = headers.get('Transfer-Encoding', '')\n        if chunked := transfer_encoding.lower() == 'chunked':\n            # aiohttp wants chunking as a param, and not a header\n            del headers['Transfer-Encoding']\n        return chunked or None\n\n    def _create_connector(self, proxy_url):\n        ssl_context = None\n        if bool(self._verify):\n            if proxy_url:\n                ssl_context = self._setup_proxy_ssl_context(proxy_url)\n                # TODO: add support for\n                #    proxies_settings.get('proxy_use_forwarding_for_https')\n            else:\n                ssl_context = self._get_ssl_context()\n\n            if ssl_context:\n                if self._cert_file:\n                    ssl_context.load_cert_chain(\n                        self._cert_file,\n                        self._key_file,\n                    )\n\n                # inline self._setup_ssl_cert\n                ca_certs = get_cert_path(self._verify)\n                if ca_certs:\n                    ssl_context.load_verify_locations(ca_certs, None, None)\n\n        return aiohttp.TCPConnector(\n            limit=self._max_pool_connections,\n            ssl=ssl_context or False,\n            **self._connector_args,\n        )\n\n    async def _get_session(self, proxy_url):\n        if not (session := self._sessions.get(proxy_url)):\n            connector = self._create_connector(proxy_url)\n            self._sessions[\n                proxy_url\n            ] = session = await self._exit_stack.enter_async_context(\n                aiohttp.ClientSession(\n                    connector=connector,\n                    timeout=self._timeout,\n                    skip_auto_headers={'CONTENT-TYPE'},\n                    auto_decompress=False,\n                ),\n            )\n\n        return session\n\n    async def close(self):\n        await self.__aexit__(None, None, None)\n\n    async def send(self, request):\n        try:\n            proxy_url = self._proxy_config.proxy_url_for(request.url)\n            proxy_headers = self._proxy_config.proxy_headers_for(request.url)\n            url = request.url\n            headers = request.headers\n            data = request.body\n\n            if ensure_boolean(\n                os.environ.get('BOTO_EXPERIMENTAL__ADD_PROXY_HOST_HEADER', '')\n            ):\n                # This is currently an \"experimental\" feature which provides\n                # no guarantees of backwards compatibility. It may be subject\n                # to change or removal in any patch version. Anyone opting in\n                # to this feature should strictly pin botocore.\n                host = urlparse(request.url).hostname\n                proxy_headers['host'] = host\n\n            headers_ = CIMultiDict(\n                (z[0], _text(z[1], encoding='utf-8')) for z in headers.items()\n            )\n\n            # https://github.com/boto/botocore/issues/1255\n            headers_['Accept-Encoding'] = 'identity'\n\n            if isinstance(data, io.IOBase):\n                data = _IOBaseWrapper(data)\n\n            url = URL(url, encoded=True)\n            session = await self._get_session(proxy_url)\n            response = await session.request(\n                request.method,\n                url=url,\n                chunked=self._chunked(headers_),\n                headers=headers_,\n                data=data,\n                proxy=proxy_url,\n                proxy_headers=proxy_headers,\n            )\n\n            http_response = aiobotocore.awsrequest.AioAWSResponse(\n                str(response.url), response.status, response.headers, response\n            )\n\n            if not request.stream_output:\n                # Cause the raw stream to be exhausted immediately. We do it\n                # this way instead of using preload_content because\n                # preload_content will never buffer chunked responses\n                await http_response.content\n\n            return http_response\n        except ClientSSLError as e:\n            raise SSLError(endpoint_url=request.url, error=e)\n        except (ClientProxyConnectionError, ClientHttpProxyError) as e:\n            raise ProxyConnectionError(\n                proxy_url=mask_proxy_url(proxy_url), error=e\n            )\n        except (\n            ServerDisconnectedError,\n            aiohttp.ClientPayloadError,\n            aiohttp.http_exceptions.BadStatusLine,\n        ) as e:\n            raise ConnectionClosedError(\n                error=e, request=request, endpoint_url=request.url\n            )\n        except ServerTimeoutError as e:\n            if str(e).lower().startswith('connect'):\n                raise ConnectTimeoutError(endpoint_url=request.url, error=e)\n            else:\n                raise ReadTimeoutError(endpoint_url=request.url, error=e)\n        except (\n            ClientConnectorError,\n            ClientConnectionError,\n            socket.gaierror,\n        ) as e:\n            raise EndpointConnectionError(endpoint_url=request.url, error=e)\n        except asyncio.TimeoutError as e:\n            raise ReadTimeoutError(endpoint_url=request.url, error=e)\n        except Exception as e:\n            message = 'Exception received when sending urllib3 HTTP request'\n            logger.debug(message, exc_info=True)\n            raise HTTPClientError(error=e)\n", "aiobotocore/eventstream.py": "from botocore.eventstream import (\n    EventStream,\n    EventStreamBuffer,\n    NoInitialResponseError,\n)\n\n\nclass AioEventStream(EventStream):\n    def __iter__(self):\n        raise NotImplementedError('Use async-for instead')\n\n    def __aiter__(self):\n        return self.__anext__()\n\n    async def __anext__(self):\n        async for event in self._event_generator:\n            parsed_event = self._parse_event(event)\n            if parsed_event:\n                yield parsed_event\n\n    async def _create_raw_event_generator(self):\n        event_stream_buffer = EventStreamBuffer()\n        async for chunk, _ in self._raw_stream.content.iter_chunks():\n            event_stream_buffer.add_data(chunk)\n            for event in event_stream_buffer:\n                yield event  # unfortunately no yield from async func support\n\n    async def get_initial_response(self):\n        try:\n            async for event in self._event_generator:\n                event_type = event.headers.get(':event-type')\n                if event_type == 'initial-response':\n                    return event\n\n                break\n        except StopIteration:\n            pass\n        raise NoInitialResponseError()\n\n    # self._raw_stream.close() is sync so no override needed\n", "aiobotocore/waiter.py": "import asyncio\n\nfrom botocore.docs.docstring import WaiterDocstring\n\n# WaiterModel is required for client.py import\nfrom botocore.exceptions import ClientError\nfrom botocore.utils import get_service_module_name\nfrom botocore.waiter import (\n    NormalizedOperationMethod as _NormalizedOperationMethod,\n)\nfrom botocore.waiter import WaiterModel  # noqa: F401 lgtm[py/unused-import]\nfrom botocore.waiter import (\n    Waiter,\n    WaiterError,\n    is_valid_waiter_error,\n    logger,\n    xform_name,\n)\n\n\ndef create_waiter_with_client(waiter_name, waiter_model, client):\n    \"\"\"\n\n    :type waiter_name: str\n    :param waiter_name: The name of the waiter.  The name should match\n        the name (including the casing) of the key name in the waiter\n        model file (typically this is CamelCasing).\n\n    :type waiter_model: botocore.waiter.WaiterModel\n    :param waiter_model: The model for the waiter configuration.\n\n    :type client: botocore.client.BaseClient\n    :param client: The botocore client associated with the service.\n\n    :rtype: botocore.waiter.Waiter\n    :return: The waiter object.\n\n    \"\"\"\n    single_waiter_config = waiter_model.get_waiter(waiter_name)\n    operation_name = xform_name(single_waiter_config.operation)\n    operation_method = NormalizedOperationMethod(\n        getattr(client, operation_name)\n    )\n\n    # Create a new wait method that will serve as a proxy to the underlying\n    # Waiter.wait method. This is needed to attach a docstring to the\n    # method.\n    async def wait(self, **kwargs):\n        return await AIOWaiter.wait(self, **kwargs)\n\n    wait.__doc__ = WaiterDocstring(\n        waiter_name=waiter_name,\n        event_emitter=client.meta.events,\n        service_model=client.meta.service_model,\n        service_waiter_model=waiter_model,\n        include_signature=False,\n    )\n\n    # Rename the waiter class based on the type of waiter.\n    waiter_class_name = str(\n        '%s.Waiter.%s'\n        % (get_service_module_name(client.meta.service_model), waiter_name)\n    )\n\n    # Create the new waiter class\n    documented_waiter_cls = type(waiter_class_name, (Waiter,), {'wait': wait})\n\n    # Return an instance of the new waiter class.\n    return documented_waiter_cls(\n        waiter_name, single_waiter_config, operation_method\n    )\n\n\nclass NormalizedOperationMethod(_NormalizedOperationMethod):\n    async def __call__(self, **kwargs):\n        try:\n            return await self._client_method(**kwargs)\n        except ClientError as e:\n            return e.response\n\n\nclass AIOWaiter(Waiter):\n    async def wait(self, **kwargs):\n        acceptors = list(self.config.acceptors)\n        current_state = 'waiting'\n        # pop the invocation specific config\n        config = kwargs.pop('WaiterConfig', {})\n        sleep_amount = config.get('Delay', self.config.delay)\n        max_attempts = config.get('MaxAttempts', self.config.max_attempts)\n        last_matched_acceptor = None\n        num_attempts = 0\n\n        while True:\n            response = await self._operation_method(**kwargs)\n            num_attempts += 1\n            for acceptor in acceptors:\n                if acceptor.matcher_func(response):\n                    last_matched_acceptor = acceptor\n                    current_state = acceptor.state\n                    break\n            else:\n                # If none of the acceptors matched, we should\n                # transition to the failure state if an error\n                # response was received.\n                if is_valid_waiter_error(response):\n                    # Transition to a failure state, which we\n                    # can just handle here by raising an exception.\n                    raise WaiterError(\n                        name=self.name,\n                        reason='An error occurred (%s): %s'\n                        % (\n                            response['Error'].get('Code', 'Unknown'),\n                            response['Error'].get('Message', 'Unknown'),\n                        ),\n                        last_response=response,\n                    )\n            if current_state == 'success':\n                logger.debug(\n                    \"Waiting complete, waiter matched the \" \"success state.\"\n                )\n                return response\n            if current_state == 'failure':\n                reason = 'Waiter encountered a terminal failure state: %s' % (\n                    acceptor.explanation\n                )\n                raise WaiterError(\n                    name=self.name,\n                    reason=reason,\n                    last_response=response,\n                )\n            if num_attempts >= max_attempts:\n                if last_matched_acceptor is None:\n                    reason = 'Max attempts exceeded'\n                else:\n                    reason = (\n                        'Max attempts exceeded. Previously accepted state: %s'\n                        % (acceptor.explanation)\n                    )\n                raise WaiterError(\n                    name=self.name,\n                    reason=reason,\n                    last_response=response,\n                )\n            await asyncio.sleep(sleep_amount)\n", "aiobotocore/credentials.py": "import asyncio\nimport datetime\nimport json\nimport logging\nimport os\nimport subprocess\nfrom copy import deepcopy\nfrom hashlib import sha1\n\nimport botocore.compat\nfrom botocore import UNSIGNED\nfrom botocore.compat import compat_shell_split\nfrom botocore.config import Config\nfrom botocore.credentials import (\n    _DEFAULT_ADVISORY_REFRESH_TIMEOUT,\n    AssumeRoleCredentialFetcher,\n    AssumeRoleProvider,\n    AssumeRoleWithWebIdentityProvider,\n    BaseAssumeRoleCredentialFetcher,\n    BotoProvider,\n    CachedCredentialFetcher,\n    CanonicalNameCredentialSourcer,\n    ConfigNotFound,\n    ConfigProvider,\n    ContainerMetadataFetcher,\n    ContainerProvider,\n    CredentialResolver,\n    CredentialRetrievalError,\n    Credentials,\n    EnvProvider,\n    InstanceMetadataProvider,\n    InvalidConfigError,\n    MetadataRetrievalError,\n    OriginalEC2Provider,\n    PartialCredentialsError,\n    ProcessProvider,\n    ProfileProviderBuilder,\n    ReadOnlyCredentials,\n    RefreshableCredentials,\n    RefreshWithMFAUnsupportedError,\n    SharedCredentialProvider,\n    SSOProvider,\n    SSOTokenLoader,\n    UnauthorizedSSOTokenError,\n    UnknownCredentialError,\n    _get_client_creator,\n    _local_now,\n    _parse_if_needed,\n    _serialize_if_needed,\n    parse,\n    resolve_imds_endpoint_mode,\n)\nfrom dateutil.tz import tzutc\n\nfrom aiobotocore._helpers import resolve_awaitable\nfrom aiobotocore.config import AioConfig\nfrom aiobotocore.tokens import AioSSOTokenProvider\nfrom aiobotocore.utils import (\n    AioContainerMetadataFetcher,\n    AioInstanceMetadataFetcher,\n)\n\nlogger = logging.getLogger(__name__)\n\n\ndef create_credential_resolver(session, cache=None, region_name=None):\n    \"\"\"Create a default credential resolver.\n    This creates a pre-configured credential resolver\n    that includes the default lookup chain for\n    credentials.\n    \"\"\"\n    profile_name = session.get_config_variable('profile') or 'default'\n    metadata_timeout = session.get_config_variable('metadata_service_timeout')\n    num_attempts = session.get_config_variable('metadata_service_num_attempts')\n    disable_env_vars = session.instance_variables().get('profile') is not None\n\n    imds_config = {\n        'ec2_metadata_service_endpoint': session.get_config_variable(\n            'ec2_metadata_service_endpoint'\n        ),\n        'ec2_metadata_service_endpoint_mode': resolve_imds_endpoint_mode(\n            session\n        ),\n        'ec2_credential_refresh_window': _DEFAULT_ADVISORY_REFRESH_TIMEOUT,\n        'ec2_metadata_v1_disabled': session.get_config_variable(\n            'ec2_metadata_v1_disabled'\n        ),\n    }\n\n    if cache is None:\n        cache = {}\n\n    env_provider = AioEnvProvider()\n    container_provider = AioContainerProvider()\n    instance_metadata_provider = AioInstanceMetadataProvider(\n        iam_role_fetcher=AioInstanceMetadataFetcher(\n            timeout=metadata_timeout,\n            num_attempts=num_attempts,\n            user_agent=session.user_agent(),\n            config=imds_config,\n        )\n    )\n\n    profile_provider_builder = AioProfileProviderBuilder(\n        session, cache=cache, region_name=region_name\n    )\n    assume_role_provider = AioAssumeRoleProvider(\n        load_config=lambda: session.full_config,\n        client_creator=_get_client_creator(session, region_name),\n        cache=cache,\n        profile_name=profile_name,\n        credential_sourcer=AioCanonicalNameCredentialSourcer(\n            [env_provider, container_provider, instance_metadata_provider]\n        ),\n        profile_provider_builder=profile_provider_builder,\n    )\n\n    pre_profile = [\n        env_provider,\n        assume_role_provider,\n    ]\n    profile_providers = profile_provider_builder.providers(\n        profile_name=profile_name,\n        disable_env_vars=disable_env_vars,\n    )\n    post_profile = [\n        AioOriginalEC2Provider(),\n        AioBotoProvider(),\n        container_provider,\n        instance_metadata_provider,\n    ]\n    providers = pre_profile + profile_providers + post_profile\n\n    if disable_env_vars:\n        # An explicitly provided profile will negate an EnvProvider.\n        # We will defer to providers that understand the \"profile\"\n        # concept to retrieve credentials.\n        # The one edge case if is all three values are provided via\n        # env vars:\n        # export AWS_ACCESS_KEY_ID=foo\n        # export AWS_SECRET_ACCESS_KEY=bar\n        # export AWS_PROFILE=baz\n        # Then, just like our client() calls, the explicit credentials\n        # will take precedence.\n        #\n        # This precedence is enforced by leaving the EnvProvider in the chain.\n        # This means that the only way a \"profile\" would win is if the\n        # EnvProvider does not return credentials, which is what we want\n        # in this scenario.\n        providers.remove(env_provider)\n        logger.debug(\n            'Skipping environment variable credential check'\n            ' because profile name was explicitly set.'\n        )\n\n    resolver = AioCredentialResolver(providers=providers)\n    return resolver\n\n\nclass AioProfileProviderBuilder(ProfileProviderBuilder):\n    def _create_process_provider(self, profile_name):\n        return AioProcessProvider(\n            profile_name=profile_name,\n            load_config=lambda: self._session.full_config,\n        )\n\n    def _create_shared_credential_provider(self, profile_name):\n        credential_file = self._session.get_config_variable('credentials_file')\n        return AioSharedCredentialProvider(\n            profile_name=profile_name,\n            creds_filename=credential_file,\n        )\n\n    def _create_config_provider(self, profile_name):\n        config_file = self._session.get_config_variable('config_file')\n        return AioConfigProvider(\n            profile_name=profile_name,\n            config_filename=config_file,\n        )\n\n    def _create_web_identity_provider(self, profile_name, disable_env_vars):\n        return AioAssumeRoleWithWebIdentityProvider(\n            load_config=lambda: self._session.full_config,\n            client_creator=_get_client_creator(\n                self._session, self._region_name\n            ),\n            cache=self._cache,\n            profile_name=profile_name,\n            disable_env_vars=disable_env_vars,\n        )\n\n    def _create_sso_provider(self, profile_name):\n        return AioSSOProvider(\n            load_config=lambda: self._session.full_config,\n            client_creator=self._session.create_client,\n            profile_name=profile_name,\n            cache=self._cache,\n            token_cache=self._sso_token_cache,\n            token_provider=AioSSOTokenProvider(\n                self._session,\n                cache=self._sso_token_cache,\n                profile_name=profile_name,\n            ),\n        )\n\n\nasync def get_credentials(session):\n    resolver = create_credential_resolver(session)\n    return await resolver.load_credentials()\n\n\ndef create_assume_role_refresher(client, params):\n    async def refresh():\n        async with client as sts:\n            response = await sts.assume_role(**params)\n        credentials = response['Credentials']\n        # We need to normalize the credential names to\n        # the values expected by the refresh creds.\n        return {\n            'access_key': credentials['AccessKeyId'],\n            'secret_key': credentials['SecretAccessKey'],\n            'token': credentials['SessionToken'],\n            'expiry_time': _serialize_if_needed(credentials['Expiration']),\n        }\n\n    return refresh\n\n\ndef create_mfa_serial_refresher(actual_refresh):\n    class _Refresher:\n        def __init__(self, refresh):\n            self._refresh = refresh\n            self._has_been_called = False\n\n        async def call(self):\n            if self._has_been_called:\n                # We can explore an option in the future to support\n                # reprompting for MFA, but for now we just error out\n                # when the temp creds expire.\n                raise RefreshWithMFAUnsupportedError()\n            self._has_been_called = True\n            return await self._refresh()\n\n    return _Refresher(actual_refresh).call\n\n\n# TODO: deprecate\ncreate_aio_mfa_serial_refresher = create_mfa_serial_refresher\n\n\nclass AioCredentials(Credentials):\n    async def get_frozen_credentials(self):\n        return ReadOnlyCredentials(\n            self.access_key, self.secret_key, self.token\n        )\n\n\nclass AioRefreshableCredentials(RefreshableCredentials):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._refresh_lock = asyncio.Lock()\n\n    # Redeclaring the properties so it doesn't call refresh\n    # Have to redeclare setter as we're overriding the getter\n    @property\n    def access_key(self):\n        # TODO: this needs to be resolved\n        raise NotImplementedError(\n            \"missing call to self._refresh. \"\n            \"Use get_frozen_credentials instead\"\n        )\n        return self._access_key\n\n    @access_key.setter\n    def access_key(self, value):\n        self._access_key = value\n\n    @property\n    def secret_key(self):\n        # TODO: this needs to be resolved\n        raise NotImplementedError(\n            \"missing call to self._refresh. \"\n            \"Use get_frozen_credentials instead\"\n        )\n        return self._secret_key\n\n    @secret_key.setter\n    def secret_key(self, value):\n        self._secret_key = value\n\n    @property\n    def token(self):\n        # TODO: this needs to be resolved\n        raise NotImplementedError(\n            \"missing call to self._refresh. \"\n            \"Use get_frozen_credentials instead\"\n        )\n        return self._token\n\n    @token.setter\n    def token(self, value):\n        self._token = value\n\n    async def _refresh(self):\n        if not self.refresh_needed(self._advisory_refresh_timeout):\n            return\n\n        # By this point we need a refresh but its not critical\n        if not self._refresh_lock.locked():\n            async with self._refresh_lock:\n                if not self.refresh_needed(self._advisory_refresh_timeout):\n                    return\n                is_mandatory_refresh = self.refresh_needed(\n                    self._mandatory_refresh_timeout\n                )\n                await self._protected_refresh(\n                    is_mandatory=is_mandatory_refresh\n                )\n                return\n        elif self.refresh_needed(self._mandatory_refresh_timeout):\n            # If we're here, we absolutely need a refresh and the\n            # lock is held so wait for it\n            async with self._refresh_lock:\n                # Might have refreshed by now\n                if not self.refresh_needed(self._mandatory_refresh_timeout):\n                    return\n                await self._protected_refresh(is_mandatory=True)\n\n    async def _protected_refresh(self, is_mandatory):\n        try:\n            # AioEnvProvider._create_credentials_fetcher is not and does not need async\n            metadata = await resolve_awaitable(self._refresh_using())\n        except Exception:\n            period_name = 'mandatory' if is_mandatory else 'advisory'\n            logger.warning(\n                \"Refreshing temporary credentials failed \"\n                \"during %s refresh period.\",\n                period_name,\n                exc_info=True,\n            )\n            if is_mandatory:\n                # If this is a mandatory refresh, then\n                # all errors that occur when we attempt to refresh\n                # credentials are propagated back to the user.\n                raise\n            # Otherwise we'll just return.\n            # The end result will be that we'll use the current\n            # set of temporary credentials we have.\n            return\n        self._set_from_data(metadata)\n        self._frozen_credentials = ReadOnlyCredentials(\n            self._access_key, self._secret_key, self._token\n        )\n        if self._is_expired():\n            msg = (\n                \"Credentials were refreshed, but the \"\n                \"refreshed credentials are still expired.\"\n            )\n            logger.warning(msg)\n            raise RuntimeError(msg)\n\n    async def get_frozen_credentials(self):\n        await self._refresh()\n        return self._frozen_credentials\n\n\nclass AioDeferredRefreshableCredentials(AioRefreshableCredentials):\n    def __init__(self, refresh_using, method, time_fetcher=_local_now):\n        self._refresh_using = refresh_using\n        self._access_key = None\n        self._secret_key = None\n        self._token = None\n        self._expiry_time = None\n        self._time_fetcher = time_fetcher\n        self._refresh_lock = asyncio.Lock()\n        self.method = method\n        self._frozen_credentials = None\n\n    def refresh_needed(self, refresh_in=None):\n        if self._frozen_credentials is None:\n            return True\n        return super().refresh_needed(refresh_in)\n\n\nclass AioCachedCredentialFetcher(CachedCredentialFetcher):\n    async def _get_credentials(self):\n        raise NotImplementedError('_get_credentials()')\n\n    async def fetch_credentials(self):\n        return await self._get_cached_credentials()\n\n    async def _get_cached_credentials(self):\n        \"\"\"Get up-to-date credentials.\n\n        This will check the cache for up-to-date credentials, calling assume\n        role if none are available.\n        \"\"\"\n        response = self._load_from_cache()\n        if response is None:\n            response = await self._get_credentials()\n            self._write_to_cache(response)\n        else:\n            logger.debug(\"Credentials for role retrieved from cache.\")\n\n        creds = response['Credentials']\n        expiration = _serialize_if_needed(creds['Expiration'], iso=True)\n        return {\n            'access_key': creds['AccessKeyId'],\n            'secret_key': creds['SecretAccessKey'],\n            'token': creds['SessionToken'],\n            'expiry_time': expiration,\n        }\n\n\nclass AioBaseAssumeRoleCredentialFetcher(\n    BaseAssumeRoleCredentialFetcher, AioCachedCredentialFetcher\n):\n    pass\n\n\nclass AioAssumeRoleCredentialFetcher(\n    AssumeRoleCredentialFetcher, AioBaseAssumeRoleCredentialFetcher\n):\n    async def _get_credentials(self):\n        \"\"\"Get credentials by calling assume role.\"\"\"\n        kwargs = self._assume_role_kwargs()\n        client = await self._create_client()\n        async with client as sts:\n            return await sts.assume_role(**kwargs)\n\n    async def _create_client(self):\n        \"\"\"Create an STS client using the source credentials.\"\"\"\n        frozen_credentials = (\n            await self._source_credentials.get_frozen_credentials()\n        )\n        return self._client_creator(\n            'sts',\n            aws_access_key_id=frozen_credentials.access_key,\n            aws_secret_access_key=frozen_credentials.secret_key,\n            aws_session_token=frozen_credentials.token,\n        )\n\n\nclass AioAssumeRoleWithWebIdentityCredentialFetcher(\n    AioBaseAssumeRoleCredentialFetcher\n):\n    def __init__(\n        self,\n        client_creator,\n        web_identity_token_loader,\n        role_arn,\n        extra_args=None,\n        cache=None,\n        expiry_window_seconds=None,\n    ):\n        self._web_identity_token_loader = web_identity_token_loader\n\n        super().__init__(\n            client_creator,\n            role_arn,\n            extra_args=extra_args,\n            cache=cache,\n            expiry_window_seconds=expiry_window_seconds,\n        )\n\n    async def _get_credentials(self):\n        \"\"\"Get credentials by calling assume role.\"\"\"\n        kwargs = self._assume_role_kwargs()\n        # Assume role with web identity does not require credentials other than\n        # the token, explicitly configure the client to not sign requests.\n        config = AioConfig(signature_version=UNSIGNED)\n        async with self._client_creator('sts', config=config) as client:\n            return await client.assume_role_with_web_identity(**kwargs)\n\n    def _assume_role_kwargs(self):\n        \"\"\"Get the arguments for assume role based on current configuration.\"\"\"\n        assume_role_kwargs = deepcopy(self._assume_kwargs)\n        identity_token = self._web_identity_token_loader()\n        assume_role_kwargs['WebIdentityToken'] = identity_token\n\n        return assume_role_kwargs\n\n\nclass AioProcessProvider(ProcessProvider):\n    def __init__(self, *args, popen=asyncio.create_subprocess_exec, **kwargs):\n        super().__init__(*args, **kwargs, popen=popen)\n\n    async def load(self):\n        credential_process = self._credential_process\n        if credential_process is None:\n            return\n\n        creds_dict = await self._retrieve_credentials_using(credential_process)\n        if creds_dict.get('expiry_time') is not None:\n            return AioRefreshableCredentials.create_from_metadata(\n                creds_dict,\n                lambda: self._retrieve_credentials_using(credential_process),\n                self.METHOD,\n            )\n\n        return AioCredentials(\n            access_key=creds_dict['access_key'],\n            secret_key=creds_dict['secret_key'],\n            token=creds_dict.get('token'),\n            method=self.METHOD,\n        )\n\n    async def _retrieve_credentials_using(self, credential_process):\n        # We're not using shell=True, so we need to pass the\n        # command and all arguments as a list.\n        process_list = compat_shell_split(credential_process)\n        p = await self._popen(\n            *process_list, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n        )\n        stdout, stderr = await p.communicate()\n        if p.returncode != 0:\n            raise CredentialRetrievalError(\n                provider=self.METHOD, error_msg=stderr.decode('utf-8')\n            )\n        parsed = botocore.compat.json.loads(stdout.decode('utf-8'))\n        version = parsed.get('Version', '<Version key not provided>')\n        if version != 1:\n            raise CredentialRetrievalError(\n                provider=self.METHOD,\n                error_msg=(\n                    f\"Unsupported version '{version}' for credential process \"\n                    f\"provider, supported versions: 1\"\n                ),\n            )\n        try:\n            return {\n                'access_key': parsed['AccessKeyId'],\n                'secret_key': parsed['SecretAccessKey'],\n                'token': parsed.get('SessionToken'),\n                'expiry_time': parsed.get('Expiration'),\n            }\n        except KeyError as e:\n            raise CredentialRetrievalError(\n                provider=self.METHOD,\n                error_msg=f\"Missing required key in response: {e}\",\n            )\n\n\nclass AioInstanceMetadataProvider(InstanceMetadataProvider):\n    async def load(self):\n        fetcher = self._role_fetcher\n        metadata = await fetcher.retrieve_iam_role_credentials()\n        if not metadata:\n            return None\n        logger.info(\n            'Found credentials from IAM Role: %s', metadata['role_name']\n        )\n\n        creds = AioRefreshableCredentials.create_from_metadata(\n            metadata,\n            method=self.METHOD,\n            refresh_using=fetcher.retrieve_iam_role_credentials,\n        )\n        return creds\n\n\nclass AioEnvProvider(EnvProvider):\n    async def load(self):\n        access_key = self.environ.get(self._mapping['access_key'], '')\n\n        if access_key:\n            logger.info('Found credentials in environment variables.')\n            fetcher = self._create_credentials_fetcher()\n            credentials = fetcher(require_expiry=False)\n\n            expiry_time = credentials['expiry_time']\n            if expiry_time is not None:\n                expiry_time = parse(expiry_time)\n                return AioRefreshableCredentials(\n                    credentials['access_key'],\n                    credentials['secret_key'],\n                    credentials['token'],\n                    expiry_time,\n                    refresh_using=fetcher,\n                    method=self.METHOD,\n                )\n\n            return AioCredentials(\n                credentials['access_key'],\n                credentials['secret_key'],\n                credentials['token'],\n                method=self.METHOD,\n            )\n        else:\n            return None\n\n\nclass AioOriginalEC2Provider(OriginalEC2Provider):\n    async def load(self):\n        if 'AWS_CREDENTIAL_FILE' in self._environ:\n            full_path = os.path.expanduser(\n                self._environ['AWS_CREDENTIAL_FILE']\n            )\n            creds = self._parser(full_path)\n            if self.ACCESS_KEY in creds:\n                logger.info('Found credentials in AWS_CREDENTIAL_FILE.')\n                access_key = creds[self.ACCESS_KEY]\n                secret_key = creds[self.SECRET_KEY]\n                # EC2 creds file doesn't support session tokens.\n                return AioCredentials(\n                    access_key, secret_key, method=self.METHOD\n                )\n        else:\n            return None\n\n\nclass AioSharedCredentialProvider(SharedCredentialProvider):\n    async def load(self):\n        try:\n            available_creds = self._ini_parser(self._creds_filename)\n        except ConfigNotFound:\n            return None\n        if self._profile_name in available_creds:\n            config = available_creds[self._profile_name]\n            if self.ACCESS_KEY in config:\n                logger.info(\n                    \"Found credentials in shared credentials file: %s\",\n                    self._creds_filename,\n                )\n                access_key, secret_key = self._extract_creds_from_mapping(\n                    config, self.ACCESS_KEY, self.SECRET_KEY\n                )\n                token = self._get_session_token(config)\n                return AioCredentials(\n                    access_key, secret_key, token, method=self.METHOD\n                )\n\n\nclass AioConfigProvider(ConfigProvider):\n    async def load(self):\n        try:\n            full_config = self._config_parser(self._config_filename)\n        except ConfigNotFound:\n            return None\n        if self._profile_name in full_config['profiles']:\n            profile_config = full_config['profiles'][self._profile_name]\n            if self.ACCESS_KEY in profile_config:\n                logger.info(\n                    \"Credentials found in config file: %s\",\n                    self._config_filename,\n                )\n                access_key, secret_key = self._extract_creds_from_mapping(\n                    profile_config, self.ACCESS_KEY, self.SECRET_KEY\n                )\n                token = self._get_session_token(profile_config)\n                return AioCredentials(\n                    access_key, secret_key, token, method=self.METHOD\n                )\n        else:\n            return None\n\n\nclass AioBotoProvider(BotoProvider):\n    async def load(self):\n        if self.BOTO_CONFIG_ENV in self._environ:\n            potential_locations = [self._environ[self.BOTO_CONFIG_ENV]]\n        else:\n            potential_locations = self.DEFAULT_CONFIG_FILENAMES\n        for filename in potential_locations:\n            try:\n                config = self._ini_parser(filename)\n            except ConfigNotFound:\n                # Move on to the next potential config file name.\n                continue\n            if 'Credentials' in config:\n                credentials = config['Credentials']\n                if self.ACCESS_KEY in credentials:\n                    logger.info(\n                        \"Found credentials in boto config file: %s\", filename\n                    )\n                    access_key, secret_key = self._extract_creds_from_mapping(\n                        credentials, self.ACCESS_KEY, self.SECRET_KEY\n                    )\n                    return AioCredentials(\n                        access_key, secret_key, method=self.METHOD\n                    )\n\n\nclass AioAssumeRoleProvider(AssumeRoleProvider):\n    async def load(self):\n        self._loaded_config = self._load_config()\n        profiles = self._loaded_config.get('profiles', {})\n        profile = profiles.get(self._profile_name, {})\n        if self._has_assume_role_config_vars(profile):\n            return await self._load_creds_via_assume_role(self._profile_name)\n\n    async def _load_creds_via_assume_role(self, profile_name):\n        role_config = self._get_role_config(profile_name)\n        source_credentials = await self._resolve_source_credentials(\n            role_config, profile_name\n        )\n\n        extra_args = {}\n        role_session_name = role_config.get('role_session_name')\n        if role_session_name is not None:\n            extra_args['RoleSessionName'] = role_session_name\n\n        external_id = role_config.get('external_id')\n        if external_id is not None:\n            extra_args['ExternalId'] = external_id\n\n        mfa_serial = role_config.get('mfa_serial')\n        if mfa_serial is not None:\n            extra_args['SerialNumber'] = mfa_serial\n\n        duration_seconds = role_config.get('duration_seconds')\n        if duration_seconds is not None:\n            extra_args['DurationSeconds'] = duration_seconds\n\n        fetcher = AioAssumeRoleCredentialFetcher(\n            client_creator=self._client_creator,\n            source_credentials=source_credentials,\n            role_arn=role_config['role_arn'],\n            extra_args=extra_args,\n            mfa_prompter=self._prompter,\n            cache=self.cache,\n        )\n        refresher = fetcher.fetch_credentials\n        if mfa_serial is not None:\n            refresher = create_mfa_serial_refresher(refresher)\n\n        # The initial credentials are empty and the expiration time is set\n        # to now so that we can delay the call to assume role until it is\n        # strictly needed.\n        return AioDeferredRefreshableCredentials(\n            method=self.METHOD,\n            refresh_using=refresher,\n            time_fetcher=_local_now,\n        )\n\n    async def _resolve_source_credentials(self, role_config, profile_name):\n        credential_source = role_config.get('credential_source')\n        if credential_source is not None:\n            return await self._resolve_credentials_from_source(\n                credential_source, profile_name\n            )\n\n        source_profile = role_config['source_profile']\n        self._visited_profiles.append(source_profile)\n        return await self._resolve_credentials_from_profile(source_profile)\n\n    async def _resolve_credentials_from_profile(self, profile_name):\n        profiles = self._loaded_config.get('profiles', {})\n        profile = profiles[profile_name]\n\n        if (\n            self._has_static_credentials(profile)\n            and not self._profile_provider_builder\n        ):\n            return self._resolve_static_credentials_from_profile(profile)\n        elif self._has_static_credentials(\n            profile\n        ) or not self._has_assume_role_config_vars(profile):\n            profile_providers = self._profile_provider_builder.providers(\n                profile_name=profile_name,\n                disable_env_vars=True,\n            )\n            profile_chain = AioCredentialResolver(profile_providers)\n            credentials = await profile_chain.load_credentials()\n            if credentials is None:\n                error_message = (\n                    'The source profile \"%s\" must have credentials.'\n                )\n                raise InvalidConfigError(\n                    error_msg=error_message % profile_name,\n                )\n            return credentials\n\n        return await self._load_creds_via_assume_role(profile_name)\n\n    def _resolve_static_credentials_from_profile(self, profile):\n        try:\n            return AioCredentials(\n                access_key=profile['aws_access_key_id'],\n                secret_key=profile['aws_secret_access_key'],\n                token=profile.get('aws_session_token'),\n            )\n        except KeyError as e:\n            raise PartialCredentialsError(\n                provider=self.METHOD, cred_var=str(e)\n            )\n\n    async def _resolve_credentials_from_source(\n        self, credential_source, profile_name\n    ):\n        credentials = await self._credential_sourcer.source_credentials(\n            credential_source\n        )\n        if credentials is None:\n            raise CredentialRetrievalError(\n                provider=credential_source,\n                error_msg=(\n                    'No credentials found in credential_source referenced '\n                    'in profile %s' % profile_name\n                ),\n            )\n        return credentials\n\n\nclass AioAssumeRoleWithWebIdentityProvider(AssumeRoleWithWebIdentityProvider):\n    async def load(self):\n        return await self._assume_role_with_web_identity()\n\n    async def _assume_role_with_web_identity(self):\n        token_path = self._get_config('web_identity_token_file')\n        if not token_path:\n            return None\n        token_loader = self._token_loader_cls(token_path)\n\n        role_arn = self._get_config('role_arn')\n        if not role_arn:\n            error_msg = (\n                'The provided profile or the current environment is '\n                'configured to assume role with web identity but has no '\n                'role ARN configured. Ensure that the profile has the role_arn'\n                'configuration set or the AWS_ROLE_ARN env var is set.'\n            )\n            raise InvalidConfigError(error_msg=error_msg)\n\n        extra_args = {}\n        role_session_name = self._get_config('role_session_name')\n        if role_session_name is not None:\n            extra_args['RoleSessionName'] = role_session_name\n\n        fetcher = AioAssumeRoleWithWebIdentityCredentialFetcher(\n            client_creator=self._client_creator,\n            web_identity_token_loader=token_loader,\n            role_arn=role_arn,\n            extra_args=extra_args,\n            cache=self.cache,\n        )\n        # The initial credentials are empty and the expiration time is set\n        # to now so that we can delay the call to assume role until it is\n        # strictly needed.\n        return AioDeferredRefreshableCredentials(\n            method=self.METHOD,\n            refresh_using=fetcher.fetch_credentials,\n        )\n\n\nclass AioCanonicalNameCredentialSourcer(CanonicalNameCredentialSourcer):\n    async def source_credentials(self, source_name):\n        \"\"\"Loads source credentials based on the provided configuration.\n\n        :type source_name: str\n        :param source_name: The value of credential_source in the config\n            file. This is the canonical name of the credential provider.\n\n        :rtype: Credentials\n        \"\"\"\n        source = self._get_provider(source_name)\n        if isinstance(source, AioCredentialResolver):\n            return await source.load_credentials()\n        return await source.load()\n\n    def _get_provider(self, canonical_name):\n        \"\"\"Return a credential provider by its canonical name.\n\n        :type canonical_name: str\n        :param canonical_name: The canonical name of the provider.\n\n        :raises UnknownCredentialError: Raised if no\n            credential provider by the provided name\n            is found.\n        \"\"\"\n        provider = self._get_provider_by_canonical_name(canonical_name)\n\n        # The AssumeRole provider should really be part of the SharedConfig\n        # provider rather than being its own thing, but it is not. It is\n        # effectively part of both the SharedConfig provider and the\n        # SharedCredentials provider now due to the way it behaves.\n        # Therefore if we want either of those providers we should return\n        # the AssumeRole provider with it.\n        if canonical_name.lower() in ['sharedconfig', 'sharedcredentials']:\n            assume_role_provider = self._get_provider_by_method('assume-role')\n            if assume_role_provider is not None:\n                # The SharedConfig or SharedCredentials provider may not be\n                # present if it was removed for some reason, but the\n                # AssumeRole provider could still be present. In that case,\n                # return the assume role provider by itself.\n                if provider is None:\n                    return assume_role_provider\n\n                # If both are present, return them both as a\n                # CredentialResolver so that calling code can treat them as\n                # a single entity.\n                return AioCredentialResolver([assume_role_provider, provider])\n\n        if provider is None:\n            raise UnknownCredentialError(name=canonical_name)\n\n        return provider\n\n\nclass AioContainerProvider(ContainerProvider):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # This will always run if no fetcher arg is provided\n        if isinstance(self._fetcher, ContainerMetadataFetcher):\n            self._fetcher = AioContainerMetadataFetcher()\n\n    async def load(self):\n        if self.ENV_VAR in self._environ or self.ENV_VAR_FULL in self._environ:\n            return await self._retrieve_or_fail()\n\n    async def _retrieve_or_fail(self):\n        if self._provided_relative_uri():\n            full_uri = self._fetcher.full_url(self._environ[self.ENV_VAR])\n        else:\n            full_uri = self._environ[self.ENV_VAR_FULL]\n        fetcher = self._create_fetcher(full_uri)\n        creds = await fetcher()\n        return AioRefreshableCredentials(\n            access_key=creds['access_key'],\n            secret_key=creds['secret_key'],\n            token=creds['token'],\n            method=self.METHOD,\n            expiry_time=_parse_if_needed(creds['expiry_time']),\n            refresh_using=fetcher,\n        )\n\n    def _create_fetcher(self, full_uri, *args, **kwargs):\n        async def fetch_creds():\n            try:\n                headers = self._build_headers()\n                response = await self._fetcher.retrieve_full_uri(\n                    full_uri, headers=headers\n                )\n            except MetadataRetrievalError as e:\n                logger.debug(\n                    \"Error retrieving container metadata: %s\", e, exc_info=True\n                )\n                raise CredentialRetrievalError(\n                    provider=self.METHOD, error_msg=str(e)\n                )\n            return {\n                'access_key': response['AccessKeyId'],\n                'secret_key': response['SecretAccessKey'],\n                'token': response['Token'],\n                'expiry_time': response['Expiration'],\n            }\n\n        return fetch_creds\n\n\nclass AioCredentialResolver(CredentialResolver):\n    async def load_credentials(self):\n        \"\"\"\n        Goes through the credentials chain, returning the first ``Credentials``\n        that could be loaded.\n        \"\"\"\n        # First provider to return a non-None response wins.\n        for provider in self.providers:\n            logger.debug(\"Looking for credentials via: %s\", provider.METHOD)\n            creds = await provider.load()\n            if creds is not None:\n                return creds\n\n        # If we got here, no credentials could be found.\n        # This feels like it should be an exception, but historically, ``None``\n        # is returned.\n        #\n        # +1\n        # -js\n        return None\n\n\nclass AioSSOCredentialFetcher(AioCachedCredentialFetcher):\n    _UTC_DATE_FORMAT = '%Y-%m-%dT%H:%M:%SZ'\n\n    def __init__(\n        self,\n        start_url,\n        sso_region,\n        role_name,\n        account_id,\n        client_creator,\n        token_loader=None,\n        cache=None,\n        expiry_window_seconds=None,\n        token_provider=None,\n        sso_session_name=None,\n    ):\n        self._client_creator = client_creator\n        self._sso_region = sso_region\n        self._role_name = role_name\n        self._account_id = account_id\n        self._start_url = start_url\n        self._token_loader = token_loader\n        self._token_provider = token_provider\n        self._sso_session_name = sso_session_name\n        super().__init__(cache, expiry_window_seconds)\n\n    def _create_cache_key(self):\n        args = {\n            'roleName': self._role_name,\n            'accountId': self._account_id,\n        }\n        if self._sso_session_name:\n            args['sessionName'] = self._sso_session_name\n        else:\n            args['startUrl'] = self._start_url\n\n        args = json.dumps(args, sort_keys=True, separators=(',', ':'))\n        argument_hash = sha1(args.encode('utf-8')).hexdigest()\n        return self._make_file_safe(argument_hash)\n\n    def _parse_timestamp(self, timestamp_ms):\n        # fromtimestamp expects seconds so: milliseconds / 1000 = seconds\n        timestamp_seconds = timestamp_ms / 1000.0\n        timestamp = datetime.datetime.fromtimestamp(timestamp_seconds, tzutc())\n        return timestamp.strftime(self._UTC_DATE_FORMAT)\n\n    async def _get_credentials(self):\n        \"\"\"Get credentials by calling SSO get role credentials.\"\"\"\n        config = Config(\n            signature_version=UNSIGNED,\n            region_name=self._sso_region,\n        )\n        async with self._client_creator('sso', config=config) as client:\n            if self._token_provider:\n                initial_token_data = self._token_provider.load_token()\n                token = (await initial_token_data.get_frozen_token()).token\n            else:\n                token = self._token_loader(self._start_url)['accessToken']\n\n            kwargs = {\n                'roleName': self._role_name,\n                'accountId': self._account_id,\n                'accessToken': token,\n            }\n            try:\n                response = await client.get_role_credentials(**kwargs)\n            except client.exceptions.UnauthorizedException:\n                raise UnauthorizedSSOTokenError()\n            credentials = response['roleCredentials']\n\n            credentials = {\n                'ProviderType': 'sso',\n                'Credentials': {\n                    'AccessKeyId': credentials['accessKeyId'],\n                    'SecretAccessKey': credentials['secretAccessKey'],\n                    'SessionToken': credentials['sessionToken'],\n                    'Expiration': self._parse_timestamp(\n                        credentials['expiration']\n                    ),\n                },\n            }\n            return credentials\n\n\nclass AioSSOProvider(SSOProvider):\n    async def load(self):\n        sso_config = self._load_sso_config()\n        if not sso_config:\n            return None\n\n        fetcher_kwargs = {\n            'start_url': sso_config['sso_start_url'],\n            'sso_region': sso_config['sso_region'],\n            'role_name': sso_config['sso_role_name'],\n            'account_id': sso_config['sso_account_id'],\n            'client_creator': self._client_creator,\n            'token_loader': SSOTokenLoader(cache=self._token_cache),\n            'cache': self.cache,\n        }\n        if 'sso_session' in sso_config:\n            fetcher_kwargs['sso_session_name'] = sso_config['sso_session']\n            fetcher_kwargs['token_provider'] = self._token_provider\n\n        sso_fetcher = AioSSOCredentialFetcher(**fetcher_kwargs)\n\n        return AioDeferredRefreshableCredentials(\n            method=self.METHOD,\n            refresh_using=sso_fetcher.fetch_credentials,\n        )\n", "aiobotocore/configprovider.py": "from botocore.configprovider import SmartDefaultsConfigStoreFactory, os\n\n\nclass AioSmartDefaultsConfigStoreFactory(SmartDefaultsConfigStoreFactory):\n    async def merge_smart_defaults(self, config_store, mode, region_name):\n        if mode == 'auto':\n            mode = await self.resolve_auto_mode(region_name)\n        default_configs = (\n            self._default_config_resolver.get_default_config_values(mode)\n        )\n        for config_var in default_configs:\n            config_value = default_configs[config_var]\n            method = getattr(self, f'_set_{config_var}', None)\n            if method:\n                method(config_store, config_value)\n\n    async def resolve_auto_mode(self, region_name):\n        current_region = None\n        if os.environ.get('AWS_EXECUTION_ENV'):\n            default_region = os.environ.get('AWS_DEFAULT_REGION')\n            current_region = os.environ.get('AWS_REGION', default_region)\n        if not current_region:\n            if self._instance_metadata_region:\n                current_region = self._instance_metadata_region\n            else:\n                try:\n                    current_region = await self._imds_region_provider.provide()\n                    self._instance_metadata_region = current_region\n                except Exception:\n                    pass\n\n        if current_region:\n            if region_name == current_region:\n                return 'in-region'\n            else:\n                return 'cross-region'\n        return 'standard'\n", "aiobotocore/client.py": "from botocore.awsrequest import prepare_request_dict\nfrom botocore.client import (\n    BaseClient,\n    ClientCreator,\n    ClientEndpointBridge,\n    PaginatorDocstring,\n    logger,\n    resolve_checksum_context,\n)\nfrom botocore.compress import maybe_compress_request\nfrom botocore.discovery import block_endpoint_discovery_required_operations\nfrom botocore.exceptions import OperationNotPageableError, UnknownServiceError\nfrom botocore.history import get_global_history_recorder\nfrom botocore.hooks import first_non_none_response\nfrom botocore.utils import get_service_module_name\nfrom botocore.waiter import xform_name\n\nfrom . import waiter\nfrom .args import AioClientArgsCreator\nfrom .credentials import AioRefreshableCredentials\nfrom .discovery import AioEndpointDiscoveryHandler, AioEndpointDiscoveryManager\nfrom .httpchecksum import apply_request_checksum\nfrom .paginate import AioPaginator\nfrom .retries import adaptive, standard\nfrom .utils import AioS3ExpressIdentityResolver, AioS3RegionRedirectorv2\n\nhistory_recorder = get_global_history_recorder()\n\n\nclass AioClientCreator(ClientCreator):\n    async def create_client(\n        self,\n        service_name,\n        region_name,\n        is_secure=True,\n        endpoint_url=None,\n        verify=None,\n        credentials=None,\n        scoped_config=None,\n        api_version=None,\n        client_config=None,\n        auth_token=None,\n    ):\n        responses = await self._event_emitter.emit(\n            'choose-service-name', service_name=service_name\n        )\n        service_name = first_non_none_response(responses, default=service_name)\n        service_model = self._load_service_model(service_name, api_version)\n        try:\n            endpoints_ruleset_data = self._load_service_endpoints_ruleset(\n                service_name, api_version\n            )\n            partition_data = self._loader.load_data('partitions')\n        except UnknownServiceError:\n            endpoints_ruleset_data = None\n            partition_data = None\n            logger.info(\n                'No endpoints ruleset found for service %s, falling back to '\n                'legacy endpoint routing.',\n                service_name,\n            )\n\n        cls = await self._create_client_class(service_name, service_model)\n        region_name, client_config = self._normalize_fips_region(\n            region_name, client_config\n        )\n        endpoint_bridge = ClientEndpointBridge(\n            self._endpoint_resolver,\n            scoped_config,\n            client_config,\n            service_signing_name=service_model.metadata.get('signingName'),\n            config_store=self._config_store,\n            service_signature_version=service_model.metadata.get(\n                'signatureVersion'\n            ),\n        )\n        client_args = self._get_client_args(\n            service_model,\n            region_name,\n            is_secure,\n            endpoint_url,\n            verify,\n            credentials,\n            scoped_config,\n            client_config,\n            endpoint_bridge,\n            auth_token,\n            endpoints_ruleset_data,\n            partition_data,\n        )\n        service_client = cls(**client_args)\n        self._register_retries(service_client)\n        self._register_s3_events(\n            client=service_client,\n            endpoint_bridge=None,\n            endpoint_url=None,\n            client_config=client_config,\n            scoped_config=scoped_config,\n        )\n        self._register_s3express_events(client=service_client)\n        self._register_s3_control_events(client=service_client)\n        self._register_endpoint_discovery(\n            service_client, endpoint_url, client_config\n        )\n        return service_client\n\n    async def _create_client_class(self, service_name, service_model):\n        class_attributes = self._create_methods(service_model)\n        py_name_to_operation_name = self._create_name_mapping(service_model)\n        class_attributes['_PY_TO_OP_NAME'] = py_name_to_operation_name\n        bases = [AioBaseClient]\n        service_id = service_model.service_id.hyphenize()\n        await self._event_emitter.emit(\n            'creating-client-class.%s' % service_id,\n            class_attributes=class_attributes,\n            base_classes=bases,\n        )\n        class_name = get_service_module_name(service_model)\n        cls = type(str(class_name), tuple(bases), class_attributes)\n        return cls\n\n    def _register_retries(self, client):\n        # botocore retry handlers may block. We add our own implementation here.\n        # botocore provides three implementations:\n        #\n        # 1) standard\n        # This one doesn't block. A threading.Lock is used in quota.RetryQuota,\n        # but it's only used to protect concurrent modifications of internal\n        # state inside multithreaded programs. When running under a single\n        # asyncio thread, this lock will be acquired and released in the same\n        # coroutine, and the coroutine will never block waiting for the lock.\n        # Thus, we don't need to redefine this strategy.\n        #\n        # 2) adaptive\n        # This one blocks when the client is applying self rate limiting.\n        # We override the corresponding definition to replace it with async\n        # objects.\n        #\n        # 3) legacy\n        # This one probably doesn't block.\n        #\n        # The code for this method comes directly from botocore. We could\n        # override `_register_v2_adaptive_retries` only. The override for\n        # `_register_retries` is only included for clarity.\n        retry_mode = client.meta.config.retries['mode']\n        if retry_mode == 'standard':\n            self._register_v2_standard_retries(client)\n        elif retry_mode == 'adaptive':\n            self._register_v2_standard_retries(client)\n            self._register_v2_adaptive_retries(client)\n        elif retry_mode == 'legacy':\n            self._register_legacy_retries(client)\n\n    def _register_v2_standard_retries(self, client):\n        max_attempts = client.meta.config.retries.get('total_max_attempts')\n        kwargs = {'client': client}\n        if max_attempts is not None:\n            kwargs['max_attempts'] = max_attempts\n        standard.register_retry_handler(**kwargs)\n\n    def _register_v2_adaptive_retries(self, client):\n        # See comment in `_register_retries`.\n        # Note that this `adaptive` module is an aiobotocore reimplementation.\n        adaptive.register_retry_handler(client)\n\n    def _register_legacy_retries(self, client):\n        endpoint_prefix = client.meta.service_model.endpoint_prefix\n        service_id = client.meta.service_model.service_id\n        service_event_name = service_id.hyphenize()\n\n        # First, we load the entire retry config for all services,\n        # then pull out just the information we need.\n        original_config = self._loader.load_data('_retry')\n        if not original_config:\n            return\n\n        retries = self._transform_legacy_retries(client.meta.config.retries)\n        retry_config = self._retry_config_translator.build_retry_config(\n            endpoint_prefix,\n            original_config.get('retry', {}),\n            original_config.get('definitions', {}),\n            retries,\n        )\n\n        logger.debug(\n            \"Registering retry handlers for service: %s\",\n            client.meta.service_model.service_name,\n        )\n        handler = self._retry_handler_factory.create_retry_handler(\n            retry_config, endpoint_prefix\n        )\n        unique_id = 'retry-config-%s' % service_event_name\n        client.meta.events.register(\n            f\"needs-retry.{service_event_name}\", handler, unique_id=unique_id\n        )\n\n    def _register_endpoint_discovery(self, client, endpoint_url, config):\n        if endpoint_url is not None:\n            # Don't register any handlers in the case of a custom endpoint url\n            return\n        # Only attach handlers if the service supports discovery\n        if client.meta.service_model.endpoint_discovery_operation is None:\n            return\n        events = client.meta.events\n        service_id = client.meta.service_model.service_id.hyphenize()\n        enabled = False\n        if config and config.endpoint_discovery_enabled is not None:\n            enabled = config.endpoint_discovery_enabled\n        elif self._config_store:\n            enabled = self._config_store.get_config_variable(\n                'endpoint_discovery_enabled'\n            )\n\n        enabled = self._normalize_endpoint_discovery_config(enabled)\n        if enabled and self._requires_endpoint_discovery(client, enabled):\n            discover = enabled is True\n            manager = AioEndpointDiscoveryManager(\n                client, always_discover=discover\n            )\n            handler = AioEndpointDiscoveryHandler(manager)\n            handler.register(events, service_id)\n        else:\n            events.register(\n                'before-parameter-build',\n                block_endpoint_discovery_required_operations,\n            )\n\n    def _register_s3express_events(\n        self,\n        client,\n        endpoint_bridge=None,\n        endpoint_url=None,\n        client_config=None,\n        scoped_config=None,\n    ):\n        if client.meta.service_model.service_name != 's3':\n            return\n        AioS3ExpressIdentityResolver(\n            client, AioRefreshableCredentials\n        ).register()\n\n    def _register_s3_events(\n        self,\n        client,\n        endpoint_bridge,\n        endpoint_url,\n        client_config,\n        scoped_config,\n    ):\n        if client.meta.service_model.service_name != 's3':\n            return\n        AioS3RegionRedirectorv2(None, client).register()\n        self._set_s3_presign_signature_version(\n            client.meta, client_config, scoped_config\n        )\n        client.meta.events.register(\n            'before-parameter-build.s3', self._inject_s3_input_parameters\n        )\n\n    def _get_client_args(\n        self,\n        service_model,\n        region_name,\n        is_secure,\n        endpoint_url,\n        verify,\n        credentials,\n        scoped_config,\n        client_config,\n        endpoint_bridge,\n        auth_token,\n        endpoints_ruleset_data,\n        partition_data,\n    ):\n        # This is a near copy of ClientCreator. What's replaced\n        # is ClientArgsCreator->AioClientArgsCreator\n        args_creator = AioClientArgsCreator(\n            self._event_emitter,\n            self._user_agent,\n            self._response_parser_factory,\n            self._loader,\n            self._exceptions_factory,\n            config_store=self._config_store,\n            user_agent_creator=self._user_agent_creator,\n        )\n        return args_creator.get_client_args(\n            service_model,\n            region_name,\n            is_secure,\n            endpoint_url,\n            verify,\n            credentials,\n            scoped_config,\n            client_config,\n            endpoint_bridge,\n            auth_token,\n            endpoints_ruleset_data,\n            partition_data,\n        )\n\n\nclass AioBaseClient(BaseClient):\n    async def _async_getattr(self, item):\n        event_name = 'getattr.{}.{}'.format(\n            self._service_model.service_id.hyphenize(), item\n        )\n        handler, event_response = await self.meta.events.emit_until_response(\n            event_name, client=self\n        )\n\n        return event_response\n\n    def __getattr__(self, item):\n        # NOTE: we can not reliably support this because if we were to make this a\n        # deferred attrgetter (See #803), it would resolve in hasattr always returning\n        # true.  This ends up breaking ddtrace for example when it tries to set a pin.\n        raise AttributeError(\n            \"'{}' object has no attribute '{}'\".format(\n                self.__class__.__name__, item\n            )\n        )\n\n    async def close(self):\n        \"\"\"Closes underlying endpoint connections.\"\"\"\n        await self._endpoint.close()\n\n    async def _make_api_call(self, operation_name, api_params):\n        operation_model = self._service_model.operation_model(operation_name)\n        service_name = self._service_model.service_name\n        history_recorder.record(\n            'API_CALL',\n            {\n                'service': service_name,\n                'operation': operation_name,\n                'params': api_params,\n            },\n        )\n        if operation_model.deprecated:\n            logger.debug(\n                'Warning: %s.%s() is deprecated', service_name, operation_name\n            )\n        request_context = {\n            'client_region': self.meta.region_name,\n            'client_config': self.meta.config,\n            'has_streaming_input': operation_model.has_streaming_input,\n            'auth_type': operation_model.auth_type,\n        }\n        api_params = await self._emit_api_params(\n            api_params=api_params,\n            operation_model=operation_model,\n            context=request_context,\n        )\n        (\n            endpoint_url,\n            additional_headers,\n            properties,\n        ) = await self._resolve_endpoint_ruleset(\n            operation_model, api_params, request_context\n        )\n        if properties:\n            # Pass arbitrary endpoint info with the Request\n            # for use during construction.\n            request_context['endpoint_properties'] = properties\n        request_dict = await self._convert_to_request_dict(\n            api_params=api_params,\n            operation_model=operation_model,\n            endpoint_url=endpoint_url,\n            context=request_context,\n            headers=additional_headers,\n        )\n        resolve_checksum_context(request_dict, operation_model, api_params)\n\n        service_id = self._service_model.service_id.hyphenize()\n        handler, event_response = await self.meta.events.emit_until_response(\n            'before-call.{service_id}.{operation_name}'.format(\n                service_id=service_id, operation_name=operation_name\n            ),\n            model=operation_model,\n            params=request_dict,\n            request_signer=self._request_signer,\n            context=request_context,\n        )\n\n        if event_response is not None:\n            http, parsed_response = event_response\n        else:\n            maybe_compress_request(\n                self.meta.config, request_dict, operation_model\n            )\n            apply_request_checksum(request_dict)\n            http, parsed_response = await self._make_request(\n                operation_model, request_dict, request_context\n            )\n\n        await self.meta.events.emit(\n            'after-call.{service_id}.{operation_name}'.format(\n                service_id=service_id, operation_name=operation_name\n            ),\n            http_response=http,\n            parsed=parsed_response,\n            model=operation_model,\n            context=request_context,\n        )\n\n        if http.status_code >= 300:\n            error_info = parsed_response.get(\"Error\", {})\n            error_code = error_info.get(\"QueryErrorCode\") or error_info.get(\n                \"Code\"\n            )\n            error_class = self.exceptions.from_code(error_code)\n            raise error_class(parsed_response, operation_name)\n        else:\n            return parsed_response\n\n    async def _make_request(\n        self, operation_model, request_dict, request_context\n    ):\n        try:\n            return await self._endpoint.make_request(\n                operation_model, request_dict\n            )\n        except Exception as e:\n            await self.meta.events.emit(\n                'after-call-error.{service_id}.{operation_name}'.format(\n                    service_id=self._service_model.service_id.hyphenize(),\n                    operation_name=operation_model.name,\n                ),\n                exception=e,\n                context=request_context,\n            )\n            raise\n\n    async def _convert_to_request_dict(\n        self,\n        api_params,\n        operation_model,\n        endpoint_url,\n        context=None,\n        headers=None,\n        set_user_agent_header=True,\n    ):\n        request_dict = self._serializer.serialize_to_request(\n            api_params, operation_model\n        )\n        if not self._client_config.inject_host_prefix:\n            request_dict.pop('host_prefix', None)\n        if headers is not None:\n            request_dict['headers'].update(headers)\n        if set_user_agent_header:\n            user_agent = self._user_agent_creator.to_string()\n        else:\n            user_agent = None\n        prepare_request_dict(\n            request_dict,\n            endpoint_url=endpoint_url,\n            user_agent=user_agent,\n            context=context,\n        )\n        return request_dict\n\n    async def _emit_api_params(self, api_params, operation_model, context):\n        # Given the API params provided by the user and the operation_model\n        # we can serialize the request to a request_dict.\n        operation_name = operation_model.name\n\n        # Emit an event that allows users to modify the parameters at the\n        # beginning of the method. It allows handlers to modify existing\n        # parameters or return a new set of parameters to use.\n        service_id = self._service_model.service_id.hyphenize()\n        responses = await self.meta.events.emit(\n            f'provide-client-params.{service_id}.{operation_name}',\n            params=api_params,\n            model=operation_model,\n            context=context,\n        )\n        api_params = first_non_none_response(responses, default=api_params)\n\n        await self.meta.events.emit(\n            f'before-parameter-build.{service_id}.{operation_name}',\n            params=api_params,\n            model=operation_model,\n            context=context,\n        )\n        return api_params\n\n    async def _resolve_endpoint_ruleset(\n        self,\n        operation_model,\n        params,\n        request_context,\n        ignore_signing_region=False,\n    ):\n        \"\"\"Returns endpoint URL and list of additional headers returned from\n        EndpointRulesetResolver for the given operation and params. If the\n        ruleset resolver is not available, for example because the service has\n        no endpoints ruleset file, the legacy endpoint resolver's value is\n        returned.\n\n        Use ignore_signing_region for generating presigned URLs or any other\n        situation where the signing region information from the ruleset\n        resolver should be ignored.\n\n        Returns tuple of URL and headers dictionary. Additionally, the\n        request_context dict is modified in place with any signing information\n        returned from the ruleset resolver.\n        \"\"\"\n        if self._ruleset_resolver is None:\n            endpoint_url = self.meta.endpoint_url\n            additional_headers = {}\n            endpoint_properties = {}\n        else:\n            endpoint_info = await self._ruleset_resolver.construct_endpoint(\n                operation_model=operation_model,\n                call_args=params,\n                request_context=request_context,\n            )\n            endpoint_url = endpoint_info.url\n            additional_headers = endpoint_info.headers\n            endpoint_properties = endpoint_info.properties\n            # If authSchemes is present, overwrite default auth type and\n            # signing context derived from service model.\n            auth_schemes = endpoint_info.properties.get('authSchemes')\n            if auth_schemes is not None:\n                auth_info = self._ruleset_resolver.auth_schemes_to_signing_ctx(\n                    auth_schemes\n                )\n                auth_type, signing_context = auth_info\n                request_context['auth_type'] = auth_type\n                if 'region' in signing_context and ignore_signing_region:\n                    del signing_context['region']\n                if 'signing' in request_context:\n                    request_context['signing'].update(signing_context)\n                else:\n                    request_context['signing'] = signing_context\n\n        return endpoint_url, additional_headers, endpoint_properties\n\n    def get_paginator(self, operation_name):\n        \"\"\"Create a paginator for an operation.\n\n        :type operation_name: string\n        :param operation_name: The operation name.  This is the same name\n            as the method name on the client.  For example, if the\n            method name is ``create_foo``, and you'd normally invoke the\n            operation as ``client.create_foo(**kwargs)``, if the\n            ``create_foo`` operation can be paginated, you can use the\n            call ``client.get_paginator(\"create_foo\")``.\n\n        :raise OperationNotPageableError: Raised if the operation is not\n            pageable.  You can use the ``client.can_paginate`` method to\n            check if an operation is pageable.\n\n        :rtype: ``botocore.paginate.Paginator``\n        :return: A paginator object.\n\n        \"\"\"\n        if not self.can_paginate(operation_name):\n            raise OperationNotPageableError(operation_name=operation_name)\n        else:\n            actual_operation_name = self._PY_TO_OP_NAME[operation_name]\n\n            # Create a new paginate method that will serve as a proxy to\n            # the underlying Paginator.paginate method. This is needed to\n            # attach a docstring to the method.\n            def paginate(self, **kwargs):\n                return AioPaginator.paginate(self, **kwargs)\n\n            paginator_config = self._cache['page_config'][\n                actual_operation_name\n            ]\n            # Add the docstring for the paginate method.\n            paginate.__doc__ = PaginatorDocstring(\n                paginator_name=actual_operation_name,\n                event_emitter=self.meta.events,\n                service_model=self.meta.service_model,\n                paginator_config=paginator_config,\n                include_signature=False,\n            )\n\n            # Rename the paginator class based on the type of paginator.\n            service_module_name = get_service_module_name(\n                self.meta.service_model\n            )\n            paginator_class_name = (\n                f\"{service_module_name}.Paginator.{actual_operation_name}\"\n            )\n\n            # Create the new paginator class\n            documented_paginator_cls = type(\n                paginator_class_name, (AioPaginator,), {'paginate': paginate}\n            )\n\n            operation_model = self._service_model.operation_model(\n                actual_operation_name\n            )\n            paginator = documented_paginator_cls(\n                getattr(self, operation_name),\n                paginator_config,\n                operation_model,\n            )\n            return paginator\n\n    # NOTE: this method does not differ from botocore, however it's important to keep\n    #   as the \"waiter\" value points to our own asyncio waiter module\n    def get_waiter(self, waiter_name):\n        \"\"\"Returns an object that can wait for some condition.\n\n        :type waiter_name: str\n        :param waiter_name: The name of the waiter to get. See the waiters\n            section of the service docs for a list of available waiters.\n\n        :returns: The specified waiter object.\n        :rtype: ``botocore.waiter.Waiter``\n        \"\"\"\n        config = self._get_waiter_config()\n        if not config:\n            raise ValueError(\"Waiter does not exist: %s\" % waiter_name)\n        model = waiter.WaiterModel(config)\n        mapping = {}\n        for name in model.waiter_names:\n            mapping[xform_name(name)] = name\n        if waiter_name not in mapping:\n            raise ValueError(\"Waiter does not exist: %s\" % waiter_name)\n\n        return waiter.create_waiter_with_client(\n            mapping[waiter_name], model, self\n        )\n\n    async def __aenter__(self):\n        await self._endpoint.http_session.__aenter__()\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        await self._endpoint.http_session.__aexit__(exc_type, exc_val, exc_tb)\n", "aiobotocore/handlers.py": "from botocore.handlers import (\n    ETree,\n    XMLParseError,\n    _get_cross_region_presigned_url,\n    _get_presigned_url_source_and_destination_regions,\n    logger,\n)\n\n\nasync def check_for_200_error(response, **kwargs):\n    # From: http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html\n    # There are two opportunities for a copy request to return an error. One\n    # can occur when Amazon S3 receives the copy request and the other can\n    # occur while Amazon S3 is copying the files. If the error occurs before\n    # the copy operation starts, you receive a standard Amazon S3 error. If the\n    # error occurs during the copy operation, the error response is embedded in\n    # the 200 OK response. This means that a 200 OK response can contain either\n    # a success or an error. Make sure to design your application to parse the\n    # contents of the response and handle it appropriately.\n    #\n    # So this handler checks for this case.  Even though the server sends a\n    # 200 response, conceptually this should be handled exactly like a\n    # 500 response (with respect to raising exceptions, retries, etc.)\n    # We're connected *before* all the other retry logic handlers, so as long\n    # as we switch the error code to 500, we'll retry the error as expected.\n    if response is None:\n        # A None response can happen if an exception is raised while\n        # trying to retrieve the response.  See Endpoint._get_response().\n        return\n    http_response, parsed = response\n    if await _looks_like_special_case_error(http_response):\n        logger.debug(\n            \"Error found for response with 200 status code, \"\n            \"errors: %s, changing status code to \"\n            \"500.\",\n            parsed,\n        )\n        http_response.status_code = 500\n\n\nasync def _looks_like_special_case_error(http_response):\n    if http_response.status_code == 200:\n        try:\n            parser = ETree.XMLParser(\n                target=ETree.TreeBuilder(), encoding='utf-8'\n            )\n            parser.feed(await http_response.content)\n            root = parser.close()\n        except XMLParseError:\n            # In cases of network disruptions, we may end up with a partial\n            # streamed response from S3. We need to treat these cases as\n            # 500 Service Errors and try again.\n            return True\n        if root.tag == 'Error':\n            return True\n    return False\n\n\nasync def inject_presigned_url_ec2(params, request_signer, model, **kwargs):\n    # The customer can still provide this, so we should pass if they do.\n    if 'PresignedUrl' in params['body']:\n        return\n    src, dest = _get_presigned_url_source_and_destination_regions(\n        request_signer, params['body']\n    )\n    url = await _get_cross_region_presigned_url(\n        request_signer, params, model, src, dest\n    )\n    params['body']['PresignedUrl'] = url\n    # EC2 Requires that the destination region be sent over the wire in\n    # addition to the source region.\n    params['body']['DestinationRegion'] = dest\n\n\nasync def inject_presigned_url_rds(params, request_signer, model, **kwargs):\n    # SourceRegion is not required for RDS operations, so it's possible that\n    # it isn't set. In that case it's probably a local copy so we don't need\n    # to do anything else.\n    if 'SourceRegion' not in params['body']:\n        return\n\n    src, dest = _get_presigned_url_source_and_destination_regions(\n        request_signer, params['body']\n    )\n\n    # Since SourceRegion isn't actually modeled for RDS, it needs to be\n    # removed from the request params before we send the actual request.\n    del params['body']['SourceRegion']\n\n    if 'PreSignedUrl' in params['body']:\n        return\n\n    url = await _get_cross_region_presigned_url(\n        request_signer, params, model, src, dest\n    )\n    params['body']['PreSignedUrl'] = url\n\n\nasync def parse_get_bucket_location(parsed, http_response, **kwargs):\n    # s3.GetBucketLocation cannot be modeled properly.  To\n    # account for this we just manually parse the XML document.\n    # The \"parsed\" passed in only has the ResponseMetadata\n    # filled out.  This handler will fill in the LocationConstraint\n    # value.\n    if http_response.raw is None:\n        return\n    response_body = await http_response.content\n    parser = ETree.XMLParser(target=ETree.TreeBuilder(), encoding='utf-8')\n    parser.feed(response_body)\n    root = parser.close()\n    region = root.text\n    parsed['LocationConstraint'] = region\n", "aiobotocore/args.py": "import copy\n\nimport botocore.parsers\nimport botocore.serialize\nfrom botocore.args import ClientArgsCreator\n\nfrom .config import AioConfig\nfrom .endpoint import DEFAULT_HTTP_SESSION_CLS, AioEndpointCreator\nfrom .regions import AioEndpointRulesetResolver\nfrom .signers import AioRequestSigner\n\n\nclass AioClientArgsCreator(ClientArgsCreator):\n    # NOTE: we override this so we can pull out the custom AioConfig params and\n    #       use an AioEndpointCreator\n    def get_client_args(\n        self,\n        service_model,\n        region_name,\n        is_secure,\n        endpoint_url,\n        verify,\n        credentials,\n        scoped_config,\n        client_config,\n        endpoint_bridge,\n        auth_token=None,\n        endpoints_ruleset_data=None,\n        partition_data=None,\n    ):\n        final_args = self.compute_client_args(\n            service_model,\n            client_config,\n            endpoint_bridge,\n            region_name,\n            endpoint_url,\n            is_secure,\n            scoped_config,\n        )\n\n        service_name = final_args['service_name']  # noqa\n        parameter_validation = final_args['parameter_validation']\n        endpoint_config = final_args['endpoint_config']\n        protocol = final_args['protocol']\n        config_kwargs = final_args['config_kwargs']\n        s3_config = final_args['s3_config']\n        partition = endpoint_config['metadata'].get('partition', None)\n        socket_options = final_args['socket_options']\n        configured_endpoint_url = final_args['configured_endpoint_url']\n\n        signing_region = endpoint_config['signing_region']\n        endpoint_region_name = endpoint_config['region_name']\n\n        event_emitter = copy.copy(self._event_emitter)\n        signer = AioRequestSigner(\n            service_model.service_id,\n            signing_region,\n            endpoint_config['signing_name'],\n            endpoint_config['signature_version'],\n            credentials,\n            event_emitter,\n            auth_token,\n        )\n\n        config_kwargs['s3'] = s3_config\n\n        # aiobotocore addition\n        if isinstance(client_config, AioConfig):\n            connector_args = client_config.connector_args\n            http_session_cls = client_config.http_session_cls\n        else:\n            connector_args = None\n            http_session_cls = DEFAULT_HTTP_SESSION_CLS\n\n        new_config = AioConfig(connector_args, **config_kwargs)\n        endpoint_creator = AioEndpointCreator(event_emitter)\n\n        endpoint = endpoint_creator.create_endpoint(\n            service_model,\n            region_name=endpoint_region_name,\n            endpoint_url=endpoint_config['endpoint_url'],\n            verify=verify,\n            response_parser_factory=self._response_parser_factory,\n            timeout=(new_config.connect_timeout, new_config.read_timeout),\n            max_pool_connections=new_config.max_pool_connections,\n            http_session_cls=http_session_cls,\n            proxies=new_config.proxies,\n            socket_options=socket_options,\n            client_cert=new_config.client_cert,\n            proxies_config=new_config.proxies_config,\n            connector_args=new_config.connector_args,\n        )\n\n        serializer = botocore.serialize.create_serializer(\n            protocol, parameter_validation\n        )\n        response_parser = botocore.parsers.create_parser(protocol)\n\n        ruleset_resolver = self._build_endpoint_resolver(\n            endpoints_ruleset_data,\n            partition_data,\n            client_config,\n            service_model,\n            endpoint_region_name,\n            region_name,\n            configured_endpoint_url,\n            endpoint,\n            is_secure,\n            endpoint_bridge,\n            event_emitter,\n        )\n\n        # Copy the session's user agent factory and adds client configuration.\n        client_ua_creator = self._session_ua_creator.with_client_config(\n            new_config\n        )\n        supplied_ua = client_config.user_agent if client_config else None\n        new_config._supplied_user_agent = supplied_ua\n\n        return {\n            'serializer': serializer,\n            'endpoint': endpoint,\n            'response_parser': response_parser,\n            'event_emitter': event_emitter,\n            'request_signer': signer,\n            'service_model': service_model,\n            'loader': self._loader,\n            'client_config': new_config,\n            'partition': partition,\n            'exceptions_factory': self._exceptions_factory,\n            'endpoint_ruleset_resolver': ruleset_resolver,\n            'user_agent_creator': client_ua_creator,\n        }\n\n    def _build_endpoint_resolver(\n        self,\n        endpoints_ruleset_data,\n        partition_data,\n        client_config,\n        service_model,\n        endpoint_region_name,\n        region_name,\n        endpoint_url,\n        endpoint,\n        is_secure,\n        endpoint_bridge,\n        event_emitter,\n    ):\n        if endpoints_ruleset_data is None:\n            return None\n\n        # The legacy EndpointResolver is global to the session, but\n        # EndpointRulesetResolver is service-specific. Builtins for\n        # EndpointRulesetResolver must not be derived from the legacy\n        # endpoint resolver's output, including final_args, s3_config,\n        # etc.\n        s3_config_raw = self.compute_s3_config(client_config) or {}\n        service_name_raw = service_model.endpoint_prefix\n        # Maintain complex logic for s3 and sts endpoints for backwards\n        # compatibility.\n        if service_name_raw in ['s3', 'sts'] or region_name is None:\n            eprv2_region_name = endpoint_region_name\n        else:\n            eprv2_region_name = region_name\n        resolver_builtins = self.compute_endpoint_resolver_builtin_defaults(\n            region_name=eprv2_region_name,\n            service_name=service_name_raw,\n            s3_config=s3_config_raw,\n            endpoint_bridge=endpoint_bridge,\n            client_endpoint_url=endpoint_url,\n            legacy_endpoint_url=endpoint.host,\n        )\n        # Client context params for s3 conflict with the available settings\n        # in the `s3` parameter on the `Config` object. If the same parameter\n        # is set in both places, the value in the `s3` parameter takes priority.\n        if client_config is not None:\n            client_context = client_config.client_context_params or {}\n        else:\n            client_context = {}\n        if self._is_s3_service(service_name_raw):\n            client_context.update(s3_config_raw)\n\n        sig_version = (\n            client_config.signature_version\n            if client_config is not None\n            else None\n        )\n        return AioEndpointRulesetResolver(\n            endpoint_ruleset_data=endpoints_ruleset_data,\n            partition_data=partition_data,\n            service_model=service_model,\n            builtins=resolver_builtins,\n            client_context=client_context,\n            event_emitter=event_emitter,\n            use_ssl=is_secure,\n            requested_auth_scheme=sig_version,\n        )\n", "aiobotocore/__init__.py": "__version__ = '2.13.1'\n", "aiobotocore/stub.py": "from botocore.stub import Stubber\n\nfrom .awsrequest import AioAWSResponse\n\n\nclass AioStubber(Stubber):\n    def _add_response(self, method, service_response, expected_params):\n        if not hasattr(self.client, method):\n            raise ValueError(\n                \"Client %s does not have method: %s\"\n                % (self.client.meta.service_model.service_name, method)\n            )  # pragma: no cover\n\n        # Create a successful http response\n        http_response = AioAWSResponse(None, 200, {}, None)\n\n        operation_name = self.client.meta.method_to_api_mapping.get(method)\n        self._validate_operation_response(operation_name, service_response)\n\n        # Add the service_response to the queue for returning responses\n        response = {\n            'operation_name': operation_name,\n            'response': (http_response, service_response),\n            'expected_params': expected_params,\n        }\n        self._queue.append(response)\n\n    def add_client_error(\n        self,\n        method,\n        service_error_code='',\n        service_message='',\n        http_status_code=400,\n        service_error_meta=None,\n        expected_params=None,\n        response_meta=None,\n        modeled_fields=None,\n    ):\n        \"\"\"\n        Adds a ``ClientError`` to the response queue.\n\n        :param method: The name of the service method to return the error on.\n        :type method: str\n\n        :param service_error_code: The service error code to return,\n                                                           e.g. ``NoSuchBucket``\n        :type service_error_code: str\n\n        :param service_message: The service message to return, e.g.\n                                        'The specified bucket does not exist.'\n        :type service_message: str\n\n        :param http_status_code: The HTTP status code to return, e.g. 404, etc\n        :type http_status_code: int\n\n        :param service_error_meta: Additional keys to be added to the\n                service Error\n        :type service_error_meta: dict\n\n        :param expected_params: A dictionary of the expected parameters to\n                be called for the provided service response. The parameters match\n                the names of keyword arguments passed to that client call. If\n                any of the parameters differ a ``StubResponseError`` is thrown.\n                You can use stub.ANY to indicate a particular parameter to ignore\n                in validation.\n\n        :param response_meta: Additional keys to be added to the\n                response's ResponseMetadata\n        :type response_meta: dict\n\n        :param modeled_fields: Additional keys to be added to the response\n                based on fields that are modeled for the particular error code.\n                These keys will be validated against the particular error shape\n                designated by the error code.\n        :type modeled_fields: dict\n\n        \"\"\"\n        http_response = AioAWSResponse(None, http_status_code, {}, None)\n\n        # We don't look to the model to build this because the caller would\n        # need to know the details of what the HTTP body would need to\n        # look like.\n        parsed_response = {\n            'ResponseMetadata': {'HTTPStatusCode': http_status_code},\n            'Error': {'Message': service_message, 'Code': service_error_code},\n        }\n\n        if service_error_meta is not None:\n            parsed_response['Error'].update(service_error_meta)\n\n        if response_meta is not None:\n            parsed_response['ResponseMetadata'].update(response_meta)\n\n        if modeled_fields is not None:\n            service_model = self.client.meta.service_model\n            shape = service_model.shape_for_error_code(service_error_code)\n            self._validate_response(shape, modeled_fields)\n            parsed_response.update(modeled_fields)\n\n        operation_name = self.client.meta.method_to_api_mapping.get(method)\n        # Note that we do not allow for expected_params while\n        # adding errors into the queue yet.\n        response = {\n            'operation_name': operation_name,\n            'response': (http_response, parsed_response),\n            'expected_params': expected_params,\n        }\n        self._queue.append(response)\n", "aiobotocore/session.py": "from botocore import UNSIGNED\nfrom botocore import __version__ as botocore_version\nfrom botocore import translate\nfrom botocore.exceptions import PartialCredentialsError\nfrom botocore.session import EVENT_ALIASES, ServiceModel\nfrom botocore.session import Session as _SyncSession\nfrom botocore.session import UnknownServiceError, copy\n\nfrom . import __version__, retryhandler\nfrom .client import AioBaseClient, AioClientCreator\nfrom .configprovider import AioSmartDefaultsConfigStoreFactory\nfrom .credentials import AioCredentials, create_credential_resolver\nfrom .hooks import AioHierarchicalEmitter\nfrom .parsers import AioResponseParserFactory\nfrom .tokens import create_token_resolver\nfrom .utils import AioIMDSRegionProvider\n\n\nclass ClientCreatorContext:\n    def __init__(self, coro):\n        self._coro = coro\n        self._client = None\n\n    async def __aenter__(self) -> AioBaseClient:\n        self._client = await self._coro\n        return await self._client.__aenter__()\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        await self._client.__aexit__(exc_type, exc_val, exc_tb)\n\n\nclass AioSession(_SyncSession):\n    # noinspection PyMissingConstructor\n    def __init__(\n        self,\n        session_vars=None,\n        event_hooks=None,\n        include_builtin_handlers=True,\n        profile=None,\n    ):\n        if event_hooks is None:\n            event_hooks = AioHierarchicalEmitter()\n\n        super().__init__(\n            session_vars, event_hooks, include_builtin_handlers, profile\n        )\n\n        self._set_user_agent_for_session()\n\n    def _set_user_agent_for_session(self):\n        # Mimic approach taken by AWS's aws-cli project\n        # https://github.com/aws/aws-cli/blob/b862122c76a3f280ff34e93c9dcafaf964e7bf9b/awscli/clidriver.py#L84\n\n        self.user_agent_name = 'aiobotocore'\n        self.user_agent_version = __version__\n        self.user_agent_extra = 'botocore/%s' % botocore_version\n\n    def _create_token_resolver(self):\n        return create_token_resolver(self)\n\n    def _create_credential_resolver(self):\n        return create_credential_resolver(\n            self, region_name=self._last_client_region_used\n        )\n\n    def _register_smart_defaults_factory(self):\n        def create_smart_defaults_factory():\n            default_config_resolver = self._get_internal_component(\n                'default_config_resolver'\n            )\n            imds_region_provider = AioIMDSRegionProvider(session=self)\n            return AioSmartDefaultsConfigStoreFactory(\n                default_config_resolver, imds_region_provider\n            )\n\n        self._internal_components.lazy_register_component(\n            'smart_defaults_factory', create_smart_defaults_factory\n        )\n\n    def _register_response_parser_factory(self):\n        self._components.register_component(\n            'response_parser_factory', AioResponseParserFactory()\n        )\n\n    def set_credentials(self, access_key, secret_key, token=None):\n        self._credentials = AioCredentials(access_key, secret_key, token)\n\n    async def get_credentials(self):\n        if self._credentials is None:\n            self._credentials = await self._components.get_component(\n                'credential_provider'\n            ).load_credentials()\n        return self._credentials\n\n    async def get_service_model(self, service_name, api_version=None):\n        service_description = await self.get_service_data(\n            service_name, api_version\n        )\n        return ServiceModel(service_description, service_name=service_name)\n\n    async def get_service_data(self, service_name, api_version=None):\n        \"\"\"\n        Retrieve the fully merged data associated with a service.\n        \"\"\"\n        data_path = service_name\n        service_data = self.get_component('data_loader').load_service_model(\n            data_path, type_name='service-2', api_version=api_version\n        )\n        service_id = EVENT_ALIASES.get(service_name, service_name)\n        await self._events.emit(\n            'service-data-loaded.%s' % service_id,\n            service_data=service_data,\n            service_name=service_name,\n            session=self,\n        )\n        return service_data\n\n    def create_client(self, *args, **kwargs):\n        return ClientCreatorContext(self._create_client(*args, **kwargs))\n\n    async def _create_client(\n        self,\n        service_name,\n        region_name=None,\n        api_version=None,\n        use_ssl=True,\n        verify=None,\n        endpoint_url=None,\n        aws_access_key_id=None,\n        aws_secret_access_key=None,\n        aws_session_token=None,\n        config=None,\n    ):\n        default_client_config = self.get_default_client_config()\n        # If a config is provided and a default config is set, then\n        # use the config resulting from merging the two.\n        if config is not None and default_client_config is not None:\n            config = default_client_config.merge(config)\n        # If a config was not provided then use the default\n        # client config from the session\n        elif default_client_config is not None:\n            config = default_client_config\n\n        region_name = self._resolve_region_name(region_name, config)\n\n        # Figure out the verify value base on the various\n        # configuration options.\n        if verify is None:\n            verify = self.get_config_variable('ca_bundle')\n\n        if api_version is None:\n            api_version = self.get_config_variable('api_versions').get(\n                service_name, None\n            )\n\n        loader = self.get_component('data_loader')\n        event_emitter = self.get_component('event_emitter')\n        response_parser_factory = self.get_component('response_parser_factory')\n        if config is not None and config.signature_version is UNSIGNED:\n            credentials = None\n        elif (\n            aws_access_key_id is not None and aws_secret_access_key is not None\n        ):\n            credentials = AioCredentials(\n                access_key=aws_access_key_id,\n                secret_key=aws_secret_access_key,\n                token=aws_session_token,\n            )\n        elif self._missing_cred_vars(aws_access_key_id, aws_secret_access_key):\n            raise PartialCredentialsError(\n                provider='explicit',\n                cred_var=self._missing_cred_vars(\n                    aws_access_key_id, aws_secret_access_key\n                ),\n            )\n        else:\n            credentials = await self.get_credentials()\n        auth_token = self.get_auth_token()\n        endpoint_resolver = self._get_internal_component('endpoint_resolver')\n        exceptions_factory = self._get_internal_component('exceptions_factory')\n        config_store = copy.copy(self.get_component('config_store'))\n        user_agent_creator = self.get_component('user_agent_creator')\n        # Session configuration values for the user agent string are applied\n        # just before each client creation because they may have been modified\n        # at any time between session creation and client creation.\n        user_agent_creator.set_session_config(\n            session_user_agent_name=self.user_agent_name,\n            session_user_agent_version=self.user_agent_version,\n            session_user_agent_extra=self.user_agent_extra,\n        )\n        defaults_mode = self._resolve_defaults_mode(config, config_store)\n        if defaults_mode != 'legacy':\n            smart_defaults_factory = self._get_internal_component(\n                'smart_defaults_factory'\n            )\n            await smart_defaults_factory.merge_smart_defaults(\n                config_store, defaults_mode, region_name\n            )\n        self._add_configured_endpoint_provider(\n            client_name=service_name,\n            config_store=config_store,\n        )\n        client_creator = AioClientCreator(\n            loader,\n            endpoint_resolver,\n            self.user_agent(),\n            event_emitter,\n            retryhandler,\n            translate,\n            response_parser_factory,\n            exceptions_factory,\n            config_store,\n            user_agent_creator=user_agent_creator,\n        )\n        client = await client_creator.create_client(\n            service_name=service_name,\n            region_name=region_name,\n            is_secure=use_ssl,\n            endpoint_url=endpoint_url,\n            verify=verify,\n            credentials=credentials,\n            scoped_config=self.get_scoped_config(),\n            client_config=config,\n            api_version=api_version,\n            auth_token=auth_token,\n        )\n        monitor = self._get_internal_component('monitor')\n        if monitor is not None:\n            monitor.register(client.meta.events)\n        return client\n\n    async def get_available_regions(\n        self, service_name, partition_name='aws', allow_non_regional=False\n    ):\n        resolver = self._get_internal_component('endpoint_resolver')\n        results = []\n        try:\n            service_data = await self.get_service_data(service_name)\n            endpoint_prefix = service_data['metadata'].get(\n                'endpointPrefix', service_name\n            )\n            results = resolver.get_available_endpoints(\n                endpoint_prefix, partition_name, allow_non_regional\n            )\n        except UnknownServiceError:\n            pass\n        return results\n\n\ndef get_session(env_vars=None):\n    \"\"\"\n    Return a new session object.\n    \"\"\"\n    return AioSession(env_vars)\n", "aiobotocore/_helpers.py": "import inspect\n\ntry:\n    from contextlib import (  # noqa: F401 lgtm[py/unused-import]\n        asynccontextmanager,\n    )\nexcept ImportError:\n    from async_generator import (  # noqa: F401 E501, lgtm[py/unused-import]\n        asynccontextmanager,\n    )\n\n\nasync def resolve_awaitable(obj):\n    if inspect.isawaitable(obj):\n        return await obj\n\n    return obj\n\n\nasync def async_any(items):\n    for item in items:\n        if await resolve_awaitable(item):\n            return True\n\n    return False\n", "aiobotocore/hooks.py": "from botocore.handlers import check_for_200_error as boto_check_for_200_error\nfrom botocore.handlers import (\n    inject_presigned_url_ec2 as boto_inject_presigned_url_ec2,\n)\nfrom botocore.handlers import (\n    inject_presigned_url_rds as boto_inject_presigned_url_rds,\n)\nfrom botocore.handlers import (\n    parse_get_bucket_location as boto_parse_get_bucket_location,\n)\nfrom botocore.hooks import HierarchicalEmitter, logger\nfrom botocore.signers import (\n    add_generate_db_auth_token as boto_add_generate_db_auth_token,\n)\nfrom botocore.signers import (\n    add_generate_presigned_post as boto_add_generate_presigned_post,\n)\nfrom botocore.signers import (\n    add_generate_presigned_url as boto_add_generate_presigned_url,\n)\n\nfrom ._helpers import resolve_awaitable\nfrom .handlers import (\n    check_for_200_error,\n    inject_presigned_url_ec2,\n    inject_presigned_url_rds,\n    parse_get_bucket_location,\n)\nfrom .signers import (\n    add_generate_db_auth_token,\n    add_generate_presigned_post,\n    add_generate_presigned_url,\n)\n\n_HANDLER_MAPPING = {\n    boto_inject_presigned_url_ec2: inject_presigned_url_ec2,\n    boto_inject_presigned_url_rds: inject_presigned_url_rds,\n    boto_add_generate_presigned_url: add_generate_presigned_url,\n    boto_add_generate_presigned_post: add_generate_presigned_post,\n    boto_add_generate_db_auth_token: add_generate_db_auth_token,\n    boto_parse_get_bucket_location: parse_get_bucket_location,\n    boto_check_for_200_error: check_for_200_error,\n}\n\n\nclass AioHierarchicalEmitter(HierarchicalEmitter):\n    async def _emit(self, event_name, kwargs, stop_on_response=False):\n        responses = []\n        # Invoke the event handlers from most specific\n        # to least specific, each time stripping off a dot.\n        handlers_to_call = self._lookup_cache.get(event_name)\n        if handlers_to_call is None:\n            handlers_to_call = self._handlers.prefix_search(event_name)\n            self._lookup_cache[event_name] = handlers_to_call\n        elif not handlers_to_call:\n            # Short circuit and return an empty response is we have\n            # no handlers to call.  This is the common case where\n            # for the majority of signals, nothing is listening.\n            return []\n        kwargs['event_name'] = event_name\n        responses = []\n        for handler in handlers_to_call:\n            logger.debug('Event %s: calling handler %s', event_name, handler)\n\n            # Await the handler if its a coroutine.\n            response = await resolve_awaitable(handler(**kwargs))\n            responses.append((handler, response))\n            if stop_on_response and response is not None:\n                return responses\n        return responses\n\n    async def emit_until_response(self, event_name, **kwargs):\n        responses = await self._emit(event_name, kwargs, stop_on_response=True)\n        if responses:\n            return responses[-1]\n        else:\n            return None, None\n\n    def _verify_and_register(\n        self,\n        event_name,\n        handler,\n        unique_id,\n        register_method,\n        unique_id_uses_count,\n    ):\n        handler = _HANDLER_MAPPING.get(handler, handler)\n\n        self._verify_is_callable(handler)\n        self._verify_accept_kwargs(handler)\n        register_method(event_name, handler, unique_id, unique_id_uses_count)\n", "aiobotocore/paginate.py": "import aioitertools\nimport jmespath\nfrom botocore.exceptions import PaginationError\nfrom botocore.paginate import PageIterator, Paginator\nfrom botocore.utils import merge_dicts, set_value_from_jmespath\n\n\nclass AioPageIterator(PageIterator):\n    def __aiter__(self):\n        return self.__anext__()\n\n    async def __anext__(self):\n        current_kwargs = self._op_kwargs\n        previous_next_token = None\n        next_token = {key: None for key in self._input_token}\n        if self._starting_token is not None:\n            # If the starting token exists, populate the next_token with the\n            # values inside it. This ensures that we have the service's\n            # pagination token on hand if we need to truncate after the\n            # first response.\n            next_token = self._parse_starting_token()[0]\n        # The number of items from result_key we've seen so far.\n        total_items = 0\n        first_request = True\n        primary_result_key = self.result_keys[0]\n        starting_truncation = 0\n        self._inject_starting_params(current_kwargs)\n\n        while True:\n            response = await self._make_request(current_kwargs)\n            parsed = self._extract_parsed_response(response)\n            if first_request:\n                # The first request is handled differently.  We could\n                # possibly have a resume/starting token that tells us where\n                # to index into the retrieved page.\n                if self._starting_token is not None:\n                    starting_truncation = self._handle_first_request(\n                        parsed, primary_result_key, starting_truncation\n                    )\n                first_request = False\n                self._record_non_aggregate_key_values(parsed)\n            else:\n                # If this isn't the first request, we have already sliced into\n                # the first request and had to make additional requests after.\n                # We no longer need to add this to truncation.\n                starting_truncation = 0\n            current_response = primary_result_key.search(parsed)\n            if current_response is None:\n                current_response = []\n            num_current_response = len(current_response)\n            truncate_amount = 0\n            if self._max_items is not None:\n                truncate_amount = (\n                    total_items + num_current_response - self._max_items\n                )\n\n            if truncate_amount > 0:\n                self._truncate_response(\n                    parsed,\n                    primary_result_key,\n                    truncate_amount,\n                    starting_truncation,\n                    next_token,\n                )\n                yield response\n                break\n            else:\n                yield response\n                total_items += num_current_response\n                next_token = self._get_next_token(parsed)\n                if all(t is None for t in next_token.values()):\n                    break\n                if (\n                    self._max_items is not None\n                    and total_items == self._max_items\n                ):\n                    # We're on a page boundary so we can set the current\n                    # next token to be the resume token.\n                    self.resume_token = next_token\n                    break\n                if (\n                    previous_next_token is not None\n                    and previous_next_token == next_token\n                ):\n                    message = (\n                        f\"The same next token was received \"\n                        f\"twice: {next_token}\"\n                    )\n                    raise PaginationError(message=message)\n                self._inject_token_into_kwargs(current_kwargs, next_token)\n                previous_next_token = next_token\n\n    async def search(self, expression):\n        compiled = jmespath.compile(expression)\n        async for page in self:\n            results = compiled.search(page)\n            if isinstance(results, list):\n                for element in results:\n                    yield element  # unfortunately yield from not avail from async f\n            else:\n                yield results\n\n    def result_key_iters(self):\n        teed_results = aioitertools.tee(self, len(self.result_keys))\n        return [\n            ResultKeyIterator(i, result_key)\n            for i, result_key in zip(teed_results, self.result_keys)\n        ]\n\n    async def build_full_result(self):\n        complete_result = {}\n        async for response in self:\n            page = response\n            # We want to try to catch operation object pagination\n            # and format correctly for those. They come in the form\n            # of a tuple of two elements: (http_response, parsed_responsed).\n            # We want the parsed_response as that is what the page iterator\n            # uses. We can remove it though once operation objects are removed.\n            if isinstance(response, tuple) and len(response) == 2:\n                page = response[1]\n            # We're incrementally building the full response page\n            # by page.  For each page in the response we need to\n            # inject the necessary components from the page\n            # into the complete_result.\n            for result_expression in self.result_keys:\n                # In order to incrementally update a result key\n                # we need to search the existing value from complete_result,\n                # then we need to search the _current_ page for the\n                # current result key value.  Then we append the current\n                # value onto the existing value, and re-set that value\n                # as the new value.\n                result_value = result_expression.search(page)\n                if result_value is None:\n                    continue\n                existing_value = result_expression.search(complete_result)\n                if existing_value is None:\n                    # Set the initial result\n                    set_value_from_jmespath(\n                        complete_result,\n                        result_expression.expression,\n                        result_value,\n                    )\n                    continue\n                # Now both result_value and existing_value contain something\n                if isinstance(result_value, list):\n                    existing_value.extend(result_value)\n                elif isinstance(result_value, (int, float, str)):\n                    # Modify the existing result with the sum or concatenation\n                    set_value_from_jmespath(\n                        complete_result,\n                        result_expression.expression,\n                        existing_value + result_value,\n                    )\n        merge_dicts(complete_result, self.non_aggregate_part)\n        if self.resume_token is not None:\n            complete_result['NextToken'] = self.resume_token\n        return complete_result\n\n\nclass AioPaginator(Paginator):\n    PAGE_ITERATOR_CLS = AioPageIterator\n\n\nclass ResultKeyIterator:\n    \"\"\"Iterates over the results of paginated responses.\n\n    Each iterator is associated with a single result key.\n    Iterating over this object will give you each element in\n    the result key list.\n\n    :param pages_iterator: An iterator that will give you\n        pages of results (a ``PageIterator`` class).\n    :param result_key: The JMESPath expression representing\n        the result key.\n\n    \"\"\"\n\n    def __init__(self, pages_iterator, result_key):\n        self._pages_iterator = pages_iterator\n        self.result_key = result_key\n\n    def __aiter__(self):\n        return self.__anext__()\n\n    async def __anext__(self):\n        async for page in self._pages_iterator:\n            results = self.result_key.search(page)\n            if results is None:\n                results = []\n            for result in results:\n                yield result  # yield from not avail from async func\n", "aiobotocore/retries/special.py": "from botocore.retries.special import RetryDDBChecksumError, crc32, logger\n\n\nclass AioRetryDDBChecksumError(RetryDDBChecksumError):\n    async def is_retryable(self, context):\n        service_name = context.operation_model.service_model.service_name\n        if service_name != self._SERVICE_NAME:\n            return False\n        if context.http_response is None:\n            return False\n        checksum = context.http_response.headers.get(self._CHECKSUM_HEADER)\n        if checksum is None:\n            return False\n        actual_crc32 = crc32(await context.http_response.content) & 0xFFFFFFFF\n        if actual_crc32 != int(checksum):\n            logger.debug(\n                \"DynamoDB crc32 checksum does not match, \"\n                \"expected: %s, actual: %s\",\n                checksum,\n                actual_crc32,\n            )\n            return True\n", "aiobotocore/retries/bucket.py": "\"\"\"An async reimplementation of the blocking elements from botocore.retries.bucket.\"\"\"\nimport asyncio\n\nfrom botocore.exceptions import CapacityNotAvailableError\n\nfrom botocore.retries.bucket import Clock as Clock  # reexport # noqa\n\n\nclass AsyncTokenBucket:\n    \"\"\"A reimplementation of TokenBucket that doesn't block.\"\"\"\n\n    # Most of the code here is pulled straight up from botocore, with slight changes\n    # to the interface to switch to async methods.\n    # This class doesn't inherit from the botocore TokenBucket, as the interface is\n    # different: the `max_rate` setter in the original class is replaced by the\n    # async `set_max_rate`.\n    # (a Python setter can't be async).\n\n    _MIN_RATE = 0.5\n\n    def __init__(self, max_rate, clock, min_rate=_MIN_RATE):\n        self._fill_rate = None\n        self._max_capacity = None\n        self._current_capacity = 0\n        self._clock = clock\n        self._last_timestamp = None\n        self._min_rate = min_rate\n        self._set_max_rate(max_rate)\n        # The main difference between this implementation and the botocore TokenBucket\n        # implementation is replacing a threading.Condition by this asyncio.Condition.\n        self._new_fill_rate_condition = asyncio.Condition()\n\n    @property\n    def max_rate(self):\n        return self._fill_rate\n\n    async def set_max_rate(self, value):\n        async with self._new_fill_rate_condition:\n            self._set_max_rate(value)\n            self._new_fill_rate_condition.notify()\n\n    def _set_max_rate(self, value):\n        # Before we can change the rate we need to fill any pending\n        # tokens we might have based on the current rate.  If we don't\n        # do this it means everything since the last recorded timestamp\n        # will accumulate at the rate we're about to set which isn't\n        # correct.\n        self._refill()\n        self._fill_rate = max(value, self._min_rate)\n        if value >= 1:\n            self._max_capacity = value\n        else:\n            self._max_capacity = 1\n        # If we're scaling down, we also can't have a capacity that's\n        # more than our max_capacity.\n        self._current_capacity = min(\n            self._current_capacity, self._max_capacity\n        )\n\n    @property\n    def max_capacity(self):\n        return self._max_capacity\n\n    @property\n    def available_capacity(self):\n        return self._current_capacity\n\n    async def acquire(self, amount=1, block=True):\n        \"\"\"Acquire token or return amount of time until next token available.\n\n        If block is True, then this method will return when there's sufficient\n        capacity to acquire the desired amount. This won't block the event loop.\n\n        If block is False, then this method will return True if capacity\n        was successfully acquired, False otherwise.\n        \"\"\"\n        async with self._new_fill_rate_condition:\n            return await self._acquire(amount=amount, block=block)\n\n    async def _acquire(self, amount, block):\n        self._refill()\n        if amount <= self._current_capacity:\n            self._current_capacity -= amount\n            return True\n        else:\n            if not block:\n                raise CapacityNotAvailableError()\n            # Not enough capacity.\n            sleep_amount = self._sleep_amount(amount)\n            while sleep_amount > 0:\n                try:\n                    await asyncio.wait_for(\n                        self._new_fill_rate_condition.wait(), sleep_amount\n                    )\n                except asyncio.TimeoutError:\n                    pass\n                self._refill()\n                sleep_amount = self._sleep_amount(amount)\n            self._current_capacity -= amount\n            return True\n\n    def _sleep_amount(self, amount):\n        return (amount - self._current_capacity) / self._fill_rate\n\n    def _refill(self):\n        timestamp = self._clock.current_time()\n        if self._last_timestamp is None:\n            self._last_timestamp = timestamp\n            return\n        current_capacity = self._current_capacity\n        fill_amount = (timestamp - self._last_timestamp) * self._fill_rate\n        new_capacity = min(self._max_capacity, current_capacity + fill_amount)\n        self._current_capacity = new_capacity\n        self._last_timestamp = timestamp\n", "aiobotocore/retries/adaptive.py": "\"\"\"An async reimplementation of the blocking elements from botocore.retries.adaptive.\"\"\"\nimport asyncio\nimport logging\n\nfrom botocore.retries import standard, throttling\n\n# The RateClocker from botocore uses a threading.Lock, but in a single-threaded asyncio\n# program, the lock will be acquired then released by the same coroutine without\n# blocking.\nfrom botocore.retries.adaptive import RateClocker\n\nfrom . import bucket\n\nlogger = logging.getLogger(__name__)\n\n\ndef register_retry_handler(client):\n    clock = bucket.Clock()\n    rate_adjustor = throttling.CubicCalculator(\n        starting_max_rate=0, start_time=clock.current_time()\n    )\n    token_bucket = bucket.AsyncTokenBucket(max_rate=1, clock=clock)\n    rate_clocker = RateClocker(clock)\n    throttling_detector = standard.ThrottlingErrorDetector(\n        retry_event_adapter=standard.RetryEventAdapter(),\n    )\n    limiter = AsyncClientRateLimiter(\n        rate_adjustor=rate_adjustor,\n        rate_clocker=rate_clocker,\n        token_bucket=token_bucket,\n        throttling_detector=throttling_detector,\n        clock=clock,\n    )\n    client.meta.events.register(\n        'before-send',\n        limiter.on_sending_request,\n    )\n    client.meta.events.register(\n        'needs-retry',\n        limiter.on_receiving_response,\n    )\n    return limiter\n\n\nclass AsyncClientRateLimiter:\n    \"\"\"An async reimplementation of ClientRateLimiter.\"\"\"\n\n    # Most of the code here comes directly from botocore. The main change is making the\n    # callbacks async.\n    # This doesn't inherit from the botocore ClientRateLimiter for two reasons:\n    # * the interface is slightly changed (methods are now async)\n    # * we rewrote the entirety of the class anyway\n\n    _MAX_RATE_ADJUST_SCALE = 2.0\n\n    def __init__(\n        self,\n        rate_adjustor,\n        rate_clocker,\n        token_bucket,\n        throttling_detector,\n        clock,\n    ):\n        self._rate_adjustor = rate_adjustor\n        self._rate_clocker = rate_clocker\n        self._token_bucket = token_bucket\n        self._throttling_detector = throttling_detector\n        self._clock = clock\n        self._enabled = False\n        self._lock = asyncio.Lock()\n\n    async def on_sending_request(self, request, **kwargs):\n        if self._enabled:\n            await self._token_bucket.acquire()\n\n    # Hooked up to needs-retry.\n    async def on_receiving_response(self, **kwargs):\n        measured_rate = self._rate_clocker.record()\n        timestamp = self._clock.current_time()\n        async with self._lock:\n            if not self._throttling_detector.is_throttling_error(**kwargs):\n                new_rate = self._rate_adjustor.success_received(timestamp)\n            else:\n                if not self._enabled:\n                    rate_to_use = measured_rate\n                else:\n                    rate_to_use = min(\n                        measured_rate, self._token_bucket.max_rate\n                    )\n                new_rate = self._rate_adjustor.error_received(\n                    rate_to_use, timestamp\n                )\n                logger.debug(\n                    \"Throttling response received, new send rate: %s \"\n                    \"measured rate: %s, token bucket capacity \"\n                    \"available: %s\",\n                    new_rate,\n                    measured_rate,\n                    self._token_bucket.available_capacity,\n                )\n                self._enabled = True\n            await self._token_bucket.set_max_rate(\n                min(new_rate, self._MAX_RATE_ADJUST_SCALE * measured_rate)\n            )\n", "aiobotocore/retries/standard.py": "from botocore.retries.standard import (\n    DEFAULT_MAX_ATTEMPTS,\n    ExponentialBackoff,\n    MaxAttemptsChecker,\n    ModeledRetryableChecker,\n    OrRetryChecker,\n    RetryEventAdapter,\n    RetryHandler,\n    RetryPolicy,\n    RetryQuotaChecker,\n    StandardRetryConditions,\n    ThrottledRetryableChecker,\n    TransientRetryableChecker,\n    logger,\n    quota,\n    special,\n)\n\nfrom .._helpers import async_any, resolve_awaitable\nfrom .special import AioRetryDDBChecksumError\n\n\ndef register_retry_handler(client, max_attempts=DEFAULT_MAX_ATTEMPTS):\n    retry_quota = RetryQuotaChecker(quota.RetryQuota())\n\n    service_id = client.meta.service_model.service_id\n    service_event_name = service_id.hyphenize()\n    client.meta.events.register(\n        f'after-call.{service_event_name}', retry_quota.release_retry_quota\n    )\n\n    handler = AioRetryHandler(\n        retry_policy=AioRetryPolicy(\n            retry_checker=AioStandardRetryConditions(\n                max_attempts=max_attempts\n            ),\n            retry_backoff=ExponentialBackoff(),\n        ),\n        retry_event_adapter=RetryEventAdapter(),\n        retry_quota=retry_quota,\n    )\n\n    unique_id = 'retry-config-%s' % service_event_name\n    client.meta.events.register(\n        'needs-retry.%s' % service_event_name,\n        handler.needs_retry,\n        unique_id=unique_id,\n    )\n    return handler\n\n\nclass AioRetryHandler(RetryHandler):\n    async def needs_retry(self, **kwargs):\n        \"\"\"Connect as a handler to the needs-retry event.\"\"\"\n        retry_delay = None\n        context = self._retry_event_adapter.create_retry_context(**kwargs)\n        if await self._retry_policy.should_retry(context):\n            # Before we can retry we need to ensure we have sufficient\n            # capacity in our retry quota.\n            if self._retry_quota.acquire_retry_quota(context):\n                retry_delay = self._retry_policy.compute_retry_delay(context)\n                logger.debug(\n                    \"Retry needed, retrying request after delay of: %s\",\n                    retry_delay,\n                )\n            else:\n                logger.debug(\n                    \"Retry needed but retry quota reached, \"\n                    \"not retrying request.\"\n                )\n        else:\n            logger.debug(\"Not retrying request.\")\n        self._retry_event_adapter.adapt_retry_response_from_context(context)\n        return retry_delay\n\n\nclass AioRetryPolicy(RetryPolicy):\n    async def should_retry(self, context):\n        return await resolve_awaitable(\n            self._retry_checker.is_retryable(context)\n        )\n\n\nclass AioStandardRetryConditions(StandardRetryConditions):\n    def __init__(\n        self, max_attempts=DEFAULT_MAX_ATTEMPTS\n    ):  # noqa: E501, lgtm [py/missing-call-to-init]\n        # Note: This class is for convenience so you can have the\n        # standard retry condition in a single class.\n        self._max_attempts_checker = MaxAttemptsChecker(max_attempts)\n        self._additional_checkers = AioOrRetryChecker(\n            [\n                TransientRetryableChecker(),\n                ThrottledRetryableChecker(),\n                ModeledRetryableChecker(),\n                AioOrRetryChecker(\n                    [\n                        special.RetryIDPCommunicationError(),\n                        AioRetryDDBChecksumError(),\n                    ]\n                ),\n            ]\n        )\n\n    async def is_retryable(self, context):\n        return self._max_attempts_checker.is_retryable(\n            context\n        ) and await resolve_awaitable(\n            self._additional_checkers.is_retryable(context)\n        )\n\n\nclass AioOrRetryChecker(OrRetryChecker):\n    async def is_retryable(self, context):\n        return await async_any(\n            checker.is_retryable(context) for checker in self._checkers\n        )\n", "examples/simple.py": "import asyncio\n\nfrom aiobotocore.session import get_session\n\nAWS_ACCESS_KEY_ID = \"xxx\"\nAWS_SECRET_ACCESS_KEY = \"xxx\"\n\n\nasync def go():\n    bucket = 'dataintake'\n    filename = 'dummy.bin'\n    folder = 'aiobotocore'\n    key = f'{folder}/{filename}'\n\n    session = get_session()\n    async with session.create_client(\n        's3',\n        region_name='us-west-2',\n        aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n        aws_access_key_id=AWS_ACCESS_KEY_ID,\n    ) as client:\n        # upload object to amazon s3\n        data = b'\\x01' * 1024\n        resp = await client.put_object(Bucket=bucket, Key=key, Body=data)\n        print(resp)\n\n        # getting s3 object properties of file we just uploaded\n        resp = await client.get_object_acl(Bucket=bucket, Key=key)\n        print(resp)\n\n        resp = await client.get_object(Bucket=bucket, Key=key)\n        async with resp['Body'] as stream:\n            await stream.read()  # if you do not read the stream the connection cannot be re-used and will be dropped\n            print(resp)\n        \"\"\"\n        This is to ensure the connection is returned to the pool as soon as possible.\n        Otherwise the connection will be released after it is GC'd\n        \"\"\"\n\n        # delete object from s3\n        resp = await client.delete_object(Bucket=bucket, Key=key)\n        print(resp)\n\n\nif __name__ == '__main__':\n    asyncio.run(go())\n", "examples/dynamodb_batch_write.py": "# Boto should get credentials from ~/.aws/credentials or the environment\nimport asyncio\n\nfrom aiobotocore.session import get_session\n\n\ndef get_items(start_num, num_items):\n    \"\"\"\n    Generate a sequence of dynamo items\n\n    :param start_num: Start index\n    :type start_num: int\n    :param num_items: Number of items\n    :type num_items: int\n    :return: List of dictionaries\n    :rtype: list of dict\n    \"\"\"\n    result = []\n    for i in range(start_num, start_num + num_items):\n        result.append({'pk': {'S': f'item{i}'}})\n    return result\n\n\ndef create_batch_write_structure(table_name, start_num, num_items):\n    \"\"\"\n    Create item structure for passing to batch_write_item\n\n    :param table_name: DynamoDB table name\n    :type table_name: str\n    :param start_num: Start index\n    :type start_num: int\n    :param num_items: Number of items\n    :type num_items: int\n    :return: dictionary of tables to write to\n    :rtype: dict\n    \"\"\"\n    return {\n        table_name: [\n            {'PutRequest': {'Item': item}}\n            for item in get_items(start_num, num_items)\n        ]\n    }\n\n\nasync def go():\n    session = get_session()\n    async with session.create_client(\n        'dynamodb', region_name='us-west-2'\n    ) as client:\n        table_name = 'test'\n\n        print('Writing to dynamo')\n        start = 0\n        while True:\n            # Loop adding 25 items to dynamo at a time\n            request_items = create_batch_write_structure(table_name, start, 25)\n            response = await client.batch_write_item(\n                RequestItems=request_items\n            )\n            if len(response['UnprocessedItems']) == 0:\n                print('Wrote 25 items to dynamo')\n            else:\n                # Hit the provisioned write limit\n                print('Hit write limit, backing off then retrying')\n                await asyncio.sleep(5)\n\n                # Items left over that haven't been inserted\n                unprocessed_items = response['UnprocessedItems']\n                print('Resubmitting items')\n                # Loop until unprocessed items are written\n                while len(unprocessed_items) > 0:\n                    response = await client.batch_write_item(\n                        RequestItems=unprocessed_items\n                    )\n                    # If any items are still left over, add them to the\n                    # list to be written\n                    unprocessed_items = response['UnprocessedItems']\n\n                    # If there are items left over, we could do with\n                    # sleeping some more\n                    if len(unprocessed_items) > 0:\n                        print('Backing off for 5 seconds')\n                        await asyncio.sleep(5)\n\n                # Inserted all the unprocessed items, exit loop\n                print('Unprocessed items successfully inserted')\n                break\n\n            start += 25\n\n        # See if DynamoDB has the last item we inserted\n        final_item = 'item' + str(start + 24)\n        print(f'Item \"{final_item}\" should exist')\n\n        response = await client.get_item(\n            TableName=table_name, Key={'pk': {'S': final_item}}\n        )\n        print(f'Response: {response[\"Item\"]}')\n\n\nif __name__ == '__main__':\n    asyncio.run(go())\n", "examples/sqs_queue_consumer.py": "#!/usr/bin/env python3\n\"\"\"\naiobotocore SQS Consumer Example\n\"\"\"\nimport asyncio\nimport sys\n\nimport botocore.exceptions\n\nfrom aiobotocore.session import get_session\n\nQUEUE_NAME = 'test_queue12'\n\n\nasync def go():\n    # Boto should get credentials from ~/.aws/credentials or the environment\n    session = get_session()\n    async with session.create_client('sqs', region_name='us-west-2') as client:\n        try:\n            response = await client.get_queue_url(QueueName=QUEUE_NAME)\n        except botocore.exceptions.ClientError as err:\n            if (\n                err.response['Error']['Code']\n                == 'AWS.SimpleQueueService.NonExistentQueue'\n            ):\n                print(f\"Queue {QUEUE_NAME} does not exist\")\n                sys.exit(1)\n            else:\n                raise\n\n        queue_url = response['QueueUrl']\n\n        print('Pulling messages off the queue')\n\n        while True:\n            try:\n                # This loop wont spin really fast as there is\n                # essentially a sleep in the receive_message call\n                response = await client.receive_message(\n                    QueueUrl=queue_url,\n                    WaitTimeSeconds=2,\n                )\n\n                if 'Messages' in response:\n                    for msg in response['Messages']:\n                        print(f'Got msg \"{msg[\"Body\"]}\"')\n                        # Need to remove msg from queue or else it'll reappear\n                        await client.delete_message(\n                            QueueUrl=queue_url,\n                            ReceiptHandle=msg['ReceiptHandle'],\n                        )\n                else:\n                    print('No messages in queue')\n            except KeyboardInterrupt:\n                break\n\n        print('Finished')\n\n\nif __name__ == '__main__':\n    asyncio.run(go())\n", "examples/sqs_queue_producer.py": "#!/usr/bin/env python3\n\"\"\"\naiobotocore SQS Producer Example\n\"\"\"\nimport asyncio\nimport random\nimport sys\n\nimport botocore.exceptions\n\nfrom aiobotocore.session import get_session\n\nQUEUE_NAME = 'test_queue12'\n\n\nasync def go():\n    # Boto should get credentials from ~/.aws/credentials or the environment\n    session = get_session()\n    async with session.create_client('sqs', region_name='us-west-2') as client:\n        try:\n            response = await client.get_queue_url(QueueName=QUEUE_NAME)\n        except botocore.exceptions.ClientError as err:\n            if (\n                err.response['Error']['Code']\n                == 'AWS.SimpleQueueService.NonExistentQueue'\n            ):\n                print(f\"Queue {QUEUE_NAME} does not exist\")\n                sys.exit(1)\n            else:\n                raise\n\n        queue_url = response['QueueUrl']\n\n        print('Putting messages on the queue')\n\n        msg_no = 1\n        while True:\n            try:\n                msg_body = f'Message #{msg_no}'\n                await client.send_message(\n                    QueueUrl=queue_url, MessageBody=msg_body\n                )\n                msg_no += 1\n\n                print(f'Pushed \"{msg_body}\" to queue')\n\n                await asyncio.sleep(random.randint(1, 4))\n            except KeyboardInterrupt:\n                break\n\n        print('Finished')\n\n\ndef main():\n    try:\n        loop = asyncio.get_event_loop()\n        loop.run_until_complete(go())\n    except KeyboardInterrupt:\n        pass\n\n\nif __name__ == '__main__':\n    main()\n", "examples/dynamodb_create_table.py": "# Boto should get credentials from ~/.aws/credentials or the environment\nimport asyncio\nimport uuid\n\nfrom aiobotocore.session import get_session\n\n\nasync def go():\n    session = get_session()\n    async with session.create_client(\n        'dynamodb', region_name='us-west-2'\n    ) as client:\n        # Create random table name\n        table_name = f'aiobotocore-{uuid.uuid4()}'\n\n        print('Requesting table creation...')\n        await client.create_table(\n            TableName=table_name,\n            AttributeDefinitions=[\n                {'AttributeName': 'testKey', 'AttributeType': 'S'},\n            ],\n            KeySchema=[\n                {'AttributeName': 'testKey', 'KeyType': 'HASH'},\n            ],\n            ProvisionedThroughput={\n                'ReadCapacityUnits': 10,\n                'WriteCapacityUnits': 10,\n            },\n        )\n\n        print(\"Waiting for table to be created...\")\n        waiter = client.get_waiter('table_exists')\n        await waiter.wait(TableName=table_name)\n        print(f\"Table {table_name} created\")\n\n\nif __name__ == '__main__':\n    asyncio.run(go())\n", "examples/sqs_queue_create.py": "# Boto should get credentials from ~/.aws/credentials or the environment\nimport asyncio\n\nfrom aiobotocore.session import get_session\n\n\nasync def go():\n    session = get_session()\n    async with session.create_client('sqs', region_name='us-west-2') as client:\n        print('Creating test_queue1')\n        response = await client.create_queue(QueueName='test_queue1')\n        queue_url = response['QueueUrl']\n\n        response = await client.list_queues()\n\n        print('Queue URLs:')\n        for queue_name in response.get('QueueUrls', []):\n            print(f' {queue_name}')\n\n        print(f'Deleting queue {queue_url}')\n        await client.delete_queue(QueueUrl=queue_url)\n\n        print('Done')\n\n\nif __name__ == '__main__':\n    asyncio.run(go())\n"}