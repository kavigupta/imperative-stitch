{"setup.py": "import os\nimport re\n\nfrom setuptools import find_packages, setup\n\n# NOTE: If updating requirements make sure to also check Pipfile for any locks\n# NOTE: When updating botocore make sure to update awscli/boto3 versions below\ninstall_requires = [\n    # pegged to also match items in `extras_require`\n    'botocore>=1.34.70,<1.34.132',\n    'aiohttp>=3.9.2,<4.0.0',\n    'wrapt>=1.10.10, <2.0.0',\n    'aioitertools>=0.5.1,<1.0.0',\n]\n\nextras_require = {\n    'awscli': ['awscli>=1.32.70,<1.33.14'],\n    'boto3': ['boto3>=1.34.70,<1.34.132'],\n}\n\n\ndef read(f):\n    return open(os.path.join(os.path.dirname(__file__), f)).read().strip()\n\n\ndef read_version():\n    regexp = re.compile(r\"^__version__\\W*=\\W*'([\\d.abrc]+)'\")\n    init_py = os.path.join(\n        os.path.dirname(__file__), 'aiobotocore', '__init__.py'\n    )\n    with open(init_py) as f:\n        for line in f:\n            match = regexp.match(line)\n            if match is not None:\n                return match.group(1)\n        raise RuntimeError('Cannot find version in aiobotocore/__init__.py')\n\n\nsetup(\n    name='aiobotocore',\n    version=read_version(),\n    description='Async client for aws services using botocore and aiohttp',\n    long_description='\\n\\n'.join((read('README.rst'), read('CHANGES.rst'))),\n    long_description_content_type='text/x-rst',\n    classifiers=[\n        'Development Status :: 4 - Beta',\n        'Intended Audience :: Developers',\n        'Intended Audience :: System Administrators',\n        'Natural Language :: English',\n        'License :: OSI Approved :: Apache Software License',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.8',\n        'Programming Language :: Python :: 3.9',\n        'Programming Language :: Python :: 3.10',\n        'Programming Language :: Python :: 3.11',\n        'Programming Language :: Python :: 3.12',\n        'Environment :: Web Environment',\n        'Framework :: AsyncIO',\n    ],\n    author=\"Nikolay Novik\",\n    author_email=\"nickolainovik@gmail.com\",\n    url='https://github.com/aio-libs/aiobotocore',\n    download_url='https://pypi.python.org/pypi/aiobotocore',\n    license='Apache License 2.0',\n    packages=find_packages(include=['aiobotocore']),\n    python_requires='>=3.8',\n    install_requires=install_requires,\n    extras_require=extras_require,\n    include_package_data=True,\n)\n", "aiobotocore/response.py": "import asyncio\n\nimport aiohttp\nimport aiohttp.client_exceptions\nimport wrapt\nfrom botocore.response import (\n    IncompleteReadError,\n    ReadTimeoutError,\n    ResponseStreamingError,\n)\n\nfrom aiobotocore import parsers\n\n\nclass AioReadTimeoutError(ReadTimeoutError, asyncio.TimeoutError):\n    pass\n\n\nclass StreamingBody(wrapt.ObjectProxy):\n    \"\"\"Wrapper class for an http response body.\n\n    This provides a few additional conveniences that do not exist\n    in the urllib3 model:\n\n        * Auto validation of content length, if the amount of bytes\n          we read does not match the content length, an exception\n          is raised.\n    \"\"\"\n\n    _DEFAULT_CHUNK_SIZE = 1024\n\n    def __init__(self, raw_stream: aiohttp.StreamReader, content_length: str):\n        super().__init__(raw_stream)\n        self._self_content_length = content_length\n        self._self_amount_read = 0\n\n    # https://github.com/GrahamDumpleton/wrapt/issues/73\n    async def __aenter__(self):\n        return await self.__wrapped__.__aenter__()\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        return await self.__wrapped__.__aexit__(exc_type, exc_val, exc_tb)\n\n    # NOTE: set_socket_timeout was only for when requests didn't support\n    #       read timeouts, so not needed\n    def readable(self):\n        return not self.at_eof()\n\n    async def read(self, amt=None):\n        \"\"\"Read at most amt bytes from the stream.\n\n        If the amt argument is omitted, read all data.\n        \"\"\"\n        # botocore to aiohttp mapping\n        try:\n            chunk = await self.__wrapped__.content.read(\n                amt if amt is not None else -1\n            )\n        except asyncio.TimeoutError as e:\n            raise AioReadTimeoutError(\n                endpoint_url=self.__wrapped__.url, error=e\n            )\n        except aiohttp.client_exceptions.ClientConnectionError as e:\n            raise ResponseStreamingError(error=e)\n\n        self._self_amount_read += len(chunk)\n        if amt is None or (not chunk and amt > 0):\n            # If the server sends empty contents or\n            # we ask to read all of the contents, then we know\n            # we need to verify the content length.\n            self._verify_content_length()\n        return chunk\n\n    async def readlines(self):\n        # assuming this is not an iterator\n        lines = [line async for line in self.iter_lines()]\n        return lines\n\n    def __aiter__(self):\n        \"\"\"Return an iterator to yield 1k chunks from the raw stream.\"\"\"\n        return self.iter_chunks(self._DEFAULT_CHUNK_SIZE)\n\n    async def __anext__(self):\n        \"\"\"Return the next 1k chunk from the raw stream.\"\"\"\n        current_chunk = await self.read(self._DEFAULT_CHUNK_SIZE)\n        if current_chunk:\n            return current_chunk\n        raise StopAsyncIteration\n\n    anext = __anext__\n\n    async def iter_lines(self, chunk_size=_DEFAULT_CHUNK_SIZE, keepends=False):\n        \"\"\"Return an iterator to yield lines from the raw stream.\n\n        This is achieved by reading chunk of bytes (of size chunk_size) at a\n        time from the raw stream, and then yielding lines from there.\n        \"\"\"\n        pending = b''\n        async for chunk in self.iter_chunks(chunk_size):\n            lines = (pending + chunk).splitlines(True)\n            for line in lines[:-1]:\n                yield line.splitlines(keepends)[0]\n            pending = lines[-1]\n        if pending:\n            yield pending.splitlines(keepends)[0]\n\n    async def iter_chunks(self, chunk_size=_DEFAULT_CHUNK_SIZE):\n        \"\"\"Return an iterator to yield chunks of chunk_size bytes from the raw\n        stream.\n        \"\"\"\n        while True:\n            current_chunk = await self.read(chunk_size)\n            if current_chunk == b\"\":\n                break\n            yield current_chunk\n\n    def _verify_content_length(self):\n        # See: https://github.com/kennethreitz/requests/issues/1855\n        # Basically, our http library doesn't do this for us, so we have\n        # to do this our self.\n        if (\n            self._self_content_length is not None\n            and self._self_amount_read != int(self._self_content_length)\n        ):\n            raise IncompleteReadError(\n                actual_bytes=self._self_amount_read,\n                expected_bytes=int(self._self_content_length),\n            )\n\n    def tell(self):\n        return self._self_amount_read\n\n\nasync def get_response(operation_model, http_response):\n    protocol = operation_model.metadata['protocol']\n    response_dict = {\n        'headers': http_response.headers,\n        'status_code': http_response.status_code,\n    }\n    # TODO: Unfortunately, we have to have error logic here.\n    # If it looks like an error, in the streaming response case we\n    # need to actually grab the contents.\n    if response_dict['status_code'] >= 300:\n        response_dict['body'] = await http_response.content\n    elif operation_model.has_streaming_output:\n        response_dict['body'] = StreamingBody(\n            http_response.raw, response_dict['headers'].get('content-length')\n        )\n    else:\n        response_dict['body'] = await http_response.content\n\n    parser = parsers.create_parser(protocol)\n    if asyncio.iscoroutinefunction(parser.parse):\n        parsed = await parser.parse(\n            response_dict, operation_model.output_shape\n        )\n    else:\n        parsed = parser.parse(response_dict, operation_model.output_shape)\n    return http_response, parsed\n", "aiobotocore/tokens.py": "import asyncio\nimport logging\nfrom datetime import timedelta\n\nimport dateutil.parser\nfrom botocore.compat import total_seconds\nfrom botocore.exceptions import ClientError, TokenRetrievalError\nfrom botocore.tokens import (\n    DeferredRefreshableToken,\n    FrozenAuthToken,\n    SSOTokenProvider,\n    TokenProviderChain,\n    _utc_now,\n)\n\nlogger = logging.getLogger(__name__)\n\n\ndef create_token_resolver(session):\n    providers = [\n        AioSSOTokenProvider(session),\n    ]\n    return TokenProviderChain(providers=providers)\n\n\nclass AioDeferredRefreshableToken(DeferredRefreshableToken):\n    def __init__(\n        self, method, refresh_using, time_fetcher=_utc_now\n    ):  # noqa: E501, lgtm [py/missing-call-to-init]\n        self._time_fetcher = time_fetcher\n        self._refresh_using = refresh_using\n        self.method = method\n\n        # The frozen token is protected by this lock\n        self._refresh_lock = asyncio.Lock()\n        self._frozen_token = None\n        self._next_refresh = None\n\n    async def get_frozen_token(self):\n        await self._refresh()\n        return self._frozen_token\n\n    async def _refresh(self):\n        # If we don't need to refresh just return\n        refresh_type = self._should_refresh()\n        if not refresh_type:\n            return None\n\n        # Block for refresh if we're in the mandatory refresh window\n        block_for_refresh = refresh_type == \"mandatory\"\n        if block_for_refresh or not self._refresh_lock.locked():\n            async with self._refresh_lock:\n                await self._protected_refresh()\n\n    async def _protected_refresh(self):\n        # This should only be called after acquiring the refresh lock\n        # Another task may have already refreshed, double check refresh\n        refresh_type = self._should_refresh()\n        if not refresh_type:\n            return None\n\n        try:\n            now = self._time_fetcher()\n            self._next_refresh = now + timedelta(seconds=self._attempt_timeout)\n            self._frozen_token = await self._refresh_using()\n        except Exception:\n            logger.warning(\n                \"Refreshing token failed during the %s refresh period.\",\n                refresh_type,\n                exc_info=True,\n            )\n            if refresh_type == \"mandatory\":\n                # This refresh was mandatory, error must be propagated back\n                raise\n\n        if self._is_expired():\n            # Fresh credentials should never be expired\n            raise TokenRetrievalError(\n                provider=self.method,\n                error_msg=\"Token has expired and refresh failed\",\n            )\n\n\nclass AioSSOTokenProvider(SSOTokenProvider):\n    async def _attempt_create_token(self, token):\n        async with self._client as client:\n            response = await client.create_token(\n                grantType=self._GRANT_TYPE,\n                clientId=token[\"clientId\"],\n                clientSecret=token[\"clientSecret\"],\n                refreshToken=token[\"refreshToken\"],\n            )\n        expires_in = timedelta(seconds=response[\"expiresIn\"])\n        new_token = {\n            \"startUrl\": self._sso_config[\"sso_start_url\"],\n            \"region\": self._sso_config[\"sso_region\"],\n            \"accessToken\": response[\"accessToken\"],\n            \"expiresAt\": self._now() + expires_in,\n            # Cache the registration alongside the token\n            \"clientId\": token[\"clientId\"],\n            \"clientSecret\": token[\"clientSecret\"],\n            \"registrationExpiresAt\": token[\"registrationExpiresAt\"],\n        }\n        if \"refreshToken\" in response:\n            new_token[\"refreshToken\"] = response[\"refreshToken\"]\n        logger.info(\"SSO Token refresh succeeded\")\n        return new_token\n\n    async def _refresh_access_token(self, token):\n        keys = (\n            \"refreshToken\",\n            \"clientId\",\n            \"clientSecret\",\n            \"registrationExpiresAt\",\n        )\n        missing_keys = [k for k in keys if k not in token]\n        if missing_keys:\n            msg = f\"Unable to refresh SSO token: missing keys: {missing_keys}\"\n            logger.info(msg)\n            return None\n\n        expiry = dateutil.parser.parse(token[\"registrationExpiresAt\"])\n        if total_seconds(expiry - self._now()) <= 0:\n            logger.info(f\"SSO token registration expired at {expiry}\")\n            return None\n\n        try:\n            return await self._attempt_create_token(token)\n        except ClientError:\n            logger.warning(\"SSO token refresh attempt failed\", exc_info=True)\n            return None\n\n    async def _refresher(self):\n        start_url = self._sso_config[\"sso_start_url\"]\n        session_name = self._sso_config[\"session_name\"]\n        logger.info(f\"Loading cached SSO token for {session_name}\")\n        token_dict = self._token_loader(start_url, session_name=session_name)\n        expiration = dateutil.parser.parse(token_dict[\"expiresAt\"])\n        logger.debug(f\"Cached SSO token expires at {expiration}\")\n\n        remaining = total_seconds(expiration - self._now())\n        if remaining < self._REFRESH_WINDOW:\n            new_token_dict = await self._refresh_access_token(token_dict)\n            if new_token_dict is not None:\n                token_dict = new_token_dict\n                expiration = token_dict[\"expiresAt\"]\n                self._token_loader.save_token(\n                    start_url, token_dict, session_name=session_name\n                )\n\n        return FrozenAuthToken(\n            token_dict[\"accessToken\"], expiration=expiration\n        )\n\n    def load_token(self):\n        if self._sso_config is None:\n            return None\n\n        return AioDeferredRefreshableToken(\n            self.METHOD, self._refresher, time_fetcher=self._now\n        )\n", "aiobotocore/parsers.py": "from botocore.parsers import (\n    LOG,\n    EC2QueryParser,\n    JSONParser,\n    NoInitialResponseError,\n    QueryParser,\n    ResponseParserError,\n    ResponseParserFactory,\n    RestJSONParser,\n    RestXMLParser,\n    lowercase_dict,\n)\n\nfrom .eventstream import AioEventStream\n\n\nclass AioResponseParserFactory(ResponseParserFactory):\n    def create_parser(self, protocol_name):\n        parser_cls = PROTOCOL_PARSERS[protocol_name]\n        return parser_cls(**self._defaults)\n\n\ndef create_parser(protocol):\n    return AioResponseParserFactory().create_parser(protocol)\n\n\nclass AioQueryParser(QueryParser):\n    def _create_event_stream(self, response, shape):\n        parser = self._event_stream_parser\n        name = response['context'].get('operation_name')\n        return AioEventStream(response['body'], shape, parser, name)\n\n\nclass AioEC2QueryParser(EC2QueryParser):\n    def _create_event_stream(self, response, shape):\n        parser = self._event_stream_parser\n        name = response['context'].get('operation_name')\n        return AioEventStream(response['body'], shape, parser, name)\n\n\nclass AioJSONParser(JSONParser):\n    async def _do_parse(self, response, shape):\n        parsed = {}\n        if shape is not None:\n            event_name = shape.event_stream_name\n            if event_name:\n                parsed = await self._handle_event_stream(\n                    response, shape, event_name\n                )\n            else:\n                parsed = self._handle_json_body(response['body'], shape)\n        self._inject_response_metadata(parsed, response['headers'])\n        return parsed\n\n    def _create_event_stream(self, response, shape):\n        parser = self._event_stream_parser\n        name = response['context'].get('operation_name')\n        return AioEventStream(response['body'], shape, parser, name)\n\n    async def _handle_event_stream(self, response, shape, event_name):\n        event_stream_shape = shape.members[event_name]\n        event_stream = self._create_event_stream(response, event_stream_shape)\n        try:\n            event = await event_stream.get_initial_response()\n        except NoInitialResponseError:\n            error_msg = 'First event was not of type initial-response'\n            raise ResponseParserError(error_msg)\n        parsed = self._handle_json_body(event.payload, shape)\n        parsed[event_name] = event_stream\n        return parsed\n\n    # this is actually from ResponseParser however for now JSONParser is the\n    # only class that needs this async\n    async def parse(self, response, shape):\n        LOG.debug('Response headers: %s', response['headers'])\n        LOG.debug('Response body:\\n%s', response['body'])\n        if response['status_code'] >= 301:\n            if self._is_generic_error_response(response):\n                parsed = self._do_generic_error_parse(response)\n            elif self._is_modeled_error_shape(shape):\n                parsed = self._do_modeled_error_parse(response, shape)\n                # We don't want to decorate the modeled fields with metadata\n                return parsed\n            else:\n                parsed = self._do_error_parse(response, shape)\n        else:\n            parsed = await self._do_parse(response, shape)\n\n        # We don't want to decorate event stream responses with metadata\n        if shape and shape.serialization.get('eventstream'):\n            return parsed\n\n        # Add ResponseMetadata if it doesn't exist and inject the HTTP\n        # status code and headers from the response.\n        if isinstance(parsed, dict):\n            response_metadata = parsed.get('ResponseMetadata', {})\n            response_metadata['HTTPStatusCode'] = response['status_code']\n            # Ensure that the http header keys are all lower cased. Older\n            # versions of urllib3 (< 1.11) would unintentionally do this for us\n            # (see urllib3#633). We need to do this conversion manually now.\n            headers = response['headers']\n            response_metadata['HTTPHeaders'] = lowercase_dict(headers)\n            parsed['ResponseMetadata'] = response_metadata\n            self._add_checksum_response_metadata(response, response_metadata)\n        return parsed\n\n\nclass AioRestJSONParser(RestJSONParser):\n    def _create_event_stream(self, response, shape):\n        parser = self._event_stream_parser\n        name = response['context'].get('operation_name')\n        return AioEventStream(response['body'], shape, parser, name)\n\n\nclass AioRestXMLParser(RestXMLParser):\n    def _create_event_stream(self, response, shape):\n        parser = self._event_stream_parser\n        name = response['context'].get('operation_name')\n        return AioEventStream(response['body'], shape, parser, name)\n\n\nPROTOCOL_PARSERS = {\n    'ec2': AioEC2QueryParser,\n    'query': AioQueryParser,\n    'json': AioJSONParser,\n    'rest-json': AioRestJSONParser,\n    'rest-xml': AioRestXMLParser,\n}\n", "aiobotocore/_endpoint_helpers.py": "import asyncio\n\nimport aiohttp.http_exceptions\nimport botocore.retryhandler\nimport wrapt\n\n# Monkey patching: We need to insert the aiohttp exception equivalents\n# The only other way to do this would be to have another config file :(\n_aiohttp_retryable_exceptions = [\n    aiohttp.ClientConnectionError,\n    aiohttp.ClientPayloadError,\n    aiohttp.ServerDisconnectedError,\n    aiohttp.http_exceptions.HttpProcessingError,\n    asyncio.TimeoutError,\n]\n\nbotocore.retryhandler.EXCEPTION_MAP['GENERAL_CONNECTION_ERROR'].extend(\n    _aiohttp_retryable_exceptions\n)\n\n\ndef _text(s, encoding='utf-8', errors='strict'):\n    if isinstance(s, bytes):\n        return s.decode(encoding, errors)\n    return s  # pragma: no cover\n\n\n# Unfortunately aiohttp changed the behavior of streams:\n#   github.com/aio-libs/aiohttp/issues/1907\n# We need this wrapper until we have a final resolution\nclass _IOBaseWrapper(wrapt.ObjectProxy):\n    def close(self):\n        # this stream should not be closed by aiohttp, like 1.x\n        pass\n", "aiobotocore/discovery.py": "import inspect\n\nfrom botocore.discovery import (\n    EndpointDiscoveryHandler,\n    EndpointDiscoveryManager,\n    EndpointDiscoveryRefreshFailed,\n    HTTPClientError,\n    logger,\n)\n\n\nclass AioEndpointDiscoveryManager(EndpointDiscoveryManager):\n    async def _refresh_current_endpoints(self, **kwargs):\n        cache_key = self._create_cache_key(**kwargs)\n        try:\n            response = self._describe_endpoints(**kwargs)\n\n            if inspect.isawaitable(response):\n                response = await response\n\n            endpoints = self._parse_endpoints(response)\n            self._cache[cache_key] = endpoints\n            self._failed_attempts.pop(cache_key, None)\n            return endpoints\n        except (ConnectionError, HTTPClientError):\n            self._failed_attempts[cache_key] = self._time() + 60\n            return None\n\n    async def describe_endpoint(self, **kwargs):\n        operation = kwargs['Operation']\n        discovery_required = self._model.discovery_required_for(operation)\n\n        if not self._always_discover and not discovery_required:\n            # Discovery set to only run on required operations\n            logger.debug(\n                'Optional discovery disabled. Skipping discovery for Operation: %s'\n                % operation\n            )\n            return None\n\n        # Get the endpoint for the provided operation and identifiers\n        cache_key = self._create_cache_key(**kwargs)\n        endpoints = self._get_current_endpoints(cache_key)\n        if endpoints:\n            return self._select_endpoint(endpoints)\n        # All known endpoints are stale\n        recently_failed = self._recently_failed(cache_key)\n        if not recently_failed:\n            # We haven't failed to discover recently, go ahead and refresh\n            endpoints = await self._refresh_current_endpoints(**kwargs)\n            if endpoints:\n                return self._select_endpoint(endpoints)\n        # Discovery has failed recently, do our best to get an endpoint\n        logger.debug('Endpoint Discovery has failed for: %s', kwargs)\n        stale_entries = self._cache.get(cache_key, None)\n        if stale_entries:\n            # We have stale entries, use those while discovery is failing\n            return self._select_endpoint(stale_entries)\n        if discovery_required:\n            # It looks strange to be checking recently_failed again but,\n            # this informs us as to whether or not we tried to refresh earlier\n            if recently_failed:\n                # Discovery is required and we haven't already refreshed\n                endpoints = await self._refresh_current_endpoints(**kwargs)\n                if endpoints:\n                    return self._select_endpoint(endpoints)\n            # No endpoints even refresh, raise hard error\n            raise EndpointDiscoveryRefreshFailed()\n        # Discovery is optional, just use the default endpoint for now\n        return None\n\n\nclass AioEndpointDiscoveryHandler(EndpointDiscoveryHandler):\n    async def discover_endpoint(self, request, operation_name, **kwargs):\n        ids = request.context.get('discovery', {}).get('identifiers')\n        if ids is None:\n            return\n        endpoint = await self._manager.describe_endpoint(\n            Operation=operation_name, Identifiers=ids\n        )\n        if endpoint is None:\n            logger.debug('Failed to discover and inject endpoint')\n            return\n        if not endpoint.startswith('http'):\n            endpoint = 'https://' + endpoint\n        logger.debug('Injecting discovered endpoint: %s', endpoint)\n        request.url = endpoint\n", "aiobotocore/awsrequest.py": "import botocore.utils\nfrom botocore.awsrequest import AWSResponse\n\n\nclass AioAWSResponse(AWSResponse):\n    # Unlike AWSResponse, these return awaitables\n\n    async def _content_prop(self):\n        \"\"\"Content of the response as bytes.\"\"\"\n\n        if self._content is None:\n            # NOTE: this will cache the data in self.raw\n            self._content = await self.raw.read() or b''\n\n        return self._content\n\n    @property\n    def content(self):\n        return self._content_prop()\n\n    async def _text_prop(self):\n        encoding = botocore.utils.get_encoding_from_headers(self.headers)\n        if encoding:\n            return (await self.content).decode(encoding)\n        else:\n            return (await self.content).decode('utf-8')\n\n    @property\n    def text(self):\n        return self._text_prop()\n", "aiobotocore/config.py": "import copy\n\nimport botocore.client\nfrom botocore.exceptions import ParamValidationError\n\nfrom aiobotocore.endpoint import DEFAULT_HTTP_SESSION_CLS\n\n\nclass AioConfig(botocore.client.Config):\n    def __init__(\n        self,\n        connector_args=None,\n        http_session_cls=DEFAULT_HTTP_SESSION_CLS,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n\n        self._validate_connector_args(connector_args)\n        self.connector_args = copy.copy(connector_args)\n        self.http_session_cls = http_session_cls\n        if not self.connector_args:\n            self.connector_args = dict()\n\n        if 'keepalive_timeout' not in self.connector_args:\n            # AWS has a 20 second idle timeout:\n            #   https://web.archive.org/web/20150926192339/https://forums.aws.amazon.com/message.jspa?messageID=215367\n            # and aiohttp default timeout is 30s so we set it to something\n            # reasonable here\n            self.connector_args['keepalive_timeout'] = 12\n\n    def merge(self, other_config):\n        # Adapted from parent class\n        config_options = copy.copy(self._user_provided_options)\n        config_options.update(other_config._user_provided_options)\n        return AioConfig(self.connector_args, **config_options)\n\n    @staticmethod\n    def _validate_connector_args(connector_args):\n        if connector_args is None:\n            return\n\n        for k, v in connector_args.items():\n            # verify_ssl is handled by verify parameter to create_client\n            if k == 'use_dns_cache':\n                if not isinstance(v, bool):\n                    raise ParamValidationError(\n                        report=f'{k} value must be a boolean'\n                    )\n            elif k == 'keepalive_timeout':\n                if v is not None and not isinstance(v, (float, int)):\n                    raise ParamValidationError(\n                        report=f'{k} value must be a float/int or None'\n                    )\n            elif k == 'force_close':\n                if not isinstance(v, bool):\n                    raise ParamValidationError(\n                        report=f'{k} value must be a boolean'\n                    )\n            # limit is handled by max_pool_connections\n            elif k == 'ssl_context':\n                import ssl\n\n                if not isinstance(v, ssl.SSLContext):\n                    raise ParamValidationError(\n                        report=f'{k} must be an SSLContext instance'\n                    )\n            elif k == \"resolver\":\n                from aiohttp.abc import AbstractResolver\n\n                if not isinstance(v, AbstractResolver):\n                    raise ParamValidationError(\n                        report=f'{k} must be an instance of a AbstractResolver'\n                    )\n            else:\n                raise ParamValidationError(report=f'invalid connector_arg:{k}')\n", "aiobotocore/endpoint.py": "import asyncio\n\nfrom botocore.endpoint import (\n    DEFAULT_TIMEOUT,\n    MAX_POOL_CONNECTIONS,\n    Endpoint,\n    EndpointCreator,\n    HTTPClientError,\n    create_request_object,\n    history_recorder,\n    is_valid_endpoint_url,\n    is_valid_ipv6_endpoint_url,\n    logger,\n)\nfrom botocore.hooks import first_non_none_response\nfrom urllib3.response import HTTPHeaderDict\n\nfrom aiobotocore.httpchecksum import handle_checksum_body\nfrom aiobotocore.httpsession import AIOHTTPSession\nfrom aiobotocore.response import StreamingBody\n\nDEFAULT_HTTP_SESSION_CLS = AIOHTTPSession\n\n\nasync def convert_to_response_dict(http_response, operation_model):\n    \"\"\"Convert an HTTP response object to a request dict.\n\n    This converts the requests library's HTTP response object to\n    a dictionary.\n\n    :type http_response: botocore.vendored.requests.model.Response\n    :param http_response: The HTTP response from an AWS service request.\n\n    :rtype: dict\n    :return: A response dictionary which will contain the following keys:\n        * headers (dict)\n        * status_code (int)\n        * body (string or file-like object)\n\n    \"\"\"\n    response_dict = {\n        # botocore converts keys to str, so make sure that they are in\n        # the expected case. See detailed discussion here:\n        # https://github.com/aio-libs/aiobotocore/pull/116\n        # aiohttp's CIMultiDict camel cases the headers :(\n        'headers': HTTPHeaderDict(\n            {\n                k.decode('utf-8').lower(): v.decode('utf-8')\n                for k, v in http_response.raw.raw_headers\n            }\n        ),\n        'status_code': http_response.status_code,\n        'context': {\n            'operation_name': operation_model.name,\n        },\n    }\n    if response_dict['status_code'] >= 300:\n        response_dict['body'] = await http_response.content\n    elif operation_model.has_event_stream_output:\n        response_dict['body'] = http_response.raw\n    elif operation_model.has_streaming_output:\n        length = response_dict['headers'].get('content-length')\n        response_dict['body'] = StreamingBody(http_response.raw, length)\n    else:\n        response_dict['body'] = await http_response.content\n    return response_dict\n\n\nclass AioEndpoint(Endpoint):\n    async def close(self):\n        await self.http_session.close()\n\n    async def create_request(self, params, operation_model=None):\n        request = create_request_object(params)\n        if operation_model:\n            request.stream_output = any(\n                [\n                    operation_model.has_streaming_output,\n                    operation_model.has_event_stream_output,\n                ]\n            )\n            service_id = operation_model.service_model.service_id.hyphenize()\n            event_name = 'request-created.{service_id}.{op_name}'.format(\n                service_id=service_id, op_name=operation_model.name\n            )\n            await self._event_emitter.emit(\n                event_name,\n                request=request,\n                operation_name=operation_model.name,\n            )\n        prepared_request = self.prepare_request(request)\n        return prepared_request\n\n    async def _send_request(self, request_dict, operation_model):\n        attempts = 1\n        context = request_dict['context']\n        self._update_retries_context(context, attempts)\n        request = await self.create_request(request_dict, operation_model)\n        success_response, exception = await self._get_response(\n            request, operation_model, context\n        )\n        while await self._needs_retry(\n            attempts,\n            operation_model,\n            request_dict,\n            success_response,\n            exception,\n        ):\n            attempts += 1\n            self._update_retries_context(context, attempts, success_response)\n            # If there is a stream associated with the request, we need\n            # to reset it before attempting to send the request again.\n            # This will ensure that we resend the entire contents of the\n            # body.\n            request.reset_stream()\n            # Create a new request when retried (including a new signature).\n            request = await self.create_request(request_dict, operation_model)\n            success_response, exception = await self._get_response(\n                request, operation_model, context\n            )\n        if (\n            success_response is not None\n            and 'ResponseMetadata' in success_response[1]\n        ):\n            # We want to share num retries, not num attempts.\n            total_retries = attempts - 1\n            success_response[1]['ResponseMetadata'][\n                'RetryAttempts'\n            ] = total_retries\n        if exception is not None:\n            raise exception\n        else:\n            return success_response\n\n    async def _get_response(self, request, operation_model, context):\n        # This will return a tuple of (success_response, exception)\n        # and success_response is itself a tuple of\n        # (http_response, parsed_dict).\n        # If an exception occurs then the success_response is None.\n        # If no exception occurs then exception is None.\n        success_response, exception = await self._do_get_response(\n            request, operation_model, context\n        )\n        kwargs_to_emit = {\n            'response_dict': None,\n            'parsed_response': None,\n            'context': context,\n            'exception': exception,\n        }\n        if success_response is not None:\n            http_response, parsed_response = success_response\n            kwargs_to_emit['parsed_response'] = parsed_response\n            kwargs_to_emit['response_dict'] = await convert_to_response_dict(\n                http_response, operation_model\n            )\n        service_id = operation_model.service_model.service_id.hyphenize()\n        await self._event_emitter.emit(\n            f\"response-received.{service_id}.{operation_model.name}\",\n            **kwargs_to_emit,\n        )\n        return success_response, exception\n\n    async def _do_get_response(self, request, operation_model, context):\n        try:\n            logger.debug(\"Sending http request: %s\", request)\n            history_recorder.record(\n                'HTTP_REQUEST',\n                {\n                    'method': request.method,\n                    'headers': request.headers,\n                    'streaming': operation_model.has_streaming_input,\n                    'url': request.url,\n                    'body': request.body,\n                },\n            )\n            service_id = operation_model.service_model.service_id.hyphenize()\n            event_name = f\"before-send.{service_id}.{operation_model.name}\"\n            responses = await self._event_emitter.emit(\n                event_name, request=request\n            )\n            http_response = first_non_none_response(responses)\n            if http_response is None:\n                http_response = await self._send(request)\n        except HTTPClientError as e:\n            return (None, e)\n        except Exception as e:\n            logger.debug(\n                \"Exception received when sending HTTP request.\", exc_info=True\n            )\n            return (None, e)\n\n        # This returns the http_response and the parsed_data.\n        response_dict = await convert_to_response_dict(\n            http_response, operation_model\n        )\n        await handle_checksum_body(\n            http_response,\n            response_dict,\n            context,\n            operation_model,\n        )\n\n        http_response_record_dict = response_dict.copy()\n        http_response_record_dict[\n            'streaming'\n        ] = operation_model.has_streaming_output\n        history_recorder.record('HTTP_RESPONSE', http_response_record_dict)\n\n        protocol = operation_model.metadata['protocol']\n        parser = self._response_parser_factory.create_parser(protocol)\n\n        if asyncio.iscoroutinefunction(parser.parse):\n            parsed_response = await parser.parse(\n                response_dict, operation_model.output_shape\n            )\n        else:\n            parsed_response = parser.parse(\n                response_dict, operation_model.output_shape\n            )\n\n        if http_response.status_code >= 300:\n            await self._add_modeled_error_fields(\n                response_dict,\n                parsed_response,\n                operation_model,\n                parser,\n            )\n        history_recorder.record('PARSED_RESPONSE', parsed_response)\n        return (http_response, parsed_response), None\n\n    async def _add_modeled_error_fields(\n        self,\n        response_dict,\n        parsed_response,\n        operation_model,\n        parser,\n    ):\n        error_code = parsed_response.get(\"Error\", {}).get(\"Code\")\n        if error_code is None:\n            return\n        service_model = operation_model.service_model\n        error_shape = service_model.shape_for_error_code(error_code)\n        if error_shape is None:\n            return\n\n        if asyncio.iscoroutinefunction(parser.parse):\n            modeled_parse = await parser.parse(response_dict, error_shape)\n        else:\n            modeled_parse = parser.parse(response_dict, error_shape)\n        # TODO: avoid naming conflicts with ResponseMetadata and Error\n        parsed_response.update(modeled_parse)\n\n    # NOTE: The only line changed here changing time.sleep to asyncio.sleep\n    async def _needs_retry(\n        self,\n        attempts,\n        operation_model,\n        request_dict,\n        response=None,\n        caught_exception=None,\n    ):\n        service_id = operation_model.service_model.service_id.hyphenize()\n        event_name = f\"needs-retry.{service_id}.{operation_model.name}\"\n        responses = await self._event_emitter.emit(\n            event_name,\n            response=response,\n            endpoint=self,\n            operation=operation_model,\n            attempts=attempts,\n            caught_exception=caught_exception,\n            request_dict=request_dict,\n        )\n        handler_response = first_non_none_response(responses)\n        if handler_response is None:\n            return False\n        else:\n            # Request needs to be retried, and we need to sleep\n            # for the specified number of times.\n            logger.debug(\n                \"Response received to retry, sleeping for %s seconds\",\n                handler_response,\n            )\n            await asyncio.sleep(handler_response)\n            return True\n\n    async def _send(self, request):\n        return await self.http_session.send(request)\n\n\nclass AioEndpointCreator(EndpointCreator):\n    def create_endpoint(\n        self,\n        service_model,\n        region_name,\n        endpoint_url,\n        verify=None,\n        response_parser_factory=None,\n        timeout=DEFAULT_TIMEOUT,\n        max_pool_connections=MAX_POOL_CONNECTIONS,\n        http_session_cls=DEFAULT_HTTP_SESSION_CLS,\n        proxies=None,\n        socket_options=None,\n        client_cert=None,\n        proxies_config=None,\n        connector_args=None,\n    ):\n        if not is_valid_endpoint_url(\n            endpoint_url\n        ) and not is_valid_ipv6_endpoint_url(endpoint_url):\n            raise ValueError(\"Invalid endpoint: %s\" % endpoint_url)\n\n        if proxies is None:\n            proxies = self._get_proxies(endpoint_url)\n        endpoint_prefix = service_model.endpoint_prefix\n\n        logger.debug('Setting %s timeout as %s', endpoint_prefix, timeout)\n        http_session = http_session_cls(\n            timeout=timeout,\n            proxies=proxies,\n            verify=self._get_verify_value(verify),\n            max_pool_connections=max_pool_connections,\n            socket_options=socket_options,\n            client_cert=client_cert,\n            proxies_config=proxies_config,\n            connector_args=connector_args,\n        )\n\n        return AioEndpoint(\n            endpoint_url,\n            endpoint_prefix=endpoint_prefix,\n            event_emitter=self._event_emitter,\n            response_parser_factory=response_parser_factory,\n            http_session=http_session,\n        )\n", "aiobotocore/regions.py": "import copy\nimport logging\n\nfrom botocore.exceptions import EndpointProviderError\nfrom botocore.regions import EndpointRulesetResolver\n\nLOG = logging.getLogger(__name__)\n\n\nclass AioEndpointRulesetResolver(EndpointRulesetResolver):\n    async def construct_endpoint(\n        self,\n        operation_model,\n        call_args,\n        request_context,\n    ):\n        \"\"\"Invokes the provider with params defined in the service's ruleset\"\"\"\n        if call_args is None:\n            call_args = {}\n\n        if request_context is None:\n            request_context = {}\n\n        provider_params = await self._get_provider_params(\n            operation_model, call_args, request_context\n        )\n        LOG.debug(\n            'Calling endpoint provider with parameters: %s' % provider_params\n        )\n        try:\n            provider_result = self._provider.resolve_endpoint(\n                **provider_params\n            )\n        except EndpointProviderError as ex:\n            botocore_exception = self.ruleset_error_to_botocore_exception(\n                ex, provider_params\n            )\n            if botocore_exception is None:\n                raise\n            else:\n                raise botocore_exception from ex\n        LOG.debug('Endpoint provider result: %s' % provider_result.url)\n\n        # The endpoint provider does not support non-secure transport.\n        if not self._use_ssl and provider_result.url.startswith('https://'):\n            provider_result = provider_result._replace(\n                url=f'http://{provider_result.url[8:]}'\n            )\n\n        # Multi-valued headers are not supported in botocore. Replace the list\n        # of values returned for each header with just its first entry,\n        # dropping any additionally entries.\n        provider_result = provider_result._replace(\n            headers={\n                key: val[0] for key, val in provider_result.headers.items()\n            }\n        )\n\n        return provider_result\n\n    async def _get_provider_params(\n        self, operation_model, call_args, request_context\n    ):\n        \"\"\"Resolve a value for each parameter defined in the service's ruleset\n\n        The resolution order for parameter values is:\n        1. Operation-specific static context values from the service definition\n        2. Operation-specific dynamic context values from API parameters\n        3. Client-specific context parameters\n        4. Built-in values such as region, FIPS usage, ...\n        \"\"\"\n        provider_params = {}\n        # Builtin values can be customized for each operation by hooks\n        # subscribing to the ``before-endpoint-resolution.*`` event.\n        customized_builtins = await self._get_customized_builtins(\n            operation_model, call_args, request_context\n        )\n        for param_name, param_def in self._param_definitions.items():\n            param_val = self._resolve_param_from_context(\n                param_name=param_name,\n                operation_model=operation_model,\n                call_args=call_args,\n            )\n            if param_val is None and param_def.builtin is not None:\n                param_val = self._resolve_param_as_builtin(\n                    builtin_name=param_def.builtin,\n                    builtins=customized_builtins,\n                )\n            if param_val is not None:\n                provider_params[param_name] = param_val\n\n        return provider_params\n\n    async def _get_customized_builtins(\n        self, operation_model, call_args, request_context\n    ):\n        service_id = self._service_model.service_id.hyphenize()\n        customized_builtins = copy.copy(self._builtins)\n        # Handlers are expected to modify the builtins dict in place.\n        await self._event_emitter.emit(\n            'before-endpoint-resolution.%s' % service_id,\n            builtins=customized_builtins,\n            model=operation_model,\n            params=call_args,\n            context=request_context,\n        )\n        return customized_builtins\n", "aiobotocore/retryhandler.py": "from botocore.retryhandler import (\n    ChecksumError,\n    CRC32Checker,\n    ExceptionRaiser,\n    HTTPStatusCodeChecker,\n    MaxAttemptsDecorator,\n    MultiChecker,\n    RetryHandler,\n    ServiceErrorCodeChecker,\n    _extract_retryable_exception,\n    crc32,\n    create_retry_action_from_config,\n    logger,\n)\n\nfrom ._helpers import resolve_awaitable\n\n\ndef create_retry_handler(config, operation_name=None):\n    checker = create_checker_from_retry_config(\n        config, operation_name=operation_name\n    )\n    action = create_retry_action_from_config(\n        config, operation_name=operation_name\n    )\n    return AioRetryHandler(checker=checker, action=action)\n\n\ndef create_checker_from_retry_config(config, operation_name=None):\n    checkers = []\n    max_attempts = None\n    retryable_exceptions = []\n    if '__default__' in config:\n        policies = config['__default__'].get('policies', [])\n        max_attempts = config['__default__']['max_attempts']\n        for key in policies:\n            current_config = policies[key]\n            checkers.append(_create_single_checker(current_config))\n            retry_exception = _extract_retryable_exception(current_config)\n            if retry_exception is not None:\n                retryable_exceptions.extend(retry_exception)\n    if operation_name is not None and config.get(operation_name) is not None:\n        operation_policies = config[operation_name]['policies']\n        for key in operation_policies:\n            checkers.append(_create_single_checker(operation_policies[key]))\n            retry_exception = _extract_retryable_exception(\n                operation_policies[key]\n            )\n            if retry_exception is not None:\n                retryable_exceptions.extend(retry_exception)\n    if len(checkers) == 1:\n        # Don't need to use a MultiChecker\n        return AioMaxAttemptsDecorator(checkers[0], max_attempts=max_attempts)\n    else:\n        multi_checker = AioMultiChecker(checkers)\n        return AioMaxAttemptsDecorator(\n            multi_checker,\n            max_attempts=max_attempts,\n            retryable_exceptions=tuple(retryable_exceptions),\n        )\n\n\ndef _create_single_checker(config):\n    if 'response' in config['applies_when']:\n        return _create_single_response_checker(\n            config['applies_when']['response']\n        )\n    elif 'socket_errors' in config['applies_when']:\n        return ExceptionRaiser()\n\n\ndef _create_single_response_checker(response):\n    if 'service_error_code' in response:\n        checker = ServiceErrorCodeChecker(\n            status_code=response['http_status_code'],\n            error_code=response['service_error_code'],\n        )\n    elif 'http_status_code' in response:\n        checker = HTTPStatusCodeChecker(\n            status_code=response['http_status_code']\n        )\n    elif 'crc32body' in response:\n        checker = AioCRC32Checker(header=response['crc32body'])\n    else:\n        # TODO: send a signal.\n        raise ValueError(\"Unknown retry policy\")\n    return checker\n\n\nclass AioRetryHandler(RetryHandler):\n    async def _call(self, attempts, response, caught_exception, **kwargs):\n        \"\"\"Handler for a retry.\n\n        Intended to be hooked up to an event handler (hence the **kwargs),\n        this will process retries appropriately.\n\n        \"\"\"\n        checker_kwargs = {\n            'attempt_number': attempts,\n            'response': response,\n            'caught_exception': caught_exception,\n        }\n        if isinstance(self._checker, MaxAttemptsDecorator):\n            retries_context = kwargs['request_dict']['context'].get('retries')\n            checker_kwargs.update({'retries_context': retries_context})\n\n        if await resolve_awaitable(self._checker(**checker_kwargs)):\n            result = self._action(attempts=attempts)\n            logger.debug(\"Retry needed, action of: %s\", result)\n            return result\n        logger.debug(\"No retry needed.\")\n\n    def __call__(self, *args, **kwargs):\n        return self._call(*args, **kwargs)  # return awaitable\n\n\nclass AioMaxAttemptsDecorator(MaxAttemptsDecorator):\n    async def _call(\n        self, attempt_number, response, caught_exception, retries_context\n    ):\n        if retries_context:\n            retries_context['max'] = max(\n                retries_context.get('max', 0), self._max_attempts\n            )\n\n        should_retry = await self._should_retry(\n            attempt_number, response, caught_exception\n        )\n        if should_retry:\n            if attempt_number >= self._max_attempts:\n                # explicitly set MaxAttemptsReached\n                if response is not None and 'ResponseMetadata' in response[1]:\n                    response[1]['ResponseMetadata'][\n                        'MaxAttemptsReached'\n                    ] = True\n                logger.debug(\n                    \"Reached the maximum number of retry attempts: %s\",\n                    attempt_number,\n                )\n                return False\n            else:\n                return should_retry\n        else:\n            return False\n\n    def __call__(self, *args, **kwargs):\n        return self._call(*args, **kwargs)\n\n    async def _should_retry(self, attempt_number, response, caught_exception):\n        if self._retryable_exceptions and attempt_number < self._max_attempts:\n            try:\n                return await resolve_awaitable(\n                    self._checker(attempt_number, response, caught_exception)\n                )\n            except self._retryable_exceptions as e:\n                logger.debug(\n                    \"retry needed, retryable exception caught: %s\",\n                    e,\n                    exc_info=True,\n                )\n                return True\n        else:\n            # If we've exceeded the max attempts we just let the exception\n            # propagate if one has occurred.\n            return await resolve_awaitable(\n                self._checker(attempt_number, response, caught_exception)\n            )\n\n\nclass AioMultiChecker(MultiChecker):\n    async def _call(self, attempt_number, response, caught_exception):\n        for checker in self._checkers:\n            checker_response = await resolve_awaitable(\n                checker(attempt_number, response, caught_exception)\n            )\n            if checker_response:\n                return checker_response\n        return False\n\n    def __call__(self, *args, **kwargs):\n        return self._call(*args, **kwargs)\n\n\nclass AioCRC32Checker(CRC32Checker):\n    async def _call(self, attempt_number, response, caught_exception):\n        if response is not None:\n            return await self._check_response(attempt_number, response)\n        elif caught_exception is not None:\n            return self._check_caught_exception(\n                attempt_number, caught_exception\n            )\n        else:\n            raise ValueError(\"Both response and caught_exception are None.\")\n\n    def __call__(self, *args, **kwargs):\n        return self._call(*args, **kwargs)\n\n    async def _check_response(self, attempt_number, response):\n        http_response = response[0]\n        expected_crc = http_response.headers.get(self._header_name)\n        if expected_crc is None:\n            logger.debug(\n                \"crc32 check skipped, the %s header is not \"\n                \"in the http response.\",\n                self._header_name,\n            )\n        else:\n            actual_crc32 = crc32(await response[0].content) & 0xFFFFFFFF\n            if not actual_crc32 == int(expected_crc):\n                logger.debug(\n                    \"retry needed: crc32 check failed, expected != actual: \"\n                    \"%s != %s\",\n                    int(expected_crc),\n                    actual_crc32,\n                )\n                raise ChecksumError(\n                    checksum_type='crc32',\n                    expected_checksum=int(expected_crc),\n                    actual_checksum=actual_crc32,\n                )\n", "aiobotocore/httpchecksum.py": "import io\n\nfrom botocore.httpchecksum import (\n    _CHECKSUM_CLS,\n    AwsChunkedWrapper,\n    FlexibleChecksumError,\n    _apply_request_header_checksum,\n    _handle_streaming_response,\n    base64,\n    conditionally_calculate_md5,\n    determine_content_length,\n    logger,\n)\n\nfrom aiobotocore._helpers import resolve_awaitable\n\n\nclass AioAwsChunkedWrapper(AwsChunkedWrapper):\n    async def _make_chunk(self):\n        # NOTE: Chunk size is not deterministic as read could return less. This\n        # means we cannot know the content length of the encoded aws-chunked\n        # stream ahead of time without ensuring a consistent chunk size\n\n        raw_chunk = await resolve_awaitable(self._raw.read(self._chunk_size))\n        hex_len = hex(len(raw_chunk))[2:].encode(\"ascii\")\n        self._complete = not raw_chunk\n\n        if self._checksum:\n            self._checksum.update(raw_chunk)\n\n        if self._checksum and self._complete:\n            name = self._checksum_name.encode(\"ascii\")\n            checksum = self._checksum.b64digest().encode(\"ascii\")\n            return b\"0\\r\\n%s:%s\\r\\n\\r\\n\" % (name, checksum)\n\n        return b\"%s\\r\\n%s\\r\\n\" % (hex_len, raw_chunk)\n\n    def __aiter__(self):\n        return self\n\n    async def __anext__(self):\n        while not self._complete:\n            return await self._make_chunk()\n        raise StopAsyncIteration()\n\n\nasync def handle_checksum_body(\n    http_response, response, context, operation_model\n):\n    headers = response[\"headers\"]\n    checksum_context = context.get(\"checksum\", {})\n    algorithms = checksum_context.get(\"response_algorithms\")\n\n    if not algorithms:\n        return\n\n    for algorithm in algorithms:\n        header_name = \"x-amz-checksum-%s\" % algorithm\n        # If the header is not found, check the next algorithm\n        if header_name not in headers:\n            continue\n\n        # If a - is in the checksum this is not valid Base64. S3 returns\n        # checksums that include a -# suffix to indicate a checksum derived\n        # from the hash of all part checksums. We cannot wrap this response\n        if \"-\" in headers[header_name]:\n            continue\n\n        if operation_model.has_streaming_output:\n            response[\"body\"] = _handle_streaming_response(\n                http_response, response, algorithm\n            )\n        else:\n            response[\"body\"] = await _handle_bytes_response(\n                http_response, response, algorithm\n            )\n\n        # Expose metadata that the checksum check actually occurred\n        checksum_context = response[\"context\"].get(\"checksum\", {})\n        checksum_context[\"response_algorithm\"] = algorithm\n        response[\"context\"][\"checksum\"] = checksum_context\n        return\n\n    logger.info(\n        f'Skipping checksum validation. Response did not contain one of the '\n        f'following algorithms: {algorithms}.'\n    )\n\n\nasync def _handle_bytes_response(http_response, response, algorithm):\n    body = await http_response.content\n    header_name = \"x-amz-checksum-%s\" % algorithm\n    checksum_cls = _CHECKSUM_CLS.get(algorithm)\n    checksum = checksum_cls()\n    checksum.update(body)\n    expected = response[\"headers\"][header_name]\n    if checksum.digest() != base64.b64decode(expected):\n        error_msg = (\n            \"Expected checksum %s did not match calculated checksum: %s\"\n            % (\n                expected,\n                checksum.b64digest(),\n            )\n        )\n        raise FlexibleChecksumError(error_msg=error_msg)\n    return body\n\n\ndef apply_request_checksum(request):\n    checksum_context = request.get(\"context\", {}).get(\"checksum\", {})\n    algorithm = checksum_context.get(\"request_algorithm\")\n\n    if not algorithm:\n        return\n\n    if algorithm == \"conditional-md5\":\n        # Special case to handle the http checksum required trait\n        conditionally_calculate_md5(request)\n    elif algorithm[\"in\"] == \"header\":\n        _apply_request_header_checksum(request)\n    elif algorithm[\"in\"] == \"trailer\":\n        _apply_request_trailer_checksum(request)\n    else:\n        raise FlexibleChecksumError(\n            error_msg=\"Unknown checksum variant: %s\" % algorithm[\"in\"]\n        )\n\n\ndef _apply_request_trailer_checksum(request):\n    checksum_context = request.get(\"context\", {}).get(\"checksum\", {})\n    algorithm = checksum_context.get(\"request_algorithm\")\n    location_name = algorithm[\"name\"]\n    checksum_cls = _CHECKSUM_CLS.get(algorithm[\"algorithm\"])\n\n    headers = request[\"headers\"]\n    body = request[\"body\"]\n\n    if location_name in headers:\n        # If the header is already set by the customer, skip calculation\n        return\n\n    # Cannot set this as aiohttp complains\n    headers[\"Transfer-Encoding\"] = \"chunked\"\n    if \"Content-Encoding\" in headers:\n        # We need to preserve the existing content encoding and add\n        # aws-chunked as a new content encoding.\n        headers[\"Content-Encoding\"] += \",aws-chunked\"\n    else:\n        headers[\"Content-Encoding\"] = \"aws-chunked\"\n    headers[\"X-Amz-Trailer\"] = location_name\n\n    content_length = determine_content_length(body)\n    if content_length is not None:\n        # Send the decoded content length if we can determine it. Some\n        # services such as S3 may require the decoded content length\n        headers[\"X-Amz-Decoded-Content-Length\"] = str(content_length)\n\n    if isinstance(body, (bytes, bytearray)):\n        body = io.BytesIO(body)\n\n    request[\"body\"] = AioAwsChunkedWrapper(\n        body,\n        checksum_cls=checksum_cls,\n        checksum_name=location_name,\n    )\n", "aiobotocore/utils.py": "import asyncio\nimport functools\nimport inspect\nimport json\nimport logging\n\nimport botocore.awsrequest\nfrom botocore.exceptions import (\n    InvalidIMDSEndpointError,\n    MetadataRetrievalError,\n)\nfrom botocore.utils import (\n    DEFAULT_METADATA_SERVICE_TIMEOUT,\n    METADATA_BASE_URL,\n    RETRYABLE_HTTP_ERRORS,\n    ArnParser,\n    BadIMDSRequestError,\n    ClientError,\n    ContainerMetadataFetcher,\n    HTTPClientError,\n    IdentityCache,\n    IMDSFetcher,\n    IMDSRegionProvider,\n    InstanceMetadataFetcher,\n    InstanceMetadataRegionFetcher,\n    ReadTimeoutError,\n    S3ExpressIdentityCache,\n    S3ExpressIdentityResolver,\n    S3RegionRedirector,\n    S3RegionRedirectorv2,\n    get_environ_proxies,\n    os,\n    resolve_imds_endpoint_mode,\n)\n\nimport aiobotocore.httpsession\nfrom aiobotocore._helpers import asynccontextmanager\n\nlogger = logging.getLogger(__name__)\n\n\nclass _RefCountedSession(aiobotocore.httpsession.AIOHTTPSession):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.__ref_count = 0\n        self.__lock = None\n\n    @asynccontextmanager\n    async def acquire(self):\n        if not self.__lock:\n            self.__lock = asyncio.Lock()\n\n        # ensure we have a session\n        async with self.__lock:\n            self.__ref_count += 1\n\n            try:\n                if self.__ref_count == 1:\n                    await self.__aenter__()\n            except BaseException:\n                self.__ref_count -= 1\n                raise\n\n        try:\n            yield self\n        finally:\n            async with self.__lock:\n                if self.__ref_count == 1:\n                    await self.__aexit__(None, None, None)\n\n                self.__ref_count -= 1\n\n\nclass AioIMDSFetcher(IMDSFetcher):\n    def __init__(\n        self,\n        timeout=DEFAULT_METADATA_SERVICE_TIMEOUT,  # noqa: E501, lgtm [py/missing-call-to-init]\n        num_attempts=1,\n        base_url=METADATA_BASE_URL,\n        env=None,\n        user_agent=None,\n        config=None,\n        session=None,\n    ):\n        self._timeout = timeout\n        self._num_attempts = num_attempts\n        if config is None:\n            config = {}\n        self._base_url = self._select_base_url(base_url, config)\n        self._config = config\n\n        if env is None:\n            env = os.environ.copy()\n        self._disabled = (\n            env.get('AWS_EC2_METADATA_DISABLED', 'false').lower() == 'true'\n        )\n        self._imds_v1_disabled = config.get('ec2_metadata_v1_disabled')\n        self._user_agent = user_agent\n\n        self._session = session or _RefCountedSession(\n            timeout=self._timeout,\n            proxies=get_environ_proxies(self._base_url),\n        )\n\n    async def _fetch_metadata_token(self):\n        self._assert_enabled()\n        url = self._construct_url(self._TOKEN_PATH)\n        headers = {\n            'x-aws-ec2-metadata-token-ttl-seconds': self._TOKEN_TTL,\n        }\n        self._add_user_agent(headers)\n\n        request = botocore.awsrequest.AWSRequest(\n            method='PUT', url=url, headers=headers\n        )\n\n        async with self._session.acquire() as session:\n            for i in range(self._num_attempts):\n                try:\n                    response = await session.send(request.prepare())\n                    if response.status_code == 200:\n                        return await response.text\n                    elif response.status_code in (404, 403, 405):\n                        return None\n                    elif response.status_code in (400,):\n                        raise BadIMDSRequestError(request)\n                except ReadTimeoutError:\n                    return None\n                except RETRYABLE_HTTP_ERRORS as e:\n                    logger.debug(\n                        \"Caught retryable HTTP exception while making metadata \"\n                        \"service request to %s: %s\",\n                        url,\n                        e,\n                        exc_info=True,\n                    )\n                except HTTPClientError as e:\n                    error = e.kwargs.get('error')\n                    if (\n                        error\n                        and getattr(error, 'errno', None) == 8\n                        or str(getattr(error, 'os_error', None))\n                        == 'Domain name not found'\n                    ):  # threaded vs async resolver\n                        raise InvalidIMDSEndpointError(endpoint=url, error=e)\n                    else:\n                        raise\n\n        return None\n\n    async def _get_request(self, url_path, retry_func, token=None):\n        self._assert_enabled()\n        if not token:\n            self._assert_v1_enabled()\n        if retry_func is None:\n            retry_func = self._default_retry\n        url = self._construct_url(url_path)\n        headers = {}\n        if token is not None:\n            headers['x-aws-ec2-metadata-token'] = token\n        self._add_user_agent(headers)\n\n        async with self._session.acquire() as session:\n            for i in range(self._num_attempts):\n                try:\n                    request = botocore.awsrequest.AWSRequest(\n                        method='GET', url=url, headers=headers\n                    )\n                    response = await session.send(request.prepare())\n                    should_retry = retry_func(response)\n                    if inspect.isawaitable(should_retry):\n                        should_retry = await should_retry\n\n                    if not should_retry:\n                        return response\n                except RETRYABLE_HTTP_ERRORS as e:\n                    logger.debug(\n                        \"Caught retryable HTTP exception while making metadata \"\n                        \"service request to %s: %s\",\n                        url,\n                        e,\n                        exc_info=True,\n                    )\n        raise self._RETRIES_EXCEEDED_ERROR_CLS()\n\n    async def _default_retry(self, response):\n        return await self._is_non_ok_response(\n            response\n        ) or await self._is_empty(response)\n\n    async def _is_non_ok_response(self, response):\n        if response.status_code != 200:\n            await self._log_imds_response(response, 'non-200', log_body=True)\n            return True\n        return False\n\n    async def _is_empty(self, response):\n        if not await response.content:\n            await self._log_imds_response(response, 'no body', log_body=True)\n            return True\n        return False\n\n    async def _log_imds_response(\n        self, response, reason_to_log, log_body=False\n    ):\n        statement = (\n            \"Metadata service returned %s response \"\n            \"with status code of %s for url: %s\"\n        )\n        logger_args = [reason_to_log, response.status_code, response.url]\n        if log_body:\n            statement += \", content body: %s\"\n            logger_args.append(await response.content)\n        logger.debug(statement, *logger_args)\n\n\nclass AioInstanceMetadataFetcher(AioIMDSFetcher, InstanceMetadataFetcher):\n    async def retrieve_iam_role_credentials(self):\n        try:\n            token = await self._fetch_metadata_token()\n            role_name = await self._get_iam_role(token)\n            credentials = await self._get_credentials(role_name, token)\n            if self._contains_all_credential_fields(credentials):\n                credentials = {\n                    'role_name': role_name,\n                    'access_key': credentials['AccessKeyId'],\n                    'secret_key': credentials['SecretAccessKey'],\n                    'token': credentials['Token'],\n                    'expiry_time': credentials['Expiration'],\n                }\n                self._evaluate_expiration(credentials)\n                return credentials\n            else:\n                if 'Code' in credentials and 'Message' in credentials:\n                    logger.debug(\n                        'Error response received when retrieving'\n                        'credentials: %s.',\n                        credentials,\n                    )\n                return {}\n        except self._RETRIES_EXCEEDED_ERROR_CLS:\n            logger.debug(\n                \"Max number of attempts exceeded (%s) when \"\n                \"attempting to retrieve data from metadata service.\",\n                self._num_attempts,\n            )\n        except BadIMDSRequestError as e:\n            logger.debug(\"Bad IMDS request: %s\", e.request)\n        return {}\n\n    async def _get_iam_role(self, token=None):\n        return await (\n            await self._get_request(\n                url_path=self._URL_PATH,\n                retry_func=self._needs_retry_for_role_name,\n                token=token,\n            )\n        ).text\n\n    async def _get_credentials(self, role_name, token=None):\n        r = await self._get_request(\n            url_path=self._URL_PATH + role_name,\n            retry_func=self._needs_retry_for_credentials,\n            token=token,\n        )\n        return json.loads(await r.text)\n\n    async def _is_invalid_json(self, response):\n        try:\n            json.loads(await response.text)\n            return False\n        except ValueError:\n            await self._log_imds_response(response, 'invalid json')\n            return True\n\n    async def _needs_retry_for_role_name(self, response):\n        return await self._is_non_ok_response(\n            response\n        ) or await self._is_empty(response)\n\n    async def _needs_retry_for_credentials(self, response):\n        return (\n            await self._is_non_ok_response(response)\n            or await self._is_empty(response)\n            or await self._is_invalid_json(response)\n        )\n\n\nclass AioIMDSRegionProvider(IMDSRegionProvider):\n    async def provide(self):\n        \"\"\"Provide the region value from IMDS.\"\"\"\n        instance_region = await self._get_instance_metadata_region()\n        return instance_region\n\n    async def _get_instance_metadata_region(self):\n        fetcher = self._get_fetcher()\n        region = await fetcher.retrieve_region()\n        return region\n\n    def _create_fetcher(self):\n        metadata_timeout = self._session.get_config_variable(\n            'metadata_service_timeout'\n        )\n        metadata_num_attempts = self._session.get_config_variable(\n            'metadata_service_num_attempts'\n        )\n        imds_config = {\n            'ec2_metadata_service_endpoint': self._session.get_config_variable(\n                'ec2_metadata_service_endpoint'\n            ),\n            'ec2_metadata_service_endpoint_mode': resolve_imds_endpoint_mode(\n                self._session\n            ),\n            'ec2_metadata_v1_disabled': self._session.get_config_variable(\n                'ec2_metadata_v1_disabled'\n            ),\n        }\n        fetcher = AioInstanceMetadataRegionFetcher(\n            timeout=metadata_timeout,\n            num_attempts=metadata_num_attempts,\n            env=self._environ,\n            user_agent=self._session.user_agent(),\n            config=imds_config,\n        )\n        return fetcher\n\n\nclass AioInstanceMetadataRegionFetcher(\n    AioIMDSFetcher, InstanceMetadataRegionFetcher\n):\n    async def retrieve_region(self):\n        try:\n            region = await self._get_region()\n            return region\n        except self._RETRIES_EXCEEDED_ERROR_CLS:\n            logger.debug(\n                \"Max number of attempts exceeded (%s) when \"\n                \"attempting to retrieve data from metadata service.\",\n                self._num_attempts,\n            )\n        return None\n\n    async def _get_region(self):\n        token = await self._fetch_metadata_token()\n        response = await self._get_request(\n            url_path=self._URL_PATH,\n            retry_func=self._default_retry,\n            token=token,\n        )\n        availability_zone = await response.text\n        region = availability_zone[:-1]\n        return region\n\n\nclass AioIdentityCache(IdentityCache):\n    async def get_credentials(self, **kwargs):\n        callback = self.build_refresh_callback(**kwargs)\n        metadata = await callback()\n        credential_entry = self._credential_cls.create_from_metadata(\n            metadata=metadata,\n            refresh_using=callback,\n            method=self.METHOD,\n            advisory_timeout=45,\n            mandatory_timeout=10,\n        )\n        return credential_entry\n\n\nclass AioS3ExpressIdentityCache(AioIdentityCache, S3ExpressIdentityCache):\n    @functools.lru_cache(maxsize=100)\n    def _get_credentials(self, bucket):\n        return asyncio.create_task(super().get_credentials(bucket=bucket))\n\n    async def get_credentials(self, bucket):\n        # upstream uses `@functools.lru_cache(maxsize=100)` to cache credentials.\n        # This is incompatible with async code.\n        # We need to implement custom caching logic.\n\n        return await self._get_credentials(bucket=bucket)\n\n    def build_refresh_callback(self, bucket):\n        async def refresher():\n            response = await self._client.create_session(Bucket=bucket)\n            creds = response['Credentials']\n            expiration = self._serialize_if_needed(\n                creds['Expiration'], iso=True\n            )\n            return {\n                \"access_key\": creds['AccessKeyId'],\n                \"secret_key\": creds['SecretAccessKey'],\n                \"token\": creds['SessionToken'],\n                \"expiry_time\": expiration,\n            }\n\n        return refresher\n\n\nclass AioS3ExpressIdentityResolver(S3ExpressIdentityResolver):\n    def __init__(self, client, credential_cls, cache=None):\n        super().__init__(client, credential_cls, cache)\n\n        if cache is None:\n            cache = AioS3ExpressIdentityCache(self._client, credential_cls)\n        self._cache = cache\n\n\nclass AioS3RegionRedirectorv2(S3RegionRedirectorv2):\n    async def redirect_from_error(\n        self,\n        request_dict,\n        response,\n        operation,\n        **kwargs,\n    ):\n        \"\"\"\n        An S3 request sent to the wrong region will return an error that\n        contains the endpoint the request should be sent to. This handler\n        will add the redirect information to the signing context and then\n        redirect the request.\n        \"\"\"\n        if response is None:\n            # This could be none if there was a ConnectionError or other\n            # transport error.\n            return\n\n        redirect_ctx = request_dict.get('context', {}).get('s3_redirect', {})\n        if ArnParser.is_arn(redirect_ctx.get('bucket')):\n            logger.debug(\n                'S3 request was previously for an Accesspoint ARN, not '\n                'redirecting.'\n            )\n            return\n\n        if redirect_ctx.get('redirected'):\n            logger.debug(\n                'S3 request was previously redirected, not redirecting.'\n            )\n            return\n\n        error = response[1].get('Error', {})\n        error_code = error.get('Code')\n        response_metadata = response[1].get('ResponseMetadata', {})\n\n        # We have to account for 400 responses because\n        # if we sign a Head* request with the wrong region,\n        # we'll get a 400 Bad Request but we won't get a\n        # body saying it's an \"AuthorizationHeaderMalformed\".\n        is_special_head_object = (\n            error_code in ('301', '400') and operation.name == 'HeadObject'\n        )\n        is_special_head_bucket = (\n            error_code in ('301', '400')\n            and operation.name == 'HeadBucket'\n            and 'x-amz-bucket-region'\n            in response_metadata.get('HTTPHeaders', {})\n        )\n        is_wrong_signing_region = (\n            error_code == 'AuthorizationHeaderMalformed' and 'Region' in error\n        )\n        is_redirect_status = response[0] is not None and response[\n            0\n        ].status_code in (301, 302, 307)\n        is_permanent_redirect = error_code == 'PermanentRedirect'\n        if not any(\n            [\n                is_special_head_object,\n                is_wrong_signing_region,\n                is_permanent_redirect,\n                is_special_head_bucket,\n                is_redirect_status,\n            ]\n        ):\n            return\n\n        bucket = request_dict['context']['s3_redirect']['bucket']\n        client_region = request_dict['context'].get('client_region')\n        new_region = await self.get_bucket_region(bucket, response)\n\n        if new_region is None:\n            logger.debug(\n                \"S3 client configured for region %s but the bucket %s is not \"\n                \"in that region and the proper region could not be \"\n                \"automatically determined.\" % (client_region, bucket)\n            )\n            return\n\n        logger.debug(\n            \"S3 client configured for region %s but the bucket %s is in region\"\n            \" %s; Please configure the proper region to avoid multiple \"\n            \"unnecessary redirects and signing attempts.\"\n            % (client_region, bucket, new_region)\n        )\n        # Adding the new region to _cache will make construct_endpoint() to\n        # use the new region as value for the AWS::Region builtin parameter.\n        self._cache[bucket] = new_region\n\n        # Re-resolve endpoint with new region and modify request_dict with\n        # the new URL, auth scheme, and signing context.\n        ep_resolver = self._client._ruleset_resolver\n        ep_info = await ep_resolver.construct_endpoint(\n            operation_model=operation,\n            call_args=request_dict['context']['s3_redirect']['params'],\n            request_context=request_dict['context'],\n        )\n        request_dict['url'] = self.set_request_url(\n            request_dict['url'], ep_info.url\n        )\n        request_dict['context']['s3_redirect']['redirected'] = True\n        auth_schemes = ep_info.properties.get('authSchemes')\n        if auth_schemes is not None:\n            auth_info = ep_resolver.auth_schemes_to_signing_ctx(auth_schemes)\n            auth_type, signing_context = auth_info\n            request_dict['context']['auth_type'] = auth_type\n            request_dict['context']['signing'] = {\n                **request_dict['context'].get('signing', {}),\n                **signing_context,\n            }\n\n        # Return 0 so it doesn't wait to retry\n        return 0\n\n    async def get_bucket_region(self, bucket, response):\n        \"\"\"\n        There are multiple potential sources for the new region to redirect to,\n        but they aren't all universally available for use. This will try to\n        find region from response elements, but will fall back to calling\n        HEAD on the bucket if all else fails.\n        :param bucket: The bucket to find the region for. This is necessary if\n            the region is not available in the error response.\n        :param response: A response representing a service request that failed\n            due to incorrect region configuration.\n        \"\"\"\n        # First try to source the region from the headers.\n        service_response = response[1]\n        response_headers = service_response['ResponseMetadata']['HTTPHeaders']\n        if 'x-amz-bucket-region' in response_headers:\n            return response_headers['x-amz-bucket-region']\n\n        # Next, check the error body\n        region = service_response.get('Error', {}).get('Region', None)\n        if region is not None:\n            return region\n\n        # Finally, HEAD the bucket. No other choice sadly.\n        try:\n            # NOTE: we don't need to aenter/aexit as we have a ref to the base client\n            response = await self._client.head_bucket(Bucket=bucket)\n            headers = response['ResponseMetadata']['HTTPHeaders']\n        except ClientError as e:\n            headers = e.response['ResponseMetadata']['HTTPHeaders']\n\n        region = headers.get('x-amz-bucket-region', None)\n        return region\n\n\nclass AioS3RegionRedirector(S3RegionRedirector):\n    async def redirect_from_error(\n        self, request_dict, response, operation, **kwargs\n    ):\n        if response is None:\n            # This could be none if there was a ConnectionError or other\n            # transport error.\n            return\n\n        if self._is_s3_accesspoint(request_dict.get('context', {})):\n            logger.debug(\n                'S3 request was previously to an accesspoint, not redirecting.'\n            )\n            return\n\n        if request_dict.get('context', {}).get('s3_redirected'):\n            logger.debug(\n                'S3 request was previously redirected, not redirecting.'\n            )\n            return\n\n        error = response[1].get('Error', {})\n        error_code = error.get('Code')\n        response_metadata = response[1].get('ResponseMetadata', {})\n\n        # We have to account for 400 responses because\n        # if we sign a Head* request with the wrong region,\n        # we'll get a 400 Bad Request but we won't get a\n        # body saying it's an \"AuthorizationHeaderMalformed\".\n        is_special_head_object = (\n            error_code in ('301', '400') and operation.name == 'HeadObject'\n        )\n        is_special_head_bucket = (\n            error_code in ('301', '400')\n            and operation.name == 'HeadBucket'\n            and 'x-amz-bucket-region'\n            in response_metadata.get('HTTPHeaders', {})\n        )\n        is_wrong_signing_region = (\n            error_code == 'AuthorizationHeaderMalformed' and 'Region' in error\n        )\n        is_redirect_status = response[0] is not None and response[\n            0\n        ].status_code in (301, 302, 307)\n        is_permanent_redirect = error_code == 'PermanentRedirect'\n        if not any(\n            [\n                is_special_head_object,\n                is_wrong_signing_region,\n                is_permanent_redirect,\n                is_special_head_bucket,\n                is_redirect_status,\n            ]\n        ):\n            return\n\n        bucket = request_dict['context']['signing']['bucket']\n        client_region = request_dict['context'].get('client_region')\n        new_region = await self.get_bucket_region(bucket, response)\n\n        if new_region is None:\n            logger.debug(\n                \"S3 client configured for region %s but the bucket %s is not \"\n                \"in that region and the proper region could not be \"\n                \"automatically determined.\" % (client_region, bucket)\n            )\n            return\n\n        logger.debug(\n            \"S3 client configured for region %s but the bucket %s is in region\"\n            \" %s; Please configure the proper region to avoid multiple \"\n            \"unnecessary redirects and signing attempts.\"\n            % (client_region, bucket, new_region)\n        )\n        endpoint = self._endpoint_resolver.resolve('s3', new_region)\n        endpoint = endpoint['endpoint_url']\n\n        signing_context = {\n            'region': new_region,\n            'bucket': bucket,\n            'endpoint': endpoint,\n        }\n        request_dict['context']['signing'] = signing_context\n\n        self._cache[bucket] = signing_context\n        self.set_request_url(request_dict, request_dict['context'])\n\n        request_dict['context']['s3_redirected'] = True\n\n        # Return 0 so it doesn't wait to retry\n        return 0\n\n    async def get_bucket_region(self, bucket, response):\n        # First try to source the region from the headers.\n        service_response = response[1]\n        response_headers = service_response['ResponseMetadata']['HTTPHeaders']\n        if 'x-amz-bucket-region' in response_headers:\n            return response_headers['x-amz-bucket-region']\n\n        # Next, check the error body\n        region = service_response.get('Error', {}).get('Region', None)\n        if region is not None:\n            return region\n\n        # Finally, HEAD the bucket. No other choice sadly.\n        try:\n            # NOTE: we don't need to aenter/aexit as we have a ref to the base client\n            response = await self._client.head_bucket(Bucket=bucket)\n            headers = response['ResponseMetadata']['HTTPHeaders']\n        except ClientError as e:\n            headers = e.response['ResponseMetadata']['HTTPHeaders']\n\n        region = headers.get('x-amz-bucket-region', None)\n        return region\n\n\nclass AioContainerMetadataFetcher(ContainerMetadataFetcher):\n    def __init__(\n        self, session=None, sleep=asyncio.sleep\n    ):  # noqa: E501, lgtm [py/missing-call-to-init]\n        if session is None:\n            session = _RefCountedSession(timeout=self.TIMEOUT_SECONDS)\n        self._session = session\n        self._sleep = sleep\n\n    async def retrieve_full_uri(self, full_url, headers=None):\n        self._validate_allowed_url(full_url)\n        return await self._retrieve_credentials(full_url, headers)\n\n    async def retrieve_uri(self, relative_uri):\n        \"\"\"Retrieve JSON metadata from container metadata.\n\n        :type relative_uri: str\n        :param relative_uri: A relative URI, e.g \"/foo/bar?id=123\"\n\n        :return: The parsed JSON response.\n\n        \"\"\"\n        full_url = self.full_url(relative_uri)\n        return await self._retrieve_credentials(full_url)\n\n    async def _retrieve_credentials(self, full_url, extra_headers=None):\n        headers = {'Accept': 'application/json'}\n        if extra_headers is not None:\n            headers.update(extra_headers)\n        attempts = 0\n        while True:\n            try:\n                return await self._get_response(\n                    full_url, headers, self.TIMEOUT_SECONDS\n                )\n            except MetadataRetrievalError as e:\n                logger.debug(\n                    \"Received error when attempting to retrieve \"\n                    \"container metadata: %s\",\n                    e,\n                    exc_info=True,\n                )\n                await self._sleep(self.SLEEP_TIME)\n                attempts += 1\n                if attempts >= self.RETRY_ATTEMPTS:\n                    raise\n\n    async def _get_response(self, full_url, headers, timeout):\n        try:\n            async with self._session.acquire() as session:\n                AWSRequest = botocore.awsrequest.AWSRequest\n                request = AWSRequest(\n                    method='GET', url=full_url, headers=headers\n                )\n                response = await session.send(request.prepare())\n                response_text = (await response.content).decode('utf-8')\n\n                if response.status_code != 200:\n                    raise MetadataRetrievalError(\n                        error_msg=(\n                            f\"Received non 200 response {response.status_code} \"\n                            f\"from container metadata: {response_text}\"\n                        )\n                    )\n                try:\n                    return json.loads(response_text)\n                except ValueError:\n                    error_msg = \"Unable to parse JSON returned from container metadata services\"\n                    logger.debug('%s:%s', error_msg, response_text)\n                    raise MetadataRetrievalError(error_msg=error_msg)\n\n        except RETRYABLE_HTTP_ERRORS as e:\n            error_msg = (\n                \"Received error when attempting to retrieve \"\n                f\"container metadata: {e}\"\n            )\n            raise MetadataRetrievalError(error_msg=error_msg)\n", "aiobotocore/signers.py": "import datetime\n\nimport botocore\nimport botocore.auth\nfrom botocore.exceptions import UnknownClientMethodError\nfrom botocore.signers import (\n    RequestSigner,\n    S3PostPresigner,\n    UnknownSignatureVersionError,\n    UnsupportedSignatureVersionError,\n    _should_use_global_endpoint,\n    create_request_object,\n    prepare_request_dict,\n)\nfrom botocore.utils import ArnParser\n\n\nclass AioRequestSigner(RequestSigner):\n    async def handler(self, operation_name=None, request=None, **kwargs):\n        # This is typically hooked up to the \"request-created\" event\n        # from a client's event emitter.  When a new request is created\n        # this method is invoked to sign the request.\n        # Don't call this method directly.\n        return await self.sign(operation_name, request)\n\n    async def sign(\n        self,\n        operation_name,\n        request,\n        region_name=None,\n        signing_type='standard',\n        expires_in=None,\n        signing_name=None,\n    ):\n        explicit_region_name = region_name\n        if region_name is None:\n            region_name = self._region_name\n\n        if signing_name is None:\n            signing_name = self._signing_name\n\n        signature_version = await self._choose_signer(\n            operation_name, signing_type, request.context\n        )\n\n        # Allow mutating request before signing\n        await self._event_emitter.emit(\n            'before-sign.{}.{}'.format(\n                self._service_id.hyphenize(), operation_name\n            ),\n            request=request,\n            signing_name=signing_name,\n            region_name=self._region_name,\n            signature_version=signature_version,\n            request_signer=self,\n            operation_name=operation_name,\n        )\n\n        if signature_version != botocore.UNSIGNED:\n            kwargs = {\n                'signing_name': signing_name,\n                'region_name': region_name,\n                'signature_version': signature_version,\n            }\n            if expires_in is not None:\n                kwargs['expires'] = expires_in\n            signing_context = request.context.get('signing', {})\n            if not explicit_region_name and signing_context.get('region'):\n                kwargs['region_name'] = signing_context['region']\n            if signing_context.get('signing_name'):\n                kwargs['signing_name'] = signing_context['signing_name']\n            if signing_context.get('request_credentials'):\n                kwargs['request_credentials'] = signing_context[\n                    'request_credentials'\n                ]\n            if signing_context.get('identity_cache') is not None:\n                self._resolve_identity_cache(\n                    kwargs,\n                    signing_context['identity_cache'],\n                    signing_context['cache_key'],\n                )\n            try:\n                auth = await self.get_auth_instance(**kwargs)\n            except UnknownSignatureVersionError as e:\n                if signing_type != 'standard':\n                    raise UnsupportedSignatureVersionError(\n                        signature_version=signature_version\n                    )\n                else:\n                    raise e\n\n            auth.add_auth(request)\n\n    async def _choose_signer(self, operation_name, signing_type, context):\n        signing_type_suffix_map = {\n            'presign-post': '-presign-post',\n            'presign-url': '-query',\n        }\n        suffix = signing_type_suffix_map.get(signing_type, '')\n\n        # operation specific signing context takes precedent over client-level\n        # defaults\n        signature_version = context.get('auth_type') or self._signature_version\n        signing = context.get('signing', {})\n        signing_name = signing.get('signing_name', self._signing_name)\n        region_name = signing.get('region', self._region_name)\n        if (\n            signature_version is not botocore.UNSIGNED\n            and not signature_version.endswith(suffix)\n        ):\n            signature_version += suffix\n\n        handler, response = await self._event_emitter.emit_until_response(\n            'choose-signer.{}.{}'.format(\n                self._service_id.hyphenize(), operation_name\n            ),\n            signing_name=signing_name,\n            region_name=region_name,\n            signature_version=signature_version,\n            context=context,\n        )\n\n        if response is not None:\n            signature_version = response\n            # The suffix needs to be checked again in case we get an improper\n            # signature version from choose-signer.\n            if (\n                signature_version is not botocore.UNSIGNED\n                and not signature_version.endswith(suffix)\n            ):\n                signature_version += suffix\n\n        return signature_version\n\n    async def get_auth_instance(\n        self,\n        signing_name,\n        region_name,\n        signature_version=None,\n        request_credentials=None,\n        **kwargs,\n    ):\n        if signature_version is None:\n            signature_version = self._signature_version\n\n        cls = botocore.auth.AUTH_TYPE_MAPS.get(signature_version)\n        if cls is None:\n            raise UnknownSignatureVersionError(\n                signature_version=signature_version\n            )\n\n        if cls.REQUIRES_TOKEN is True:\n            frozen_token = None\n            if self._auth_token is not None:\n                frozen_token = await self._auth_token.get_frozen_token()\n            auth = cls(frozen_token)\n            return auth\n\n        credentials = request_credentials or self._credentials\n        if getattr(cls, \"REQUIRES_IDENTITY_CACHE\", None) is True:\n            cache = kwargs[\"identity_cache\"]\n            key = kwargs[\"cache_key\"]\n            credentials = await cache.get_credentials(key)\n            del kwargs[\"cache_key\"]\n\n        frozen_credentials = None\n        if credentials is not None:\n            frozen_credentials = await credentials.get_frozen_credentials()\n        kwargs['credentials'] = frozen_credentials\n        if cls.REQUIRES_REGION:\n            if self._region_name is None:\n                raise botocore.exceptions.NoRegionError()\n            kwargs['region_name'] = region_name\n            kwargs['service_name'] = signing_name\n        auth = cls(**kwargs)\n        return auth\n\n    # Alias get_auth for backwards compatibility.\n    get_auth = get_auth_instance\n\n    async def generate_presigned_url(\n        self,\n        request_dict,\n        operation_name,\n        expires_in=3600,\n        region_name=None,\n        signing_name=None,\n    ):\n        request = create_request_object(request_dict)\n        await self.sign(\n            operation_name,\n            request,\n            region_name,\n            'presign-url',\n            expires_in,\n            signing_name,\n        )\n\n        request.prepare()\n        return request.url\n\n\ndef add_generate_db_auth_token(class_attributes, **kwargs):\n    class_attributes['generate_db_auth_token'] = generate_db_auth_token\n\n\nasync def generate_db_auth_token(\n    self, DBHostname, Port, DBUsername, Region=None\n):\n    \"\"\"Generates an auth token used to connect to a db with IAM credentials.\n\n    :type DBHostname: str\n    :param DBHostname: The hostname of the database to connect to.\n\n    :type Port: int\n    :param Port: The port number the database is listening on.\n\n    :type DBUsername: str\n    :param DBUsername: The username to log in as.\n\n    :type Region: str\n    :param Region: The region the database is in. If None, the client\n        region will be used.\n\n    :return: A presigned url which can be used as an auth token.\n    \"\"\"\n    region = Region\n    if region is None:\n        region = self.meta.region_name\n\n    params = {\n        'Action': 'connect',\n        'DBUser': DBUsername,\n    }\n\n    request_dict = {\n        'url_path': '/',\n        'query_string': '',\n        'headers': {},\n        'body': params,\n        'method': 'GET',\n    }\n\n    # RDS requires that the scheme not be set when sent over. This can cause\n    # issues when signing because the Python url parsing libraries follow\n    # RFC 1808 closely, which states that a netloc must be introduced by `//`.\n    # Otherwise the url is presumed to be relative, and thus the whole\n    # netloc would be treated as a path component. To work around this we\n    # introduce https here and remove it once we're done processing it.\n    scheme = 'https://'\n    endpoint_url = f'{scheme}{DBHostname}:{Port}'\n    prepare_request_dict(request_dict, endpoint_url)\n    presigned_url = await self._request_signer.generate_presigned_url(\n        operation_name='connect',\n        request_dict=request_dict,\n        region_name=region,\n        expires_in=900,\n        signing_name='rds-db',\n    )\n    return presigned_url[len(scheme) :]\n\n\nclass AioS3PostPresigner(S3PostPresigner):\n    async def generate_presigned_post(\n        self,\n        request_dict,\n        fields=None,\n        conditions=None,\n        expires_in=3600,\n        region_name=None,\n    ):\n        if fields is None:\n            fields = {}\n\n        if conditions is None:\n            conditions = []\n\n        # Create the policy for the post.\n        policy = {}\n\n        # Create an expiration date for the policy\n        datetime_now = datetime.datetime.utcnow()\n        expire_date = datetime_now + datetime.timedelta(seconds=expires_in)\n        policy['expiration'] = expire_date.strftime(botocore.auth.ISO8601)\n\n        # Append all of the conditions that the user supplied.\n        policy['conditions'] = []\n        for condition in conditions:\n            policy['conditions'].append(condition)\n\n        # Store the policy and the fields in the request for signing\n        request = create_request_object(request_dict)\n        request.context['s3-presign-post-fields'] = fields\n        request.context['s3-presign-post-policy'] = policy\n\n        await self._request_signer.sign(\n            'PutObject', request, region_name, 'presign-post'\n        )\n        # Return the url and the fields for th form to post.\n        return {'url': request.url, 'fields': fields}\n\n\ndef add_generate_presigned_url(class_attributes, **kwargs):\n    class_attributes['generate_presigned_url'] = generate_presigned_url\n\n\nasync def generate_presigned_url(\n    self, ClientMethod, Params=None, ExpiresIn=3600, HttpMethod=None\n):\n    \"\"\"Generate a presigned url given a client, its method, and arguments\n\n    :type ClientMethod: string\n    :param ClientMethod: The client method to presign for\n\n    :type Params: dict\n    :param Params: The parameters normally passed to\n        ``ClientMethod``.\n\n    :type ExpiresIn: int\n    :param ExpiresIn: The number of seconds the presigned url is valid\n        for. By default it expires in an hour (3600 seconds)\n\n    :type HttpMethod: string\n    :param HttpMethod: The http method to use on the generated url. By\n        default, the http method is whatever is used in the method's model.\n\n    :returns: The presigned url\n    \"\"\"\n    client_method = ClientMethod\n    params = Params\n    if params is None:\n        params = {}\n    expires_in = ExpiresIn\n    http_method = HttpMethod\n    context = {\n        'is_presign_request': True,\n        'use_global_endpoint': _should_use_global_endpoint(self),\n    }\n\n    request_signer = self._request_signer\n\n    try:\n        operation_name = self._PY_TO_OP_NAME[client_method]\n    except KeyError:\n        raise UnknownClientMethodError(method_name=client_method)\n\n    operation_model = self.meta.service_model.operation_model(operation_name)\n    params = await self._emit_api_params(\n        api_params=params,\n        operation_model=operation_model,\n        context=context,\n    )\n    bucket_is_arn = ArnParser.is_arn(params.get('Bucket', ''))\n    (\n        endpoint_url,\n        additional_headers,\n        properties,\n    ) = await self._resolve_endpoint_ruleset(\n        operation_model,\n        params,\n        context,\n        ignore_signing_region=(not bucket_is_arn),\n    )\n\n    request_dict = await self._convert_to_request_dict(\n        api_params=params,\n        operation_model=operation_model,\n        endpoint_url=endpoint_url,\n        context=context,\n        headers=additional_headers,\n        set_user_agent_header=False,\n    )\n\n    # Switch out the http method if user specified it.\n    if http_method is not None:\n        request_dict['method'] = http_method\n\n    # Generate the presigned url.\n    return await request_signer.generate_presigned_url(\n        request_dict=request_dict,\n        expires_in=expires_in,\n        operation_name=operation_name,\n    )\n\n\ndef add_generate_presigned_post(class_attributes, **kwargs):\n    class_attributes['generate_presigned_post'] = generate_presigned_post\n\n\nasync def generate_presigned_post(\n    self, Bucket, Key, Fields=None, Conditions=None, ExpiresIn=3600\n):\n    bucket = Bucket\n    key = Key\n    fields = Fields\n    conditions = Conditions\n    expires_in = ExpiresIn\n\n    if fields is None:\n        fields = {}\n    else:\n        fields = fields.copy()\n\n    if conditions is None:\n        conditions = []\n\n    context = {\n        'is_presign_request': True,\n        'use_global_endpoint': _should_use_global_endpoint(self),\n    }\n\n    post_presigner = AioS3PostPresigner(self._request_signer)\n\n    # We choose the CreateBucket operation model because its url gets\n    # serialized to what a presign post requires.\n    operation_model = self.meta.service_model.operation_model('CreateBucket')\n    params = await self._emit_api_params(\n        api_params={'Bucket': bucket},\n        operation_model=operation_model,\n        context=context,\n    )\n    bucket_is_arn = ArnParser.is_arn(params.get('Bucket', ''))\n    (\n        endpoint_url,\n        additional_headers,\n        properties,\n    ) = await self._resolve_endpoint_ruleset(\n        operation_model,\n        params,\n        context,\n        ignore_signing_region=(not bucket_is_arn),\n    )\n\n    request_dict = await self._convert_to_request_dict(\n        api_params=params,\n        operation_model=operation_model,\n        endpoint_url=endpoint_url,\n        context=context,\n        headers=additional_headers,\n        set_user_agent_header=False,\n    )\n\n    # Append that the bucket name to the list of conditions.\n    conditions.append({'bucket': bucket})\n\n    # If the key ends with filename, the only constraint that can be\n    # imposed is if it starts with the specified prefix.\n    if key.endswith('${filename}'):\n        conditions.append([\"starts-with\", '$key', key[: -len('${filename}')]])\n    else:\n        conditions.append({'key': key})\n\n    # Add the key to the fields.\n    fields['key'] = key\n\n    return await post_presigner.generate_presigned_post(\n        request_dict=request_dict,\n        fields=fields,\n        conditions=conditions,\n        expires_in=expires_in,\n    )\n", "aiobotocore/httpsession.py": "import asyncio\nimport contextlib\nimport io\nimport os\nimport socket\nfrom typing import Dict, Optional\n\nimport aiohttp  # lgtm [py/import-and-import-from]\nfrom aiohttp import (\n    ClientConnectionError,\n    ClientConnectorError,\n    ClientHttpProxyError,\n    ClientProxyConnectionError,\n    ClientSSLError,\n    ServerDisconnectedError,\n    ServerTimeoutError,\n)\nfrom aiohttp.client import URL\nfrom botocore.httpsession import (\n    MAX_POOL_CONNECTIONS,\n    ConnectionClosedError,\n    ConnectTimeoutError,\n    EndpointConnectionError,\n    HTTPClientError,\n    InvalidProxiesConfigError,\n    LocationParseError,\n    ProxyConfiguration,\n    ProxyConnectionError,\n    ReadTimeoutError,\n    SSLError,\n    _is_ipaddress,\n    create_urllib3_context,\n    ensure_boolean,\n    get_cert_path,\n    logger,\n    mask_proxy_url,\n    parse_url,\n    urlparse,\n)\nfrom multidict import CIMultiDict\n\nimport aiobotocore.awsrequest\nfrom aiobotocore._endpoint_helpers import _IOBaseWrapper, _text\n\n\nclass AIOHTTPSession:\n    def __init__(\n        self,\n        verify: bool = True,\n        proxies: Dict[str, str] = None,  # {scheme: url}\n        timeout: float = None,\n        max_pool_connections: int = MAX_POOL_CONNECTIONS,\n        socket_options=None,\n        client_cert=None,\n        proxies_config=None,\n        connector_args=None,\n    ):\n        self._exit_stack = contextlib.AsyncExitStack()\n\n        # TODO: handle socket_options\n        # keep track of sessions by proxy url (if any)\n        self._sessions: Dict[Optional[str], aiohttp.ClientSession] = {}\n        self._verify = verify\n        self._proxy_config = ProxyConfiguration(\n            proxies=proxies, proxies_settings=proxies_config\n        )\n        if isinstance(timeout, (list, tuple)):\n            conn_timeout, read_timeout = timeout\n        else:\n            conn_timeout = read_timeout = timeout\n\n        timeout = aiohttp.ClientTimeout(\n            sock_connect=conn_timeout, sock_read=read_timeout\n        )\n\n        self._cert_file = None\n        self._key_file = None\n        if isinstance(client_cert, str):\n            self._cert_file = client_cert\n        elif isinstance(client_cert, tuple):\n            self._cert_file, self._key_file = client_cert\n\n        self._timeout = timeout\n        self._connector_args = connector_args\n        if self._connector_args is None:\n            # AWS has a 20 second idle timeout:\n            #   https://web.archive.org/web/20150926192339/https://forums.aws.amazon.com/message.jspa?messageID=215367\n            # aiohttp default timeout is 30s so set something reasonable here\n            self._connector_args = dict(keepalive_timeout=12)\n\n        self._max_pool_connections = max_pool_connections\n        self._socket_options = socket_options\n        if socket_options is None:\n            self._socket_options = []\n\n        # aiohttp handles 100 continue so we shouldn't need AWSHTTP[S]ConnectionPool\n        # it also pools by host so we don't need a manager, and can pass proxy via\n        # request so don't need proxy manager\n\n    async def __aenter__(self):\n        assert not self._sessions\n\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        self._sessions.clear()\n        await self._exit_stack.aclose()\n\n    def _get_ssl_context(self):\n        return create_urllib3_context()\n\n    def _setup_proxy_ssl_context(self, proxy_url):\n        proxies_settings = self._proxy_config.settings\n        proxy_ca_bundle = proxies_settings.get('proxy_ca_bundle')\n        proxy_cert = proxies_settings.get('proxy_client_cert')\n        if proxy_ca_bundle is None and proxy_cert is None:\n            return None\n\n        context = self._get_ssl_context()\n        try:\n            url = parse_url(proxy_url)\n            # urllib3 disables this by default but we need it for proper\n            # proxy tls negotiation when proxy_url is not an IP Address\n            if not _is_ipaddress(url.host):\n                context.check_hostname = True\n            if proxy_ca_bundle is not None:\n                context.load_verify_locations(cafile=proxy_ca_bundle)\n\n            if isinstance(proxy_cert, tuple):\n                context.load_cert_chain(proxy_cert[0], keyfile=proxy_cert[1])\n            elif isinstance(proxy_cert, str):\n                context.load_cert_chain(proxy_cert)\n\n            return context\n        except (OSError, LocationParseError) as e:\n            raise InvalidProxiesConfigError(error=e)\n\n    def _chunked(self, headers):\n        transfer_encoding = headers.get('Transfer-Encoding', '')\n        if chunked := transfer_encoding.lower() == 'chunked':\n            # aiohttp wants chunking as a param, and not a header\n            del headers['Transfer-Encoding']\n        return chunked or None\n\n    def _create_connector(self, proxy_url):\n        ssl_context = None\n        if bool(self._verify):\n            if proxy_url:\n                ssl_context = self._setup_proxy_ssl_context(proxy_url)\n                # TODO: add support for\n                #    proxies_settings.get('proxy_use_forwarding_for_https')\n            else:\n                ssl_context = self._get_ssl_context()\n\n            if ssl_context:\n                if self._cert_file:\n                    ssl_context.load_cert_chain(\n                        self._cert_file,\n                        self._key_file,\n                    )\n\n                # inline self._setup_ssl_cert\n                ca_certs = get_cert_path(self._verify)\n                if ca_certs:\n                    ssl_context.load_verify_locations(ca_certs, None, None)\n\n        return aiohttp.TCPConnector(\n            limit=self._max_pool_connections,\n            ssl=ssl_context or False,\n            **self._connector_args,\n        )\n\n    async def _get_session(self, proxy_url):\n        if not (session := self._sessions.get(proxy_url)):\n            connector = self._create_connector(proxy_url)\n            self._sessions[\n                proxy_url\n            ] = session = await self._exit_stack.enter_async_context(\n                aiohttp.ClientSession(\n                    connector=connector,\n                    timeout=self._timeout,\n                    skip_auto_headers={'CONTENT-TYPE'},\n                    auto_decompress=False,\n                ),\n            )\n\n        return session\n\n    async def close(self):\n        await self.__aexit__(None, None, None)\n\n    async def send(self, request):\n        try:\n            proxy_url = self._proxy_config.proxy_url_for(request.url)\n            proxy_headers = self._proxy_config.proxy_headers_for(request.url)\n            url = request.url\n            headers = request.headers\n            data = request.body\n\n            if ensure_boolean(\n                os.environ.get('BOTO_EXPERIMENTAL__ADD_PROXY_HOST_HEADER', '')\n            ):\n                # This is currently an \"experimental\" feature which provides\n                # no guarantees of backwards compatibility. It may be subject\n                # to change or removal in any patch version. Anyone opting in\n                # to this feature should strictly pin botocore.\n                host = urlparse(request.url).hostname\n                proxy_headers['host'] = host\n\n            headers_ = CIMultiDict(\n                (z[0], _text(z[1], encoding='utf-8')) for z in headers.items()\n            )\n\n            # https://github.com/boto/botocore/issues/1255\n            headers_['Accept-Encoding'] = 'identity'\n\n            if isinstance(data, io.IOBase):\n                data = _IOBaseWrapper(data)\n\n            url = URL(url, encoded=True)\n            session = await self._get_session(proxy_url)\n            response = await session.request(\n                request.method,\n                url=url,\n                chunked=self._chunked(headers_),\n                headers=headers_,\n                data=data,\n                proxy=proxy_url,\n                proxy_headers=proxy_headers,\n            )\n\n            http_response = aiobotocore.awsrequest.AioAWSResponse(\n                str(response.url), response.status, response.headers, response\n            )\n\n            if not request.stream_output:\n                # Cause the raw stream to be exhausted immediately. We do it\n                # this way instead of using preload_content because\n                # preload_content will never buffer chunked responses\n                await http_response.content\n\n            return http_response\n        except ClientSSLError as e:\n            raise SSLError(endpoint_url=request.url, error=e)\n        except (ClientProxyConnectionError, ClientHttpProxyError) as e:\n            raise ProxyConnectionError(\n                proxy_url=mask_proxy_url(proxy_url), error=e\n            )\n        except (\n            ServerDisconnectedError,\n            aiohttp.ClientPayloadError,\n            aiohttp.http_exceptions.BadStatusLine,\n        ) as e:\n            raise ConnectionClosedError(\n                error=e, request=request, endpoint_url=request.url\n            )\n        except ServerTimeoutError as e:\n            if str(e).lower().startswith('connect'):\n                raise ConnectTimeoutError(endpoint_url=request.url, error=e)\n            else:\n                raise ReadTimeoutError(endpoint_url=request.url, error=e)\n        except (\n            ClientConnectorError,\n            ClientConnectionError,\n            socket.gaierror,\n        ) as e:\n            raise EndpointConnectionError(endpoint_url=request.url, error=e)\n        except asyncio.TimeoutError as e:\n            raise ReadTimeoutError(endpoint_url=request.url, error=e)\n        except Exception as e:\n            message = 'Exception received when sending urllib3 HTTP request'\n            logger.debug(message, exc_info=True)\n            raise HTTPClientError(error=e)\n", "aiobotocore/eventstream.py": "from botocore.eventstream import (\n    EventStream,\n    EventStreamBuffer,\n    NoInitialResponseError,\n)\n\n\nclass AioEventStream(EventStream):\n    def __iter__(self):\n        raise NotImplementedError('Use async-for instead')\n\n    def __aiter__(self):\n        return self.__anext__()\n\n    async def __anext__(self):\n        async for event in self._event_generator:\n            parsed_event = self._parse_event(event)\n            if parsed_event:\n                yield parsed_event\n\n    async def _create_raw_event_generator(self):\n        event_stream_buffer = EventStreamBuffer()\n        async for chunk, _ in self._raw_stream.content.iter_chunks():\n            event_stream_buffer.add_data(chunk)\n            for event in event_stream_buffer:\n                yield event  # unfortunately no yield from async func support\n\n    async def get_initial_response(self):\n        try:\n            async for event in self._event_generator:\n                event_type = event.headers.get(':event-type')\n                if event_type == 'initial-response':\n                    return event\n\n                break\n        except StopIteration:\n            pass\n        raise NoInitialResponseError()\n\n    # self._raw_stream.close() is sync so no override needed\n", "aiobotocore/waiter.py": "import asyncio\n\nfrom botocore.docs.docstring import WaiterDocstring\n\n# WaiterModel is required for client.py import\nfrom botocore.exceptions import ClientError\nfrom botocore.utils import get_service_module_name\nfrom botocore.waiter import (\n    NormalizedOperationMethod as _NormalizedOperationMethod,\n)\nfrom botocore.waiter import WaiterModel  # noqa: F401 lgtm[py/unused-import]\nfrom botocore.waiter import (\n    Waiter,\n    WaiterError,\n    is_valid_waiter_error,\n    logger,\n    xform_name,\n)\n\n\ndef create_waiter_with_client(waiter_name, waiter_model, client):\n    \"\"\"\n\n    :type waiter_name: str\n    :param waiter_name: The name of the waiter.  The name should match\n        the name (including the casing) of the key name in the waiter\n        model file (typically this is CamelCasing).\n\n    :type waiter_model: botocore.waiter.WaiterModel\n    :param waiter_model: The model for the waiter configuration.\n\n    :type client: botocore.client.BaseClient\n    :param client: The botocore client associated with the service.\n\n    :rtype: botocore.waiter.Waiter\n    :return: The waiter object.\n\n    \"\"\"\n    single_waiter_config = waiter_model.get_waiter(waiter_name)\n    operation_name = xform_name(single_waiter_config.operation)\n    operation_method = NormalizedOperationMethod(\n        getattr(client, operation_name)\n    )\n\n    # Create a new wait method that will serve as a proxy to the underlying\n    # Waiter.wait method. This is needed to attach a docstring to the\n    # method.\n    async def wait(self, **kwargs):\n        return await AIOWaiter.wait(self, **kwargs)\n\n    wait.__doc__ = WaiterDocstring(\n        waiter_name=waiter_name,\n        event_emitter=client.meta.events,\n        service_model=client.meta.service_model,\n        service_waiter_model=waiter_model,\n        include_signature=False,\n    )\n\n    # Rename the waiter class based on the type of waiter.\n    waiter_class_name = str(\n        '%s.Waiter.%s'\n        % (get_service_module_name(client.meta.service_model), waiter_name)\n    )\n\n    # Create the new waiter class\n    documented_waiter_cls = type(waiter_class_name, (Waiter,), {'wait': wait})\n\n    # Return an instance of the new waiter class.\n    return documented_waiter_cls(\n        waiter_name, single_waiter_config, operation_method\n    )\n\n\nclass NormalizedOperationMethod(_NormalizedOperationMethod):\n    async def __call__(self, **kwargs):\n        try:\n            return await self._client_method(**kwargs)\n        except ClientError as e:\n            return e.response\n\n\nclass AIOWaiter(Waiter):\n    async def wait(self, **kwargs):\n        acceptors = list(self.config.acceptors)\n        current_state = 'waiting'\n        # pop the invocation specific config\n        config = kwargs.pop('WaiterConfig', {})\n        sleep_amount = config.get('Delay', self.config.delay)\n        max_attempts = config.get('MaxAttempts', self.config.max_attempts)\n        last_matched_acceptor = None\n        num_attempts = 0\n\n        while True:\n            response = await self._operation_method(**kwargs)\n            num_attempts += 1\n            for acceptor in acceptors:\n                if acceptor.matcher_func(response):\n                    last_matched_acceptor = acceptor\n                    current_state = acceptor.state\n                    break\n            else:\n                # If none of the acceptors matched, we should\n                # transition to the failure state if an error\n                # response was received.\n                if is_valid_waiter_error(response):\n                    # Transition to a failure state, which we\n                    # can just handle here by raising an exception.\n                    raise WaiterError(\n                        name=self.name,\n                        reason='An error occurred (%s): %s'\n                        % (\n                            response['Error'].get('Code', 'Unknown'),\n                            response['Error'].get('Message', 'Unknown'),\n                        ),\n                        last_response=response,\n                    )\n            if current_state == 'success':\n                logger.debug(\n                    \"Waiting complete, waiter matched the \" \"success state.\"\n                )\n                return response\n            if current_state == 'failure':\n                reason = 'Waiter encountered a terminal failure state: %s' % (\n                    acceptor.explanation\n                )\n                raise WaiterError(\n                    name=self.name,\n                    reason=reason,\n                    last_response=response,\n                )\n            if num_attempts >= max_attempts:\n                if last_matched_acceptor is None:\n                    reason = 'Max attempts exceeded'\n                else:\n                    reason = (\n                        'Max attempts exceeded. Previously accepted state: %s'\n                        % (acceptor.explanation)\n                    )\n                raise WaiterError(\n                    name=self.name,\n                    reason=reason,\n                    last_response=response,\n                )\n            await asyncio.sleep(sleep_amount)\n", "aiobotocore/credentials.py": "import asyncio\nimport datetime\nimport json\nimport logging\nimport os\nimport subprocess\nfrom copy import deepcopy\nfrom hashlib import sha1\n\nimport botocore.compat\nfrom botocore import UNSIGNED\nfrom botocore.compat import compat_shell_split\nfrom botocore.config import Config\nfrom botocore.credentials import (\n    _DEFAULT_ADVISORY_REFRESH_TIMEOUT,\n    AssumeRoleCredentialFetcher,\n    AssumeRoleProvider,\n    AssumeRoleWithWebIdentityProvider,\n    BaseAssumeRoleCredentialFetcher,\n    BotoProvider,\n    CachedCredentialFetcher,\n    CanonicalNameCredentialSourcer,\n    ConfigNotFound,\n    ConfigProvider,\n    ContainerMetadataFetcher,\n    ContainerProvider,\n    CredentialResolver,\n    CredentialRetrievalError,\n    Credentials,\n    EnvProvider,\n    InstanceMetadataProvider,\n    InvalidConfigError,\n    MetadataRetrievalError,\n    OriginalEC2Provider,\n    PartialCredentialsError,\n    ProcessProvider,\n    ProfileProviderBuilder,\n    ReadOnlyCredentials,\n    RefreshableCredentials,\n    RefreshWithMFAUnsupportedError,\n    SharedCredentialProvider,\n    SSOProvider,\n    SSOTokenLoader,\n    UnauthorizedSSOTokenError,\n    UnknownCredentialError,\n    _get_client_creator,\n    _local_now,\n    _parse_if_needed,\n    _serialize_if_needed,\n    parse,\n    resolve_imds_endpoint_mode,\n)\nfrom dateutil.tz import tzutc\n\nfrom aiobotocore._helpers import resolve_awaitable\nfrom aiobotocore.config import AioConfig\nfrom aiobotocore.tokens import AioSSOTokenProvider\nfrom aiobotocore.utils import (\n    AioContainerMetadataFetcher,\n    AioInstanceMetadataFetcher,\n)\n\nlogger = logging.getLogger(__name__)\n\n\ndef create_credential_resolver(session, cache=None, region_name=None):\n    \"\"\"Create a default credential resolver.\n    This creates a pre-configured credential resolver\n    that includes the default lookup chain for\n    credentials.\n    \"\"\"\n    profile_name = session.get_config_variable('profile') or 'default'\n    metadata_timeout = session.get_config_variable('metadata_service_timeout')\n    num_attempts = session.get_config_variable('metadata_service_num_attempts')\n    disable_env_vars = session.instance_variables().get('profile') is not None\n\n    imds_config = {\n        'ec2_metadata_service_endpoint': session.get_config_variable(\n            'ec2_metadata_service_endpoint'\n        ),\n        'ec2_metadata_service_endpoint_mode': resolve_imds_endpoint_mode(\n            session\n        ),\n        'ec2_credential_refresh_window': _DEFAULT_ADVISORY_REFRESH_TIMEOUT,\n        'ec2_metadata_v1_disabled': session.get_config_variable(\n            'ec2_metadata_v1_disabled'\n        ),\n    }\n\n    if cache is None:\n        cache = {}\n\n    env_provider = AioEnvProvider()\n    container_provider = AioContainerProvider()\n    instance_metadata_provider = AioInstanceMetadataProvider(\n        iam_role_fetcher=AioInstanceMetadataFetcher(\n            timeout=metadata_timeout,\n            num_attempts=num_attempts,\n            user_agent=session.user_agent(),\n            config=imds_config,\n        )\n    )\n\n    profile_provider_builder = AioProfileProviderBuilder(\n        session, cache=cache, region_name=region_name\n    )\n    assume_role_provider = AioAssumeRoleProvider(\n        load_config=lambda: session.full_config,\n        client_creator=_get_client_creator(session, region_name),\n        cache=cache,\n        profile_name=profile_name,\n        credential_sourcer=AioCanonicalNameCredentialSourcer(\n            [env_provider, container_provider, instance_metadata_provider]\n        ),\n        profile_provider_builder=profile_provider_builder,\n    )\n\n    pre_profile = [\n        env_provider,\n        assume_role_provider,\n    ]\n    profile_providers = profile_provider_builder.providers(\n        profile_name=profile_name,\n        disable_env_vars=disable_env_vars,\n    )\n    post_profile = [\n        AioOriginalEC2Provider(),\n        AioBotoProvider(),\n        container_provider,\n        instance_metadata_provider,\n    ]\n    providers = pre_profile + profile_providers + post_profile\n\n    if disable_env_vars:\n        # An explicitly provided profile will negate an EnvProvider.\n        # We will defer to providers that understand the \"profile\"\n        # concept to retrieve credentials.\n        # The one edge case if is all three values are provided via\n        # env vars:\n        # export AWS_ACCESS_KEY_ID=foo\n        # export AWS_SECRET_ACCESS_KEY=bar\n        # export AWS_PROFILE=baz\n        # Then, just like our client() calls, the explicit credentials\n        # will take precedence.\n        #\n        # This precedence is enforced by leaving the EnvProvider in the chain.\n        # This means that the only way a \"profile\" would win is if the\n        # EnvProvider does not return credentials, which is what we want\n        # in this scenario.\n        providers.remove(env_provider)\n        logger.debug(\n            'Skipping environment variable credential check'\n            ' because profile name was explicitly set.'\n        )\n\n    resolver = AioCredentialResolver(providers=providers)\n    return resolver\n\n\nclass AioProfileProviderBuilder(ProfileProviderBuilder):\n    def _create_process_provider(self, profile_name):\n        return AioProcessProvider(\n            profile_name=profile_name,\n            load_config=lambda: self._session.full_config,\n        )\n\n    def _create_shared_credential_provider(self, profile_name):\n        credential_file = self._session.get_config_variable('credentials_file')\n        return AioSharedCredentialProvider(\n            profile_name=profile_name,\n            creds_filename=credential_file,\n        )\n\n    def _create_config_provider(self, profile_name):\n        config_file = self._session.get_config_variable('config_file')\n        return AioConfigProvider(\n            profile_name=profile_name,\n            config_filename=config_file,\n        )\n\n    def _create_web_identity_provider(self, profile_name, disable_env_vars):\n        return AioAssumeRoleWithWebIdentityProvider(\n            load_config=lambda: self._session.full_config,\n            client_creator=_get_client_creator(\n                self._session, self._region_name\n            ),\n            cache=self._cache,\n            profile_name=profile_name,\n            disable_env_vars=disable_env_vars,\n        )\n\n    def _create_sso_provider(self, profile_name):\n        return AioSSOProvider(\n            load_config=lambda: self._session.full_config,\n            client_creator=self._session.create_client,\n            profile_name=profile_name,\n            cache=self._cache,\n            token_cache=self._sso_token_cache,\n            token_provider=AioSSOTokenProvider(\n                self._session,\n                cache=self._sso_token_cache,\n                profile_name=profile_name,\n            ),\n        )\n\n\nasync def get_credentials(session):\n    resolver = create_credential_resolver(session)\n    return await resolver.load_credentials()\n\n\ndef create_assume_role_refresher(client, params):\n    async def refresh():\n        async with client as sts:\n            response = await sts.assume_role(**params)\n        credentials = response['Credentials']\n        # We need to normalize the credential names to\n        # the values expected by the refresh creds.\n        return {\n            'access_key': credentials['AccessKeyId'],\n            'secret_key': credentials['SecretAccessKey'],\n            'token': credentials['SessionToken'],\n            'expiry_time': _serialize_if_needed(credentials['Expiration']),\n        }\n\n    return refresh\n\n\ndef create_mfa_serial_refresher(actual_refresh):\n    class _Refresher:\n        def __init__(self, refresh):\n            self._refresh = refresh\n            self._has_been_called = False\n\n        async def call(self):\n            if self._has_been_called:\n                # We can explore an option in the future to support\n                # reprompting for MFA, but for now we just error out\n                # when the temp creds expire.\n                raise RefreshWithMFAUnsupportedError()\n            self._has_been_called = True\n            return await self._refresh()\n\n    return _Refresher(actual_refresh).call\n\n\n# TODO: deprecate\ncreate_aio_mfa_serial_refresher = create_mfa_serial_refresher\n\n\nclass AioCredentials(Credentials):\n    async def get_frozen_credentials(self):\n        return ReadOnlyCredentials(\n            self.access_key, self.secret_key, self.token\n        )\n\n\nclass AioRefreshableCredentials(RefreshableCredentials):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._refresh_lock = asyncio.Lock()\n\n    # Redeclaring the properties so it doesn't call refresh\n    # Have to redeclare setter as we're overriding the getter\n    @property\n    def access_key(self):\n        # TODO: this needs to be resolved\n        raise NotImplementedError(\n            \"missing call to self._refresh. \"\n            \"Use get_frozen_credentials instead\"\n        )\n        return self._access_key\n\n    @access_key.setter\n    def access_key(self, value):\n        self._access_key = value\n\n    @property\n    def secret_key(self):\n        # TODO: this needs to be resolved\n        raise NotImplementedError(\n            \"missing call to self._refresh. \"\n            \"Use get_frozen_credentials instead\"\n        )\n        return self._secret_key\n\n    @secret_key.setter\n    def secret_key(self, value):\n        self._secret_key = value\n\n    @property\n    def token(self):\n        # TODO: this needs to be resolved\n        raise NotImplementedError(\n            \"missing call to self._refresh. \"\n            \"Use get_frozen_credentials instead\"\n        )\n        return self._token\n\n    @token.setter\n    def token(self, value):\n        self._token = value\n\n    async def _refresh(self):\n        if not self.refresh_needed(self._advisory_refresh_timeout):\n            return\n\n        # By this point we need a refresh but its not critical\n        if not self._refresh_lock.locked():\n            async with self._refresh_lock:\n                if not self.refresh_needed(self._advisory_refresh_timeout):\n                    return\n                is_mandatory_refresh = self.refresh_needed(\n                    self._mandatory_refresh_timeout\n                )\n                await self._protected_refresh(\n                    is_mandatory=is_mandatory_refresh\n                )\n                return\n        elif self.refresh_needed(self._mandatory_refresh_timeout):\n            # If we're here, we absolutely need a refresh and the\n            # lock is held so wait for it\n            async with self._refresh_lock:\n                # Might have refreshed by now\n                if not self.refresh_needed(self._mandatory_refresh_timeout):\n                    return\n                await self._protected_refresh(is_mandatory=True)\n\n    async def _protected_refresh(self, is_mandatory):\n        try:\n            # AioEnvProvider._create_credentials_fetcher is not and does not need async\n            metadata = await resolve_awaitable(self._refresh_using())\n        except Exception:\n            period_name = 'mandatory' if is_mandatory else 'advisory'\n            logger.warning(\n                \"Refreshing temporary credentials failed \"\n                \"during %s refresh period.\",\n                period_name,\n                exc_info=True,\n            )\n            if is_mandatory:\n                # If this is a mandatory refresh, then\n                # all errors that occur when we attempt to refresh\n                # credentials are propagated back to the user.\n                raise\n            # Otherwise we'll just return.\n            # The end result will be that we'll use the current\n            # set of temporary credentials we have.\n            return\n        self._set_from_data(metadata)\n        self._frozen_credentials = ReadOnlyCredentials(\n            self._access_key, self._secret_key, self._token\n        )\n        if self._is_expired():\n            msg = (\n                \"Credentials were refreshed, but the \"\n                \"refreshed credentials are still expired.\"\n            )\n            logger.warning(msg)\n            raise RuntimeError(msg)\n\n    async def get_frozen_credentials(self):\n        await self._refresh()\n        return self._frozen_credentials\n\n\nclass AioDeferredRefreshableCredentials(AioRefreshableCredentials):\n    def __init__(self, refresh_using, method, time_fetcher=_local_now):\n        self._refresh_using = refresh_using\n        self._access_key = None\n        self._secret_key = None\n        self._token = None\n        self._expiry_time = None\n        self._time_fetcher = time_fetcher\n        self._refresh_lock = asyncio.Lock()\n        self.method = method\n        self._frozen_credentials = None\n\n    def refresh_needed(self, refresh_in=None):\n        if self._frozen_credentials is None:\n            return True\n        return super().refresh_needed(refresh_in)\n\n\nclass AioCachedCredentialFetcher(CachedCredentialFetcher):\n    async def _get_credentials(self):\n        raise NotImplementedError('_get_credentials()')\n\n    async def fetch_credentials(self):\n        return await self._get_cached_credentials()\n\n    async def _get_cached_credentials(self):\n        \"\"\"Get up-to-date credentials.\n\n        This will check the cache for up-to-date credentials, calling assume\n        role if none are available.\n        \"\"\"\n        response = self._load_from_cache()\n        if response is None:\n            response = await self._get_credentials()\n            self._write_to_cache(response)\n        else:\n            logger.debug(\"Credentials for role retrieved from cache.\")\n\n        creds = response['Credentials']\n        expiration = _serialize_if_needed(creds['Expiration'], iso=True)\n        return {\n            'access_key': creds['AccessKeyId'],\n            'secret_key': creds['SecretAccessKey'],\n            'token': creds['SessionToken'],\n            'expiry_time': expiration,\n        }\n\n\nclass AioBaseAssumeRoleCredentialFetcher(\n    BaseAssumeRoleCredentialFetcher, AioCachedCredentialFetcher\n):\n    pass\n\n\nclass AioAssumeRoleCredentialFetcher(\n    AssumeRoleCredentialFetcher, AioBaseAssumeRoleCredentialFetcher\n):\n    async def _get_credentials(self):\n        \"\"\"Get credentials by calling assume role.\"\"\"\n        kwargs = self._assume_role_kwargs()\n        client = await self._create_client()\n        async with client as sts:\n            return await sts.assume_role(**kwargs)\n\n    async def _create_client(self):\n        \"\"\"Create an STS client using the source credentials.\"\"\"\n        frozen_credentials = (\n            await self._source_credentials.get_frozen_credentials()\n        )\n        return self._client_creator(\n            'sts',\n            aws_access_key_id=frozen_credentials.access_key,\n            aws_secret_access_key=frozen_credentials.secret_key,\n            aws_session_token=frozen_credentials.token,\n        )\n\n\nclass AioAssumeRoleWithWebIdentityCredentialFetcher(\n    AioBaseAssumeRoleCredentialFetcher\n):\n    def __init__(\n        self,\n        client_creator,\n        web_identity_token_loader,\n        role_arn,\n        extra_args=None,\n        cache=None,\n        expiry_window_seconds=None,\n    ):\n        self._web_identity_token_loader = web_identity_token_loader\n\n        super().__init__(\n            client_creator,\n            role_arn,\n            extra_args=extra_args,\n            cache=cache,\n            expiry_window_seconds=expiry_window_seconds,\n        )\n\n    async def _get_credentials(self):\n        \"\"\"Get credentials by calling assume role.\"\"\"\n        kwargs = self._assume_role_kwargs()\n        # Assume role with web identity does not require credentials other than\n        # the token, explicitly configure the client to not sign requests.\n        config = AioConfig(signature_version=UNSIGNED)\n        async with self._client_creator('sts', config=config) as client:\n            return await client.assume_role_with_web_identity(**kwargs)\n\n    def _assume_role_kwargs(self):\n        \"\"\"Get the arguments for assume role based on current configuration.\"\"\"\n        assume_role_kwargs = deepcopy(self._assume_kwargs)\n        identity_token = self._web_identity_token_loader()\n        assume_role_kwargs['WebIdentityToken'] = identity_token\n\n        return assume_role_kwargs\n\n\nclass AioProcessProvider(ProcessProvider):\n    def __init__(self, *args, popen=asyncio.create_subprocess_exec, **kwargs):\n        super().__init__(*args, **kwargs, popen=popen)\n\n    async def load(self):\n        credential_process = self._credential_process\n        if credential_process is None:\n            return\n\n        creds_dict = await self._retrieve_credentials_using(credential_process)\n        if creds_dict.get('expiry_time') is not None:\n            return AioRefreshableCredentials.create_from_metadata(\n                creds_dict,\n                lambda: self._retrieve_credentials_using(credential_process),\n                self.METHOD,\n            )\n\n        return AioCredentials(\n            access_key=creds_dict['access_key'],\n            secret_key=creds_dict['secret_key'],\n            token=creds_dict.get('token'),\n            method=self.METHOD,\n        )\n\n    async def _retrieve_credentials_using(self, credential_process):\n        # We're not using shell=True, so we need to pass the\n        # command and all arguments as a list.\n        process_list = compat_shell_split(credential_process)\n        p = await self._popen(\n            *process_list, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n        )\n        stdout, stderr = await p.communicate()\n        if p.returncode != 0:\n            raise CredentialRetrievalError(\n                provider=self.METHOD, error_msg=stderr.decode('utf-8')\n            )\n        parsed = botocore.compat.json.loads(stdout.decode('utf-8'))\n        version = parsed.get('Version', '<Version key not provided>')\n        if version != 1:\n            raise CredentialRetrievalError(\n                provider=self.METHOD,\n                error_msg=(\n                    f\"Unsupported version '{version}' for credential process \"\n                    f\"provider, supported versions: 1\"\n                ),\n            )\n        try:\n            return {\n                'access_key': parsed['AccessKeyId'],\n                'secret_key': parsed['SecretAccessKey'],\n                'token': parsed.get('SessionToken'),\n                'expiry_time': parsed.get('Expiration'),\n            }\n        except KeyError as e:\n            raise CredentialRetrievalError(\n                provider=self.METHOD,\n                error_msg=f\"Missing required key in response: {e}\",\n            )\n\n\nclass AioInstanceMetadataProvider(InstanceMetadataProvider):\n    async def load(self):\n        fetcher = self._role_fetcher\n        metadata = await fetcher.retrieve_iam_role_credentials()\n        if not metadata:\n            return None\n        logger.info(\n            'Found credentials from IAM Role: %s', metadata['role_name']\n        )\n\n        creds = AioRefreshableCredentials.create_from_metadata(\n            metadata,\n            method=self.METHOD,\n            refresh_using=fetcher.retrieve_iam_role_credentials,\n        )\n        return creds\n\n\nclass AioEnvProvider(EnvProvider):\n    async def load(self):\n        access_key = self.environ.get(self._mapping['access_key'], '')\n\n        if access_key:\n            logger.info('Found credentials in environment variables.')\n            fetcher = self._create_credentials_fetcher()\n            credentials = fetcher(require_expiry=False)\n\n            expiry_time = credentials['expiry_time']\n            if expiry_time is not None:\n                expiry_time = parse(expiry_time)\n                return AioRefreshableCredentials(\n                    credentials['access_key'],\n                    credentials['secret_key'],\n                    credentials['token'],\n                    expiry_time,\n                    refresh_using=fetcher,\n                    method=self.METHOD,\n                )\n\n            return AioCredentials(\n                credentials['access_key'],\n                credentials['secret_key'],\n                credentials['token'],\n                method=self.METHOD,\n            )\n        else:\n            return None\n\n\nclass AioOriginalEC2Provider(OriginalEC2Provider):\n    async def load(self):\n        if 'AWS_CREDENTIAL_FILE' in self._environ:\n            full_path = os.path.expanduser(\n                self._environ['AWS_CREDENTIAL_FILE']\n            )\n            creds = self._parser(full_path)\n            if self.ACCESS_KEY in creds:\n                logger.info('Found credentials in AWS_CREDENTIAL_FILE.')\n                access_key = creds[self.ACCESS_KEY]\n                secret_key = creds[self.SECRET_KEY]\n                # EC2 creds file doesn't support session tokens.\n                return AioCredentials(\n                    access_key, secret_key, method=self.METHOD\n                )\n        else:\n            return None\n\n\nclass AioSharedCredentialProvider(SharedCredentialProvider):\n    async def load(self):\n        try:\n            available_creds = self._ini_parser(self._creds_filename)\n        except ConfigNotFound:\n            return None\n        if self._profile_name in available_creds:\n            config = available_creds[self._profile_name]\n            if self.ACCESS_KEY in config:\n                logger.info(\n                    \"Found credentials in shared credentials file: %s\",\n                    self._creds_filename,\n                )\n                access_key, secret_key = self._extract_creds_from_mapping(\n                    config, self.ACCESS_KEY, self.SECRET_KEY\n                )\n                token = self._get_session_token(config)\n                return AioCredentials(\n                    access_key, secret_key, token, method=self.METHOD\n                )\n\n\nclass AioConfigProvider(ConfigProvider):\n    async def load(self):\n        try:\n            full_config = self._config_parser(self._config_filename)\n        except ConfigNotFound:\n            return None\n        if self._profile_name in full_config['profiles']:\n            profile_config = full_config['profiles'][self._profile_name]\n            if self.ACCESS_KEY in profile_config:\n                logger.info(\n                    \"Credentials found in config file: %s\",\n                    self._config_filename,\n                )\n                access_key, secret_key = self._extract_creds_from_mapping(\n                    profile_config, self.ACCESS_KEY, self.SECRET_KEY\n                )\n                token = self._get_session_token(profile_config)\n                return AioCredentials(\n                    access_key, secret_key, token, method=self.METHOD\n                )\n        else:\n            return None\n\n\nclass AioBotoProvider(BotoProvider):\n    async def load(self):\n        if self.BOTO_CONFIG_ENV in self._environ:\n            potential_locations = [self._environ[self.BOTO_CONFIG_ENV]]\n        else:\n            potential_locations = self.DEFAULT_CONFIG_FILENAMES\n        for filename in potential_locations:\n            try:\n                config = self._ini_parser(filename)\n            except ConfigNotFound:\n                # Move on to the next potential config file name.\n                continue\n            if 'Credentials' in config:\n                credentials = config['Credentials']\n                if self.ACCESS_KEY in credentials:\n                    logger.info(\n                        \"Found credentials in boto config file: %s\", filename\n                    )\n                    access_key, secret_key = self._extract_creds_from_mapping(\n                        credentials, self.ACCESS_KEY, self.SECRET_KEY\n                    )\n                    return AioCredentials(\n                        access_key, secret_key, method=self.METHOD\n                    )\n\n\nclass AioAssumeRoleProvider(AssumeRoleProvider):\n    async def load(self):\n        self._loaded_config = self._load_config()\n        profiles = self._loaded_config.get('profiles', {})\n        profile = profiles.get(self._profile_name, {})\n        if self._has_assume_role_config_vars(profile):\n            return await self._load_creds_via_assume_role(self._profile_name)\n\n    async def _load_creds_via_assume_role(self, profile_name):\n        role_config = self._get_role_config(profile_name)\n        source_credentials = await self._resolve_source_credentials(\n            role_config, profile_name\n        )\n\n        extra_args = {}\n        role_session_name = role_config.get('role_session_name')\n        if role_session_name is not None:\n            extra_args['RoleSessionName'] = role_session_name\n\n        external_id = role_config.get('external_id')\n        if external_id is not None:\n            extra_args['ExternalId'] = external_id\n\n        mfa_serial = role_config.get('mfa_serial')\n        if mfa_serial is not None:\n            extra_args['SerialNumber'] = mfa_serial\n\n        duration_seconds = role_config.get('duration_seconds')\n        if duration_seconds is not None:\n            extra_args['DurationSeconds'] = duration_seconds\n\n        fetcher = AioAssumeRoleCredentialFetcher(\n            client_creator=self._client_creator,\n            source_credentials=source_credentials,\n            role_arn=role_config['role_arn'],\n            extra_args=extra_args,\n            mfa_prompter=self._prompter,\n            cache=self.cache,\n        )\n        refresher = fetcher.fetch_credentials\n        if mfa_serial is not None:\n            refresher = create_mfa_serial_refresher(refresher)\n\n        # The initial credentials are empty and the expiration time is set\n        # to now so that we can delay the call to assume role until it is\n        # strictly needed.\n        return AioDeferredRefreshableCredentials(\n            method=self.METHOD,\n            refresh_using=refresher,\n            time_fetcher=_local_now,\n        )\n\n    async def _resolve_source_credentials(self, role_config, profile_name):\n        credential_source = role_config.get('credential_source')\n        if credential_source is not None:\n            return await self._resolve_credentials_from_source(\n                credential_source, profile_name\n            )\n\n        source_profile = role_config['source_profile']\n        self._visited_profiles.append(source_profile)\n        return await self._resolve_credentials_from_profile(source_profile)\n\n    async def _resolve_credentials_from_profile(self, profile_name):\n        profiles = self._loaded_config.get('profiles', {})\n        profile = profiles[profile_name]\n\n        if (\n            self._has_static_credentials(profile)\n            and not self._profile_provider_builder\n        ):\n            return self._resolve_static_credentials_from_profile(profile)\n        elif self._has_static_credentials(\n            profile\n        ) or not self._has_assume_role_config_vars(profile):\n            profile_providers = self._profile_provider_builder.providers(\n                profile_name=profile_name,\n                disable_env_vars=True,\n            )\n            profile_chain = AioCredentialResolver(profile_providers)\n            credentials = await profile_chain.load_credentials()\n            if credentials is None:\n                error_message = (\n                    'The source profile \"%s\" must have credentials.'\n                )\n                raise InvalidConfigError(\n                    error_msg=error_message % profile_name,\n                )\n            return credentials\n\n        return await self._load_creds_via_assume_role(profile_name)\n\n    def _resolve_static_credentials_from_profile(self, profile):\n        try:\n            return AioCredentials(\n                access_key=profile['aws_access_key_id'],\n                secret_key=profile['aws_secret_access_key'],\n                token=profile.get('aws_session_token'),\n            )\n        except KeyError as e:\n            raise PartialCredentialsError(\n                provider=self.METHOD, cred_var=str(e)\n            )\n\n    async def _resolve_credentials_from_source(\n        self, credential_source, profile_name\n    ):\n        credentials = await self._credential_sourcer.source_credentials(\n            credential_source\n        )\n        if credentials is None:\n            raise CredentialRetrievalError(\n                provider=credential_source,\n                error_msg=(\n                    'No credentials found in credential_source referenced '\n                    'in profile %s' % profile_name\n                ),\n            )\n        return credentials\n\n\nclass AioAssumeRoleWithWebIdentityProvider(AssumeRoleWithWebIdentityProvider):\n    async def load(self):\n        return await self._assume_role_with_web_identity()\n\n    async def _assume_role_with_web_identity(self):\n        token_path = self._get_config('web_identity_token_file')\n        if not token_path:\n            return None\n        token_loader = self._token_loader_cls(token_path)\n\n        role_arn = self._get_config('role_arn')\n        if not role_arn:\n            error_msg = (\n                'The provided profile or the current environment is '\n                'configured to assume role with web identity but has no '\n                'role ARN configured. Ensure that the profile has the role_arn'\n                'configuration set or the AWS_ROLE_ARN env var is set.'\n            )\n            raise InvalidConfigError(error_msg=error_msg)\n\n        extra_args = {}\n        role_session_name = self._get_config('role_session_name')\n        if role_session_name is not None:\n            extra_args['RoleSessionName'] = role_session_name\n\n        fetcher = AioAssumeRoleWithWebIdentityCredentialFetcher(\n            client_creator=self._client_creator,\n            web_identity_token_loader=token_loader,\n            role_arn=role_arn,\n            extra_args=extra_args,\n            cache=self.cache,\n        )\n        # The initial credentials are empty and the expiration time is set\n        # to now so that we can delay the call to assume role until it is\n        # strictly needed.\n        return AioDeferredRefreshableCredentials(\n            method=self.METHOD,\n            refresh_using=fetcher.fetch_credentials,\n        )\n\n\nclass AioCanonicalNameCredentialSourcer(CanonicalNameCredentialSourcer):\n    async def source_credentials(self, source_name):\n        \"\"\"Loads source credentials based on the provided configuration.\n\n        :type source_name: str\n        :param source_name: The value of credential_source in the config\n            file. This is the canonical name of the credential provider.\n\n        :rtype: Credentials\n        \"\"\"\n        source = self._get_provider(source_name)\n        if isinstance(source, AioCredentialResolver):\n            return await source.load_credentials()\n        return await source.load()\n\n    def _get_provider(self, canonical_name):\n        \"\"\"Return a credential provider by its canonical name.\n\n        :type canonical_name: str\n        :param canonical_name: The canonical name of the provider.\n\n        :raises UnknownCredentialError: Raised if no\n            credential provider by the provided name\n            is found.\n        \"\"\"\n        provider = self._get_provider_by_canonical_name(canonical_name)\n\n        # The AssumeRole provider should really be part of the SharedConfig\n        # provider rather than being its own thing, but it is not. It is\n        # effectively part of both the SharedConfig provider and the\n        # SharedCredentials provider now due to the way it behaves.\n        # Therefore if we want either of those providers we should return\n        # the AssumeRole provider with it.\n        if canonical_name.lower() in ['sharedconfig', 'sharedcredentials']:\n            assume_role_provider = self._get_provider_by_method('assume-role')\n            if assume_role_provider is not None:\n                # The SharedConfig or SharedCredentials provider may not be\n                # present if it was removed for some reason, but the\n                # AssumeRole provider could still be present. In that case,\n                # return the assume role provider by itself.\n                if provider is None:\n                    return assume_role_provider\n\n                # If both are present, return them both as a\n                # CredentialResolver so that calling code can treat them as\n                # a single entity.\n                return AioCredentialResolver([assume_role_provider, provider])\n\n        if provider is None:\n            raise UnknownCredentialError(name=canonical_name)\n\n        return provider\n\n\nclass AioContainerProvider(ContainerProvider):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # This will always run if no fetcher arg is provided\n        if isinstance(self._fetcher, ContainerMetadataFetcher):\n            self._fetcher = AioContainerMetadataFetcher()\n\n    async def load(self):\n        if self.ENV_VAR in self._environ or self.ENV_VAR_FULL in self._environ:\n            return await self._retrieve_or_fail()\n\n    async def _retrieve_or_fail(self):\n        if self._provided_relative_uri():\n            full_uri = self._fetcher.full_url(self._environ[self.ENV_VAR])\n        else:\n            full_uri = self._environ[self.ENV_VAR_FULL]\n        fetcher = self._create_fetcher(full_uri)\n        creds = await fetcher()\n        return AioRefreshableCredentials(\n            access_key=creds['access_key'],\n            secret_key=creds['secret_key'],\n            token=creds['token'],\n            method=self.METHOD,\n            expiry_time=_parse_if_needed(creds['expiry_time']),\n            refresh_using=fetcher,\n        )\n\n    def _create_fetcher(self, full_uri, *args, **kwargs):\n        async def fetch_creds():\n            try:\n                headers = self._build_headers()\n                response = await self._fetcher.retrieve_full_uri(\n                    full_uri, headers=headers\n                )\n            except MetadataRetrievalError as e:\n                logger.debug(\n                    \"Error retrieving container metadata: %s\", e, exc_info=True\n                )\n                raise CredentialRetrievalError(\n                    provider=self.METHOD, error_msg=str(e)\n                )\n            return {\n                'access_key': response['AccessKeyId'],\n                'secret_key': response['SecretAccessKey'],\n                'token': response['Token'],\n                'expiry_time': response['Expiration'],\n            }\n\n        return fetch_creds\n\n\nclass AioCredentialResolver(CredentialResolver):\n    async def load_credentials(self):\n        \"\"\"\n        Goes through the credentials chain, returning the first ``Credentials``\n        that could be loaded.\n        \"\"\"\n        # First provider to return a non-None response wins.\n        for provider in self.providers:\n            logger.debug(\"Looking for credentials via: %s\", provider.METHOD)\n            creds = await provider.load()\n            if creds is not None:\n                return creds\n\n        # If we got here, no credentials could be found.\n        # This feels like it should be an exception, but historically, ``None``\n        # is returned.\n        #\n        # +1\n        # -js\n        return None\n\n\nclass AioSSOCredentialFetcher(AioCachedCredentialFetcher):\n    _UTC_DATE_FORMAT = '%Y-%m-%dT%H:%M:%SZ'\n\n    def __init__(\n        self,\n        start_url,\n        sso_region,\n        role_name,\n        account_id,\n        client_creator,\n        token_loader=None,\n        cache=None,\n        expiry_window_seconds=None,\n        token_provider=None,\n        sso_session_name=None,\n    ):\n        self._client_creator = client_creator\n        self._sso_region = sso_region\n        self._role_name = role_name\n        self._account_id = account_id\n        self._start_url = start_url\n        self._token_loader = token_loader\n        self._token_provider = token_provider\n        self._sso_session_name = sso_session_name\n        super().__init__(cache, expiry_window_seconds)\n\n    def _create_cache_key(self):\n        args = {\n            'roleName': self._role_name,\n            'accountId': self._account_id,\n        }\n        if self._sso_session_name:\n            args['sessionName'] = self._sso_session_name\n        else:\n            args['startUrl'] = self._start_url\n\n        args = json.dumps(args, sort_keys=True, separators=(',', ':'))\n        argument_hash = sha1(args.encode('utf-8')).hexdigest()\n        return self._make_file_safe(argument_hash)\n\n    def _parse_timestamp(self, timestamp_ms):\n        # fromtimestamp expects seconds so: milliseconds / 1000 = seconds\n        timestamp_seconds = timestamp_ms / 1000.0\n        timestamp = datetime.datetime.fromtimestamp(timestamp_seconds, tzutc())\n        return timestamp.strftime(self._UTC_DATE_FORMAT)\n\n    async def _get_credentials(self):\n        \"\"\"Get credentials by calling SSO get role credentials.\"\"\"\n        config = Config(\n            signature_version=UNSIGNED,\n            region_name=self._sso_region,\n        )\n        async with self._client_creator('sso', config=config) as client:\n            if self._token_provider:\n                initial_token_data = self._token_provider.load_token()\n                token = (await initial_token_data.get_frozen_token()).token\n            else:\n                token = self._token_loader(self._start_url)['accessToken']\n\n            kwargs = {\n                'roleName': self._role_name,\n                'accountId': self._account_id,\n                'accessToken': token,\n            }\n            try:\n                response = await client.get_role_credentials(**kwargs)\n            except client.exceptions.UnauthorizedException:\n                raise UnauthorizedSSOTokenError()\n            credentials = response['roleCredentials']\n\n            credentials = {\n                'ProviderType': 'sso',\n                'Credentials': {\n                    'AccessKeyId': credentials['accessKeyId'],\n                    'SecretAccessKey': credentials['secretAccessKey'],\n                    'SessionToken': credentials['sessionToken'],\n                    'Expiration': self._parse_timestamp(\n                        credentials['expiration']\n                    ),\n                },\n            }\n            return credentials\n\n\nclass AioSSOProvider(SSOProvider):\n    async def load(self):\n        sso_config = self._load_sso_config()\n        if not sso_config:\n            return None\n\n        fetcher_kwargs = {\n            'start_url': sso_config['sso_start_url'],\n            'sso_region': sso_config['sso_region'],\n            'role_name': sso_config['sso_role_name'],\n            'account_id': sso_config['sso_account_id'],\n            'client_creator': self._client_creator,\n            'token_loader': SSOTokenLoader(cache=self._token_cache),\n            'cache': self.cache,\n        }\n        if 'sso_session' in sso_config:\n            fetcher_kwargs['sso_session_name'] = sso_config['sso_session']\n            fetcher_kwargs['token_provider'] = self._token_provider\n\n        sso_fetcher = AioSSOCredentialFetcher(**fetcher_kwargs)\n\n        return AioDeferredRefreshableCredentials(\n            method=self.METHOD,\n            refresh_using=sso_fetcher.fetch_credentials,\n        )\n", "aiobotocore/configprovider.py": "from botocore.configprovider import SmartDefaultsConfigStoreFactory, os\n\n\nclass AioSmartDefaultsConfigStoreFactory(SmartDefaultsConfigStoreFactory):\n    async def merge_smart_defaults(self, config_store, mode, region_name):\n        if mode == 'auto':\n            mode = await self.resolve_auto_mode(region_name)\n        default_configs = (\n            self._default_config_resolver.get_default_config_values(mode)\n        )\n        for config_var in default_configs:\n            config_value = default_configs[config_var]\n            method = getattr(self, f'_set_{config_var}', None)\n            if method:\n                method(config_store, config_value)\n\n    async def resolve_auto_mode(self, region_name):\n        current_region = None\n        if os.environ.get('AWS_EXECUTION_ENV'):\n            default_region = os.environ.get('AWS_DEFAULT_REGION')\n            current_region = os.environ.get('AWS_REGION', default_region)\n        if not current_region:\n            if self._instance_metadata_region:\n                current_region = self._instance_metadata_region\n            else:\n                try:\n                    current_region = await self._imds_region_provider.provide()\n                    self._instance_metadata_region = current_region\n                except Exception:\n                    pass\n\n        if current_region:\n            if region_name == current_region:\n                return 'in-region'\n            else:\n                return 'cross-region'\n        return 'standard'\n", "aiobotocore/client.py": "from botocore.awsrequest import prepare_request_dict\nfrom botocore.client import (\n    BaseClient,\n    ClientCreator,\n    ClientEndpointBridge,\n    PaginatorDocstring,\n    logger,\n    resolve_checksum_context,\n)\nfrom botocore.compress import maybe_compress_request\nfrom botocore.discovery import block_endpoint_discovery_required_operations\nfrom botocore.exceptions import OperationNotPageableError, UnknownServiceError\nfrom botocore.history import get_global_history_recorder\nfrom botocore.hooks import first_non_none_response\nfrom botocore.utils import get_service_module_name\nfrom botocore.waiter import xform_name\n\nfrom . import waiter\nfrom .args import AioClientArgsCreator\nfrom .credentials import AioRefreshableCredentials\nfrom .discovery import AioEndpointDiscoveryHandler, AioEndpointDiscoveryManager\nfrom .httpchecksum import apply_request_checksum\nfrom .paginate import AioPaginator\nfrom .retries import adaptive, standard\nfrom .utils import AioS3ExpressIdentityResolver, AioS3RegionRedirectorv2\n\nhistory_recorder = get_global_history_recorder()\n\n\nclass AioClientCreator(ClientCreator):\n    async def create_client(\n        self,\n        service_name,\n        region_name,\n        is_secure=True,\n        endpoint_url=None,\n        verify=None,\n        credentials=None,\n        scoped_config=None,\n        api_version=None,\n        client_config=None,\n        auth_token=None,\n    ):\n        responses = await self._event_emitter.emit(\n            'choose-service-name', service_name=service_name\n        )\n        service_name = first_non_none_response(responses, default=service_name)\n        service_model = self._load_service_model(service_name, api_version)\n        try:\n            endpoints_ruleset_data = self._load_service_endpoints_ruleset(\n                service_name, api_version\n            )\n            partition_data = self._loader.load_data('partitions')\n        except UnknownServiceError:\n            endpoints_ruleset_data = None\n            partition_data = None\n            logger.info(\n                'No endpoints ruleset found for service %s, falling back to '\n                'legacy endpoint routing.',\n                service_name,\n            )\n\n        cls = await self._create_client_class(service_name, service_model)\n        region_name, client_config = self._normalize_fips_region(\n            region_name, client_config\n        )\n        endpoint_bridge = ClientEndpointBridge(\n            self._endpoint_resolver,\n            scoped_config,\n            client_config,\n            service_signing_name=service_model.metadata.get('signingName'),\n            config_store=self._config_store,\n            service_signature_version=service_model.metadata.get(\n                'signatureVersion'\n            ),\n        )\n        client_args = self._get_client_args(\n            service_model,\n            region_name,\n            is_secure,\n            endpoint_url,\n            verify,\n            credentials,\n            scoped_config,\n            client_config,\n            endpoint_bridge,\n            auth_token,\n            endpoints_ruleset_data,\n            partition_data,\n        )\n        service_client = cls(**client_args)\n        self._register_retries(service_client)\n        self._register_s3_events(\n            client=service_client,\n            endpoint_bridge=None,\n            endpoint_url=None,\n            client_config=client_config,\n            scoped_config=scoped_config,\n        )\n        self._register_s3express_events(client=service_client)\n        self._register_s3_control_events(client=service_client)\n        self._register_endpoint_discovery(\n            service_client, endpoint_url, client_config\n        )\n        return service_client\n\n    async def _create_client_class(self, service_name, service_model):\n        class_attributes = self._create_methods(service_model)\n        py_name_to_operation_name = self._create_name_mapping(service_model)\n        class_attributes['_PY_TO_OP_NAME'] = py_name_to_operation_name\n        bases = [AioBaseClient]\n        service_id = service_model.service_id.hyphenize()\n        await self._event_emitter.emit(\n            'creating-client-class.%s' % service_id,\n            class_attributes=class_attributes,\n            base_classes=bases,\n        )\n        class_name = get_service_module_name(service_model)\n        cls = type(str(class_name), tuple(bases), class_attributes)\n        return cls\n\n    def _register_retries(self, client):\n        # botocore retry handlers may block. We add our own implementation here.\n        # botocore provides three implementations:\n        #\n        # 1) standard\n        # This one doesn't block. A threading.Lock is used in quota.RetryQuota,\n        # but it's only used to protect concurrent modifications of internal\n        # state inside multithreaded programs. When running under a single\n        # asyncio thread, this lock will be acquired and released in the same\n        # coroutine, and the coroutine will never block waiting for the lock.\n        # Thus, we don't need to redefine this strategy.\n        #\n        # 2) adaptive\n        # This one blocks when the client is applying self rate limiting.\n        # We override the corresponding definition to replace it with async\n        # objects.\n        #\n        # 3) legacy\n        # This one probably doesn't block.\n        #\n        # The code for this method comes directly from botocore. We could\n        # override `_register_v2_adaptive_retries` only. The override for\n        # `_register_retries` is only included for clarity.\n        retry_mode = client.meta.config.retries['mode']\n        if retry_mode == 'standard':\n            self._register_v2_standard_retries(client)\n        elif retry_mode == 'adaptive':\n            self._register_v2_standard_retries(client)\n            self._register_v2_adaptive_retries(client)\n        elif retry_mode == 'legacy':\n            self._register_legacy_retries(client)\n\n    def _register_v2_standard_retries(self, client):\n        max_attempts = client.meta.config.retries.get('total_max_attempts')\n        kwargs = {'client': client}\n        if max_attempts is not None:\n            kwargs['max_attempts'] = max_attempts\n        standard.register_retry_handler(**kwargs)\n\n    def _register_v2_adaptive_retries(self, client):\n        # See comment in `_register_retries`.\n        # Note that this `adaptive` module is an aiobotocore reimplementation.\n        adaptive.register_retry_handler(client)\n\n    def _register_legacy_retries(self, client):\n        endpoint_prefix = client.meta.service_model.endpoint_prefix\n        service_id = client.meta.service_model.service_id\n        service_event_name = service_id.hyphenize()\n\n        # First, we load the entire retry config for all services,\n        # then pull out just the information we need.\n        original_config = self._loader.load_data('_retry')\n        if not original_config:\n            return\n\n        retries = self._transform_legacy_retries(client.meta.config.retries)\n        retry_config = self._retry_config_translator.build_retry_config(\n            endpoint_prefix,\n            original_config.get('retry', {}),\n            original_config.get('definitions', {}),\n            retries,\n        )\n\n        logger.debug(\n            \"Registering retry handlers for service: %s\",\n            client.meta.service_model.service_name,\n        )\n        handler = self._retry_handler_factory.create_retry_handler(\n            retry_config, endpoint_prefix\n        )\n        unique_id = 'retry-config-%s' % service_event_name\n        client.meta.events.register(\n            f\"needs-retry.{service_event_name}\", handler, unique_id=unique_id\n        )\n\n    def _register_endpoint_discovery(self, client, endpoint_url, config):\n        if endpoint_url is not None:\n            # Don't register any handlers in the case of a custom endpoint url\n            return\n        # Only attach handlers if the service supports discovery\n        if client.meta.service_model.endpoint_discovery_operation is None:\n            return\n        events = client.meta.events\n        service_id = client.meta.service_model.service_id.hyphenize()\n        enabled = False\n        if config and config.endpoint_discovery_enabled is not None:\n            enabled = config.endpoint_discovery_enabled\n        elif self._config_store:\n            enabled = self._config_store.get_config_variable(\n                'endpoint_discovery_enabled'\n            )\n\n        enabled = self._normalize_endpoint_discovery_config(enabled)\n        if enabled and self._requires_endpoint_discovery(client, enabled):\n            discover = enabled is True\n            manager = AioEndpointDiscoveryManager(\n                client, always_discover=discover\n            )\n            handler = AioEndpointDiscoveryHandler(manager)\n            handler.register(events, service_id)\n        else:\n            events.register(\n                'before-parameter-build',\n                block_endpoint_discovery_required_operations,\n            )\n\n    def _register_s3express_events(\n        self,\n        client,\n        endpoint_bridge=None,\n        endpoint_url=None,\n        client_config=None,\n        scoped_config=None,\n    ):\n        if client.meta.service_model.service_name != 's3':\n            return\n        AioS3ExpressIdentityResolver(\n            client, AioRefreshableCredentials\n        ).register()\n\n    def _register_s3_events(\n        self,\n        client,\n        endpoint_bridge,\n        endpoint_url,\n        client_config,\n        scoped_config,\n    ):\n        if client.meta.service_model.service_name != 's3':\n            return\n        AioS3RegionRedirectorv2(None, client).register()\n        self._set_s3_presign_signature_version(\n            client.meta, client_config, scoped_config\n        )\n        client.meta.events.register(\n            'before-parameter-build.s3', self._inject_s3_input_parameters\n        )\n\n    def _get_client_args(\n        self,\n        service_model,\n        region_name,\n        is_secure,\n        endpoint_url,\n        verify,\n        credentials,\n        scoped_config,\n        client_config,\n        endpoint_bridge,\n        auth_token,\n        endpoints_ruleset_data,\n        partition_data,\n    ):\n        # This is a near copy of ClientCreator. What's replaced\n        # is ClientArgsCreator->AioClientArgsCreator\n        args_creator = AioClientArgsCreator(\n            self._event_emitter,\n            self._user_agent,\n            self._response_parser_factory,\n            self._loader,\n            self._exceptions_factory,\n            config_store=self._config_store,\n            user_agent_creator=self._user_agent_creator,\n        )\n        return args_creator.get_client_args(\n            service_model,\n            region_name,\n            is_secure,\n            endpoint_url,\n            verify,\n            credentials,\n            scoped_config,\n            client_config,\n            endpoint_bridge,\n            auth_token,\n            endpoints_ruleset_data,\n            partition_data,\n        )\n\n\nclass AioBaseClient(BaseClient):\n    async def _async_getattr(self, item):\n        event_name = 'getattr.{}.{}'.format(\n            self._service_model.service_id.hyphenize(), item\n        )\n        handler, event_response = await self.meta.events.emit_until_response(\n            event_name, client=self\n        )\n\n        return event_response\n\n    def __getattr__(self, item):\n        # NOTE: we can not reliably support this because if we were to make this a\n        # deferred attrgetter (See #803), it would resolve in hasattr always returning\n        # true.  This ends up breaking ddtrace for example when it tries to set a pin.\n        raise AttributeError(\n            \"'{}' object has no attribute '{}'\".format(\n                self.__class__.__name__, item\n            )\n        )\n\n    async def close(self):\n        \"\"\"Closes underlying endpoint connections.\"\"\"\n        await self._endpoint.close()\n\n    async def _make_api_call(self, operation_name, api_params):\n        operation_model = self._service_model.operation_model(operation_name)\n        service_name = self._service_model.service_name\n        history_recorder.record(\n            'API_CALL',\n            {\n                'service': service_name,\n                'operation': operation_name,\n                'params': api_params,\n            },\n        )\n        if operation_model.deprecated:\n            logger.debug(\n                'Warning: %s.%s() is deprecated', service_name, operation_name\n            )\n        request_context = {\n            'client_region': self.meta.region_name,\n            'client_config': self.meta.config,\n            'has_streaming_input': operation_model.has_streaming_input,\n            'auth_type': operation_model.auth_type,\n        }\n        api_params = await self._emit_api_params(\n            api_params=api_params,\n            operation_model=operation_model,\n            context=request_context,\n        )\n        (\n            endpoint_url,\n            additional_headers,\n            properties,\n        ) = await self._resolve_endpoint_ruleset(\n            operation_model, api_params, request_context\n        )\n        if properties:\n            # Pass arbitrary endpoint info with the Request\n            # for use during construction.\n            request_context['endpoint_properties'] = properties\n        request_dict = await self._convert_to_request_dict(\n            api_params=api_params,\n            operation_model=operation_model,\n            endpoint_url=endpoint_url,\n            context=request_context,\n            headers=additional_headers,\n        )\n        resolve_checksum_context(request_dict, operation_model, api_params)\n\n        service_id = self._service_model.service_id.hyphenize()\n        handler, event_response = await self.meta.events.emit_until_response(\n            'before-call.{service_id}.{operation_name}'.format(\n                service_id=service_id, operation_name=operation_name\n            ),\n            model=operation_model,\n            params=request_dict,\n            request_signer=self._request_signer,\n            context=request_context,\n        )\n\n        if event_response is not None:\n            http, parsed_response = event_response\n        else:\n            maybe_compress_request(\n                self.meta.config, request_dict, operation_model\n            )\n            apply_request_checksum(request_dict)\n            http, parsed_response = await self._make_request(\n                operation_model, request_dict, request_context\n            )\n\n        await self.meta.events.emit(\n            'after-call.{service_id}.{operation_name}'.format(\n                service_id=service_id, operation_name=operation_name\n            ),\n            http_response=http,\n            parsed=parsed_response,\n            model=operation_model,\n            context=request_context,\n        )\n\n        if http.status_code >= 300:\n            error_info = parsed_response.get(\"Error\", {})\n            error_code = error_info.get(\"QueryErrorCode\") or error_info.get(\n                \"Code\"\n            )\n            error_class = self.exceptions.from_code(error_code)\n            raise error_class(parsed_response, operation_name)\n        else:\n            return parsed_response\n\n    async def _make_request(\n        self, operation_model, request_dict, request_context\n    ):\n        try:\n            return await self._endpoint.make_request(\n                operation_model, request_dict\n            )\n        except Exception as e:\n            await self.meta.events.emit(\n                'after-call-error.{service_id}.{operation_name}'.format(\n                    service_id=self._service_model.service_id.hyphenize(),\n                    operation_name=operation_model.name,\n                ),\n                exception=e,\n                context=request_context,\n            )\n            raise\n\n    async def _convert_to_request_dict(\n        self,\n        api_params,\n        operation_model,\n        endpoint_url,\n        context=None,\n        headers=None,\n        set_user_agent_header=True,\n    ):\n        request_dict = self._serializer.serialize_to_request(\n            api_params, operation_model\n        )\n        if not self._client_config.inject_host_prefix:\n            request_dict.pop('host_prefix', None)\n        if headers is not None:\n            request_dict['headers'].update(headers)\n        if set_user_agent_header:\n            user_agent = self._user_agent_creator.to_string()\n        else:\n            user_agent = None\n        prepare_request_dict(\n            request_dict,\n            endpoint_url=endpoint_url,\n            user_agent=user_agent,\n            context=context,\n        )\n        return request_dict\n\n    async def _emit_api_params(self, api_params, operation_model, context):\n        # Given the API params provided by the user and the operation_model\n        # we can serialize the request to a request_dict.\n        operation_name = operation_model.name\n\n        # Emit an event that allows users to modify the parameters at the\n        # beginning of the method. It allows handlers to modify existing\n        # parameters or return a new set of parameters to use.\n        service_id = self._service_model.service_id.hyphenize()\n        responses = await self.meta.events.emit(\n            f'provide-client-params.{service_id}.{operation_name}',\n            params=api_params,\n            model=operation_model,\n            context=context,\n        )\n        api_params = first_non_none_response(responses, default=api_params)\n\n        await self.meta.events.emit(\n            f'before-parameter-build.{service_id}.{operation_name}',\n            params=api_params,\n            model=operation_model,\n            context=context,\n        )\n        return api_params\n\n    async def _resolve_endpoint_ruleset(\n        self,\n        operation_model,\n        params,\n        request_context,\n        ignore_signing_region=False,\n    ):\n        \"\"\"Returns endpoint URL and list of additional headers returned from\n        EndpointRulesetResolver for the given operation and params. If the\n        ruleset resolver is not available, for example because the service has\n        no endpoints ruleset file, the legacy endpoint resolver's value is\n        returned.\n\n        Use ignore_signing_region for generating presigned URLs or any other\n        situation where the signing region information from the ruleset\n        resolver should be ignored.\n\n        Returns tuple of URL and headers dictionary. Additionally, the\n        request_context dict is modified in place with any signing information\n        returned from the ruleset resolver.\n        \"\"\"\n        if self._ruleset_resolver is None:\n            endpoint_url = self.meta.endpoint_url\n            additional_headers = {}\n            endpoint_properties = {}\n        else:\n            endpoint_info = await self._ruleset_resolver.construct_endpoint(\n                operation_model=operation_model,\n                call_args=params,\n                request_context=request_context,\n            )\n            endpoint_url = endpoint_info.url\n            additional_headers = endpoint_info.headers\n            endpoint_properties = endpoint_info.properties\n            # If authSchemes is present, overwrite default auth type and\n            # signing context derived from service model.\n            auth_schemes = endpoint_info.properties.get('authSchemes')\n            if auth_schemes is not None:\n                auth_info = self._ruleset_resolver.auth_schemes_to_signing_ctx(\n                    auth_schemes\n                )\n                auth_type, signing_context = auth_info\n                request_context['auth_type'] = auth_type\n                if 'region' in signing_context and ignore_signing_region:\n                    del signing_context['region']\n                if 'signing' in request_context:\n                    request_context['signing'].update(signing_context)\n                else:\n                    request_context['signing'] = signing_context\n\n        return endpoint_url, additional_headers, endpoint_properties\n\n    def get_paginator(self, operation_name):\n        \"\"\"Create a paginator for an operation.\n\n        :type operation_name: string\n        :param operation_name: The operation name.  This is the same name\n            as the method name on the client.  For example, if the\n            method name is ``create_foo``, and you'd normally invoke the\n            operation as ``client.create_foo(**kwargs)``, if the\n            ``create_foo`` operation can be paginated, you can use the\n            call ``client.get_paginator(\"create_foo\")``.\n\n        :raise OperationNotPageableError: Raised if the operation is not\n            pageable.  You can use the ``client.can_paginate`` method to\n            check if an operation is pageable.\n\n        :rtype: ``botocore.paginate.Paginator``\n        :return: A paginator object.\n\n        \"\"\"\n        if not self.can_paginate(operation_name):\n            raise OperationNotPageableError(operation_name=operation_name)\n        else:\n            actual_operation_name = self._PY_TO_OP_NAME[operation_name]\n\n            # Create a new paginate method that will serve as a proxy to\n            # the underlying Paginator.paginate method. This is needed to\n            # attach a docstring to the method.\n            def paginate(self, **kwargs):\n                return AioPaginator.paginate(self, **kwargs)\n\n            paginator_config = self._cache['page_config'][\n                actual_operation_name\n            ]\n            # Add the docstring for the paginate method.\n            paginate.__doc__ = PaginatorDocstring(\n                paginator_name=actual_operation_name,\n                event_emitter=self.meta.events,\n                service_model=self.meta.service_model,\n                paginator_config=paginator_config,\n                include_signature=False,\n            )\n\n            # Rename the paginator class based on the type of paginator.\n            service_module_name = get_service_module_name(\n                self.meta.service_model\n            )\n            paginator_class_name = (\n                f\"{service_module_name}.Paginator.{actual_operation_name}\"\n            )\n\n            # Create the new paginator class\n            documented_paginator_cls = type(\n                paginator_class_name, (AioPaginator,), {'paginate': paginate}\n            )\n\n            operation_model = self._service_model.operation_model(\n                actual_operation_name\n            )\n            paginator = documented_paginator_cls(\n                getattr(self, operation_name),\n                paginator_config,\n                operation_model,\n            )\n            return paginator\n\n    # NOTE: this method does not differ from botocore, however it's important to keep\n    #   as the \"waiter\" value points to our own asyncio waiter module\n    def get_waiter(self, waiter_name):\n        \"\"\"Returns an object that can wait for some condition.\n\n        :type waiter_name: str\n        :param waiter_name: The name of the waiter to get. See the waiters\n            section of the service docs for a list of available waiters.\n\n        :returns: The specified waiter object.\n        :rtype: ``botocore.waiter.Waiter``\n        \"\"\"\n        config = self._get_waiter_config()\n        if not config:\n            raise ValueError(\"Waiter does not exist: %s\" % waiter_name)\n        model = waiter.WaiterModel(config)\n        mapping = {}\n        for name in model.waiter_names:\n            mapping[xform_name(name)] = name\n        if waiter_name not in mapping:\n            raise ValueError(\"Waiter does not exist: %s\" % waiter_name)\n\n        return waiter.create_waiter_with_client(\n            mapping[waiter_name], model, self\n        )\n\n    async def __aenter__(self):\n        await self._endpoint.http_session.__aenter__()\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        await self._endpoint.http_session.__aexit__(exc_type, exc_val, exc_tb)\n", "aiobotocore/handlers.py": "from botocore.handlers import (\n    ETree,\n    XMLParseError,\n    _get_cross_region_presigned_url,\n    _get_presigned_url_source_and_destination_regions,\n    logger,\n)\n\n\nasync def check_for_200_error(response, **kwargs):\n    # From: http://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectCOPY.html\n    # There are two opportunities for a copy request to return an error. One\n    # can occur when Amazon S3 receives the copy request and the other can\n    # occur while Amazon S3 is copying the files. If the error occurs before\n    # the copy operation starts, you receive a standard Amazon S3 error. If the\n    # error occurs during the copy operation, the error response is embedded in\n    # the 200 OK response. This means that a 200 OK response can contain either\n    # a success or an error. Make sure to design your application to parse the\n    # contents of the response and handle it appropriately.\n    #\n    # So this handler checks for this case.  Even though the server sends a\n    # 200 response, conceptually this should be handled exactly like a\n    # 500 response (with respect to raising exceptions, retries, etc.)\n    # We're connected *before* all the other retry logic handlers, so as long\n    # as we switch the error code to 500, we'll retry the error as expected.\n    if response is None:\n        # A None response can happen if an exception is raised while\n        # trying to retrieve the response.  See Endpoint._get_response().\n        return\n    http_response, parsed = response\n    if await _looks_like_special_case_error(http_response):\n        logger.debug(\n            \"Error found for response with 200 status code, \"\n            \"errors: %s, changing status code to \"\n            \"500.\",\n            parsed,\n        )\n        http_response.status_code = 500\n\n\nasync def _looks_like_special_case_error(http_response):\n    if http_response.status_code == 200:\n        try:\n            parser = ETree.XMLParser(\n                target=ETree.TreeBuilder(), encoding='utf-8'\n            )\n            parser.feed(await http_response.content)\n            root = parser.close()\n        except XMLParseError:\n            # In cases of network disruptions, we may end up with a partial\n            # streamed response from S3. We need to treat these cases as\n            # 500 Service Errors and try again.\n            return True\n        if root.tag == 'Error':\n            return True\n    return False\n\n\nasync def inject_presigned_url_ec2(params, request_signer, model, **kwargs):\n    # The customer can still provide this, so we should pass if they do.\n    if 'PresignedUrl' in params['body']:\n        return\n    src, dest = _get_presigned_url_source_and_destination_regions(\n        request_signer, params['body']\n    )\n    url = await _get_cross_region_presigned_url(\n        request_signer, params, model, src, dest\n    )\n    params['body']['PresignedUrl'] = url\n    # EC2 Requires that the destination region be sent over the wire in\n    # addition to the source region.\n    params['body']['DestinationRegion'] = dest\n\n\nasync def inject_presigned_url_rds(params, request_signer, model, **kwargs):\n    # SourceRegion is not required for RDS operations, so it's possible that\n    # it isn't set. In that case it's probably a local copy so we don't need\n    # to do anything else.\n    if 'SourceRegion' not in params['body']:\n        return\n\n    src, dest = _get_presigned_url_source_and_destination_regions(\n        request_signer, params['body']\n    )\n\n    # Since SourceRegion isn't actually modeled for RDS, it needs to be\n    # removed from the request params before we send the actual request.\n    del params['body']['SourceRegion']\n\n    if 'PreSignedUrl' in params['body']:\n        return\n\n    url = await _get_cross_region_presigned_url(\n        request_signer, params, model, src, dest\n    )\n    params['body']['PreSignedUrl'] = url\n\n\nasync def parse_get_bucket_location(parsed, http_response, **kwargs):\n    # s3.GetBucketLocation cannot be modeled properly.  To\n    # account for this we just manually parse the XML document.\n    # The \"parsed\" passed in only has the ResponseMetadata\n    # filled out.  This handler will fill in the LocationConstraint\n    # value.\n    if http_response.raw is None:\n        return\n    response_body = await http_response.content\n    parser = ETree.XMLParser(target=ETree.TreeBuilder(), encoding='utf-8')\n    parser.feed(response_body)\n    root = parser.close()\n    region = root.text\n    parsed['LocationConstraint'] = region\n", "aiobotocore/args.py": "import copy\n\nimport botocore.parsers\nimport botocore.serialize\nfrom botocore.args import ClientArgsCreator\n\nfrom .config import AioConfig\nfrom .endpoint import DEFAULT_HTTP_SESSION_CLS, AioEndpointCreator\nfrom .regions import AioEndpointRulesetResolver\nfrom .signers import AioRequestSigner\n\n\nclass AioClientArgsCreator(ClientArgsCreator):\n    # NOTE: we override this so we can pull out the custom AioConfig params and\n    #       use an AioEndpointCreator\n    def get_client_args(\n        self,\n        service_model,\n        region_name,\n        is_secure,\n        endpoint_url,\n        verify,\n        credentials,\n        scoped_config,\n        client_config,\n        endpoint_bridge,\n        auth_token=None,\n        endpoints_ruleset_data=None,\n        partition_data=None,\n    ):\n        final_args = self.compute_client_args(\n            service_model,\n            client_config,\n            endpoint_bridge,\n            region_name,\n            endpoint_url,\n            is_secure,\n            scoped_config,\n        )\n\n        service_name = final_args['service_name']  # noqa\n        parameter_validation = final_args['parameter_validation']\n        endpoint_config = final_args['endpoint_config']\n        protocol = final_args['protocol']\n        config_kwargs = final_args['config_kwargs']\n        s3_config = final_args['s3_config']\n        partition = endpoint_config['metadata'].get('partition', None)\n        socket_options = final_args['socket_options']\n        configured_endpoint_url = final_args['configured_endpoint_url']\n\n        signing_region = endpoint_config['signing_region']\n        endpoint_region_name = endpoint_config['region_name']\n\n        event_emitter = copy.copy(self._event_emitter)\n        signer = AioRequestSigner(\n            service_model.service_id,\n            signing_region,\n            endpoint_config['signing_name'],\n            endpoint_config['signature_version'],\n            credentials,\n            event_emitter,\n            auth_token,\n        )\n\n        config_kwargs['s3'] = s3_config\n\n        # aiobotocore addition\n        if isinstance(client_config, AioConfig):\n            connector_args = client_config.connector_args\n            http_session_cls = client_config.http_session_cls\n        else:\n            connector_args = None\n            http_session_cls = DEFAULT_HTTP_SESSION_CLS\n\n        new_config = AioConfig(connector_args, **config_kwargs)\n        endpoint_creator = AioEndpointCreator(event_emitter)\n\n        endpoint = endpoint_creator.create_endpoint(\n            service_model,\n            region_name=endpoint_region_name,\n            endpoint_url=endpoint_config['endpoint_url'],\n            verify=verify,\n            response_parser_factory=self._response_parser_factory,\n            timeout=(new_config.connect_timeout, new_config.read_timeout),\n            max_pool_connections=new_config.max_pool_connections,\n            http_session_cls=http_session_cls,\n            proxies=new_config.proxies,\n            socket_options=socket_options,\n            client_cert=new_config.client_cert,\n            proxies_config=new_config.proxies_config,\n            connector_args=new_config.connector_args,\n        )\n\n        serializer = botocore.serialize.create_serializer(\n            protocol, parameter_validation\n        )\n        response_parser = botocore.parsers.create_parser(protocol)\n\n        ruleset_resolver = self._build_endpoint_resolver(\n            endpoints_ruleset_data,\n            partition_data,\n            client_config,\n            service_model,\n            endpoint_region_name,\n            region_name,\n            configured_endpoint_url,\n            endpoint,\n            is_secure,\n            endpoint_bridge,\n            event_emitter,\n        )\n\n        # Copy the session's user agent factory and adds client configuration.\n        client_ua_creator = self._session_ua_creator.with_client_config(\n            new_config\n        )\n        supplied_ua = client_config.user_agent if client_config else None\n        new_config._supplied_user_agent = supplied_ua\n\n        return {\n            'serializer': serializer,\n            'endpoint': endpoint,\n            'response_parser': response_parser,\n            'event_emitter': event_emitter,\n            'request_signer': signer,\n            'service_model': service_model,\n            'loader': self._loader,\n            'client_config': new_config,\n            'partition': partition,\n            'exceptions_factory': self._exceptions_factory,\n            'endpoint_ruleset_resolver': ruleset_resolver,\n            'user_agent_creator': client_ua_creator,\n        }\n\n    def _build_endpoint_resolver(\n        self,\n        endpoints_ruleset_data,\n        partition_data,\n        client_config,\n        service_model,\n        endpoint_region_name,\n        region_name,\n        endpoint_url,\n        endpoint,\n        is_secure,\n        endpoint_bridge,\n        event_emitter,\n    ):\n        if endpoints_ruleset_data is None:\n            return None\n\n        # The legacy EndpointResolver is global to the session, but\n        # EndpointRulesetResolver is service-specific. Builtins for\n        # EndpointRulesetResolver must not be derived from the legacy\n        # endpoint resolver's output, including final_args, s3_config,\n        # etc.\n        s3_config_raw = self.compute_s3_config(client_config) or {}\n        service_name_raw = service_model.endpoint_prefix\n        # Maintain complex logic for s3 and sts endpoints for backwards\n        # compatibility.\n        if service_name_raw in ['s3', 'sts'] or region_name is None:\n            eprv2_region_name = endpoint_region_name\n        else:\n            eprv2_region_name = region_name\n        resolver_builtins = self.compute_endpoint_resolver_builtin_defaults(\n            region_name=eprv2_region_name,\n            service_name=service_name_raw,\n            s3_config=s3_config_raw,\n            endpoint_bridge=endpoint_bridge,\n            client_endpoint_url=endpoint_url,\n            legacy_endpoint_url=endpoint.host,\n        )\n        # Client context params for s3 conflict with the available settings\n        # in the `s3` parameter on the `Config` object. If the same parameter\n        # is set in both places, the value in the `s3` parameter takes priority.\n        if client_config is not None:\n            client_context = client_config.client_context_params or {}\n        else:\n            client_context = {}\n        if self._is_s3_service(service_name_raw):\n            client_context.update(s3_config_raw)\n\n        sig_version = (\n            client_config.signature_version\n            if client_config is not None\n            else None\n        )\n        return AioEndpointRulesetResolver(\n            endpoint_ruleset_data=endpoints_ruleset_data,\n            partition_data=partition_data,\n            service_model=service_model,\n            builtins=resolver_builtins,\n            client_context=client_context,\n            event_emitter=event_emitter,\n            use_ssl=is_secure,\n            requested_auth_scheme=sig_version,\n        )\n", "aiobotocore/__init__.py": "__version__ = '2.13.1'\n", "aiobotocore/stub.py": "from botocore.stub import Stubber\n\nfrom .awsrequest import AioAWSResponse\n\n\nclass AioStubber(Stubber):\n    def _add_response(self, method, service_response, expected_params):\n        if not hasattr(self.client, method):\n            raise ValueError(\n                \"Client %s does not have method: %s\"\n                % (self.client.meta.service_model.service_name, method)\n            )  # pragma: no cover\n\n        # Create a successful http response\n        http_response = AioAWSResponse(None, 200, {}, None)\n\n        operation_name = self.client.meta.method_to_api_mapping.get(method)\n        self._validate_operation_response(operation_name, service_response)\n\n        # Add the service_response to the queue for returning responses\n        response = {\n            'operation_name': operation_name,\n            'response': (http_response, service_response),\n            'expected_params': expected_params,\n        }\n        self._queue.append(response)\n\n    def add_client_error(\n        self,\n        method,\n        service_error_code='',\n        service_message='',\n        http_status_code=400,\n        service_error_meta=None,\n        expected_params=None,\n        response_meta=None,\n        modeled_fields=None,\n    ):\n        \"\"\"\n        Adds a ``ClientError`` to the response queue.\n\n        :param method: The name of the service method to return the error on.\n        :type method: str\n\n        :param service_error_code: The service error code to return,\n                                                           e.g. ``NoSuchBucket``\n        :type service_error_code: str\n\n        :param service_message: The service message to return, e.g.\n                                        'The specified bucket does not exist.'\n        :type service_message: str\n\n        :param http_status_code: The HTTP status code to return, e.g. 404, etc\n        :type http_status_code: int\n\n        :param service_error_meta: Additional keys to be added to the\n                service Error\n        :type service_error_meta: dict\n\n        :param expected_params: A dictionary of the expected parameters to\n                be called for the provided service response. The parameters match\n                the names of keyword arguments passed to that client call. If\n                any of the parameters differ a ``StubResponseError`` is thrown.\n                You can use stub.ANY to indicate a particular parameter to ignore\n                in validation.\n\n        :param response_meta: Additional keys to be added to the\n                response's ResponseMetadata\n        :type response_meta: dict\n\n        :param modeled_fields: Additional keys to be added to the response\n                based on fields that are modeled for the particular error code.\n                These keys will be validated against the particular error shape\n                designated by the error code.\n        :type modeled_fields: dict\n\n        \"\"\"\n        http_response = AioAWSResponse(None, http_status_code, {}, None)\n\n        # We don't look to the model to build this because the caller would\n        # need to know the details of what the HTTP body would need to\n        # look like.\n        parsed_response = {\n            'ResponseMetadata': {'HTTPStatusCode': http_status_code},\n            'Error': {'Message': service_message, 'Code': service_error_code},\n        }\n\n        if service_error_meta is not None:\n            parsed_response['Error'].update(service_error_meta)\n\n        if response_meta is not None:\n            parsed_response['ResponseMetadata'].update(response_meta)\n\n        if modeled_fields is not None:\n            service_model = self.client.meta.service_model\n            shape = service_model.shape_for_error_code(service_error_code)\n            self._validate_response(shape, modeled_fields)\n            parsed_response.update(modeled_fields)\n\n        operation_name = self.client.meta.method_to_api_mapping.get(method)\n        # Note that we do not allow for expected_params while\n        # adding errors into the queue yet.\n        response = {\n            'operation_name': operation_name,\n            'response': (http_response, parsed_response),\n            'expected_params': expected_params,\n        }\n        self._queue.append(response)\n", "aiobotocore/session.py": "from botocore import UNSIGNED\nfrom botocore import __version__ as botocore_version\nfrom botocore import translate\nfrom botocore.exceptions import PartialCredentialsError\nfrom botocore.session import EVENT_ALIASES, ServiceModel\nfrom botocore.session import Session as _SyncSession\nfrom botocore.session import UnknownServiceError, copy\n\nfrom . import __version__, retryhandler\nfrom .client import AioBaseClient, AioClientCreator\nfrom .configprovider import AioSmartDefaultsConfigStoreFactory\nfrom .credentials import AioCredentials, create_credential_resolver\nfrom .hooks import AioHierarchicalEmitter\nfrom .parsers import AioResponseParserFactory\nfrom .tokens import create_token_resolver\nfrom .utils import AioIMDSRegionProvider\n\n\nclass ClientCreatorContext:\n    def __init__(self, coro):\n        self._coro = coro\n        self._client = None\n\n    async def __aenter__(self) -> AioBaseClient:\n        self._client = await self._coro\n        return await self._client.__aenter__()\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        await self._client.__aexit__(exc_type, exc_val, exc_tb)\n\n\nclass AioSession(_SyncSession):\n    # noinspection PyMissingConstructor\n    def __init__(\n        self,\n        session_vars=None,\n        event_hooks=None,\n        include_builtin_handlers=True,\n        profile=None,\n    ):\n        if event_hooks is None:\n            event_hooks = AioHierarchicalEmitter()\n\n        super().__init__(\n            session_vars, event_hooks, include_builtin_handlers, profile\n        )\n\n        self._set_user_agent_for_session()\n\n    def _set_user_agent_for_session(self):\n        # Mimic approach taken by AWS's aws-cli project\n        # https://github.com/aws/aws-cli/blob/b862122c76a3f280ff34e93c9dcafaf964e7bf9b/awscli/clidriver.py#L84\n\n        self.user_agent_name = 'aiobotocore'\n        self.user_agent_version = __version__\n        self.user_agent_extra = 'botocore/%s' % botocore_version\n\n    def _create_token_resolver(self):\n        return create_token_resolver(self)\n\n    def _create_credential_resolver(self):\n        return create_credential_resolver(\n            self, region_name=self._last_client_region_used\n        )\n\n    def _register_smart_defaults_factory(self):\n        def create_smart_defaults_factory():\n            default_config_resolver = self._get_internal_component(\n                'default_config_resolver'\n            )\n            imds_region_provider = AioIMDSRegionProvider(session=self)\n            return AioSmartDefaultsConfigStoreFactory(\n                default_config_resolver, imds_region_provider\n            )\n\n        self._internal_components.lazy_register_component(\n            'smart_defaults_factory', create_smart_defaults_factory\n        )\n\n    def _register_response_parser_factory(self):\n        self._components.register_component(\n            'response_parser_factory', AioResponseParserFactory()\n        )\n\n    def set_credentials(self, access_key, secret_key, token=None):\n        self._credentials = AioCredentials(access_key, secret_key, token)\n\n    async def get_credentials(self):\n        if self._credentials is None:\n            self._credentials = await self._components.get_component(\n                'credential_provider'\n            ).load_credentials()\n        return self._credentials\n\n    async def get_service_model(self, service_name, api_version=None):\n        service_description = await self.get_service_data(\n            service_name, api_version\n        )\n        return ServiceModel(service_description, service_name=service_name)\n\n    async def get_service_data(self, service_name, api_version=None):\n        \"\"\"\n        Retrieve the fully merged data associated with a service.\n        \"\"\"\n        data_path = service_name\n        service_data = self.get_component('data_loader').load_service_model(\n            data_path, type_name='service-2', api_version=api_version\n        )\n        service_id = EVENT_ALIASES.get(service_name, service_name)\n        await self._events.emit(\n            'service-data-loaded.%s' % service_id,\n            service_data=service_data,\n            service_name=service_name,\n            session=self,\n        )\n        return service_data\n\n    def create_client(self, *args, **kwargs):\n        return ClientCreatorContext(self._create_client(*args, **kwargs))\n\n    async def _create_client(\n        self,\n        service_name,\n        region_name=None,\n        api_version=None,\n        use_ssl=True,\n        verify=None,\n        endpoint_url=None,\n        aws_access_key_id=None,\n        aws_secret_access_key=None,\n        aws_session_token=None,\n        config=None,\n    ):\n        default_client_config = self.get_default_client_config()\n        # If a config is provided and a default config is set, then\n        # use the config resulting from merging the two.\n        if config is not None and default_client_config is not None:\n            config = default_client_config.merge(config)\n        # If a config was not provided then use the default\n        # client config from the session\n        elif default_client_config is not None:\n            config = default_client_config\n\n        region_name = self._resolve_region_name(region_name, config)\n\n        # Figure out the verify value base on the various\n        # configuration options.\n        if verify is None:\n            verify = self.get_config_variable('ca_bundle')\n\n        if api_version is None:\n            api_version = self.get_config_variable('api_versions').get(\n                service_name, None\n            )\n\n        loader = self.get_component('data_loader')\n        event_emitter = self.get_component('event_emitter')\n        response_parser_factory = self.get_component('response_parser_factory')\n        if config is not None and config.signature_version is UNSIGNED:\n            credentials = None\n        elif (\n            aws_access_key_id is not None and aws_secret_access_key is not None\n        ):\n            credentials = AioCredentials(\n                access_key=aws_access_key_id,\n                secret_key=aws_secret_access_key,\n                token=aws_session_token,\n            )\n        elif self._missing_cred_vars(aws_access_key_id, aws_secret_access_key):\n            raise PartialCredentialsError(\n                provider='explicit',\n                cred_var=self._missing_cred_vars(\n                    aws_access_key_id, aws_secret_access_key\n                ),\n            )\n        else:\n            credentials = await self.get_credentials()\n        auth_token = self.get_auth_token()\n        endpoint_resolver = self._get_internal_component('endpoint_resolver')\n        exceptions_factory = self._get_internal_component('exceptions_factory')\n        config_store = copy.copy(self.get_component('config_store'))\n        user_agent_creator = self.get_component('user_agent_creator')\n        # Session configuration values for the user agent string are applied\n        # just before each client creation because they may have been modified\n        # at any time between session creation and client creation.\n        user_agent_creator.set_session_config(\n            session_user_agent_name=self.user_agent_name,\n            session_user_agent_version=self.user_agent_version,\n            session_user_agent_extra=self.user_agent_extra,\n        )\n        defaults_mode = self._resolve_defaults_mode(config, config_store)\n        if defaults_mode != 'legacy':\n            smart_defaults_factory = self._get_internal_component(\n                'smart_defaults_factory'\n            )\n            await smart_defaults_factory.merge_smart_defaults(\n                config_store, defaults_mode, region_name\n            )\n        self._add_configured_endpoint_provider(\n            client_name=service_name,\n            config_store=config_store,\n        )\n        client_creator = AioClientCreator(\n            loader,\n            endpoint_resolver,\n            self.user_agent(),\n            event_emitter,\n            retryhandler,\n            translate,\n            response_parser_factory,\n            exceptions_factory,\n            config_store,\n            user_agent_creator=user_agent_creator,\n        )\n        client = await client_creator.create_client(\n            service_name=service_name,\n            region_name=region_name,\n            is_secure=use_ssl,\n            endpoint_url=endpoint_url,\n            verify=verify,\n            credentials=credentials,\n            scoped_config=self.get_scoped_config(),\n            client_config=config,\n            api_version=api_version,\n            auth_token=auth_token,\n        )\n        monitor = self._get_internal_component('monitor')\n        if monitor is not None:\n            monitor.register(client.meta.events)\n        return client\n\n    async def get_available_regions(\n        self, service_name, partition_name='aws', allow_non_regional=False\n    ):\n        resolver = self._get_internal_component('endpoint_resolver')\n        results = []\n        try:\n            service_data = await self.get_service_data(service_name)\n            endpoint_prefix = service_data['metadata'].get(\n                'endpointPrefix', service_name\n            )\n            results = resolver.get_available_endpoints(\n                endpoint_prefix, partition_name, allow_non_regional\n            )\n        except UnknownServiceError:\n            pass\n        return results\n\n\ndef get_session(env_vars=None):\n    \"\"\"\n    Return a new session object.\n    \"\"\"\n    return AioSession(env_vars)\n", "aiobotocore/_helpers.py": "import inspect\n\ntry:\n    from contextlib import (  # noqa: F401 lgtm[py/unused-import]\n        asynccontextmanager,\n    )\nexcept ImportError:\n    from async_generator import (  # noqa: F401 E501, lgtm[py/unused-import]\n        asynccontextmanager,\n    )\n\n\nasync def resolve_awaitable(obj):\n    if inspect.isawaitable(obj):\n        return await obj\n\n    return obj\n\n\nasync def async_any(items):\n    for item in items:\n        if await resolve_awaitable(item):\n            return True\n\n    return False\n", "aiobotocore/hooks.py": "from botocore.handlers import check_for_200_error as boto_check_for_200_error\nfrom botocore.handlers import (\n    inject_presigned_url_ec2 as boto_inject_presigned_url_ec2,\n)\nfrom botocore.handlers import (\n    inject_presigned_url_rds as boto_inject_presigned_url_rds,\n)\nfrom botocore.handlers import (\n    parse_get_bucket_location as boto_parse_get_bucket_location,\n)\nfrom botocore.hooks import HierarchicalEmitter, logger\nfrom botocore.signers import (\n    add_generate_db_auth_token as boto_add_generate_db_auth_token,\n)\nfrom botocore.signers import (\n    add_generate_presigned_post as boto_add_generate_presigned_post,\n)\nfrom botocore.signers import (\n    add_generate_presigned_url as boto_add_generate_presigned_url,\n)\n\nfrom ._helpers import resolve_awaitable\nfrom .handlers import (\n    check_for_200_error,\n    inject_presigned_url_ec2,\n    inject_presigned_url_rds,\n    parse_get_bucket_location,\n)\nfrom .signers import (\n    add_generate_db_auth_token,\n    add_generate_presigned_post,\n    add_generate_presigned_url,\n)\n\n_HANDLER_MAPPING = {\n    boto_inject_presigned_url_ec2: inject_presigned_url_ec2,\n    boto_inject_presigned_url_rds: inject_presigned_url_rds,\n    boto_add_generate_presigned_url: add_generate_presigned_url,\n    boto_add_generate_presigned_post: add_generate_presigned_post,\n    boto_add_generate_db_auth_token: add_generate_db_auth_token,\n    boto_parse_get_bucket_location: parse_get_bucket_location,\n    boto_check_for_200_error: check_for_200_error,\n}\n\n\nclass AioHierarchicalEmitter(HierarchicalEmitter):\n    async def _emit(self, event_name, kwargs, stop_on_response=False):\n        responses = []\n        # Invoke the event handlers from most specific\n        # to least specific, each time stripping off a dot.\n        handlers_to_call = self._lookup_cache.get(event_name)\n        if handlers_to_call is None:\n            handlers_to_call = self._handlers.prefix_search(event_name)\n            self._lookup_cache[event_name] = handlers_to_call\n        elif not handlers_to_call:\n            # Short circuit and return an empty response is we have\n            # no handlers to call.  This is the common case where\n            # for the majority of signals, nothing is listening.\n            return []\n        kwargs['event_name'] = event_name\n        responses = []\n        for handler in handlers_to_call:\n            logger.debug('Event %s: calling handler %s', event_name, handler)\n\n            # Await the handler if its a coroutine.\n            response = await resolve_awaitable(handler(**kwargs))\n            responses.append((handler, response))\n            if stop_on_response and response is not None:\n                return responses\n        return responses\n\n    async def emit_until_response(self, event_name, **kwargs):\n        responses = await self._emit(event_name, kwargs, stop_on_response=True)\n        if responses:\n            return responses[-1]\n        else:\n            return None, None\n\n    def _verify_and_register(\n        self,\n        event_name,\n        handler,\n        unique_id,\n        register_method,\n        unique_id_uses_count,\n    ):\n        handler = _HANDLER_MAPPING.get(handler, handler)\n\n        self._verify_is_callable(handler)\n        self._verify_accept_kwargs(handler)\n        register_method(event_name, handler, unique_id, unique_id_uses_count)\n", "aiobotocore/paginate.py": "import aioitertools\nimport jmespath\nfrom botocore.exceptions import PaginationError\nfrom botocore.paginate import PageIterator, Paginator\nfrom botocore.utils import merge_dicts, set_value_from_jmespath\n\n\nclass AioPageIterator(PageIterator):\n    def __aiter__(self):\n        return self.__anext__()\n\n    async def __anext__(self):\n        current_kwargs = self._op_kwargs\n        previous_next_token = None\n        next_token = {key: None for key in self._input_token}\n        if self._starting_token is not None:\n            # If the starting token exists, populate the next_token with the\n            # values inside it. This ensures that we have the service's\n            # pagination token on hand if we need to truncate after the\n            # first response.\n            next_token = self._parse_starting_token()[0]\n        # The number of items from result_key we've seen so far.\n        total_items = 0\n        first_request = True\n        primary_result_key = self.result_keys[0]\n        starting_truncation = 0\n        self._inject_starting_params(current_kwargs)\n\n        while True:\n            response = await self._make_request(current_kwargs)\n            parsed = self._extract_parsed_response(response)\n            if first_request:\n                # The first request is handled differently.  We could\n                # possibly have a resume/starting token that tells us where\n                # to index into the retrieved page.\n                if self._starting_token is not None:\n                    starting_truncation = self._handle_first_request(\n                        parsed, primary_result_key, starting_truncation\n                    )\n                first_request = False\n                self._record_non_aggregate_key_values(parsed)\n            else:\n                # If this isn't the first request, we have already sliced into\n                # the first request and had to make additional requests after.\n                # We no longer need to add this to truncation.\n                starting_truncation = 0\n            current_response = primary_result_key.search(parsed)\n            if current_response is None:\n                current_response = []\n            num_current_response = len(current_response)\n            truncate_amount = 0\n            if self._max_items is not None:\n                truncate_amount = (\n                    total_items + num_current_response - self._max_items\n                )\n\n            if truncate_amount > 0:\n                self._truncate_response(\n                    parsed,\n                    primary_result_key,\n                    truncate_amount,\n                    starting_truncation,\n                    next_token,\n                )\n                yield response\n                break\n            else:\n                yield response\n                total_items += num_current_response\n                next_token = self._get_next_token(parsed)\n                if all(t is None for t in next_token.values()):\n                    break\n                if (\n                    self._max_items is not None\n                    and total_items == self._max_items\n                ):\n                    # We're on a page boundary so we can set the current\n                    # next token to be the resume token.\n                    self.resume_token = next_token\n                    break\n                if (\n                    previous_next_token is not None\n                    and previous_next_token == next_token\n                ):\n                    message = (\n                        f\"The same next token was received \"\n                        f\"twice: {next_token}\"\n                    )\n                    raise PaginationError(message=message)\n                self._inject_token_into_kwargs(current_kwargs, next_token)\n                previous_next_token = next_token\n\n    async def search(self, expression):\n        compiled = jmespath.compile(expression)\n        async for page in self:\n            results = compiled.search(page)\n            if isinstance(results, list):\n                for element in results:\n                    yield element  # unfortunately yield from not avail from async f\n            else:\n                yield results\n\n    def result_key_iters(self):\n        teed_results = aioitertools.tee(self, len(self.result_keys))\n        return [\n            ResultKeyIterator(i, result_key)\n            for i, result_key in zip(teed_results, self.result_keys)\n        ]\n\n    async def build_full_result(self):\n        complete_result = {}\n        async for response in self:\n            page = response\n            # We want to try to catch operation object pagination\n            # and format correctly for those. They come in the form\n            # of a tuple of two elements: (http_response, parsed_responsed).\n            # We want the parsed_response as that is what the page iterator\n            # uses. We can remove it though once operation objects are removed.\n            if isinstance(response, tuple) and len(response) == 2:\n                page = response[1]\n            # We're incrementally building the full response page\n            # by page.  For each page in the response we need to\n            # inject the necessary components from the page\n            # into the complete_result.\n            for result_expression in self.result_keys:\n                # In order to incrementally update a result key\n                # we need to search the existing value from complete_result,\n                # then we need to search the _current_ page for the\n                # current result key value.  Then we append the current\n                # value onto the existing value, and re-set that value\n                # as the new value.\n                result_value = result_expression.search(page)\n                if result_value is None:\n                    continue\n                existing_value = result_expression.search(complete_result)\n                if existing_value is None:\n                    # Set the initial result\n                    set_value_from_jmespath(\n                        complete_result,\n                        result_expression.expression,\n                        result_value,\n                    )\n                    continue\n                # Now both result_value and existing_value contain something\n                if isinstance(result_value, list):\n                    existing_value.extend(result_value)\n                elif isinstance(result_value, (int, float, str)):\n                    # Modify the existing result with the sum or concatenation\n                    set_value_from_jmespath(\n                        complete_result,\n                        result_expression.expression,\n                        existing_value + result_value,\n                    )\n        merge_dicts(complete_result, self.non_aggregate_part)\n        if self.resume_token is not None:\n            complete_result['NextToken'] = self.resume_token\n        return complete_result\n\n\nclass AioPaginator(Paginator):\n    PAGE_ITERATOR_CLS = AioPageIterator\n\n\nclass ResultKeyIterator:\n    \"\"\"Iterates over the results of paginated responses.\n\n    Each iterator is associated with a single result key.\n    Iterating over this object will give you each element in\n    the result key list.\n\n    :param pages_iterator: An iterator that will give you\n        pages of results (a ``PageIterator`` class).\n    :param result_key: The JMESPath expression representing\n        the result key.\n\n    \"\"\"\n\n    def __init__(self, pages_iterator, result_key):\n        self._pages_iterator = pages_iterator\n        self.result_key = result_key\n\n    def __aiter__(self):\n        return self.__anext__()\n\n    async def __anext__(self):\n        async for page in self._pages_iterator:\n            results = self.result_key.search(page)\n            if results is None:\n                results = []\n            for result in results:\n                yield result  # yield from not avail from async func\n", "aiobotocore/retries/special.py": "from botocore.retries.special import RetryDDBChecksumError, crc32, logger\n\n\nclass AioRetryDDBChecksumError(RetryDDBChecksumError):\n    async def is_retryable(self, context):\n        service_name = context.operation_model.service_model.service_name\n        if service_name != self._SERVICE_NAME:\n            return False\n        if context.http_response is None:\n            return False\n        checksum = context.http_response.headers.get(self._CHECKSUM_HEADER)\n        if checksum is None:\n            return False\n        actual_crc32 = crc32(await context.http_response.content) & 0xFFFFFFFF\n        if actual_crc32 != int(checksum):\n            logger.debug(\n                \"DynamoDB crc32 checksum does not match, \"\n                \"expected: %s, actual: %s\",\n                checksum,\n                actual_crc32,\n            )\n            return True\n", "aiobotocore/retries/bucket.py": "\"\"\"An async reimplementation of the blocking elements from botocore.retries.bucket.\"\"\"\nimport asyncio\n\nfrom botocore.exceptions import CapacityNotAvailableError\n\nfrom botocore.retries.bucket import Clock as Clock  # reexport # noqa\n\n\nclass AsyncTokenBucket:\n    \"\"\"A reimplementation of TokenBucket that doesn't block.\"\"\"\n\n    # Most of the code here is pulled straight up from botocore, with slight changes\n    # to the interface to switch to async methods.\n    # This class doesn't inherit from the botocore TokenBucket, as the interface is\n    # different: the `max_rate` setter in the original class is replaced by the\n    # async `set_max_rate`.\n    # (a Python setter can't be async).\n\n    _MIN_RATE = 0.5\n\n    def __init__(self, max_rate, clock, min_rate=_MIN_RATE):\n        self._fill_rate = None\n        self._max_capacity = None\n        self._current_capacity = 0\n        self._clock = clock\n        self._last_timestamp = None\n        self._min_rate = min_rate\n        self._set_max_rate(max_rate)\n        # The main difference between this implementation and the botocore TokenBucket\n        # implementation is replacing a threading.Condition by this asyncio.Condition.\n        self._new_fill_rate_condition = asyncio.Condition()\n\n    @property\n    def max_rate(self):\n        return self._fill_rate\n\n    async def set_max_rate(self, value):\n        async with self._new_fill_rate_condition:\n            self._set_max_rate(value)\n            self._new_fill_rate_condition.notify()\n\n    def _set_max_rate(self, value):\n        # Before we can change the rate we need to fill any pending\n        # tokens we might have based on the current rate.  If we don't\n        # do this it means everything since the last recorded timestamp\n        # will accumulate at the rate we're about to set which isn't\n        # correct.\n        self._refill()\n        self._fill_rate = max(value, self._min_rate)\n        if value >= 1:\n            self._max_capacity = value\n        else:\n            self._max_capacity = 1\n        # If we're scaling down, we also can't have a capacity that's\n        # more than our max_capacity.\n        self._current_capacity = min(\n            self._current_capacity, self._max_capacity\n        )\n\n    @property\n    def max_capacity(self):\n        return self._max_capacity\n\n    @property\n    def available_capacity(self):\n        return self._current_capacity\n\n    async def acquire(self, amount=1, block=True):\n        \"\"\"Acquire token or return amount of time until next token available.\n\n        If block is True, then this method will return when there's sufficient\n        capacity to acquire the desired amount. This won't block the event loop.\n\n        If block is False, then this method will return True if capacity\n        was successfully acquired, False otherwise.\n        \"\"\"\n        async with self._new_fill_rate_condition:\n            return await self._acquire(amount=amount, block=block)\n\n    async def _acquire(self, amount, block):\n        self._refill()\n        if amount <= self._current_capacity:\n            self._current_capacity -= amount\n            return True\n        else:\n            if not block:\n                raise CapacityNotAvailableError()\n            # Not enough capacity.\n            sleep_amount = self._sleep_amount(amount)\n            while sleep_amount > 0:\n                try:\n                    await asyncio.wait_for(\n                        self._new_fill_rate_condition.wait(), sleep_amount\n                    )\n                except asyncio.TimeoutError:\n                    pass\n                self._refill()\n                sleep_amount = self._sleep_amount(amount)\n            self._current_capacity -= amount\n            return True\n\n    def _sleep_amount(self, amount):\n        return (amount - self._current_capacity) / self._fill_rate\n\n    def _refill(self):\n        timestamp = self._clock.current_time()\n        if self._last_timestamp is None:\n            self._last_timestamp = timestamp\n            return\n        current_capacity = self._current_capacity\n        fill_amount = (timestamp - self._last_timestamp) * self._fill_rate\n        new_capacity = min(self._max_capacity, current_capacity + fill_amount)\n        self._current_capacity = new_capacity\n        self._last_timestamp = timestamp\n", "aiobotocore/retries/adaptive.py": "\"\"\"An async reimplementation of the blocking elements from botocore.retries.adaptive.\"\"\"\nimport asyncio\nimport logging\n\nfrom botocore.retries import standard, throttling\n\n# The RateClocker from botocore uses a threading.Lock, but in a single-threaded asyncio\n# program, the lock will be acquired then released by the same coroutine without\n# blocking.\nfrom botocore.retries.adaptive import RateClocker\n\nfrom . import bucket\n\nlogger = logging.getLogger(__name__)\n\n\ndef register_retry_handler(client):\n    clock = bucket.Clock()\n    rate_adjustor = throttling.CubicCalculator(\n        starting_max_rate=0, start_time=clock.current_time()\n    )\n    token_bucket = bucket.AsyncTokenBucket(max_rate=1, clock=clock)\n    rate_clocker = RateClocker(clock)\n    throttling_detector = standard.ThrottlingErrorDetector(\n        retry_event_adapter=standard.RetryEventAdapter(),\n    )\n    limiter = AsyncClientRateLimiter(\n        rate_adjustor=rate_adjustor,\n        rate_clocker=rate_clocker,\n        token_bucket=token_bucket,\n        throttling_detector=throttling_detector,\n        clock=clock,\n    )\n    client.meta.events.register(\n        'before-send',\n        limiter.on_sending_request,\n    )\n    client.meta.events.register(\n        'needs-retry',\n        limiter.on_receiving_response,\n    )\n    return limiter\n\n\nclass AsyncClientRateLimiter:\n    \"\"\"An async reimplementation of ClientRateLimiter.\"\"\"\n\n    # Most of the code here comes directly from botocore. The main change is making the\n    # callbacks async.\n    # This doesn't inherit from the botocore ClientRateLimiter for two reasons:\n    # * the interface is slightly changed (methods are now async)\n    # * we rewrote the entirety of the class anyway\n\n    _MAX_RATE_ADJUST_SCALE = 2.0\n\n    def __init__(\n        self,\n        rate_adjustor,\n        rate_clocker,\n        token_bucket,\n        throttling_detector,\n        clock,\n    ):\n        self._rate_adjustor = rate_adjustor\n        self._rate_clocker = rate_clocker\n        self._token_bucket = token_bucket\n        self._throttling_detector = throttling_detector\n        self._clock = clock\n        self._enabled = False\n        self._lock = asyncio.Lock()\n\n    async def on_sending_request(self, request, **kwargs):\n        if self._enabled:\n            await self._token_bucket.acquire()\n\n    # Hooked up to needs-retry.\n    async def on_receiving_response(self, **kwargs):\n        measured_rate = self._rate_clocker.record()\n        timestamp = self._clock.current_time()\n        async with self._lock:\n            if not self._throttling_detector.is_throttling_error(**kwargs):\n                new_rate = self._rate_adjustor.success_received(timestamp)\n            else:\n                if not self._enabled:\n                    rate_to_use = measured_rate\n                else:\n                    rate_to_use = min(\n                        measured_rate, self._token_bucket.max_rate\n                    )\n                new_rate = self._rate_adjustor.error_received(\n                    rate_to_use, timestamp\n                )\n                logger.debug(\n                    \"Throttling response received, new send rate: %s \"\n                    \"measured rate: %s, token bucket capacity \"\n                    \"available: %s\",\n                    new_rate,\n                    measured_rate,\n                    self._token_bucket.available_capacity,\n                )\n                self._enabled = True\n            await self._token_bucket.set_max_rate(\n                min(new_rate, self._MAX_RATE_ADJUST_SCALE * measured_rate)\n            )\n", "aiobotocore/retries/standard.py": "from botocore.retries.standard import (\n    DEFAULT_MAX_ATTEMPTS,\n    ExponentialBackoff,\n    MaxAttemptsChecker,\n    ModeledRetryableChecker,\n    OrRetryChecker,\n    RetryEventAdapter,\n    RetryHandler,\n    RetryPolicy,\n    RetryQuotaChecker,\n    StandardRetryConditions,\n    ThrottledRetryableChecker,\n    TransientRetryableChecker,\n    logger,\n    quota,\n    special,\n)\n\nfrom .._helpers import async_any, resolve_awaitable\nfrom .special import AioRetryDDBChecksumError\n\n\ndef register_retry_handler(client, max_attempts=DEFAULT_MAX_ATTEMPTS):\n    retry_quota = RetryQuotaChecker(quota.RetryQuota())\n\n    service_id = client.meta.service_model.service_id\n    service_event_name = service_id.hyphenize()\n    client.meta.events.register(\n        f'after-call.{service_event_name}', retry_quota.release_retry_quota\n    )\n\n    handler = AioRetryHandler(\n        retry_policy=AioRetryPolicy(\n            retry_checker=AioStandardRetryConditions(\n                max_attempts=max_attempts\n            ),\n            retry_backoff=ExponentialBackoff(),\n        ),\n        retry_event_adapter=RetryEventAdapter(),\n        retry_quota=retry_quota,\n    )\n\n    unique_id = 'retry-config-%s' % service_event_name\n    client.meta.events.register(\n        'needs-retry.%s' % service_event_name,\n        handler.needs_retry,\n        unique_id=unique_id,\n    )\n    return handler\n\n\nclass AioRetryHandler(RetryHandler):\n    async def needs_retry(self, **kwargs):\n        \"\"\"Connect as a handler to the needs-retry event.\"\"\"\n        retry_delay = None\n        context = self._retry_event_adapter.create_retry_context(**kwargs)\n        if await self._retry_policy.should_retry(context):\n            # Before we can retry we need to ensure we have sufficient\n            # capacity in our retry quota.\n            if self._retry_quota.acquire_retry_quota(context):\n                retry_delay = self._retry_policy.compute_retry_delay(context)\n                logger.debug(\n                    \"Retry needed, retrying request after delay of: %s\",\n                    retry_delay,\n                )\n            else:\n                logger.debug(\n                    \"Retry needed but retry quota reached, \"\n                    \"not retrying request.\"\n                )\n        else:\n            logger.debug(\"Not retrying request.\")\n        self._retry_event_adapter.adapt_retry_response_from_context(context)\n        return retry_delay\n\n\nclass AioRetryPolicy(RetryPolicy):\n    async def should_retry(self, context):\n        return await resolve_awaitable(\n            self._retry_checker.is_retryable(context)\n        )\n\n\nclass AioStandardRetryConditions(StandardRetryConditions):\n    def __init__(\n        self, max_attempts=DEFAULT_MAX_ATTEMPTS\n    ):  # noqa: E501, lgtm [py/missing-call-to-init]\n        # Note: This class is for convenience so you can have the\n        # standard retry condition in a single class.\n        self._max_attempts_checker = MaxAttemptsChecker(max_attempts)\n        self._additional_checkers = AioOrRetryChecker(\n            [\n                TransientRetryableChecker(),\n                ThrottledRetryableChecker(),\n                ModeledRetryableChecker(),\n                AioOrRetryChecker(\n                    [\n                        special.RetryIDPCommunicationError(),\n                        AioRetryDDBChecksumError(),\n                    ]\n                ),\n            ]\n        )\n\n    async def is_retryable(self, context):\n        return self._max_attempts_checker.is_retryable(\n            context\n        ) and await resolve_awaitable(\n            self._additional_checkers.is_retryable(context)\n        )\n\n\nclass AioOrRetryChecker(OrRetryChecker):\n    async def is_retryable(self, context):\n        return await async_any(\n            checker.is_retryable(context) for checker in self._checkers\n        )\n", "examples/simple.py": "import asyncio\n\nfrom aiobotocore.session import get_session\n\nAWS_ACCESS_KEY_ID = \"xxx\"\nAWS_SECRET_ACCESS_KEY = \"xxx\"\n\n\nasync def go():\n    bucket = 'dataintake'\n    filename = 'dummy.bin'\n    folder = 'aiobotocore'\n    key = f'{folder}/{filename}'\n\n    session = get_session()\n    async with session.create_client(\n        's3',\n        region_name='us-west-2',\n        aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n        aws_access_key_id=AWS_ACCESS_KEY_ID,\n    ) as client:\n        # upload object to amazon s3\n        data = b'\\x01' * 1024\n        resp = await client.put_object(Bucket=bucket, Key=key, Body=data)\n        print(resp)\n\n        # getting s3 object properties of file we just uploaded\n        resp = await client.get_object_acl(Bucket=bucket, Key=key)\n        print(resp)\n\n        resp = await client.get_object(Bucket=bucket, Key=key)\n        async with resp['Body'] as stream:\n            await stream.read()  # if you do not read the stream the connection cannot be re-used and will be dropped\n            print(resp)\n        \"\"\"\n        This is to ensure the connection is returned to the pool as soon as possible.\n        Otherwise the connection will be released after it is GC'd\n        \"\"\"\n\n        # delete object from s3\n        resp = await client.delete_object(Bucket=bucket, Key=key)\n        print(resp)\n\n\nif __name__ == '__main__':\n    asyncio.run(go())\n", "examples/dynamodb_batch_write.py": "# Boto should get credentials from ~/.aws/credentials or the environment\nimport asyncio\n\nfrom aiobotocore.session import get_session\n\n\ndef get_items(start_num, num_items):\n    \"\"\"\n    Generate a sequence of dynamo items\n\n    :param start_num: Start index\n    :type start_num: int\n    :param num_items: Number of items\n    :type num_items: int\n    :return: List of dictionaries\n    :rtype: list of dict\n    \"\"\"\n    result = []\n    for i in range(start_num, start_num + num_items):\n        result.append({'pk': {'S': f'item{i}'}})\n    return result\n\n\ndef create_batch_write_structure(table_name, start_num, num_items):\n    \"\"\"\n    Create item structure for passing to batch_write_item\n\n    :param table_name: DynamoDB table name\n    :type table_name: str\n    :param start_num: Start index\n    :type start_num: int\n    :param num_items: Number of items\n    :type num_items: int\n    :return: dictionary of tables to write to\n    :rtype: dict\n    \"\"\"\n    return {\n        table_name: [\n            {'PutRequest': {'Item': item}}\n            for item in get_items(start_num, num_items)\n        ]\n    }\n\n\nasync def go():\n    session = get_session()\n    async with session.create_client(\n        'dynamodb', region_name='us-west-2'\n    ) as client:\n        table_name = 'test'\n\n        print('Writing to dynamo')\n        start = 0\n        while True:\n            # Loop adding 25 items to dynamo at a time\n            request_items = create_batch_write_structure(table_name, start, 25)\n            response = await client.batch_write_item(\n                RequestItems=request_items\n            )\n            if len(response['UnprocessedItems']) == 0:\n                print('Wrote 25 items to dynamo')\n            else:\n                # Hit the provisioned write limit\n                print('Hit write limit, backing off then retrying')\n                await asyncio.sleep(5)\n\n                # Items left over that haven't been inserted\n                unprocessed_items = response['UnprocessedItems']\n                print('Resubmitting items')\n                # Loop until unprocessed items are written\n                while len(unprocessed_items) > 0:\n                    response = await client.batch_write_item(\n                        RequestItems=unprocessed_items\n                    )\n                    # If any items are still left over, add them to the\n                    # list to be written\n                    unprocessed_items = response['UnprocessedItems']\n\n                    # If there are items left over, we could do with\n                    # sleeping some more\n                    if len(unprocessed_items) > 0:\n                        print('Backing off for 5 seconds')\n                        await asyncio.sleep(5)\n\n                # Inserted all the unprocessed items, exit loop\n                print('Unprocessed items successfully inserted')\n                break\n\n            start += 25\n\n        # See if DynamoDB has the last item we inserted\n        final_item = 'item' + str(start + 24)\n        print(f'Item \"{final_item}\" should exist')\n\n        response = await client.get_item(\n            TableName=table_name, Key={'pk': {'S': final_item}}\n        )\n        print(f'Response: {response[\"Item\"]}')\n\n\nif __name__ == '__main__':\n    asyncio.run(go())\n", "examples/sqs_queue_consumer.py": "#!/usr/bin/env python3\n\"\"\"\naiobotocore SQS Consumer Example\n\"\"\"\nimport asyncio\nimport sys\n\nimport botocore.exceptions\n\nfrom aiobotocore.session import get_session\n\nQUEUE_NAME = 'test_queue12'\n\n\nasync def go():\n    # Boto should get credentials from ~/.aws/credentials or the environment\n    session = get_session()\n    async with session.create_client('sqs', region_name='us-west-2') as client:\n        try:\n            response = await client.get_queue_url(QueueName=QUEUE_NAME)\n        except botocore.exceptions.ClientError as err:\n            if (\n                err.response['Error']['Code']\n                == 'AWS.SimpleQueueService.NonExistentQueue'\n            ):\n                print(f\"Queue {QUEUE_NAME} does not exist\")\n                sys.exit(1)\n            else:\n                raise\n\n        queue_url = response['QueueUrl']\n\n        print('Pulling messages off the queue')\n\n        while True:\n            try:\n                # This loop wont spin really fast as there is\n                # essentially a sleep in the receive_message call\n                response = await client.receive_message(\n                    QueueUrl=queue_url,\n                    WaitTimeSeconds=2,\n                )\n\n                if 'Messages' in response:\n                    for msg in response['Messages']:\n                        print(f'Got msg \"{msg[\"Body\"]}\"')\n                        # Need to remove msg from queue or else it'll reappear\n                        await client.delete_message(\n                            QueueUrl=queue_url,\n                            ReceiptHandle=msg['ReceiptHandle'],\n                        )\n                else:\n                    print('No messages in queue')\n            except KeyboardInterrupt:\n                break\n\n        print('Finished')\n\n\nif __name__ == '__main__':\n    asyncio.run(go())\n", "examples/sqs_queue_producer.py": "#!/usr/bin/env python3\n\"\"\"\naiobotocore SQS Producer Example\n\"\"\"\nimport asyncio\nimport random\nimport sys\n\nimport botocore.exceptions\n\nfrom aiobotocore.session import get_session\n\nQUEUE_NAME = 'test_queue12'\n\n\nasync def go():\n    # Boto should get credentials from ~/.aws/credentials or the environment\n    session = get_session()\n    async with session.create_client('sqs', region_name='us-west-2') as client:\n        try:\n            response = await client.get_queue_url(QueueName=QUEUE_NAME)\n        except botocore.exceptions.ClientError as err:\n            if (\n                err.response['Error']['Code']\n                == 'AWS.SimpleQueueService.NonExistentQueue'\n            ):\n                print(f\"Queue {QUEUE_NAME} does not exist\")\n                sys.exit(1)\n            else:\n                raise\n\n        queue_url = response['QueueUrl']\n\n        print('Putting messages on the queue')\n\n        msg_no = 1\n        while True:\n            try:\n                msg_body = f'Message #{msg_no}'\n                await client.send_message(\n                    QueueUrl=queue_url, MessageBody=msg_body\n                )\n                msg_no += 1\n\n                print(f'Pushed \"{msg_body}\" to queue')\n\n                await asyncio.sleep(random.randint(1, 4))\n            except KeyboardInterrupt:\n                break\n\n        print('Finished')\n\n\ndef main():\n    try:\n        loop = asyncio.get_event_loop()\n        loop.run_until_complete(go())\n    except KeyboardInterrupt:\n        pass\n\n\nif __name__ == '__main__':\n    main()\n", "examples/dynamodb_create_table.py": "# Boto should get credentials from ~/.aws/credentials or the environment\nimport asyncio\nimport uuid\n\nfrom aiobotocore.session import get_session\n\n\nasync def go():\n    session = get_session()\n    async with session.create_client(\n        'dynamodb', region_name='us-west-2'\n    ) as client:\n        # Create random table name\n        table_name = f'aiobotocore-{uuid.uuid4()}'\n\n        print('Requesting table creation...')\n        await client.create_table(\n            TableName=table_name,\n            AttributeDefinitions=[\n                {'AttributeName': 'testKey', 'AttributeType': 'S'},\n            ],\n            KeySchema=[\n                {'AttributeName': 'testKey', 'KeyType': 'HASH'},\n            ],\n            ProvisionedThroughput={\n                'ReadCapacityUnits': 10,\n                'WriteCapacityUnits': 10,\n            },\n        )\n\n        print(\"Waiting for table to be created...\")\n        waiter = client.get_waiter('table_exists')\n        await waiter.wait(TableName=table_name)\n        print(f\"Table {table_name} created\")\n\n\nif __name__ == '__main__':\n    asyncio.run(go())\n", "examples/sqs_queue_create.py": "# Boto should get credentials from ~/.aws/credentials or the environment\nimport asyncio\n\nfrom aiobotocore.session import get_session\n\n\nasync def go():\n    session = get_session()\n    async with session.create_client('sqs', region_name='us-west-2') as client:\n        print('Creating test_queue1')\n        response = await client.create_queue(QueueName='test_queue1')\n        queue_url = response['QueueUrl']\n\n        response = await client.list_queues()\n\n        print('Queue URLs:')\n        for queue_name in response.get('QueueUrls', []):\n            print(f' {queue_name}')\n\n        print(f'Deleting queue {queue_url}')\n        await client.delete_queue(QueueUrl=queue_url)\n\n        print('Done')\n\n\nif __name__ == '__main__':\n    asyncio.run(go())\n"}