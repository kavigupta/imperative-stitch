{"setup.py": "import platform\nimport sys\nfrom pathlib import Path\n\nimport pkg_resources\nfrom setuptools import find_packages, setup\n\n\ndef read_version(fname=\"whisper/version.py\"):\n    exec(compile(open(fname, encoding=\"utf-8\").read(), fname, \"exec\"))\n    return locals()[\"__version__\"]\n\n\nrequirements = []\nif sys.platform.startswith(\"linux\") and platform.machine() == \"x86_64\":\n    requirements.append(\"triton>=2.0.0,<3\")\n\nsetup(\n    name=\"openai-whisper\",\n    py_modules=[\"whisper\"],\n    version=read_version(),\n    description=\"Robust Speech Recognition via Large-Scale Weak Supervision\",\n    long_description=open(\"README.md\", encoding=\"utf-8\").read(),\n    long_description_content_type=\"text/markdown\",\n    readme=\"README.md\",\n    python_requires=\">=3.8\",\n    author=\"OpenAI\",\n    url=\"https://github.com/openai/whisper\",\n    license=\"MIT\",\n    packages=find_packages(exclude=[\"tests*\"]),\n    install_requires=[\n        str(r)\n        for r in pkg_resources.parse_requirements(\n            Path(__file__).with_name(\"requirements.txt\").open()\n        )\n    ],\n    entry_points={\n        \"console_scripts\": [\"whisper=whisper.transcribe:cli\"],\n    },\n    include_package_data=True,\n    extras_require={\"dev\": [\"pytest\", \"scipy\", \"black\", \"flake8\", \"isort\"]},\n)\n", "whisper/model.py": "import base64\nimport gzip\nfrom dataclasses import dataclass\nfrom typing import Dict, Iterable, Optional\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import Tensor, nn\n\nfrom .decoding import decode as decode_function\nfrom .decoding import detect_language as detect_language_function\nfrom .transcribe import transcribe as transcribe_function\n\n\n@dataclass\nclass ModelDimensions:\n    n_mels: int\n    n_audio_ctx: int\n    n_audio_state: int\n    n_audio_head: int\n    n_audio_layer: int\n    n_vocab: int\n    n_text_ctx: int\n    n_text_state: int\n    n_text_head: int\n    n_text_layer: int\n\n\nclass LayerNorm(nn.LayerNorm):\n    def forward(self, x: Tensor) -> Tensor:\n        return super().forward(x.float()).type(x.dtype)\n\n\nclass Linear(nn.Linear):\n    def forward(self, x: Tensor) -> Tensor:\n        return F.linear(\n            x,\n            self.weight.to(x.dtype),\n            None if self.bias is None else self.bias.to(x.dtype),\n        )\n\n\nclass Conv1d(nn.Conv1d):\n    def _conv_forward(\n        self, x: Tensor, weight: Tensor, bias: Optional[Tensor]\n    ) -> Tensor:\n        return super()._conv_forward(\n            x, weight.to(x.dtype), None if bias is None else bias.to(x.dtype)\n        )\n\n\ndef sinusoids(length, channels, max_timescale=10000):\n    \"\"\"Returns sinusoids for positional embedding\"\"\"\n    assert channels % 2 == 0\n    log_timescale_increment = np.log(max_timescale) / (channels // 2 - 1)\n    inv_timescales = torch.exp(-log_timescale_increment * torch.arange(channels // 2))\n    scaled_time = torch.arange(length)[:, np.newaxis] * inv_timescales[np.newaxis, :]\n    return torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, n_state: int, n_head: int):\n        super().__init__()\n        self.n_head = n_head\n        self.query = Linear(n_state, n_state)\n        self.key = Linear(n_state, n_state, bias=False)\n        self.value = Linear(n_state, n_state)\n        self.out = Linear(n_state, n_state)\n\n    def forward(\n        self,\n        x: Tensor,\n        xa: Optional[Tensor] = None,\n        mask: Optional[Tensor] = None,\n        kv_cache: Optional[dict] = None,\n    ):\n        q = self.query(x)\n\n        if kv_cache is None or xa is None or self.key not in kv_cache:\n            # hooks, if installed (i.e. kv_cache is not None), will prepend the cached kv tensors;\n            # otherwise, perform key/value projections for self- or cross-attention as usual.\n            k = self.key(x if xa is None else xa)\n            v = self.value(x if xa is None else xa)\n        else:\n            # for cross-attention, calculate keys and values once and reuse in subsequent calls.\n            k = kv_cache[self.key]\n            v = kv_cache[self.value]\n\n        wv, qk = self.qkv_attention(q, k, v, mask)\n        return self.out(wv), qk\n\n    def qkv_attention(\n        self, q: Tensor, k: Tensor, v: Tensor, mask: Optional[Tensor] = None\n    ):\n        n_batch, n_ctx, n_state = q.shape\n        scale = (n_state // self.n_head) ** -0.25\n        q = q.view(*q.shape[:2], self.n_head, -1).permute(0, 2, 1, 3) * scale\n        k = k.view(*k.shape[:2], self.n_head, -1).permute(0, 2, 3, 1) * scale\n        v = v.view(*v.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)\n\n        qk = q @ k\n        if mask is not None:\n            qk = qk + mask[:n_ctx, :n_ctx]\n        qk = qk.float()\n\n        w = F.softmax(qk, dim=-1).to(q.dtype)\n        return (w @ v).permute(0, 2, 1, 3).flatten(start_dim=2), qk.detach()\n\n\nclass ResidualAttentionBlock(nn.Module):\n    def __init__(self, n_state: int, n_head: int, cross_attention: bool = False):\n        super().__init__()\n\n        self.attn = MultiHeadAttention(n_state, n_head)\n        self.attn_ln = LayerNorm(n_state)\n\n        self.cross_attn = (\n            MultiHeadAttention(n_state, n_head) if cross_attention else None\n        )\n        self.cross_attn_ln = LayerNorm(n_state) if cross_attention else None\n\n        n_mlp = n_state * 4\n        self.mlp = nn.Sequential(\n            Linear(n_state, n_mlp), nn.GELU(), Linear(n_mlp, n_state)\n        )\n        self.mlp_ln = LayerNorm(n_state)\n\n    def forward(\n        self,\n        x: Tensor,\n        xa: Optional[Tensor] = None,\n        mask: Optional[Tensor] = None,\n        kv_cache: Optional[dict] = None,\n    ):\n        x = x + self.attn(self.attn_ln(x), mask=mask, kv_cache=kv_cache)[0]\n        if self.cross_attn:\n            x = x + self.cross_attn(self.cross_attn_ln(x), xa, kv_cache=kv_cache)[0]\n        x = x + self.mlp(self.mlp_ln(x))\n        return x\n\n\nclass AudioEncoder(nn.Module):\n    def __init__(\n        self, n_mels: int, n_ctx: int, n_state: int, n_head: int, n_layer: int\n    ):\n        super().__init__()\n        self.conv1 = Conv1d(n_mels, n_state, kernel_size=3, padding=1)\n        self.conv2 = Conv1d(n_state, n_state, kernel_size=3, stride=2, padding=1)\n        self.register_buffer(\"positional_embedding\", sinusoids(n_ctx, n_state))\n\n        self.blocks: Iterable[ResidualAttentionBlock] = nn.ModuleList(\n            [ResidualAttentionBlock(n_state, n_head) for _ in range(n_layer)]\n        )\n        self.ln_post = LayerNorm(n_state)\n\n    def forward(self, x: Tensor):\n        \"\"\"\n        x : torch.Tensor, shape = (batch_size, n_mels, n_ctx)\n            the mel spectrogram of the audio\n        \"\"\"\n        x = F.gelu(self.conv1(x))\n        x = F.gelu(self.conv2(x))\n        x = x.permute(0, 2, 1)\n\n        assert x.shape[1:] == self.positional_embedding.shape, \"incorrect audio shape\"\n        x = (x + self.positional_embedding).to(x.dtype)\n\n        for block in self.blocks:\n            x = block(x)\n\n        x = self.ln_post(x)\n        return x\n\n\nclass TextDecoder(nn.Module):\n    def __init__(\n        self, n_vocab: int, n_ctx: int, n_state: int, n_head: int, n_layer: int\n    ):\n        super().__init__()\n\n        self.token_embedding = nn.Embedding(n_vocab, n_state)\n        self.positional_embedding = nn.Parameter(torch.empty(n_ctx, n_state))\n\n        self.blocks: Iterable[ResidualAttentionBlock] = nn.ModuleList(\n            [\n                ResidualAttentionBlock(n_state, n_head, cross_attention=True)\n                for _ in range(n_layer)\n            ]\n        )\n        self.ln = LayerNorm(n_state)\n\n        mask = torch.empty(n_ctx, n_ctx).fill_(-np.inf).triu_(1)\n        self.register_buffer(\"mask\", mask, persistent=False)\n\n    def forward(self, x: Tensor, xa: Tensor, kv_cache: Optional[dict] = None):\n        \"\"\"\n        x : torch.LongTensor, shape = (batch_size, <= n_ctx)\n            the text tokens\n        xa : torch.Tensor, shape = (batch_size, n_audio_ctx, n_audio_state)\n            the encoded audio features to be attended on\n        \"\"\"\n        offset = next(iter(kv_cache.values())).shape[1] if kv_cache else 0\n        x = (\n            self.token_embedding(x)\n            + self.positional_embedding[offset : offset + x.shape[-1]]\n        )\n        x = x.to(xa.dtype)\n\n        for block in self.blocks:\n            x = block(x, xa, mask=self.mask, kv_cache=kv_cache)\n\n        x = self.ln(x)\n        logits = (\n            x @ torch.transpose(self.token_embedding.weight.to(x.dtype), 0, 1)\n        ).float()\n\n        return logits\n\n\nclass Whisper(nn.Module):\n    def __init__(self, dims: ModelDimensions):\n        super().__init__()\n        self.dims = dims\n        self.encoder = AudioEncoder(\n            self.dims.n_mels,\n            self.dims.n_audio_ctx,\n            self.dims.n_audio_state,\n            self.dims.n_audio_head,\n            self.dims.n_audio_layer,\n        )\n        self.decoder = TextDecoder(\n            self.dims.n_vocab,\n            self.dims.n_text_ctx,\n            self.dims.n_text_state,\n            self.dims.n_text_head,\n            self.dims.n_text_layer,\n        )\n        # use the last half among the decoder layers for time alignment by default;\n        # to use a specific set of heads, see `set_alignment_heads()` below.\n        all_heads = torch.zeros(\n            self.dims.n_text_layer, self.dims.n_text_head, dtype=torch.bool\n        )\n        all_heads[self.dims.n_text_layer // 2 :] = True\n        self.register_buffer(\"alignment_heads\", all_heads.to_sparse(), persistent=False)\n\n    def set_alignment_heads(self, dump: bytes):\n        array = np.frombuffer(\n            gzip.decompress(base64.b85decode(dump)), dtype=bool\n        ).copy()\n        mask = torch.from_numpy(array).reshape(\n            self.dims.n_text_layer, self.dims.n_text_head\n        )\n        self.register_buffer(\"alignment_heads\", mask.to_sparse(), persistent=False)\n\n    def embed_audio(self, mel: torch.Tensor):\n        return self.encoder(mel)\n\n    def logits(self, tokens: torch.Tensor, audio_features: torch.Tensor):\n        return self.decoder(tokens, audio_features)\n\n    def forward(\n        self, mel: torch.Tensor, tokens: torch.Tensor\n    ) -> Dict[str, torch.Tensor]:\n        return self.decoder(tokens, self.encoder(mel))\n\n    @property\n    def device(self):\n        return next(self.parameters()).device\n\n    @property\n    def is_multilingual(self):\n        return self.dims.n_vocab >= 51865\n\n    @property\n    def num_languages(self):\n        return self.dims.n_vocab - 51765 - int(self.is_multilingual)\n\n    def install_kv_cache_hooks(self, cache: Optional[dict] = None):\n        \"\"\"\n        The `MultiHeadAttention` module optionally accepts `kv_cache` which stores the key and value\n        tensors calculated for the previous positions. This method returns a dictionary that stores\n        all caches, and the necessary hooks for the key and value projection modules that save the\n        intermediate tensors to be reused during later calculations.\n\n        Returns\n        -------\n        cache : Dict[nn.Module, torch.Tensor]\n            A dictionary object mapping the key/value projection modules to its cache\n        hooks : List[RemovableHandle]\n            List of PyTorch RemovableHandle objects to stop the hooks to be called\n        \"\"\"\n        cache = {**cache} if cache is not None else {}\n        hooks = []\n\n        def save_to_cache(module, _, output):\n            if module not in cache or output.shape[1] > self.dims.n_text_ctx:\n                # save as-is, for the first token or cross attention\n                cache[module] = output\n            else:\n                cache[module] = torch.cat([cache[module], output], dim=1).detach()\n            return cache[module]\n\n        def install_hooks(layer: nn.Module):\n            if isinstance(layer, MultiHeadAttention):\n                hooks.append(layer.key.register_forward_hook(save_to_cache))\n                hooks.append(layer.value.register_forward_hook(save_to_cache))\n\n        self.decoder.apply(install_hooks)\n        return cache, hooks\n\n    detect_language = detect_language_function\n    transcribe = transcribe_function\n    decode = decode_function\n", "whisper/transcribe.py": "import argparse\nimport os\nimport traceback\nimport warnings\nfrom typing import TYPE_CHECKING, List, Optional, Tuple, Union\n\nimport numpy as np\nimport torch\nimport tqdm\n\nfrom .audio import (\n    FRAMES_PER_SECOND,\n    HOP_LENGTH,\n    N_FRAMES,\n    N_SAMPLES,\n    SAMPLE_RATE,\n    log_mel_spectrogram,\n    pad_or_trim,\n)\nfrom .decoding import DecodingOptions, DecodingResult\nfrom .timing import add_word_timestamps\nfrom .tokenizer import LANGUAGES, TO_LANGUAGE_CODE, get_tokenizer\nfrom .utils import (\n    exact_div,\n    format_timestamp,\n    get_end,\n    get_writer,\n    make_safe,\n    optional_float,\n    optional_int,\n    str2bool,\n)\n\nif TYPE_CHECKING:\n    from .model import Whisper\n\n\ndef transcribe(\n    model: \"Whisper\",\n    audio: Union[str, np.ndarray, torch.Tensor],\n    *,\n    verbose: Optional[bool] = None,\n    temperature: Union[float, Tuple[float, ...]] = (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),\n    compression_ratio_threshold: Optional[float] = 2.4,\n    logprob_threshold: Optional[float] = -1.0,\n    no_speech_threshold: Optional[float] = 0.6,\n    condition_on_previous_text: bool = True,\n    initial_prompt: Optional[str] = None,\n    word_timestamps: bool = False,\n    prepend_punctuations: str = \"\\\"'\u201c\u00bf([{-\",\n    append_punctuations: str = \"\\\"'.\u3002,\uff0c!\uff01?\uff1f:\uff1a\u201d)]}\u3001\",\n    clip_timestamps: Union[str, List[float]] = \"0\",\n    hallucination_silence_threshold: Optional[float] = None,\n    **decode_options,\n):\n    \"\"\"\n    Transcribe an audio file using Whisper\n\n    Parameters\n    ----------\n    model: Whisper\n        The Whisper model instance\n\n    audio: Union[str, np.ndarray, torch.Tensor]\n        The path to the audio file to open, or the audio waveform\n\n    verbose: bool\n        Whether to display the text being decoded to the console. If True, displays all the details,\n        If False, displays minimal details. If None, does not display anything\n\n    temperature: Union[float, Tuple[float, ...]]\n        Temperature for sampling. It can be a tuple of temperatures, which will be successively used\n        upon failures according to either `compression_ratio_threshold` or `logprob_threshold`.\n\n    compression_ratio_threshold: float\n        If the gzip compression ratio is above this value, treat as failed\n\n    logprob_threshold: float\n        If the average log probability over sampled tokens is below this value, treat as failed\n\n    no_speech_threshold: float\n        If the no_speech probability is higher than this value AND the average log probability\n        over sampled tokens is below `logprob_threshold`, consider the segment as silent\n\n    condition_on_previous_text: bool\n        if True, the previous output of the model is provided as a prompt for the next window;\n        disabling may make the text inconsistent across windows, but the model becomes less prone to\n        getting stuck in a failure loop, such as repetition looping or timestamps going out of sync.\n\n    word_timestamps: bool\n        Extract word-level timestamps using the cross-attention pattern and dynamic time warping,\n        and include the timestamps for each word in each segment.\n\n    prepend_punctuations: str\n        If word_timestamps is True, merge these punctuation symbols with the next word\n\n    append_punctuations: str\n        If word_timestamps is True, merge these punctuation symbols with the previous word\n\n    initial_prompt: Optional[str]\n        Optional text to provide as a prompt for the first window. This can be used to provide, or\n        \"prompt-engineer\" a context for transcription, e.g. custom vocabularies or proper nouns\n        to make it more likely to predict those word correctly.\n\n    decode_options: dict\n        Keyword arguments to construct `DecodingOptions` instances\n\n    clip_timestamps: Union[str, List[float]]\n        Comma-separated list start,end,start,end,... timestamps (in seconds) of clips to process.\n        The last end timestamp defaults to the end of the file.\n\n    hallucination_silence_threshold: Optional[float]\n        When word_timestamps is True, skip silent periods longer than this threshold (in seconds)\n        when a possible hallucination is detected\n\n    Returns\n    -------\n    A dictionary containing the resulting text (\"text\") and segment-level details (\"segments\"), and\n    the spoken language (\"language\"), which is detected when `decode_options[\"language\"]` is None.\n    \"\"\"\n    dtype = torch.float16 if decode_options.get(\"fp16\", True) else torch.float32\n    if model.device == torch.device(\"cpu\"):\n        if torch.cuda.is_available():\n            warnings.warn(\"Performing inference on CPU when CUDA is available\")\n        if dtype == torch.float16:\n            warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n            dtype = torch.float32\n\n    if dtype == torch.float32:\n        decode_options[\"fp16\"] = False\n\n    # Pad 30-seconds of silence to the input audio, for slicing\n    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)\n    content_frames = mel.shape[-1] - N_FRAMES\n    content_duration = float(content_frames * HOP_LENGTH / SAMPLE_RATE)\n\n    if decode_options.get(\"language\", None) is None:\n        if not model.is_multilingual:\n            decode_options[\"language\"] = \"en\"\n        else:\n            if verbose:\n                print(\n                    \"Detecting language using up to the first 30 seconds. Use `--language` to specify the language\"\n                )\n            mel_segment = pad_or_trim(mel, N_FRAMES).to(model.device).to(dtype)\n            _, probs = model.detect_language(mel_segment)\n            decode_options[\"language\"] = max(probs, key=probs.get)\n            if verbose is not None:\n                print(\n                    f\"Detected language: {LANGUAGES[decode_options['language']].title()}\"\n                )\n\n    language: str = decode_options[\"language\"]\n    task: str = decode_options.get(\"task\", \"transcribe\")\n    tokenizer = get_tokenizer(\n        model.is_multilingual,\n        num_languages=model.num_languages,\n        language=language,\n        task=task,\n    )\n\n    if isinstance(clip_timestamps, str):\n        clip_timestamps = [\n            float(ts) for ts in (clip_timestamps.split(\",\") if clip_timestamps else [])\n        ]\n    seek_points: List[int] = [round(ts * FRAMES_PER_SECOND) for ts in clip_timestamps]\n    if len(seek_points) == 0:\n        seek_points.append(0)\n    if len(seek_points) % 2 == 1:\n        seek_points.append(content_frames)\n    seek_clips: List[Tuple[int, int]] = list(zip(seek_points[::2], seek_points[1::2]))\n\n    punctuation = \"\\\"'\u201c\u00bf([{-\\\"'.\u3002,\uff0c!\uff01?\uff1f:\uff1a\u201d)]}\u3001\"\n\n    if word_timestamps and task == \"translate\":\n        warnings.warn(\"Word-level timestamps on translations may not be reliable.\")\n\n    def decode_with_fallback(segment: torch.Tensor) -> DecodingResult:\n        temperatures = (\n            [temperature] if isinstance(temperature, (int, float)) else temperature\n        )\n        decode_result = None\n\n        for t in temperatures:\n            kwargs = {**decode_options}\n            if t > 0:\n                # disable beam_size and patience when t > 0\n                kwargs.pop(\"beam_size\", None)\n                kwargs.pop(\"patience\", None)\n            else:\n                # disable best_of when t == 0\n                kwargs.pop(\"best_of\", None)\n\n            options = DecodingOptions(**kwargs, temperature=t)\n            decode_result = model.decode(segment, options)\n\n            needs_fallback = False\n            if (\n                compression_ratio_threshold is not None\n                and decode_result.compression_ratio > compression_ratio_threshold\n            ):\n                needs_fallback = True  # too repetitive\n            if (\n                logprob_threshold is not None\n                and decode_result.avg_logprob < logprob_threshold\n            ):\n                needs_fallback = True  # average log probability is too low\n            if (\n                no_speech_threshold is not None\n                and decode_result.no_speech_prob > no_speech_threshold\n            ):\n                needs_fallback = False  # silence\n            if not needs_fallback:\n                break\n\n        return decode_result\n\n    clip_idx = 0\n    seek = seek_clips[clip_idx][0]\n    input_stride = exact_div(\n        N_FRAMES, model.dims.n_audio_ctx\n    )  # mel frames per output token: 2\n    time_precision = (\n        input_stride * HOP_LENGTH / SAMPLE_RATE\n    )  # time per output token: 0.02 (seconds)\n    all_tokens = []\n    all_segments = []\n    prompt_reset_since = 0\n\n    if initial_prompt is not None:\n        initial_prompt_tokens = tokenizer.encode(\" \" + initial_prompt.strip())\n        all_tokens.extend(initial_prompt_tokens)\n    else:\n        initial_prompt_tokens = []\n\n    def new_segment(\n        *, start: float, end: float, tokens: torch.Tensor, result: DecodingResult\n    ):\n        tokens = tokens.tolist()\n        text_tokens = [token for token in tokens if token < tokenizer.eot]\n        return {\n            \"seek\": seek,\n            \"start\": start,\n            \"end\": end,\n            \"text\": tokenizer.decode(text_tokens),\n            \"tokens\": tokens,\n            \"temperature\": result.temperature,\n            \"avg_logprob\": result.avg_logprob,\n            \"compression_ratio\": result.compression_ratio,\n            \"no_speech_prob\": result.no_speech_prob,\n        }\n\n    # show the progress bar when verbose is False (if True, transcribed text will be printed)\n    with tqdm.tqdm(\n        total=content_frames, unit=\"frames\", disable=verbose is not False\n    ) as pbar:\n        last_speech_timestamp = 0.0\n        # NOTE: This loop is obscurely flattened to make the diff readable.\n        # A later commit should turn this into a simpler nested loop.\n        # for seek_clip_start, seek_clip_end in seek_clips:\n        #     while seek < seek_clip_end\n        while clip_idx < len(seek_clips):\n            seek_clip_start, seek_clip_end = seek_clips[clip_idx]\n            if seek < seek_clip_start:\n                seek = seek_clip_start\n            if seek >= seek_clip_end:\n                clip_idx += 1\n                if clip_idx < len(seek_clips):\n                    seek = seek_clips[clip_idx][0]\n                continue\n            time_offset = float(seek * HOP_LENGTH / SAMPLE_RATE)\n            window_end_time = float((seek + N_FRAMES) * HOP_LENGTH / SAMPLE_RATE)\n            segment_size = min(N_FRAMES, content_frames - seek, seek_clip_end - seek)\n            mel_segment = mel[:, seek : seek + segment_size]\n            segment_duration = segment_size * HOP_LENGTH / SAMPLE_RATE\n            mel_segment = pad_or_trim(mel_segment, N_FRAMES).to(model.device).to(dtype)\n\n            decode_options[\"prompt\"] = all_tokens[prompt_reset_since:]\n            result: DecodingResult = decode_with_fallback(mel_segment)\n            tokens = torch.tensor(result.tokens)\n\n            if no_speech_threshold is not None:\n                # no voice activity check\n                should_skip = result.no_speech_prob > no_speech_threshold\n                if (\n                    logprob_threshold is not None\n                    and result.avg_logprob > logprob_threshold\n                ):\n                    # don't skip if the logprob is high enough, despite the no_speech_prob\n                    should_skip = False\n\n                if should_skip:\n                    seek += segment_size  # fast-forward to the next segment boundary\n                    continue\n\n            previous_seek = seek\n            current_segments = []\n\n            # anomalous words are very long/short/improbable\n            def word_anomaly_score(word: dict) -> float:\n                probability = word.get(\"probability\", 0.0)\n                duration = word[\"end\"] - word[\"start\"]\n                score = 0.0\n                if probability < 0.15:\n                    score += 1.0\n                if duration < 0.133:\n                    score += (0.133 - duration) * 15\n                if duration > 2.0:\n                    score += duration - 2.0\n                return score\n\n            def is_segment_anomaly(segment: Optional[dict]) -> bool:\n                if segment is None or not segment[\"words\"]:\n                    return False\n                words = [w for w in segment[\"words\"] if w[\"word\"] not in punctuation]\n                words = words[:8]\n                score = sum(word_anomaly_score(w) for w in words)\n                return score >= 3 or score + 0.01 >= len(words)\n\n            def next_words_segment(segments: List[dict]) -> Optional[dict]:\n                return next((s for s in segments if s[\"words\"]), None)\n\n            timestamp_tokens: torch.Tensor = tokens.ge(tokenizer.timestamp_begin)\n            single_timestamp_ending = timestamp_tokens[-2:].tolist() == [False, True]\n\n            consecutive = torch.where(timestamp_tokens[:-1] & timestamp_tokens[1:])[0]\n            consecutive.add_(1)\n            if len(consecutive) > 0:\n                # if the output contains two consecutive timestamp tokens\n                slices = consecutive.tolist()\n                if single_timestamp_ending:\n                    slices.append(len(tokens))\n\n                last_slice = 0\n                for current_slice in slices:\n                    sliced_tokens = tokens[last_slice:current_slice]\n                    start_timestamp_pos = (\n                        sliced_tokens[0].item() - tokenizer.timestamp_begin\n                    )\n                    end_timestamp_pos = (\n                        sliced_tokens[-1].item() - tokenizer.timestamp_begin\n                    )\n                    current_segments.append(\n                        new_segment(\n                            start=time_offset + start_timestamp_pos * time_precision,\n                            end=time_offset + end_timestamp_pos * time_precision,\n                            tokens=sliced_tokens,\n                            result=result,\n                        )\n                    )\n                    last_slice = current_slice\n\n                if single_timestamp_ending:\n                    # single timestamp at the end means no speech after the last timestamp.\n                    seek += segment_size\n                else:\n                    # otherwise, ignore the unfinished segment and seek to the last timestamp\n                    last_timestamp_pos = (\n                        tokens[last_slice - 1].item() - tokenizer.timestamp_begin\n                    )\n                    seek += last_timestamp_pos * input_stride\n            else:\n                duration = segment_duration\n                timestamps = tokens[timestamp_tokens.nonzero().flatten()]\n                if (\n                    len(timestamps) > 0\n                    and timestamps[-1].item() != tokenizer.timestamp_begin\n                ):\n                    # no consecutive timestamps but it has a timestamp; use the last one.\n                    last_timestamp_pos = (\n                        timestamps[-1].item() - tokenizer.timestamp_begin\n                    )\n                    duration = last_timestamp_pos * time_precision\n\n                current_segments.append(\n                    new_segment(\n                        start=time_offset,\n                        end=time_offset + duration,\n                        tokens=tokens,\n                        result=result,\n                    )\n                )\n                seek += segment_size\n\n            if word_timestamps:\n                add_word_timestamps(\n                    segments=current_segments,\n                    model=model,\n                    tokenizer=tokenizer,\n                    mel=mel_segment,\n                    num_frames=segment_size,\n                    prepend_punctuations=prepend_punctuations,\n                    append_punctuations=append_punctuations,\n                    last_speech_timestamp=last_speech_timestamp,\n                )\n\n                if not single_timestamp_ending:\n                    last_word_end = get_end(current_segments)\n                    if last_word_end is not None and last_word_end > time_offset:\n                        seek = round(last_word_end * FRAMES_PER_SECOND)\n\n                # skip silence before possible hallucinations\n                if hallucination_silence_threshold is not None:\n                    threshold = hallucination_silence_threshold\n                    if not single_timestamp_ending:\n                        last_word_end = get_end(current_segments)\n                        if last_word_end is not None and last_word_end > time_offset:\n                            remaining_duration = window_end_time - last_word_end\n                            if remaining_duration > threshold:\n                                seek = round(last_word_end * FRAMES_PER_SECOND)\n                            else:\n                                seek = previous_seek + segment_size\n\n                    # if first segment might be a hallucination, skip leading silence\n                    first_segment = next_words_segment(current_segments)\n                    if first_segment is not None and is_segment_anomaly(first_segment):\n                        gap = first_segment[\"start\"] - time_offset\n                        if gap > threshold:\n                            seek = previous_seek + round(gap * FRAMES_PER_SECOND)\n                            continue\n\n                    # skip silence before any possible hallucination that is surrounded\n                    # by silence or more hallucinations\n                    hal_last_end = last_speech_timestamp\n                    for si in range(len(current_segments)):\n                        segment = current_segments[si]\n                        if not segment[\"words\"]:\n                            continue\n                        if is_segment_anomaly(segment):\n                            next_segment = next_words_segment(\n                                current_segments[si + 1 :]\n                            )\n                            if next_segment is not None:\n                                hal_next_start = next_segment[\"words\"][0][\"start\"]\n                            else:\n                                hal_next_start = time_offset + segment_duration\n                            silence_before = (\n                                segment[\"start\"] - hal_last_end > threshold\n                                or segment[\"start\"] < threshold\n                                or segment[\"start\"] - time_offset < 2.0\n                            )\n                            silence_after = (\n                                hal_next_start - segment[\"end\"] > threshold\n                                or is_segment_anomaly(next_segment)\n                                or window_end_time - segment[\"end\"] < 2.0\n                            )\n                            if silence_before and silence_after:\n                                seek = round(\n                                    max(time_offset + 1, segment[\"start\"])\n                                    * FRAMES_PER_SECOND\n                                )\n                                if content_duration - segment[\"end\"] < threshold:\n                                    seek = content_frames\n                                current_segments[si:] = []\n                                break\n                        hal_last_end = segment[\"end\"]\n\n                last_word_end = get_end(current_segments)\n                if last_word_end is not None:\n                    last_speech_timestamp = last_word_end\n\n            if verbose:\n                for segment in current_segments:\n                    start, end, text = segment[\"start\"], segment[\"end\"], segment[\"text\"]\n                    line = f\"[{format_timestamp(start)} --> {format_timestamp(end)}] {text}\"\n                    print(make_safe(line))\n\n            # if a segment is instantaneous or does not contain text, clear it\n            for i, segment in enumerate(current_segments):\n                if segment[\"start\"] == segment[\"end\"] or segment[\"text\"].strip() == \"\":\n                    segment[\"text\"] = \"\"\n                    segment[\"tokens\"] = []\n                    segment[\"words\"] = []\n\n            all_segments.extend(\n                [\n                    {\"id\": i, **segment}\n                    for i, segment in enumerate(\n                        current_segments, start=len(all_segments)\n                    )\n                ]\n            )\n            all_tokens.extend(\n                [token for segment in current_segments for token in segment[\"tokens\"]]\n            )\n\n            if not condition_on_previous_text or result.temperature > 0.5:\n                # do not feed the prompt tokens if a high temperature was used\n                prompt_reset_since = len(all_tokens)\n\n            # update progress bar\n            pbar.update(min(content_frames, seek) - previous_seek)\n\n    return dict(\n        text=tokenizer.decode(all_tokens[len(initial_prompt_tokens) :]),\n        segments=all_segments,\n        language=language,\n    )\n\n\ndef cli():\n    from . import available_models\n\n    def valid_model_name(name):\n        if name in available_models() or os.path.exists(name):\n            return name\n        raise ValueError(\n            f\"model should be one of {available_models()} or path to a model checkpoint\"\n        )\n\n    # fmt: off\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\"audio\", nargs=\"+\", type=str, help=\"audio file(s) to transcribe\")\n    parser.add_argument(\"--model\", default=\"small\", type=valid_model_name, help=\"name of the Whisper model to use\")\n    parser.add_argument(\"--model_dir\", type=str, default=None, help=\"the path to save model files; uses ~/.cache/whisper by default\")\n    parser.add_argument(\"--device\", default=\"cuda\" if torch.cuda.is_available() else \"cpu\", help=\"device to use for PyTorch inference\")\n    parser.add_argument(\"--output_dir\", \"-o\", type=str, default=\".\", help=\"directory to save the outputs\")\n    parser.add_argument(\"--output_format\", \"-f\", type=str, default=\"all\", choices=[\"txt\", \"vtt\", \"srt\", \"tsv\", \"json\", \"all\"], help=\"format of the output file; if not specified, all available formats will be produced\")\n    parser.add_argument(\"--verbose\", type=str2bool, default=True, help=\"whether to print out the progress and debug messages\")\n\n    parser.add_argument(\"--task\", type=str, default=\"transcribe\", choices=[\"transcribe\", \"translate\"], help=\"whether to perform X->X speech recognition ('transcribe') or X->English translation ('translate')\")\n    parser.add_argument(\"--language\", type=str, default=None, choices=sorted(LANGUAGES.keys()) + sorted([k.title() for k in TO_LANGUAGE_CODE.keys()]), help=\"language spoken in the audio, specify None to perform language detection\")\n\n    parser.add_argument(\"--temperature\", type=float, default=0, help=\"temperature to use for sampling\")\n    parser.add_argument(\"--best_of\", type=optional_int, default=5, help=\"number of candidates when sampling with non-zero temperature\")\n    parser.add_argument(\"--beam_size\", type=optional_int, default=5, help=\"number of beams in beam search, only applicable when temperature is zero\")\n    parser.add_argument(\"--patience\", type=float, default=None, help=\"optional patience value to use in beam decoding, as in https://arxiv.org/abs/2204.05424, the default (1.0) is equivalent to conventional beam search\")\n    parser.add_argument(\"--length_penalty\", type=float, default=None, help=\"optional token length penalty coefficient (alpha) as in https://arxiv.org/abs/1609.08144, uses simple length normalization by default\")\n\n    parser.add_argument(\"--suppress_tokens\", type=str, default=\"-1\", help=\"comma-separated list of token ids to suppress during sampling; '-1' will suppress most special characters except common punctuations\")\n    parser.add_argument(\"--initial_prompt\", type=str, default=None, help=\"optional text to provide as a prompt for the first window.\")\n    parser.add_argument(\"--condition_on_previous_text\", type=str2bool, default=True, help=\"if True, provide the previous output of the model as a prompt for the next window; disabling may make the text inconsistent across windows, but the model becomes less prone to getting stuck in a failure loop\")\n    parser.add_argument(\"--fp16\", type=str2bool, default=True, help=\"whether to perform inference in fp16; True by default\")\n\n    parser.add_argument(\"--temperature_increment_on_fallback\", type=optional_float, default=0.2, help=\"temperature to increase when falling back when the decoding fails to meet either of the thresholds below\")\n    parser.add_argument(\"--compression_ratio_threshold\", type=optional_float, default=2.4, help=\"if the gzip compression ratio is higher than this value, treat the decoding as failed\")\n    parser.add_argument(\"--logprob_threshold\", type=optional_float, default=-1.0, help=\"if the average log probability is lower than this value, treat the decoding as failed\")\n    parser.add_argument(\"--no_speech_threshold\", type=optional_float, default=0.6, help=\"if the probability of the <|nospeech|> token is higher than this value AND the decoding has failed due to `logprob_threshold`, consider the segment as silence\")\n    parser.add_argument(\"--word_timestamps\", type=str2bool, default=False, help=\"(experimental) extract word-level timestamps and refine the results based on them\")\n    parser.add_argument(\"--prepend_punctuations\", type=str, default=\"\\\"\\'\u201c\u00bf([{-\", help=\"if word_timestamps is True, merge these punctuation symbols with the next word\")\n    parser.add_argument(\"--append_punctuations\", type=str, default=\"\\\"\\'.\u3002,\uff0c!\uff01?\uff1f:\uff1a\u201d)]}\u3001\", help=\"if word_timestamps is True, merge these punctuation symbols with the previous word\")\n    parser.add_argument(\"--highlight_words\", type=str2bool, default=False, help=\"(requires --word_timestamps True) underline each word as it is spoken in srt and vtt\")\n    parser.add_argument(\"--max_line_width\", type=optional_int, default=None, help=\"(requires --word_timestamps True) the maximum number of characters in a line before breaking the line\")\n    parser.add_argument(\"--max_line_count\", type=optional_int, default=None, help=\"(requires --word_timestamps True) the maximum number of lines in a segment\")\n    parser.add_argument(\"--max_words_per_line\", type=optional_int, default=None, help=\"(requires --word_timestamps True, no effect with --max_line_width) the maximum number of words in a segment\")\n    parser.add_argument(\"--threads\", type=optional_int, default=0, help=\"number of threads used by torch for CPU inference; supercedes MKL_NUM_THREADS/OMP_NUM_THREADS\")\n    parser.add_argument(\"--clip_timestamps\", type=str, default=\"0\", help=\"comma-separated list start,end,start,end,... timestamps (in seconds) of clips to process, where the last end timestamp defaults to the end of the file\")\n    parser.add_argument(\"--hallucination_silence_threshold\", type=optional_float, help=\"(requires --word_timestamps True) skip silent periods longer than this threshold (in seconds) when a possible hallucination is detected\")\n    # fmt: on\n\n    args = parser.parse_args().__dict__\n    model_name: str = args.pop(\"model\")\n    model_dir: str = args.pop(\"model_dir\")\n    output_dir: str = args.pop(\"output_dir\")\n    output_format: str = args.pop(\"output_format\")\n    device: str = args.pop(\"device\")\n    os.makedirs(output_dir, exist_ok=True)\n\n    if model_name.endswith(\".en\") and args[\"language\"] not in {\"en\", \"English\"}:\n        if args[\"language\"] is not None:\n            warnings.warn(\n                f\"{model_name} is an English-only model but receipted '{args['language']}'; using English instead.\"\n            )\n        args[\"language\"] = \"en\"\n\n    temperature = args.pop(\"temperature\")\n    if (increment := args.pop(\"temperature_increment_on_fallback\")) is not None:\n        temperature = tuple(np.arange(temperature, 1.0 + 1e-6, increment))\n    else:\n        temperature = [temperature]\n\n    if (threads := args.pop(\"threads\")) > 0:\n        torch.set_num_threads(threads)\n\n    from . import load_model\n\n    model = load_model(model_name, device=device, download_root=model_dir)\n\n    writer = get_writer(output_format, output_dir)\n    word_options = [\n        \"highlight_words\",\n        \"max_line_count\",\n        \"max_line_width\",\n        \"max_words_per_line\",\n    ]\n    if not args[\"word_timestamps\"]:\n        for option in word_options:\n            if args[option]:\n                parser.error(f\"--{option} requires --word_timestamps True\")\n    if args[\"max_line_count\"] and not args[\"max_line_width\"]:\n        warnings.warn(\"--max_line_count has no effect without --max_line_width\")\n    if args[\"max_words_per_line\"] and args[\"max_line_width\"]:\n        warnings.warn(\"--max_words_per_line has no effect with --max_line_width\")\n    writer_args = {arg: args.pop(arg) for arg in word_options}\n    for audio_path in args.pop(\"audio\"):\n        try:\n            result = transcribe(model, audio_path, temperature=temperature, **args)\n            writer(result, audio_path, **writer_args)\n        except Exception as e:\n            traceback.print_exc()\n            print(f\"Skipping {audio_path} due to {type(e).__name__}: {str(e)}\")\n\n\nif __name__ == \"__main__\":\n    cli()\n", "whisper/utils.py": "import json\nimport os\nimport re\nimport sys\nimport zlib\nfrom typing import Callable, List, Optional, TextIO\n\nsystem_encoding = sys.getdefaultencoding()\n\nif system_encoding != \"utf-8\":\n\n    def make_safe(string):\n        # replaces any character not representable using the system default encoding with an '?',\n        # avoiding UnicodeEncodeError (https://github.com/openai/whisper/discussions/729).\n        return string.encode(system_encoding, errors=\"replace\").decode(system_encoding)\n\nelse:\n\n    def make_safe(string):\n        # utf-8 can encode any Unicode code point, so no need to do the round-trip encoding\n        return string\n\n\ndef exact_div(x, y):\n    assert x % y == 0\n    return x // y\n\n\ndef str2bool(string):\n    str2val = {\"True\": True, \"False\": False}\n    if string in str2val:\n        return str2val[string]\n    else:\n        raise ValueError(f\"Expected one of {set(str2val.keys())}, got {string}\")\n\n\ndef optional_int(string):\n    return None if string == \"None\" else int(string)\n\n\ndef optional_float(string):\n    return None if string == \"None\" else float(string)\n\n\ndef compression_ratio(text) -> float:\n    text_bytes = text.encode(\"utf-8\")\n    return len(text_bytes) / len(zlib.compress(text_bytes))\n\n\ndef format_timestamp(\n    seconds: float, always_include_hours: bool = False, decimal_marker: str = \".\"\n):\n    assert seconds >= 0, \"non-negative timestamp expected\"\n    milliseconds = round(seconds * 1000.0)\n\n    hours = milliseconds // 3_600_000\n    milliseconds -= hours * 3_600_000\n\n    minutes = milliseconds // 60_000\n    milliseconds -= minutes * 60_000\n\n    seconds = milliseconds // 1_000\n    milliseconds -= seconds * 1_000\n\n    hours_marker = f\"{hours:02d}:\" if always_include_hours or hours > 0 else \"\"\n    return (\n        f\"{hours_marker}{minutes:02d}:{seconds:02d}{decimal_marker}{milliseconds:03d}\"\n    )\n\n\ndef get_start(segments: List[dict]) -> Optional[float]:\n    return next(\n        (w[\"start\"] for s in segments for w in s[\"words\"]),\n        segments[0][\"start\"] if segments else None,\n    )\n\n\ndef get_end(segments: List[dict]) -> Optional[float]:\n    return next(\n        (w[\"end\"] for s in reversed(segments) for w in reversed(s[\"words\"])),\n        segments[-1][\"end\"] if segments else None,\n    )\n\n\nclass ResultWriter:\n    extension: str\n\n    def __init__(self, output_dir: str):\n        self.output_dir = output_dir\n\n    def __call__(\n        self, result: dict, audio_path: str, options: Optional[dict] = None, **kwargs\n    ):\n        audio_basename = os.path.basename(audio_path)\n        audio_basename = os.path.splitext(audio_basename)[0]\n        output_path = os.path.join(\n            self.output_dir, audio_basename + \".\" + self.extension\n        )\n\n        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n            self.write_result(result, file=f, options=options, **kwargs)\n\n    def write_result(\n        self, result: dict, file: TextIO, options: Optional[dict] = None, **kwargs\n    ):\n        raise NotImplementedError\n\n\nclass WriteTXT(ResultWriter):\n    extension: str = \"txt\"\n\n    def write_result(\n        self, result: dict, file: TextIO, options: Optional[dict] = None, **kwargs\n    ):\n        for segment in result[\"segments\"]:\n            print(segment[\"text\"].strip(), file=file, flush=True)\n\n\nclass SubtitlesWriter(ResultWriter):\n    always_include_hours: bool\n    decimal_marker: str\n\n    def iterate_result(\n        self,\n        result: dict,\n        options: Optional[dict] = None,\n        *,\n        max_line_width: Optional[int] = None,\n        max_line_count: Optional[int] = None,\n        highlight_words: bool = False,\n        max_words_per_line: Optional[int] = None,\n    ):\n        options = options or {}\n        max_line_width = max_line_width or options.get(\"max_line_width\")\n        max_line_count = max_line_count or options.get(\"max_line_count\")\n        highlight_words = highlight_words or options.get(\"highlight_words\", False)\n        max_words_per_line = max_words_per_line or options.get(\"max_words_per_line\")\n        preserve_segments = max_line_count is None or max_line_width is None\n        max_line_width = max_line_width or 1000\n        max_words_per_line = max_words_per_line or 1000\n\n        def iterate_subtitles():\n            line_len = 0\n            line_count = 1\n            # the next subtitle to yield (a list of word timings with whitespace)\n            subtitle: List[dict] = []\n            last: float = get_start(result[\"segments\"]) or 0.0\n            for segment in result[\"segments\"]:\n                chunk_index = 0\n                words_count = max_words_per_line\n                while chunk_index < len(segment[\"words\"]):\n                    remaining_words = len(segment[\"words\"]) - chunk_index\n                    if max_words_per_line > len(segment[\"words\"]) - chunk_index:\n                        words_count = remaining_words\n                    for i, original_timing in enumerate(\n                        segment[\"words\"][chunk_index : chunk_index + words_count]\n                    ):\n                        timing = original_timing.copy()\n                        long_pause = (\n                            not preserve_segments and timing[\"start\"] - last > 3.0\n                        )\n                        has_room = line_len + len(timing[\"word\"]) <= max_line_width\n                        seg_break = i == 0 and len(subtitle) > 0 and preserve_segments\n                        if (\n                            line_len > 0\n                            and has_room\n                            and not long_pause\n                            and not seg_break\n                        ):\n                            # line continuation\n                            line_len += len(timing[\"word\"])\n                        else:\n                            # new line\n                            timing[\"word\"] = timing[\"word\"].strip()\n                            if (\n                                len(subtitle) > 0\n                                and max_line_count is not None\n                                and (long_pause or line_count >= max_line_count)\n                                or seg_break\n                            ):\n                                # subtitle break\n                                yield subtitle\n                                subtitle = []\n                                line_count = 1\n                            elif line_len > 0:\n                                # line break\n                                line_count += 1\n                                timing[\"word\"] = \"\\n\" + timing[\"word\"]\n                            line_len = len(timing[\"word\"].strip())\n                        subtitle.append(timing)\n                        last = timing[\"start\"]\n                    chunk_index += max_words_per_line\n            if len(subtitle) > 0:\n                yield subtitle\n\n        if len(result[\"segments\"]) > 0 and \"words\" in result[\"segments\"][0]:\n            for subtitle in iterate_subtitles():\n                subtitle_start = self.format_timestamp(subtitle[0][\"start\"])\n                subtitle_end = self.format_timestamp(subtitle[-1][\"end\"])\n                subtitle_text = \"\".join([word[\"word\"] for word in subtitle])\n                if highlight_words:\n                    last = subtitle_start\n                    all_words = [timing[\"word\"] for timing in subtitle]\n                    for i, this_word in enumerate(subtitle):\n                        start = self.format_timestamp(this_word[\"start\"])\n                        end = self.format_timestamp(this_word[\"end\"])\n                        if last != start:\n                            yield last, start, subtitle_text\n\n                        yield start, end, \"\".join(\n                            [\n                                re.sub(r\"^(\\s*)(.*)$\", r\"\\1<u>\\2</u>\", word)\n                                if j == i\n                                else word\n                                for j, word in enumerate(all_words)\n                            ]\n                        )\n                        last = end\n                else:\n                    yield subtitle_start, subtitle_end, subtitle_text\n        else:\n            for segment in result[\"segments\"]:\n                segment_start = self.format_timestamp(segment[\"start\"])\n                segment_end = self.format_timestamp(segment[\"end\"])\n                segment_text = segment[\"text\"].strip().replace(\"-->\", \"->\")\n                yield segment_start, segment_end, segment_text\n\n    def format_timestamp(self, seconds: float):\n        return format_timestamp(\n            seconds=seconds,\n            always_include_hours=self.always_include_hours,\n            decimal_marker=self.decimal_marker,\n        )\n\n\nclass WriteVTT(SubtitlesWriter):\n    extension: str = \"vtt\"\n    always_include_hours: bool = False\n    decimal_marker: str = \".\"\n\n    def write_result(\n        self, result: dict, file: TextIO, options: Optional[dict] = None, **kwargs\n    ):\n        print(\"WEBVTT\\n\", file=file)\n        for start, end, text in self.iterate_result(result, options, **kwargs):\n            print(f\"{start} --> {end}\\n{text}\\n\", file=file, flush=True)\n\n\nclass WriteSRT(SubtitlesWriter):\n    extension: str = \"srt\"\n    always_include_hours: bool = True\n    decimal_marker: str = \",\"\n\n    def write_result(\n        self, result: dict, file: TextIO, options: Optional[dict] = None, **kwargs\n    ):\n        for i, (start, end, text) in enumerate(\n            self.iterate_result(result, options, **kwargs), start=1\n        ):\n            print(f\"{i}\\n{start} --> {end}\\n{text}\\n\", file=file, flush=True)\n\n\nclass WriteTSV(ResultWriter):\n    \"\"\"\n    Write a transcript to a file in TSV (tab-separated values) format containing lines like:\n    <start time in integer milliseconds>\\t<end time in integer milliseconds>\\t<transcript text>\n\n    Using integer milliseconds as start and end times means there's no chance of interference from\n    an environment setting a language encoding that causes the decimal in a floating point number\n    to appear as a comma; also is faster and more efficient to parse & store, e.g., in C++.\n    \"\"\"\n\n    extension: str = \"tsv\"\n\n    def write_result(\n        self, result: dict, file: TextIO, options: Optional[dict] = None, **kwargs\n    ):\n        print(\"start\", \"end\", \"text\", sep=\"\\t\", file=file)\n        for segment in result[\"segments\"]:\n            print(round(1000 * segment[\"start\"]), file=file, end=\"\\t\")\n            print(round(1000 * segment[\"end\"]), file=file, end=\"\\t\")\n            print(segment[\"text\"].strip().replace(\"\\t\", \" \"), file=file, flush=True)\n\n\nclass WriteJSON(ResultWriter):\n    extension: str = \"json\"\n\n    def write_result(\n        self, result: dict, file: TextIO, options: Optional[dict] = None, **kwargs\n    ):\n        json.dump(result, file)\n\n\ndef get_writer(\n    output_format: str, output_dir: str\n) -> Callable[[dict, TextIO, dict], None]:\n    writers = {\n        \"txt\": WriteTXT,\n        \"vtt\": WriteVTT,\n        \"srt\": WriteSRT,\n        \"tsv\": WriteTSV,\n        \"json\": WriteJSON,\n    }\n\n    if output_format == \"all\":\n        all_writers = [writer(output_dir) for writer in writers.values()]\n\n        def write_all(\n            result: dict, file: TextIO, options: Optional[dict] = None, **kwargs\n        ):\n            for writer in all_writers:\n                writer(result, file, options, **kwargs)\n\n        return write_all\n\n    return writers[output_format](output_dir)\n", "whisper/triton_ops.py": "from functools import lru_cache\n\nimport numpy as np\nimport torch\n\ntry:\n    import triton\n    import triton.language as tl\nexcept ImportError:\n    raise RuntimeError(\"triton import failed; try `pip install --pre triton`\")\n\n\n@triton.jit\ndef dtw_kernel(\n    cost, trace, x, x_stride, cost_stride, trace_stride, N, M, BLOCK_SIZE: tl.constexpr\n):\n    offsets = tl.arange(0, BLOCK_SIZE)\n    mask = offsets < M\n\n    for k in range(1, N + M + 1):  # k = i + j\n        tl.debug_barrier()\n\n        p0 = cost + (k - 1) * cost_stride\n        p1 = cost + k * cost_stride\n        p2 = cost + k * cost_stride + 1\n\n        c0 = tl.load(p0 + offsets, mask=mask)\n        c1 = tl.load(p1 + offsets, mask=mask)\n        c2 = tl.load(p2 + offsets, mask=mask)\n\n        x_row = tl.load(x + (k - 1) * x_stride + offsets, mask=mask, other=0)\n        cost_row = x_row + tl.minimum(tl.minimum(c0, c1), c2)\n\n        cost_ptr = cost + (k + 1) * cost_stride + 1\n        tl.store(cost_ptr + offsets, cost_row, mask=mask)\n\n        trace_ptr = trace + (k + 1) * trace_stride + 1\n        tl.store(trace_ptr + offsets, 2, mask=mask & (c2 <= c0) & (c2 <= c1))\n        tl.store(trace_ptr + offsets, 1, mask=mask & (c1 <= c0) & (c1 <= c2))\n        tl.store(trace_ptr + offsets, 0, mask=mask & (c0 <= c1) & (c0 <= c2))\n\n\n@lru_cache(maxsize=None)\ndef median_kernel(filter_width: int):\n    @triton.jit\n    def kernel(\n        y, x, x_stride, y_stride, BLOCK_SIZE: tl.constexpr\n    ):  # x.shape[-1] == filter_width\n        row_idx = tl.program_id(0)\n        offsets = tl.arange(0, BLOCK_SIZE)\n        mask = offsets < y_stride\n\n        x_ptr = x + row_idx * x_stride  # noqa: F841\n        y_ptr = y + row_idx * y_stride\n\n        LOAD_ALL_ROWS_HERE  # noqa: F821\n\n        BUBBLESORT_HERE  # noqa: F821\n\n        tl.store(y_ptr + offsets, MIDDLE_ROW_HERE, mask=mask)  # noqa: F821\n\n    kernel = triton.JITFunction(kernel.fn)\n    kernel.src = kernel.src.replace(\n        \"    LOAD_ALL_ROWS_HERE\",\n        \"\\n\".join(\n            [\n                f\"    row{i} = tl.load(x_ptr + offsets + {i}, mask=mask)\"\n                for i in range(filter_width)\n            ]\n        ),\n    )\n    kernel.src = kernel.src.replace(\n        \"    BUBBLESORT_HERE\",\n        \"\\n\\n\".join(\n            [\n                \"\\n\\n\".join(\n                    [\n                        \"\\n\".join(\n                            [\n                                f\"    smaller = tl.where(row{j} < row{j + 1}, row{j}, row{j + 1})\",\n                                f\"    larger = tl.where(row{j} > row{j + 1}, row{j}, row{j + 1})\",\n                                f\"    row{j} = smaller\",\n                                f\"    row{j + 1} = larger\",\n                            ]\n                        )\n                        for j in range(filter_width - i - 1)\n                    ]\n                )\n                for i in range(filter_width // 2 + 1)\n            ]\n        ),\n    )\n    kernel.src = kernel.src.replace(\"MIDDLE_ROW_HERE\", f\"row{filter_width // 2}\")\n\n    return kernel\n\n\ndef median_filter_cuda(x: torch.Tensor, filter_width: int):\n    \"\"\"Apply a median filter of given width along the last dimension of x\"\"\"\n    slices = x.contiguous().unfold(-1, filter_width, 1)\n    grid = np.prod(slices.shape[:-2])\n\n    kernel = median_kernel(filter_width)\n    y = torch.empty_like(slices[..., 0])\n\n    BLOCK_SIZE = 1 << (y.stride(-2) - 1).bit_length()\n    kernel[(grid,)](y, x, x.stride(-2), y.stride(-2), BLOCK_SIZE=BLOCK_SIZE)\n\n    return y\n", "whisper/version.py": "__version__ = \"20231117\"\n", "whisper/timing.py": "import itertools\nimport subprocess\nimport warnings\nfrom dataclasses import dataclass\nfrom typing import TYPE_CHECKING, List\n\nimport numba\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\nfrom .audio import HOP_LENGTH, SAMPLE_RATE, TOKENS_PER_SECOND\nfrom .tokenizer import Tokenizer\n\nif TYPE_CHECKING:\n    from .model import Whisper\n\n\ndef median_filter(x: torch.Tensor, filter_width: int):\n    \"\"\"Apply a median filter of width `filter_width` along the last dimension of `x`\"\"\"\n    pad_width = filter_width // 2\n    if x.shape[-1] <= pad_width:\n        # F.pad requires the padding width to be smaller than the input dimension\n        return x\n\n    if (ndim := x.ndim) <= 2:\n        # `F.pad` does not support 1D or 2D inputs for reflect padding but supports 3D and 4D\n        x = x[None, None, :]\n\n    assert (\n        filter_width > 0 and filter_width % 2 == 1\n    ), \"`filter_width` should be an odd number\"\n\n    result = None\n    x = F.pad(x, (filter_width // 2, filter_width // 2, 0, 0), mode=\"reflect\")\n    if x.is_cuda:\n        try:\n            from .triton_ops import median_filter_cuda\n\n            result = median_filter_cuda(x, filter_width)\n        except (RuntimeError, subprocess.CalledProcessError):\n            warnings.warn(\n                \"Failed to launch Triton kernels, likely due to missing CUDA toolkit; \"\n                \"falling back to a slower median kernel implementation...\"\n            )\n\n    if result is None:\n        # sort() is faster than torch.median (https://github.com/pytorch/pytorch/issues/51450)\n        result = x.unfold(-1, filter_width, 1).sort()[0][..., filter_width // 2]\n\n    if ndim <= 2:\n        result = result[0, 0]\n\n    return result\n\n\n@numba.jit(nopython=True)\ndef backtrace(trace: np.ndarray):\n    i = trace.shape[0] - 1\n    j = trace.shape[1] - 1\n    trace[0, :] = 2\n    trace[:, 0] = 1\n\n    result = []\n    while i > 0 or j > 0:\n        result.append((i - 1, j - 1))\n\n        if trace[i, j] == 0:\n            i -= 1\n            j -= 1\n        elif trace[i, j] == 1:\n            i -= 1\n        elif trace[i, j] == 2:\n            j -= 1\n        else:\n            raise ValueError(\"Unexpected trace[i, j]\")\n\n    result = np.array(result)\n    return result[::-1, :].T\n\n\n@numba.jit(nopython=True, parallel=True)\ndef dtw_cpu(x: np.ndarray):\n    N, M = x.shape\n    cost = np.ones((N + 1, M + 1), dtype=np.float32) * np.inf\n    trace = -np.ones((N + 1, M + 1), dtype=np.float32)\n\n    cost[0, 0] = 0\n    for j in range(1, M + 1):\n        for i in range(1, N + 1):\n            c0 = cost[i - 1, j - 1]\n            c1 = cost[i - 1, j]\n            c2 = cost[i, j - 1]\n\n            if c0 < c1 and c0 < c2:\n                c, t = c0, 0\n            elif c1 < c0 and c1 < c2:\n                c, t = c1, 1\n            else:\n                c, t = c2, 2\n\n            cost[i, j] = x[i - 1, j - 1] + c\n            trace[i, j] = t\n\n    return backtrace(trace)\n\n\ndef dtw_cuda(x, BLOCK_SIZE=1024):\n    from .triton_ops import dtw_kernel\n\n    M, N = x.shape\n    assert M < BLOCK_SIZE, f\"M should be smaller than {BLOCK_SIZE=}\"\n\n    x_skew = (\n        F.pad(x, (0, M + 1), value=np.inf).flatten()[: M * (N + M)].reshape(M, N + M)\n    )\n    x_skew = x_skew.T.contiguous()\n    cost = torch.ones(N + M + 2, M + 2) * np.inf\n    cost[0, 0] = 0\n    cost = cost.cuda()\n    trace = torch.zeros_like(cost, dtype=torch.int32)\n\n    dtw_kernel[(1,)](\n        cost,\n        trace,\n        x_skew,\n        x_skew.stride(0),\n        cost.stride(0),\n        trace.stride(0),\n        N,\n        M,\n        BLOCK_SIZE=BLOCK_SIZE,\n    )\n\n    trace = trace.T.flatten()[: (M + 1) * (M + N + 3)].reshape(M + 1, M + N + 3)[\n        :, : N + 1\n    ]\n    return backtrace(trace.cpu().numpy())\n\n\ndef dtw(x: torch.Tensor) -> np.ndarray:\n    if x.is_cuda:\n        try:\n            return dtw_cuda(x)\n        except (RuntimeError, subprocess.CalledProcessError):\n            warnings.warn(\n                \"Failed to launch Triton kernels, likely due to missing CUDA toolkit; \"\n                \"falling back to a slower DTW implementation...\"\n            )\n\n    return dtw_cpu(x.double().cpu().numpy())\n\n\n@dataclass\nclass WordTiming:\n    word: str\n    tokens: List[int]\n    start: float\n    end: float\n    probability: float\n\n\ndef find_alignment(\n    model: \"Whisper\",\n    tokenizer: Tokenizer,\n    text_tokens: List[int],\n    mel: torch.Tensor,\n    num_frames: int,\n    *,\n    medfilt_width: int = 7,\n    qk_scale: float = 1.0,\n) -> List[WordTiming]:\n    if len(text_tokens) == 0:\n        return []\n\n    tokens = torch.tensor(\n        [\n            *tokenizer.sot_sequence,\n            tokenizer.no_timestamps,\n            *text_tokens,\n            tokenizer.eot,\n        ]\n    ).to(model.device)\n\n    # install hooks on the cross attention layers to retrieve the attention weights\n    QKs = [None] * model.dims.n_text_layer\n    hooks = [\n        block.cross_attn.register_forward_hook(\n            lambda _, ins, outs, index=i: QKs.__setitem__(index, outs[-1][0])\n        )\n        for i, block in enumerate(model.decoder.blocks)\n    ]\n\n    with torch.no_grad():\n        logits = model(mel.unsqueeze(0), tokens.unsqueeze(0))[0]\n        sampled_logits = logits[len(tokenizer.sot_sequence) :, : tokenizer.eot]\n        token_probs = sampled_logits.softmax(dim=-1)\n        text_token_probs = token_probs[np.arange(len(text_tokens)), text_tokens]\n        text_token_probs = text_token_probs.tolist()\n\n    for hook in hooks:\n        hook.remove()\n\n    # heads * tokens * frames\n    weights = torch.stack([QKs[_l][_h] for _l, _h in model.alignment_heads.indices().T])\n    weights = weights[:, :, : num_frames // 2]\n    weights = (weights * qk_scale).softmax(dim=-1)\n    std, mean = torch.std_mean(weights, dim=-2, keepdim=True, unbiased=False)\n    weights = (weights - mean) / std\n    weights = median_filter(weights, medfilt_width)\n\n    matrix = weights.mean(axis=0)\n    matrix = matrix[len(tokenizer.sot_sequence) : -1]\n    text_indices, time_indices = dtw(-matrix)\n\n    words, word_tokens = tokenizer.split_to_word_tokens(text_tokens + [tokenizer.eot])\n    if len(word_tokens) <= 1:\n        # return on eot only\n        # >>> np.pad([], (1, 0))\n        # array([0.])\n        # This results in crashes when we lookup jump_times with float, like\n        # IndexError: arrays used as indices must be of integer (or boolean) type\n        return []\n    word_boundaries = np.pad(np.cumsum([len(t) for t in word_tokens[:-1]]), (1, 0))\n\n    jumps = np.pad(np.diff(text_indices), (1, 0), constant_values=1).astype(bool)\n    jump_times = time_indices[jumps] / TOKENS_PER_SECOND\n    start_times = jump_times[word_boundaries[:-1]]\n    end_times = jump_times[word_boundaries[1:]]\n    word_probabilities = [\n        np.mean(text_token_probs[i:j])\n        for i, j in zip(word_boundaries[:-1], word_boundaries[1:])\n    ]\n\n    return [\n        WordTiming(word, tokens, start, end, probability)\n        for word, tokens, start, end, probability in zip(\n            words, word_tokens, start_times, end_times, word_probabilities\n        )\n    ]\n\n\ndef merge_punctuations(alignment: List[WordTiming], prepended: str, appended: str):\n    # merge prepended punctuations\n    i = len(alignment) - 2\n    j = len(alignment) - 1\n    while i >= 0:\n        previous = alignment[i]\n        following = alignment[j]\n        if previous.word.startswith(\" \") and previous.word.strip() in prepended:\n            # prepend it to the following word\n            following.word = previous.word + following.word\n            following.tokens = previous.tokens + following.tokens\n            previous.word = \"\"\n            previous.tokens = []\n        else:\n            j = i\n        i -= 1\n\n    # merge appended punctuations\n    i = 0\n    j = 1\n    while j < len(alignment):\n        previous = alignment[i]\n        following = alignment[j]\n        if not previous.word.endswith(\" \") and following.word in appended:\n            # append it to the previous word\n            previous.word = previous.word + following.word\n            previous.tokens = previous.tokens + following.tokens\n            following.word = \"\"\n            following.tokens = []\n        else:\n            i = j\n        j += 1\n\n\ndef add_word_timestamps(\n    *,\n    segments: List[dict],\n    model: \"Whisper\",\n    tokenizer: Tokenizer,\n    mel: torch.Tensor,\n    num_frames: int,\n    prepend_punctuations: str = \"\\\"'\u201c\u00bf([{-\",\n    append_punctuations: str = \"\\\"'.\u3002,\uff0c!\uff01?\uff1f:\uff1a\u201d)]}\u3001\",\n    last_speech_timestamp: float,\n    **kwargs,\n):\n    if len(segments) == 0:\n        return\n\n    text_tokens_per_segment = [\n        [token for token in segment[\"tokens\"] if token < tokenizer.eot]\n        for segment in segments\n    ]\n\n    text_tokens = list(itertools.chain.from_iterable(text_tokens_per_segment))\n    alignment = find_alignment(model, tokenizer, text_tokens, mel, num_frames, **kwargs)\n    word_durations = np.array([t.end - t.start for t in alignment])\n    word_durations = word_durations[word_durations.nonzero()]\n    median_duration = np.median(word_durations) if len(word_durations) > 0 else 0.0\n    median_duration = min(0.7, float(median_duration))\n    max_duration = median_duration * 2\n\n    # hack: truncate long words at sentence boundaries.\n    # a better segmentation algorithm based on VAD should be able to replace this.\n    if len(word_durations) > 0:\n        sentence_end_marks = \".\u3002!\uff01?\uff1f\"\n        # ensure words at sentence boundaries are not longer than twice the median word duration.\n        for i in range(1, len(alignment)):\n            if alignment[i].end - alignment[i].start > max_duration:\n                if alignment[i].word in sentence_end_marks:\n                    alignment[i].end = alignment[i].start + max_duration\n                elif alignment[i - 1].word in sentence_end_marks:\n                    alignment[i].start = alignment[i].end - max_duration\n\n    merge_punctuations(alignment, prepend_punctuations, append_punctuations)\n\n    time_offset = segments[0][\"seek\"] * HOP_LENGTH / SAMPLE_RATE\n    word_index = 0\n\n    for segment, text_tokens in zip(segments, text_tokens_per_segment):\n        saved_tokens = 0\n        words = []\n\n        while word_index < len(alignment) and saved_tokens < len(text_tokens):\n            timing = alignment[word_index]\n\n            if timing.word:\n                words.append(\n                    dict(\n                        word=timing.word,\n                        start=round(time_offset + timing.start, 2),\n                        end=round(time_offset + timing.end, 2),\n                        probability=timing.probability,\n                    )\n                )\n\n            saved_tokens += len(timing.tokens)\n            word_index += 1\n\n        # hack: truncate long words at segment boundaries.\n        # a better segmentation algorithm based on VAD should be able to replace this.\n        if len(words) > 0:\n            # ensure the first and second word after a pause is not longer than\n            # twice the median word duration.\n            if words[0][\"end\"] - last_speech_timestamp > median_duration * 4 and (\n                words[0][\"end\"] - words[0][\"start\"] > max_duration\n                or (\n                    len(words) > 1\n                    and words[1][\"end\"] - words[0][\"start\"] > max_duration * 2\n                )\n            ):\n                if (\n                    len(words) > 1\n                    and words[1][\"end\"] - words[1][\"start\"] > max_duration\n                ):\n                    boundary = max(words[1][\"end\"] / 2, words[1][\"end\"] - max_duration)\n                    words[0][\"end\"] = words[1][\"start\"] = boundary\n                words[0][\"start\"] = max(0, words[0][\"end\"] - max_duration)\n\n            # prefer the segment-level start timestamp if the first word is too long.\n            if (\n                segment[\"start\"] < words[0][\"end\"]\n                and segment[\"start\"] - 0.5 > words[0][\"start\"]\n            ):\n                words[0][\"start\"] = max(\n                    0, min(words[0][\"end\"] - median_duration, segment[\"start\"])\n                )\n            else:\n                segment[\"start\"] = words[0][\"start\"]\n\n            # prefer the segment-level end timestamp if the last word is too long.\n            if (\n                segment[\"end\"] > words[-1][\"start\"]\n                and segment[\"end\"] + 0.5 < words[-1][\"end\"]\n            ):\n                words[-1][\"end\"] = max(\n                    words[-1][\"start\"] + median_duration, segment[\"end\"]\n                )\n            else:\n                segment[\"end\"] = words[-1][\"end\"]\n\n            last_speech_timestamp = segment[\"end\"]\n\n        segment[\"words\"] = words\n", "whisper/decoding.py": "from dataclasses import dataclass, field, replace\nfrom typing import TYPE_CHECKING, Dict, Iterable, List, Optional, Sequence, Tuple, Union\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom torch.distributions import Categorical\n\nfrom .audio import CHUNK_LENGTH\nfrom .tokenizer import Tokenizer, get_tokenizer\nfrom .utils import compression_ratio\n\nif TYPE_CHECKING:\n    from .model import Whisper\n\n\n@torch.no_grad()\ndef detect_language(\n    model: \"Whisper\", mel: Tensor, tokenizer: Tokenizer = None\n) -> Tuple[Tensor, List[dict]]:\n    \"\"\"\n    Detect the spoken language in the audio, and return them as list of strings, along with the ids\n    of the most probable language tokens and the probability distribution over all language tokens.\n    This is performed outside the main decode loop in order to not interfere with kv-caching.\n\n    Returns\n    -------\n    language_tokens : Tensor, shape = (n_audio,)\n        ids of the most probable language tokens, which appears after the startoftranscript token.\n    language_probs : List[Dict[str, float]], length = n_audio\n        list of dictionaries containing the probability distribution over all languages.\n    \"\"\"\n    if tokenizer is None:\n        tokenizer = get_tokenizer(\n            model.is_multilingual, num_languages=model.num_languages\n        )\n    if (\n        tokenizer.language is None\n        or tokenizer.language_token not in tokenizer.sot_sequence\n    ):\n        raise ValueError(\n            \"This model doesn't have language tokens so it can't perform lang id\"\n        )\n\n    single = mel.ndim == 2\n    if single:\n        mel = mel.unsqueeze(0)\n\n    # skip encoder forward pass if already-encoded audio features were given\n    if mel.shape[-2:] != (model.dims.n_audio_ctx, model.dims.n_audio_state):\n        mel = model.encoder(mel)\n\n    # forward pass using a single token, startoftranscript\n    n_audio = mel.shape[0]\n    x = torch.tensor([[tokenizer.sot]] * n_audio).to(mel.device)  # [n_audio, 1]\n    logits = model.logits(x, mel)[:, 0]\n\n    # collect detected languages; suppress all non-language tokens\n    mask = torch.ones(logits.shape[-1], dtype=torch.bool)\n    mask[list(tokenizer.all_language_tokens)] = False\n    logits[:, mask] = -np.inf\n    language_tokens = logits.argmax(dim=-1)\n    language_token_probs = logits.softmax(dim=-1).cpu()\n    language_probs = [\n        {\n            c: language_token_probs[i, j].item()\n            for j, c in zip(tokenizer.all_language_tokens, tokenizer.all_language_codes)\n        }\n        for i in range(n_audio)\n    ]\n\n    if single:\n        language_tokens = language_tokens[0]\n        language_probs = language_probs[0]\n\n    return language_tokens, language_probs\n\n\n@dataclass(frozen=True)\nclass DecodingOptions:\n    # whether to perform X->X \"transcribe\" or X->English \"translate\"\n    task: str = \"transcribe\"\n\n    # language that the audio is in; uses detected language if None\n    language: Optional[str] = None\n\n    # sampling-related options\n    temperature: float = 0.0\n    sample_len: Optional[int] = None  # maximum number of tokens to sample\n    best_of: Optional[int] = None  # number of independent sample trajectories, if t > 0\n    beam_size: Optional[int] = None  # number of beams in beam search, if t == 0\n    patience: Optional[float] = None  # patience in beam search (arxiv:2204.05424)\n\n    # \"alpha\" in Google NMT, or None for length norm, when ranking generations\n    # to select which to return among the beams or best-of-N samples\n    length_penalty: Optional[float] = None\n\n    # text or tokens to feed as the prompt or the prefix; for more info:\n    # https://github.com/openai/whisper/discussions/117#discussioncomment-3727051\n    prompt: Optional[Union[str, List[int]]] = None  # for the previous context\n    prefix: Optional[Union[str, List[int]]] = None  # to prefix the current context\n\n    # list of tokens ids (or comma-separated token ids) to suppress\n    # \"-1\" will suppress a set of symbols as defined in `tokenizer.non_speech_tokens()`\n    suppress_tokens: Optional[Union[str, Iterable[int]]] = \"-1\"\n    suppress_blank: bool = True  # this will suppress blank outputs\n\n    # timestamp sampling options\n    without_timestamps: bool = False  # use <|notimestamps|> to sample text tokens only\n    max_initial_timestamp: Optional[float] = 1.0\n\n    # implementation details\n    fp16: bool = True  # use fp16 for most of the calculation\n\n\n@dataclass(frozen=True)\nclass DecodingResult:\n    audio_features: Tensor\n    language: str\n    language_probs: Optional[Dict[str, float]] = None\n    tokens: List[int] = field(default_factory=list)\n    text: str = \"\"\n    avg_logprob: float = np.nan\n    no_speech_prob: float = np.nan\n    temperature: float = np.nan\n    compression_ratio: float = np.nan\n\n\nclass Inference:\n    def logits(self, tokens: Tensor, audio_features: Tensor) -> Tensor:\n        \"\"\"Perform a forward pass on the decoder and return per-token logits\"\"\"\n        raise NotImplementedError\n\n    def rearrange_kv_cache(self, source_indices) -> None:\n        \"\"\"Update the key-value cache according to the updated beams\"\"\"\n        raise NotImplementedError\n\n    def cleanup_caching(self) -> None:\n        \"\"\"Clean up any resources or hooks after decoding is finished\"\"\"\n        pass\n\n\nclass PyTorchInference(Inference):\n    def __init__(self, model: \"Whisper\", initial_token_length: int):\n        self.model: \"Whisper\" = model\n        self.initial_token_length = initial_token_length\n        self.kv_cache = {}\n        self.hooks = []\n\n        key_modules = [block.attn.key for block in self.model.decoder.blocks]\n        value_modules = [block.attn.value for block in self.model.decoder.blocks]\n        self.kv_modules = key_modules + value_modules\n\n    def logits(self, tokens: Tensor, audio_features: Tensor) -> Tensor:\n        if not self.kv_cache:\n            self.kv_cache, self.hooks = self.model.install_kv_cache_hooks()\n\n        if tokens.shape[-1] > self.initial_token_length:\n            # only need to use the last token except in the first forward pass\n            tokens = tokens[:, -1:]\n\n        return self.model.decoder(tokens, audio_features, kv_cache=self.kv_cache)\n\n    def cleanup_caching(self):\n        for hook in self.hooks:\n            hook.remove()\n\n        self.kv_cache = {}\n        self.hooks = []\n\n    def rearrange_kv_cache(self, source_indices):\n        if source_indices != list(range(len(source_indices))):\n            for module in self.kv_modules:\n                # update the key/value cache to contain the selected sequences\n                self.kv_cache[module] = self.kv_cache[module][source_indices].detach()\n\n\nclass SequenceRanker:\n    def rank(\n        self, tokens: List[List[Tensor]], sum_logprobs: List[List[float]]\n    ) -> List[int]:\n        \"\"\"\n        Given a list of groups of samples and their cumulative log probabilities,\n        return the indices of the samples in each group to select as the final result\n        \"\"\"\n        raise NotImplementedError\n\n\nclass MaximumLikelihoodRanker(SequenceRanker):\n    \"\"\"\n    Select the sample with the highest log probabilities, penalized using either\n    a simple length normalization or Google NMT paper's length penalty\n    \"\"\"\n\n    def __init__(self, length_penalty: Optional[float]):\n        self.length_penalty = length_penalty\n\n    def rank(self, tokens: List[List[Tensor]], sum_logprobs: List[List[float]]):\n        def scores(logprobs, lengths):\n            result = []\n            for logprob, length in zip(logprobs, lengths):\n                if self.length_penalty is None:\n                    penalty = length\n                else:\n                    # from the Google NMT paper\n                    penalty = ((5 + length) / 6) ** self.length_penalty\n                result.append(logprob / penalty)\n            return result\n\n        # get the sequence with the highest score\n        lengths = [[len(t) for t in s] for s in tokens]\n        return [np.argmax(scores(p, l)) for p, l in zip(sum_logprobs, lengths)]\n\n\nclass TokenDecoder:\n    def reset(self):\n        \"\"\"Initialize any stateful variables for decoding a new sequence\"\"\"\n\n    def update(\n        self, tokens: Tensor, logits: Tensor, sum_logprobs: Tensor\n    ) -> Tuple[Tensor, bool]:\n        \"\"\"Specify how to select the next token, based on the current trace and logits\n\n        Parameters\n        ----------\n        tokens : Tensor, shape = (n_batch, current_sequence_length)\n            all tokens in the context so far, including the prefix and sot_sequence tokens\n\n        logits : Tensor, shape = (n_batch, vocab_size)\n            per-token logits of the probability distribution at the current step\n\n        sum_logprobs : Tensor, shape = (n_batch)\n            cumulative log probabilities for each sequence\n\n        Returns\n        -------\n        tokens : Tensor, shape = (n_batch, current_sequence_length + 1)\n            the tokens, appended with the selected next token\n\n        completed : bool\n            True if all sequences has reached the end of text\n\n        \"\"\"\n        raise NotImplementedError\n\n    def finalize(\n        self, tokens: Tensor, sum_logprobs: Tensor\n    ) -> Tuple[Sequence[Sequence[Tensor]], List[List[float]]]:\n        \"\"\"Finalize search and return the final candidate sequences\n\n        Parameters\n        ----------\n        tokens : Tensor, shape = (n_audio, n_group, current_sequence_length)\n            all tokens in the context so far, including the prefix and sot_sequence\n\n        sum_logprobs : Tensor, shape = (n_audio, n_group)\n            cumulative log probabilities for each sequence\n\n        Returns\n        -------\n        tokens : Sequence[Sequence[Tensor]], length = n_audio\n            sequence of Tensors containing candidate token sequences, for each audio input\n\n        sum_logprobs : List[List[float]], length = n_audio\n            sequence of cumulative log probabilities corresponding to the above\n\n        \"\"\"\n        raise NotImplementedError\n\n\nclass GreedyDecoder(TokenDecoder):\n    def __init__(self, temperature: float, eot: int):\n        self.temperature = temperature\n        self.eot = eot\n\n    def update(\n        self, tokens: Tensor, logits: Tensor, sum_logprobs: Tensor\n    ) -> Tuple[Tensor, bool]:\n        if self.temperature == 0:\n            next_tokens = logits.argmax(dim=-1)\n        else:\n            next_tokens = Categorical(logits=logits / self.temperature).sample()\n\n        logprobs = F.log_softmax(logits.float(), dim=-1)\n        current_logprobs = logprobs[torch.arange(logprobs.shape[0]), next_tokens]\n        sum_logprobs += current_logprobs * (tokens[:, -1] != self.eot)\n\n        next_tokens[tokens[:, -1] == self.eot] = self.eot\n        tokens = torch.cat([tokens, next_tokens[:, None]], dim=-1)\n\n        completed = (tokens[:, -1] == self.eot).all()\n        return tokens, completed\n\n    def finalize(self, tokens: Tensor, sum_logprobs: Tensor):\n        # make sure each sequence has at least one EOT token at the end\n        tokens = F.pad(tokens, (0, 1), value=self.eot)\n        return tokens, sum_logprobs.tolist()\n\n\nclass BeamSearchDecoder(TokenDecoder):\n    def __init__(\n        self,\n        beam_size: int,\n        eot: int,\n        inference: Inference,\n        patience: Optional[float] = None,\n    ):\n        self.beam_size = beam_size\n        self.eot = eot\n        self.inference = inference\n        self.patience = patience or 1.0\n        self.max_candidates: int = round(beam_size * self.patience)\n        self.finished_sequences = None\n\n        assert (\n            self.max_candidates > 0\n        ), f\"Invalid beam size ({beam_size}) or patience ({patience})\"\n\n    def reset(self):\n        self.finished_sequences = None\n\n    def update(\n        self, tokens: Tensor, logits: Tensor, sum_logprobs: Tensor\n    ) -> Tuple[Tensor, bool]:\n        if tokens.shape[0] % self.beam_size != 0:\n            raise ValueError(f\"{tokens.shape}[0] % {self.beam_size} != 0\")\n\n        n_audio = tokens.shape[0] // self.beam_size\n        if self.finished_sequences is None:  # for the first update\n            self.finished_sequences = [{} for _ in range(n_audio)]\n\n        logprobs = F.log_softmax(logits.float(), dim=-1)\n        next_tokens, source_indices, finished_sequences = [], [], []\n        for i in range(n_audio):\n            scores, sources, finished = {}, {}, {}\n\n            # STEP 1: calculate the cumulative log probabilities for possible candidates\n            for j in range(self.beam_size):\n                idx = i * self.beam_size + j\n                prefix = tokens[idx].tolist()\n                for logprob, token in zip(*logprobs[idx].topk(self.beam_size + 1)):\n                    new_logprob = (sum_logprobs[idx] + logprob).item()\n                    sequence = tuple(prefix + [token.item()])\n                    scores[sequence] = new_logprob\n                    sources[sequence] = idx\n\n            # STEP 2: rank the candidates and keep the top beam_size sequences for each audio\n            saved = 0\n            for sequence in sorted(scores, key=scores.get, reverse=True):\n                if sequence[-1] == self.eot:\n                    finished[sequence] = scores[sequence]\n                else:\n                    sum_logprobs[len(next_tokens)] = scores[sequence]\n                    next_tokens.append(sequence)\n                    source_indices.append(sources[sequence])\n\n                    saved += 1\n                    if saved == self.beam_size:\n                        break\n\n            finished_sequences.append(finished)\n\n        tokens = torch.tensor(next_tokens, device=tokens.device)\n        self.inference.rearrange_kv_cache(source_indices)\n\n        # add newly finished sequences to self.finished_sequences\n        assert len(self.finished_sequences) == len(finished_sequences)\n        for previously_finished, newly_finished in zip(\n            self.finished_sequences, finished_sequences\n        ):\n            for seq in sorted(newly_finished, key=newly_finished.get, reverse=True):\n                if len(previously_finished) >= self.max_candidates:\n                    break  # the candidate list is full\n                previously_finished[seq] = newly_finished[seq]\n\n        # mark as completed if all audio has enough number of samples\n        completed = all(\n            len(sequences) >= self.max_candidates\n            for sequences in self.finished_sequences\n        )\n        return tokens, completed\n\n    def finalize(self, preceding_tokens: Tensor, sum_logprobs: Tensor):\n        # collect all finished sequences, including patience, and add unfinished ones if not enough\n        sum_logprobs = sum_logprobs.cpu()\n        for i, sequences in enumerate(self.finished_sequences):\n            if (\n                len(sequences) < self.beam_size\n            ):  # when not enough sequences are finished\n                for j in list(np.argsort(sum_logprobs[i]))[::-1]:\n                    sequence = preceding_tokens[i, j].tolist() + [self.eot]\n                    sequences[tuple(sequence)] = sum_logprobs[i][j].item()\n                    if len(sequences) >= self.beam_size:\n                        break\n\n        tokens: List[List[Tensor]] = [\n            [torch.tensor(seq) for seq in sequences.keys()]\n            for sequences in self.finished_sequences\n        ]\n        sum_logprobs: List[List[float]] = [\n            list(sequences.values()) for sequences in self.finished_sequences\n        ]\n        return tokens, sum_logprobs\n\n\nclass LogitFilter:\n    def apply(self, logits: Tensor, tokens: Tensor) -> None:\n        \"\"\"Apply any filtering or masking to logits in-place\n\n        Parameters\n        ----------\n        logits : Tensor, shape = (n_batch, vocab_size)\n            per-token logits of the probability distribution at the current step\n\n        tokens : Tensor, shape = (n_batch, current_sequence_length)\n            all tokens in the context so far, including the prefix and sot_sequence tokens\n\n        \"\"\"\n        raise NotImplementedError\n\n\nclass SuppressBlank(LogitFilter):\n    def __init__(self, tokenizer: Tokenizer, sample_begin: int):\n        self.tokenizer = tokenizer\n        self.sample_begin = sample_begin\n\n    def apply(self, logits: Tensor, tokens: Tensor):\n        if tokens.shape[1] == self.sample_begin:\n            logits[:, self.tokenizer.encode(\" \") + [self.tokenizer.eot]] = -np.inf\n\n\nclass SuppressTokens(LogitFilter):\n    def __init__(self, suppress_tokens: Sequence[int]):\n        self.suppress_tokens = list(suppress_tokens)\n\n    def apply(self, logits: Tensor, tokens: Tensor):\n        logits[:, self.suppress_tokens] = -np.inf\n\n\nclass ApplyTimestampRules(LogitFilter):\n    def __init__(\n        self,\n        tokenizer: Tokenizer,\n        sample_begin: int,\n        max_initial_timestamp_index: Optional[int],\n    ):\n        self.tokenizer = tokenizer\n        self.sample_begin = sample_begin\n        self.max_initial_timestamp_index = max_initial_timestamp_index\n\n    def apply(self, logits: Tensor, tokens: Tensor):\n        # suppress <|notimestamps|> which is handled by without_timestamps\n        if self.tokenizer.no_timestamps is not None:\n            logits[:, self.tokenizer.no_timestamps] = -np.inf\n\n        # timestamps have to appear in pairs, except directly before EOT; mask logits accordingly\n        for k in range(tokens.shape[0]):\n            sampled_tokens = tokens[k, self.sample_begin :]\n            seq = [t for t in sampled_tokens.tolist()]\n            last_was_timestamp = (\n                len(seq) >= 1 and seq[-1] >= self.tokenizer.timestamp_begin\n            )\n            penultimate_was_timestamp = (\n                len(seq) < 2 or seq[-2] >= self.tokenizer.timestamp_begin\n            )\n\n            if last_was_timestamp:\n                if penultimate_was_timestamp:  # has to be non-timestamp\n                    logits[k, self.tokenizer.timestamp_begin :] = -np.inf\n                else:  # cannot be normal text tokens\n                    logits[k, : self.tokenizer.eot] = -np.inf\n\n            timestamps = sampled_tokens[\n                sampled_tokens.ge(self.tokenizer.timestamp_begin)\n            ]\n            if timestamps.numel() > 0:\n                # timestamps shouldn't decrease; forbid timestamp tokens smaller than the last\n                # also force each segment to have a nonzero length, to prevent infinite looping\n                if last_was_timestamp and not penultimate_was_timestamp:\n                    timestamp_last = timestamps[-1]\n                else:\n                    timestamp_last = timestamps[-1] + 1\n                logits[k, self.tokenizer.timestamp_begin : timestamp_last] = -np.inf\n\n        if tokens.shape[1] == self.sample_begin:\n            # suppress generating non-timestamp tokens at the beginning\n            logits[:, : self.tokenizer.timestamp_begin] = -np.inf\n\n            # apply the `max_initial_timestamp` option\n            if self.max_initial_timestamp_index is not None:\n                last_allowed = (\n                    self.tokenizer.timestamp_begin + self.max_initial_timestamp_index\n                )\n                logits[:, last_allowed + 1 :] = -np.inf\n\n        # if sum of probability over timestamps is above any other token, sample timestamp\n        logprobs = F.log_softmax(logits.float(), dim=-1)\n        for k in range(tokens.shape[0]):\n            timestamp_logprob = logprobs[k, self.tokenizer.timestamp_begin :].logsumexp(\n                dim=-1\n            )\n            max_text_token_logprob = logprobs[k, : self.tokenizer.timestamp_begin].max()\n            if timestamp_logprob > max_text_token_logprob:\n                logits[k, : self.tokenizer.timestamp_begin] = -np.inf\n\n\nclass DecodingTask:\n    inference: Inference\n    sequence_ranker: SequenceRanker\n    decoder: TokenDecoder\n    logit_filters: List[LogitFilter]\n\n    def __init__(self, model: \"Whisper\", options: DecodingOptions):\n        self.model = model\n\n        language = options.language or \"en\"\n        tokenizer = get_tokenizer(\n            model.is_multilingual,\n            num_languages=model.num_languages,\n            language=language,\n            task=options.task,\n        )\n        self.tokenizer: Tokenizer = tokenizer\n        self.options: DecodingOptions = self._verify_options(options)\n\n        self.n_group: int = options.beam_size or options.best_of or 1\n        self.n_ctx: int = model.dims.n_text_ctx\n        self.sample_len: int = options.sample_len or model.dims.n_text_ctx // 2\n\n        self.sot_sequence: Tuple[int] = tokenizer.sot_sequence\n        if self.options.without_timestamps:\n            self.sot_sequence = tokenizer.sot_sequence_including_notimestamps\n\n        self.initial_tokens: Tuple[int] = self._get_initial_tokens()\n        self.sample_begin: int = len(self.initial_tokens)\n        self.sot_index: int = self.initial_tokens.index(tokenizer.sot)\n\n        # inference: implements the forward pass through the decoder, including kv caching\n        self.inference = PyTorchInference(model, len(self.initial_tokens))\n\n        # sequence ranker: implements how to rank a group of sampled sequences\n        self.sequence_ranker = MaximumLikelihoodRanker(options.length_penalty)\n\n        # decoder: implements how to select the next tokens, given the autoregressive distribution\n        if options.beam_size is not None:\n            self.decoder = BeamSearchDecoder(\n                options.beam_size, tokenizer.eot, self.inference, options.patience\n            )\n        else:\n            self.decoder = GreedyDecoder(options.temperature, tokenizer.eot)\n\n        # logit filters: applies various rules to suppress or penalize certain tokens\n        self.logit_filters = []\n        if self.options.suppress_blank:\n            self.logit_filters.append(SuppressBlank(self.tokenizer, self.sample_begin))\n        if self.options.suppress_tokens:\n            self.logit_filters.append(SuppressTokens(self._get_suppress_tokens()))\n        if not options.without_timestamps:\n            precision = CHUNK_LENGTH / model.dims.n_audio_ctx  # usually 0.02 seconds\n            max_initial_timestamp_index = None\n            if options.max_initial_timestamp:\n                max_initial_timestamp_index = round(\n                    self.options.max_initial_timestamp / precision\n                )\n            self.logit_filters.append(\n                ApplyTimestampRules(\n                    tokenizer, self.sample_begin, max_initial_timestamp_index\n                )\n            )\n\n    def _verify_options(self, options: DecodingOptions) -> DecodingOptions:\n        if options.beam_size is not None and options.best_of is not None:\n            raise ValueError(\"beam_size and best_of can't be given together\")\n        if options.temperature == 0:\n            if options.best_of is not None:\n                raise ValueError(\"best_of with greedy sampling (T=0) is not compatible\")\n        if options.patience is not None and options.beam_size is None:\n            raise ValueError(\"patience requires beam_size to be given\")\n        if options.length_penalty is not None and not (\n            0 <= options.length_penalty <= 1\n        ):\n            raise ValueError(\"length_penalty (alpha) should be a value between 0 and 1\")\n\n        return options\n\n    def _get_initial_tokens(self) -> Tuple[int]:\n        tokens = list(self.sot_sequence)\n\n        if prefix := self.options.prefix:\n            prefix_tokens = (\n                self.tokenizer.encode(\" \" + prefix.strip())\n                if isinstance(prefix, str)\n                else prefix\n            )\n            if self.sample_len is not None:\n                max_prefix_len = self.n_ctx // 2 - self.sample_len\n                prefix_tokens = prefix_tokens[-max_prefix_len:]\n            tokens = tokens + prefix_tokens\n\n        if prompt := self.options.prompt:\n            prompt_tokens = (\n                self.tokenizer.encode(\" \" + prompt.strip())\n                if isinstance(prompt, str)\n                else prompt\n            )\n            tokens = (\n                [self.tokenizer.sot_prev]\n                + prompt_tokens[-(self.n_ctx // 2 - 1) :]\n                + tokens\n            )\n\n        return tuple(tokens)\n\n    def _get_suppress_tokens(self) -> Tuple[int]:\n        suppress_tokens = self.options.suppress_tokens\n\n        if isinstance(suppress_tokens, str):\n            suppress_tokens = [int(t) for t in suppress_tokens.split(\",\")]\n\n        if -1 in suppress_tokens:\n            suppress_tokens = [t for t in suppress_tokens if t >= 0]\n            suppress_tokens.extend(self.tokenizer.non_speech_tokens)\n        elif suppress_tokens is None or len(suppress_tokens) == 0:\n            suppress_tokens = []  # interpret empty string as an empty list\n        else:\n            assert isinstance(suppress_tokens, list), \"suppress_tokens must be a list\"\n\n        suppress_tokens.extend(\n            [\n                self.tokenizer.transcribe,\n                self.tokenizer.translate,\n                self.tokenizer.sot,\n                self.tokenizer.sot_prev,\n                self.tokenizer.sot_lm,\n            ]\n        )\n        if self.tokenizer.no_speech is not None:\n            # no-speech probability is collected separately\n            suppress_tokens.append(self.tokenizer.no_speech)\n\n        return tuple(sorted(set(suppress_tokens)))\n\n    def _get_audio_features(self, mel: Tensor):\n        if self.options.fp16:\n            mel = mel.half()\n\n        if mel.shape[-2:] == (\n            self.model.dims.n_audio_ctx,\n            self.model.dims.n_audio_state,\n        ):\n            # encoded audio features are given; skip audio encoding\n            audio_features = mel\n        else:\n            audio_features = self.model.encoder(mel)\n\n        if audio_features.dtype != (\n            torch.float16 if self.options.fp16 else torch.float32\n        ):\n            return TypeError(\n                f\"audio_features has an incorrect dtype: {audio_features.dtype}\"\n            )\n\n        return audio_features\n\n    def _detect_language(self, audio_features: Tensor, tokens: Tensor):\n        languages = [self.options.language] * audio_features.shape[0]\n        lang_probs = None\n\n        if self.options.language is None or self.options.task == \"lang_id\":\n            lang_tokens, lang_probs = self.model.detect_language(\n                audio_features, self.tokenizer\n            )\n            languages = [max(probs, key=probs.get) for probs in lang_probs]\n            if self.options.language is None:\n                tokens[:, self.sot_index + 1] = lang_tokens  # write language tokens\n\n        return languages, lang_probs\n\n    def _main_loop(self, audio_features: Tensor, tokens: Tensor):\n        n_batch = tokens.shape[0]\n        sum_logprobs: Tensor = torch.zeros(n_batch, device=audio_features.device)\n        no_speech_probs = [np.nan] * n_batch\n\n        try:\n            for i in range(self.sample_len):\n                logits = self.inference.logits(tokens, audio_features)\n\n                if (\n                    i == 0 and self.tokenizer.no_speech is not None\n                ):  # save no_speech_probs\n                    probs_at_sot = logits[:, self.sot_index].float().softmax(dim=-1)\n                    no_speech_probs = probs_at_sot[:, self.tokenizer.no_speech].tolist()\n\n                # now we need to consider the logits at the last token only\n                logits = logits[:, -1]\n\n                # apply the logit filters, e.g. for suppressing or applying penalty to\n                for logit_filter in self.logit_filters:\n                    logit_filter.apply(logits, tokens)\n\n                # expand the tokens tensor with the selected next tokens\n                tokens, completed = self.decoder.update(tokens, logits, sum_logprobs)\n\n                if completed or tokens.shape[-1] > self.n_ctx:\n                    break\n        finally:\n            self.inference.cleanup_caching()\n\n        return tokens, sum_logprobs, no_speech_probs\n\n    @torch.no_grad()\n    def run(self, mel: Tensor) -> List[DecodingResult]:\n        self.decoder.reset()\n        tokenizer: Tokenizer = self.tokenizer\n        n_audio: int = mel.shape[0]\n\n        audio_features: Tensor = self._get_audio_features(mel)  # encoder forward pass\n        tokens: Tensor = torch.tensor([self.initial_tokens]).repeat(n_audio, 1)\n\n        # detect language if requested, overwriting the language token\n        languages, language_probs = self._detect_language(audio_features, tokens)\n        if self.options.task == \"lang_id\":\n            return [\n                DecodingResult(\n                    audio_features=features, language=language, language_probs=probs\n                )\n                for features, language, probs in zip(\n                    audio_features, languages, language_probs\n                )\n            ]\n\n        # repeat text tensors by the group size, for beam search or best-of-n sampling\n        tokens = tokens.repeat_interleave(self.n_group, dim=0).to(audio_features.device)\n\n        # call the main sampling loop\n        tokens, sum_logprobs, no_speech_probs = self._main_loop(audio_features, tokens)\n\n        # reshape the tensors to have (n_audio, n_group) as the first two dimensions\n        audio_features = audio_features[:: self.n_group]\n        no_speech_probs = no_speech_probs[:: self.n_group]\n        assert audio_features.shape[0] == len(no_speech_probs) == n_audio\n\n        tokens = tokens.reshape(n_audio, self.n_group, -1)\n        sum_logprobs = sum_logprobs.reshape(n_audio, self.n_group)\n\n        # get the final candidates for each group, and slice between the first sampled token and EOT\n        tokens, sum_logprobs = self.decoder.finalize(tokens, sum_logprobs)\n        tokens: List[List[Tensor]] = [\n            [t[self.sample_begin : (t == tokenizer.eot).nonzero()[0, 0]] for t in s]\n            for s in tokens\n        ]\n\n        # select the top-ranked sample in each group\n        selected = self.sequence_ranker.rank(tokens, sum_logprobs)\n        tokens: List[List[int]] = [t[i].tolist() for i, t in zip(selected, tokens)]\n        texts: List[str] = [tokenizer.decode(t).strip() for t in tokens]\n\n        sum_logprobs: List[float] = [lp[i] for i, lp in zip(selected, sum_logprobs)]\n        avg_logprobs: List[float] = [\n            lp / (len(t) + 1) for t, lp in zip(tokens, sum_logprobs)\n        ]\n\n        fields = (\n            texts,\n            languages,\n            tokens,\n            audio_features,\n            avg_logprobs,\n            no_speech_probs,\n        )\n        if len(set(map(len, fields))) != 1:\n            raise RuntimeError(f\"inconsistent result lengths: {list(map(len, fields))}\")\n\n        return [\n            DecodingResult(\n                audio_features=features,\n                language=language,\n                tokens=tokens,\n                text=text,\n                avg_logprob=avg_logprob,\n                no_speech_prob=no_speech_prob,\n                temperature=self.options.temperature,\n                compression_ratio=compression_ratio(text),\n            )\n            for text, language, tokens, features, avg_logprob, no_speech_prob in zip(\n                *fields\n            )\n        ]\n\n\n@torch.no_grad()\ndef decode(\n    model: \"Whisper\",\n    mel: Tensor,\n    options: DecodingOptions = DecodingOptions(),\n    **kwargs,\n) -> Union[DecodingResult, List[DecodingResult]]:\n    \"\"\"\n    Performs decoding of 30-second audio segment(s), provided as Mel spectrogram(s).\n\n    Parameters\n    ----------\n    model: Whisper\n        the Whisper model instance\n\n    mel: torch.Tensor, shape = (80, 3000) or (*, 80, 3000)\n        A tensor containing the Mel spectrogram(s)\n\n    options: DecodingOptions\n        A dataclass that contains all necessary options for decoding 30-second segments\n\n    Returns\n    -------\n    result: Union[DecodingResult, List[DecodingResult]]\n        The result(s) of decoding contained in `DecodingResult` dataclass instance(s)\n    \"\"\"\n    if single := mel.ndim == 2:\n        mel = mel.unsqueeze(0)\n\n    if kwargs:\n        options = replace(options, **kwargs)\n\n    result = DecodingTask(model, options).run(mel)\n\n    return result[0] if single else result\n", "whisper/audio.py": "import os\nfrom functools import lru_cache\nfrom subprocess import CalledProcessError, run\nfrom typing import Optional, Union\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\nfrom .utils import exact_div\n\n# hard-coded audio hyperparameters\nSAMPLE_RATE = 16000\nN_FFT = 400\nHOP_LENGTH = 160\nCHUNK_LENGTH = 30\nN_SAMPLES = CHUNK_LENGTH * SAMPLE_RATE  # 480000 samples in a 30-second chunk\nN_FRAMES = exact_div(N_SAMPLES, HOP_LENGTH)  # 3000 frames in a mel spectrogram input\n\nN_SAMPLES_PER_TOKEN = HOP_LENGTH * 2  # the initial convolutions has stride 2\nFRAMES_PER_SECOND = exact_div(SAMPLE_RATE, HOP_LENGTH)  # 10ms per audio frame\nTOKENS_PER_SECOND = exact_div(SAMPLE_RATE, N_SAMPLES_PER_TOKEN)  # 20ms per audio token\n\n\ndef load_audio(file: str, sr: int = SAMPLE_RATE):\n    \"\"\"\n    Open an audio file and read as mono waveform, resampling as necessary\n\n    Parameters\n    ----------\n    file: str\n        The audio file to open\n\n    sr: int\n        The sample rate to resample the audio if necessary\n\n    Returns\n    -------\n    A NumPy array containing the audio waveform, in float32 dtype.\n    \"\"\"\n\n    # This launches a subprocess to decode audio while down-mixing\n    # and resampling as necessary.  Requires the ffmpeg CLI in PATH.\n    # fmt: off\n    cmd = [\n        \"ffmpeg\",\n        \"-nostdin\",\n        \"-threads\", \"0\",\n        \"-i\", file,\n        \"-f\", \"s16le\",\n        \"-ac\", \"1\",\n        \"-acodec\", \"pcm_s16le\",\n        \"-ar\", str(sr),\n        \"-\"\n    ]\n    # fmt: on\n    try:\n        out = run(cmd, capture_output=True, check=True).stdout\n    except CalledProcessError as e:\n        raise RuntimeError(f\"Failed to load audio: {e.stderr.decode()}\") from e\n\n    return np.frombuffer(out, np.int16).flatten().astype(np.float32) / 32768.0\n\n\ndef pad_or_trim(array, length: int = N_SAMPLES, *, axis: int = -1):\n    \"\"\"\n    Pad or trim the audio array to N_SAMPLES, as expected by the encoder.\n    \"\"\"\n    if torch.is_tensor(array):\n        if array.shape[axis] > length:\n            array = array.index_select(\n                dim=axis, index=torch.arange(length, device=array.device)\n            )\n\n        if array.shape[axis] < length:\n            pad_widths = [(0, 0)] * array.ndim\n            pad_widths[axis] = (0, length - array.shape[axis])\n            array = F.pad(array, [pad for sizes in pad_widths[::-1] for pad in sizes])\n    else:\n        if array.shape[axis] > length:\n            array = array.take(indices=range(length), axis=axis)\n\n        if array.shape[axis] < length:\n            pad_widths = [(0, 0)] * array.ndim\n            pad_widths[axis] = (0, length - array.shape[axis])\n            array = np.pad(array, pad_widths)\n\n    return array\n\n\n@lru_cache(maxsize=None)\ndef mel_filters(device, n_mels: int) -> torch.Tensor:\n    \"\"\"\n    load the mel filterbank matrix for projecting STFT into a Mel spectrogram.\n    Allows decoupling librosa dependency; saved using:\n\n        np.savez_compressed(\n            \"mel_filters.npz\",\n            mel_80=librosa.filters.mel(sr=16000, n_fft=400, n_mels=80),\n            mel_128=librosa.filters.mel(sr=16000, n_fft=400, n_mels=128),\n        )\n    \"\"\"\n    assert n_mels in {80, 128}, f\"Unsupported n_mels: {n_mels}\"\n\n    filters_path = os.path.join(os.path.dirname(__file__), \"assets\", \"mel_filters.npz\")\n    with np.load(filters_path, allow_pickle=False) as f:\n        return torch.from_numpy(f[f\"mel_{n_mels}\"]).to(device)\n\n\ndef log_mel_spectrogram(\n    audio: Union[str, np.ndarray, torch.Tensor],\n    n_mels: int = 80,\n    padding: int = 0,\n    device: Optional[Union[str, torch.device]] = None,\n):\n    \"\"\"\n    Compute the log-Mel spectrogram of\n\n    Parameters\n    ----------\n    audio: Union[str, np.ndarray, torch.Tensor], shape = (*)\n        The path to audio or either a NumPy array or Tensor containing the audio waveform in 16 kHz\n\n    n_mels: int\n        The number of Mel-frequency filters, only 80 is supported\n\n    padding: int\n        Number of zero samples to pad to the right\n\n    device: Optional[Union[str, torch.device]]\n        If given, the audio tensor is moved to this device before STFT\n\n    Returns\n    -------\n    torch.Tensor, shape = (80, n_frames)\n        A Tensor that contains the Mel spectrogram\n    \"\"\"\n    if not torch.is_tensor(audio):\n        if isinstance(audio, str):\n            audio = load_audio(audio)\n        audio = torch.from_numpy(audio)\n\n    if device is not None:\n        audio = audio.to(device)\n    if padding > 0:\n        audio = F.pad(audio, (0, padding))\n    window = torch.hann_window(N_FFT).to(audio.device)\n    stft = torch.stft(audio, N_FFT, HOP_LENGTH, window=window, return_complex=True)\n    magnitudes = stft[..., :-1].abs() ** 2\n\n    filters = mel_filters(audio.device, n_mels)\n    mel_spec = filters @ magnitudes\n\n    log_spec = torch.clamp(mel_spec, min=1e-10).log10()\n    log_spec = torch.maximum(log_spec, log_spec.max() - 8.0)\n    log_spec = (log_spec + 4.0) / 4.0\n    return log_spec\n", "whisper/__main__.py": "from .transcribe import cli\n\ncli()\n", "whisper/tokenizer.py": "import base64\nimport os\nimport string\nfrom dataclasses import dataclass, field\nfrom functools import cached_property, lru_cache\nfrom typing import Dict, List, Optional, Tuple\n\nimport tiktoken\n\nLANGUAGES = {\n    \"en\": \"english\",\n    \"zh\": \"chinese\",\n    \"de\": \"german\",\n    \"es\": \"spanish\",\n    \"ru\": \"russian\",\n    \"ko\": \"korean\",\n    \"fr\": \"french\",\n    \"ja\": \"japanese\",\n    \"pt\": \"portuguese\",\n    \"tr\": \"turkish\",\n    \"pl\": \"polish\",\n    \"ca\": \"catalan\",\n    \"nl\": \"dutch\",\n    \"ar\": \"arabic\",\n    \"sv\": \"swedish\",\n    \"it\": \"italian\",\n    \"id\": \"indonesian\",\n    \"hi\": \"hindi\",\n    \"fi\": \"finnish\",\n    \"vi\": \"vietnamese\",\n    \"he\": \"hebrew\",\n    \"uk\": \"ukrainian\",\n    \"el\": \"greek\",\n    \"ms\": \"malay\",\n    \"cs\": \"czech\",\n    \"ro\": \"romanian\",\n    \"da\": \"danish\",\n    \"hu\": \"hungarian\",\n    \"ta\": \"tamil\",\n    \"no\": \"norwegian\",\n    \"th\": \"thai\",\n    \"ur\": \"urdu\",\n    \"hr\": \"croatian\",\n    \"bg\": \"bulgarian\",\n    \"lt\": \"lithuanian\",\n    \"la\": \"latin\",\n    \"mi\": \"maori\",\n    \"ml\": \"malayalam\",\n    \"cy\": \"welsh\",\n    \"sk\": \"slovak\",\n    \"te\": \"telugu\",\n    \"fa\": \"persian\",\n    \"lv\": \"latvian\",\n    \"bn\": \"bengali\",\n    \"sr\": \"serbian\",\n    \"az\": \"azerbaijani\",\n    \"sl\": \"slovenian\",\n    \"kn\": \"kannada\",\n    \"et\": \"estonian\",\n    \"mk\": \"macedonian\",\n    \"br\": \"breton\",\n    \"eu\": \"basque\",\n    \"is\": \"icelandic\",\n    \"hy\": \"armenian\",\n    \"ne\": \"nepali\",\n    \"mn\": \"mongolian\",\n    \"bs\": \"bosnian\",\n    \"kk\": \"kazakh\",\n    \"sq\": \"albanian\",\n    \"sw\": \"swahili\",\n    \"gl\": \"galician\",\n    \"mr\": \"marathi\",\n    \"pa\": \"punjabi\",\n    \"si\": \"sinhala\",\n    \"km\": \"khmer\",\n    \"sn\": \"shona\",\n    \"yo\": \"yoruba\",\n    \"so\": \"somali\",\n    \"af\": \"afrikaans\",\n    \"oc\": \"occitan\",\n    \"ka\": \"georgian\",\n    \"be\": \"belarusian\",\n    \"tg\": \"tajik\",\n    \"sd\": \"sindhi\",\n    \"gu\": \"gujarati\",\n    \"am\": \"amharic\",\n    \"yi\": \"yiddish\",\n    \"lo\": \"lao\",\n    \"uz\": \"uzbek\",\n    \"fo\": \"faroese\",\n    \"ht\": \"haitian creole\",\n    \"ps\": \"pashto\",\n    \"tk\": \"turkmen\",\n    \"nn\": \"nynorsk\",\n    \"mt\": \"maltese\",\n    \"sa\": \"sanskrit\",\n    \"lb\": \"luxembourgish\",\n    \"my\": \"myanmar\",\n    \"bo\": \"tibetan\",\n    \"tl\": \"tagalog\",\n    \"mg\": \"malagasy\",\n    \"as\": \"assamese\",\n    \"tt\": \"tatar\",\n    \"haw\": \"hawaiian\",\n    \"ln\": \"lingala\",\n    \"ha\": \"hausa\",\n    \"ba\": \"bashkir\",\n    \"jw\": \"javanese\",\n    \"su\": \"sundanese\",\n    \"yue\": \"cantonese\",\n}\n\n# language code lookup by name, with a few language aliases\nTO_LANGUAGE_CODE = {\n    **{language: code for code, language in LANGUAGES.items()},\n    \"burmese\": \"my\",\n    \"valencian\": \"ca\",\n    \"flemish\": \"nl\",\n    \"haitian\": \"ht\",\n    \"letzeburgesch\": \"lb\",\n    \"pushto\": \"ps\",\n    \"panjabi\": \"pa\",\n    \"moldavian\": \"ro\",\n    \"moldovan\": \"ro\",\n    \"sinhalese\": \"si\",\n    \"castilian\": \"es\",\n    \"mandarin\": \"zh\",\n}\n\n\n@dataclass\nclass Tokenizer:\n    \"\"\"A thin wrapper around `tiktoken` providing quick access to special tokens\"\"\"\n\n    encoding: tiktoken.Encoding\n    num_languages: int\n    language: Optional[str] = None\n    task: Optional[str] = None\n    sot_sequence: Tuple[int] = ()\n    special_tokens: Dict[str, int] = field(default_factory=dict)\n\n    def __post_init__(self):\n        for special in self.encoding.special_tokens_set:\n            special_token = self.encoding.encode_single_token(special)\n            self.special_tokens[special] = special_token\n\n        sot: int = self.special_tokens[\"<|startoftranscript|>\"]\n        translate: int = self.special_tokens[\"<|translate|>\"]\n        transcribe: int = self.special_tokens[\"<|transcribe|>\"]\n\n        langs = tuple(LANGUAGES.keys())[: self.num_languages]\n        sot_sequence = [sot]\n        if self.language is not None:\n            sot_sequence.append(sot + 1 + langs.index(self.language))\n        if self.task is not None:\n            task_token: int = transcribe if self.task == \"transcribe\" else translate\n            sot_sequence.append(task_token)\n\n        self.sot_sequence = tuple(sot_sequence)\n\n    def encode(self, text, **kwargs):\n        return self.encoding.encode(text, **kwargs)\n\n    def decode(self, token_ids: List[int], **kwargs) -> str:\n        token_ids = [t for t in token_ids if t < self.timestamp_begin]\n        return self.encoding.decode(token_ids, **kwargs)\n\n    def decode_with_timestamps(self, token_ids: List[int], **kwargs) -> str:\n        \"\"\"\n        Timestamp tokens are above other special tokens' id range and are ignored by `decode()`.\n        This method decodes given tokens with timestamps tokens annotated, e.g. \"<|1.08|>\".\n        \"\"\"\n        return self.encoding.decode(token_ids, **kwargs)\n\n    @cached_property\n    def eot(self) -> int:\n        return self.encoding.eot_token\n\n    @cached_property\n    def transcribe(self) -> int:\n        return self.special_tokens[\"<|transcribe|>\"]\n\n    @cached_property\n    def translate(self) -> int:\n        return self.special_tokens[\"<|translate|>\"]\n\n    @cached_property\n    def sot(self) -> int:\n        return self.special_tokens[\"<|startoftranscript|>\"]\n\n    @cached_property\n    def sot_lm(self) -> int:\n        return self.special_tokens[\"<|startoflm|>\"]\n\n    @cached_property\n    def sot_prev(self) -> int:\n        return self.special_tokens[\"<|startofprev|>\"]\n\n    @cached_property\n    def no_speech(self) -> int:\n        return self.special_tokens[\"<|nospeech|>\"]\n\n    @cached_property\n    def no_timestamps(self) -> int:\n        return self.special_tokens[\"<|notimestamps|>\"]\n\n    @cached_property\n    def timestamp_begin(self) -> int:\n        return self.special_tokens[\"<|0.00|>\"]\n\n    @cached_property\n    def language_token(self) -> int:\n        \"\"\"Returns the token id corresponding to the value of the `language` field\"\"\"\n        if self.language is None:\n            raise ValueError(\"This tokenizer does not have language token configured\")\n\n        return self.to_language_token(self.language)\n\n    def to_language_token(self, language):\n        if token := self.special_tokens.get(f\"<|{language}|>\", None):\n            return token\n\n        raise KeyError(f\"Language {language} not found in tokenizer.\")\n\n    @cached_property\n    def all_language_tokens(self) -> Tuple[int]:\n        result = []\n        for token, token_id in self.special_tokens.items():\n            if token.strip(\"<|>\") in LANGUAGES:\n                result.append(token_id)\n        return tuple(result)[: self.num_languages]\n\n    @cached_property\n    def all_language_codes(self) -> Tuple[str]:\n        return tuple(self.decode([_l]).strip(\"<|>\") for _l in self.all_language_tokens)\n\n    @cached_property\n    def sot_sequence_including_notimestamps(self) -> Tuple[int]:\n        return tuple(list(self.sot_sequence) + [self.no_timestamps])\n\n    @cached_property\n    def non_speech_tokens(self) -> Tuple[int]:\n        \"\"\"\n        Returns the list of tokens to suppress in order to avoid any speaker tags or non-speech\n        annotations, to prevent sampling texts that are not actually spoken in the audio, e.g.\n\n        - \u266a\u266a\u266a\n        - ( SPEAKING FOREIGN LANGUAGE )\n        - [DAVID] Hey there,\n\n        keeping basic punctuations like commas, periods, question marks, exclamation points, etc.\n        \"\"\"\n        symbols = list('\"#()*+/:;<=>@[\\\\]^_`{|}~\u300c\u300d\u300e\u300f')\n        symbols += (\n            \"<< >> <<< >>> -- --- -( -[ (' (\\\" (( )) ((( ))) [[ ]] {{ }} \u266a\u266a \u266a\u266a\u266a\".split()\n        )\n\n        # symbols that may be a single token or multiple tokens depending on the tokenizer.\n        # In case they're multiple tokens, suppress the first token, which is safe because:\n        # These are between U+2640 and U+267F miscellaneous symbols that are okay to suppress\n        # in generations, and in the 3-byte UTF-8 representation they share the first two bytes.\n        miscellaneous = set(\"\u2669\u266a\u266b\u266c\u266d\u266e\u266f\")\n        assert all(0x2640 <= ord(c) <= 0x267F for c in miscellaneous)\n\n        # allow hyphens \"-\" and single quotes \"'\" between words, but not at the beginning of a word\n        result = {self.encoding.encode(\" -\")[0], self.encoding.encode(\" '\")[0]}\n        for symbol in symbols + list(miscellaneous):\n            for tokens in [\n                self.encoding.encode(symbol),\n                self.encoding.encode(\" \" + symbol),\n            ]:\n                if len(tokens) == 1 or symbol in miscellaneous:\n                    result.add(tokens[0])\n\n        return tuple(sorted(result))\n\n    def split_to_word_tokens(self, tokens: List[int]):\n        if self.language in {\"zh\", \"ja\", \"th\", \"lo\", \"my\", \"yue\"}:\n            # These languages don't typically use spaces, so it is difficult to split words\n            # without morpheme analysis. Here, we instead split words at any\n            # position where the tokens are decoded as valid unicode points\n            return self.split_tokens_on_unicode(tokens)\n\n        return self.split_tokens_on_spaces(tokens)\n\n    def split_tokens_on_unicode(self, tokens: List[int]):\n        decoded_full = self.decode_with_timestamps(tokens)\n        replacement_char = \"\\ufffd\"\n\n        words = []\n        word_tokens = []\n        current_tokens = []\n        unicode_offset = 0\n\n        for token in tokens:\n            current_tokens.append(token)\n            decoded = self.decode_with_timestamps(current_tokens)\n\n            if (\n                replacement_char not in decoded\n                or decoded_full[unicode_offset + decoded.index(replacement_char)]\n                == replacement_char\n            ):\n                words.append(decoded)\n                word_tokens.append(current_tokens)\n                current_tokens = []\n                unicode_offset += len(decoded)\n\n        return words, word_tokens\n\n    def split_tokens_on_spaces(self, tokens: List[int]):\n        subwords, subword_tokens_list = self.split_tokens_on_unicode(tokens)\n        words = []\n        word_tokens = []\n\n        for subword, subword_tokens in zip(subwords, subword_tokens_list):\n            special = subword_tokens[0] >= self.eot\n            with_space = subword.startswith(\" \")\n            punctuation = subword.strip() in string.punctuation\n            if special or with_space or punctuation or len(words) == 0:\n                words.append(subword)\n                word_tokens.append(subword_tokens)\n            else:\n                words[-1] = words[-1] + subword\n                word_tokens[-1].extend(subword_tokens)\n\n        return words, word_tokens\n\n\n@lru_cache(maxsize=None)\ndef get_encoding(name: str = \"gpt2\", num_languages: int = 99):\n    vocab_path = os.path.join(os.path.dirname(__file__), \"assets\", f\"{name}.tiktoken\")\n    ranks = {\n        base64.b64decode(token): int(rank)\n        for token, rank in (line.split() for line in open(vocab_path) if line)\n    }\n    n_vocab = len(ranks)\n    special_tokens = {}\n\n    specials = [\n        \"<|endoftext|>\",\n        \"<|startoftranscript|>\",\n        *[f\"<|{lang}|>\" for lang in list(LANGUAGES.keys())[:num_languages]],\n        \"<|translate|>\",\n        \"<|transcribe|>\",\n        \"<|startoflm|>\",\n        \"<|startofprev|>\",\n        \"<|nospeech|>\",\n        \"<|notimestamps|>\",\n        *[f\"<|{i * 0.02:.2f}|>\" for i in range(1501)],\n    ]\n\n    for token in specials:\n        special_tokens[token] = n_vocab\n        n_vocab += 1\n\n    return tiktoken.Encoding(\n        name=os.path.basename(vocab_path),\n        explicit_n_vocab=n_vocab,\n        pat_str=r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\",\n        mergeable_ranks=ranks,\n        special_tokens=special_tokens,\n    )\n\n\n@lru_cache(maxsize=None)\ndef get_tokenizer(\n    multilingual: bool,\n    *,\n    num_languages: int = 99,\n    language: Optional[str] = None,\n    task: Optional[str] = None,  # Literal[\"transcribe\", \"translate\", None]\n) -> Tokenizer:\n    if language is not None:\n        language = language.lower()\n        if language not in LANGUAGES:\n            if language in TO_LANGUAGE_CODE:\n                language = TO_LANGUAGE_CODE[language]\n            else:\n                raise ValueError(f\"Unsupported language: {language}\")\n\n    if multilingual:\n        encoding_name = \"multilingual\"\n        language = language or \"en\"\n        task = task or \"transcribe\"\n    else:\n        encoding_name = \"gpt2\"\n        language = None\n        task = None\n\n    encoding = get_encoding(name=encoding_name, num_languages=num_languages)\n\n    return Tokenizer(\n        encoding=encoding, num_languages=num_languages, language=language, task=task\n    )\n", "whisper/__init__.py": "import hashlib\nimport io\nimport os\nimport urllib\nimport warnings\nfrom typing import List, Optional, Union\n\nimport torch\nfrom tqdm import tqdm\n\nfrom .audio import load_audio, log_mel_spectrogram, pad_or_trim\nfrom .decoding import DecodingOptions, DecodingResult, decode, detect_language\nfrom .model import ModelDimensions, Whisper\nfrom .transcribe import transcribe\nfrom .version import __version__\n\n_MODELS = {\n    \"tiny.en\": \"https://openaipublic.azureedge.net/main/whisper/models/d3dd57d32accea0b295c96e26691aa14d8822fac7d9d27d5dc00b4ca2826dd03/tiny.en.pt\",\n    \"tiny\": \"https://openaipublic.azureedge.net/main/whisper/models/65147644a518d12f04e32d6f3b26facc3f8dd46e5390956a9424a650c0ce22b9/tiny.pt\",\n    \"base.en\": \"https://openaipublic.azureedge.net/main/whisper/models/25a8566e1d0c1e2231d1c762132cd20e0f96a85d16145c3a00adf5d1ac670ead/base.en.pt\",\n    \"base\": \"https://openaipublic.azureedge.net/main/whisper/models/ed3a0b6b1c0edf879ad9b11b1af5a0e6ab5db9205f891f668f8b0e6c6326e34e/base.pt\",\n    \"small.en\": \"https://openaipublic.azureedge.net/main/whisper/models/f953ad0fd29cacd07d5a9eda5624af0f6bcf2258be67c92b79389873d91e0872/small.en.pt\",\n    \"small\": \"https://openaipublic.azureedge.net/main/whisper/models/9ecf779972d90ba49c06d968637d720dd632c55bbf19d441fb42bf17a411e794/small.pt\",\n    \"medium.en\": \"https://openaipublic.azureedge.net/main/whisper/models/d7440d1dc186f76616474e0ff0b3b6b879abc9d1a4926b7adfa41db2d497ab4f/medium.en.pt\",\n    \"medium\": \"https://openaipublic.azureedge.net/main/whisper/models/345ae4da62f9b3d59415adc60127b97c714f32e89e936602e85993674d08dcb1/medium.pt\",\n    \"large-v1\": \"https://openaipublic.azureedge.net/main/whisper/models/e4b87e7e0bf463eb8e6956e646f1e277e901512310def2c24bf0e11bd3c28e9a/large-v1.pt\",\n    \"large-v2\": \"https://openaipublic.azureedge.net/main/whisper/models/81f7c96c852ee8fc832187b0132e569d6c3065a3252ed18e56effd0b6a73e524/large-v2.pt\",\n    \"large-v3\": \"https://openaipublic.azureedge.net/main/whisper/models/e5b1a55b89c1367dacf97e3e19bfd829a01529dbfdeefa8caeb59b3f1b81dadb/large-v3.pt\",\n    \"large\": \"https://openaipublic.azureedge.net/main/whisper/models/e5b1a55b89c1367dacf97e3e19bfd829a01529dbfdeefa8caeb59b3f1b81dadb/large-v3.pt\",\n}\n\n# base85-encoded (n_layers, n_heads) boolean arrays indicating the cross-attention heads that are\n# highly correlated to the word-level timing, i.e. the alignment between audio and text tokens.\n_ALIGNMENT_HEADS = {\n    \"tiny.en\": b\"ABzY8J1N>@0{>%R00Bk>$p{7v037`oCl~+#00\",\n    \"tiny\": b\"ABzY8bu8Lr0{>%RKn9Fp%m@SkK7Kt=7ytkO\",\n    \"base.en\": b\"ABzY8;40c<0{>%RzzG;p*o+Vo09|#PsxSZm00\",\n    \"base\": b\"ABzY8KQ!870{>%RzyTQH3`Q^yNP!>##QT-<FaQ7m\",\n    \"small.en\": b\"ABzY8>?_)10{>%RpeA61k&I|OI3I$65C{;;pbCHh0B{qLQ;+}v00\",\n    \"small\": b\"ABzY8DmU6=0{>%Rpa?J`kvJ6qF(V^F86#Xh7JUGMK}P<N0000\",\n    \"medium.en\": b\"ABzY8usPae0{>%R7<zz_OvQ{)4kMa0BMw6u5rT}kRKX;$NfYBv00*Hl@qhsU00\",\n    \"medium\": b\"ABzY8B0Jh+0{>%R7}kK1fFL7w6%<-Pf*t^=N)Qr&0RR9\",\n    \"large-v1\": b\"ABzY8r9j$a0{>%R7#4sLmoOs{s)o3~84-RPdcFk!JR<kSfC2yj\",\n    \"large-v2\": b\"ABzY8zd+h!0{>%R7=D0pU<_bnWW*tkYAhobTNnu$jnkEkXqp)j;w1Tzk)UH3X%SZd&fFZ2fC2yj\",\n    \"large-v3\": b\"ABzY8gWO1E0{>%R7(9S+Kn!D~%ngiGaR?*L!iJG9p-nab0JQ=-{D1-g00\",\n    \"large\": b\"ABzY8gWO1E0{>%R7(9S+Kn!D~%ngiGaR?*L!iJG9p-nab0JQ=-{D1-g00\",\n}\n\n\ndef _download(url: str, root: str, in_memory: bool) -> Union[bytes, str]:\n    os.makedirs(root, exist_ok=True)\n\n    expected_sha256 = url.split(\"/\")[-2]\n    download_target = os.path.join(root, os.path.basename(url))\n\n    if os.path.exists(download_target) and not os.path.isfile(download_target):\n        raise RuntimeError(f\"{download_target} exists and is not a regular file\")\n\n    if os.path.isfile(download_target):\n        with open(download_target, \"rb\") as f:\n            model_bytes = f.read()\n        if hashlib.sha256(model_bytes).hexdigest() == expected_sha256:\n            return model_bytes if in_memory else download_target\n        else:\n            warnings.warn(\n                f\"{download_target} exists, but the SHA256 checksum does not match; re-downloading the file\"\n            )\n\n    with urllib.request.urlopen(url) as source, open(download_target, \"wb\") as output:\n        with tqdm(\n            total=int(source.info().get(\"Content-Length\")),\n            ncols=80,\n            unit=\"iB\",\n            unit_scale=True,\n            unit_divisor=1024,\n        ) as loop:\n            while True:\n                buffer = source.read(8192)\n                if not buffer:\n                    break\n\n                output.write(buffer)\n                loop.update(len(buffer))\n\n    model_bytes = open(download_target, \"rb\").read()\n    if hashlib.sha256(model_bytes).hexdigest() != expected_sha256:\n        raise RuntimeError(\n            \"Model has been downloaded but the SHA256 checksum does not not match. Please retry loading the model.\"\n        )\n\n    return model_bytes if in_memory else download_target\n\n\ndef available_models() -> List[str]:\n    \"\"\"Returns the names of available models\"\"\"\n    return list(_MODELS.keys())\n\n\ndef load_model(\n    name: str,\n    device: Optional[Union[str, torch.device]] = None,\n    download_root: str = None,\n    in_memory: bool = False,\n) -> Whisper:\n    \"\"\"\n    Load a Whisper ASR model\n\n    Parameters\n    ----------\n    name : str\n        one of the official model names listed by `whisper.available_models()`, or\n        path to a model checkpoint containing the model dimensions and the model state_dict.\n    device : Union[str, torch.device]\n        the PyTorch device to put the model into\n    download_root: str\n        path to download the model files; by default, it uses \"~/.cache/whisper\"\n    in_memory: bool\n        whether to preload the model weights into host memory\n\n    Returns\n    -------\n    model : Whisper\n        The Whisper ASR model instance\n    \"\"\"\n\n    if device is None:\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    if download_root is None:\n        default = os.path.join(os.path.expanduser(\"~\"), \".cache\")\n        download_root = os.path.join(os.getenv(\"XDG_CACHE_HOME\", default), \"whisper\")\n\n    if name in _MODELS:\n        checkpoint_file = _download(_MODELS[name], download_root, in_memory)\n        alignment_heads = _ALIGNMENT_HEADS[name]\n    elif os.path.isfile(name):\n        checkpoint_file = open(name, \"rb\").read() if in_memory else name\n        alignment_heads = None\n    else:\n        raise RuntimeError(\n            f\"Model {name} not found; available models = {available_models()}\"\n        )\n\n    with (\n        io.BytesIO(checkpoint_file) if in_memory else open(checkpoint_file, \"rb\")\n    ) as fp:\n        checkpoint = torch.load(fp, map_location=device)\n    del checkpoint_file\n\n    dims = ModelDimensions(**checkpoint[\"dims\"])\n    model = Whisper(dims)\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n\n    if alignment_heads is not None:\n        model.set_alignment_heads(alignment_heads)\n\n    return model.to(device)\n", "whisper/normalizers/english.py": "import json\nimport os\nimport re\nfrom fractions import Fraction\nfrom typing import Iterator, List, Match, Optional, Union\n\nfrom more_itertools import windowed\n\nfrom .basic import remove_symbols_and_diacritics\n\n\nclass EnglishNumberNormalizer:\n    \"\"\"\n    Convert any spelled-out numbers into arabic numbers, while handling:\n\n    - remove any commas\n    - keep the suffixes such as: `1960s`, `274th`, `32nd`, etc.\n    - spell out currency symbols after the number. e.g. `$20 million` -> `20000000 dollars`\n    - spell out `one` and `ones`\n    - interpret successive single-digit numbers as nominal: `one oh one` -> `101`\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n        self.zeros = {\"o\", \"oh\", \"zero\"}\n        self.ones = {\n            name: i\n            for i, name in enumerate(\n                [\n                    \"one\",\n                    \"two\",\n                    \"three\",\n                    \"four\",\n                    \"five\",\n                    \"six\",\n                    \"seven\",\n                    \"eight\",\n                    \"nine\",\n                    \"ten\",\n                    \"eleven\",\n                    \"twelve\",\n                    \"thirteen\",\n                    \"fourteen\",\n                    \"fifteen\",\n                    \"sixteen\",\n                    \"seventeen\",\n                    \"eighteen\",\n                    \"nineteen\",\n                ],\n                start=1,\n            )\n        }\n        self.ones_plural = {\n            \"sixes\" if name == \"six\" else name + \"s\": (value, \"s\")\n            for name, value in self.ones.items()\n        }\n        self.ones_ordinal = {\n            \"zeroth\": (0, \"th\"),\n            \"first\": (1, \"st\"),\n            \"second\": (2, \"nd\"),\n            \"third\": (3, \"rd\"),\n            \"fifth\": (5, \"th\"),\n            \"twelfth\": (12, \"th\"),\n            **{\n                name + (\"h\" if name.endswith(\"t\") else \"th\"): (value, \"th\")\n                for name, value in self.ones.items()\n                if value > 3 and value != 5 and value != 12\n            },\n        }\n        self.ones_suffixed = {**self.ones_plural, **self.ones_ordinal}\n\n        self.tens = {\n            \"twenty\": 20,\n            \"thirty\": 30,\n            \"forty\": 40,\n            \"fifty\": 50,\n            \"sixty\": 60,\n            \"seventy\": 70,\n            \"eighty\": 80,\n            \"ninety\": 90,\n        }\n        self.tens_plural = {\n            name.replace(\"y\", \"ies\"): (value, \"s\") for name, value in self.tens.items()\n        }\n        self.tens_ordinal = {\n            name.replace(\"y\", \"ieth\"): (value, \"th\")\n            for name, value in self.tens.items()\n        }\n        self.tens_suffixed = {**self.tens_plural, **self.tens_ordinal}\n\n        self.multipliers = {\n            \"hundred\": 100,\n            \"thousand\": 1_000,\n            \"million\": 1_000_000,\n            \"billion\": 1_000_000_000,\n            \"trillion\": 1_000_000_000_000,\n            \"quadrillion\": 1_000_000_000_000_000,\n            \"quintillion\": 1_000_000_000_000_000_000,\n            \"sextillion\": 1_000_000_000_000_000_000_000,\n            \"septillion\": 1_000_000_000_000_000_000_000_000,\n            \"octillion\": 1_000_000_000_000_000_000_000_000_000,\n            \"nonillion\": 1_000_000_000_000_000_000_000_000_000_000,\n            \"decillion\": 1_000_000_000_000_000_000_000_000_000_000_000,\n        }\n        self.multipliers_plural = {\n            name + \"s\": (value, \"s\") for name, value in self.multipliers.items()\n        }\n        self.multipliers_ordinal = {\n            name + \"th\": (value, \"th\") for name, value in self.multipliers.items()\n        }\n        self.multipliers_suffixed = {\n            **self.multipliers_plural,\n            **self.multipliers_ordinal,\n        }\n        self.decimals = {*self.ones, *self.tens, *self.zeros}\n\n        self.preceding_prefixers = {\n            \"minus\": \"-\",\n            \"negative\": \"-\",\n            \"plus\": \"+\",\n            \"positive\": \"+\",\n        }\n        self.following_prefixers = {\n            \"pound\": \"\u00a3\",\n            \"pounds\": \"\u00a3\",\n            \"euro\": \"\u20ac\",\n            \"euros\": \"\u20ac\",\n            \"dollar\": \"$\",\n            \"dollars\": \"$\",\n            \"cent\": \"\u00a2\",\n            \"cents\": \"\u00a2\",\n        }\n        self.prefixes = set(\n            list(self.preceding_prefixers.values())\n            + list(self.following_prefixers.values())\n        )\n        self.suffixers = {\n            \"per\": {\"cent\": \"%\"},\n            \"percent\": \"%\",\n        }\n        self.specials = {\"and\", \"double\", \"triple\", \"point\"}\n\n        self.words = set(\n            [\n                key\n                for mapping in [\n                    self.zeros,\n                    self.ones,\n                    self.ones_suffixed,\n                    self.tens,\n                    self.tens_suffixed,\n                    self.multipliers,\n                    self.multipliers_suffixed,\n                    self.preceding_prefixers,\n                    self.following_prefixers,\n                    self.suffixers,\n                    self.specials,\n                ]\n                for key in mapping\n            ]\n        )\n        self.literal_words = {\"one\", \"ones\"}\n\n    def process_words(self, words: List[str]) -> Iterator[str]:\n        prefix: Optional[str] = None\n        value: Optional[Union[str, int]] = None\n        skip = False\n\n        def to_fraction(s: str):\n            try:\n                return Fraction(s)\n            except ValueError:\n                return None\n\n        def output(result: Union[str, int]):\n            nonlocal prefix, value\n            result = str(result)\n            if prefix is not None:\n                result = prefix + result\n            value = None\n            prefix = None\n            return result\n\n        if len(words) == 0:\n            return\n\n        for prev, current, next in windowed([None] + words + [None], 3):\n            if skip:\n                skip = False\n                continue\n\n            next_is_numeric = next is not None and re.match(r\"^\\d+(\\.\\d+)?$\", next)\n            has_prefix = current[0] in self.prefixes\n            current_without_prefix = current[1:] if has_prefix else current\n            if re.match(r\"^\\d+(\\.\\d+)?$\", current_without_prefix):\n                # arabic numbers (potentially with signs and fractions)\n                f = to_fraction(current_without_prefix)\n                assert f is not None\n                if value is not None:\n                    if isinstance(value, str) and value.endswith(\".\"):\n                        # concatenate decimals / ip address components\n                        value = str(value) + str(current)\n                        continue\n                    else:\n                        yield output(value)\n\n                prefix = current[0] if has_prefix else prefix\n                if f.denominator == 1:\n                    value = f.numerator  # store integers as int\n                else:\n                    value = current_without_prefix\n            elif current not in self.words:\n                # non-numeric words\n                if value is not None:\n                    yield output(value)\n                yield output(current)\n            elif current in self.zeros:\n                value = str(value or \"\") + \"0\"\n            elif current in self.ones:\n                ones = self.ones[current]\n\n                if value is None:\n                    value = ones\n                elif isinstance(value, str) or prev in self.ones:\n                    if (\n                        prev in self.tens and ones < 10\n                    ):  # replace the last zero with the digit\n                        assert value[-1] == \"0\"\n                        value = value[:-1] + str(ones)\n                    else:\n                        value = str(value) + str(ones)\n                elif ones < 10:\n                    if value % 10 == 0:\n                        value += ones\n                    else:\n                        value = str(value) + str(ones)\n                else:  # eleven to nineteen\n                    if value % 100 == 0:\n                        value += ones\n                    else:\n                        value = str(value) + str(ones)\n            elif current in self.ones_suffixed:\n                # ordinal or cardinal; yield the number right away\n                ones, suffix = self.ones_suffixed[current]\n                if value is None:\n                    yield output(str(ones) + suffix)\n                elif isinstance(value, str) or prev in self.ones:\n                    if prev in self.tens and ones < 10:\n                        assert value[-1] == \"0\"\n                        yield output(value[:-1] + str(ones) + suffix)\n                    else:\n                        yield output(str(value) + str(ones) + suffix)\n                elif ones < 10:\n                    if value % 10 == 0:\n                        yield output(str(value + ones) + suffix)\n                    else:\n                        yield output(str(value) + str(ones) + suffix)\n                else:  # eleven to nineteen\n                    if value % 100 == 0:\n                        yield output(str(value + ones) + suffix)\n                    else:\n                        yield output(str(value) + str(ones) + suffix)\n                value = None\n            elif current in self.tens:\n                tens = self.tens[current]\n                if value is None:\n                    value = tens\n                elif isinstance(value, str):\n                    value = str(value) + str(tens)\n                else:\n                    if value % 100 == 0:\n                        value += tens\n                    else:\n                        value = str(value) + str(tens)\n            elif current in self.tens_suffixed:\n                # ordinal or cardinal; yield the number right away\n                tens, suffix = self.tens_suffixed[current]\n                if value is None:\n                    yield output(str(tens) + suffix)\n                elif isinstance(value, str):\n                    yield output(str(value) + str(tens) + suffix)\n                else:\n                    if value % 100 == 0:\n                        yield output(str(value + tens) + suffix)\n                    else:\n                        yield output(str(value) + str(tens) + suffix)\n            elif current in self.multipliers:\n                multiplier = self.multipliers[current]\n                if value is None:\n                    value = multiplier\n                elif isinstance(value, str) or value == 0:\n                    f = to_fraction(value)\n                    p = f * multiplier if f is not None else None\n                    if f is not None and p.denominator == 1:\n                        value = p.numerator\n                    else:\n                        yield output(value)\n                        value = multiplier\n                else:\n                    before = value // 1000 * 1000\n                    residual = value % 1000\n                    value = before + residual * multiplier\n            elif current in self.multipliers_suffixed:\n                multiplier, suffix = self.multipliers_suffixed[current]\n                if value is None:\n                    yield output(str(multiplier) + suffix)\n                elif isinstance(value, str):\n                    f = to_fraction(value)\n                    p = f * multiplier if f is not None else None\n                    if f is not None and p.denominator == 1:\n                        yield output(str(p.numerator) + suffix)\n                    else:\n                        yield output(value)\n                        yield output(str(multiplier) + suffix)\n                else:  # int\n                    before = value // 1000 * 1000\n                    residual = value % 1000\n                    value = before + residual * multiplier\n                    yield output(str(value) + suffix)\n                value = None\n            elif current in self.preceding_prefixers:\n                # apply prefix (positive, minus, etc.) if it precedes a number\n                if value is not None:\n                    yield output(value)\n\n                if next in self.words or next_is_numeric:\n                    prefix = self.preceding_prefixers[current]\n                else:\n                    yield output(current)\n            elif current in self.following_prefixers:\n                # apply prefix (dollars, cents, etc.) only after a number\n                if value is not None:\n                    prefix = self.following_prefixers[current]\n                    yield output(value)\n                else:\n                    yield output(current)\n            elif current in self.suffixers:\n                # apply suffix symbols (percent -> '%')\n                if value is not None:\n                    suffix = self.suffixers[current]\n                    if isinstance(suffix, dict):\n                        if next in suffix:\n                            yield output(str(value) + suffix[next])\n                            skip = True\n                        else:\n                            yield output(value)\n                            yield output(current)\n                    else:\n                        yield output(str(value) + suffix)\n                else:\n                    yield output(current)\n            elif current in self.specials:\n                if next not in self.words and not next_is_numeric:\n                    # apply special handling only if the next word can be numeric\n                    if value is not None:\n                        yield output(value)\n                    yield output(current)\n                elif current == \"and\":\n                    # ignore \"and\" after hundreds, thousands, etc.\n                    if prev not in self.multipliers:\n                        if value is not None:\n                            yield output(value)\n                        yield output(current)\n                elif current == \"double\" or current == \"triple\":\n                    if next in self.ones or next in self.zeros:\n                        repeats = 2 if current == \"double\" else 3\n                        ones = self.ones.get(next, 0)\n                        value = str(value or \"\") + str(ones) * repeats\n                        skip = True\n                    else:\n                        if value is not None:\n                            yield output(value)\n                        yield output(current)\n                elif current == \"point\":\n                    if next in self.decimals or next_is_numeric:\n                        value = str(value or \"\") + \".\"\n                else:\n                    # should all have been covered at this point\n                    raise ValueError(f\"Unexpected token: {current}\")\n            else:\n                # all should have been covered at this point\n                raise ValueError(f\"Unexpected token: {current}\")\n\n        if value is not None:\n            yield output(value)\n\n    def preprocess(self, s: str):\n        # replace \"<number> and a half\" with \"<number> point five\"\n        results = []\n\n        segments = re.split(r\"\\band\\s+a\\s+half\\b\", s)\n        for i, segment in enumerate(segments):\n            if len(segment.strip()) == 0:\n                continue\n            if i == len(segments) - 1:\n                results.append(segment)\n            else:\n                results.append(segment)\n                last_word = segment.rsplit(maxsplit=2)[-1]\n                if last_word in self.decimals or last_word in self.multipliers:\n                    results.append(\"point five\")\n                else:\n                    results.append(\"and a half\")\n\n        s = \" \".join(results)\n\n        # put a space at number/letter boundary\n        s = re.sub(r\"([a-z])([0-9])\", r\"\\1 \\2\", s)\n        s = re.sub(r\"([0-9])([a-z])\", r\"\\1 \\2\", s)\n\n        # but remove spaces which could be a suffix\n        s = re.sub(r\"([0-9])\\s+(st|nd|rd|th|s)\\b\", r\"\\1\\2\", s)\n\n        return s\n\n    def postprocess(self, s: str):\n        def combine_cents(m: Match):\n            try:\n                currency = m.group(1)\n                integer = m.group(2)\n                cents = int(m.group(3))\n                return f\"{currency}{integer}.{cents:02d}\"\n            except ValueError:\n                return m.string\n\n        def extract_cents(m: Match):\n            try:\n                return f\"\u00a2{int(m.group(1))}\"\n            except ValueError:\n                return m.string\n\n        # apply currency postprocessing; \"$2 and \u00a27\" -> \"$2.07\"\n        s = re.sub(r\"([\u20ac\u00a3$])([0-9]+) (?:and )?\u00a2([0-9]{1,2})\\b\", combine_cents, s)\n        s = re.sub(r\"[\u20ac\u00a3$]0.([0-9]{1,2})\\b\", extract_cents, s)\n\n        # write \"one(s)\" instead of \"1(s)\", just for the readability\n        s = re.sub(r\"\\b1(s?)\\b\", r\"one\\1\", s)\n\n        return s\n\n    def __call__(self, s: str):\n        s = self.preprocess(s)\n        s = \" \".join(word for word in self.process_words(s.split()) if word is not None)\n        s = self.postprocess(s)\n\n        return s\n\n\nclass EnglishSpellingNormalizer:\n    \"\"\"\n    Applies British-American spelling mappings as listed in [1].\n\n    [1] https://www.tysto.com/uk-us-spelling-list.html\n    \"\"\"\n\n    def __init__(self):\n        mapping_path = os.path.join(os.path.dirname(__file__), \"english.json\")\n        self.mapping = json.load(open(mapping_path))\n\n    def __call__(self, s: str):\n        return \" \".join(self.mapping.get(word, word) for word in s.split())\n\n\nclass EnglishTextNormalizer:\n    def __init__(self):\n        self.ignore_patterns = r\"\\b(hmm|mm|mhm|mmm|uh|um)\\b\"\n        self.replacers = {\n            # common contractions\n            r\"\\bwon't\\b\": \"will not\",\n            r\"\\bcan't\\b\": \"can not\",\n            r\"\\blet's\\b\": \"let us\",\n            r\"\\bain't\\b\": \"aint\",\n            r\"\\by'all\\b\": \"you all\",\n            r\"\\bwanna\\b\": \"want to\",\n            r\"\\bgotta\\b\": \"got to\",\n            r\"\\bgonna\\b\": \"going to\",\n            r\"\\bi'ma\\b\": \"i am going to\",\n            r\"\\bimma\\b\": \"i am going to\",\n            r\"\\bwoulda\\b\": \"would have\",\n            r\"\\bcoulda\\b\": \"could have\",\n            r\"\\bshoulda\\b\": \"should have\",\n            r\"\\bma'am\\b\": \"madam\",\n            # contractions in titles/prefixes\n            r\"\\bmr\\b\": \"mister \",\n            r\"\\bmrs\\b\": \"missus \",\n            r\"\\bst\\b\": \"saint \",\n            r\"\\bdr\\b\": \"doctor \",\n            r\"\\bprof\\b\": \"professor \",\n            r\"\\bcapt\\b\": \"captain \",\n            r\"\\bgov\\b\": \"governor \",\n            r\"\\bald\\b\": \"alderman \",\n            r\"\\bgen\\b\": \"general \",\n            r\"\\bsen\\b\": \"senator \",\n            r\"\\brep\\b\": \"representative \",\n            r\"\\bpres\\b\": \"president \",\n            r\"\\brev\\b\": \"reverend \",\n            r\"\\bhon\\b\": \"honorable \",\n            r\"\\basst\\b\": \"assistant \",\n            r\"\\bassoc\\b\": \"associate \",\n            r\"\\blt\\b\": \"lieutenant \",\n            r\"\\bcol\\b\": \"colonel \",\n            r\"\\bjr\\b\": \"junior \",\n            r\"\\bsr\\b\": \"senior \",\n            r\"\\besq\\b\": \"esquire \",\n            # prefect tenses, ideally it should be any past participles, but it's harder..\n            r\"'d been\\b\": \" had been\",\n            r\"'s been\\b\": \" has been\",\n            r\"'d gone\\b\": \" had gone\",\n            r\"'s gone\\b\": \" has gone\",\n            r\"'d done\\b\": \" had done\",  # \"'s done\" is ambiguous\n            r\"'s got\\b\": \" has got\",\n            # general contractions\n            r\"n't\\b\": \" not\",\n            r\"'re\\b\": \" are\",\n            r\"'s\\b\": \" is\",\n            r\"'d\\b\": \" would\",\n            r\"'ll\\b\": \" will\",\n            r\"'t\\b\": \" not\",\n            r\"'ve\\b\": \" have\",\n            r\"'m\\b\": \" am\",\n        }\n        self.standardize_numbers = EnglishNumberNormalizer()\n        self.standardize_spellings = EnglishSpellingNormalizer()\n\n    def __call__(self, s: str):\n        s = s.lower()\n\n        s = re.sub(r\"[<\\[][^>\\]]*[>\\]]\", \"\", s)  # remove words between brackets\n        s = re.sub(r\"\\(([^)]+?)\\)\", \"\", s)  # remove words between parenthesis\n        s = re.sub(self.ignore_patterns, \"\", s)\n        s = re.sub(r\"\\s+'\", \"'\", s)  # when there's a space before an apostrophe\n\n        for pattern, replacement in self.replacers.items():\n            s = re.sub(pattern, replacement, s)\n\n        s = re.sub(r\"(\\d),(\\d)\", r\"\\1\\2\", s)  # remove commas between digits\n        s = re.sub(r\"\\.([^0-9]|$)\", r\" \\1\", s)  # remove periods not followed by numbers\n        s = remove_symbols_and_diacritics(s, keep=\".%$\u00a2\u20ac\u00a3\")  # keep numeric symbols\n\n        s = self.standardize_numbers(s)\n        s = self.standardize_spellings(s)\n\n        # now remove prefix/suffix symbols that are not preceded/followed by numbers\n        s = re.sub(r\"[.$\u00a2\u20ac\u00a3]([^0-9])\", r\" \\1\", s)\n        s = re.sub(r\"([^0-9])%\", r\"\\1 \", s)\n\n        s = re.sub(r\"\\s+\", \" \", s)  # replace any successive whitespaces with a space\n\n        return s\n", "whisper/normalizers/basic.py": "import re\nimport unicodedata\n\nimport regex\n\n# non-ASCII letters that are not separated by \"NFKD\" normalization\nADDITIONAL_DIACRITICS = {\n    \"\u0153\": \"oe\",\n    \"\u0152\": \"OE\",\n    \"\u00f8\": \"o\",\n    \"\u00d8\": \"O\",\n    \"\u00e6\": \"ae\",\n    \"\u00c6\": \"AE\",\n    \"\u00df\": \"ss\",\n    \"\u1e9e\": \"SS\",\n    \"\u0111\": \"d\",\n    \"\u0110\": \"D\",\n    \"\u00f0\": \"d\",\n    \"\u00d0\": \"D\",\n    \"\u00fe\": \"th\",\n    \"\u00de\": \"th\",\n    \"\u0142\": \"l\",\n    \"\u0141\": \"L\",\n}\n\n\ndef remove_symbols_and_diacritics(s: str, keep=\"\"):\n    \"\"\"\n    Replace any other markers, symbols, and punctuations with a space,\n    and drop any diacritics (category 'Mn' and some manual mappings)\n    \"\"\"\n    return \"\".join(\n        c\n        if c in keep\n        else ADDITIONAL_DIACRITICS[c]\n        if c in ADDITIONAL_DIACRITICS\n        else \"\"\n        if unicodedata.category(c) == \"Mn\"\n        else \" \"\n        if unicodedata.category(c)[0] in \"MSP\"\n        else c\n        for c in unicodedata.normalize(\"NFKD\", s)\n    )\n\n\ndef remove_symbols(s: str):\n    \"\"\"\n    Replace any other markers, symbols, punctuations with a space, keeping diacritics\n    \"\"\"\n    return \"\".join(\n        \" \" if unicodedata.category(c)[0] in \"MSP\" else c\n        for c in unicodedata.normalize(\"NFKC\", s)\n    )\n\n\nclass BasicTextNormalizer:\n    def __init__(self, remove_diacritics: bool = False, split_letters: bool = False):\n        self.clean = (\n            remove_symbols_and_diacritics if remove_diacritics else remove_symbols\n        )\n        self.split_letters = split_letters\n\n    def __call__(self, s: str):\n        s = s.lower()\n        s = re.sub(r\"[<\\[][^>\\]]*[>\\]]\", \"\", s)  # remove words between brackets\n        s = re.sub(r\"\\(([^)]+?)\\)\", \"\", s)  # remove words between parenthesis\n        s = self.clean(s).lower()\n\n        if self.split_letters:\n            s = \" \".join(regex.findall(r\"\\X\", s, regex.U))\n\n        s = re.sub(\n            r\"\\s+\", \" \", s\n        )  # replace any successive whitespace characters with a space\n\n        return s\n", "whisper/normalizers/__init__.py": "from .basic import BasicTextNormalizer as BasicTextNormalizer\nfrom .english import EnglishTextNormalizer as EnglishTextNormalizer\n"}