{"setup.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\"\"\"\nDeepSpeed library\n\nTo build wheel on Windows:\n1. Install pytorch, such as pytorch 1.12 + cuda 11.6.\n2. Install visual cpp build tool.\n3. Include cuda toolkit.\n4. Launch cmd console with Administrator privilege for creating required symlink folders.\n\n\nCreate a new wheel via the following command:\nbuild_win.bat\n\nThe wheel will be located at: dist/*.whl\n\"\"\"\n\nimport pathlib\nimport os\nimport shutil\nimport sys\nimport subprocess\nfrom setuptools import setup, find_packages\nfrom setuptools.command import egg_info\nimport time\nimport typing\n\ntorch_available = True\ntry:\n    import torch\nexcept ImportError:\n    torch_available = False\n    print('[WARNING] Unable to import torch, pre-compiling ops will be disabled. ' \\\n        'Please visit https://pytorch.org/ to see how to properly install torch on your system.')\n\nfrom op_builder import get_default_compute_capabilities, OpBuilder\nfrom op_builder.all_ops import ALL_OPS, accelerator_name\nfrom op_builder.builder import installed_cuda_version\n\n# Fetch rocm state.\nis_rocm_pytorch = OpBuilder.is_rocm_pytorch()\nrocm_version = OpBuilder.installed_rocm_version()\n\nRED_START = '\\033[31m'\nRED_END = '\\033[0m'\nERROR = f\"{RED_START} [ERROR] {RED_END}\"\n\n\ndef abort(msg):\n    print(f\"{ERROR} {msg}\")\n    assert False, msg\n\n\ndef fetch_requirements(path):\n    with open(path, 'r') as fd:\n        return [r.strip() for r in fd.readlines()]\n\n\ndef is_env_set(key):\n    \"\"\"\n    Checks if an environment variable is set and not \"\".\n    \"\"\"\n    return bool(os.environ.get(key, None))\n\n\ndef get_env_if_set(key, default: typing.Any = \"\"):\n    \"\"\"\n    Returns an environment variable if it is set and not \"\",\n    otherwise returns a default value. In contrast, the fallback\n    parameter of os.environ.get() is skipped if the variable is set to \"\".\n    \"\"\"\n    return os.environ.get(key, None) or default\n\n\ninstall_requires = fetch_requirements('requirements/requirements.txt')\nextras_require = {\n    '1bit': [],  # add cupy based on cuda/rocm version\n    '1bit_mpi': fetch_requirements('requirements/requirements-1bit-mpi.txt'),\n    'readthedocs': fetch_requirements('requirements/requirements-readthedocs.txt'),\n    'dev': fetch_requirements('requirements/requirements-dev.txt'),\n    'autotuning': fetch_requirements('requirements/requirements-autotuning.txt'),\n    'autotuning_ml': fetch_requirements('requirements/requirements-autotuning-ml.txt'),\n    'sparse_attn': fetch_requirements('requirements/requirements-sparse_attn.txt'),\n    'sparse': fetch_requirements('requirements/requirements-sparse_pruning.txt'),\n    'inf': fetch_requirements('requirements/requirements-inf.txt'),\n    'sd': fetch_requirements('requirements/requirements-sd.txt'),\n    'triton': fetch_requirements('requirements/requirements-triton.txt'),\n}\n\n# Add specific cupy version to both onebit extension variants.\nif torch_available and torch.cuda.is_available():\n    cupy = None\n    if is_rocm_pytorch:\n        rocm_major, rocm_minor = rocm_version\n        # XXX cupy support for rocm 5 is not available yet.\n        if rocm_major <= 4:\n            cupy = f\"cupy-rocm-{rocm_major}-{rocm_minor}\"\n    else:\n        cuda_major_ver, cuda_minor_ver = installed_cuda_version()\n        if (cuda_major_ver < 11) or ((cuda_major_ver == 11) and (cuda_minor_ver < 3)):\n            cupy = f\"cupy-cuda{cuda_major_ver}{cuda_minor_ver}\"\n        else:\n            cupy = f\"cupy-cuda{cuda_major_ver}x\"\n\n    if cupy:\n        extras_require['1bit'].append(cupy)\n        extras_require['1bit_mpi'].append(cupy)\n\n# Make an [all] extra that installs all needed dependencies.\nall_extras = set()\nfor extra in extras_require.items():\n    for req in extra[1]:\n        all_extras.add(req)\nextras_require['all'] = list(all_extras)\n\ncmdclass = {}\n\n# For any pre-installed ops force disable ninja.\nif torch_available:\n    from accelerator import get_accelerator\n    use_ninja = is_env_set(\"DS_ENABLE_NINJA\")\n    cmdclass['build_ext'] = get_accelerator().build_extension().with_options(use_ninja=use_ninja)\n\nif torch_available:\n    TORCH_MAJOR = torch.__version__.split('.')[0]\n    TORCH_MINOR = torch.__version__.split('.')[1]\nelse:\n    TORCH_MAJOR = \"0\"\n    TORCH_MINOR = \"0\"\n\nif torch_available and not torch.cuda.is_available():\n    # Fix to allow docker builds, similar to https://github.com/NVIDIA/apex/issues/486.\n    print(\"[WARNING] Torch did not find cuda available, if cross-compiling or running with cpu only \"\n          \"you can ignore this message. Adding compute capability for Pascal, Volta, and Turing \"\n          \"(compute capabilities 6.0, 6.1, 6.2)\")\n    if not is_env_set(\"TORCH_CUDA_ARCH_LIST\"):\n        os.environ[\"TORCH_CUDA_ARCH_LIST\"] = get_default_compute_capabilities()\n\next_modules = []\n\n# Default to pre-install kernels to false so we rely on JIT on Linux, opposite on Windows.\nBUILD_OP_PLATFORM = 1 if sys.platform == \"win32\" else 0\nBUILD_OP_DEFAULT = int(get_env_if_set('DS_BUILD_OPS', BUILD_OP_PLATFORM))\nprint(f\"DS_BUILD_OPS={BUILD_OP_DEFAULT}\")\n\nif BUILD_OP_DEFAULT:\n    assert torch_available, \"Unable to pre-compile ops without torch installed. Please install torch before attempting to pre-compile ops.\"\n\n\ndef command_exists(cmd):\n    if sys.platform == \"win32\":\n        result = subprocess.Popen(f'{cmd}', stdout=subprocess.PIPE, shell=True)\n        return result.wait() == 1\n    else:\n        result = subprocess.Popen(f'type {cmd}', stdout=subprocess.PIPE, shell=True)\n        return result.wait() == 0\n\n\ndef op_envvar(op_name):\n    assert hasattr(ALL_OPS[op_name], 'BUILD_VAR'), \\\n        f\"{op_name} is missing BUILD_VAR field\"\n    return ALL_OPS[op_name].BUILD_VAR\n\n\ndef op_enabled(op_name):\n    env_var = op_envvar(op_name)\n    return int(get_env_if_set(env_var, BUILD_OP_DEFAULT))\n\n\ninstall_ops = dict.fromkeys(ALL_OPS.keys(), False)\nfor op_name, builder in ALL_OPS.items():\n    op_compatible = builder.is_compatible()\n\n    # If op is requested but not available, throw an error.\n    if op_enabled(op_name) and not op_compatible:\n        env_var = op_envvar(op_name)\n        if not is_env_set(env_var):\n            builder.warning(f\"One can disable {op_name} with {env_var}=0\")\n        abort(f\"Unable to pre-compile {op_name}\")\n\n    # If op is compatible but install is not enabled (JIT mode).\n    if is_rocm_pytorch and op_compatible and not op_enabled(op_name):\n        builder.hipify_extension()\n\n    # If op install enabled, add builder to extensions.\n    if op_enabled(op_name) and op_compatible:\n        assert torch_available, f\"Unable to pre-compile {op_name}, please first install torch\"\n        install_ops[op_name] = op_enabled(op_name)\n        ext_modules.append(builder.builder())\n\nprint(f'Install Ops={install_ops}')\n\n# Write out version/git info.\ngit_hash_cmd = \"git rev-parse --short HEAD\"\ngit_branch_cmd = \"git rev-parse --abbrev-ref HEAD\"\nif command_exists('git') and not is_env_set('DS_BUILD_STRING'):\n    try:\n        result = subprocess.check_output(git_hash_cmd, shell=True)\n        git_hash = result.decode('utf-8').strip()\n        result = subprocess.check_output(git_branch_cmd, shell=True)\n        git_branch = result.decode('utf-8').strip()\n    except subprocess.CalledProcessError:\n        git_hash = \"unknown\"\n        git_branch = \"unknown\"\nelse:\n    git_hash = \"unknown\"\n    git_branch = \"unknown\"\n\nif sys.platform == \"win32\":\n    shutil.rmtree('.\\\\deepspeed\\\\ops\\\\csrc', ignore_errors=True)\n    pathlib.Path('.\\\\deepspeed\\\\ops\\\\csrc').unlink(missing_ok=True)\n    shutil.copytree('.\\\\csrc', '.\\\\deepspeed\\\\ops\\\\csrc', dirs_exist_ok=True)\n    shutil.rmtree('.\\\\deepspeed\\\\ops\\\\op_builder', ignore_errors=True)\n    pathlib.Path('.\\\\deepspeed\\\\ops\\\\op_builder').unlink(missing_ok=True)\n    shutil.copytree('.\\\\op_builder', '.\\\\deepspeed\\\\ops\\\\op_builder', dirs_exist_ok=True)\n    shutil.rmtree('.\\\\deepspeed\\\\accelerator', ignore_errors=True)\n    pathlib.Path('.\\\\deepspeed\\\\accelerator').unlink(missing_ok=True)\n    shutil.copytree('.\\\\accelerator', '.\\\\deepspeed\\\\accelerator', dirs_exist_ok=True)\n    egg_info.manifest_maker.template = 'MANIFEST_win.in'\n\n# Parse the DeepSpeed version string from version.txt.\nversion_str = open('version.txt', 'r').read().strip()\n\n# Build specifiers like .devX can be added at install time. Otherwise, add the git hash.\n# Example: DS_BUILD_STRING=\".dev20201022\" python setup.py sdist bdist_wheel.\n\n# Building wheel for distribution, update version file.\nif is_env_set('DS_BUILD_STRING'):\n    # Build string env specified, probably building for distribution.\n    with open('build.txt', 'w') as fd:\n        fd.write(os.environ['DS_BUILD_STRING'])\n    version_str += os.environ['DS_BUILD_STRING']\nelif os.path.isfile('build.txt'):\n    # build.txt exists, probably installing from distribution.\n    with open('build.txt', 'r') as fd:\n        version_str += fd.read().strip()\nelse:\n    # None of the above, probably installing from source.\n    version_str += f'+{git_hash}'\n\ntorch_version = \".\".join([TORCH_MAJOR, TORCH_MINOR])\nbf16_support = False\n# Set cuda_version to 0.0 if cpu-only.\ncuda_version = \"0.0\"\nnccl_version = \"0.0\"\n# Set hip_version to 0.0 if cpu-only.\nhip_version = \"0.0\"\nif torch_available and torch.version.cuda is not None:\n    cuda_version = \".\".join(torch.version.cuda.split('.')[:2])\n    if sys.platform != \"win32\":\n        if isinstance(torch.cuda.nccl.version(), int):\n            # This will break if minor version > 9.\n            nccl_version = \".\".join(str(torch.cuda.nccl.version())[:2])\n        else:\n            nccl_version = \".\".join(map(str, torch.cuda.nccl.version()[:2]))\n    if hasattr(torch.cuda, 'is_bf16_supported') and torch.cuda.is_available():\n        bf16_support = torch.cuda.is_bf16_supported()\nif torch_available and hasattr(torch.version, 'hip') and torch.version.hip is not None:\n    hip_version = \".\".join(torch.version.hip.split('.')[:2])\ntorch_info = {\n    \"version\": torch_version,\n    \"bf16_support\": bf16_support,\n    \"cuda_version\": cuda_version,\n    \"nccl_version\": nccl_version,\n    \"hip_version\": hip_version\n}\n\nprint(f\"version={version_str}, git_hash={git_hash}, git_branch={git_branch}\")\nwith open('deepspeed/git_version_info_installed.py', 'w') as fd:\n    fd.write(f\"version='{version_str}'\\n\")\n    fd.write(f\"git_hash='{git_hash}'\\n\")\n    fd.write(f\"git_branch='{git_branch}'\\n\")\n    fd.write(f\"installed_ops={install_ops}\\n\")\n    fd.write(f\"accelerator_name='{accelerator_name}'\\n\")\n    fd.write(f\"torch_info={torch_info}\\n\")\n\nprint(f'install_requires={install_requires}')\nprint(f'ext_modules={ext_modules}')\n\n# Parse README.md to make long_description for PyPI page.\nthisdir = os.path.abspath(os.path.dirname(__file__))\nwith open(os.path.join(thisdir, 'README.md'), encoding='utf-8') as fin:\n    readme_text = fin.read()\n\nstart_time = time.time()\n\nsetup(name='deepspeed',\n      version=version_str,\n      description='DeepSpeed library',\n      long_description=readme_text,\n      long_description_content_type='text/markdown',\n      author='DeepSpeed Team',\n      author_email='deepspeed-info@microsoft.com',\n      url='http://deepspeed.ai',\n      project_urls={\n          'Documentation': 'https://deepspeed.readthedocs.io',\n          'Source': 'https://github.com/microsoft/DeepSpeed',\n      },\n      install_requires=install_requires,\n      extras_require=extras_require,\n      packages=find_packages(include=['deepspeed', 'deepspeed.*']),\n      include_package_data=True,\n      scripts=[\n          'bin/deepspeed', 'bin/deepspeed.pt', 'bin/ds', 'bin/ds_ssh', 'bin/ds_report', 'bin/ds_bench', 'bin/dsr',\n          'bin/ds_elastic'\n      ],\n      classifiers=[\n          'Programming Language :: Python :: 3.6', 'Programming Language :: Python :: 3.7',\n          'Programming Language :: Python :: 3.8', 'Programming Language :: Python :: 3.9',\n          'Programming Language :: Python :: 3.10'\n      ],\n      license='Apache Software License 2.0',\n      ext_modules=ext_modules,\n      cmdclass=cmdclass)\n\nend_time = time.time()\nprint(f'deepspeed build time = {end_time - start_time} secs')\n", "accelerator/cuda_accelerator.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport functools\nimport os\nimport pkgutil\nimport importlib\nimport sys\n\nfrom .abstract_accelerator import DeepSpeedAccelerator\n# During setup stage torch may not be installed, pass on no torch will\n# allow op builder related API to be executed.\ntry:\n    import torch.cuda\nexcept ImportError:\n    pass\n\n# Delay import pynvml to avoid import error when CUDA is not available\npynvml = None\n\n\nclass CUDA_Accelerator(DeepSpeedAccelerator):\n\n    def __init__(self):\n        self._name = 'cuda'\n        self._communication_backend_name = 'nccl' if sys.platform != 'win32' else 'gloo'\n        self._compile_backend = \"inductor\"\n        if pynvml is None:\n            self._init_pynvml()\n\n    def _init_pynvml(self):\n        global pynvml\n        try:\n            import pynvml\n        except ImportError:\n            return\n        try:\n            pynvml.nvmlInit()\n        except pynvml.NVMLError:\n            pynvml = None\n            return\n\n    def is_synchronized_device(self):\n        return False\n\n    def use_host_timers(self):\n        return self.is_synchronized_device()\n\n    def resolves_data_dependency(self):\n        return self.is_synchronized_device()\n\n    def handles_memory_backpressure(self):\n        return self.is_synchronized_device()\n\n    # Device APIs\n    def device_name(self, device_index=None):\n        if device_index is None:\n            return 'cuda'\n        return 'cuda:{}'.format(device_index)\n\n    def device(self, device_index=None):\n        return torch.cuda.device(device_index)\n\n    def set_device(self, device_index):\n        torch.cuda.set_device(device_index)\n\n    def current_device(self):\n        return torch.cuda.current_device()\n\n    def current_device_name(self):\n        return 'cuda:{}'.format(torch.cuda.current_device())\n\n    def device_count(self):\n        return torch.cuda.device_count()\n\n    def synchronize(self, device_index=None):\n        return torch.cuda.synchronize(device_index)\n\n    # RNG APIs\n    def random(self):\n        return torch.random\n\n    def set_rng_state(self, new_state, device_index=None):\n        if device_index is None:\n            return torch.cuda.set_rng_state(new_state)\n\n        return torch.cuda.set_rng_state(new_state, device_index)\n\n    def get_rng_state(self, device_index=None):\n        if device_index is None:\n            return torch.cuda.get_rng_state()\n\n        return torch.cuda.get_rng_state(device_index)\n\n    def manual_seed(self, seed):\n        return torch.cuda.manual_seed(seed)\n\n    def manual_seed_all(self, seed):\n        return torch.cuda.manual_seed_all(seed)\n\n    def initial_seed(self):\n        return torch.cuda.initial_seed()\n\n    def default_generator(self, device_index):\n        return torch.cuda.default_generators[device_index]\n\n    # Streams/Events\n    @property\n    def Stream(self):\n        return torch.cuda.Stream\n\n    def stream(self, stream):\n        return torch.cuda.stream(stream)\n\n    def current_stream(self, device_index=None):\n        return torch.cuda.current_stream(device_index)\n\n    def default_stream(self, device_index=None):\n        return torch.cuda.default_stream(device_index)\n\n    @property\n    def Event(self):\n        return torch.cuda.Event\n\n    # Memory management\n    def empty_cache(self):\n        return torch.cuda.empty_cache()\n\n    def memory_allocated(self, device_index=None):\n        return torch.cuda.memory_allocated(device_index)\n\n    def max_memory_allocated(self, device_index=None):\n        return torch.cuda.max_memory_allocated(device_index)\n\n    def reset_max_memory_allocated(self, device_index=None):\n        return torch.cuda.reset_max_memory_allocated(device_index)\n\n    def memory_cached(self, device_index=None):\n        return torch.cuda.memory_cached(device_index)\n\n    def max_memory_cached(self, device_index=None):\n        return torch.cuda.max_memory_cached(device_index)\n\n    def reset_max_memory_cached(self, device_index=None):\n        return torch.cuda.reset_max_memory_cached(device_index)\n\n    def memory_stats(self, device_index=None):\n        if hasattr(torch.cuda, 'memory_stats'):\n            return torch.cuda.memory_stats(device_index)\n\n    def reset_peak_memory_stats(self, device_index=None):\n        if hasattr(torch.cuda, 'reset_peak_memory_stats'):\n            return torch.cuda.reset_peak_memory_stats(device_index)\n\n    def memory_reserved(self, device_index=None):\n        if hasattr(torch.cuda, 'memory_reserved'):\n            return torch.cuda.memory_reserved(device_index)\n\n    def max_memory_reserved(self, device_index=None):\n        if hasattr(torch.cuda, 'max_memory_reserved'):\n            return torch.cuda.max_memory_reserved(device_index)\n\n    def total_memory(self, device_index=None):\n        return torch.cuda.get_device_properties(device_index).total_memory\n\n    def _get_nvml_gpu_id(self, torch_gpu_id):\n        \"\"\"\n        credit: https://discuss.pytorch.org/t/making-pynvml-match-torch-device-ids-cuda-visible-devices/103020\n\n        Remap torch device id to nvml device id, respecting CUDA_VISIBLE_DEVICES.\n\n        If the latter isn't set return the same id\n        \"\"\"\n        # if CUDA_VISIBLE_DEVICES is used automagically remap the id since pynvml ignores this env var\n        if \"CUDA_VISIBLE_DEVICES\" in os.environ:\n            ids = list(map(int, os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"\").split(\",\")))\n            return ids[torch_gpu_id]  # remap\n        else:\n            return torch_gpu_id\n\n    def available_memory(self, device_index=None):\n        if pynvml:\n            if device_index is None:\n                device_index = self.current_device()\n            handle = pynvml.nvmlDeviceGetHandleByIndex(self._get_nvml_gpu_id(device_index))\n            info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n            return info.free\n        else:\n            return self.total_memory(device_index) - self.memory_allocated(device_index)\n\n    # Data types\n    def is_bf16_supported(self):\n        if not torch.cuda.is_available():\n            return True\n        return torch.cuda.is_bf16_supported()\n\n    def is_fp16_supported(self):\n        if not torch.cuda.is_available():\n            return True\n        # See https://docs.nvidia.com/deeplearning/tensorrt/support-matrix/index.html#hardware-precision-matrix\n        # FP16 on compute capability 6.x is deprecated\n        allow_deprecated_fp16 = os.environ.get('DS_ALLOW_DEPRECATED_FP16', '0') == '1'\n        major, _ = torch.cuda.get_device_capability()\n        if major >= 7:\n            return True\n        elif major == 6 and allow_deprecated_fp16:\n            return True\n        else:\n            return False\n\n    def supported_dtypes(self):\n        supported_dtypes = [torch.float]\n        if self.is_fp16_supported():\n            supported_dtypes.append(torch.half)\n        if self.is_bf16_supported():\n            supported_dtypes.append(torch.bfloat16)\n        return supported_dtypes\n\n    # Misc\n    def amp(self):\n        if hasattr(torch.cuda, 'amp'):\n            return torch.cuda.amp\n        return None\n\n    def is_available(self):\n        return torch.cuda.is_available()\n\n    def range_push(self, msg):\n        if hasattr(torch.cuda.nvtx, 'range_push'):\n            return torch.cuda.nvtx.range_push(msg)\n\n    def range_pop(self):\n        if hasattr(torch.cuda.nvtx, 'range_pop'):\n            return torch.cuda.nvtx.range_pop()\n\n    def lazy_call(self, callback):\n        return torch.cuda._lazy_call(callback)\n\n    def communication_backend_name(self):\n        return self._communication_backend_name\n\n    def is_triton_supported(self):\n        major, _ = torch.cuda.get_device_capability()\n        if major >= 8:\n            return True\n        else:\n            return False\n\n    # Graph operations\n    def create_graph(self):\n        return torch.cuda.CUDAGraph()\n\n    def capture_to_graph(self, graph, pool=None, stream=None):\n        return torch.cuda.graph(graph, pool, stream)\n\n    def replay_graph(self, graph):\n        graph.replay()\n        return\n\n    # Tensor operations\n\n    @property\n    def BFloat16Tensor(self):\n        return functools.partial(torch.tensor, dtype=torch.bfloat16, device='cuda')\n\n    @property\n    def ByteTensor(self):\n        return functools.partial(torch.tensor, dtype=torch.uint8, device='cuda')\n\n    @property\n    def DoubleTensor(self):\n        return functools.partial(torch.tensor, dtype=torch.double, device='cuda')\n\n    @property\n    def FloatTensor(self):\n        return functools.partial(torch.tensor, dtype=torch.float, device='cuda')\n\n    @property\n    def HalfTensor(self):\n        return functools.partial(torch.tensor, dtype=torch.half, device='cuda')\n\n    @property\n    def IntTensor(self):\n        return functools.partial(torch.tensor, dtype=torch.int, device='cuda')\n\n    @property\n    def LongTensor(self):\n        return functools.partial(torch.tensor, dtype=torch.long, device='cuda')\n\n    def pin_memory(self, tensor, align_bytes=1):\n        return tensor.pin_memory()\n\n    def is_pinned(self, tensor):\n        return tensor.is_pinned()\n\n    def on_accelerator(self, tensor):\n        device_str = str(tensor.device)\n        if device_str.startswith('cuda:'):\n            return True\n        else:\n            return False\n\n    def op_builder_dir(self):\n        try:\n            # is op_builder from deepspeed or a 3p version? this should only succeed if it's deepspeed\n            # if successful this also means we're doing a local install and not JIT compile path\n            from op_builder import __deepspeed__  # noqa: F401 # type: ignore\n            return \"op_builder\"\n        except ImportError:\n            return \"deepspeed.ops.op_builder\"\n\n    # dict that holds class name <--> class type mapping i.e.\n    # 'AsyncIOBuilder': <class 'op_builder.async_io.AsyncIOBuilder'>\n    # this dict will be filled at init stage\n    class_dict = None\n\n    def _lazy_init_class_dict(self):\n        if self.class_dict is not None:\n            return\n        else:\n            self.class_dict = {}\n            # begin initialize for create_op_builder()\n            # put all valid class name <--> class type mapping into class_dict\n            op_builder_dir = self.op_builder_dir()\n            op_builder_module = importlib.import_module(op_builder_dir)\n            op_builder_absolute_path = os.path.dirname(op_builder_module.__file__)\n            for _, module_name, _ in pkgutil.iter_modules([op_builder_absolute_path]):\n                # avoid self references,\n                # skip sub_directories which contains ops for other backend(cpu, npu, etc.).\n                if module_name != 'all_ops' and module_name != 'builder' and not os.path.isdir(\n                        os.path.join(op_builder_absolute_path, module_name)):\n                    module = importlib.import_module(\"{}.{}\".format(op_builder_dir, module_name))\n                    for member_name in module.__dir__():\n                        if member_name.endswith(\n                                'Builder'\n                        ) and member_name != \"OpBuilder\" and member_name != \"CUDAOpBuilder\" and member_name != \"TorchCPUOpBuilder\":  # avoid abstract classes\n                            if not member_name in self.class_dict:\n                                self.class_dict[member_name] = getattr(module, member_name)\n            # end initialize for create_op_builder()\n\n    # create an instance of op builder and return, name specified by class_name\n    def create_op_builder(self, class_name):\n        self._lazy_init_class_dict()\n        if class_name in self.class_dict:\n            return self.class_dict[class_name]()\n        else:\n            return None\n\n    # return an op builder class, name specified by class_name\n    def get_op_builder(self, class_name):\n        self._lazy_init_class_dict()\n        if class_name in self.class_dict:\n            return self.class_dict[class_name]\n        else:\n            return None\n\n    def build_extension(self):\n        from torch.utils.cpp_extension import BuildExtension\n        return BuildExtension\n\n    def export_envs(self):\n        return ['NCCL']\n\n    def visible_devices_envs(self):\n        return ['CUDA_VISIBLE_DEVICES']\n\n    def set_visible_devices_envs(self, current_env, local_accelerator_ids):\n        for env in self.visible_devices_envs():\n            current_env[env] = \",\".join(map(str, local_accelerator_ids))\n\n    def get_compile_backend(self):\n        return self._compile_backend\n\n    def set_compile_backend(self, backend):\n        supported_backends = torch._dynamo.list_backends(exclude_tags=())\n        if backend in supported_backends:\n            self._compile_backend = backend\n        else:\n            raise ValueError(\n                f\"{backend} not supported by {self.device_name()}. Supported Backends are {supported_backends}\")\n", "accelerator/xpu_accelerator.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nfrom deepspeed.accelerator.abstract_accelerator import DeepSpeedAccelerator\nimport intel_extension_for_pytorch as ipex  # noqa: F401 # type: ignore\nimport oneccl_bindings_for_pytorch  # noqa: F401 # type: ignore\nimport functools\n\nimport importlib\nimport inspect\n\n\nclass XPU_Accelerator(DeepSpeedAccelerator):\n\n    def __init__(self):\n        self._name = 'xpu'\n        self._communication_backend_name = 'ccl'\n        self._compile_backend = \"inductor\"\n        self.aligned_tensors = []\n        self.class_dict = None\n\n    def is_synchronized_device(self):\n        return False\n\n    def use_host_timers(self):\n        return self.is_synchronized_device()\n\n    def resolves_data_dependency(self):\n        return self.is_synchronized_device()\n\n    def handles_memory_backpressure(self):\n        return self.is_synchronized_device()\n\n    # Device APIs\n    def device_name(self, device_index=None):\n        if device_index == None:\n            return 'xpu'\n        return 'xpu:{}'.format(device_index)\n\n    def device(self, device_index=None):\n        return torch.xpu.device(device_index)\n\n    def set_device(self, device_index):\n        torch.xpu.set_device(device_index)\n\n    def current_device(self):\n        return torch.xpu.current_device()\n\n    def current_device_name(self):\n        return 'xpu:{}'.format(torch.xpu.current_device())\n\n    def device_count(self):\n        return torch.xpu.device_count()\n\n    def synchronize(self, device_index=None):\n        return torch.xpu.synchronize(device_index)\n\n    # RNG APIs\n    def random(self):\n        return torch.xpu.random\n\n    def set_rng_state(self, new_state, device_index=None):\n        if device_index == None:\n            return torch.xpu.set_rng_state(new_state)\n        return torch.xpu.set_rng_state(new_state, device_index)\n\n    def get_rng_state(self, device_index=None):\n        if device_index == None:\n            return torch.xpu.get_rng_state()\n        return torch.xpu.get_rng_state(device_index)\n\n    def manual_seed(self, seed):\n        return torch.xpu.manual_seed(seed)\n\n    def manual_seed_all(self, seed):\n        return torch.xpu.manual_seed_all(seed)\n\n    def initial_seed(self):\n        return torch.xpu.initial_seed()\n\n    def default_generator(self, device_index):\n        return torch.xpu.default_generators[device_index]\n\n    # Streams/Events\n    @property\n    def Stream(self):\n        return torch.xpu.Stream\n\n    def stream(self, stream):\n        return torch.xpu.stream(stream)\n\n    def current_stream(self, device_index=None):\n        return torch.xpu.current_stream(device_index)\n\n    def default_stream(self, device_index=None):\n        # torch.xpu does not support the sync behavior of default stream as cuda\n        # use current_stream as workaround\n        # see https://pytorch.org/docs/stable/notes/cuda.html#cuda-streams\n        return torch.xpu.current_stream(device_index)\n\n    @property\n    def Event(self):\n        return torch.xpu.Event\n\n    # Memory management\n    def empty_cache(self):\n        return torch.xpu.empty_cache()\n\n    def memory_allocated(self, device_index=None):\n        return torch.xpu.memory_allocated(device_index)\n\n    def max_memory_allocated(self, device_index=None):\n        return torch.xpu.max_memory_allocated(device_index)\n\n    def reset_max_memory_allocated(self, device_index=None):\n        return torch.xpu.reset_max_memory_allocated(device_index)\n\n    def memory_cached(self, device_index=None):\n        return torch.xpu.memory_reserved(device_index)\n\n    def max_memory_cached(self, device_index=None):\n        return torch.xpu.max_memory_reserved(device_index)\n\n    def reset_max_memory_cached(self, device_index=None):\n        return torch.xpu.reset_max_memory_reserved(device_index)\n\n    def memory_stats(self, device_index=None):\n        return torch.xpu.memory_stats(device_index)\n\n    def reset_peak_memory_stats(self, device_index=None):\n        return torch.xpu.reset_peak_memory_stats(device_index)\n\n    def memory_reserved(self, device_index=None):\n        return torch.xpu.memory_reserved(device_index)\n\n    def max_memory_reserved(self, device_index=None):\n        return torch.xpu.max_memory_reserved(device_index)\n\n    def total_memory(self, device_index=None):\n        return torch.xpu.get_device_properties(device_index).total_memory\n\n    def available_memory(self, device_index=None):\n        return self.total_memory(device_index) - self.memory_allocated(device_index)\n\n    # Misc\n    def amp(self):\n        return torch.xpu.amp\n\n    def is_available(self):\n        return torch.xpu.is_available()\n\n    def range_push(self, msg):\n        # TODO itt is currently not supported yet\n        # return torch.profiler.itt.range_push(msg)\n        return\n\n    def range_pop(self):\n        # TODO itt is currently not supported yet\n        # return torch.profiler.itt.range_pop()\n        return\n\n    def lazy_call(self, callback):\n        if hasattr(torch.xpu, \"_lazy_call\"):\n            return torch.xpu._lazy_call(callback)\n        else:\n            return torch.xpu.lazy_init._lazy_call(callback)\n\n    def communication_backend_name(self):\n        return self._communication_backend_name\n\n    def is_triton_supported(self):\n        return False\n\n    # Graph operations\n    def create_graph(self):\n        return None\n\n    def capture_to_graph(self, graph, pool=None, stream=None):\n        from deepspeed.runtime.utils import noop_context\n        return noop_context()\n\n    def replay_graph(self, graph):\n        return\n\n    # Data types\n    def is_bf16_supported(self):\n        return True\n\n    def is_fp16_supported(self):\n        return True\n\n    def supported_dtypes(self):\n        return [torch.float, torch.half, torch.bfloat16]\n\n    # Tensor operations\n\n    @property\n    def BFloat16Tensor(self):\n        return functools.partial(torch.tensor, dtype=torch.bfloat16, device=self._name)\n\n    @property\n    def ByteTensor(self):\n        return functools.partial(torch.tensor, dtype=torch.uint8, device=self._name)\n\n    @property\n    def DoubleTensor(self):\n        return functools.partial(torch.tensor, dtype=torch.double, device=self._name)\n\n    @property\n    def FloatTensor(self):\n        return functools.partial(torch.tensor, dtype=torch.float, device=self._name)\n\n    @property\n    def HalfTensor(self):\n        return functools.partial(torch.tensor, dtype=torch.half, device=self._name)\n\n    @property\n    def IntTensor(self):\n        return functools.partial(torch.tensor, dtype=torch.int, device=self._name)\n\n    @property\n    def LongTensor(self):\n        return functools.partial(torch.tensor, dtype=torch.long, device=self._name)\n\n    def pin_memory(self, tensor, align_bytes=1):\n        if align_bytes == 1:\n            return tensor.pin_memory(device=self.current_device_name())\n        elif align_bytes == 0:\n            from deepspeed.ops.op_builder.xpu import AsyncIOBuilder\n            self.aio_handle = AsyncIOBuilder().load().aio_handle(128 * 1024, 8, False, False, False)\n            aligned_t = self.aio_handle.new_cpu_locked_tensor(tensor.numel(), tensor)\n            aligned_t = aligned_t[:tensor.numel()].copy_(tensor)\n            self.aligned_tensors.append([aligned_t.data_ptr(), aligned_t[-1].data_ptr()])\n            return aligned_t\n\n    def is_pinned(self, tensor):\n        if tensor.is_pinned(device=self.current_device_name()):\n            return True\n        else:\n            for begin, end in self.aligned_tensors:\n                if begin <= tensor.data_ptr() and tensor.data_ptr() <= end:\n                    return True\n        return False\n\n    def op_builder_dir(self):\n        try:\n            # is op_builder from deepspeed or a 3p version? this should only succeed if it's deepspeed\n            # if successful this also means we're doing a local install and not JIT compile path\n            from op_builder import __deepspeed__  # noqa: F401 # type: ignore\n            return \"op_builder.xpu\"\n        except ImportError:\n            return \"deepspeed.ops.op_builder.xpu\"\n\n    def on_accelerator(self, tensor):\n        device_str = str(tensor.device)\n        if device_str.startswith('xpu:'):\n            return True\n        else:\n            return False\n\n    def _lazy_init_class_dict(self):\n        if self.class_dict:\n            return\n\n        op_builder_module = importlib.import_module(self.op_builder_dir())\n\n        # get op builder class from op_builder/xpu/__init__.py\n        self.class_dict = {}\n        for class_name, class_obj in inspect.getmembers(op_builder_module, inspect.isclass):\n            self.class_dict[class_name] = class_obj\n\n    # create an instance of op builder and return, name specified by class_name\n    def create_op_builder(self, class_name):\n        builder_class = self.get_op_builder(class_name)\n        return builder_class()\n\n    # return an op builder class, name specified by class_name\n    def get_op_builder(self, class_name):\n        self._lazy_init_class_dict()\n        if class_name in self.class_dict:\n            return self.class_dict[class_name]\n        else:\n            return self.class_dict['NotImplementedBuilder']\n\n    def build_extension(self):\n        try:\n            from intel_extension_for_pytorch.xpu.cpp_extension import DpcppBuildExtension\n        except ImportError:\n            from intel_extension_for_pytorch.xpu.utils import DpcppBuildExtension\n        return DpcppBuildExtension\n\n    def export_envs(self):\n        return []\n\n    def visible_devices_envs(self):\n        return ['ZE_AFFINITY_MASK']\n\n    def set_visible_devices_envs(self, current_env, local_accelerator_ids):\n        for env in self.visible_devices_envs():\n            current_env[env] = \",\".join(map(str, local_accelerator_ids))\n\n    def get_compile_backend(self):\n        return self._compile_backend\n\n    def set_compile_backend(self, backend):\n        supported_backends = torch._dynamo.list_backends(exclude_tags=())\n        if backend in supported_backends:\n            self._compile_backend = backend\n        else:\n            raise ValueError(\n                f\"{backend} not supported by {self.device_name()}. Supported Backends are {supported_backends}\")\n", "accelerator/mps_accelerator.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\n\nfrom .abstract_accelerator import DeepSpeedAccelerator\n\n# During setup stage torch may not be installed, pass on no torch will\n# allow op builder related API to be executed.\ntry:\n    import torch.mps\nexcept ImportError:\n    pass\n\n\nclass MPS_Accelerator(DeepSpeedAccelerator):\n\n    def __init__(self):\n        self._name = \"mps\"\n        self._communication_backend_name = None\n        self._compile_backend = \"inductor\"\n\n    def is_synchronized_device(self):\n        return False\n\n    def use_host_timers(self):\n        return self.is_synchronized_device()\n\n    def resolves_data_dependency(self):\n        return self.is_synchronized_device()\n\n    def handles_memory_backpressure(self):\n        return self.is_synchronized_device()\n\n    # Device APIs\n    def device_name(self, device_index=None):\n        if device_index is None:\n            return \"mps\"\n        return \"mps:{}\".format(device_index)\n\n    def device(self, device_index):\n        return torch.device(\"mps\", index=0)\n\n    def set_device(self, device_index):\n        return\n\n    def current_device(self):\n        return torch.device(\"mps\", index=0)\n\n    def current_device_name(self):\n        return \"mps:0\"\n\n    def device_count(self):\n        return 1\n\n    def synchronize(self, device_index=None):\n        return torch.mps.synchronize()\n\n    # RNG APIs\n    def random(self):\n        return torch.random\n\n    def set_rng_state(self, new_state, device_index=None):\n        return torch.mps.set_rng_state(new_state)\n\n    def get_rng_state(self, device_index=None):\n        return torch.mps.get_rng_state()\n\n    def manual_seed(self, seed):\n        return torch.mps.manual_seed(seed)\n\n    def manual_seed_all(self, seed):\n        return torch.mps.manual_seed(seed)\n\n    def seed(self):\n        return torch.mps.seed()\n\n    def initial_seed(self):\n        return\n\n    def default_generator(self, device_index):\n        return\n\n    # Streams/Events\n    @property\n    def Stream(self):\n        return None\n\n    def stream(self, stream):\n        return None\n\n    def current_stream(self, device_index=None):\n        return None\n\n    def default_stream(self, device_index=None):\n        return None\n\n    @property\n    def Event(self):\n        return None\n\n    # Memory management\n    def empty_cache(self):\n        return torch.mps.empty_cache()\n\n    def memory_allocated(self, device_index=None):\n        return torch.mps.current_allocated_memory()\n\n    def max_memory_allocated(self, device_index=None):\n        return torch.mps.driver_allocated_memory()\n\n    def set_per_process_memory_fraction(self, fraction):\n        return torch.mps.set_per_process_memory_fraction(fraction)\n\n    def reset_max_memory_allocated(self, device_index=None):\n        return\n\n    def memory_cached(self, device_index=None):\n        return\n\n    def max_memory_cached(self, device_index=None):\n        return\n\n    def reset_max_memory_cached(self, device_index=None):\n        return\n\n    def memory_stats(self, device_index=None):\n        return\n\n    def reset_peak_memory_stats(self, device_index=None):\n        return\n\n    def memory_reserved(self, device_index=None):\n        return\n\n    def max_memory_reserved(self, device_index=None):\n        return\n\n    def total_memory(self, device_index=None):\n        return\n\n    def available_memory(self, device_index=None):\n        return\n\n    # Data types\n    def is_bf16_supported(self):\n        return False\n\n    def is_fp16_supported(self):\n        return False\n\n    def supported_dtypes(self):\n        return [torch.float]\n\n    # Misc\n    def amp(self):\n        return\n\n    def is_available(self):\n        return hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n\n    def range_push(self, msg):\n        return\n\n    def range_pop(self):\n        return\n\n    def lazy_call(self, callback):\n        return\n\n    def communication_backend_name(self):\n        return self._communication_backend_name\n\n    def is_triton_supported(self):\n        return False\n\n    # Graph operations\n    def create_graph(self):\n        return None\n\n    def capture_to_graph(self, graph, pool=None, stream=None):\n        from deepspeed.runtime.utils import noop_context\n        return noop_context()\n\n    def replay_graph(self, graph):\n        return\n\n    # Tensor operations\n    @property\n    def BFloat16Tensor(self):\n        return\n\n    @property\n    def ByteTensor(self):\n        return\n\n    @property\n    def DoubleTensor(self):\n        return\n\n    @property\n    def FloatTensor(self):\n        return\n\n    @property\n    def HalfTensor(self):\n        return\n\n    @property\n    def IntTensor(self):\n        return\n\n    @property\n    def LongTensor(self):\n        return\n\n    def pin_memory(self, tensor, align_bytes=1):\n        return tensor.pin_memory()\n\n    def is_pinned(self, tensor):\n        return tensor.is_pinned()\n\n    def on_accelerator(self, tensor):\n        device_str = str(tensor.device)\n        if device_str.startswith(\"mps\"):\n            return True\n        else:\n            return False\n\n    def op_builder_dir(self):\n        try:\n            # is op_builder from deepspeed or a 3p version? this should only succeed if it's deepspeed\n            # if successful this also means we're doing a local install and not JIT compile path\n            from op_builder import __deepspeed__  # noqa: F401 # type: ignore\n\n            return \"op_builder\"\n        except ImportError:\n            return \"deepspeed.ops.op_builder\"\n\n    # create an instance of op builder, specified by class_name\n    def create_op_builder(self, op_name):\n        builder_class = self.get_op_builder(op_name)\n        if builder_class is not None:\n            return builder_class()\n        return None\n\n    # return an op builder class, specified by class_name\n    def get_op_builder(self, class_name):\n        from deepspeed.ops.op_builder.cpu import NotImplementedBuilder\n\n        return NotImplementedBuilder\n\n    def build_extension(self):\n        from torch.utils.cpp_extension import BuildExtension\n\n        return BuildExtension\n\n    def export_envs(self):\n        return []\n\n    # TODO: mpu's visible envs is confirmed, keep as CUDA_VISIBLE_DEVICES\n    def visible_devices_envs(self):\n        # TODO: could not find visible devices env for mps\n        return ['CUDA_VISIBLE_DEVICES']\n\n    def set_visible_devices_envs(self, current_env, local_accelerator_ids):\n        for env in self.visible_devices_envs():\n            current_env[env] = \",\".join(map(str, local_accelerator_ids))\n\n    def get_compile_backend(self):\n        return self._compile_backend\n\n    def set_compile_backend(self, backend):\n        supported_backends = torch._dynamo.list_backends(exclude_tags=())\n        if backend in supported_backends:\n            self._compile_backend = backend\n        else:\n            raise ValueError(\n                f\"{backend} not supported by {self.device_name()}. Supported Backends are {supported_backends}\")\n", "accelerator/hpu_accelerator.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport os\nimport pkgutil\nimport importlib\nimport torch\n\nfrom .abstract_accelerator import DeepSpeedAccelerator\n\n\nclass HPU_Accelerator(DeepSpeedAccelerator):\n\n    def __init__(self):\n        self._name = 'hpu'\n        self._communication_backend_name = 'hccl'\n        self._compile_backend = \"hpu_backend\"\n        try:\n            import habana_frameworks.torch.hpu as hpu\n            hpu.setDeterministic(True)\n            self.hpu = hpu\n        except ImportError as e:\n            raise ValueError(\n                f\"HPU_Accelerator requires habana_frameworks.torch.hpu, which is not installed on this system.\")\n\n        self.fp16_supported = None\n\n    # Device APIs\n    def is_synchronized_device(self):\n        return False\n\n    def use_host_timers(self):\n        return False\n\n    def resolves_data_dependency(self):\n        return True\n\n    def handles_memory_backpressure(self):\n        return True\n\n    def device_name(self, device_index=None):\n        if device_index is None:\n            return 'hpu'\n        return 'hpu:{}'.format(device_index)\n\n    def device(self, device_index=None):\n        return torch.device(self.device_name(device_index))\n\n    def set_device(self, device_index):\n        self.hpu.set_device(device_index)\n\n    def current_device(self):\n        return (self.hpu.current_device())\n\n    def current_device_name(self):\n        return 'hpu:{}'.format(self.current_device())\n\n    def device_count(self):\n        return self.hpu.device_count()\n\n    def synchronize(self, device_index=None):\n        return self.hpu.synchronize()\n\n    # RNG APIs\n    def random(self):\n        return torch.random\n\n    def set_rng_state(self, new_state, device_index=None):\n        self.hpu.random.set_rng_state(new_state)\n\n    def get_rng_state(self, device_index=None):\n        return self.hpu.random.get_rng_state()\n\n    def manual_seed(self, seed):\n        return self.hpu.random.manual_seed(seed)\n\n    def manual_seed_all(self, seed):\n        self.hpu.random.manual_seed_all(seed)\n\n    def initial_seed(self):\n        return self.hpu.random.initial_seed()\n\n    def default_generator(self, device_index):\n        return self.hpu.random.default_generators[device_index]\n\n    # Streams/Events\n    @property\n    def Stream(self):\n        return self.hpu.Stream\n\n    def stream(self, stream):\n        return self.hpu.stream(stream)\n\n    def current_stream(self, device_index=None):\n        return self.hpu.current_stream()\n\n    def default_stream(self, device_index=None):\n        return self.hpu.default_stream()\n\n    @property\n    def Event(self):\n        import habana_frameworks.torch.core as htcore\n        return htcore.hpu.Event\n\n    # Memory management\n    def empty_cache(self):\n        return\n\n    def memory_allocated(self, device_index=None):\n        return self.hpu.memory_allocated()\n\n    def max_memory_allocated(self, device_index=None):\n        return self.hpu.max_memory_allocated()\n\n    def reset_max_memory_allocated(self, device_index=None):\n        return self.hpu.reset_max_memory_allocated()\n\n    def memory_cached(self, device_index=None):\n        return self.hpu.memory_cached(device_index)\n\n    def max_memory_cached(self, device_index=None):\n        return self.hpu.max_memory_cached(device_index)\n\n    def reset_max_memory_cached(self, device_index=None):\n        return None\n\n    def memory_stats(self, device_index=None):\n        return self.hpu.memory_stats(device_index)\n\n    def reset_peak_memory_stats(self, device_index=None):\n        self.hpu.reset_peak_memory_stats(device_index)\n\n    def memory_reserved(self, device_index=None):\n        return self.hpu.memory_reserved(device_index)\n\n    def max_memory_reserved(self, device_index=None):\n        return self.hpu.max_memory_reserved(device_index)\n\n    def total_memory(self, device_index=None):\n        return self.memory_stats(device_index)['Limit']\n\n    def available_memory(self, device_index=None):\n        return self.total_memory(device_index) - self.memory_allocated(device_index)\n\n    # Data types\n    def is_bf16_supported(self):\n        return True\n\n    def is_fp16_supported(self):\n        if self.fp16_supported is None:\n            import habana_frameworks.torch.utils.experimental as htexp\n            self.fp16_supported = htexp._is_fp16_supported()\n        return self.fp16_supported\n\n    def supported_dtypes(self):\n        supported_dtypes = [torch.float, torch.bfloat16]\n        if self.is_fp16_supported():\n            supported_dtypes.append(torch.half)\n        return supported_dtypes\n\n    # Misc\n    def amp(self):\n        return None\n\n    def is_available(self):\n        return self.hpu.is_available()\n\n    def range_push(self, msg):\n        return\n\n    def range_pop(self):\n        return\n\n    def lazy_call(self, callback):\n        callback()\n\n    def communication_backend_name(self):\n        return self._communication_backend_name\n\n    def is_triton_supported(self):\n        return False\n\n    # Graph operations\n    def create_graph(self):\n        return self.hpu.HPUGraph()\n\n    def capture_to_graph(self, graph, pool=None, stream=None):\n        return self.hpu.graph(graph, stream=stream)\n\n    def replay_graph(self, graph):\n        graph.replay()\n        return\n\n    # Tensor operations\n    @property\n    def BFloat16Tensor(self):\n        return self.hpu.BFloat16Tensor\n\n    @property\n    def ByteTensor(self):\n        return self.hpu.ByteTensor\n\n    @property\n    def DoubleTensor(self):\n        return self.hpu.DoubleTensor\n\n    @property\n    def FloatTensor(self):\n        return self.hpu.FloatTensor\n\n    @property\n    def HalfTensor(self):\n        return self.hpu.HalfTensor\n\n    @property\n    def IntTensor(self):\n        return self.hpu.IntTensor\n\n    @property\n    def LongTensor(self):\n        return self.hpu.LongTensor\n\n    def pin_memory(self, tensor, align_bytes=1):\n        return tensor.pin_memory(self.device())\n\n    def is_pinned(self, tensor):\n        return tensor.is_pinned()\n\n    def on_accelerator(self, tensor):\n        device_str = str(tensor.device)\n        if device_str.startswith('hpu:'):\n            return True\n        else:\n            return False\n\n    def op_builder_dir(self):\n        try:\n            # is op_builder from deepspeed or a 3p version? this should only succeed if it's deepspeed\n            # if successful this also means we're doing a local install and not JIT compile path\n            from op_builder import __deepspeed__  # noqa: F401 # type: ignore\n            return \"op_builder.hpu\"\n        except ImportError:\n            return \"deepspeed.ops.op_builder.hpu\"\n\n    # dict that holds class name <--> class type mapping i.e.\n    # 'AsyncIOBuilder': <class 'op_builder.async_io.AsyncIOBuilder'>\n    # this dict will be filled at init stage\n    class_dict = None\n\n    def _lazy_init_class_dict(self):\n        if self.class_dict is not None:\n            return\n        else:\n            self.class_dict = {}\n            # begin initialize for create_op_builder()\n            # put all valid class name <--> class type mapping into class_dict\n            op_builder_dir = self.op_builder_dir()\n            op_builder_module = importlib.import_module(op_builder_dir)\n            op_builder_absolute_path = os.path.dirname(op_builder_module.__file__)\n            for _, module_name, _ in pkgutil.iter_modules([op_builder_absolute_path]):\n                # avoid self references,\n                # skip sub_directories which contains ops for other backend(cpu, npu, etc.).\n                if module_name != 'all_ops' and module_name != 'builder' and not os.path.isdir(\n                        os.path.join(op_builder_absolute_path, module_name)):\n                    module = importlib.import_module(\"{}.{}\".format(op_builder_dir, module_name))\n                    for member_name in module.__dir__():\n                        if member_name.endswith(\n                                'Builder'\n                        ) and member_name != \"OpBuilder\" and member_name != \"CPUOpBuilder\" and member_name != \"TorchCPUOpBuilder\":  # avoid abstract classes\n                            if not member_name in self.class_dict:\n                                self.class_dict[member_name] = getattr(module, member_name)\n            # end initialize for create_op_builder()\n\n    # create an instance of op builder and return, name specified by class_name\n    def create_op_builder(self, class_name):\n        self._lazy_init_class_dict()\n        if class_name in self.class_dict:\n            return self.class_dict[class_name]()\n        else:\n            return None\n\n    # return an op builder class, name specified by class_name\n    def get_op_builder(self, class_name):\n        self._lazy_init_class_dict()\n        if class_name in self.class_dict:\n            return self.class_dict[class_name]\n        else:\n            return self.class_dict['NotImplementedBuilder'] if 'NotImplementedBuilder' in self.class_dict else None\n\n    def build_extension(self):\n        from torch.utils.cpp_extension import BuildExtension\n        return BuildExtension\n\n    def export_envs(self):\n        return []\n\n    def visible_devices_envs(self):\n        return ['HABANA_VISIBLE_MODULES']\n\n    def set_visible_devices_envs(self, current_env, local_accelerator_ids):\n        for env in self.visible_devices_envs():\n            current_env[env] = \",\".join(map(str, local_accelerator_ids))\n\n    def get_compile_backend(self):\n        return self._compile_backend\n\n    def set_compile_backend(self, backend):\n        supported_backends = torch._dynamo.list_backends(exclude_tags=())\n        if backend in supported_backends:\n            self._compile_backend = backend\n        else:\n            raise ValueError(\n                f\"{backend} not supported by {self.device_name()}. Supported Backends are {supported_backends}\")\n", "accelerator/cpu_accelerator.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nfrom .abstract_accelerator import DeepSpeedAccelerator\n\ntry:\n    import oneccl_bindings_for_pytorch  # noqa: F401 # type: ignore\n    oneccl_imported_p = True\nexcept ImportError as e:\n    oneccl_imported_p = False\n\nimport os\n\n\n# accelerator for Intel CPU\nclass CPU_Accelerator(DeepSpeedAccelerator):\n\n    def __init__(self):\n        self._name = 'cpu'\n        self._compile_backend = \"inductor\"\n        if oneccl_imported_p:\n            self._communication_backend_name = 'ccl'\n        else:\n            # fallback to gloo if oneccl_binding_for_pytorch is not installed\n            self._communication_backend_name = 'gloo'\n        try:\n            import psutil\n            mem = psutil.Process().memory_info().rss\n            self.max_mem = mem\n        except ImportError as e:\n            self.max_mem = 0\n\n    def is_synchronized_device(self):\n        return True\n\n    def use_host_timers(self):\n        return self.is_synchronized_device()\n\n    def resolves_data_dependency(self):\n        return self.is_synchronized_device()\n\n    def handles_memory_backpressure(self):\n        return self.is_synchronized_device()\n\n    # Device APIs\n    def device_name(self, device_index=None):\n        return 'cpu'\n\n    def device(self, device_index=None):\n        return None\n\n    def set_device(self, device_index):\n        return\n\n    def current_device(self):\n        return os.environ.get('LOCAL_RANK', 0)\n\n    def current_device_name(self):\n        return 'cpu'\n\n    def device_count(self):\n        device_count = int(os.environ.get('LOCAL_SIZE', 0))\n        if device_count > 0:\n            return device_count\n        else:\n            from deepspeed.utils.numa import get_numa_cores\n            # Count NUMA node for number of cpu accelerators. On machine with HBM\n            # In flat mode, HBM is in separate NUMA node with no cores on this node.\n            # Ignore these NUMA nodes with no cores.\n            numa_core_lists = get_numa_cores()\n            numa_count = 0\n            prev_core_list = []\n            for core_list in numa_core_lists:\n                if len(core_list) > 0 and core_list != prev_core_list:\n                    numa_count += 1\n                    prev_core_list = core_list\n            return numa_count\n\n    def synchronize(self, device_index=None):\n        return\n\n    # RNG APIs\n    def random(self):\n        return torch.random\n\n    def set_rng_state(self, new_state, device_index=None):\n        if device_index is None:\n            return torch.set_rng_state(new_state)\n        return torch.set_rng_state(new_state, device_index)\n\n    def get_rng_state(self, device_index=None):\n        return torch.get_rng_state()\n\n    def manual_seed(self, seed):\n        return torch.manual_seed(seed)\n\n    def manual_seed_all(self, seed):\n        return torch.manual_seed(seed)\n\n    def initial_seed(self):\n        return torch.initial_seed()\n\n    def default_generator(self, device_index):\n        return torch.default_generator\n\n    # Streams/Events\n    @property\n    def Stream(self):\n        return None\n\n    def stream(self, stream):\n        from deepspeed.runtime.utils import noop_context\n        return noop_context()\n\n    def current_stream(self, device_index=None):\n        return None\n\n    def default_stream(self, device_index=None):\n        return None\n\n    @property\n    def Event(self):\n        return None\n\n    # Memory management\n    def empty_cache(self):\n        return\n\n    def get_rss(self):\n        import psutil\n        mem = psutil.Process().memory_info().rss\n        if mem > self.max_mem:\n            self.max_mem = mem\n        return mem\n\n    def reset_rss(self):\n        import psutil\n        mem = psutil.Process().memory_info().rss\n        self.max_mem = mem\n        return mem\n\n    def memory_allocated(self, device_index=None):\n        return self.get_rss()\n\n    def max_memory_allocated(self, device_index=None):\n        self.get_rss()\n        return self.max_mem\n\n    def reset_max_memory_allocated(self, device_index=None):\n        self.reset_rss()\n        return\n\n    def memory_cached(self, device_index=None):\n        return self.get_rss()\n\n    def max_memory_cached(self, device_index=None):\n        self.get_rss()\n        return self.max_mem\n\n    def reset_max_memory_cached(self, device_index=None):\n        self.reset_rss()\n        return\n\n    def memory_stats(self, device_index=None):\n        mem = self.get_rss()\n        mem_stat = {}\n        mem_stat['allocated_bytes.all.current'] = mem\n        mem_stat['allocated_bytes.all.peak'] = self.max_mem\n        return mem_stat\n\n    def reset_peak_memory_stats(self, device_index=None):\n        self.reset_rss()\n        return\n\n    def memory_reserved(self, device_index=None):\n        return self.get_rss()\n\n    def max_memory_reserved(self, device_index=None):\n        self.get_rss()\n        return self.max_mem\n\n    def total_memory(self, device_index=None):\n        import psutil\n        return psutil.virtual_memory().total\n\n    def available_memory(self, device_index=None):\n        import psutil\n        return psutil.virtual_memory().available\n\n    # Misc\n    def amp(self):\n        return torch.cpu.amp\n\n    def is_available(self):\n        return True\n\n    def range_push(self, msg):\n        # TODO itt is currently not supported yet\n        # return torch.profiler.itt.range_push(msg)\n        return\n\n    def range_pop(self):\n        # TODO itt is currently not supported yet\n        # return torch.profiler.itt.range_pop()\n        return\n\n    def lazy_call(self, callback):\n        return callback()\n\n    def communication_backend_name(self):\n        return self._communication_backend_name\n\n    def is_triton_supported(self):\n        return False\n\n    # Data types\n    def is_bf16_supported(self):\n        return True\n\n    def is_fp16_supported(self):\n        return False\n\n    def supported_dtypes(self):\n        return [torch.float, torch.bfloat16]\n\n    # Graph operations\n    def create_graph(self):\n        return None\n\n    def capture_to_graph(self, graph, pool=None, stream=None):\n        from deepspeed.runtime.utils import noop_context\n        return noop_context()\n\n    def replay_graph(self, graph):\n        return\n\n    # Tensor operations\n    @property\n    def BFloat16Tensor(self):\n        return torch.BFloat16Tensor\n\n    @property\n    def ByteTensor(self):\n        return torch.ByteTensor\n\n    @property\n    def DoubleTensor(self):\n        return torch.DoubleTensor\n\n    @property\n    def FloatTensor(self):\n        return torch.FloatTensor\n\n    @property\n    def HalfTensor(self):\n        return torch.HalfTensor\n\n    @property\n    def IntTensor(self):\n        return torch.IntTensor\n\n    @property\n    def LongTensor(self):\n        return torch.LongTensor\n\n    def pin_memory(self, tensor, align_bytes=1):\n        return tensor\n\n    def is_pinned(self, tensor):\n        return tensor.is_pinned()\n\n    def op_builder_dir(self):\n        try:\n            # is op_builder from deepspeed or a 3p version? this should only succeed if it's deepspeed\n            # if successful this also means we're doing a local install and not JIT compile path\n            from op_builder import __deepspeed__  # noqa: F401 # type: ignore\n            return \"op_builder.cpu\"\n        except ImportError:\n            return \"deepspeed.ops.op_builder.cpu\"\n\n    def on_accelerator(self, tensor):\n        device_str = str(tensor.device)\n        if device_str.startswith('cpu'):\n            return True\n        else:\n            return False\n\n    # create an instance of op builder and return, name specified by class_name\n    def create_op_builder(self, op_name):\n        builder_class = self.get_op_builder(op_name)\n        if builder_class is not None:\n            return builder_class()\n        return None\n\n    # return an op builder class, name specified by class_name\n    def get_op_builder(self, class_name):\n        try:\n            # is op_builder from deepspeed or a 3p version? this should only succeed if it's deepspeed\n            # if successful this also means we're doing a local install and not JIT compile path\n            from op_builder import __deepspeed__  # noqa: F401 # type: ignore\n            from op_builder.cpu import CCLCommBuilder, ShareMemCommBuilder, FusedAdamBuilder, CPUAdamBuilder, NotImplementedBuilder\n        except ImportError:\n            from deepspeed.ops.op_builder.cpu import CCLCommBuilder, ShareMemCommBuilder, FusedAdamBuilder, CPUAdamBuilder, NotImplementedBuilder\n\n        if class_name == \"CCLCommBuilder\":\n            return CCLCommBuilder\n        elif class_name == \"ShareMemCommBuilder\":\n            return ShareMemCommBuilder\n        elif class_name == \"FusedAdamBuilder\":\n            return FusedAdamBuilder\n        elif class_name == \"CPUAdamBuilder\":\n            return CPUAdamBuilder\n        else:\n            # return a NotImplementedBuilder to avoid get NoneType[Name] in unit tests\n            return NotImplementedBuilder\n\n    def build_extension(self):\n        from torch.utils.cpp_extension import BuildExtension\n        return BuildExtension\n\n    def export_envs(self):\n        return []\n\n    # TODO: cpu's visible envs is confirmed, keep as CUDA_VISIBLE_DEVICES\n    def visible_devices_envs(self):\n        return ['CUDA_VISIBLE_DEVICES']\n\n    def set_visible_devices_envs(self, current_env, local_accelerator_ids):\n        for env in self.visible_devices_envs():\n            current_env[env] = \",\".join(map(str, local_accelerator_ids))\n\n    def get_compile_backend(self):\n        return self._compile_backend\n\n    def set_compile_backend(self, backend):\n        supported_backends = torch._dynamo.list_backends(exclude_tags=())\n        if backend in supported_backends:\n            self._compile_backend = backend\n        else:\n            raise ValueError(\n                f\"{backend} not supported by {self.device_name()}. Supported Backends are {supported_backends}\")\n", "accelerator/npu_accelerator.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\nimport importlib\nimport inspect\n\nfrom .abstract_accelerator import DeepSpeedAccelerator\n# During setup stage torch may not be installed, pass on no torch will\n# allow op builder related API to be executed.\ntry:\n    import torch.npu\nexcept ImportError:\n    pass\n\n\nclass NPU_Accelerator(DeepSpeedAccelerator):\n\n    def __init__(self):\n        super().__init__()\n        self._name = 'npu'\n        self._communication_backend_name = 'hccl'\n        self._compile_backend = \"inductor\"\n        # dict that holds class name <--> class type mapping i.e.\n        # 'AsyncIOBuilder': <class 'op_builder.async_io.AsyncIOBuilder'>\n        # this dict will be filled at init stage\n        self.class_dict = None\n\n    def is_synchronized_device(self):\n        return False\n\n    def use_host_timers(self):\n        return self.is_synchronized_device()\n\n    def resolves_data_dependency(self):\n        return self.is_synchronized_device()\n\n    def handles_memory_backpressure(self):\n        return self.is_synchronized_device()\n\n    # Device APIs\n    def device_name(self, device_index=None):\n        if device_index is None:\n            return 'npu'\n        return 'npu:{}'.format(device_index)\n\n    def device(self, device_index=None):\n        return torch.npu.device(device_index)\n\n    def set_device(self, device_index):\n        torch.npu.set_device(device_index)\n\n    def current_device(self):\n        return torch.npu.current_device()\n\n    def current_device_name(self):\n        return 'npu:{}'.format(torch.npu.current_device())\n\n    def device_count(self):\n        return torch.npu.device_count()\n\n    def synchronize(self, device_index=None):\n        return torch.npu.synchronize(device_index)\n\n    # RNG APIs\n    def random(self):\n        return torch.random\n\n    def set_rng_state(self, new_state, device_index=None):\n        if device_index is None:\n            return torch.npu.set_rng_state(new_state)\n\n        return torch.npu.set_rng_state(new_state, device_index)\n\n    def get_rng_state(self, device_index=None):\n        if device_index is None:\n            return torch.npu.get_rng_state()\n\n        return torch.npu.get_rng_state(device_index)\n\n    def manual_seed(self, seed):\n        return torch.npu.manual_seed(seed)\n\n    def manual_seed_all(self, seed):\n        return torch.npu.manual_seed_all(seed)\n\n    def initial_seed(self):\n        return torch.npu.initial_seed()\n\n    def default_generator(self, device_index):\n        return torch.npu.default_generators[device_index]\n\n    # Streams/Events\n    @property\n    def Stream(self):\n        return torch.npu.Stream\n\n    def stream(self, stream):\n        return torch.npu.stream(stream)\n\n    def current_stream(self, device_index=None):\n        return torch.npu.current_stream(device_index)\n\n    def default_stream(self, device_index=None):\n        return torch.npu.default_stream(device_index)\n\n    @property\n    def Event(self):\n        return torch.npu.Event\n\n    # Memory management\n    def empty_cache(self):\n        return torch.npu.empty_cache()\n\n    def memory_allocated(self, device_index=None):\n        return torch.npu.memory_allocated(device_index)\n\n    def max_memory_allocated(self, device_index=None):\n        return torch.npu.max_memory_allocated(device_index)\n\n    def reset_max_memory_allocated(self, device_index=None):\n        return torch.npu.reset_max_memory_allocated(device_index)\n\n    def memory_cached(self, device_index=None):\n        return torch.npu.memory_cached(device_index)\n\n    def max_memory_cached(self, device_index=None):\n        return torch.npu.max_memory_cached(device_index)\n\n    def reset_max_memory_cached(self, device_index=None):\n        return torch.npu.reset_max_memory_cached(device_index)\n\n    def memory_stats(self, device_index=None):\n        if hasattr(torch.npu, 'memory_stats'):\n            return torch.npu.memory_stats(device_index)\n\n    def reset_peak_memory_stats(self, device_index=None):\n        if hasattr(torch.npu, 'reset_peak_memory_stats'):\n            return torch.npu.reset_peak_memory_stats(device_index)\n\n    def memory_reserved(self, device_index=None):\n        if hasattr(torch.npu, 'memory_reserved'):\n            return torch.npu.memory_reserved(device_index)\n\n    def max_memory_reserved(self, device_index=None):\n        if hasattr(torch.npu, 'max_memory_reserved'):\n            return torch.npu.max_memory_reserved(device_index)\n\n    def total_memory(self, device_index=None):\n        return torch.npu.get_device_properties(device_index).total_memory\n\n    def available_memory(self, device_index=None):\n        return self.total_memory(device_index) - self.memory_allocated(device_index)\n\n    # Data types\n    def is_bf16_supported(self):\n        return torch.npu.is_bf16_supported()\n\n    def is_fp16_supported(self):\n        return True\n\n    def supported_dtypes(self):\n        return [torch.float, torch.half, torch.bfloat16]\n\n    # Misc\n    def amp(self):\n        if hasattr(torch.npu, 'amp'):\n            return torch.npu.amp\n        return None\n\n    def is_available(self):\n        return torch.npu.is_available()\n\n    def range_push(self, msg):\n        return\n\n    def range_pop(self):\n        return\n\n    def lazy_call(self, callback):\n        return torch.npu._lazy_call(callback)\n\n    def communication_backend_name(self):\n        return self._communication_backend_name\n\n    def is_triton_supported(self):\n        return False\n\n    # Graph operations\n    def create_graph(self):\n        return None\n\n    def capture_to_graph(self, graph, pool=None, stream=None):\n        from deepspeed.runtime.utils import noop_context\n        return noop_context()\n\n    def replay_graph(self, graph):\n        return\n\n    # Tensor operations\n\n    @property\n    def BFloat16Tensor(self):\n        return torch.npu.BFloat16Tensor\n\n    @property\n    def ByteTensor(self):\n        return torch.npu.ByteTensor\n\n    @property\n    def DoubleTensor(self):\n        return torch.npu.DoubleTensor\n\n    @property\n    def FloatTensor(self):\n        return torch.npu.FloatTensor\n\n    @property\n    def HalfTensor(self):\n        return torch.npu.HalfTensor\n\n    @property\n    def IntTensor(self):\n        return torch.npu.IntTensor\n\n    @property\n    def LongTensor(self):\n        return torch.npu.LongTensor\n\n    def pin_memory(self, tensor, align_bytes=1):\n        return tensor.pin_memory()\n\n    def is_pinned(self, tensor):\n        return tensor.is_pinned()\n\n    def on_accelerator(self, tensor):\n        device_str = str(tensor.device)\n        if device_str.startswith('npu:'):\n            return True\n        else:\n            return False\n\n    def op_builder_dir(self):\n        try:\n            # is op_builder from deepspeed or a 3p version? this should only succeed if it's deepspeed\n            # if successful this also means we're doing a local install and not JIT compile path\n            from op_builder import __deepspeed__  # noqa: F401 # type: ignore\n            return \"op_builder.npu\"\n        except ImportError:\n            return \"deepspeed.ops.op_builder.npu\"\n\n    def _lazy_init_class_dict(self):\n        if self.class_dict:\n            return\n\n        op_builder_module = importlib.import_module(self.op_builder_dir())\n\n        # get op builder class from op_builder/npu/__init__.py\n        self.class_dict = {}\n        for class_name, class_obj in inspect.getmembers(op_builder_module, inspect.isclass):\n            self.class_dict[class_name] = class_obj\n\n    # create an instance of op builder and return, name specified by class_name\n    def create_op_builder(self, class_name):\n        builder_class = self.get_op_builder(class_name)\n        return None if builder_class is None else builder_class()\n\n    # return an op builder class, name specified by class_name\n    def get_op_builder(self, class_name):\n        self._lazy_init_class_dict()\n        if class_name in self.class_dict:\n            return self.class_dict[class_name]\n        else:\n            return self.class_dict['NotImplementedBuilder'] if 'NotImplementedBuilder' in self.class_dict else None\n\n    def build_extension(self):\n        from torch.utils.cpp_extension import BuildExtension\n        return BuildExtension\n\n    def export_envs(self):\n        return ['ASCEND', 'HCCL', 'LD_LIBRARY', 'PATH']\n\n    def visible_devices_envs(self):\n        return ['ASCEND_RT_VISIBLE_DEVICES']\n\n    def set_visible_devices_envs(self, current_env, local_accelerator_ids):\n        for env in self.visible_devices_envs():\n            current_env[env] = \",\".join(map(str, local_accelerator_ids))\n\n    def get_compile_backend(self):\n        return self._compile_backend\n\n    def set_compile_backend(self, backend):\n        supported_backends = torch._dynamo.list_backends(exclude_tags=())\n        if backend in supported_backends:\n            self._compile_backend = backend\n        else:\n            raise ValueError(\n                f\"{backend} not supported by {self.device_name()}. Supported Backends are {supported_backends }\")\n", "accelerator/real_accelerator.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\nimport os\n\ntry:\n    # Importing logger currently requires that torch is installed, hence the try...except\n    # TODO: Remove logger dependency on torch.\n    from deepspeed.utils import logger as accel_logger\nexcept ImportError as e:\n    accel_logger = None\n\ntry:\n    from accelerator.abstract_accelerator import DeepSpeedAccelerator as dsa1\nexcept ImportError as e:\n    dsa1 = None\ntry:\n    from deepspeed.accelerator.abstract_accelerator import DeepSpeedAccelerator as dsa2\nexcept ImportError as e:\n    dsa2 = None\n\nSUPPORTED_ACCELERATOR_LIST = ['cuda', 'cpu', 'xpu', 'xpu.external', 'npu', 'mps', 'hpu']\n\nds_accelerator = None\n\n\ndef _validate_accelerator(accel_obj):\n    # because abstract_accelerator has different path during\n    # build time (accelerator.abstract_accelerator)\n    # and run time (deepspeed.accelerator.abstract_accelerator)\n    # and extension would import the\n    # run time abstract_accelerator/DeepSpeedAccelerator as its base\n    # class, so we need to compare accel_obj with both base class.\n    # if accel_obj is instance of DeepSpeedAccelerator in one of\n    # accelerator.abstractor_accelerator\n    # or deepspeed.accelerator.abstract_accelerator, consider accel_obj\n    # is a conforming object\n    if not ((dsa1 is not None and isinstance(accel_obj, dsa1)) or (dsa2 is not None and isinstance(accel_obj, dsa2))):\n        raise AssertionError(f\"{accel_obj.__class__.__name__} accelerator is not subclass of DeepSpeedAccelerator\")\n\n    # TODO: turn off is_available test since this breaks tests\n    # assert accel_obj.is_available(), \\\n    #    f'{accel_obj.__class__.__name__} accelerator fails is_available() test'\n\n\ndef is_current_accelerator_supported():\n    return get_accelerator().device_name() in SUPPORTED_ACCELERATOR_LIST\n\n\ndef get_accelerator():\n    global ds_accelerator\n    if ds_accelerator is not None:\n        return ds_accelerator\n\n    accelerator_name = None\n    ds_set_method = None\n    # 1. Detect whether there is override of DeepSpeed accelerators from environment variable.\n    if \"DS_ACCELERATOR\" in os.environ.keys():\n        accelerator_name = os.environ[\"DS_ACCELERATOR\"]\n        if accelerator_name == \"xpu\":\n            try:\n                import intel_extension_for_pytorch as ipex\n                assert ipex._C._has_xpu(), \"XPU_Accelerator requires an intel_extension_for_pytorch that supports XPU.\"\n            except ImportError as e:\n                raise ValueError(\n                    f\"XPU_Accelerator requires intel_extension_for_pytorch, which is not installed on this system.\")\n        elif accelerator_name == \"xpu.external\":\n            try:\n                import intel_extension_for_deepspeed  # noqa: F401 # type: ignore\n            except ImportError as e:\n                raise ValueError(\n                    f\"XPU_Accelerator external requires intel_extension_for_deepspeed, which is not installed on this system.\"\n                )\n        elif accelerator_name == \"cpu\":\n            pass\n        elif accelerator_name == \"npu\":\n            try:\n                import torch_npu  # noqa: F401 # type: ignore\n            except ImportError as e:\n                raise ValueError(f\"NPU_Accelerator requires torch_npu, which is not installed on this system.\")\n            pass\n        elif accelerator_name == \"mps\":\n            try:\n                import torch.mps\n\n                # should use torch.mps.is_available() if it exists someday but this is used as proxy\n                torch.mps.current_allocated_memory()\n            except (RuntimeError, ImportError) as e:\n                raise ValueError(f\"MPS_Accelerator requires torch.mps, which is not installed on this system.\")\n        elif accelerator_name == \"hpu\":\n            try:\n                import habana_frameworks.torch.hpu  # noqa: F401\n            except ImportError as e:\n                raise ValueError(\n                    f\"HPU_Accelerator requires habana_frameworks.torch.hpu, which is not installed on this system.\")\n        elif accelerator_name not in SUPPORTED_ACCELERATOR_LIST:\n            raise ValueError(f'DS_ACCELERATOR must be one of {SUPPORTED_ACCELERATOR_LIST}. '\n                             f'Value \"{accelerator_name}\" is not supported')\n        ds_set_method = \"override\"\n\n    # 2. If no override, detect which accelerator to use automatically\n    if accelerator_name is None:\n        # We need a way to choose among different accelerator types.\n        # Currently we detect which accelerator extension is installed\n        # in the environment and use it if the installing answer is True.\n        # An alternative might be detect whether CUDA device is installed on\n        # the system but this comes with two pitfalls:\n        # 1. the system may not have torch pre-installed, so\n        #    get_accelerator().is_available() may not work.\n        # 2. Some scenario like install on login node (without CUDA device)\n        #    and run on compute node (with CUDA device) may cause mismatch\n        #    between installation time and runtime.\n\n        try:\n            from intel_extension_for_deepspeed import XPU_Accelerator  # noqa: F401,F811 # type: ignore\n            accelerator_name = \"xpu.external\"\n        except ImportError as e:\n            pass\n        if accelerator_name is None:\n            try:\n                import intel_extension_for_pytorch as ipex\n                if ipex._C._has_xpu():\n                    accelerator_name = \"xpu\"\n                else:\n                    accelerator_name = \"cpu\"\n            except ImportError as e:\n                pass\n        if accelerator_name is None:\n            try:\n                import torch_npu  # noqa: F401,F811 # type: ignore\n\n                accelerator_name = \"npu\"\n            except ImportError as e:\n                pass\n        if accelerator_name is None:\n            try:\n                import torch.mps\n\n                # should use torch.mps.is_available() if it exists someday but this is used as proxy\n                torch.mps.current_allocated_memory()\n                accelerator_name = \"mps\"\n            except (RuntimeError, ImportError) as e:\n                pass\n        if accelerator_name is None:\n            try:\n                import habana_frameworks.torch.hpu  # noqa: F401,F811\n\n                accelerator_name = \"hpu\"\n            except ImportError as e:\n                pass\n        if accelerator_name is None:\n            # borrow this log from PR#5084\n            try:\n                import torch\n\n                # Determine if we are on a GPU or x86 CPU with torch.\n                if torch.cuda.is_available():  #ignore-cuda\n                    accelerator_name = \"cuda\"\n                else:\n                    if accel_logger is not None:\n                        accel_logger.warn(\n                            \"Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.\"\n                        )\n                    accelerator_name = \"cpu\"\n            except (RuntimeError, ImportError) as e:\n                # TODO need a more decent way to detect which accelerator to use, consider using nvidia-smi command for detection\n                accelerator_name = \"cuda\"\n                pass\n\n        ds_set_method = \"auto detect\"\n\n    # 3. Set ds_accelerator accordingly\n    if accelerator_name == \"cuda\":\n        from .cuda_accelerator import CUDA_Accelerator\n\n        ds_accelerator = CUDA_Accelerator()\n    elif accelerator_name == \"cpu\":\n        from .cpu_accelerator import CPU_Accelerator\n\n        ds_accelerator = CPU_Accelerator()\n    elif accelerator_name == \"xpu.external\":\n        # XPU_Accelerator is already imported in detection stage\n        ds_accelerator = XPU_Accelerator()\n    elif accelerator_name == \"xpu\":\n        from .xpu_accelerator import XPU_Accelerator\n\n        ds_accelerator = XPU_Accelerator()\n    elif accelerator_name == \"npu\":\n        from .npu_accelerator import NPU_Accelerator\n\n        ds_accelerator = NPU_Accelerator()\n    elif accelerator_name == \"mps\":\n        from .mps_accelerator import MPS_Accelerator\n\n        ds_accelerator = MPS_Accelerator()\n    elif accelerator_name == 'hpu':\n        from .hpu_accelerator import HPU_Accelerator\n\n        ds_accelerator = HPU_Accelerator()\n    _validate_accelerator(ds_accelerator)\n    if accel_logger is not None:\n        accel_logger.info(f\"Setting ds_accelerator to {ds_accelerator._name} ({ds_set_method})\")\n    return ds_accelerator\n\n\ndef set_accelerator(accel_obj):\n    global ds_accelerator\n    _validate_accelerator(accel_obj)\n    if accel_logger is not None:\n        accel_logger.info(f\"Setting ds_accelerator to {accel_obj._name} (model specified)\")\n    ds_accelerator = accel_obj\n\n\n\"\"\"\n-----------[code] test_get.py -----------\nfrom deepspeed.accelerator import get_accelerator\nmy_accelerator = get_accelerator()\nlogger.info(f'{my_accelerator._name=}')\nlogger.info(f'{my_accelerator._communication_backend=}')\nlogger.info(f'{my_accelerator.HalfTensor().device=}')\nlogger.info(f'{my_accelerator.total_memory()=}')\n-----------[code] test_get.py -----------\n\n---[output] python test_get.py---------\nmy_accelerator.name()='cuda'\nmy_accelerator.communication_backend='nccl'\nmy_accelerator.HalfTensor().device=device(type='cuda', index=0)\nmy_accelerator.total_memory()=34089730048\n---[output] python test_get.py---------\n\n**************************************************************************\n-----------[code] test_set.py -----------\nfrom deepspeed.accelerator.cuda_accelerator import CUDA_Accelerator\ncu_accel = CUDA_Accelerator()\nlogger.info(f'{id(cu_accel)=}')\nfrom deepspeed.accelerator import set_accelerator, get_accelerator\nset_accelerator(cu_accel)\n\nmy_accelerator = get_accelerator()\nlogger.info(f'{id(my_accelerator)=}')\nlogger.info(f'{my_accelerator._name=}')\nlogger.info(f'{my_accelerator._communication_backend=}')\nlogger.info(f'{my_accelerator.HalfTensor().device=}')\nlogger.info(f'{my_accelerator.total_memory()=}')\n-----------[code] test_set.py -----------\n\n\n---[output] python test_set.py---------\nid(cu_accel)=139648165478304\nmy_accelerator=<deepspeed.accelerator.cuda_accelerator.CUDA_Accelerator object at 0x7f025f4bffa0>\nmy_accelerator.name='cuda'\nmy_accelerator.communication_backend='nccl'\nmy_accelerator.HalfTensor().device=device(type='cuda', index=0)\nmy_accelerator.total_memory()=34089730048\n---[output] python test_set.py---------\n\"\"\"\n", "accelerator/abstract_accelerator.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport abc\nfrom abc import ABC\n\n\nclass DeepSpeedAccelerator(ABC):\n\n    def __init__(self):\n        self._name = None\n        self._communication_backend_name = None\n        self._compile_backend = None\n\n    @abc.abstractmethod\n    def is_synchronized_device(self):\n        ...\n\n    @abc.abstractmethod\n    def use_host_timers(self):\n        ...\n\n    @abc.abstractmethod\n    def resolves_data_dependency(self):\n        ...\n\n    @abc.abstractmethod\n    def handles_memory_backpressure(self):\n        ...\n\n    # Device APIs\n    @abc.abstractmethod\n    def device_name(self, device_index):\n        ...\n\n    @abc.abstractmethod\n    def device(self, device_index):\n        ...\n\n    @abc.abstractmethod\n    def set_device(self, device_index):\n        ...\n\n    @abc.abstractmethod\n    def current_device(self):\n        ...\n\n    @abc.abstractmethod\n    def current_device_name(self):\n        ...\n\n    @abc.abstractmethod\n    def device_count(self):\n        ...\n\n    @abc.abstractmethod\n    def synchronize(self, device_index=None):\n        ...\n\n    # RNG APIs\n    @abc.abstractmethod\n    def random(self):\n        ...\n\n    @abc.abstractmethod\n    def set_rng_state(self, new_state, device_index=None):\n        ...\n\n    @abc.abstractmethod\n    def get_rng_state(self, device_index=None):\n        ...\n\n    @abc.abstractmethod\n    def manual_seed(self, seed):\n        ...\n\n    @abc.abstractmethod\n    def manual_seed_all(self, seed):\n        ...\n\n    @abc.abstractmethod\n    def initial_seed(self):\n        ...\n\n    @abc.abstractmethod\n    def default_generator(self, device_index):\n        ...\n\n    # Streams/Events\n    @property\n    @abc.abstractmethod\n    def Stream(self):\n        ...\n\n    @abc.abstractmethod\n    def stream(self, stream):\n        ...\n\n    @abc.abstractmethod\n    def current_stream(self, device_index=None):\n        ...\n\n    @abc.abstractmethod\n    def default_stream(self, device_index=None):\n        ...\n\n    @property\n    @abc.abstractmethod\n    def Event(self):\n        ...\n\n    # Memory management\n    @abc.abstractmethod\n    def empty_cache(self):\n        ...\n\n    @abc.abstractmethod\n    def memory_allocated(self, device_index=None):\n        ...\n\n    @abc.abstractmethod\n    def max_memory_allocated(self, device_index=None):\n        ...\n\n    @abc.abstractmethod\n    def reset_max_memory_allocated(self, device_index=None):\n        ...\n\n    @abc.abstractmethod\n    def memory_cached(self, device_index=None):\n        ...\n\n    @abc.abstractmethod\n    def max_memory_cached(self, device_index=None):\n        ...\n\n    @abc.abstractmethod\n    def reset_max_memory_cached(self, device_index=None):\n        ...\n\n    @abc.abstractmethod\n    def memory_stats(self, device_index=None):\n        ...\n\n    @abc.abstractmethod\n    def reset_peak_memory_stats(self, device_index=None):\n        ...\n\n    @abc.abstractmethod\n    def memory_reserved(self, device_index=None):\n        ...\n\n    @abc.abstractmethod\n    def max_memory_reserved(self, device_index=None):\n        ...\n\n    @abc.abstractmethod\n    def total_memory(self, device_index=None):\n        ...\n\n    @abc.abstractmethod\n    def available_memory(self, device_index=None):\n        ...\n\n    # Data types\n    @abc.abstractmethod\n    def is_bf16_supported(self):\n        ...\n\n    @abc.abstractmethod\n    def is_fp16_supported(self):\n        ...\n\n    @abc.abstractmethod\n    def supported_dtypes(self):\n        ...\n\n    # Misc\n    @abc.abstractmethod\n    def amp(self):\n        ...\n\n    @abc.abstractmethod\n    def is_available(self):\n        ...\n\n    @abc.abstractmethod\n    def range_push(self, msg):\n        ...\n\n    @abc.abstractmethod\n    def range_pop(self):\n        ...\n\n    @abc.abstractmethod\n    def lazy_call(self, callback):\n        ...\n\n    @abc.abstractmethod\n    def communication_backend_name(self):\n        ...\n\n    @abc.abstractmethod\n    def is_triton_supported(self):\n        ...\n\n    # Graph operations\n    @abc.abstractmethod\n    def create_graph(self):\n        ...\n\n    @abc.abstractmethod\n    def capture_to_graph(self, graph, pool=None, stream=None):\n        ...\n\n    @abc.abstractmethod\n    def replay_graph(self, graph):\n        ...\n\n    # Tensor operations\n    @property\n    @abc.abstractmethod\n    def BFloat16Tensor(self):\n        ...\n\n    @property\n    @abc.abstractmethod\n    def ByteTensor(self):\n        ...\n\n    @property\n    @abc.abstractmethod\n    def DoubleTensor(self):\n        ...\n\n    @property\n    @abc.abstractmethod\n    def FloatTensor(self):\n        ...\n\n    @property\n    @abc.abstractmethod\n    def HalfTensor(self):\n        ...\n\n    @property\n    @abc.abstractmethod\n    def IntTensor(self):\n        ...\n\n    @property\n    @abc.abstractmethod\n    def LongTensor(self):\n        ...\n\n    @abc.abstractmethod\n    def pin_memory(self, tensor, align_bytes=1):\n        ...\n\n    @abc.abstractmethod\n    def is_pinned(self, tensor):\n        ...\n\n    @abc.abstractmethod\n    def on_accelerator(self, tensor):\n        ...\n\n    @abc.abstractmethod\n    def op_builder_dir(self):\n        ...\n\n    # create an instance of op builder, specified by class_name\n    @abc.abstractmethod\n    def create_op_builder(self, class_name):\n        ...\n\n    # return an op builder class, specified by class_name\n    @abc.abstractmethod\n    def get_op_builder(self, class_name):\n        ...\n\n    @abc.abstractmethod\n    def build_extension(self):\n        ...\n\n    @abc.abstractmethod\n    def export_envs(self):\n        ...\n\n    @abc.abstractmethod\n    def visible_devices_envs(self):\n        ...\n\n    @abc.abstractmethod\n    def set_visible_devices_envs(self, current_env, local_accelerator_ids):\n        ...\n\n    @abc.abstractmethod\n    def get_compile_backend(self):\n        ...\n\n    @abc.abstractmethod\n    def set_compile_backend(self, backend):\n        ...\n", "accelerator/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .abstract_accelerator import DeepSpeedAccelerator\nfrom .real_accelerator import get_accelerator, set_accelerator, is_current_accelerator_supported\n", "deepspeed/env_report.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport os\nimport torch\nimport deepspeed\nimport subprocess\nimport argparse\nfrom .ops.op_builder.all_ops import ALL_OPS\nfrom .git_version_info import installed_ops, torch_info, accelerator_name\nfrom deepspeed.accelerator import get_accelerator\n\nGREEN = '\\033[92m'\nRED = '\\033[91m'\nYELLOW = '\\033[93m'\nEND = '\\033[0m'\nSUCCESS = f\"{GREEN} [SUCCESS] {END}\"\nOKAY = f\"{GREEN}[OKAY]{END}\"\nWARNING = f\"{YELLOW}[WARNING]{END}\"\nFAIL = f'{RED}[FAIL]{END}'\nINFO = '[INFO]'\n\ncolor_len = len(GREEN) + len(END)\nokay = f\"{GREEN}[OKAY]{END}\"\nwarning = f\"{YELLOW}[WARNING]{END}\"\n\n\ndef op_report(verbose=True):\n    max_dots = 23\n    max_dots2 = 11\n    h = [\"op name\", \"installed\", \"compatible\"]\n    print(\"-\" * (max_dots + max_dots2 + len(h[0]) + len(h[1])))\n    print(\"DeepSpeed C++/CUDA extension op report\")\n    print(\"-\" * (max_dots + max_dots2 + len(h[0]) + len(h[1])))\n\n    print(\"NOTE: Ops not installed will be just-in-time (JIT) compiled at\\n\"\n          \"      runtime if needed. Op compatibility means that your system\\n\"\n          \"      meet the required dependencies to JIT install the op.\")\n\n    print(\"-\" * (max_dots + max_dots2 + len(h[0]) + len(h[1])))\n    print(\"JIT compiled ops requires ninja\")\n    ninja_status = OKAY if ninja_installed() else FAIL\n    print('ninja', \".\" * (max_dots - 5), ninja_status)\n    print(\"-\" * (max_dots + max_dots2 + len(h[0]) + len(h[1])))\n    print(h[0], \".\" * (max_dots - len(h[0])), h[1], \".\" * (max_dots2 - len(h[1])), h[2])\n    print(\"-\" * (max_dots + max_dots2 + len(h[0]) + len(h[1])))\n    installed = f\"{GREEN}[YES]{END}\"\n    no = f\"{YELLOW}[NO]{END}\"\n    for op_name, builder in ALL_OPS.items():\n        dots = \".\" * (max_dots - len(op_name))\n        is_compatible = OKAY if builder.is_compatible(verbose) else no\n        is_installed = installed if installed_ops.get(op_name,\n                                                      False) and accelerator_name == get_accelerator()._name else no\n        dots2 = '.' * ((len(h[1]) + (max_dots2 - len(h[1]))) - (len(is_installed) - color_len))\n        print(op_name, dots, is_installed, dots2, is_compatible)\n    print(\"-\" * (max_dots + max_dots2 + len(h[0]) + len(h[1])))\n\n\ndef ninja_installed():\n    try:\n        import ninja  # noqa: F401 # type: ignore\n    except ImportError:\n        return False\n    return True\n\n\ndef nvcc_version():\n    import torch.utils.cpp_extension\n    cuda_home = torch.utils.cpp_extension.CUDA_HOME\n    if cuda_home is None:\n        return f\"{RED} [FAIL] cannot find CUDA_HOME via torch.utils.cpp_extension.CUDA_HOME={torch.utils.cpp_extension.CUDA_HOME} {END}\"\n    try:\n        output = subprocess.check_output([cuda_home + \"/bin/nvcc\", \"-V\"], universal_newlines=True)\n    except FileNotFoundError:\n        return f\"{RED} [FAIL] nvcc missing {END}\"\n    output_split = output.split()\n    release_idx = output_split.index(\"release\")\n    release = output_split[release_idx + 1].replace(',', '').split(\".\")\n    return \".\".join(release)\n\n\ndef installed_cann_path():\n    if \"ASCEND_HOME_PATH\" in os.environ or os.path.exists(os.environ[\"ASCEND_HOME_PATH\"]):\n        return os.environ[\"ASCEND_HOME_PATH\"]\n    return None\n\n\ndef installed_cann_version():\n    import re\n    ascend_path = installed_cann_path()\n    if ascend_path is None:\n        return f\"CANN_HOME does not exist, unable to compile NPU op(s)\"\n    cann_version = \"\"\n    for dirpath, _, filenames in os.walk(os.path.realpath(ascend_path)):\n        if cann_version:\n            break\n        install_files = [file for file in filenames if re.match(r\"ascend_.*_install\\.info\", file)]\n        if install_files:\n            filepath = os.path.join(dirpath, install_files[0])\n            with open(filepath, \"r\") as f:\n                for line in f:\n                    if line.find(\"version\") != -1:\n                        cann_version = line.strip().split(\"=\")[-1]\n                        break\n    return cann_version\n\n\ndef get_shm_size():\n    try:\n        shm_stats = os.statvfs('/dev/shm')\n    except (OSError, FileNotFoundError, ValueError):\n        return \"UNKNOWN\", None\n\n    shm_size = shm_stats.f_frsize * shm_stats.f_blocks\n    shm_hbytes = human_readable_size(shm_size)\n    warn = []\n    if shm_size < 512 * 1024**2:\n        warn.append(\n            f\" {YELLOW} [WARNING] /dev/shm size might be too small, if running in docker increase to at least --shm-size='1gb' {END}\"\n        )\n        if get_accelerator().communication_backend_name() == \"nccl\":\n            warn.append(\n                f\" {YELLOW} [WARNING] see more details about NCCL requirements: https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/troubleshooting.html#sharing-data {END}\"\n            )\n    return shm_hbytes, warn\n\n\ndef human_readable_size(size):\n    units = ['B', 'KB', 'MB', 'GB', 'TB']\n    i = 0\n    while size >= 1024 and i < len(units) - 1:\n        size /= 1024\n        i += 1\n    return f'{size:.2f} {units[i]}'\n\n\ndef debug_report():\n    max_dots = 33\n\n    report = [(\"torch install path\", torch.__path__), (\"torch version\", torch.__version__),\n              (\"deepspeed install path\", deepspeed.__path__),\n              (\"deepspeed info\", f\"{deepspeed.__version__}, {deepspeed.__git_hash__}, {deepspeed.__git_branch__}\")]\n    if get_accelerator().device_name() == 'cuda':\n        hip_version = getattr(torch.version, \"hip\", None)\n        report.extend([(\"torch cuda version\", torch.version.cuda), (\"torch hip version\", hip_version),\n                       (\"nvcc version\", (None if hip_version else nvcc_version())),\n                       (\"deepspeed wheel compiled w.\", f\"torch {torch_info['version']}, \" +\n                        (f\"hip {torch_info['hip_version']}\" if hip_version else f\"cuda {torch_info['cuda_version']}\"))\n                       ])\n    elif get_accelerator().device_name() == 'npu':\n        import torch_npu\n        report.extend([(\"deepspeed wheel compiled w.\", f\"torch {torch_info['version']}\"),\n                       (\"torch_npu install path\", torch_npu.__path__), (\"torch_npu version\", torch_npu.__version__),\n                       (\"ascend_cann version\", installed_cann_version())])\n    else:\n        report.extend([(\"deepspeed wheel compiled w.\", f\"torch {torch_info['version']} \")])\n\n    report.append((\"shared memory (/dev/shm) size\", get_shm_size()))\n\n    print(\"DeepSpeed general environment info:\")\n    for name, value in report:\n        warns = []\n        if isinstance(value, tuple):\n            value, warns = value\n        print(name, \".\" * (max_dots - len(name)), value)\n        if warns:\n            for warn in warns:\n                print(warn)\n\n\ndef parse_arguments():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--hide_operator_status',\n                        action='store_true',\n                        help='Suppress display of installation and compatibility statuses of DeepSpeed operators. ')\n    parser.add_argument('--hide_errors_and_warnings', action='store_true', help='Suppress warning and error messages.')\n    args = parser.parse_args()\n    return args\n\n\ndef main(hide_operator_status=False, hide_errors_and_warnings=False):\n    if not hide_operator_status:\n        op_report(verbose=not hide_errors_and_warnings)\n    debug_report()\n\n\ndef cli_main():\n    args = parse_arguments()\n    main(hide_operator_status=args.hide_operator_status, hide_errors_and_warnings=args.hide_errors_and_warnings)\n\n\nif __name__ == \"__main__\":\n    main()\n", "deepspeed/git_version_info.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\ntry:\n    #  This is populated by setup.py\n    from .git_version_info_installed import *  # noqa: F401 # type: ignore\nexcept ModuleNotFoundError:\n    import os\n    if os.path.isfile('version.txt'):\n        # Will be missing from checkouts that haven't been installed (e.g., readthedocs)\n        version = open('version.txt', 'r').read().strip()\n    else:\n        version = \"0.0.0\"\n    git_hash = '[none]'\n    git_branch = '[none]'\n\n    from .ops.op_builder.all_ops import ALL_OPS\n    installed_ops = dict.fromkeys(ALL_OPS.keys(), False)\n    accelerator_name = \"\"\n    torch_info = {'version': \"0.0\", \"cuda_version\": \"0.0\", \"hip_version\": \"0.0\"}\n\n# compatible_ops list is recreated for each launch\nfrom .ops.op_builder.all_ops import ALL_OPS\n\ncompatible_ops = dict.fromkeys(ALL_OPS.keys(), False)\nfor op_name, builder in ALL_OPS.items():\n    op_compatible = builder.is_compatible()\n    compatible_ops[op_name] = op_compatible\n    compatible_ops[\"deepspeed_not_implemented\"] = False\n", "deepspeed/pydantic_v1.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\"\"\"Pydantic v1 compatibility module.\n\nPydantic v2 introduced breaking changes that hinder its adoption:\nhttps://docs.pydantic.dev/latest/migration/. To provide deepspeed users the option to\nmigrate to pydantic v2 on their own timeline, deepspeed uses this compatibility module\nas a pydantic-version-agnostic alias for pydantic's v1 API.\n\"\"\"\n\ntry:\n    from pydantic.v1 import *  # noqa: F401\nexcept ImportError:\n    from pydantic import *  # noqa: F401\n", "deepspeed/constants.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport os\nfrom datetime import timedelta\n\n#############################################\n# Torch distributed constants\n#############################################\nTORCH_DISTRIBUTED_DEFAULT_PORT = 29500\n\n# Default process group wide timeout, if applicable.\n# This only applies to the gloo and nccl backends\n# (only if NCCL_BLOCKING_WAIT or NCCL_ASYNC_ERROR_HANDLING is set to 1).\n# To make an attempt at backwards compatibility with THD, we use an\n# extraordinarily high default timeout, given that THD did not have timeouts.\ndefault_pg_timeout = timedelta(minutes=int(os.getenv(\"DEEPSPEED_TIMEOUT\", default=30)))\nINFERENCE_GENERIC_MODE = 'generic'\nINFERENCE_SPECIALIZED_MODE = 'specialized'\n", "deepspeed/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport sys\nimport types\nimport json\nfrom typing import Optional, Union\nimport torch\nfrom torch.optim import Optimizer\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom packaging import version as pkg_version\n\n# Skip Triton import for AMD due to pytorch-triton-rocm module breaking device API in DeepSpeed\nif not (hasattr(torch.version, 'hip') and torch.version.hip is not None):\n    try:\n        import triton  # noqa: F401 # type: ignore\n        HAS_TRITON = True\n    except ImportError:\n        HAS_TRITON = False\nelse:\n    HAS_TRITON = False\n\nfrom . import ops\nfrom . import module_inject\n\nfrom .accelerator import get_accelerator\nfrom .constants import TORCH_DISTRIBUTED_DEFAULT_PORT\nfrom .runtime.engine import DeepSpeedEngine, DeepSpeedOptimizerCallable, DeepSpeedSchedulerCallable\nfrom .runtime.engine import ADAM_OPTIMIZER, LAMB_OPTIMIZER\nfrom .runtime.hybrid_engine import DeepSpeedHybridEngine\nfrom .runtime.pipe.engine import PipelineEngine\nfrom .inference.engine import InferenceEngine\nfrom .inference.config import DeepSpeedInferenceConfig\nfrom .runtime.lr_schedules import add_tuning_arguments\nfrom .runtime.config import DeepSpeedConfig, DeepSpeedConfigError\nfrom .runtime.activation_checkpointing import checkpointing\nfrom .ops.transformer import DeepSpeedTransformerLayer, DeepSpeedTransformerConfig\nfrom .module_inject import replace_transformer_layer, revert_transformer_layer\n\nfrom .utils import log_dist, OnDevice, logger\nfrom .comm.comm import init_distributed\n\nfrom .runtime import zero\nfrom .runtime.compiler import is_compile_supported\n\nfrom .pipe import PipelineModule\n\nfrom .git_version_info import version, git_hash, git_branch\n\n\ndef _parse_version(version_str):\n    '''Parse a version string and extract the major, minor, and patch versions.'''\n    ver = pkg_version.parse(version_str)\n    return ver.major, ver.minor, ver.micro\n\n\n# Export version information\n__version__ = version\n__version_major__, __version_minor__, __version_patch__ = _parse_version(__version__)\n__git_hash__ = git_hash\n__git_branch__ = git_branch\n\n# Set to torch's distributed package or deepspeed.comm based inside DeepSpeedEngine init\ndist = None\n\n\ndef initialize(args=None,\n               model: torch.nn.Module = None,\n               optimizer: Optional[Union[Optimizer, DeepSpeedOptimizerCallable]] = None,\n               model_parameters: Optional[torch.nn.Module] = None,\n               training_data: Optional[torch.utils.data.Dataset] = None,\n               lr_scheduler: Optional[Union[_LRScheduler, DeepSpeedSchedulerCallable]] = None,\n               distributed_port: int = TORCH_DISTRIBUTED_DEFAULT_PORT,\n               mpu=None,\n               dist_init_required: Optional[bool] = None,\n               collate_fn=None,\n               config=None,\n               config_params=None):\n    \"\"\"Initialize the DeepSpeed Engine.\n\n    Arguments:\n        args: an object containing local_rank and deepspeed_config fields.\n            This is optional if `config` is passed.\n\n        model: Required: nn.module class before apply any wrappers\n\n        optimizer: Optional: a user defined Optimizer or Callable that returns an Optimizer object.\n            This overrides any optimizer definition in the DeepSpeed json config.\n\n        model_parameters: Optional: An iterable of torch.Tensors or dicts.\n            Specifies what Tensors should be optimized.\n\n        training_data: Optional: Dataset of type torch.utils.data.Dataset\n\n        lr_scheduler: Optional: Learning Rate Scheduler Object or a Callable that takes an Optimizer and returns a Scheduler object.\n            The scheduler object should define a get_lr(), step(), state_dict(), and load_state_dict() methods\n\n        distributed_port: Optional: Master node (rank 0)'s free port that needs to be used for communication during distributed training\n\n        mpu: Optional: A model parallelism unit object that implements\n            get_{model,data}_parallel_{rank,group,world_size}()\n\n        dist_init_required: Optional: None will auto-initialize torch distributed if needed,\n            otherwise the user can force it to be initialized or not via boolean.\n\n        collate_fn: Optional: Merges a list of samples to form a\n            mini-batch of Tensor(s).  Used when using batched loading from a\n            map-style dataset.\n\n        config: Optional: Instead of requiring args.deepspeed_config you can pass your deepspeed config\n            as an argument instead, as a path or a dictionary.\n\n        config_params: Optional: Same as `config`, kept for backwards compatibility.\n\n    Returns:\n        A tuple of ``engine``, ``optimizer``, ``training_dataloader``, ``lr_scheduler``\n\n        * ``engine``: DeepSpeed runtime engine which wraps the client model for distributed training.\n\n        * ``optimizer``: Wrapped optimizer if a user defined ``optimizer`` is supplied, or if\n          optimizer is specified in json config else ``None``.\n\n        * ``training_dataloader``: DeepSpeed dataloader if ``training_data`` was supplied,\n          otherwise ``None``.\n\n        * ``lr_scheduler``: Wrapped lr scheduler if user ``lr_scheduler`` is passed, or\n          if ``lr_scheduler`` specified in JSON configuration. Otherwise ``None``.\n    \"\"\"\n    log_dist(\"DeepSpeed info: version={}, git-hash={}, git-branch={}\".format(__version__, __git_hash__,\n                                                                             __git_branch__),\n             ranks=[0])\n\n    # Disable zero.Init context if it's currently enabled\n    zero.partition_parameters.shutdown_init_context()\n\n    assert model is not None, \"deepspeed.initialize requires a model\"\n\n    global dist\n    from deepspeed import comm as dist\n    dist_backend = get_accelerator().communication_backend_name()\n    dist.init_distributed(dist_backend=dist_backend,\n                          distributed_port=distributed_port,\n                          dist_init_required=dist_init_required)\n\n    # Set config using config_params for backwards compat\n    if config is None and config_params is not None:\n        config = config_params\n\n    # Check for deepscale_config for backwards compat\n    if hasattr(args, \"deepscale_config\") and args.deepscale_config is not None:\n        logger.warning(\"************ --deepscale_config is deprecated, please use --deepspeed_config ************\")\n        if hasattr(args, \"deepspeed_config\"):\n            assert (args.deepspeed_config is\n                    None), \"Not sure how to proceed, we were given both a deepscale_config and deepspeed_config\"\n        args.deepspeed_config = args.deepscale_config\n        args.deepscale_config = None\n\n    # Check that we have only one config passed\n    if hasattr(args, \"deepspeed_config\") and args.deepspeed_config is not None:\n        assert config is None, \"Not sure how to proceed, we were given deepspeed configs in the deepspeed arguments and deepspeed.initialize() function call\"\n        config = args.deepspeed_config\n    assert config is not None, \"DeepSpeed requires --deepspeed_config to specify configuration file\"\n\n    if not isinstance(model, PipelineModule):\n        config_class = DeepSpeedConfig(config, mpu)\n        if config_class.hybrid_engine.enabled:\n            engine = DeepSpeedHybridEngine(args=args,\n                                           model=model,\n                                           optimizer=optimizer,\n                                           model_parameters=model_parameters,\n                                           training_data=training_data,\n                                           lr_scheduler=lr_scheduler,\n                                           mpu=mpu,\n                                           dist_init_required=dist_init_required,\n                                           collate_fn=collate_fn,\n                                           config=config,\n                                           config_class=config_class)\n        else:\n            engine = DeepSpeedEngine(args=args,\n                                     model=model,\n                                     optimizer=optimizer,\n                                     model_parameters=model_parameters,\n                                     training_data=training_data,\n                                     lr_scheduler=lr_scheduler,\n                                     mpu=mpu,\n                                     dist_init_required=dist_init_required,\n                                     collate_fn=collate_fn,\n                                     config=config,\n                                     config_class=config_class)\n    else:\n        assert mpu is None, \"mpu must be None with pipeline parallelism\"\n        mpu = model.mpu()\n        config_class = DeepSpeedConfig(config, mpu)\n        engine = PipelineEngine(args=args,\n                                model=model,\n                                optimizer=optimizer,\n                                model_parameters=model_parameters,\n                                training_data=training_data,\n                                lr_scheduler=lr_scheduler,\n                                mpu=mpu,\n                                dist_init_required=dist_init_required,\n                                collate_fn=collate_fn,\n                                config=config,\n                                config_class=config_class)\n\n    # Restore zero.Init context if necessary\n    zero.partition_parameters.restore_init_context()\n\n    return_items = [engine, engine.optimizer, engine.training_dataloader, engine.lr_scheduler]\n    return tuple(return_items)\n\n\ndef _add_core_arguments(parser):\n    r\"\"\"Helper (internal) function to update an argument parser with an argument group of the core DeepSpeed arguments.\n        The core set of DeepSpeed arguments include the following:\n        1) --deepspeed: boolean flag to enable DeepSpeed\n        2) --deepspeed_config <json file path>: path of a json configuration file to configure DeepSpeed runtime.\n\n        This is a helper function to the public add_config_arguments()\n\n    Arguments:\n        parser: argument parser\n    Return:\n        parser: Updated Parser\n    \"\"\"\n    group = parser.add_argument_group('DeepSpeed', 'DeepSpeed configurations')\n\n    group.add_argument('--deepspeed',\n                       default=False,\n                       action='store_true',\n                       help='Enable DeepSpeed (helper flag for user code, no impact on DeepSpeed backend)')\n\n    group.add_argument('--deepspeed_config', default=None, type=str, help='DeepSpeed json configuration file.')\n\n    group.add_argument('--deepscale',\n                       default=False,\n                       action='store_true',\n                       help='Deprecated enable DeepSpeed (helper flag for user code, no impact on DeepSpeed backend)')\n\n    group.add_argument('--deepscale_config',\n                       default=None,\n                       type=str,\n                       help='Deprecated DeepSpeed json configuration file.')\n\n    return parser\n\n\ndef add_config_arguments(parser):\n    r\"\"\"Update the argument parser to enabling parsing of DeepSpeed command line arguments.\n        The set of DeepSpeed arguments include the following:\n        1) --deepspeed: boolean flag to enable DeepSpeed\n        2) --deepspeed_config <json file path>: path of a json configuration file to configure DeepSpeed runtime.\n\n    Arguments:\n        parser: argument parser\n    Return:\n        parser: Updated Parser\n    \"\"\"\n    parser = _add_core_arguments(parser)\n\n    return parser\n\n\ndef default_inference_config():\n    \"\"\"\n        Return a default DeepSpeed inference configuration dictionary.\n    \"\"\"\n    return DeepSpeedInferenceConfig().dict()\n\n\ndef init_inference(model, config=None, **kwargs):\n    \"\"\"Initialize the DeepSpeed InferenceEngine.\n\n    Description: all four cases are valid and supported in DS init_inference() API.\n\n    # Case 1: user provides no config and no kwargs. Default config will be used.\n\n    .. code-block:: python\n\n        generator.model = deepspeed.init_inference(generator.model)\n        string = generator(\"DeepSpeed is\")\n        print(string)\n\n    # Case 2: user provides a config and no kwargs. User supplied config will be used.\n\n    .. code-block:: python\n\n        generator.model = deepspeed.init_inference(generator.model, config=config)\n        string = generator(\"DeepSpeed is\")\n        print(string)\n\n    # Case 3: user provides no config and uses keyword arguments (kwargs) only.\n\n    .. code-block:: python\n\n        generator.model = deepspeed.init_inference(generator.model,\n                                                    tensor_parallel={\"tp_size\": world_size},\n                                                    dtype=torch.half,\n                                                    replace_with_kernel_inject=True)\n        string = generator(\"DeepSpeed is\")\n        print(string)\n\n    # Case 4: user provides config and keyword arguments (kwargs). Both config and kwargs are merged and kwargs take precedence.\n\n    .. code-block:: python\n\n        generator.model = deepspeed.init_inference(generator.model, config={\"dtype\": torch.half}, replace_with_kernel_inject=True)\n        string = generator(\"DeepSpeed is\")\n        print(string)\n\n    Arguments:\n        model: Required: original nn.module object without any wrappers\n\n        config: Optional: instead of arguments, you can pass in a DS inference config dict or path to JSON file\n\n    Returns:\n        A deepspeed.InferenceEngine wrapped model.\n    \"\"\"\n    log_dist(\"DeepSpeed info: version={}, git-hash={}, git-branch={}\".format(__version__, __git_hash__,\n                                                                             __git_branch__),\n             ranks=[0])\n\n    # Load config_dict from config first\n    if config is None:\n        config = {}\n    if isinstance(config, str):\n        with open(config, \"r\") as f:\n            config_dict = json.load(f)\n    elif isinstance(config, dict):\n        config_dict = config\n    else:\n        raise ValueError(f\"'config' argument expected string or dictionary, got {type(config)}\")\n\n    # Update with values from kwargs, ensuring no conflicting overlap between config and kwargs\n    overlap_keys = set(config_dict.keys()).intersection(kwargs.keys())\n    # If there is overlap, error out if values are different\n    for key in overlap_keys:\n        if config_dict[key] != kwargs[key]:\n            raise ValueError(f\"Conflicting argument '{key}' in 'config':{config_dict[key]} and kwargs:{kwargs[key]}\")\n    config_dict.update(kwargs)\n\n    ds_inference_config = DeepSpeedInferenceConfig(**config_dict)\n\n    engine = InferenceEngine(model, config=ds_inference_config)\n\n    return engine\n", "deepspeed/autotuning/config.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom deepspeed.runtime.config_utils import get_scalar_param, get_dict_param, DeepSpeedConfigObject\nfrom deepspeed.autotuning.constants import *\n\n\nclass DeepSpeedAutotuningConfig(DeepSpeedConfigObject):\n\n    def __init__(self, param_dict):\n        super(DeepSpeedAutotuningConfig, self).__init__()\n\n        self.enabled = None\n        self.start_step = None\n        self.end_step = None\n        self.metric_path = None\n        self.arg_mappings = None\n        self.metric = None\n        self.model_info = None\n        self.results_dir = None\n        self.exps_dir = None\n        self.overwrite = None\n\n        if param_dict and AUTOTUNING in param_dict.keys():\n            autotuning_dict = param_dict[AUTOTUNING]\n        else:\n            autotuning_dict = {}\n\n        self._initialize(autotuning_dict)\n\n    def _initialize(self, autotuning_dict):\n        self.enabled = get_scalar_param(autotuning_dict, AUTOTUNING_ENABLED, AUTOTUNING_ENABLED_DEFAULT)\n\n        self.fast = get_scalar_param(autotuning_dict, AUTOTUNING_FAST, AUTOTUNING_FAST_DEFAULT)\n\n        self.results_dir = get_scalar_param(autotuning_dict, AUTOTUNING_RESULTS_DIR, AUTOTUNING_RESULTS_DIR_DEFAULT)\n        assert self.results_dir, \"results_dir cannot be empty\"\n        self.exps_dir = get_scalar_param(autotuning_dict, AUTOTUNING_EXPS_DIR, AUTOTUNING_EXPS_DIR_DEFAULT)\n        assert self.exps_dir, \"exps_dir cannot be empty\"\n        self.overwrite = get_scalar_param(autotuning_dict, AUTOTUNING_OVERWRITE, AUTOTUNING_OVERWRITE_DEFAULT)\n\n        self.start_profile_step = get_scalar_param(autotuning_dict, AUTOTUNING_START_PROFILE_STEP,\n                                                   AUTOTUNING_START_PROFILE_STEP_DEFAULT)\n\n        self.end_profile_step = get_scalar_param(autotuning_dict, AUTOTUNING_END_PROFILE_STEP,\n                                                 AUTOTUNING_END_PROFILE_STEP_DEFAULT)\n\n        self.metric = get_scalar_param(autotuning_dict, AUTOTUNING_METRIC, AUTOTUNING_METRIC_DEFAULT)\n\n        self.metric_path = get_scalar_param(autotuning_dict, AUTOTUNING_METRIC_PATH, AUTOTUNING_METRIC_PATH_DEFAULT)\n\n        self.tuner_type = get_scalar_param(autotuning_dict, AUTOTUNING_TUNER_TYPE, AUTOTUNING_TUNER_TYPE_DEFAULT)\n\n        self.tuner_early_stopping = get_scalar_param(autotuning_dict, AUTOTUNING_TUNER_EARLY_STOPPING,\n                                                     AUTOTUNING_TUNER_EARLY_STOPPING_DEFAULT)\n\n        self.tuner_num_trials = get_scalar_param(autotuning_dict, AUTOTUNING_TUNER_NUM_TRIALS,\n                                                 AUTOTUNING_TUNER_NUM_TRIALS_DEFAULT)\n\n        self.arg_mappings = get_dict_param(autotuning_dict, AUTOTUNING_ARG_MAPPINGS, AUTOTUNING_ARG_MAPPINGS_DEFAULT)\n\n        self.model_info = get_model_info_config(autotuning_dict)\n\n        self.model_info_path = get_scalar_param(autotuning_dict, AUTOTUNING_MODEL_INFO_PATH,\n                                                AUTOTUNING_MODEL_INFO_PATH_DEFAULT)\n        self.mp_size = get_scalar_param(autotuning_dict, AUTOTUNING_MP_SIZE, AUTOTUNING_MP_SIZE_DEFAULT)\n\n        self.max_train_batch_size = get_dict_param(autotuning_dict, AUTOTUNING_MAX_TRAIN_BATCH_SIZE,\n                                                   AUTOTUNING_MAX_TRAIN_BATCH_SIZE_DEFAULT)\n\n        self.min_train_batch_size = get_dict_param(autotuning_dict, AUTOTUNING_MIN_TRAIN_BATCH_SIZE,\n                                                   AUTOTUNING_MIN_TRAIN_BATCH_SIZE_DEFAULT)\n\n        self.max_train_micro_batch_size_per_gpu = get_dict_param(\n            autotuning_dict, AUTOTUNING_MAX_TRAIN_MICRO_BATCH_SIZE_PER_GPU,\n            AUTOTUNING_MAX_TRAIN_MICRO_BATCH_SIZE_PER_GPU_DEFAULT)\n\n        self.min_train_micro_batch_size_per_gpu = get_dict_param(\n            autotuning_dict, AUTOTUNING_MIN_TRAIN_MICRO_BATCH_SIZE_PER_GPU,\n            AUTOTUNING_MIN_TRAIN_MICRO_BATCH_SIZE_PER_GPU_DEFAULT)\n\n        self.num_tuning_micro_batch_sizes = get_dict_param(autotuning_dict, AUTOTUNING_NUM_TUNING_MICRO_BATCH_SIZES,\n                                                           AUTOTUNING_NUM_TUNING_MICRO_BATCH_SIZES_DEFAULT)\n\n\ndef get_model_info_config(param_dict):\n    if MODEL_INFO in param_dict and param_dict[MODEL_INFO] is not None:\n        model_info_config = {}\n        for key, default_value in MODEL_INFO_KEY_DEFAULT_DICT.items():\n            model_info_config[key] = get_scalar_param(param_dict[MODEL_INFO], key, default_value)\n        return model_info_config\n    return None\n\n\ndef get_default_model_info_config():\n    return MODEL_INFO_KEY_DEFAULT_DICT\n", "deepspeed/autotuning/autotuner.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport shutil\nimport subprocess\nimport time\nimport datetime\nimport math\nimport hjson\n\nfrom ..runtime.config_utils import dict_raise_error_on_duplicate_keys\nfrom ..runtime.constants import *\n\nfrom ..runtime.zero.config import ZERO_OPTIMIZATION, ZeroStageEnum\nfrom ..utils import logger\nfrom .config import DeepSpeedAutotuningConfig\nfrom .constants import *\nfrom .scheduler import ResourceManager\nfrom .tuner import GridSearchTuner, RandomTuner, ModelBasedTuner\nfrom .utils import *\nfrom deepspeed.accelerator import get_accelerator\n\ntry:\n    from tabulate import tabulate\nexcept ImportError:\n    tabulate = None\n\ntry:\n    import mlflow\n    has_mlflow = True\nexcept Exception as e:\n    has_mlflow = False\n\nZERO_OPTIMIZATION_STAGE = \"stage\"\nOFFLOAD_OPTIMIZER = \"offload_optimizer\"\nOFFLOAD_PARAM = \"offload_param\"\nZERO_OPTIMIZATION_STAGE_DEFAULT = ZeroStageEnum.disabled\n\n\nclass Autotuner:\n    \"\"\"The DeepSpeed Autotuner automatically discovers the optimal DeepSpeed configuration that delivers good training speed. The Autotuner uses model information, system information, and heuristics to efficiently tune system knobs that affect compute and memory efficiencies, such as ZeRO optimization stages, micro-batch sizes, and many other ZeRO optimization configurations. It not only reduces the time and resources user spend on tuning, but also can discover configurations better than hand-tuned methods.\n    Autotuning with DeepSpeed requires no code change from DeepSpeed users. Please refer to the README for usage details.\n    \"\"\"\n\n    def __init__(self, args, active_resources):\n        self.args = args\n        self.selected_exp_dir = None\n\n        assert tabulate is not None, \"Missing required package `tabulate`, please install with `pip install deepspeed[autotuning]`.\"\n\n        logger.debug(f\"autotuning args={args}\")\n\n        self.user_config = self._get_user_config(args.user_args)\n        assert self.user_config is not None, \"DeepSpeed configuration is not provided\"\n\n        self.autotuning_config = DeepSpeedAutotuningConfig(self.user_config)\n        if self.user_config[AUTOTUNING]:\n            if AUTOTUNING_EXPS_DIR in self.user_config[AUTOTUNING].keys():\n                del self.user_config[AUTOTUNING][AUTOTUNING_EXPS_DIR]\n            if AUTOTUNING_RESULTS_DIR in self.user_config[AUTOTUNING].keys():\n                del self.user_config[AUTOTUNING][AUTOTUNING_RESULTS_DIR]\n\n        self.exps_dir = self.autotuning_config.exps_dir\n        if self.autotuning_config.overwrite and os.path.exists(self.exps_dir):\n            shutil.rmtree(self.exps_dir, ignore_errors=True)\n        if not os.path.exists(self.exps_dir):\n            try:\n                os.makedirs(self.exps_dir, exist_ok=True)\n                logger.info(f\"Created autotuning experiments directory: {self.exps_dir}\")\n            except:\n                logger.error(\n                    f\"Failed to create {self.exps_dir}, please check `exps_dir` in the autotuning config file is accessible by all the nodes in the job.\"\n                )\n                exit(-1)\n\n        self.results_dir = self.autotuning_config.results_dir\n        if self.autotuning_config.overwrite and os.path.exists(self.results_dir):\n            shutil.rmtree(self.results_dir, ignore_errors=True)\n        if not os.path.exists(self.results_dir):\n            try:\n                os.makedirs(self.results_dir, exist_ok=True)\n                logger.info(f\"Created autotuning results directory: {self.exps_dir}\")\n            except:\n                logger.error(\n                    f\"Failed to create {self.results_dir}, please check `results_dir` in the autotuning config file is accessible by all the nodes in the job.\"\n                )\n                exit(-1)\n\n        # set the active resource for the autotuner resource manager\n        self.rm = self._get_resource_manager(active_resources)\n\n        # get resource requirement for each autotuning experiment\n        self.exp_num_nodes, self.exp_num_gpus = self._get_exp_resources(args)\n\n        assert self.exp_num_gpus <= self.rm.num_gpus_per_node, \"num_gpus in the autotuning configuration must not be less than the --num_gpus value in the train script if any\"\n        assert self.exp_num_nodes <= len(\n            self.rm.nodes\n        ), \"num_nodes in the autotuning configuration must not be less than the --num_nodes value in the train script if any\"\n\n        self.records = {}\n        self.optimal_cmd = None\n        self.optimal_ds_config = None\n\n        self.mlflow_parent_id = None\n\n    def print_tuning_results(self):\n        \"\"\"Print the autotuning results in tabular format.\n        \"\"\"\n        best_space_records = self.get_best_space_records()\n        tab = []\n        if best_space_records:\n            for key, val in best_space_records.items():\n                if not val:\n                    continue\n                row = []\n                row.append(key)\n                num_exps = 0\n                if key == GLOBAL_TUNING_SPACE:\n                    cnt = 0\n                    for k, v in best_space_records.items():\n                        if k != GLOBAL_TUNING_SPACE:\n                            cnt += v[2]\n                    num_exps = cnt\n                else:\n                    num_exps = val[2]\n                row.append(num_exps)\n                row.append(val[1])\n                row.append(val[0]['name'])\n                tab.append(row)\n            summary = tabulate(tab,\n                               headers=[\"tuning_space\", \"num_experiments\", \"best_metric_val\", \"best_exp_name\"],\n                               tablefmt=\"pipe\")\n            print(summary)\n            with open(os.path.join(self.results_dir, 'summary.txt'), 'w', buffering=BUFSIZE) as fd:\n                fd.write(summary)\n                fd.flush()\n                os.fsync(fd)\n\n        if GLOBAL_TUNING_SPACE in best_space_records:\n            best_exp, best_metric_val, total_num_exps = best_space_records[GLOBAL_TUNING_SPACE]\n            if best_exp:\n                logger.info(\n                    f\"{best_exp['name']} is the optimal setup after tuning. The exp result is at {best_exp['result_dir']}.\"\n                )\n            else:\n                logger.info(f\"No optimal setup is found. Please check that experiments were run successfully.\")\n            tuning_duration = datetime.timedelta(seconds=(time.time() - self.start_time))\n\n            logger.info(f\"Tuning completed in {tuning_duration}\")\n            with open(os.path.join(self.results_dir, 'summary.txt'), 'a') as f:\n                f.write(\n                    f\"\\n\\nTuning completed in {tuning_duration}. Total number of experiments: {self.rm.experiment_count - 1}.\"\n                )\n                f.flush()\n\n    def _get_user_config(self, user_args):\n        \"\"\"Get DeepSpeed configuration from the user arguments passed to the launcher.\n\n        Args:\n            user_args ([list]): user arguments passed to the DeepSpeed launcher\n\n        Returns:\n            [dict]: DeepSpeed configuration dictionary\n        \"\"\"\n        user_config_file = None\n        if \"--deepspeed_config\" in user_args:\n            idx = user_args.index(\"--deepspeed_config\")\n            assert \".json\" in user_args[\n                idx + 1], \"DeepSpeed --deepspeed_config requires a json file to specify the configuration\"\n\n            user_config_file = user_args[idx + 1]\n        elif \"--deepspeed\" in user_args:\n            idx = user_args.index(\"--deepspeed\")\n            if \".json\" in user_args[idx + 1]:\n                user_config_file = user_args[idx + 1]\n\n        logger.debug(f\"user_config_file = {user_config_file}\")\n        if user_config_file is not None:\n            assert os.path.isfile(user_config_file), \"DeepSpeed configuration file: {} is not an existing file\".format(\n                user_config_file)\n            if os.path.exists(user_config_file):\n                return json.load(open(user_config_file, \"r\"), object_pairs_hook=dict_raise_error_on_duplicate_keys)\n\n        return None\n\n    def _get_resource_manager(self, active_resources):\n        \"\"\"Initialize and return a resource manager\n\n        Args:\n            active_resources ([dict]): A dictionary of hostname and its slots (GPUs), e.g. {\"worker-0\": \"0,1,2,3,4,5,6,7,8\"}\n\n        Raises:\n            RuntimeError: raises the error if no GPU is available\n\n        Returns:\n            [ResourceManager]: A resource manager that schedules and runs autotuning experiments.\n        \"\"\"\n        logger.info(f\"active_resources = {active_resources}\")\n\n        hosts = []\n        ngpus_per_node = 100\n        for hostname, slots in active_resources.items():\n            hosts.append(hostname)\n            ngpus_per_node = min(len(slots), ngpus_per_node)\n\n        assert ngpus_per_node > 0, \"no gpu is available\"\n\n        return ResourceManager(args=self.args,\n                               hosts=hosts,\n                               num_gpus_per_node=ngpus_per_node,\n                               results_dir=self.results_dir,\n                               exps_dir=self.exps_dir,\n                               arg_mappings=self.autotuning_config.arg_mappings)\n\n    def _get_exp_resources(self, args):\n        \"\"\"Get resource requirement for each autotuning experiment\n\n        Args:\n            args (dict): user args\n\n        Returns:\n            num_nodes, num_gpus: the number of gpus and number of nodes used in the autotuning experiments\n        \"\"\"\n        if args.num_nodes > 0:\n            num_nodes = args.num_nodes\n        else:\n            num_nodes = len(self.rm.nodes)\n\n        if args.num_gpus > 0:\n            num_gpus = args.num_gpus\n        else:\n            num_gpus = self.rm.num_gpus_per_node\n\n        return num_nodes, num_gpus\n\n    def metric(self):\n        return self.autotuning_config.metric\n\n    def fast_enabled(self):\n        return self.autotuning_config.fast\n\n    def max_train_batch_size(self):\n        return self.autotuning_config.max_train_batch_size\n\n    def mp_size(self):\n        return self.autotuning_config.mp_size\n\n    def max_train_micro_batch_size_per_gpu(self):\n        if self.max_train_batch_size(\n        ) and self.max_train_batch_size() > 0:  # if the user specifies a max_train_batch_size\n            max_train_micro_batch_size = self.max_train_batch_size() * self.mp_size() // (\n                self.exp_num_gpus * self.exp_num_nodes)  # gradient accumulation steps >=1\n            return min(self.autotuning_config.max_train_micro_batch_size_per_gpu, max_train_micro_batch_size)\n        else:\n            return self.autotuning_config.max_train_micro_batch_size_per_gpu\n\n    def min_train_micro_batch_size_per_gpu(self):\n        return self.autotuning_config.min_train_micro_batch_size_per_gpu\n\n    def num_tuning_micro_batch_sizes(self):\n        return self.autotuning_config.num_tuning_micro_batch_sizes\n\n    def fp16_enabled(self):\n        if FP16 in self.user_config.keys():\n            return self.user_config[FP16].get(FP16_ENABLED, FP16_ENABLED_DEFAULT)\n        else:\n            return False\n\n    def get_gpu_memory_info(self):\n        return get_accelerator().total_memory()\n\n    def get_activation_memory_per_gpu(self):\n        if self.model_info and \"activation_mem_per_gpu\" in self.model_info:\n            return self.model_info[\"activation_mem_per_gpu\"]\n\n    def get_instantiation_memory_required_per_gpu(self, zero_stage):\n        num_params = self.get_model_num_params()\n        total_gpus = self.exp_num_nodes * self.exp_num_gpus\n        fp16_enabled = self.fp16_enabled()\n\n        if not num_params:\n            return 0\n        # assume the model uses Adam optimizer\n        # ZeroStageEnum.disabled:\n        params_mem = num_params * (2 if fp16_enabled else 4)\n        gradients_mem = num_params * (2 if fp16_enabled else 4)\n        optimizer_mem = num_params * (16 if fp16_enabled else 8)\n\n        if zero_stage >= ZeroStageEnum.optimizer_states:\n            optimizer_mem = optimizer_mem / total_gpus\n\n        if zero_stage >= ZeroStageEnum.gradients:\n            gradients_mem = gradients_mem / total_gpus\n\n        if zero_stage >= ZeroStageEnum.weights:\n            params_mem = params_mem / total_gpus\n\n        mem_per_gpu = (params_mem + gradients_mem + optimizer_mem) / self.mp_size()\n\n        return mem_per_gpu\n\n    def _generate_experiments(self, tuning_space, max_train_batch_size_per_gpu):\n        \"\"\"Generates a list of autotuning experiments given a tuning_space.\n            The corresponding parameter values are replaced by user-defined values in the DeepSpeed configuration file.\n        Args:\n            tuning_space ([dict]): A DeepSpeed configuration dictionary where a value can be a list (called a tuning parameter). For example,\n                {\n                    \"zero_optimization\": {\n                        \"stage\": 1,\n                        \"reduce_bucket_size\": [5e7,\n                                            5e8,\n                                            1e9],\n                        \"allgather_bucket_size\": [5e7,\n                                                5e8,\n                                                1e9],\n                    }\n                }\n                reduce_bucket_size and allgather_bucket_size are the tuning parameters in this tuning space.\n        Returns:\n            [list]: a list of experiments generated by taking combinations of values of the tuning space. The above tuning space generates 3*3 = 9 experiments if the user DeepSpeed configuration file does not overwrite the two tuning parameters or define more tuning parameters.\n        \"\"\"\n        exps = []\n\n        # each zero stage uses a different template configuration file\n        config_zero = tuning_space.get(ZERO_OPTIMIZATION, {})\n        stage = config_zero.get(ZERO_OPTIMIZATION_STAGE, ZERO_OPTIMIZATION_STAGE_DEFAULT)\n        template_config = {}\n        if stage == 0:\n            template_path = DEFAULT_TEMPLATE_PATH_ZERO_0\n            template_config = hjson.load(open(template_path, 'r'))\n            prefix = \"z0_\"\n\n        elif stage == 1:\n            template_path = DEFAULT_TEMPLATE_PATH_ZERO_1\n            template_config = hjson.load(open(template_path, 'r'))\n            prefix = \"z1_\"\n\n        elif stage == 2:\n            template_path = DEFAULT_TEMPLATE_PATH_ZERO_2\n            template_config = hjson.load(open(template_path, 'r'))\n            prefix = \"z2_\"\n\n        elif stage == 3:\n            template_path = DEFAULT_TEMPLATE_PATH_ZERO_3\n            template_config = hjson.load(open(template_path, 'r'))\n            model_info = self.model_info\n            if model_info and \"hidden_size\" in model_info:\n                hs = model_info[\"hidden_size\"]\n                template_config[ZERO_OPTIMIZATION]['reduce_bucket_size'] = hs * hs\n                template_config[ZERO_OPTIMIZATION]['stage3_prefetch_bucket_size'] = 0.9 * hs * hs\n                template_config[ZERO_OPTIMIZATION]['stage3_param_persistence_threshold'] = 10 * hs\n            prefix = \"z3_\"\n        else:\n            return exps\n\n        # replace the corresponding parameter values if the user specifies them in the DeepSpeed configuration file\n        replace_dict(tuning_space, self.user_config, [ZERO_OPTIMIZATION, TRAIN_MICRO_BATCH_SIZE_PER_GPU])\n\n        logger.debug(f\"tuning_space = {json.dumps(tuning_space)}\")\n\n        all_configs = get_all_configs(tuning_space, ignore_keys=[\"optimizer\"])\n\n        tuning_keys = get_tuning_keys(tuning_space)\n\n        logger.debug(f\"tuning_keys = {tuning_keys}\")\n\n        logger.debug(f\"before pruning total configs = {len(all_configs)}\")\n\n        pruned_list = prune_configs(all_configs)\n\n        logger.debug(f\"after pruning total configs = {len(pruned_list)}\")\n\n        for config in pruned_list:\n            exp_config = copy.deepcopy(template_config)\n            # fill the template with the expr config\n            replace_dict(exp_config, config)\n\n            # if the config does not use offloading, remove the offloading section\n            config_zero = config.get(ZERO_OPTIMIZATION, None)\n            if config_zero:\n                if OFFLOAD_OPTIMIZER not in config_zero and OFFLOAD_OPTIMIZER in exp_config[ZERO_OPTIMIZATION]:\n                    del exp_config[ZERO_OPTIMIZATION][OFFLOAD_OPTIMIZER]\n                if OFFLOAD_PARAM not in config_zero and OFFLOAD_PARAM in exp_config[ZERO_OPTIMIZATION]:\n                    del exp_config[ZERO_OPTIMIZATION][OFFLOAD_PARAM]\n            # set gradient accumulation steps according to max_train_batch_size_per_gpu\n            mbs = exp_config[TRAIN_MICRO_BATCH_SIZE_PER_GPU]\n            gas = max_train_batch_size_per_gpu // mbs\n            exp_config[GRADIENT_ACCUMULATION_STEPS] = gas\n            exp_config[TRAIN_BATCH_SIZE] = mbs * gas * \\\n                self.exp_num_gpus * self.exp_num_nodes // self.mp_size()\n            exp = {}\n            # generate the expr name\n            exp_name = canonical_name(exp_config, tuning_keys, prefix)\n            exp['name'] = exp_name\n            exp[DS_CONFIG] = exp_config\n            exp['num_gpus'] = self.exp_num_gpus\n            exp['num_nodes'] = self.exp_num_nodes\n            exps.append(exp)\n\n        return exps\n\n    def tune(self):\n        \"\"\" Tunes Zero stages, micro batch size per GPU, and other Zero configurations. Performance metrics of different tuning spaces are recorded in self.records.\n        \"\"\"\n        if has_mlflow:\n            self.mlflow_parent_id = os.environ['MLFLOW_RUN_ID']\n            mlflow.start_run(run_id=self.mlflow_parent_id)\n\n        self.start_time = time.time()\n        if self.fast_enabled():\n            logger.info(f\"Fast mode is enabled. Tuning micro batch size only.\")\n\n        # model info profile run with DEFAULT_MIN_MEM_CONFIG\n        model_info = self.model_info_profile_run()\n        if model_info:\n            self.model_info = model_info\n        else:\n            return\n\n        logger.info(f\"The model has {number_to_string(self.get_model_num_params())} parameters.\")\n\n        self.gpu_mem = self.get_gpu_memory_info()\n        logger.info(f\"Memory per GPU in the system is {memory_to_string(self.gpu_mem, postfix='B')}.\")\n\n        self.activation_mem = self.get_activation_memory_per_gpu()\n        logger.info(\n            f\"The model requires at least {memory_to_string(self.activation_mem, postfix='B')} activation memory for micro batch size 1.\"\n        )\n\n        stage = self.user_config.get(ZERO_OPTIMIZATION, {}).get(ZERO_OPTIMIZATION_STAGE, 0)\n\n        user_zero_stages = [stage] if not isinstance(stage, list) else stage\n        logger.info(f\"User-defined zero stages are {stage}.\")\n\n        mbs = 0\n        max_mbs = 0\n        metric_val = 0\n\n        required_gpu_mem = self.get_instantiation_memory_required_per_gpu(ZeroStageEnum.disabled) + self.activation_mem\n        if self.gpu_mem > required_gpu_mem:\n            if \"all\" in user_zero_stages or ZeroStageEnum.disabled in user_zero_stages:\n                logger.info(\n                    f\"The model might be runable with ZERO 0 (which requires at least {memory_to_string(required_gpu_mem, postfix='B')} memory with mbs = 1), adding DEFAULT_TUNING_SPACE_ZERO_0 to the global tuning space\"\n                )\n                next_max_mbs, next_mbs, next_metric_val = self.tune_space(DEFAULT_TUNING_SPACE_ZERO_0)\n                if next_mbs > mbs:\n                    mbs = next_mbs\n                    max_mbs = next_max_mbs\n                    metric_val = next_metric_val\n                if has_mlflow:\n                    mlflow.log_metric(f\"z0{self.metric()}\", next_metric_val)\n        else:\n            logger.info(\n                f\"The model is not runable with ZERO stage {ZeroStageEnum.disabled} (which requires at least {memory_to_string(required_gpu_mem, postfix='B')} memory with mbs = 1)\"\n            )\n\n        required_gpu_mem = self.get_instantiation_memory_required_per_gpu(\n            ZeroStageEnum.optimizer_states) + self.activation_mem\n        if self.gpu_mem > required_gpu_mem:\n            if \"all\" in user_zero_stages or ZeroStageEnum.optimizer_states in user_zero_stages:\n                logger.info(\n                    f\"The model might be runable with ZERO 1 (which requires at least {memory_to_string(required_gpu_mem, postfix='B')} memory), adding DEFAULT_TUNING_SPACE_ZERO_1 to the global tuning space\"\n                )\n                next_max_mbs, next_mbs, next_metric_val = self.tune_space(DEFAULT_TUNING_SPACE_ZERO_1,\n                                                                          prev_max_mbs=max_mbs,\n                                                                          prev_best_mbs=mbs,\n                                                                          prev_best_metric_val=metric_val)\n                if next_mbs > mbs:\n                    mbs = next_mbs\n                    max_mbs = next_max_mbs\n                    metric_val = next_metric_val\n                if has_mlflow:\n                    mlflow.log_metric(f\"z1{self.metric()}\", next_metric_val)\n        else:\n            logger.info(\n                f\"The model is not runable with ZERO stage {ZeroStageEnum.optimizer_states} (which requires at least {memory_to_string(required_gpu_mem, postfix='B')} memory with mbs = 1)\"\n            )\n\n        required_gpu_mem = self.get_instantiation_memory_required_per_gpu(\n            ZeroStageEnum.gradients) + self.activation_mem\n        if self.gpu_mem > required_gpu_mem:\n            if \"all\" in user_zero_stages or ZeroStageEnum.gradients in user_zero_stages:\n                logger.info(\n                    f\"The model might be runable with ZERO 2 (which requires at least {memory_to_string(required_gpu_mem, postfix='B')} memory), adding DEFAULT_TUNING_SPACE_ZERO_2 to the global tuning space\"\n                )\n                next_max_mbs, next_mbs, next_metric_val = self.tune_space(DEFAULT_TUNING_SPACE_ZERO_2,\n                                                                          prev_max_mbs=max_mbs,\n                                                                          prev_best_mbs=mbs,\n                                                                          prev_best_metric_val=metric_val)\n                if next_mbs > mbs:\n                    mbs = next_mbs\n                    max_mbs = next_max_mbs\n                    metric_val = next_metric_val\n                if has_mlflow:\n                    mlflow.log_metric(f\"z2{self.metric()}\", next_metric_val)\n        else:\n            logger.info(\n                f\"The model is not runable with ZERO stage {ZeroStageEnum.gradients} (which requires at least {memory_to_string(required_gpu_mem, postfix='B')} memory with mbs = 1)\"\n            )\n\n        required_gpu_mem = self.get_instantiation_memory_required_per_gpu(ZeroStageEnum.weights) + self.activation_mem\n        if self.gpu_mem > required_gpu_mem:\n            if \"all\" in user_zero_stages or ZeroStageEnum.weights in user_zero_stages:\n                logger.info(\n                    f\"The model might be runable with ZERO 3 (which requires at least {memory_to_string(required_gpu_mem, postfix='B')} memory), adding DEFAULT_TUNING_SPACE_ZERO_3 to the global tuning space\"\n                )\n                _, _, next_metric_val = self.tune_space(DEFAULT_TUNING_SPACE_ZERO_3,\n                                                        prev_max_mbs=max_mbs,\n                                                        prev_best_mbs=mbs,\n                                                        prev_best_metric_val=metric_val)\n                if has_mlflow:\n                    mlflow.log_metric(f\"z3{self.metric()}\", next_metric_val)\n        else:\n            logger.info(\n                f\"The model has {self.get_model_num_params()} parameters and requires at least {memory_to_string(required_gpu_mem, postfix='B')} memory per GPU with DeepSpeed Zero stage {ZeroStageEnum.weights} optimization. Memory per GPU in system is {memory_to_string(self.gpu_mem)}. No tuning is performed.\"\n            )\n            return\n        if has_mlflow:\n            mlflow.end_run()\n\n    def tune_space(self, tuning_space, prev_max_mbs=0, prev_best_mbs=0, prev_best_metric_val=0):\n        config_zero = tuning_space.get(ZERO_OPTIMIZATION, {})\n        stage = config_zero.get(ZERO_OPTIMIZATION_STAGE, None)\n        tuning_space_name = TUNING_MICRO_BATCH_SIZE_PREFIX + str(stage)\n        tuning_micro_batch_sizes = []\n        max_train_batch_size_per_gpu = 0\n        tuning_micro_batch_sizes_overwritten = False\n\n        # calculate max micro batch size using gpu memory, model instantiation memory and activation memory\n        # calculated_max_micro_batch_size = (memory_per_gpu - instantiation_memory) // activation_memory_micro_batch_size_1\n        calculated_max_micro_batch_size = int(\n            self.gpu_mem - self.get_instantiation_memory_required_per_gpu(stage)) // self.activation_mem\n        logger.info(\n            f\"Start tuning for space {tuning_space_name}, calculated_max_micro_batch_size = {calculated_max_micro_batch_size}\"\n        )\n\n        if calculated_max_micro_batch_size < prev_max_mbs:\n            logger.info(f\"No need to tune Zero stage {stage}. End tuning for space {tuning_space_name}\")\n            return 0, 0, 0\n\n        if TRAIN_MICRO_BATCH_SIZE_PER_GPU in self.user_config and isinstance(\n                self.user_config[TRAIN_MICRO_BATCH_SIZE_PER_GPU], list):\n            # user-specified micro batch size per gpu is a list which overwrites the default tuning behavior\n            tuning_micro_batch_sizes = [\n                s for s in self.user_config[TRAIN_MICRO_BATCH_SIZE_PER_GPU] if isinstance(s, int)\n            ]\n            gas = self.get_gas_from_user_config()\n            min_micro_batch_size = min(tuning_micro_batch_sizes)\n            max_micro_batch_size = max(tuning_micro_batch_sizes)\n            max_train_batch_size_per_gpu = max_micro_batch_size * gas\n            tuning_micro_batch_sizes_overwritten = True\n        else:\n            # auto-detects the list of micro batch sizes to tune\n            min_micro_batch_size, max_micro_batch_size = self.get_min_max_micro_batch_size(\n                stage, prev_max_mbs, calculated_max_micro_batch_size)\n\n            if max_micro_batch_size < prev_max_mbs:\n                logger.info(f\"No need to tune Zero stage {stage}. End tuning for space {tuning_space_name}\")\n                return 0, 0, 0\n\n            tuning_micro_batch_sizes, max_train_batch_size_per_gpu = self.get_tuning_micro_batch_size_list(\n                min_micro_batch_size,\n                max_micro_batch_size,\n                num_tuning_micro_batch_sizes=self.num_tuning_micro_batch_sizes())\n\n        logger.info(\n            f\"tuning_micro_batch_sizes = {tuning_micro_batch_sizes}, max_train_batch_size_per_gpu = {max_train_batch_size_per_gpu}\"\n        )\n\n        # return if the tuning_micro_batch_sizes list is empty\n        if not tuning_micro_batch_sizes:\n            logger.info(f\"End tuning for space {tuning_space_name}\")\n            return 0, 0, 0\n\n        # tune micro batch sizes and gradient accumulation steps given max_train_batch_size_per_gpu\n        tuning_micro_batch_sizes = self.run_tuning_micro_batch_sizes(tuning_micro_batch_sizes,\n                                                                     max_train_batch_size_per_gpu,\n                                                                     min_micro_batch_size, stage,\n                                                                     tuning_micro_batch_sizes_overwritten)\n\n        fast_best_record = self.get_best_space_record(tuning_space_name)\n        fast_best_metric_val = fast_best_record[1] if fast_best_record else 0\n        fast_best_mbs = fast_best_record[0][DS_CONFIG][TRAIN_MICRO_BATCH_SIZE_PER_GPU] if fast_best_record else 0\n        logger.info(f\"fast_best_mbs = {fast_best_mbs}, name = {fast_best_record[0]['name']}\")\n\n        if self.fast_enabled() or stage == 0:\n            logger.info(f\"End tuning for space: {tuning_space_name}\")\n            return max_micro_batch_size, fast_best_mbs, fast_best_metric_val\n\n        # if the best metric or the micro batch size for that best metric in the current Zero stage after tuning micro batch size is less than the corresponding value in the previous Zero stage, return, do not tune other Zero configuration parameters\n        if stage > 0:\n            if fast_best_mbs <= prev_best_mbs or fast_best_metric_val < prev_best_metric_val:\n                logger.info(\n                    f\"End tuning for space: {tuning_space_name}. No need to tune other Zero configuration parameters.\")\n                return max_micro_batch_size, fast_best_mbs, fast_best_metric_val\n\n        tuning_space[TRAIN_MICRO_BATCH_SIZE_PER_GPU] = tuning_micro_batch_sizes\n        tuning_space_name = canonical_name(tuning_space,\n                                           tuning_keys=get_tuning_keys(tuning_space),\n                                           prefix=\"z\" + str(stage) + \"_\",\n                                           omit_val=True)\n\n        logger.info(f'Tuning space is {tuning_space}')\n        logger.info(f'Tuning space name is {tuning_space_name}')\n\n        exps = self._generate_experiments(tuning_space, max_train_batch_size_per_gpu)\n\n        logger.info(f'Tuner type is {self.autotuning_config.tuner_type}')\n        if self.autotuning_config.tuner_type == AUTOTUNING_TUNER_MODELBASED:\n            t = ModelBasedTuner(exps, self.rm, self.metric(), tuning_space)\n        elif self.autotuning_config.tuner_type == AUTOTUNING_TUNER_RANDOM:\n            t = RandomTuner(exps, self.rm, self.metric())\n        else:\n            t = GridSearchTuner(exps, self.rm, self.metric())\n\n        sample_size = len(self.rm.nodes) * self.rm.num_gpus_per_node // (self.exp_num_gpus * self.exp_num_nodes)\n        num_exps = t.tune(sample_size=sample_size,\n                          n_trials=self.autotuning_config.tuner_num_trials,\n                          early_stopping=self.autotuning_config.tuner_early_stopping)\n        exp = t.best_exp\n        metric_val = t.best_metric_val\n        if exp:\n            self.update_records(tuning_space_name, exp, metric_val, num_exps)\n\n        full_best_record = self.get_best_space_record(tuning_space_name)\n        full_best_metric_val = full_best_record[1] if full_best_record else -1\n\n        if full_best_metric_val > fast_best_metric_val:\n            best_metric_val = full_best_metric_val\n            best_mbs = full_best_record[0][DS_CONFIG][TRAIN_MICRO_BATCH_SIZE_PER_GPU] if full_best_record else -1\n        else:\n            best_metric_val = fast_best_metric_val\n            best_mbs = fast_best_mbs\n\n        logger.info(f\"End tuning for space: {tuning_space_name}\")\n        return max_micro_batch_size, best_mbs, best_metric_val\n\n    def get_plateau_mbs(self, tuning_space_name):\n        if tuning_space_name not in self.records:\n            return 0\n        space_records = self.records[tuning_space_name]\n        sorted_space_records = sorted(space_records, key=lambda x: x[0][DS_CONFIG][TRAIN_MICRO_BATCH_SIZE_PER_GPU])\n        prev_metric_val = None\n        prev_micro_batch_size = 0\n        for (exp, metric_val, _) in sorted_space_records:\n            if prev_metric_val:\n                if metric_val < prev_metric_val:\n                    break\n                if (metric_val >= prev_metric_val\n                        and (metric_val - prev_metric_val) / prev_metric_val < METRIC_PERCENT_DIFF_CONST):\n                    break\n            prev_metric_val = metric_val\n            prev_micro_batch_size = exp[DS_CONFIG][TRAIN_MICRO_BATCH_SIZE_PER_GPU]\n        plateau_mbs = prev_micro_batch_size\n        return plateau_mbs\n\n    def get_model_num_params(self):\n        if self.model_info and \"num_params\" in self.model_info:\n            return self.model_info[\"num_params\"]\n\n    def model_info_profile_run(self):\n        \"\"\"Does a model information profiling experiment that collects the number of model parameters and activation memory.\\\n            The experiment produces a \"profile_model_info\" folder under self.results_dir.\n        Returns:\n            [dict]: a model information dictionary, e.g., {\"num_params\": 335144976, \"trainable_num_params\": 335144976, \"activation_mem_per_gpu\": 324358144, \"rank\": 0}\n        \"\"\"\n        logger.info(\"Starting model info profile run.\")\n        model_info = self.autotuning_config.model_info\n        if model_info and MODEL_INFO_NUM_PARAMS in model_info:\n            return model_info\n\n        ds_config = copy.deepcopy(self.user_config)\n        replace_dict(ds_config, DEFAULT_MIN_MEM_CONFIG)\n\n        model_info_path = os.path.join(self.results_dir, \"profile_model_info\", \"model_info.json\")\n        ds_config[AUTOTUNING] = {\"enabled\": True, \"model_info_path\": model_info_path, \"model_info\": {\"profile\": True}}\n\n        exp_config = {}\n        exp_name = \"profile_model_info\"\n        exp_config['name'] = exp_name\n        exp_config[DS_CONFIG] = ds_config\n        exp_config['num_gpus'] = self.exp_num_gpus\n        exp_config['num_nodes'] = self.exp_num_nodes\n        exp_config['hostfile'] = self.args.hostfile\n        exp_path = os.path.join(self.exps_dir, f'{exp_name}.json')\n\n        with open(exp_path, 'w', buffering=BUFSIZE) as fd:\n            json.dump(exp_config, fd)\n            fd.flush()\n            os.fsync(fd)\n\n        self.rm.schedule_experiments([exp_path])\n        self.rm.run()\n\n        for exp_id, (exp_json, err) in self.rm.finished_experiments.items():\n            self.rm.clear()\n            if err:\n                logger.error(f\"The model is not runnable with DeepSpeed with error = {err}\")\n                return None\n\n        if os.path.exists(model_info_path):\n            with open(model_info_path, 'r') as f:\n                model_info = hjson.load(f)\n                return model_info\n\n    def update_records(self, space_name, exp, metric_val, num_exps):\n        if space_name not in self.records:\n            self.records[space_name] = [(exp, metric_val, num_exps)]\n        else:\n            self.records[space_name].append((exp, metric_val, num_exps))\n\n    def get_best_space_record(self, space_name):\n        if space_name not in self.records:\n            return None\n        space_records = self.records[space_name]\n        best_space_record = None\n        space_num_exps = 0\n        for (exp, metric_val, num_exps) in space_records:\n            space_num_exps += num_exps\n            if best_space_record is None or metric_val > best_space_record[1]:\n                best_space_record = (exp, metric_val)\n        if best_space_record:\n            best_space_record = best_space_record + (space_num_exps, )\n        return best_space_record\n\n    def get_best_space_records(self):\n        best_space_records = {}\n        global_best_record = None\n        for space_name, space_records in self.records.items():\n            best_space_record = self.get_best_space_record(space_name)\n            if best_space_record:\n                best_space_records[space_name] = best_space_record\n                if not global_best_record or best_space_record[1] > global_best_record[1]:\n                    global_best_record = best_space_record\n        if global_best_record:\n            best_space_records[GLOBAL_TUNING_SPACE] = global_best_record\n        return best_space_records\n\n    def run_tuning_micro_batch_sizes(self, tuning_micro_batch_sizes, max_train_batch_size_per_gpu,\n                                     min_micro_batch_size, stage, tuning_micro_batch_sizes_overwritten):\n        assert tuning_micro_batch_sizes, \"the tuning micro batch size list is empty\"\n        tuning_micro_batch_sizes.sort()\n        max_micro_batch_size = tuning_micro_batch_sizes[-1]\n        max_micro_batch_size_metric_val = 0\n\n        ds_config = get_first_config(self.user_config)\n        ds_config[ZERO_OPTIMIZATION] = {ZERO_OPTIMIZATION_STAGE: stage}\n        tuning_space_name = TUNING_MICRO_BATCH_SIZE_PREFIX + str(stage)\n\n        exp_paths = []\n        for mbs in tuning_micro_batch_sizes:\n            ds_config[TRAIN_MICRO_BATCH_SIZE_PER_GPU] = mbs\n            gas = max_train_batch_size_per_gpu // mbs\n            ds_config[GRADIENT_ACCUMULATION_STEPS] = gas\n            ds_config[TRAIN_BATCH_SIZE] = mbs * gas * \\\n                self.exp_num_gpus * self.exp_num_nodes // self.mp_size()\n            exp_name = tuning_space_name + \"_gas\" + str(gas) + \"_tmbspg\" + str(mbs)\n            exp_config = {}\n            exp_config['name'] = exp_name\n            exp_config[DS_CONFIG] = ds_config\n            exp_config['num_gpus'] = self.exp_num_gpus\n            exp_config['num_nodes'] = self.exp_num_nodes\n            exp_config['hostfile'] = self.args.hostfile\n            exp_path = os.path.join(self.exps_dir, f'{exp_name}.json')\n\n            with open(exp_path, 'w', buffering=BUFSIZE) as fd:\n                json.dump(exp_config, fd)\n                fd.flush()\n                os.fsync(fd)\n            exp_paths.append(exp_path)\n\n        self.rm.schedule_experiments(exp_paths)\n        self.rm.run()\n\n        for exp_id, (exp, err) in self.rm.finished_experiments.items():\n            if exp:\n                metric_file = exp[DS_CONFIG][AUTOTUNING][AUTOTUNING_METRIC_PATH]\n                if os.path.exists(metric_file):\n\n                    with open(metric_file, 'r') as f:\n                        results = hjson.load(f)\n                        metric_val = results[self.metric()]\n                        self.update_records(tuning_space_name, exp, metric_val, 1)\n                        if max_micro_batch_size == exp[DS_CONFIG][TRAIN_MICRO_BATCH_SIZE_PER_GPU]:\n                            max_micro_batch_size_metric_val = metric_val\n                        if has_mlflow:\n                            os.environ.pop('MLFLOW_RUN_ID')\n                            mlflow.start_run(nested=True, run_name=exp['name'])\n                            for metric in results:\n                                mlflow.log_metric(metric, results[metric])\n                            mlflow.end_run()\n                            os.environ['MLFLOW_RUN_ID'] = self.mlflow_parent_id\n                else:\n                    self.update_records(tuning_space_name, exp, 0, 1)\n            else:\n                mbs = exp[DS_CONFIG][TRAIN_MICRO_BATCH_SIZE_PER_GPU]\n                logger.info(f\"micro batch size = {mbs} was not run successfully\")\n\n        self.rm.clear()\n\n        if tuning_micro_batch_sizes_overwritten:\n            return tuning_micro_batch_sizes\n\n        # in a auto-detected tuning_micro_batch_sizes list, max_micro_batch_size might not be performant as the memory consumption is close to max\n        # try smaller values while gas stays the same\n        # if finding a more performant mbs value, use it to replace max_micro_batch_size in the list\n        min_micro_batch_size_with_same_gas = (tuning_micro_batch_sizes[-2] +\n                                              1) if len(tuning_micro_batch_sizes) > 1 else min_micro_batch_size\n\n        prev_best_metric_val = max_micro_batch_size_metric_val\n        prev_best_mbs = max_micro_batch_size\n\n        stride = (max_micro_batch_size - min_micro_batch_size_with_same_gas) // 3\n        if stride == 0:\n            stride = 1\n        for mbs in reversed(range(min_micro_batch_size_with_same_gas, max_micro_batch_size, stride)):\n            ds_config[TRAIN_MICRO_BATCH_SIZE_PER_GPU] = mbs\n            gas = max_train_batch_size_per_gpu // mbs\n            ds_config[GRADIENT_ACCUMULATION_STEPS] = gas\n            ds_config[TRAIN_BATCH_SIZE] = mbs * gas * \\\n                self.exp_num_gpus * self.exp_num_nodes // self.mp_size()\n            exp_name = tuning_space_name + \"_gas\" + str(gas) + \"_tmbspg\" + str(mbs)\n            exp, metric_val = self.run_ds_config(ds_config, exp_name)\n\n            if metric_val:\n                with open(metric_file, 'r') as f:\n                    results = hjson.load(f)\n                    metric_val = results[self.metric()]\n                    if has_mlflow:\n                        os.environ.pop('MLFLOW_RUN_ID')\n                        mlflow.start_run(nested=True, run_name=exp_name)\n                        for metric in results:\n                            mlflow.log_metric(metric, results[metric])\n                        mlflow.end_run()\n                        os.environ['MLFLOW_RUN_ID'] = self.mlflow_parent_id\n                self.update_records(tuning_space_name, exp, metric_val, 1)\n                if metric_val > prev_best_metric_val * (1 + METRIC_PERCENT_DIFF_CONST):\n                    prev_best_metric_val = metric_val\n                    prev_best_mbs = mbs\n                else:\n                    break\n            else:\n                self.update_records(tuning_space_name, exp, 0, 1)\n                break\n        if prev_best_mbs != max_micro_batch_size:\n            tuning_micro_batch_sizes[-1] = prev_best_mbs\n        return tuning_micro_batch_sizes\n\n    def get_min_max_micro_batch_size(self, stage, min_micro_batch_size, calculated_max_micro_batch_size):\n        # get min and max micro batch size with gradient accumulation steps = 1\n        if min_micro_batch_size > calculated_max_micro_batch_size:\n            return -1, -1\n\n        used_micro_batch_sizes = []\n        tuning_space_name = TUNING_MICRO_BATCH_SIZE_PREFIX + str(stage)\n\n        ds_config = get_first_config(self.user_config)\n        ds_config[ZERO_OPTIMIZATION] = {ZERO_OPTIMIZATION_STAGE: stage}\n        gas = self.get_gas_from_user_config()\n        ds_config[GRADIENT_ACCUMULATION_STEPS] = gas\n\n        # search for the min micro batch size\n        if min_micro_batch_size < 1:\n            if TRAIN_MICRO_BATCH_SIZE_PER_GPU in self.user_config and isinstance(\n                    self.user_config[TRAIN_MICRO_BATCH_SIZE_PER_GPU], int):\n                # user specifies train_micro_batch_size_per_gpu as an int\n                mbs = int(self.user_config[TRAIN_MICRO_BATCH_SIZE_PER_GPU])\n            else:\n                # user does not specify train_micro_batch_size_per_gpu or sets it to \"auto\" when using Hugging Face\n                val = self.get_val_from_user_args(TRAIN_MICRO_BATCH_SIZE_PER_GPU)\n                if val:\n                    mbs = int(val)\n                else:\n                    mbs = 1\n            assert mbs > 0, \"The micro batch size per GPU must be greater than 0.\"\n            ds_config[TRAIN_MICRO_BATCH_SIZE_PER_GPU] = mbs\n            ds_config[GRADIENT_ACCUMULATION_STEPS] = gas\n            ds_config[TRAIN_BATCH_SIZE] = mbs * gas * \\\n                self.exp_num_gpus * self.exp_num_nodes // self.mp_size()\n            exp_name = tuning_space_name + \"_gas\" + str(gas) + \"_tmbspg\" + str(mbs)\n            exp, metric_val = self.run_ds_config(ds_config, exp_name)\n            if metric_val:\n                self.update_records(tuning_space_name, exp, metric_val, 1)\n                used_micro_batch_sizes.append(mbs)\n                min_micro_batch_size = mbs\n            else:\n                self.update_records(tuning_space_name, exp, 0, 1)\n                logger.info(f\"User-specified micro batch size per GPU {mbs} does not run\")\n                if self.min_train_micro_batch_size_per_gpu() == mbs:\n                    return -1, -1\n                mbs = self.min_train_micro_batch_size_per_gpu()\n                ds_config[TRAIN_MICRO_BATCH_SIZE_PER_GPU] = mbs\n                ds_config[GRADIENT_ACCUMULATION_STEPS] = gas\n                ds_config[TRAIN_BATCH_SIZE] = mbs * gas * \\\n                    self.exp_num_gpus * self.exp_num_nodes // self.mp_size()\n                exp_name = tuning_space_name + \"_gas\" + str(gas) + \"_tmbspg\" + str(mbs)\n                exp, metric_val = self.run_ds_config(ds_config, exp_name)\n                if not metric_val:\n                    self.update_records(tuning_space_name, exp, 0, 1)\n                    logger.info(f\"min_train_micro_batch_size_per_gpu {mbs} is not runnable.\")\n                    return -1, -1\n                self.update_records(tuning_space_name, exp, metric_val, 1)\n                min_micro_batch_size = mbs\n                used_micro_batch_sizes.append(mbs)\n        else:\n            ds_config[TRAIN_MICRO_BATCH_SIZE_PER_GPU] = min_micro_batch_size\n            ds_config[GRADIENT_ACCUMULATION_STEPS] = gas\n            ds_config[TRAIN_BATCH_SIZE] = min_micro_batch_size * gas * \\\n                self.exp_num_gpus * self.exp_num_nodes // self.mp_size()\n            exp_name = tuning_space_name + \"_gas\" + str(gas) + \"_tmbspg\" + str(min_micro_batch_size)\n            exp, metric_val = self.run_ds_config(ds_config, exp_name)\n            if metric_val:\n                self.update_records(tuning_space_name, exp, metric_val, 1)\n                used_micro_batch_sizes.append(min_micro_batch_size)\n            else:\n                self.update_records(tuning_space_name, exp, 0, 1)\n                return -1, -1\n\n        # search for the max micro batch size\n        max_micro_batch_size = min(calculated_max_micro_batch_size, self.max_train_micro_batch_size_per_gpu())\n        for mbs in [math.ceil(1.05 * max_micro_batch_size), max_micro_batch_size, int(0.95 * max_micro_batch_size)]:\n            if mbs > self.max_train_micro_batch_size_per_gpu():\n                continue\n            if mbs in used_micro_batch_sizes:\n                return min_micro_batch_size, mbs\n            ds_config[TRAIN_MICRO_BATCH_SIZE_PER_GPU] = mbs\n            ds_config[TRAIN_BATCH_SIZE] = mbs * gas * \\\n                self.exp_num_gpus * self.exp_num_nodes // self.mp_size()\n            exp_name = tuning_space_name + \"_gas\" + str(gas) + \"_tmbspg\" + str(mbs)\n            exp, metric_val = self.run_ds_config(ds_config, exp_name)\n\n            if metric_val:\n                logger.info(f\"mbs = {mbs} is found as max mbs\")\n                self.update_records(tuning_space_name, exp, metric_val, 1)\n                used_micro_batch_sizes.append(mbs)\n                return min_micro_batch_size, mbs\n            else:\n                self.update_records(tuning_space_name, exp, 0, 1)\n\n        space_records = self.records[tuning_space_name] if tuning_space_name in self.records else []\n        if space_records:\n            prev_idx = min(range(len(space_records)),\n                           key=lambda i: abs(space_records[i][0][DS_CONFIG][TRAIN_MICRO_BATCH_SIZE_PER_GPU] -\n                                             min_micro_batch_size))\n            prev_metric_val = space_records[prev_idx][1]\n        else:\n            prev_metric_val = None\n\n        low = min_micro_batch_size\n        high = max_micro_batch_size\n        # binary search until low is the smallest micro batch size that OOMs.\n        while low <= high:\n            mid = int((low + high) // 2)\n            logger.debug(f\"trying mbs = {mid}, low = {low}, high = {high}\")\n            if mid not in used_micro_batch_sizes:\n                ds_config[TRAIN_MICRO_BATCH_SIZE_PER_GPU] = mid\n                ds_config[TRAIN_BATCH_SIZE] = mid * gas * \\\n                    self.exp_num_gpus * self.exp_num_nodes // self.mp_size()\n                exp_name = tuning_space_name + \"_gas\" + str(gas) + \"_tmbspg\" + str(mid)\n                exp, metric_val = self.run_ds_config(ds_config, exp_name)\n                if metric_val:\n                    low = mid + 1\n                    self.update_records(tuning_space_name, exp, metric_val, 1)\n                    used_micro_batch_sizes.append(mid)\n                    if prev_metric_val and (\n                        (metric_val - prev_metric_val) / prev_metric_val) < METRIC_PERCENT_DIFF_CONST:\n                        logger.info(f\"performance plateaus at mbs = {low}\")\n                        break\n                    prev_metric_val = metric_val\n                else:\n                    self.update_records(tuning_space_name, exp, 0, 1)\n                    high = mid - 1\n            else:\n                low = mid + 1\n        max_micro_batch_size = low - 1\n\n        logger.info(f\"min_micro_batch_size = {min_micro_batch_size}, max_micro_batch_size = {max_micro_batch_size}.\")\n\n        return min_micro_batch_size, max_micro_batch_size\n\n    def get_gas_from_user_config(self):\n        gas = 1\n        if GRADIENT_ACCUMULATION_STEPS in self.user_config:\n            gas_in_config = self.user_config[GRADIENT_ACCUMULATION_STEPS]\n            if isinstance(gas_in_config, int):\n                gas = gas_in_config\n            elif gas_in_config == \"auto\":  # GRADIENT_ACCUMULATION_STEPS: \"auto\"\n                val = self.get_val_from_user_args(GRADIENT_ACCUMULATION_STEPS)\n                if val:\n                    gas = int(val)\n            elif isinstance(gas_in_config, list):\n                logger.info(\n                    f\"Specifying a list of {GRADIENT_ACCUMULATION_STEPS} to tune is not supported. 1 would be used.\")\n        assert gas > 0, \"Gradient accumulation steps must be positive.\"\n        return gas\n\n    def get_val_from_user_args(self, ds_name):\n        arg_mappings = self.autotuning_config.arg_mappings\n        user_args = self.args.user_args\n        if arg_mappings and ds_name in arg_mappings:\n            arg_name = arg_mappings[ds_name]\n            if arg_name in user_args:\n                idx = user_args.index(arg_name)\n                if user_args[idx + 1].isnumeric():\n                    return (user_args[idx + 1])\n        return None\n\n    def get_tuning_micro_batch_size_list(self, min_micro_batch_size, max_micro_batch_size,\n                                         num_tuning_micro_batch_sizes):\n        \"\"\"Get a list of micro batch sizes to tune based on min and max values, as well as the size of the list.\n        Args:\n            min_micro_batch_size ([int]): min micro batch size per GPU\n            max_micro_batch_size ([int]): max micro batch size per GPU\n            num_tuning_micro_batch_sizes (int): the number of items in the returned list\n\n        Returns:\n            [list]: a list of micro batch sizes to tune.\n        \"\"\"\n        if min_micro_batch_size <= 0 or max_micro_batch_size <= 0:\n            logger.info(\n                f\"min_micro_batch_size = {min_micro_batch_size}, max_micro_batch_size = {max_micro_batch_size}\")\n            return [], 0\n\n        # NUM_GPUS=$(( ${NUM_WORKERS} * ${NUM_GPUS_PER_WORKER} ))\n        # DP_SIZE=$(( ${NUM_GPUS} / (${PP_SIZE} * ${MP_SIZE}) ))\n        # GRAD_ACC_STEPS=$(( ${TARGET_GLOBAL_BATCH_SIZE} / (${BATCH_SIZE} * ${DP_SIZE}) ))\n        if self.max_train_batch_size(\n        ) and self.max_train_batch_size() > 0:  # if the user specifies a max_train_batch_size\n            max_train_batch_size_per_gpu = self.max_train_batch_size() * self.mp_size() // (self.exp_num_gpus *\n                                                                                            self.exp_num_nodes)\n        else:\n            gas = self.get_gas_from_user_config()\n            max_train_batch_size_per_gpu = max_micro_batch_size * gas // self.mp_size()\n        logger.info(f\"max_train_batch_size_per_gpu = {max_train_batch_size_per_gpu}\")\n        if min_micro_batch_size < max_micro_batch_size // 2:\n            min_micro_batch_size = max_micro_batch_size // 2\n\n        # constant stride\n        stride = (max_micro_batch_size - min_micro_batch_size) // num_tuning_micro_batch_sizes\n        if stride == 0:\n            stride = 1\n        ls = []\n        min_gas = max_train_batch_size_per_gpu // max_micro_batch_size\n        # if gas is the same as min_gas, do not add mbs to the tuning list\n        for mbs in range(min_micro_batch_size, max_micro_batch_size, stride):\n            if max_train_batch_size_per_gpu // mbs != min_gas:\n                ls.append(mbs)\n        ls.append(max_micro_batch_size)\n\n        return ls, max_train_batch_size_per_gpu\n\n    def run_ds_config(self, ds_config, exp_name):\n        exp_config = {}\n        exp_config['name'] = exp_name\n        exp_config[DS_CONFIG] = ds_config\n        exp_config['num_gpus'] = self.exp_num_gpus\n        exp_config['num_nodes'] = self.exp_num_nodes\n        exp_config['hostfile'] = self.args.hostfile\n        exp_path = os.path.join(self.exps_dir, f'{exp_name}.json')\n\n        logger.debug(f'run_ds_config exp_name = {exp_name}')\n\n        with open(exp_path, 'w', buffering=BUFSIZE) as fd:\n            json.dump(exp_config, fd)\n            fd.flush()\n            os.fsync(fd)\n        self.rm.schedule_experiments([exp_path])\n        self.rm.run()\n        exp, metric_val = self.rm.parse_results(self.metric())\n        self.rm.clear()\n        return exp, metric_val\n\n    def write_optimal_config(self):\n        best_space_records = self.get_best_space_records()\n        if GLOBAL_TUNING_SPACE not in best_space_records:\n            return\n        best_exp, best_metric_val, _ = best_space_records[GLOBAL_TUNING_SPACE]\n        if best_exp:\n            exp_dir = best_exp[\"result_dir\"]\n            cmd = None\n            with open(os.path.join(exp_dir, \"cmd.txt\"), \"r\") as f:\n                cmd = [str(i) for i in f.read().split()]\n\n            ds_config = hjson.load(open(os.path.join(exp_dir, \"ds_config.json\"), \"r\"))\n            ds_config.pop(AUTOTUNING)\n\n            ds_config_path = os.path.join(self.results_dir, \"ds_config_optimal.json\")\n            json.dump(ds_config, open(ds_config_path, \"w\"))\n\n            cmd_path = os.path.join(self.results_dir, \"cmd_optimal.txt\")\n            with open(cmd_path, \"w\") as fd:\n                fd.write(\" \".join(cmd))\n                fd.write(\"\\n\")\n                fd.flush()\n            self.optimal_cmd = cmd\n            self.optimal_ds_config = ds_config\n            logger.info(\n                f\"Wrote the optimal DeepSpeed configuration found by autotuning to {ds_config_path}, and the corresponding DeepSpeed command to {cmd_path}\"\n            )\n\n    def run_after_tuning(self):\n        \"\"\" Launches the training with the optimal DeepSpeed configuration found through the autotuning process.\n            \"ds_config_optimal.json\" describing the optimal DeepSpeed configuration as well the command used to launch training \"cmd_optimal.txt\" are saved to self.results_dir.\n        \"\"\"\n        if self.optimal_cmd:\n            result = subprocess.Popen(self.optimal_cmd)\n            result.wait()\n\n            logger.info(f\"Done running with the optimal DeepSpeed configuration using {self.optimal_cmd}\")\n        else:\n            logger.info(f\"No optimal DeepSpeed configuration found by autotuning.\")\n", "deepspeed/autotuning/utils.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport re\nimport collections.abc\nimport os\nimport json\nfrom deepspeed.runtime.constants import GRADIENT_ACCUMULATION_STEPS, TRAIN_MICRO_BATCH_SIZE_PER_GPU\nimport itertools\nimport copy\n\nfrom ..utils import logger\n\n\ndef search_error(filename):\n    if not os.path.exists(filename):\n        return \"stderr.log does not exist\"\n    with open(filename) as f:\n        for line in f:\n            for s in [\"Error\", \"error\", \"ERROR\"]:\n                idx = line.find(s)\n                if idx != -1:\n                    return line[idx + len(s):].lstrip(\": \")\n    return None\n\n\ndef was_interruptted(filename):\n    if not os.path.exists(filename):\n        return \"stderr.log does not exist\"\n    with open(filename) as f:\n        for line in f:\n            s = \"KeyboardInterrupt\"\n            idx = line.find(s)\n            if idx != -1:\n                return True\n    return False\n\n\ndef find_replace_str(value, replace_dict):\n    if not isinstance(value, str):\n        return str(value)\n\n    matches = re.findall(r\"\\$[\\w]+\", value)\n    for var in matches:\n        var_key = var.replace(\"$\", \"\").lower()\n        if var_key == \"nvme_path\":\n            continue\n        assert var_key in replace_dict, f\"unknown var key: {var_key}, in {replace_dict}\"\n        if isinstance(replace_dict[var_key], str):\n            value = value.replace(var, replace_dict[var_key])\n        else:\n            assert len(matches) == 1, \"unable to replace multiple non-string matches\"\n            value = replace_dict[var_key]\n    return value\n\n\ndef find_replace(target, replace_dict):\n    if isinstance(target, dict):\n        for key, value in target.items():\n            if isinstance(value, str):\n                target[key] = find_replace_str(value, replace_dict)\n            if isinstance(value, list):\n                for i in range(len(value)):\n                    value[i] = find_replace_str(value[i], replace_dict)\n            if isinstance(value, dict):\n                find_replace(value, replace_dict)\n    elif isinstance(target, list):\n        for i in range(len(target)):\n            target[i] = str(find_replace_str(target[i], replace_dict))\n\n\ndef get_list(val):\n    if not isinstance(val, list):\n        return [val]\n    else:\n        return val\n\n\ndef combine_dict(d, u):\n    for k, v in u.items():\n        if isinstance(v, collections.abc.Mapping):\n            d[k] = combine_dict(d.get(k, {}), v)\n        else:\n            if k not in d:\n                d[k] = v\n            else:\n                if not isinstance(d[k], list):\n                    d[k] = [d[k]]\n                d[k].extend(i for i in get_list(v) if i not in d[k])\n    return d\n\n\ndef del_if_exists(t, d):\n    \"\"\"Deletes a key from a dictionary if it exists.\n\n    Args:\n        t (string): target key to delete\n        d (dict): dictionary to delete from\n    \"\"\"\n    if t in d:\n        del d[t]\n        return\n    for k, v in d.items():\n        if isinstance(v, collections.abc.Mapping):\n            del_if_exists(t, v)\n\n\ndef replace_dict(d, u, ignored_keys=[]):\n    \"\"\"Replaces values in dict d with values in dict u.\n\n    Args:\n        d (dict): the target dict to overwrite\n        u (dict): the dict containing the values to overwrite the target dict\n\n    Returns:\n        dict d with values overwritten by the corresponding ones in dict u.\n    \"\"\"\n    if u is not None:\n        for k, v in u.items():\n            if k not in ignored_keys:\n                if v is None:\n                    del_if_exists(k, d)\n                    continue\n                if isinstance(v, collections.abc.Mapping):\n                    d[k] = replace_dict(d.get(k, {}), v, ignored_keys)\n                else:\n                    d[k] = v\n    return d\n\n\ndef get_val_by_key(d: dict, k):\n    if k in d:\n        return d[k]\n    for v in d.values():\n        if isinstance(v, dict):\n            return get_val_by_key(v, k)\n    return None\n\n\ndef set_val_by_key(d: dict, k, vv):\n    if k in d:\n        d[k] = vv\n    for v in d.values():\n        if isinstance(v, dict):\n            set_val_by_key(v, k, vv)\n\n\ndef fetch_hostfile(hostfile_path):\n    if not os.path.isfile(hostfile_path):\n        logger.warning(\"Unable to find hostfile, will proceed with training \"\n                       \"with local resources only.\")\n        return None\n\n    # e.g., worker-0 slots=16\n    with open(hostfile_path, 'r') as fd:\n        resource_pool = collections.OrderedDict()\n        for line in fd.readlines():\n            line = line.strip()\n            if line == '':\n                # skip empty lines\n                continue\n            try:\n                hostname, slots = line.split()\n                _, slot_count = slots.split(\"=\")\n                slot_count = int(slot_count)\n            except ValueError as err:\n                logger.error(\"Hostfile is not formatted correctly, unable to \"\n                             \"proceed with training.\")\n                raise err\n            if hostname in resource_pool:\n                logger.error(\"Hostfile contains duplicate hosts, unable to \"\n                             \"proceed with training.\")\n                raise ValueError(\"host {} is already defined\".format(hostname))\n            resource_pool[hostname] = slot_count\n\n    return resource_pool\n\n\ndef validate_ds_config(config: dict):\n\n    def is_False(config: dict, key):\n        if config is None:\n            return False\n        return bool(config.get(key))\n\n    config_zero = config.get(\"zero_optimization\", {})\n    if not config_zero:\n        return True\n    stage = config_zero.get(\"stage\")\n    offload = False\n    if stage == 1:\n        return True\n    elif stage == 2:\n        if is_False(config_zero, \"cpu_offload\") and is_False(config_zero, \"cpu_offload_params\"):\n            return False\n    elif stage == 3:\n        offload_devices = [\"cpu\", \"nvme\"]\n        if config_zero.get(\"offload_optimizer\", {}).get(\"device\") in offload_devices:\n            offload = True\n        if config_zero.get(\"offload_param\", {}).get(\"device\") in offload_devices:\n            offload = True\n    else:\n        return True\n\n    # HF requires that \"ZeRO Offload can only work with DeepSpeed optimizers\"\n    if offload and not config.get(\"optimizer\"):\n        return False\n\n    return True\n\n\ndef remove_dupe_dicts(l):\n    \"\"\" Removes duplicate dictionaries from a list. Uses list comprehension and the json library to sort and stringify each dictionary and the set data type to ensure unique values. Works with nested data structures.\n\n    Args:\n        l (list): a list of (nested) data structures.\n\n    Returns:\n        A list of unique values.\n    \"\"\"\n    list_of_strings = [json.dumps(d, sort_keys=True) for d in l]\n    list_of_strings = set(list_of_strings)\n    return [json.loads(s) for s in list_of_strings]\n\n\ndef prune_config(config, ignored_keys=[]):\n    \"\"\" Prunes the input configurations\n\n    Args:\n        configs (dict): A configuration dictionary.\n        ignored_keys (list, optional): the keys of the sections to delete. Defaults to [].\n\n    Returns:\n        A configuration dictionary.\n    \"\"\"\n    if ignored_keys:\n        for k in ignored_keys:\n\n            def find_del_key(d: dict, k: str):\n                if k in d:\n                    del d[k]\n                else:\n                    for dd in d.values():\n                        if isinstance(dd, dict):\n                            find_del_key(dd, k)\n\n            find_del_key(config, k)\n\n\ndef prune_configs(configs, ignored_keys=[]):\n    \"\"\" Prunes the input list of configurations\n\n    Args:\n        configs (list): A list of configuration dictionaries.\n        ignored_keys (list, optional): the keys of the sections to delete. Defaults to [].\n\n    Returns:\n        A list of valid and unique configuration dictionaries.\n    \"\"\"\n    pruned_list = []\n    for config in configs:\n        prune_config(config, ignored_keys)\n        pruned_list.append(config)\n\n    return remove_dupe_dicts(pruned_list)\n\n\ndef get_tuning_keys(tuning_space: dict):\n    \"\"\"Outputs the list of tunable parameters in the tuning space dict.\n\n    Args:\n        tuning_space (dict): a configuration dictionary containing tunable parameters as lists of values.\n\n    Returns:\n        A list of strings\n    \"\"\"\n    tuning_keys = []\n    for key, val in tuning_space.items():\n        if isinstance(val, dict):\n            tuning_keys.extend(get_tuning_keys(val))\n        if isinstance(val, list) and len(val) > 1:\n            tuning_keys.append(key)\n    return tuning_keys\n\n\ndef get_all_configs(tuning_space: dict, ignore_keys=None):\n    \"\"\" Splits the tuning space dictionary to result in all combinations of values.\n\n    Args:\n        tuning_space (dict): the tuning space where tunable parameters are lists of values.\n    \"\"\"\n\n    def gen_combinations(d: dict):\n        keys, values = d.keys(), d.values()\n        for v in values:\n            if not isinstance(v, list):\n                v = [v]\n        values_choices = (gen_combinations(v) if isinstance(v, dict) else get_list(v) for v in values)\n        for comb in itertools.product(*values_choices):\n            yield dict(zip(keys, comb))\n\n    all_configs = []\n    ignored_key_vals = {}\n    for ik in ignore_keys:\n        ignored_key_vals[ik] = tuning_space.get(ik, {})\n        del_if_exists(ik, tuning_space)\n    for c in gen_combinations(tuning_space):\n        replace_dict(c, ignored_key_vals)\n        all_configs.append(c)\n    return all_configs\n\n\ndef canonical_name(config: dict, tuning_keys=None, prefix=\"\", omit_val=False):\n    \"\"\" Generates a name from the acronyms of the tuning keys in the config dict. TRAIN_MICRO_BATCH_SIZE_PER_GPU is always included in the tuning keys.\n    Args:\n        config (dict): the config dict used to generate the name\n        tuning_keys (list, optional):  the tuning keys used to generate the name. Defaults to None.\n        prefix (str, optional): a string added to the beginning of the name. Defaults to None.\n    \"\"\"\n    if TRAIN_MICRO_BATCH_SIZE_PER_GPU not in tuning_keys:\n        tuning_keys.append(TRAIN_MICRO_BATCH_SIZE_PER_GPU)\n    if GRADIENT_ACCUMULATION_STEPS not in tuning_keys:\n        tuning_keys.append(GRADIENT_ACCUMULATION_STEPS)\n    tuning_keys.sort()\n\n    def get_offload_name(offload_config):\n        cname = \"\"\n        if offload_config is None:\n            return \"None_\"\n        for key, val in offload_config.items():\n            key = \"\".join(map(lambda c: c[0], key.split('_')))\n            if (isinstance(val, int) or isinstance(val, float)) and val > 9000:\n                cname += key + '{:.1e}'.format(val) + \"_\"\n            else:\n                if isinstance(val, bool):\n                    val = \"T\" if val else \"F\"\n                cname += f\"{key}{val}_\"\n        return cname\n\n    def get_name_by_keys(config: dict, tuning_keys=None, omit_val=False):\n        cname = \"\"\n        if not tuning_keys or config is None:\n            return cname\n        for key, val in config.items():\n            # skip the arg_mappings section when naming the exp file\n            if key == \"arg_mappings\":\n                continue\n            if key == \"offload_param\":\n                cname += \"op_\"\n                if not omit_val:\n                    cname += get_offload_name(val)\n                continue\n            if key == \"offload_optimizer\":\n                cname += \"oo_\"\n                if not omit_val:\n                    cname += get_offload_name(val)\n                continue\n            # recursively call the func to get name for the child dicts\n            if isinstance(val, dict):\n                n = get_name_by_keys(val, tuning_keys, omit_val=omit_val)\n                if n != \"\":\n                    cname += n + \"_\"\n            if tuning_keys and key not in tuning_keys:\n                continue\n\n            key_str = \"\".join(map(lambda c: c[0], key.split('_')))\n\n            if not omit_val:\n                if (isinstance(val, int) or isinstance(val, float)) and val > 9000:\n                    cname += key_str + '{:.1e}'.format(val) + \"_\"\n                else:\n                    if isinstance(val, bool):\n                        val = \"T\" if val else \"F\"\n                    cname += f\"{key_str}{val}_\"\n            else:\n                cname += key_str + \"_\"\n\n        return cname[:-1]\n\n    name = get_name_by_keys(config, tuning_keys, omit_val=omit_val)\n\n    return prefix + (name if name != \"\" else \"exp\")\n\n\ndef get_first_config(config: dict):\n    if not config:\n        return None\n    cfg = copy.deepcopy(config)\n\n    for key, val in cfg.items():\n        if isinstance(val, dict):\n            if key == \"optimizer\":  # use user defined optimizer which might have lists of values as params\n                cfg[key] = val\n            else:\n                cfg[key] = get_first_config(val)\n        if isinstance(val, list) and len(val) > 0:\n            cfg[key] = val[0]\n    return cfg\n\n\ndef write_experiments(exps: list, exps_dir: str):\n    exp_paths = []\n    for exp in exps:\n        exp_name = exp['name']\n        # write the expr config to a json file\n        exp_path = os.path.join(exps_dir, f'{exp_name}.json')\n        with open(exp_path, 'w') as fd:\n\n            json.dump(exp, fd)\n            exp_paths.append(exp_path)\n    return exp_paths\n\n\ndef memory_to_string(n, postfix=\"\", units=None, precision=2):\n    if units is None:\n        if n // 10**12 > 0:\n            return str(round(n / 1024**4, precision)) + \" T\" + postfix\n        if n // 10**9 > 0:\n            return str(round(n / 1024**3, precision)) + \" G\" + postfix\n        elif n // 10**6 > 0:\n            return str(round(n / 1024**2, precision)) + \" M\" + postfix\n        elif n // 10**3 > 0:\n            return str(round(n / 1014, precision)) + \" K\" + postfix\n        else:\n            return str(n) + \" \"\n    else:\n        if units == \"T\":\n            return str(round(n / 1024**4, precision)) + \" \" + units\n        if units == \"G\" + postfix:\n            return str(round(n / 1024**3, precision)) + \" \" + units\n        elif units == \"M\" + postfix:\n            return str(round(n / 1024**2, precision)) + \" \" + units\n        elif units == \"K\" + postfix:\n            return str(round(n / 1024, precision)) + \" \" + units\n        else:\n            return str(n) + \" \"\n\n\ndef number_to_string(n, postfix=\"\", units=None, precision=2):\n    if units is None:\n        if n // 10**9 > 0:\n            return str(round(n / 1000**3, precision)) + \" B\" + postfix\n        if n // 10**6 > 0:\n            return str(round(n / 1000**2, precision)) + \" M\" + postfix\n        elif n // 10**3 > 0:\n            return str(round(n / 1000**1, precision)) + \" K\" + postfix\n        else:\n            return str(n) + \" \"\n    else:\n        if units == \"B\" + postfix:\n            return str(round(n / 1000**3, precision)) + \" \" + units\n        elif units == \"M\" + postfix:\n            return str(round(n / 1000**2, precision)) + \" \" + units\n        elif units == \"K\" + postfix:\n            return str(round(n / 1000**1, precision)) + \" \" + units\n        else:\n            return str(n) + \" \"\n", "deepspeed/autotuning/scheduler.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport copy\n\nimport json\nimport subprocess\nimport sys\nimport threading\nimport time\nimport base64\n\nimport os\nimport hjson\nfrom tqdm import tqdm\n\nfrom ..utils import logger\nfrom .constants import AUTOTUNING, AUTOTUNING_METRIC_PATH, BUFSIZE\nfrom .utils import get_val_by_key, search_error, was_interruptted\n\"\"\"\nthread-0: loop over experiment queue dispatching experiments if they become available\nthread-N: start each experiment in its own thread\n\"\"\"\n\nfrom deepspeed import comm as dist\n\nTIMEOUT = 5\n\n\nclass ResourceManager:\n\n    def __init__(self, args, hosts, num_gpus_per_node, results_dir, exps_dir, arg_mappings):\n        self.results_dir = results_dir\n        self.exps_dir = exps_dir\n\n        self.nodes = []\n        self.num_gpus_per_node = num_gpus_per_node\n        for host in hosts:\n            self.nodes.append(Node(host, num_gpus_per_node))\n\n        self.experiment_queue = []\n        self.running_experiments = {}\n        self.finished_experiments = {}\n        self.experiment_count = 0\n        self.exp_paths = set()\n        self.args = args\n\n        self.arg_mappings = {}\n        if arg_mappings is not None:\n            for k, v in arg_mappings.items():\n                k = k.strip()\n                v = v.strip()\n                if k not in self.arg_mappings:\n                    self.arg_mappings[k] = v\n\n    def schedule_experiments(self, exp_paths):\n        for exp_path in exp_paths:\n            if exp_path in self.exp_paths:\n                continue\n            else:\n                self.exp_paths.add(exp_path)\n                with open(exp_path, \"r\") as fd:\n                    exp = hjson.load(fd)\n                    exp[\"exp_id\"] = self.experiment_count\n                    self.experiment_count += 1\n\n                    result_dir = exp[\"result_dir\"] = os.path.join(self.results_dir, exp['name'])\n                    if AUTOTUNING in exp[\"ds_config\"]:\n                        metric_file = os.path.join(result_dir, \"metrics.json\")\n                        exp[\"ds_config\"][AUTOTUNING][AUTOTUNING_METRIC_PATH] = metric_file\n                    stderr_file = os.path.join(result_dir, \"stderr.log\")\n                    model_info_file = os.path.join(result_dir, \"model_info.json\")\n                    metric_file = os.path.join(result_dir, \"metrics.json\")\n\n                    # skip existing experiments (except for the ones that were interrupted)\n                    if os.path.exists(result_dir) and os.path.exists(stderr_file):\n                        if not was_interruptted(stderr_file):\n                            err = search_error(stderr_file)\n                            exp_id = exp[\"exp_id\"]\n                            self.finished_experiments[exp_id] = (exp, err)\n                            if err or os.path.exists(metric_file) or os.path.exists(model_info_file):\n                                logger.info(f\"Skipping exp {exp['name']} whose result already exists\")\n                                continue\n\n                    self.experiment_queue.append(exp)\n\n    def run_job(self, exp: dict, reservations):\n        exp_id = exp[\"exp_id\"]\n        exp[\"master_port\"] = self.args.master_port + exp_id\n        exp[\"result_dir\"] = os.path.join(self.results_dir, exp['name'])\n        user_script = self.args.user_script\n        user_args = self.args.user_args\n\n        # overwrite the user arg in the arg_mappings\n        for key, val in self.arg_mappings.items():\n            nval = get_val_by_key(exp, key)\n            if nval and str(nval) != \"auto\":\n                if val in user_args:\n                    idx = user_args.index(val)\n                    user_args[idx + 1] = str(nval)\n                else:\n                    user_args.append(val)\n                    user_args.append(str(nval))\n\n        t = threading.Thread(target=run_experiment, args=(exp, reservations, user_script, user_args))\n        t.start()\n        self.running_experiments[exp_id] = (t, exp, reservations, time.time())\n\n    def experiment_check(self, pbar):\n        finished_exps = []\n        for exp_id, exp_data in self.running_experiments.items():\n            thread, exp_json, reservations, start_time = exp_data\n            logger.debug(f\"Checking exp_id = {exp_id}, alive = {thread.is_alive()}\")\n            thread.join(timeout=TIMEOUT)\n            if not thread.is_alive():\n                exp_dir = exp_json[\"result_dir\"]\n                stderr_file = os.path.join(exp_dir, \"stderr.log\")\n                err = search_error(stderr_file)\n                finished_exps.append((exp_id, reservations))\n                self.finished_experiments[exp_id] = (exp_json, err)\n                duration = time.time() - start_time\n                logger.debug(f\"Finished exp_id = {exp_id}, duration={duration:.2f} sec\")\n                pbar.update(len(finished_exps))\n        for exp_id, reservations in finished_exps:\n            for reservation in reservations:\n                reservation.restore_slots()\n            self.running_experiments.pop(exp_id)\n        time.sleep(TIMEOUT)\n\n    def resource_request(self, exp):\n        num_gpus, num_nodes = exp['num_gpus'], exp['num_nodes']\n        slot_request = num_gpus\n        reservations = []\n        for node in self.nodes:\n            if num_nodes == 0:\n                break\n            slots = node.reserve_slots(slot_request=slot_request)\n            if slots:\n                reservations.append(Reservation(node=node, slots=slots))\n                num_nodes -= 1\n\n        if num_nodes == 0:\n            # request satisfied\n            return reservations\n        else:\n            # request not satisfied\n            for reservation in reservations:\n                reservation.restore_slots()\n\n    def status(self):\n        status = \"\"\n        for node in self.nodes:\n            status += f\"{node.host} ({len(node.idle_slots)} idle gpus), \"\n        return status[:-1]\n\n    def run(self):\n        pbar = tqdm(total=len(self.experiment_queue))\n\n        while len(self.experiment_queue) > 0:\n            exp = self.experiment_queue.pop(0)\n            logger.debug(f'Popped exp_id = {exp[\"exp_id\"]} from the queue')\n            logger.debug(f'Resource status: {self.status()}')\n            reservations = self.resource_request(exp)\n\n            if not reservations:\n                logger.debug(f'Unable to schedule exp_id = {exp[\"exp_id\"]}')\n                self.experiment_queue.insert(0, exp)\n                logger.debug(f'Put exp_id = {exp[\"exp_id\"]} back into the queue')\n                self.experiment_check(pbar)\n            else:\n                desc = \"\"\n                for reservation in reservations:\n                    reservation.slots.sort()\n                    slots = \",\".join(map(str, reservation.slots))\n                    desc += f\"{reservation.node.host}:{slots}@\"\n                desc = desc[:-1]\n                logger.debug(f'Running exp_id = {exp[\"exp_id\"]} on {desc}')\n                self.run_job(exp, reservations)\n\n        # All pending experiments are scheduled, waiting for them to complete\n        while len(self.running_experiments) > 0:\n            self.experiment_check(pbar)\n\n    def save_exp_results_to_database(self, message, ranks=None, path=None):\n        \"\"\"Print message when one of following condition meets\n\n        + not dist.is_initialized()\n        + dist.get_rank() in ranks if ranks is not None or ranks = [-1]\n\n    Args:\n            message (str)\n            ranks (list)\n            path (str)\n\n        \"\"\"\n        should_log = not dist.is_initialized()\n        ranks = ranks or []\n        my_rank = dist.get_rank() if dist.is_initialized() else -1\n        if ranks and not should_log:\n            should_log = ranks[0] == -1\n            should_log = should_log or (my_rank in set(ranks))\n        logger.debug(f\"*** Should log: {should_log}\")\n        if should_log:\n            message['rank'] = my_rank\n            with open(path, 'a') as outfile:\n                json.dump(message, outfile)\n                outfile.write('\\n')\n\n    def parse_results(self, metric):\n        \"\"\" Parses the metric file of the finished experiments to select the optimal DeepSpeed configuration.\n\n        Args:\n            finished_experiments (dcit): a dictionary of experiment id and experiment description.\n\n        Returns:\n            The path to the result folder of the experiment with the optimal configuration.\n        \"\"\"\n        max_throughput = sys.float_info.min\n        best_exp_id = -1\n        for exp_id, (exp, err) in self.finished_experiments.items():\n            if err:\n                logger.info(\n                    f\"The experiment exp_id = {exp_id}, exp_name = {exp['name']}, did not run successfully with error = {err}, thus a metrics.txt does not exist for it. Check the stderr.log in {exp['result_dir']}\"\n                )\n                continue\n\n            metric_file = exp[\"ds_config\"][AUTOTUNING][AUTOTUNING_METRIC_PATH]\n\n            if os.path.exists(metric_file):\n                with open(metric_file, 'r') as f:\n                    results = hjson.load(f)\n                    curr_throughput = results[metric]\n                    if curr_throughput > max_throughput:\n                        max_throughput = curr_throughput\n                        best_exp_id = exp_id\n                    exp['results'] = results\n\n        if best_exp_id != -1:\n            best_exp, _ = self.finished_experiments[best_exp_id]\n            return best_exp, max_throughput\n\n        return exp, None\n\n    def clear(self):\n        \"\"\"Clear experiment queues, does not reset self.experiment_count\n        \"\"\"\n        self.experiment_queue = []\n        # clean up the running experiments\n        for exp_id, exp_data in self.running_experiments.items():\n            thread, exp_json, reservations, start_time = exp_data\n            clean_up(exp_json, reservations)\n        self.running_experiments = {}\n        self.finished_experiments = {}\n        self.exp_paths = set()\n\n\nclass Node:\n\n    def __init__(self, host, max_slots):\n        self.host = host\n        self.max_slots = max_slots\n        self.idle_slots = list(range(max_slots))\n\n    def reserve_slots(self, slot_request: int) -> list:\n        if len(self.idle_slots) >= slot_request:\n            return [self.idle_slots.pop(0) for _ in range(slot_request)]\n\n    def restore_slots(self, slots: list):\n        self.idle_slots += slots\n\n\nclass Reservation:\n\n    def __init__(self, node, slots):\n        self.node = node\n        self.slots = slots\n\n    def restore_slots(self):\n        self.node.restore_slots(self.slots)\n\n    def desc(self):\n        slots = \",\".join(map(str, self.slots))\n        return f\"{self.node.host}:{slots}@\"\n\n\ndef get_job_id():\n    # Infrastructure-specific job-id\n    infra_job_id = None\n    if \"DLWS_JOB_ID\" in os.environ:\n        infra_job_id = os.environ[\"DLWS_JOB_ID\"]\n    elif \"DLTS_JOB_ID\" in os.environ:\n        infra_job_id = os.environ[\"DLTS_JOB_ID\"]\n    else:\n        infra_job_id = \"unknown-job-id\"\n\n    return infra_job_id\n\n\ndef get_user():\n    user = None\n    if \"USER\" in os.environ:\n        user = os.environ[\"USER\"]\n    else:\n        user = \"unknown-user\"\n    return user\n\n\ndef run_experiment(exp: dict, reservations, user_script, user_args):\n    include_str = \"\"\n    for reservation in reservations:\n        reservation.slots.sort()\n        slots = \",\".join(map(str, reservation.slots))\n        include_str += f\"{reservation.node.host}:{slots}@\"\n    include_str = include_str[:-1]\n    master_port = exp[\"master_port\"]\n    hostfile = exp[\"hostfile\"]\n    exp[\"launcher_args\"] = [\n        \"--hostfile\",\n        f\"{hostfile}\",\n        \"--include\",\n        f\"{include_str}\",\n        \"--master_port\",\n        str(master_port),\n    ]\n    logger.debug(f'launcher args={exp[\"launcher_args\"]}')\n\n    exp[\"user\"] = get_user()\n    exp[\"job_id\"] = get_job_id()\n    exp_dir = exp[\"result_dir\"]\n    os.makedirs(exp_dir, exist_ok=True)\n    ds_config_path = os.path.join(exp_dir, \"ds_config.json\")\n    exp[\"ds_config_path\"] = ds_config_path\n\n    ds_config = copy.deepcopy(exp[\"ds_config\"])\n    ds_config_json = json.dumps(ds_config).encode('utf-8')\n\n    exp[\"ds_config_base64\"] = base64.urlsafe_b64encode(ds_config_json).decode('utf-8')\n\n    with open(exp[\"ds_config_path\"], \"w\", buffering=BUFSIZE) as fd:\n        json.dump(ds_config, fd)\n        fd.flush()\n        os.fsync(fd)\n        path = exp[\"ds_config_path\"]\n        logger.info(f\"Scheduler wrote ds_config to {path}, {os.path.abspath(path)}\")\n\n    with open(os.path.join(exp_dir, \"exp.json\"), \"w\", buffering=BUFSIZE) as fd:\n        json.dump(exp, fd)\n        fd.flush()\n        os.fsync(fd)\n        path = os.path.join(exp_dir, \"exp.json\")\n        logger.info(f\"Scheduler wrote exp to {path}, {os.path.abspath(path)}\")\n\n    # remove \"--deepspeed_config ds_config.json\" from user_args\n    if user_args:\n        if \"--deepspeed_config\" in user_args:\n            idx = user_args.index(\"--deepspeed_config\")\n        # \"--deepspeed_config\" is omitted in HF\n        elif \"--deepspeed\" in user_args:\n            idx = user_args.index(\"--deepspeed\")\n        assert idx < len(user_args), \"there is no ds_config file specified after --deepspeed_config or --deepspeed\"\n        # user_args[idx + 1] = exp[\"ds_config_path\"]\n        # pass base64 serialized ds_config to launcher\n        user_args[idx + 1] = exp[\"ds_config_base64\"]\n\n    exp[\"user_script\"] = user_script\n    exp[\"user_args\"] = user_args\n\n    cmd = [\"deepspeed\"] + exp[\"launcher_args\"] + [user_script] + user_args\n\n    assert len(exp[\"launcher_args\"]) > 0, \"must provide launcher args\"\n\n    with open(os.path.join(exp_dir, \"cmd.txt\"), \"w\", buffering=BUFSIZE) as fd:\n        fd.write(\" \".join(cmd))\n        fd.write(\"\\n\")\n        fd.flush()\n        os.fsync(fd)\n\n    logger.info(\n        f\"Launching exp_id = {exp['exp_id']}, exp_name = {exp['name']}, with resource = {include_str}, and ds_config = {os.path.abspath(ds_config_path)}\"\n    )\n\n    with open(os.path.join(exp_dir, \"stdout.log\"), \"wb\") as out, open(os.path.join(exp_dir, \"stderr.log\"),\n                                                                      \"wb\") as err:\n        result = subprocess.Popen(cmd, stdout=out, stderr=err)\n        result.wait()\n        out.flush()\n        err.flush()\n        os.fsync(out)\n        os.fsync(err)\n\n    clean_up(exp, reservations)\n\n    logger.info(f\"Done running exp_id = {exp['exp_id']}, exp_name = {exp['name']}, with resource = {include_str}\")\n\n\nPDSH_MAX_FAN_OUT = 1024\n\n\ndef clean_up(exp: dict, reservations):\n    env = os.environ.copy()\n    env['PDSH_RCMD_TYPE'] = 'ssh'\n\n    nodes_str = \"\"\n    for reservation in reservations:\n        nodes_str += f\"{reservation.node.host},\"\n    nodes_str = nodes_str[:-1]\n    logger.debug(f\"Cleaning up exp_id = {exp['exp_id']} on the following workers: {nodes_str}\")\n\n    # PDSH flags for max node fan out and specific hosts to launch on\n    # See https://linux.die.net/man/1/pdsh for flag details\n    pdsh_cmd = ['pdsh', '-f', str(PDSH_MAX_FAN_OUT), '-w', nodes_str]\n\n    kill_cmd = [\n        'pkill',\n        '-f',\n        exp['name'],\n    ]\n    cmd = pdsh_cmd + kill_cmd\n    logger.debug(\"cmd = {}\".format(' '.join(cmd)))\n\n    result = subprocess.Popen(cmd, env=env)\n    result.wait()\n\n    # In case of failure must propagate the error-condition back to the caller (usually shell). The\n    # actual error and traceback should have been printed in the subprocess, so in order to avoid\n    # unnecessary noise we just quietly exit here with the same code as the subprocess\n    if result.returncode > 0:\n        sys.exit(result.returncode)\n\n    logger.info(f\"Done cleaning up exp_id = {exp['exp_id']} on the following workers: {nodes_str}\")\n", "deepspeed/autotuning/constants.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\n#########################################\n# autotuner implementation constants\n#########################################\n\nimport os\n\nDEFAULT_TEMPLATE_PATH_ZERO_0 = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"config_templates\",\n                                            \"template_zero0.json\")\nDEFAULT_TEMPLATE_PATH_ZERO_1 = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"config_templates\",\n                                            \"template_zero1.json\")\nDEFAULT_TEMPLATE_PATH_ZERO_2 = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"config_templates\",\n                                            \"template_zero2.json\")\nDEFAULT_TEMPLATE_PATH_ZERO_3 = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"config_templates\",\n                                            \"template_zero3.json\")\n\nMETRIC_PERCENT_DIFF_CONST = 0.05\nDS_CONFIG = \"ds_config\"\nBUFSIZE = 1  # line buffer size for writing files\n\n#########################################\n# autotuner configuration constants\n#########################################\n# Autotuner. By default, this feature is not enabled.\n# Users can configure in ds_config.json as below example:\nAUTOTUNING_FORMAT = \"\"\"\nautotuner should be enabled as:\n\"session_params\": {\n  \"autotuning\": {\n    \"enabled\": true,\n    \"start_step\": 5,\n    \"end_step\": 15\n    }\n}\n\"\"\"\n\nAUTOTUNING = \"autotuning\"\n\nAUTOTUNING_ENABLED = \"enabled\"\nAUTOTUNING_ENABLED_DEFAULT = False\n\nAUTOTUNING_FAST = \"fast\"\nAUTOTUNING_FAST_DEFAULT = True\n\nAUTOTUNING_RESULTS_DIR = \"results_dir\"\nAUTOTUNING_RESULTS_DIR_DEFAULT = \"autotuning_results\"\n\nAUTOTUNING_EXPS_DIR = \"exps_dir\"\nAUTOTUNING_EXPS_DIR_DEFAULT = \"autotuning_exps\"\n\nAUTOTUNING_OVERWRITE = \"overwrite\"\nAUTOTUNING_OVERWRITE_DEFAULT = True\n\nAUTOTUNING_START_PROFILE_STEP = \"start_profile_step\"\nAUTOTUNING_START_PROFILE_STEP_DEFAULT = 3\n\nAUTOTUNING_END_PROFILE_STEP = \"end_profile_step\"\nAUTOTUNING_END_PROFILE_STEP_DEFAULT = 5\nAUTOTUNING_METRIC_PATH = \"metric_path\"\nAUTOTUNING_METRIC_PATH_DEFAULT = None\n\nAUTOTUNING_TUNER_TYPE = \"tuner_type\"\nAUTOTUNING_TUNER_GRIDSEARCH = \"gridsearch\"\nAUTOTUNING_TUNER_RANDOM = \"random\"\nAUTOTUNING_TUNER_MODELBASED = \"model_based\"\nAUTOTUNING_TUNER_TYPE_DEFAULT = AUTOTUNING_TUNER_GRIDSEARCH\nAUTOTUNING_TUNER_EARLY_STOPPING = \"tuner_early_stopping\"\nAUTOTUNING_TUNER_EARLY_STOPPING_DEFAULT = 5\nAUTOTUNING_TUNER_NUM_TRIALS = \"tuner_num_trials\"\nAUTOTUNING_TUNER_NUM_TRIALS_DEFAULT = 50\n\nAUTOTUNING_ARG_MAPPINGS = \"arg_mappings\"\nAUTOTUNING_ARG_MAPPINGS_DEFAULT = None\n\nAUTOTUNING_MAX_TRAIN_BATCH_SIZE = \"max_train_batch_size\"\nAUTOTUNING_MAX_TRAIN_BATCH_SIZE_DEFAULT = None\nAUTOTUNING_MIN_TRAIN_BATCH_SIZE = \"min_train_batch_size\"\nAUTOTUNING_MIN_TRAIN_BATCH_SIZE_DEFAULT = 1\nAUTOTUNING_MAX_TRAIN_MICRO_BATCH_SIZE_PER_GPU = \"max_train_micro_batch_size_per_gpu\"\nAUTOTUNING_MAX_TRAIN_MICRO_BATCH_SIZE_PER_GPU_DEFAULT = 1024\nAUTOTUNING_MIN_TRAIN_MICRO_BATCH_SIZE_PER_GPU = \"min_train_micro_batch_size_per_gpu\"\nAUTOTUNING_MIN_TRAIN_MICRO_BATCH_SIZE_PER_GPU_DEFAULT = 1\nAUTOTUNING_NUM_TUNING_MICRO_BATCH_SIZES = \"num_tuning_micro_batch_sizes\"\nAUTOTUNING_NUM_TUNING_MICRO_BATCH_SIZES_DEFAULT = 3\n\nAUTOTUNING_MP_SIZE = \"mp_size\"\nAUTOTUNING_MP_SIZE_DEFAULT = 1\n\nAUTOTUNING_METRIC = \"metric\"\nAUTOTUNING_METRIC_LATENCY = \"latency\"\nAUTOTUNING_METRIC_THROUGHPUT = \"throughput\"\nAUTOTUNING_METRIC_FLOPS = \"flops\"\nAUTOTUNING_METRIC_FORWARD = \"forward\"\nAUTOTUNING_METRIC_BACKWRAD = \"flops\"\nAUTOTUNING_METRIC_STEPS = \"step\"\nAUTOTUNING_METRIC_DEFAULT = AUTOTUNING_METRIC_THROUGHPUT\n\n#########################################\n# MODEL INFO\n#########################################\nAUTOTUNING_MODEL_INFO_PATH = \"model_info_path\"\nAUTOTUNING_MODEL_INFO_PATH_DEFAULT = None\n\nMODEL_INFO_FORMAT = '''\n\"model_info\": {\n  \"num_params\": 1000000000,\n  \"hidden_size\": 10,\n  \"num_layers\": 12,\n}\n'''\nMODEL_INFO = \"model_info\"\nMODEL_INFO_PROFILE = \"profile\"\nMODEL_INFO_PROFILE_DEFAULT = False\nMODEL_INFO_NUM_PARAMS = \"num_params\"\nMODEL_INFO_NUM_PARAMS_DEFAULT = None\nMODEL_INFO_HIDDEN_SIZE = \"hidden_size\"\nMODEL_INFO_HIDDEN_SIZE_DEFAULT = None\nMODEL_INFO_NUM_LAYERS = \"num_layers\"\nMODEL_INFO_NUM_LAYERS_DEFAULT = None\n\nMODEL_INFO_KEY_DEFAULT_DICT = {\n    MODEL_INFO_PROFILE: MODEL_INFO_PROFILE_DEFAULT,\n    MODEL_INFO_NUM_PARAMS: MODEL_INFO_NUM_PARAMS_DEFAULT,\n    MODEL_INFO_HIDDEN_SIZE: MODEL_INFO_HIDDEN_SIZE_DEFAULT,\n    MODEL_INFO_NUM_LAYERS: MODEL_INFO_NUM_LAYERS_DEFAULT\n}\n\n#########################################\n# autotuner search space constants\n#########################################\n\nDEFAULT_HF_CONFIG = {\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\": \"auto\",\n    \"gradient_accumulation_steps\": \"auto\",\n}\n\nDEFAULT_MIN_MEM_CONFIG = {\n    \"train_micro_batch_size_per_gpu\": 1,\n    \"zero_optimization\": {\n        \"stage\": 3\n    },\n    \"memory_break_down\": False\n}\n\nDEFAULT_TUNING_SPACE_ZERO_0 = {\"zero_optimization\": {\"stage\": 0}}\n\nDEFAULT_TUNING_SPACE_ZERO_1 = {\n    \"zero_optimization\": {\n        \"stage\": 1,\n        \"reduce_bucket_size\": [5e7, 5e8, 1e9],\n        \"allgather_bucket_size\": [5e7, 5e8, 1e9],\n    }\n}\n\nDEFAULT_TUNING_SPACE_ZERO_2 = {\n    \"zero_optimization\": {\n        \"stage\": 2,\n        \"overlap_comm\": [True, False],\n        \"reduce_scatter\": [False, True],\n        \"reduce_bucket_size\": [5e7, 5e8, 1e9],\n        \"allgather_bucket_size\": [5e7, 5e8, 1e9],\n        \"contiguous_gradients\": [False, True]\n    },\n}\n\nDEFAULT_TUNING_SPACE_ZERO_3 = {\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"overlap_comm\": [True, False],\n        \"reduce_scatter\": [False, True],\n        \"reduce_bucket_size\": [5e7, 5e8, 1e9],\n        \"allgather_partitions\": [True, False],\n        \"allgather_bucket_size\": [5e7, 5e8, 1e9],\n        \"contiguous_gradients\": [False, True]\n    },\n}\n\nGLOBAL_TUNING_SPACE = 'global'\n# TUNING_MICRO_BATCH_SIZE_PREFIX=\"tune_micro_batch_size_z\"\nTUNING_MICRO_BATCH_SIZE_PREFIX = \"z\"\n", "deepspeed/autotuning/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .autotuner import Autotuner\n", "deepspeed/autotuning/tuner/utils.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport numpy as np\nimport itertools\nfrom ..utils import *\nimport collections.abc\n\n\ndef index_to_feature(p, dims):\n    \"\"\"convert index form (single integer) to feature form (vector)\"\"\"\n    feature = []\n    for dim in dims:\n        feature.append(p % dim)\n        p //= dim\n    return feature\n\n\ndef feature_to_index(feature, dims):\n    \"\"\"convert feature form (vector) to index form (single integer)\"\"\"\n    p = 0\n    for j, k in enumerate(feature):\n        print(\"j:\", \"k:\", k, \"dims\", dims[:j])\n        p += int(np.prod(dims[:j])) * k\n    return p\n\n\ndef dict_to_dims(tuning_space):\n\n    dims = []\n\n    for key, val in tuning_space.items():\n        if isinstance(val, dict):\n            dims.extend(dict_to_dims(val))\n        elif isinstance(val, list):\n            dims.append(len(val))\n        else:\n            dims.append(1)\n\n    return dims\n\n\ndef gen_combinations(d: dict):\n    keys, values = d.keys(), d.values()\n    for v in values:\n        if not isinstance(v, list):\n            v = [v]\n    values_choices = (gen_combinations(v) if isinstance(v, dict) else get_list(v) for v in values)\n    for comb in itertools.product(*values_choices):\n        yield dict(zip(keys, comb))\n\n\ndef flatten(d, parent_key='', sep='_'):\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n\ndef dict_to_feature(feature_dict, keys, max_value=None):\n    \"\"\"Extract values from dict\"\"\"\n    feature = []\n    for key, val in feature_dict.items():  # First level\n        if key not in keys:\n            continue\n        if val is None or val == \"auto\" or key == \"autotuning\" or val == \"\":\n            continue\n        if isinstance(val, dict):\n            feature.append(dict_to_feature(val, max_value))\n        else:\n            feature.append(float(val))\n\n    # normalization, should not matter in tree models\n    if max_value is not None:\n        norm_feature = []\n        for f, mv in zip(feature, max_value):\n            norm_feature.append(f / mv)\n        feature = norm_feature\n\n    return feature\n", "deepspeed/autotuning/tuner/model_based_tuner.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport hjson\n\nfrom ..constants import AUTOTUNING, AUTOTUNING_METRIC_PATH\nfrom .base_tuner import BaseTuner\nfrom .cost_model import XGBoostCostModel\nfrom .utils import *\nfrom ..utils import *\nimport numbers\nfrom ..constants import AUTOTUNING_METRIC_LATENCY\n\nINIT_NUM = 2\n\n\nclass ModelBasedTuner(BaseTuner):\n    \"\"\"Exploring the search space with a cost model\"\"\"\n\n    def __init__(self, exps: list, resource_manager, metric, tuning_space):\n        super().__init__(exps, resource_manager, metric)\n        self.tuning_space = tuning_space\n        self.best_iter = 0\n\n        self.all_configs = [e['ds_config'] for e in exps]\n        self.num_all_configs = len(self.all_configs)\n\n        self.dims = dict_to_dims(self.tuning_space)\n\n        logger.info(f\"Create config dim: {self.dims}, all configs: {self.num_all_configs}\")\n\n        self.visited = set([])\n\n        self.trials = []\n        self.trial_pt = 0\n\n        init_num = min(INIT_NUM, self.num_all_configs)\n\n        for _ in range(init_num):\n            exp_feature = np.random.randint(self.num_all_configs)\n            exp_feature = 0\n            while exp_feature in self.visited:\n                exp_feature = np.random.randint(self.num_all_configs)\n            self.trials.append(exp_feature)\n            self.visited.add(exp_feature)\n\n        self.cost_model = XGBoostCostModel(\"rank\")\n\n        self.evaluated_configs = []\n        self.evaluated_perf = []\n\n        self.train_ct = 0\n\n        self.random_exploration_ratio = 0.2  # do random exploration\n\n    def find_estimated_top_configs(self):\n        \"\"\"Use the cost model to predict the estimated performance of configurations and find the top ones for the next round of evaluation\"\"\"\n\n        configs = []\n\n        for c in self.all_configs:\n            flattened_ds_config = flatten(c)\n            feature_val = []\n            for k, v in flattened_ds_config.items():\n                if isinstance(v, numbers.Number):\n                    feature_val.append(v)\n            configs.append(feature_val)\n        # print(configs)\n        # TODO the current implementation requires that all configs have the same shape.\n        configs = np.array(configs, dtype=np.float32)\n        estimates = self.cost_model.predict(configs)\n\n        n = len(estimates)\n        top_idx = np.argsort(estimates)\n        top_idx_ret = top_idx if self.metric == AUTOTUNING_METRIC_LATENCY else top_idx[::-1][:n]\n\n        # top_configs = [self.all_configs[i] for i in top_idx]\n\n        return top_idx_ret\n\n    def next_batch(self, sample_size):\n        sampled_batch = []\n\n        counter = 0\n        while counter < sample_size:\n\n            if len(self.visited) >= self.num_all_configs:\n                break\n\n            while self.trial_pt < len(self.trials):\n                logger.debug(f\"trials: {self.trials}\")\n                # Select top promising trials\n                index = self.trials[self.trial_pt]\n                if index not in self.visited:\n                    break\n                self.trial_pt += 1\n\n            # To avoid over-exploitation, randomly select one that has not been explored.\n            rand = np.random.rand()\n            if rand < self.random_exploration_ratio:\n                # Do normal selection\n                feature = np.random.choice(self.trials)\n                while index in self.visited:\n                    index = np.random.randint(self.num_all_configs)\n\n            # Need to track both the sampled configs and indices\n\n            sampled_batch.append(self.all_exps[index])\n            self.visited.add(index)\n            counter += 1\n\n        return sampled_batch\n\n    def has_next(self):\n        return len(self.visited) < self.num_all_configs\n\n    def update(self):\n        for exp_id, (exp, err) in self.rm.finished_experiments.items():\n            feature_val = []\n            if err:\n                logger.info(\n                    f\"Skipping exp_id = {exp_id}, exp_name = {exp['name']}, the experiment did not run successfully with error = {err}, thus a metrics.txt does not exist for it. Please check the stderr.log in {exp['result_dir']}\"\n                )\n                ds_config = exp[\"ds_config\"]\n                flattened_ds_config = flatten(ds_config)\n                for k, v in flattened_ds_config.items():\n                    if isinstance(v, numbers.Number):\n                        feature_val.append(v)\n                self.evaluated_configs.append(feature_val)\n                self.evaluated_perf.append(0.0)\n                continue\n\n            p = exp[\"ds_config\"][AUTOTUNING][AUTOTUNING_METRIC_PATH]\n            with open(p, 'r') as f:\n                results = hjson.load(f)\n                curr_iter = results[self.metric]\n                logger.debug(f\"parsing the results for {exp_id}\uff0c Result is {curr_iter}\")\n\n                ds_config = exp[\"ds_config\"]\n                flattened_ds_config = flatten(ds_config)\n                for k, v in flattened_ds_config.items():\n                    if isinstance(v, numbers.Number):\n                        feature_val.append(v)\n                self.evaluated_configs.append(feature_val)\n                self.evaluated_perf.append(curr_iter)\n\n        logger.debug(f\"**Evaluated configs: {len(self.evaluated_configs)}, evaluated perf: {self.evaluated_perf}\")\n\n        self.cost_model.fit(self.evaluated_configs, self.evaluated_perf)\n\n        estimated_top_configs = self.find_estimated_top_configs()\n\n        self.trials = estimated_top_configs\n        self.trial_pt = 0\n        self.train_ct += 1\n", "deepspeed/autotuning/tuner/index_based_tuner.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport random\n\nfrom .base_tuner import BaseTuner\n\n\nclass RandomTuner(BaseTuner):\n    \"\"\"Explore the search space in random order\"\"\"\n\n    def __init__(self, exps: list, resource_manager, metric):\n        super().__init__(exps, resource_manager, metric)\n\n    def next_batch(self, sample_size=1):\n        if sample_size > len(self.all_exps):\n            sample_size = len(self.all_exps)\n\n        sampled_batch = random.sample(self.all_exps, sample_size)\n        self.all_exps = [x for x in self.all_exps if x not in sampled_batch]\n\n        return sampled_batch\n\n\nclass GridSearchTuner(BaseTuner):\n    \"\"\"Explore the search space in sequential order\"\"\"\n\n    def __init__(self, exps: list, resource_manager, metric):\n        super().__init__(exps, resource_manager, metric)\n\n    def next_batch(self, sample_size=1):\n        if sample_size > len(self.all_exps):\n            sample_size = len(self.all_exps)\n\n        sampled_batch = self.all_exps[0:sample_size]\n        self.all_exps = [x for x in self.all_exps if x not in sampled_batch]\n\n        return sampled_batch\n", "deepspeed/autotuning/tuner/base_tuner.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport sys\n\nfrom deepspeed.autotuning.constants import *\nfrom deepspeed.autotuning.utils import write_experiments\nfrom deepspeed.utils import logger\n\n\nclass BaseTuner:\n\n    def __init__(self, exps, resource_manager, metric):\n        self.all_exps = exps\n        self.rm = resource_manager\n        self.best_iter = 0\n        self.best_exp = None\n        self.best_metric_val = None\n        self.metric = metric if metric else AUTOTUNING_METRIC_DEFAULT\n        logger.info(f\"total number of exps =  {len(self.all_exps)}\")\n\n    def has_next(self):\n        \"\"\"Whether there exists more configurations for evaluation\"\"\"\n        if len(self.all_exps) > 0:\n            return True\n        else:\n            return False\n\n    def next_batch(self, sample_size):\n        \"\"\"Select the next batch of configurations for evaluation\"\"\"\n        raise NotImplementedError\n\n    def update(self):\n        \"\"\"\"Update the tuner with what configurations have been evaluated and their performance results\"\"\"\n\n    def tune(self, sample_size=1, n_trials=1000, early_stopping=None):\n        i = 0\n        try:\n            while i < n_trials and self.has_next():\n                # Select the next batch of configuration for evaluation\n                sampled_exps = self.next_batch(sample_size)\n                # Generate experiments for measurement of performance\n                exp_paths = write_experiments(sampled_exps, self.rm.exps_dir)\n                self.rm.schedule_experiments(exp_paths)\n                self.rm.run()\n                exp, metric_val = self.rm.parse_results(self.metric)\n                if self.best_exp is None or self.best_metric_val is None or (metric_val\n                                                                             and metric_val > self.best_metric_val):\n                    # logger.info(f\"tuner finds better = {exp}\")\n                    self.best_exp = exp\n                    self.best_metric_val = metric_val\n                    self.best_iter = i\n\n                i += len(sampled_exps)\n\n                # Update the tuner with evaluated performance results\n                self.update()\n\n                self.rm.clear()\n\n                # Early stop if no more promising configurations are likely to be found\n                if early_stopping and i >= self.best_iter + early_stopping:\n                    logger.info(\n                        f\"Tuner early stopped at iteration {i}. Best iteration is {self.best_iter}. Early stopping threshold is {early_stopping}\"\n                    )\n                    break\n            return i\n        except:\n            logger.info(\"Tuner Error:\", sys.exc_info()[0])\n            return i\n", "deepspeed/autotuning/tuner/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .index_based_tuner import RandomTuner, GridSearchTuner\n# from .ga_tuner import GATuner\nfrom .model_based_tuner import ModelBasedTuner\n", "deepspeed/autotuning/tuner/cost_model.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .utils import *\n\ntry:\n    import xgboost as xgb\nexcept ImportError:\n    xgb = None\n\n\nclass XGBoostCostModel():\n\n    def __init__(self, loss_type, num_threads=None, log_interval=25, upper_model=None):\n\n        assert xgb is not None, \"missing requirements, please install deepspeed w. 'autotuning_ml' extra.\"\n\n        self.loss_type = loss_type\n\n        if loss_type == \"reg\":\n            self.xgb_params = {\n                \"max_depth\": 3,\n                \"gamma\": 0.0001,\n                \"min_child_weight\": 1,\n                \"subsample\": 1.0,\n                \"eta\": 0.3,\n                \"lambda\": 1.0,\n                \"alpha\": 0,\n                \"objective\": \"reg:linear\",\n            }\n        elif loss_type == \"rank\":\n            self.xgb_params = {\n                \"max_depth\": 3,\n                \"gamma\": 0.0001,\n                \"min_child_weight\": 1,\n                \"subsample\": 1.0,\n                \"eta\": 0.3,\n                \"lambda\": 1.0,\n                \"alpha\": 0,\n                \"objective\": \"rank:pairwise\",\n            }\n        else:\n            raise RuntimeError(\"Invalid loss type: \" + loss_type)\n\n        self.xgb_params[\"verbosity\"] = 0\n        if num_threads:\n            self.xgb_params[\"nthread\"] = num_threads\n\n    def fit(self, xs, ys):\n        x_train = np.array(xs, dtype=np.float32)\n        y_train = np.array(ys, dtype=np.float32)\n        y_max = np.max(y_train)\n        y_train = y_train / max(y_max, 1e-9)\n\n        index = np.random.permutation(len(x_train))\n        dtrain = xgb.DMatrix(x_train[index], y_train[index])\n\n        self.bst = xgb.train(self.xgb_params, dtrain)\n\n    def predict(self, xs):\n\n        features = xgb.DMatrix(xs)\n\n        return self.bst.predict(features)\n", "deepspeed/utils/debug.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\n# For lazy import with printflock()\nfcntl = None\n\n# for debug purposes map module and param objects to their fully qualified names\nmodule_names = {}\nparam_names = {}\n\n\ndef debug_clear_module_and_param_names():\n    global module_names\n    global param_names\n    module_names = {}\n    param_names = {}\n\n\ndef debug_extract_module_and_param_names(model):\n    # extract the fully qualified names as soon as the model is acquired\n    global module_names\n    global param_names\n    # XXX: can probably make a map of param2module and vice-versa\n    module_names = {module: name for name, module in model.named_modules()}\n    param_names = {param: name for name, param in model.named_parameters()}\n\n\ndef debug_module2name(module):\n    if module in module_names:\n        return module_names[module]\n    else:\n        return \"unknown\"\n\n\ndef debug_module2name_id(module):\n    return f\"name={debug_module2name(module)} id={module.id}\"\n\n\ndef debug_module2name_class(module):\n    return f\"name={debug_module2name(module)} {module.__class__.__name__}\"\n\n\ndef debug_param2name(param):\n    if param in param_names:\n        return param_names[param]\n    else:\n        return \"unknown\"\n\n\ndef debug_param2name_id(param):\n    return f\"name={debug_param2name(param)} id={param.ds_id}\"\n\n\ndef debug_param2name_id_shape(param):\n    return f\"name={debug_param2name(param)} id={param.ds_id} shape={param.data.shape}\"\n\n\ndef debug_param2name_id_shape_device(param):\n    return f\"name={debug_param2name(param)} id={param.ds_id} shape={param.data.shape} device={param.device}\"\n\n\ndef debug_param2name_id_numel(param):\n    return f\"name={debug_param2name(param)} id={param.ds_id} numel={param.numel()}\"\n\n\ndef debug_param2name_id_shape_status(param):\n    return f\"name={debug_param2name(param)} id={param.ds_id} shape={param.data.shape} status={param.ds_status}\"\n\n\ndef printflock(*msgs):\n    \"\"\"\n\n    For printing messages for all concurrent gpus w/o getting interleaved text.\n\n    This is useful when debugging issues where multi-gpus don't sync.\n\n    1. Enable the force debug in say partitioning and zero3 files\n    2. Override the usual versions with ::\n\n        def print_rank_0(message, debug=False, force=False):\n            rank = deepspeed.comm.get_rank()\n            printflock(f\"[{rank}] {message}\")\n    3. run the program and you get both logs non-interleaved\n\n    But this makes it very difficult to make sense of the output, so the ``log_rank_file`` helper\n    function might be more useful, as it's easier to send each log stream into a separate file and\n    then compare those.\n\n    \"\"\"\n    global fcntl\n    if fcntl is None:\n        import fcntl\n\n    with open(__file__, \"r\") as fh:\n        fcntl.flock(fh, fcntl.LOCK_EX)\n        try:\n            print(*msgs)\n        finally:\n            fcntl.flock(fh, fcntl.LOCK_UN)\n\n\nfh = None\n\n\ndef log_rank_file(rank, *msgs):\n    \"\"\"\n    Print to a log file of the given rank\n\n    This is useful for debugging hanging in sync processes. Here is a possible workflow:\n\n    1. Enable the force debug in say partitioning and zero3 files\n    2. Override the usual versions of print_rank_0 in those files with ::\n\n        def print_rank_0(message, debug=False, force=False):\n            rank = deepspeed.comm.get_rank()\n            log_rank_file(rank, message)\n\n    3. run the program\n    4. fix up the expected differences, e.g. different cuda numbers ::\n\n        perl -pi -e 's|cuda:1|cuda:0|' log_rank_*\n\n    5. now diff and see where names and ids diverge - you will find where the gpus don't do the same\n    work (e.g. when some layers get conditionally skipped on one gpu but not all)\n\n        diff -u log_rank_0.txt log_rank_1.txt | less\n\n    \"\"\"\n    global fh\n    if fh is None:\n        fh = open(f\"log_rank_{rank}.txt\", \"w\")\n    for m in msgs:\n        fh.write(f\"{m}\\n\")\n    fh.flush()\n\n\ndef print_backward_tensors(tensor):\n\n    def _print_bwd_tensors(grad_fn):\n        print(f\"Backward tensors in {grad_fn}\")\n        for funcs in grad_fn.next_functions:\n            if funcs[0]:\n                try:\n                    tensor = getattr(funcs[0], 'variable')\n                    print(funcs[0])\n                    print(f\"Tensor - id: {id(tensor)}, shape: {tensor.shape}, data: {tensor}, grad: {tensor.grad}\")\n                except AttributeError as e:\n                    _print_bwd_tensors(funcs[0])\n\n    if hasattr(tensor, 'grad_fn'):\n        _print_bwd_tensors(tensor.grad_fn)\n", "deepspeed/utils/timer.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport time\nfrom numpy import mean\nfrom deepspeed.utils.logging import log_dist\nfrom deepspeed.accelerator import get_accelerator\n\nFORWARD_MICRO_TIMER = 'fwd_microstep'\nFORWARD_GLOBAL_TIMER = 'fwd'\nBACKWARD_MICRO_TIMER = 'bwd_microstep'\nBACKWARD_GLOBAL_TIMER = 'bwd'\nBACKWARD_INNER_MICRO_TIMER = 'bwd_inner_microstep'\nBACKWARD_INNER_GLOBAL_TIMER = 'bwd_inner'\nBACKWARD_REDUCE_MICRO_TIMER = 'bwd_allreduce_microstep'\nBACKWARD_REDUCE_GLOBAL_TIMER = 'bwd_allreduce'\nSTEP_MICRO_TIMER = 'step_microstep'\nSTEP_GLOBAL_TIMER = 'step'\nTIME_EPSILON = 1e-6\n\ntry:\n    import psutil\n\n    PSUTILS_INSTALLED = True\nexcept ImportError:\n    PSUTILS_INSTALLED = False\n    pass\n\n\nclass CudaEventTimer(object):\n\n    def __init__(self, start_event: get_accelerator().Event, end_event: get_accelerator().Event):\n        self.start_event = start_event\n        self.end_event = end_event\n\n    def get_elapsed_msec(self):\n        get_accelerator().current_stream().wait_event(self.end_event)\n        self.end_event.synchronize()\n        return self.start_event.elapsed_time(self.end_event)\n\n\nclass SynchronizedWallClockTimer:\n    \"\"\"Group of timers. Borrowed from Nvidia Megatron code\"\"\"\n\n    class Timer:\n        \"\"\"Timer.\"\"\"\n\n        def __init__(self, name):\n            self.name_ = name\n            self.started_ = False\n            self.event_timers = []\n            self.use_host_timer = get_accelerator().use_host_timers()\n            self.start_event = None\n            self.elapsed_records = None\n            self.start_time = 0.0\n            self.end_time = 0.0\n\n        def start(self):\n            \"\"\"Start the timer.\"\"\"\n            assert not self.started_, f\"{self.name_} timer has already been started\"\n            if self.use_host_timer:\n                self.start_time = time.time()\n            else:\n                event_class = get_accelerator().Event\n                self.start_event = event_class(enable_timing=True)\n                self.start_event.record()\n            self.started_ = True\n\n        def stop(self, reset=False, record=False):\n            \"\"\"Stop the timer.\"\"\"\n            assert self.started_, \"timer is not started\"\n            event_class = get_accelerator().Event\n            if self.use_host_timer:\n                self.end_time = time.time()\n                self.event_timers.append(self.end_time - self.start_time)\n            else:\n                event_class = get_accelerator().Event\n                end_event = event_class(enable_timing=True)\n                end_event.record()\n                self.event_timers.append(CudaEventTimer(self.start_event, end_event))\n                self.start_event = None\n            self.started_ = False\n\n        def _get_elapsed_msec(self):\n            if self.use_host_timer:\n                self.elapsed_records = [et * 1000.0 for et in self.event_timers]\n            else:\n                self.elapsed_records = [et.get_elapsed_msec() for et in self.event_timers]\n            self.event_timers.clear()\n            return sum(self.elapsed_records)\n\n        def reset(self):\n            \"\"\"Reset timer.\"\"\"\n            self.started_ = False\n            self.start_event = None\n            self.elapsed_records = None\n            self.event_timers.clear()\n\n        def elapsed(self, reset=True):\n            \"\"\"Calculate the elapsed time.\"\"\"\n            started_ = self.started_\n            # If the timing in progress, end it first.\n            if self.started_:\n                self.stop()\n            # Get the elapsed time.\n            elapsed_ = self._get_elapsed_msec()\n            # Reset the elapsed time\n            if reset:\n                self.reset()\n            # If timing was in progress, set it back.\n            if started_:\n                self.start()\n            return elapsed_\n\n        def mean(self):\n            self.elapsed(reset=False)\n            return trim_mean(self.elapsed_records, 0.1)\n\n    def __init__(self):\n        self.timers = {}\n\n    def get_timers(self):\n        return self.timers\n\n    def __call__(self, name):\n        if name not in self.timers:\n            self.timers[name] = self.Timer(name)\n        return self.timers[name]\n\n    @staticmethod\n    def memory_usage():\n        alloc = \"mem_allocated: {:.4f} GB\".format(get_accelerator().memory_allocated() / (1024 * 1024 * 1024))\n        max_alloc = \"max_mem_allocated: {:.4f} GB\".format(get_accelerator().max_memory_allocated() /\n                                                          (1024 * 1024 * 1024))\n        cache = \"cache_allocated: {:.4f} GB\".format(get_accelerator().memory_cached() / (1024 * 1024 * 1024))\n        max_cache = \"max_cache_allocated: {:.4f} GB\".format(get_accelerator().max_memory_cached() /\n                                                            (1024 * 1024 * 1024))\n        return \" | {} | {} | {} | {}\".format(alloc, max_alloc, cache, max_cache)\n\n    def log(self, names, normalizer=1.0, reset=True, memory_breakdown=False, ranks=None):\n        \"\"\"Log a group of timers.\"\"\"\n        assert normalizer > 0.0\n        string = f\"time (ms)\"\n        for name in names:\n            if name in self.timers:\n                elapsed_time = (self.timers[name].elapsed(reset=reset) / normalizer)\n                string += \" | {}: {:.2f}\".format(name, elapsed_time)\n\n        log_dist(string, ranks=ranks or [0])\n\n    def get_mean(self, names, normalizer=1.0, reset=True):\n        \"\"\"Get the mean of a group of timers.\"\"\"\n        assert normalizer > 0.0\n        means = {}\n        for name in names:\n            if name in self.timers:\n                elapsed_time = (self.timers[name].mean() * 1000.0 / normalizer)\n                means[name] = elapsed_time\n        return means\n\n\nclass NoopTimer:\n\n    class Timer:\n\n        def start(self):\n            ...\n\n        def reset(self):\n            ...\n\n        def stop(self, **kwargs):\n            ...\n\n        def elapsed(self, **kwargs):\n            return 0\n\n        def mean(self):\n            return 0\n\n    def __init__(self):\n        self.timer = self.Timer()\n\n    def __call__(self, name):\n        return self.timer\n\n    def get_timers(self):\n        return {}\n\n    def log(self, names, normalizer=1.0, reset=True, memory_breakdown=False, ranks=None):\n        ...\n\n    def get_mean(self, names, normalizer=1.0, reset=True):\n        ...\n\n\nclass ThroughputTimer:\n\n    def __init__(self, config, batch_size, start_step=2, steps_per_output=50, monitor_memory=False, logging_fn=None):\n        from deepspeed.utils import logger\n        self.config = config\n        self.start_time = 0\n        self.end_time = 0\n        self.started = False\n        self.batch_size = 1 if batch_size is None else batch_size\n        self.start_step = start_step\n        self.epoch_count = 0\n        self.micro_step_count = 0\n        self.global_step_count = 0\n        self.total_elapsed_time = 0\n        self.step_elapsed_time = 0\n        self.steps_per_output = steps_per_output\n        self.monitor_memory = monitor_memory\n        self.logging = logging_fn\n        if self.logging is None:\n            self.logging = logger.info\n        self.initialized = False\n\n        if self.monitor_memory and not PSUTILS_INSTALLED:\n            raise ImportError(\"Unable to import 'psutils', please install package\")\n\n    def update_epoch_count(self):\n        self.epoch_count += 1\n        self.micro_step_count = 0\n\n    def _init_timer(self):\n        self.initialized = True\n\n    def start(self):\n        if not self.config.enabled:\n            return\n        self._init_timer()\n        self.started = True\n        if self.global_step_count >= self.start_step:\n            if self.config.synchronized:\n                get_accelerator().synchronize()\n            self.start_time = time.time()\n\n    def stop(self, global_step=False, report_speed=True):\n        if not self.config.enabled or not self.started:\n            return\n        self.started = False\n        self.micro_step_count += 1\n        if global_step:\n            self.global_step_count += 1\n\n        if self.start_time > 0:\n            if self.config.synchronized:\n                get_accelerator().synchronize()\n            self.end_time = time.time()\n            duration = self.end_time - self.start_time\n            self.total_elapsed_time += duration\n            self.step_elapsed_time += duration\n\n            if global_step:\n                if report_speed and self.global_step_count % self.steps_per_output == 0:\n                    self.logging(\n                        \"epoch={}/micro_step={}/global_step={}, RunningAvgSamplesPerSec={}, CurrSamplesPerSec={}, \"\n                        \"MemAllocated={}GB, MaxMemAllocated={}GB\".format(\n                            self.epoch_count,\n                            self.micro_step_count,\n                            self.global_step_count,\n                            self.avg_samples_per_sec(),\n                            self.batch_size / (self.step_elapsed_time + TIME_EPSILON),\n                            round(get_accelerator().memory_allocated() / 1024**3, 2),\n                            round(get_accelerator().max_memory_allocated() / 1024**3, 2),\n                        ))\n                    if self.monitor_memory:\n                        virt_mem = psutil.virtual_memory()\n                        swap = psutil.swap_memory()\n                        self.logging(\"epoch={}/micro_step={}/global_step={}, vm %: {}, swap %: {}\".format(\n                            self.epoch_count,\n                            self.micro_step_count,\n                            self.global_step_count,\n                            virt_mem.percent,\n                            swap.percent,\n                        ))\n                self.step_elapsed_time = 0\n\n    def avg_samples_per_sec(self):\n        if self.global_step_count > 0:\n            total_step_offset = self.global_step_count - self.start_step\n            avg_time_per_step = self.total_elapsed_time / total_step_offset\n            # training samples per second\n            return self.batch_size / avg_time_per_step\n        return float(\"-inf\")\n\n\ndef trim_mean(data, trim_percent):\n    \"\"\"Compute the trimmed mean of a list of numbers.\n\n    Args:\n        data (list): List of numbers.\n        trim_percent (float): Percentage of data to trim.\n\n    Returns:\n        float: Trimmed mean.\n    \"\"\"\n    assert 0.0 <= trim_percent <= 1.0\n    n = len(data)\n    # Account for edge case of empty list\n    if len(data) == 0:\n        return 0\n    data.sort()\n    k = int(round(n * (trim_percent)))\n    return mean(data[k:n - k])\n", "deepspeed/utils/logging.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport functools\nimport logging\nimport sys\nimport os\n\nlog_levels = {\n    \"debug\": logging.DEBUG,\n    \"info\": logging.INFO,\n    \"warning\": logging.WARNING,\n    \"error\": logging.ERROR,\n    \"critical\": logging.CRITICAL,\n}\n\n\nclass LoggerFactory:\n\n    @staticmethod\n    def create_logger(name=None, level=logging.INFO):\n        \"\"\"create a logger\n\n        Args:\n            name (str): name of the logger\n            level: level of logger\n\n        Raises:\n            ValueError is name is None\n        \"\"\"\n\n        if name is None:\n            raise ValueError(\"name for logger cannot be None\")\n\n        formatter = logging.Formatter(\"[%(asctime)s] [%(levelname)s] \"\n                                      \"[%(filename)s:%(lineno)d:%(funcName)s] %(message)s\")\n\n        logger_ = logging.getLogger(name)\n        logger_.setLevel(level)\n        logger_.propagate = False\n        ch = logging.StreamHandler(stream=sys.stdout)\n        ch.setLevel(level)\n        ch.setFormatter(formatter)\n        logger_.addHandler(ch)\n        return logger_\n\n\nlogger = LoggerFactory.create_logger(name=\"DeepSpeed\", level=logging.INFO)\n\n\n@functools.lru_cache(None)\ndef warning_once(*args, **kwargs):\n    \"\"\"\n    This method is identical to `logger.warning()`, but will emit the warning with the same message only once\n\n    Note: The cache is for the function arguments, so 2 different callers using the same arguments will hit the cache.\n    The assumption here is that all warning messages are unique across the code. If they aren't then need to switch to\n    another type of cache that includes the caller frame information in the hashing function.\n    \"\"\"\n    logger.warning(*args, **kwargs)\n\n\nlogger.warning_once = warning_once\n\n\ndef print_configuration(args, name):\n    logger.info(\"{}:\".format(name))\n    for arg in sorted(vars(args)):\n        dots = \".\" * (29 - len(arg))\n        logger.info(\"  {} {} {}\".format(arg, dots, getattr(args, arg)))\n\n\ndef log_dist(message, ranks=None, level=logging.INFO):\n    from deepspeed import comm as dist\n    \"\"\"Log message when one of following condition meets\n\n    + not dist.is_initialized()\n    + dist.get_rank() in ranks if ranks is not None or ranks = [-1]\n\n    Args:\n        message (str)\n        ranks (list)\n        level (int)\n\n    \"\"\"\n    should_log = not dist.is_initialized()\n    ranks = ranks or []\n    my_rank = dist.get_rank() if dist.is_initialized() else -1\n    if ranks and not should_log:\n        should_log = ranks[0] == -1\n        should_log = should_log or (my_rank in set(ranks))\n    if should_log:\n        final_message = \"[Rank {}] {}\".format(my_rank, message)\n        logger.log(level, final_message)\n\n\ndef print_json_dist(message, ranks=None, path=None):\n    from deepspeed import comm as dist\n    \"\"\"Print message when one of following condition meets\n\n    + not dist.is_initialized()\n    + dist.get_rank() in ranks if ranks is not None or ranks = [-1]\n\n    Args:\n        message (str)\n        ranks (list)\n        path (str)\n\n    \"\"\"\n    should_log = not dist.is_initialized()\n    ranks = ranks or []\n    my_rank = dist.get_rank() if dist.is_initialized() else -1\n    if ranks and not should_log:\n        should_log = ranks[0] == -1\n        should_log = should_log or (my_rank in set(ranks))\n    if should_log:\n        message['rank'] = my_rank\n        import json\n        with open(path, 'w') as outfile:\n            json.dump(message, outfile)\n            os.fsync(outfile)\n\n\ndef get_current_level():\n    \"\"\"\n    Return logger's current log level\n    \"\"\"\n    return logger.getEffectiveLevel()\n\n\ndef should_log_le(max_log_level_str):\n    \"\"\"\n    Args:\n        max_log_level_str: maximum log level as a string\n\n    Returns ``True`` if the current log_level is less or equal to the specified log level. Otherwise ``False``.\n\n    Example:\n\n        ``should_log_le(\"info\")`` will return ``True`` if the current log level is either ``logging.INFO`` or ``logging.DEBUG``\n    \"\"\"\n\n    if not isinstance(max_log_level_str, str):\n        raise ValueError(f\"{max_log_level_str} is not a string\")\n\n    max_log_level_str = max_log_level_str.lower()\n    if max_log_level_str not in log_levels:\n        raise ValueError(f\"{max_log_level_str} is not one of the `logging` levels\")\n\n    return get_current_level() <= log_levels[max_log_level_str]\n", "deepspeed/utils/torch.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom packaging import version as pkg_version\n\nimport torch\n\n\ndef required_torch_version(min_version=None, max_version=None):\n    assert min_version or max_version, \"Must provide a min_version or max_version argument\"\n\n    torch_version = pkg_version.parse(torch.__version__)\n\n    if min_version and pkg_version.parse(str(min_version)) > torch_version:\n        return False\n\n    if max_version and pkg_version.parse(str(max_version)) < torch_version:\n        return False\n\n    return True\n", "deepspeed/utils/config.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom deepspeed.runtime.config_utils import DeepSpeedConfigModel\n\n#########################################\n# Timers\n#########################################\n# Timers. By default, timers are enabled.\n# Users can configure in ds_config.json as below example:\nTIMERS_FORMAT = '''\nTimers should be enabled as:\n\"timers\": {\n  \"throughput\": {\n    \"enabled\": true,\n    \"synchronized\": true\n  }\n}\n'''\n\nTIMERS = \"timers\"\nTIMERS_THROUGHPUT = \"throughput\"\n\n\ndef get_timers_config(param_dict):\n    if param_dict and TIMERS in param_dict and TIMERS_THROUGHPUT in param_dict[TIMERS]:\n        timers_config_dict = param_dict[TIMERS][TIMERS_THROUGHPUT]\n    else:\n        timers_config_dict = {}\n    return DeepSpeedThroughputTimerConfig(**timers_config_dict)\n\n\nclass DeepSpeedThroughputTimerConfig(DeepSpeedConfigModel):\n    \"\"\" Configure throughput timers \"\"\"\n\n    enabled: bool = True\n    \"\"\" Turn on/off throughput timers \"\"\"\n\n    synchronized: bool = True\n    \"\"\" Whether to synchronize a device when measuring the time.\n        Synchronizing a device is required to produce the most accurate timer measurements.\n        However, this comes at the expense of performance degradation. The CPU timer provides\n        sufficient accuracy in many cases.\n      \"\"\"\n", "deepspeed/utils/zero_to_fp32.py": "#!/usr/bin/env python\n\n# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\n# This script extracts fp32 consolidated weights from a zero 1, 2 and 3 DeepSpeed checkpoints. It gets\n# copied into the top level checkpoint dir, so the user can easily do the conversion at any point in\n# the future. Once extracted, the weights don't require DeepSpeed and can be used in any\n# application.\n#\n# example: python zero_to_fp32.py . pytorch_model.bin\n\nimport argparse\nimport torch\nimport glob\nimport math\nimport os\nimport re\nfrom collections import OrderedDict\nfrom dataclasses import dataclass\n\n# while this script doesn't use deepspeed to recover data, since the checkpoints are pickled with\n# DeepSpeed data structures it has to be available in the current python environment.\nfrom deepspeed.utils import logger\nfrom deepspeed.checkpoint.constants import (DS_VERSION, OPTIMIZER_STATE_DICT, SINGLE_PARTITION_OF_FP32_GROUPS,\n                                            FP32_FLAT_GROUPS, ZERO_STAGE, PARTITION_COUNT, PARAM_SHAPES, BUFFER_NAMES,\n                                            FROZEN_PARAM_SHAPES, FROZEN_PARAM_FRAGMENTS)\n\n\n@dataclass\nclass zero_model_state:\n    buffers: dict()\n    param_shapes: dict()\n    shared_params: list\n    ds_version: int\n    frozen_param_shapes: dict()\n    frozen_param_fragments: dict()\n\n\ndebug = 0\n\n# load to cpu\ndevice = torch.device('cpu')\n\n\ndef atoi(text):\n    return int(text) if text.isdigit() else text\n\n\ndef natural_keys(text):\n    '''\n    alist.sort(key=natural_keys) sorts in human order\n    http://nedbatchelder.com/blog/200712/human_sorting.html\n    (See Toothy's implementation in the comments)\n    '''\n    return [atoi(c) for c in re.split(r'(\\d+)', text)]\n\n\ndef get_model_state_file(checkpoint_dir, zero_stage):\n    if not os.path.isdir(checkpoint_dir):\n        raise FileNotFoundError(f\"Directory '{checkpoint_dir}' doesn't exist\")\n\n    # there should be only one file\n    if zero_stage <= 2:\n        file = os.path.join(checkpoint_dir, \"mp_rank_00_model_states.pt\")\n    elif zero_stage == 3:\n        file = os.path.join(checkpoint_dir, \"zero_pp_rank_0_mp_rank_00_model_states.pt\")\n\n    if not os.path.exists(file):\n        raise FileNotFoundError(f\"can't find model states file at '{file}'\")\n\n    return file\n\n\ndef get_checkpoint_files(checkpoint_dir, glob_pattern):\n    # XXX: need to test that this simple glob rule works for multi-node setup too\n    ckpt_files = sorted(glob.glob(os.path.join(checkpoint_dir, glob_pattern)), key=natural_keys)\n\n    if len(ckpt_files) == 0:\n        raise FileNotFoundError(f\"can't find {glob_pattern} files in directory '{checkpoint_dir}'\")\n\n    return ckpt_files\n\n\ndef get_optim_files(checkpoint_dir):\n    return get_checkpoint_files(checkpoint_dir, \"*_optim_states.pt\")\n\n\ndef get_model_state_files(checkpoint_dir):\n    return get_checkpoint_files(checkpoint_dir, \"*_model_states.pt\")\n\n\ndef parse_model_states(files):\n    zero_model_states = []\n    for file in files:\n        state_dict = torch.load(file, map_location=device)\n\n        if BUFFER_NAMES not in state_dict:\n            raise ValueError(f\"{file} is not a model state checkpoint\")\n        buffer_names = state_dict[BUFFER_NAMES]\n        if debug:\n            print(\"Found buffers:\", buffer_names)\n\n        # recover just the buffers while restoring them to fp32 if they were saved in fp16\n        buffers = {k: v.float() for k, v in state_dict[\"module\"].items() if k in buffer_names}\n        param_shapes = state_dict[PARAM_SHAPES]\n\n        # collect parameters that are included in param_shapes\n        param_names = []\n        for s in param_shapes:\n            for name in s.keys():\n                param_names.append(name)\n\n        # update with frozen parameters\n        frozen_param_shapes = state_dict.get(FROZEN_PARAM_SHAPES, None)\n        if frozen_param_shapes is not None:\n            if debug:\n                print(f\"Found frozen_param_shapes: {frozen_param_shapes}\")\n            param_names += list(frozen_param_shapes.keys())\n\n        # handle shared params\n        shared_params = [[k, v] for k, v in state_dict[\"shared_params\"].items()]\n\n        ds_version = state_dict.get(DS_VERSION, None)\n\n        frozen_param_fragments = state_dict.get(FROZEN_PARAM_FRAGMENTS, None)\n\n        z_model_state = zero_model_state(buffers=buffers,\n                                         param_shapes=param_shapes,\n                                         shared_params=shared_params,\n                                         ds_version=ds_version,\n                                         frozen_param_shapes=frozen_param_shapes,\n                                         frozen_param_fragments=frozen_param_fragments)\n        zero_model_states.append(z_model_state)\n\n    return zero_model_states\n\n\ndef parse_optim_states(files, ds_checkpoint_dir):\n\n    total_files = len(files)\n    state_dicts = []\n    for f in files:\n        state_dict = torch.load(f, map_location=device)\n        # immediately discard the potentially huge 2 optimizer states as we only care for fp32 master weights\n        # and also handle the case where it was already removed by another helper script\n        state_dict[\"optimizer_state_dict\"].pop(\"optimizer_state_dict\", None)\n        state_dicts.append(state_dict)\n\n    if not ZERO_STAGE in state_dicts[0][OPTIMIZER_STATE_DICT]:\n        raise ValueError(f\"{files[0]} is not a zero checkpoint\")\n    zero_stage = state_dicts[0][OPTIMIZER_STATE_DICT][ZERO_STAGE]\n    world_size = state_dicts[0][OPTIMIZER_STATE_DICT][PARTITION_COUNT]\n\n    # For ZeRO-2 each param group can have different partition_count as data parallelism for expert\n    # parameters can be different from data parallelism for non-expert parameters. So we can just\n    # use the max of the partition_count to get the dp world_size.\n\n    if type(world_size) is list:\n        world_size = max(world_size)\n\n    if world_size != total_files:\n        raise ValueError(\n            f\"Expected {world_size} of '*_optim_states.pt' under '{ds_checkpoint_dir}' but found {total_files} files. \"\n            \"Possibly due to an overwrite of an old checkpoint, or a checkpoint didn't get saved by one or more processes.\"\n        )\n\n    # the groups are named differently in each stage\n    if zero_stage <= 2:\n        fp32_groups_key = SINGLE_PARTITION_OF_FP32_GROUPS\n    elif zero_stage == 3:\n        fp32_groups_key = FP32_FLAT_GROUPS\n    else:\n        raise ValueError(f\"unknown zero stage {zero_stage}\")\n\n    if zero_stage <= 2:\n        fp32_flat_groups = [state_dicts[i][OPTIMIZER_STATE_DICT][fp32_groups_key] for i in range(len(state_dicts))]\n    elif zero_stage == 3:\n        # if there is more than one param group, there will be multiple flattened tensors - one\n        # flattened tensor per group - for simplicity merge them into a single tensor\n        #\n        # XXX: could make the script more memory efficient for when there are multiple groups - it\n        # will require matching the sub-lists of param_shapes for each param group flattened tensor\n\n        fp32_flat_groups = [\n            torch.cat(state_dicts[i][OPTIMIZER_STATE_DICT][fp32_groups_key], 0) for i in range(len(state_dicts))\n        ]\n\n    return zero_stage, world_size, fp32_flat_groups\n\n\ndef _get_fp32_state_dict_from_zero_checkpoint(ds_checkpoint_dir, exclude_frozen_parameters):\n    \"\"\"\n    Returns fp32 state_dict reconstructed from ds checkpoint\n\n    Args:\n        - ``ds_checkpoint_dir``: path to the deepspeed checkpoint folder (where the optimizer files are)\n\n    \"\"\"\n    print(f\"Processing zero checkpoint '{ds_checkpoint_dir}'\")\n\n    optim_files = get_optim_files(ds_checkpoint_dir)\n    zero_stage, world_size, fp32_flat_groups = parse_optim_states(optim_files, ds_checkpoint_dir)\n    print(f\"Detected checkpoint of type zero stage {zero_stage}, world_size: {world_size}\")\n\n    model_files = get_model_state_files(ds_checkpoint_dir)\n\n    zero_model_states = parse_model_states(model_files)\n    print(f'Parsing checkpoint created by deepspeed=={zero_model_states[0].ds_version}')\n\n    if zero_stage <= 2:\n        return _get_fp32_state_dict_from_zero2_checkpoint(world_size, fp32_flat_groups, zero_model_states,\n                                                          exclude_frozen_parameters)\n    elif zero_stage == 3:\n        return _get_fp32_state_dict_from_zero3_checkpoint(world_size, fp32_flat_groups, zero_model_states,\n                                                          exclude_frozen_parameters)\n\n\ndef _zero2_merge_frozen_params(state_dict, zero_model_states):\n    if zero_model_states[0].frozen_param_shapes is None or len(zero_model_states[0].frozen_param_shapes) == 0:\n        return\n\n    frozen_param_shapes = zero_model_states[0].frozen_param_shapes\n    frozen_param_fragments = zero_model_states[0].frozen_param_fragments\n\n    if debug:\n        num_elem = sum(s.numel() for s in frozen_param_shapes.values())\n        print(f'rank 0: {FROZEN_PARAM_SHAPES}.numel = {num_elem}')\n\n        wanted_params = len(frozen_param_shapes)\n        wanted_numel = sum(s.numel() for s in frozen_param_shapes.values())\n        avail_numel = sum([p.numel() for p in frozen_param_fragments.values()])\n        print(f'Frozen params: Have {avail_numel} numels to process.')\n        print(f'Frozen params: Need {wanted_numel} numels in {wanted_params} params')\n\n    total_params = 0\n    total_numel = 0\n    for name, shape in frozen_param_shapes.items():\n        total_params += 1\n        unpartitioned_numel = shape.numel()\n        total_numel += unpartitioned_numel\n\n        state_dict[name] = frozen_param_fragments[name]\n\n        if debug:\n            print(f\"{name} full shape: {shape} unpartitioned numel {unpartitioned_numel} \")\n\n    print(f\"Reconstructed Frozen fp32 state dict with {total_params} params {total_numel} elements\")\n\n\ndef _has_callable(obj, fn):\n    attr = getattr(obj, fn, None)\n    return callable(attr)\n\n\ndef _zero2_merge_trainable_params(state_dict, world_size, fp32_flat_groups, zero_model_states):\n    param_shapes = zero_model_states[0].param_shapes\n\n    # Reconstruction protocol:\n    #\n    # XXX: document this\n\n    if debug:\n        for i in range(world_size):\n            for j in range(len(fp32_flat_groups[0])):\n                print(f\"{FP32_FLAT_GROUPS}[{i}][{j}].shape={fp32_flat_groups[i][j].shape}\")\n\n    # XXX: memory usage doubles here (zero2)\n    num_param_groups = len(fp32_flat_groups[0])\n    merged_single_partition_of_fp32_groups = []\n    for i in range(num_param_groups):\n        merged_partitions = [sd[i] for sd in fp32_flat_groups]\n        full_single_fp32_vector = torch.cat(merged_partitions, 0)\n        merged_single_partition_of_fp32_groups.append(full_single_fp32_vector)\n    avail_numel = sum(\n        [full_single_fp32_vector.numel() for full_single_fp32_vector in merged_single_partition_of_fp32_groups])\n\n    if debug:\n        wanted_params = sum([len(shapes) for shapes in param_shapes])\n        wanted_numel = sum([sum(shape.numel() for shape in shapes.values()) for shapes in param_shapes])\n        # not asserting if there is a mismatch due to possible padding\n        print(f\"Have {avail_numel} numels to process.\")\n        print(f\"Need {wanted_numel} numels in {wanted_params} params.\")\n\n    # params\n    # XXX: for huge models that can't fit into the host's RAM we will have to recode this to support\n    # out-of-core computing solution\n    total_numel = 0\n    total_params = 0\n    for shapes, full_single_fp32_vector in zip(param_shapes, merged_single_partition_of_fp32_groups):\n        offset = 0\n        avail_numel = full_single_fp32_vector.numel()\n        for name, shape in shapes.items():\n\n            unpartitioned_numel = shape.numel() if _has_callable(shape, 'numel') else math.prod(shape)\n            total_numel += unpartitioned_numel\n            total_params += 1\n\n            if debug:\n                print(f\"{name} full shape: {shape} unpartitioned numel {unpartitioned_numel} \")\n            state_dict[name] = full_single_fp32_vector.narrow(0, offset, unpartitioned_numel).view(shape)\n            offset += unpartitioned_numel\n\n        # Z2 started to align to 2*world_size to improve nccl performance. Therefore both offset and\n        # avail_numel can differ by anywhere between 0..2*world_size. Due to two unrelated complex\n        # paddings performed in the code it's almost impossible to predict the exact numbers w/o the\n        # live optimizer object, so we are checking that the numbers are within the right range\n        align_to = 2 * world_size\n\n        def zero2_align(x):\n            return align_to * math.ceil(x / align_to)\n\n        if debug:\n            print(f\"original offset={offset}, avail_numel={avail_numel}\")\n\n        offset = zero2_align(offset)\n        avail_numel = zero2_align(avail_numel)\n\n        if debug:\n            print(f\"aligned  offset={offset}, avail_numel={avail_numel}\")\n\n        # Sanity check\n        if offset != avail_numel:\n            raise ValueError(f\"consumed {offset} numels out of {avail_numel} - something is wrong\")\n\n    print(f\"Reconstructed fp32 state dict with {total_params} params {total_numel} elements\")\n\n\ndef _get_fp32_state_dict_from_zero2_checkpoint(world_size, fp32_flat_groups, zero_model_states,\n                                               exclude_frozen_parameters):\n    state_dict = OrderedDict()\n\n    # buffers\n    buffers = zero_model_states[0].buffers\n    state_dict.update(buffers)\n    if debug:\n        print(f\"added {len(buffers)} buffers\")\n\n    if not exclude_frozen_parameters:\n        _zero2_merge_frozen_params(state_dict, zero_model_states)\n\n    _zero2_merge_trainable_params(state_dict, world_size, fp32_flat_groups, zero_model_states)\n\n    # recover shared parameters\n    for pair in zero_model_states[0].shared_params:\n        if pair[1] in state_dict:\n            state_dict[pair[0]] = state_dict[pair[1]]\n\n    return state_dict\n\n\ndef zero3_partitioned_param_info(unpartitioned_numel, world_size):\n    remainder = unpartitioned_numel % world_size\n    padding_numel = (world_size - remainder) if remainder else 0\n    partitioned_numel = math.ceil(unpartitioned_numel / world_size)\n    return partitioned_numel, padding_numel\n\n\ndef _zero3_merge_frozen_params(state_dict, world_size, zero_model_states):\n    if zero_model_states[0].frozen_param_shapes is None or len(zero_model_states[0].frozen_param_shapes) == 0:\n        return\n\n    if debug:\n        for i in range(world_size):\n            num_elem = sum(s.numel() for s in zero_model_states[i].frozen_param_fragments.values())\n            print(f'rank {i}: {FROZEN_PARAM_SHAPES}.numel = {num_elem}')\n\n        frozen_param_shapes = zero_model_states[0].frozen_param_shapes\n        wanted_params = len(frozen_param_shapes)\n        wanted_numel = sum(s.numel() for s in frozen_param_shapes.values())\n        avail_numel = sum([p.numel() for p in zero_model_states[0].frozen_param_fragments.values()]) * world_size\n        print(f'Frozen params: Have {avail_numel} numels to process.')\n        print(f'Frozen params: Need {wanted_numel} numels in {wanted_params} params')\n\n    total_params = 0\n    total_numel = 0\n    for name, shape in zero_model_states[0].frozen_param_shapes.items():\n        total_params += 1\n        unpartitioned_numel = shape.numel()\n        total_numel += unpartitioned_numel\n\n        param_frags = tuple(model_state.frozen_param_fragments[name] for model_state in zero_model_states)\n        state_dict[name] = torch.cat(param_frags, 0).narrow(0, 0, unpartitioned_numel).view(shape)\n\n        partitioned_numel, partitioned_padding_numel = zero3_partitioned_param_info(unpartitioned_numel, world_size)\n\n        if debug:\n            print(\n                f\"Frozen params: {total_params} {name} full shape: {shape} partition0 numel={partitioned_numel} partitioned_padding_numel={partitioned_padding_numel}\"\n            )\n\n    print(f\"Reconstructed Frozen fp32 state dict with {total_params} params {total_numel} elements\")\n\n\ndef _zero3_merge_trainable_params(state_dict, world_size, fp32_flat_groups, zero_model_states):\n    param_shapes = zero_model_states[0].param_shapes\n    avail_numel = fp32_flat_groups[0].numel() * world_size\n    # Reconstruction protocol: For zero3 we need to zip the partitions together at boundary of each\n    # param, re-consolidating each param, while dealing with padding if any\n\n    # merge list of dicts, preserving order\n    param_shapes = {k: v for d in param_shapes for k, v in d.items()}\n\n    if debug:\n        for i in range(world_size):\n            print(f\"{FP32_FLAT_GROUPS}[{i}].shape={fp32_flat_groups[i].shape}\")\n\n        wanted_params = len(param_shapes)\n        wanted_numel = sum(shape.numel() for shape in param_shapes.values())\n        # not asserting if there is a mismatch due to possible padding\n        avail_numel = fp32_flat_groups[0].numel() * world_size\n        print(f\"Trainable params: Have {avail_numel} numels to process.\")\n        print(f\"Trainable params: Need {wanted_numel} numels in {wanted_params} params.\")\n\n    # params\n    # XXX: for huge models that can't fit into the host's RAM we will have to recode this to support\n    # out-of-core computing solution\n    offset = 0\n    total_numel = 0\n    total_params = 0\n    for name, shape in param_shapes.items():\n\n        unpartitioned_numel = shape.numel()\n        total_numel += unpartitioned_numel\n        total_params += 1\n\n        partitioned_numel, partitioned_padding_numel = zero3_partitioned_param_info(unpartitioned_numel, world_size)\n\n        if debug:\n            print(\n                f\"Trainable params: {total_params} {name} full shape: {shape} partition0 numel={partitioned_numel} partitioned_padding_numel={partitioned_padding_numel}\"\n            )\n\n        # XXX: memory usage doubles here\n        state_dict[name] = torch.cat(\n            tuple(fp32_flat_groups[i].narrow(0, offset, partitioned_numel) for i in range(world_size)),\n            0).narrow(0, 0, unpartitioned_numel).view(shape)\n        offset += partitioned_numel\n\n    offset *= world_size\n\n    # Sanity check\n    if offset != avail_numel:\n        raise ValueError(f\"consumed {offset} numels out of {avail_numel} - something is wrong\")\n\n    print(f\"Reconstructed Trainable fp32 state dict with {total_params} params {total_numel} elements\")\n\n\ndef _get_fp32_state_dict_from_zero3_checkpoint(world_size, fp32_flat_groups, zero_model_states,\n                                               exclude_frozen_parameters):\n    state_dict = OrderedDict()\n\n    # buffers\n    buffers = zero_model_states[0].buffers\n    state_dict.update(buffers)\n    if debug:\n        print(f\"added {len(buffers)} buffers\")\n\n    if not exclude_frozen_parameters:\n        _zero3_merge_frozen_params(state_dict, world_size, zero_model_states)\n\n    _zero3_merge_trainable_params(state_dict, world_size, fp32_flat_groups, zero_model_states)\n\n    # recover shared parameters\n    for pair in zero_model_states[0].shared_params:\n        if pair[1] in state_dict:\n            state_dict[pair[0]] = state_dict[pair[1]]\n\n    return state_dict\n\n\ndef get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir, tag=None, exclude_frozen_parameters=False):\n    \"\"\"\n    Convert ZeRO 2 or 3 checkpoint into a single fp32 consolidated state_dict that can be loaded with\n    ``load_state_dict()`` and used for training without DeepSpeed or shared with others, for example\n    via a model hub.\n\n    Args:\n        - ``checkpoint_dir``: path to the desired checkpoint folder\n        - ``tag``: checkpoint tag used as a unique identifier for checkpoint. If not provided will attempt to load tag in 'latest' file. e.g., ``global_step14``\n        - ``exclude_frozen_parameters``: exclude frozen parameters\n\n    Returns:\n        - pytorch ``state_dict``\n\n    Note: this approach may not work if your application doesn't have sufficient free CPU memory and\n    you may need to use the offline approach using the ``zero_to_fp32.py`` script that is saved with\n    the checkpoint.\n\n    A typical usage might be ::\n\n        from deepspeed.utils.zero_to_fp32 import get_fp32_state_dict_from_zero_checkpoint\n        # do the training and checkpoint saving\n        state_dict = get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir) # already on cpu\n        model = model.cpu() # move to cpu\n        model.load_state_dict(state_dict)\n        # submit to model hub or save the model to share with others\n\n    In this example the ``model`` will no longer be usable in the deepspeed context of the same\n    application. i.e. you will need to re-initialize the deepspeed engine, since\n    ``model.load_state_dict(state_dict)`` will remove all the deepspeed magic from it.\n\n    If you want it all done for you, use ``load_state_dict_from_zero_checkpoint`` instead.\n\n    \"\"\"\n    if tag is None:\n        latest_path = os.path.join(checkpoint_dir, 'latest')\n        if os.path.isfile(latest_path):\n            with open(latest_path, 'r') as fd:\n                tag = fd.read().strip()\n        else:\n            raise ValueError(f\"Unable to find 'latest' file at {latest_path}\")\n\n    ds_checkpoint_dir = os.path.join(checkpoint_dir, tag)\n\n    if not os.path.isdir(ds_checkpoint_dir):\n        raise FileNotFoundError(f\"Directory '{ds_checkpoint_dir}' doesn't exist\")\n\n    return _get_fp32_state_dict_from_zero_checkpoint(ds_checkpoint_dir, exclude_frozen_parameters)\n\n\ndef convert_zero_checkpoint_to_fp32_state_dict(checkpoint_dir, output_file, tag=None, exclude_frozen_parameters=False):\n    \"\"\"\n    Convert ZeRO 2 or 3 checkpoint into a single fp32 consolidated ``state_dict`` file that can be\n    loaded with ``torch.load(file)`` + ``load_state_dict()`` and used for training without DeepSpeed.\n\n    Args:\n        - ``checkpoint_dir``: path to the desired checkpoint folder. (one that contains the tag-folder, like ``global_step14``)\n        - ``output_file``: path to the pytorch fp32 state_dict output file (e.g. path/pytorch_model.bin)\n        - ``tag``: checkpoint tag used as a unique identifier for checkpoint. If not provided will attempt to load tag in the file named ``latest`` in the checkpoint folder, e.g., ``global_step14``\n        - ``exclude_frozen_parameters``: exclude frozen parameters\n    \"\"\"\n\n    state_dict = get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir, tag, exclude_frozen_parameters)\n    print(f\"Saving fp32 state dict to {output_file}\")\n    torch.save(state_dict, output_file)\n\n\ndef load_state_dict_from_zero_checkpoint(model, checkpoint_dir, tag=None):\n    \"\"\"\n    1. Put the provided model to cpu\n    2. Convert ZeRO 2 or 3 checkpoint into a single fp32 consolidated ``state_dict``\n    3. Load it into the provided model\n\n    Args:\n        - ``model``: the model object to update\n        - ``checkpoint_dir``: path to the desired checkpoint folder. (one that contains the tag-folder, like ``global_step14``)\n        - ``tag``: checkpoint tag used as a unique identifier for checkpoint. If not provided will attempt to load tag in the file named ``latest`` in the checkpoint folder, e.g., ``global_step14``\n\n    Returns:\n        - ``model`: modified model\n\n    Make sure you have plenty of CPU memory available before you call this function. If you don't\n    have enough use the ``zero_to_fp32.py`` utility to do the conversion. You will find it\n    conveniently placed for you in the checkpoint folder.\n\n    A typical usage might be ::\n\n        from deepspeed.utils.zero_to_fp32 import load_state_dict_from_zero_checkpoint\n        model = load_state_dict_from_zero_checkpoint(trainer.model, checkpoint_dir)\n        # submit to model hub or save the model to share with others\n\n    Note, that once this was run, the ``model`` will no longer be usable in the deepspeed context\n    of the same application. i.e. you will need to re-initialize the deepspeed engine, since\n    ``model.load_state_dict(state_dict)`` will remove all the deepspeed magic from it.\n\n    \"\"\"\n    logger.info(f\"Extracting fp32 weights\")\n    state_dict = get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir, tag)\n\n    logger.info(f\"Overwriting model with fp32 weights\")\n    model = model.cpu()\n    model.load_state_dict(state_dict, strict=False)\n\n    return model\n\n\nif __name__ == \"__main__\":\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"checkpoint_dir\",\n                        type=str,\n                        help=\"path to the desired checkpoint folder, e.g., path/checkpoint-12\")\n    parser.add_argument(\n        \"output_file\",\n        type=str,\n        help=\"path to the pytorch fp32 state_dict output file (e.g. path/checkpoint-12/pytorch_model.bin)\")\n    parser.add_argument(\"-t\",\n                        \"--tag\",\n                        type=str,\n                        default=None,\n                        help=\"checkpoint tag used as a unique identifier for checkpoint. e.g., global_step1\")\n    parser.add_argument(\"--exclude_frozen_parameters\", action='store_true', help=\"exclude frozen parameters\")\n    parser.add_argument(\"-d\", \"--debug\", action='store_true', help=\"enable debug\")\n    args = parser.parse_args()\n\n    debug = args.debug\n\n    convert_zero_checkpoint_to_fp32_state_dict(args.checkpoint_dir,\n                                               args.output_file,\n                                               tag=args.tag,\n                                               exclude_frozen_parameters=args.exclude_frozen_parameters)\n", "deepspeed/utils/exceptions.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\n\nclass DeprecatedException(Exception):\n    pass\n", "deepspeed/utils/tensor_fragment.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nfrom dataclasses import dataclass\nfrom deepspeed import comm as dist\nfrom typing import Dict\n\n\n@dataclass\nclass fragment_address:\n    numel: int\n    start: int\n\n\n@dataclass\nclass tensor_fragment:\n    lp_fragment: torch.Tensor\n    lp_fragment_address: fragment_address\n    hp_fragment: torch.Tensor\n    hp_fragment_address: fragment_address\n    gradient_dict: Dict\n    offload_gradient_dict: Dict\n    use_offload: bool\n    param_group_index: int\n    optim_fragment: Dict = None\n\n    def update_hp(self):\n        self.hp_fragment.data.copy_(self.lp_fragment.data)\n\n    def update_lp(self):\n        self.lp_fragment.data.copy_(self.hp_fragment.data)\n\n    def get_optim_state_fragment(self, key):\n        if key in self.optim_fragment:\n            return self.optim_fragment[key]\n        else:\n            raise ValueError(f'{key} not found in optimizer state fragment')\n\n    def set_optim_state_fragment(self, flat_hp_partition, optim_fragment):\n        self.optim_fragment = {\n            key: value.narrow(0, self.hp_fragment_address.start, self.hp_fragment_address.numel)\n            for key, value in optim_fragment.items()\n            if torch.is_tensor(value) and value.shape == flat_hp_partition.shape\n        }\n\n    def get_hp_fragment_address(self):\n        return self.hp_fragment_address\n\n    def get_optim_state_keys(self):\n        return list(self.optim_fragment.keys())\n\n    def get_hp_fragment(self, optim_state_key=None):\n        if optim_state_key is None:\n            return self.hp_fragment\n        return self.get_optim_state_fragment(optim_state_key)\n\n\ndef map_to_flat_opt_states(flat_hp_tensor, lp_tensors, optim_state, opt_keys):\n    for key in opt_keys:\n        hp_param = flat_hp_tensor\n        buffer = torch.zeros_like(hp_param)\n\n        for lp in lp_tensors:\n            if lp._hp_mapping is not None:\n                hp_fragment_address = lp._hp_mapping.get_hp_fragment_address()\n                hp_fragment = buffer.narrow(0, hp_fragment_address.start, hp_fragment_address.numel)\n                hp_fragment.data.copy_(lp._hp_mapping.get_hp_fragment(optim_state_key=key).data)\n                lp._hp_mapping.hp_fragment = hp_fragment\n\n        optim_state[hp_param][key] = buffer\n\n\ndef get_full_hp_param(self, optim_state_key=None):\n    reduce_buffer = torch.zeros_like(self, dtype=torch.float32).flatten()\n    if self._hp_mapping is not None:\n        lp_frag_address = self._hp_mapping.lp_fragment_address\n        reduce_fragment = torch.narrow(reduce_buffer, 0, lp_frag_address.start, lp_frag_address.numel)\n        hp_fragment = self._hp_mapping.get_hp_fragment(optim_state_key)\n        reduce_fragment.data.copy_(hp_fragment.data)\n    dist.all_reduce(reduce_buffer, group=self._dp_group)\n    return reduce_buffer.reshape_as(self)\n\n\ndef set_full_hp_param(self, value, optim_state_key=None):\n    if self._hp_mapping is not None:\n        lp_frag_address = self._hp_mapping.lp_fragment_address\n        value_fragment = torch.narrow(value.flatten(), 0, lp_frag_address.start, lp_frag_address.numel)\n        hp_fragment = self._hp_mapping.get_hp_fragment(optim_state_key)\n        hp_fragment.data.copy_(value_fragment.data)\n\n\ndef get_full_hp_grad(self):\n    reduce_buffer = torch.zeros_like(self, dtype=torch.float32).flatten()\n    if self._hp_mapping is not None:\n        hp_mapping = self._hp_mapping\n\n        if hp_mapping.use_offload:\n            gradient_dict = hp_mapping.offload_gradient_dict\n        else:\n            gradient_dict = hp_mapping.gradient_dict\n\n        if hp_mapping.param_group_index not in gradient_dict or gradient_dict[hp_mapping.param_group_index] is None:\n            raise ValueError(\"Gradients are only available immediately after backward and before engine step\")\n\n        lp_grad_fragment = gradient_dict[hp_mapping.param_group_index][self._index_in_param_group]\n        hp_grad_fragment = lp_grad_fragment.to(torch.float32).flatten()\n\n        lp_frag_address = self._hp_mapping.lp_fragment_address\n        reduce_fragment = torch.narrow(reduce_buffer, 0, lp_frag_address.start, lp_frag_address.numel)\n\n        if self.view(-1).shape == hp_grad_fragment.shape:\n            reduce_buffer.data.copy_(hp_grad_fragment.data)\n        else:\n            reduce_fragment.data.copy_(hp_grad_fragment.data)\n\n    dist.all_reduce(reduce_buffer, group=self._dp_group)\n    return reduce_buffer.reshape_as(self)\n\n\ndef safe_get_full_fp32_param(param):\n    \"\"\"Assemble and return the fp32 parameter of a low-precision (e.g., fp16) parameter.\n\n        Args:\n            param (``torch.nn.Parameter``): A model parameter\n    \"\"\"\n    # ZeRO stage 3 param\n    if hasattr(param, 'ds_id'):\n        return param._z3_optimizer.get_full_hp_param(param)\n\n    # ZeRO stage 1, 2, and bf16_optimizer params\n    if hasattr(param, '_hp_mapping'):\n        return param.get_full_hp_param()\n    return None\n\n\ndef safe_set_full_fp32_param(param, value):\n    \"\"\"Update the partitioned fp32 parameter of a low-precision (e.g., fp16) parameter.\n\n        Args:\n            param (``torch.nn.Parameter``): A model parameter\n            value (``torch.Tensor``): New value\n    \"\"\"\n    # ZeRO stage 3 param\n    if hasattr(param, 'ds_id'):\n        param._z3_optimizer.set_full_hp_param(value, param)\n\n    # ZeRO stage 1, 2, and bf16_optimizer params\n    if hasattr(param, '_hp_mapping'):\n        param.set_full_hp_param(value)\n\n\ndef safe_get_full_optimizer_state(param, optim_state_key):\n    \"\"\"Assemble and return the fp32 optimizer state of a low-precision (e.g., fp16) parameter.\n\n        Args:\n            param (``torch.nn.Parameter``): A model parameter\n            optim_state_key (``string``): Key value of optimizer state (e.g., `exp_avg` in Adam optimizer)\n    \"\"\"\n    # ZeRO stage 3 param\n    if hasattr(param, 'ds_id'):\n        return param._z3_optimizer.get_full_hp_param(param, optim_state_key)\n\n    # ZeRO stage 1, 2, and bf16_optimizer params\n    if hasattr(param, '_hp_mapping'):\n        return param.get_full_hp_param(optim_state_key)\n    return None\n\n\ndef safe_set_full_optimizer_state(param, value, optim_state_key):\n    \"\"\"Update the partitioned fp32 optimizer state of a low-precision (e.g., fp16) parameter.\n\n        Args:\n            param (``torch.nn.Parameter``): A model parameter\n            value (``torch.Tensor``): New value\n            optim_state_key (``string``): Key value of optimizer state (e.g., `exp_avg` in Adam optimizer)\n    \"\"\"\n    # ZeRO stage 3 param\n    if hasattr(param, 'ds_id'):\n        param._z3_optimizer.set_full_hp_param(value, param, optim_state_key)\n\n    # ZeRO stage 1, 2, and bf16_optimizer params\n    if hasattr(param, '_hp_mapping'):\n        param.set_full_hp_param(value, optim_state_key)\n\n\n# TODO: Figure out the correct return dtype\ndef safe_get_full_grad(param):\n    \"\"\"Assemble and return the fp32 gradient of a low-precision (e.g., fp16) parameter.\n\n        Args:\n            param (``torch.nn.Parameter``): A model parameter\n    \"\"\"\n    if param.grad is not None:\n        return param.grad\n\n    # ZeRO stage 3 param\n    if hasattr(param, 'ds_id'):\n        return param._z3_optimizer.get_fp32_grad_for_param(param)\n\n    # ZeRO stage 1, 2, and bf16_optimizer params\n    if hasattr(param, '_hp_mapping'):\n        return param.get_full_hp_grad()\n\n    return None\n\n\n### Local API  START ###\ndef safe_get_local_grad(param):\n    \"\"\"Get the fp32 gradient of a partitioned parameter.\n        Args:\n            param (``torch.nn.Parameter``): A model parameter\n    \"\"\"\n    if param.grad is not None:\n        return param.grad\n\n    # ZeRO stage 3 param\n    if hasattr(param, 'ds_id'):\n        return param._z3_optimizer.get_local_fp32_grad_for_param(param)\n\n    return None\n\n\ndef safe_get_local_fp32_param(param):\n    \"\"\"Get the fp32 partitioned parameter.\n        Args:\n            param (``torch.nn.Parameter``): A model parameter\n    \"\"\"\n    # ZeRO stage 3 param\n    if hasattr(param, 'ds_id'):\n        return param._z3_optimizer.get_local_fp32_param(param)\n\n    return None\n\n\ndef safe_get_local_optimizer_state(param, optim_state_key):\n    \"\"\"Get the fp32 optimizer state of a partitioned parameter.\n        Args:\n            param (``torch.nn.Parameter``): A model parameter\n            optim_state_key (``string``): Key value of optimizer state (e.g., `exp_avg` in Adam optimizer)\n    \"\"\"\n    # ZeRO stage 3 param\n    if hasattr(param, 'ds_id'):\n        return param._z3_optimizer.get_local_fp32_param(param, optim_state_key)\n\n    return None\n\n\ndef safe_set_local_optimizer_state(param, value, optim_state_key):\n    \"\"\"Update the fp32 optimizer state of a partitioned parameter.\n        Args:\n            param (``torch.nn.Parameter``): A model parameter\n            value (``torch.Tensor``): New value\n            optim_state_key (``string``): Key value of optimizer state (e.g., `exp_avg` in Adam optimizer)\n    \"\"\"\n    # ZeRO stage 3 param\n    if hasattr(param, 'ds_id'):\n        param._z3_optimizer.set_local_hp_param(value, param, optim_state_key)\n\n\ndef safe_set_local_fp32_param(param, value):\n    \"\"\"Update the partitioned fp32 parameter.\n        Args:\n            param (``torch.nn.Parameter``): A model parameter\n            value (``torch.Tensor``): New value\n    \"\"\"\n    # ZeRO stage 3 param\n    if hasattr(param, 'ds_id'):\n        param._z3_optimizer.set_local_hp_param(value, param)\n\n\n### Local API  END ###\n\n# TODO: Implement API for setting ZeRO partitioned gradients\n\n\ndef get_hp_fragment_mapping(lp_param, lp_start, flat_hp_partition, gradient_dict, offload_gradient_dict, use_offload,\n                            param_group_index, partition_start, partition_size):\n    lp_end = lp_param.numel() + lp_start\n    hp_start = partition_start\n    hp_end = partition_start + partition_size\n\n    fragment_start = max(lp_start, hp_start)\n    fragment_end = min(lp_end, hp_end)\n    assert fragment_start < fragment_end, \\\n        f'fragment start {fragment_start} should be < fragment_end {fragment_end}'\n\n    fragment_numel = fragment_end - fragment_start\n    hp_frag_address = fragment_address(start=fragment_start - hp_start, numel=fragment_numel)\n    hp_fragment_tensor = flat_hp_partition.narrow(0, hp_frag_address.start, hp_frag_address.numel)\n\n    lp_frag_address = fragment_address(start=fragment_start - lp_start, numel=fragment_numel)\n    lp_fragment_tensor = lp_param.flatten().narrow(0, lp_frag_address.start, lp_frag_address.numel)\n\n    return tensor_fragment(lp_fragment=lp_fragment_tensor,\n                           lp_fragment_address=lp_frag_address,\n                           hp_fragment=hp_fragment_tensor,\n                           hp_fragment_address=hp_frag_address,\n                           gradient_dict=gradient_dict,\n                           offload_gradient_dict=offload_gradient_dict,\n                           use_offload=use_offload,\n                           param_group_index=param_group_index)\n\n\n'''\nLogic for lp_param to hp_param mapping\n\nlp      lp0 lp1 lp2         lp3  lp4            <-------  indices/names\nlp      [  ][  ][          ][   ][         ]    <-------- tensors\nflat_lp [                                  ]     <-------- flat lp params\nflat_hp            [                 ]   <------------------ flat hp partition on current rank\nfull_hp [                                        ] <------- full flat hp params\n\n\nlp2\n full numel = 16\n lp_frag\n   numel = 12\n   frag_start = 3\n   frag_end  = 15\n hp_frag\n    numel = 12\n    frag_start = 0\n    frag_end = 11\n\n hp_frag.copy_(lp_frag)\n\n\nlp3:\n  full numel = 4\n  lp_frag\n     numel = 4\n     start = 0\n     end = 3\n  hp_frag\n     numel = 4\n     start = 12\n     end = 15\n\n\nlp4:\n   full numel = 12\n   lp_frag\n     numel = 4\n     start = 0\n     end = 3\n  hp_frag\n     numel = 4\n     start = 16\n     end = 19\n\n\n\nVisual depiction of above\nlp              {         }\nflat_lp [                                ]\nflat_hp            (                 )\n\n\nflat_lp [       {  (      }          )   ]\n                lx  hx   ly          hy\n                    ly-hx\n\n\nlp                             {       }\nflat_lp [                                ]\nflat_hp            (                 )\n\n\nflat_lp [          (            {     ) }  ]\n                   hx           lx   hy ly\n                                   hy-lx\n\nlp                        {   }\nflat_lp [                                ]\nflat_hp            (                 )\n\n\nflat_lp [          (       {   }      )   ]\n                   hx      lx  ly    hy\n                             ly-lx\n\nlp -> (lx, hy)\nflat_hp -> (hx, hy)\n'''\n", "deepspeed/utils/bwc.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\n\ndef bwc_tensor_model_parallel_rank(mpu=None):\n    \"\"\"Backwards-compatible way of querying the tensor model parallel rank from\n    an ``mpu`` object.\n\n    *Tensor* model parallelism means that tensors are physically split across\n    processes. This contrasts with *pipeline* model parallelism, in which the\n    layers are partitioned but tensors left intact.\n\n    The API for tensor model parallelism has changed across versions and this\n    helper provides a best-effort implementation across versions of ``mpu``\n    objects.  The preferred mechanism is\n    ``mpu.get_tensor_model_parallel_rank()``.\n\n    This should \"just work\" with both Megatron-LM and DeepSpeed's pipeline\n    parallelism.\n\n    Args:\n        mpu (model parallel unit, optional): The tensor model parallel rank.\n            If ``mpu=None``, returns 0. Defaults to ``None``.\n\n    Returns:\n        int: the rank\n    \"\"\"\n    if mpu is None:\n        # No model parallelism in easy :)\n        return 0\n\n    if hasattr(mpu, 'get_tensor_model_parallel_rank'):\n        # New Megatron and DeepSpeed convention (post pipeline-parallelism release)\n        return mpu.get_tensor_model_parallel_rank()\n    elif hasattr(mpu, 'get_slice_parallel_rank'):\n        # Some DeepSpeed + pipeline parallelism versions\n        return mpu.get_slice_parallel_rank()\n    else:\n        # Deprecated Megatron and DeepSpeed convention\n        return mpu.get_model_parallel_rank()\n\n\ndef bwc_tensor_model_parallel_world_size(mpu=None):\n    \"\"\"Backwards-compatible way of querying the tensor model parallel world size.\n       Similar to bwc_tensor_model_parallel_rank.\n    \"\"\"\n    if mpu is None:\n        return 1\n\n    if hasattr(mpu, 'get_tensor_model_parallel_world_size'):\n        # New Megatron and DeepSpeed convention (post pipeline-parallelism release)\n        return mpu.get_tensor_model_parallel_world_size()\n    elif hasattr(mpu, 'get_slice_parallel_world_size'):\n        # Some DeepSpeed + pipeline parallelism versions\n        return mpu.get_slice_parallel_world_size()\n    else:\n        # Deprecated Megatron and DeepSpeed convention\n        return mpu.get_model_parallel_world_size()\n\n\ndef bwc_tensor_model_parallel_group(mpu=None):\n    \"\"\"Backwards-compatible way of querying the tensor model parallel group.\n       Similar to bwc_tensor_model_parallel_rank.\n    \"\"\"\n    if mpu is None:\n        return None\n\n    if hasattr(mpu, 'get_tensor_model_parallel_group'):\n        # New Megatron and DeepSpeed convention (post pipeline-parallelism release)\n        return mpu.get_tensor_model_parallel_group()\n    elif hasattr(mpu, 'get_slice_parallel_group'):\n        # Some DeepSpeed + pipeline parallelism versions\n        return mpu.get_slice_parallel_group()\n    else:\n        # Deprecated Megatron and DeepSpeed convention\n        return mpu.get_model_parallel_group()\n\n\ndef bwc_pipeline_parallel_world_size(mpu=None):\n    \"\"\"Backwards-compatible way of querying the pipeline parallel world size.\"\"\"\n    world_size = 1\n    if mpu is not None:\n        if hasattr(mpu, 'get_pipeline_model_parallel_world_size'):\n            # New Megatron and DeepSpeed convention (post pipeline-parallelism release)\n            world_size = mpu.get_pipeline_model_parallel_world_size()\n        elif hasattr(mpu, 'get_pipe_parallel_world_size'):\n            # DeepSpeed Topology\n            world_size = mpu.get_pipe_parallel_world_size()\n    return world_size\n\n\ndef bwc_pipeline_parallel_group(mpu=None):\n    \"\"\"Backwards-compatible way of querying the pipeline parallel group.\"\"\"\n    if mpu is None:\n        return None\n    if hasattr(mpu, 'get_pipeline_model_parallel_group'):\n        # Megatron\n        return mpu.get_pipeline_model_parallel_group()\n    elif hasattr(mpu, 'get_pipe_parallel_group'):\n        # DeepSpeed Topology\n        return mpu.get_pipe_parallel_group()\n    assert False, 'mpu does not support pipeline parallel group'\n", "deepspeed/utils/z3_leaf_module.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nfrom typing import List, Type\n\n\ndef z3_leaf_module(model: torch.nn.Module) -> bool:\n    \"\"\"Returns whether a module in `model` has been flagged as a 'leaf' module.\n        See `set_z3_leaf_modules` for more details.\n        Args:\n            model (torch.nn.Module): The model to which the leaf module flag will be applied.\n        Returns:\n            bool: Whether the module has been flagged as a 'leaf' module.\n    \"\"\"\n    return hasattr(model, '_z3_leaf') and model._z3_leaf\n\n\ndef z3_leaf_parameter(model: torch.nn.Parameter) -> bool:\n    \"\"\"Returns whether a parameter belongs to a leaf module.\n        See `set_z3_leaf_modules` for more details.\n        Args:\n            model (torch.nn.Parameter): The parameter to which the leaf module flag will be applied.\n        Returns:\n            bool: Whether the parameter belongs to a leaf module.\n    \"\"\"\n    return hasattr(model, 'ds_z3_leaf_module')\n\n\ndef get_z3_leaf_modules(model: torch.nn.Module) -> List[torch.nn.Module]:\n    \"\"\"Returns a list of modules in `model` that have been flagged as 'leaf' modules.\n        See `set_z3_leaf_modules` for more details.\n        Args:\n            model (torch.nn.Module): The model to which the leaf module flag will be applied.\n        Returns:\n            List[torch.nn.Module]: A list of modules that have been flagged as 'leaf' modules.\n    \"\"\"\n    return [module for module in model.modules() if z3_leaf_module(module)]\n\n\ndef _do_set_z3_leaf_modules(model: torch.nn.Module, leaf_module_classes: List[Type],\n                            flag: bool) -> List[torch.nn.Module]:\n    assert all(isinstance(module_class, type) for module_class in leaf_module_classes), \\\n        f'leaf_module_classes must be a list of types, got {leaf_module_classes}'\n\n    leaf_modules = []\n\n    def _set_z3_leaf_flag(model: torch.nn.Module):\n        nonlocal leaf_modules\n        if model.__class__ in leaf_module_classes:\n            model._z3_leaf = flag\n            leaf_modules.append(model)\n\n    model.apply(_set_z3_leaf_flag)\n\n    if len(leaf_modules) == 0:\n        raise ValueError(f'No modules of type {leaf_module_classes} found in model {model}')\n\n    return leaf_modules\n\n\ndef set_z3_leaf_modules(model: torch.nn.Module, leaf_module_classes: List[Type]) -> List[torch.nn.Module]:\n    \"\"\"Sets a flag within a module in `model` to instruct ZeRO3 to stop setting hooks recursively when it encounters a module class listed in `leaf_module_classes`.\n       This is particularly useful in the context of Mixture of Experts (MoE) models. In MoE models, the computation order of experts varies across forward passes. This variability can disrupt ZeRO3's functionality, as ZeRO3 relies on tracking the computation order of modules to prefetch parameters efficiently. By designating a module as a 'leaf' node, ZeRO3 will prefetch parameters for all child modules upon entering the module.\n       Another scenario where this functionality is beneficial is in models with excessively fine-grained nested modules, where it helps to avoid the overhead associated with hooks.\n        Args:\n            model (torch.nn.Module): The model to which the leaf module flag will be applied.\n            leaf_module_classes (List[Type]): A list of module classes that should be flagged as 'leaf' modules.\n        Returns:\n            List[torch.nn.Module]: A list of modules that match the module classes in `leaf_module_classes`.\n    \"\"\"\n    return _do_set_z3_leaf_modules(model, leaf_module_classes, True)\n\n\ndef unset_z3_leaf_modules(model: torch.nn.Module, leaf_module_classes: List[Type]) -> List[torch.nn.Module]:\n    \"\"\"Unsets a flag within a module in `model` to instruct ZeRO3 to resume setting hooks recursively when it encounters a module class listed in `leaf_module_classes`.\n        See `set_z3_leaf_modules` for more details.\n        Args:\n            model (torch.nn.Module): The model to which the leaf module flag will be applied.\n            leaf_module_classes (List[Type]): A list of module classes that should be flagged as 'leaf' modules.\n        Returns:\n            List[torch.nn.Module]: A list of modules that match the module classes in `leaf_module_classes`.\n    \"\"\"\n    return _do_set_z3_leaf_modules(model, leaf_module_classes, False)\n", "deepspeed/utils/groups.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\n# The file has been adapted from https://github.com/NVIDIA/Megatron-LM and retains the following license from the original file\n\n# Copyright (c) 2019, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\n Support different forms of parallelism in DeepSpeed using multiple process groups.\n Given that there are multiple scenarios and use-cases, this file is going to be updated\n frequently. For now, the group creation needed for the training scenario is being implemented.\n For inference and other new scenarios, the code will be either reused or added to this file.\n\"\"\"\n\nfrom deepspeed import comm as dist\nfrom deepspeed.utils import log_dist\nfrom deepspeed.utils.bwc import bwc_tensor_model_parallel_world_size, bwc_pipeline_parallel_world_size\nfrom deepspeed.utils.exceptions import DeprecatedException\nfrom deepspeed.accelerator import get_accelerator\n# Expert parallel group that the current rank belongs to.\n_EXPERT_PARALLEL_GROUP = {}\n# Expert data parallel group that the current rank belongs to.\n_EXPERT_DATA_PARALLEL_GROUP = {}\n# dist world group needs to be cloned for some cases\n_WORLD_GROUP = None\n# ZeRO parameter  partitioning group that the current rank belongs to.\n_ZERO_PARAM_INTRA_PARALLEL_GROUP = None\n# global object to maintain mpu object if passed by a Megatron client\nmpu = None\n# global object that stores tensor parallel world size for experts\nexpert_tensor_parallel_world_size = 1\n# All to All quantized graident communication groups\n_ALL_TO_ALL_GROUP = {}\n\n_DATA_PARALLEL_GROUP = None\n\n\n# Deprecated groups initialize function.\ndef initialize(ep_size=1, mpu=None):\n    \"\"\" Deprecated function. Retained to inform the users.\"\"\"\n    raise DeprecatedException(\n        \"Please do not use the groups.initialize() API as it is deprecated. Instead, pass the desired ep_size to deepspeed.moe.layer.MoE(..,ep_size,..)\"\n    )\n\n\ndef _ensure_divisibility(numerator, denominator):\n    \"\"\"Ensure that numerator is divisible by the denominator.\"\"\"\n    assert numerator % denominator == 0, '{} is not divisible by {}'.format(numerator, denominator)\n\n\n# Not currently used. Helper function to create a model (tensor) parallel group.\ndef _create_model_parallel(model_parallel_size_):\n    \"\"\"\n    Initialize model data parallel groups.\n\n    Arguments:\n        model_parallel_size: number of GPUs used to parallelize model.\n\n    Returns:\n        Tuple of data parallel group and model parallel group\n\n    Let's say we have a total of 8 GPUs denoted by g0 ... g7 and we\n    use 2 GPUs to parallelize the model. The present function will\n    create 4 model parallel groups and 2 data parallel groups as:\n        4 model parallel groups:\n            [g0, g1], [g2, g3], [g4, g5], [g6, g7]\n        2 data parallel groups:\n            [g0, g2, g4, g6], [g1, g3, g5, g7]\n    Note that for efficiency, the caller should make sure adjacent ranks\n    are on the same DGX box. For example if we are using 2 DGX-1 boxes\n    with a total of 16 GPUs, rank 0 to 7 belong to the first box and\n    ranks 8 to 15 belong to the second box.\n    \"\"\"\n    log_dist(f'Creating model parallel group with size {model_parallel_size_}', ranks=[0])\n    # Get world size and rank. Ensure some consistencies.\n    assert dist.is_initialized()\n    world_size = dist.get_world_size()\n    model_parallel_size = min(model_parallel_size_, world_size)\n    _ensure_divisibility(world_size, model_parallel_size)\n    rank = dist.get_rank()\n\n    _DATA_PARALLEL_GROUP = None\n    _MODEL_PARALLEL_GROUP = None\n    # Build the data parallel groups.\n    for i in range(model_parallel_size):\n        ranks = range(i, world_size, model_parallel_size)\n        group = dist.new_group(ranks)\n        if i == (rank % model_parallel_size):\n            _DATA_PARALLEL_GROUP = group\n\n    # Build the model parallel groups.\n    for i in range(world_size // model_parallel_size):\n        ranks = range(i * model_parallel_size, (i + 1) * model_parallel_size)\n        group = dist.new_group(ranks)\n        if i == (rank // model_parallel_size):\n            _MODEL_PARALLEL_GROUP = group\n\n    return _DATA_PARALLEL_GROUP, _MODEL_PARALLEL_GROUP\n\n\ndef _create_expert_and_data_parallel(expert_parallel_size_, use_data_before_expert_parallel_=False):\n    \"\"\"\n        Create expert and data parallel groups.\n\n        Note: Caller of this function is responsible to check if the groups already exist.\n\n        Example - E + D parallel\n        world_size = 16\n        expert_parallel_size = 2 # number of experts in same group\n        expert_data_parallel_group = [0,2,4,6,8,10,12,14], [1,3,5,7,9,11,13,15] - all reduce is only on MoE params\n        expert_parallel_group = [0, 1], [2,3], [4,5], [6,7], [8,9] - no all reduce, but all to all\n        data_parallel_group = [0,1,...,15] - all reduce is only on non-MoE\n        use_data_before_expert_parallel_ (bool): Use the D + E instead of E + D topology\n    \"\"\"\n    assert dist.is_initialized()\n\n    log_dist(f'Creating expert and data parallel groups with size {expert_parallel_size_}', ranks=[0])\n    world_size = dist.get_world_size()\n    pp_world_size = 1 if mpu is None else bwc_pipeline_parallel_world_size(mpu)\n    rank = dist.get_rank()\n\n    pp_stride = world_size // pp_world_size\n    _ensure_divisibility(pp_stride, expert_parallel_size_)\n\n    group_name = f\"ep_size_{expert_parallel_size_}\"\n\n    # Build the expert data parallel groups.\n    global _EXPERT_DATA_PARALLEL_GROUP\n\n    ep_stride = pp_stride // expert_parallel_size_\n\n    # Only create group if it does not already exist\n    if group_name not in _EXPERT_DATA_PARALLEL_GROUP:\n        for pp_stage_start in range(0, world_size, pp_stride):\n            for i in range(expert_parallel_size_):\n                if use_data_before_expert_parallel_:\n                    ranks = range(pp_stage_start + i * ep_stride, pp_stage_start + (i + 1) * ep_stride)\n                else:\n                    ranks = range(pp_stage_start + i, pp_stage_start + pp_stride, expert_parallel_size_)\n                group = dist.new_group(ranks)\n                log_dist(\n                    f'Creating expert data parallel process group named {group_name} '\n                    f'with ranks: {list(ranks)}', [0])\n                if rank in ranks:\n                    _EXPERT_DATA_PARALLEL_GROUP[group_name] = group\n\n    # Build the expert parallel groups.\n    global _EXPERT_PARALLEL_GROUP\n\n    # Only create group if it does not already exist\n    if group_name not in _EXPERT_PARALLEL_GROUP:\n        if use_data_before_expert_parallel_:\n            for pp_stage_start in range(0, world_size, pp_stride):\n                for i in range(ep_stride):\n                    ranks = range(pp_stage_start + i, pp_stage_start + pp_stride, ep_stride)\n                    group = dist.new_group(ranks)\n                    log_dist(\n                        f'creating expert parallel process group named {group_name} '\n                        f'with ranks: {list(ranks)}', [0])\n                    if rank in ranks:\n                        _EXPERT_PARALLEL_GROUP[group_name] = group\n        else:\n            for i in range(world_size // expert_parallel_size_):\n                ranks = range(i * expert_parallel_size_, (i + 1) * expert_parallel_size_)\n                group = dist.new_group(ranks)\n                log_dist(f'creating expert parallel process group named {group_name} '\n                         f'with ranks: {list(ranks)}', [0])\n                if rank in ranks:\n                    _EXPERT_PARALLEL_GROUP[group_name] = group\n\n\ndef _get_expert_parallel_ranks(world_size,\n                               tensor_parallel_size_,\n                               expert_parallel_size_,\n                               pipeline_parallel_size_=1,\n                               use_data_before_expert_parallel_=False):\n    \"\"\"Generate expert parallel and expert data parallel group ranks list.\n\n        Example - E + M + D parallel\n        world_size = 16\n        model_degree = 2\n        expert_degree = 4 # number of experts in same group\n        mp_group = [0, 1], [2,3], [4,5] ...\n        data_parallel_group =[0,2,4,6,8,10, 12,14],                 [1,3,5,7,9,11,13,15]\n        expert_parallel_group = [0,2,4,6], [8,10,12,14]             [1,3,5,7], [9,11,13,15]\n        expert_data_parallel_group = [0,8],[2,10],[4,12],[6,14],    [1,9],[3,11],[5,13],[7,15]\n\n    Args:\n        world_size (int): Distributed world size.\n        tensor_parallel_size_ (int): Tensor parallel group size.\n        expert_parallel_size_ (int): Expert parallel group size.\n        pipeline_parallel_size_ (int): Pipeline parallel group size\n        use_data_before_expert_parallel_ (bool): Use the D + E instead of E + D topology\n    Returns:\n        Expert parallel group ranks and Expert data parallel group ranks list.\n    \"\"\"\n    _ensure_divisibility(world_size, tensor_parallel_size_ * pipeline_parallel_size_)\n    dp_world_size = world_size // (tensor_parallel_size_ * pipeline_parallel_size_)\n    _ensure_divisibility(dp_world_size, expert_parallel_size_)\n\n    # Generate data parallel groups\n    data_parallel_groups = []\n    dp_group_size = tensor_parallel_size_\n    pp_stride = world_size // pipeline_parallel_size_\n\n    if use_data_before_expert_parallel_:\n        dp_stride = world_size // expert_parallel_size_ // tensor_parallel_size_ // pipeline_parallel_size_\n        for pp_stage_start in range(0, world_size, pp_stride):\n            pp_stage_next = pp_stage_start + pp_stride\n            for i in range(dp_group_size):\n                data_parallel_groups.append(list())\n                for ds in range(dp_stride):\n                    # [0, 4, 8, 12, 16, 20, 24, 28, 2, 6, 10, 14, 18, 22, 26, 30]\n                    # [1, 5, 9, 13, 17, 21, 25, 29, 3, 7, 11, 15, 19, 23, 27, 31]\n                    data_parallel_groups[-1].extend(\n                        list(\n                            range(pp_stage_start + i + ds * tensor_parallel_size_, pp_stage_next,\n                                  dp_stride * tensor_parallel_size_)))\n    else:\n        for pp_stage_start in range(0, world_size, pp_stride):\n            pp_stage_next = pp_stage_start + pp_stride\n            for i in range(dp_group_size):\n                data_parallel_groups.append(list(range(pp_stage_start + i, pp_stage_next, dp_group_size)))\n\n    expert_parallel_groups = []\n    expert_data_parallel_groups = []\n    for dp_ranks in data_parallel_groups:\n        # partition of expert parallel groups, e.g. [0,2,4,6], [8,10,12,14]\n        part_ep_groups = []\n        for i in range(0, dp_world_size, expert_parallel_size_):\n            part_ep_groups.append(dp_ranks[i:i + expert_parallel_size_])\n        expert_parallel_groups.extend(part_ep_groups)\n\n        # zip part_ep_groups get expert data parallel ranks, e.g [0,8],[2,10],[4,12],[6,14]\n        for expert_dp_ranks in zip(*part_ep_groups):\n            expert_data_parallel_groups.append(list(expert_dp_ranks))\n\n    return expert_parallel_groups, expert_data_parallel_groups\n\n\ndef _create_expert_data_and_model_parallel(expert_parallel_size_, mpu, use_data_before_expert_parallel_=False):\n    \"\"\"\n        Create expert and data parallel groups based on MPU (model parallel) group.\n\n        Note: Caller of this function is responsible to check if the groups already exist.\n\n        Example - E + M + D parallel\n        world_size = 16\n        model_degree = 2\n        expert_degree = 4 # number of experts in same group\n        mp_group = [0, 1], [2,3], [4,5] ...\n        data_parallel_group =[0,2,4,6,8,10, 12,14],                 [1,3,5,7,9,11,13,15]\n        expert_parallel_group = [0,2,4,6], [8,10,12,14]             [1,3,5,7], [9,11,13,15]\n        expert_data_parallel_group = [0,8],[2,10],[4,12],[6,14],    [1,9],[3,11],[5,13],[7,15]\n    \"\"\"\n    assert dist.is_initialized(), \"dist is not initialized\"\n    tensor_parallel_size_ = bwc_tensor_model_parallel_world_size(mpu)\n\n    global expert_tensor_parallel_world_size\n    expert_tensor_parallel_world_size = tensor_parallel_size_\n\n    world_size = dist.get_world_size()\n    rank = dist.get_rank()\n    dp_world_size = mpu.get_data_parallel_world_size()\n    pp_world_size = 1 if mpu is None else bwc_pipeline_parallel_world_size(mpu)\n\n    _ensure_divisibility(world_size, tensor_parallel_size_)\n    _ensure_divisibility(dp_world_size, expert_parallel_size_)\n\n    log_dist(\n        f\"Creating deepspeed groups with model parallel size {tensor_parallel_size_}, \"\n        f\"pipeline parallel size {pp_world_size}, expert parallel size {expert_parallel_size_}, \"\n        f\"world size {world_size}, dp world size {dp_world_size}\", [0])\n\n    global _EXPERT_PARALLEL_GROUP, _EXPERT_DATA_PARALLEL_GROUP\n\n    group_name = f\"ep_size_{expert_parallel_size_}\"\n\n    # Only create groups if they don't already exist\n    # Need to check conditions outside the group creation loop because of the way torch.dist group creation works\n    if group_name not in _EXPERT_DATA_PARALLEL_GROUP and group_name not in _EXPERT_PARALLEL_GROUP:\n        expert_parallel_groups, expert_data_parallel_groups = _get_expert_parallel_ranks(\n            world_size, tensor_parallel_size_, expert_parallel_size_, pp_world_size, use_data_before_expert_parallel_)\n        for ranks in expert_parallel_groups:\n            group = dist.new_group(ranks)\n            if rank in list(ranks):\n                _EXPERT_PARALLEL_GROUP[group_name] = group\n\n        for ranks in expert_data_parallel_groups:\n            group = dist.new_group(ranks)\n            if rank in list(ranks):\n                _EXPERT_DATA_PARALLEL_GROUP[group_name] = group\n\n\ndef _get_max_expert_size():\n    \"\"\"Get the maximum ep_size from all the created groups.\"\"\"\n    assert _EXPERT_PARALLEL_GROUP is not None, \"Warning! Process group not initialized\"\n    keylist = []\n    for key in _EXPERT_PARALLEL_GROUP.keys():\n        # index 2 is ep_size in the group name: ep_size_<ep_size>\n        index = 2\n        keylist.append(int(key.split('_')[index]))\n    return max(keylist) if len(keylist) > 0 else None\n\n\ndef _get_max_expert_size_name():\n    \"\"\"Get the name of the group with max. ep_size\"\"\"\n    return f'ep_size_{_get_max_expert_size()}'\n\n\ndef _get_max_expert_parallel_group():\n    \"\"\"Get the max expert parallel size.\"\"\"\n    return _get_expert_parallel_group(_get_max_expert_size_name())\n\n\ndef _get_expert_parallel_group(group_name):\n    \"\"\"Get the expert parallel group the caller rank belongs to.\"\"\"\n    assert group_name in _EXPERT_PARALLEL_GROUP, \\\n        'expert parallel group is not initialized'\n    return _EXPERT_PARALLEL_GROUP[group_name]\n\n\ndef _get_expert_parallel_group_dict():\n    \"\"\"Get the expert parallel group dict.\"\"\"\n    return _EXPERT_PARALLEL_GROUP\n\n\ndef _get_expert_data_parallel_group(group_name):\n    \"\"\"Get the expert data parallel group the caller rank belongs to.\"\"\"\n    assert group_name in _EXPERT_DATA_PARALLEL_GROUP, \\\n        'expert data parallel group is not initialized'\n    return _EXPERT_DATA_PARALLEL_GROUP[group_name]\n\n\ndef _get_expert_data_parallel_group_dict():\n    \"\"\"Get the expert data parallel group dict.\"\"\"\n    return _EXPERT_DATA_PARALLEL_GROUP\n\n\ndef _clone_world_group():\n    \"\"\"Create a clone of the world group\n        Note: We need to clone the dist world group because we\n        use dist.get_global_rank() utility function in DeepSpeed at many places.\n        As that function does not work on dist.group.WORLD, we\n        need to keep a clone of it.\n    \"\"\"\n    assert dist.is_initialized(), \"dist is not initialized\"\n    global _WORLD_GROUP\n    if _WORLD_GROUP is None:\n        # If not cloned already, clone the world group\n        _WORLD_GROUP = dist.new_group(ranks=range(dist.get_world_size()))\n    return _WORLD_GROUP\n\n\ndef _get_local_all_to_all_group():\n    assert dist.is_initialized(), 'dist is not initialized'\n    global _ALL_TO_ALL_GROUP\n    device_per_node = get_accelerator().device_count()\n    num_local = dist.get_world_size() // device_per_node\n    if num_local == 0 and dist.get_world_size() > 0:\n        assert dist.get_world_size() >= 1, 'num_gpus must >=1, cannot initialize All-To-All'\n        cur_rank = []\n        for i in range(dist.get_world_size()):\n            cur_rank.append(i)\n        _ALL_TO_ALL_GROUP['local_0'] = dist.new_group(ranks=cur_rank)\n    elif num_local == 1:\n        assert dist.get_world_size(\n        ) == device_per_node, 'num_gpus not equal to device per node, cannot initialize All-To-All'\n        _ALL_TO_ALL_GROUP['local_0'] = dist.new_group(ranks=[i for i in range(device_per_node)])\n    else:\n        assert dist.get_world_size() > device_per_node, 'num_nodes<2 cannot initialize All-To-All'\n        for i in range(num_local):\n            local_rank = [j + device_per_node * i for j in range(device_per_node)]\n            _ALL_TO_ALL_GROUP[f\"local_{i}\"] = dist.new_group(ranks=local_rank)\n\n        for i in range(device_per_node):\n            cur_rank = []\n            for j in range(num_local):\n                cur_rank.append(i + j * device_per_node)\n            _ALL_TO_ALL_GROUP[f\"global_{i}\"] = dist.new_group(ranks=cur_rank)\n    return _ALL_TO_ALL_GROUP\n\n\ndef _get_data_parallel_group():\n    \"\"\"Get the data parallel group the caller rank belongs to.\"\"\"\n    assert dist.is_initialized(), 'dist is not initialized'\n    global mpu\n    if mpu is not None:\n        return mpu.get_data_parallel_group()\n    # Return the clone of dist world group\n    return _clone_world_group()\n\n\ndef _get_broadcast_src_rank():\n    return dist.get_global_rank(_get_sequence_data_parallel_group(), 0)\n\n\ndef _get_expert_broadcast_src_rank(group_name):\n    return dist.get_global_rank(_get_expert_data_parallel_group(group_name), 0)\n\n\ndef _get_expert_parallel_world_size(group_name):\n    \"\"\"Return world size for the expert parallel group.\"\"\"\n    return dist.get_world_size(group=_get_expert_parallel_group(group_name))\n\n\ndef _get_expert_data_parallel_world_size(group_name):\n    \"\"\"Return world size for the expert data parallel group.\"\"\"\n    return dist.get_world_size(group=_get_expert_data_parallel_group(group_name))\n\n\ndef _get_expert_parallel_rank(group_name):\n    \"\"\"Return my rank for the expert parallel group.\"\"\"\n    return dist.get_rank(group=_get_expert_parallel_group(group_name))\n\n\ndef _get_expert_parallel_src_rank(group_name):\n    \"\"\"Calculate the global rank corresponding to a local rank zero\n    in the expert parallel group.\"\"\"\n    global_rank = dist.get_rank()\n    local_world_size = _get_expert_parallel_world_size(group_name)\n    return (global_rank // local_world_size) * local_world_size\n\n\ndef _get_expert_data_parallel_rank(group_name):\n    \"\"\"Return my rank for the expert data parallel group.\"\"\"\n    return dist.get_rank(group=_get_expert_data_parallel_group(group_name))\n\n\ndef _get_data_parallel_world_size():\n    \"\"\"Return world size for the data parallel group.\"\"\"\n    global mpu\n    if mpu is not None:\n        return mpu.get_data_parallel_world_size()\n    return dist.get_world_size(group=_get_data_parallel_group())\n\n\ndef _get_model_parallel_world_size():\n    \"\"\"Return world size for the model parallel group.\"\"\"\n    global mpu\n    if mpu is not None:\n        return mpu.get_model_parallel_world_size()\n    return 1\n\n\ndef _get_data_parallel_rank():\n    \"\"\"Return my rank for the data parallel group.\"\"\"\n    return dist.get_rank(group=_get_data_parallel_group())\n\n\ndef _get_sequence_parallel_world_size():\n    \"\"\"Return world size for the model parallel group.\"\"\"\n    global mpu\n    if mpu is not None and hasattr(mpu, 'get_sequence_parallel_world_size'):\n        return mpu.get_sequence_parallel_world_size()\n    return 1\n\n\ndef _get_sequence_parallel_rank():\n    \"\"\"Return my rank for the data parallel group.\"\"\"\n    global mpu\n    if mpu is not None and hasattr(mpu, 'get_sequence_parallel_rank'):\n        return mpu.get_sequence_parallel_rank()\n    return 0\n\n\ndef _get_sequence_parallel_group():\n    global mpu\n    if mpu is not None and hasattr(mpu, 'get_sequence_parallel_group'):\n        return mpu.get_sequence_parallel_group()\n    return None\n\n\ndef _get_sequence_data_parallel_world_size():\n    \"\"\"Return world size for the model parallel group.\"\"\"\n    global mpu\n    if mpu is not None and hasattr(mpu, 'get_sequence_data_parallel_world_size'):\n        return mpu.get_sequence_data_parallel_world_size()\n    return _get_data_parallel_world_size()\n\n\ndef _get_sequence_data_parallel_rank():\n    \"\"\"Return my rank for the data parallel group.\"\"\"\n    global mpu\n    if mpu is not None and hasattr(mpu, 'get_sequence_data_parallel_rank'):\n        return mpu.get_sequence_data_parallel_rank()\n    return _get_data_parallel_rank()\n\n\ndef _get_sequence_data_parallel_group():\n    global mpu\n    # When sequence parallelism is enabled, the process group for zero sharding and\n    # gradient allreduce must be across both dimensions of data and sequence parallelism.\n    if mpu is not None and hasattr(mpu, 'get_sequence_data_parallel_group'):\n        return mpu.get_sequence_data_parallel_group()\n    return _get_data_parallel_group()\n\n\ndef _get_expert_model_parallel_world_size():\n    global expert_tensor_parallel_world_size\n    return expert_tensor_parallel_world_size\n\n\ndef _create_zero_param_parallel_group(group_size):\n    \"\"\"\n        Create parameter partitioning group within ZeRO data parallel groups.\n\n        Example - ZP + D parallel\n        world_size = 16\n        zero_hpz_partition_size = 2 # number of ranks with replicated params (dual partitioning)\n        zero_param_intra_parallel_group = [0, 1], [2,3], [4,5], [6,7], [8,9] - segmented (subgroup) with rep partition\n        data_parallel_group = [0,1,...,15] - all reduce is on ZeRO model\n    \"\"\"\n    assert dist.is_initialized()\n    global _ZERO_PARAM_INTRA_PARALLEL_GROUP\n    # Only create group if it does not already exist\n    assert _ZERO_PARAM_INTRA_PARALLEL_GROUP is None, \\\n        'ZeRO parameter intra parallel group is already initialized'\n\n    world_size = dist.get_world_size()\n    rank = dist.get_rank()\n\n    zero_param_parallel_size_ = min(group_size, world_size)\n    _ensure_divisibility(world_size, zero_param_parallel_size_)\n\n    # Build the ZeRO param intra parallel groups.\n    for i in range(world_size // zero_param_parallel_size_):\n        ranks = range(i * zero_param_parallel_size_, (i + 1) * zero_param_parallel_size_)\n        group = dist.new_group(ranks)\n        if i == (rank // zero_param_parallel_size_):\n            _ZERO_PARAM_INTRA_PARALLEL_GROUP = group\n\n\ndef _get_zero_param_intra_parallel_group():\n    \"\"\"Get the ZeRO parameter partitioning intra parallel group the caller rank belongs to.\"\"\"\n    #assert _ZERO_PARAM_INTRA_PARALLEL_GROUP is not None, \\\n    #    'ZeRO parameter partitioning group is not initialized'\n    #TODO: Add warning\n    return _ZERO_PARAM_INTRA_PARALLEL_GROUP\n\n\ndef _zero_param_parallel_is_initialized():\n    \"\"\"Check if ZeRO data parallel with parameter partititioning groups are initialized.\"\"\"\n    ###TODO: assert that MPU is not set\n    if _ZERO_PARAM_INTRA_PARALLEL_GROUP is None and _DATA_PARALLEL_GROUP is None:\n        return False\n\n\ndef _get_zero_param_intra_parallel_rank_in_mygroup():\n    \"\"\"Return my rank for the ZeRO parameter inter parallel group.\"\"\"\n    return dist.get_rank(group=_get_zero_param_intra_parallel_group())\n\n\ndef _get_zero_param_intra_parallel_group_world_size():\n    \"\"\"Return world size for the ZeRO parameter parallel group.\"\"\"\n    return dist.get_world_size(group=_get_zero_param_intra_parallel_group())\n\n\ndef _get_zero_param_intra_parallel_group_ranks():\n    \"\"\"Return all ranks for the ZeRO parameter intra parallel group.\"\"\"\n    return dist.get_all_ranks_from_group(group=_get_zero_param_intra_parallel_group())\n", "deepspeed/utils/types.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom enum import IntEnum\n\n\nclass ActivationFuncType(IntEnum):\n    UNKNOWN = 0\n    GELU = 1\n    ReLU = 2\n    GATED_GELU = 3\n    GATED_SILU = 4\n\n\nGATED_ACTIVATION_TYPES = [\n    ActivationFuncType.GATED_GELU,\n    ActivationFuncType.GATED_SILU,\n]\n\n\nclass NormType(IntEnum):\n    UNKNOWN = 0\n    LayerNorm = 1\n    GroupNorm = 2\n    RMSNorm = 3\n", "deepspeed/utils/nvtx.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom deepspeed.accelerator import get_accelerator\n\n\ndef instrument_w_nvtx(func):\n    \"\"\"decorator that causes an NVTX range to be recorded for the duration of the\n    function call.\"\"\"\n\n    def wrapped_fn(*args, **kwargs):\n        get_accelerator().range_push(func.__qualname__)\n        ret_val = func(*args, **kwargs)\n        get_accelerator().range_pop()\n        return ret_val\n\n    return wrapped_fn\n", "deepspeed/utils/mixed_precision_linkage.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport types\nfrom deepspeed.utils import get_full_hp_param, get_full_hp_grad, get_hp_fragment_mapping\nfrom deepspeed.utils import set_full_hp_param\n\n\ndef link_hp_params(lp_param_list, flat_hp_partition, gradient_dict, offload_gradient_dict, use_offload,\n                   param_group_index, partition_start, partition_size, dp_group):\n    local_lp_param_and_offset = _init_lp_to_hp_mapping(lp_param_list, partition_start, partition_size, dp_group)\n\n    for lp_param, lp_start in local_lp_param_and_offset:\n        lp_param._hp_mapping = get_hp_fragment_mapping(lp_param, lp_start, flat_hp_partition, gradient_dict,\n                                                       offload_gradient_dict, use_offload, param_group_index,\n                                                       partition_start, partition_size)\n\n\ndef lazy_init_hp_params_optimizer_state(lp_param_list, flat_hp_partition, optimizer_state):\n    for lp in lp_param_list:\n        if lp._hp_mapping is not None:\n            lp._hp_mapping.set_optim_state_fragment(flat_hp_partition, optimizer_state[flat_hp_partition])\n\n\ndef _init_lp_to_hp_mapping(lp_param_list, partition_start, partition_size, dp_group):\n    current_offset = 0\n    param_and_offset_list = []\n    partition_end = partition_start + partition_size\n    index_in_param_group = 0\n    for i, lp_param in enumerate(lp_param_list):\n        lp_param._hp_mapping = None\n        lp_param._dp_group = dp_group\n        lp_param.get_full_hp_param = types.MethodType(get_full_hp_param, lp_param)\n        lp_param.get_full_hp_grad = types.MethodType(get_full_hp_grad, lp_param)\n        lp_param.set_full_hp_param = types.MethodType(set_full_hp_param, lp_param)\n\n        # lp_param overlaps with partition if both are true\n        # 1) current_offset < partition_end,\n        # 2) current_offset + lp_param.numel() >= partition_start\n        lp_param_end = current_offset + lp_param.numel()\n        if current_offset < partition_end and lp_param_end > partition_start:\n            param_and_offset_list.append((lp_param, current_offset))\n            lp_param._index_in_param_group = index_in_param_group\n            # Indices for params in this partition/GPU\n            index_in_param_group += 1\n        current_offset += lp_param.numel()\n\n    return param_and_offset_list\n", "deepspeed/utils/comms_logging.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport math\nfrom deepspeed.utils import log_dist\n\n\ndef get_caller_func(frame=3):\n    import sys\n    return sys._getframe(frame).f_code.co_name\n\n\ndef print_rank_0(message):\n    import deepspeed.comm as dist\n    if dist.get_rank() == 0:\n        print(message)\n\n\n# Helper function to pretty-print message sizes\ndef convert_size(size_bytes):\n    if size_bytes == 0:\n        return \"0B\"\n    size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\n    i = int(math.floor(math.log(size_bytes, 1024)))\n    p = math.pow(1024, i)\n    s = round(size_bytes / p, 2)\n    return \"%s %s\" % (s, size_name[i])\n\n\n# Helper function to calculate algbw and busbw.\n# See https://gist.github.com/jeffra/b5e80466b4c86be00ea3b6f130fb7a36 and https://github.com/NVIDIA/nccl-tests/blob/master/doc/PERFORMANCE.md\ndef calc_bw_log(comm_op, size, duration):\n    import deepspeed.comm as dist\n\n    n = dist.get_world_size()\n    tput = 0\n    busbw = 0\n    if comm_op == \"all_to_all_single\":\n        tput = (size / duration)\n        busbw = (size / duration) * ((n - 1) / n)\n    elif comm_op == \"all_gather\" or comm_op == \"all_gather_into_tensor\" or comm_op == \"reduce_scatter\" or comm_op == \"reduce_scatter_tensor\":\n        size *= n\n        tput = (size / duration)\n        busbw = (size / duration) * ((n - 1) / n)\n    elif comm_op == \"all_reduce\" or comm_op == \"all_reduce_coalesced\" or comm_op == \"inference_all_reduce\":\n        tput = (size * 2 / duration)\n        busbw = (size / duration) * (2 * (n - 1) / n)\n    elif comm_op == \"send\" or comm_op == \"recv\" or comm_op == \"isend\" or comm_op == \"irecv\" or comm_op == \"broadcast\" or comm_op == \"reduce\" or comm_op == \"gather\" or comm_op == \"scatter\" or comm_op == \"barrier\":\n        tput = (size / duration)\n        busbw = tput\n    else:\n        print_rank_0(\"wrong comm_op specified\")  # noqa: F821\n        exit(0)\n\n    # convert to Gbps\n    tput *= 8\n    busbw *= 8\n\n    tput /= 1e6\n    busbw /= 1e6\n\n    return tput, busbw\n\n\nclass CommsLogger:\n\n    def __init__(self):\n        from deepspeed.comm.constants import COMMS_LOGGER_VERBOSE_DEFAULT, COMMS_LOGGER_DEBUG_DEFAULT, COMMS_LOGGER_PROF_OPS_DEFAULT, COMMS_LOGGER_PROF_ALL_DEFAULT, COMMS_LOGGER_ENABLED_DEFAULT\n        self.comms_dict = {}\n        self.verbose = COMMS_LOGGER_VERBOSE_DEFAULT\n        self.debug = COMMS_LOGGER_DEBUG_DEFAULT\n        self.prof_ops = COMMS_LOGGER_PROF_OPS_DEFAULT\n        self.prof_all = COMMS_LOGGER_PROF_ALL_DEFAULT\n        self.enabled = COMMS_LOGGER_ENABLED_DEFAULT\n\n    def configure(self, comms_config):\n        self.enabled = comms_config.comms_logger_enabled\n        if self.enabled:\n            self.verbose = comms_config.comms_logger.verbose\n            self.debug = comms_config.comms_logger.debug\n            self.prof_ops = comms_config.comms_logger.prof_ops\n            self.prof_all = comms_config.comms_logger.prof_all\n\n    # There are three settings for the op profiler:\n    # - Global profiling (profile all comms)\n    # - Op-type profiling (e.g. profile all all_reduce comms)\n    # - Op profiling (e.g. profile a specific all_reduce op)\n    def start_profiling_comms(self):\n        self.prof_all = True\n\n    def stop_profiling_comms(self):\n        self.prof_all = True\n\n    # E.g. start_profiling_op('all_reduce')\n    def start_profiling_op(self, op_name_list):\n        self.prof_ops = list(set(self.prof_ops) | set(op_name_list))\n\n    def stop_profiling_op(self, op_name_list):\n        self.prof_ops = [op for op in self.prof_ops if op not in op_name_list]\n\n    # Add log entry\n    def append(self, raw_name, record_name, latency, msg_size):\n        algbw, busbw = calc_bw_log(raw_name, msg_size, latency)\n        if record_name in self.comms_dict.keys():\n            # If this comm_op has already been logged with this message size, just add to existing record\n            if msg_size in self.comms_dict[record_name].keys():\n                self.comms_dict[record_name][msg_size][0] += 1\n                self.comms_dict[record_name][msg_size][1].append(latency)\n                self.comms_dict[record_name][msg_size][2].append(algbw)\n                self.comms_dict[record_name][msg_size][3].append(busbw)\n            # If this is a new message size for this comm_op, add new record under existing comm_op\n            else:\n                self.comms_dict[record_name][msg_size] = [1, [latency], [algbw], [busbw]]\n        else:\n            # Create entirely new record\n            self.comms_dict[record_name] = {msg_size: [1, [latency], [algbw], [busbw]]}\n        # If verbose, print every comm op\n        # TODO: Add to tensorboard\n        if self.verbose:\n            log_str = f\"comm op: {record_name} | time (ms): {latency:.2f} | msg size: {convert_size(msg_size)} | algbw (Gbps): {algbw:.2f} | busbw (Gbps): {busbw:.2f}\"\n            log_dist(log_str, [0])\n\n    # Print summary at end of iteration, epoch, or training\n    def log_all(self, print_log=True, show_straggler=False):\n        import torch\n        from deepspeed.utils.timer import trim_mean\n        import deepspeed.comm as dist\n        from deepspeed.comm.reduce_op import ReduceOp\n        if print_log:\n            print(\n                f\"{'Comm. Op': <20}{'Message Size': <20}{'Count': <20}{'Total Latency(ms)': <20}{'Avg Latency(ms)': <20}{'tput_avg (Gbps)': <20}{'busbw_avg (Gbps)': <20}\"\n            )\n        for record_name in self.comms_dict.keys():\n            if print_log:\n                print(record_name)\n            for msg_size, vals in sorted(self.comms_dict[record_name].items()):\n                # vals[0] is the count for each msg size\n                count = vals[0]\n                # vals[1] is a list of latency records for each msg size\n                total_lat = sum(vals[1])\n                # vals[2] and vals[3] are the lists of algbw and busbw, respectively\n                # Get rid of outliers when we print\n                avg_lat = trim_mean(vals[1], 0.1)\n                avg_algbw = trim_mean(vals[2], 0.1)\n                avg_busbw = trim_mean(vals[3], 0.1)\n                if print_log:\n                    print(\n                        f\"{' ': <20}{convert_size(msg_size): <20}{count: <20}{total_lat: <20.2f}{avg_lat: <20.2f}{avg_algbw: <20.2f}{avg_busbw: <20.2f}\"\n                    )\n\n        if show_straggler:\n            if print_log:\n                print(\"_______________________________\")\n                print(\"Breakdown with straggler effect\")\n                print(\"-------------------------------\")\n                print(\n                    f\"{'Comm. Op': <20}{'Message Size': <20}{'Count': <20}{'Total comm lat(ms)': <20}{'Total straggler(ms)': <20}{'Avg comm lat(ms)': <20}{'Avg straggler(ms)': <20}\"\n                )\n            for record_name in self.comms_dict.keys():\n                if print_log:\n                    print(record_name)\n                for msg_size, vals in sorted(self.comms_dict[record_name].items()):\n                    # vals[0] is the count for each msg size\n                    count = vals[0]\n                    # vals[1] is a list of latency records for each msg size\n                    lats = torch.tensor(vals[1])\n                    min_lats = torch.tensor(vals[1])\n                    dist.all_reduce(min_lats, op=ReduceOp.MIN)\n                    total_lat = min_lats.sum().item()\n                    total_straggler = (lats - min_lats).sum().item()\n                    avg_lat = trim_mean(min_lats.tolist(), 0.1)\n                    avg_straggler = trim_mean((lats - min_lats).tolist(), 0.1)\n                    if print_log:\n                        print(\n                            f\"{' ': <20}{convert_size(msg_size): <20}{count: <20}{total_lat: <20.2f}{total_straggler: <20.2f}{avg_lat: <20.2f}{avg_straggler: <20.2f}\"\n                        )\n", "deepspeed/utils/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .logging import logger, log_dist\nfrom .comms_logging import get_caller_func\n#from .distributed import init_distributed\nfrom .init_on_device import OnDevice\nfrom .groups import *\nfrom .nvtx import instrument_w_nvtx\n# TODO: Move tensor fragment and mixed precision to zero utils\nfrom .tensor_fragment import tensor_fragment, get_full_hp_param, get_hp_fragment_mapping, fragment_address, get_full_hp_grad, map_to_flat_opt_states\nfrom .tensor_fragment import safe_get_full_fp32_param, safe_get_full_grad, safe_get_full_optimizer_state\nfrom .tensor_fragment import set_full_hp_param\nfrom .tensor_fragment import safe_set_full_fp32_param, safe_set_full_optimizer_state\nfrom .tensor_fragment import safe_get_local_fp32_param, safe_get_local_grad, safe_get_local_optimizer_state\nfrom .tensor_fragment import safe_set_local_fp32_param, safe_set_local_optimizer_state\nfrom .z3_leaf_module import set_z3_leaf_modules, unset_z3_leaf_modules, get_z3_leaf_modules, z3_leaf_module, z3_leaf_parameter\nfrom .mixed_precision_linkage import link_hp_params, lazy_init_hp_params_optimizer_state\nfrom deepspeed.runtime.dataloader import RepeatingLoader\nfrom .numa import get_numactl_cmd\n", "deepspeed/utils/init_on_device.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nfrom typing import Callable\nfrom torch import Tensor\nfrom packaging import version as pkg_version\n\n\nclass OnDevice(object):\n    \"\"\"\n    Create modules/tensors w. specific devices and dtypes. Examples:\n\n    Create MyModule which consists of many different sub-modules and parameters. In this case we can create\n    MyModule as a collection of 'meta' tensors by passing `device='meta'` or we can create the module _directly_\n    on a CUDA device by passing `device=f'cuda:{local_rank}'` (where `local_rank` is the local GPU id.\n\n    with OnDevice(dtype=torch.float16, device='meta'):\n        model = MyModel()\n\n    with OnDevice(dtype=torch.float16, device=f'cuda:{local_rank}'):\n        model = MyModel()\n\n    \"\"\"\n\n    _orig_torch_empty = torch.empty\n    _orig_torch_zeros = torch.zeros\n    _orig_torch_ones = torch.ones\n    _orig_torch_full = torch.full\n\n    def __init__(self, dtype, device=\"meta\", enabled=True):\n        self.dtype = dtype\n        self.enabled = enabled\n        self.device = device\n\n        if device == \"meta\":\n            if pkg_version.parse('1.10') > pkg_version.parse(torch.__version__):\n                raise NotImplementedError(\"Meta tensor support is not available, please upgrade to torch 1.10+\")\n\n    def fp_tensor_constructor(self, fn: Callable, target_fp_dtype: torch.dtype) -> Callable:\n\n        def wrapped_fn(*args, **kwargs) -> Tensor:\n            if kwargs.get(\"device\", None) is None:\n                kwargs['device'] = self.device\n            tensor: Tensor = fn(*args, **kwargs)\n            if tensor.is_floating_point():\n                tensor = tensor.to(target_fp_dtype)\n            return tensor\n\n        return wrapped_fn\n\n    def get_new_tensor_fn_for_dtype(self, dtype: torch.dtype) -> Callable:\n\n        def new_tensor(cls, *args) -> Tensor:\n            tensor = OnDevice._orig_torch_empty(0, device=self.device).new_empty(*args)\n            if tensor.is_floating_point():\n                tensor = tensor.to(dtype)\n            return tensor\n\n        return new_tensor\n\n    def __enter__(self):\n        if not self.enabled:\n            return\n        torch.Tensor.__old_new__ = torch.Tensor.__new__\n        torch.Tensor.__new__ = self.get_new_tensor_fn_for_dtype(self.dtype)\n        torch.empty = self.fp_tensor_constructor(self._orig_torch_empty, self.dtype)\n        torch.zeros = self.fp_tensor_constructor(self._orig_torch_zeros, self.dtype)\n        torch.ones = self.fp_tensor_constructor(self._orig_torch_ones, self.dtype)\n        torch.full = self.fp_tensor_constructor(self._orig_torch_full, self.dtype)\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        if not self.enabled:\n            return\n        torch.Tensor.__new__ = torch.Tensor.__old_new__\n        torch.empty = self._orig_torch_empty\n        torch.zeros = self._orig_torch_zeros\n        torch.ones = self._orig_torch_ones\n        torch.full = self._orig_torch_full\n", "deepspeed/utils/numa.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n# return a list of list for cores to numa mapping\n# [\n#     [ cores for numa 0 ]\n#     [ cores belong to numa 1 ]\n#     ...\n# ]\n\nimport distutils\nimport os\nimport psutil\nimport subprocess\n\n\n# return a list of list for cores to numa mapping\n# [\n#     [ cores for numa 0 ]\n#     [ cores belong to numa 1 ]\n#     ...\n# ]\ndef get_numa_cores():\n    ret = []\n    output = subprocess.check_output(['numactl', '--hardware']).decode(\"utf-8\")\n    lines = output.split('\\n')\n    for line in lines:\n        if line.startswith('available:'):\n            num_numas = int(line.split(' ')[1])\n            break\n    for numa in range(num_numas):\n        for line in lines:\n            if line.startswith(f'node {numa} cpus:'):\n                cores = line.split(' ')[3:]\n                ret.append([int(core) for core in cores])\n    return ret\n\n\ndef check_for_numactl_pkg():\n    libs = dict(\n        dpkg=[\"-l\", \"numactl\", \"apt\"],\n        pacman=[\"-Q\", \"numactl\", \"pacman\"],\n        rpm=[\"-q\", \"numactl\", \"yum\"],\n    )\n\n    found = False\n    for pkgmgr, data in libs.items():\n        flag, lib, tool = data\n        path = distutils.spawn.find_executable(pkgmgr)\n        if path is not None:\n            cmd = f\"{pkgmgr} {flag} {lib}\"\n            result = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n            if result.wait() == 0:\n                found = True\n            else:\n                print(f\"please install the {lib} package with {tool}\")\n            break\n    return found\n\n\ndef parse_range(rng):\n    try:\n        value = int(rng)\n        return range(value, value + 1)\n    except ValueError:\n        # value is not a single number\n        parts = rng.split('-')\n        if len(parts) != 2:\n            raise ValueError(\"Bad range: '%s', range must be either a number or two number separated by dash\" %\n                             (rng, ))\n        start = int(parts[0])\n        end = int(parts[1])\n        if start > end:\n            raise ValueError(\"Bad range: '%s', range end must larger than or equal to start\" % (rng, ))\n        return range(start, end + 1)\n\n\n# parse comma and dash separated range list into list\n# i.e. \"0,2-4,6\" --> [0, 2, 3, 4, 6]\n# rules:\n# 1. Range list number be comma separated, each item are either a single number,\n#    or a range marked by two numbers (both number are included in the range)\n# 2. Sub ranges must be in ascend order and not overlap with each other\n# 3. No space in the range expression\ndef parse_range_list(range_str):\n    number_list = []\n    last = -1\n    range_list = range_str.split(',')\n    for sub_range in range_list:\n        sub_number_list = parse_range(sub_range)\n        if sub_number_list[0] <= last:\n            raise ValueError(\n                \"Bad range: '%s', sub ranges must not overlap with each other and should be in ascend order\" %\n                (range_str, ))\n        last = sub_number_list[-1]\n        number_list.extend(sub_number_list)\n    return number_list\n\n\ndef get_numactl_cmd(bind_core_list, num_local_procs, local_rank):\n    numactl_cmd = []\n    check_for_numactl_pkg()\n    if 'KMP_AFFINITY' in os.environ.keys():\n        raise ValueError(\"Environment variable KMP_AFFINITY conflicts with numactl \"\n                         \"because it interfere with how many CPU cores numactl can set. \"\n                         \"Unset KMP_AFFINITY before launching deepspeed.\\n\\n\"\n                         \"\\t$ unset KMP_AFFINITY\\n\"\n                         \"\\t$ deepspeed <deepspeed command parameters>\")\n    if bind_core_list is not None:\n        core_list = parse_range_list(bind_core_list)\n        total_cores = len(core_list)\n    else:\n        total_cores = psutil.cpu_count(logical=False)\n        core_list = range(total_cores)\n    cores_per_rank = total_cores // num_local_procs\n    assert cores_per_rank >= 1, \"At least one core needs to be assigned to each rank\"\n    core_list_for_rank = core_list[cores_per_rank * local_rank:cores_per_rank * (local_rank + 1)]\n    numactl_cmd.append(\"numactl\")\n\n    # check if all cores belong to same numa, if true, bind process to that numa domain with -m parameter\n    numa_cores = get_numa_cores()\n    num_numas = len(numa_cores)\n\n    numa_mode = \"normal\"\n\n    non_empty_numa_list = []\n    empty_numa_list = []\n    previous_numa_cores = []\n    numa_node_list = []\n    numa_node_list_list = []\n    for i in range(num_numas):\n        # look for empty numa which is HBM numa\n        if numa_cores[i] == []:\n            empty_numa_list.append(i)\n        else:\n            non_empty_numa_list.append(i)\n\n            # check for fakenuma\n            if numa_cores[i] == previous_numa_cores:\n                if numa_node_list == []:\n                    #first duplication, add previous node into list\n                    numa_node_list.append(i - 1)\n                numa_node_list.append(i)\n            else:\n                if numa_node_list != []:\n                    numa_node_list_list.append(numa_node_list)\n                    numa_node_list = []\n        previous_numa_cores = numa_cores[i]\n    if numa_node_list != []:\n        numa_node_list_list.append(numa_node_list)\n\n    if empty_numa_list != [] and len(empty_numa_list) == len(non_empty_numa_list):\n        numa_mode = \"flat_hbm\"\n        numa_dict = dict(zip(non_empty_numa_list, empty_numa_list))\n    elif numa_node_list_list != []:\n        numa_mode = \"fake\"\n\n    if numa_mode == \"normal\":\n        for i in range(num_numas):\n            if set(core_list_for_rank) <= set(numa_cores[i]):\n                numactl_cmd.append(\"-m\")\n                numactl_cmd.append(f\"{i}\")\n                break\n    elif numa_mode == \"flat_hbm\":\n        for i in range(num_numas):\n            if set(core_list_for_rank) <= set(numa_cores[i]):\n                numactl_cmd.append(\"-p\")\n                numactl_cmd.append(f\"{numa_dict[i]}\")\n                break\n    elif numa_mode == \"fake\":\n        for i in range(num_numas):\n            if set(core_list_for_rank) <= set(numa_cores[i]):\n                for nodes in numa_node_list_list:\n                    if i in nodes:\n                        numactl_cmd.append(\"-m\")\n                        numactl_cmd.append(f\"{','.join(map(str, nodes))}\")\n                        break\n                # the following construct break the outer loop if inner loop breaks\n                else:\n                    continue\n                break\n\n    numactl_cmd.append(\"-C\")\n    last_core = core_list_for_rank[0]\n    first_core = last_core\n    core_list_str = f\"{last_core}\"\n    for core_id in core_list_for_rank[1:]:\n        if core_id == last_core + 1:\n            last_core = core_id\n            continue\n        else:\n            if first_core == last_core:\n                core_list_str = f\"{core_list_str},{core_id}\"\n            else:\n                core_list_str = f\"{core_list_str}-{last_core},{core_id}\"\n            first_core = core_id\n            last_core = core_id\n    if first_core != last_core:\n        core_list_str = f\"{core_list_str}-{last_core}\"\n    numactl_cmd.append(f\"{core_list_str}\")\n    return cores_per_rank, numactl_cmd\n", "deepspeed/compression/config.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .constants import *\nimport copy\nfrom ..runtime.config_utils import get_scalar_param, get_list_param\n\n\ndef get_compression_config(param_dict):\n    #\n    output = {}\n\n    if COMPRESSION_TRAINING not in param_dict.keys():\n        param_dict[COMPRESSION_TRAINING] = {}\n    sub_param_dict = param_dict[COMPRESSION_TRAINING]\n    output[WEIGHT_QUANTIZATION] = get_weight_quantization(sub_param_dict)\n    output[ACTIVATION_QUANTIZATION] = get_activation_quantization(sub_param_dict)\n    output[SPARSE_PRUNING] = get_sparse_pruning(sub_param_dict)\n    output[ROW_PRUNING] = get_row_pruning(sub_param_dict)\n    output[HEAD_PRUNING] = get_head_pruning(sub_param_dict)\n    output[CHANNEL_PRUNING] = get_channel_pruning(sub_param_dict)\n\n    output[LAYER_REDUCTION] = get_layer_reduction(sub_param_dict)\n\n    return output\n\n\ndef get_layer_reduction(param_dict):\n    output = {}\n    output[LAYER_REDUCTION_ENABLED] = LAYER_REDUCTION_ENABLED_DEFAULT\n    if get_layer_reduction_enabled(param_dict):\n        output[LAYER_REDUCTION_ENABLED] = get_layer_reduction_enabled(param_dict)\n        for key, val in get_layer_reduction_params(param_dict).items():\n            output[key] = val\n    return output\n\n\ndef get_layer_reduction_enabled(param_dict):\n    if LAYER_REDUCTION in param_dict.keys():\n        return get_scalar_param(param_dict[LAYER_REDUCTION], LAYER_REDUCTION_ENABLED, LAYER_REDUCTION_ENABLED_DEFAULT)\n    else:\n        return False\n\n\ndef get_layer_reduction_params(param_dict):\n    if LAYER_REDUCTION in param_dict.keys():\n        layer_reduction_params = copy.copy(param_dict[LAYER_REDUCTION])\n        layer_reduction_params.pop(LAYER_REDUCTION_ENABLED)\n        return layer_reduction_params\n    else:\n        return False\n\n\ndef get_quantize_enabled(param_dict):\n    if COMPRESSION_TRAINING not in param_dict.keys():\n        return False\n\n    sub_param_dict = param_dict[COMPRESSION_TRAINING]\n    output = get_weight_quantization_shared_parameters(sub_param_dict)\n    return output[WEIGHT_QUANTIZE_ENABLED]\n\n\ndef get_weight_quantization(param_dict):\n    output = {}\n    if WEIGHT_QUANTIZATION not in param_dict.keys():\n        param_dict[WEIGHT_QUANTIZATION] = {SHARED_PARAMETERS: {}, DIFFERENT_GROUPS: {}}\n    sub_param_dict = param_dict[WEIGHT_QUANTIZATION]\n    # shared parameters\n    output[SHARED_PARAMETERS] = get_weight_quantization_shared_parameters(sub_param_dict)\n    # each sub-groups\n    if output[SHARED_PARAMETERS][WEIGHT_QUANTIZE_ENABLED]:\n        assert DIFFERENT_GROUPS in sub_param_dict.keys(\n        ), f\"Weigh Quantization is enabled, {DIFFERENT_GROUPS} must be specified\"\n    output[DIFFERENT_GROUPS] = get_weight_quantization_different_groups(sub_param_dict)\n    return output\n\n\ndef get_weight_quantization_shared_parameters(param_dict):\n    output = {}\n    if SHARED_PARAMETERS in param_dict.keys():\n        sub_param_dict = param_dict[SHARED_PARAMETERS]\n        output[WEIGHT_QUANTIZE_ENABLED] = get_scalar_param(sub_param_dict, WEIGHT_QUANTIZE_ENABLED,\n                                                           WEIGHT_QUANTIZE_ENABLED_DEFAULT)\n        output[WEIGHT_QUANTIZE_KERNEL] = get_scalar_param(sub_param_dict, WEIGHT_QUANTIZE_KERNEL,\n                                                          WEIGHT_QUANTIZE_KERNEL_DEFAULT)\n        output[WEIGHT_QUANTIZE_SCHEDULE_OFFSET] = get_scalar_param(sub_param_dict, WEIGHT_QUANTIZE_SCHEDULE_OFFSET,\n                                                                   WEIGHT_QUANTIZE_SCHEDULE_OFFSET_DEFAULT)\n        output[WEIGHT_QUANTIZE_GROUPS] = get_scalar_param(sub_param_dict, WEIGHT_QUANTIZE_GROUPS,\n                                                          WEIGHT_QUANTIZE_GROUPS_DEFAULT)\n        output[WEIGHT_QUANTIZE_VERBOSE] = get_scalar_param(sub_param_dict, WEIGHT_QUANTIZE_VERBOSE,\n                                                           WEIGHT_QUANTIZE_VERBOSE_DEFAULT)\n        output[WEIGHT_QUANTIZE_TYPE] = get_scalar_param(sub_param_dict, WEIGHT_QUANTIZE_TYPE,\n                                                        WEIGHT_QUANTIZE_TYPE_DEFAULT)\n        output[WEIGHT_QUANTIZE_IN_FORWARD_ENABLED] = get_scalar_param(sub_param_dict,\n                                                                      WEIGHT_QUANTIZE_IN_FORWARD_ENABLED,\n                                                                      WEIGHT_QUANTIZE_IN_FORWARD_ENABLED_DEFAULT)\n        assert output[WEIGHT_QUANTIZE_TYPE] in [\n            WEIGHT_QUANTIZE_SYMMETRIC, WEIGHT_QUANTIZE_ASYMMETRIC\n        ], f\"Invalid weight quantize type. Supported types: [{WEIGHT_QUANTIZE_SYMMETRIC}, {WEIGHT_QUANTIZE_ASYMMETRIC}]\"\n        output[WEIGHT_QUANTIZE_ROUNDING] = get_scalar_param(sub_param_dict, WEIGHT_QUANTIZE_ROUNDING,\n                                                            WEIGHT_QUANTIZE_ROUNDING_DEFAULT)\n        assert output[WEIGHT_QUANTIZE_ROUNDING] in [\n            WEIGHT_QUANTIZE_NEAREST_ROUNDING, WEIGHT_QUANTIZE_STOCHASTIC_ROUNDING\n        ], f\"Invalid weight quantize rounding. Supported types: [{WEIGHT_QUANTIZE_NEAREST_ROUNDING}, {WEIGHT_QUANTIZE_STOCHASTIC_ROUNDING}]\"\n        if WEIGHT_QUANTIZE_FP16_MIXED_QUANTIZE in sub_param_dict.keys():\n            output[WEIGHT_QUANTIZE_FP16_MIXED_QUANTIZE] = get_scalar_param(\n                sub_param_dict[WEIGHT_QUANTIZE_FP16_MIXED_QUANTIZE], WEIGHT_QUANTIZE_FP16_MIXED_QUANTIZE_ENABLED,\n                WEIGHT_QUANTIZE_FP16_MIXED_QUANTIZE_ENABLED_DEFAULT)\n            output[WEIGHT_QUANTIZE_CHANGE_RATIO] = get_scalar_param(\n                sub_param_dict[WEIGHT_QUANTIZE_FP16_MIXED_QUANTIZE], WEIGHT_QUANTIZE_CHANGE_RATIO,\n                WEIGHT_QUANTIZE_CHANGE_RATIO_DEFAULT)\n        else:\n            output[WEIGHT_QUANTIZE_FP16_MIXED_QUANTIZE] = WEIGHT_QUANTIZE_FP16_MIXED_QUANTIZE_ENABLED_DEFAULT\n            output[WEIGHT_QUANTIZE_CHANGE_RATIO] = WEIGHT_QUANTIZE_CHANGE_RATIO_DEFAULT\n    else:\n        output[WEIGHT_QUANTIZE_ENABLED] = WEIGHT_QUANTIZE_ENABLED_DEFAULT\n        output[WEIGHT_QUANTIZE_KERNEL] = WEIGHT_QUANTIZE_KERNEL_DEFAULT\n        output[WEIGHT_QUANTIZE_SCHEDULE_OFFSET] = WEIGHT_QUANTIZE_SCHEDULE_OFFSET_DEFAULT\n        output[WEIGHT_QUANTIZE_GROUPS] = WEIGHT_QUANTIZE_GROUPS_DEFAULT\n        output[WEIGHT_QUANTIZE_VERBOSE] = WEIGHT_QUANTIZE_VERBOSE_DEFAULT\n        output[WEIGHT_QUANTIZE_TYPE] = WEIGHT_QUANTIZE_TYPE_DEFAULT\n        output[WEIGHT_QUANTIZE_ROUNDING] = WEIGHT_QUANTIZE_ROUNDING_DEFAULT\n        output[WEIGHT_QUANTIZE_FP16_MIXED_QUANTIZE] = WEIGHT_QUANTIZE_FP16_MIXED_QUANTIZE_ENABLED_DEFAULT\n        output[WEIGHT_QUANTIZE_CHANGE_RATIO] = WEIGHT_QUANTIZE_CHANGE_RATIO_DEFAULT\n    return output\n\n\ndef get_weight_quantization_different_groups(param_dict):\n    output = {}\n    sub_param_dict = param_dict[DIFFERENT_GROUPS]\n\n    def get_params(name, group_dict):\n        assert WEIGHT_QUANTIZE_START_BITS in group_dict.keys(\n        ), f\"{WEIGHT_QUANTIZE_START_BITS} must be specified for weight quantization group {name}\"\n        assert WEIGHT_QUANTIZE_TARGET_BITS in group_dict.keys(\n        ), f\"{WEIGHT_QUANTIZE_TARGET_BITS} must be specified for weight quantization group {name}\"\n        group_dict[WEIGHT_QUANTIZATION_PERIOD] = get_scalar_param(group_dict, WEIGHT_QUANTIZATION_PERIOD,\n                                                                  WEIGHT_QUANTIZATION_PERIOD_DEFAULT)\n        return group_dict\n\n    for k, v in sub_param_dict.items():\n        output[k] = {}\n        output[k][DIFFERENT_GROUPS_PARAMETERS] = get_params(k, sub_param_dict[k][DIFFERENT_GROUPS_PARAMETERS])\n        output[k][DIFFERENT_GROUPS_MODULE_SCOPE] = get_scalar_param(sub_param_dict[k], DIFFERENT_GROUPS_MODULE_SCOPE,\n                                                                    DIFFERENT_GROUPS_MODULE_SCOPE_DEFAULT)\n        output[k][DIFFERENT_GROUPS_RELATED_MODULE_SCOPE] = get_scalar_param(\n            sub_param_dict[k], DIFFERENT_GROUPS_RELATED_MODULE_SCOPE, DIFFERENT_GROUPS_RELATED_MODULE_SCOPE_DEFAULT)\n\n    return output\n\n\ndef get_activation_quantization(param_dict):\n    output = {}\n    if ACTIVATION_QUANTIZATION not in param_dict.keys():\n        param_dict[ACTIVATION_QUANTIZATION] = {SHARED_PARAMETERS: {}, DIFFERENT_GROUPS: {}}\n    sub_param_dict = param_dict[ACTIVATION_QUANTIZATION]\n    # shared parameters\n    output[SHARED_PARAMETERS] = get_activation_quantization_shared_parameters(sub_param_dict)\n    # each sub-groups\n    if output[SHARED_PARAMETERS][ACTIVATION_QUANTIZATION_ENABLED]:\n        assert DIFFERENT_GROUPS in sub_param_dict.keys(\n        ), f\"Activation Quantization is enabled, {DIFFERENT_GROUPS} must be specified\"\n    output[DIFFERENT_GROUPS] = get_activation_quantization_different_groups(sub_param_dict)\n    return output\n\n\ndef get_activation_quantization_shared_parameters(param_dict):\n    output = {}\n    if SHARED_PARAMETERS in param_dict.keys():\n        sub_param_dict = param_dict[SHARED_PARAMETERS]\n        output[ACTIVATION_QUANTIZATION_ENABLED] = get_scalar_param(sub_param_dict, ACTIVATION_QUANTIZATION_ENABLED,\n                                                                   ACTIVATION_QUANTIZATION_ENABLED_DEFAULT)\n        output[ACTIVATION_QUANTIZE_TYPE] = get_scalar_param(sub_param_dict, ACTIVATION_QUANTIZE_TYPE,\n                                                            ACTIVATION_QUANTIZE_TYPE_DEFAULT)\n        assert output[ACTIVATION_QUANTIZE_TYPE] in [\n            ACTIVATION_QUANTIZE_SYMMETRIC, ACTIVATION_QUANTIZE_ASYMMETRIC\n        ], f\"Invalid activation quantize type. Supported types: [{ACTIVATION_QUANTIZE_SYMMETRIC}, {ACTIVATION_QUANTIZE_ASYMMETRIC}]\"\n        output[ACTIVATION_QUANTIZE_RANGE] = get_scalar_param(sub_param_dict, ACTIVATION_QUANTIZE_RANGE,\n                                                             ACTIVATION_QUANTIZE_RANGE_DEFAULT)\n        assert output[ACTIVATION_QUANTIZE_RANGE] in [\n            ACTIVATION_QUANTIZE_RANGE_DYNAMIC, ACTIVATION_QUANTIZE_RANGE_STATIC\n        ], f\"Invalid activation quantize range calibration. Supported types: [{ACTIVATION_QUANTIZE_RANGE_DYNAMIC}, {ACTIVATION_QUANTIZE_RANGE_STATIC}]\"\n        output[ACTIVATION_QUANTIZE_SCHEDULE_OFFSET] = get_scalar_param(sub_param_dict,\n                                                                       ACTIVATION_QUANTIZE_SCHEDULE_OFFSET,\n                                                                       ACTIVATION_QUANTIZE_SCHEDULE_OFFSET_DEFAULT)\n    else:\n        output[ACTIVATION_QUANTIZATION_ENABLED] = ACTIVATION_QUANTIZATION_ENABLED_DEFAULT\n        output[ACTIVATION_QUANTIZE_TYPE] = ACTIVATION_QUANTIZE_TYPE_DEFAULT\n        output[ACTIVATION_QUANTIZE_RANGE] = ACTIVATION_QUANTIZE_RANGE_DEFAULT\n        output[ACTIVATION_QUANTIZE_SCHEDULE_OFFSET] = ACTIVATION_QUANTIZE_SCHEDULE_OFFSET_DEFAULT\n    return output\n\n\ndef get_activation_quantization_different_groups(param_dict):\n    output = {}\n    sub_param_dict = param_dict[DIFFERENT_GROUPS]\n\n    def get_params(name, group_dict):\n        assert ACTIVATION_QUANTIZE_BITS in group_dict.keys(\n        ), f\"{ACTIVATION_QUANTIZE_BITS} must be specified for activation quantization group {name}\"\n        return group_dict\n\n    for k, v in sub_param_dict.items():\n        output[k] = {}\n        output[k][DIFFERENT_GROUPS_PARAMETERS] = get_params(k, sub_param_dict[k][DIFFERENT_GROUPS_PARAMETERS])\n        output[k][DIFFERENT_GROUPS_MODULE_SCOPE] = get_scalar_param(sub_param_dict[k], DIFFERENT_GROUPS_MODULE_SCOPE,\n                                                                    DIFFERENT_GROUPS_MODULE_SCOPE_DEFAULT)\n        output[k][DIFFERENT_GROUPS_RELATED_MODULE_SCOPE] = get_scalar_param(\n            sub_param_dict[k], DIFFERENT_GROUPS_RELATED_MODULE_SCOPE, DIFFERENT_GROUPS_RELATED_MODULE_SCOPE_DEFAULT)\n\n    return output\n\n\ndef get_sparse_pruning(param_dict):\n    output = {}\n    if SPARSE_PRUNING not in param_dict.keys():\n        param_dict[SPARSE_PRUNING] = {SHARED_PARAMETERS: {}, DIFFERENT_GROUPS: {}}\n    sub_param_dict = param_dict[SPARSE_PRUNING]\n    # shared parameters\n    output[SHARED_PARAMETERS] = get_sparse_pruning_shared_parameters(sub_param_dict)\n    # each sub-groups\n    if output[SHARED_PARAMETERS][SPARSE_PRUNING_ENABLED] and output[SHARED_PARAMETERS][\n            SPARSE_PRUNING_METHOD] != SPARSE_PRUNING_METHOD_SNIP_MOMENTUM:\n        assert DIFFERENT_GROUPS in sub_param_dict.keys(\n        ), f\"Sparse Pruning is enabled and not snip_momentum method, {DIFFERENT_GROUPS} must be specified\"\n    output[DIFFERENT_GROUPS] = get_sparse_pruning_different_groups(sub_param_dict)\n    return output\n\n\ndef get_sparse_pruning_shared_parameters(param_dict):\n    output = {}\n\n    if SHARED_PARAMETERS in param_dict.keys():\n        sub_param_dict = param_dict[SHARED_PARAMETERS]\n        output[SPARSE_PRUNING_ENABLED] = get_scalar_param(sub_param_dict, SPARSE_PRUNING_ENABLED,\n                                                          SPARSE_PRUNING_ENABLED_DEFAULT)\n        output[SPARSE_PRUNING_METHOD] = get_scalar_param(sub_param_dict, SPARSE_PRUNING_METHOD,\n                                                         SPARSE_PRUNING_METHOD_DEFAULT)\n        assert output[SPARSE_PRUNING_METHOD] in [\n            SPARSE_PRUNING_METHOD_L1, SPARSE_PRUNING_METHOD_TOPK, SPARSE_PRUNING_METHOD_SNIP_MOMENTUM\n        ], f\"Invalid sparse pruning method. Supported types: [{SPARSE_PRUNING_METHOD_L1}, {SPARSE_PRUNING_METHOD_TOPK}, {SPARSE_PRUNING_METHOD_SNIP_MOMENTUM}]\"\n        output[SPARSE_PRUNING_SCHEDULE_OFFSET] = get_scalar_param(sub_param_dict, SPARSE_PRUNING_SCHEDULE_OFFSET,\n                                                                  SPARSE_PRUNING_SCHEDULE_OFFSET_DEFAULT)\n        if output[SPARSE_PRUNING_METHOD] == SPARSE_PRUNING_METHOD_SNIP_MOMENTUM:\n            output[SPARSE_PRUNING_BLOCK_PATTERN] = get_scalar_param(sub_param_dict, SPARSE_PRUNING_BLOCK_PATTERN,\n                                                                    SPARSE_PRUNING_BLOCK_PATTERN_DEFAULT)\n            output[SPARSE_PRUNING_DENSE_RATIO] = get_scalar_param(sub_param_dict, SPARSE_PRUNING_DENSE_RATIO,\n                                                                  SPARSE_PRUNING_DENSE_RATIO_DEFAULT)\n            assert output[SPARSE_PRUNING_DENSE_RATIO] > 0 and output[\n                SPARSE_PRUNING_DENSE_RATIO] < 1, f\"Invalid dense_ratio value. Must be less than 1\"\n            output[SPARSE_PRUNING_SCHEDULE_OFFSET_STRIDE] = get_scalar_param(\n                sub_param_dict, SPARSE_PRUNING_SCHEDULE_OFFSET_STRIDE, SPARSE_PRUNING_SCHEDULE_OFFSET_STRIDE_DEFAULT)\n            output[SPARSE_PRUNING_EXCLUDED_MODULES] = get_list_param(sub_param_dict, SPARSE_PRUNING_EXCLUDED_MODULES,\n                                                                     SPARSE_PRUNING_EXCLUDED_MODULES_DEFAULT)\n            output[SPARSE_PRUNING_SCHEDULE_OFFSET_END] = get_scalar_param(sub_param_dict,\n                                                                          SPARSE_PRUNING_SCHEDULE_OFFSET_END,\n                                                                          output[SPARSE_PRUNING_SCHEDULE_OFFSET])\n            assert output[SPARSE_PRUNING_SCHEDULE_OFFSET] <= output[\n                SPARSE_PRUNING_SCHEDULE_OFFSET_END], f\"Invalid schedule_offset and schedule_offset_end values\"\n    else:\n        output[SPARSE_PRUNING_ENABLED] = SPARSE_PRUNING_ENABLED_DEFAULT\n        output[SPARSE_PRUNING_METHOD] = SPARSE_PRUNING_METHOD_DEFAULT\n        output[SPARSE_PRUNING_SCHEDULE_OFFSET] = SPARSE_PRUNING_SCHEDULE_OFFSET_DEFAULT\n    return output\n\n\ndef get_sparse_pruning_different_groups(param_dict):\n    output = {}\n    sub_param_dict = param_dict[DIFFERENT_GROUPS]\n\n    def get_params(name, group_dict):\n        assert SPARSE_PRUNING_DENSE_RATIO in group_dict.keys(\n        ), f\"{SPARSE_PRUNING_DENSE_RATIO} must be specified for sparse pruning group {name}\"\n        return group_dict\n\n    for k, v in sub_param_dict.items():\n        output[k] = {}\n        output[k][DIFFERENT_GROUPS_PARAMETERS] = get_params(k, sub_param_dict[k][DIFFERENT_GROUPS_PARAMETERS])\n        output[k][DIFFERENT_GROUPS_MODULE_SCOPE] = get_scalar_param(sub_param_dict[k], DIFFERENT_GROUPS_MODULE_SCOPE,\n                                                                    DIFFERENT_GROUPS_MODULE_SCOPE_DEFAULT)\n        output[k][DIFFERENT_GROUPS_RELATED_MODULE_SCOPE] = get_scalar_param(\n            sub_param_dict[k], DIFFERENT_GROUPS_RELATED_MODULE_SCOPE, DIFFERENT_GROUPS_RELATED_MODULE_SCOPE_DEFAULT)\n\n    return output\n\n\ndef get_row_pruning(param_dict):\n    output = {}\n    if ROW_PRUNING not in param_dict.keys():\n        param_dict[ROW_PRUNING] = {SHARED_PARAMETERS: {}, DIFFERENT_GROUPS: {}}\n    sub_param_dict = param_dict[ROW_PRUNING]\n    # shared parameters\n    output[SHARED_PARAMETERS] = get_row_pruning_shared_parameters(sub_param_dict)\n    # each sub-groups\n    if output[SHARED_PARAMETERS][ROW_PRUNING_ENABLED]:\n        assert DIFFERENT_GROUPS in sub_param_dict.keys(\n        ), f\"Row Pruning is enabled, {DIFFERENT_GROUPS} must be specified\"\n    output[DIFFERENT_GROUPS] = get_row_pruning_different_groups(sub_param_dict)\n    return output\n\n\ndef get_row_pruning_shared_parameters(param_dict):\n    output = {}\n    if SHARED_PARAMETERS in param_dict.keys():\n        sub_param_dict = param_dict[SHARED_PARAMETERS]\n        output[ROW_PRUNING_ENABLED] = get_scalar_param(sub_param_dict, ROW_PRUNING_ENABLED,\n                                                       ROW_PRUNING_ENABLED_DEFAULT)\n        output[ROW_PRUNING_METHOD] = get_scalar_param(sub_param_dict, ROW_PRUNING_METHOD, ROW_PRUNING_METHOD_DEFAULT)\n        assert output[ROW_PRUNING_METHOD] in [\n            ROW_PRUNING_METHOD_L1, ROW_PRUNING_METHOD_TOPK\n        ], f\"Invalid row pruning method. Supported types: [{ROW_PRUNING_METHOD_L1}, {ROW_PRUNING_METHOD_TOPK}]\"\n        output[ROW_PRUNING_SCHEDULE_OFFSET] = get_scalar_param(sub_param_dict, ROW_PRUNING_SCHEDULE_OFFSET,\n                                                               ROW_PRUNING_SCHEDULE_OFFSET_DEFAULT)\n    else:\n        output[ROW_PRUNING_ENABLED] = ROW_PRUNING_ENABLED_DEFAULT\n        output[ROW_PRUNING_METHOD] = ROW_PRUNING_METHOD_DEFAULT\n        output[ROW_PRUNING_SCHEDULE_OFFSET] = ROW_PRUNING_SCHEDULE_OFFSET_DEFAULT\n    return output\n\n\ndef get_row_pruning_different_groups(param_dict):\n    output = {}\n    sub_param_dict = param_dict[DIFFERENT_GROUPS]\n\n    def get_params(name, group_dict):\n        assert ROW_PRUNING_DENSE_RATIO in group_dict.keys(\n        ), f\"{ROW_PRUNING_DENSE_RATIO} must be specified for row pruning group {name}\"\n        return group_dict\n\n    for k, v in sub_param_dict.items():\n        output[k] = {}\n        output[k][DIFFERENT_GROUPS_PARAMETERS] = get_params(k, sub_param_dict[k][DIFFERENT_GROUPS_PARAMETERS])\n        output[k][DIFFERENT_GROUPS_MODULE_SCOPE] = get_scalar_param(sub_param_dict[k], DIFFERENT_GROUPS_MODULE_SCOPE,\n                                                                    DIFFERENT_GROUPS_MODULE_SCOPE_DEFAULT)\n        output[k][DIFFERENT_GROUPS_RELATED_MODULE_SCOPE] = get_scalar_param(\n            sub_param_dict[k], DIFFERENT_GROUPS_RELATED_MODULE_SCOPE, DIFFERENT_GROUPS_RELATED_MODULE_SCOPE_DEFAULT)\n    return output\n\n\ndef get_head_pruning(param_dict):\n    output = {}\n    if HEAD_PRUNING not in param_dict.keys():\n        param_dict[HEAD_PRUNING] = {SHARED_PARAMETERS: {}, DIFFERENT_GROUPS: {}}\n    sub_param_dict = param_dict[HEAD_PRUNING]\n    # shared parameters\n    output[SHARED_PARAMETERS] = get_head_pruning_shared_parameters(sub_param_dict)\n    # each sub-groups\n    if output[SHARED_PARAMETERS][HEAD_PRUNING_ENABLED]:\n        assert DIFFERENT_GROUPS in sub_param_dict.keys(\n        ), f\"Head Pruning is enabled, {DIFFERENT_GROUPS} must be specified\"\n    output[DIFFERENT_GROUPS] = get_head_pruning_different_groups(sub_param_dict)\n    return output\n\n\ndef get_head_pruning_shared_parameters(param_dict):\n    output = {}\n    if SHARED_PARAMETERS in param_dict.keys():\n        sub_param_dict = param_dict[SHARED_PARAMETERS]\n        output[HEAD_PRUNING_ENABLED] = get_scalar_param(sub_param_dict, HEAD_PRUNING_ENABLED,\n                                                        HEAD_PRUNING_ENABLED_DEFAULT)\n        output[HEAD_PRUNING_METHOD] = get_scalar_param(sub_param_dict, HEAD_PRUNING_METHOD,\n                                                       HEAD_PRUNING_METHOD_DEFAULT)\n        assert output[HEAD_PRUNING_METHOD] in [\n            HEAD_PRUNING_METHOD_L1, HEAD_PRUNING_METHOD_TOPK\n        ], f\"Invalid head pruning method. Supported types: [{HEAD_PRUNING_METHOD_L1}, {HEAD_PRUNING_METHOD_TOPK}]\"\n        output[HEAD_PRUNING_SCHEDULE_OFFSET] = get_scalar_param(sub_param_dict, HEAD_PRUNING_SCHEDULE_OFFSET,\n                                                                HEAD_PRUNING_SCHEDULE_OFFSET_DEFAULT)\n        if output[HEAD_PRUNING_ENABLED]:\n            assert HEAD_PRUNING_NUM_HEADS in sub_param_dict.keys(\n            ), f\"{HEAD_PRUNING_NUM_HEADS} must be specified for head pruning\"\n            output[HEAD_PRUNING_NUM_HEADS] = sub_param_dict[HEAD_PRUNING_NUM_HEADS]\n    else:\n        output[HEAD_PRUNING_ENABLED] = HEAD_PRUNING_ENABLED_DEFAULT\n        output[HEAD_PRUNING_METHOD] = HEAD_PRUNING_METHOD_DEFAULT\n        output[HEAD_PRUNING_SCHEDULE_OFFSET] = HEAD_PRUNING_SCHEDULE_OFFSET_DEFAULT\n    return output\n\n\ndef get_head_pruning_different_groups(param_dict):\n    output = {}\n    sub_param_dict = param_dict[DIFFERENT_GROUPS]\n\n    def get_params(name, group_dict):\n        assert HEAD_PRUNING_DENSE_RATIO in group_dict.keys(\n        ), f\"dense_ratio must be specified for head pruning group {name}\"\n        return group_dict\n\n    for k, v in sub_param_dict.items():\n        output[k] = {}\n        output[k][DIFFERENT_GROUPS_PARAMETERS] = get_params(k, sub_param_dict[k][DIFFERENT_GROUPS_PARAMETERS])\n        output[k][DIFFERENT_GROUPS_MODULE_SCOPE] = get_scalar_param(sub_param_dict[k], DIFFERENT_GROUPS_MODULE_SCOPE,\n                                                                    DIFFERENT_GROUPS_MODULE_SCOPE_DEFAULT)\n        output[k][DIFFERENT_GROUPS_RELATED_MODULE_SCOPE] = get_scalar_param(\n            sub_param_dict[k], DIFFERENT_GROUPS_RELATED_MODULE_SCOPE, DIFFERENT_GROUPS_RELATED_MODULE_SCOPE_DEFAULT)\n    return output\n\n\ndef get_channel_pruning(param_dict):\n    output = {}\n    if CHANNEL_PRUNING not in param_dict.keys():\n        param_dict[CHANNEL_PRUNING] = {SHARED_PARAMETERS: {}, DIFFERENT_GROUPS: {}}\n    sub_param_dict = param_dict[CHANNEL_PRUNING]\n    # shared parameters\n    output[SHARED_PARAMETERS] = get_channel_pruning_shared_parameters(sub_param_dict)\n    # each sub-groups\n    if output[SHARED_PARAMETERS][CHANNEL_PRUNING_ENABLED]:\n        assert DIFFERENT_GROUPS in sub_param_dict.keys(\n        ), f\"Sparse Pruning is enabled, {DIFFERENT_GROUPS} must be specified\"\n    output[DIFFERENT_GROUPS] = get_channel_pruning_different_groups(sub_param_dict)\n    return output\n\n\ndef get_channel_pruning_shared_parameters(param_dict):\n    output = {}\n    if SHARED_PARAMETERS in param_dict.keys():\n        sub_param_dict = param_dict[SHARED_PARAMETERS]\n        output[CHANNEL_PRUNING_ENABLED] = get_scalar_param(sub_param_dict, CHANNEL_PRUNING_ENABLED,\n                                                           CHANNEL_PRUNING_ENABLED_DEFAULT)\n        output[CHANNEL_PRUNING_METHOD] = get_scalar_param(sub_param_dict, CHANNEL_PRUNING_METHOD,\n                                                          CHANNEL_PRUNING_METHOD_DEFAULT)\n        assert output[CHANNEL_PRUNING_METHOD] in [\n            CHANNEL_PRUNING_METHOD_L1, CHANNEL_PRUNING_METHOD_TOPK\n        ], f\"Invalid channel pruning method. Supported types: [{CHANNEL_PRUNING_METHOD_L1}, {CHANNEL_PRUNING_METHOD_TOPK}]\"\n        output[CHANNEL_PRUNING_SCHEDULE_OFFSET] = get_scalar_param(sub_param_dict, CHANNEL_PRUNING_SCHEDULE_OFFSET,\n                                                                   CHANNEL_PRUNING_SCHEDULE_OFFSET_DEFAULT)\n    else:\n        output[CHANNEL_PRUNING_ENABLED] = CHANNEL_PRUNING_ENABLED_DEFAULT\n        output[CHANNEL_PRUNING_METHOD] = CHANNEL_PRUNING_METHOD_DEFAULT\n        output[CHANNEL_PRUNING_SCHEDULE_OFFSET] = CHANNEL_PRUNING_SCHEDULE_OFFSET_DEFAULT\n    return output\n\n\ndef get_channel_pruning_different_groups(param_dict):\n    output = {}\n    sub_param_dict = param_dict[DIFFERENT_GROUPS]\n\n    def get_params(name, group_dict):\n        assert CHANNEL_PRUNING_DENSE_RATIO in group_dict.keys(\n        ), f\"{CHANNEL_PRUNING_DENSE_RATIO} must be specified for channel pruning group {name}\"\n        return group_dict\n\n    for k, v in sub_param_dict.items():\n        output[k] = {}\n        output[k][DIFFERENT_GROUPS_PARAMETERS] = get_params(k, sub_param_dict[k][DIFFERENT_GROUPS_PARAMETERS])\n        output[k][DIFFERENT_GROUPS_MODULE_SCOPE] = get_scalar_param(sub_param_dict[k], DIFFERENT_GROUPS_MODULE_SCOPE,\n                                                                    DIFFERENT_GROUPS_MODULE_SCOPE_DEFAULT)\n        output[k][DIFFERENT_GROUPS_RELATED_MODULE_SCOPE] = get_scalar_param(\n            sub_param_dict[k], DIFFERENT_GROUPS_RELATED_MODULE_SCOPE, DIFFERENT_GROUPS_RELATED_MODULE_SCOPE_DEFAULT)\n\n    return output\n", "deepspeed/compression/compress.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport re\nfrom .helper import compression_preparation, fix_compression, recursive_getattr, is_module_compressible\nfrom .config import get_compression_config\nfrom ..runtime.config_utils import dict_raise_error_on_duplicate_keys\nfrom .constants import *\nimport os\nimport json\n\ntry:\n    import neural_compressor as nc\nexcept ImportError as e:\n    nc = None\n\n\ndef check_deepspeed_config(config):\n    if isinstance(config, dict):\n        return config\n    elif os.path.exists(config):\n        return json.load(open(config, \"r\"), object_pairs_hook=dict_raise_error_on_duplicate_keys)\n    else:\n        raise ValueError(\n            f\"Expected a string path to an existing deepspeed config, or a dictionary. Received: {config}\")\n\n\ndef get_module_name(group_name, model, key_word, exist_module_name, mpu=None, verbose=True):\n    '''\n    get the associated module name from the model based on the key_word provided by users\n    '''\n    return_module_name = []\n    for name, module in model.named_modules():\n\n        module_check = is_module_compressible(module, mpu)\n\n        if re.search(key_word, name) is not None and module_check:\n            if name in exist_module_name and verbose:\n                # logger.warning\n                raise ValueError(\n                    f\"{name} is already added to compression, please check your config file for {group_name}.\")\n            if name not in exist_module_name:\n                exist_module_name.add(name)\n                return_module_name.append(name)\n    return return_module_name, exist_module_name\n\n\ndef get_compress_methods(model, compress_methods, mpu=None):\n    # extract the compression module for each method in compress_methods\n    layer_added_compress_methods = []\n    for method, method_content in compress_methods.items():\n        if LAYER_REDUCTION in method:\n            continue\n        # for loop different methods, i.e., weight quantization, activation quantization etc\n        exist_module_name = set()\n        shared_parameters = method_content[SHARED_PARAMETERS]  # get all the shared parameters\n        for group_name, method_parameters in method_content[DIFFERENT_GROUPS].items():\n            # for loop different groups, i.e., weight quantization group 1, weight quantization group 2 etc\n            module_name_list = []\n            related_module_name_list = []\n            if method_parameters[DIFFERENT_GROUPS_RELATED_MODULE_SCOPE]:\n                # this is used for head/row/channel pruning, if users provide the related module scope, we can shrink the layer dim for them\n                # otherwise we just mask those as zeros\n                for key_word, related_key_words in zip(method_parameters[DIFFERENT_GROUPS_MODULE_SCOPE],\n                                                       method_parameters[DIFFERENT_GROUPS_RELATED_MODULE_SCOPE]):\n                    module_name, exist_module_name = get_module_name(group_name,\n                                                                     model,\n                                                                     key_word,\n                                                                     exist_module_name,\n                                                                     mpu=mpu)\n                    module_name_list.append(module_name)\n                    tmp_related_module_name_list = []\n                    for rkw in related_key_words:\n                        # related key word can be a list, for instance the QKV for O matrix in Attention\n                        module_name, _ = get_module_name(group_name, model, rkw, set(), mpu=mpu)\n                        tmp_related_module_name_list.append(module_name)\n                    related_module_name_list.append(tmp_related_module_name_list)\n            else:\n                for key_word in method_parameters[DIFFERENT_GROUPS_MODULE_SCOPE]:\n                    module_name, exist_module_name = get_module_name(group_name,\n                                                                     model,\n                                                                     key_word,\n                                                                     exist_module_name,\n                                                                     mpu=mpu)\n                    module_name_list.append(module_name)\n\n            if module_name_list:\n                # combine shared parameters with each group\n                combined_method_parameters = {\n                    **(method_parameters.copy().pop(DIFFERENT_GROUPS_PARAMETERS)),\n                    **shared_parameters\n                }\n                compression_item = [module_name_list, related_module_name_list, {method: combined_method_parameters}]\n                layer_added_compress_methods.append(compression_item)\n    return layer_added_compress_methods\n\n\ndef init_compression(model, deepspeed_config, teacher_model=None, mpu=None):\n    \"\"\"\n    Compress a model: replace linear/conv2d layer with deepspeed compression-aware modules\n    Args:\n        model (`torch.nn.Module`)\n            The model to compress.\n        deepspeed_config (`DeepSpeedConfig`)\n            The path of ds_config\n        mpu\n            The mpu module for Row/Column parallelism\n    \"\"\"\n    compress_methods = get_compression_config(check_deepspeed_config(deepspeed_config))\n    if hasattr(model, 'module'):\n        c_model = model.module\n    else:\n        c_model = model\n\n    # For layer reduction\n    if compress_methods[LAYER_REDUCTION][LAYER_REDUCTION_ENABLED]:\n        assert teacher_model is not None, \"Teacher model is required for layer reduction\"\n        student_initialization(c_model, teacher_model, deepspeed_config)\n\n    layer_added_compress_methods = get_compress_methods(c_model, compress_methods, mpu=mpu)\n    compression_preparation(c_model, layer_added_compress_methods, mpu)\n\n    # For sparse pruning snip_momentum method\n    shared_parameters = compress_methods[SPARSE_PRUNING][SHARED_PARAMETERS]\n    if shared_parameters[SPARSE_PRUNING_ENABLED] and \\\n        shared_parameters[SPARSE_PRUNING_METHOD] == SPARSE_PRUNING_METHOD_SNIP_MOMENTUM:\n\n        assert nc is not None, \"please ensure the neural_compressor python package is installed by pip or conda if user wants to use snip_momentum sparse pruning\"\n\n        from .helper import generate_pruners, register_on_step_begin\n        from nc import WeightPruningConfig\n\n        config = WeightPruningConfig(target_sparsity=1 - shared_parameters[SPARSE_PRUNING_DENSE_RATIO],\n                                     pattern=shared_parameters[SPARSE_PRUNING_BLOCK_PATTERN],\n                                     pruning_frequency=shared_parameters[SPARSE_PRUNING_SCHEDULE_OFFSET_STRIDE],\n                                     start_step=shared_parameters[SPARSE_PRUNING_SCHEDULE_OFFSET],\n                                     end_step=shared_parameters[SPARSE_PRUNING_SCHEDULE_OFFSET_END],\n                                     excluded_op_names=shared_parameters[SPARSE_PRUNING_EXCLUDED_MODULES])\n        pruners = generate_pruners(config, c_model)\n        c_model.pruners = pruners\n        register_on_step_begin(c_model)\n\n    return model\n\n\ndef redundancy_clean(model, deepspeed_config, mpu=None):\n    \"\"\"\n    Remove the redundancy of a model\n    Args:\n        model (`torch.nn.Module`)\n            The model to compress.\n        deepspeed_config (`DeepSpeedConfig`)\n            The path of ds_config\n        mpu\n            The mpu module for Row/Column parallelism\n    \"\"\"\n    compress_methods = get_compression_config(check_deepspeed_config(deepspeed_config))\n    if hasattr(model, 'module'):\n        c_model = model.module\n    else:\n        c_model = model\n\n    layer_added_compress_methods_tmp = get_compress_methods(c_model, compress_methods, mpu=mpu)\n    # sort methods\n    order_list = [\n        WEIGHT_QUANTIZATION, SPARSE_PRUNING, ROW_PRUNING, HEAD_PRUNING, CHANNEL_PRUNING, ACTIVATION_QUANTIZATION\n    ]\n    layer_added_compress_methods = sorted(layer_added_compress_methods_tmp,\n                                          key=lambda x: order_list.index(list(x[2].keys())[0]))\n\n    for module_name_lists, related_module_name_lists, compression_technique in layer_added_compress_methods:\n        stored_mask = []\n        need_mask = True if related_module_name_lists else False\n        for i, mnl in enumerate(module_name_lists):\n            for module_name in mnl:\n                mask = fix_compression(c_model, module_name, compression_technique, dim_reduction=need_mask)\n                if need_mask:\n                    stored_mask.append(mask)\n            if need_mask:\n                for rmnl in related_module_name_lists[i]:\n                    for j, module_name in enumerate(rmnl):\n                        mask = fix_compression(c_model,\n                                               module_name,\n                                               compression_technique,\n                                               mask=stored_mask[j],\n                                               dim_reduction=True)\n    return model\n\n\ndef student_initialization(student_model, teacher_model, deepspeed_config):\n    '''\n    Given a student model and a teacher model, select the\n    Args:\n        student_model (`torch.nn.Module`)\n            The model we will update weight\n        teacher_model (`torch.nn.Module`)\n            The model guide the student to learn\n        deepspeed_config (`DeepSpeedConfig`)\n            The path of ds_config\n    '''\n    config = get_compression_config(check_deepspeed_config(deepspeed_config))\n    compress_methods = config[LAYER_REDUCTION]\n\n    module_name_prefix = compress_methods[MODULE_NAME_PREFIX]\n    teacher_layer = compress_methods[TEACHER_LAYER]\n    student_layer = [i for i in range(len(teacher_layer))]\n    other_module_name = compress_methods[OTHER_MODULE_NAME]\n    '''\n        name_prefix (`str`)\n            The prefix name before the layer #.\n            Example 1: bert.encoder.layer, for BERT_base model's prefix name\n            Example 2: transformer.h, for GPT-2 hugging face prefix name\n        teacher_layer (`list of integers`)\n            The layer of teacher will be used for student's reinitialization\n            Example 1: [1,3,5,7,9], means we want to matches the 2nd/4th/6th/8th/10th layer of teacher to the first 5 layers of student\n        student_layer (`list` or None)\n            The layer of student need to be re-initialized\n            Example 1: None, means we want to reinitialize all the layers\n            Example 1: [0,1,2,3,4], means  we want to reinitialize the first 5 layers\n        other_module_name (`list of string`)\n            The modules will be used for student's reinitialization\n            Example 1: ['bert.pooler', 'bert.embeddings', 'classifier'], means we want to apply the weight in teacher's embedding/pooler/classier module to the student\n            Example 2: ['transformer.w', 'transformer.ln_f', 'lm_head'], means we want to apply the weight in teacher's embedding layers module to the student\n    Note that teacher_layer should matches student layer\n    '''\n    assert len(student_layer) == len(teacher_layer)\n    for s_name, t_name in zip(student_layer, teacher_layer):\n        s_module = recursive_getattr(student_model, module_name_prefix + '.' + str(s_name))\n        t_module = recursive_getattr(teacher_model, module_name_prefix + '.' + str(t_name))\n        for s_param, t_param in zip(s_module.parameters(), t_module.parameters()):\n            s_param.data.copy_(t_param.data)\n    for name in other_module_name:\n        s_module = recursive_getattr(student_model, name)\n        t_module = recursive_getattr(teacher_model, name)\n        print(name)\n        for s_param, t_param in zip(s_module.parameters(), t_module.parameters()):\n            s_param.data.copy_(t_param.data)\n", "deepspeed/compression/utils.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nfrom torch import autograd\nimport math\n\n\nclass TopKBinarizer(autograd.Function):\n    \"\"\"\n    Top-k Binarizer.\n    Computes a binary mask M from a real value matrix S such that `M_{i,j} = 1` if and only if `S_{i,j}`\n    is among the k% highest values of S.\n    Implementation is inspired from:\n        https://github.com/yaozhewei/MLPruning\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, inputs: torch.tensor, threshold: float, sigmoid: bool):\n        \"\"\"\n        Args:\n            inputs (`torch.FloatTensor`)\n                The input matrix from which the binarizer computes the binary mask.\n            threshold (`float`)\n                The percentage of weights to keep (the rest is pruned).\n                `threshold` is a float between 0 and 1.\n            sigmoid (`bool`)\n                Whether to apply a sigmoid on the threshold\n        Returns:\n            mask (`torch.FloatTensor`)\n                Binary matrix of the same size as `inputs` acting as a mask (1 - the associated weight is\n                retained, 0 - the associated weight is pruned).\n        \"\"\"\n        # Get the subnetwork by sorting the inputs and using the top threshold\n        if sigmoid:\n            threshold = torch.sigmoid(threshold).item()\n        ctx.sigmoid = sigmoid\n        mask = inputs.clone()\n\n        _, idx = inputs.flatten().sort(descending=True)\n        j = math.ceil(threshold * inputs.numel())\n\n        # flat_out and mask access the same memory.\n        flat_out = mask.flatten()\n        flat_out[idx[j:]] = 0.\n        flat_out[idx[:j]] = 1.\n        ctx.save_for_backward(mask)\n\n        return mask\n\n    @staticmethod\n    def backward(ctx, gradOutput):\n        mask, = ctx.saved_tensors\n        if ctx.sigmoid:\n            return gradOutput.clone(), ((gradOutput * mask).sum()).view(-1), None\n        else:\n            return gradOutput.clone(), None, None\n\n\nclass SymQuantizer(torch.autograd.Function):\n    \"\"\"\n    Symmetric quantization\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, input, num_bits, min_value=None, max_value=None, num_groups=1):\n        \"\"\"\n        Args:\n            inputs (`torch.FloatTensor`)\n                The input which needs to be quantized\n            num_bits (int, >=4)\n                Number of bits to use for quantization\n            min_value/max_value (torch.FloatTensor)\n                Used for static activation quantization\n            num_groups (int)\n                How many groups to partition the quantization into\n        Returns:\n            quantized_input (`torch.FloatTensor`)\n                Quantized input\n        \"\"\"\n        assert (min_value is None and max_value is None) or (min_value is not None and max_value is not None\n                                                             and num_groups == 1)\n        q_range = 2**num_bits\n        input_shape = input.shape\n        if min_value is None:\n            input = input.reshape(num_groups, -1)\n            max_input = torch.amax(torch.abs(input), dim=-1).view(num_groups, -1)\n        else:\n            max_input = torch.max(min_value.abs(), max_value).view(-1)\n\n        scale = 2 * max_input / q_range\n        output = (input / scale).round().clamp(-q_range // 2, q_range // 2 - 1) * scale\n        output = output.reshape(input_shape).contiguous()\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        grad_input = grad_output.clone()\n        return grad_input, None, None, None, None\n\n\nclass AsymQuantizer(torch.autograd.Function):\n    \"\"\"\n    Asymmetric quantization\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, input, num_bits, min_value=None, max_value=None, num_groups=1):\n        \"\"\"\n        Args:\n            inputs (`torch.FloatTensor`)\n                The input which needs to be quantized\n            num_bits (int, >=4)\n                Number of bits to use for quantization\n            min_value/max_value (torch.FloatTensor)\n                Used for static activation quantization\n            num_groups (int)\n                How many groups to partition the quantization into\n        Returns:\n            quantized_input (`torch.FloatTensor`)\n                Quantized input\n        \"\"\"\n\n        assert (min_value is None and max_value is None) or (min_value is not None and max_value is not None\n                                                             and num_groups == 1)\n        q_range = 2**num_bits\n        input_shape = input.shape\n        if min_value is None:\n            input = input.reshape(num_groups, -1)\n            min_value = input.amin(dim=-1, keepdim=True)\n            max_value = input.amax(dim=-1, keepdim=True)\n\n        scale = (max_value - min_value) / q_range\n        zero_point = (min_value / scale).round() * scale\n\n        output = ((input - zero_point) / scale).round().clamp(0, q_range - 1) * scale + zero_point\n        output = output.reshape(input_shape).contiguous()\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        grad_input = grad_output.clone()\n        return grad_input, None, None, None, None\n\n\nclass TernaryQuantizer(torch.autograd.Function):\n    \"\"\"\n    Ternary quantization\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, input, num_bits, min_value=None, max_value=None, num_groups=1):\n        \"\"\"\n        Args:\n            inputs (`torch.FloatTensor`)\n                The input which needs to be quantized\n            num_bits (int)\n                Dummy variable\n            min_value/max_value (torch.FloatTensor)\n                Used for static activation quantization; for now they are dummy variable\n            num_groups (int)\n                How many groups to partition the quantization into\n        Returns:\n            quantized_input (`torch.FloatTensor`)\n                Quantized input\n        \"\"\"\n\n        assert (min_value is None and max_value is None)\n        input_flat = input.reshape(num_groups, -1)\n        n = input_flat.shape[1]\n        m = input_flat.norm(p=1, dim=1).div(n)\n        thres = (0.7 * m).view(-1, 1)\n        pos = (input_flat > thres).type(input.type())\n        neg = (input_flat < -thres).type(input.type())\n        mask = (input_flat.abs() > thres).type(input.type())\n        alpha = ((mask * input_flat).abs().sum(dim=1) / mask.sum(dim=1)).view(-1, 1)\n        output = alpha * pos - alpha * neg\n        output = output.reshape(input.shape).contiguous()\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        grad_input = grad_output.clone()\n        return grad_input, None, None, None, None\n\n\nclass BinaryQuantizer(torch.autograd.Function):\n    \"\"\"\n    Binary quantization\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, input, num_bits, min_value=None, max_value=None, num_groups=1):\n        \"\"\"\n        Args:\n            inputs (`torch.FloatTensor`)\n                The input which needs to be quantized\n            num_bits (int)\n                Dummy variable\n            min_value/max_value (torch.FloatTensor)\n                Used for static activation quantization; for now they are dummy variable\n            num_groups (int)\n                How many groups to partition the quantization into\n        Returns:\n            quantized_input (`torch.FloatTensor`)\n                Quantized input\n        \"\"\"\n\n        assert (min_value is None and max_value is None)\n        input_flat = input.reshape(num_groups, -1)\n        n = input_flat.shape[1]\n        m = input_flat.norm(p=1, dim=1, keepdim=True).div(n)\n        output = input_flat.sign().mul(m)\n        output = output.reshape(input.shape).contiguous()\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        grad_input = grad_output.clone()\n        return grad_input, None, None, None, None\n", "deepspeed/compression/basic_layer.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nimport math\nfrom torch import nn\nfrom torch.nn import init\nimport deepspeed.comm as dist\nfrom .utils import TopKBinarizer, SymQuantizer, AsymQuantizer, TernaryQuantizer, BinaryQuantizer\nfrom deepspeed.utils import logger\n\ng_mpu = None\n\n\nclass QuantAct(nn.Module):\n    \"\"\"\n    Class to quantize given activations. Note that when using this function, the input activation quantization range will be fixed for all\n    tokens/images for inference. This generally will affect some accuracy but achieve better latency performance.\n    Parameters:\n    ----------\n    act_range_momentum : float, default 0.95\n        Momentum for updating the activation quantization range.\n    quant_mode : str, default 'symmetric'\n    \"\"\"\n\n    def __init__(self, act_range_momentum=0.95, quant_mode='symmetric'):\n        super(QuantAct, self).__init__()\n\n        self.act_range_momentum = act_range_momentum\n        self.quant_mode = quant_mode\n        if quant_mode == 'symmetric':\n            self.act_function = SymQuantizer.apply\n        else:\n            self.act_function = AsymQuantizer.apply\n\n        self.register_buffer('x_min_max', torch.zeros(2))\n\n    def forward(self, x, num_bits, *args):\n        \"\"\"\n        x: the activation that we need to quantize\n        num_bits: the number of bits we need to quantize the activation to\n        *args: some extra arguments that are useless but needed for align with the interface of other quantization functions\n        \"\"\"\n\n        if self.training:\n            x_min = x.data.min()\n            x_max = x.data.max()\n\n            # Initialization\n            if self.x_min_max[0] == self.x_min_max[1]:\n                self.x_min_max[0] = x_min\n                self.x_min_max[1] = x_max\n\n            # if do not need momentum, please set self.act_range_momentum = 0\n            self.x_min_max[0] = self.x_min_max[0] * self.act_range_momentum + x_min * (1 - self.act_range_momentum)\n            self.x_min_max[1] = self.x_min_max[1] * self.act_range_momentum + x_max * (1 - self.act_range_momentum)\n\n        x_q = self.act_function(x, num_bits, self.x_min_max[0], self.x_min_max[1])\n\n        return x_q\n\n\nclass Embedding_Compress(nn.Embedding):\n\n    def __init__(self, *kargs):\n        super(Embedding_Compress, self).__init__(*kargs)\n        self.weight.start_bits = None\n        self.weight.target_bits = None\n        self.weight.q_period = None\n        self.weight_quantization_enabled_in_forward = False\n        self.weight_quantization_enabled = False\n\n    def extra_repr(self):\n        return 'num_embeddings={}, embedding_dim={}, weight_quantization={}'.format(\n            self.num_embeddings, self.embedding_dim, self.weight.target_bits)\n\n    def enable_weight_quantization(self, start_bits, target_bits, quantization_period,\n                                   weight_quantization_enabled_in_forward, quantization_type, num_groups):\n        self.weight.start_bits = start_bits\n        self.weight.target_bits = target_bits\n        self.weight.q_period = quantization_period\n        self.weight_quantization_enabled_in_forward = weight_quantization_enabled_in_forward\n        if self.weight_quantization_enabled_in_forward:\n            logger.warning(\n                \"************ A lot of MoQ features are not supported in quantize_weight_in_forward mode, please consider to use DS-FP16 optimizer************\"\n            )\n            if self.weight.target_bits >= 3:\n                if quantization_type == 'symmetric':\n                    self.weight_quantizer = SymQuantizer.apply\n                else:\n                    self.weight_quantizer = AsymQuantizer.apply\n            elif self.weight.target_bits == 2:\n                assert quantization_type == 'symmetric', 'Only symmetric quantization is supported for ternary weight quantization'\n                self.weight_quantizer = TernaryQuantizer.apply\n            elif self.weight.target_bits == 1:\n                assert quantization_type == 'symmetric', 'Only symmetric quantization is supported for binary weight quantization'\n                self.weight_quantizer = BinaryQuantizer.apply\n            # for embedding, we always use token-wise quantization\n            self.weight_quantize_num_groups = self.weight.size(0)\n\n    def fix_weight_quantization(self):\n        self.weight.data = self.weight_quantizer(self.weight, self.weight.target_bits, None, None,\n                                                 self.weight_quantize_num_groups).data\n        self.weight_quantization_enabled_in_forward = False\n        return None\n\n    def forward(self, input):\n        if self.weight_quantization_enabled_in_forward and self.weight_quantization_enabled:\n            weight = self.weight_quantizer(self.weight, self.weight.target_bits, None, None,\n                                           self.weight_quantize_num_groups)\n        else:\n            weight = self.weight\n\n        out = nn.functional.embedding(input, weight, self.padding_idx, self.max_norm, self.norm_type,\n                                      self.scale_grad_by_freq, self.sparse)\n        return out\n\n\nclass LinearLayer_Compress(nn.Linear):\n    \"\"\"\n    Linear layer with compression.\n    \"\"\"\n\n    def __init__(self, *kargs, bias=True):\n        super(LinearLayer_Compress, self).__init__(*kargs, bias=bias)\n        self.sparse_pruning_method = None\n        self.row_pruning_method = None\n        self.head_pruning_method = None\n        self.activation_quantization_method = None\n        self.weight.start_bits = None\n        self.weight.target_bits = None\n        self.weight.q_period = None\n        self.weight_quantization_enabled_in_forward = False\n        self.weight_quantization_enabled = False\n        self.sparse_pruning_enabled = False\n        self.row_pruning_enabled = False\n        self.head_pruning_enabled = False\n        self.activation_quantization_enabled = False\n\n    def extra_repr(self):\n        return 'in_features={}, out_features={}, bias={}, sparse pruning={}, row pruning={}, head pruning={}, activation quantization={}, weight_quantization={}'.format(\n            self.in_features, self.out_features, self.bias is not None, self.sparse_pruning_method is not None, \\\n            self.row_pruning_method is not None, self.head_pruning_method is not None, self.activation_quantization_method is not None, self.weight.target_bits)\n\n    def enable_sparse_pruning(self, ratio, method):\n        # Here, we support two cases: L1 norm based pruning and topk based pruning\n        self.sparse_pruning_ratio = ratio\n        self.sparse_pruning_method = method\n        if method == 'l1':\n            weight_norm = torch.abs(self.weight.data)\n            mask = TopKBinarizer.apply(weight_norm, self.sparse_pruning_ratio, False)\n            mask = mask.view(self.weight.size())\n            mask = mask.to(self.weight.device)\n        elif method == 'topk':\n            self.sparse_mask_scores = nn.Parameter(torch.Tensor(self.weight.size()))\n            self.sparse_mask_scores.data = self.sparse_mask_scores.data.to(self.weight.device)\n            init.kaiming_uniform_(self.sparse_mask_scores, a=math.sqrt(5))\n            mask = None\n        else:\n            raise NotImplementedError\n\n        self.register_buffer('sparse_pruning_mask', mask)\n\n    def enable_row_pruning(self, ratio, method):\n        # Here, we support two cases: L1 norm based pruning and topk based pruning\n        self.row_pruning_ratio = ratio\n        self.row_pruning_method = method\n\n        if method == 'l1':\n            # compute the l1 norm of each column\n            weight_norm = torch.linalg.norm(self.weight.data, ord=1, dim=1)\n            mask = TopKBinarizer.apply(weight_norm, self.row_pruning_ratio, False)\n            mask = mask.view(-1, 1)\n            mask = mask.to(self.weight.device)\n        elif method == 'topk':\n            self.row_mask_scores = nn.Parameter(torch.Tensor(self.weight.size(0), 1))\n            self.row_mask_scores.data = self.row_mask_scores.data.to(self.weight.device)\n            init.kaiming_uniform_(self.row_mask_scores, a=math.sqrt(5))\n            mask = None\n        else:\n            raise NotImplementedError\n\n        self.register_buffer('row_pruning_mask', mask)\n\n    def enable_head_pruning(self, ratio, method, num_heads):\n        # Here, we support only topk based pruning\n        self.num_heads = num_heads\n        self.head_pruning_ratio = ratio\n        self.head_pruning_method = method\n\n        if method not in ['topk']:\n            raise NotImplementedError\n        else:\n            self.head_pruning_ratio = ratio\n            self.head_pruning_scores = nn.Parameter(torch.Tensor(1,\n                                                                 self.num_heads))  # we apply the pruning to O matrix\n            self.head_pruning_scores.data = self.head_pruning_scores.data.to(self.weight.device)\n            init.kaiming_uniform_(self.head_pruning_scores, a=math.sqrt(5))\n\n    def fix_sparse_pruning_helper(self):\n        mask = self.get_mask(pruning_type='sparse')\n        self.weight.data = self.weight.data * mask\n        del self.sparse_pruning_mask\n        if self.sparse_pruning_method == 'topk':\n            del self.sparse_mask_scores\n        self.sparse_pruning_method = None\n        self.sparse_pruning_enabled = False\n        return None\n\n    def fix_row_col_pruning_helper(self, mask=None, dim_reduction=False):\n        # This function is used for row/col pruning\n        # particularly, if we have two back-to-back layers, F1 and F2; when\n        # we remove rows from F1, we also need to remove columns from F2\n        # However, if we only have one layer, F1, then we only need to mask pruned\n        # rows as 0 in F1\n        if mask is None:\n            mask = self.get_mask(pruning_type='row').bool()\n            if dim_reduction:\n                start_bits = self.weight.start_bits\n                target_bits = self.weight.target_bits\n                q_period = self.weight.q_period\n                self.weight = nn.Parameter(self.weight.data[mask.view(-1), :])\n                self.weight.start_bits = start_bits\n                self.weight.target_bits = target_bits\n                self.weight.q_period = q_period\n                if self.bias is not None:\n                    self.bias = nn.Parameter(self.bias.data[mask.view(-1)])\n                self.out_features = self.weight.size(0)\n            else:\n                self.weight.data = self.weight.data * mask.view(-1, 1)\n                if self.bias is not None:\n                    self.bias.data = self.bias.data * mask.view(-1)\n\n            del self.row_pruning_mask\n            if self.row_pruning_method == 'topk':\n                del self.row_mask_scores\n            self.row_pruning_method = None\n        else:\n            # this is generally for column pruning\n            start_bits = self.weight.start_bits\n            target_bits = self.weight.target_bits\n            q_period = self.weight.q_period\n            self.weight = nn.Parameter(self.weight.data[:, mask.view(-1)])\n            self.weight.start_bits = start_bits\n            self.weight.target_bits = target_bits\n            self.weight.q_period = q_period\n            self.in_features = self.weight.size(1)\n            mask = None\n        self.row_pruning_enabled = False\n        return mask\n\n    def fix_head_pruning_helper(self, mask=None, num_heads=None, dim_reduction=False):\n        # similar as row/col pruning, head pruning also needs to prune QKV which is associated with O matrix\n        num_heads = num_heads if num_heads else self.num_heads\n        if mask is None:\n            if self.head_pruning_method == 'topk':\n                mask = self.get_mask(pruning_type='head').bool()\n                if dim_reduction:\n                    shape = self.weight.size(0)\n                    start_bits = self.weight.start_bits\n                    target_bits = self.weight.target_bits\n                    q_period = self.weight.q_period\n                    self.weight = nn.Parameter(self.weight.data.t().reshape(num_heads,\n                                                                            -1)[mask.view(-1), :].reshape(-1,\n                                                                                                          shape).t())\n                    self.weight.start_bits = start_bits\n                    self.weight.target_bits = target_bits\n                    self.weight.q_period = q_period\n                else:\n\n                    shape = self.weight.size()\n                    self.weight.data = (self.weight.data.t().reshape(self.num_heads, -1) * mask.view(-1, 1)).reshape(\n                        shape[1], shape[0]).t()\n\n                if self.head_pruning_method == 'topk':\n                    del self.head_pruning_scores\n                self.head_pruning_method = None\n            else:\n                raise NotImplementedError\n        else:\n            start_bits = self.weight.start_bits\n            target_bits = self.weight.target_bits\n            q_period = self.weight.q_period\n            shape = self.weight.size(1)\n            self.weight = nn.Parameter(self.weight.data.reshape(num_heads, -1)[mask.view(-1), :].reshape(-1, shape))\n            self.weight.start_bits = start_bits\n            self.weight.target_bits = target_bits\n            self.weight.q_period = q_period\n            if self.bias is not None:\n                self.bias = nn.Parameter(self.bias.data.reshape(num_heads, -1)[mask.view(-1), :].reshape(-1))\n        self.head_pruning_enabled = False\n        return mask\n\n    def get_mask(self, pruning_type='row'):\n        if pruning_type == 'sparse':\n            if self.sparse_pruning_method == 'l1':\n                return self.sparse_pruning_mask.to(self.weight.device)\n            elif self.sparse_pruning_method == 'topk':\n                return TopKBinarizer.apply(self.sparse_mask_scores, self.sparse_pruning_ratio, False)\n            else:\n                raise NotImplementedError\n        if pruning_type == 'row':\n            if self.row_pruning_method == 'l1':\n                return self.row_pruning_mask.to(self.weight.device)\n            elif self.row_pruning_method == 'topk':\n                return TopKBinarizer.apply(self.row_mask_scores, self.row_pruning_ratio, False)\n            else:\n                raise NotImplementedError\n        elif pruning_type == 'head':\n            if self.head_pruning_method == 'topk':\n                return TopKBinarizer.apply(self.head_pruning_scores, self.head_pruning_ratio, False)\n            else:\n                raise NotImplementedError\n        else:\n            raise NotImplementedError\n\n    def enable_weight_quantization(self, start_bits, target_bits, quantization_period,\n                                   weight_quantization_enabled_in_forward, quantization_type, num_groups):\n        self.weight.start_bits = start_bits\n        self.weight.target_bits = target_bits\n        self.weight.q_period = quantization_period\n        self.weight_quantization_enabled_in_forward = weight_quantization_enabled_in_forward\n        if self.weight_quantization_enabled_in_forward:\n            logger.warning(\n                \"************ A lot of MoQ features are not supported in quantize_weight_in_forward mode, please consider to use DS-FP16 optimizer************\"\n            )\n            if self.weight.target_bits >= 3:\n                if quantization_type == 'symmetric':\n                    self.weight_quantizer = SymQuantizer.apply\n                else:\n                    self.weight_quantizer = AsymQuantizer.apply\n            elif self.weight.target_bits == 2:\n                assert quantization_type == 'symmetric', 'Only symmetric quantization is supported for ternary weight quantization'\n                self.weight_quantizer = TernaryQuantizer.apply\n            elif self.weight.target_bits == 1:\n                assert quantization_type == 'symmetric', 'Only symmetric quantization is supported for binary weight quantization'\n                self.weight_quantizer = BinaryQuantizer.apply\n            self.weight_quantize_num_groups = num_groups\n\n    def fix_weight_quantization(self):\n        self.weight.data = self.weight_quantizer(self.weight, self.weight.target_bits, None, None,\n                                                 self.weight_quantize_num_groups).data\n        self.weight_quantization_enabled_in_forward = False\n        return None\n\n    def enable_activation_quantization(self, bits, quantization_type, range_calibration):\n        assert bits in [4, 8], 'Only 4/8 bits activation quantization are supported for now'\n        self.activation_quantization_bits = bits\n        self.activation_quantization_method = f\"{quantization_type}_{range_calibration}\"\n        if range_calibration == 'static':\n            self.activation_quantizer = QuantAct(quant_mode=quantization_type)\n        else:\n            if quantization_type == 'symmetric':\n                self.activation_quantizer = SymQuantizer.apply\n            else:\n                self.activation_quantizer = AsymQuantizer.apply\n\n    def head_pruning_reshape(self, w, mask):\n        shape = w.shape\n        return (w.t().reshape(self.num_heads, -1) * mask.view(-1, 1)).reshape(shape[1], shape[0]).t()\n\n    def forward(self, input, skip_bias_add=False):\n\n        if self.weight_quantization_enabled_in_forward and self.weight_quantization_enabled:\n            weight = self.weight_quantizer(self.weight, self.weight.target_bits, None, None,\n                                           self.weight_quantize_num_groups)\n            bias = self.bias\n        else:\n            weight = self.weight\n            bias = self.bias\n\n        if self.sparse_pruning_enabled and self.sparse_pruning_method:\n            mask = self.get_mask(pruning_type='sparse')\n            weight = weight * mask.view(self.weight.size())\n\n        if self.row_pruning_enabled and self.row_pruning_method:\n            mask = self.get_mask(pruning_type='row')\n            weight = weight * mask.view(-1, 1)\n            if bias is not None:\n                bias = bias * mask.view(-1)\n\n        if self.head_pruning_enabled and self.head_pruning_method:\n            mask = self.get_mask(pruning_type='head')\n            weight = self.head_pruning_reshape(weight, mask)\n\n        if self.activation_quantization_enabled:\n            if 'dynamic' in self.activation_quantization_method:\n                num_groups = input.numel() // input.size(-1)\n            else:\n                num_groups = 1\n            input = self.activation_quantizer(input, self.activation_quantization_bits, None, None, num_groups)\n\n        if skip_bias_add:\n            # used for mpu linear layers\n            output = nn.functional.linear(input, weight, None)\n            return output, bias\n        else:\n            output = nn.functional.linear(input, weight, bias)\n            return output\n\n\nclass Conv2dLayer_Compress(nn.Conv2d):\n    \"\"\"\n    Conv2D layer with compression.\n    \"\"\"\n\n    def __init__(self, *kargs):\n        super(Conv2dLayer_Compress, self).__init__(*kargs)\n        self.sparse_pruning_method = None\n        self.channel_pruning_method = None\n        self.activation_quantization_method = None\n        self.weight.start_bits = None\n        self.weight.target_bits = None\n        self.weight.q_period = None\n        self.weight_quantization_enabled_in_forward = False\n        self.sparse_pruning_enabled = False\n        self.channel_pruning_enabled = False\n        self.activation_quantization_enabled = False\n\n    def __repr__(self):\n        s = ('{in_channels}, {out_channels}, kernel_size={kernel_size}'\n             ', stride={stride}')\n        if self.padding != (0, ) * len(self.padding):\n            s += ', padding={padding}'\n        if self.dilation != (1, ) * len(self.dilation):\n            s += ', dilation={dilation}'\n        if self.output_padding != (0, ) * len(self.output_padding):\n            s += ', output_padding={output_padding}'\n        if self.groups != 1:\n            s += ', groups={groups}'\n        if self.bias is None:\n            s += ', bias=False'\n        if self.padding_mode != 'zeros':\n            s += ', padding_mode={padding_mode}'\n        output = s.format(**self.__dict__)\n\n        return output + ' sparse pruning={}, channel pruning={}, activation quantization={}, weight_quantization={}'.format(\n            self.sparse_pruning_method is not None, self.channel_pruning_method is not None,\n            self.activation_quantization_method is not None, self.weight.target_bits)\n\n    def enable_sparse_pruning(self, ratio, method):\n        self.sparse_pruning_ratio = ratio\n        self.sparse_pruning_method = method\n        if method == 'l1':\n            weight_norm = torch.abs(self.weight.data)\n            mask = TopKBinarizer.apply(weight_norm, self.sparse_pruning_ratio, False)\n            mask = mask.view(self.weight.size())\n            mask = mask.to(self.weight.device)\n        elif method == 'topk':\n            self.sparse_mask_scores = nn.Parameter(torch.Tensor(self.weight.size()))\n            self.sparse_mask_scores.data = self.sparse_mask_scores.data.to(self.weight.device)\n            init.kaiming_uniform_(self.sparse_mask_scores, a=math.sqrt(5))\n            mask = None\n        else:\n            raise NotImplementedError\n\n        self.register_buffer('sparse_pruning_mask', mask)\n\n    def enable_channel_pruning(self, ratio, method):\n        # Here, we support two cases: L1 norm based pruning and topk based pruning\n        self.channel_pruning_ratio = ratio\n        self.channel_pruning_method = method\n\n        if method == 'l1':\n            # compute the l1 norm of each conv2d kernel (the last three dimension)\n            weight_norm = torch.linalg.norm(self.weight.data, ord=1, dim=[1, 2, 3])\n            mask = TopKBinarizer.apply(weight_norm, self.channel_pruning_ratio, False)\n            mask = mask.view(-1, 1, 1, 1)\n            mask = mask.to(self.weight.device)\n        elif method == 'topk':\n            self.channel_mask_scores = nn.Parameter(torch.Tensor(self.weight.size(0), 1, 1, 1))\n            self.channel_mask_scores.data = self.channel_mask_scores.data.to(self.weight.device)\n            init.kaiming_uniform_(self.channel_mask_scores, a=math.sqrt(5))\n            mask = None\n        else:\n            raise NotImplementedError\n\n        self.register_buffer('channel_pruning_mask', mask)\n\n    def fix_sparse_pruning_helper(self):\n        mask = self.get_mask(pruning_type='sparse')\n        self.weight.data = self.weight.data * mask\n        del self.sparse_pruning_mask\n        if self.sparse_pruning_method == 'topk':\n            del self.sparse_mask_scores\n        self.sparse_pruning_method = None\n        self.sparse_pruning_enabled = False\n        return None\n\n    def fix_channel_pruning_helper(self, mask=None, dim_reduction=False):\n        if mask is None:\n            if self.channel_pruning_method in ['l1', 'topk']:\n                mask = self.get_mask(pruning_type='channel').bool()\n                if dim_reduction:\n                    start_bits = self.weight.start_bits\n                    target_bits = self.weight.target_bits\n                    q_period = self.weight.q_period\n                    self.weight = nn.Parameter(self.weight.data[mask.view(-1), ...])\n                    self.weight.start_bits = start_bits\n                    self.weight.target_bits = target_bits\n                    self.weight.q_period = q_period\n                    if self.bias is not None:\n                        self.bias = nn.Parameter(self.bias.data[mask.view(-1)])\n                else:\n                    self.weight.data = self.weight.data * mask.view(-1, 1, 1, 1)\n                    if self.bias is not None:\n                        self.bias.data = self.bias.data * mask.view(-1)\n                del self.channel_pruning_mask\n                if self.channel_pruning_method == 'topk':\n                    del self.channel_mask_scores\n                self.channel_pruning_method = None\n            else:\n                raise NotImplementedError\n        else:\n            start_bits = self.weight.start_bits\n            target_bits = self.weight.target_bits\n            q_period = self.weight.q_period\n            self.weight = nn.Parameter(self.weight.data[:, mask.view(-1), ...])\n            self.weight.start_bits = start_bits\n            self.weight.target_bits = target_bits\n            self.weight.q_period = q_period\n            mask = None\n        self.channel_pruning_enabled = False\n        return mask\n\n    def get_mask(self, pruning_type='sparse'):\n        if pruning_type == 'sparse':\n            if self.sparse_pruning_method == 'l1':\n                return self.sparse_pruning_mask.to(self.weight.device)\n            elif self.sparse_pruning_method == 'topk':\n                return TopKBinarizer.apply(self.sparse_mask_scores, self.sparse_pruning_ratio, False)\n            else:\n                raise NotImplementedError\n        elif pruning_type == 'channel':\n            if self.channel_pruning_method == 'l1':\n                return self.channel_pruning_mask.to(self.weight.device)\n            elif self.channel_pruning_method == 'topk':\n                return TopKBinarizer.apply(self.channel_mask_scores, self.channel_pruning_ratio, False)\n            else:\n                raise NotImplementedError\n        else:\n            raise NotImplementedError\n\n    def fix_weight_quantization(self):\n        self.weight.data = self.weight_quantizer(self.weight, self.weight.target_bits, None, None,\n                                                 self.weight_quantize_num_groups).data\n        self.weight_quantization_enabled_in_forward = False\n        return None\n\n    def enable_weight_quantization(self, start_bits, target_bits, quantization_period,\n                                   weight_quantization_enabled_in_forward, quantization_type, num_groups):\n        self.weight.start_bits = start_bits\n        self.weight.target_bits = target_bits\n        self.weight.q_period = quantization_period\n        self.weight_quantization_enabled_in_forward = weight_quantization_enabled_in_forward\n        if self.weight_quantization_enabled_in_forward:\n            assert self.weight.target_bits >= 4, 'Only >=4 bits weight quantization are supported during forward pass for now'\n            logger.warning(\n                \"************ A lot of MoQ features are not supported in quantize_weight_in_forward mode, please consider to use DS-FP16 optimizer************\"\n            )\n            if quantization_type == 'symmetric':\n                self.weight_quantizer = SymQuantizer.apply\n            else:\n                self.weight_quantizer = AsymQuantizer.apply\n            self.weight_quantize_num_groups = num_groups\n\n    def enable_activation_quantization(self, bits, quantization_type, range_calibration):\n        assert bits in [4, 8], 'Only 4/8 bits activation quantization are supported for now'\n        self.activation_quantization_bits = bits\n        self.activation_quantization_method = f\"{quantization_type}_{range_calibration}\"\n        if range_calibration == 'static':\n            self.activation_quantizer = QuantAct(quant_mode=quantization_type)\n        else:\n            if quantization_type == 'symmetric':\n                self.activation_quantizer = SymQuantizer.apply\n            else:\n                self.activation_quantizer = AsymQuantizer.apply\n\n    def forward(self, input):\n\n        if self.weight_quantization_enabled_in_forward and self.weight_quantization_enabled:\n            weight = self.weight_quantizer(self.weight, self.weight.target_bits, None, None,\n                                           self.weight_quantize_num_groups)\n            bias = self.bias\n        else:\n            weight = self.weight\n            bias = self.bias\n\n        if self.sparse_pruning_enabled and self.sparse_pruning_method:\n            mask = self.get_mask(pruning_type='sparse')\n            weight = weight * mask.view(self.weight.size())\n\n        if self.channel_pruning_enabled:\n            mask = self.get_mask(pruning_type='channel')\n            weight = weight * mask.view(-1, 1, 1, 1)\n            if bias is not None:\n                bias = bias * mask.view(-1)\n\n        if self.activation_quantization_enabled:\n            if 'dynamic' in self.activation_quantization_method:\n                num_groups = input.numel() // input[0].numel()\n            else:\n                num_groups = 1\n            input = self.activation_quantizer(input, self.activation_quantization_bits, None, None, num_groups)\n\n        return nn.functional.conv2d(input, weight, bias, self.stride, self.padding, self.dilation, self.groups)\n\n\nclass BNLayer_Compress(nn.BatchNorm2d):\n\n    def fix_channel_pruning_helper(self, mask, dim_reduction=True):\n        self.weight = nn.Parameter(self.weight.data[mask.view(-1)])\n        self.bias = nn.Parameter(self.bias.data[mask.view(-1)])\n        self.running_mean = self.running_mean[mask.view(-1)]\n        self.running_var = self.running_var[mask.view(-1)]\n\n\ndef _reduce(input_):\n    \"\"\"All-reduce the input tensor across model parallel group.\"\"\"\n    group = g_mpu.get_model_parallel_group()\n\n    # Bypass the function if we are using only 1 GPU.\n    if dist.get_world_size(group=group) == 1:\n        return input_\n\n    # All-reduce.\n    dist.all_reduce(input_, group=group)\n\n    return input_\n\n\ndef split_tensor_along_last_dim(tensor, num_partitions, contiguous_split_chunks=False):\n    \"\"\"Split a tensor along its last dimension.\n    Arguments:\n        tensor: input tensor.\n        num_partitions: number of partitions to split the tensor\n        contiguous_split_chunks: If True, make each chunk contiguous\n                                 in memory.\n    \"\"\"\n    # Get the size and dimension.\n    last_dim = tensor.dim() - 1\n    assert tensor.size()[last_dim] % num_partitions == 0\n    last_dim_size = tensor.size()[last_dim] // num_partitions\n    # Split.\n    tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)\n    # Note: torch.split does not create contiguous tensors by default.\n    if contiguous_split_chunks:\n        return tuple(chunk.contiguous() for chunk in tensor_list)\n\n    return tensor_list\n\n\ndef _split(input_):\n    \"\"\"Split the tensor along its last dimension and keep the\n    corresponding slice.\"\"\"\n    group = g_mpu.get_model_parallel_group()\n\n    # Bypass the function if we are using only 1 GPU.\n    if dist.get_world_size(group=group) == 1:\n        return input_\n\n    # Split along last dimension.\n    world_size = dist.get_world_size(group=group)\n    input_list = split_tensor_along_last_dim(input_, world_size)\n\n    # Note: torch.split does not create contiguous tensors by default.\n    rank = dist.get_rank(group=group)\n    output = input_list[rank].contiguous()\n\n    return output\n\n\ndef _gather(input_):\n    \"\"\"Gather tensors and concatenate along the last dimension.\"\"\"\n    group = g_mpu.get_model_parallel_group()\n\n    # Bypass the function if we are using only 1 GPU.\n    if dist.get_world_size(group=group) == 1:\n        return input_\n\n    # Size and dimension.\n    last_dim = input_.dim() - 1\n    rank = dist.get_rank(group=group)\n    world_size = dist.get_world_size(group=group)\n\n    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]\n    tensor_list[rank] = input_\n    dist.all_gather(tensor_list, input_, group=group)\n\n    # Note: torch.cat already creates a contiguous tensor.\n    output = torch.cat(tensor_list, dim=last_dim).contiguous()\n\n    return output\n\n\nclass _CopyToModelParallelRegion(torch.autograd.Function):\n    \"\"\"Pass the input to the model parallel region.\"\"\"\n\n    @staticmethod\n    def forward(ctx, input_):\n        return input_\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return _reduce(grad_output)\n\n\nclass _ReduceFromModelParallelRegion(torch.autograd.Function):\n    \"\"\"All-reduce the input from the model parallel region.\"\"\"\n\n    @staticmethod\n    def forward(ctx, input_):\n        return _reduce(input_)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output\n\n\nclass _ScatterToModelParallelRegion(torch.autograd.Function):\n    \"\"\"Split the input and keep only the corresponding chuck to the rank.\"\"\"\n\n    @staticmethod\n    def forward(ctx, input_):\n        return _split(input_)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return _gather(grad_output)\n\n\nclass _GatherFromModelParallelRegion(torch.autograd.Function):\n    \"\"\"Gather the input from model parallel region and concatenate.\"\"\"\n\n    @staticmethod\n    def forward(ctx, input_):\n        return _gather(input_)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return _split(grad_output)\n\n\n# -----------------\n# Helper functions.\n# -----------------\n\n\ndef copy_to_model_parallel_region(input_):\n    return _CopyToModelParallelRegion.apply(input_)\n\n\ndef reduce_from_model_parallel_region(input_):\n    return _ReduceFromModelParallelRegion.apply(input_)\n\n\ndef scatter_to_model_parallel_region(input_):\n    return _ScatterToModelParallelRegion.apply(input_)\n\n\ndef gather_from_model_parallel_region(input_):\n    return _GatherFromModelParallelRegion.apply(input_)\n\n\nclass ColumnParallelLinear_Compress(LinearLayer_Compress):\n\n    def __init__(self, mpu, input_size, output_size, bias=True, gather_output=True, skip_bias_add=False):\n        # Keep input parameters\n        global g_mpu\n        g_mpu = mpu\n        self.input_size = input_size\n        self.output_size = output_size\n        self.gather_output = gather_output\n        self.skip_bias_add = skip_bias_add\n\n        # Divide the weight matrix along the last dimension.\n        world_size = mpu.get_model_parallel_world_size()\n        assert output_size % world_size == 0\n        self.output_size_per_partition = output_size // world_size\n\n        super(ColumnParallelLinear_Compress, self).__init__(self.input_size, self.output_size_per_partition, bias=bias)\n\n    def forward(self, input_):\n        # Set up backprop all-reduce.\n        input_parallel = copy_to_model_parallel_region(input_)\n        # Matrix multiply.\n        if self.skip_bias_add:\n            output_parallel, bias = super().forward(input_parallel, True)\n        else:\n            output_parallel = super().forward(input_parallel)\n            bias = None\n        if self.gather_output:\n            # All-gather across the partitions.\n            output = gather_from_model_parallel_region(output_parallel)\n        else:\n            output = output_parallel\n        return output, bias\n\n\nclass RowParallelLinear_Compress(LinearLayer_Compress):\n\n    def __init__(self, mpu, input_size, output_size, bias=True, input_is_parallel=False, skip_bias_add=False):\n        # Keep input parameters\n        global g_mpu\n        g_mpu = mpu\n        self.input_size = input_size\n        self.output_size = output_size\n        self.input_is_parallel = input_is_parallel\n        self.skip_bias_add = skip_bias_add\n\n        # Divide the weight matrix along the last dimension.\n        world_size = mpu.get_model_parallel_world_size()\n        assert input_size % world_size == 0\n        self.input_size_per_partition = input_size // world_size\n\n        super(RowParallelLinear_Compress, self).__init__(self.input_size_per_partition, self.output_size, bias=bias)\n\n    def forward(self, input_):\n        # Set up backprop all-reduce.\n        if self.input_is_parallel:\n            input_parallel = input_\n        else:\n            input_parallel = scatter_to_model_parallel_region(input_)\n        # Matrix multiply.\n        output_parallel, bias = super().forward(input_parallel, True)\n\n        # All-reduce across all the partitions.\n        output_ = reduce_from_model_parallel_region(output_parallel)\n        if not self.skip_bias_add:\n            if bias is not None:\n                output = output_ + bias\n            else:\n                output = output_\n            output_bias = None\n        else:\n            output = output_\n            output_bias = bias\n        return output, output_bias\n", "deepspeed/compression/scheduler.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .compress import get_module_name\nfrom .constants import *\nfrom .helper import recursive_getattr\nfrom deepspeed.utils import logger\n\n\nclass compression_scheduler():\n    '''\n    Used to schedule different compression methods\n    '''\n\n    def __init__(self, model, compression_config):\n        self.model = model\n        self.compression_config = compression_config\n        self.make_init()\n        self.training_steps = 0\n        self.weight_quantization_enabled = False\n\n        self.verbose = {\n            WEIGHT_QUANTIZATION: False,\n            ACTIVATION_QUANTIZATION: False,\n            SPARSE_PRUNING: False,\n            HEAD_PRUNING: False,\n            ROW_PRUNING: False,\n            CHANNEL_PRUNING: False\n        }\n\n    def make_init(self):\n        self.different_compression_methods = {}\n        for method, method_content in self.compression_config.items():\n            if LAYER_REDUCTION in method:\n                continue\n            self.different_compression_methods[method] = {\n                TECHNIQUE_ENABLED: False,\n                SHARED_PARAMETERS: None,\n                DIFFERENT_GROUPS: []\n            }\n            exist_module_name = set()\n            shared_parameters = method_content[SHARED_PARAMETERS]\n            self.different_compression_methods[method][TECHNIQUE_ENABLED] = shared_parameters[TECHNIQUE_ENABLED]\n            self.different_compression_methods[method][SHARED_PARAMETERS] = shared_parameters\n\n            for group_name, method_parameters in method_content[DIFFERENT_GROUPS].items():\n                module_name_list = []\n                for key_word in method_parameters[DIFFERENT_GROUPS_MODULE_SCOPE]:\n                    module_name, exist_module_name = get_module_name(group_name,\n                                                                     self.model,\n                                                                     key_word,\n                                                                     exist_module_name,\n                                                                     verbose=False)\n                    module_name_list.extend(module_name)\n                if module_name_list:\n                    self.different_compression_methods[method][DIFFERENT_GROUPS].append(\n                        [group_name, module_name_list,\n                         method_parameters.copy().pop('params')])\n\n    def check_weight_quantization(self):\n        # check weight quantization\n        wq = self.different_compression_methods[WEIGHT_QUANTIZATION]\n        if not wq[TECHNIQUE_ENABLED]:\n            return\n        else:\n            shared_parameters = wq[SHARED_PARAMETERS]\n            if self.training_steps >= shared_parameters[TECHNIQUE_SCHEDULE_OFFSET]:\n                for group_name, module_name_list, method_parameters in wq[DIFFERENT_GROUPS]:\n                    for module_name in module_name_list:\n                        module = recursive_getattr(self.model, module_name)\n                        module.weight_quantization_enabled = True\n\n                if not self.verbose[WEIGHT_QUANTIZATION]:\n                    logger.info(f'Weight quantization is enabled at step {self.training_steps}')\n                    self.weight_quantization_enabled = True\n                    self.verbose[WEIGHT_QUANTIZATION] = True\n\n    def check_activation_quantization(self):\n        # check activation quantization\n        aq = self.different_compression_methods[ACTIVATION_QUANTIZATION]\n        if not aq[TECHNIQUE_ENABLED]:\n            return\n        else:\n            shared_parameters = aq[SHARED_PARAMETERS]\n            if self.training_steps >= shared_parameters[TECHNIQUE_SCHEDULE_OFFSET]:\n                for group_name, module_name_list, method_parameters in aq[DIFFERENT_GROUPS]:\n                    for module_name in module_name_list:\n                        module = recursive_getattr(self.model, module_name)\n                        module.activation_quantization_enabled = True\n                if not self.verbose[ACTIVATION_QUANTIZATION]:\n                    logger.info(f'Activation quantization is enabled at step {self.training_steps}')\n                    self.verbose[ACTIVATION_QUANTIZATION] = True\n\n    def check_sparse_pruning(self):\n        # check sparse pruning\n        sp = self.different_compression_methods[SPARSE_PRUNING]\n        if not sp[TECHNIQUE_ENABLED]:\n            return\n        else:\n            shared_parameters = sp[SHARED_PARAMETERS]\n            if shared_parameters[TECHNIQUE_SCHEDULE_OFFSET] <= self.training_steps <= shared_parameters[\n                    TECHNIQUE_SCHEDULE_OFFSET_END]:\n                for group_name, module_name_list, method_parameters in sp[DIFFERENT_GROUPS]:\n                    for module_name in module_name_list:\n                        module = recursive_getattr(self.model, module_name)\n                        module.sparse_pruning_enabled = True\n                if not self.verbose[SPARSE_PRUNING]:\n                    logger.info(f'Sparse pruning is enabled at step {self.training_steps}')\n                    self.verbose[SPARSE_PRUNING] = True\n\n    def check_head_pruning(self):\n        # check head pruning\n        hp = self.different_compression_methods[HEAD_PRUNING]\n        if not hp[TECHNIQUE_ENABLED]:\n            return\n        else:\n            shared_parameters = hp[SHARED_PARAMETERS]\n            if self.training_steps >= shared_parameters[TECHNIQUE_SCHEDULE_OFFSET]:\n                for group_name, module_name_list, method_parameters in hp[DIFFERENT_GROUPS]:\n                    for module_name in module_name_list:\n                        module = recursive_getattr(self.model, module_name)\n                        module.head_pruning_enabled = True\n                if not self.verbose[HEAD_PRUNING]:\n                    logger.info(f'Head pruning is enabled at step {self.training_steps}')\n                    self.verbose[HEAD_PRUNING] = True\n\n    def check_row_pruning(self):\n        # check row pruning\n        rp = self.different_compression_methods[ROW_PRUNING]\n        if not rp[TECHNIQUE_ENABLED]:\n            return\n        else:\n            shared_parameters = rp[SHARED_PARAMETERS]\n            if self.training_steps >= shared_parameters[TECHNIQUE_SCHEDULE_OFFSET]:\n                for group_name, module_name_list, method_parameters in rp[DIFFERENT_GROUPS]:\n                    for module_name in module_name_list:\n                        module = recursive_getattr(self.model, module_name)\n                        module.row_pruning_enabled = True\n                if not self.verbose[ROW_PRUNING]:\n                    logger.info(f'Row pruning is enabled at step {self.training_steps}')\n                    self.verbose[ROW_PRUNING] = True\n\n    def check_channel_pruning(self):\n        # check channel pruning\n        cp = self.different_compression_methods[CHANNEL_PRUNING]\n        if not cp[TECHNIQUE_ENABLED]:\n            return\n        else:\n            shared_parameters = cp[SHARED_PARAMETERS]\n            if self.training_steps >= shared_parameters[TECHNIQUE_SCHEDULE_OFFSET]:\n                for group_name, module_name_list, method_parameters in cp[DIFFERENT_GROUPS]:\n                    for module_name in module_name_list:\n                        module = recursive_getattr(self.model, module_name)\n                        module.channel_pruning_enabled = True\n                if not self.verbose[CHANNEL_PRUNING]:\n                    logger.info(f'Channel pruning is enabled at step {self.training_steps}')\n                    self.verbose[CHANNEL_PRUNING] = True\n\n    def check_all_modules(self):\n        # check all different compression methods we have\n        self.check_weight_quantization()\n        self.check_activation_quantization()\n        self.check_sparse_pruning()\n        self.check_head_pruning()\n        self.check_row_pruning()\n        self.check_channel_pruning()\n\n    def step(self, step_zero_check=False):\n        if not step_zero_check:\n            self.training_steps += 1\n        self.check_all_modules()\n", "deepspeed/compression/constants.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\n#########################################\n# Compression Methods\n# It has several sub-components\n# #########################################\nCOMPRESSION_TRAINING = \"compression_training\"\nSHARED_PARAMETERS = \"shared_parameters\"\nDIFFERENT_GROUPS = \"different_groups\"\nTECHNIQUE_ENABLED = \"enabled\"\nTECHNIQUE_SCHEDULE_OFFSET = \"schedule_offset\"\nTECHNIQUE_SCHEDULE_OFFSET_END = \"schedule_offset_end\"\nDIFFERENT_GROUPS_PARAMETERS = \"params\"\nDIFFERENT_GROUPS_MODULE_SCOPE = \"modules\"\nDIFFERENT_GROUPS_MODULE_SCOPE_DEFAULT = \"*\"\nDIFFERENT_GROUPS_RELATED_MODULE_SCOPE = \"related_modules\"\nDIFFERENT_GROUPS_RELATED_MODULE_SCOPE_DEFAULT = None\n# COMPRESSION_TRAINING_ENABLED = \"enabled\"\n# COMPRESSION_TRAINING_ENABLED_DEFAULT = False\n\n####\n# Layer Reduction\n####\nLAYER_REDUCTION = \"layer_reduction\"\nLAYER_REDUCTION_ENABLED = \"enabled\"\nLAYER_REDUCTION_ENABLED_DEFAULT = False\nKEEP_NUMBER_LAYER = \"keep_number_layer\"\nMODULE_NAME_PREFIX = \"module_name_prefix\"\nTEACHER_LAYER = \"teacher_layer\"\nOTHER_MODULE_NAME = \"other_module_name\"\n\n####\n# Weight Quantization\n####\nWEIGHT_QUANTIZATION = \"weight_quantization\"\n\nWEIGHT_QUANTIZATION_PERIOD = \"quantization_period\"\nWEIGHT_QUANTIZATION_PERIOD_DEFAULT = 1\n\nWEIGHT_QUANTIZE_IN_FORWARD_ENABLED = \"quantize_weight_in_forward\"\nWEIGHT_QUANTIZE_IN_FORWARD_ENABLED_DEFAULT = False\n\nWEIGHT_QUANTIZE_ENABLED = TECHNIQUE_ENABLED\nWEIGHT_QUANTIZE_ENABLED_DEFAULT = False\n\nWEIGHT_QUANTIZE_KERNEL = \"quantizer_kernel\"\nWEIGHT_QUANTIZE_KERNEL_DEFAULT = False\n\nWEIGHT_QUANTIZE_SCHEDULE_OFFSET = TECHNIQUE_SCHEDULE_OFFSET\nWEIGHT_QUANTIZE_SCHEDULE_OFFSET_DEFAULT = 0\n\nWEIGHT_QUANTIZE_GROUPS = \"quantize_groups\"\nWEIGHT_QUANTIZE_GROUPS_DEFAULT = 1\n\nWEIGHT_QUANTIZE_VERBOSE = \"quantize_verbose\"\nWEIGHT_QUANTIZE_VERBOSE_DEFAULT = False\n\nWEIGHT_QUANTIZE_TYPE = \"quantization_type\"\nWEIGHT_QUANTIZE_TYPE_DEFAULT = \"symmetric\"\nWEIGHT_QUANTIZE_SYMMETRIC = \"symmetric\"\nWEIGHT_QUANTIZE_ASYMMETRIC = \"asymmetric\"\n\nWEIGHT_QUANTIZE_ROUNDING = \"rounding\"\nWEIGHT_QUANTIZE_ROUNDING_DEFAULT = \"nearest\"\nWEIGHT_QUANTIZE_STOCHASTIC_ROUNDING = \"stochastic\"\nWEIGHT_QUANTIZE_NEAREST_ROUNDING = \"nearest\"\n# maybe deleted for a cleaner version\nWEIGHT_QUANTIZE_FP16_MIXED_QUANTIZE = \"fp16_mixed_quantize\"\n\nWEIGHT_QUANTIZE_FP16_MIXED_QUANTIZE_ENABLED = \"enabled\"\nWEIGHT_QUANTIZE_FP16_MIXED_QUANTIZE_ENABLED_DEFAULT = False\n\nWEIGHT_QUANTIZE_CHANGE_RATIO = \"quantize_change_ratio\"\nWEIGHT_QUANTIZE_CHANGE_RATIO_DEFAULT = 0.001\n\nWEIGHT_QUANTIZE_START_BITS = \"start_bits\"\nWEIGHT_QUANTIZE_TARGET_BITS = \"target_bits\"\n###\n# Activation Quantization\n###\nACTIVATION_QUANTIZATION = \"activation_quantization\"\n\nACTIVATION_QUANTIZATION_ENABLED = TECHNIQUE_ENABLED\nACTIVATION_QUANTIZATION_ENABLED_DEFAULT = False\n\nACTIVATION_QUANTIZE_SCHEDULE_OFFSET = TECHNIQUE_SCHEDULE_OFFSET\nACTIVATION_QUANTIZE_SCHEDULE_OFFSET_DEFAULT = 1000\n\nACTIVATION_QUANTIZE_TYPE = \"quantization_type\"\nACTIVATION_QUANTIZE_TYPE_DEFAULT = \"symmetric\"\nACTIVATION_QUANTIZE_SYMMETRIC = \"symmetric\"\nACTIVATION_QUANTIZE_ASYMMETRIC = \"asymmetric\"\n\nACTIVATION_QUANTIZE_RANGE = 'range_calibration'\nACTIVATION_QUANTIZE_RANGE_DEFAULT = 'dynamic'\nACTIVATION_QUANTIZE_RANGE_STATIC = 'static'\nACTIVATION_QUANTIZE_RANGE_DYNAMIC = 'dynamic'\n\nACTIVATION_QUANTIZE_BITS = \"bits\"\n###\n# Sparse Pruning\n###\nSPARSE_PRUNING = \"sparse_pruning\"\n\nSPARSE_PRUNING_ENABLED = TECHNIQUE_ENABLED\nSPARSE_PRUNING_ENABLED_DEFAULT = False\n\nSPARSE_PRUNING_METHOD = \"method\"\nSPARSE_PRUNING_METHOD_DEFAULT = \"l1\"\nSPARSE_PRUNING_METHOD_L1 = \"l1\"\nSPARSE_PRUNING_METHOD_TOPK = \"topk\"\nSPARSE_PRUNING_METHOD_SNIP_MOMENTUM = \"snip_momentum\"\n\nSPARSE_PRUNING_BLOCK_PATTERN = \"block_pattern\"\nSPARSE_PRUNING_BLOCK_PATTERN_DEFAULT = \"4x1\"\n\nSPARSE_PRUNING_SCHEDULE_OFFSET_STRIDE = \"schedule_offset_stride\"\nSPARSE_PRUNING_SCHEDULE_OFFSET_STRIDE_DEFAULT = 1\n\nSPARSE_PRUNING_SCHEDULE_OFFSET = TECHNIQUE_SCHEDULE_OFFSET\nSPARSE_PRUNING_SCHEDULE_OFFSET_DEFAULT = 1000\n\nSPARSE_PRUNING_SCHEDULE_OFFSET_END = TECHNIQUE_SCHEDULE_OFFSET_END\nSPARSE_PRUNING_SCHEDULE_OFFSET_END_DEFAULT = SPARSE_PRUNING_SCHEDULE_OFFSET_DEFAULT\n\nSPARSE_PRUNING_DENSE_RATIO = \"dense_ratio\"\nSPARSE_PRUNING_DENSE_RATIO_DEFAULT = 0.1\n\nSPARSE_PRUNING_EXCLUDED_MODULES = \"excluded_modules\"\nSPARSE_PRUNING_EXCLUDED_MODULES_DEFAULT = []\n###\n# Row Pruning\n###\nROW_PRUNING = \"row_pruning\"\n\nROW_PRUNING_ENABLED = TECHNIQUE_ENABLED\nROW_PRUNING_ENABLED_DEFAULT = False\n\nROW_PRUNING_METHOD = \"method\"\nROW_PRUNING_METHOD_DEFAULT = \"l1\"\nROW_PRUNING_METHOD_L1 = \"l1\"\nROW_PRUNING_METHOD_TOPK = \"topk\"\n\nROW_PRUNING_SCHEDULE_OFFSET = TECHNIQUE_SCHEDULE_OFFSET\nROW_PRUNING_SCHEDULE_OFFSET_DEFAULT = 1000\n\nROW_PRUNING_DENSE_RATIO = \"dense_ratio\"\n\n###\n# Head Pruning\n###\nHEAD_PRUNING = \"head_pruning\"\n\nHEAD_PRUNING_ENABLED = TECHNIQUE_ENABLED\nHEAD_PRUNING_ENABLED_DEFAULT = False\n\nHEAD_PRUNING_METHOD = \"method\"\nHEAD_PRUNING_METHOD_DEFAULT = \"topk\"\nHEAD_PRUNING_METHOD_L1 = \"l1\"\nHEAD_PRUNING_METHOD_TOPK = \"topk\"\n\nHEAD_PRUNING_SCHEDULE_OFFSET = TECHNIQUE_SCHEDULE_OFFSET\nHEAD_PRUNING_SCHEDULE_OFFSET_DEFAULT = 1000\n\nHEAD_PRUNING_NUM_HEADS = \"num_heads\"\n\nHEAD_PRUNING_DENSE_RATIO = \"dense_ratio\"\n\n###\n# Channel Pruning\n###\nCHANNEL_PRUNING = \"channel_pruning\"\n\nCHANNEL_PRUNING_ENABLED = TECHNIQUE_ENABLED\nCHANNEL_PRUNING_ENABLED_DEFAULT = False\n\nCHANNEL_PRUNING_METHOD = \"method\"\nCHANNEL_PRUNING_METHOD_DEFAULT = \"l1\"\nCHANNEL_PRUNING_METHOD_L1 = \"l1\"\nCHANNEL_PRUNING_METHOD_TOPK = \"topk\"\n\nCHANNEL_PRUNING_SCHEDULE_OFFSET = TECHNIQUE_SCHEDULE_OFFSET\nCHANNEL_PRUNING_SCHEDULE_OFFSET_DEFAULT = 1000\n\nCHANNEL_PRUNING_DENSE_RATIO = \"dense_ratio\"\n", "deepspeed/compression/helper.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nfrom .basic_layer import Embedding_Compress, LinearLayer_Compress, Conv2dLayer_Compress, BNLayer_Compress, ColumnParallelLinear_Compress, RowParallelLinear_Compress\nfrom .constants import *\nfrom deepspeed.utils import logger\n\ntry:\n    from neural_compressor.compression import pruner as nc_pruner\nexcept ImportError as e:\n    nc_pruner = None\n\n\ndef recursive_getattr(model, module_name):\n    \"\"\"\n    Recursively get the attribute of a module.\n    Args:\n        model (`torch.nn.Module`)\n            The model to get the attribute from.\n        module_name (`str`)\n            The name of the module to get the attribute from.\n    \"\"\"\n    split_list = module_name.split('.')\n    output = model\n    for name in split_list:\n        output = getattr(output, name)\n    return output\n\n\ndef recursive_setattr(model, module_name, module):\n    \"\"\"\n    Recursively set the attribute of a module.\n    Args:\n        model (`torch.nn.Module`)\n            The model to set the attribute in.\n        module_name (`str`)\n            The name of the module to set the attribute in.\n        module (`torch.nn.Module`)\n            The module to set the attribute to.\n    \"\"\"\n    split_list = module_name.split('.')\n    output = model\n    for name in split_list[:-1]:\n        output = getattr(output, name)\n    output.__setattr__(split_list[-1], module)\n\n\ndef module_replacement(model, module_name, compression_technique=None, mpu=None):\n    \"\"\"\n    Replace a module with a new module.\n    Args:\n        model (`torch.nn.Module`)\n            The model to replace the module in.\n        module_name (`str`)\n            The name of the module to replace.\n        compression_technique (`str`)\n            The compression technique to use for the new module.\n    \"\"\"\n\n    # Get the old module\n    old_module = recursive_getattr(model, module_name)\n\n    need_bias = False\n    if hasattr(old_module, 'bias') and old_module.bias is not None:\n        need_bias = True\n\n    # Initialize the new module\n    if isinstance(old_module, LinearLayer_Compress) or isinstance(old_module, torch.nn.Linear):\n        if isinstance(old_module, LinearLayer_Compress):\n            new_module = old_module\n        else:\n            new_module = LinearLayer_Compress(old_module.in_features, old_module.out_features,\n                                              bias=need_bias).to(device=old_module.weight.device,\n                                                                 dtype=old_module.weight.dtype)\n            new_module.weight.data = old_module.weight.data\n            if need_bias:\n                new_module.bias.data = old_module.bias.data\n    elif isinstance(old_module, Conv2dLayer_Compress) or isinstance(old_module, torch.nn.Conv2d):\n        if isinstance(old_module, Conv2dLayer_Compress):\n            new_module = old_module\n        else:\n            new_module = Conv2dLayer_Compress(old_module.in_channels, old_module.out_channels, old_module.kernel_size, old_module.stride, old_module.padding, \\\n                                            old_module.dilation, old_module.groups, need_bias, \\\n                                            old_module.padding_mode).to(device=old_module.weight.device, dtype=old_module.weight.dtype)\n            new_module.weight.data = old_module.weight.data\n            if need_bias:\n                new_module.bias.data = old_module.bias.data\n    elif isinstance(old_module, torch.nn.BatchNorm2d):\n        new_module = BNLayer_Compress(old_module.num_features, old_module.eps, old_module.momentum, old_module.affine,\n                                      old_module.track_running_stats).to(old_module.weight.device,\n                                                                         old_module.weight.dtype)\n        new_module.weight.data = old_module.weight.data\n        if need_bias:\n            new_module.bias.data = old_module.bias.data\n        new_module.running_mean.data = old_module.running_mean.data\n        new_module.running_var.data = old_module.running_var.data\n    elif isinstance(old_module, Embedding_Compress) or isinstance(old_module, torch.nn.Embedding):\n        if isinstance(old_module, Embedding_Compress):\n            new_module = old_module\n        else:\n            new_module = Embedding_Compress(old_module.num_embeddings, old_module.embedding_dim, old_module.padding_idx, old_module.max_norm, old_module.norm_type, \\\n                                        old_module.scale_grad_by_freq, old_module.sparse).to(device=old_module.weight.device, dtype=old_module.weight.dtype)\n            new_module.weight.data = old_module.weight.data\n    elif mpu is not None and (isinstance(old_module, ColumnParallelLinear_Compress)\n                              or isinstance(old_module, mpu.ColumnParallelLinear)):\n        if isinstance(old_module, ColumnParallelLinear_Compress):\n            new_module = old_module\n        else:\n            new_module = ColumnParallelLinear_Compress(mpu,\n                                                       old_module.input_size,\n                                                       old_module.output_size,\n                                                       gather_output=old_module.gather_output,\n                                                       skip_bias_add=old_module.skip_bias_add,\n                                                       bias=need_bias).to(device=old_module.weight.device,\n                                                                          dtype=old_module.weight.dtype)\n            new_module.weight.data = old_module.weight.data\n            if need_bias:\n                new_module.bias.data = old_module.bias.data\n    elif mpu is not None and (isinstance(old_module, RowParallelLinear_Compress)\n                              or isinstance(old_module, mpu.RowParallelLinear)):\n        if isinstance(old_module, RowParallelLinear_Compress):\n            new_module = old_module\n        else:\n            new_module = RowParallelLinear_Compress(mpu,\n                                                    old_module.input_size,\n                                                    old_module.output_size,\n                                                    input_is_parallel=old_module.input_is_parallel,\n                                                    skip_bias_add=old_module.skip_bias_add,\n                                                    bias=need_bias).to(device=old_module.weight.device,\n                                                                       dtype=old_module.weight.dtype)\n            new_module.weight.data = old_module.weight.data\n            if need_bias:\n                new_module.bias.data = old_module.bias.data\n    else:\n        new_module = None\n\n    if compression_technique is not None:\n        for k, v in compression_technique.items():\n            if k == SPARSE_PRUNING:\n                if v[SPARSE_PRUNING_ENABLED]:\n                    new_module.enable_sparse_pruning(v[SPARSE_PRUNING_DENSE_RATIO], v[SPARSE_PRUNING_METHOD])\n            elif k == ROW_PRUNING:\n                if v[ROW_PRUNING_ENABLED]:\n                    new_module.enable_row_pruning(v[ROW_PRUNING_DENSE_RATIO], v[ROW_PRUNING_METHOD])\n            elif k == HEAD_PRUNING:\n                if v[HEAD_PRUNING_ENABLED]:\n                    new_module.enable_head_pruning(v[HEAD_PRUNING_DENSE_RATIO], v[HEAD_PRUNING_METHOD],\n                                                   v[HEAD_PRUNING_NUM_HEADS])\n            elif k == ACTIVATION_QUANTIZATION:\n                if v[ACTIVATION_QUANTIZATION_ENABLED]:\n                    new_module.enable_activation_quantization(v[ACTIVATION_QUANTIZE_BITS], v[ACTIVATION_QUANTIZE_TYPE],\n                                                              v[ACTIVATION_QUANTIZE_RANGE])\n            elif k == WEIGHT_QUANTIZATION:\n                if v[WEIGHT_QUANTIZE_ENABLED]:\n                    new_module.enable_weight_quantization(v[WEIGHT_QUANTIZE_START_BITS],\n                                                          v[WEIGHT_QUANTIZE_TARGET_BITS],\n                                                          v[WEIGHT_QUANTIZATION_PERIOD],\n                                                          v[WEIGHT_QUANTIZE_IN_FORWARD_ENABLED],\n                                                          v[WEIGHT_QUANTIZE_TYPE], v[WEIGHT_QUANTIZE_GROUPS])\n            elif k == CHANNEL_PRUNING:\n                if v[CHANNEL_PRUNING_ENABLED]:\n                    new_module.enable_channel_pruning(v[CHANNEL_PRUNING_DENSE_RATIO], v[CHANNEL_PRUNING_METHOD])\n            else:\n                raise NotImplementedError('Compression technique {} is not implemented'.format(k))\n\n    # Replace the old module with the new one\n    recursive_setattr(model, module_name, new_module)\n\n\ndef is_module_compressible(module, mpu=None):\n    ret = isinstance(module, torch.nn.Linear) or \\\n          isinstance(module, torch.nn.Conv2d) or \\\n          isinstance(module, torch.nn.Embedding) or \\\n          isinstance(module, torch.nn.BatchNorm2d)\n\n    if mpu is not None:\n        ret = ret or isinstance(module, mpu.RowParallelLinear) or isinstance(module, mpu.ColumnParallelLinear)\n\n    return ret\n\n\ndef compression_preparation(model, compression_technique_list, mpu):\n    \"\"\"\n    Prepare the compression techniques of a model.\n    Args:\n        model (`torch.nn.Module`)\n            The model to prepare the compression techniques of.\n        compression_technique_list (`list`)\n            The list of compression techniques to prepare the model to.\n            list[]\n    \"\"\"\n    # Here we first replace all module with our linear wrapper\n    for module_name, module in model.named_modules():\n        if is_module_compressible(module, mpu):\n            module_replacement(model, module_name, mpu=mpu)\n    for module_name_lists, _, compression_technique in compression_technique_list:\n        for mnl in module_name_lists:\n            for module_name in mnl:\n                module_replacement(model, module_name, compression_technique)\n\n    return model\n\n\ndef fix_compression(model, module_name, compression_technique, mask=None, dim_reduction=False):\n    \"\"\"\n    Fix the compression technique of a module.\n    Args:\n        model (`torch.nn.Module`)\n            The model to fix the compression technique of.\n        module_name (`str`)\n            The name of the module to fix the compression technique of.\n        compression_technique (`str`)\n            The compression technique to fix the module to.\n    \"\"\"\n    # Here we can make things much simpler by just replacing the module\n    module = recursive_getattr(model, module_name)\n    for k, v in compression_technique.items():\n        if k == WEIGHT_QUANTIZATION and v[WEIGHT_QUANTIZE_IN_FORWARD_ENABLED] and v[WEIGHT_QUANTIZE_ENABLED]:\n            return module.fix_weight_quantization()\n        elif k == SPARSE_PRUNING and v[SPARSE_PRUNING_ENABLED]:\n            return module.fix_sparse_pruning_helper()\n        elif k == ROW_PRUNING and (v[ROW_PRUNING_ENABLED] or mask is not None):\n            return module.fix_row_col_pruning_helper(mask, dim_reduction=dim_reduction)\n        elif k == HEAD_PRUNING and (v[HEAD_PRUNING_ENABLED] or mask is not None):\n            return module.fix_head_pruning_helper(mask, v[HEAD_PRUNING_NUM_HEADS], dim_reduction=dim_reduction)\n        elif k == CHANNEL_PRUNING and (v[CHANNEL_PRUNING_ENABLED] or mask is not None):\n            return module.fix_channel_pruning_helper(mask, dim_reduction=dim_reduction)\n\n\ndef convert_conv1d_to_linear(model, convert_type):\n    '''\n    This is a help function to convert conv1d to linear (e.g., convert GPT2 from HF)\n    '''\n    if hasattr(model, 'module'):\n        c_model = model.module\n    else:\n        c_model = model\n\n    for name, module in c_model.named_modules():\n        if isinstance(module, convert_type):\n            old_module = recursive_getattr(c_model, name)\n            new_module = torch.nn.Linear(old_module.weight.data.size(0),\n                                         old_module.weight.data.size(1),\n                                         bias=True if old_module.bias is not None else False)\n            new_module.weight.data = old_module.weight.data.t().contiguous()\n            if new_module.bias is not None:\n                new_module.bias.data = old_module.bias.data.view(-1)\n\n            recursive_setattr(c_model, name, new_module)\n\n    return model\n\n\ndef generate_pruners(config, model):\n    \"\"\"Generate pruners.\n    Args:\n        config (`neural_compressor.WeightPruningConfig`)\n            The object to the class WeightPruningConfig.\n        model (`torch.nn.module`)\n            The torch module object to be pruned.\n    \"\"\"\n    assert nc_pruner is not None, \"please ensure the neural_compressor python package is installed by pip or conda if user wants to use snip_momentum sparse pruning\"\n    from nc_pruner.utils import process_config, parse_to_prune\n    from nc_pruner.pruners import get_pruner\n    assert isinstance(model, torch.nn.Module)\n    pruners_info = process_config(config)\n    pruners = []\n    for info in pruners_info:\n        modules = parse_to_prune(info, model)\n        if modules == {}:\n            logger.warning(\"one pruner hooks no layers, please have a check\")\n\n        pruners.append(get_pruner(info, modules))\n        info['modules'] = [key for key in modules.keys()]\n        info['len_of_modules'] = len(info['modules'])\n        logger.info(info)\n    return pruners\n\n\ndef register_on_step_begin(model):\n    \"\"\"Mount on_step_begin to the model.\n    Args:\n        model (`torch.nn.module`)\n            The torch module object to be pruned.\n    \"\"\"\n\n    def hook(module, input):\n        for pruner in module.pruners:\n            pruner.on_step_begin(0)\n\n    hook_handle = model.register_forward_pre_hook(hook)\n    return hook_handle\n\n\ndef rewrite_optimizer_step(opt: torch.optim.Optimizer):\n    \"\"\"Mount on_before/after_optimizer_step to the optimizer.\n    Args:\n        model (`torch.opt.Optimizer`)\n            The torch optimizer object to be hooked.\n    \"\"\"\n\n    def new_step(self, closure=None):\n        if hasattr(self, \"pruners\"):\n            for pruner in self.pruners:\n                pruner.on_before_optimizer_step()\n\n        if closure is not None:\n            res = self.orig_step(closure)\n        else:\n            res = self.orig_step()\n        if hasattr(self, \"pruners\"):\n            for pruner in self.pruners:\n                pruner.on_after_optimizer_step()\n        return res\n\n    opt.orig_step = opt.step\n    import types\n    opt.step = types.MethodType(new_step, opt)\n    return opt\n", "deepspeed/compression/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .compress import init_compression, redundancy_clean\nfrom .scheduler import compression_scheduler\nfrom .helper import convert_conv1d_to_linear\n", "deepspeed/module_inject/inject.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport copy\nimport torch\nfrom deepspeed.ops.transformer import DeepSpeedTransformerLayer, DeepSpeedTransformerConfig\n\n\ndef module_inject(layer_obj, model, config, micro_batch_size, max_seq_length, seed, preln, fp16=True):\n    for name, child in model.named_children():\n        if isinstance(child, layer_obj):\n            print('REPLACING BertLayer')\n\n            cuda_config = DeepSpeedTransformerConfig(batch_size=micro_batch_size,\n                                                     max_seq_length=max_seq_length,\n                                                     hidden_size=config.hidden_size,\n                                                     heads=config.num_attention_heads,\n                                                     attn_dropout_ratio=config.attention_probs_dropout_prob,\n                                                     hidden_dropout_ratio=config.hidden_dropout_prob,\n                                                     num_hidden_layers=config.num_hidden_layers,\n                                                     initializer_range=config.initializer_range,\n                                                     seed=seed,\n                                                     fp16=fp16,\n                                                     pre_layer_norm=preln)\n\n            new_module = DeepSpeedTransformerLayer(cuda_config)\n\n            # copy relevant state from child -> new module\n            qw = child.attention.self.query.weight\n            qb = child.attention.self.query.bias\n            kw = child.attention.self.key.weight\n            kb = child.attention.self.key.bias\n            vw = child.attention.self.value.weight\n            vb = child.attention.self.value.bias\n\n            qkvw = torch.cat((qw, kw, vw), 0)\n            qkvb = torch.cat((qb, kb, vb), 0)\n\n            new_module.attn_qkvw.data = qkvw\n            new_module.attn_qkvb.data = qkvb\n            new_module.attn_ow.data = child.attention.output.dense.weight\n            new_module.attn_ob.data = child.attention.output.dense.bias\n            if preln:\n                attention_layerNorm = child.PostAttentionLayerNorm\n            else:\n                attention_layerNorm = child.attention.output.LayerNorm\n            new_module.attn_nw.data = attention_layerNorm.weight\n            new_module.attn_nb.data = attention_layerNorm.bias\n            if preln:\n                intermediate_FF = child.intermediate.dense_act\n            else:\n                intermediate_FF = child.intermediate.dense\n            new_module.inter_w.data = intermediate_FF.weight\n            new_module.inter_b.data = intermediate_FF.bias\n            new_module.output_w.data = child.output.dense.weight\n            new_module.output_b.data = child.output.dense.bias\n            if preln:\n                transformer_LayerNorm = child.PreAttentionLayerNorm\n            else:\n                transformer_LayerNorm = child.output.LayerNorm\n            new_module.norm_w.data = transformer_LayerNorm.weight\n            new_module.norm_b.data = transformer_LayerNorm.bias\n\n            setattr(model, name, copy.deepcopy(new_module))\n\n        else:\n            module_inject(layer_obj, child, config, micro_batch_size, max_seq_length, seed, preln, fp16)\n\n    return model\n\n\ndef test_hi():\n    from turing.nvidia_modelingpreln import BertConfig as BertConfigPreLN\n    from turing.nvidia_modelingpreln import BertForQuestionAnswering as BertForQuestionAnsweringPreLN\n    from turing.nvidia_modelingpreln import BertLayer\n    bert_model_config = {\n        \"vocab_size_or_config_json_file\": 119547,\n        \"hidden_size\": 1024,\n        \"num_hidden_layers\": 1,\n        \"num_attention_heads\": 16,\n        \"intermediate_size\": 4096,\n        \"hidden_act\": \"gelu\",\n        \"hidden_dropout_prob\": 0.1,\n        \"attention_probs_dropout_prob\": 0.1,\n        \"hidden_dropout_prob\": 0.1,\n        \"attention_probs_dropout_prob\": 0.1,\n        \"max_position_embeddings\": 512,\n        \"type_vocab_size\": 2,\n        \"initializer_range\": 0.02\n    }\n    bert_config = BertConfigPreLN(**bert_model_config)\n    base_model = BertForQuestionAnsweringPreLN(bert_config, args=None)\n\n    #base_model = LinearStack()\n\n    test_model = copy.deepcopy(base_model)\n    test_model = module_inject(BertLayer, test_model, bert_config, 4, 384, 1234)\n\n    print('BASE', base_model)\n    print('TEST', test_model)\n\n    #base_model.eval()\n    #test_model.eval()\n\n    #test_input = torch.rand(1, base_model.input_dim)\n\n    #base_output = base_model(test_input)\n    #test_output = test_model(test_input)\n    #\n    #assert torch.allclose(base_output, test_output, atol=3e-8)\n", "deepspeed/module_inject/replace_module.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport os\nimport torch\nimport tqdm\nimport deepspeed\nimport deepspeed.ops.transformer as transformer_inference\nfrom deepspeed.ops.transformer.inference.diffusers_attention import DeepSpeedDiffusersAttention\nfrom deepspeed.ops.transformer.inference.diffusers_transformer_block import DeepSpeedDiffusersTransformerBlock\nfrom deepspeed.ops.transformer.inference.diffusers_2d_transformer import Diffusers2DTransformerConfig\nfrom deepspeed.accelerator import get_accelerator\nfrom .replace_policy import replace_policies, generic_policies\nfrom .auto_tp import AutoTP, ReplaceWithTensorSlicing, Loading\nfrom .layers import TensorParallelOcShardConv2d, TensorParallelIcShardConv2d\n\nfrom deepspeed import comm as dist\nfrom deepspeed.module_inject.tp_shard import set_num_kv_heads, set_n_embd, set_num_attention_heads\n\nfrom .load_checkpoint import load_model_with_checkpoint\nimport time\n\nfrom .utils import policy_to_ds_container\nimport gc\n\n\ndef get_transformer_name(replaced_module):\n    from .containers import supported_models\n    from torch.nn import ModuleList\n    transformer_name = ''\n    for n, c in replaced_module.named_children():\n        if c.__class__ in supported_models:\n            transformer_name += n + '.'\n            for name, child in c.named_children():\n                if child.__class__ is ModuleList:\n                    transformer_name += name\n                    break\n            break\n    return transformer_name\n\n\nclass GroupQuantizer:\n\n    def __init__(self, q_int8=True, group_size=1, num_bits=8, num_groups=0):\n        self.group_size = group_size\n        self.num_bits = num_bits\n        self.q_int8 = q_int8\n\n        self.num_groups = num_groups\n\n    def quantize(self, inputs, qkv=True, count=1, parallel_dim=0):\n        if not self.q_int8 or not qkv:\n            inputs = torch.nn.Parameter(inputs, requires_grad=False)\n            inputs.scale = torch.empty(1)\n            return inputs\n        q_range = 2**self.num_bits\n        num_groups = self.num_groups if self.num_groups > 0 else inputs.shape[0] // self.group_size\n        inputs = inputs.to(get_accelerator().current_device_name())\n        input_flat = inputs.reshape(num_groups, -1).contiguous()\n        input_min = torch.min(input_flat, dim=1, keepdim=True)[0].float()\n        input_max = torch.max(input_flat, dim=1, keepdim=True)[0].float()\n        scale = torch.max(input_min.abs(), input_max.abs()) * 2.0 / (q_range)\n        input_flat = (input_flat / scale).round().clamp(-q_range // 2, q_range // 2 - 1)\n        inputs_q = input_flat.reshape(inputs.shape).to(torch.int8).contiguous()\n        out = torch.nn.Parameter(inputs_q, requires_grad=False)\n        inputs_split = inputs.split(inputs.shape[parallel_dim] // 2, dim=parallel_dim)\n        input_flat = [inputs_split[i].reshape(num_groups, -1).contiguous() for i in range(2)]\n        input_min = [torch.min(input_flat[i], dim=1, keepdim=True)[0].float() for i in range(2)]\n        input_max = [torch.max(input_flat[i], dim=1, keepdim=True)[0].float() for i in range(2)]\n        scale1 = [(torch.max(input_min[i].abs(), input_max[i].abs()) * 2.0 / (q_range)).squeeze().unsqueeze(0)\n                  for i in range(2)]\n\n        out.scale = torch.cat([scale.squeeze().unsqueeze(0), scale1[0], scale1[1]], dim=0).reshape(num_groups,\n                                                                                                   -1).contiguous()\n        return out\n\n\ndef _module_match(module):\n    for policy in generic_policies:\n        policy = policy()\n        if policy.match(module):\n            return policy\n    return None\n\n\ndef generic_injection(module, dtype=None, enable_cuda_graph=True):\n\n    def replace_attn(child, policy):\n        policy_attn = policy.attention(child)\n        if policy_attn is None:\n            return child\n        if len(policy_attn) == 5:\n            qkvw, attn_ow, attn_ob, hidden_size, heads = policy_attn\n        else:\n            qw, kw, vw, attn_ow, attn_ob, hidden_size, heads = policy_attn\n\n        config = transformer_inference.DeepSpeedInferenceConfig(\n            hidden_size=hidden_size,\n            heads=heads,\n            dtype=dtype,\n            triangular_masking=False,\n            max_out_tokens=4096,\n        )\n        attn_module = DeepSpeedDiffusersAttention(config)\n\n        def transpose(data):\n            data = data.contiguous()\n            data.reshape(-1).copy_(data.transpose(-1, -2).contiguous().reshape(-1))\n            data = data.reshape(data.shape[-1], data.shape[-2])\n            data.to(get_accelerator().current_device_name())\n            return data\n\n        if len(policy_attn) == 5:\n            attn_module.attn_qkvw.data = transpose(qkvw.data)\n        else:\n            attn_module.attn_qkvw = None\n            attn_module.attn_qw.data = transpose(qw.data)\n            attn_module.attn_kw.data = transpose(kw.data)\n            attn_module.attn_vw.data = transpose(vw.data)\n\n        attn_module.attn_qkvb = None\n        attn_module.attn_ow.data = transpose(attn_ow.data)\n        attn_module.attn_ob.data.copy_(attn_ob.data.to(get_accelerator().current_device_name()))\n        return attn_module\n\n    def replace_attn_block(child, policy):\n        config = Diffusers2DTransformerConfig()\n        return DeepSpeedDiffusersTransformerBlock(child, config)\n\n    if isinstance(module, torch.nn.Module):\n        pass\n    else:\n        if dtype not in [torch.float16, torch.half]:\n            raise ValueError(\"Generic injection only supported with FP16\")\n\n        try:\n            import diffusers\n            if hasattr(diffusers.models.attention, 'CrossAttention'):\n                cross_attention = diffusers.models.attention.CrossAttention\n            else:\n                cross_attention = diffusers.models.attention_processor.Attention\n            attention_block = diffusers.models.attention.BasicTransformerBlock\n            new_policies = {\n                cross_attention: replace_attn,\n                attention_block: replace_attn_block,\n            }\n        except ImportError:\n            new_policies = {}\n\n        #replace_transformer_layer(None,\n        #                          module.text_encoder,\n        #                          training=False,\n        #                          replace_with_kernel_inject=True,\n        #                          triangular_masking=True,\n        #                          max_out_tokens=8192)\n        from ..model_implementations.transformers.clip_encoder import DSClipEncoder\n        cg_encoder = DSClipEncoder(module.text_encoder, enable_cuda_graph=enable_cuda_graph)\n        setattr(module, 'text_encoder', cg_encoder)\n        for name in module.__dict__.keys():\n            sub_module = getattr(module, name)\n            policy = _module_match(sub_module)\n\n            if policy is not None:\n\n                def _replace_module(module, policy):\n                    for name, child in module.named_children():\n                        _replace_module(child, policy)\n                        if child.__class__ in new_policies:\n                            replaced_module = new_policies[child.__class__](child, policy)\n                            setattr(module, name, replaced_module)\n\n                _replace_module(sub_module, policy)\n                new_module = policy.apply(sub_module, enable_cuda_graph=enable_cuda_graph)\n                print(f\"**** found and replaced {name} w. {type(new_module)}\")\n                setattr(module, name, new_module)\n\n\ncontainer_g = None\n\n\ndef replace_transformer_layer(orig_layer_impl, model, checkpoint_dict, config, model_config):\n    \"\"\" Replace bert-style transformer layers with DeepSpeed's transformer layer\n    Arguments:\n        orig_layer_impl (torch.nn.Module): the original transformer layer implementation to look for,\n            e.g., transformers.models.bert.modeling_bert.BertLayer or transformers.BertLayer\n        model (torch.nn.Module): user's nn.module representing their model\n        checkpoint_dict: Dictionary for checkpoint passed from the Inference Engine\n        config: top-level DS Inference config defined in inference/config.py\n        model_config: HuggingFace model config passed from the inference/engine.py\n    Returns:\n        Updated nn.module with replaced transformer layers\n    \"\"\"\n    # defining globals as internally defined functions inherit these everywhere\n    quantize = (config.dtype == torch.int8)\n    # todo: Refactor later. In future, let's minimize the style used above and use config.** instead\n\n    linear_layer_setting = None\n    '''\n        linear_layer_setting (tuple of modules) [Optional]: shows which two classes are used for linear layers and embedding layers\n    '''\n    micro_batch_size = -1\n    seed = -1\n    local_rank = -1\n\n    mp_replace = ReplaceWithTensorSlicing(mp_group=config.tensor_parallel.tp_group,\n                                          mp_size=config.tensor_parallel.tp_size)  #, out_dim=0, in_dim=1)\n\n    def replace_with_policy(child, policy_cls, triangular_masking, inference=False, layer_id=0):\n        policy = policy_cls(child, inference=inference)\n        if not policy.cuda_graph_supported:\n            # policy says cuda graph is not supported raise an error if set\n            assert not config.enable_cuda_graph, \"cuda graph is not supported with this model, please disable\"\n\n        from deepspeed.moe.layer import MoE\n        moe = False\n        if hasattr(child, 'mlp') and isinstance(child.mlp, MoE):\n            num_experts = child.mlp.num_experts\n            moe = True\n\n        # 1. Create a model-specific container object using the policy object.\n        _container = policy_to_ds_container(policy=policy,\n                                            config=config,\n                                            model_config=model_config,\n                                            layer_id=layer_id,\n                                            child=child)\n        _container.set_moe(moe)\n\n        # 2. Set the tensor parallelism config\n        _container.set_tensor_parallel_config(config.tensor_parallel.tp_size, config.tensor_parallel.tp_group)\n\n        # 3. Initialize tensors\n        _container.initialize_tensors()\n\n        # 4. deal with data types -- needs refactor to use dtype instead of fp16\n        if config.dtype in [torch.float16, torch.bfloat16, torch.int8]:\n            _container.convert_to_required_dtype()\n\n        # 5. Set the quantization config\n        quantizer = GroupQuantizer(q_int8=quantize)\n        _container.set_quantization_config(quantizer)\n\n        # 6. create a DS Inference config object\n        _container.create_ds_model_config()\n\n        # 7. use the config and create the module\n        _container.create_module()\n\n        # 8. transpose the weights and bias if needed\n        _container.transpose()\n\n        # 9. deal with tensor parallelism.\n        _container.apply_tensor_parallelism(mp_replace)\n\n        # 10. copy the tensors from the model-specific container to the new module\n        _container.copy_data_to_new_module()\n\n        # 11. set global for generic checkpoint loading\n        global container_g\n\n        if container_g is None:\n            container_g = _container\n\n        return _container.module\n\n    def replace_wo_policy(module, all_reduce_linears, prefix=\"\", state_dict=None):\n        #mp_replace = ReplaceWithTensorSlicing(mp_group=config.tensor_parallel.tp_group)\n\n        # 1. Create AutoTP object\n        _autotp = AutoTP(module, all_reduce_linears, prefix, state_dict, linear_layer_setting, orig_layer_impl)\n\n        # 2. Set the tensor parallelism config\n        _autotp.set_tensor_parallel_config(config.tensor_parallel.tp_size, config.tensor_parallel.tp_group)\n\n        # 3. Try to get num_key_heads from model_config.num_key_value_heads\n        num_kv_heads = _autotp.get_model_num_kv_heads(model_config)\n\n        # 4. When we have num_kv_heads defined, uneven division is possible, otherwise enforce even division\n        set_num_kv_heads(num_kv_heads)\n\n        # 4.1 Get n_embd\n        n_embd = None\n        multi_query_n_embd_names = ['n_embd']\n        for name in multi_query_n_embd_names:\n            if hasattr(model_config, name):\n                n_embd = getattr(model_config, name)\n            if n_embd != None:\n                break\n\n        # 4.2 set n_embd\n        set_n_embd(n_embd)\n\n        # 4.3 set attention_heads\n        if hasattr(model_config, 'num_attention_heads'):\n            set_num_attention_heads(getattr(model_config, 'num_attention_heads'))\n\n        # 5. Set linear policies\n        _autotp.update_linear_policies()\n\n        # 6. Replace modules\n        if \"lm_head\" in all_reduce_linears or \"embed_out\" in all_reduce_linears:\n            return _autotp._replace_last_linear_module(module)\n        return _autotp._replace_module(module)\n\n    def replace_fn(child, _policy, layer_id=0, prefix=\"\", state_dict=None):\n        training = False  # todo: refactor this part to go in the config\n        if training:\n            # copy relevant state from child -> new module\n            new_module = replace_with_policy(child, _policy, config.triangular_masking)\n\n        else:\n            # copy relevant state from child -> new module\n            if config.replace_with_kernel_inject:\n                new_module = replace_with_policy(child,\n                                                 _policy,\n                                                 config.triangular_masking,\n                                                 inference=True,\n                                                 layer_id=layer_id)\n            else:\n                new_module = replace_wo_policy(child, _policy, prefix=prefix, state_dict=state_dict)\n\n        return new_module\n\n    def set_lm_head(module):\n        embedding_weight = None\n        for n, p in module.named_parameters():\n            if \"word_embeddings.\" in n or \"embed_tokens.\" in n or \"wte.\" in n:\n                embedding_weight = p\n        if embedding_weight is not None and hasattr(module, \"lm_head\") and hasattr(\n                module.lm_head, \"weight\") and module.lm_head.weight.is_meta:\n            module.lm_head.weight = embedding_weight\n        # enable tensor parallel for the last linear\n        if hasattr(module, \"lm_head\") and hasattr(module.lm_head,\n                                                  \"weight\") and not module.lm_head.weight.is_meta and isinstance(\n                                                      module.lm_head, torch.nn.Linear):\n            module = replace_wo_policy(module, (\"lm_head\", ), 0, \"lm_head\")\n        elif hasattr(module, \"embed_out\") and hasattr(module.embed_out,\n                                                      \"weight\") and not module.embed_out.weight.is_meta and isinstance(\n                                                          module.embed_out, torch.nn.Linear):\n            module = replace_wo_policy(module, (\"embed_out\", ), 0, \"embed_out\")\n        return module\n\n    def conv2d_parallel_shard_weights(model, rank, world_size):\n        # add conv policy\n        shard_oc_name = [\"conv1\"]\n        shard_ic_name = [\"conv2\"]\n        for name, sub_m in model.named_children():\n            for l_name, l_sub_m in sub_m.named_children():\n                if l_name in shard_oc_name:\n                    TPConv2d = TensorParallelOcShardConv2d(\n                        l_sub_m,\n                        rank,\n                        world_size,\n                    )\n                    setattr(sub_m, l_name, TPConv2d)\n                if l_name in shard_ic_name:\n                    TPConv2d = TensorParallelIcShardConv2d(\n                        l_sub_m,\n                        rank,\n                        world_size,\n                    )\n                    setattr(sub_m, l_name, TPConv2d)\n            conv2d_parallel_shard_weights(sub_m, rank, world_size)\n\n    if checkpoint_dict is not None and not config.replace_with_kernel_inject:\n        # AutoTP shard loading\n        checkpoint = checkpoint_dict[\"checkpoints\"]\n        pbar = tqdm.tqdm(total=len(checkpoint), desc=f\"Loading {len(checkpoint)} checkpoint shards\")\n        for i in range(len(checkpoint)):\n            checkpoint_file = os.path.join(config.base_dir, checkpoint[i])\n            replaced_module = replace_module(model=model,\n                                             orig_class=orig_layer_impl,\n                                             replace_fn=replace_fn,\n                                             _replace_policy=config.injection_policy_tuple,\n                                             checkpoint=checkpoint_file)\n            pbar.update(1)\n            gc.collect()\n        replaced_module = set_lm_head(replaced_module)\n        # conv2d tp module replace\n        # Now is for yuan model. Add model list and conv policy to decide whether to replace conv.\n        if 'Yuan' in str(replaced_module):\n            conv2d_parallel_shard_weights(replaced_module, dist.get_rank(), dist.get_world_size())\n    else:\n        replaced_module = replace_module(model=model,\n                                         orig_class=orig_layer_impl,\n                                         replace_fn=replace_fn,\n                                         _replace_policy=config.injection_policy_tuple)\n\n    quantizer = GroupQuantizer(q_int8=quantize)\n    world_size = dist.get_world_size() if dist.is_initialized() else 1\n    rank = dist.get_rank() if dist.is_initialized() else 0\n    if checkpoint_dict is not None and config.replace_with_kernel_inject:\n        assert container_g.ckpt_load_enabled, \\\n               f\"Meta Tensor checkpoint loading not supported in {container_g.__class__.__name__} container\"\n        start_time = time.time()\n        checkpoint = checkpoint_dict['checkpoints']\n        ckpt_list = checkpoint[\"tp\"] if type(checkpoint) is dict else checkpoint\n        ckpt_type = checkpoint_dict.get('parallelization', 'pp')\n        ckpt_mp_size = checkpoint_dict.get('tp_size', len(ckpt_list))\n        ckpt_mp_size = checkpoint_dict.get('mp_size', ckpt_mp_size)\n        base_dir1 = checkpoint_dict.get('base_dir', config.base_dir)\n\n        if ckpt_type == 'pp' and type(checkpoint) is list:\n            pbar = tqdm.tqdm(total=len(checkpoint), desc=f\"Loading {len(checkpoint)} checkpoint shards\")\n\n            for i in range(len(checkpoint)):\n                sd = [torch.load(os.path.join(base_dir1, checkpoint[i]), map_location='cpu')]\n                load_model_with_checkpoint(replaced_module,\n                                           sd,\n                                           mp_replace,\n                                           ckpt_type,\n                                           ckpt_mp_size,\n                                           quantizer,\n                                           container=container_g)\n                pbar.update(1)\n        else:\n            num_checkpoints = len(ckpt_list) // ckpt_mp_size\n            tp_split_size = (world_size / ckpt_mp_size)\n            sd_offset = int(rank / tp_split_size)\n            sd_count = int((rank + max(1, tp_split_size)) / tp_split_size) - sd_offset\n            pbar = tqdm.tqdm(total=num_checkpoints, desc=f\"Loading {num_checkpoints} checkpoint shards\")\n            for i in range(num_checkpoints):\n                pbar.update(1)\n                ckpt_index = i * ckpt_mp_size + sd_offset\n                ckpt_files = [\n                    os.path.join(base_dir1, ckpt_list[ckpt_index + j]) if base_dir1 else ckpt_list[ckpt_index + j]\n                    for j in range(sd_count)\n                ]\n                sds = [torch.load(ckpt_file, map_location='cpu') for ckpt_file in ckpt_files]\n                load_model_with_checkpoint(replaced_module,\n                                           sds,\n                                           mp_replace,\n                                           ckpt_type,\n                                           ckpt_mp_size,\n                                           quantizer,\n                                           int(rank % tp_split_size),\n                                           container=container_g)\n                sds = [None for _ in sds]\n                gc.collect()\n\n            if \"non_tp\" in checkpoint:\n                pbar = tqdm.tqdm(total=len(checkpoint[\"non_tp\"]),\n                                 desc=f\"Loading {len(checkpoint['non_tp'])} checkpoint shards\")\n\n                for i in range(len(checkpoint[\"non_tp\"])):\n                    pbar.update(1)\n                    ckpt_file = os.path.join(base_dir1,\n                                             checkpoint[\"non_tp\"][i]) if base_dir1 else checkpoint[\"non_tp\"][i]\n                    sds = [torch.load(ckpt_file, map_location='cpu')]\n                    load_model_with_checkpoint(replaced_module,\n                                               sds,\n                                               mp_replace,\n                                               ckpt_type,\n                                               ckpt_mp_size,\n                                               quantizer,\n                                               int(rank % tp_split_size),\n                                               container=container_g)\n                    sds = [None for _ in sds]\n                    gc.collect()\n        set_lm_head(replaced_module)\n        print(f\"checkpoint loading time at rank {rank}: {time.time()-start_time} sec\")\n\n    if config.save_mp_checkpoint_path is not None:\n        from collections import OrderedDict\n        import json\n        num_partitions = 8\n\n        if checkpoint_dict is None:\n            ckpt_name = \"ds_model\"\n            try:\n                from transformers.models.bloom.modeling_bloom import BloomForCausalLM\n                if isinstance(model, BloomForCausalLM):\n                    ckpt_name = \"bloom\"\n            except ImportError:\n                ckpt_name = \"ds_model\"\n        else:\n            ckpt_name = checkpoint_dict['type']\n        if dist.is_initialized():\n            dist.barrier()\n        transformer_name = get_transformer_name(replaced_module)\n        non_tp_ckpt_name = f'non-tp.pt'\n        ckpt_files = [non_tp_ckpt_name]\n        os.makedirs(config.save_mp_checkpoint_path, exist_ok=True)\n\n        if not dist.is_initialized() or dist.get_rank() == 0:\n            print(\"Saving tp-sharded checkpoints\")\n            torch.save(\n                OrderedDict({k: v\n                             for k, v in dict(replaced_module.state_dict()).items()\n                             if transformer_name not in k}), f'{config.save_mp_checkpoint_path}/{non_tp_ckpt_name}')\n\n            dtype_reprs = {\n                torch.float32: 'float32',\n                torch.float16: 'float16',\n                torch.int8: 'int8',\n                torch.bfloat16: 'bfloat16'\n            }\n\n            ckpt_config = json.dumps({\n                'type': ckpt_name,\n                'base_dir': f'{config.save_mp_checkpoint_path}',\n                'checkpoints': {\n                    \"non_tp\": ckpt_files,\n                    \"tp\": [f'tp_{r:0>2d}_{m:0>2d}.pt' for m in range(num_partitions) for r in range(world_size)]\n                },\n                'version': 1.0,\n                'parallelization': 'tp',\n                'tp_size': world_size,\n                'dtype': dtype_reprs[config.dtype]\n            })\n            with open(f\"{config.save_mp_checkpoint_path}/ds_inference_config.json\", \"w\") as cfg:\n                cfg.write(ckpt_config)\n\n        rep_sd = replaced_module.state_dict()\n        for n, p in replaced_module.named_parameters():\n            if hasattr(p, 'scale'):\n                rep_sd[n] = [p, p.scale]\n        keys = list(rep_sd.keys())\n        partition_size = (len(keys) // num_partitions + 1)\n        for m in range(num_partitions):\n            torch.save(\n                OrderedDict({\n                    k: [rep_sd[k], rep_sd[k].scale] if hasattr(rep_sd[k], 'scale') else rep_sd[k]\n                    for k in keys[m * partition_size:(m + 1) * partition_size] if transformer_name in k\n                }), f'{config.save_mp_checkpoint_path}/tp_{rank:0>2d}_{m:0>2d}.pt')\n\n    return replaced_module\n\n\ndef revert_transformer_layer(orig_layer_impl, model, config, preln=False):\n    \"\"\" Revert DeepSpeed's transformer layer back to original bert-style transformer layer\n    Arguments:\n        orig_layer_impl (torch.nn.Module): the original transformer layer implementation that was replaced,\n            e.g., transformers.models.bert.modeling_bert.BertLayer or transformers.BertLayer\n        model (torch.nn.Module): user's nn.module representing their model\n        config (dict): model config containing hidden size, attention heads, etc.\n    Returns:\n        Updated nn.module with original bert-style transformer layers\n    \"\"\"\n\n    def replace_fn(child, _replace_policy, layer_id):\n        #from turing.nvidia_modelingpreln import BertLayer\n        orig_module = orig_layer_impl(config)\n\n        # copy relevant state from child -> original module\n        qkvw = child.attn_qkvw.data\n        qkvb = child.attn_qkvb.data\n\n        qw, kw, vw = torch.chunk(qkvw, 3, axis=0)\n        qb, kb, vb = torch.chunk(qkvb, 3, axis=0)\n\n        orig_module.attention.self.query.weight.data = qw\n        orig_module.attention.self.query.bias.data = qb\n        orig_module.attention.self.key.weight.data = kw\n        orig_module.attention.self.key.bias.data = kb\n        orig_module.attention.self.value.weight.data = vw\n        orig_module.attention.self.value.bias.data = vb\n\n        orig_module.attention.output.dense.weight.data = child.attn_ow.data\n        orig_module.attention.output.dense.bias.data = child.attn_ob.data\n\n        attn_ln_w = child.attn_nw.data\n        attn_ln_b = child.attn_nb.data\n        if preln:\n            orig_module.PostAttentionLayerNorm.weight.data = attn_ln_w\n            orig_module.PostAttentionLayerNorm.bias.data = attn_ln_b\n        else:\n            orig_module.attention.output.LayerNorm.weight.data = attn_ln_w\n            orig_module.attention.output.LayerNorm.bias.data = attn_ln_b\n\n        inter_ff_w = child.inter_w.data\n        inter_ff_b = child.inter_b.data\n        if preln:\n            orig_module.intermediate.dense_act.weight.data = inter_ff_w\n            orig_module.intermediate.dense_act.bias.data = inter_ff_b\n        else:\n            orig_module.intermediate.dense.weight.data = inter_ff_w\n            orig_module.intermediate.dense.bias.data = inter_ff_b\n\n        orig_module.output.dense.weight.data = child.output_w.data\n        orig_module.output.dense.bias.data = child.output_b.data\n\n        transformer_ln_w = child.norm_w.data\n        transformer_ln_b = child.norm_b.data\n        if preln:\n            orig_module.PreAttentionLayerNorm.weight.data = transformer_ln_w\n            orig_module.PreAttentionLayerNorm.bias.data = transformer_ln_b\n        else:\n            orig_module.output.LayerNorm.weight.data = transformer_ln_w\n            orig_module.output.LayerNorm.bias.data = transformer_ln_b\n        return orig_module\n\n    return replace_module(model=model,\n                          orig_class=deepspeed.DeepSpeedTransformerLayer,\n                          replace_fn=replace_fn,\n                          _replace_policy=None)\n\n\ndef replace_module(model, orig_class, replace_fn, _replace_policy, checkpoint=None):\n    \"\"\" Scan the model for instances of ``orig_clas:`` to replace using ``replace_fn``.\n    Arguments:\n        model (torch.nn.Module): the model to augment\n        orig_class (torch.nn.Module): the module to search for\n        replace_fn (method): a method to convert instances of ``orig_class`` to the\n                             desired type and return a new instance.\n    Returns:\n        A modified ``model``.\n    \"\"\"\n    sd = None\n    if checkpoint is not None:\n        if checkpoint.endswith(\".safetensors\"):\n            from safetensors.torch import load_file\n            sd = load_file(checkpoint)\n        else:\n            sd = torch.load(checkpoint, map_location='cpu')\n\n    policy = {}\n    if orig_class is not None:\n        policy.update({orig_class: (replace_fn, _replace_policy)})\n    else:\n        for plcy in replace_policies:\n            # instantiate a throw-away policy in order to populate the _orig_layer_class\n            _ = plcy(None)\n            if isinstance(plcy._orig_layer_class, list):\n                for orig_layer_class in plcy._orig_layer_class:\n                    policy.update({orig_layer_class: (replace_fn, plcy)})\n            elif plcy._orig_layer_class is not None:\n                policy.update({plcy._orig_layer_class: (replace_fn, plcy)})\n    assert len(policy.items()) > 0,\\\n        \"No default policy found! Please specify your policy injection_policy (like {BertLayer:HFBEertLayerPolicy}).\" +\\\n        \"You can find some samples here: https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/module_inject/replace_policy.py\"\n\n    replaced_module, _ = _replace_module(model, policy, state_dict=sd)\n    return replaced_module\n\n\nfrom ..pipe import PipelineModule\n\nimport re\n\n\ndef skip_level_0_prefix(model, state_dict):\n    model = str(model)\n    key = re.search(r\": (.*?)Model\", model)\n    if key is None:\n        key = re.search(r\": (.*?)Stack\", model)\n    if key is None:\n        key = re.match(r\"(.*?)Model\", model)\n    # if keys start with 'model.', don't skip level 0 prefix\n    if state_dict is not None:\n        for item in state_dict.keys():\n            if re.match(\"^model[.]\", item):\n                return False\n    if key is not None and key.group(1).lower() in [\"bloom\", \"opt\"]:\n        return True\n    return False\n\n\ndef _replace_module(model, policies, prefix='', layer_id=0, level_id=0, state_dict=None):\n    \"\"\" Traverse model's children recursively and apply any transformations in ``policies``.\n    Arguments:\n        model (torch.nn.Module): model to augment\n        policies (dict): Mapping of source class to replacement function.\n    Returns:\n        Modified ``model``.\n    \"\"\"\n    for name, child in model.named_children():\n        if child.__class__ in policies:\n            replaced_module = policies[child.__class__][0](child,\n                                                           policies[child.__class__][-1],\n                                                           layer_id,\n                                                           prefix=prefix + name,\n                                                           state_dict=state_dict)\n            setattr(model, name, replaced_module)\n            if isinstance(model, PipelineModule):\n                assert hasattr(model, 'forward_funcs'),\\\n                    \"we require pipe-module to have the list of fwd_functions\"\n                model.forward_funcs[model.fwd_map[name]] = replaced_module\n            layer_id += 1\n        else:\n            checking_key = prefix + name + '.'\n            if Loading.is_load_module(child) and state_dict is not None:\n                if any(checking_key in item for item in state_dict):\n                    Loading.load(\n                        child,\n                        state_dict,\n                        checking_key,\n                    )\n                else:\n                    continue\n            if len(child._buffers) != 0 and state_dict is not None:\n                Loading.load_buffer(child, state_dict, checking_key)\n            _, layer_id = _replace_module(child,\n                                          policies,\n                                          prefix if level_id == 0 and skip_level_0_prefix(model, state_dict) else \\\n                                          prefix + name + '.',\n                                          layer_id=layer_id,\n                                          level_id=level_id + 1,\n                                          state_dict=state_dict)\n\n    # Add the reset_cache func to the model, so that it can be called in the beginning of text-generation.\n    model.reset_cache = transformer_inference.DeepSpeedTransformerInference.reset_cache\n    return model, layer_id\n", "deepspeed/module_inject/tp_shard.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom deepspeed import comm as dist\nglobal num_kv_heads\n\n\ndef set_num_kv_heads(num):\n    global num_kv_heads\n    num_kv_heads = num\n\n\ndef set_num_attention_heads(num):\n    global num_attention_heads\n    num_attention_heads = num\n\n\ndef set_n_embd(num):\n    global n_embd\n    n_embd = num\n\n\ndef get_num_kv_heads():\n    global num_kv_heads\n    return num_kv_heads\n\n\ndef get_num_attention_heads():\n    global num_attention_heads\n    return num_attention_heads\n\n\ndef get_shard_size(total_size, mp_size, name=None, rank=None):\n    global num_kv_heads\n    last_linear = [\"lm_head\", \"embed_out\"]\n    # When we have num_kv_heads defined, uneven division is possible, otherwise enforce near even division\n    if rank == None:\n        rank = dist.get_rank()\n    if num_kv_heads != None and total_size % num_kv_heads == 0 and \"mlp\" not in str(name) and str(\n            name) not in last_linear:\n        my_slices = (num_kv_heads // mp_size) + (1 if rank < (num_kv_heads % mp_size) else 0)\n        return total_size * my_slices // num_kv_heads\n    else:\n        if total_size >= 64:\n            grain_size = total_size // 64\n            return (grain_size // mp_size + (1 if rank < (grain_size % mp_size) else 0)) * 64\n        else:\n            return total_size // mp_size + (1 if rank < (total_size % mp_size) else 0)\n\n\ndef get_n_embd():\n    global n_embd\n    return n_embd\n\n\ndef get_shard_size_list(total_size, mp_size, name=None):\n    shard_sizes = []\n    for i in range(mp_size):\n        shard_sizes.append(get_shard_size(total_size, mp_size, name, i))\n    return shard_sizes\n", "deepspeed/module_inject/utils.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom deepspeed.utils import log_dist\n\n\n# helper function to map between DS policies and DS containers\ndef policy_to_ds_container(**kwargs):\n    from .containers import HFGPT2LayerPolicy, DS_GPT2Container\n    from .containers import HFBertLayerPolicy, DS_BERTContainer\n    from .containers import BLOOMLayerPolicy, DS_BloomContainer\n    from .containers import HFGPTJLayerPolicy, DS_GPTJContainer\n    from .containers import HFGPTNEOLayerPolicy, DS_GPTNEOContainer\n    from .containers import GPTNEOXLayerPolicy, DS_GPTNEOXContainer\n    from .containers import HFOPTLayerPolicy, DS_OPTContainer\n    from .containers import MegatronLayerPolicy, DS_MegatronGPTContainer\n    from .containers import HFDistilBertLayerPolicy, DS_DistilBERTContainer\n    from .containers import LLAMALayerPolicy, DS_LLAMAContainer\n    from .containers import LLAMA2LayerPolicy, DS_LLAMA2Container\n    from .containers import InternLMLayerPolicy, DS_InternLMContainer\n\n    policy_to_container = {\n        HFGPT2LayerPolicy: DS_GPT2Container,\n        HFBertLayerPolicy: DS_BERTContainer,\n        BLOOMLayerPolicy: DS_BloomContainer,\n        HFGPTJLayerPolicy: DS_GPTJContainer,\n        HFGPTNEOLayerPolicy: DS_GPTNEOContainer,\n        GPTNEOXLayerPolicy: DS_GPTNEOXContainer,\n        HFOPTLayerPolicy: DS_OPTContainer,\n        MegatronLayerPolicy: DS_MegatronGPTContainer,\n        HFDistilBertLayerPolicy: DS_DistilBERTContainer,\n        LLAMALayerPolicy: DS_LLAMAContainer,\n        LLAMA2LayerPolicy: DS_LLAMA2Container,\n        InternLMLayerPolicy: DS_InternLMContainer\n    }\n\n    container = None\n    policy = kwargs['policy']\n    assert policy is not None, \"Policy cannot be None\"\n    policy_type = type(policy)\n\n    if policy_type not in policy_to_container:\n        log_dist(f\"Policy type {policy_type} not supported\", [0])\n    else:\n        container = policy_to_container[policy_type](**kwargs)\n\n    return container\n", "deepspeed/module_inject/auto_tp.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\n# Automatic Tensor Parallelism\nimport re\n\nfrom torch import nn\nfrom .replace_policy import replace_policies\nfrom typing import Optional\nimport torch\nfrom deepspeed import comm as dist\nfrom .layers import LinearAllreduce, LinearLayer, LmHeadLinearAllreduce\nfrom deepspeed.accelerator import get_accelerator\nfrom .fusedqkv_utils import require_tp_fused_qkvw, prepare_tp_fused_qkvw, shard_value_with_share_qk, shard_chunk_mlp\nfrom deepspeed.module_inject.tp_shard import get_shard_size, get_shard_size_list\n\n\ndef move(tensor, device):\n    if tensor.is_meta:\n        return torch.empty_like(tensor, device=device)\n    else:\n        # Using new tensors help in freeing memory (after split for example) was done before by calling clone().\n        # Using copy=True instead of clone() will help in case of cpu --> cpu.\n        # Otherwise to() will not create a new copy for the view of the full tensor, and it will not be de-referenced.\n        return tensor.to(device, copy=True)\n\n\nclass ReplaceWithTensorSlicing:\n\n    def __init__(self, mp_group=None, mp_size=1, out_dim=1, in_dim=0):\n        if mp_group is not None:\n            self.gpu_index = dist.get_rank(group=mp_group)\n        else:\n            self.gpu_index = 0\n        self.out_dim = out_dim\n        self.in_dim = in_dim\n        self.mp_size = mp_size\n\n    def merge_assert(self, dim1, dim2):\n        assert dim1 > dim2, \\\n            'Merging tensors is not allowed here! Please use deepspeed load_checkpoint\\\n            for merging your checkpoints before replacing the transformer layer with\\\n            inference-kernels'\n\n    def strided_copy(self,\n                     dst: Optional[torch.Tensor],\n                     src: Optional[torch.Tensor],\n                     num_splits: int,\n                     int8: bool = False,\n                     allocate_tensor: bool = False):\n        if src is None:\n            return src\n        src_shape = src.shape\n        dst_shape = dst.shape\n\n        outer_dim = 0 if int8 else -1\n\n        if allocate_tensor:\n            dst = torch.empty_like(dst)\n\n        src_split = torch.split(src.data, src.shape[outer_dim] // num_splits, dim=outer_dim)\n        if (len(src_shape) == 2 and len(dst_shape) == 2):\n            if src_shape[outer_dim] == dst_shape[self.out_dim]:\n                try:\n                    dst = dst.reshape(-1).data.copy_(src.data.reshape(-1)).reshape(src.shape)\n                except:\n                    print(dst.shape, src.shape)\n                    exit()\n                dst = torch.nn.parameter.Parameter(dst, requires_grad=False)\n                if hasattr(src, 'scale'):\n                    dst.scale = src.scale\n                return dst\n            self.merge_assert(src_shape[outer_dim], dst_shape[self.out_dim])\n            qkv_size = dst_shape[self.out_dim] // num_splits\n            qkv_split = [torch.split(src_s, qkv_size, dim=outer_dim) for src_s in src_split]\n            weight_split = [\n                torch.cat([qkv_s[i] for qkv_s in qkv_split], axis=outer_dim) for i in range(len(qkv_split[0]))\n            ]\n            dst = dst.reshape(-1).data.copy_(weight_split[self.gpu_index].contiguous().reshape(-1)).reshape(\n                weight_split[self.gpu_index].shape)\n        else:\n            if src_shape[0] == dst_shape[0]:\n                return torch.nn.parameter.Parameter(src)\n            qkv_size = dst_shape[0] // num_splits\n            qkv_split = [torch.split(src_s, qkv_size, dim=0) for src_s in src_split]\n            bias_split = [torch.cat([qkv_s[i] for qkv_s in qkv_split], axis=0) for i in range(len(qkv_split[0]))]\n            dst.data.copy_(bias_split[self.gpu_index].contiguous())\n\n        dst = torch.nn.parameter.Parameter(dst, requires_grad=False)\n        if hasattr(src, 'scale'):\n            dst.scale = src.scale\n        return dst\n\n    def copy(self, dst, src, int8=False, allocate_tensor=False):\n        if src is None:\n            return src\n        assert not dst.data.is_meta  # the torch.Tensor.copy_ method used below will silently fail on meta tensors\n        if allocate_tensor:\n            dst = torch.empty_like(dst)\n        outer_dim = 0 if int8 else 1\n        inner_dim = 1 if int8 else 0\n        src_shape = src.shape\n        dst_shape = dst.shape\n        if (len(src_shape) == 2 and len(dst_shape) == 2):\n\n            if src_shape[inner_dim] == dst_shape[self.in_dim] and src_shape[outer_dim] == dst_shape[self.out_dim]:\n                dst = dst.reshape(-1).data.copy_(src.data.reshape(-1)).reshape(src.shape)\n            else:\n                if src_shape[inner_dim] != dst_shape[self.in_dim]:\n                    self.merge_assert(src_shape[inner_dim], dst_shape[self.in_dim])\n                    dst.data.copy_(src[:, self.gpu_index * dst_shape[self.in_dim]: (self.gpu_index + 1) * dst_shape[self.in_dim]] if inner_dim == 1 else \\\n                                   src[self.gpu_index * dst_shape[self.in_dim]: (self.gpu_index + 1) * dst_shape[self.in_dim], :])\n                else:\n                    self.merge_assert(src_shape[outer_dim], dst_shape[self.out_dim])\n                    dst.data.copy_(src[:, self.gpu_index * dst_shape[self.out_dim]: (self.gpu_index + 1) * dst_shape[self.out_dim]] if outer_dim == 1 else \\\n                                   src[self.gpu_index * dst_shape[self.out_dim]: (self.gpu_index + 1) * dst_shape[self.out_dim], :])\n        else:\n            if src_shape[0] == dst_shape[0]:\n                dst = src if src.dtype == dst.dtype else dst.data.copy_(src)\n            else:\n                dst.data.copy_(src[self.gpu_index * dst_shape[-1]:(self.gpu_index + 1) * dst_shape[-1]])\n        dst = torch.nn.parameter.Parameter(dst, requires_grad=False)\n        if hasattr(src, 'scale'):\n            dst.scale = src.scale\n        return dst\n\n\nclass Loading():\n\n    def is_load_module(module):\n        load_layers = [nn.Linear, nn.Embedding, nn.LayerNorm]\n        load_layer_names = [\n            \"LPLayerNorm\", \"SharedEmbedding\", \"OPTLearnedPositionalEmbedding\", \"LlamaRMSNorm\", \"FalconLinear\",\n            \"MistralRMSNorm\", \"T5LayerNorm\", \"MixtralRMSNorm\", \"Phi3RotaryEmbedding\", \"Phi3SuScaledRotaryEmbedding\",\n            \"Phi3RMSNorm\", \"YuanRMSNorm\", \"YuanRotaryEmbedding\"\n        ]\n        return module.__class__ in load_layers or module._get_name() in load_layer_names\n\n    def load_buffer(module, state_dict, prefix):\n        for name in module._buffers.keys():\n            if module._buffers[name].data.is_meta:\n                module._buffers[name] = torch.nn.parameter.Parameter(\n                    data=torch.empty_like(module._buffers[name].data, device=\"cpu\"),\n                    requires_grad=module._buffers[name].data.requires_grad)\n            if prefix + name in state_dict.keys():\n                module._buffers[name].data.copy_(state_dict[prefix + name])\n\n    def load(module, state_dict, prefix, mp_group=None):\n        mp_replace = ReplaceWithTensorSlicing(mp_group=mp_group)\n        if hasattr(module, 'weight'):\n            if module.weight.data.is_meta:\n                # meta tensor cannot be casted or copied to, so we need to replace it with a normal tensor here\n                module.weight = torch.nn.parameter.Parameter(data=torch.empty_like(module.weight.data, device=\"cpu\"),\n                                                             requires_grad=module.weight.data.requires_grad)\n                if 'query_key_value' in prefix:\n                    module.weight = mp_replace.strided_copy(module.weight.data,\n                                                            state_dict[prefix + 'weight'],\n                                                            num_splits=3)\n                else:\n                    module.weight = mp_replace.copy(module.weight.data, state_dict[prefix + 'weight'])\n        else:\n            if hasattr(module, 'norm') and hasattr(module.norm, 'weight'):\n                if module.norm.weight.data.is_meta:\n                    # meta tensor cannot be casted or copied to, so we need to replace it with a normal tensor here\n                    module.norm.weight = torch.nn.parameter.Parameter(\n                        data=torch.empty_like(module.norm.weight.data, device=\"cpu\"),\n                        requires_grad=module.norm.weight.data.requires_grad)\n                module.norm.weight = mp_replace.copy(module.norm.weight.data, state_dict[prefix + 'weight'])\n\n        if prefix + 'bias' in state_dict.keys():\n            if hasattr(module, 'bias'):\n                if module.bias.data.is_meta:\n                    # meta tensor cannot be casted or copied to, so we need to replace it with a normal tensor here\n                    module.bias = torch.nn.parameter.Parameter(data=torch.empty_like(module.bias.data, device=\"cpu\"),\n                                                               requires_grad=module.bias.data.requires_grad)\n                module.bias = mp_replace.copy(module.bias, state_dict[prefix + 'bias'])\n            else:\n                if hasattr(module, 'norm') and hasattr(module.norm, 'bias'):\n                    if module.norm.bias.data.is_meta:\n                        # meta tensor cannot be casted or copied to, so we need to replace it with a normal tensor here\n                        module.norm.bias = torch.nn.parameter.Parameter(\n                            data=torch.empty_like(module.norm.bias.data, device=\"cpu\"),\n                            requires_grad=module.norm.bias.data.requires_grad)\n                    module.norm.bias = mp_replace.copy(module.norm.bias, state_dict[prefix + 'bias'])\n\n\nclass AutoTP():\n\n    def __init__(self, module, all_reduce_linears, prefix, state_dict, linear_layer_setting, orig_layer_impl):\n        self.module = module\n        self.all_reduce_linears = all_reduce_linears\n        self.prefix = prefix\n        self.state_dict = state_dict\n\n        self.mp_size = None\n        self.mp_group = None\n        self.linear_layer_setting = linear_layer_setting\n        self.orig_layer_impl = orig_layer_impl\n        self.linear_policies = None\n        self.conv_linear_layer = False\n\n    def in_module_list(module, module_list):\n        for item in module_list:\n            if type(item).__name__ == type(module).__name__:\n                return True\n        return False\n\n    def get_module_list(model):\n        mlist = []\n        for child in model.children():\n            if isinstance(child, nn.ModuleList):\n                for module in child.children():\n                    if not mlist:\n                        mlist = [module]\n                    elif not AutoTP.in_module_list(module, mlist):\n                        mlist = mlist + [module]\n            else:\n                mlist = mlist + AutoTP.get_module_list(child)\n        return mlist\n\n    def supported(model):\n        unsupported = ['deberta', 'flaubert', 'fsmt', 'gpt2', 'led', 'longformer', 'xlm', 'xlnet']\n        model = str(model)\n        key = re.search(r\": (.*?)Model\", model)\n        if key is None:\n            key = re.search(r\": (.*?)Stack\", model)\n        if key is None:\n            key = re.match(r\"(.*?)Model\", model)\n        assert key is not None, \"Not able to determine model policy automatically. Please provide policy.\"\n        if key.group(1).lower() in unsupported:\n            return False\n        return True\n\n    def get_layers(parent, module):\n        layer_list = []\n        for key, submodule in module._modules.items():\n            if isinstance(submodule, nn.Linear):\n                layer_list = layer_list + [parent + \".\" + key]\n            elif isinstance(submodule, nn.LayerNorm) or key == 'LayerNorm' or key == 'layer_norm':\n                layer_list = layer_list + [\"ln\"]\n            else:\n                layer_list = layer_list + AutoTP.get_layers(key, submodule)\n        return layer_list\n\n    def update_policy_list(policy_list, new_module, new_gems):\n        if len(policy_list):\n            for i, policy in enumerate(policy_list):\n                # if module already exists in policy, combine gems and remove duplicates\n                if policy[0] == type(new_module):\n                    new_gems = set(new_gems + policy[1])\n                    policy_list[i] = tuple([type(new_module), new_gems])\n                    return policy_list\n        policy_list.append(tuple([type(new_module), new_gems]))\n        return policy_list\n\n    def kernel_supported(module_list):\n        policy = []\n        for plcy in replace_policies:\n            # instantiate a throw-away policy in order to populate the _orig_layer_class\n            _ = plcy(None)\n            if isinstance(plcy._orig_layer_class, list):\n                for orig_layer_class in plcy._orig_layer_class:\n                    policy.append(orig_layer_class)\n            elif plcy._orig_layer_class is not None:\n                policy.append(plcy._orig_layer_class)\n        for child in module_list:\n            if child.__class__ in policy:\n                return True\n        return False\n\n    def tp_parser(model):\n        policy_list = []\n        module_list = []\n        layer_list = []\n        gem_list = []\n\n        module_list = AutoTP.get_module_list(model)\n        assert AutoTP.supported(model), \"AutoTP not supported for model. Please use kernel injection since container policy for model exists.\" \\\n        if AutoTP.kernel_supported(module_list) else \"AutoTP not supported for model. Please provide policy.\"\n        norm_layer_name_list = ['LayerNorm', 'layer_norm', 'ln_1', 'ln_2']\n        #ln_1 , ln_2 for Qwen\n        for module in module_list:\n            for key, submodule in module._modules.items():\n                if isinstance(submodule, nn.Linear):\n                    layer_list = layer_list + [\".\" + key]\n                elif isinstance(submodule, nn.LayerNorm) or key in norm_layer_name_list:\n                    layer_list = layer_list + [\"ln\"]\n                else:\n                    layer_list = layer_list + AutoTP.get_layers(key, submodule)\n            for i, layer in enumerate(layer_list):\n                if layer == 'ln':\n                    if layer_list[i - 1] != 'ln':\n                        gem_list = gem_list + [layer_list[i - 1]]\n                elif 'out_proj' in layer:\n                    gem_list = gem_list + [layer]\n                elif 'o_proj' in layer:\n                    gem_list = gem_list + [layer]\n                elif 'down_proj' in layer:\n                    gem_list = gem_list + [layer]\n                elif 'attention.dense' in layer and 'GPTNeoX' in str(model):\n                    gem_list = gem_list + [layer]\n                elif 'self_attention.dense' in layer and 'falcon' in str(\n                        type(module)):  # this is a hack to get the right linear layer for this model!\n                    gem_list = gem_list + [layer]\n                # Mixtral-7x8b used w2*act(w1*w3) linear. need to replace w2 to linearallreduce.\n                elif 'w2' in layer and 'Mixtral' in str(type(module)):\n                    gem_list = gem_list + [layer]\n                elif 'self_attn.dense' in layer and 'Phi' in str(type(module)):\n                    gem_list = gem_list + [layer]\n\n            layer_list = []\n            if gem_list != []:\n                gem_list = list(set(gem_list))\n                policy_list = AutoTP.update_policy_list(policy_list, module, gem_list)\n                gem_list = []\n        assert len(policy_list), \"AutoTP not supported for model. Please use kernel injection since container policy for model exists.\" \\\n        if AutoTP.kernel_supported(module_list) else \"Not able to determine model policy automatically. Please provide policy.\"\n        return policy_list\n\n    def set_tensor_parallel_config(self, mp_size, mp_group):\n        self.mp_size = mp_size\n        self.mp_group = mp_group\n\n    def _replace(self, child, name, conv_linear_layer):\n        if getattr(child, \"replaced\", False) == True:\n            return\n        weight_shape = child.weight.shape\n        mp_replace = ReplaceWithTensorSlicing(mp_group=self.mp_group)\n        # For mixtral-7x8b, need to skip MoE gate linear replace.\n        if name == \"block_sparse_moe.gate\":\n            return child\n        # For Yuan model\n        if 'Yuan' in str(self.module):\n            if 'v_proj' in name:\n                weight, bias = shard_value_with_share_qk(child.weight.data, child.bias, dist.get_rank(),\n                                                         dist.get_world_size(), True)\n                return LinearLayer(weight=weight, bias=bias)\n            elif 'o_proj' in name:\n                weight, bias = shard_value_with_share_qk(child.weight.data, child.bias, dist.get_rank(),\n                                                         dist.get_world_size(), False)\n                return LinearAllreduce(weight, bias, self.mp_group)\n        # for phi3.\n        if 'gate_up_proj' in name:\n            weight, bias = shard_chunk_mlp(child.weight.data, child.bias, dist.get_rank(), dist.get_world_size())\n            return LinearLayer(weight=weight, bias=bias)\n        if name in self.all_reduce_linears:\n            # if conv_linear_layer [weight_shape[1], weight_shape[0] // mp_size]\n            # else [weight_shape[0], weight_shape[1] // mp_size]\n\n            if self.conv_linear_layer:\n                child.weight.data = child.weight.data.transpose(-1, -2).contiguous()\n            data = child.weight.data.split(get_shard_size_list(\n                weight_shape[0] if self.conv_linear_layer else weight_shape[1], self.mp_size, name),\n                                           dim=1)\n            data_dc = move(data[mp_replace.gpu_index], get_accelerator().current_device_name()).detach()\n            del data\n\n            setattr(child, \"replaced\", True)\n            if name == \"lm_head\" or name == 'embed_out':\n                return LmHeadLinearAllreduce(\n                    torch.nn.parameter.Parameter(data_dc, requires_grad=False), dist.get_rank(), dist.get_world_size(),\n                    child.bias if child.bias is None else torch.nn.parameter.Parameter(\n                        move(child.bias,\n                             get_accelerator().current_device_name())), self.mp_group)\n            return LinearAllreduce(torch.nn.parameter.Parameter(data_dc, requires_grad=False), child.bias if child.bias is None else \\\n                        torch.nn.parameter.Parameter(move(child.bias, get_accelerator().current_device_name())), self.mp_group)\n        else:\n\n            # if conv_linear_layer [weight_shape[1], weight_shape[0] // mp_size]\n            # else [weight_shape[0] // mp_size, weight_shape[1]]\n            if self.conv_linear_layer:\n                child.weight.data = child.weight.data.transpose(-1, -2).contiguous()\n\n            if require_tp_fused_qkvw(name, self.mp_size):\n                #Check and handle fused qkv for TP\n                #The copy is a regular copy, The shape of dst and src is the same\n                data_dc = move(\n                    prepare_tp_fused_qkvw(self.module, child.weight.data, self.mp_size, mp_replace.gpu_index),\n                    get_accelerator().current_device_name())\n\n                bias_data_dc = None if child.bias is None else move(\n                    prepare_tp_fused_qkvw(self.module, child.bias.data, self.mp_size, mp_replace.gpu_index),\n                    get_accelerator().current_device_name())\n            else:\n                data = child.weight.data.split(get_shard_size_list(weight_shape[0], self.mp_size, name),\n                                               dim=1 if self.conv_linear_layer else 0)\n                data_dc = move(data[mp_replace.gpu_index], get_accelerator().current_device_name()).detach()\n                del data\n\n                if child.bias is not None:\n                    bias_data = child.bias.data.split(get_shard_size_list(\n                        weight_shape[1] if self.conv_linear_layer else weight_shape[0], self.mp_size, name),\n                                                      dim=0)\n                    bias_data = move(bias_data[mp_replace.gpu_index], get_accelerator().current_device_name())\n                    bias_data_dc = torch.nn.parameter.Parameter(bias_data, requires_grad=False)\n                    del bias_data\n                else:\n                    bias_data_dc = None\n\n            setattr(child, \"replaced\", True)\n            return LinearLayer(weight=torch.nn.parameter.Parameter(data_dc, requires_grad=False), bias=bias_data_dc)\n\n    def _slice_embedding(self, child, name, conv_linear_layer):\n        if getattr(child, \"replaced\", False) == True:\n            return\n        mp_replace = ReplaceWithTensorSlicing(mp_group=self.mp_group)\n\n        if hasattr(child.weight, 'ds_tensor'):\n            data = child.weight.ds_tensor.data.split(get_shard_size_list(child.weight.shape[1], self.mp_size), dim=1)\n        else:\n            data = child.weight.data.split(get_shard_size_list(child.weight.shape[1], self.mp_size, name), dim=1)\n        data = data[mp_replace.gpu_index].to(get_accelerator().current_device_name())\n        data = torch.nn.parameter.Parameter(data, requires_grad=False)\n\n        new_embedding = nn.Embedding(child.weight.shape[0], get_shard_size(child.weight.shape[1], self.mp_size, name))\n        new_embedding.weight.data.copy_(data)\n        setattr(child, \"replaced\", True)\n        return new_embedding\n\n    def update_mp_params(self, child):\n        if getattr(child, \"replaced\", False) == True:\n            return\n        param_list = [\n            \"n_heads\", \"inner_dim\", \"num_heads\", \"num_kv\", \"num_attention_heads\", \"num_attn_heads\", \"all_head_size\",\n            \"embed_dim\", \"hidden_size\", \"num_key_value_heads\", \"num_kv_heads\", \"kv_n_heads\", \"d_model\"\n        ]\n        for param in param_list:\n            if \"Yuan\" in str(child) and 'embed_dim' in param_list:\n                param_list.remove('embed_dim')\n            if hasattr(child, param):\n                param_val = getattr(child, param)\n                setattr(child, param, get_shard_size(param_val, self.mp_size))\n        setattr(child, \"replaced\", True)\n\n    def update_linear_policies(self):\n        self.conv_linear_layer = False\n        if self.linear_layer_setting is not None:\n            self.linear_policies = {self.linear_layer_setting[0]: self._replace}\n            if len(self.linear_layer_setting) == 2:\n                self.linear_policies.update({self.linear_layer_setting[1]: self._slice_embedding})\n        else:\n            import transformers\n            if self.orig_layer_impl is transformers.models.gpt2.modeling_gpt2.GPT2Block:\n                try:\n                    self.conv_linear_layer = True\n                    self.linear_policies = {transformers.pytorch_utils.Conv1D: self._replace}\n                except ImportError:\n                    self.linear_policies = {nn.Linear: self._replace}\n            else:\n                self.linear_policies = {nn.Linear: self._replace, nn.Embedding: self._slice_embedding}\n\n    def _replace_module(self, r_module, prev_name='', prev_class_name=''):\n        for name, child in r_module.named_children():\n            if prev_class_name == \"\":\n                class_name = prev_name\n            elif prev_name == \"\":\n                class_name = prev_class_name\n            else:\n                class_name = prev_class_name + '.' + prev_name\n            checking_key = self.prefix + '.' + class_name + '.' + name + '.' if class_name != \"\" else self.prefix + '.' + name + '.'\n            if Loading.is_load_module(child) and self.state_dict is not None:\n                if any(checking_key in item for item in self.state_dict):\n                    Loading.load(child, self.state_dict, checking_key, self.mp_group)\n                else:\n                    continue\n            if len(child._buffers) != 0 and self.state_dict is not None:\n                Loading.load_buffer(child, self.state_dict, checking_key)\n            if child.__class__ in self.linear_policies:\n                setattr(r_module, name, self.linear_policies[child.__class__](child, prev_name + '.' + name,\n                                                                              self.conv_linear_layer))\n            elif any(isinstance(child, lp) for lp in self.linear_policies):\n                # Added for falcon model support\n                # Note: isinstance will account for class inheritance, child.__class__ does not\n                key = None\n                for lp in self.linear_policies:\n                    if isinstance(child, lp):\n                        key = lp\n                        break\n                assert key is not None\n                setattr(r_module, name, self.linear_policies[key](child, prev_name + '.' + name,\n                                                                  self.conv_linear_layer))\n            else:\n                self.update_mp_params(child)\n                self._replace_module(child, name, class_name)\n        return r_module\n\n    def get_model_num_kv_heads(self, config):\n        num_kv_heads = None\n        kv_head_names = ['num_kv_heads', 'num_key_value_heads', 'num_attention_heads', 'n_heads']\n        for name in kv_head_names:\n            if hasattr(config, name):\n                num_kv_heads = getattr(config, name)\n                if num_kv_heads is not None:\n                    break\n        return num_kv_heads\n\n    def _replace_last_linear_module(self, r_module):\n        if hasattr(r_module, \"lm_head\"):\n            name = \"lm_head\"\n            child = r_module.lm_head\n        elif hasattr(r_module, \"embed_out\"):\n            name = \"embed_out\"\n            child = r_module.embed_out\n        else:\n            return r_module\n        if child.__class__ in self.linear_policies:\n            setattr(r_module, name, self.linear_policies[child.__class__](child, name, self.conv_linear_layer))\n        return r_module\n", "deepspeed/module_inject/fusedqkv_utils.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\nimport torch\nfrom deepspeed.utils.logging import warning_once\nfrom deepspeed.module_inject.tp_shard import get_shard_size, get_shard_size_list, get_num_kv_heads, get_n_embd, get_num_attention_heads\n\n\ndef split_by_qkvlist_and_refuse(qkv_list, split_size, split_dim=0, cat_dim=0):\n    qkv_split_list = [torch.split(mat, split_size, dim=split_dim) for mat in qkv_list]\n    tp_fusedqkv_list = [\n        torch.cat([qkv_s[i] for qkv_s in qkv_split_list], dim=cat_dim) for i in range(len(qkv_split_list[0]))\n    ]\n    return tp_fusedqkv_list\n\n\ndef require_tp_fused_qkvw(name, mp_size):\n    fused_qkvw_name_list = ['qkv_proj', 'query_key_value', 'attn.Wqkv', 'self_attn.W_pack', 'c_attn']\n\n    if mp_size == 1:\n        return False\n    for fused_name in fused_qkvw_name_list:\n        if fused_name in name:\n            return True\n    return False\n\n\ndef prepare_tp_fused_qkvw(module, src, mp_size, gpu_index):\n\n    module_str = str(module).strip()\n    if src is None:\n        return\n    fused_type_dict = {\n        'CodeGenBlock': 'codegentype',\n        'BloomBlock': 'bloomtype',\n        'GLMBlock': 'glmtype',\n        \"MPTBlock\": 'glmtype',\n        \"MptBlock\": 'glmtype',\n        \"BaichuanLayer\": 'glmtype',\n        \"QWenBlock\": 'qwentype',\n        \"FalconDecoderLayer\": 'bloomtype',\n        \"GPTBigCodeBlock\": 'bigcodetype',\n        \"DecoderLayer\": 'glmtype',\n        \"Phi3DecoderLayer\": \"phi3type\"\n    }\n\n    def _codegen_type_transpose(input, mp_size, codegen_mp_num=4):\n        # codegen_mp_num defined in https://github.com/huggingface/transformers/blob/main/src/transformers/models/codegen/modeling_codegen.py\n        assert get_num_kv_heads() % (\n            mp_size * codegen_mp_num) == 0, \"codgen autoTP requires num_kv_heads % (mp_size*codegen_mp_num) == 0\"\n        #input : [3*hidden_dim, hidden_dim](weight) or [3*hidden_dim](bias)\n\n        shape = input.shape\n        dst_shape = get_shard_size(shape[0], mp_size)\n        num_mp_blocks = input.reshape(codegen_mp_num, shape[0] // codegen_mp_num, shape[1])\n\n        #num_mp_blocks : [codegen_mp_num, 3*hidden_dim/codegen_mp_num, :]\n        src_split = list(torch.split(num_mp_blocks, num_mp_blocks.shape[1] // 3, dim=1))\n        src_split = [x.reshape(codegen_mp_num * mp_size, -1, shape[1]) for x in src_split]\n\n        split_fusedqkv = split_by_qkvlist_and_refuse(src_split, get_shard_size(shape[0] // 3, mp_size), 0, 1)\n        tp_fuseqkv_weight = torch.cat(split_fusedqkv, dim=0).reshape(shape[0], -1)\n\n        return tp_fuseqkv_weight[gpu_index * dst_shape:(gpu_index + 1) * dst_shape]\n\n    def _glm_type_transpose(input, mp_size):\n        #input : [3*hidden_dim, hidden_dim](weight) or [3*hidden_dim](bias)\n\n        shape = input.shape\n        src_split = torch.split(input, shape[0] // 3, dim=0)\n\n        split_fusedqkv = split_by_qkvlist_and_refuse(src_split, get_shard_size_list(shape[0] // 3, mp_size))\n        return split_fusedqkv[gpu_index]\n\n    def _bloom_type_transpose(input, mp_size):\n        shape = input.shape\n\n        split_fusedqkv = input.split(get_shard_size_list(shape[0], mp_size), dim=0)\n        return split_fusedqkv[gpu_index]\n\n    def _qwen_type_transpose(input, mp_size, module):\n        if not hasattr(module, \"_ds_fusedqkv_entered\"):\n            # Adjust splitting absolute value variables\n            setattr(module, \"_ds_fusedqkv_entered\", True)\n            module.attn.split_size = get_shard_size(module.attn.split_size, mp_size)\n        return _glm_type_transpose(input, mp_size)\n\n    def _bigcode_type_transpose(input, mp_size):\n        n_embd = get_n_embd()\n        q = input[:n_embd]\n        kv = input[n_embd:]\n        shape = q.shape\n        split_q = q.split(get_shard_size_list(shape[0], mp_size), dim=0)\n        return torch.cat((split_q[gpu_index], kv), dim=0)\n\n    def _phi3_type_transpose(input, mp_size):\n        num_kv_heads = get_num_kv_heads()\n        num_heads = get_num_attention_heads()\n        hidden_size = input.shape[1]\n        head_dim = hidden_size // num_heads\n        q_pos = input.shape[0] - 2 * num_kv_heads * head_dim\n        q = input[:q_pos]\n        k = input[q_pos:q_pos + num_kv_heads * head_dim]\n        v = input[q_pos + num_kv_heads * head_dim:]\n        split_q = q.split(get_shard_size_list(q.shape[0], mp_size), dim=0)\n        split_k = k.split(get_shard_size_list(k.shape[0], mp_size), dim=0)\n        split_v = v.split(get_shard_size_list(v.shape[0], mp_size), dim=0)\n        return torch.cat((split_q[gpu_index], split_k[gpu_index], split_v[gpu_index]), dim=0)\n\n    def _transpose_fused_qkvw(src, mp_size, fused_qkv_type=None, module=None):\n\n        # suppose num_heads=n, q(n)_w means the n-th q head linear weight, the weight format are as following\n        # bloomtype: [q(1)_w,k(1)_w,v(1)_w,q(2)_w,k(2)_w,v(2)_w,...,q(n)_w,k(n)_w,v(n)_w]\n        # glmtype:  [q(1)_w, q(2)_w,...,q(n)_w,k(1)_w,k(2)_w,...,k(n)_w,v(1)_w,v(2)_w,...,v(n)_w]\n        # codegentype: [q(1)_w,q(2)_w,...,q(n/t)_w,k(1)_w,k(2)_w,...,k(n/t)_w,v(1)_2,v(2)_w,...v(n/t)_w,q(n/t+1)_w,...], where t is a const defined in model file.\n\n        if fused_qkv_type == 'bloomtype':\n            return _bloom_type_transpose(src, mp_size)\n        elif fused_qkv_type == 'codegentype':\n            return _codegen_type_transpose(src, mp_size)\n        elif fused_qkv_type == 'glmtype':\n            return _glm_type_transpose(src, mp_size)\n        elif fused_qkv_type == 'qwentype':\n            return _qwen_type_transpose(src, mp_size, module)\n        elif fused_qkv_type == 'bigcodetype':\n            return _bigcode_type_transpose(src, mp_size)\n        elif fused_qkv_type == 'phi3type':\n            return _phi3_type_transpose(src, mp_size)\n\n        raise ValueError(\"unknown fused_qkv_type\")\n\n    module_name_matches = [k for k in fused_type_dict.keys() if k in module_str]\n    if module_name_matches:\n        # There can be overlap with matches (e.g., \"DecoderLayer\" and \"FalconDecoderLayer\").\n        # We take the longest matching module_name\n        module_name = max(module_name_matches, key=len)\n        fused_type = fused_type_dict[module_name]\n        return _transpose_fused_qkvw(src, mp_size, fused_type, module)\n    warning_once(f\"Unrecognized fusedkqv weight type, default to using bloom type,\"\n                 f\"please check in prepare_tp_fused_qkvw() to avoid potential calculation errors\")\n    return _bloom_type_transpose(src, mp_size)\n\n\n# For share qk type:\n# q = [q1,...,q_{n/4}, q_{n/2+1},...,q_{3n/4}, k1,...,k_{n/4}, k_{n/2+1},...,k_{3n/4}]\n# k = [q_{n/4+1},...,q_{n/2}, q_{3n/4+1},...,qn, k_{n/4+1},...,k_{n/2}, k{3n/4+1},...,kn]\n# Avoid modifying the modeling code. We adjust the value and oproj weight to fit this qk type.\ndef shard_value_with_share_qk(\n        weight,\n        bias,\n        rank,\n        world_size,\n        shard_value=True  # True -> shard_value; False -> shard_oproj\n):\n    if shard_value:\n        total_size = weight.shape[0]\n        weight_cat_dim = 0\n    else:\n        total_size = weight.shape[1]\n        weight_cat_dim = 1\n    num_heads = get_num_kv_heads()\n    head_dim = total_size // num_heads\n    assert (num_heads % world_size == 0)\n    if world_size > num_heads // 2:\n        RuntimeError(f\"world_size {world_size} is larger than half of num_heads {num_heads}\")\n    head_per_rank = num_heads // world_size\n    q_head_start = rank * head_per_rank\n    # mapping q_head to v_head\n    v_head_ids = []\n    i = 0\n    # mapping neighbor q_head to v_head\n    while i < head_per_rank:\n        v_head_ids.append(q_head_start // 2)\n        q_head_start += 2\n        i = i + 2\n\n    # mapping neighbor k_head to v_head\n    v_head_ids.extend([i + num_heads // 2 for i in v_head_ids])\n    sharded_weight = []\n    sharded_bias = []\n    for head_id in v_head_ids:\n        if shard_value:\n            sharded_weight.append(weight[head_id * head_dim:(head_id + 1) * head_dim])\n            if bias is not None:\n                sharded_bias.append(bias.data[head_id * head_dim:(head_id + 1) * head_dim])\n        else:\n            sharded_weight.append(weight[:, head_id * head_dim:(head_id + 1) * head_dim])\n    sharded_weight = torch.cat(sharded_weight, dim=weight_cat_dim)\n    if bias is not None:\n        if shard_value:\n            sharded_bias = torch.cat(sharded_bias, dim=0)\n        else:\n            bias = bias / float(world_size)\n        return torch.nn.Parameter(sharded_weight), torch.nn.Parameter(sharded_bias)\n    else:\n        return torch.nn.Parameter(sharded_weight), None\n\n\n# For phi3 with chunk mlp, adjust the weight order.\ndef shard_chunk_mlp(\n    weight,\n    bias,\n    rank,\n    world_size,\n):\n    weight_gate, weight_states = weight.chunk(2, dim=0)\n    total_size = weight_gate.shape[0]\n    split_weight_gate = weight_gate.split(get_shard_size_list(total_size, world_size, \"mlp\"), dim=0)\n    split_weight_states = weight_states.split(get_shard_size_list(total_size, world_size, \"mlp\"), dim=0)\n    shard_weight = torch.cat((split_weight_gate[rank], split_weight_states[rank]), dim=0)\n    if bias is not None:\n        bias_gate, bias_states = bias.chunk(2, dim=0)\n        split_bias_gate = bias_gate.split(get_shard_size_list(total_size, world_size, \"mlp\"), dim=0)\n        split_bias_states = bias_states.split(get_shard_size_list(total_size, world_size, \"mlp\"), dim=0)\n        return shard_weight, torch.cat((split_bias_gate[rank], split_bias_states[rank]), dim=0)\n\n    return shard_weight, None\n", "deepspeed/module_inject/replace_policy.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .containers import HFGPT2LayerPolicy\nfrom .containers import HFBertLayerPolicy\nfrom .containers import BLOOMLayerPolicy\nfrom .containers import HFGPTJLayerPolicy\nfrom .containers import HFGPTNEOLayerPolicy\nfrom .containers import GPTNEOXLayerPolicy\nfrom .containers import HFOPTLayerPolicy\nfrom .containers import MegatronLayerPolicy\nfrom .containers import HFDistilBertLayerPolicy\nfrom .containers import HFCLIPLayerPolicy\nfrom .containers import LLAMALayerPolicy\nfrom .containers import UNetPolicy\nfrom .containers import VAEPolicy\nfrom .containers import LLAMA2LayerPolicy\nfrom .containers import InternLMLayerPolicy\n\n# transformer-based policies\nreplace_policies = [\n    HFBertLayerPolicy, HFGPTNEOLayerPolicy, GPTNEOXLayerPolicy, HFGPTJLayerPolicy, MegatronLayerPolicy,\n    HFGPT2LayerPolicy, BLOOMLayerPolicy, HFOPTLayerPolicy, HFCLIPLayerPolicy, HFDistilBertLayerPolicy,\n    LLAMALayerPolicy, LLAMA2LayerPolicy, InternLMLayerPolicy\n]\n\n# non-transformer-based policies\ngeneric_policies = [UNetPolicy, VAEPolicy]\n", "deepspeed/module_inject/policy.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom abc import ABC, abstractmethod\nfrom deepspeed.utils.types import ActivationFuncType, NormType\nimport torch\nfrom deepspeed.accelerator import get_accelerator\n\ntransformer_param_names = (\n        'attn_qkvw', \\\n        'attn_qkvb', \\\n        'attn_ow' , \\\n        'attn_ob', \\\n        'inter_w', \\\n        'inter_b', \\\n        'output_w', \\\n        'output_b', \\\n        'attn_nw', \\\n        'attn_nb', \\\n        'norm_w', \\\n        'norm_b')\n\n\nclass DSPolicy(ABC):\n    _orig_layer_class = None\n\n    def __init__(self):\n        self.cuda_graph_supported = False\n\n    @abstractmethod\n    def attention(self):\n        \"\"\"\n        Returns attention qkv and dense parameters\n        weight: (3*hidden, hidden) and (hidden, hidden)\n        bias: (3*hidden) and (hidden)\n        \"\"\"\n        raise NotImplementedError\n\n\nclass TransformerPolicy(DSPolicy):\n    # a static class variable containing the HuggingFace model configuration.\n    # see e.g., transformers.models.opt.configuration_opt.OPTConfig\n    hf_model_config = None\n\n    def __init__(\n            self,\n            inference=True,\n            linear_layer=True,\n            scale_attention=True,\n            megatron_v2=False,\n            use_mup=False,\n            # the type of activation function used in MLP\n            mlp_act_func_type=ActivationFuncType.GELU,\n            # applies layer norm before attention if `pre_attn_norm` is set to True\n            pre_attn_norm=True,\n            # this flag shows whether or not using prefix in loading the checkpoint\n            use_load_prefix=False,\n            # whether or not the qkv is stored in the split-format\n            split_qkv=True,\n            # Type of normalization to perform\n            norm_type=NormType.LayerNorm):\n        super().__init__()\n        self.cuda_graph_supported = False\n        self.inference = inference\n        self.linear_layer = linear_layer\n        self.scale_attention = scale_attention\n        self.is_megatron_v2 = megatron_v2\n        self.use_mup = use_mup\n        self.mlp_act_func_type = mlp_act_func_type\n        self.pre_attn_norm = pre_attn_norm\n        self.use_load_prefix = use_load_prefix\n        self.split_qkv = split_qkv\n        self.norm_type = norm_type\n\n    @abstractmethod\n    def attention(self):\n        \"\"\"\n        Returns attention qkv and dense parameters\n        weight: (3*hidden, hidden) and (hidden, hidden)\n        bias: (3*hidden) and (hidden)\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def get_hidden_heads(self):\n        \"\"\"\n        return hidden_size and number of heads\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def mlp(self):\n        \"\"\"\n        Returns mlp intermediate and output\n        weight: (intermediate, hidden) and (hidden, intermediate)\n        bias: (intermediate) and (hidden)\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def layernorm(self):\n        \"\"\"\n        Returns LayerNorms used in transformer layer\n        Post-Attention and pre/post layer norm\n        gamma and beta with shape: (hidden)\n        \"\"\"\n        raise NotImplementedError\n\n\n# TODO (lekurile): This function exists in base container as well, consolidate as some point\ndef transpose(data):\n    with torch.no_grad():\n        data = data.contiguous()\n        data1 = data.transpose(-1, -2).reshape(-1)\n        data.reshape(-1).copy_(data1)\n        data1 = None\n    return data.reshape(data.shape[-1], data.shape[-2])\n\n\n# TODO (lekurile): This function exists in megatron feature container as well, consolidate as some point\ndef _transpose(x, heads=1, mp_replace=None):\n    heads = heads // mp_replace.mp_size  # type: ignore\n    outer_dim = -1\n    attention_head_size = x.shape[outer_dim] // heads\n    new_x_shape = x.size()[:outer_dim] + (heads, attention_head_size)\n    x_1 = x.view(*new_x_shape)\n    (q, k, v) = torch.split(x_1, (x_1.shape[-1] // 3), dim=-1)\n    if len(q.shape) > 2:\n        new_shape = (q.shape[0], ) + (-1, )\n        return torch.cat((q.reshape(new_shape), k.reshape(new_shape), v.reshape(new_shape)),\n                         dim=outer_dim).reshape(x.shape)\n    else:\n        return torch.cat((q.reshape(-1), k.reshape(-1), v.reshape(-1)), dim=-1).reshape(x.shape)\n\n\n# This checks if the parameter exits in the checkpoint file and maybe copies it into the corresponding destination tensor.\n# Note that not all parameters are saved in one checkpoint, that's why we always need to check if they exist!\ndef maybe_copy(module,\n               sd,\n               weight_quantizer,\n               mp_replace,\n               dst_name,\n               src_name,\n               qkv=False,\n               megatron_v2=False,\n               split_qkv=False,\n               heads=1):\n    if src_name in sd:\n        dst = getattr(module, dst_name)\n        tmp = sd[src_name]\n        if len(dst.shape) == 1:\n            if split_qkv:\n                dst = mp_replace.strided_copy(dst, tmp, num_splits=3)\n            else:\n                dst = mp_replace.copy(dst, tmp)\n            if qkv and megatron_v2:\n                dst = torch.nn.parameter.Parameter(_transpose(dst, heads=heads, mp_replace=mp_replace).contiguous())\n        else:\n            if split_qkv:\n                dst = mp_replace.strided_copy(dst, weight_quantizer.quantize(tmp if weight_quantizer.q_int8 else \\\n                                                (transpose(tmp).contiguous())), num_splits=3, int8=weight_quantizer.q_int8)\n            else:\n                if qkv and megatron_v2:\n                    tmp = _transpose(transpose(tmp), heads=heads, mp_replace=mp_replace).contiguous()\n                    if weight_quantizer.q_int8:\n                        tmp = transpose(tmp)\n                dst = mp_replace.copy(dst, weight_quantizer.quantize(tmp if weight_quantizer.q_int8 else \\\n                                                transpose(tmp)), int8=weight_quantizer.q_int8)\n        setattr(module, dst_name, dst)\n\n\n# Extending the maybe_copy function for when the q, k, and v are in separate parameters!\ndef maybe_copy_qkv(module, sd, weight_quantizer, mp_replace, dst_name, src_names, split_qkv=False):\n    if src_names[0] in sd:\n        q = sd[src_names[0]]\n        k = sd[src_names[1]]\n        v = sd[src_names[2]]\n        qkv_data = torch.cat((q, k, v), dim=0)\n        dst = getattr(module, dst_name)\n        if len(dst.shape) == 1:\n            if split_qkv:\n                dst = mp_replace.strided_copy(dst, qkv_data.contiguous(), num_splits=3)\n            else:\n                dst = mp_replace.copy(dst, qkv_data)\n        else:\n            if split_qkv:\n                dst = mp_replace.strided_copy(dst, weight_quantizer.quantize(qkv_data.to(get_accelerator().device_name()) if weight_quantizer.q_int8 else \\\n                                                ((transpose(qkv_data)).contiguous())), num_splits=3, int8=weight_quantizer.q_int8)\n            else:\n                dst = mp_replace.copy(dst, weight_quantizer.quantize(qkv_data.to(get_accelerator().device_name()) if weight_quantizer.q_int8 else \\\n                                                transpose(qkv_data)), int8=weight_quantizer.q_int8)\n        setattr(module, dst_name, dst)\n\n\n# Extending the `maybe_copy` function for when mlp1 is in separate parameters for GeGLU\ndef maybe_copy_geglu(module, sd, weight_quantizer, mp_replace, dst_name, src_names):\n    if src_names[0] in sd:\n        reg_proj = sd[src_names[0]]\n        gate_proj = sd[src_names[1]]\n\n        mlp1_data = torch.cat((reg_proj, gate_proj), dim=0)\n        dst = getattr(module, dst_name)\n\n        dst = mp_replace.strided_copy(dst, weight_quantizer.quantize(mlp1_data.to(get_accelerator().device_name()) if weight_quantizer.q_int8 else \\\n                                            transpose(mlp1_data)), num_splits=2, int8=weight_quantizer.q_int8)\n        setattr(module, dst_name, dst)\n\n\ndef pack_lora_weights(p):\n    return [\n        p.lora_right_weight, \\\n        p.lora_left_weight, \\\n        p.lora_scaling\n    ]\n\n\ndef maybe_get_lora(p):\n    if hasattr(p, 'lora_right_weight'):\n        lora_param = pack_lora_weights(p)\n    else:\n        lora_param = []\n    return lora_param\n", "deepspeed/module_inject/module_quantize.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\n\n\ndef quantize_transformer_layer(orig_layer_impl, model, megatron=False, preln=False):\n    \"\"\" Quantize bert-style transformer layers with DeepSpeed's transformer layer\n    Arguments:\n        orig_layer_impl (torch.nn.Module): the original transformer layer implementation to look for,\n            e.g., transformers.models.bert.modeling_bert.BertLayer or transformers.BertLayer\n        model (torch.nn.Module): user's nn.module representing their model\n\n        megatron (bool): megatron model-parallel implementation (this is supported for inference only)\n        preln (bool): does the original layer implementation do pre or post layer norm?\n\n        Note: For Bert kind of models, we inject based on the DeepSpeed-Example models, if not setting huggingface flag.\n\n    Returns:\n        Updated nn.module with quantized transformer layers\n    \"\"\"\n\n    def quantize_weight(weight):\n        return weight.to(torch.int8)\n\n    def megatron_layer_quantize(layer):\n        layer.attention.query_key_value.weight.data = quantize_weight(layer.attention.query_key_value.weight.data)\n        layer.attention.dense.weight.data = quantize_weight(layer.attention.dense.weight.data)\n        layer.mlp.dense_h_to_4h.weight.data = quantize_weight(layer.mlp.dense_h_to_4h.weight.data)\n        layer.mlp.dense_4h_to_h.weight.data = quantize_weight(layer.mlp.dense_4h_to_h.weight.data)\n\n    def bert_layer_quantize(layer):\n        layer.attention.self.query.weight.data = quantize_weight(layer.attention.self.query.weight.data)\n        layer.attention.self.key.weight.data = quantize_weight(layer.attention.self.key.weight.data)\n        layer.attention.self.value.weight.data = quantize_weight(layer.attention.self.value.weight.data)\n        layer.attention.output.dense.weight.data = quantize_weight(layer.attention.output.dense.weight.data)\n        if preln:\n            layer.intermediate.dense_act.weight.data = quantize_weight(layer.intermediate.dense_act.weight.data)\n        else:\n            layer.intermediate.dense.weight.data = quantize_weight(layer.intermediate.dense.weight.data)\n        layer.output.dense.weight.data = quantize_weight(layer.output.dense.weight.data)\n\n    def quantize_fn(child):\n        if megatron:\n            # Quantize megatron GPT2 / GPT3 trained model\n            megatron_layer_quantize(child)\n        else:\n            # Quantize either DeepSpeed or HuggingFace trained model\n            bert_layer_quantize(child)\n\n        return child\n\n    return quantize_module(model=model, orig_class=orig_layer_impl, quantize_fn=quantize_fn)\n\n\ndef quantize_module(model, orig_class, quantize_fn):\n    policy = {orig_class: quantize_fn}\n    return _quantize_module(model, policy)\n\n\ndef _quantize_module(model, policies):\n    for name, child in model.named_children():\n        if child.__class__ in policies:\n            orig = repr(child)\n            setattr(model, name, policies[child.__class__](child))\n            new = getattr(model, name)\n        else:\n            _quantize_module(child, policies)\n\n    return model\n", "deepspeed/module_inject/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .replace_module import replace_transformer_layer, revert_transformer_layer, ReplaceWithTensorSlicing, GroupQuantizer, generic_injection\nfrom .module_quantize import quantize_transformer_layer\nfrom .replace_policy import HFBertLayerPolicy\nfrom .layers import LinearAllreduce, LinearLayer, EmbeddingLayer, Normalize\nfrom .policy import DSPolicy\n", "deepspeed/module_inject/layers.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nfrom deepspeed import comm as dist\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom torch.nn.parameter import Parameter\nfrom deepspeed.accelerator import get_accelerator\nfrom deepspeed.module_inject.tp_shard import get_shard_size, get_shard_size_list\n\n\nclass TensorParallelConv2d(nn.Module):\n\n    def __init__(self, conv, rank, world_size, shard_by_oc):\n        super().__init__()\n        self.rank = rank\n        self.world_size = world_size\n        self.shard_by_oc = shard_by_oc\n        self.shard_weights(conv)\n\n    # Split along the input/output channel depending on whether it is the last conv layer.\n    def shard_weights(self, conv):\n        if self.shard_by_oc:\n            total_size = conv.weight.shape[0]\n        else:\n            total_size = conv.weight.shape[1]\n        bias_data = None\n        cols_per_rank = [0]\n        for i in range(self.world_size - 1, -1, -1):\n            cols = total_size // self.world_size\n            if i < total_size % self.world_size:\n                cols += 1\n            cols_per_rank.append(cols_per_rank[-1] + cols)\n        weight_data = conv.weight.data\n        if self.shard_by_oc:\n            # not last conv layer, split output channel\n            weight_data = weight_data[cols_per_rank[self.rank]:cols_per_rank[self.rank + 1]]\n            if conv.bias is not None:\n                bias_data = conv.bias.data[cols_per_rank[self.rank]:cols_per_rank[self.rank + 1]]\n        else:\n            # last conv layer, split input channel\n            weight_data = weight_data[:, cols_per_rank[self.rank]:cols_per_rank[self.rank + 1]]\n            if conv.bias is not None:\n                bias_data = conv.bias.data / float(self.world_size)\n        self.conv = nn.Conv2d(weight_data.shape[1], weight_data.shape[0], conv.kernel_size, conv.stride, conv.padding,\n                              conv.dilation, conv.groups, conv.bias is not None, conv.padding_mode)\n        self.conv.weight = torch.nn.Parameter(weight_data)\n        if conv.bias is not None:\n            self.conv.bias = torch.nn.Parameter(bias_data)\n        del conv\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        return self.conv(input)\n\n\nclass TensorParallelOcShardConv2d(TensorParallelConv2d):\n\n    def __init__(self, conv, rank, world_size):\n        super().__init__(conv, rank, world_size, True)\n\n\nclass TensorParallelIcShardConv2d(TensorParallelConv2d):\n\n    def __init__(self, conv, rank, world_size):\n        super().__init__(conv, rank, world_size, False)\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        out = self.conv(input)\n        if self.world_size > 1:\n            dist.inference_all_reduce(out)\n        return out\n\n\nclass LinearAllreduce(nn.Module):\n\n    def __init__(self, weight, bias=None, mp_group=None):\n        super(LinearAllreduce, self).__init__()\n        self.weight = weight\n        self.bias = bias\n        self.mp_group = mp_group\n\n    def forward(self, input):\n        output = torch.matmul(input, self.weight.transpose(-1, -2))\n        if self.mp_group is not None:\n            dist.inference_all_reduce(output, group=self.mp_group)\n        if self.bias is not None:\n            output += self.bias\n        return output\n\n\nclass LmHeadLinearAllreduce(nn.Module):\n\n    def __init__(\n        self,\n        weight,\n        rank,\n        world_size,\n        bias=None,\n        mp_group=None,\n    ):\n        super(LmHeadLinearAllreduce, self).__init__()\n        self.weight = weight\n        self.bias = bias\n        self.mp_group = mp_group\n        self.rank = rank\n        self.world_size = world_size\n\n    def forward(self, input):\n        input_shard_size = get_shard_size(input.shape[-1], self.world_size, \"lm_head\")\n        input_shard_offset = sum(get_shard_size_list(input.shape[-1], self.world_size, \"lm_head\")[0:self.rank])\n        output = torch.matmul(input[:, :, input_shard_offset:input_shard_offset + input_shard_size],\n                              self.weight.transpose(-1, -2))\n        if self.mp_group is not None:\n            dist.inference_all_reduce(output, group=self.mp_group)\n        if self.bias is not None:\n            output += self.bias\n        return output\n\n\nclass LinearLayer(nn.Module):\n\n    def __init__(self, weight_shape=None, dtype=torch.half, weight=None, bias=None):\n        super(LinearLayer, self).__init__()\n        if weight is not None:\n            self.weight = weight\n            self.bias = bias\n        else:\n            self.weight = Parameter(\n                torch.empty(weight_shape, dtype=dtype, device=get_accelerator().current_device_name()))\n\n            self.bias = Parameter(\n                torch.empty(weight_shape[0],\n                            dtype=dtype,\n                            device=get_accelerator().current_device_name())) \\\n                if bias is not None else None\n\n    def forward(self, input):\n        output = torch.matmul(input, self.weight.transpose(-1, -2))\n        if self.bias is not None:\n            output += self.bias\n        return output\n\n\nclass Normalize(nn.Module):\n\n    def __init__(self, dim=None, dtype=torch.float, eps=1e-5, weight=None, bias=None):\n        super(Normalize, self).__init__()\n        if weight is not None:\n            self.weight = weight\n            self.bias = bias\n        else:\n            self.norm = nn.LayerNorm(dim, eps=eps).to(dtype).to(get_accelerator().current_device_name())\n            self.weight = self.norm.weight\n            self.bias = self.norm.bias\n\n        self.eps = eps\n\n    def forward(self, input):\n        return nn.functional.layer_norm(input, input.shape[-1:], self.weight, self.bias, eps=self.eps)\n\n\nclass EmbeddingLayer(nn.Module):\n\n    def __init__(self, weight_shape=None, dtype=torch.half, weight=None, bias=None):\n        super(EmbeddingLayer, self).__init__()\n        if weight is None:\n            self.weight = Parameter(\n                torch.empty(weight_shape[0],\n                            weight_shape[1],\n                            dtype=dtype,\n                            device=get_accelerator().current_device_name()))\n        else:\n            self.weight = weight\n\n    def forward(self, input):\n        return F.embedding(input, self.weight)\n\n\nclass OPTEmbedding(EmbeddingLayer):\n    \"\"\"\n    This module learns positional embeddings up to a fixed maximum size.\n    \"\"\"\n\n    def __init__(self, weight_shape=None, weight=None, bias=None):\n        # OPT is set up so that if padding_idx is specified then offset the embedding ids by 2\n        # and adjust num_embeddings appropriately. Other models don't have this hack\n        self.offset = 2\n        super().__init__(weight_shape, weight=weight)\n\n    def forward(self, attention_mask: torch.LongTensor, past_key_values_length: int = 0):\n        \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n        attention_mask = attention_mask.long()\n\n        # create positions depending on attention_mask\n        positions = (torch.cumsum(attention_mask, dim=1).type_as(attention_mask) * attention_mask).long() - 1\n\n        # cut positions if `past_key_values_length` is > 0\n        positions = positions[:, past_key_values_length:]\n\n        return super().forward(positions + self.offset)\n\n\nclass RMSNormalize(nn.Module):\n\n    def __init__(self, dim=None, dtype=torch.float, eps=1e-5, weight=None):\n        super(RMSNormalize, self).__init__()\n        if weight is not None:\n            self.weight = weight\n        else:\n            self.weight = nn.Parameter(torch.ones(dim, dtype=dtype, device=get_accelerator().current_device_name()))\n\n        self.eps = eps\n\n    def forward(self, hidden_states):\n        variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.eps)\n        if self.weight.dtype in [torch.float16, torch.bfloat16]:\n            hidden_states = hidden_states.to(self.weight.dtype)\n\n        return hidden_states * self.weight\n", "deepspeed/module_inject/load_checkpoint.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom torch import nn\nfrom deepspeed.model_implementations.transformers.ds_bloom import DeepSpeedBloomInference\nfrom deepspeed.model_implementations.transformers.ds_gpt import DeepSpeedGPTInference\nfrom deepspeed.model_implementations.transformers.ds_bert import DeepSpeedBERTInference\nfrom deepspeed.model_implementations.transformers.ds_megatron_gpt import DeepSpeedMegatronGPTInference\nfrom deepspeed.model_implementations.transformers.ds_opt import DeepSpeedOPTInference\nfrom deepspeed.model_implementations.transformers.ds_llama2 import DeepSpeedLlama2Inference\n\nimport deepspeed.ops.transformer as transformer_inference\nfrom .layers import LinearLayer, Normalize, EmbeddingLayer, OPTEmbedding, RMSNormalize\nimport torch\nimport gc\nfrom deepspeed.accelerator import get_accelerator\nimport re\n\n\ndef load_model_with_checkpoint(r_module,\n                               sd,\n                               mp_replace,\n                               ckpt_type,\n                               ckpt_mp_size,\n                               weight_quantizer=None,\n                               rank=0,\n                               container=None):\n    error_msgs = []\n\n    def prefix_check():\n        # if keys start with 'model.' or 'transformer.', don't skip level 0 prefix\n        for key in sd[0].keys():\n            # OPT models\n            if re.match(\"^model[.]\", key):\n                return False\n            # BLOOM models\n            if re.match(\"^transformer[.]\", key):\n                return False\n        return True\n\n    skip_level_0_prefix = prefix_check() and container.policy.use_load_prefix\n\n    def transpose(data):\n        with torch.no_grad():\n            data = data.contiguous()\n            data1 = data.transpose(-1, -2).reshape(-1)\n            data.reshape(-1).copy_(data1)\n            data1 = None\n        return data.reshape(data.shape[-1], data.shape[-2])\n\n    def load(module, prefix):\n        args = (sd[0], prefix, {}, True, [], [], error_msgs)\n\n        if hasattr(module, 'weight'):\n            module.weight = mp_replace.copy(module.weight.data, sd[0][prefix + 'weight'])\n        if prefix + 'bias' in sd[0].keys():\n            if module.bias.data.is_meta:\n                # meta tensor cannot be casted or copied to, so we need to replace it with a normal tensor here\n                module.bias = torch.nn.parameter.Parameter(data=torch.empty_like(module.bias.data, device=\"cpu\"),\n                                                           requires_grad=module.bias.data.requires_grad)\n            module.bias = mp_replace.copy(module.bias.data, sd[0][prefix + 'bias'])\n        args = None\n        gc.collect()\n\n    def load_transformer_layer(module, prefix):\n        if ckpt_type == \"tp\":\n\n            def load_parameters(module, prefix):\n                for n, p in module.named_parameters():\n                    if prefix + n in sd[0] and len(n.split('.')) == 1:\n                        if type(sd[0][prefix + n]) is list:\n                            tmp_data, scale = sd[0][prefix + n]\n                            tmp_data = tmp_data\n                            scale = scale.to(get_accelerator().current_device_name())\n                            # set the quantizer number of groups using the checkpoint scale shape\n                            weight_quantizer.num_groups = scale.shape[0]\n                        else:\n                            tmp_data = sd[0][prefix + n].to(get_accelerator().current_device_name())\n                            scale = None\n                        src_shape = tmp_data.shape\n                        dst_shape = p.shape\n                        inner_dim = 1 if tmp_data.dtype == torch.int8 else 0\n                        outer_dim = 0 if tmp_data.dtype == torch.int8 else 1\n                        if (len(src_shape) == 2 and len(dst_shape) == 2):\n                            if (src_shape[inner_dim] == dst_shape[0] and src_shape[outer_dim] == dst_shape[1]):\n                                if tmp_data.dtype != torch.int8:\n                                    p = weight_quantizer.quantize(\n                                        transpose(tmp_data) if weight_quantizer.q_int8 else tmp_data)\n                                else:\n                                    p = torch.nn.parameter.Parameter(tmp_data, requires_grad=False)\n                                    p.scale = scale\n                                setattr(module, n, p)\n                            else:\n                                dim = inner_dim if src_shape[inner_dim] != dst_shape[0] else outer_dim\n                                dim1 = 0 if src_shape[inner_dim] != dst_shape[0] else 1\n                                if src_shape[dim] > dst_shape[dim1]:\n                                    weight_partition = torch.split(tmp_data, dst_shape[dim1], dim=dim)[rank].to(\n                                        get_accelerator().current_device_name())\n                                    assert tmp_data.dtype != torch.int8 or scale.numel() > weight_quantizer.num_groups * (rank+1), \\\n                                        '''ERROR: We require the quantization scales for larger TP-size when loading INT8 checkpoint!\\\n                                           Please use the FP16 checkpoint to generate INT8 checkpoint with the sharding parameters!'''\n                                    scale = scale.view(-1)[weight_quantizer.num_groups * (rank + 1):].reshape(\n                                        weight_quantizer.num_groups, -1).contiguous()\n                                else:\n                                    assert tmp_data.dtype != torch.int8, \\\n                                        '''Merging of the checkpoints are not supported when using INT8 checkpoint! \\\n                                          Please use a as many GPUs as TP-size for the checkpoint'''\n                                    all_data = [\n                                        sd[j][prefix + n] if type(sd[j][prefix + n]) is list else sd[j][prefix + n].to(\n                                            get_accelerator().current_device_name()) for j in range(len(sd))\n                                    ]\n                                    # Check if the weight tensor is for the QKV parameter\n                                    if src_shape[1] == (3 * src_shape[0]) // ckpt_mp_size:\n                                        qkv_size = src_shape[outer_dim] // 3\n                                        src_split = [\n                                            torch.split(src[0].data, qkv_size, dim=outer_dim) for src in all_data\n                                        ]\n\n                                        weight_partition = torch.cat([\n                                            torch.cat([qkv_s[i] for qkv_s in src_split], axis=outer_dim)\n                                            for i in range(len(src_split[0]))\n                                        ],\n                                                                     dim=dim)\n                                    else:\n                                        weight_partition = torch.cat([\n                                            ad[0].to(get_accelerator().current_device_name())\n                                            if type(ad) is list else ad for ad in all_data\n                                        ],\n                                                                     dim=dim)\n                                    if tmp_data.dtype == torch.int8:\n                                        scale = torch.cat(\n                                            [ad[1].to(get_accelerator().current_device_name()) for ad in all_data],\n                                            dim=dim)\n\n                                if tmp_data.dtype != torch.int8:\n                                    weight_partition = weight_quantizer.quantize(\n                                        transpose(weight_partition), \\\n                                        parallel_dim=(0 if dim == 1 else 1)) if weight_quantizer.q_int8 else \\\n                                        weight_quantizer.quantize(weight_partition)\n                                else:\n                                    weight_partition = torch.nn.parameter.Parameter(weight_partition,\n                                                                                    requires_grad=False)\n                                    weight_partition.scale = scale\n                                setattr(module, n, weight_partition)\n                        else:\n                            if src_shape[0] == dst_shape[0]:\n                                p.data.copy_(tmp_data)\n                            else:\n                                if src_shape[0] > dst_shape[0]:\n                                    bias_split = torch.split(tmp_data, dst_shape[-1])[rank].to(\n                                        get_accelerator().current_device_name()).contiguous()\n                                    p.data.copy_(bias_split)\n                                else:\n                                    # Check if the weight tensor is for the QKV parameter\n                                    if src_shape[0] == (3 * r_module.config.hidden_size) // ckpt_mp_size:\n                                        qkv_size = src_shape[0] // 3\n                                        src_split = [\n                                            torch.split(sd[j][prefix + n], qkv_size, dim=0) for j in range(len(sd))\n                                        ]\n\n                                        p.data.copy_(\n                                            torch.cat([\n                                                torch.cat([qkv_s[i] for qkv_s in src_split], axis=0)\n                                                for i in range(len(src_split[0]))\n                                            ],\n                                                      dim=0).to(get_accelerator().current_device_name()).contiguous())\n                                    else:\n                                        p.data.copy_(\n                                            torch.cat([sd[j][prefix + n] for j in range(len(sd))],\n                                                      dim=0).to(get_accelerator().current_device_name()).contiguous())\n\n            load_parameters(module, prefix)\n            for n, child in module.named_children():\n                load_parameters(child, prefix + n + '.')\n        else:\n            container.load_params(module, sd[0], weight_quantizer, mp_replace, prefix)\n\n    try:\n        import transformers\n        OPTLearnedPositionalEmbedding = transformers.models.opt.modeling_opt.OPTLearnedPositionalEmbedding\n        if hasattr(transformers.models, \"llama\"):\n            LlamaRMSNorm = transformers.models.llama.modeling_llama.LlamaRMSNorm\n        else:\n            LlamaRMSNorm = None\n    except:\n        OPTLearnedPositionalEmbedding = None\n    try:\n        from fairscale.nn.model_parallel.layers import (\n            ColumnParallelLinear,\n            ParallelEmbedding,\n            RowParallelLinear,\n        )\n    except:\n        ColumnParallelLinear = None\n        ParallelEmbedding = None\n        RowParallelLinear = None\n    try:\n        from llama.model import RMSNorm\n    except:\n        RMSNorm = None\n    layer_policies = {\n        nn.Linear: load,\n        nn.Embedding: load,\n        nn.LayerNorm: load,\n        EmbeddingLayer: load,\n        LinearLayer: load,\n        Normalize: load,\n        transformer_inference.DeepSpeedTransformerInference: load_transformer_layer,\n        DeepSpeedBloomInference: load_transformer_layer,\n        DeepSpeedGPTInference: load_transformer_layer,\n        DeepSpeedBERTInference: load_transformer_layer,\n        DeepSpeedMegatronGPTInference: load_transformer_layer,\n        DeepSpeedOPTInference: load_transformer_layer,\n        DeepSpeedLlama2Inference: load_transformer_layer,\n        OPTLearnedPositionalEmbedding: load,\n        OPTEmbedding: load,\n        LlamaRMSNorm: load,\n        RMSNormalize: load,\n        ColumnParallelLinear: load,\n        ParallelEmbedding: load,\n        RowParallelLinear: load,\n        RMSNorm: load\n    }\n\n    all_ds_ids = {}\n\n    def load_module_recursive(module, prefix='', level=0):\n        for name, child in module.named_children():\n            if child.__class__ in layer_policies:\n                checking_key = prefix + name + '.'\n                if not any(checking_key in item for item in sd[0].keys()):\n                    if hasattr(child, 'weight') and \\\n                        (hasattr(child.weight, 'ds_id') and \\\n                        child.weight.ds_id in all_ds_ids):\n                        prefix1 = all_ds_ids[child.weight.ds_id]\n                        if child.__class__ is nn.Linear:\n                            child = LinearLayer(weight=all_ds_ids[child.weight.ds_id])\n                            setattr(module, name, child)\n                    continue\n                child_params = list(child.parameters())\n                if len(child_params) > 0 and (child_params[0].numel() == 0 or child_params[0].is_meta):\n                    if child.weight.is_meta:\n                        ds_shape = child.weight.shape\n                    else:\n                        ds_shape = child.weight.ds_shape\n                    if child.__class__ is nn.LayerNorm:\n                        child = Normalize(dim=ds_shape[-1], dtype=child.weight.dtype, eps=child.eps)\n                        setattr(module, name, child)\n                    elif child.__class__ in [nn.Linear, ColumnParallelLinear, RowParallelLinear]:\n                        child = LinearLayer(weight_shape=child.weight.shape, dtype=child.weight.dtype, bias=child.bias)\n                        setattr(module, name, child)\n                    elif child.__class__ is OPTLearnedPositionalEmbedding:\n                        child = OPTEmbedding(weight_shape=ds_shape)\n                        setattr(module, name, child)\n                    elif child.__class__ in [LlamaRMSNorm, RMSNorm]:\n                        child = RMSNormalize(dim=ds_shape[-1],\n                                             dtype=child.weight.dtype,\n                                             eps=child.eps if hasattr(child, 'eps') else child.variance_epsilon)\n                        setattr(module, name, child)\n                    else:\n                        ds_id = None\n                        if hasattr(child.weight, 'ds_id'):\n                            ds_id = child.weight.ds_id\n                        child = EmbeddingLayer(weight_shape=ds_shape, dtype=child.weight.dtype)\n                        if ds_id is not None:\n                            all_ds_ids[ds_id] = child.weight\n                        setattr(module, name, child)\n                layer_policies[child.__class__](child, prefix + name + '.')\n            else:\n                load_module_recursive(\n                    child,\n                    prefix if (level == 0 and ckpt_type == 'pp') and skip_level_0_prefix else \\\n                    prefix + name + '.',\n                    level + 1)\n\n    load_module_recursive(r_module)\n\n    for sd_ in sd:\n        del sd_\n    sd = None\n    gc.collect()\n", "deepspeed/module_inject/auto_tp_model_utils.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom deepspeed import comm as dist\nimport torch\nfrom typing import Optional\nfrom deepspeed.module_inject.tp_shard import get_shard_size, get_shard_size_list\n\n\ndef build_bloom_alibi_tensor(attention_mask: torch.Tensor, num_heads: int, dtype: torch.dtype) -> torch.Tensor:\n    \"\"\"\n    Link to paper: https://arxiv.org/abs/2108.12409 Alibi tensor is not causal as the original paper mentions, it\n    relies on a translation invariance of softmax for quick implementation: with l being a tensor, and a fixed value\n    `softmax(l+a) = softmax(l)`. Based on\n    https://github.com/ofirpress/attention_with_linear_biases/blob/a35aaca144e0eb6b789dfcb46784c4b8e31b7983/fairseq/models/transformer.py#L742\n    TODO @thomasw21 this doesn't work as nicely due to the masking strategy, and so masking varies slightly.\n\n    Args:\n    Returns tensor shaped (batch_size * num_heads, 1, max_seq_len)\n        attention_mask (`torch.Tensor`):\n            Token-wise attention mask, this should be of shape (batch_size, max_seq_len).\n        num_heads (`int`, *required*):\n            number of heads\n        dtype (`torch.dtype`, *optional*, default=`torch.bfloat16`):\n            dtype of the output tensor\n    \"\"\"\n    import math\n    batch_size, seq_length = attention_mask.shape\n    closest_power_of_2 = 2**math.floor(math.log2(num_heads))\n    base = torch.tensor(2**(-(2**-(math.log2(closest_power_of_2) - 3))),\n                        device=attention_mask.device,\n                        dtype=torch.float32)\n    powers = torch.arange(1, 1 + closest_power_of_2, device=attention_mask.device, dtype=torch.int32)\n    slopes = torch.pow(base, powers)\n\n    if closest_power_of_2 != num_heads:\n        extra_base = torch.tensor(2**(-(2**-(math.log2(2 * closest_power_of_2) - 3))),\n                                  device=attention_mask.device,\n                                  dtype=torch.float32)\n        num_remaining_heads = min(closest_power_of_2, num_heads - closest_power_of_2)\n        extra_powers = torch.arange(1, 1 + 2 * num_remaining_heads, 2, device=attention_mask.device, dtype=torch.int32)\n        slopes = torch.cat([slopes, torch.pow(extra_base, extra_powers)], dim=0)\n\n    # Note: alibi will added to the attention bias that will be applied to the query, key product of attention\n    # => therefore alibi will have to be of shape (batch_size, num_heads, query_length, key_length)\n    # => here we set (batch_size=1, num_heads=num_heads, query_length=1, key_length=max_length)\n    # => the query_length dimension will then be broadcasted correctly\n    # This is more or less identical to T5's relative position bias:\n    # https://github.com/huggingface/transformers/blob/f681437203baa7671de3174b0fa583c349d9d5e1/src/transformers/models/t5/modeling_t5.py#L527\n    arange_tensor = ((attention_mask.cumsum(dim=-1) - 1) * attention_mask)[:, None, :]\n    alibi = slopes[..., None] * arange_tensor\n    if dist.is_initialized():\n        num_heads_per_rank = get_shard_size(num_heads, dist.get_world_size())\n        offset = sum(get_shard_size_list(num_heads, dist.get_world_size())[0:dist.get_rank()])\n        alibi = alibi.view(batch_size, num_heads, 1, seq_length)\n        alibi = alibi[:, offset:num_heads_per_rank + offset, :, :]\n        return alibi.reshape(batch_size * num_heads_per_rank, 1, seq_length).to(dtype)\n    else:\n        return alibi.reshape(batch_size * num_heads, 1, seq_length).to(dtype)\n\n\ndef get_alibi_mask(self, tensor, seq_length_with_past):\n    mask = self.get_alibi_mask_orig(tensor, seq_length_with_past)\n    if not self.training and dist.is_initialized():\n        num_heads_per_rank = get_shard_size(self.n_head, dist.get_world_size())\n        offset = sum(get_shard_size_list(self.n_head, dist.get_world_size())[0:dist.get_rank()])\n        mask = mask[offset:num_heads_per_rank + offset, :seq_length_with_past, :seq_length_with_past]\n\n    return mask\n\n\ndef build_mpt_atten_bias_tensor(self,\n                                device,\n                                dtype,\n                                attention_mask: Optional[torch.ByteTensor] = None,\n                                prefix_mask: Optional[torch.ByteTensor] = None,\n                                sequence_id: Optional[torch.LongTensor] = None):\n    (attn_bias, attention_mask) = self._attn_bias_orig(device,\n                                                       dtype,\n                                                       attention_mask=attention_mask,\n                                                       prefix_mask=prefix_mask,\n                                                       sequence_id=sequence_id)\n    if dist.is_initialized():\n        num_heads_per_rank = get_shard_size(self.config.n_heads, dist.get_world_size())\n        offset = sum(get_shard_size_list(self.config.n_heads, dist.get_world_size())[0:dist.get_rank()])\n        attn_bias = attn_bias[:, offset:num_heads_per_rank + offset, :, :]\n    return attn_bias, attention_mask\n\n\ndef build_mpt_alibi_tensor(self, num_heads, sequence_length, alibi_bias_max=8, device=None) -> torch.Tensor:\n    r\"\"\"\n    Link to paper: https://arxiv.org/abs/2108.12409 - Alibi tensor is not causal as the original paper mentions, it\n    relies on a translation invariance of softmax for quick implementation. This implementation has been copied from\n    the alibi implementation of MPT source code that led to slightly different results than the Bloom alibi:\n    https://huggingface.co/mosaicml/mpt-7b/blob/main/attention.py#L292\n    \"\"\"\n    alibi = self.build_mpt_alibi_tensor_orig(num_heads, sequence_length, alibi_bias_max, device)\n    if dist.is_initialized():\n        num_heads_per_rank = int(num_heads / dist.get_world_size())\n        offset = dist.get_rank() * num_heads_per_rank\n        alibi = alibi[offset:num_heads_per_rank + offset, :, :]\n    return alibi\n", "deepspeed/module_inject/containers/bloom.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .base import *\nfrom .features.meta_tensor import MetaTensorContainer\nfrom .features.hybrid_engine import HybridEngineContainer\nfrom deepspeed.model_implementations.transformers.ds_bloom import DeepSpeedBloomInference\nfrom ..policy import TransformerPolicy\nfrom ..policy import transformer_param_names\nfrom ..policy import maybe_copy\n\nfrom ..policy import maybe_get_lora\n\nsupported_models = {None}\n\n\nclass DS_BloomContainer(MetaTensorContainer, HybridEngineContainer, BaseTransformerContainer):\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n        # All model specific things should be defined here instead of the base class.\n        self.bigscience_bloom = True\n        self.triangular_masking = False\n\n    def create_module(self, config=None):\n        _config = config if config is not None else self.ds_model_config\n\n        self.module = DeepSpeedBloomInference(_config, mp_group=self.mp_group)\n        self.module.config.scale_attention = self.scale_attention\n        self.module.config.invert_mask = False\n        return self.module\n\n    def attention_qkv_mp(self, mp_replace, reversed_dim=False):\n        self.module.attention.attn_qkvw = mp_replace.copy(self.module.attention.attn_qkvw, self.qkvw)\n        self.module.attention.attn_qkvb = mp_replace.copy(self.module.attention.attn_qkvb, self.qkvb)\n\n    def get_lora_matched_pair(self):\n        \"\"\"\n        Necessary to implement for `HybridEngineContainer`\n        \"\"\"\n        fc1_lora, fc2_lora, qkv_lora, out_lora = self.get_lora_params()\n        ret = [(fc1_lora, self._h4h_w), (fc2_lora, self._4hh_w), (qkv_lora, self.qkvw), (out_lora, self.dense_w)]\n        return ret\n\n    def set_lora_params(self):\n        \"\"\"\n        Necessary to implement for `HybridEngineContainer`\n        \"\"\"\n        self.lora_params = [\n            maybe_get_lora(p) for p in [\n                self.policy.client_module.mlp.dense_h_to_4h, self.policy.client_module.mlp.dense_4h_to_h, self.policy.\n                client_module.self_attention.query_key_value, self.policy.client_module.self_attention.dense\n            ]\n        ]\n\n    def load_params(self, module, sd, weight_quantizer, mp_replace, prefix):\n        param_names = (\n            'self_attention.query_key_value.weight', \\\n            'self_attention.query_key_value.bias', \\\n            'self_attention.dense.weight', \\\n            'self_attention.dense.bias', \\\n            'mlp.dense_h_to_4h.weight', \\\n            'mlp.dense_h_to_4h.bias', \\\n            'mlp.dense_4h_to_h.weight', \\\n            'mlp.dense_4h_to_h.bias', \\\n            'post_attention_layernorm.weight', \\\n            'post_attention_layernorm.bias', \\\n            'input_layernorm.weight', \\\n            'input_layernorm.bias'\n        )\n        for i in range(0, 2):\n            maybe_copy(module.attention,\n                       sd,\n                       weight_quantizer,\n                       mp_replace,\n                       transformer_param_names[i],\n                       prefix + param_names[i],\n                       qkv=True,\n                       megatron_v2=self.policy.is_megatron_v2,\n                       split_qkv=self.policy.split_qkv)\n        for i in range(2, 4):\n            maybe_copy(module.attention, sd, weight_quantizer, mp_replace, transformer_param_names[i],\n                       prefix + param_names[i])\n        for i in range(4, 10):\n            maybe_copy(module.mlp, sd, weight_quantizer, mp_replace, transformer_param_names[i],\n                       prefix + param_names[i])\n        for i in range(10, 12):\n            maybe_copy(module, sd, weight_quantizer, mp_replace, transformer_param_names[i], prefix + param_names[i])\n\n\nclass BLOOMLayerPolicy(TransformerPolicy):\n    _orig_layer_class = None\n\n    def __init__(self, client_module, inference=True, use_load_prefix=True, split_qkv=False):\n        super().__init__(inference, linear_layer=True, use_load_prefix=use_load_prefix, split_qkv=split_qkv)\n        self.client_module = client_module\n        try:\n            import transformers\n            BLOOMLayerPolicy._orig_layer_class = transformers.models.bloom.modeling_bloom.BloomBlock\n            global supported_models\n            supported_models.update({transformers.models.bloom.modeling_bloom.BloomModel})\n        except Exception as e:\n            print(f\"WARNING! Setting BLOOMLayerPolicy._orig_layer_class to None due to Exception: {e}\")\n            BLOOMLayerPolicy._orig_layer_class = None\n\n    def get_hidden_heads(self):\n        return self.client_module.self_attention.hidden_size, \\\n                self.client_module.self_attention.num_heads, \\\n                self.client_module.input_layernorm.eps, \\\n                DEFAULT_INTERMEDIATE_SIZE\n\n    def attention(self, enable_training=False):\n        return self.client_module.self_attention.query_key_value.weight, \\\n                self.client_module.self_attention.query_key_value.bias, \\\n                self.client_module.self_attention.dense.weight, \\\n                self.client_module.self_attention.dense.bias,\n\n    def mlp(self, enable_training=False):\n        return self.client_module.mlp.dense_h_to_4h.weight, \\\n               self.client_module.mlp.dense_h_to_4h.bias, \\\n               self.client_module.mlp.dense_4h_to_h.weight, \\\n               self.client_module.mlp.dense_4h_to_h.bias\n\n    def layernorm(self):\n        return self.client_module.post_attention_layernorm.weight, \\\n               self.client_module.post_attention_layernorm.bias, \\\n               self.client_module.input_layernorm.weight, \\\n               self.client_module.input_layernorm.bias\n", "deepspeed/module_inject/containers/opt.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .base import *\nfrom .features import MetaTensorContainer, HybridSplitQKVContainer\nfrom deepspeed.model_implementations.transformers.ds_opt import DeepSpeedOPTInference\nimport torch\nfrom torch.nn.parameter import Parameter\nfrom ..policy import TransformerPolicy\nfrom ..policy import transformer_param_names\nfrom ..policy import maybe_copy\nfrom ..policy import maybe_copy_qkv\nfrom ..policy import maybe_get_lora\nfrom deepspeed.utils.types import ActivationFuncType\n\n\nclass DS_OPTContainer(MetaTensorContainer, HybridSplitQKVContainer, BaseTransformerContainer):\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n        # All model specific things should be defined here instead of the base class.\n\n    def create_module(self, config=None):\n        _config = config if config is not None else self.ds_model_config\n        self.module = DeepSpeedOPTInference(_config, mp_group=self.mp_group)\n        self.module.config.scale_attention = self.scale_attention\n        return self.module\n\n    def set_lora_params(self):\n        \"\"\"\n        Necessary to implement for `HybridEngineContainer`\n        \"\"\"\n        self.lora_params = [\n            maybe_get_lora(p) for p in [\n                self.policy.client_module.fc1,\n                self.policy.client_module.fc2,\n                self.policy.client_module.self_attn.q_proj,\n                self.policy.client_module.self_attn.k_proj,\n                self.policy.client_module.self_attn.v_proj,\n                self.policy.client_module.self_attn.out_proj,\n            ]\n        ]\n\n    def set_q_k_v(self):\n        \"\"\"\n        Necessary to implement for `HybridSplitQKVContainer`\n        \"\"\"\n        self.qw = self.policy.client_module.self_attn.q_proj.weight\n        self.qb = self.policy.client_module.self_attn.q_proj.bias\n        self.kw = self.policy.client_module.self_attn.k_proj.weight\n        self.kb = self.policy.client_module.self_attn.k_proj.bias\n        self.vw = self.policy.client_module.self_attn.v_proj.weight\n        self.vb = self.policy.client_module.self_attn.v_proj.bias\n\n    def get_lora_matched_pair(self):\n        fc1_lora, fc2_lora, q_lora, k_lora, v_lora, out_lora = self.get_lora_params()\n        ret = [(fc1_lora, self._h4h_w), (fc2_lora, self._4hh_w), (out_lora, self.dense_w), (q_lora, self.qw),\n               (k_lora, self.kw), (v_lora, self.vw)]\n        return ret\n\n    def load_params(self, module, sd, weight_quantizer, mp_replace, prefix):\n        param_names = (\n            'self_attn.q_proj.weight', \\\n            'self_attn.k_proj.weight', \\\n            'self_attn.v_proj.weight', \\\n            'self_attn.q_proj.bias', \\\n            'self_attn.k_proj.bias', \\\n            'self_attn.v_proj.bias', \\\n            'self_attn.out_proj.weight', \\\n            'self_attn.out_proj.bias', \\\n            'fc1.weight', \\\n            'fc1.bias', \\\n            'fc2.weight', \\\n            'fc2.bias', \\\n            'final_layer_norm.weight', \\\n            'final_layer_norm.bias', \\\n            'self_attn_layer_norm.weight', \\\n            'self_attn_layer_norm.bias'\n        )\n\n        for i in range(0, 6, 3):\n            maybe_copy_qkv(module.attention,\n                           sd,\n                           weight_quantizer,\n                           mp_replace,\n                           transformer_param_names[i // 3],\n                           [prefix + param_names[i], prefix + param_names[i + 1], prefix + param_names[i + 2]],\n                           split_qkv=self.policy.split_qkv)\n        for i in range(6, 8):\n            maybe_copy(module.attention, sd, weight_quantizer, mp_replace, transformer_param_names[i - 4],\n                       prefix + param_names[i])\n        for i in range(8, 14):\n            maybe_copy(module.mlp, sd, weight_quantizer, mp_replace, transformer_param_names[i - 4],\n                       prefix + param_names[i])\n        for i in range(14, 16):\n            maybe_copy(module, sd, weight_quantizer, mp_replace, transformer_param_names[i - 4],\n                       prefix + param_names[i])\n\n\nclass HFOPTLayerPolicy(TransformerPolicy):\n    _orig_layer_class = None\n\n    def __init__(self, client_module, inference=True, use_load_prefix=True):\n        super().__init__(inference, linear_layer=True, pre_attn_norm=True, use_load_prefix=use_load_prefix)\n        self.client_module = client_module\n        try:\n            import transformers\n            HFOPTLayerPolicy._orig_layer_class = transformers.models.opt.modeling_opt.OPTDecoderLayer\n        except:\n            HFOPTLayerPolicy._orig_layer_class = None\n\n        if hasattr(TransformerPolicy, \"hf_model_config\") and hasattr(TransformerPolicy.hf_model_config,\n                                                                     \"activation_function\"):\n            if TransformerPolicy.hf_model_config.activation_function == \"relu\":\n                self.mlp_act_func_type = ActivationFuncType.ReLU\n            elif TransformerPolicy.hf_model_config.activation_function in [\"gelu\", \"gelu_new\"]:\n                self.mlp_act_func_type = ActivationFuncType.GELU\n            else:\n                raise ValueError(\"Unsupported activation function: {}\".format(\n                    TransformerPolicy.hf_model_config.activation_function))\n        else:\n            self.mlp_act_func_type = ActivationFuncType.ReLU  # default\n\n    def get_hidden_heads(self):\n        return self.client_module.self_attn.embed_dim, \\\n                self.client_module.self_attn.num_heads, \\\n                self.client_module.self_attn_layer_norm.eps, \\\n                DEFAULT_INTERMEDIATE_SIZE\n\n    def attention(self, enable_training=False):\n        qw = self.client_module.self_attn.q_proj.weight\n        qb = self.client_module.self_attn.q_proj.bias\n\n        kw = self.client_module.self_attn.k_proj.weight\n        kb = self.client_module.self_attn.k_proj.bias\n\n        vw = self.client_module.self_attn.v_proj.weight\n        vb = self.client_module.self_attn.v_proj.bias\n\n        qkvw = Parameter(torch.cat((qw, kw, vw), dim=0), requires_grad=enable_training)\n        qkvb = Parameter(torch.cat((qb, kb, vb), dim=0), requires_grad=enable_training)\n        return qkvw, \\\n               qkvb, \\\n               self.client_module.self_attn.out_proj.weight, \\\n               self.client_module.self_attn.out_proj.bias\n\n    def mlp(self, enable_training=False):\n        return self.client_module.fc1.weight, \\\n               self.client_module.fc1.bias, \\\n               self.client_module.fc2.weight, \\\n               self.client_module.fc2.bias\n\n    def layernorm(self):\n        return self.client_module.final_layer_norm.weight, \\\n               self.client_module.final_layer_norm.bias, \\\n               self.client_module.self_attn_layer_norm.weight, \\\n               self.client_module.self_attn_layer_norm.bias\n", "deepspeed/module_inject/containers/gptneox.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .base import *\nfrom .features.meta_tensor import MetaTensorContainer\nfrom .features.hybrid_megatron import HybridMegatronContainer\nfrom deepspeed.model_implementations.transformers.ds_gpt import DeepSpeedGPTInference\nimport torch\nfrom ..policy import TransformerPolicy\nfrom ..policy import transformer_param_names\nfrom ..policy import maybe_copy\nfrom packaging import version as pkg_version\n\nfrom ..policy import maybe_get_lora\n\n\nclass DS_GPTNEOXContainer(MetaTensorContainer, HybridMegatronContainer, BaseTransformerContainer):\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n        # All model specific things should be defined here instead of the base class.\n\n    def create_module(self, config=None):\n        _config = config if config is not None else self.ds_model_config\n        self.module = DeepSpeedGPTInference(_config, mp_group=self.mp_group)\n        self.module.config.scale_attention = self.scale_attention\n\n        if self.megatron_v2:\n            self.module.config.rotate_half = True\n            self.module.config.rotate_every_two = False\n\n        return self.module\n\n    def get_lora_matched_pair(self):\n        \"\"\"\n        Necessary to implement for `HybridEngineContainer`\n        \"\"\"\n        fc1_lora, fc2_lora, qkv_lora, out_lora = self.get_lora_params()\n        ret = [(fc1_lora, self._h4h_w), (fc2_lora, self._4hh_w), (qkv_lora, self.qkvw), (out_lora, self.dense_w)]\n        return ret\n\n    def set_lora_params(self):\n        \"\"\"\n        Necessary to implement for `HybridEngineContainer`\n        \"\"\"\n        if GPTNEOXLayerPolicy.version == 0:\n            attention = self.policy.client_module.attention\n        else:\n            attention = self.policy.client_module.self_attention\n\n        self.lora_params = [\n            maybe_get_lora(p) for p in [\n                self.policy.client_module.mlp.dense_h_to_4h, self.policy.client_module.mlp.dense_4h_to_h,\n                attention.query_key_value, attention.dense\n            ]\n        ]\n\n    def load_params(self, module, sd, weight_quantizer, mp_replace, prefix):\n        param_names = (\n            'attention.query_key_value.weight', \\\n            'attention.query_key_value.bias', \\\n            'attention.dense.weight', \\\n            'attention.dense.bias', \\\n            'mlp.dense_h_to_4h.weight', \\\n            'mlp.dense_h_to_4h.bias', \\\n            'mlp.dense_4h_to_h.weight', \\\n            'mlp.dense_4h_to_h.bias', \\\n            'post_attention_layernorm.weight', \\\n            'post_attention_layernorm.bias', \\\n            'input_layernorm.weight', \\\n            'input_layernorm.bias'\n        )\n        for i in range(0, 2):\n            maybe_copy(module.attention,\n                       sd,\n                       weight_quantizer,\n                       mp_replace,\n                       transformer_param_names[i],\n                       prefix + param_names[i],\n                       qkv=True,\n                       megatron_v2=self.policy.is_megatron_v2,\n                       split_qkv=self.policy.split_qkv,\n                       heads=self.policy.client_module.attention.num_attention_heads)\n        for i in range(2, 4):\n            maybe_copy(module.attention, sd, weight_quantizer, mp_replace, transformer_param_names[i],\n                       prefix + param_names[i])\n        for i in range(4, 10):\n            maybe_copy(module.mlp, sd, weight_quantizer, mp_replace, transformer_param_names[i],\n                       prefix + param_names[i])\n        for i in range(10, 12):\n            maybe_copy(module, sd, weight_quantizer, mp_replace, transformer_param_names[i], prefix + param_names[i])\n\n\nclass GPTNEOXLayerPolicy(TransformerPolicy):\n    _orig_layer_class = None\n    version = 0\n\n    def __init__(self, client_module, inference=True, megatron_v2=True, split_qkv=False):\n        super().__init__(inference, megatron_v2=megatron_v2, split_qkv=split_qkv)\n        self.client_module = client_module\n        if GPTNEOXLayerPolicy._orig_layer_class is None:\n            if pkg_version.parse(torch.__version__) <= pkg_version.parse(\"1.2\"):\n                GPTNEOXLayerPolicy._orig_layer_class = None\n            else:\n                try:\n                    from transformers import GPTNeoXLayer\n                    GPTNEOXLayerPolicy._orig_layer_class = GPTNeoXLayer\n                except ImportError:\n                    GPTNEOXLayerPolicy._orig_layer_class = None\n\n    def get_hidden_heads(self):\n        if GPTNEOXLayerPolicy.version == 0:\n            attention = self.client_module.attention\n        else:\n            attention = self.client_module.self_attention\n\n        return self.client_module.attention.hidden_size, \\\n                self.client_module.attention.num_attention_heads, \\\n                self.client_module.input_layernorm.eps, \\\n                DEFAULT_INTERMEDIATE_SIZE\n\n    def attention(self, enable_training=False):\n        if GPTNEOXLayerPolicy.version == 0:\n            attention = self.client_module.attention\n        else:\n            attention = self.client_module.self_attention\n\n        return attention.query_key_value.weight, \\\n               attention.query_key_value.bias, \\\n               attention.dense.weight, \\\n               attention.dense.bias\n\n    def mlp(self, enable_training=False):\n        return self.client_module.mlp.dense_h_to_4h.weight, \\\n               self.client_module.mlp.dense_h_to_4h.bias, \\\n               self.client_module.mlp.dense_4h_to_h.weight, \\\n               self.client_module.mlp.dense_4h_to_h.bias\n\n    def layernorm(self):\n        return self.client_module.post_attention_layernorm.weight, \\\n               self.client_module.post_attention_layernorm.bias, \\\n               self.client_module.input_layernorm.weight, \\\n               self.client_module.input_layernorm.bias\n", "deepspeed/module_inject/containers/llama.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .base import *\nfrom .features import HybridSplitQKVContainer, HybridGatedMLPContainer, MetaTensorContainer\nfrom deepspeed.utils.types import ActivationFuncType, NormType\nfrom deepspeed.model_implementations.transformers.ds_gpt import DeepSpeedGPTInference\nimport torch\nfrom torch.nn.parameter import Parameter\n\nfrom ..policy import (\n    TransformerPolicy,\n    transformer_param_names,\n    maybe_copy,\n    maybe_copy_qkv,\n    maybe_copy_geglu,\n    maybe_get_lora,\n)\n\n\nclass DS_LLAMAContainer(MetaTensorContainer, HybridGatedMLPContainer, HybridSplitQKVContainer,\n                        BaseTransformerContainer):\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n        # All model specific things should be defined here instead of the base class.\n\n    def create_module(self, config=None):\n        _config = config if config is not None else self.ds_model_config\n\n        _config.rotate_half = True\n        _config.rotate_every_two = False\n        _config.rotary_dim = self.hidden_size // self.num_attention_heads\n        _config.rope_theta = self.policy.client_module.self_attn.rope_theta\n        self.module = DeepSpeedGPTInference(_config, mp_group=self.mp_group)\n\n        return self.module\n\n    def set_lora_params(self):\n        \"\"\"\n        Necessary to implement for `HybridEngineContainer`\n        \"\"\"\n        self.lora_params = [\n            maybe_get_lora(p) for p in [\n                self.policy.client_module.mlp.up_proj.weight, self.policy.client_module.mlp.gate_proj.weight,\n                self.policy.client_module.mlp.down_proj.weight, self.policy.client_module.self_attn.q_proj.weight,\n                self.policy.client_module.self_attn.k_proj.weight, self.policy.client_module.self_attn.v_proj.weight,\n                self.policy.client_module.self_attn.o_proj.weight\n            ]\n        ]\n\n    def get_lora_matched_pair(self):\n        up_proj_lora, gate_proj_lora, down_proj_lora, q_lora, k_lora, v_lora, out_lora = self.get_lora_params()\n        ret = [(up_proj_lora, self.inter_up_w), (gate_proj_lora, self.inter_gate_w), (down_proj_lora, self._4hh_w),\n               (out_lora, self.dense_w), (q_lora, self.qw), (k_lora, self.kw), (v_lora, self.vw)]\n        return ret\n\n    def set_q_k_v(self):\n        \"\"\"\n        Necessary to implement for `HybridSplitQKVContainer`\n        \"\"\"\n        self.qw = self.policy.client_module.self_attn.q_proj.weight\n        self.qb = None\n        self.kw = self.policy.client_module.self_attn.k_proj.weight\n        self.kb = None\n        self.vw = self.policy.client_module.self_attn.v_proj.weight\n        self.vb = None\n\n    def set_mlp_gate(self):\n        \"\"\"\n        Necessary to implement for `HybridGatedMLPContainer`\n        \"\"\"\n        self.inter_up_w = self.policy.client_module.mlp.up_proj.weight\n        self.inter_up_b = None\n        self.inter_gate_w = self.policy.client_module.mlp.gate_proj.weight\n        self.inter_gate_b = None\n\n    def load_params(self, module, sd, weight_quantizer, mp_replace, prefix):\n        param_names = (\n            'self_attn.q_proj.weight', \\\n            'self_attn.k_proj.weight', \\\n            'self_attn.v_proj.weight', \\\n            'self_attn.o_proj.weight', \\\n            'mlp.up_proj.weight', \\\n            'mlp.gate_proj.weight', \\\n            'mlp.down_proj.weight', \\\n            'post_attention_layernorm.weight', \\\n            'input_layernorm.weight',\n        )\n\n        maybe_copy_qkv(module.attention,\n                       sd,\n                       weight_quantizer,\n                       mp_replace,\n                       'attn_qkvw', [prefix + param_names[0], prefix + param_names[1], prefix + param_names[2]],\n                       split_qkv=self.policy.split_qkv)\n        for i in range(3, 4):\n            maybe_copy(module.attention, sd, weight_quantizer, mp_replace, transformer_param_names[i - 1],\n                       prefix + param_names[i])\n        maybe_copy_geglu(module.mlp, sd, weight_quantizer, mp_replace, 'inter_w',\n                         [prefix + param_names[4], prefix + param_names[5]])\n        maybe_copy(module.mlp, sd, weight_quantizer, mp_replace, 'output_w', prefix + param_names[6])\n\n        maybe_copy(module.mlp, sd, weight_quantizer, mp_replace, transformer_param_names[8], prefix + param_names[7])\n        maybe_copy(module, sd, weight_quantizer, mp_replace, transformer_param_names[10], prefix + param_names[8])\n\n        # This line is necessary for proper output when kernels + meta tensors are used in Llama models\n        # TODO: Investigate root-cause and fix meta tensor loading\n        module.mlp.output_b = None\n\n\nclass LLAMALayerPolicy(TransformerPolicy):\n\n    def __init__(self, client_module, inference=True):\n        super().__init__(\n            inference,\n            mlp_act_func_type=ActivationFuncType.GATED_SILU,\n            norm_type=NormType.RMSNorm,\n        )\n        self.client_module = client_module\n        try:\n            import transformers\n            LLAMALayerPolicy._orig_layer_class = transformers.models.llama.modeling_llama.LlamaDecoderLayer  # type: ignore\n        except:\n            LLAMALayerPolicy._orig_layer_class = None\n\n    def get_hidden_heads(self):\n        hidden_heads = (\n            getattr(self.client_module.self_attn.q_proj.weight, \"ds_shape\",\n                    self.client_module.self_attn.q_proj.weight.shape)[1],\n            self.client_module.self_attn.num_heads,\n            self.client_module.input_layernorm.variance_epsilon,\n            getattr(self.client_module.mlp.gate_proj.weight, \"ds_shape\",\n                    self.client_module.mlp.gate_proj.weight.shape)[0],\n        )\n        return hidden_heads\n\n    def attention(self, enable_training=False):\n        qw = self.client_module.self_attn.q_proj.weight\n        kw = self.client_module.self_attn.k_proj.weight\n        vw = self.client_module.self_attn.v_proj.weight\n\n        qkvw = Parameter(torch.cat((qw, kw, vw), dim=0), requires_grad=enable_training)\n\n        return qkvw, \\\n                None, \\\n                self.client_module.self_attn.o_proj.weight, \\\n                None\n\n    def mlp(self, enable_training=False):\n        mlp1_up = self.client_module.mlp.up_proj.weight\n        mlp1_gate = self.client_module.mlp.gate_proj.weight\n        mlp2 = self.client_module.mlp.down_proj.weight\n\n        mlp1 = Parameter(torch.cat((mlp1_up, mlp1_gate), dim=0), requires_grad=enable_training)\n\n        return mlp1, None, mlp2, None\n\n    def layernorm(self):\n        return self.client_module.post_attention_layernorm.weight, \\\n               None, \\\n               self.client_module.input_layernorm.weight, \\\n               None\n", "deepspeed/module_inject/containers/megatron_gpt.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .base import *\nfrom .features.megatron import MegatronContainer\nfrom deepspeed.model_implementations.transformers.ds_megatron_gpt import DeepSpeedMegatronGPTInference\nimport torch\nfrom ..policy import TransformerPolicy\nfrom packaging import version as pkg_version\n\n\nclass DS_MegatronGPTContainer(MegatronContainer, BaseTransformerContainer):\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n        # All model specific things should be defined here instead of the base class.\n\n    def create_module(self, config=None):\n        _config = config if config is not None else self.ds_model_config\n        self.module = DeepSpeedMegatronGPTInference(_config, mp_group=self.mp_group)\n        self.module.config.scale_attention = self.scale_attention\n\n        if self.megatron_v2:\n            self.module.config.rotate_half = True\n            self.module.config.rotate_every_two = False\n\n        return self.module\n\n\n# TODO: Megatron GPT MoE inherits from Megatron policy and replaces mlp\n# TODO: Generalize MoE overall goal, expand beyond Megatron\nclass MegatronLayerPolicy(TransformerPolicy):\n    _orig_layer_class = None\n    version = 0\n    moe_type = 'standard'\n    megatron_v2 = True\n    use_mup = False\n\n    def __init__(self, client_module, inference=True):\n        super().__init__(inference, megatron_v2=MegatronLayerPolicy.megatron_v2, use_mup=MegatronLayerPolicy.use_mup)\n        self.client_module = client_module\n        # we use megatron version to differentiate between the old and new\n        # megatron-lm source code\n        if MegatronLayerPolicy._orig_layer_class is None:\n            if pkg_version.parse(torch.__version__) <= pkg_version.parse(\"1.2\"):\n                MegatronLayerPolicy._orig_layer_class = None\n            else:\n                try:\n                    from megatron.model.transformer import ParallelTransformerLayer\n                    MegatronLayerPolicy._orig_layer_class = ParallelTransformerLayer\n                    MegatronLayerPolicy.version = 1\n                except ImportError:\n                    MegatronLayerPolicy._orig_layer_class = None\n\n    def get_hidden_heads(self):\n        if MegatronLayerPolicy.version == 0:\n            return self.client_module.attention.query_key_value.weight.shape[1], \\\n                    self.client_module.attention.num_attention_heads, \\\n                    self.client_module.input_layernorm.eps, \\\n                    DEFAULT_INTERMEDIATE_SIZE\n        else:\n            return self.client_module.self_attention.query_key_value.weight.shape[1], \\\n                    self.client_module.self_attention.num_attention_heads, \\\n                    self.client_module.input_layernorm.eps, \\\n                    DEFAULT_INTERMEDIATE_SIZE\n\n    def attention(self, enable_training=False):\n        if self.inference:\n            if MegatronLayerPolicy.version == 0:\n                attention = self.client_module.attention\n            else:\n                attention = self.client_module.self_attention\n\n        return attention.query_key_value.weight, \\\n               attention.query_key_value.bias, \\\n               attention.dense.weight, \\\n               attention.dense.bias\n\n    def mlp(self, moe_type='standard', enable_training=False):\n        from deepspeed.moe.utils import has_moe_layers\n        moe, _ = has_moe_layers(self.client_module)\n\n        if moe:\n            moe_experts = self.client_module.mlp.deepspeed_moe.experts.deepspeed_experts if moe_type == 'standard' else \\\n                            self.client_module.mlp.moe.deepspeed_moe.experts.deepspeed_experts\n            num_experts = len(moe_experts)\n            if moe_type == 'standard':\n                return [moe_experts[i].dense_h_to_4h.weight for i in range(num_experts)], \\\n                       [moe_experts[i].dense_h_to_4h.bias for i in range(num_experts)], \\\n                       [moe_experts[i].dense_4h_to_h.weight for i in range(num_experts)], \\\n                       [moe_experts[i].dense_4h_to_h.bias for i in range(num_experts)]\n            else:\n\n                return [moe_experts[i].dense_h_to_4h.weight for i in range(num_experts)], \\\n                       [moe_experts[i].dense_h_to_4h.bias for i in range(num_experts)], \\\n                       [moe_experts[i].dense_4h_to_h.weight for i in range(num_experts)], \\\n                       [moe_experts[i].dense_4h_to_h.bias for i in range(num_experts)], \\\n                       self.client_module.mlp.mlp.dense_h_to_4h.weight, \\\n                       self.client_module.mlp.mlp.dense_h_to_4h.bias, \\\n                       self.client_module.mlp.mlp.dense_4h_to_h.weight, \\\n                       self.client_module.mlp.mlp.dense_4h_to_h.bias, \\\n                       self.client_module.mlp.coefficient.weight\n\n        else:\n            return self.client_module.mlp.dense_h_to_4h.weight, \\\n                   self.client_module.mlp.dense_h_to_4h.bias, \\\n                   self.client_module.mlp.dense_4h_to_h.weight, \\\n                   self.client_module.mlp.dense_4h_to_h.bias\n\n    def layernorm(self):\n        return self.client_module.post_attention_layernorm.weight, \\\n               self.client_module.post_attention_layernorm.bias, \\\n               self.client_module.input_layernorm.weight, \\\n               self.client_module.input_layernorm.bias\n", "deepspeed/module_inject/containers/llama2.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .base import *\nfrom .features import HybridSplitQKVContainer, HybridGatedMLPContainer, MetaTensorContainer\nfrom deepspeed.utils.types import ActivationFuncType, NormType\nfrom deepspeed.model_implementations.transformers.ds_llama2 import DeepSpeedLlama2Inference\nimport torch\nfrom torch.nn.parameter import Parameter\n\nfrom ..policy import (\n    TransformerPolicy,\n    transformer_param_names,\n    maybe_copy,\n    maybe_copy_qkv,\n    maybe_copy_geglu,\n    maybe_get_lora,\n)\n\n\nclass DS_LLAMA2Container(MetaTensorContainer, HybridGatedMLPContainer, HybridSplitQKVContainer,\n                         BaseTransformerContainer):\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n        # All model specific things should be defined here instead of the base class.\n\n    def create_module(self, config=None):\n        _config = config if config is not None else self.ds_model_config\n\n        _config.rotate_half = False\n        _config.rotate_every_two = True\n        _config.rotary_dim = self.hidden_size // self.num_attention_heads\n        _config.num_kv = self.policy.client_module.attention.n_kv_heads\n        self.module = DeepSpeedLlama2Inference(_config, mp_group=self.mp_group)\n\n        return self.module\n\n    def set_lora_params(self):\n        \"\"\"\n        Necessary to implement for `HybridEngineContainer`\n        \"\"\"\n        self.lora_params = [\n            maybe_get_lora(p) for p in [\n                self.policy.client_module.feed_forward.w3.weight, self.policy.client_module.feed_forward.w1.weight,\n                self.policy.client_module.feed_forward.w2.weight, self.policy.client_module.attention.wq.weight,\n                self.policy.client_module.attention.wk.weight, self.policy.client_module.attention.wv.weight,\n                self.policy.client_module.attention.wo.weight\n            ]\n        ]\n\n    def get_lora_matched_pair(self):\n        up_proj_lora, gate_proj_lora, down_proj_lora, q_lora, k_lora, v_lora, out_lora = self.get_lora_params()\n        ret = [(up_proj_lora, self.inter_up_w), (gate_proj_lora, self.inter_gate_w), (down_proj_lora, self._4hh_w),\n               (out_lora, self.dense_w), (q_lora, self.qw), (k_lora, self.kw), (v_lora, self.vw)]\n        return ret\n\n    def set_q_k_v(self):\n        \"\"\"\n        Necessary to implement for `HybridSplitQKVContainer`\n        \"\"\"\n        self.qw = self.policy.client_module.attention.wq.weight\n        self.qb = None\n        self.kw = self.policy.client_module.attention.wk.weight\n        self.kb = None\n        self.vw = self.policy.client_module.attention.wv.weight\n        self.vb = None\n\n    def set_mlp_gate(self):\n        \"\"\"\n        Necessary to implement for `HybridGatedMLPContainer`\n        \"\"\"\n        self.inter_up_w = self.policy.client_module.feed_forward.w2.weight\n        self.inter_up_b = None\n        self.inter_gate_w = self.policy.client_module.feed_forward.w1.weight\n        self.inter_gate_b = None\n\n    def load_params(self, module, sd, weight_quantizer, mp_replace, prefix):\n        param_names = (\n            'attention.wq.weight', \\\n            'attention.wk.weight', \\\n            'attention.wv.weight', \\\n            'attention.wo.weight', \\\n            'feed_forward.w3.weight', \\\n            'feed_forward.w1.weight', \\\n            'feed_forward.w2.weight', \\\n            'ffn_norm.weight', \\\n            'attention_norm.weight'\n        )\n\n        maybe_copy_qkv(module.attention,\n                       sd,\n                       weight_quantizer,\n                       mp_replace,\n                       'attn_qkvw', [prefix + param_names[0], prefix + param_names[1], prefix + param_names[2]],\n                       split_qkv=self.policy.split_qkv)\n        for i in range(3, 4):\n            maybe_copy(module.attention, sd, weight_quantizer, mp_replace, transformer_param_names[i - 1],\n                       prefix + param_names[i])\n        maybe_copy_geglu(module.mlp, sd, weight_quantizer, mp_replace, 'inter_w',\n                         [prefix + param_names[4], prefix + param_names[5]])\n        maybe_copy(module.mlp, sd, weight_quantizer, mp_replace, 'output_w', prefix + param_names[6])\n\n        maybe_copy(module.mlp, sd, weight_quantizer, mp_replace, transformer_param_names[8], prefix + param_names[7])\n        maybe_copy(module, sd, weight_quantizer, mp_replace, transformer_param_names[10], prefix + param_names[8])\n\n\nclass LLAMA2LayerPolicy(TransformerPolicy):\n\n    def __init__(self, client_module, inference=True):\n        super().__init__(\n            inference,\n            mlp_act_func_type=ActivationFuncType.GATED_SILU,\n            norm_type=NormType.RMSNorm,\n        )\n        self.client_module = client_module\n        try:\n            import llama\n            LLAMA2LayerPolicy._orig_layer_class = llama.model.TransformerBlock  # type: ignore\n        except:\n            LLAMA2LayerPolicy._orig_layer_class = None\n\n    def get_hidden_heads(self):\n        return self.client_module.attention.wq.weight.shape[1], \\\n                self.client_module.n_heads, \\\n                self.client_module.ffn_norm.eps, \\\n                (self.client_module.feed_forward.w1.weight.shape[0] * \\\n                    deepspeed.comm.get_world_size() if deepspeed.comm.is_initialized() else 1) # this is a hack to inject when model is already partitioned!\n\n    def attention(self, enable_training=False):\n        qw = self.client_module.attention.wq.weight\n        kw = self.client_module.attention.wk.weight\n        vw = self.client_module.attention.wv.weight\n\n        qkvw = Parameter(torch.cat((qw, kw, vw), dim=0), requires_grad=enable_training)\n\n        return qkvw, \\\n                None, \\\n                self.client_module.attention.wo.weight, \\\n                None\n\n    def mlp(self, enable_training=False):\n        mlp1_up = self.client_module.feed_forward.w3.weight\n        mlp1_gate = self.client_module.feed_forward.w1.weight\n        mlp2 = self.client_module.feed_forward.w2.weight\n\n        mlp1 = Parameter(torch.cat((mlp1_up, mlp1_gate), dim=0), requires_grad=enable_training)\n\n        return mlp1, None, mlp2, None\n\n    def layernorm(self):\n        return self.client_module.ffn_norm.weight, \\\n               None, \\\n               self.client_module.attention_norm.weight, \\\n               None\n", "deepspeed/module_inject/containers/base.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\n# Create a container object to save model-specific tensors using the policy file above.\nfrom abc import ABC\n\nimport torch\n\nimport deepspeed\nfrom deepspeed.ops.transformer.inference.config import DeepSpeedInferenceConfig\nfrom deepspeed.accelerator import get_accelerator\n\n# If the intermediate size attribute is set DEFAULT_INTERMEDIATE_SIZE\n# it is assumed the intermediate size is 4x the embedding dimension\nDEFAULT_INTERMEDIATE_SIZE = -1\n\n\nclass BaseConvolutionContainer(ABC):\n    # not implemented\n    def __init__(self):\n        pass\n\n\nclass BaseTransformerContainer(ABC):\n\n    def __init__(self, policy, config, model_config, layer_id, child):\n        self.policy = policy\n        self.config = config\n        self.model_config = model_config\n        self.layer_id = layer_id\n        self.child = child\n\n        self.megatron_v2 = self.policy.is_megatron_v2\n        self.scale_attention = self.policy.scale_attention\n        self.ckpt_load_enabled = False\n\n        # configuration for models. todo: can this be moved to a pydantic model config?\n        self.hidden_size = None\n        self.intermediate_size = None\n        self.num_attention_heads = None\n        self.mp_size = self.config.tensor_parallel.tp_size\n        self.pre_layer_norm = self.model_config.do_layer_norm_before if \\\n            hasattr(self.model_config, 'do_layer_norm_before') else self.policy.pre_attn_norm\n        self.dtype = self.config.dtype\n        self.attn_linear_layer = self.policy.linear_layer\n        self.mlp_linear_layer = self.policy.linear_layer\n        self.return_tuple = self.config.return_tuple\n        self.triangular_masking = True\n        self.local_attention = ((self.model_config.attention_layers[self.layer_id] == \"local\") if hasattr(\n            self.model_config, 'attention_layers') else False)\n        self.window_size = getattr(self.model_config, \"window_size\", 1)\n        self.mlp_act_func_type = self.policy.mlp_act_func_type\n        self.norm_type = self.policy.norm_type\n        self.training_mp_size = self.config.training_mp_size\n        self.bigscience_bloom = False\n        self.max_out_tokens = self.config.max_out_tokens\n        self.min_out_tokens = self.config.min_out_tokens\n        self.scale_attn_by_inverse_layer_idx = getattr(self.config, \"scale_attn_by_inverse_layer_idx\", False)\n        self.use_mup = self.policy.use_mup\n        self.return_single_tuple = False\n        self.rotary_dim = self.get_rotary_dim()\n        self.mlp_after_attn = (self.rotary_dim is None or self.rotary_dim < 0)\n\n        # Attention tensors\n        self.qkvw = None\n        self.qkvb = None\n        self.dense_w = None\n        self.dense_b = None\n        # MLP tensors\n        self._h4h_w = None\n        self._h4h_b = None\n        self._4hh_w = None\n        self._4hh_b = None\n        # LayerNorm tensors\n        self.attn_nw = None\n        self.attn_nb = None\n        self.input_nw = None\n        self.input_nb = None\n\n        self.mp_group = None\n        self.use_triton = False\n\n        # Triton\n        self.use_triton = config.use_triton and deepspeed.HAS_TRITON\n\n    def create_ds_model_config(self):\n        self.set_hidden_heads(*self.policy.get_hidden_heads())\n        assert self.num_attention_heads % self.mp_size == 0,\\\n                \"To run the model parallel across the GPUs, the attention_heads require to be divisible by the world_size!\" +\\\n                \"This is because the attention computation is partitioned evenly among the parallel GPUs.\"\n\n        self.ds_model_config = DeepSpeedInferenceConfig(\n            hidden_size=self.hidden_size,\n            intermediate_size=self.intermediate_size,\n            heads=self.num_attention_heads,\n            layer_norm_eps=self.layernorm_epsilon,\n            dtype=self.dtype,\n            pre_layer_norm=self.pre_layer_norm,\n            norm_type=self.norm_type,\n            mp_size=self.mp_size,\n            return_tuple=self.return_tuple,\n            triangular_masking=self.triangular_masking,\n            local_attention=self.local_attention,\n            window_size=self.window_size,\n            rotary_dim=self.rotary_dim,\n            mlp_after_attn=self.mlp_after_attn,\n            mlp_act_func_type=self.mlp_act_func_type,\n            training_mp_size=self.training_mp_size,\n            bigscience_bloom=self.bigscience_bloom,\n            max_out_tokens=self.max_out_tokens,\n            min_out_tokens=self.min_out_tokens,\n            scale_attn_by_inverse_layer_idx=self.scale_attn_by_inverse_layer_idx,\n            use_mup=self.use_mup,\n            return_single_tuple=self.return_single_tuple,\n            set_empty_params=self.config.set_empty_params,\n            transposed_mode=self.config.transposed_mode,\n            use_triton=self.use_triton,\n            triton_autotune=self.config.triton_autotune)\n\n        if self.use_triton and deepspeed.HAS_TRITON:\n            from .bert import DS_BERTContainer\n            if not isinstance(self, DS_BERTContainer):\n                raise NotImplementedError(\"Triton kernels are only for BERT-like models yet\")\n\n            if not self.config.triton_autotune:\n                from deepspeed.ops.transformer.inference.triton.matmul_ext import fp16_matmul\n                fp16_matmul.skip_autotune()\n\n        return self.ds_model_config\n\n    def check_meta_tensor_support(self):\n        if hasattr(self.qkvw, 'is_meta'):\n            if self.qkvw.is_meta:\n                assert self.ckpt_load_enabled, \"Meta tensors are not supported for this model currently.\"\n        else:\n            raise NotImplementedError(\"Meta tensor support is not available, please upgrade to torch 1.10+\")\n\n    def initialize_tensors(self, enable_training=False):\n        # Set the tensors from policy (user module) to container (DS module)\n        self.set_attention(*self.policy.attention(enable_training=enable_training))\n        self.set_mlp(*self.policy.mlp(enable_training=enable_training))\n        self.set_layernorm(*self.policy.layernorm())\n        #self.check_meta_tensor_support()\n\n    def convert_to_required_dtype(self):\n        # Note: converting tensors to fp16 requires that we do it in-place using self.__dict__ and not make a list/dict copy\n        if self.dtype in [torch.half, torch.bfloat16]:\n            for k, v in self.__dict__.items():\n                # The list comprehension is used for MoE tensor lists\n                if isinstance(v, list) and all((isinstance(tensor, torch.Tensor) \\\n                   or isinstance(tensor, torch.nn.Parameter)) for tensor in v):\n                    self.__dict__[k] = [moe_tensor.to(self.dtype) for moe_tensor in v]\n\n                if isinstance(v, torch.Tensor) or isinstance(v, torch.nn.Parameter):\n                    self.__dict__[k] = v.to(self.dtype)\n\n    def get_rotary_dim(self):\n        if hasattr(self.model_config, 'rotary_dim'):\n            return self.model_config.rotary_dim\n        if hasattr(self.child, 'attention') and hasattr(self.child.attention, 'rotary_ndims'):\n            return self.child.attention.rotary_ndims\n        return -1\n\n    def set_moe(self, moe=False):\n        self.moe = moe\n\n    def set_tensor_parallel_config(self, mp_size, mp_group):\n        self.mp_size = mp_size\n        self.mp_group = mp_group\n\n    def set_quantization_config(self, quantizer):\n        self.quantizer = quantizer\n\n    def set_hidden_heads(self, hidden_size, num_attention_heads, epsilon, intermediate_size):\n        \"\"\"\n        Args:\n            hidden_size: embedding dimension of the model\n            num_attention_heads: number of attention heads in the model\n            epsilon: epsilon value for layer norm (same value used for all norms)\n            intermediate_size: Size of MLP projection. If `DEFAULT_INTERMEDIATE_SIZE` is passed\n                it is assumed to be `4 * hidden_size`\n        \"\"\"\n        self.hidden_size = hidden_size\n        if intermediate_size == DEFAULT_INTERMEDIATE_SIZE:\n            self.intermediate_size = 4 * hidden_size\n        else:\n            self.intermediate_size = intermediate_size\n        self.num_attention_heads = num_attention_heads\n        self.layernorm_epsilon = epsilon\n\n    def set_attention(self, qkvw, qkvb, dense_w, dense_b):\n        self.qkvw = qkvw\n        self.qkvb = qkvb\n        self.dense_w = dense_w\n        self.dense_b = dense_b\n\n    def set_mlp(self, _h4h_w, _h4h_b, _4hh_w, _4hh_b):\n        self._h4h_w = _h4h_w\n        self._h4h_b = _h4h_b\n        self._4hh_w = _4hh_w\n        self._4hh_b = _4hh_b\n\n    def set_layernorm(self, attn_nw, attn_nb, input_nw, input_nb):\n        self.attn_nw = attn_nw\n        self.attn_nb = attn_nb\n        self.input_nw = input_nw\n        self.input_nb = input_nb\n\n    def apply_weight_quantization(self):\n        # quantize attention weights\n        self.attention_quantization()\n\n        # quantize mlp weights\n        self.mlp_quantization()\n\n    def attention_quantization(self):\n        self.module.attention.attn_qkvw = self.quantizer.quantize(self.module.attention.attn_qkvw)\n        self.module.attention.attn_ow = self.quantizer.quantize(self.module.attention.attn_ow)\n\n    def mlp_quantization(self):\n        self.module.mlp.inter_w = self.quantizer.quantize(self.module.mlp.inter_w)\n        self.module.mlp.output_w = self.quantizer.quantize(self.module.mlp.output_w)\n\n    def apply_tensor_parallelism(self, mp_replace):\n        # setup the new Attention module\n        self.attention_qkv_mp(mp_replace)\n        self.attention_o_mp(mp_replace)\n\n        # setup the new MLP module\n        self.mlp_inter_mp(mp_replace)\n        self.mlp_output_mp(mp_replace)\n\n        # Apply weight quantization\n        # TODO(cmikeh2): Re-enable this once verified\n        #self.apply_weight_quantization()\n\n    def attention_qkv_mp(self, mp_replace, reversed_dim=False):\n        self.module.attention.attn_qkvw = mp_replace.strided_copy(self.module.attention.attn_qkvw,\n                                                                  self.qkvw,\n                                                                  num_splits=3,\n                                                                  int8=reversed_dim)\n        self.module.attention.attn_qkvb = mp_replace.strided_copy(self.module.attention.attn_qkvb,\n                                                                  self.qkvb,\n                                                                  num_splits=3,\n                                                                  int8=reversed_dim)\n\n    def attention_o_mp(self, mp_replace, reversed_dim=False):\n        self.module.attention.attn_ow = mp_replace.copy(self.module.attention.attn_ow, self.dense_w, int8=reversed_dim)\n        self.module.attention.attn_ob = mp_replace.copy(self.module.attention.attn_ob,\n                                                        self.dense_b,\n                                                        int8=reversed_dim,\n                                                        allocate_tensor=reversed_dim)\n\n    def mlp_inter_mp(self, mp_replace, reversed_dim=False):\n        self.module.mlp.inter_w = mp_replace.copy(self.module.mlp.inter_w, self._h4h_w, int8=reversed_dim)\n        self.module.mlp.inter_b = mp_replace.copy(self.module.mlp.inter_b, self._h4h_b, int8=reversed_dim)\n\n    def mlp_output_mp(self, mp_replace, reversed_dim=False):\n        self.module.mlp.output_w = mp_replace.copy(self.module.mlp.output_w, self._4hh_w, int8=reversed_dim)\n        self.module.mlp.output_b = mp_replace.copy(self.module.mlp.output_b,\n                                                   self._4hh_b,\n                                                   int8=reversed_dim,\n                                                   allocate_tensor=reversed_dim)\n\n    def copy_data_to_new_module(self):\n        params = {'attn_nw': self.attn_nw, 'attn_nb': self.attn_nb}\n        for key in params:\n            if params[key] is None:\n                setattr(self.module.mlp, key, None)\n            else:\n                setattr(self.module.mlp, key,\n                        torch.nn.parameter.Parameter(params[key].to(get_accelerator().current_device_name())))\n\n        params = {'norm_w': self.input_nw, 'norm_b': self.input_nb}\n        for key in params:\n            if params[key] is None:\n                setattr(self.module, key, None)\n            else:\n                setattr(self.module, key,\n                        torch.nn.parameter.Parameter(params[key].to(get_accelerator().current_device_name())))\n\n    def transpose(self):\n        self.transpose_attention()\n        self.transpose_mlp()\n\n    def transpose_attention(self):\n        if self.attn_linear_layer:\n            self.qkvw = self.transpose_impl(self.qkvw.data)\n            self.dense_w = self.transpose_impl(self.dense_w.data)\n\n    def transpose_mlp(self):\n        if self.mlp_linear_layer:\n            self._h4h_w = self.transpose_impl(self._h4h_w.data)\n            self._4hh_w = self.transpose_impl(self._4hh_w.data)\n\n    def transpose_impl(self, data):\n        data = data.contiguous()\n        data.reshape(-1).copy_(data.transpose(-1, -2).contiguous().reshape(-1))\n        data = data.reshape(data.shape[-1], data.shape[-2])\n        data.to(get_accelerator().current_device_name())\n        return data\n\n    def get_all_params(self):\n        params = [\n            self.attn_nw,\n            self.attn_nb,\n            self.input_nw,\n            self.input_nb,\n        ]\n\n        params.extend(self.get_attn_params())\n        params.extend(self.get_mlp_params())\n\n        return params\n\n    def get_attn_params(self):\n        return [self.qkvw, self.qkvb, self.dense_w, self.dense_b]\n\n    def get_mlp_params(self):\n        return [self._h4h_w, self._h4h_b, self._4hh_w, self._4hh_b]\n", "deepspeed/module_inject/containers/gptneo.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .base import *\nfrom .features.meta_tensor import MetaTensorContainer\nfrom .features.split_qkv import HybridSplitQKVContainer\nfrom deepspeed.model_implementations.transformers.ds_gpt import DeepSpeedGPTInference\nimport torch\nfrom torch.nn.parameter import Parameter\nfrom ..policy import TransformerPolicy\nfrom ..policy import transformer_param_names\nfrom ..policy import maybe_copy\nfrom ..policy import maybe_copy_qkv\n\nfrom ..policy import maybe_get_lora\n\n\nclass DS_GPTNEOContainer(MetaTensorContainer, HybridSplitQKVContainer, BaseTransformerContainer):\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n        # All model specific things should be defined here instead of the base class.\n\n    def create_module(self, config=None):\n        _config = config if config is not None else self.ds_model_config\n        self.module = DeepSpeedGPTInference(_config, mp_group=self.mp_group)\n        self.module.config.scale_attention = self.scale_attention\n        return self.module\n\n    def set_lora_params(self):\n        \"\"\"\n        Necessary to implement for `HybridEngineContainer`\n        \"\"\"\n        self.lora_params = [\n            maybe_get_lora(p) for p in [\n                self.policy.client_module.mlp.c_fc, self.policy.client_module.mlp.c_proj,\n                self.policy.client_module.attn.attention.q_proj, self.policy.client_module.attn.attention.k_proj,\n                self.policy.client_module.attn.attention.v_proj, self.policy.client_module.attn.attention.out_proj\n            ]\n        ]\n\n    def set_q_k_v(self):\n        \"\"\"\n        Necessary to implement for `HybridSplitQKVContainer`\n        \"\"\"\n        self.qw = self.policy.client_module.attn.attention.q_proj.weight\n        self.qb = None\n        self.kw = self.policy.client_module.attn.attention.k_proj.weight\n        self.kb = None\n        self.vw = self.policy.client_module.attn.attention.v_proj.weight\n        self.vb = None\n\n    def get_lora_matched_pair(self):\n        \"\"\"\n        Necessary to implement for `HybridEngineContainer`\n        \"\"\"\n        fc1_lora, fc2_lora, q_lora, k_lora, v_lora, out_lora = self.get_lora_params()\n        ret = [(fc1_lora, self._h4h_w), (fc2_lora, self._4hh_w), (out_lora, self.dense_w), (q_lora, self.qw),\n               (k_lora, self.kw), (v_lora, self.vw)]\n        return ret\n\n    def load_params(self, module, sd, weight_quantizer, mp_replace, prefix):\n        param_names = (\n            'attn.attention.q_proj.weight', \\\n            'attn.attention.k_proj.weight', \\\n            'attn.attention.v_proj.weight', \\\n            'attn.attention.out_proj.weight', \\\n            'attn.attention.out_proj.bias', \\\n            'mlp.c_fc.weight', \\\n            'mlp.c_fc.bias', \\\n            'mlp.c_proj.weight', \\\n            'mlp.c_proj.bias', \\\n            'ln_2.weight', \\\n            'ln_2.bias', \\\n            'ln_1.weight', \\\n            'ln_1.bias'\n        )\n        maybe_copy_qkv(module.attention,\n                       sd,\n                       weight_quantizer,\n                       mp_replace,\n                       'attn_qkvw', [prefix + param_names[0], prefix + param_names[1], prefix + param_names[2]],\n                       split_qkv=self.policy.split_qkv)\n        for i in range(3, 5):\n            maybe_copy(module.attention, sd, weight_quantizer, mp_replace, transformer_param_names[i - 1],\n                       prefix + param_names[i])\n        for i in range(5, 11):\n            maybe_copy(module.mlp, sd, weight_quantizer, mp_replace, transformer_param_names[i - 1],\n                       prefix + param_names[i])\n        for i in range(11, 13):\n            maybe_copy(module, sd, weight_quantizer, mp_replace, transformer_param_names[i - 1],\n                       prefix + param_names[i])\n\n\nclass HFGPTNEOLayerPolicy(TransformerPolicy):\n\n    def __init__(self, client_module, inference=True):\n        super().__init__(inference, scale_attention=False)\n        self.client_module = client_module\n        try:\n            import transformers\n            HFGPTNEOLayerPolicy._orig_layer_class = transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoBlock\n        except:\n            HFGPTNEOLayerPolicy._orig_layer_class = None\n\n    def get_hidden_heads(self):\n        return self.client_module.attn.attention.embed_dim, \\\n                self.client_module.attn.attention.num_heads, \\\n                self.client_module.ln_1.eps, \\\n                DEFAULT_INTERMEDIATE_SIZE\n\n    def get_q_k_v(self):\n        return self.client_module.attn.attention.q_proj.weight, \\\n               None, \\\n               self.client_module.attn.attention.k_proj.weight, \\\n               None, \\\n               self.client_module.attn.attention.v_proj.weight, \\\n               None\n\n    def attention(self, enable_training=False):\n        qw = self.client_module.attn.attention.q_proj.weight\n        kw = self.client_module.attn.attention.k_proj.weight\n        vw = self.client_module.attn.attention.v_proj.weight\n\n        qkvw = Parameter(torch.cat((qw, kw, vw), dim=0), requires_grad=enable_training)\n\n        return qkvw, \\\n               None, \\\n               self.client_module.attn.attention.out_proj.weight, \\\n               self.client_module.attn.attention.out_proj.bias\n\n    def mlp(self, enable_training=False):\n        return self.client_module.mlp.c_fc.weight, \\\n               self.client_module.mlp.c_fc.bias, \\\n               self.client_module.mlp.c_proj.weight, \\\n               self.client_module.mlp.c_proj.bias\n\n    def layernorm(self):\n        return self.client_module.ln_2.weight, \\\n               self.client_module.ln_2.bias, \\\n               self.client_module.ln_1.weight, \\\n               self.client_module.ln_1.bias\n", "deepspeed/module_inject/containers/vae.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom ..policy import DSPolicy\nfrom ...model_implementations.diffusers.vae import DSVAE\n\n\nclass VAEPolicy(DSPolicy):\n\n    def __init__(self):\n        super().__init__()\n        try:\n            import diffusers\n            if hasattr(diffusers.models, \"autoencoders\"):\n                # Diffusers >= 0.25.0\n                # Changes location to 'autoencoders' directory\n                self._orig_layer_class = diffusers.models.autoencoders.autoencoder_kl.AutoencoderKL\n            elif hasattr(diffusers.models.vae, \"AutoencoderKL\"):\n                # Diffusers < 0.12.0\n                self._orig_layer_class = diffusers.models.vae.AutoencoderKL\n            else:\n                # Diffusers >= 0.12.0 & < 0.25.0\n                # Changes location of AutoencoderKL\n                self._orig_layer_class = diffusers.models.autoencoder_kl.AutoencoderKL\n        except ImportError:\n            self._orig_layer_class = None\n\n    def match(self, module):\n        return isinstance(module, self._orig_layer_class)\n\n    def match_replaced(self, module):\n        return isinstance(module, DSVAE)\n\n    def apply(self, module, enable_cuda_graph=True):\n        # TODO(cmikeh2): Enable cuda graph should be an inference configuration\n        return DSVAE(module, enable_cuda_graph=enable_cuda_graph)\n\n    # NOTE (lekurile): Should we have a diffusers policy class?\n    def attention(self, client_module):\n        pass\n", "deepspeed/module_inject/containers/internlm.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport importlib\n\nimport torch\nfrom torch.nn.parameter import Parameter\n\nfrom deepspeed.model_implementations.transformers.ds_gpt import DeepSpeedGPTInference\nfrom deepspeed.utils.types import ActivationFuncType, NormType\n\nfrom ..policy import (TransformerPolicy, maybe_copy, maybe_copy_geglu, maybe_copy_qkv, maybe_get_lora,\n                      transformer_param_names)\nfrom .base import *\nfrom .features import HybridGatedMLPContainer, HybridSplitQKVContainer\n\n\nclass DS_InternLMContainer(HybridGatedMLPContainer, HybridSplitQKVContainer, BaseTransformerContainer):\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n        # All model specific things should be defined here instead of the base class.\n\n    def create_module(self, config=None):\n        _config = config if config is not None else self.ds_model_config\n\n        _config.rotate_half = True\n        _config.rotate_every_two = False\n        _config.rotary_dim = self.hidden_size // self.num_attention_heads\n        self.module = DeepSpeedGPTInference(_config, mp_group=self.mp_group)\n\n        return self.module\n\n    def set_lora_params(self):\n        \"\"\"\n        Necessary to implement for `HybridEngineContainer`\n        \"\"\"\n        self.lora_params = [\n            maybe_get_lora(p) for p in [\n                self.policy.client_module.mlp.up_proj.weight, self.policy.client_module.mlp.gate_proj.weight,\n                self.policy.client_module.mlp.down_proj.weight, self.policy.client_module.self_attn.q_proj.weight,\n                self.policy.client_module.self_attn.k_proj.weight, self.policy.client_module.self_attn.v_proj.weight,\n                self.policy.client_module.self_attn.o_proj.weight\n            ]\n        ]\n\n    def get_lora_matched_pair(self):\n        up_proj_lora, gate_proj_lora, down_proj_lora, q_lora, k_lora, v_lora, out_lora = self.get_lora_params()\n        ret = [(up_proj_lora, self.inter_up_w), (gate_proj_lora, self.inter_gate_w), (down_proj_lora, self._4hh_w),\n               (out_lora, self.dense_w), (q_lora, self.qw), (k_lora, self.kw), (v_lora, self.vw)]\n        return ret\n\n    def set_q_k_v(self):\n        \"\"\"\n        Necessary to implement for `HybridSplitQKVContainer`\n        \"\"\"\n        self.qw = self.policy.client_module.self_attn.q_proj.weight\n        self.qb = self.policy.client_module.self_attn.q_proj.bias\n        self.kw = self.policy.client_module.self_attn.k_proj.weight\n        self.kb = self.policy.client_module.self_attn.k_proj.bias\n        self.vw = self.policy.client_module.self_attn.v_proj.weight\n        self.vb = self.policy.client_module.self_attn.v_proj.bias\n\n    def set_mlp_gate(self):\n        \"\"\"\n        Necessary to implement for `HybridGatedMLPContainer`\n        \"\"\"\n        self.inter_up_w = self.policy.client_module.mlp.up_proj.weight\n        self.inter_up_b = None\n        self.inter_gate_w = self.policy.client_module.mlp.gate_proj.weight\n        self.inter_gate_b = None\n\n    def load_params(self, module, sd, weight_quantizer, mp_replace, prefix):\n        param_names = (\n            'self_attn.q_proj.weight', \\\n            'self_attn.k_proj.weight', \\\n            'self_attn.v_proj.weight', \\\n            'self_attn.o_proj.weight', \\\n            'mlp.up_proj.weight', \\\n            'mlp.gate_proj.weight', \\\n            'mlp.down_proj.weight', \\\n            'input_layernorm.weight', \\\n            'post_attention_layernorm.weight'\n            'self_attn.q_proj.bias', \\\n            'self_attn.k_proj.bias', \\\n            'self_attn.v_proj.bias', \\\n            'self_attn.o_proj.bias', \\\n        )\n\n        maybe_copy_qkv(module.attention,\n                       sd,\n                       weight_quantizer,\n                       mp_replace,\n                       'attn_qkvw', [prefix + param_names[0], prefix + param_names[1], prefix + param_names[2]],\n                       split_qkv=self.policy.split_qkv)\n        maybe_copy_qkv(module.attention,\n                       sd,\n                       weight_quantizer,\n                       mp_replace,\n                       'attn_qkvb', [prefix + param_names[9], prefix + param_names[10], prefix + param_names[11]],\n                       split_qkv=self.policy.split_qkv)\n        maybe_copy(module.attention, sd, weight_quantizer, mp_replace, transformer_param_names[2],\n                   prefix + param_names[3])\n        maybe_copy(module.attention, sd, weight_quantizer, mp_replace, transformer_param_names[3],\n                   prefix + param_names[12])\n        maybe_copy_geglu(module.mlp, sd, weight_quantizer, mp_replace, 'inter_w',\n                         [prefix + param_names[4], prefix + param_names[5]])\n        maybe_copy(module.mlp, sd, weight_quantizer, mp_replace, 'output_w', prefix + param_names[6])\n\n        maybe_copy(module.mlp, sd, weight_quantizer, mp_replace, transformer_param_names[8], prefix + param_names[7])\n        maybe_copy(module, sd, weight_quantizer, mp_replace, transformer_param_names[10], prefix + param_names[8])\n\n\nclass InternLMLayerPolicy(TransformerPolicy):\n    _orig_layer_class = []\n    _orig_layer_class_inited = False\n\n    def __init__(self, client_module, inference=True):\n        super().__init__(\n            inference,\n            mlp_act_func_type=ActivationFuncType.GATED_SILU,\n            norm_type=NormType.RMSNorm,\n        )\n        self.client_module = client_module\n\n        self._init_orig_layer_class_once()\n\n    def _init_orig_layer_class_once(self):\n        if InternLMLayerPolicy._orig_layer_class_inited:\n            return\n\n        for sub_pkg in ['', '.internlm-7b', '.internlm-chat-7b']:\n            try:\n                from transformers.utils import TRANSFORMERS_DYNAMIC_MODULE_NAME\n                module = importlib.import_module(f\"{TRANSFORMERS_DYNAMIC_MODULE_NAME}{sub_pkg}.modeling_internlm\")\n                if module.InternLMDecoderLayer not in InternLMLayerPolicy._orig_layer_class:\n                    InternLMLayerPolicy._orig_layer_class.append(module.InternLMDecoderLayer)\n            except ImportError:\n                continue\n\n        InternLMLayerPolicy._orig_layer_class_inited = True\n\n    def get_hidden_heads(self):\n        return self.client_module.self_attn.q_proj.weight.shape[1], \\\n                self.client_module.self_attn.num_heads, \\\n                self.client_module.input_layernorm.variance_epsilon, \\\n                self.client_module.mlp.gate_proj.weight.shape[0]\n\n    def attention(self, enable_training=False):\n        qw = self.client_module.self_attn.q_proj.weight\n        kw = self.client_module.self_attn.k_proj.weight\n        vw = self.client_module.self_attn.v_proj.weight\n        qb = self.client_module.self_attn.q_proj.bias\n        kb = self.client_module.self_attn.k_proj.bias\n        vb = self.client_module.self_attn.v_proj.bias\n\n        qkvw = Parameter(torch.cat((qw, kw, vw), dim=0), requires_grad=enable_training)\n        qkvb = Parameter(torch.cat((qb, kb, vb), dim=0), requires_grad=enable_training)\n\n        return qkvw, \\\n                qkvb, \\\n                self.client_module.self_attn.o_proj.weight, \\\n                self.client_module.self_attn.o_proj.bias\n\n    def mlp(self, enable_training=False):\n        mlp1_up = self.client_module.mlp.up_proj.weight\n        mlp1_gate = self.client_module.mlp.gate_proj.weight\n        mlp2 = self.client_module.mlp.down_proj.weight\n\n        mlp1 = Parameter(torch.cat((mlp1_up, mlp1_gate), dim=0), requires_grad=enable_training)\n\n        return mlp1, None, mlp2, None\n\n    def layernorm(self):\n        return self.client_module.post_attention_layernorm.weight, \\\n               None, \\\n               self.client_module.input_layernorm.weight, \\\n               None\n", "deepspeed/module_inject/containers/base_moe.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\n# Create a container object to save model-specific tensors using the policy file above.\nfrom .base import *\nfrom deepspeed import comm as dist\nimport deepspeed.ops.transformer as transformer_inference\nfrom deepspeed.accelerator import get_accelerator\n\n\nclass BaseTransformerMoEContainer(BaseTransformerContainer):\n\n    def __init__(self, **kwargs):\n        # Call the init function of the parent class to initialize the tensors and configs from parent class\n        super().__init__(**kwargs)\n\n        self.num_experts = self.policy.get_num_experts()\n        self.ep_world_size = dist.get_world_size()\n        self.local_ep_size = 1 if self.num_experts < self.ep_world_size else self.num_experts // self.ep_world_size\n\n        self.layer_norm_eps = self.config.layer_norm_eps if hasattr(self.config, 'layer_norm_eps') else 1e-12,\n\n        # MoE models will have a list of mlp related tensors\n        self._h4h_w = []\n        self._h4h_b = []\n        self._4hh_w = []\n        self._4hh_b = []\n\n        # Residual MoE needs extra parameters\n        self._res_h4h_w = None\n        self._res_h4h_b = None\n        self._res_4hh_w = None\n        self._res_4hh_b = None\n        self._res_coef = None\n\n    def create_ds_model_config(self):\n        self.set_hidden_heads(*self.policy.get_hidden_heads())\n        assert self.num_attention_heads % self.mp_size == 0,\\\n                \"To run the model parallel across the GPUs, the attention_heads require to be divisible by the world_size!\" +\\\n                \"This is because the attention computation is partitioned evenly among the parallel GPUs.\"\n\n        self.ds_model_config = transformer_inference.DeepSpeedMoEInferenceConfig(\n            hidden_size=self.hidden_size,\n            heads=self.num_attention_heads,\n            layer_norm_eps=self.layer_norm_eps,\n            fp16=self.fp16,\n            pre_layer_norm=self.pre_layer_norm,\n            mp_size=self.mp_size,\n            q_int8=self.quantize,\n            moe_experts=self.local_ep_size,\n            global_experts=self.num_experts,\n            mlp_type=self.config.moe.type,\n            scale_attn_by_inverse_layer_idx=self.scale_attn_by_inverse_layer_idx,\n        )\n\n        return self.ds_model_config\n\n    def initialize_tensors(self):\n        # Set the tensors from policy (user module) to container (DS module)\n        self.set_attention(*self.policy.attention())\n        self.set_mlp(self.config.moe.type)\n        self.set_layernorm(*self.policy.layernorm())\n\n    def set_mlp(self, config_moe_type):\n        if config_moe_type == 'standard':\n            self._h4h_w, self._h4h_b, \\\n            self._4hh_w, self._4hh_b = self.policy.mlp()\n        else:\n            self._h4h_w, self._h4h_b, self._4hh_w, \\\n            self._4hh_b, self._res_h4h_w, self._res_h4h_b, \\\n            self._res_4hh_w, self._res_4hh_b, \\\n            self._res_coef = self.policy.mlp(config_moe_type)\n\n    def transpose(self):\n        self.transpose_attention()\n        self.transpose_mlp()\n\n        if self.config.moe.type == 'residual':\n            self.transpose_residual()\n\n    def transpose_mlp(self):\n        self._h4h_w = [self.transpose_impl(moe_w1.data) for moe_w1 in self._h4h_w]\n        self._4hh_w = [self.transpose_impl(moe_w1.data) for moe_w1 in self._4hh_w]\n\n    def transpose_residual(self):\n        self._res_h4h_w.data = self.transpose_impl(self._res_h4h_w.data)\n        self._res_4hh_w.data = self.transpose_impl(self._res_4hh_w.data)\n        self._res_coef.data = self.transpose_impl(self._res_coef.data)\n\n    def apply_tensor_parallelism(self, mp_replace):\n        # setup the new Attention module\n        self.attention_qkv_mp(mp_replace)\n        self.attention_o_mp(mp_replace)\n\n        # quantize attention weights\n        self.attention_quantization()\n\n        # setup the new MLP module\n        self.mlp_mp()\n\n    def mlp_mp(self):\n        gpu_index = dist.get_rank()\n        for ep_index in range(self.local_ep_size):\n            # mlp inter\n            self.module.mlp[ep_index].inter_w.data = self._h4h_w[gpu_index * self.local_ep_size + ep_index].to(\n                get_accelerator().current_device_name())\n            self.module.mlp[ep_index].inter_b.data = self._h4h_b[gpu_index * self.local_ep_size + ep_index].to(\n                get_accelerator().current_device_name())\n\n            # mlp output\n            self.module.mlp[ep_index].output_w.data = self._4hh_w[gpu_index * self.local_ep_size + ep_index].to(\n                get_accelerator().current_device_name())\n            self.module.mlp[ep_index].output_b.data = self._4hh_b[gpu_index * self.local_ep_size + ep_index].to(\n                get_accelerator().current_device_name())\n\n    def copy_data_to_new_module(self):\n        self.module.attn_nw.data = self.attn_nw.to(get_accelerator().current_device_name())\n        self.module.attn_nb.data = self.attn_nb.to(get_accelerator().current_device_name())\n\n        self.module.norm_w.data.copy_(self.input_nw.to(get_accelerator().current_device_name()))\n        self.module.norm_b.data.copy_(self.input_nb.to(get_accelerator().current_device_name()))\n\n        if self.config.moe.type == 'residual':\n            self.module.res_mlp.inter_w.data = self._res_h4h_w.to(get_accelerator().current_device_name())\n            self.module.res_mlp.inter_b.data = self._res_h4h_b.to(get_accelerator().current_device_name())\n            self.module.res_mlp.output_w.data = self._res_4hh_w.to(get_accelerator().current_device_name())\n            self.module.res_mlp.output_b.data = self._res_4hh_b.to(get_accelerator().current_device_name())\n            self.module.res_coef.data = self._res_coef.to(get_accelerator().current_device_name())\n", "deepspeed/module_inject/containers/gptj.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .base import *\nfrom .features.meta_tensor import MetaTensorContainer\nfrom .features.split_qkv import HybridSplitQKVContainer\nfrom deepspeed.model_implementations.transformers.ds_gpt import DeepSpeedGPTInference\nimport torch\nfrom torch.nn.parameter import Parameter\nfrom ..policy import TransformerPolicy\nfrom ..policy import transformer_param_names\nfrom ..policy import maybe_copy\nfrom ..policy import maybe_copy_qkv\n\nfrom ..policy import maybe_get_lora\n\n\nclass DS_GPTJContainer(MetaTensorContainer, HybridSplitQKVContainer, BaseTransformerContainer):\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n        # All model specific things should be defined here instead of the base class.\n\n    def create_module(self, config=None):\n        _config = config if config is not None else self.ds_model_config\n        self.module = DeepSpeedGPTInference(_config, mp_group=self.mp_group)\n        self.module.config.scale_attention = self.scale_attention\n        return self.module\n\n    def set_lora_params(self):\n        \"\"\"\n        Necessary to implement for `HybridEngineContainer`\n        \"\"\"\n        self.lora_params = [\n            maybe_get_lora(p) for p in [\n                self.policy.client_module.mlp.fc_in, self.policy.client_module.mlp.fc_out,\n                self.policy.client_module.attn.q_proj, self.policy.client_module.attn.k_proj,\n                self.policy.client_module.attn.v_proj, self.policy.client_module.attn.out_proj\n            ]\n        ]\n\n    def get_lora_matched_pair(self):\n        fc1_lora, fc2_lora, q_lora, k_lora, v_lora, out_lora = self.get_lora_params()\n        ret = [(fc1_lora, self._h4h_w), (fc2_lora, self._4hh_w), (out_lora, self.dense_w), (q_lora, self.qw),\n               (k_lora, self.kw), (v_lora, self.vw)]\n        return ret\n\n    def set_q_k_v(self):\n        \"\"\"\n        Necessary to implement for `HybridSplitQKVContainer`\n        \"\"\"\n        self.qw = self.policy.client_module.attn.q_proj.weight\n        self.qb = None\n        self.kw = self.policy.client_module.attn.k_proj.weight\n        self.kb = None\n        self.vw = self.policy.client_module.attn.v_proj.weight\n        self.vb = None\n\n    def load_params(self, module, sd, weight_quantizer, mp_replace, prefix):\n        param_names = (\n            'attn.q_proj.weight', \\\n            'attn.k_proj.weight', \\\n            'attn.v_proj.weight', \\\n            'attn.out_proj.weight', \\\n            'mlp.fc_in.weight', \\\n            'mlp.fc_in.bias', \\\n            'mlp.fc_out.weight', \\\n            'mlp.fc_out.bias', \\\n            'ln_1.weight', \\\n            'ln_1.bias'\n        )\n        maybe_copy_qkv(module.attention,\n                       sd,\n                       weight_quantizer,\n                       mp_replace,\n                       'attn_qkvw', [prefix + param_names[0], prefix + param_names[1], prefix + param_names[2]],\n                       split_qkv=self.policy.split_qkv)\n        for i in range(3, 4):\n            maybe_copy(module.attention, sd, weight_quantizer, mp_replace, transformer_param_names[i - 1],\n                       prefix + param_names[i])\n        for i in range(4, 8):\n            maybe_copy(module.mlp, sd, weight_quantizer, mp_replace, transformer_param_names[i],\n                       prefix + param_names[i])\n        for i in range(8, 10):\n            maybe_copy(module, sd, weight_quantizer, mp_replace, transformer_param_names[i + 2],\n                       prefix + param_names[i])\n\n\nclass HFGPTJLayerPolicy(TransformerPolicy):\n    _orig_layer_class = None\n\n    def __init__(self, client_module, inference=True):\n        super().__init__(inference, scale_attention=True)\n        self.client_module = client_module\n        try:\n            import transformers\n            HFGPTJLayerPolicy._orig_layer_class = transformers.models.gptj.modeling_gptj.GPTJBlock\n        except:\n            HFGPTJLayerPolicy._orig_layer_class = None\n\n    def get_hidden_heads(self):\n        return self.client_module.attn.embed_dim, \\\n                self.client_module.attn.num_attention_heads, \\\n                self.client_module.ln_1.eps, \\\n                DEFAULT_INTERMEDIATE_SIZE\n\n    def attention(self, enable_training=False):\n        qw = self.client_module.attn.q_proj.weight\n        kw = self.client_module.attn.k_proj.weight\n        vw = self.client_module.attn.v_proj.weight\n\n        qkvw = Parameter(torch.cat((qw, kw, vw), dim=0), requires_grad=enable_training)\n\n        return qkvw, \\\n               None, \\\n               self.client_module.attn.out_proj.weight, \\\n               None,\n\n    def mlp(self, enable_training=False):\n        return self.client_module.mlp.fc_in.weight, \\\n               self.client_module.mlp.fc_in.bias, \\\n               self.client_module.mlp.fc_out.weight, \\\n               self.client_module.mlp.fc_out.bias\n\n    def layernorm(self):\n        return None, \\\n               None, \\\n               self.client_module.ln_1.weight, \\\n               self.client_module.ln_1.bias\n", "deepspeed/module_inject/containers/bert.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .base import *\nfrom deepspeed.model_implementations.transformers.ds_bert import DeepSpeedBERTInference\nimport torch\nfrom torch.nn.parameter import Parameter\nfrom ..policy import TransformerPolicy\n\n\nclass DS_BERTContainer(BaseTransformerContainer):\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n        # All model specific things should be defined here instead of the base class.\n        self.return_tuple = True\n        self.triangular_masking = False\n        self.use_triton = kwargs['config'].use_triton and deepspeed.HAS_TRITON\n\n    def create_module(self, config=None):\n        _config = config if config is not None else self.ds_model_config\n        self.module = DeepSpeedBERTInference(_config, mp_group=self.mp_group)\n        self.module.config.scale_attention = self.scale_attention\n        return self.module\n\n\nclass HFBertLayerPolicy(TransformerPolicy):\n\n    def __init__(self, client_module, inference=False):\n        super().__init__(inference, pre_attn_norm=False)\n        self.client_module = client_module\n        self.cuda_graph_supported = True\n\n        if HFBertLayerPolicy._orig_layer_class is None:\n            try:\n                import transformers\n                HFBertLayerPolicy._orig_layer_class = [\n                    transformers.models.bert.modeling_bert.BertLayer,\n                    transformers.models.roberta.modeling_roberta.RobertaLayer\n                ]\n            except:\n                HFBertLayerPolicy._orig_layer_class = None\n\n    def get_hidden_heads(self):\n        if self.pre_attn_norm:\n            attention_layernorm = self.client_module.PostAttentionLayerNorm\n        else:\n            attention_layernorm = self.client_module.attention.output.LayerNorm\n        return self.client_module.attention.self.query.weight.shape[1], \\\n                self.client_module.attention.self.num_attention_heads, \\\n                attention_layernorm.eps, \\\n                DEFAULT_INTERMEDIATE_SIZE\n\n    def attention(self, enable_training=False):\n        qw = self.client_module.attention.self.query.weight\n        qb = self.client_module.attention.self.query.bias\n        kw = self.client_module.attention.self.key.weight\n        kb = self.client_module.attention.self.key.bias\n        vw = self.client_module.attention.self.value.weight\n        vb = self.client_module.attention.self.value.bias\n\n        qkvw = Parameter(torch.cat((qw, kw, vw), dim=0), requires_grad=enable_training)\n        qkvb = Parameter(torch.cat((qb, kb, vb), dim=0), requires_grad=enable_training)\n\n        return qkvw, \\\n               qkvb, \\\n               self.client_module.attention.output.dense.weight, \\\n               self.client_module.attention.output.dense.bias, \\\n\n    def mlp(self, enable_training=False):\n        if self.pre_attn_norm:\n            intermediate_ff = self.client_module.intermediate.dense_act\n        else:\n            intermediate_ff = self.client_module.intermediate.dense\n\n        return intermediate_ff.weight, intermediate_ff.bias, \\\n            self.client_module.output.dense.weight, \\\n            self.client_module.output.dense.bias\n\n    def layernorm(self):\n        if self.pre_attn_norm:\n            attention_layernorm = self.client_module.PostAttentionLayerNorm\n            transformer_layernorm = self.client_module.PreAttentionLayerNorm\n        else:\n            attention_layernorm = self.client_module.attention.output.LayerNorm\n            transformer_layernorm = self.client_module.output.LayerNorm\n        return attention_layernorm.weight, \\\n               attention_layernorm.bias, \\\n               transformer_layernorm.weight, \\\n               transformer_layernorm.bias\n", "deepspeed/module_inject/containers/gpt2.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .base import *\nfrom deepspeed.model_implementations.transformers.ds_gpt import DeepSpeedGPTInference\nfrom ..policy import TransformerPolicy\n\n\nclass DS_GPT2Container(BaseTransformerContainer):\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n        # All model specific things should be defined here instead of the base class.\n\n    def create_module(self, config=None):\n        _config = config if config is not None else self.ds_model_config\n        self.module = DeepSpeedGPTInference(_config, mp_group=self.mp_group)\n        self.module.config.scale_attention = self.scale_attention\n        return self.module\n\n\nclass HFGPT2LayerPolicy(TransformerPolicy):\n    _orig_layer_class = None\n\n    def __init__(self, client_module, inference=True):\n        # HuggingFace GPT2 uses convolutional layer instead of linear layer\n        super().__init__(inference, linear_layer=False)\n        self.client_module = client_module\n        try:\n            import transformers\n            HFGPT2LayerPolicy._orig_layer_class = transformers.models.gpt2.modeling_gpt2.GPT2Block\n        except:\n            HFGPT2LayerPolicy._orig_layer_class = None\n\n    def get_hidden_heads(self):\n        return self.client_module.attn.embed_dim, \\\n                self.client_module.attn.num_heads, \\\n                self.client_module.ln_1.eps, \\\n                DEFAULT_INTERMEDIATE_SIZE\n\n    def attention(self, enable_training=False):\n        return  self.client_module.attn.c_attn.weight, \\\n                self.client_module.attn.c_attn.bias, \\\n                self.client_module.attn.c_proj.weight, \\\n                self.client_module.attn.c_proj.bias\n\n    def mlp(self, enable_training=False):\n        return self.client_module.mlp.c_fc.weight, \\\n               self.client_module.mlp.c_fc.bias, \\\n               self.client_module.mlp.c_proj.weight, \\\n               self.client_module.mlp.c_proj.bias\n\n    def layernorm(self):\n        return self.client_module.ln_2.weight, \\\n               self.client_module.ln_2.bias, \\\n               self.client_module.ln_1.weight, \\\n               self.client_module.ln_1.bias\n", "deepspeed/module_inject/containers/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .bert import DS_BERTContainer, HFBertLayerPolicy\nfrom .bloom import DS_BloomContainer, BLOOMLayerPolicy, supported_models\nfrom .distil_bert import DS_DistilBERTContainer, HFDistilBertLayerPolicy\nfrom .gpt2 import DS_GPT2Container, HFGPT2LayerPolicy\nfrom .gptj import DS_GPTJContainer, HFGPTJLayerPolicy\nfrom .gptneo import DS_GPTNEOContainer, HFGPTNEOLayerPolicy\nfrom .gptneox import DS_GPTNEOXContainer, GPTNEOXLayerPolicy\nfrom .llama import DS_LLAMAContainer, LLAMALayerPolicy\nfrom .llama2 import LLAMA2LayerPolicy, DS_LLAMA2Container\nfrom .internlm import DS_InternLMContainer, InternLMLayerPolicy\nfrom .megatron_gpt import DS_MegatronGPTContainer, MegatronLayerPolicy\nfrom .megatron_gpt_moe import DS_MegatronGPTMoEContainer, MegatronMoELayerPolicy\nfrom .opt import DS_OPTContainer, HFOPTLayerPolicy\nfrom .clip import DS_CLIPContainer, HFCLIPLayerPolicy\nfrom .unet import UNetPolicy\nfrom .vae import VAEPolicy\n", "deepspeed/module_inject/containers/distil_bert.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .base import *\nfrom deepspeed.model_implementations.transformers.ds_bert import DeepSpeedBERTInference\nimport torch\nfrom torch.nn.parameter import Parameter\nfrom ..policy import TransformerPolicy\n\n\nclass DS_DistilBERTContainer(BaseTransformerContainer):\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n        # All model specific things should be defined here instead of the base class.\n        self.triangular_masking = False\n        self.return_single_tuple = True\n        self.use_triton = kwargs['config'].use_triton and deepspeed.HAS_TRITON\n\n    def create_module(self, config=None):\n        _config = config if config is not None else self.ds_model_config\n        self.module = DeepSpeedBERTInference(_config, mp_group=self.mp_group)\n        self.module.config.scale_attention = self.scale_attention\n        return self.module\n\n\nclass HFDistilBertLayerPolicy(TransformerPolicy):\n    _orig_layer_class = None\n\n    def __init__(self, client_module, inference=False, preln=False):\n        super().__init__(inference)\n        self.client_module = client_module\n        self.preln = preln\n        self.cuda_graph_supported = True\n        if HFDistilBertLayerPolicy._orig_layer_class is None:\n            try:\n                import transformers\n                HFDistilBertLayerPolicy._orig_layer_class = [\n                    transformers.models.distilbert.modeling_distilbert.TransformerBlock,\n                ]\n            except:\n                HFDistilBertLayerPolicy._orig_layer_class = None\n\n    def get_hidden_heads(self):\n        return self.client_module.attention.q_lin.weight.shape[1], \\\n                self.client_module.attention.n_heads, \\\n                self.client_module.sa_layer_norm.eps, \\\n                DEFAULT_INTERMEDIATE_SIZE\n\n    def attention(self, enable_training=False):\n        qw = self.client_module.attention.q_lin.weight\n        qb = self.client_module.attention.q_lin.bias\n        kw = self.client_module.attention.k_lin.weight\n        kb = self.client_module.attention.k_lin.bias\n        vw = self.client_module.attention.v_lin.weight\n        vb = self.client_module.attention.v_lin.bias\n\n        qkvw = Parameter(torch.cat((qw, kw, vw), dim=0), requires_grad=enable_training)\n        qkvb = Parameter(torch.cat((qb, kb, vb), dim=0), requires_grad=enable_training)\n\n        return qkvw, \\\n               qkvb, \\\n               self.client_module.attention.out_lin.weight, \\\n               self.client_module.attention.out_lin.bias\n\n    def mlp(self, enable_training=False):\n        intermediate_ff = self.client_module.ffn.lin1\n\n        return intermediate_ff.weight, intermediate_ff.bias, \\\n            self.client_module.ffn.lin2.weight, \\\n            self.client_module.ffn.lin2.bias\n\n    def layernorm(self):\n        attention_layernorm = self.client_module.sa_layer_norm\n        transformer_layernorm = self.client_module.output_layer_norm\n        return attention_layernorm.weight, \\\n               attention_layernorm.bias, \\\n               transformer_layernorm.weight, \\\n               transformer_layernorm.bias\n", "deepspeed/module_inject/containers/unet.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nfrom torch.nn.parameter import Parameter\n\nfrom ..policy import DSPolicy\nfrom ...model_implementations.diffusers.unet import DSUNet\n\n\nclass UNetPolicy(DSPolicy):\n\n    def __init__(self):\n        super().__init__()\n        try:\n            import diffusers\n            self._orig_layer_class = diffusers.models.unet_2d_condition.UNet2DConditionModel\n        except AttributeError:\n            self._orig_layer_class = diffusers.models.unets.unet_2d_condition.UNet2DConditionModel\n        except ImportError:\n            self._orig_layer_class = None\n\n    def match(self, module):\n        return isinstance(module, self._orig_layer_class)\n\n    def match_replaced(self, module):\n        return isinstance(module, DSUNet)\n\n    def apply(self, module, enable_cuda_graph=True):\n        # TODO(cmikeh2): Enable cuda graph should be an inference configuration\n        return DSUNet(module, enable_cuda_graph=enable_cuda_graph)\n\n    def attention(self, client_module):\n        qw = client_module.to_q.weight\n        kw = client_module.to_k.weight\n        vw = client_module.to_v.weight\n\n        if qw.shape[1] == kw.shape[1]:\n            qkvw = Parameter(torch.cat((qw, kw, vw), dim=0), requires_grad=False)\n\n            return qkvw, \\\n                   client_module.to_out[0].weight, \\\n                   client_module.to_out[0].bias, \\\n                   qw.shape[-1], \\\n                   client_module.heads\n        else:\n            #return None\n            #kvw = Parameter(torch.cat((kw, vw), dim=0), requires_grad=False)\n            return qw, \\\n                   kw, vw, \\\n                   client_module.to_out[0].weight, \\\n                   client_module.to_out[0].bias, \\\n                   qw.shape[-1], \\\n                   client_module.heads\n", "deepspeed/module_inject/containers/clip.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .base import *\nfrom deepspeed.model_implementations.transformers.ds_gpt import DeepSpeedGPTInference\nimport torch\nfrom torch.nn.parameter import Parameter\nfrom ..policy import TransformerPolicy\n\n\nclass DS_CLIPContainer(BaseTransformerContainer):\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n        # All model specific things should be defined here instead of the base class.\n\n    def create_module(self, config=None):\n        _config = config if config is not None else self.ds_model_config\n        self.module = DeepSpeedGPTInference(_config, mp_group=self.mp_group)\n        self.module.config.scale_attention = self.scale_attention\n        return self.module\n\n\nclass HFCLIPLayerPolicy(TransformerPolicy):\n\n    def __init__(self, client_module, inference=False):\n        super().__init__(inference, pre_attn_norm=True, scale_attention=True)\n        self.client_module = client_module\n        self.cuda_graph_supported = True\n\n        if HFCLIPLayerPolicy._orig_layer_class is None:\n            try:\n                import transformers\n                HFCLIPLayerPolicy._orig_layer_class = transformers.models.clip.modeling_clip.CLIPEncoderLayer\n            except:\n                HFCLIPLayerPolicy._orig_layer_class = None\n\n    def get_hidden_heads(self):\n        return self.client_module.self_attn.q_proj.weight.shape[1], \\\n                self.client_module.self_attn.num_heads, \\\n                self.client_module.layer_norm1.eps, \\\n                DEFAULT_INTERMEDIATE_SIZE\n\n    def attention(self, enable_training=False):\n        qw = self.client_module.self_attn.q_proj.weight\n        qb = self.client_module.self_attn.q_proj.bias\n        kw = self.client_module.self_attn.k_proj.weight\n        kb = self.client_module.self_attn.k_proj.bias\n        vw = self.client_module.self_attn.v_proj.weight\n        vb = self.client_module.self_attn.v_proj.bias\n\n        qkvw = Parameter(torch.cat((qw, kw, vw), dim=0), requires_grad=enable_training)\n        qkvb = Parameter(torch.cat((qb, kb, vb), dim=0), requires_grad=enable_training)\n\n        return qkvw, \\\n               qkvb, \\\n               self.client_module.self_attn.out_proj.weight, \\\n               self.client_module.self_attn.out_proj.bias\n\n    def mlp(self, enable_training=False):\n        return self.client_module.mlp.fc1.weight, \\\n               self.client_module.mlp.fc1.bias, \\\n               self.client_module.mlp.fc2.weight, \\\n               self.client_module.mlp.fc2.bias\n\n    def layernorm(self):\n        return self.client_module.layer_norm2.weight, \\\n               self.client_module.layer_norm2.bias, \\\n               self.client_module.layer_norm1.weight, \\\n               self.client_module.layer_norm1.bias\n", "deepspeed/module_inject/containers/megatron_gpt_moe.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .base import *\nfrom .base_moe import *\nfrom .features.megatron import MegatronContainer\nfrom deepspeed.model_implementations.transformers.ds_megatron_gpt import DeepSpeedMegatronGPTInference\nimport torch\nfrom .megatron_gpt import MegatronLayerPolicy\nfrom packaging import version as pkg_version\n\n\nclass DS_MegatronGPTMoEContainer(MegatronContainer, BaseTransformerMoEContainer):\n\n    def __init__(self, policy, config, model_config, layer_id):\n        super().__init__(policy, config, model_config, layer_id)\n\n        # All model specific things should be defined here instead of the base class.\n\n    def create_module(self, config=None):\n        _config = config if config is not None else self.ds_model_config\n        self.module = DeepSpeedMegatronGPTInference(_config, mp_group=self.mp_group)\n        self.module.config.scale_attention = self.scale_attention\n\n        if self.megatron_v2:\n            self.module.config.rotate_half = True\n            self.module.config.rotate_every_two = False\n\n        return self.module\n\n\n# TODO: Megatron GPT MoE inherits from Megatron policy and replaces mlp\n# TODO: Generalize MoE overall goal, expand beyond Megatron\nclass MegatronMoELayerPolicy(MegatronLayerPolicy):\n    _orig_layer_class = None\n    version = 0\n    moe_type = 'standard'\n    num_experts = 1\n\n    def __init__(self, client_module, inference=True):\n        super().__init__(inference)\n        self.client_module = client_module\n        # we use megatron version to differentiate between the old and new\n        # megatron-lm source code\n        if MegatronMoELayerPolicy._orig_layer_class is None:\n            if pkg_version.parse(torch.__version__) <= pkg_version.parse(\"1.2\"):\n                MegatronMoELayerPolicy._orig_layer_class = None\n            else:\n                try:\n                    from megatron.model.transformer import ParallelTransformerLayer\n                    MegatronMoELayerPolicy._orig_layer_class = ParallelTransformerLayer\n                except ImportError:\n                    MegatronMoELayerPolicy._orig_layer_class = None\n\n    def get_num_experts(self):\n        return self.num_experts\n\n    def mlp(self, moe_type='standard', enable_training=False):\n        # for now, all of this is tightly coupled to megatron-deepspeed moe implementation\n        # todo: think and refactor this to be more general\n\n        #from deepspeed.moe.utils import has_moe_layers\n        #moe, _ = has_moe_layers(self.client_module)\n\n        moe_experts = self.client_module.mlp.deepspeed_moe.experts.deepspeed_experts if moe_type == 'standard' else \\\n                        self.client_module.mlp.moe.deepspeed_moe.experts.deepspeed_experts\n        num_experts = len(moe_experts)\n        self.num_experts = num_experts\n\n        if moe_type == 'standard':\n            return [moe_experts[i].dense_h_to_4h.weight for i in range(num_experts)], \\\n                    [moe_experts[i].dense_h_to_4h.bias for i in range(num_experts)], \\\n                    [moe_experts[i].dense_4h_to_h.weight for i in range(num_experts)], \\\n                    [moe_experts[i].dense_4h_to_h.bias for i in range(num_experts)]\n        else:\n            return [moe_experts[i].dense_h_to_4h.weight for i in range(num_experts)], \\\n                    [moe_experts[i].dense_h_to_4h.bias for i in range(num_experts)], \\\n                    [moe_experts[i].dense_4h_to_h.weight for i in range(num_experts)], \\\n                    [moe_experts[i].dense_4h_to_h.bias for i in range(num_experts)], \\\n                    self.client_module.mlp.mlp.dense_h_to_4h.weight, \\\n                    self.client_module.mlp.mlp.dense_h_to_4h.bias, \\\n                    self.client_module.mlp.mlp.dense_4h_to_h.weight, \\\n                    self.client_module.mlp.mlp.dense_4h_to_h.bias, \\\n                    self.client_module.mlp.coefficient.weight\n", "deepspeed/module_inject/containers/features/gated_mlp.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom abc import abstractmethod\n\nfrom .hybrid_engine import HybridEngineContainer\n\n\nclass HybridGatedMLPContainer(HybridEngineContainer):\n    \"\"\"\n    The HybridGatedMLPContainer supports models for which the first MLP layer\n    is represented with two separate weights, one for the activation function\n    and one for the gating function.\n    \"\"\"\n\n    def set_mlp(self, _h4h_w, _h4h_b, _4hh_w, _4hh_b):\n        super().set_mlp(_h4h_w, _h4h_b, _4hh_w, _4hh_b)\n        self.set_mlp_gate()\n\n    @abstractmethod\n    def set_mlp_gate(self):\n        \"\"\"\n        In `set_mlp_gate`, it is necessary to populate the following variables (where appropriate)\n        for the given model:\n            self.inter_up_w: inter up weight\n            self.inter_up_b: inter up bias\n            self.inter_gate_w: inter gate weight\n            self.inter_gate_b: inter gate bias\n        If the parameter does not exist in the original model, set the attribute to None.\n        \"\"\"\n        raise NotImplementedError(\"A set_mlp_gate() function must be defined in the model container \\\n                                    in order to set the unfused inter up and gate tensors.\")\n\n    def mlp_inter_mp(self, mp_replace, reversed_dim=False):\n        # Only need to alter behavior if we can't do the normal destructive copy\n        if self.module.mlp.inter_w is None:\n            params = [\n                (self.module.mlp.inter_up_w, self.inter_up_w),\n                (self.module.mlp.inter_up_b, self.inter_up_b),\n                (self.module.mlp.inter_gate_w, self.inter_gate_w),\n                (self.module.mlp.inter_gate_b, self.inter_gate_b),\n            ]\n            for dst, src in params:\n                dst = mp_replace.copy(dst[:self.inter_up_w.shape[0] // mp_replace.mp_size],\n                                      src,\n                                      int8=reversed_dim,\n                                      allocate_tensor=reversed_dim) if src is not None else None\n        else:\n            self.module.mlp.inter_w = mp_replace.strided_copy(self.module.mlp.inter_w,\n                                                              self._h4h_w,\n                                                              num_splits=2,\n                                                              int8=reversed_dim)\n            self.module.mlp.inter_b = mp_replace.strided_copy(self.module.mlp.inter_b,\n                                                              self._h4h_b,\n                                                              num_splits=2,\n                                                              int8=reversed_dim)\n\n    def release_mlp(self):\n        super().release_mlp()\n        gated_mlp_params = [\n            (self.module.mlp.inter_up_w, self.inter_up_w),\n            (self.module.mlp.inter_up_b, self.inter_up_b),\n            (self.module.mlp.inter_gate_w, self.inter_gate_w),\n            (self.module.mlp.inter_gate_b, self.inter_gate_b),\n        ]\n\n        self._release_params(gated_mlp_params)\n\n    def reset_mlp(self):\n        self._h4h_w.data[:self.inter_up_w.shape[0]] = self.inter_up_w.data\n        self._h4h_w.data[self.inter_up_w.shape[0]:] = self.inter_gate_w.data\n\n        if self.inter_up_b is not None:\n            self._h4h_b.data[:self.inter_up_b.shape[0]] = self.inter_up_b.data\n            self._h4h_b.data[self.inter_up_b.shape[0]:] = self.inter_gate_b.data\n\n        inter_data = [self.inter_up_w.data, self.inter_gate_w.data]\n        if self.inter_up_b is not None:\n            inter_data.extend([self.inter_up_b.data, self.inter_gate_b.data])\n\n        self.inter_up_w.data = self._h4h_w.data[:self.inter_up_w.shape[0]]\n        self.inter_gate_w.data = self._h4h_w.data[self.inter_up_w.shape[0]:]\n\n        if self.inter_up_b is not None:\n            self.inter_up_b.data = self._h4h_b.data[:self.inter_up_b.shape[0]]\n            self.inter_gate_b.data = self._h4h_b.data[self.inter_up_b.shape[0]:]\n\n        for data in inter_data:\n            del data\n\n    def set_mlp_params_wo_copy(self, Z3_enabled=False):\n        self.module.mlp.output_w = self._4hh_w\n        self.module.mlp.output_b = self._4hh_b\n\n        if not Z3_enabled:\n            # In initialize_tensors, we create a fused inter projection with the appropriate shape\n            # and copy the up projection and gate projection into it\n            self.module.mlp.inter_w = self._h4h_w\n            self.module.mlp.inter_b = self._h4h_b\n\n            self.inter_up_w.data = self._h4h_w[:self.inter_up_w.shape[0], :]\n            self.inter_gate_w.data = self._h4h_w[self.inter_up_w.shape[0]:, :]\n\n            if self.inter_up_b is not None:\n                self.inter_up_b.data = self._h4h_b[:self.inter_up_w.shape[0]] if self._h4h_b is not None else None\n                self.inter_gate_b.data = self._h4h_b[self.inter_up_w.shape[0]:] if self._h4h_b is not None else None\n        else:\n            self.module.mlp.inter_up_w = self.inter_up_w\n            self.module.mlp.inter_up_b = self.inter_up_b\n            self.module.mlp.inter_gate_w = self.inter_gate_w\n            self.module.mlp.inter_gate_b = self.inter_gate_b\n\n    def get_mlp_params(self):\n        params = super().get_mlp_params()\n        params.extend([self.inter_up_w, self.inter_up_b, self.inter_gate_w, self.inter_gate_b])\n        return params\n", "deepspeed/module_inject/containers/features/meta_tensor.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom abc import ABC, abstractmethod\nfrom packaging import version as pkg_version\nimport torch\n\n\nclass MetaTensorContainer(ABC):\n    \"\"\"\n    NOTE: If you are using this feature with a container that\n    also inherits from `HybridEngineContainer`, ensure that `MetaTensorContainer`\n    is inherited before `HybridEngineContainer` in the class definition.\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        if pkg_version.parse('1.10') > pkg_version.parse(torch.__version__):\n            raise NotImplementedError(\"Meta tensor support is not available, please upgrade to torch 1.10+\")\n        super().__init__(**kwargs)\n        self.is_meta = False\n        self.ckpt_load_enabled = True\n\n    def initialize_tensors(self, enable_training=False):\n        super().initialize_tensors(enable_training=enable_training)\n        self.is_meta = self.qkvw.is_meta\n\n    def apply_tensor_parallelism(self, mp_replace, **kwargs):\n        if self.is_meta:\n            if self.qkvb is None:\n                self.module.attention.attn_qkvb = None\n            if self.dense_b is None:\n                self.module.attention.attn_ob = None\n        else:\n            super().apply_tensor_parallelism(mp_replace, **kwargs)\n\n    def copy_data_to_new_module(self):\n        if self.is_meta:\n            if self.attn_nw is None:\n                self.module.mlp.attn_nw = self.attn_nw\n                self.module.mlp.attn_nb = self.attn_nb\n        else:\n            super().copy_data_to_new_module()\n\n    def transpose(self):\n        if not self.is_meta:\n            super().transpose()\n\n    @abstractmethod\n    def load_params(self, module, sd, weight_quantizer, mp_replace, prefix):\n        \"\"\"\n        Load all the transformer parameter from the checkpoint file (sd).\n        In addition to the parameter names, we require two\n        more parameters to help read the data correctly\n        from the checkpoint and split the qkv heads in the\n        right order:\n            1. `use_load_prefix` (Default: False): this specifies\n                whether we need to use the name of first abstraction\n                layer of the model for searching the parameter's name\n                in a checkpoint file. For more information of how this\n                is used please see\n                https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/module_inject/load_checkpoint.py\n            2. `split_qkv` (Default: True): we use this flag when splitting\n                the qkv parameter into heads. If it is False, it means the heads\n                of q, k, and v are stored together and needs to split in the\n                DeepSpeed-Inference API.\n        \"\"\"\n        raise NotImplementedError(\"A load_params() function must be defined in the model container \\\n                                  when inheriting the MetaTensorContainer feature\")\n", "deepspeed/module_inject/containers/features/megatron.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nfrom abc import ABC\n\n\nclass MegatronContainer(ABC):\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.megatron_v2 = self.policy.is_megatron_v2\n\n    def _align_qkv_transposed(self, x):\n        attention_head_size = x.shape[-1] // self.num_attention_heads\n        new_x_shape = x.size()[:-1] + (self.num_attention_heads, attention_head_size)\n        x_1 = x.view(*new_x_shape)\n        (q, k, v) = torch.split(x_1, (x_1.shape[-1] // 3), dim=(x_1.dim() - 1))\n        if len(q.shape) > 2:\n            return torch.cat((q.reshape(q.shape[0], -1), k.reshape(q.shape[0], -1), v.reshape(q.shape[0], -1)),\n                             dim=-1).reshape(x.shape)\n        else:\n            return torch.cat((q.reshape(-1), k.reshape(-1), v.reshape(-1)), dim=-1).reshape(x.shape)\n\n    def transpose(self):\n        super().transpose()\n        if self.megatron_v2:\n            self.qkvw = torch.nn.parameter.Parameter(self._align_qkv_transposed(self.qkvw).contiguous())\n            self.qkvb = torch.nn.parameter.Parameter(self._align_qkv_transposed(self.qkvb).contiguous())\n", "deepspeed/module_inject/containers/features/split_qkv.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom abc import abstractmethod\nimport torch\n\nfrom .hybrid_engine import HybridEngineContainer\n\n\nclass HybridSplitQKVContainer(HybridEngineContainer):\n\n    def set_attention(self, qkvw, qkvb, dense_w, dense_b):\n        super().set_attention(qkvw, qkvb, dense_w, dense_b)\n        self.set_q_k_v()\n\n    @abstractmethod\n    def set_q_k_v(self):\n        \"\"\"\n        In `set_q_k_v`, it is necessary to populate the following variables (where appropriate)\n        for the given model:\n            self.qw: q weight\n            self.qb: q bias\n            self.kw: k weight\n            self.kb: k bias\n            self.vw: v weight\n            self.vb: v bias\n        \"\"\"\n        raise NotImplementedError(\"A set_q_k_v() function must be defined in the model container \\\n                                    in order to set the unfused q, k, and v tensors.\")\n\n    def attention_qkv_mp(self, mp_replace, reversed_dim=False):\n        # Only need to alter\n        if self.module.attention.attn_qkvw is None:\n            params = [\n                (self.module.attention.attn_qw, self.qw),\n                (self.module.attention.attn_qb, self.qb),\n                (self.module.attention.attn_kw, self.kw),\n                (self.module.attention.attn_kb, self.kb),\n                (self.module.attention.attn_vw, self.vw),\n                (self.module.attention.attn_vb, self.vb),\n            ]\n            for dst, src in params:\n                dst = mp_replace.copy(\n                    dst[:self.qw.shape[0] // mp_replace.mp_size], src, int8=reversed_dim,\n                    allocate_tensor=reversed_dim) if src is not None else None\n        else:\n            super().attention_qkv_mp(mp_replace)\n\n    def release_qkv(self):\n        super().release_qkv()\n        split_qkv_params = [\n            (self.module.attention.attn_qw, self.qw),\n            (self.module.attention.attn_qb, self.qb),\n            (self.module.attention.attn_kw, self.kw),\n            (self.module.attention.attn_kb, self.kb),\n            (self.module.attention.attn_vw, self.vw),\n            (self.module.attention.attn_vb, self.vb),\n        ]\n\n        self._release_params(split_qkv_params)\n\n    def reset_qkv(self):\n        self.qkvw.data[:self.qw.shape[0]] = self.qw.data\n        self.qkvw.data[self.qw.shape[0]:2 * self.qw.shape[0]] = self.kw.data\n        self.qkvw.data[2 * self.qw.shape[0]:] = self.vw.data\n\n        qkv_data = [self.qw.data, self.kw.data, self.vw.data]\n\n        self.qw.data = self.qkvw.data[:self.qw.shape[0]]\n        self.kw.data = self.qkvw.data[self.qw.shape[0]:2 * self.qw.shape[0]]\n        self.vw.data = self.qkvw.data[2 * self.qw.shape[0]:]\n\n        if self.qkvb is not None:\n            self.qkvb.data[:self.qw.shape[0]] = self.qb.data\n            self.qkvb.data[self.qw.shape[0]:2 * self.qw.shape[0]] = self.kb.data\n            self.qkvb.data[2 * self.qw.shape[0]:] = self.vb.data\n\n            qkv_data.extend([self.qb.data, self.kb.data, self.vb.data])\n\n            self.qb.data = self.qkvb.data[:self.qw.shape[0]]\n            self.kb.data = self.qkvb.data[self.qw.shape[0]:2 * self.qw.shape[0]]\n            self.vb.data = self.qkvb.data[2 * self.qw.shape[0]:]\n\n        for data in qkv_data:\n            del data\n\n    def reset_qkv_experimental(self):\n        \"\"\"\n        WIP - experimental and likely to be changed/improved.\n        Unused by keeping for now.\n        \"\"\"\n        if self.module.attention.attn_qkvw is None:\n            self.module.attention.attn_qkvw = torch.empty(self.qw.shape[0] * 3,\n                                                          self.qw.shape[0],\n                                                          dtype=self.qw.dtype,\n                                                          device=self.qw.device)\n            self.module.attention.attn_qkvb = torch.empty(self.qw.shape[0] * 3,\n                                                          dtype=self.qw.dtype,\n                                                          device=self.qw.device)\n        self.module.attention.attn_qkvw.data[:self.qw.shape[0]] = self.qw.data\n        self.module.attention.attn_qkvb.data[:self.qw.shape[0]] = self.qb.data\n        self.module.attention.attn_qkvw.data[self.qw.shape[0]:2 * self.qw.shape[0]] = self.kw.data\n        self.module.attention.attn_qkvb.data[self.qw.shape[0]:2 * self.qw.shape[0]] = self.kb.data\n        self.module.attention.attn_qkvw.data[2 * self.qw.shape[0]:] = self.vw.data\n        self.module.attention.attn_qkvb.data[2 * self.qw.shape[0]:] = self.vb.data\n\n        qkv_data = [self.qw.data, \\\n                    self.qb.data, \\\n                    self.kw.data, \\\n                    self.kb.data, \\\n                    self.vw.data, \\\n                    self.vb.data]\n\n        self.qw.data = self.module.attention.attn_qkvw.data[:self.qw.shape[0]]\n        self.qb.data = self.module.attention.attn_qkvb.data[:self.qw.shape[0]]\n        self.kw.data = self.module.attention.attn_qkvw.data[self.qw.shape[0]:2 * self.qw.shape[0]]\n        self.kb.data = self.module.attention.attn_qkvb.data[self.qw.shape[0]:2 * self.qw.shape[0]]\n        self.vw.data = self.module.attention.attn_qkvw.data[2 * self.qw.shape[0]:]\n        self.vb.data = self.module.attention.attn_qkvb.data[2 * self.qw.shape[0]:]\n\n        for data in qkv_data:\n            del data\n\n    def set_attn_params_wo_copy(self, Z3_enabled=False):\n        self.module.attention.attn_ow = self.dense_w\n        self.module.attention.attn_ob = self.dense_b\n        if not Z3_enabled:\n            # In initialize_tensors, we create a fused qkvw with the appropriate shape\n            # and copy the qw, qb, kw, kb, vw, vb into it\n            self.module.attention.attn_qkvw = self.qkvw\n            self.module.attention.attn_qkvb = self.qkvb\n\n            # We reset the data for qw (which is the original model parameter) to point\n            # to the fused weight matrix we have created here\n            self.qw.data = self.qkvw[:self.qw.shape[0], :]\n            self.kw.data = self.qkvw[self.qw.shape[0]:2 * self.qw.shape[0], :]\n            self.vw.data = self.qkvw[self.qw.shape[0] * 2:, :]\n\n            # Assume if one of the biases is not None, then all of them are not None\n            if self.qb is not None:\n                self.qb.data = self.qkvb[:self.qw.shape[0]]\n                self.kb.data = self.qkvb[self.qw.shape[0]:2 * self.qw.shape[0]]\n                self.vb.data = self.qkvb[self.qw.shape[0] * 2:]\n        else:\n            # In ZeRO-3 this will be managed by ZeRO and handled separately in the\n            # forward of ds_attention\n            self.module.attention.attn_qw = self.qw\n            self.module.attention.attn_qb = self.qb\n            self.module.attention.attn_kw = self.kw\n            self.module.attention.attn_kb = self.kb\n            self.module.attention.attn_vw = self.vw\n            self.module.attention.attn_vb = self.vb\n\n    def get_attn_params(self):\n        params = super().get_attn_params()\n        params.extend([self.qw, self.qb, self.kw, self.kb, self.vw, self.vb])\n        return params\n", "deepspeed/module_inject/containers/features/hybrid_engine.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom abc import ABC, abstractmethod\nfrom typing import List, Tuple\n\nimport torch\n\n\nclass HybridEngineContainer(ABC):\n    \"\"\"\n    This container identifies which methods need to be overridden in addition to\n    the base container to enable use in the RLHF pipeline. These methods are not\n    necessary for inference alone.\n\n    NOTE: If you are using this feature with a container that\n    also inherits from `MetaTensorContainer`, ensure that `MetaTensorContainer`\n    is inherited before `HybridEngineContainer` in the class definition.\n    \"\"\"\n\n    def initialize_tensors(self, enable_training=False):\n        \"\"\"\n        Same purposes as the base container, but also grabs the hooks for any LoRA\n        parameters. If it's necessary to override specific sub-components of the model,\n        it's best to augment the specific `set_[component]` itself rather than modifying\n        the `initialize_tensors` method. See the `HybridSplitQKVContainer` for an example.\n        \"\"\"\n        super().initialize_tensors(enable_training=enable_training)\n        self.set_lora_params()\n\n    def transform_for_training(self):\n        \"\"\"\n        If the views on certain parameters are largely incompatible, it may be necessary to do\n        more substantial transformations to the parameters. This method should be overridden to\n        transform the inference format to what is necessary for training.\n        \"\"\"\n        pass\n\n    def transform_for_inference(self):\n        \"\"\"\n        If the views on certain parameters are largely incompatible, it may be necessary to do\n        more substantial transformations to the parameters. This method should be overridden to\n        transform the training format to what is necessary for inference.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def set_lora_params(self):\n        \"\"\"\n        If available, set the LoRA parameters for the module.  An implementation\n        for this would iterate over all parameters of the model and use the `maybe_get_lora` helper\n        method to check if the parameter does in fact have any LoRA params.\n        \"\"\"\n        raise NotImplementedError(\"A set_lora_params() function must be defined for the relevant parameters.\")\n\n    @abstractmethod\n    def get_lora_matched_pair(self):\n        \"\"\"Get the pair of lora params and its matched model parameters.\"\"\"\n        raise NotImplementedError(\"get_lora_matched_pair() must be defined for the relevant parameters.\")\n\n    def fuse_lora(self):\n        \"\"\"Fuse the LoRA parameters for the inference mode.\"\"\"\n        for maybe_lora_param, param in self.get_lora_matched_pair():\n            if len(maybe_lora_param) == 3:\n                lora_right_weight, \\\n                lora_left_weight, \\\n                lora_scaling = maybe_lora_param\n                param.data += lora_scaling * torch.matmul(lora_left_weight.t(), lora_right_weight.t())\n\n    def unfuse_lora(self):\n        \"\"\"Unfuse the LoRA parameters for the training mode.\"\"\"\n        for maybe_lora_param, param in self.get_lora_matched_pair():\n            if len(maybe_lora_param) == 3:\n                lora_right_weight, \\\n                lora_left_weight, \\\n                lora_scaling = maybe_lora_param\n                param.data -= lora_scaling * torch.matmul(lora_left_weight.t(), lora_right_weight.t())\n\n    def apply_tensor_parallelism(self, mp_replace, reversed_dim=False):\n        \"\"\"\n        Add support for reversed dim in tensor parallelism. If necessary, override\n        the called methods to handle partitioned weights (i.e. if qkv is split, override\n        the `attention_qkv_mp` method). If the model component is not split, it should\n        be safe to use the default implementation.\n        \"\"\"\n        # Setup the new Attention module\n        self.attention_qkv_mp(mp_replace, reversed_dim=reversed_dim)\n        self.attention_o_mp(mp_replace, reversed_dim=reversed_dim)\n\n        # Setup the new MLP module\n        self.mlp_inter_mp(mp_replace, reversed_dim=reversed_dim)\n        self.mlp_output_mp(mp_replace, reversed_dim=reversed_dim)\n\n        # Apply weight quantization\n        # TODO(cmikeh2): Re-enable this once verified\n        #self.apply_weight_quantization()\n\n    def _release_params(self, param_pairs: List[Tuple[torch.Tensor, torch.Tensor]]):\n        \"\"\"\n        Helper for `release_[component]` methods. Accepts a list of tuples where the first\n        element is the module param that needs to be deleted, and the second is the reassignment\n        from the container.\n        \"\"\"\n        for module_param, container_param in param_pairs:\n            if module_param is not None:\n                del module_param\n            module_param = container_param\n\n    def release_memory(self):\n        \"\"\"\n        Delete module parameters if they exist and point them back to the container. The primary\n        purpose of this is for TP-inference with ZeRO-3. In this scenario, we need to delete the\n        parameters we've created for inference to free their memory.\n        \"\"\"\n        general_params = [\n            (self.module.attention.attn_ow, self.dense_w),\n            (self.module.attention.attn_ob, self.dense_b),\n            (self.module.mlp.attn_nw, self.attn_nw),\n            (self.module.mlp.attn_nb, self.attn_nb),\n            (self.module.norm_w, self.input_nw),\n            (self.module.norm_b, self.input_nb),\n        ]\n\n        self._release_params(general_params)\n\n        self.release_qkv()\n        self.release_mlp()\n\n    def release_qkv(self):\n        \"\"\"\n        Release for QKV parameters (as well as any aliases).\n        \"\"\"\n        qkv_params = [\n            (self.module.attention.attn_qkvw, self.qkvw),\n            (self.module.attention.attn_qkvb, self.qkvb),\n        ]\n\n        self._release_params(qkv_params)\n\n    def release_mlp(self):\n        \"\"\"\n        Release for MLP parameters (as well as any aliases).\n        \"\"\"\n        mlp_params = [\n            (self.module.mlp.inter_w, self._h4h_w),\n            (self.module.mlp.inter_b, self._h4h_b),\n            (self.module.mlp.output_w, self._4hh_w),\n            (self.module.mlp.output_b, self._4hh_b),\n        ]\n\n        self._release_params(mlp_params)\n\n    def reset_params(self):\n        \"\"\"\n        The purpose of reset params is to get the weights from the FP16 training\n        copy of the model and copy to them to contiguous inference view. This only needs\n        to be performed when the container parameters cannot be used directly for inference.\n        \"\"\"\n        self.reset_qkv()\n        self.reset_mlp()\n\n    def reset_qkv(self):\n        \"\"\"\n        Perform any necessary resets of the model parameters for the QKV components.\n        \"\"\"\n        pass\n\n    def reset_mlp(self):\n        \"\"\"\n        Perform any necessary resets of the model parameters for the MLP components.\n        \"\"\"\n        pass\n\n    def get_lora_params(self):\n        \"\"\"\n        Return a list of all parameters that would have LoRA for the module.\n        \"\"\"\n        if not hasattr(self, \"lora_params\"):\n            self.set_lora_params()\n        return self.lora_params\n\n    def set_params_wo_copy(self, Z3_enabled=False):\n        \"\"\"\n        Rather than copying into, set the parameters directly. This is necessary to provide\n        an inexpensive (low-memory-overhead) view onto the FP16 forward weights.\n        \"\"\"\n        self.module.mlp.attn_nw = self.attn_nw\n        self.module.mlp.attn_nb = self.attn_nb\n        self.module.norm_w = self.input_nw\n        self.module.norm_b = self.input_nb\n        self.set_attn_params_wo_copy(Z3_enabled=Z3_enabled)\n        self.set_mlp_params_wo_copy(Z3_enabled=Z3_enabled)\n\n    def set_attn_params_wo_copy(self, **kwargs):\n        \"\"\"\n        Narrower sub-method for finer grained overriding.\n        \"\"\"\n        self.module.attention.attn_ow = self.dense_w\n        self.module.attention.attn_ob = self.dense_b\n        self.module.attention.attn_qkvw = self.qkvw\n        self.module.attention.attn_qkvb = self.qkvb\n\n    def set_mlp_params_wo_copy(self, **kwargs):\n        \"\"\"\n        Narrower sub-method for finer grained overriding.\n        \"\"\"\n        self.module.mlp.inter_w = self._h4h_w\n        self.module.mlp.inter_b = self._h4h_b\n        self.module.mlp.output_w = self._4hh_w\n        self.module.mlp.output_b = self._4hh_b\n", "deepspeed/module_inject/containers/features/hybrid_megatron.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\n\nfrom .hybrid_engine import HybridEngineContainer\nfrom .megatron import MegatronContainer\n\n\nclass HybridMegatronContainer(MegatronContainer, HybridEngineContainer):\n\n    def _align_qkv(self, x: torch.Tensor):\n        \"\"\"\n        Internal helper for accepting the head-contiguous weight matrix and chunking\n        the query, key, and value components.\n        \"\"\"\n        attention_head_size = x.shape[0] // self.num_attention_heads\n        new_x_shape = (self.num_attention_heads, attention_head_size) + x.size()[1:]\n        x_1 = x.view(*new_x_shape)\n        div_dim = len(x_1.size()) - 2 if len(x.shape) == 2 else -1\n        (q, k, v) = torch.split(x_1, (x_1.shape[div_dim] // 3), dim=div_dim)\n        if len(q.shape) > 2:\n            x.data.copy_(\n                torch.cat((q.reshape(-1, q.shape[-1]), k.reshape(-1, q.shape[-1]), v.reshape(-1, q.shape[-1])),\n                          dim=0).reshape(x.shape))\n        else:\n            x.data.copy_(torch.cat((q.reshape(-1), k.reshape(-1), v.reshape(-1)), dim=-1).reshape(x.shape))\n\n    def transform_for_inference(self) -> None:\n        \"\"\"\n        Overrides the HybridEngineContainer implementation.\n\n        The alternative layout of the QKV matrix for Megatron is such that each head's Q, K, and V\n        are sequential in memory. This is different from the default layout in which all of the Qs\n        are sequential, followed by all of the Ks, and then all of the Vs. Here, we take the default\n        layout and transform it to the inference layout.\n        \"\"\"\n        if hasattr(self.qkvw, 'ds_id'):\n            from deepspeed.runtime.zero import GatheredParameters\n            from deepspeed.runtime.zero.partition_parameters import ZeroParamStatus\n            param_list = [self.qkvw, self.qkvb]\n            non_active_params = [param for param in param_list if (hasattr(param, 'ds_id') and \\\n                            param.ds_status == ZeroParamStatus.NOT_AVAILABLE)]\n            with GatheredParameters(non_active_params):\n                self._align_qkv(self.qkvw)\n                self._align_qkv(self.qkvb)\n        else:\n            self._align_qkv(self.qkvw)\n            self._align_qkv(self.qkvb)\n\n    def _partition_qkv(self, x: torch.Tensor):\n        \"\"\"\n        Internal helper for taking contiguous QKV and partitioning it for contiguous\n        heads.\n        \"\"\"\n        q_k_v = torch.split(x, (x.shape[0] // 3), dim=0)\n        attention_head_size = q_k_v[0].shape[0] // self.num_attention_heads\n        new_x_shape = (self.num_attention_heads, attention_head_size) + x.size()[1:]\n        q, k, v = [data.view(*new_x_shape) for data in q_k_v]\n        if len(q.shape) > 2:\n            x.data.copy_(torch.cat((q, k, v), dim=-2).reshape(-1, q.shape[-1]))\n        else:\n            x.data.copy_(torch.cat((q, k, v), dim=-1).reshape(-1))\n\n    def transform_for_training(self):\n        \"\"\"\n        Overrides the HybridEngineContainer implementation.\n\n        The alternative layout of the QKV matrix for Megatron is such that each head's Q, K, and V\n        are sequential in memory. This is different from the default layout in which all of the Qs\n        are sequential, followed by all of the Ks, and then all of the Vs. This function takes the inference format and reverts it back to the default format.\n        \"\"\"\n        # If parameter is distributed, handle gathering it\n        if hasattr(self.qkvw, 'ds_id'):\n            from deepspeed.runtime.zero import GatheredParameters\n            from deepspeed.runtime.zero.partition_parameters import ZeroParamStatus\n            param_list = [self.qkvw, self.qkvb]\n            non_active_params = [param for param in param_list if (hasattr(param, 'ds_id') and \\\n                            param.ds_status == ZeroParamStatus.NOT_AVAILABLE)]\n            with GatheredParameters(non_active_params):\n                self._partition_qkv(self.qkvw)\n                self._partition_qkv(self.qkvb)\n        else:\n            self._partition_qkv(self.qkvw)\n            self._partition_qkv(self.qkvb)\n", "deepspeed/module_inject/containers/features/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .gated_mlp import HybridGatedMLPContainer\nfrom .megatron import MegatronContainer\nfrom .meta_tensor import MetaTensorContainer\nfrom .split_qkv import HybridSplitQKVContainer\n", "deepspeed/model_implementations/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .transformers.ds_transformer import DeepSpeedTransformerInference\nfrom .transformers.clip_encoder import DSClipEncoder\n", "deepspeed/model_implementations/features/cuda_graph.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom abc import ABC, abstractmethod\n\n\nclass CUDAGraph(ABC):\n\n    def __init__(self, enable_cuda_graph=False):\n        super().__init__()\n        self.enable_cuda_graph = enable_cuda_graph\n\n    @abstractmethod\n    def _create_cuda_graph(self):\n        \"\"\"\n        Create CUDA graph(s)\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def _graph_replay(self):\n        \"\"\"\n        Replay CUDA graph(s)\n        \"\"\"\n        raise NotImplementedError\n", "deepspeed/model_implementations/features/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n'''Copyright The Microsoft DeepSpeed Team'''\n", "deepspeed/model_implementations/diffusers/vae.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nfrom deepspeed.accelerator import get_accelerator\nfrom ..features.cuda_graph import CUDAGraph\n\n\nclass DSVAE(CUDAGraph, torch.nn.Module):\n\n    def __init__(self, vae, enable_cuda_graph=True):\n        super().__init__(enable_cuda_graph=enable_cuda_graph)\n        self.vae = vae\n        self.config = vae.config\n        self.device = self.vae.device\n        self.dtype = self.vae.dtype\n        self.vae.requires_grad_(requires_grad=False)\n        self.decoder_cuda_graph_created = False\n        self.encoder_cuda_graph_created = False\n        self.all_cuda_graph_created = False\n\n    def _graph_replay_decoder(self, *inputs, **kwargs):\n        for i in range(len(inputs)):\n            if torch.is_tensor(inputs[i]):\n                self.static_decoder_inputs[i].copy_(inputs[i])\n        for k in kwargs:\n            if torch.is_tensor(kwargs[k]):\n                self.static_decoder_kwargs[k].copy_(kwargs[k])\n        get_accelerator().replay_graph(self._decoder_cuda_graph)\n        return self.static_decoder_output\n\n    def _decode(self, x, return_dict=True, generator=None):\n        return self.vae.decode(x, return_dict=return_dict)\n\n    def _create_cuda_graph_decoder(self, *inputs, **kwargs):\n        # warmup to create the workspace and cublas handle\n        cuda_stream = torch.cuda.Stream()\n        cuda_stream.wait_stream(torch.cuda.current_stream())\n        with torch.cuda.stream(cuda_stream):\n            for i in range(3):\n                ret = self._decode(*inputs, **kwargs)\n        torch.cuda.current_stream().wait_stream(cuda_stream)\n\n        # create cuda_graph and assign static_inputs and static_outputs\n        self._decoder_cuda_graph = get_accelerator().create_graph()\n        self.static_decoder_inputs = inputs\n        self.static_decoder_kwargs = kwargs\n\n        with get_accelerator().capture_to_graph(self._decoder_cuda_graph):\n            self.static_decoder_output = self._decode(*self.static_decoder_inputs, **self.static_decoder_kwargs)\n\n        self.decoder_cuda_graph_created = True\n\n    def decode(self, *inputs, **kwargs):\n        if self.enable_cuda_graph:\n            if self.decoder_cuda_graph_created:\n                outputs = self._graph_replay_decoder(*inputs, **kwargs)\n            else:\n                self._create_cuda_graph_decoder(*inputs, **kwargs)\n                outputs = self._graph_replay_decoder(*inputs, **kwargs)\n            return outputs\n        else:\n            return self._decode(*inputs, **kwargs)\n\n    def _graph_replay_encoder(self, *inputs, **kwargs):\n        for i in range(len(inputs)):\n            if torch.is_tensor(inputs[i]):\n                self.static_encoder_inputs[i].copy_(inputs[i])\n        for k in kwargs:\n            if torch.is_tensor(kwargs[k]):\n                self.static_encoder_kwargs[k].copy_(kwargs[k])\n        get_accelerator().replay_graph(self._encoder_cuda_graph)\n        return self.static_encoder_output\n\n    def _encode(self, x, return_dict=True):\n        return self.vae.encode(x, return_dict=return_dict)\n\n    def _create_cuda_graph_encoder(self, *inputs, **kwargs):\n        # warmup to create the workspace and cublas handle\n        cuda_stream = torch.cuda.Stream()\n        cuda_stream.wait_stream(torch.cuda.current_stream())\n        with torch.cuda.stream(cuda_stream):\n            for i in range(3):\n                ret = self._encode(*inputs, **kwargs)\n        torch.cuda.current_stream().wait_stream(cuda_stream)\n\n        # create cuda_graph and assign static_inputs and static_outputs\n        self._encoder_cuda_graph = get_accelerator().create_graph()\n        self.static_encoder_inputs = inputs\n        self.static_encoder_kwargs = kwargs\n\n        with get_accelerator().capture_to_graph(self._encoder_cuda_graph):\n            self.static_encoder_output = self._encode(*self.static_encoder_inputs, **self.static_encoder_kwargs)\n\n        self.encoder_cuda_graph_created = True\n\n    def encode(self, *inputs, **kwargs):\n        if self.enable_cuda_graph:\n            if self.encoder_cuda_graph_created:\n                outputs = self._graph_replay_encoder(*inputs, **kwargs)\n            else:\n                self._create_cuda_graph_encoder(*inputs, **kwargs)\n                outputs = self._graph_replay_encoder(*inputs, **kwargs)\n            return outputs\n        else:\n            return self._encode(*inputs, **kwargs)\n\n    def _graph_replay(self, *inputs, **kwargs):\n        for i in range(len(inputs)):\n            if torch.is_tensor(inputs[i]):\n                self.static_inputs[i].copy_(inputs[i])\n        for k in kwargs:\n            if torch.is_tensor(kwargs[k]):\n                self.static_kwargs[k].copy_(kwargs[k])\n        get_accelerator().replay_graph(self._all_cuda_graph)\n        return self.static_output\n\n    def forward(self, *inputs, **kwargs):\n        if self.enable_cuda_graph:\n            if self.cuda_graph_created:\n                outputs = self._graph_replay(*inputs, **kwargs)\n            else:\n                self._create_cuda_graph(*inputs, **kwargs)\n                outputs = self._graph_replay(*inputs, **kwargs)\n            return outputs\n        else:\n            return self._forward(*inputs, **kwargs)\n\n    def _create_cuda_graph(self, *inputs, **kwargs):\n        # warmup to create the workspace and cublas handle\n        cuda_stream = torch.cuda.Stream()\n        cuda_stream.wait_stream(torch.cuda.current_stream())\n        with torch.cuda.stream(cuda_stream):\n            for i in range(3):\n                ret = self._forward(*inputs, **kwargs)\n        torch.cuda.current_stream().wait_stream(cuda_stream)\n\n        # create cuda_graph and assign static_inputs and static_outputs\n        self._all_cuda_graph = get_accelerator().create_graph()\n        self.static_inputs = inputs\n        self.static_kwargs = kwargs\n\n        with get_accelerator().capture_to_graph(self._all_cuda_graph):\n            self.static_output = self._forward(*self.static_inputs, **self.static_kwargs)\n\n        self.all_cuda_graph_created = True\n\n    def _forward(self, sample, timestamp, encoder_hidden_states, return_dict=True):\n        return self.vae(sample, timestamp, encoder_hidden_states, return_dict)\n", "deepspeed/model_implementations/diffusers/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n'''Copyright The Microsoft DeepSpeed Team'''\n", "deepspeed/model_implementations/diffusers/unet.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nfrom deepspeed.accelerator import get_accelerator\nfrom ..features.cuda_graph import CUDAGraph\n\n\nclass DSUNet(CUDAGraph, torch.nn.Module):\n\n    def __init__(self, unet, enable_cuda_graph=True):\n        super().__init__(enable_cuda_graph=enable_cuda_graph)\n        self.unet = unet\n        # SD pipeline accesses this attribute\n        self.in_channels = unet.in_channels\n        self.device = self.unet.device\n        self.dtype = self.unet.dtype\n        self.config = self.unet.config\n        self.fwd_count = 0\n        self.unet.requires_grad_(requires_grad=False)\n        self.unet.to(memory_format=torch.channels_last)\n        self.cuda_graph_created = False\n\n    def _graph_replay(self, *inputs, **kwargs):\n        for i in range(len(inputs)):\n            if torch.is_tensor(inputs[i]):\n                self.static_inputs[i].copy_(inputs[i])\n        for k in kwargs:\n            if torch.is_tensor(kwargs[k]):\n                self.static_kwargs[k].copy_(kwargs[k])\n        get_accelerator().replay_graph(self._cuda_graphs)\n        return self.static_output\n\n    def forward(self, *inputs, **kwargs):\n        if self.enable_cuda_graph:\n            if self.cuda_graph_created:\n                outputs = self._graph_replay(*inputs, **kwargs)\n            else:\n                self._create_cuda_graph(*inputs, **kwargs)\n                outputs = self._graph_replay(*inputs, **kwargs)\n            return outputs\n        else:\n            return self._forward(*inputs, **kwargs)\n\n    def _create_cuda_graph(self, *inputs, **kwargs):\n        # warmup to create the workspace and cublas handle\n        cuda_stream = torch.cuda.Stream()\n        cuda_stream.wait_stream(torch.cuda.current_stream())\n        with torch.cuda.stream(cuda_stream):\n            for i in range(3):\n                ret = self._forward(*inputs, **kwargs)\n        torch.cuda.current_stream().wait_stream(cuda_stream)\n\n        # create cuda_graph and assign static_inputs and static_outputs\n        self._cuda_graphs = get_accelerator().create_graph()\n        self.static_inputs = inputs\n        self.static_kwargs = kwargs\n\n        with get_accelerator().capture_to_graph(self._cuda_graphs):\n            self.static_output = self._forward(*self.static_inputs, **self.static_kwargs)\n\n        self.cuda_graph_created = True\n\n    def _forward(self,\n                 sample,\n                 timestamp,\n                 encoder_hidden_states,\n                 return_dict=True,\n                 cross_attention_kwargs=None,\n                 timestep_cond=None,\n                 added_cond_kwargs=None):\n        if cross_attention_kwargs:\n            return self.unet(sample,\n                             timestamp,\n                             encoder_hidden_states,\n                             return_dict,\n                             cross_attention_kwargs=cross_attention_kwargs)\n        else:\n            return self.unet(sample, timestamp, encoder_hidden_states, return_dict)\n", "deepspeed/model_implementations/transformers/ds_base.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch.nn as nn\n\n\nclass DeepSpeedTransformerBase(nn.module):\n\n    def __init__(self):\n        pass\n\n    # this would be the new clean base class that will replace DeepSpeedTransformerInference.\n    # we currently don't know how this will look like but keeping it here as a placeholder.\n", "deepspeed/model_implementations/transformers/ds_gpt.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom deepspeed.model_implementations.transformers.ds_transformer import DeepSpeedTransformerInference\n\n\nclass DeepSpeedGPTInference(DeepSpeedTransformerInference):\n    \"\"\"Initialize the DeepSpeed GPT Transformer Layer.\n    \"\"\"\n\n    def __init__(self,\n                 config,\n                 mp_group=None,\n                 quantize_scales=None,\n                 quantize_groups=1,\n                 merge_count=1,\n                 mlp_extra_grouping=False):\n        super().__init__(config, mp_group, quantize_scales, quantize_groups, merge_count, mlp_extra_grouping)\n", "deepspeed/model_implementations/transformers/ds_llama2.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nfrom deepspeed import comm as dist\nfrom deepspeed.model_implementations.transformers.ds_transformer import DeepSpeedTransformerInference\n\ninference_module = None\n\n\nclass DeepSpeedLlama2Inference(DeepSpeedTransformerInference):\n    \"\"\"Initialize the DeepSpeed OPT Transformer Layer.\n    \"\"\"\n\n    def __init__(self,\n                 config,\n                 mp_group=None,\n                 quantize_scales=None,\n                 quantize_groups=1,\n                 merge_count=1,\n                 mlp_extra_grouping=False):\n        super().__init__(config, mp_group, quantize_scales, quantize_groups, merge_count, mlp_extra_grouping)\n\n    def forward(self, *args, **kwargs):\n\n        input = args[0]\n        input_mask = None\n        # Allocate memory only on first layer forward\n        if self.config.layer_id == 0 and self._alloc_workspace:\n            self.allocate_workspace(self.config.hidden_size, self.config.heads,\n                                    input.size()[1],\n                                    input.size()[0], DeepSpeedTransformerInference.layer_id, self.config.mp_size,\n                                    self.config.bigscience_bloom,\n                                    dist.get_rank() if dist.is_initialized() else 0, self.config.max_out_tokens,\n                                    self.config.min_out_tokens)\n            self._alloc_workspace = False\n\n        get_present = True\n\n        # We set the prev key/value to None when there is a prompt\n        if input.shape[1] > 1:\n            self.layer_past = None\n        layer_past = self.layer_past\n\n        input_type = input.dtype\n\n        if (self.config.dtype in [torch.float16, torch.bfloat16, torch.int8]) \\\n            and input.dtype == torch.float:\n            target_dtype = torch.half if self.dtype == torch.int8 else self.dtype\n            input = input.to(target_dtype)\n\n        with torch.no_grad():\n            attention_output, key, value, context_outputtn_ctx, inp_norm = \\\n                                     self.attention(input,\n                                              input_mask,\n                                              None,\n                                              layer_past,\n                                              get_present,\n                                              None, None, None,\n                                              self.norm_w,\n                                              self.norm_b,\n                                              None)\n            self.layer_past = (key, value)\n            output = self.mlp(attention_output, input, inp_norm, self.attention.attn_ob)\n\n            output = output.to(input_type)\n        return output\n", "deepspeed/model_implementations/transformers/ds_bert.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom deepspeed.model_implementations.transformers.ds_transformer import DeepSpeedTransformerInference\n\n\nclass DeepSpeedBERTInference(DeepSpeedTransformerInference):\n    \"\"\"Initialize the DeepSpeed BERT Transformer Layer.\n    \"\"\"\n\n    def __init__(self,\n                 config,\n                 mp_group=None,\n                 quantize_scales=None,\n                 quantize_groups=1,\n                 merge_count=1,\n                 mlp_extra_grouping=False):\n        super().__init__(config, mp_group, quantize_scales, quantize_groups, merge_count, mlp_extra_grouping)\n", "deepspeed/model_implementations/transformers/ds_transformer.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nimport torch.nn as nn\nfrom deepspeed import comm as dist\nfrom deepspeed.utils.logging import log_dist\n\nfrom deepspeed.ops.transformer.inference.ds_mlp import DeepSpeedMLP\nfrom deepspeed.ops.transformer.inference.ds_attention import DeepSpeedSelfAttention, BloomSelfAttention\nfrom deepspeed.accelerator import get_accelerator\nfrom deepspeed.ops.op_builder import InferenceBuilder\nimport deepspeed\nif deepspeed.HAS_TRITON:\n    from deepspeed.ops.transformer.inference.triton.mlp import TritonMLP\n    from deepspeed.ops.transformer.inference.triton.attention import TritonSelfAttention\n\ninference_module = None\n\n\nclass DeepSpeedTransformerInference(nn.Module):\n    \"\"\"Initialize the DeepSpeed Transformer Layer.\n        Arguments:\n            layer_id: The layer index starting from 0, e.g. if model has 24 transformer layers,\n                layer_id will be 0,1,2...23 when each layer object is instantiated\n            config: An object of DeepSpeedInferenceConfig\n            mp_group: Model parallelism group initialized on the modeling side.\n            quantize_scales: This argument groups all the layers' scales used for quantization\n            quantize_groups: Number of groups used for quantizing the model\n            merge_count: Shows the number of model-parallel checkpoints merged before running inference.\n                We use this argument to control the quantization scale for the model parameters if a bigger\n                quantize-grouping than 1 is used.\n            mlp_extra_grouping: This flag is used to show a 2x higher number of groups used for the MLP part\n                of a Transformer layer. We use this feature for quantization to reduce the convergence impact\n                for specific downstream tasks.\n    \"\"\"\n    layer_id = 0\n\n    def __init__(self,\n                 config,\n                 mp_group=None,\n                 quantize_scales=None,\n                 quantize_groups=1,\n                 merge_count=1,\n                 mlp_extra_grouping=False):\n        super(DeepSpeedTransformerInference, self).__init__()\n\n        self.config = config\n        self.config.layer_id = DeepSpeedTransformerInference.layer_id\n        DeepSpeedTransformerInference.layer_id += 1\n\n        data_type = torch.half if self.config.dtype == torch.int8 else self.config.dtype\n        global inference_module\n        if inference_module is None:\n            builder = InferenceBuilder()\n            inference_module = builder.load()\n\n        if DeepSpeedTransformerInference.layer_id == 1:\n            log_dist(f\"DeepSpeed-Inference config: {self.config.__dict__}\", [0])\n            if deepspeed.HAS_TRITON and self.config.use_triton:\n                log_dist(f\"Injecting Triton kernels ...\", [0])\n\n        if self.config.bigscience_bloom:\n            self.attention = BloomSelfAttention(self.config, mp_group, quantize_scales, quantize_groups, merge_count)\n            assert not self.config.use_triton\n        else:\n            if deepspeed.HAS_TRITON and self.config.use_triton:\n                self.attention = TritonSelfAttention(self.config)\n            else:\n                self.attention = DeepSpeedSelfAttention(self.config, mp_group, quantize_scales, quantize_groups,\n                                                        merge_count)\n\n        if deepspeed.HAS_TRITON and self.config.use_triton:\n            self.mlp = TritonMLP(self.config)\n        else:\n            self.mlp = DeepSpeedMLP(self.config, mp_group, quantize_scales, quantize_groups, merge_count,\n                                    mlp_extra_grouping)\n\n        device = get_accelerator().current_device_name()  # if config.bigscience_bloom else 'cpu'\n        if self.config.set_empty_params:\n            self.norm_w = None\n            self.norm_b = None\n        else:\n            self.norm_w = nn.Parameter(torch.empty(self.config.hidden_size, dtype=data_type, device=device),\n                                       requires_grad=False)\n            self.norm_b = nn.Parameter(torch.empty(self.config.hidden_size, dtype=data_type, device=device),\n                                       requires_grad=False)\n        self.layer_past = None\n        try:\n            if config.dtype == torch.float32:\n                self.allocate_workspace = inference_module.allocate_workspace_fp32\n            elif config.dtype == torch.bfloat16:\n                self.allocate_workspace = inference_module.allocate_workspace_bf16\n            else:\n                self.allocate_workspace = inference_module.allocate_workspace_fp32\n            self._alloc_workspace = True\n        except AttributeError:\n            self.allocate_workspace = None\n            self._alloc_workspace = False\n\n    @classmethod\n    def reset_cache(cls):\n        if inference_module is not None:\n            inference_module.reset_cache()\n\n    def forward(\n            self,\n            input=None,\n            input_mask=None,\n            attention_mask=None,\n            attn_mask=None,\n            head_mask=None,\n            layer_past=None,\n            get_key_value=False,\n            get_present=False,\n            encoder_output=None,\n            enc_dec_attn_mask=None,\n            x=None,\n            encoder_hidden_states=None,\n            encoder_attention_mask=None,\n            use_cache=False,\n            alibi=None,\n            output_attentions=False,\n            # TODO(arashb): 'layer_head_mask' and 'past_key_value' are only added to satisfy the OPT models API.\n            # This needs to be redesigned later!\n            layer_head_mask=None,\n            past_key_value=None,\n            **kwargs):\n\n        if x is not None:\n            input = x\n        if \"hidden_states\" in kwargs:\n            input = kwargs[\"hidden_states\"]\n\n        input_mask = (input_mask if attn_mask is None else attn_mask) if attention_mask is None else attention_mask\n\n        # Allocate memory only on first layer forward\n        if self.config.layer_id == 0 and self._alloc_workspace:\n            self.allocate_workspace(self.config.hidden_size, self.config.heads,\n                                    input.size()[1],\n                                    input.size()[0], DeepSpeedTransformerInference.layer_id, self.config.mp_size,\n                                    self.config.bigscience_bloom,\n                                    dist.get_rank() if dist.is_initialized() else 0, self.config.max_out_tokens,\n                                    self.config.min_out_tokens)\n            self._alloc_workspace = False\n\n        get_present = (get_present or get_key_value or use_cache)\n        input_mask = input_mask if attention_mask is None else attention_mask\n\n        # We set the prev key/value to None when there is a prompt\n        if input.shape[1] > 1:\n            self.layer_past = None\n        layer_past = layer_past if layer_past is not None else self.layer_past\n        head_mask = layer_head_mask if layer_head_mask is not None else head_mask\n\n        attn_mask = None\n        if isinstance(input, tuple):\n            attn_mask = input[1]\n            input = input[0]\n        input_type = input.dtype\n\n        if (self.config.dtype in [torch.float16, torch.bfloat16, torch.int8]) \\\n            and input.dtype == torch.float:\n            target_dtype = torch.half if self.config.dtype == torch.int8 else self.config.dtype\n            input = input.to(target_dtype)\n\n        with torch.no_grad():\n            attention_output, key, value, context_outputtn_ctx, inp_norm = \\\n                                     self.attention(input,\n                                              input_mask,\n                                              head_mask,\n                                              layer_past,\n                                              get_present,\n                                              encoder_hidden_states,\n                                              encoder_attention_mask,\n                                              output_attentions,\n                                              self.norm_w,\n                                              self.norm_b,\n                                              alibi)\n\n            presents = (key, value)\n            self.layer_past = presents if layer_past is None else None\n            output = self.mlp(attention_output, input, inp_norm, self.attention.attn_ob)\n\n            if not self.config.pre_layer_norm:\n                output = inference_module.layer_norm(output, self.norm_w, self.norm_b, self.config.epsilon)\n\n            output = output.to(input_type)\n        if get_present:\n            output = (output, presents)\n\n        if self.config.return_single_tuple:\n            return (output, )\n        elif self.config.return_tuple:\n            return output if type(output) is tuple else (output, attn_mask)\n        else:\n            return output\n", "deepspeed/model_implementations/transformers/ds_opt.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom deepspeed.model_implementations.transformers.ds_transformer import DeepSpeedTransformerInference\n\n\nclass DeepSpeedOPTInference(DeepSpeedTransformerInference):\n    \"\"\"Initialize the DeepSpeed OPT Transformer Layer.\n    \"\"\"\n\n    def __init__(self,\n                 config,\n                 mp_group=None,\n                 quantize_scales=None,\n                 quantize_groups=1,\n                 merge_count=1,\n                 mlp_extra_grouping=False):\n        super().__init__(config, mp_group, quantize_scales, quantize_groups, merge_count, mlp_extra_grouping)\n", "deepspeed/model_implementations/transformers/ds_bloom.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom deepspeed.model_implementations.transformers.ds_transformer import DeepSpeedTransformerInference\n\n\nclass DeepSpeedBloomInference(DeepSpeedTransformerInference):\n    \"\"\"Initialize the DeepSpeed Bloom Transformer Layer.\n    \"\"\"\n\n    def __init__(self,\n                 config,\n                 mp_group=None,\n                 quantize_scales=None,\n                 quantize_groups=1,\n                 merge_count=1,\n                 mlp_extra_grouping=False):\n        super().__init__(config, mp_group, quantize_scales, quantize_groups, merge_count, mlp_extra_grouping)\n", "deepspeed/model_implementations/transformers/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n'''Copyright The Microsoft DeepSpeed Team'''\n", "deepspeed/model_implementations/transformers/clip_encoder.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nfrom deepspeed.accelerator import get_accelerator\nfrom ..features.cuda_graph import CUDAGraph\n\n\nclass DSClipEncoder(CUDAGraph, torch.nn.Module):\n\n    def __init__(self, enc, enable_cuda_graph=False):\n        super().__init__(enable_cuda_graph=enable_cuda_graph)\n        enc.text_model._build_causal_attention_mask = self._build_causal_attention_mask\n        self.enc = enc\n        self.device = self.enc.device\n        self.dtype = self.enc.dtype\n        self.cuda_graph_created = [False, False]\n        self.static_inputs = [None, None]\n        self.static_kwargs = [None, None]\n        self.static_output = [None, None]\n        self._cuda_graphs = [None, None]\n        self.iter = 0\n        self.config = self.enc.config\n\n    def _build_causal_attention_mask(self, bsz, seq_len, dtype):\n        mask = torch.empty(bsz, seq_len, seq_len, dtype=dtype, device=get_accelerator().current_device_name())\n        mask.fill_(torch.tensor(torch.finfo(dtype).min))\n        mask.triu_(1)\n        mask = mask.unsqueeze(1)\n        return mask\n\n    def _graph_replay(self, *inputs, **kwargs):\n        for i in range(len(inputs)):\n            if torch.is_tensor(inputs[i]):\n                self.static_inputs[self.iter][i].copy_(inputs[i])\n        for k in kwargs:\n            if torch.is_tensor(kwargs[k]):\n                self.static_kwargs[self.iter][k].copy_(kwargs[k])\n        get_accelerator().replay_graph(self._cuda_graphs[self.iter])\n        return self.static_output[self.iter]\n\n    def forward(self, *inputs, **kwargs):\n        if self.enable_cuda_graph:\n            if self.cuda_graph_created[self.iter]:\n                outputs = self._graph_replay(*inputs, **kwargs)\n            else:\n                self._create_cuda_graph(*inputs, **kwargs)\n                outputs = self._graph_replay(*inputs, **kwargs)\n            self.iter = (self.iter + 1) % 2\n            return outputs\n        else:\n            return self.enc(*inputs, **kwargs)\n\n    def _create_cuda_graph(self, *inputs, **kwargs):\n        # warmup to create the workspace and cublas handle\n        cuda_stream = torch.cuda.Stream()\n        cuda_stream.wait_stream(torch.cuda.current_stream())\n        with torch.cuda.stream(cuda_stream):\n            for i in range(3):\n                ret = self._forward(*inputs, **kwargs)\n        torch.cuda.current_stream().wait_stream(cuda_stream)\n\n        # create cuda_graph and assign static_inputs and static_outputs\n        self._cuda_graphs[self.iter] = get_accelerator().create_graph()\n        self.static_inputs[self.iter] = inputs\n        self.static_kwargs[self.iter] = kwargs\n\n        with get_accelerator().capture_to_graph(self._cuda_graphs[self.iter]):\n            self.static_output[self.iter] = self._forward(*self.static_inputs[self.iter],\n                                                          **self.static_kwargs[self.iter])\n\n        self.cuda_graph_created[self.iter] = True\n\n    def _forward(self, *inputs, **kwargs):\n        return self.enc(*inputs, **kwargs)\n", "deepspeed/model_implementations/transformers/ds_megatron_gpt.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom deepspeed.model_implementations.transformers.ds_transformer import DeepSpeedTransformerInference\n\n\nclass DeepSpeedMegatronGPTInference(DeepSpeedTransformerInference):\n    \"\"\"Initialize the DeepSpeed Megatron GPT Transformer Layer.\n    \"\"\"\n\n    def __init__(self,\n                 config,\n                 mp_group=None,\n                 quantize_scales=None,\n                 quantize_groups=1,\n                 merge_count=1,\n                 mlp_extra_grouping=False):\n        super().__init__(config, mp_group, quantize_scales, quantize_groups, merge_count, mlp_extra_grouping)\n", "deepspeed/sequence/layer.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\n\nfrom typing import Any, Tuple\nfrom torch import Tensor\nfrom torch.nn import Module\n\nimport deepspeed.comm as dist\n\n\ndef single_all_to_all(input, scatter_idx, gather_idx, group):\n    seq_world_size = dist.get_world_size(group)\n    inp_shape = list(input.shape)\n    inp_shape[scatter_idx] = inp_shape[scatter_idx] // seq_world_size\n    if scatter_idx < 2:\n        input_t = input.reshape(\n            [seq_world_size, inp_shape[scatter_idx]] + \\\n            inp_shape[scatter_idx + 1:]\n        ).contiguous()\n    else:\n        # transpose groups of heads with the seq-len parallel dimension, so that we can scatter them!\n        input_t = input.reshape(\n            [-1, seq_world_size, inp_shape[scatter_idx]] + \\\n            inp_shape[scatter_idx + 1:]\n        ).transpose(0, 1).contiguous()\n\n    output = torch.empty_like(input_t)\n    dist.all_to_all_single(output, input_t, group=group)\n\n    # if scattering the seq-dim, transpose the heads back to the original dimension\n    if scatter_idx < 2:\n        output = output.transpose(0, 2).contiguous()\n\n    return output.reshape(\n        inp_shape[: gather_idx] + \\\n        [inp_shape[gather_idx] * seq_world_size,] + \\\n        inp_shape[gather_idx + 1:]).contiguous()\n\n\nclass _SeqAllToAll(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx: Any, group: dist.ProcessGroup, input: Tensor, scatter_idx: int, gather_idx: int) -> Tensor:\n\n        ctx.group = group\n        ctx.scatter_idx = scatter_idx\n        ctx.gather_idx = gather_idx\n\n        return single_all_to_all(input, scatter_idx, gather_idx, group)\n\n    @staticmethod\n    def backward(ctx: Any, *grad_output: Tensor) -> Tuple[None, Tensor, None, None]:\n        return (None, _SeqAllToAll.apply(ctx.group, *grad_output, ctx.gather_idx, ctx.scatter_idx), None, None)\n\n\nclass DistributedAttention(torch.nn.Module):\n    \"\"\"Initialization.\n\n    Arguments:\n        local_attention (Module): local attention with q,k,v\n        sequence_process_group (ProcessGroup): sequence parallel process group\n        scatter_idx (int): scatter_idx for all2all comm\n        gather_idx (int): gather_idx for all2all comm\n    \"\"\"\n\n    def __init__(\n        self,\n        local_attention: Module,\n        sequence_process_group: dist.ProcessGroup,\n        scatter_idx: int = 2,\n        gather_idx: int = 0,\n    ) -> None:\n\n        super(DistributedAttention, self).__init__()\n        self.local_attn = local_attention\n        self.spg = sequence_process_group\n        self.scatter_idx = scatter_idx\n        self.gather_idx = gather_idx\n\n    def forward(self, query: Tensor, key: Tensor, value: Tensor, *args: Any, **kwargs) -> Tensor:\n        \"\"\" forward\n\n        Arguments:\n            query (Tensor): query input to the layer\n            key (Tensor): key input to the layer\n            value (Tensor): value input to the layer\n            args: other args\n\n        Returns:\n            * output (Tensor): context output\n        \"\"\"\n        # TODO Merge three alltoall calls into one\n        # TODO (Reza): change the api on the megatron-deepspeed side so that we only receive all data (q,k, and v) together!\n        #in shape : e.g.,  [s/p:h:]\n        query_layer = _SeqAllToAll.apply(self.spg, query, self.scatter_idx, self.gather_idx)\n        key_layer = _SeqAllToAll.apply(self.spg, key, self.scatter_idx, self.gather_idx)\n        value_layer = _SeqAllToAll.apply(self.spg, value, self.scatter_idx, self.gather_idx)\n\n        #out shape : e.g., [s:h/p:]\n        context_layer = self.local_attn(query_layer, key_layer, value_layer, *args, **kwargs)\n\n        output = _SeqAllToAll.apply(self.spg, context_layer, self.gather_idx, self.scatter_idx)\n\n        #out e.g., [s/p::h]\n        return output\n", "deepspeed/sequence/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n", "deepspeed/moe/utils.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom collections import defaultdict\nfrom typing import Any, Dict, List, Set, Tuple, Union, cast\n\nimport torch\nfrom torch import nn\n\nfrom .layer import MoE\n\n\ndef has_moe_layers(m: nn.Module) -> Tuple[bool, int]:\n    has_moe = False\n    num_experts = 0\n\n    for module in m.modules():\n        if isinstance(module, MoE):\n            has_moe = True\n            num_experts = module.num_experts\n            break\n    return has_moe, num_experts\n\n\ndef is_moe_param(param: torch.Tensor) -> bool:\n    if hasattr(param, \"allreduce\") and not param.allreduce:\n        return True\n    return False\n\n\ndef split_params_into_shared_and_expert_params(\n        params: List[torch.nn.Parameter]) -> Tuple[List[torch.nn.Parameter], List[torch.nn.Parameter]]:\n    shared_params: List[nn.Parameter] = []\n    expert_params: List[nn.Parameter] = []\n\n    for p in params:\n        if is_moe_param(p):\n            expert_params.append(p)\n        else:\n            shared_params.append(p)\n    return shared_params, expert_params\n\n\ndef split_params_grads_into_shared_and_expert_params(\n        group: List[torch.nn.Parameter]) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n    \"\"\"Split grad of parameters into grads of non-expert params\n    and grads of expert params. This is useful while computing\n    grad-norms for clipping and overflow detection\n\n        group (List[torch.nn.Parameter]):\n    Args:\n            The group of parameters to split\n\n    Returns:\n        Tuple[List[torch.Tensor], List[torch.Tensor]]:\n        list of gradients for non MoE params, list of gradients of MoE params\n    \"\"\"\n    expert_grads: List[torch.Tensor] = []\n    shared_grads: List[torch.Tensor] = []\n\n    for p in group:\n        if p.grad is not None:\n            if is_moe_param(p):\n                expert_grads.append(p.grad.to(p.dtype))\n            else:\n                shared_grads.append(p.grad.to(p.dtype))\n    return shared_grads, expert_grads\n\n\ndef split_params_into_different_moe_groups_for_optimizer(\n        param_groups: Union[Dict[str, Any], Tuple[Dict[str, Any], ...], List[Dict[str, Any]]],\n        max_group_size: Union[int, float] = 178956971) -> List[Dict[str, Any]]:\n    \"\"\"Split parameters into different MoE groups for optimizer\n\n    Args:\n        param_groups (Union[Dict[str, Any], Tuple[Dict[str, Any], ...], List[Dict[str, Any]]])\n            The list of parameter groups to split\n\n    Returns:\n        List[Dict[str, Any]]:\n        list of MoE/non-MoE groups for optimizer\n    \"\"\"\n    if isinstance(param_groups, tuple):\n        param_groups = list(param_groups)  # Tuple cannot be modified\n    elif isinstance(param_groups, dict):\n        param_groups = [param_groups]\n    elif not isinstance(param_groups, list):\n        raise ValueError(f\"Unknown param group type of {type(param_groups)}\")\n\n    # gather all data parallel group names\n    data_parallel_group_names: Set[str] = set()\n    for param_group in param_groups:\n        for param in cast(List[nn.Parameter], param_group[\"params\"]):\n            if is_moe_param(param):\n                data_parallel_group_names.add(param.group_name)\n\n    # Create the param MoE groups, leave param assign to next step\n    group_moe: Dict[str, Dict[str, Dict[str, Any]]] = defaultdict(lambda: defaultdict(dict))\n    for param_group in param_groups:\n        for key in data_parallel_group_names:\n            group_moe[param_group['name']][key] = {\n                **param_group,\n                'name': key,\n                'moe': True,\n                'params': [],\n            }\n\n    # Assign param\n    for param_group in param_groups:\n        new_params: List[nn.Parameter] = []\n\n        for param in cast(List[nn.Parameter], param_group['params']):\n            if is_moe_param(param):\n                group_moe[param_group['name']][param.group_name]['params'].append(param)\n            else:\n                new_params.append(param)\n        param_group['params'] = new_params\n\n    # Flatten the moe groups\n    if max_group_size is not None:\n        for moe_group in group_moe.values():\n            for param_group in moe_group.values():\n                cur_group: List[nn.Parameter] = []\n                all_groups: List[List[nn.Parameter]] = []\n                size_of_cur_group = 0\n\n                for param in cast(List[nn.Parameter], param_group['params']):\n                    if size_of_cur_group + param.numel() <= max_group_size:\n                        cur_group.append(param)\n                        size_of_cur_group += param.numel()\n                    else:\n                        all_groups.append(cur_group)\n                        cur_group = [param]\n                        size_of_cur_group = param.numel()\n\n                if cur_group:\n                    all_groups.append(cur_group)\n\n                for group in all_groups:\n                    param_groups.append({**param_group, 'params': group})\n    else:\n        for moe_group in group_moe.values():\n            for param_group in moe_group.values():\n                param_groups.append(param_group)\n\n    return param_groups\n\n\ndef is_moe_param_group(param_group):\n    return param_group.get('moe', False)\n\n\ndef configure_moe_param_groups(model_parameters: List):\n    assert isinstance(model_parameters, list), \"model_parameters must be a list\"\n\n    for p in model_parameters:\n        # match torch.optim.Optimizer expectations,\n        # see: https://github.com/pytorch/pytorch/blob/2ffab6e663b9c6951048b8c8ba82d2cc5ca5c2fc/torch/optim/optimizer.py#L270-L272\n        if not isinstance(p, (torch.Tensor, dict)):\n            raise TypeError(\"param argument that would be given to the optimizer should be \"\n                            f\"an iterable of Tensors or dicts, but got {type(p)}\")\n\n    # peak at the first element to determine how to proceed\n    first = model_parameters[0]\n\n    # Case 1: model_parameters is a list of torch.nn.Parameter\n    #   -> need to create moe compatible param groups\n    if isinstance(first, torch.nn.Parameter):\n        param_group = {'params': model_parameters, 'name': 'dense-params'}\n        return split_params_into_different_moe_groups_for_optimizer(param_group)\n\n    # Case 2: model_parameters is a list of param groups List[dict]\n    #   -> moe compatible param groups might already exist, if not create them\n    elif isinstance(first, dict):\n        #there are no moe groups created\n        if not any(['moe' in param_group for param_group in model_parameters]):\n            return split_params_into_different_moe_groups_for_optimizer(model_parameters)\n        else:\n            # moe groups exist, nothing to do\n            return model_parameters\n", "deepspeed/moe/sharded_moe.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\"\"\"\nThe file has been adapted from two fairscale files:\n (1) https://github.com/facebookresearch/fairscale/blob/master/fairscale/nn/moe/moe_layer.py\n (2) https://github.com/facebookresearch/fairscale/blob/master/fairscale/nn/moe/top2gate.py\n Git commit hash: 34df606902a240567a0d898037ece55c2f1336cf\n We retain the following license from the original files:\n\"\"\"\n\n# Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.\n#\n# This source code is licensed under the BSD license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom deepspeed.utils.timer import SynchronizedWallClockTimer\nfrom deepspeed.utils import logger\nfrom deepspeed.utils.bwc import bwc_tensor_model_parallel_world_size\nfrom typing import Callable, Dict, TYPE_CHECKING, Any, Optional, Tuple, Union\n\nimport torch\nfrom torch import Tensor\nfrom torch.nn import Module\nimport torch.nn.functional as F\nfrom deepspeed.utils import groups\nfrom .mappings import drop_tokens, gather_tokens\n\nif TYPE_CHECKING:\n    Base = Module[Tensor]\nelse:\n    Base = Module\n\nTOPK_GATE_TIMER = 'topk_gate'\nMOE_TIMER = 'moe'\nFIRST_ALLTOALL_TIMER = '1st_a2a'\nSECOND_ALLTOALL_TIMER = '2nd_a2a'\n\nuniform_map: Dict[torch.device, Callable] = {}\ngumbel_map: Dict[torch.device, Callable] = {}\nexp_selection_uniform_map: Dict[torch.device, Callable] = {}\n\ntry:\n    # To enable Tutel MoE optimizations:\n    #   python3 -m pip install --user --upgrade git+https://github.com/microsoft/tutel@v0.1.x\n    from tutel import moe as tutel_moe\n    TUTEL_INSTALLED = True\nexcept:\n    # Fail silently so we don't spam logs unnecessarily if user isn't using tutel\n    TUTEL_INSTALLED = False\n    pass\n\n\ndef multiplicative_jitter(x, device: torch.device, epsilon=1e-2):\n    \"\"\"\n    Modified from switch transformer paper. mesh transformers\n    Multiply values by a random number between 1-epsilon and 1+epsilon.\n    Makes models more resilient to rounding errors introduced by bfloat16.\n    This seems particularly important for logits.\n    Args:\n        x: a torch.tensor\n        device: torch.device\n        epsilon: a floating point value\n    Returns:\n        a jittered x.\n    \"\"\"\n    if epsilon == 0:\n        return x\n    uniform = uniform_map.get(device)\n    if uniform is None:\n        uniform = torch.distributions.uniform.Uniform(low=torch.tensor(1.0 - epsilon, device=device),\n                                                      high=torch.tensor(1.0 + epsilon,\n                                                                        device=device)).rsample  # type: ignore\n        uniform_map[device] = uniform\n    return x * uniform(x.shape)\n\n\ndef gumbel_rsample(shape: Tuple, device: torch.device) -> Tensor:\n    gumbel = gumbel_map.get(device)\n    if gumbel is None:\n        one = torch.tensor(1.0, device=device)\n        zero = torch.tensor(0.0, device=device)\n        gumbel = torch.distributions.gumbel.Gumbel(zero, one).rsample  # type: ignore\n        gumbel_map[device] = gumbel\n    return gumbel(shape)\n\n\nfrom deepspeed import comm as dist\n\n# einsum dimensions: (g)roup, (s)equence, (e)xpert, (m)odel, (c)apacity\n# See https://arxiv.org/pdf/2006.16668.pdf for details.\n\n\n# Based on https://github.com/pytorch/pytorch/pull/40762\nclass _AllToAll(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx: Any, group: dist.ProcessGroup, input: Tensor) -> Tensor:  # type: ignore\n        ctx.group = group\n        input = input.contiguous()\n        output = torch.empty_like(input)\n        dist.all_to_all_single(output, input, group=group)\n        return output\n\n    @staticmethod\n    def backward(ctx: Any, *grad_output: Tensor) -> Tuple[None, Tensor]:\n        return (None, _AllToAll.apply(ctx.group, *grad_output))\n\n\n# einsum rewrites are on par or more performant\n# switch can be bubbled up in future\nUSE_EINSUM = True\n\n\n# einsum dimensions: (g)roup, (s)equence, (e)xpert, (m)odel, (c)apacity\n# See https://arxiv.org/pdf/2006.16668.pdf for details.\ndef einsum(rule, a, b):\n    if USE_EINSUM:\n        return torch.einsum(rule, a, b)\n    elif rule == 's,se->se':\n        return a.reshape(a.shape[0], -1) * b\n    elif rule == 'se,sc->sec':\n        return a.unsqueeze(2) * b.unsqueeze(1)\n    elif rule == 'se,se->s':\n        return torch.bmm(a.unsqueeze(1), b.unsqueeze(2)).reshape(-1)\n    elif rule == 'sec,sm->ecm':\n        s = a.shape[0]\n        e = a.shape[1]\n        c = a.shape[2]\n        m = b.shape[1]\n        return torch.matmul(a.reshape(s, -1).t(), b).reshape(e, c, m)\n    elif rule == 'sec,ecm->sm':\n        return torch.matmul(a.reshape(a.shape[0], -1), b.reshape(-1, b.shape[-1]))\n    elif rule == 'ks,ksm->sm':\n        k = b.shape[0]\n        s = b.shape[1]\n        m = b.shape[2]\n        # [k, s] -> [s, k] -> [s, 1, k]\n        a = a.t().unsqueeze(1)\n        # [k,s,m] -> [k, sm] -> [sm, k] -> [s, m, k]\n        b = b.reshape(k, -1).t().reshape(s, m, k)\n        # bmm([s, 1, k], [s, m, k]^t) -> [s, m, 1]\n        return torch.bmm(a, b.transpose(1, 2)).squeeze(2)\n    else:\n        return torch.einsum(rule, a, b)\n\n\n# The following functions are extracted and scripted\n# because otherwise during a torch.jit.trace, the non-Tensor\n# values used in the calculations get recorded as constants.\n# torch.jit.script coerces them into Tensors and preserves\n# their dynamic shapes. This enables ONNX export.\n# We can't script the entire top1gating function because it\n# includes stateful caching logic which is incompatible with ONNX.\n\n\n@torch.jit.script\ndef _capacity(gates: Tensor, capacity_factor: Tensor, min_capacity: Tensor) -> Tensor:\n    # gates has shape of SE\n    num_tokens = gates.shape[0]\n    num_experts = gates.shape[1]\n    # to(torch.int64) works around a bug in torch.onnx.export:\n    # it should cast k to int64 when converting torch.topk but it doesn't.\n    capacity = torch.ceil((num_tokens / num_experts) * capacity_factor).to(torch.int64)\n    if capacity < min_capacity:\n        capacity = min_capacity.to(torch.int64)\n    return capacity\n\n\n@torch.jit.script\ndef _top_idx(source, k):\n    return torch.topk(source, k=k, dim=0)[1]\n\n\n@torch.jit.script\ndef _one_hot_to_float(x, num_classes):\n    return F.one_hot(x, num_classes=num_classes).float()\n\n\ndef top1gating(logits: Tensor,\n               capacity_factor: float,\n               min_capacity: int,\n               used_token: Tensor = None,\n               noisy_gate_policy: Optional[str] = None,\n               drop_tokens: bool = True,\n               use_rts: bool = True,\n               ep_group: Union[torch.distributed.ProcessGroup, None] = None,\n               use_tutel: bool = False) -> Tuple[Tensor, Tensor, Tensor, Tensor]:\n    \"\"\"Implements Top1Gating on logits.\"\"\"\n    if noisy_gate_policy == 'RSample':\n        logits_w_noise = logits + gumbel_rsample(logits.shape, device=logits.device)\n    # everything is in fp32 in this function\n    gates = F.softmax(logits, dim=1)\n\n    capacity = _capacity(gates, torch.tensor(capacity_factor), torch.tensor(min_capacity))\n\n    # Create a mask for 1st's expert per token\n    # noisy gating\n    indices1_s = torch.argmax(logits_w_noise if noisy_gate_policy == 'RSample' else gates, dim=1)\n    num_experts = int(gates.shape[1])\n    mask1 = F.one_hot(indices1_s, num_classes=num_experts)\n\n    # mask only used tokens\n    if used_token is not None:\n        mask1 = einsum(\"s,se->se\", used_token, mask1)\n\n    # gating decisions\n    exp_counts = torch.sum(mask1, dim=0).detach().to('cpu')\n\n    # if we don't want to drop any tokens\n    if not drop_tokens:\n        new_capacity = torch.max(exp_counts).to(logits.device)\n        # Communicate across expert processes to pick the maximum capacity.\n        if ep_group is not None:\n            dist.all_reduce(new_capacity, op=dist.ReduceOp.MAX, group=ep_group)\n        if groups._get_expert_model_parallel_world_size() == 1:\n            # If the non-expert is tensor-parallel, we need to pad the capacity to 'tp'.\n            # This is since we are going to activate drop_tokens() to drop duplicate tokens.\n            tp = 1 if groups.mpu is None else bwc_tensor_model_parallel_world_size(mpu=groups.mpu)\n            new_capacity = torch.ceil(new_capacity / tp).mul(tp).to(new_capacity.dtype)\n        # Make sure the capacity value does not exceed the number of tokens.\n        capacity = min(new_capacity, torch.tensor(mask1.size(0)).to(new_capacity.device))\n\n    # Compute l_aux\n    me = torch.mean(gates, dim=0)\n    ce = torch.mean(mask1.float(), dim=0)\n    l_aux = torch.sum(me * ce) * num_experts\n\n    # Random Token Selection\n    if use_rts:\n        uniform = exp_selection_uniform_map.get(logits.device)\n        if uniform is None:\n            uniform = torch.distributions.uniform.Uniform(low=torch.tensor(0.0, device=logits.device),\n                                                          high=torch.tensor(1.0, device=logits.device)).rsample\n            exp_selection_uniform_map[logits.device] = uniform\n\n        mask1_rand = mask1 * uniform(mask1.shape)\n    else:\n        mask1_rand = mask1\n\n    assert logits.shape[\n        0] >= min_capacity, \"No. of tokens (batch-size) should be greater than min_capacity. Either set min_capacity to 0 or increase your batch size.\"\n\n    top_idx = _top_idx(mask1_rand, capacity)\n\n    new_mask1 = mask1 * torch.zeros_like(mask1).scatter_(0, top_idx, 1)\n    mask1 = new_mask1\n\n    if use_tutel:\n        # Tutel doesn't support index values masked with zero\n        # so we need to replace masked indices with -1\n        indices_mask = mask1.sum(dim=1) * num_experts - 1\n        indices1_s = torch.min(indices1_s, indices_mask)\n\n    # Compute locations in capacity buffer\n    if use_tutel:\n        locations1 = tutel_moe.fast_cumsum_sub_one(mask1)\n    else:\n        locations1 = torch.cumsum(mask1, dim=0) - 1\n\n    if use_tutel:\n        gates1_s = (gates * mask1).sum(dim=1)\n        locations1_s = torch.sum(locations1 * mask1, dim=1)\n        return l_aux, capacity, num_experts, [\n            indices1_s,\n        ], [\n            locations1_s,\n        ], [\n            gates1_s,\n        ], exp_counts\n\n    # Store the capacity location for each token\n    locations1_s = torch.sum(locations1 * mask1, dim=1)\n\n    # Normalize gate probabilities\n    mask1_float = mask1.float()\n    gates = gates * mask1_float\n\n    locations1_sc = _one_hot_to_float(locations1_s, capacity)\n    combine_weights = einsum(\"se,sc->sec\", gates, locations1_sc)\n\n    dispatch_mask = combine_weights.bool()\n\n    return l_aux, combine_weights, dispatch_mask, exp_counts\n\n\ndef top2gating(logits: Tensor,\n               capacity_factor: float,\n               min_capacity: int,\n               drop_tokens: bool = True,\n               ep_group: Union[torch.distributed.ProcessGroup, None] = None,\n               top2_2nd_expert_sampling: bool = True) -> Tuple[Tensor, Tensor, Tensor, Tensor]:\n    \"\"\"Implements Top2Gating on logits.\"\"\"\n    # everything is in fp32 in this function\n    gates = F.softmax(logits, dim=1)\n\n    # Create a mask for 1st's expert per token\n    indices1_s = torch.argmax(gates, dim=1)\n    num_experts = int(gates.shape[1])\n    mask1 = F.one_hot(indices1_s, num_classes=num_experts)\n\n    if top2_2nd_expert_sampling:\n        # Create a mask for 2nd's expert per token using Gumbel-max trick\n        # https://timvieira.github.io/blog/post/2014/07/31/gumbel-max-trick/\n        logits += gumbel_rsample(logits.shape, device=logits.device)\n\n    # Replace top-expert with min value\n    logits_except1 = logits.masked_fill(mask1.bool(), float(\"-inf\"))\n    indices2_s = torch.argmax(logits_except1, dim=1)\n    mask2 = F.one_hot(indices2_s, num_classes=num_experts)\n\n    # Compute locations in capacity buffer\n    locations1 = torch.cumsum(mask1, dim=0) - 1\n    locations2 = torch.cumsum(mask2, dim=0) - 1\n    # Update 2nd's location by accounting for locations of 1st\n    locations2 += torch.sum(mask1, dim=0, keepdim=True)\n\n    # Compute l_aux\n    me = torch.mean(gates, dim=0)\n    ce = torch.mean(mask1.float(), dim=0)\n    l_aux = torch.mean(me * ce) * num_experts * num_experts\n\n    # gating decisions\n    exp_counts = torch.sum(mask1 + mask2, dim=0)\n\n    if drop_tokens:\n        # Calculate configured capacity and remove locations outside capacity from mask\n        capacity = _capacity(gates, torch.tensor(capacity_factor * 2), torch.tensor(min_capacity))\n        mask1 *= torch.lt(locations1, capacity)\n        mask2 *= torch.lt(locations2, capacity)\n    else:\n        # Do not drop tokens - set capacity according to current expert assignments\n        new_capacity = torch.max(exp_counts)\n        if ep_group is not None:\n            dist.all_reduce(new_capacity, op=dist.ReduceOp.MAX, group=ep_group)\n        if groups._get_expert_model_parallel_world_size() == 1:\n            # If the non-expert is tensor-parallel, we need to pad the capacity to 'tp'.\n            # This is since we are going to activate drop_tokens() to drop duplicate tokens.\n            tp = 1 if groups.mpu is None else bwc_tensor_model_parallel_world_size(mpu=groups.mpu)\n            new_capacity = torch.ceil(new_capacity / tp).mul(tp).to(new_capacity.dtype)\n        capacity = new_capacity\n\n    # Store the capacity location for each token\n    locations1_s = torch.sum(locations1 * mask1, dim=1)\n    locations2_s = torch.sum(locations2 * mask2, dim=1)\n\n    # Normalize gate probabilities\n    mask1_float = mask1.float()\n    mask2_float = mask2.float()\n    gates1_s = einsum(\"se,se->s\", gates, mask1_float)\n    gates2_s = einsum(\"se,se->s\", gates, mask2_float)\n    denom_s = gates1_s + gates2_s\n    # Avoid divide-by-zero\n    denom_s = torch.clamp(denom_s, min=torch.finfo(denom_s.dtype).eps)\n    gates1_s /= denom_s\n    gates2_s /= denom_s\n\n    # Calculate combine_weights and dispatch_mask\n    gates1 = einsum(\"s,se->se\", gates1_s, mask1_float)\n    gates2 = einsum(\"s,se->se\", gates2_s, mask2_float)\n    locations1_sc = _one_hot_to_float(locations1_s, capacity)\n    locations2_sc = _one_hot_to_float(locations2_s, capacity)\n    combine1_sec = einsum(\"se,sc->sec\", gates1, locations1_sc)\n    combine2_sec = einsum(\"se,sc->sec\", gates2, locations2_sc)\n    combine_weights = combine1_sec + combine2_sec\n    dispatch_mask = combine_weights.bool()\n\n    return l_aux, combine_weights, dispatch_mask, exp_counts.detach().to('cpu')\n\n\nclass TopKGate(Module):\n    \"\"\"Gate module which implements Top2Gating as described in Gshard_.\n    ::\n\n        gate = TopKGate(model_dim, num_experts)\n        l_aux, combine_weights, dispatch_mask = gate(input)\n\n    .. Gshard_: https://arxiv.org/pdf/2006.16668.pdf\n\n    Args:\n        model_dim (int):\n            size of model embedding dimension\n        num_experts (int):\n            number of experts in model\n    \"\"\"\n\n    wg: torch.nn.Linear\n\n    def __init__(self,\n                 model_dim: int,\n                 num_experts: int,\n                 k: int = 1,\n                 capacity_factor: float = 1.0,\n                 eval_capacity_factor: float = 1.0,\n                 min_capacity: int = 8,\n                 noisy_gate_policy: Optional[str] = None,\n                 drop_tokens: bool = True,\n                 use_rts: bool = True,\n                 ep_group: Union[torch.distributed.ProcessGroup, None] = None,\n                 top2_2nd_expert_sampling: bool = True) -> None:\n        super().__init__()\n\n        # Only top-1 and top-2 are supported at the moment.\n        if k != 1 and k != 2:\n            raise ValueError('Only top-1 and top-2 gatings are supported.')\n        self.wg = torch.nn.Linear(model_dim, num_experts, bias=False)\n        self.ep_group = ep_group\n        self.k = k\n        self.capacity_factor = capacity_factor\n        self.eval_capacity_factor = eval_capacity_factor\n        self.min_capacity = min_capacity\n        self.noisy_gate_policy = noisy_gate_policy\n        self.timers = SynchronizedWallClockTimer()\n        self.wall_clock_breakdown = False\n        self.gate_time = 0.0\n        self.drop_tokens = drop_tokens\n        self.use_rts = use_rts\n        self.top2_2nd_expert_sampling = top2_2nd_expert_sampling\n\n    def _set_ep_group(self, ep_group):\n        assert self.ep_group is None, f'Attempting to override an existing ep_group'\n        self.ep_group = ep_group\n\n    def forward(self,\n                input: torch.Tensor,\n                used_token: torch.Tensor = None,\n                use_tutel: bool = False) -> Tuple[Tensor, Tensor, Tensor]:  # type: ignore\n\n        if self.wall_clock_breakdown:\n            self.timers(TOPK_GATE_TIMER).start()\n\n        input_fp32 = input.float()\n        # input jittering\n        if self.noisy_gate_policy == 'Jitter' and self.training:\n            input_fp32 = multiplicative_jitter(input_fp32, device=input.device)\n        logits = torch.nn.functional.linear(input_fp32, weight=self.wg.weight.float(), bias=None)\n\n        if self.k == 1:\n            gate_output = top1gating(logits, self.capacity_factor if self.training else self.eval_capacity_factor,\n                                     self.min_capacity, used_token, self.noisy_gate_policy if self.training else None,\n                                     self.drop_tokens, self.use_rts, self.ep_group, use_tutel)\n\n        else:\n            gate_output = top2gating(logits, self.capacity_factor if self.training else self.eval_capacity_factor,\n                                     self.min_capacity, self.drop_tokens, self.ep_group, self.top2_2nd_expert_sampling)\n\n        if self.wall_clock_breakdown:\n            self.timers(TOPK_GATE_TIMER).stop()\n            self.gate_time = self.timers(TOPK_GATE_TIMER).elapsed(reset=False)\n\n        return gate_output\n\n\nclass MOELayer(Base):\n    \"\"\"MOELayer module which implements MixtureOfExperts as described in Gshard_.\n    ::\n\n        gate = TopKGate(model_dim, num_experts)\n        moe = MOELayer(gate, expert)\n        output = moe(input)\n        l_aux = moe.l_aux\n\n    .. Gshard_: https://arxiv.org/pdf/2006.16668.pdf\n\n    Args:\n        gate (torch.nn.Module):\n            gate network\n        expert (torch.nn.Module):\n            expert network\n    \"\"\"\n\n    def __init__(self,\n                 gate: Module,\n                 experts: Module,\n                 ep_group_name,\n                 ep_size,\n                 num_local_experts: int,\n                 use_tutel: bool = False) -> None:\n        super().__init__()\n        self.gate = gate\n        self.experts = experts\n        self.ep_group = None\n        self.ep_size = ep_size\n        self.ep_group_name = ep_group_name\n        self.num_local_experts = num_local_experts\n        self.time_falltoall = 0.0\n        self.time_salltoall = 0.0\n        self.time_moe = 0.0\n        self.timers = SynchronizedWallClockTimer()\n        self.wall_clock_breakdown = False\n\n        self.use_tutel = use_tutel and TUTEL_INSTALLED and gate.k == 1\n\n        if self.use_tutel:\n            logger.info('Using Tutel optimizations.')\n        elif use_tutel and not TUTEL_INSTALLED:\n            logger.warning(\"Tutel optimization requested but not installed. \"\n                           \"Proceeding without Tutel.\")\n        elif use_tutel and TUTEL_INSTALLED and gate.k != 1:\n            logger.warning(\"To enable Tutel optimization, use top-1 instead of top-2 gate. \"\n                           \"Proceeding without Tutel.\")\n\n    def _set_ep_group(self, ep_group):\n        self.ep_group = ep_group\n        self.gate._set_ep_group(ep_group)\n\n    def forward(self, *input: Tensor, **kwargs: Any) -> Tensor:\n\n        if self.wall_clock_breakdown:\n            self.timers(MOE_TIMER).start()\n\n        # Implement Algorithm 2 from GShard paper.\n        d_model = input[0].shape[-1]\n\n        # Initial implementation -> Reshape into S tokens by dropping sequence dimension.\n        # Reshape into G groups so that each group can distribute tokens equally\n        # group_size = kwargs['group_size'] if 'group_size' in kwargs.keys() else 1\n        reshaped_input = input[0].reshape(-1, d_model)\n\n        if self.use_tutel:\n            self.l_aux, C, E, indices_, locations_, gates_, self.exp_counts = self.gate(reshaped_input, input[1], True)\n            S, M = reshaped_input.size(0), reshaped_input.size(1)\n\n            if not hasattr(self, '_tutel_dispatcher'):\n                self._tutel_dispatcher = tutel_moe.fast_dispatcher(E, C, M, dispatch_dtype=reshaped_input.dtype)\n            self._tutel_dispatcher.update(indices_, locations_, gates_, capacity=C)\n            dispatched_input = self._tutel_dispatcher.encode(reshaped_input)\n        else:\n            self.l_aux, combine_weights, dispatch_mask, self.exp_counts = self.gate(reshaped_input, input[1])\n            dispatched_input = einsum(\"sec,sm->ecm\", dispatch_mask.type_as(input[0]), reshaped_input)\n\n        if self.wall_clock_breakdown:\n            self.timers(FIRST_ALLTOALL_TIMER).start()\n\n        if groups._get_expert_model_parallel_world_size() == 1:\n            # If the non-expert is tensor-parallel, it will create\n            # duplicate tokens on the tensor-parallel ranks.\n            # Since our experts are not tensor-parallel, these duplicates\n            # need to be dropped to ensure correctness.\n            # this also doubles up as a communication optimization as we are\n            # reducing the all-to-all communication volume.\n            dispatched_input = drop_tokens(dispatched_input, dim=1)\n\n        dispatched_input = _AllToAll.apply(self.ep_group, dispatched_input)\n\n        if self.wall_clock_breakdown:\n            self.timers(FIRST_ALLTOALL_TIMER).stop()\n            self.time_falltoall = self.timers(FIRST_ALLTOALL_TIMER).elapsed(reset=False)\n\n        # Re-shape after all-to-all: ecm -> gecm\n        dispatched_input = dispatched_input.reshape(self.ep_size, self.num_local_experts, -1, d_model)\n\n        expert_output = self.experts(dispatched_input)\n\n        if self.wall_clock_breakdown:\n            self.timers(SECOND_ALLTOALL_TIMER).start()\n\n        expert_output = _AllToAll.apply(self.ep_group, expert_output)\n\n        if self.wall_clock_breakdown:\n            self.timers(SECOND_ALLTOALL_TIMER).stop()\n            self.time_salltoall = self.timers(SECOND_ALLTOALL_TIMER).elapsed(reset=False)\n\n        # Re-shape back: gecm -> ecm\n        expert_output = expert_output.reshape(self.ep_size * self.num_local_experts, -1, d_model)\n\n        if groups._get_expert_model_parallel_world_size() == 1:\n            # the dropped duplicate tokens need to be gathered on each\n            # tensor parallel rank again for the tensor-parallel\n            # non-expert of the next layer.\n            expert_output = gather_tokens(expert_output, dim=1)\n\n        if self.use_tutel:\n            combined_output = self._tutel_dispatcher.decode(expert_output.view(E * C, M))\n        else:\n            combined_output = einsum(\"sec,ecm->sm\", combine_weights.type_as(input[0]), expert_output)\n\n        a = combined_output.reshape(input[0].shape)\n\n        if self.wall_clock_breakdown:\n            self.timers(MOE_TIMER).stop()\n            self.time_moe = self.timers(MOE_TIMER).elapsed(reset=False)\n\n        return a\n", "deepspeed/moe/mappings.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\n# The file has been adapted from the following Megatron-LM file:\n# https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/mpu/mappings.py\n# Git commit hash: 9dc3c42a84aa656f583703cf8b6b4f79f712b796\n# We retain the following copyright from the original files:\n\n# Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport torch\nimport deepspeed\nfrom deepspeed.utils.bwc import (bwc_tensor_model_parallel_world_size, bwc_tensor_model_parallel_rank,\n                                 bwc_tensor_model_parallel_group)\n\n\ndef _gather_tokens(input_, dim=0):\n    \"\"\"Gather tensors and concatenate them along a dimension\"\"\"\n    mpu = deepspeed.utils.groups.mpu\n\n    input_ = input_.contiguous()\n    # Size and dimension.\n    rank = bwc_tensor_model_parallel_rank(mpu)\n\n    tensor_list = [torch.empty_like(input_) for _ in range(bwc_tensor_model_parallel_world_size(mpu))]\n    tensor_list[rank] = input_\n    deepspeed.comm.all_gather(tensor_list, input_, group=bwc_tensor_model_parallel_group(mpu))\n\n    # Note: torch.cat already creates a contiguous tensor.\n    output = torch.cat(tensor_list, dim=dim).contiguous()\n\n    return output\n\n\ndef _drop_tokens(input_, dim=0):\n    \"\"\"Divide a tensor among the tensor parallel ranks\"\"\"\n    mpu = deepspeed.utils.groups.mpu\n\n    total_chunks = bwc_tensor_model_parallel_world_size(mpu)\n    this_chunk = bwc_tensor_model_parallel_rank(mpu)\n    assert input_.shape[\n        dim] % total_chunks == 0, f\"input dimension {dim} ({input_.shape[dim]}) is not divisible by tensor parallel world size ({total_chunks})\"\n    chunk_size = input_.shape[dim] // total_chunks\n\n    return torch.narrow(input_, dim, this_chunk * chunk_size, chunk_size)\n\n\nclass _GatherTokens(torch.autograd.Function):\n    \"\"\"All gather tokens among the tensor parallel ranks\"\"\"\n\n    @staticmethod\n    def symbolic(graph, input_, dim):\n        return _gather_tokens(input_, dim)\n\n    @staticmethod\n    def forward(ctx, input_, dim):\n        ctx.dim = dim\n        return _gather_tokens(input_, dim)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return _drop_tokens(grad_output, ctx.dim), None\n\n\nclass _DropTokens(torch.autograd.Function):\n    \"Divide tokens equally among the tensor parallel ranks\"\n\n    @staticmethod\n    def symbolic(graph, input_, dim):\n        return _drop_tokens(input_, dim)\n\n    @staticmethod\n    def forward(ctx, input_, dim):\n        ctx.dim = dim\n        return _drop_tokens(input_, dim)\n\n    @staticmethod\n    def backward(ctx, input_):\n        return _gather_tokens(input_, ctx.dim), None\n\n\ndef gather_tokens(input_, dim=0):\n    mpu = deepspeed.utils.groups.mpu\n    if mpu is None or bwc_tensor_model_parallel_world_size(mpu) == 1:\n        # no tensor parallelism for non-experts\n        return input_\n    return _GatherTokens.apply(input_, dim)\n\n\ndef drop_tokens(input_, dim=0):\n    mpu = deepspeed.utils.groups.mpu\n    if mpu is None or bwc_tensor_model_parallel_world_size(mpu) == 1:\n        # no tensor parallelism for non-experts\n        return input_\n    return _DropTokens.apply(input_, dim)\n", "deepspeed/moe/layer.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Optional, Tuple\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom deepspeed.utils import groups, log_dist\nfrom .experts import Experts\nfrom .sharded_moe import MOELayer, TopKGate\n\n\nclass MoE(nn.Module):\n    \"\"\"Initialize an MoE layer.\n\n    Arguments:\n        hidden_size (int): the hidden dimension of the model, importantly this is also the input and output dimension.\n        expert (nn.Module): the torch module that defines the expert (e.g., MLP, torch.linear).\n        num_experts (int, optional): default=1, the total number of experts per layer.\n        ep_size (int, optional): default=1, number of ranks in the expert parallel world or group.\n        k (int, optional): default=1, top-k gating value, only supports k=1 or k=2.\n        capacity_factor (float, optional): default=1.0, the capacity of the expert at training time.\n        eval_capacity_factor (float, optional): default=1.0, the capacity of the expert at eval time.\n        min_capacity (int, optional): default=4, the minimum capacity per expert regardless of the capacity_factor.\n        use_residual (bool, optional): default=False, make this MoE layer a Residual MoE (https://arxiv.org/abs/2201.05596) layer.\n        noisy_gate_policy (str, optional): default=None, noisy gate policy, valid options are 'Jitter', 'RSample' or 'None'.\n        drop_tokens (bool, optional): default=True, whether to drop tokens - (setting to False is equivalent to infinite capacity).\n        use_rts (bool, optional): default=True, whether to use Random Token Selection.\n        use_tutel (bool, optional): default=False, whether to use Tutel optimizations (if installed).\n        enable_expert_tensor_parallelism (bool, optional): default=False, whether to use tensor parallelism for experts\n        top2_2nd_expert_sampling (bool, optional): default=True, whether to perform sampling for 2nd expert\n    \"\"\"\n\n    def __init__(self,\n                 hidden_size: int,\n                 expert: nn.Module,\n                 num_experts: int = 1,\n                 ep_size: int = 1,\n                 k: int = 1,\n                 capacity_factor: float = 1.0,\n                 eval_capacity_factor: float = 1.0,\n                 min_capacity: int = 4,\n                 use_residual: bool = False,\n                 noisy_gate_policy: Optional[str] = None,\n                 drop_tokens: bool = True,\n                 use_rts: bool = True,\n                 use_tutel: bool = False,\n                 enable_expert_tensor_parallelism: bool = False,\n                 top2_2nd_expert_sampling: bool = True) -> None:\n\n        super(MoE, self).__init__()\n\n        self.use_residual = use_residual\n        self.enable_expert_tensor_parallelism = enable_expert_tensor_parallelism\n        assert num_experts % ep_size == 0, f\"Number of experts ({num_experts}) should be divisible by expert parallel size ({ep_size})\"\n        self.ep_size = ep_size\n        self.expert_group_name = f\"ep_size_{self.ep_size}\"\n        self.num_experts = num_experts\n        self.num_local_experts = num_experts // self.ep_size\n\n        log_dist(\n            f'Creating MoE layer with num_experts: {num_experts} | num_local_experts: {self.num_local_experts} | expert_parallel_size: {self.ep_size}',\n            [0])\n\n        assert noisy_gate_policy is None or noisy_gate_policy in ['None', 'Jitter', 'RSample'], \\\n            'Unsupported noisy_gate_policy: ' + noisy_gate_policy\n\n        experts = Experts(expert, self.num_local_experts, self.expert_group_name)\n        self.deepspeed_moe = MOELayer(TopKGate(hidden_size, num_experts, k, capacity_factor, eval_capacity_factor,\n                                               min_capacity, noisy_gate_policy, drop_tokens, use_rts, None,\n                                               top2_2nd_expert_sampling),\n                                      experts,\n                                      self.expert_group_name,\n                                      self.ep_size,\n                                      self.num_local_experts,\n                                      use_tutel=use_tutel)\n        if self.use_residual:\n            self.mlp = expert\n            # coefficient is used for weighted sum of the output of expert and mlp\n            self.coefficient = nn.Linear(hidden_size, 2)\n\n    def set_deepspeed_parallelism(self, use_data_before_expert_parallel_: bool = False) -> None:\n        self._create_process_groups(use_data_before_expert_parallel_=use_data_before_expert_parallel_)\n\n    def _create_process_groups(self, use_data_before_expert_parallel_: bool = False) -> None:\n        # Create process group for a layer if needed\n        if self.expert_group_name not in groups._get_expert_parallel_group_dict():\n            print(f\"No existing process group found, creating a new group named: {self.expert_group_name}\")\n            if (groups.mpu is None) or (not self.enable_expert_tensor_parallelism):\n                # Condition 1 - no groups.mpu means no tensor parallelism\n                # Condition 2 - disabling expert tensor parallelism on purpose\n                groups._create_expert_and_data_parallel(\n                    self.ep_size, use_data_before_expert_parallel_=use_data_before_expert_parallel_)\n            else:\n                # expert tensor parallelism is enabled\n                groups._create_expert_data_and_model_parallel(\n                    self.ep_size, mpu=groups.mpu, use_data_before_expert_parallel_=use_data_before_expert_parallel_)\n        # Set the group handle for the MOELayer (deepspeed_moe) object\n        self.deepspeed_moe._set_ep_group(groups._get_expert_parallel_group(self.expert_group_name))\n\n    def forward(self,\n                hidden_states: torch.Tensor,\n                used_token: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\" MoE forward\n\n        Arguments:\n            hidden_states (Tensor): input to the layer\n            used_token (Tensor, optional): default: None, mask only used tokens\n\n        Returns:\n            A tuple including output, gate loss, and expert count.\n\n            * output (Tensor): output of the model\n\n            * l_aux (Tensor): gate loss value\n\n            * exp_counts (Tensor): expert count\n        \"\"\"\n        output = self.deepspeed_moe(hidden_states, used_token)\n        if self.use_residual:\n            # Residual MoE\n            output_mlp = self.mlp(hidden_states)\n            if isinstance(output_mlp, tuple):\n                output_mlp = output_mlp[0]  # Ignore the bias term for now\n            coef = self.coefficient(hidden_states)\n            coef = F.softmax(coef, dim=-1)\n            output = output * coef[..., 0:1] + output_mlp * coef[..., 1:]\n        return output, self.deepspeed_moe.l_aux, self.deepspeed_moe.exp_counts\n", "deepspeed/moe/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n'''Copyright The Microsoft DeepSpeed Team'''\n", "deepspeed/moe/experts.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport copy\nfrom typing import List, Optional\n\nimport torch\nfrom torch import nn\n\n\nclass Experts(nn.Module):\n\n    def __init__(self, expert: nn.Module, num_local_experts: int = 1, expert_group_name: Optional[str] = None) -> None:\n        super(Experts, self).__init__()\n\n        self.deepspeed_experts = nn.ModuleList([copy.deepcopy(expert) for _ in range(num_local_experts)])\n        self.num_local_experts = num_local_experts\n\n        # TODO: revisit allreduce for moe.gate...\n        for expert in self.deepspeed_experts:\n            # TODO: Create param groups to handle expert + data case (e.g. param.group = moe_group)\n            for param in expert.parameters():\n                param.allreduce = False\n                param.group_name = expert_group_name\n\n    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n        chunks = inputs.chunk(self.num_local_experts, dim=1)\n        expert_outputs: List[torch.Tensor] = []\n\n        for chunk, expert in zip(chunks, self.deepspeed_experts):\n            out = expert(chunk)\n            if isinstance(out, tuple):\n                out = out[0]  # Ignore the bias term for now\n            expert_outputs += [out]\n\n        return torch.cat(expert_outputs, dim=1)\n", "deepspeed/comm/torch.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport deepspeed\nfrom deepspeed import utils\n\nfrom .utils import *\nfrom .backend import *\nfrom .comm import *\nfrom ..runtime import compiler\nfrom deepspeed.utils.torch import required_torch_version\nimport os\n\nDS_COMM_ALL_GATHER_OFF = False\nDS_COMM_REDUCE_SCATTER_OFF = False\nDS_COMM_BROADCAST_OFF = False\nDS_COMM_ALL_REDUCE_OFF = False\nDS_COMM_REDUCE_OFF = False\n\n\ndef build_shm_op():\n    builder = get_accelerator().create_op_builder(\"ShareMemCommBuilder\")\n    if builder is None or not deepspeed.ops.__compatible_ops__[builder.NAME]:\n        return None\n    shm_cpp_module = builder.load()\n    print(f'DeepSpeed {builder.absolute_name()} built successfully')\n    return shm_cpp_module\n\n\ndef has_coalescing_manager():\n    has_c10d = hasattr(torch.distributed, 'distributed_c10d')\n    return has_c10d and hasattr(torch.distributed.distributed_c10d, '_coalescing_manager')\n\n\ndef has_all_reduce_coalesced():\n    return hasattr(torch.distributed, \"all_reduce_coalesced\") and required_torch_version(min_version=1.13)\n\n\ndef get_coalescing_manager(group, device, reqs, async_op):\n    if required_torch_version(min_version=2.0, max_version=2.0):\n        return torch.distributed.distributed_c10d._coalescing_manager(group, device=device, reqs=reqs)\n    elif required_torch_version(min_version=2.1):\n        return torch.distributed.distributed_c10d._coalescing_manager(group, device=device, async_ops=async_op)\n    else:\n        return torch.distributed.distributed_c10d._coalescing_manager(group, reqs)\n\n\n##Utilities to turn comm off\n##TODO: move to base comm (wrapper)\ndef all_gather_comm_off(flag=False):\n    global DS_COMM_ALL_GATHER_OFF\n    DS_COMM_ALL_GATHER_OFF = flag\n\n\ndef reduce_scatter_comm_off(flag=False):\n    global DS_COMM_REDUCE_SCATTER_OFF\n    DS_COMM_REDUCE_SCATTER_OFF = flag\n\n\ndef broadcast_comm_off(flag=False):\n    global DS_COMM_BROADCAST_OFF\n    DS_COMM_BROADCAST_OFF = flag\n\n\ndef all_reduce_comm_off(flag=False):\n    global DS_COMM_ALL_REDUCE_OFF\n    DS_COMM_ALL_REDUCE_OFF = flag\n\n\ndef reduce_comm_off(flag=False):\n    global DS_COMM_REDUCE_OFF\n    DS_COMM_REDUCE_OFF = flag\n\n\n#assumption: all_gather and reduce scatter\n## are what we care about\ndef backward_comm_off(flag=False):\n    all_gather_comm_off(flag)\n    reduce_scatter_comm_off(flag)\n\n\nclass Noop:\n\n    def wait(self):\n        return None\n\n\nclass TorchBackend(Backend):\n    \"\"\"\n        A light-weight wrapper class for torch.distributed API.\n        Only a subset of functions are wrapped. Once the init_process_group\n        is initialized, standard torch.distributed.* can be used directly\n        so no need to wrap all the functions. We can keep adding wrappers as\n        needed.\n    \"\"\"\n\n    def __init__(self, backend, timeout, init_method, rank=-1, world_size=-1, name='torch'):\n        super(TorchBackend, self).__init__()\n        self.shm_comm_op = build_shm_op()\n        self.has_all_reduce_coalesced = has_all_reduce_coalesced()\n        self.has_coalescing_manager = has_coalescing_manager()\n        self.all_gather_function = self.get_all_gather_function()\n        self.reduce_scatter_function = self.get_reduce_scatter_function()\n        self.initialized = True\n        self.name = name\n        # Future functionality to support ds.initialize() on a single GPU\n        # The idea is to fake that dist backend is initialized even when\n        # it is not so we can run on a single GPU without doing any init_process_group\n        self.single_gpu_mode = True\n        self.init_process_group(backend, timeout, init_method, rank, world_size)\n        if self.shm_comm_op != None:\n            self.shm_comm_op.initialize(self.get_world_size(), self.get_rank())\n\n    @classmethod\n    @compiler.disable\n    def get_all_gather_function(self):\n        if hasattr(torch.distributed, \"all_gather_into_tensor\"):\n            return torch.distributed.all_gather_into_tensor\n        elif hasattr(torch.distributed, \"_all_gather_base\"):\n            return torch.distributed._all_gather_base\n        return None\n\n    @classmethod\n    @compiler.disable\n    def get_reduce_scatter_function(self):\n        if hasattr(torch.distributed, \"reduce_scatter_tensor\"):\n            return torch.distributed.reduce_scatter_tensor\n        elif hasattr(torch.distributed, \"_reduce_scatter_base\"):\n            return torch.distributed._reduce_scatter_base\n        return None\n\n    def has_all_gather_into_tensor(self):\n        return self.all_gather_function is not None\n\n    def has_reduce_scatter_tensor(self):\n        return self.reduce_scatter_function is not None\n\n    def init_process_group(self, backend, timeout, init_method, rank, world_size):\n        if not torch.distributed.is_initialized():\n            torch.distributed.init_process_group(backend,\n                                                 timeout=timeout,\n                                                 init_method=init_method,\n                                                 rank=rank,\n                                                 world_size=world_size)\n        self.using_mpi = torch.distributed.get_backend() == 'mpi'\n\n    @compiler.disable\n    def all_reduce(self, tensor, op=torch.distributed.ReduceOp.SUM, group=None, async_op=False):\n        op = self._reduce_op(op)\n        return torch.distributed.all_reduce(tensor=tensor, op=op, group=group, async_op=async_op)\n\n    @compiler.disable\n    def inference_all_reduce(self, tensor, op, group=None):\n        if self.shm_comm_op == None or self.shm_comm_op.inference_all_reduce(tensor, op) == -1:\n            op = self._reduce_op(op)\n            return torch.distributed.all_reduce(tensor=tensor, op=op, group=group, async_op=False)\n\n    @compiler.disable\n    def all_reduce_coalesced(self, tensors, op=torch.distributed.ReduceOp.SUM, group=None, async_op=False):\n        \"\"\" proxy func to torch.distributed.all_reduce_coalesced,\n        which is included in PyTorch 1.13 and above\n        \"\"\"\n        if not self.has_all_reduce_coalesced:\n            raise RuntimeError(f\"Current torch version does not have all_reduce_coalesced \"\n                               f\"api (torch.__version__: {torch.__version__})\")\n        op = self._reduce_op(op)\n        return torch.distributed.all_reduce_coalesced(tensors=tensors, op=op, group=group, async_op=async_op)\n\n    @compiler.disable\n    def reduce(self, tensor, dst, op=ReduceOp.SUM, group=None, async_op=False):\n        if DS_COMM_REDUCE_OFF:\n            if int(os.getenv('RANK', '0')) == 0:\n                utils.logger.warning(\"REDUCE is OFF\")\n            return Noop()\n        return torch.distributed.reduce(tensor=tensor, dst=dst, op=self._reduce_op(op), group=group, async_op=async_op)\n\n    @compiler.disable\n    def reduce_scatter(self, output, input_list, op=ReduceOp.SUM, group=None, async_op=False):\n        if DS_COMM_REDUCE_SCATTER_OFF:\n            if int(os.getenv('RANK', '0')) == 0:\n                utils.logger.warning(\"REDUCE SCATTER  is OFF\")\n            return Noop()\n        else:\n            return torch.distributed.reduce_scatter(output=output,\n                                                    input_list=input_list,\n                                                    op=self._reduce_op(op),\n                                                    group=group,\n                                                    async_op=async_op)\n\n    @compiler.disable\n    def broadcast(self, tensor, src, group=None, async_op=False):\n        if DS_COMM_BROADCAST_OFF:\n            if int(os.getenv('RANK', '0')) == 0:\n                utils.logger.warning(\"BROADCAST  is OFF\")\n            return Noop()\n        else:\n            return torch.distributed.broadcast(tensor=tensor, src=src, group=group, async_op=async_op)\n\n    @compiler.disable\n    def all_gather(self, tensor_list, tensor, group=None, async_op=False):\n        if DS_COMM_ALL_GATHER_OFF:\n            if int(os.getenv('RANK', '0')) == 0:\n                utils.logger.warning(\"All Gather is OFF\")\n            return Noop()\n        else:\n            return torch.distributed.all_gather(tensor_list=tensor_list, tensor=tensor, group=group, async_op=async_op)\n\n    @compiler.disable\n    def all_gather_into_tensor(self, output_tensor, input_tensor, group=None, async_op=False):\n        if self.has_all_gather_into_tensor():\n            return self.all_gather_function(output_tensor=output_tensor,\n                                            input_tensor=input_tensor,\n                                            group=group,\n                                            async_op=async_op)\n\n    @compiler.disable\n    def all_gather_base(self, output_tensor, input_tensor, group=None, async_op=False):\n        if DS_COMM_ALL_GATHER_OFF:\n            if int(os.getenv('RANK', '0')) == 0:\n                utils.logger.warning(\"All Gather is OFF\")\n            return Noop()\n        else:\n            if self.has_allgather_base:\n                return torch.distributed.distributed_c10d._all_gather_base(output_tensor=output_tensor,\n                                                                           input_tensor=input_tensor,\n                                                                           group=group,\n                                                                           async_op=async_op)\n            else:\n                utils.logger.warning(\"unable to find torch.distributed._all_gather_base. will fall back to \"\n                                     \"torch.distributed.reduce_scatter which will result in suboptimal performance. \"\n                                     \"please consider upgrading your pytorch installation.\")\n                pass\n\n    @compiler.disable\n    def all_gather_coalesced(self, output_tensors, input_tensors, group=None, async_op=False):\n        \"\"\"\"\"\"\n        assert len(output_tensors) == len(input_tensors), \"\"\n        if hasattr(torch.distributed.distributed_c10d, '_all_gather_base_coalesced'):\n            # customized PyTorch\n            return torch.distributed.distributed_c10d._all_gather_base_coalesced(output_tensors,\n                                                                                 input_tensors,\n                                                                                 group=group,\n                                                                                 async_op=async_op)\n        elif has_coalescing_manager():\n            reqs = []\n            with get_coalescing_manager(group, input_tensors[0].device, reqs, async_op):\n                for output, input in zip(output_tensors, input_tensors):\n                    handle = torch.distributed.distributed_c10d.all_gather_into_tensor(output,\n                                                                                       input,\n                                                                                       group=group,\n                                                                                       async_op=True)\n                    reqs.append(handle)\n            if async_op:\n                return reqs[-1]\n            else:\n                reqs[-1].wait()\n\n    @compiler.disable\n    def reduce_scatter_tensor(self, output_tensor, input_tensor, op=ReduceOp.SUM, group=None, async_op=False):\n        if self.has_reduce_scatter_tensor():\n            return self.reduce_scatter_function(output_tensor,\n                                                input_tensor,\n                                                op=self._reduce_op(op),\n                                                group=group,\n                                                async_op=async_op)\n        else:\n            utils.logger.warning(\"unable to find torch.distributed.reduce_scatter_tensor. will fall back to \"\n                                 \"torch.distributed.reduce_scatter which will result in suboptimal performance. \"\n                                 \"please consider upgrading your pytorch installation.\")\n            pass\n\n    @compiler.disable\n    def all_to_all_single(self,\n                          output,\n                          input,\n                          output_split_sizes=None,\n                          input_split_sizes=None,\n                          group=None,\n                          async_op=False):\n        return torch.distributed.all_to_all_single(output=output,\n                                                   input=input,\n                                                   output_split_sizes=output_split_sizes,\n                                                   input_split_sizes=input_split_sizes,\n                                                   group=group,\n                                                   async_op=async_op)\n\n    @compiler.disable\n    def all_to_all(self, output_tensor_list, input_tensor_list, group=None, async_op=False):\n        return torch.distributed.all_to_all(output_tensor_list, input_tensor_list, group=group, async_op=async_op)\n\n    @compiler.disable\n    def send(self, tensor, dst, group=None, tag=0):\n        return torch.distributed.send(tensor=tensor, dst=dst, group=group, tag=tag)\n\n    @compiler.disable\n    def recv(self, tensor, src=None, group=None, tag=0):\n        return torch.distributed.recv(tensor=tensor, src=src, group=group, tag=tag)\n\n    @compiler.disable\n    def isend(self, tensor, dst, group=None, tag=0):\n        return torch.distributed.isend(tensor=tensor, dst=dst, group=group, tag=tag)\n\n    @compiler.disable\n    def irecv(self, tensor, src=None, group=None, tag=0):\n        return torch.distributed.irecv(tensor=tensor, src=src, group=group, tag=tag)\n\n    @compiler.disable\n    def gather(self, tensor, gather_list=None, dst=0, group=None, async_op=False):\n        return torch.distributed.gather(tensor=tensor,\n                                        gather_list=gather_list,\n                                        dst=dst,\n                                        group=group,\n                                        async_op=async_op)\n\n    @compiler.disable\n    def scatter(self, tensor, scatter_list=None, src=0, group=None, async_op=False):\n        return torch.distributed.scatter(tensor=tensor,\n                                         scatter_list=scatter_list,\n                                         src=src,\n                                         group=group,\n                                         async_op=async_op)\n\n    @compiler.disable\n    def barrier(self, group=torch.distributed.GroupMember.WORLD, async_op=False, device_ids=None):\n        if group is None:\n            group = torch.distributed.GroupMember.WORLD\n        return torch.distributed.barrier(group=group, async_op=async_op, device_ids=device_ids)\n\n    @compiler.disable\n    def monitored_barrier(self, group=torch.distributed.GroupMember.WORLD, timeout=None, wait_all_ranks=False):\n        if group is None:\n            group = torch.distributed.GroupMember.WORLD\n        return torch.distributed.monitored_barrier(group=group, timeout=timeout, wait_all_ranks=wait_all_ranks)\n\n    def get_rank(self, group=None):\n        return torch.distributed.get_rank(group=group)\n\n    def get_world_size(self, group=None):\n        return torch.distributed.get_world_size(group=group)\n\n    def is_initialized(self):\n        return torch.distributed.is_initialized()\n\n    def get_backend(self, group=None):\n        return torch.distributed.get_backend(group=group)\n\n    def new_group(self, ranks):\n        return torch.distributed.new_group(ranks)\n\n    def get_global_rank(self, group, group_rank):\n        if hasattr(torch.distributed.distributed_c10d, \"get_global_rank\"):\n            from torch.distributed.distributed_c10d import get_global_rank as _get_global_rank\n        else:\n            from torch.distributed.distributed_c10d import _get_global_rank\n        return _get_global_rank(group, group_rank)\n\n    def get_world_group(self):\n        return torch.distributed.group.WORLD\n\n    def destroy_process_group(self, group=None):\n        return torch.distributed.destroy_process_group(group=group)\n\n    def _reduce_op(self, op):\n        '''\n            Helper function. If the op provided is not a torch.dist.ReduceOp, convert it and return\n        '''\n        if not isinstance(op, torch.distributed.ReduceOp):\n            if op == ReduceOp.SUM:\n                op = torch.distributed.ReduceOp.SUM\n            elif op == ReduceOp.PRODUCT:\n                op = torch.distributed.ReduceOp.PRODUCT\n            elif op == ReduceOp.AVG:\n                op = torch.distributed.ReduceOp.AVG\n            elif op == ReduceOp.MIN:\n                op = torch.distributed.ReduceOp.MIN\n            elif op == ReduceOp.MAX:\n                op = torch.distributed.ReduceOp.MAX\n            elif op == ReduceOp.BAND:\n                op = torch.distributed.ReduceOp.BAND\n            elif op == ReduceOp.BOR:\n                op = torch.distributed.ReduceOp.BOR\n            elif op == ReduceOp.BXOR:\n                op = torch.distributed.ReduceOp.BXOR\n        return op\n\n\n# This will become a light-weight wrapper around torch.distributed functions\n# TODO: create some example to show how this wrapper can help profile communication\n# TODO: make sure there is no performance regression with this approach\n# TODO: explore monkey-patching if this does not work\n", "deepspeed/comm/reduce_op.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom enum import Enum\n\n\nclass ReduceOp(Enum):\n    SUM = 0\n    PRODUCT = 1\n    MIN = 2\n    MAX = 3\n    BAND = 4\n    BOR = 5\n    BXOR = 6\n    AVG = 7\n    UNUSED = 8\n", "deepspeed/comm/ccl.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n'''\nCopyright 2021 The Microsoft DeepSpeed Team\n'''\n\nimport torch\nfrom deepspeed.accelerator import get_accelerator\nfrom deepspeed.ops.op_builder import NotImplementedBuilder\nfrom .reduce_op import ReduceOp\nfrom .torch import TorchBackend\n\n\ndef build_ccl_op():\n    builder = get_accelerator().create_op_builder(\"CCLCommBuilder\")\n    if builder is None or NotImplementedBuilder:\n        return None\n    ccl_cpp_module = builder.load()\n    print(f'DeepSpeed {builder.absolute_name()} built successfully')\n    return ccl_cpp_module\n\n\nclass CCLHandler():\n\n    def __init__(self, ccl_comm_op=None):\n        self.ccl_comm_op = ccl_comm_op\n\n    def wait(self):\n        # backend covered it\n        pass\n\n\nclass CCLBackend(TorchBackend):\n\n    def __init__(self, name='ccl', rank=-1, world_size=-1, mpu=None, timeout=None, init_method=None):\n        self.ccl_comm_op = build_ccl_op()\n        if self.ccl_comm_op is None:\n            # set CCLBackend to uninitialized state if CCLCommBuilder cannot be loaded\n            self.initialized = False\n            return\n        super(CCLBackend, self).__init__(backend='ccl',\n                                         name='torch',\n                                         rank=rank,\n                                         world_size=world_size,\n                                         timeout=timeout,\n                                         init_method=init_method)\n        self.name = 'ccl'\n        size = self.get_world_size()\n        rank = self.get_rank()\n        main_kvs = self.ccl_comm_op.get_kvs_addr(rank)\n        main_kvs = torch.tensor(main_kvs).to(torch.uint8).to(get_accelerator().current_device_name())\n        super(CCLBackend, self).broadcast(main_kvs, 0)\n        self.ccl_comm_op.initialize(size, rank, main_kvs)\n        self.initialized = True\n        self.groups = [tuple(range(self.get_world_size()))]\n        self.available_coll = self.ccl_comm_op.get_available_coll()\n\n    def is_initialized(self):\n        return self.initialized\n\n    def run_collective(self, name, **kwargs):\n        if name in self.available_coll:\n            if 'group' in kwargs:\n                kwargs['group'] = self.get_all_ranks_from_group(kwargs['group'])\n            if 'dst' in kwargs:\n                kwargs['dst'] = kwargs['group'].index(kwargs['dst'])\n            if 'src' in kwargs:\n                kwargs['src'] = kwargs['group'].index(kwargs['src'])\n            func = \"self.ccl_comm_op.\" + name\n            eval(func)(*(kwargs.values()))\n            return CCLHandler(self.ccl_comm_op)\n        else:\n            func = \"super(CCLBackend, self).\" + name\n            eval(func)(*(kwargs.values()))\n            return CCLHandler(self.ccl_comm_op)\n\n    def all_reduce(self, tensor, op=ReduceOp.SUM, group=None, async_op=False):\n        use_caching = False\n        if use_caching:\n            match_id = f\"{tensor.size()}-{op}\"\n            name = \"all_reduce_caching\"\n            if name in self.available_coll:\n                group = self.get_all_ranks_from_group(group)\n                return self.ccl_comm_op.all_reduce_caching(tensor, op, match_id, group, async_op)\n            else:\n                return self.run_collective(name=name,\n                                           tensor=tensor,\n                                           op=op,\n                                           match_id=match_id,\n                                           group=group,\n                                           async_op=async_op)\n        else:\n            name = \"all_reduce\"\n            if name in self.available_coll:\n                group = self.get_all_ranks_from_group(group)\n                return self.ccl_comm_op.all_reduce(tensor, op, group, async_op)\n            else:\n                return self.run_collective(name=name, tensor=tensor, op=op, group=group, async_op=async_op)\n\n    def inference_all_reduce(self, tensor, op=ReduceOp.SUM, group=None):\n        name = \"inference_all_reduce\"\n        if name in self.available_coll:\n            return self.ccl_comm_op.inference_all_reduce(tensor, op)\n        else:\n            return self.run_collective(name=name, tensor=tensor, op=op, group=None, async_op=False)\n\n    def broadcast(self, tensor, src, group=None, async_op=False):\n        return self.run_collective(name=\"broadcast\", tensor=tensor, src=src, group=group, async_op=async_op)\n\n    def all_gather(self, tensor_list, tensor, group=None, async_op=False):\n        return self.run_collective(name=\"all_gather\",\n                                   tensor_list=tensor_list,\n                                   tensor=tensor,\n                                   group=group,\n                                   async_op=async_op)\n\n    def reduce_scatter_tensor(self, output_tensor, input_tensor, op, group=None, async_op=False):\n        return self.run_collective(name=\"reduce_scatter_tensor\",\n                                   output_tensor=output_tensor,\n                                   input_tensor=input_tensor,\n                                   op=op,\n                                   group=group)\n\n    def all_gather_into_tensor(self, output_tensor, input_tensor, group=None, async_op=False):\n        return self.run_collective(name=\"all_gather_into_tensor\",\n                                   output_tensor=output_tensor,\n                                   input_tensor=input_tensor,\n                                   group=group)\n\n    def all_to_all_single(self, output, input, output_split_sizes, input_split_sizes, group=None, async_op=False):\n        return self.run_collective(name=\"all_to_all_single\",\n                                   output=output,\n                                   input=input,\n                                   output_split_sizes=output_split_sizes,\n                                   input_split_sizes=input_split_sizes,\n                                   group=group)\n\n    def send(self, tensor, dst, group=None, tag=0):\n        return self.run_collective(name=\"send\", tensor=tensor, dst=dst, group=group, tag=tag)\n\n    def recv(self, tensor, src, group=None, tag=0):\n        return self.run_collective(name=\"recv\", tensor=tensor, src=src, group=group, tag=tag)\n\n    def gather(self, tensor, gather_list, dst, group=None, async_op=False):\n        return self.run_collective(name=\"gather\", tensor=tensor, gather_list=gather_list, dst=dst, group=group)\n\n    def scatter(self, tensor, gather_list, dst, group=None, async_op=False):\n        return self.run_collective(name=\"scatter\", tensor=tensor, gather_list=gather_list, dst=dst, group=group)\n\n    def barrier(self, group=None, async_op=False):\n        return self.run_collective(name=\"barrier\", group=group, async_op=async_op)\n\n    def monitored_barrier(self, group=None, timeout=None, wait_all_ranks=False):\n        return self.run_collective(name=\"monitored_barrier\", group=group)\n\n    def reduce_scatter(self, output, input_list, op=ReduceOp.SUM, group=None, async_op=False):\n        return self.run_collective(name=\"reduce_scatter\",\n                                   output=output,\n                                   input_list=input_list,\n                                   op=op,\n                                   group=group,\n                                   async_op=async_op)\n\n    def reduce(self, tensor, dst, op=ReduceOp.SUM, group=None, async_op=False):\n        return self.run_collective(name=\"reduce\", tensor=tensor, dst=dst, op=op, group=group, async_op=async_op)\n\n    def new_group(self, ranks):\n        return super(CCLBackend, self).new_group(ranks)\n\n    def _new_group(self, ranks, group):\n        size = len(ranks)\n        rank = self.get_rank()\n        sub_main_kvs = self.ccl_comm_op.get_sub_kvs_addr(rank == ranks[0])\n        sub_main_kvs = torch.tensor(sub_main_kvs).to(torch.uint8).to(get_accelerator().current_device_name())\n        super(CCLBackend, self).broadcast(sub_main_kvs, ranks[0], group)\n        self.ccl_comm_op.initialize_sub_comm(size, ranks.index(rank), sub_main_kvs, ranks)\n        self.groups.append(tuple(ranks))\n\n    def get_all_ranks_from_group(self, group):\n        if group is None:\n            return list(range(self.get_world_size()))\n        rank = 0\n        results = []\n        try:\n            while True:\n                results.append(super(CCLBackend, self).get_global_rank(group, rank))\n                rank += 1\n        except (ValueError, RuntimeError):\n            pass\n        if tuple(results) not in self.groups:\n            self._new_group(results, group)\n        return results\n", "deepspeed/comm/config.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .constants import *\nfrom ..pydantic_v1 import BaseModel\n\n\nclass CommsConfig(BaseModel):\n\n    class Config:\n        validate_all = True\n        validate_assignment = True\n        use_enum_values = True\n        extra = 'forbid'\n\n\nclass CommsLoggerConfig(CommsConfig):\n    enabled: bool = COMMS_LOGGER_ENABLED_DEFAULT\n    prof_all: bool = COMMS_LOGGER_PROF_ALL_DEFAULT\n    prof_ops: list = COMMS_LOGGER_PROF_OPS_DEFAULT\n    verbose: bool = COMMS_LOGGER_VERBOSE_DEFAULT\n    debug: bool = COMMS_LOGGER_DEBUG_DEFAULT\n\n\nclass DeepSpeedCommsConfig:\n\n    def __init__(self, ds_config):\n        self.comms_logger_enabled = 'comms_logger' in ds_config\n\n        if self.comms_logger_enabled:\n            self.comms_logger = CommsLoggerConfig(**ds_config['comms_logger'])\n", "deepspeed/comm/utils.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport os\nimport inspect\nfrom deepspeed.utils import get_caller_func\n\n\ndef get_local_rank_from_launcher():\n\n    # DeepSpeed launcher will set it so get from there\n    rank = os.environ.get('LOCAL_RANK')\n\n    if rank is None:\n        rank = os.environ.get('OMPI_COMM_WORLD_LOCAL_RANK')\n\n    # Make it a single process job and set rank to 0\n    if rank is None:\n        rank = 0\n\n    return int(rank)\n\n\ndef get_world_rank_from_launcher():\n\n    # DeepSpeed launcher will set it so get from there\n    rank = os.environ.get('RANK')\n\n    if rank is None:\n        rank = os.environ.get('OMPI_COMM_WORLD_RANK')\n\n    # Make it a single process job and set rank to 0\n    if rank is None:\n        rank = 0\n\n    return int(rank)\n\n\ndef get_world_size_from_launcher():\n    # DeepSpeed launcher will set it so get from there\n    size = os.environ.get('WORLD_SIZE')\n    rank = os.environ.get('RANK')\n\n    if size is None:\n        size = os.environ.get('OMPI_COMM_WORLD_SIZE')\n\n    # Make it a single process job and set size to 1\n    if size is None:\n        size = 1\n\n    if rank == 0:\n        print(f\"set world size to {size}\")\n\n    return int(size)\n\n\ndef get_default_args(func):\n    signature = inspect.signature(func)\n    return {k: v.default for k, v in signature.parameters.items() if v.default is not inspect.Parameter.empty}\n\n\n# We need this hacky function since torch doesn't consistently name or place the input tensor args\ndef get_tensor_position(func):\n    sig_params = inspect.signature(func).parameters\n    arg = None\n    # most colls\n    if 'tensor' in sig_params:\n        arg = 'tensor'\n    # all_reduce_coalesced coll\n    elif 'tensors' in sig_params:\n        arg = 'tensors'\n    # reduce scatter coll\n    elif 'input_list' in sig_params:\n        arg = 'input_list'\n    # all_to_all and torch multiGPU colls\n    elif 'input_tensor_list' in sig_params:\n        arg = 'input_tensor_list'\n    if arg is None:\n        return -1\n    else:\n        return list(sig_params).index(arg)\n\n\ndef get_tensor_kwarg(func, kwargs):\n    func_args = get_default_args(func)\n    func_args.update(kwargs)\n    arg = None\n\n    if 'tensor' in func_args:\n        arg = func_args['tensor']\n    elif 'tensors' in func_args:\n        arg = func_args['tensors']\n    elif 'input_list' in func_args:\n        arg = func_args['input_list']\n    elif 'input_tensor_list' in func_args:\n        arg = func_args['input_tensor_list']\n    return arg\n\n\ndef get_msg_size_from_args(func, *args, **kwargs):\n    # 3 cases:\n    #   - tensor arg is in args\n    #   - tensor arg is in kwargs\n    #   - tensor arg is not present (e.g. barrier)\n    tensor_arg_position = -1\n    tensor_arg = None\n    # check if tensor arg is in args\n    if len(args) > 0:\n        tensor_arg_position = get_tensor_position(func)\n        if tensor_arg_position > -1:\n            tensor_arg = args[get_tensor_position(func)]\n    # check if tensor arg is in kwargs\n    if tensor_arg is None and len(kwargs) > 0:\n        tensor_arg = get_tensor_kwarg(func, kwargs)\n    # if tensor arg is not present, no data is being transmitted\n    if tensor_arg is None:\n        return 0\n    else:\n        # Sum of tensor sizes for list colls such as torch's all_to_all\n        # NOTE: msg_size for list colls will not be the actual size transmitted by a given MPI/NCCL call within the coll op. Instead, it's the total amount of data transmitted.\n        if type(tensor_arg) is list:\n            return sum(x.element_size() * x.nelement() for x in tensor_arg)\n        else:\n            return tensor_arg.element_size() * tensor_arg.nelement()\n\n\ndef get_debug_log_name(func_args, debug):\n    if debug:\n        return func_args['log_name'] + ' | [Caller Func: ' + get_caller_func() + ']'\n    else:\n        return func_args['log_name']\n", "deepspeed/comm/backend.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\"\"\"\nDeepSpeed Communication Backend.\n\nIn the future, directly use NCCL/MPI/Gloo/etc without requiring torch.distributed. Simply wrap torch.distributed for now.\n\n# Custom DS Backends -- Direct C/Ops\n - NCCL -- [EXPERIMENTAL]\n - MPI -- [EXPERIMENTAL]\n - RCCL -- [EXPERIMENTAL]\n - GLOO -- [EXPERIMENTAL]\n# DS backend wrapper for torch.distributed [DEFAULT]\n - T-NCCL -- [DEFAULT]\n - T-GLOO\n - T-MPI\n\n# Backend is the base class\n     -- NcclBackend, MpiBackend, and TorchBackend are the main subclasses. TorchBackend is the only officially supported backend for now.\n\"\"\"\n\n\nclass Backend(object):\n\n    def __init__(self, name='backend', rank=0, size=1):\n        self.name = name\n        # The world size and rank of the world process group\n        self.world_group = None\n        self.world_size = size\n        self.world_rank = rank\n        # Single process group (pg) implementation for now but keep a list for future\n        self.process_groups = []\n        self.initialized = False\n\n    def is_initialized(self):\n        return self.initialized\n\n    def new_group(self):\n        # create a new pg and add it to pg list\n        pass\n\n    def init_process_group(self):\n        # subclasses will initialize them fully\n        # - initialize a default world process group and add it to pg list\n        self.initialized = True\n", "deepspeed/comm/constants.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nNCCL_BACKEND = 'nccl'\nCCL_BACKEND = 'ccl'\nMPI_BACKEND = 'mpi'\nGLOO_BACKEND = 'gloo'\nSCCL_BACKEND = 'sccl'\nHCCL_BACKEND = 'hccl'\n\nDEFAULT_AML_MASTER_PORT = \"54965\"\nDEFAULT_AML_NCCL_SOCKET_IFNAME = \"^docker0,lo\"\n\n#########################################\n# Comms Logger\n#########################################\n# Comms Logger. By default, this feature is not enabled.\n# Users can configure in ds_config.json as below example:\nCOMMS_LOGGER_FORMAT = '''\nThe Comms Logger can be specified as:\n\"comms_logger\": {\n  \"enabled\": true,\n  \"verbose\": false,\n  \"prof_all\": true,\n  \"debug\": false,\n  \"prof_ops\": [\"all_reduce\", \"custom_all_reduce_name\"]\n}\n'''\nCOMMS_LOGGER = \"comms_logger\"\n\n# Comms logger enable signal\nCOMMS_LOGGER_ENABLED = \"enabled\"\nCOMMS_LOGGER_ENABLED_DEFAULT = False\n\n# Comms logger verbose signal\nCOMMS_LOGGER_VERBOSE = \"verbose\"\nCOMMS_LOGGER_VERBOSE_DEFAULT = False\n\n# comms logger profile all ops signal\nCOMMS_LOGGER_PROF_ALL = \"prof_all\"\nCOMMS_LOGGER_PROF_ALL_DEFAULT = True\n\n# comms logger show all ops signal\nCOMMS_LOGGER_DEBUG = \"debug\"\nCOMMS_LOGGER_DEBUG_DEFAULT = False\n\n# comms logger profile specific ops in list\nCOMMS_LOGGER_PROF_OPS = \"prof_ops\"\nCOMMS_LOGGER_PROF_OPS_DEFAULT = []\n", "deepspeed/comm/comm.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\"\"\"\n    DeepSpeed Communication Package: deepspeed.comm\n    deepspeed.comm\n        -- import and use deepspeed.ops.comm\n        -- use torch.distributed directly if both this package and torch.distributed use the same NCCL version\n        -- use custom collectives\n            -- can either use torch.dist or ds.ops.comm?\n        Note: the old 1-bit compressed allreduce variants that resided in deepspeed.runtime.comm will be moved here as well.\n    deepspeed.comm API\n        -- must be kept fully compatible (same signatures) as torch.dist API to ensure backward/cross-framework compatibility.\n        -- e.g. if a client code used\n            from deepspeed import comm as dist\n            instead of\n            import torch.distributed as dist\n            The code should work without breaking any of the public torch.distributed functionality\n    Future:\n        -- deepspeed groups API should be brought into ds.comm\n\"\"\"\n\nimport torch\nimport os\n\nfrom ..constants import TORCH_DISTRIBUTED_DEFAULT_PORT, default_pg_timeout\nfrom .constants import *\nfrom .reduce_op import *\nfrom deepspeed.accelerator import get_accelerator\nfrom deepspeed.comm.ccl import CCLBackend\n\nfrom deepspeed.utils.comms_logging import CommsLogger\nfrom deepspeed.utils import timer, get_caller_func\nfrom deepspeed.comm.torch import TorchBackend\n\nfrom deepspeed import utils\nfrom datetime import timedelta\n\n# Current deepspeed.comm backend (cdb) global object for simple access by client code\ncdb = None\n\n# Create global timer for ops\ntimers = timer.SynchronizedWallClockTimer()\ntimer_summary = {}\n\ncomms_logger = CommsLogger()\n\n# Maintain objects of all initialized ds backends and assign them using the API functions in this file\nnccl_backend = None\nmpi_backend = None\nccl_backend = None\nhccl_backend = None\n\n# This should be set here so all rank/size information from the launcher can be propagated\nfrom deepspeed.comm.utils import *\n\n\nclass ProcessGroup():\n\n    def __init__(self, comm_id, ranks=[]):\n        self.ranks = ranks\n        self.comm_id = comm_id\n        self.size = len(ranks)\n\n\ndef _configure_using_config_file(config):\n    if config.comms_logger_enabled:\n        comms_logger.configure(config)\n\n\ndef configure(\n    deepspeed_config=None,\n    enabled=None,\n    prof_all=None,\n    prof_ops=None,\n    verbose=None,\n    debug=None,\n):\n\n    if deepspeed_config is not None:\n        _configure_using_config_file(deepspeed_config.comms_config)\n\n    if enabled is not None:\n        comms_logger.enabled = enabled\n\n    if prof_all is not None:\n        comms_logger.prof_all = prof_all\n\n    if prof_ops is not None:\n        comms_logger.prof_ops = prof_ops\n\n    if verbose is not None:\n        comms_logger.verbose = verbose\n\n    if debug is not None:\n        comms_logger.debug = debug\n\n\n# Logging wrapper for timing ops\ndef timed_op(func):\n\n    def log_wrapper(*args, **kwargs):\n        # Add enabled flag so that overhead to each comm op is two if conditions at most\n        if comms_logger.enabled:\n            if ('prof' in kwargs\n                    and kwargs['prof']) or comms_logger.prof_all or ('log_name' in kwargs\n                                                                     and kwargs['log_name'] in comms_logger.prof_ops):\n                # Need func args for their defaults\n                func_args = get_default_args(func)\n                func_args.update(kwargs)\n                msg_size = get_msg_size_from_args(func, *args, **kwargs)\n                log_name = get_debug_log_name(func_args, comms_logger.debug)\n                timers(log_name).start()\n        # Return the op, then stop the op's timer\n        try:\n            return func(*args, **kwargs)\n        finally:\n            if comms_logger.enabled:\n                # Need to make op blocking for accurate logging\n                get_accelerator().synchronize()\n                # If we're using MPI, we can't simply sync the stream\n                if cdb.using_mpi:\n                    cdb.barrier()\n                if ('prof' in kwargs and kwargs['prof']) or comms_logger.prof_all or (\n                        'log_name' in kwargs and kwargs['log_name'] in comms_logger.prof_ops):\n                    log_name = get_debug_log_name(func_args, comms_logger.debug)\n                    raw_name = func.__name__\n                    timers(log_name).stop()\n                    # need temp var since 'elapsed' resets events\n                    time_elapsed = timers(log_name).elapsed(reset=False)\n                    comms_logger.append(raw_name, log_name, time_elapsed, msg_size)\n\n    return log_wrapper\n\n\n# For compatibility with torch distributed's init_process_group, we shall retain the signature from PyTorch code.\n# DeepSpeed NCCL/MPI backend may not need all these params as we will have our own implementation.\n# Please read full torch.distributed API docs from https://pytorch.org/docs/stable/distributed.html\n\n\n# UNUSED: Future helper function to initialize DS backends\ndef init_deepspeed_backend(ds_backend, timeout, init_method):\n    global cdb\n    global nccl_backend\n    global mpi_backend\n    global ccl_backend\n    global hccl_backend\n\n    rank = int(os.getenv('RANK', '-1'))\n    size = int(os.getenv('WORLD_SIZE', '-1'))\n\n    if ds_backend == NCCL_BACKEND:\n        utils.logger.debug(\"NCCL backend in DeepSpeed not yet implemented\")\n    elif ds_backend == MPI_BACKEND:\n        utils.logger.debug(\"MPI backend in DeepSpeed not yet implemented\")\n    elif ds_backend == GLOO_BACKEND:\n        utils.logger.debug(\"Gloo backend in DeepSpeed not yet implemented\")\n    elif ds_backend == CCL_BACKEND:\n        ccl_backend = CCLBackend(rank=rank, world_size=size, timeout=timeout, init_method=init_method)\n        utils.logger.info(f\"Initialize {ds_backend} backend\")\n    elif ds_backend == HCCL_BACKEND:\n        utils.logger.debug(\"HCCL backend in DeepSpeed not yet implemented\")\n    else:\n        utils.logger.debug(f\"DeepSpeed does not support {ds_backend} backend\")\n\n\ndef is_initialized():\n    #assert cdb is not None, 'DeepSpeed backend not set, please initialize it using init_process_group()'\n    if cdb is None:\n        return False\n    else:\n        return cdb.is_initialized()\n\n\ndef destroy_process_group(group=None):\n    global cdb\n    return cdb.destroy_process_group(group=group)\n\n\ndef new_group(ranks):\n    global cdb\n    assert cdb is not None and cdb.is_initialized(\n    ), 'DeepSpeed backend not set, please initialize it using init_process_group()'\n    return cdb.new_group(ranks)\n\n\ndef is_available() -> bool:\n\n    # Returns ``True`` if the deepspeed comm package is available.\n\n    # TODO: load other ops. Clients including deepspeed itself should use deepspeed.comm to import\n    # any communication related primitives from this package.\n    # use hasattr(deepspeed.csrc.ops, \"_comm\") or something\n    return True\n\n\ndef set_backend():\n    global cdb\n    global nccl_backend\n    global mpi_backend\n    global ccl_backend\n    global hccl_backend\n\n    backend_name = get_accelerator().communication_backend_name()\n\n    if backend_name == NCCL_BACKEND:\n        if nccl_backend is not None and nccl_backend.is_initialized():\n            cdb = nccl_backend\n    elif backend_name == MPI_BACKEND:\n        if mpi_backend is not None and mpi_backend.is_initialized():\n            cdb = mpi_backend\n    elif backend_name == CCL_BACKEND:\n        if ccl_backend is not None and ccl_backend.is_initialized():\n            cdb = ccl_backend\n    elif backend_name == HCCL_BACKEND:\n        if hccl_backend is not None and hccl_backend.is_initialized():\n            cdb = hccl_backend\n\n\n@timed_op\ndef broadcast(tensor, src, group=None, async_op=False, prof=False, log_name='broadcast', debug=get_caller_func()):\n    global cdb\n    return cdb.broadcast(tensor=tensor, src=src, group=group, async_op=async_op)\n\n\n@timed_op\ndef all_gather(tensor_list,\n               tensor,\n               group=None,\n               async_op=False,\n               prof=False,\n               log_name='all_gather',\n               debug=get_caller_func()):\n    global cdb\n    return cdb.all_gather(tensor_list=tensor_list, tensor=tensor, group=group, async_op=async_op)\n\n\ndef has_reduce_scatter_tensor():\n    global cdb\n    assert cdb is not None and cdb.is_initialized(\n    ), 'DeepSpeed backend not set, please initialize it using init_process_group()'\n    return cdb.has_reduce_scatter_tensor()\n\n\ndef reduce_scatter_fn(output_tensor,\n                      tensor,\n                      op=ReduceOp.SUM,\n                      group=None,\n                      async_op=False,\n                      prof=False,\n                      debug=get_caller_func()):\n    global cdb\n    assert cdb is not None and cdb.is_initialized(\n    ), 'DeepSpeed backend not set, please initialize it using init_process_group()'\n    if cdb.has_reduce_scatter_tensor():\n        return reduce_scatter_tensor(output_tensor,\n                                     tensor,\n                                     op=op,\n                                     group=group,\n                                     async_op=async_op,\n                                     prof=prof,\n                                     debug=debug)\n    else:\n        if get_rank() == 0:\n            utils.logger.warning_once(\"unable to find torch.distributed.reduce_scatter_tensor. will fall back to \"\n                                      \"torch.distributed.reduce_scatter which will result in suboptimal performance. \"\n                                      \"please consider upgrading your pytorch installation.\")\n        input_tensor_lst = list(torch.chunk(tensor, cdb.get_world_size(group)))\n        return reduce_scatter(output_tensor,\n                              input_tensor_lst,\n                              op=op,\n                              group=group,\n                              async_op=async_op,\n                              prof=prof,\n                              debug=debug)\n\n\n@timed_op\ndef reduce_scatter_tensor(output_tensor,\n                          tensor,\n                          op=ReduceOp.SUM,\n                          group=None,\n                          async_op=False,\n                          prof=False,\n                          log_name='reduce_scatter_tensor',\n                          debug=get_caller_func()):\n    global cdb\n    return cdb.reduce_scatter_tensor(output_tensor=output_tensor,\n                                     input_tensor=tensor,\n                                     op=op,\n                                     group=group,\n                                     async_op=async_op)\n\n\n@timed_op\ndef all_gather_into_tensor(output_tensor,\n                           tensor,\n                           group=None,\n                           async_op=False,\n                           prof=False,\n                           log_name='all_gather_into_tensor',\n                           debug=get_caller_func()):\n    global cdb\n    return cdb.all_gather_into_tensor(output_tensor=output_tensor, input_tensor=tensor, group=group, async_op=async_op)\n\n\ndef has_all_gather_into_tensor():\n    global cdb\n    assert cdb is not None and cdb.is_initialized(\n    ), 'DeepSpeed backend not set, please initialize it using init_process_group()'\n    return cdb.has_all_gather_into_tensor()\n\n\ndef allgather_fn(output_tensor, input_tensor, group=None, async_op=False, debug=get_caller_func()):\n    global cdb\n    assert cdb is not None and cdb.is_initialized(\n    ), 'DeepSpeed backend not set, please initialize it using init_process_group()'\n    if cdb.has_all_gather_into_tensor():\n        return all_gather_into_tensor(output_tensor, input_tensor, group=group, async_op=async_op, debug=debug)\n    else:\n        if get_rank() == 0:\n            utils.logger.warning_once(\"unable to find torch.distributed.all_gather_into_tensor. will fall back to \"\n                                      \"torch.distributed.all_gather which will result in suboptimal performance. \"\n                                      \"please consider upgrading your pytorch installation.\")\n        output_tensors = list(torch.chunk(output_tensor, cdb.get_world_size(group)))\n        return all_gather(output_tensors, input_tensor, group=group, async_op=async_op, debug=debug)\n\n\n@timed_op\ndef all_to_all_single(output,\n                      tensor,\n                      output_split_sizes=None,\n                      input_split_sizes=None,\n                      group=None,\n                      async_op=False,\n                      prof=False,\n                      log_name='all_to_all_single',\n                      debug=get_caller_func()):\n    global cdb\n    return cdb.all_to_all_single(output=output,\n                                 input=tensor,\n                                 output_split_sizes=output_split_sizes,\n                                 input_split_sizes=input_split_sizes,\n                                 group=group,\n                                 async_op=async_op)\n\n\n@timed_op\ndef all_to_all(output_tensor_list, input_tensor_list, group=None, async_op=False):\n    global cdb\n    return cdb.all_to_all(output_tensor_list, input_tensor_list, group=group, async_op=async_op)\n\n\n@timed_op\ndef send(tensor, dst, group=None, tag=0, prof=False, log_name='send', debug=get_caller_func()):\n    global cdb\n    return cdb.send(tensor=tensor, dst=dst, group=group, tag=tag)\n\n\n@timed_op\ndef recv(tensor, src=None, group=None, tag=0, prof=False, log_name='recv', debug=get_caller_func()):\n    global cdb\n    return cdb.recv(tensor=tensor, src=src, group=group, tag=tag)\n\n\n@timed_op\ndef isend(tensor, dst, group=None, tag=0, prof=False, log_name='isend', debug=get_caller_func()):\n    global cdb\n    return cdb.send(tensor=tensor, dst=dst, group=group, tag=tag)\n\n\n@timed_op\ndef irecv(tensor, src=None, group=None, tag=0, prof=False, log_name='irecv', debug=get_caller_func()):\n    global cdb\n    return cdb.recv(tensor=tensor, src=src, group=group, tag=tag)\n\n\n@timed_op\ndef gather(tensor,\n           gather_list=None,\n           dst=0,\n           group=None,\n           async_op=False,\n           prof=False,\n           log_name='gather',\n           debug=get_caller_func()):\n    global cdb\n    return cdb.gather(tensor=tensor, gather_list=gather_list, dst=dst, group=group, async_op=async_op)\n\n\n@timed_op\ndef scatter(tensor,\n            scatter_list=None,\n            src=0,\n            group=None,\n            async_op=False,\n            prof=False,\n            log_name='scatter',\n            debug=get_caller_func()):\n    global cdb\n    return cdb.scatter(tensor=tensor, scatter_list=scatter_list, src=src, group=group, async_op=async_op)\n\n\n@timed_op\ndef barrier(group=None, async_op=False, device_ids=None, prof=False, log_name='barrier', debug=get_caller_func()):\n    global cdb\n    return cdb.barrier(group=group, async_op=async_op)\n\n\n@timed_op\ndef monitored_barrier(group=None,\n                      timeout=None,\n                      wait_all_ranks=False,\n                      prof=False,\n                      log_name='monitored_barrier',\n                      debug=get_caller_func()):\n    global cdb\n    return cdb.monitored_barrier(group=group, timeout=timeout, wait_all_ranks=wait_all_ranks)\n\n\ndef log_summary(show_straggler=False):\n    global cdb\n    barrier(log_name='log_summary_barrier')\n    if cdb.get_rank() == 0:\n        comms_logger.log_all(print_log=True, show_straggler=show_straggler)\n    else:\n        comms_logger.log_all(print_log=False, show_straggler=show_straggler)\n    barrier(log_name='log_summary_barrier')\n\n\n@timed_op\ndef reduce(tensor,\n           dst,\n           op=ReduceOp.SUM,\n           group=None,\n           async_op=False,\n           prof=False,\n           log_name='reduce',\n           debug=get_caller_func()):\n    global cdb\n    return cdb.reduce(tensor=tensor, dst=dst, op=op, group=group, async_op=async_op)\n\n\n@timed_op\ndef reduce_scatter(output,\n                   input_list,\n                   op=ReduceOp.SUM,\n                   group=None,\n                   async_op=False,\n                   prof=False,\n                   log_name='reduce_scatter',\n                   debug=get_caller_func()):\n    global cdb\n    return cdb.reduce_scatter(output=output, input_list=input_list, op=op, group=group, async_op=async_op)\n\n\ndef has_all_reduce_coalesced():\n    \"\"\"\"\"\"\n    global cdb\n    assert cdb is not None and cdb.is_initialized(\n    ), 'DeepSpeed backend not set, please initialize it using init_process_group()'\n    assert cdb.has_all_reduce_coalesced is not None, 'has_all_reduce_coalesced is not yet defined'\n    return cdb.has_all_reduce_coalesced\n\n\ndef has_coalescing_manager():\n    global cdb\n    assert cdb is not None and cdb.is_initialized(\n    ), 'DeepSpeed backend not set, please initialize it using init_process_group()'\n    assert cdb.has_coalescing_manager is not None, 'has_coalescing_manager is not yet defined'\n    return cdb.has_coalescing_manager\n\n\ndef all_gather_coalesced(output_tensors, input_tensors, group=None, async_op=False):\n    global cdb\n    assert cdb is not None and cdb.is_initialized(\n    ), 'DeepSpeed backend not set, please initialize it using init_process_group()'\n    return cdb.all_gather_coalesced(output_tensors, input_tensors, group=group, async_op=async_op)\n\n\n@timed_op\ndef all_reduce(tensor,\n               op=ReduceOp.SUM,\n               group=None,\n               async_op=False,\n               prof=False,\n               log_name='all_reduce',\n               debug=get_caller_func()):\n    #if profile_comm:\n    # context of the timers?\n    # timers.start()\n    # TensorBoard logging for comm calls.?\n    global cdb\n    #print(f'op = {op}, cdb= {cdb.name}')\n    return cdb.all_reduce(tensor, op, group, async_op)\n\n\n@timed_op\ndef inference_all_reduce(tensor,\n                         op=ReduceOp.SUM,\n                         group=None,\n                         async_op=False,\n                         prof=False,\n                         log_name='all_reduce',\n                         debug=get_caller_func()):\n    global cdb\n    return cdb.inference_all_reduce(tensor, op, group)\n\n\n@timed_op\ndef all_reduce_coalesced(tensors,\n                         op=ReduceOp.SUM,\n                         group=None,\n                         async_op=False,\n                         prof=False,\n                         log_name='all_reduce',\n                         debug=get_caller_func()):\n    global cdb\n    return cdb.all_reduce_coalesced(tensors, op, group, async_op)\n\n\ndef get_world_group():\n    global cdb\n    assert cdb is not None and cdb.is_initialized(\n    ), 'DeepSpeed backend not set, please initialize it using init_process_group()'\n    return cdb.get_world_group()\n\n\ndef get_world_size(group=None) -> int:\n    \"\"\"\n    Returns the number of processes in the current process group\n    Args:\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n    Returns:\n        The world size of the process group\n        -1, if not part of the group\n    \"\"\"\n    global cdb\n\n    assert cdb is not None and cdb.is_initialized(\n    ), 'DeepSpeed backend not set, please initialize it using init_process_group()'\n    return cdb.get_world_size(group)\n\n\ndef get_rank(group=None):\n    \"\"\"\n    Returns the rank of the current process in the provided ``group`` or the\n    default group if none was provided.\n    Rank is a unique identifier assigned to each process within a distributed\n    process group. They are always consecutive integers ranging from 0 to\n    ``world_size``.\n    Args:\n        group (ProcessGroup, optional): The process group to work on. If None,\n            the default process group will be used.\n    Returns:\n        The rank of the process group\n        -1, if not part of the group\n    \"\"\"\n    global cdb\n    assert cdb is not None and cdb.is_initialized(\n    ), 'DeepSpeed backend not set, please initialize it using init_process_group()'\n    return cdb.get_rank(group)\n\n\ndef get_local_rank():\n    \"\"\"\n        Helper function to get local rank after a backend has been set and initialized\n        Args:\n            None\n        Returns:\n            local rank (= GPU device ID)\n    \"\"\"\n    global cdb\n    assert cdb is not None and cdb.is_initialized(\n    ), 'DeepSpeed backend not set, please initialize it using init_process_group()'\n    return get_local_rank_from_launcher()\n\n\ndef get_global_rank(group=None, group_rank=0):\n    global cdb\n    assert cdb is not None and cdb.is_initialized(\n    ), 'DeepSpeed backend not set, please initialize it using init_process_group()'\n    return cdb.get_global_rank(group, group_rank)\n\n\ndef get_all_ranks_from_group(group=None):\n    global cdb\n    assert cdb is not None and cdb.is_initialized(\n    ), 'DeepSpeed backend not set, please initialize it using init_process_group()'\n    rank = 0\n    group_ranks = []\n    try:\n        while True:\n            group_ranks.append(cdb.get_global_rank(group, rank))\n            rank += 1\n    except (RuntimeError, ValueError):\n        pass\n    return group_ranks\n\n\n# Main DeepSpeed Comms. public API.\ndef init_distributed(dist_backend=None,\n                     auto_mpi_discovery=True,\n                     distributed_port=TORCH_DISTRIBUTED_DEFAULT_PORT,\n                     verbose=True,\n                     timeout=default_pg_timeout,\n                     init_method=None,\n                     dist_init_required=None,\n                     config=None,\n                     rank=-1,\n                     world_size=-1):\n    ''' Initialize dist backend, potentially performing MPI discovery if needed\n\n    Arguments:\n        dist_backend: Optional (str). torch distributed backend, e.g., nccl, mpi, gloo, hccl\n        auto_mpi_discovery Optional (bool). if distributed environment variables are not set, attempt to discover them from MPI\n        distributed_port: Optional (int). torch distributed backend port\n        verbose: Optional (bool). verbose logging\n        timeout: Optional (timedelta). Timeout for operations executed against the process group. The default value of 30 minutes can be overridden by the environment variable `DEEPSPEED_TIMEOUT`.\n        init_method: Optional (string). Torch distributed, URL specifying how to initialize the process group. Default is \u201cenv://\u201d if no init_method or store is specified.\n        config: Optional (dict). DeepSpeed configuration for setting up comms options (e.g. Comms profiling)\n        rank: Optional (int). The current manually specified rank. Some init_method like \u201ctcp://\u201d need the rank and world_size as well (see: https://pytorch.org/docs/stable/distributed.html#tcp-initialization)\n        world_size: Optional (int). Desired world_size for the TCP or Shared file-system initialization.\n    '''\n    global cdb\n\n    configure(deepspeed_config=config)\n\n    if dist_init_required is None:\n        dist_init_required = cdb is None or not cdb.is_initialized()\n\n    if cdb is None:\n        init_deepspeed_backend(get_accelerator().communication_backend_name(), timeout, init_method)\n        set_backend()\n        utils.logger.info(f'cdb={cdb}')\n    if cdb is None and torch.distributed.is_initialized():\n        # The user initialized torch.dist themselves, create cdb and short-circuit\n        cdb = TorchBackend(dist_backend, timeout, init_method)\n        return\n\n    if dist_init_required is False:\n        assert (\n            cdb is not None and cdb.is_initialized() is True\n        ), \"Distributed backend is not initialized. Please set dist_init_required to True or initialize before calling deepspeed.initialize()\"\n    else:\n        # Initialize torch distributed if needed\n        required_env = [\"RANK\", \"WORLD_SIZE\", \"MASTER_ADDR\", \"MASTER_PORT\", \"LOCAL_RANK\"]\n        if auto_mpi_discovery and not all(map(lambda v: v in os.environ, required_env)):\n            if verbose:\n                utils.logger.info(\"Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\")\n            if in_aml() and not in_dlts():\n                patch_aml_env_for_torch_nccl_backend(verbose=verbose)\n            elif in_aws_sm():\n                patch_aws_sm_env_for_torch_nccl_backend(verbose=verbose)\n            else:\n                mpi_discovery(distributed_port=distributed_port, verbose=verbose)\n\n        if cdb is not None and cdb.is_initialized():\n            if int(os.getenv('RANK', '0')) == 0:\n                utils.logger.info('Distributed backend already initialized')\n        else:\n            assert isinstance(timeout, timedelta)\n            if dist_backend is None:\n                dist_backend = get_accelerator().communication_backend_name()\n            if int(os.getenv('RANK', '0')) == 0:\n                utils.logger.info('Initializing TorchBackend in DeepSpeed with backend {}'.format(dist_backend))\n            # Create a torch backend object, initialize torch distributed, and assign to cdb\n            cdb = TorchBackend(dist_backend, timeout, init_method, rank, world_size)\n\n\ndef mpi_discovery(distributed_port=TORCH_DISTRIBUTED_DEFAULT_PORT, verbose=True):\n    '''\n    Discovery MPI environment via mpi4py and map to relevant dist state\n    '''\n    from mpi4py import MPI\n    import subprocess\n    comm = MPI.COMM_WORLD\n    rank = comm.Get_rank()\n    world_size = comm.Get_size()\n\n    master_addr = None\n    if rank == 0:\n        hostname_cmd = [\"hostname -I\"]\n        result = subprocess.check_output(hostname_cmd, shell=True)\n        master_addr = result.decode('utf-8').split()[0]\n    master_addr = comm.bcast(master_addr, root=0)\n\n    # Determine local rank by assuming hostnames are unique\n    proc_name = MPI.Get_processor_name()\n    all_procs = comm.allgather(proc_name)\n    local_rank = sum([i == proc_name for i in all_procs[:rank]])\n\n    os.environ['RANK'] = str(rank)\n    os.environ['WORLD_SIZE'] = str(world_size)\n    os.environ['LOCAL_RANK'] = str(local_rank)\n    os.environ['MASTER_ADDR'] = master_addr\n    os.environ['MASTER_PORT'] = str(distributed_port)\n\n    if verbose:\n        utils.logger.info(\n            \"Discovered MPI settings of world_rank={}, local_rank={}, world_size={}, master_addr={}, master_port={}\".\n            format(os.environ['RANK'], os.environ['LOCAL_RANK'], os.environ['WORLD_SIZE'], os.environ['MASTER_ADDR'],\n                   os.environ['MASTER_PORT']))\n\n    if cdb is not None and cdb.is_initialized():\n        assert cdb.get_rank() == rank, \"MPI rank {} does not match torch rank {}\".format(rank, cdb.get_rank())\n        assert cdb.get_world_size() == world_size, \"MPI world size {} does not match torch world size {}\".format(\n            world_size, cdb.get_world_size())\n\n\ndef in_aml():\n    # Are we running inside an Azure Machine Learning (AML) environment?\n    return 'AZUREML_EXPERIMENT_ID' in os.environ\n\n\ndef in_aws_sm():\n    # Are we running inside an AWS SageMaker environment?\n    return 'SM_TRAINING_ENV' in os.environ\n\n\ndef in_dlts():\n    # Are we running on a DLTS cluster?\n    return 'DLTS_JOB_ID' in os.environ\n\n\ndef patch_aml_env_for_torch_nccl_backend(master_port=6105, verbose=True):\n    \"\"\"Helper routine to get and set environment variables.\n    This is adapted from Azure ML's documentation available from:\n    https://azure.github.io/azureml-web/docs/cheatsheet/distributed-training/#environment-variables-from-openmpi\n    \"\"\"\n    os.environ[\"RANK\"] = os.environ[\"OMPI_COMM_WORLD_RANK\"]\n    os.environ[\"WORLD_SIZE\"] = os.environ[\"OMPI_COMM_WORLD_SIZE\"]\n    single_node = int(os.environ[\"OMPI_COMM_WORLD_LOCAL_SIZE\"]) == int(os.environ[\"WORLD_SIZE\"])\n\n    if not single_node:\n        master_node_params = os.environ[\"AZ_BATCH_MASTER_NODE\"].split(\":\")\n        os.environ[\"MASTER_ADDR\"] = master_node_params[0]\n        # Do not overwrite master port with that defined in AZ_BATCH_MASTER_NODE\n        if \"MASTER_PORT\" not in os.environ:\n            os.environ[\"MASTER_PORT\"] = str(master_port)\n    else:\n        os.environ[\"MASTER_ADDR\"] = os.environ[\"AZ_BATCHAI_MPI_MASTER_NODE\"]\n        os.environ[\"MASTER_PORT\"] = DEFAULT_AML_MASTER_PORT\n\n    if verbose:\n        utils.logger.info(\"NCCL_SOCKET_IFNAME original value = {}\".format(os.environ[\"NCCL_SOCKET_IFNAME\"]))\n\n    os.environ[\"NCCL_SOCKET_IFNAME\"] = DEFAULT_AML_NCCL_SOCKET_IFNAME\n    os.environ['LOCAL_RANK'] = os.environ[\"OMPI_COMM_WORLD_LOCAL_RANK\"]\n\n    if verbose:\n        utils.logger.info(\n            \"Discovered AzureML settings of world_rank={}, local_rank={}, world_size={}, master_addr={}, master_port={}\"\n            .format(os.environ['RANK'], os.environ['LOCAL_RANK'], os.environ['WORLD_SIZE'], os.environ['MASTER_ADDR'],\n                    os.environ['MASTER_PORT']))\n\n\ndef patch_aws_sm_env_for_torch_nccl_backend(verbose=True):\n    \"\"\"Helper routine to get and set environment variables when running inside an AWS SageMaker environment.\n    \"\"\"\n    os.environ[\"RANK\"] = os.environ[\"OMPI_COMM_WORLD_RANK\"]\n    os.environ['LOCAL_RANK'] = os.environ[\"OMPI_COMM_WORLD_LOCAL_RANK\"]\n    os.environ[\"WORLD_SIZE\"] = os.environ[\"OMPI_COMM_WORLD_SIZE\"]\n\n    if verbose:\n        utils.logger.info(\n            \"Discovered AWS SageMaker settings of world_rank={}, local_rank={}, world_size={}, master_addr={}, master_port={}\"\n            .format(os.environ['RANK'], os.environ['LOCAL_RANK'], os.environ['WORLD_SIZE'], os.environ['MASTER_ADDR'],\n                    os.environ['MASTER_PORT']))\n", "deepspeed/comm/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .utils import *\nfrom .comm import *\n", "deepspeed/pipe/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom ..runtime.pipe import PipelineModule, LayerSpec, TiedLayerSpec\n", "deepspeed/checkpoint/deepspeed_checkpoint.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport os\nimport re\nfrom typing import Dict\nimport torch\n\nfrom .reshape_3d_utils import model_3d_desc\nfrom .reshape_utils import (basic_folder_validation, merge_state, partition_data, get_files, get_files_with_prefix)\n\nfrom .constants import (MODEL_FILE_PREFIX, LAYER_FILE_PREFIX)\n\nfrom .reshape_meg_2d import reshape_meg_2d_parallel, meg_2d_parallel_map\nfrom .zero_checkpoint import ZeROCheckpoint\nfrom .constants import *\n\nEMBEDDING_LAYER_INDEX = 0\nFINAL_LAYER_NORM_INDEX = -1\nARGS_KEY = 'args'\nCHECKPOINT_INFO_KEY = 'checkpoint_info'\nITERATION_KEY = 'iteration'\nLAYER_FILE_PREFIX_PATTERN = r'layer_(\\d+)-model_.*'\n\nSEQUENTIAL_LAYERS = [\n    'input_layernorm.weight', 'input_layernorm.bias', 'self_attention.dense.bias', 'post_attention_layernorm.weight',\n    'post_attention_layernorm.bias', 'mlp.dense_4h_to_h.bias', 'position_embeddings.weight'\n]\n\nLAYER_CONCAT_DIM = {'self_attention.dense.weight': 1, 'mlp.dense_4h_to_h.weight': 1}\n\n\nclass DeepSpeedCheckpoint(object):\n\n    def __init__(self,\n                 dir,\n                 tp_degree=None,\n                 pp_degree=None,\n                 dp_degree=None,\n                 final_layer_norm_idx=FINAL_LAYER_NORM_INDEX):\n        self.final_layer_norm_idx = final_layer_norm_idx\n        self.dir = dir\n\n        pipeline_parallel = len(get_files_with_prefix(get_files(dir), LAYER_FILE_PREFIX)) > 0\n\n        self._validate_folder(dir, pipeline_parallel)\n\n        self.zero_checkpoint = ZeROCheckpoint(dir)\n\n        self.file_list = get_files(dir)\n        self.layer_files = get_files_with_prefix(self.file_list, LAYER_FILE_PREFIX)\n        self.mp_rank_files = get_files_with_prefix(self.file_list, MODEL_FILE_PREFIX)\n\n        self.layer_keys = self._get_layer_keys()\n        self.layer_count = len(self.layer_keys)\n\n        self.tp_degree = self.zero_checkpoint.get_src_tp_degree() if tp_degree is None else tp_degree\n        self.pp_degree = self.zero_checkpoint.get_src_pp_degree() if pp_degree is None else pp_degree\n        self.dp_degree = self.zero_checkpoint.get_src_dp_degree() if dp_degree is None else dp_degree\n\n        self.original_world_size = self.zero_checkpoint.get_src_tp_degree() * self.zero_checkpoint.get_src_pp_degree(\n        ) * self.zero_checkpoint.get_src_dp_degree()\n        self.world_size = self.tp_degree * self.pp_degree * self.dp_degree\n\n        self.old_2d_map = meg_2d_parallel_map(self.zero_checkpoint.get_src_pp_degree(),\n                                              self.zero_checkpoint.get_src_tp_degree())\n        self.old_2d_map.simple_init()\n        self.new_2d_map = reshape_meg_2d_parallel(old_pp_degree=self.zero_checkpoint.get_src_pp_degree(),\n                                                  old_tp_degree=self.zero_checkpoint.get_src_tp_degree(),\n                                                  new_pp_degree=self.pp_degree,\n                                                  new_tp_degree=self.tp_degree)\n\n        if self.is_change_pp_degree() or self.is_change_tp_degree() or self.is_change_dp_degree():\n            self.zero_checkpoint.reshape(model_3d_desc(self.pp_degree, self.tp_degree, self.dp_degree))\n\n        self.global_state = {}\n\n        self._sanity_check()\n        self.pp_to_transformer_map = self._build_pp_transformer_map()\n        self.transformer_file_map = self._build_transformer_file_map()\n        self.tp_to_embedding_map = self._build_tp_other_layer_map(EMBEDDING_LAYER_INDEX)\n        self.tp_to_final_norm_map = self._build_tp_other_layer_map(self.final_layer_norm_idx)\n        self._build_global_state()\n\n    def is_change_tp_degree(self):\n        return self.tp_degree != self.zero_checkpoint.get_src_tp_degree()\n\n    def is_change_pp_degree(self):\n        return self.pp_degree != self.zero_checkpoint.get_src_pp_degree()\n\n    def is_change_dp_degree(self):\n        return self.dp_degree != self.zero_checkpoint.get_src_dp_degree()\n\n    def show_2d_mapping(self):\n        print(f'reshaped 2d map ---- begin')\n\n        for i in range(self.pp_degree):\n            for j in range(self.tp_degree):\n                file_list = self.get_2d_parallel_files(pp_index=i, tp_index=j)\n                print(f'[{i}, {j}] = {file_list}')\n\n        print(f'reshaped 2d map ---- end')\n\n    def show_tp_embedding_map(self):\n        self._dump_mapping(self.tp_to_embedding_map, 'tp_to_embedding_layers')\n\n    def show_tp_final_norm_map(self):\n        self._dump_mapping(self.tp_to_final_norm_map, 'tp_to_final_norm_layers')\n\n    def show_pp_transformer_map(self):\n        self._dump_mapping(self.pp_to_transformer_map, 'pp_to_transformer_layers')\n\n    def show_transformer_file_map(self):\n        self._dump_mapping(self.transformer_file_map, 'rank_to_transformer_files')\n\n    def _build_global_state(self):\n        sd = torch.load(self.mp_rank_files[0], map_location=torch.device('cpu'))\n        self.global_state[ITERATION_KEY] = sd.get(ITERATION_KEY, 0)\n        self.global_state[ARGS_KEY] = sd.get(ARGS_KEY, None)\n\n    def get_zero_checkpoint_state(self, pp_index, tp_index, dp_index) -> dict:\n        return self.zero_checkpoint.get_state_for_rank(pp_index=pp_index,\n                                                       tp_index=tp_index,\n                                                       dp_index=dp_index,\n                                                       keys_to_ignore=[PARAM_SHAPES])\n\n    def get_zero_files(self, pp_index, tp_index, dp_index) -> list:\n        return self.zero_checkpoint.get_files_for_rank(pp_index=pp_index, tp_index=tp_index, dp_index=dp_index)\n\n    def get_embedding_layer_id(self):\n        return self.layer_keys[EMBEDDING_LAYER_INDEX]\n\n    def get_final_norm_layer_id(self):\n        return self.layer_keys[self.final_layer_norm_idx]\n\n    def get_iteration(self):\n        if not ITERATION_KEY in self.global_state:\n            sd = torch.load(self.mp_rank_files[0], map_location=torch.device('cpu'))\n            self.global_state[ITERATION_KEY] = sd.get(ITERATION_KEY, 0)\n\n        return self.global_state[ITERATION_KEY]\n\n    def get_embedding_state(self, tp_index: int) -> Dict:\n        assert tp_index in self.tp_to_embedding_map.keys()\n        sd_list = [torch.load(fname, map_location=torch.device('cpu')) for fname in self.tp_to_embedding_map[tp_index]]\n        sd = self._merge_state_dicts(sd_list)\n        return sd\n\n    def get_embedding_files(self, tp_index: int) -> list:\n        assert tp_index in self.tp_to_embedding_map.keys()\n        return self.tp_to_embedding_map[tp_index]\n\n    def _get_checkpoint_value(self, key):\n        if not key in self.global_state:\n            sd = torch.load(self.mp_rank_files[0], map_location=torch.device('cpu'))\n            self.global_state[key] = sd.get(key, None)\n\n        return self.global_state[key]\n\n    def get_args(self):\n        return self._get_checkpoint_value(ARGS_KEY)\n\n    def get_checkpoint_info(self, info_key=CHECKPOINT_INFO_KEY):\n        return self._get_checkpoint_value(info_key)\n\n    def get_2d_parallel_state(self, tp_index: int, pp_index: int) -> dict:\n        assert tp_index < self.tp_degree\n        assert pp_index < self.pp_degree\n        fname_list = self.get_2d_parallel_files(tp_index=tp_index, pp_index=pp_index)\n        sd_list = [torch.load(fname, map_location=torch.device('cpu')) for fname in fname_list]\n\n        merged_sd = None\n        for sd in sd_list:\n            if merged_sd is None:\n                merged_sd = sd\n            else:\n                merged_sd = merge_state(merged_sd, sd)\n\n        return merged_sd\n\n    def get_transformer_state(self, tp_index: int, pp_index: int) -> list:\n        assert tp_index < self.tp_degree\n        assert pp_index < self.pp_degree\n        t_list = []\n        for fname_list in self.transformer_file_map[(tp_index, pp_index)]:\n            sd_list = [torch.load(fname, map_location=torch.device('cpu')) for fname in fname_list]\n            sd = self._merge_state_dicts(sd_list)\n            t_list.append(sd)\n        return t_list\n\n    def get_pp_transformer_map(self, pp_index: int) -> list:\n        assert pp_index < self.pp_degree\n        return self.pp_to_transformer_map[pp_index]\n\n    def get_final_norm_state(self, tp_index: int) -> Dict:\n        assert tp_index in self.tp_to_final_norm_map.keys()\n        sd = torch.load(self.tp_to_final_norm_map[tp_index][0], map_location=torch.device('cpu'))\n        return sd\n\n    def get_final_norm_files(self, tp_index: int) -> list:\n        assert tp_index in self.tp_to_final_norm_map.keys()\n        return self.tp_to_final_norm_map[tp_index]\n\n    def _build_tp_other_layer_map(self, layer_index: int):\n        data_map = {}\n        if len(self.layer_files) < 1:\n            return data_map\n        assert layer_index <= len(self.layer_files)\n        layer_files = get_files_with_prefix(self.layer_files, self.layer_keys[layer_index])\n        layer_file_partitions = partition_data(layer_files, self.tp_degree)\n        data_map = {i: flist for i, flist in enumerate(layer_file_partitions)}\n        return data_map\n\n    def get_2d_parallel_files(self, tp_index: int, pp_index: int) -> list:\n        assert tp_index < self.tp_degree\n        assert pp_index < self.pp_degree\n        file_indices = self.new_2d_map.get_data(pp_index=pp_index, tp_index=tp_index)\n        return [self.mp_rank_files[i] for i in file_indices]\n\n    def _build_pp_transformer_map(self):\n        data_map = {}\n        if self.pp_degree > 0:\n            transformer_layers = self.layer_keys[1:self.final_layer_norm_idx]\n            layers_per_pp = len(transformer_layers) // self.pp_degree\n            data_map = {\n                i: transformer_layers[i * layers_per_pp:(i + 1) * layers_per_pp]\n                for i in range(0, self.pp_degree)\n            }\n        return data_map\n\n    def _dump_mapping(self, data_map, map_tag=None):\n        if map_tag is not None:\n            print(f'Dump mapping: {map_tag}')\n        for k, v in data_map.items():\n            print(f'{k} = {v}')\n\n    def _build_transformer_file_map(self):\n        transformer_layer_keys = self.layer_keys[1:self.final_layer_norm_idx]\n        file_map = {}\n        # XXX: this is not guaranteed\n        layers_per_pp = 1\n        if self.pp_degree > 0:\n            layers_per_pp = len(transformer_layer_keys) // self.pp_degree\n        #print(f\"{transformer_layer_keys} {layers_per_pp}\")\n        for key_index, layer_key in enumerate(transformer_layer_keys):\n            pp_index = key_index // layers_per_pp\n            layer_files = get_files_with_prefix(self.layer_files, layer_key + '-')\n            layer_file_partitions = partition_data(layer_files, self.tp_degree)\n            for tp_index in range(self.tp_degree):\n                map_key = (tp_index, pp_index)\n                if not map_key in file_map.keys():\n                    file_map[map_key] = []\n                file_map[map_key].append(layer_file_partitions[tp_index])\n\n        return file_map\n\n    def _sanity_check(self):\n        assert len(self.mp_rank_files) % self.tp_degree == 0\n        assert self.zero_checkpoint.num_files % (self.pp_degree * self.tp_degree) == 0\n        assert self.zero_checkpoint.num_files % (self.tp_degree) == 0\n        # XXX: fix me - isn't always the case\n        # only true with  --pp-partition-method 'type:transformer|embedding' \\\n        # assert (len(self.layer_keys) - 2) % self.pp_degree == 0\n\n    def validate_files(self):\n        for file in self.file_list:\n            if not os.path.isfile(file):\n                print(f'Error: {file} is not existent')\n\n    def _get_layer_keys(self):\n        key_set = set()\n        for file_path in self.layer_files:\n            _, fname = os.path.split(file_path)\n            layer_id = re.search(LAYER_FILE_PREFIX_PATTERN, fname).group(1)\n            key_set.add(layer_id)\n        sorted_ids = sorted(list(key_set), key=int)\n        layer_keys = [LAYER_FILE_PREFIX + str(layer_id) for layer_id in sorted_ids]\n        return layer_keys\n\n    def _merge_state_dicts(self, sd_list):\n        merged_sd = {}\n        for key in sd_list[0].keys():\n            if not key in SEQUENTIAL_LAYERS:\n                cat_dim = LAYER_CONCAT_DIM.get(key, 0)\n                merged_sd[key] = torch.cat([sd[key] for sd in sd_list], dim=cat_dim)\n            else:\n                merged_sd[key] = sd_list[0][key]\n\n        return merged_sd\n\n    def _validate_folder(self, dir, pipeline_parallel):\n        basic_folder_validation(dir)\n\n        file_list = get_files(dir)\n        file_prefix_list = [MODEL_FILE_PREFIX]\n        if pipeline_parallel:\n            file_prefix_list.extend([LAYER_FILE_PREFIX, f'{LAYER_FILE_PREFIX}01'])\n        for file_prefix in file_prefix_list:\n            ckpt_files = get_files_with_prefix(file_list, file_prefix)\n            assert len(\n                ckpt_files\n            ) > 0, f'{dir} seems a bogus DeepSpeed checkpoint folder: Cannot find {file_prefix}* files in there.'\n", "deepspeed/checkpoint/reshape_meg_2d.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .reshape_utils import partition_data\n\n\nclass meg_2d_parallel_map(object):\n\n    def __init__(self, pp_degree, tp_degree):\n        self.pp_degree = pp_degree\n        self.tp_degree = tp_degree\n        self.map = {}\n\n    def simple_init(self):\n        self.map = {\n            self._make_key(i // self.tp_degree, i % self.tp_degree): [i]\n            for i in range(self.pp_degree * self.tp_degree)\n        }\n\n    def add_data(self, pp_index, tp_index, data):\n        self._validate_indices(pp_index, tp_index)\n        assert type(data) is list\n\n        key = self._make_key(pp_index, tp_index)\n        if not key in self.map.keys():\n            self.map[key] = []\n        self.map[key] += data\n\n    def get_data(self, pp_index=None, tp_index=None):\n        self._validate_indices(pp_index, tp_index)\n        pp_indices = list(range(self.pp_degree)) if pp_index is None else [pp_index]\n        tp_indices = list(range(self.tp_degree)) if tp_index is None else [tp_index]\n\n        result = []\n        for i in pp_indices:\n            for j in tp_indices:\n                result += self.map[self._make_key(i, j)]\n\n        return result\n\n    def print_data(self, tag):\n        print(f'{tag}')\n        for key, value in self.map.items():\n            print(f'{key} = {value}')\n\n    def _validate_indices(self, pp_index, tp_index):\n        assert pp_index is None or pp_index < self.pp_degree\n        assert tp_index is None or tp_index < self.tp_degree\n\n    def _make_key(self, i, j):\n        return f'{i},{j}'\n\n\ndef _reshape_tp_dimension(old_2d_map, new_tp_degree):\n    old_pp_degree = old_2d_map.pp_degree\n    new_2d_map = meg_2d_parallel_map(old_pp_degree, new_tp_degree)\n    for i in range(old_pp_degree):\n        ranks_for_pp_index = old_2d_map.get_data(pp_index=i, tp_index=None)\n        split_ranks = partition_data(ranks_for_pp_index, new_tp_degree)\n        for j in range(new_tp_degree):\n            new_2d_map.add_data(i, j, split_ranks[j])\n\n    return new_2d_map\n\n\ndef _reshape_pp_dimension(old_2d_map, new_pp_degree):\n    old_tp_degree = old_2d_map.tp_degree\n    new_2d_map = meg_2d_parallel_map(new_pp_degree, old_tp_degree)\n    for i in range(old_tp_degree):\n        ranks_for_tp_index = old_2d_map.get_data(pp_index=None, tp_index=i)\n        split_ranks = partition_data(ranks_for_tp_index, new_pp_degree)\n        for j in range(new_pp_degree):\n            new_2d_map.add_data(j, i, split_ranks[j])\n\n    return new_2d_map\n\n\ndef reshape_meg_2d_parallel(old_pp_degree, old_tp_degree, new_pp_degree, new_tp_degree, verbose=False):\n    assert new_pp_degree <= old_pp_degree\n    assert new_tp_degree <= old_tp_degree\n\n    old_2d_map = meg_2d_parallel_map(old_pp_degree, old_tp_degree)\n    old_2d_map.simple_init()\n    if verbose:\n        old_2d_map.print_data(f'original_2d_map:')\n\n    if old_tp_degree != new_tp_degree:\n        new_tp_map = _reshape_tp_dimension(old_2d_map, new_tp_degree)\n    else:\n        new_tp_map = old_2d_map\n    if verbose:\n        new_tp_map.print_data(f'after_tp_reshape:')\n\n    if old_pp_degree != new_pp_degree:\n        final_map = _reshape_pp_dimension(new_tp_map, new_pp_degree)\n    else:\n        final_map = new_tp_map\n\n    if verbose:\n        final_map.print_data(f'final_2d_map:')\n\n    return final_map\n\n\ndef get_mpu_ranks(tp_size=1, pp_size=1, dp_size=1, virtual_pp_size=None):\n    \"\"\"\n    Initialize model data parallel groups.\n\n    Arguments:\n        tp_size: number of GPUs used to parallelize model tensor.\n        pp_size: number of GPUs used to parallelize model pipeline.\n        dp_size: number of GPUs used to parallelize model data.\n\n    Let's say we have a total of 16 GPUs denoted by g0 ... g15 and we\n    use 2 GPUs to parallelize the model tensor, and 4 GPUs to parallelize\n    the model pipeline. The present function will\n    create 8 tensor model-parallel groups, 4 pipeline model-parallel groups\n    and 8 data-parallel groups as:\n        8 data_parallel groups:\n            [g0, g2], [g1, g3], [g4, g6], [g5, g7], [g8, g10], [g9, g11], [g12, g14], [g13, g15]\n        8 tensor model-parallel groups:\n            [g0, g1], [g2, g3], [g4, g5], [g6, g7], [g8, g9], [g10, g11], [g12, g13], [g14, g15]\n        4 pipeline model-parallel groups:\n            [g0, g4, g8, g12], [g1, g5, g9, g13], [g2, g6, g10, g14], [g3, g7, g11, g15]\n    Note that for efficiency, the caller should make sure adjacent ranks\n    are on the same DGX box. For example if we are using 2 DGX-1 boxes\n    with a total of 16 GPUs, rank 0 to 7 belong to the first box and\n    ranks 8 to 15 belong to the second box.\n    \"\"\"\n\n    world_size = tp_size * pp_size * dp_size\n\n    print(f\"\\n\\n*** tp={tp_size}, pp={pp_size}, dp={dp_size}, world={world_size}\")\n\n    tensor_model_parallel_size = min(tp_size, world_size)\n    pipeline_model_parallel_size = min(pp_size, world_size)\n    data_parallel_size = world_size // (tensor_model_parallel_size * pipeline_model_parallel_size)\n\n    num_tensor_model_parallel_groups = world_size // tensor_model_parallel_size\n    num_pipeline_model_parallel_groups = world_size // pipeline_model_parallel_size\n    num_data_parallel_groups = world_size // data_parallel_size\n\n    # Build the data-parallel groups.\n    all_dp_group_ranks = []\n    for i in range(pipeline_model_parallel_size):\n        start_rank = i * num_pipeline_model_parallel_groups\n        end_rank = (i + 1) * num_pipeline_model_parallel_groups\n        for j in range(tensor_model_parallel_size):\n            ranks = range(start_rank + j, end_rank, tensor_model_parallel_size)\n            all_dp_group_ranks.append(list(ranks))\n\n    print(\"DP\", all_dp_group_ranks)\n\n    # Build the model-parallel groups.\n    all_pp_group_ranks = []\n    for i in range(data_parallel_size):\n        ranks = [data_parallel_group_ranks[i] for data_parallel_group_ranks in all_dp_group_ranks]\n        all_pp_group_ranks.append(list(ranks))\n\n    print(f\"PP\", all_pp_group_ranks)\n\n    # Build the tensor model-parallel groups.\n    all_tp_group_ranks = []\n    for i in range(num_tensor_model_parallel_groups):\n        ranks = range(i * tensor_model_parallel_size, (i + 1) * tensor_model_parallel_size)\n        all_tp_group_ranks.append(list(ranks))\n\n    print(f\"TP\", all_tp_group_ranks)\n\n    return all_tp_group_ranks, all_pp_group_ranks, all_dp_group_ranks\n\n    # # Build the pipeline model-parallel groups and embedding groups\n    # # (first and last rank in each pipeline model-parallel group).\n    # for i in range(num_pipeline_model_parallel_groups):\n    #     ranks = range(i, world_size,\n    #                   num_pipeline_model_parallel_groups)\n    #     print(f\"EMB{i}\", list(ranks))\n\n\ndef reshape(src, tgt):\n    \"\"\"\n    reshape([tp_size_src, pp_size_src, dp_size_src],\n            [tp_size_tgt, pp_size_tgt, dp_size_tgt])\n    \"\"\"\n\n    print(f\"\\n\\n*** Reshaping: {src} => {tgt}\")\n\n    tp_size_src, pp_size_src, dp_size_src = src\n    tp_size_tgt, pp_size_tgt, dp_size_tgt = tgt\n\n    tp_ranks1, pp_ranks1, dp_ranks1 = get_mpu_ranks(tp_size=tp_size_src, pp_size=pp_size_src, dp_size=dp_size_src)\n    tp_ranks2, pp_ranks2, dp_ranks2 = get_mpu_ranks(tp_size=tp_size_tgt, pp_size=pp_size_src, dp_size=dp_size_src)\n    tp_ranks3, pp_ranks3, dp_ranks3 = get_mpu_ranks(tp_size=tp_size_tgt, pp_size=pp_size_tgt, dp_size=dp_size_src)\n\n    # handle tp contraction first\n    print(\"\\n*** TP contraction:\")\n\n    for i, r in enumerate(tp_ranks1):\n        print(f'{tp_ranks1[i]} => {tp_ranks2[i]}')\n\n    # handle pp contraction next\n\n    print(\"\\n*** PP contraction:\")\n\n    for i, r in enumerate(pp_ranks1):\n        print(f'{pp_ranks2[i]} => {pp_ranks3[i]}')\n\n\n# easy\n#reshape([2,2,1],[1,1,1])\n\n# probably need more logic to suggest how to pack\n#reshape([4,4,1],[2,2,1])\n\n#reshape([2,4,2], [8,32,1])\n\n# get_mpu_ranks(2,2,2)\n# get_mpu_ranks(4,2,1)\n# get_mpu_ranks(2,4,1)\n# get_mpu_ranks(1,1,8)\n", "deepspeed/checkpoint/reshape_utils.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport os\nimport re\nimport torch\nfrom collections import OrderedDict\nfrom .constants import (ZERO_FILE_PREFIX, FP16_ZERO_FILE_PREFIX, BF16_ZERO_FILE_PREFIX, MODEL_FILE_PREFIX)\n\n\ndef basic_folder_validation(dir):\n    assert os.path.exists(dir), f'{dir} path does not exist'\n    assert os.path.isdir(dir), f'{dir} is not a folder'\n\n\ndef get_files_with_prefix(all_files, prefix):\n    file_list = []\n    for file_path in all_files:\n        _, fname = os.path.split(file_path)\n        if fname.startswith(prefix):\n            file_list.append(file_path)\n\n    return sorted(file_list)\n\n\ndef validate_files(file_list):\n    for file in file_list:\n        if not os.path.isfile(file):\n            print(f'Error: {file} is not existent')\n\n\ndef get_files(dir):\n    file_list = []\n    for root, _, files in os.walk(dir):\n        for file in files:\n            file_list.append(os.path.join(root, file))\n    return file_list\n\n\ndef sort_zero_files(files, prefix):\n    pattern = f\"{prefix}([0-9]+)_{MODEL_FILE_PREFIX}([0-9]+)\"\n    rank_pairs = []\n    for f in files:\n        m = re.search(pattern, f)\n        if m:\n            dp_rank = int(m.group(1))\n            mp_rank = int(m.group(2))\n            rank_pairs.append((dp_rank, mp_rank, f))\n        else:\n            raise ValueError(f\"Cannot parse dp_rank and mp_rank from {f}\")\n\n    sorted_files = sorted(rank_pairs, key=lambda x: (x[0], x[1]))\n    return [f for _, _, f in sorted_files]\n\n\ndef get_zero_files(dir):\n    file_list = get_files(dir)\n    for prefix in [ZERO_FILE_PREFIX, FP16_ZERO_FILE_PREFIX, BF16_ZERO_FILE_PREFIX]:\n        zero_files = get_files_with_prefix(file_list, prefix)\n        if len(zero_files) > 0:\n            return sort_zero_files(zero_files, prefix)\n\n    return []\n\n\ndef partition_data(data_list, num_partitions):\n    num_elems = len(data_list)\n    assert num_elems % num_partitions == 0\n    partition_size = num_elems // num_partitions\n    partitions_list = [data_list[i:i + partition_size] for i in range(0, num_elems, partition_size)]\n    return partitions_list\n\n\ndef _key_list_to_string(key_list):\n    return '.'.join(key_list)\n\n\ndef merge_state_dict(dict_a, dict_b, key_list):\n    merged_dict = type(dict_a)({})\n\n    for key, value in dict_b.items():\n        if key in dict_a.keys():\n            merged_dict[key] = merge_state(dict_a[key], dict_b[key], [str(key)])\n        else:\n            merged_dict[key] = value\n\n    return merged_dict\n\n\ndef merge_state_list(list_a, list_b, key_list):\n    if len(list_a) != len(list_b):\n        print(f'{_key_list_to_string(key_list)}')\n        raise ValueError(f'Cannot merge lists of different lengths, a = {len(list_a)} b = {len(list_b)}')\n\n    return [merge_state(a, b, key_list) for a, b in zip(list_a, list_b)]\n\n\ndef merge_state(state_a, state_b, key_list=[]):\n    if type(state_a) != type(state_b):\n        key_list_string = _key_list_to_string(key_list)\n        print(f'key_list = {key_list_string}')\n        raise ValueError(f'Cannot merge two states of types {type(state_a)} and type {type(state_b)}')\n\n    if type(state_a) in (dict, OrderedDict):\n        return merge_state_dict(state_a, state_b, key_list)\n    elif type(state_a) in (list, tuple):\n        return type(state_a)(merge_state_list(state_a, state_b, key_list))\n    elif torch.is_tensor(state_a):\n        return torch.cat([state_a, state_b], 0)\n    else:\n        return state_a\n", "deepspeed/checkpoint/utils.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport os\nimport torch\nfrom .constants import (MODEL_FILE_PREFIX, MODEL_FILE_SUFFIX, OPTIM_FILE_SUFFIX, ZERO_FILE_PREFIX)\n\n\ndef get_model_ckpt_name_for_rank(base_folder, mp_rank_str):\n    ckpt_name = os.path.join(\n        base_folder,\n        MODEL_FILE_PREFIX + mp_rank_str + MODEL_FILE_SUFFIX,\n    )\n    return ckpt_name\n\n\ndef get_zero_ckpt_name_for_rank(base_folder, dp_rank, mp_rank):\n    zero_prefix = f'{ZERO_FILE_PREFIX}{dp_rank}'\n    mp_rank_string = f'_{MODEL_FILE_PREFIX}{mp_rank:02d}'\n    zero_ckpt_name = os.path.join(\n        base_folder,\n        zero_prefix + mp_rank_string + OPTIM_FILE_SUFFIX,\n    )\n    return zero_ckpt_name\n\n\ndef get_layer_ckpt_name_for_rank(base_folder, layer_id, tp_rank):\n    ckpt_file = f'{layer_id}-model_{tp_rank:02d}{MODEL_FILE_SUFFIX}'\n    ckpt_path = os.path.join(base_folder, ckpt_file)\n    return ckpt_path\n\n\n# We pass cloned tensors to torch.save() to avoid checkpoint bloat that occurs when torch.save()\n# saves the underlying storage rather than the slice of the storage corresponding to individual tensors.\n# This is a problem in DeepSpeed because we often allocate tensors using slices of large flattened buffers.\n# Tensor cloning helps to avoid this problem because the storage of cloned tensors are closer to the true size.\n# It is expected that the garbage collector will reclaim the cloned tensor storage to avoid memory bloat.\n# See https://pytorch.org/docs/stable/notes/serialization.html#preserve-storage-sharing\ndef clone_tensors_for_torch_save(item, device=torch.device('cpu')):\n    \"\"\"\n    Returns a copy of ``item`` with all enclosed tensors replaced by clones on a specified device.\n    Works on individual tensors, and tensors contained/nested in lists, tuples, and dicts.\n\n    Parameters:\n        - ``item``: tensor to clone or (possibly nested) container of tensors to clone.\n        - ``device``: target device (defaults to 'cpu')\n\n    Returns:\n        - copy of ``item`` with cloned tensors on target device\n    \"\"\"\n    if torch.is_tensor(item):\n        return item.detach().clone().to(device)\n    elif isinstance(item, list):\n        return [clone_tensors_for_torch_save(v, device) for v in item]\n    elif isinstance(item, tuple):\n        return tuple([clone_tensors_for_torch_save(v, device) for v in item])\n    elif isinstance(item, dict):\n        return type(item)({k: clone_tensors_for_torch_save(v, device) for k, v in item.items()})\n    else:\n        return item\n", "deepspeed/checkpoint/universal_checkpoint.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport os\nimport re\nimport torch\nimport types\nfrom typing import List, Tuple, Union\nfrom dataclasses import dataclass\nfrom .constants import (FP32_WEIGHT_KEY, PARAM, VOCAB_TENSOR, CAT_DIM, PARAM_N_SUB_PARAMS, SUB_PARAM_SHAPE)\n\n\n@dataclass\nclass SubparamShape:\n    patterns: List[str]\n    shape: Tuple[Union[Tuple[int], int]]\n    partition_dim: int\n\n\ndef load_hp_checkpoint_state(self, folder, tp_rank, tp_world_size):\n    hp_mapping = self._hp_mapping\n    hp_mapping.optim_fragment = {}\n\n    hp_keys = []\n    for file in os.listdir(folder):\n        # We expect files named something like \"exp_avg.pt\", \"exp_avg_sq.pt\", \"fp32.pt\"\n        pattern = r'(.+).pt'\n        match = re.search(pattern, file)\n        if match:\n            hp_keys.append(match.group(1))\n\n    step = None\n    for key in hp_keys:\n        ckpt_file = os.path.join(folder, f\"{key}.pt\")\n        ckpt_dict = torch.load(ckpt_file)\n\n        if key == \"step\":\n            step = ckpt_dict\n            continue\n\n        full_hp_param = ckpt_dict[PARAM]\n\n        # need to deal with slices that were averaged.\n        # the opposite of averaging here becomes an exact copy of the first slice\n        # I thought of 2 ways:\n        # implementation a. find a way for a client to pass a dict with patterns\n        # if any(re.search(pattern, folder) for pattern in WEIGHTS_TO_AVERAGE_PATTERNS):\n        #     tp_rank = 0\n        #     tp_world_size = 1\n        # the other approach is to assume that the saved data is correct and if full_hp_param.shape ==\n        # self.shape that means we automatically copy?\n        # implementation b.\n        # this version requires no additional data passed from the client\n        # if the shapes already match it must be slices that were averaged - so we just hack around those\n        if full_hp_param.shape == self.shape:\n            tp_rank = 0\n            tp_world_size = 1\n\n        # special case for word_embeddings weights which get padded differently depending on TP degree.\n        # the converter to universal currently strips the original padding completely so the saved\n        # weight is padding-free and we just need to add new padding depending on the target TP\n        # degree\n        is_vocab_tensor = ckpt_dict.get(VOCAB_TENSOR, False)\n        if is_vocab_tensor:\n            # In the absence of data passed from the user wrt new padded vocab specific to tp degree\n            # we can again derive that data by reverse engineering the target shapes like so:\n            padded_target_vocab_size = self.shape[0] * tp_world_size\n            assert padded_target_vocab_size >= full_hp_param.shape[0], \\\n                f'Vocab tensor padded size {padded_target_vocab_size} < loaded universal size {full_hp_param.shape[0]}'\n            if padded_target_vocab_size > full_hp_param.shape[0]:\n                padding_size = padded_target_vocab_size - full_hp_param.shape[0]\n                full_hp_param = torch.nn.functional.pad(full_hp_param, (0, 0, 0, padding_size), \"constant\", 0)\n\n        full_param_numel = full_hp_param.numel()\n        tp_slice_numel = self.numel()\n        #        if key == FP32_WEIGHT_KEY and 'word_embeddings.weight' in folder:\n        #            print_rank_0(f'{full_hp_param[:10]=}', force=True)\n\n\n        assert full_param_numel == tp_world_size * tp_slice_numel, \\\n            f'Loading {ckpt_file} full param numel {full_param_numel} != tensor slice numel {tp_slice_numel} * tp_world_size {tp_world_size}'\n\n        #        print(f\"{full_hp_param.shape=} {full_param_numel=} {folder=}\")\n        #        print(f\"{dst_tensor.shape=} {dst_tensor.numel()=}{folder=}\")\n\n        sub_param_shape = ckpt_dict.get(SUB_PARAM_SHAPE, None)\n        # since when we do many to 1 on tp we cat sometimes on dim=0 and other times on dim=1 we have to do exactly the same in reverse\n        # special case is when a single parameter is effectively a container for multiple sub parameters\n        # (more details at PARAM_N_SUB_PARAMS definition)\n        chunk_dim = ckpt_dict.get(CAT_DIM, 0)\n        n_sub_params = ckpt_dict.get(PARAM_N_SUB_PARAMS, 1)\n        if sub_param_shape:\n            partition_dim = sub_param_shape.partition_dim\n            sub_dim_sizes = sub_param_shape.shape[partition_dim]\n            if not isinstance(sub_dim_sizes, tuple):\n                sub_dim_sizes = (sub_dim_sizes, )\n\n            partition_shape = [sum(d) if isinstance(d, tuple) else d for d in sub_param_shape.shape]\n            full_hp_param = full_hp_param.view(partition_shape)\n\n            offset = 0\n            merged_chunks = []\n            for sub_dim_size in sub_dim_sizes:\n                sub_params_tp_slice = full_hp_param.narrow(partition_dim,\n                                                           offset, sub_dim_size).chunk(tp_world_size,\n                                                                                       dim=partition_dim)[tp_rank]\n                merged_chunks.append(sub_params_tp_slice)\n                offset += sub_dim_size\n            tp_hp_slice = torch.cat(merged_chunks, dim=partition_dim)\n\n        elif n_sub_params > 1:\n            sub_params = full_hp_param.chunk(n_sub_params, dim=chunk_dim)\n            sub_params_tp_slice = [p.chunk(tp_world_size, dim=chunk_dim)[tp_rank] for p in sub_params]\n            tp_hp_slice = torch.cat(sub_params_tp_slice, dim=chunk_dim)\n        else:\n            # this performs the opposite of cat when merging TP slices\n            tp_hp_slice = full_hp_param.chunk(tp_world_size, chunk_dim)[tp_rank]\n\n        tp_hp_slice = tp_hp_slice.flatten()\n\n        lp_frag_address = hp_mapping.lp_fragment_address\n        tp_hp_fragment = tp_hp_slice.narrow(0, lp_frag_address.start, lp_frag_address.numel)\n\n        #        print(f\"{key} SHAPE: {tp_hp_slice.shape=}\")\n        #        print(f\"{key} SHAPE: {dst_tensor.shape=}\")\n        #        print(f\"{key} SHAPE: {tp_hp_fragment.shape=}\")\n\n        if key == FP32_WEIGHT_KEY:\n            dst_tensor = hp_mapping.get_hp_fragment()\n            assert dst_tensor.numel() == lp_frag_address.numel, \\\n                f'Load checkpoint {key} dst numel {dst_tensor.numel()} != src numel {lp_frag_address.numel}'\n            dst_tensor.data.copy_(tp_hp_fragment.data)\n        else:\n            assert tp_hp_fragment.numel() == lp_frag_address.numel, \\\n                f'Load checkpoint {key} dst numel {tp_hp_fragment.numel()} != src numel {lp_frag_address.numel}'\n\n            hp_mapping.optim_fragment[key] = tp_hp_fragment.clone().detach()\n\n    return step\n\n\ndef enable_universal_checkpoint(param_list):\n    for param in param_list:\n        param.load_hp_checkpoint_state = types.MethodType(load_hp_checkpoint_state, param)\n", "deepspeed/checkpoint/zero_checkpoint.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\n\nfrom .constants import (BASE_OPTIMIZER_STATE, GROUP_PADDINGS, OPTIMIZER_STATE_DICT, PARTITION_COUNT)\n\nfrom .reshape_utils import (basic_folder_validation, get_zero_files, merge_state)\n\nfrom .reshape_3d_utils import (model_3d_desc, get_model_3d_descriptor)\n\nGROUP_STATE_KEY = 'state'\n\n\nclass ZeROCheckpoint(object):\n\n    def __init__(self, dir):\n        basic_folder_validation(dir)\n        self.dir = dir\n        self.file_list = get_zero_files(dir)\n        self.num_files = len(self.file_list)\n        assert self.num_files > 0, f'No ZeRO files found in {dir}'\n\n        self.src_3d = get_model_3d_descriptor(dir)\n        self.target_3d = model_3d_desc(pp_degree=self.src_3d.pp_degree,\n                                       tp_degree=self.src_3d.tp_degree,\n                                       dp_degree=self.src_3d.dp_degree)\n        self._3d_file_map = self.src_3d.reshape(self.target_3d)\n\n    def get_src_world_size(self):\n        return self.src_3d.world_size()\n\n    def get_src_tp_degree(self):\n        return self.src_3d.tp_degree\n\n    def get_src_pp_degree(self):\n        return self.src_3d.pp_degree\n\n    def get_src_dp_degree(self):\n        return self.src_3d.dp_degree\n\n    def get_file_indices_for_rank(self, pp_index, tp_index, dp_index):\n        assert dp_index < len(self._3d_file_map), f'DP index {dp_index} >= DP degree {len(self._3d_file_map)}'\n        dp_2d_map = self._3d_file_map[dp_index]\n        return dp_2d_map.get_data(pp_index, tp_index)\n\n    def get_files_for_rank(self, pp_index, tp_index, dp_index):\n        file_idx_list = self.get_file_indices_for_rank(pp_index, tp_index, dp_index)\n        return [self.file_list[idx] for idx in file_idx_list]\n\n    def get_state_for_rank(self, pp_index, tp_index, dp_index, keys_to_ignore=[], strip_tensor_paddings=True):\n        state_file_list = self.get_files_for_rank(pp_index, tp_index, dp_index)\n        merged_sd = None\n        for state_file in state_file_list:\n            sd = torch.load(state_file, map_location=torch.device('cpu'))\n            for key in keys_to_ignore:\n                sd.pop(key, None)\n\n            if strip_tensor_paddings:\n                self._strip_tensor_paddings(sd)\n\n            if merged_sd is None:\n                merged_sd = sd\n            else:\n                merged_sd = merge_state(merged_sd, sd)\n\n            self._update_partition_count(merged_sd)\n            if strip_tensor_paddings:\n                self._clear_group_paddings(merged_sd)\n\n        return merged_sd\n\n    def print_3d_index_map(self, tag=None):\n        if tag:\n            print(f'3D index map: {tag}')\n        for dp_index, _2d_map in enumerate(self._3d_file_map):\n            _2d_map.print_data(f'dp = {dp_index}')\n\n    def print_3d_file_map(self, tag=None):\n        if tag:\n            print(f'3D file map: {tag}')\n        for dp_index, _2d_map in enumerate(self._3d_file_map):\n            for pp_index in _2d_map.pp_degree:\n                for tp_index in _2d_map.tp_degree:\n                    file_index_list = _2d_map.get_data(pp_index, tp_index)\n                    file_list = [self.file_list[idx] for idx in file_index_list]\n                    print(f'{pp_index}, {tp_index}, {dp_index} => {file_list}')\n\n    def reshape(self, target_3d_desc: model_3d_desc):\n        self.target_3d = target_3d_desc\n        self._3d_file_map = self.src_3d.reshape(self.target_3d)\n\n    def _strip_tensor_paddings(self, sd):\n        param_group_states = self._get_param_group_states(sd)\n        if param_group_states is None:\n            return\n\n        group_paddings = self._get_optimizer_state(sd, GROUP_PADDINGS)\n        if group_paddings is None:\n            return\n\n        for key, group_state in param_group_states.items():\n            if group_paddings[key] == 0:\n                continue\n            for state_name, state_value in group_state.items():\n                if state_name != \"step\" and torch.is_tensor(state_value):\n                    raw_length = state_value.numel() - group_paddings[key]\n                    group_state[state_name] = torch.narrow(state_value, 0, 0, raw_length).clone()\n                else:\n                    group_state[state_name] = state_value\n\n    def _clear_group_paddings(self, sd):\n        group_paddings = self._get_optimizer_state(sd, GROUP_PADDINGS)\n        if group_paddings:\n            num_groups = len(group_paddings)\n            sd[OPTIMIZER_STATE_DICT][GROUP_PADDINGS] = [0] * num_groups\n\n    def _get_optimizer_state(self, sd, state_key):\n        optimizer_state = sd.get(OPTIMIZER_STATE_DICT, None)\n        if optimizer_state is None:\n            return None\n\n        return optimizer_state.get(state_key, None)\n\n    def _get_param_group_states(self, sd):\n        optimizer_state = sd.get(OPTIMIZER_STATE_DICT, None)\n        if optimizer_state is None:\n            return None\n\n        base_optimizer_state = optimizer_state.get(BASE_OPTIMIZER_STATE, None)\n        if base_optimizer_state is None:\n            return None\n\n        return base_optimizer_state.get(GROUP_STATE_KEY, None)\n\n    def _update_partition_count(self, sd):\n        partition_counts = self._get_optimizer_state(sd, PARTITION_COUNT)\n        if partition_counts:\n            num_groups = len(partition_counts)\n            sd[OPTIMIZER_STATE_DICT][PARTITION_COUNT] = [self.target_3d.dp_degree] * num_groups\n", "deepspeed/checkpoint/constants.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\"\"\"\nVarious symbolic constants used for model checkpointing\n\"\"\"\n\n#########################################\n# Optimizer checkpoint keys\n#########################################\nOPTIMIZER_STATE_DICT = \"optimizer_state_dict\"\nFP32_GROUPS = \"fp32_groups\"\nFP32_FLAT_GROUPS = 'fp32_flat_groups'\n\nBASE_OPTIMIZER_STATE = 'base_optimizer_state'\nBASE_OPTIMIZER_STATE_STEP = 'base_optimizer_state_step'\nSINGLE_PARTITION_OF_FP32_GROUPS = \"single_partition_of_fp32_groups\"\nPARAM_GROUPS = 'param_groups'\nGROUP_PADDINGS = 'group_paddings'\nPARTITION_COUNT = 'partition_count'\nZERO_STAGE = 'zero_stage'\nCLIP_GRAD = 'clip_grad'\nFP32_WEIGHT_KEY = \"fp32\"\nLOSS_SCALER = 'loss_scaler'\n\n#########################################\n# Module checkpoint keys\n#########################################\nPARAM = 'param'\nPARAM_SHAPES = 'param_shapes'\nBUFFER_NAMES = 'buffer_names'\nFROZEN_PARAM_SHAPES = 'frozen_param_shapes'\nFROZEN_PARAM_FRAGMENTS = 'frozen_param_fragments'\n\n#########################################\n# Checkpoint naming constants\n#########################################\nMODEL_FILE_PREFIX = 'mp_rank_'\nZERO_FILE_PREFIX = 'zero_pp_rank_'\nOPTIM_FILE_SUFFIX = '_optim_states.pt'\nMODEL_FILE_SUFFIX = '_model_states.pt'\nLAYER_FILE_PREFIX = 'layer_'\nBF16_ZERO_FILE_PREFIX = 'bf16_' + ZERO_FILE_PREFIX\nFP16_ZERO_FILE_PREFIX = 'fp16_' + ZERO_FILE_PREFIX\n\n#########################################\n# Checkpoint utility keys\n#########################################\nDS_VERSION = 'ds_version'\n\n#########################################\n# Universal Checkpoint keys\n#########################################\nUNIVERSAL_CHECKPOINT_INFO = 'universal_checkpoint_info'\nUNIVERSAL_CHECKPOINT_VERSION_KEY = 'universal_checkpoint_version'\n# Reserve version 0.1  for the hardcoded logic used in BLOOM-176B training\nUNIVERSAL_CHECKPOINT_VERSION_VALUE = 0.2\n\n# Vocabulary padding\nVOCAB_TENSOR = 'vocab_tensor'\nPADDED_VOCAB_SIZE = 'padded_vocab_size'\nORIGINAL_VOCAB_SIZE = 'original_vocab_size'\n\n# Parameter splitting/merging\nPARAM_SLICE_MAPPINGS = 'param_slice_mappings'\nCAT_DIM = \"cat_dim\"\n# Following is a special case where a parameter effectively contains sub parameters.\n# As an example, consider Megatron-DeepSpeed GPT SWIGLU implementation (mlp.h_to_4h).\n# In this case, a single parameter ia allocated contiguously, but used as separate parameters.\n# When using universal checkpoint, we have to normalize the representation of the full parameter.\n# We normalize it by concatenating all slices of the sub params and then concatenating the sub params.\n# All concat operations are done on CAT_DIM (currently, no support for different concat dims sub params and TP slicing).\n# Similarly, load_hp_checkpoint_state has to take the needed actions when loading from universal.\nPARAM_N_SUB_PARAMS = \"param_n_sub_params\"\n\nSUB_PARAM_SHAPE = \"sub_param_shape\"\n\n# Regex list of parameters that require special handling\nVOCABULARY_PARAMETER_PATTERNS = 'vocabulary_parameter_patterns'\nPIPELINE_REPLICATED_PARAMETER_PATTERNS = 'pipeline_replicated_parameter_patterns'\nPARAMETER_TO_AVERAGE_PATTERNS = 'parameter_to_average_patterns'\nPARAMETER_WITH_ROW_PARALLELISM_PATTERNS = 'parameter_with_row_parallelism_patterns'\nTP_REPLICATED_PARAMETER_PATTERNS = 'tp_replicated_parameter_patterns'\nPARAMETER_WITH_2_SUB_PARAMS_CAT_DIM_0 = 'parameter_with_2_sub_params_cat_dim_0'\nPARAMETER_WITH_SUB_PARAMS = 'parameter_with_sub_params'\nSUB_PARAMS_SHAPE = 'sub_params_shape'\n", "deepspeed/checkpoint/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .reshape_meg_2d import reshape_meg_2d_parallel\n\nfrom .deepspeed_checkpoint import DeepSpeedCheckpoint\n\nfrom .utils import (get_layer_ckpt_name_for_rank, get_model_ckpt_name_for_rank, get_zero_ckpt_name_for_rank)\n\nfrom .reshape_utils import (merge_state)\n\nfrom .reshape_3d_utils import (model_3d_desc, get_model_3d_descriptor)\n\nfrom .zero_checkpoint import ZeROCheckpoint\n\nfrom .universal_checkpoint import enable_universal_checkpoint, SubparamShape\n\nfrom .constants import *\n", "deepspeed/checkpoint/ds_to_universal.py": "#!/usr/bin/env python\n\n# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom functools import partial\nfrom itertools import chain\nimport argparse\nimport glob\nimport itertools\nfrom concurrent.futures import ProcessPoolExecutor\nimport os\nimport re\nimport shutil\nimport torch\nimport tqdm\n#from pprint import pprint\n\nfrom deepspeed.checkpoint import DeepSpeedCheckpoint\nfrom deepspeed.checkpoint import (\n    OPTIMIZER_STATE_DICT,\n    BASE_OPTIMIZER_STATE,\n    SINGLE_PARTITION_OF_FP32_GROUPS,\n    PARAM_GROUPS,\n    PARAM_SLICE_MAPPINGS,\n    PARAM_SHAPES,\n    PARAM,\n    CAT_DIM,\n    PARAM_N_SUB_PARAMS,\n    SUB_PARAM_SHAPE,\n    VOCAB_TENSOR,\n    UNIVERSAL_CHECKPOINT_INFO,\n    VOCABULARY_PARAMETER_PATTERNS,\n    PIPELINE_REPLICATED_PARAMETER_PATTERNS,\n    TP_REPLICATED_PARAMETER_PATTERNS,\n    PARAMETER_TO_AVERAGE_PATTERNS,\n    PARAMETER_WITH_ROW_PARALLELISM_PATTERNS,\n    PARAMETER_WITH_2_SUB_PARAMS_CAT_DIM_0,\n    PARAMETER_WITH_SUB_PARAMS,\n    SubparamShape,\n)\n\n\ndef parse_arguments():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--input_folder', type=str, required=True, help='Input DeepSpeed Checkpoint folder')\n    parser.add_argument('--output_folder', type=str, required=True, help='Output DeepSpeed checkpoint folder')\n    parser.add_argument('--num_extract_workers',\n                        default=4,\n                        type=int,\n                        help='How many parallel processes to extract zero shards')\n    parser.add_argument(\n        '--num_merge_workers',\n        default=2,\n        type=int,\n        help=\n        'How many parallel processes to merge tp slices (more memory intensive, use much fewer than --num_extract_workers))'\n    )\n    parser.add_argument('--keep_temp_folder',\n                        action='store_true',\n                        help='Preserve temporary folder of intermediate checkpoint slice files. Useful for debugging.')\n    parser.add_argument('--no_strict',\n                        dest='strict',\n                        action='store_false',\n                        help='Do not perform validity checks on converted checkpoint.')\n    args = parser.parse_args()\n    print(f'args = {args}')\n    return args\n\n\ndef _create_checkpoint_paths(base_folder, iteration, tp_degree, pp_degree):\n    path_list = []\n    iter_folder = f'iter_{iteration:07d}'\n    for i in range(0, tp_degree):\n        path_list.append([])\n        for j in range(0, pp_degree):\n            rank_folder = f'mp_rank_{i:02d}' if pp_degree == 1 else f'mp_rank_{i:02d}_{j:03d}'\n            ckpt_path = os.path.join(rank_folder, 'model_optim_rng.pt')\n            path_list[i].append(os.path.join(base_folder, iter_folder, ckpt_path))\n\n    return path_list\n\n\ndef _save_checkpoint(file_path, chkpt_sd):\n    dir, _ = os.path.split(file_path)\n    os.makedirs(dir, exist_ok=True)\n    torch.save(chkpt_sd, file_path)\n\n\ndef extract_zero_shards(dir, ds_checkpoint, indices_3D):\n    pp_index, tp_index, dp_index = indices_3D\n    sd = ds_checkpoint.get_zero_checkpoint_state(pp_index=pp_index, tp_index=tp_index, dp_index=dp_index)\n\n    # pprint(f\"Processing {dp_index=} {pp_index=}, {tp_index=}\")\n\n    optim_sd = sd[OPTIMIZER_STATE_DICT]\n    param_slice_mappings = optim_sd[PARAM_SLICE_MAPPINGS]\n    universal_checkpoint_info = ds_checkpoint.get_checkpoint_info(UNIVERSAL_CHECKPOINT_INFO)\n    pipeline_replicated_params = universal_checkpoint_info.get(PIPELINE_REPLICATED_PARAMETER_PATTERNS, [])\n    # print(f'{pipeline_replicated_params=}')\n\n    # dict\n    state_groups = optim_sd[BASE_OPTIMIZER_STATE][\"state\"]\n    # list\n    fp32_groups = optim_sd[SINGLE_PARTITION_OF_FP32_GROUPS]\n    param_groups_cnt = len(state_groups)\n\n    for param_group_id in range(param_groups_cnt):\n\n        flat_state = dict(\n            exp_avg=state_groups[param_group_id][\"exp_avg\"],\n            exp_avg_sq=state_groups[param_group_id][\"exp_avg_sq\"],\n            fp32=fp32_groups[param_group_id],\n        )\n\n        if \"step\" in state_groups[param_group_id]:\n            flat_state[\"step\"] = state_groups[param_group_id][\"step\"]\n\n        for name, fragment_mapping in param_slice_mappings[param_group_id].items():\n            if pp_index > 0 and any(re.match(pattern, name) for pattern in pipeline_replicated_params):\n                # Skip tied weights that are replicated in first and last pp stages\n                continue\n\n            # pprint(f\"dpt{dp_index}{pp_index}{tp_index} {param_group_id} {name} => {fragment_mapping.start}:{fragment_mapping.numel}\")\n            for state_key in flat_state.keys():\n                dump_param_fragment(dir, tp_index, dp_index, state_key, flat_state[state_key], name,\n                                    fragment_mapping.start, fragment_mapping.numel)\n\n\ncnt = 0\n\n\ndef dp_index_to_str(dp_index):\n    return f\"{dp_index:0>2d}\"\n\n\ndef dump_param_fragment(dir, tp_index, dp_index, state_name, state_flat_tensor, param_name, offset, numel):\n\n    global cnt  # temp hack\n\n    param_base_path = os.path.join(dir, param_name, str(tp_index))\n    os.makedirs(param_base_path, exist_ok=True)\n\n    cnt += 1\n\n    path = os.path.join(param_base_path, f\"{state_name}.{dp_index_to_str(dp_index)}\")\n\n    #print(f\"{param_name}: {offset}: {numel} => {path}\")\n\n    # State might be a python int or a tensor\n    if state_name != \"step\" and torch.is_tensor(state_flat_tensor):\n        state_flat_tensor = state_flat_tensor.narrow(0, offset, numel).clone()\n    _save_checkpoint(path, state_flat_tensor)\n\n\ndef _merge_zero_shards(param_base_path, state, tp_degree, slice_shape):\n    slices = []\n    for tp_index in range(tp_degree):\n        prefix_path = os.path.join(param_base_path, str(tp_index), f\"{state}\")\n        paths = glob.glob(f\"{prefix_path}.*\")\n\n        if len(paths) == 0:\n            continue\n\n        pattern = re.compile(f\"{prefix_path}\\\\.([0-9]+)\")\n        dp_indices = set()\n        for p in paths:\n            m = pattern.match(p)\n            if m:\n                dp_indices.add(int(m.group(1)))\n            else:\n                raise ValueError(f\"Cannot parse dp_rank from {p}\")\n\n        paths = [f\"{prefix_path}.{dp_index_to_str(dp_index)}\" for dp_index in sorted(list(dp_indices))]\n        shards = [torch.load(p) for p in paths]\n\n        if state == \"step\":\n            assert all(v == shards[0] for v in shards), \"All shards must have the same step value\"\n            slice = shards[0]\n        else:\n            slice = torch.cat(shards, dim=0).reshape(slice_shape)\n\n        slices.append(slice)\n    return slices\n\n\ndef merge_tp_slices(ds_checkpoint, dir, slice_dir, tp_degree, name_and_shape):\n\n    name, shape = name_and_shape\n    slice_base_path = os.path.join(slice_dir, name)\n    param_base_path = os.path.join(dir, name)\n\n    universal_checkpoint_info = ds_checkpoint.get_checkpoint_info(UNIVERSAL_CHECKPOINT_INFO)\n    replicated_parameters = universal_checkpoint_info.get(TP_REPLICATED_PARAMETER_PATTERNS, [])\n    parameters_to_average = universal_checkpoint_info.get(PARAMETER_TO_AVERAGE_PATTERNS, [])\n    parameters_with_row_parallelism = universal_checkpoint_info.get(PARAMETER_WITH_ROW_PARALLELISM_PATTERNS, [])\n    vocabulary_parameters = universal_checkpoint_info.get(VOCABULARY_PARAMETER_PATTERNS, [])\n    parameters_with_2_sub_params_cat_dim_0 = universal_checkpoint_info.get(PARAMETER_WITH_2_SUB_PARAMS_CAT_DIM_0, [])\n    parameter_with_sub_params = universal_checkpoint_info.get(PARAMETER_WITH_SUB_PARAMS, [])\n\n    unmatched_patterns = set(replicated_parameters + parameters_to_average + parameters_with_row_parallelism +\n                             vocabulary_parameters + parameters_with_2_sub_params_cat_dim_0)\n    unmatched_patterns.update(chain.from_iterable(SubparamShape(**s).patterns for s in parameter_with_sub_params))\n\n    def get_matched_pattern(patterns_, name_):\n        matched_ = [pattern_ for pattern_ in patterns_ if re.match(pattern_, name_)]\n        assert len(matched_) <= 1, f'Got more than one matching patterns={matched_} for {name_}'\n        if matched_:\n            pattern_ = matched_[0]\n            unmatched_patterns.discard(pattern_)\n            return pattern_\n        return None\n\n    def get_matched_sub_params_pattern(name_):\n        for subparam_shape_dict in parameter_with_sub_params:\n            subparam_shape = SubparamShape(**subparam_shape_dict)\n            for pattern_ in subparam_shape.patterns:\n                if re.match(pattern_, name_):\n                    unmatched_patterns.discard(pattern_)\n                    return subparam_shape\n        return None\n\n    matched_sub_params_shape = get_matched_sub_params_pattern(name)\n\n    step_merged = _merge_zero_shards(slice_base_path, \"step\", tp_degree, shape)\n    if step_merged:\n        _save_checkpoint(os.path.join(param_base_path, f\"step.pt\"), step_merged[0])\n\n    for state in (\"fp32\", \"exp_avg\", \"exp_avg_sq\"):\n        slices = _merge_zero_shards(slice_base_path, state, tp_degree, shape)\n        final_path = os.path.join(param_base_path, f\"{state}.pt\")\n\n        #print(f\"Expected shape: {shape}\")\n        #print(f\"Fragment sizes:\", list(frag.shape for frag in slices))\n        ckpt_dict = {}\n        if get_matched_pattern(replicated_parameters, name):\n            if len(slices) > 1:\n                assert all([slices[0].equal(other_slice) for other_slice in slices[1:]])\n            param = slices[0]\n            # print(f'replicate {name} using first slice')\n        elif get_matched_pattern(parameters_to_average, name):\n            param = sum(slices) / len(slices)\n            # print(f'merge {name} using average')\n        elif get_matched_pattern(parameters_with_2_sub_params_cat_dim_0, name):\n            cat_dim = 0\n            chunked_slices = [torch.chunk(s, 2, dim=cat_dim) for s in slices]\n            merged_chunks_0 = torch.cat([s[0] for s in chunked_slices], dim=cat_dim)\n            merged_chunks_1 = torch.cat([s[1] for s in chunked_slices], dim=cat_dim)\n            param = torch.cat([merged_chunks_0, merged_chunks_1], dim=cat_dim)\n            ckpt_dict[CAT_DIM] = cat_dim\n            ckpt_dict[PARAM_N_SUB_PARAMS] = 2\n        elif matched_sub_params_shape:\n            merged_chunks = []\n            partition_dim = matched_sub_params_shape.partition_dim\n\n            sub_dim_sizes = matched_sub_params_shape.shape[partition_dim]\n            if not isinstance(sub_dim_sizes, tuple):\n                sub_dim_sizes = (sub_dim_sizes, )\n\n            partition_shape = [sum(d) if isinstance(d, tuple) else d for d in matched_sub_params_shape.shape]\n            partition_shape = [d // tp_degree if i == partition_dim else d for i, d in enumerate(partition_shape)]\n            slices = [s.view(partition_shape) for s in slices]\n\n            offset = 0\n            for sub_dim_size in sub_dim_sizes:\n                part_sub_dim_size = sub_dim_size // tp_degree\n                merged_chunks.append(\n                    torch.cat([s.narrow(partition_dim, offset, part_sub_dim_size) for s in slices], dim=partition_dim))\n                offset += part_sub_dim_size\n            param = torch.cat(merged_chunks, dim=partition_dim)\n            ckpt_dict[SUB_PARAM_SHAPE] = matched_sub_params_shape\n        else:\n            cat_dim = 1 if get_matched_pattern(parameters_with_row_parallelism, name) else 0\n            # print(f\"merge {name} with CAT DIM: {cat_dim}\")\n            param = torch.cat(slices, dim=cat_dim)\n            ckpt_dict[CAT_DIM] = cat_dim\n\n        if get_matched_pattern(vocabulary_parameters, name):\n            #print(f\"Before {param.shape=}\")\n            # strip padding\n            original_vocab_size = universal_checkpoint_info['original_vocab_size']\n            param = param[:original_vocab_size, :]\n            ckpt_dict[VOCAB_TENSOR] = True\n            #print(f\"After {param.shape=}\")\n\n        #print(f\"Final shape: {param.shape}\")\n        ckpt_dict[PARAM] = param\n        _save_checkpoint(final_path, ckpt_dict)\n\n    return unmatched_patterns\n\n\ndef _do_parallel_work(do_work, work_chunks, num_workers):\n    results = []\n    if num_workers > 1:\n        with ProcessPoolExecutor(max_workers=num_workers) as executor:\n            future_list = [executor.submit(do_work, work) for work in work_chunks]\n            for f in tqdm.tqdm(future_list):\n                results.append(f.result())\n    else:\n        # No parallel pass for unit testing\n        # We can't create child processes in tests\n        for work in tqdm.tqdm(work_chunks):\n            results.append(do_work(work))\n    return results\n\n\ndef _extract_zero_shard_files(args, ds_checkpoint, temp_dir):\n    _3d_range_list = list(\n        itertools.product(range(ds_checkpoint.pp_degree), range(ds_checkpoint.tp_degree),\n                          range(ds_checkpoint.dp_degree)))\n    #pprint(f'{_3d_range_list=}')\n\n    do_work = partial(extract_zero_shards, temp_dir, ds_checkpoint)\n    _do_parallel_work(do_work, _3d_range_list, args.num_extract_workers)\n\n\ndef _merge_tp_slice_files(args, ds_checkpoint, slice_shapes, temp_dir):\n    zero_output_folder = os.path.join(args.output_folder, \"zero\")\n    do_work = partial(merge_tp_slices, ds_checkpoint, zero_output_folder, temp_dir, ds_checkpoint.tp_degree)\n    unmatched_patterns_lists = _do_parallel_work(do_work, list(slice_shapes.items()), args.num_merge_workers)\n\n    # verify that all patterns were used\n    # if a pattern was not used by any of the workers, then it was not used at all -> assert/alert\n    sets = [set(lst) for lst in unmatched_patterns_lists]\n    unmatched_patterns = list(set.intersection(*sets))\n    if args.strict:\n        assert not unmatched_patterns, f'Unused patterns={unmatched_patterns} while merging tp slices'\n    elif unmatched_patterns:\n        print(f'Warning: Unused patterns={unmatched_patterns} while merging tp slices')\n\n\ndef _save_optimizer_state(args, ds_checkpoint):\n    sharded_states = [BASE_OPTIMIZER_STATE, PARAM_SLICE_MAPPINGS, SINGLE_PARTITION_OF_FP32_GROUPS]\n    sd = ds_checkpoint.get_zero_checkpoint_state(pp_index=0, tp_index=0, dp_index=0)\n\n    optim_sd = sd[OPTIMIZER_STATE_DICT]\n    output_sd = {k: v for k, v in optim_sd.items() if k not in sharded_states}\n    output_sd[PARAM_GROUPS] = optim_sd[BASE_OPTIMIZER_STATE][PARAM_GROUPS]\n    zero_output_folder = os.path.join(args.output_folder, \"zero\")\n    output_file_path = os.path.join(zero_output_folder, f\"optimizer_state.pt\")\n    _save_checkpoint(output_file_path, output_sd)\n\n\ndef _check_for_required_state(ds_checkpoint):\n    universal_checkpoint_info = ds_checkpoint.get_checkpoint_info(UNIVERSAL_CHECKPOINT_INFO)\n    assert universal_checkpoint_info is not None, f'Required {UNIVERSAL_CHECKPOINT_INFO} state is missing in checkpoint. Verify that client creates this state.'\n\n\ndef main(args):\n    print(f'Convert DeepSpeed Checkpoint to Universal Checkpoint')\n\n    print(f'Converting DeepSpeed checkpoint in {args.input_folder} to Universal checkpoint in {args.output_folder}')\n\n    ds_checkpoint = DeepSpeedCheckpoint(args.input_folder)\n    _check_for_required_state(ds_checkpoint)\n\n    iteration = ds_checkpoint.get_iteration()\n    #_create_latest_file(args.output_folder, iteration)\n    checkpoint_paths = _create_checkpoint_paths(args.output_folder, iteration, ds_checkpoint.tp_degree,\n                                                ds_checkpoint.pp_degree)\n\n    slice_shapes = []\n    for mp_rank_file in ds_checkpoint.mp_rank_files:\n        mp_sd = torch.load(mp_rank_file, map_location=torch.device('cpu'))\n        slice_shapes += mp_sd[PARAM_SHAPES]\n\n    # fix back to normal flat dict, merge duplicates for tp>1\n    slice_shapes = dict((k, v) for d in slice_shapes for k, v in d.items())\n    temp_dir = os.path.join(args.output_folder, 'tmp')\n\n    print('*** 1. Extracting ZeRO fragments')\n    _extract_zero_shard_files(args, ds_checkpoint, temp_dir)\n\n    print('*** 2. Merging slices .....')\n    _merge_tp_slice_files(args, ds_checkpoint, slice_shapes, temp_dir)\n\n    print('*** 3. Saving common optimizer states')\n    _save_optimizer_state(args, ds_checkpoint)\n\n    if not args.keep_temp_folder:\n        shutil.rmtree(temp_dir, ignore_errors=True)\n\n    # Copy mp* files into output folder\n    for f in glob.glob(os.path.join(args.input_folder, 'mp*')):\n        shutil.copy2(f, args.output_folder)\n\n    # Update latest to output folder\n    checkpoint_root_folder, step_folder = os.path.split(args.output_folder)\n    latest_file = os.path.join(checkpoint_root_folder, 'latest_universal')\n    with open(latest_file, \"w\") as f:\n        f.write(step_folder)\n\n    print('*** Done!')\n\n\nif __name__ == \"__main__\":\n    args = parse_arguments()\n    main(args)\n", "deepspeed/checkpoint/reshape_3d_utils.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .reshape_utils import (get_files, get_files_with_prefix, partition_data, get_zero_files)\n\nfrom .constants import (MODEL_FILE_PREFIX, LAYER_FILE_PREFIX)\n\nfrom .reshape_meg_2d import (reshape_meg_2d_parallel, meg_2d_parallel_map)\n\nPP_DIM = 'PP'\nTP_DIM = 'TP'\nDP_DIM = 'DP'\n\n\nclass model_3d_desc(object):\n\n    def __init__(self, pp_degree=1, tp_degree=1, dp_degree=1):\n        self.pp_degree = pp_degree\n        self.tp_degree = tp_degree\n        self.dp_degree = dp_degree\n\n    def reshape(self, target_3d_desc, verbose=False):\n        valid_reshape, reshape_errors = self.can_reshape(target_3d_desc)\n        assert valid_reshape, ','.join(reshape_errors)\n        tgt_2d_map = reshape_meg_2d_parallel(old_pp_degree=self.pp_degree,\n                                             old_tp_degree=self.tp_degree,\n                                             new_pp_degree=target_3d_desc.pp_degree,\n                                             new_tp_degree=target_3d_desc.tp_degree,\n                                             verbose=verbose)\n\n        flat_3d_map = flatten_dp_dimension(meg_2d_map=tgt_2d_map,\n                                           src_2d_size=self.pp_degree * self.tp_degree,\n                                           dp_degree=self.dp_degree)\n\n        return unflatten_dp_dimension(meg_2d_map=flat_3d_map, dp_degree=target_3d_desc.dp_degree)\n\n    def get_desc(self):\n        return f'{PP_DIM},{TP_DIM},{DP_DIM} = ({self.pp_degree}, {self.tp_degree}, {self.dp_degree})'\n\n    def world_size(self):\n        return self.pp_degree * self.tp_degree * self.dp_degree\n\n    def is_valid(self, pp_index, tp_index, dp_index):\n        err_msg = []\n        valid = True\n        for index, degree, dim_name in [(pp_index, self.pp_degree, PP_DIM), (tp_index, self.tp_degree, TP_DIM),\n                                        (dp_index, self.dp_degree, DP_DIM)]:\n            if index >= degree:\n                valid = False\n                err_msg.append(f'{dim_name} indexing error: index {index} >= degree {degree}')\n\n        return valid, err_msg\n\n    def can_reshape(self, target_3d_desc):\n        err_msg = []\n        if target_3d_desc.pp_degree > self.pp_degree:\n            err_msg.append(\n                f'Expansion reshape not supported - {PP_DIM}: {self.pp_degree} ---> {target_3d_desc.pp_degree}')\n\n        if target_3d_desc.tp_degree > self.tp_degree:\n            err_msg.append(\n                f'Expansion reshape not supported - {TP_DIM}: {self.tp_degree} ---> {target_3d_desc.tp_degree}')\n\n        if target_3d_desc.dp_degree > self.dp_degree:\n            err_msg.append(\n                f'Expansion reshape not supported - {DP_DIM}: {self.dp_degree} ---> {target_3d_desc.dp_degree}')\n\n        return len(err_msg) == 0, err_msg\n\n\ndef get_model_3d_descriptor(dir):\n    file_list = get_files(dir)\n    zero_file_list = get_zero_files(dir)\n    num_pp0_files = len(get_files_with_prefix(file_list, f'{LAYER_FILE_PREFIX}01'))\n    if num_pp0_files > 0:\n        tp_degree = num_pp0_files\n        pp_degree = len(get_files_with_prefix(file_list, MODEL_FILE_PREFIX)) // tp_degree\n        dp_degree = max(1, len(zero_file_list) // (pp_degree * tp_degree))\n    else:\n        tp_degree = len(get_files_with_prefix(file_list, MODEL_FILE_PREFIX))\n        dp_degree = max(1, len(zero_file_list) // tp_degree)\n        pp_degree = 1\n\n    return model_3d_desc(pp_degree, tp_degree, dp_degree)\n\n\ndef flatten_dp_dimension(meg_2d_map, src_2d_size, dp_degree):\n    new_meg_2d_map = meg_2d_parallel_map(meg_2d_map.pp_degree, meg_2d_map.tp_degree)\n    for pp_index in range(meg_2d_map.pp_degree):\n        for tp_index in range(meg_2d_map.tp_degree):\n            dp0_indices = meg_2d_map.get_data(pp_index, tp_index)\n            for idx in dp0_indices:\n                dpX_indices = [idx + (i * src_2d_size) for i in range(dp_degree)]\n                new_meg_2d_map.add_data(pp_index, tp_index, dpX_indices)\n    return new_meg_2d_map\n\n\ndef unflatten_dp_dimension(meg_2d_map, dp_degree):\n    pp_degree = meg_2d_map.pp_degree\n    tp_degree = meg_2d_map.tp_degree\n    meg_2d_map_list = [meg_2d_parallel_map(pp_degree=pp_degree, tp_degree=tp_degree) for _ in range(dp_degree)]\n    for pp_index in range(pp_degree):\n        for tp_index in range(tp_degree):\n            flat_dp_indices = meg_2d_map.get_data(pp_index, tp_index)\n            partitioned_dp_indices = partition_data(flat_dp_indices, dp_degree)\n            for dp_indices, _2d_map in zip(partitioned_dp_indices, meg_2d_map_list):\n                _2d_map.add_data(pp_index, tp_index, dp_indices)\n\n    return meg_2d_map_list\n", "deepspeed/elasticity/config.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport json\nfrom .constants import *\n\n\nclass ElasticityError(Exception):\n    \"\"\"\n    Base exception for all elasticity related errors\n    \"\"\"\n\n\nclass ElasticityConfigError(ElasticityError):\n    \"\"\"\n    Elasticity configuration error\n    \"\"\"\n\n\nclass ElasticityIncompatibleWorldSize(ElasticityError):\n    \"\"\"\n    Attempting to run a world size that is incompatible with a given elastic config\n    \"\"\"\n\n\nclass ElasticityConfig:\n    \"\"\"\n    Elastic config object, constructed from a param dictionary that only contains elastic\n    config parameters, example below:\n\n    If elasticity is enabled, user must specify (at least) max_train_batch_size\n    and micro_batch_sizes.\n\n    {\n        \"enabled\": true,\n        \"max_train_batch_size\": 2000,\n        \"micro_batch_sizes\": [2,4,6],\n        \"min_gpus\": 1,\n        \"max_gpus\" : 10000\n        \"min_time\": 20\n        \"ignore_non_elastic_batch_info\": false\n        \"version\": 0.1\n    }\n    \"\"\"\n\n    def __init__(self, param_dict):\n        self.enabled = param_dict.get(ENABLED, ENABLED_DEFAULT)\n        if self.enabled:\n            if MAX_ACCEPTABLE_BATCH_SIZE in param_dict:\n                self.max_acceptable_batch_size = param_dict[MAX_ACCEPTABLE_BATCH_SIZE]\n            else:\n                raise ElasticityConfigError(f\"Elasticity config missing {MAX_ACCEPTABLE_BATCH_SIZE}\")\n            if MICRO_BATCHES in param_dict:\n                self.micro_batches = param_dict[MICRO_BATCHES]\n            else:\n                raise ElasticityConfigError(f\"Elasticity config missing {MICRO_BATCHES}\")\n        else:\n            self.max_acceptable_batch_size = param_dict.get(MAX_ACCEPTABLE_BATCH_SIZE,\n                                                            MAX_ACCEPTABLE_BATCH_SIZE_DEFAULT)\n            self.micro_batches = param_dict.get(MICRO_BATCHES, MICRO_BATCHES_DEFAULT)\n\n        if not isinstance(self.micro_batches, list):\n            raise ElasticityConfigError(\n                f\"Elasticity expected value of {MICRO_BATCHES} to be a \"\n                f\"list of micro batches, instead is: {type(self.micro_batches)}, containing: {self.micro_batches}\")\n\n        if not all(map(lambda m: isinstance(m, int), self.micro_batches)):\n            raise ElasticityConfigError(f\"Elasticity expected {MICRO_BATCHES} to only contain a list of integers, \"\n                                        f\"instead contains: f{self.micro_batches}\")\n\n        if not all(map(lambda m: m > 0, self.micro_batches)):\n            raise ElasticityConfigError(f\"Elasticity expected {MICRO_BATCHES} to only contain positive integers, \"\n                                        f\"instead contains: f{self.micro_batches}\")\n\n        self.min_gpus = param_dict.get(MIN_GPUS, MIN_GPUS_DEFAULT)\n        self.max_gpus = param_dict.get(MAX_GPUS, MAX_GPUS_DEFAULT)\n\n        if self.min_gpus < 1 or self.max_gpus < 1:\n            raise ElasticityConfigError(\"Elasticity min/max gpus must be > 0, \"\n                                        f\"given min_gpus: {self.min_gpus}, max_gpus: {self.max_gpus}\")\n        if self.max_gpus < self.min_gpus:\n            raise ElasticityConfigError(\"Elasticity min_gpus cannot be greater than max_gpus, \"\n                                        f\"given min_gpus: {self.min_gpus}, max_gpus: {self.max_gpus}\")\n\n        self.model_parallel_size = param_dict.get(MODEL_PARALLEL_SIZE, MODEL_PARALLEL_SIZE_DEFAULT)\n        if self.model_parallel_size < 1:\n            raise ElasticityConfigError(\"Model-Parallel size cannot be less than 1, \"\n                                        f\"given model-parallel size: {self.model_parallel_size}\")\n\n        self.num_gpus_per_node = param_dict.get(NUM_GPUS_PER_NODE, NUM_GPUS_PER_NODE_DEFAULT)\n        if self.num_gpus_per_node < 1:\n            raise ElasticityConfigError(\"Number of GPUs per node cannot be less than 1, \"\n                                        f\"given number of GPUs per node: {self.num_gpus_per_node}\")\n\n        self.min_time = param_dict.get(MIN_TIME, MIN_TIME_DEFAULT)\n        if self.min_time < 0:\n            raise ElasticityConfigError(f\"Elasticity min time needs to be >= 0: given {self.min_time}\")\n\n        self.version = param_dict.get(VERSION, VERSION_DEFAULT)\n        self.prefer_larger_batch_size = param_dict.get(PREFER_LARGER_BATCH, PREFER_LARGER_BATCH_DEFAULT)\n        self.ignore_non_elastic_batch_info = param_dict.get(IGNORE_NON_ELASTIC_BATCH_INFO,\n                                                            IGNORE_NON_ELASTIC_BATCH_INFO_DEFAULT)\n\n    def repr(self):\n        return self.__dict__\n\n    def __repr__(self):\n        return json.dumps(self.__dict__, sort_keys=True, indent=4)\n", "deepspeed/elasticity/elasticity.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport os\nimport json\nimport numpy as np\nimport math\nfrom packaging import version as pkg_version\n\nfrom .config import ElasticityConfig, ElasticityConfigError, ElasticityError, \\\n    ElasticityIncompatibleWorldSize\nfrom .constants import ELASTICITY, ENABLED, ENABLED_DEFAULT, LATEST_ELASTICITY_VERSION, \\\n    MINIMUM_DEEPSPEED_VERSION, DEEPSPEED_ELASTICITY_CONFIG\nfrom ..git_version_info import version as __version__\nfrom ..utils import logger\n\n# Thirty eight smallest highly composite numbers. The list should\n# be enough to support up to 720K batch size.\nHCN_LIST = [\n    1, 2, 4, 6, 12, 24, 36, 48, 60, 120, 180, 240, 360, 720, 840, 1260, 1680, 2520, 5040, 7560, 10080, 15120, 20160,\n    25200, 27720, 45360, 50400, 55440, 83160, 110880, 166320, 221760, 277200, 332640, 498960, 554400, 665280, 720720\n]\n\n\ndef get_candidate_batch_sizes(base_list, max_acceptable_batch_size):\n    candidate_batch_size = []\n    for base in base_list:\n        if base >= max_acceptable_batch_size:\n            candidate_batch_size.append(base)\n        else:\n            value = max_acceptable_batch_size // base\n            index = np.argmax(np.asarray(HCN_LIST) > value)\n            candidate_batch_size.append(HCN_LIST[index - 1] * base)\n    candidate_batch_size = list(set(candidate_batch_size))\n    logger.info(f\"Candidate batch size: {candidate_batch_size}\")\n    return candidate_batch_size\n\n\ndef get_valid_gpus(batch_size, micro_batches, min_valid_gpus, max_valid_gpus):\n    valid_gpus = []\n    for micro_batch in micro_batches:\n        if batch_size % micro_batch == 0:\n\n            max_gpus = batch_size // micro_batch\n            if min_valid_gpus <= max_gpus <= max_valid_gpus:\n                valid_gpus.append(max_gpus)\n\n            # find all factors less than max_gpus / 2\n            for i in range(1, max_gpus // 2 + 1):\n                if i > max_valid_gpus:\n                    break\n                if i < min_valid_gpus:\n                    continue\n                if max_gpus % i == 0:\n                    valid_gpus.append(i)\n    valid_gpus = set(valid_gpus)\n    valid_gpus = sorted(list(valid_gpus))\n    return valid_gpus\n\n\ndef get_best_candidates(candidate_batch_sizes, micro_batches, min_gpus, max_gpus, prefer_larger):\n\n    max_valid_gpus = 0\n    valid_gpus = None\n    final_batch_size = int(min(micro_batches))\n\n    for batch_size in candidate_batch_sizes:\n\n        current_valid_gpus = get_valid_gpus(batch_size, micro_batches, min_gpus, max_gpus)\n\n        if (len(current_valid_gpus) > max_valid_gpus or (len(current_valid_gpus) == max_valid_gpus and\n                                                         ((prefer_larger and batch_size > final_batch_size) or\n                                                          (not prefer_larger and batch_size < final_batch_size)))):\n            max_valid_gpus = len(current_valid_gpus)\n            valid_gpus = current_valid_gpus\n            final_batch_size = batch_size\n\n    return final_batch_size, valid_gpus\n\n\ndef _get_compatible_gpus_v01(micro_batches,\n                             max_acceptable_batch_size,\n                             min_gpus=None,\n                             max_gpus=None,\n                             prefer_larger=True):\n    '''We use two heuristics to compute the batch size\n        1. We use the Lowest Common Multiple of the micro-batches\n    as the base batch size and scale it by a HCN such that the result is\n    the largest batch size less than the max_acceptable batch size\n        2. We use each of the micro batches as a base and scale it\n    by a HCN such that the result is the largest batch size less than the\n    max_acceptable batch size.\n\n    We then use brute force to count the number of compatible GPU count for\n    each of the aforementioned cases, and return the batch size with the most number of\n    compatible GPU counts in the min-max GPU range if provided, other wise\n    we return the batch size with the most number of total compatible GPU counts.\n\n    Returns:\n        final_batch_size\n        valid_gpus\n    '''\n    min_gpus = min_gpus or 1\n    max_gpus = max_gpus or max_acceptable_batch_size // min(micro_batches)\n\n    if not all(mb <= max_acceptable_batch_size for mb in micro_batches):\n        raise ValueError(f\"All micro batches must be less than \\\n            or equal to max_acceptable_batch_size: {max_acceptable_batch_size}\")\n\n    lcm = np.lcm.reduce(micro_batches)\n\n    base_list = []\n    base_list.extend(micro_batches)\n    base_list.append(lcm)\n\n    candidate_batch_sizes = get_candidate_batch_sizes(base_list, max_acceptable_batch_size)\n\n    final_batch_size, valid_gpus = get_best_candidates(candidate_batch_sizes, micro_batches, min_gpus, max_gpus,\n                                                       prefer_larger)\n\n    return final_batch_size, valid_gpus\n\n\ndef _get_compatible_gpus_v02(micro_batches,\n                             max_acceptable_batch_size,\n                             current_num_gpus,\n                             min_gpus=None,\n                             max_gpus=None,\n                             prefer_larger=True,\n                             num_gpus_per_node=1,\n                             model_parallel_size=1):\n    '''\n    Returns:\n        final_batch_size\n        valid_gpus\n        micro-batch size\n    '''\n    if num_gpus_per_node % model_parallel_size != 0:\n        raise ElasticityError(\n            f\"In Elasticity v0.2, number of GPUs per node:\" \\\n            f\"{num_gpus_per_node} should be divisible by \" \\\n            f\"model parallel size {model_parallel_size}\")\n\n    def get_microbatch(final_batch_size):\n        candidate_microbatch = None\n\n        for micro_batch in micro_batches:\n            if final_batch_size // current_num_gpus % micro_batch == 0:\n                if candidate_microbatch is None:\n                    candidate_microbatch = micro_batch\n                if prefer_larger and candidate_microbatch < micro_batch:\n                    candidate_microbatch = micro_batch\n        return candidate_microbatch\n\n    dp_size_per_node = num_gpus_per_node // model_parallel_size\n\n    final_batch_size, valid_world_size = _get_compatible_gpus_v01(\n        micro_batches,\n        int(max_acceptable_batch_size / dp_size_per_node),\n        int(min_gpus / num_gpus_per_node),\n        int(max_gpus / num_gpus_per_node),  # Passing number of max nodes as Elasticity v2 works at node level\n        prefer_larger=prefer_larger)\n\n    final_batch_size = int(final_batch_size) * dp_size_per_node\n    valid_dp_world_size = [i * dp_size_per_node for i in valid_world_size]\n    if current_num_gpus // model_parallel_size in valid_dp_world_size:\n        candidate_microbatch = get_microbatch(final_batch_size)\n        return final_batch_size, valid_dp_world_size, candidate_microbatch\n\n    current_dp_size = (current_num_gpus / num_gpus_per_node) * dp_size_per_node\n    candidate_batch_sizes = []\n    for micro_batch in micro_batches:\n        min_batch_size = micro_batch * current_dp_size\n\n        factor = math.floor(max_acceptable_batch_size / float(min_batch_size))\n        candidate_batch_sizes.append(factor * min_batch_size)\n\n    used_microbatch = None\n    if prefer_larger:\n        candidate_batch_size = max(candidate_batch_sizes)\n    else:\n        candidate_batch_size = min(candidate_batch_sizes)\n\n    candidate_microbatch = get_microbatch(candidate_batch_size)\n\n    return candidate_batch_size, [int(current_dp_size)], candidate_microbatch\n\n\ndef _compatible_ds_version_check(target_deepspeed_version: str):\n    min_version = pkg_version.parse(MINIMUM_DEEPSPEED_VERSION)\n    target_version = pkg_version.parse(target_deepspeed_version)\n\n    err_str = f\"Target deepspeed version of {target_deepspeed_version} is not compatible \" \\\n        f\"with minimum version {MINIMUM_DEEPSPEED_VERSION} supporting elasticity.\"\n    if target_version < min_version:\n        raise ElasticityError(err_str)\n    return True\n\n\ndef elasticity_enabled(ds_config: dict):\n    if ELASTICITY not in ds_config:\n        return False\n    return ds_config[ELASTICITY].get(ENABLED, ENABLED_DEFAULT)\n\n\ndef ensure_immutable_elastic_config(runtime_elastic_config_dict: dict):\n    \"\"\"\n    Ensure the resource scheduler saw the same elastic config we are using at runtime\n    \"\"\"\n    if DEEPSPEED_ELASTICITY_CONFIG in os.environ:\n        scheduler_elastic_config_dict = json.loads(os.environ[DEEPSPEED_ELASTICITY_CONFIG])\n        scheduler_elastic_config = ElasticityConfig(scheduler_elastic_config_dict)\n        runtime_elastic_config = ElasticityConfig(runtime_elastic_config_dict)\n        err_str = \"Elastic config '{}={}' seen by resource scheduler does not match config passed to runtime {}={}\"\n        if runtime_elastic_config.max_acceptable_batch_size != scheduler_elastic_config.max_acceptable_batch_size:\n            raise ElasticityConfigError(\n                err_str.format('max_acceptable_batch_size', scheduler_elastic_config.max_acceptable_batch_size,\n                               'max_acceptable_batch_size', runtime_elastic_config.max_acceptable_batch_size))\n        if runtime_elastic_config.micro_batches != scheduler_elastic_config.micro_batches:\n            raise ElasticityConfigError(\n                err_str.format('micro_batches', scheduler_elastic_config.micro_batches, 'micro_batches',\n                               runtime_elastic_config.micro_batches))\n        if runtime_elastic_config.version != scheduler_elastic_config.version:\n            raise ElasticityConfigError(\n                err_str.format('version', scheduler_elastic_config.version, 'version', runtime_elastic_config.version))\n    else:\n        logger.warning(\"Unable to find DEEPSPEED_ELASTICITY_CONFIG environment variable, cannot \" \\\n            \"guarantee resource scheduler will scale this job using compatible GPU counts.\")\n\n\ndef compute_elastic_config(ds_config: dict, target_deepspeed_version: str, world_size=0, return_microbatch=False):\n    \"\"\"Core deepspeed elasticity API. Given an elastic config (similar to the example below)\n    DeepSpeed will compute a total train batch size corresponding valid GPU count list that\n    provides a high level of elasticity. Elasticity in this case means we are safe to scale\n    the training job up/down across the GPU count list *without* any negative impacts on\n    training convergence. This is achievable primarily due to DeepSpeed's gradient accumulation\n    feature which allows us to decompose a global training batch size into:\n    micro-batch-size * gradient-accumulation-steps * world-size.\n\n    \"elasticity\": {\n        \"enabled\": true,\n        \"max_train_batch_size\": 2000,\n        \"micro_batch_sizes\": [2,4,6],\n        \"min_gpus\": 1,\n        \"max_gpus\" : 10000\n        \"min_time\": 20\n        \"version\": 0.1\n    }\n\n    Intended to be called both by scheduling infrastructure and deepspeed runtime.\n    For the same `ds_config` we should return deterministic results.\n\n    Args:\n        ds_config (dict): DeepSpeed config dictionary/json\n        target_deepspeed_version (str): When called from scheduling\n            infrastructure we want to ensure that the target deepspeed version is\n            compatible with the elasticity version used in the backend.\n        world_size (int, optional): Intended/current DP world size, will do some sanity\n            checks to ensure world size is actually valid with the config.\n        return_microbatch (bool, optional): whether to return micro batch size or not.\n\n    Raises:\n        ElasticityConfigError: Missing required elasticity config or elasticity disabled\n        ElasticityError: If target deepspeed version is not compatible with current version\n\n    Returns:\n        final_batch_size (int): total batch size used for training\n        valid_gpus (list(int)): list of valid GPU counts with this config\n        micro_batch_size (int, optional): if world_size is provided will return\n            specific micro batch size\n    \"\"\"\n    if not isinstance(ds_config, dict):\n        raise ValueError(\"Expected ds_config to be a dictionary but received \" \\\n            f\"a {type(ds_config)}, containing: {ds_config}\")\n\n    if ELASTICITY not in ds_config:\n        raise ElasticityConfigError(f\"'{ELASTICITY}' is missing from config json,\" \\\n            \" please add it if running an elastic training job.\")\n\n    elastic_config_dict = ds_config[ELASTICITY]\n    if not elastic_config_dict.get(ENABLED, ENABLED_DEFAULT):\n        raise ElasticityConfigError(\"Elasticity is disabled, please enable it \" \\\n            \"('enabled':true) if running an elastic training job.\")\n\n    elastic_config = ElasticityConfig(elastic_config_dict)\n    model_parallel_size = elastic_config.model_parallel_size\n    num_gpus_per_node = elastic_config.num_gpus_per_node\n\n    if model_parallel_size > 1 and float(elastic_config.version) != 0.2:\n        raise ElasticityConfigError(f\"Elasticity V{elastic_config.version} \" \\\n            f\"does not support model-parallel training. Given model-parallel size: \" \\\n            f\"{model_parallel_size}\")\n\n    if float(elastic_config.version) > LATEST_ELASTICITY_VERSION:\n        raise ElasticityConfigError(\"Attempting to run elasticity version \" \\\n            f\"{elastic_config.version} but runtime only supports up \" \\\n            f\"to {LATEST_ELASTICITY_VERSION}\")\n\n    # Ensure target deepspeed version works with intended elasticity version\n    if not _compatible_ds_version_check(target_deepspeed_version):\n        raise ElasticityError(\"Unable to run elasticity on target deepspeed version of\" \\\n            f\" {target_deepspeed_version}, currently {__version__}\")\n\n    if float(elastic_config.version) == 0.1:\n        final_batch_size, valid_gpus = _get_compatible_gpus_v01(\n            micro_batches=elastic_config.micro_batches,\n            max_acceptable_batch_size=elastic_config.max_acceptable_batch_size,\n            min_gpus=elastic_config.min_gpus,\n            max_gpus=elastic_config.max_gpus,\n            prefer_larger=elastic_config.prefer_larger_batch_size)\n        # ensure batch size is int dtype\n        final_batch_size = int(final_batch_size)\n    elif float(elastic_config.version) == 0.2:\n        if world_size != 0:\n            current_num_gpus = world_size\n        else:\n            if \"WORLD_SIZE\" in os.environ and \\\n                os.getenv('WORLD_SIZE').isnumeric():\n                current_num_gpus = int(os.getenv('WORLD_SIZE'))\n            else:\n                WORLD_SIZE = os.getenv('WORLD_SIZE')\n                raise ElasticityConfigError(\n                    'Elasticity V 0.2 needs WORLD_SIZE '\\\n                    'to compute valid batch size. '\\\n                    'Either give it as argument to function compute_elastic_config '\\\n                    'or set it as an environment variable. '\\\n                    f'Value of WORLD_SIZE as environment variable is {WORLD_SIZE}')\n\n        final_batch_size, valid_gpus, candidate_microbatch_size = _get_compatible_gpus_v02(\n            micro_batches=elastic_config.micro_batches,\n            max_acceptable_batch_size=elastic_config.max_acceptable_batch_size,\n            current_num_gpus=current_num_gpus,\n            min_gpus=elastic_config.min_gpus,\n            max_gpus=elastic_config.max_gpus,\n            prefer_larger=elastic_config.prefer_larger_batch_size,\n            num_gpus_per_node=num_gpus_per_node,\n            model_parallel_size=model_parallel_size)\n        # ensure batch size is int dtype\n        final_batch_size = int(final_batch_size)\n    else:\n        raise NotImplementedError(f\"Unable to find elastic logic for version: {elastic_config.version}\")\n\n    logger.info(f\"Valid World Size (GPUs / Model Parallel Size): {valid_gpus}\")\n\n    if world_size > 0:\n        if world_size not in valid_gpus:\n            raise ElasticityIncompatibleWorldSize(f\"World size ({world_size}) is not valid \" \\\n        f\"with the current list of valid GPU counts: {valid_gpus}\")\n\n        # Pick largest valid micro batch size\n        micro_batch_size = None\n        for mbsz in sorted(list(set(elastic_config.micro_batches)), reverse=True):\n            if final_batch_size // world_size % mbsz == 0:\n                micro_batch_size = mbsz\n                break\n        assert micro_batch_size is not None, \"Unable to find divisible micro batch size\" \\\n            f\" world_size={world_size}, final_batch_size={final_batch_size}, and \" \\\n            f\" micro_batches={elastic_config.micro_batches}.\"\n        return final_batch_size, valid_gpus, micro_batch_size\n\n    if return_microbatch:\n        # Pick a valid micro batch size\n        if float(elastic_config.version) == 0.2:\n            return final_batch_size, valid_gpus, candidate_microbatch_size\n        else:\n            micro_batch_size = None\n            for mbsz in sorted(list(set(elastic_config.micro_batches)), reverse=True):\n                if final_batch_size // world_size % mbsz == 0:\n                    micro_batch_size = mbsz\n                    break\n            assert micro_batch_size is not None, \"Unable to find divisible micro batch size\" \\\n                    f\" world_size={world_size}, final_batch_size={final_batch_size}, and \" \\\n                    f\" micro_batches={elastic_config.micro_batches}.\"\n            return final_batch_size, valid_gpus, micro_batch_size\n\n    return final_batch_size, valid_gpus\n", "deepspeed/elasticity/utils.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom deepspeed.utils.torch import required_torch_version\n\n\ndef is_torch_elastic_compatible():\n    '''\n        Helper to lookup torch version. Elastic training is\n        introduced in 1.11.x\n    '''\n    return required_torch_version(min_version=1.11)\n", "deepspeed/elasticity/constants.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\n#########################################\n# Elasticity\n#########################################\n''' Elasticity Utility in DeepSpeed can be used to create highly elastic jobs compatible\nwith a large number of GPUs. For elastic jobs, DeepSpeed will provide a batch size that\ncan support a large number of GPUs based on the user specified parameters\n'''\nFORMAT = '''\nElasticity should be enabled as:\n\"elasticity\": {\n  \"enabled\": true,\n  \"max_train_batch_size\": 2000,\n  \"micro_batch_sizes\": [2,4,6],\n  \"min_gpus\": 1,\n  \"max_gpus\" : 10000,\n  \"min_time\": 20,\n  \"prefer_larger_batch\": true,\n  \"ignore_non_elastic_batch_info\": false,\n  \"version\": 0.1\n}\n'''\n\nELASTICITY = 'elasticity'\n\n# Current elasticity version\nLATEST_ELASTICITY_VERSION = 0.2\n\nENABLED = 'enabled'\nENABLED_DEFAULT = False\n\n# Max acceptable train_batch_size\nMAX_ACCEPTABLE_BATCH_SIZE = 'max_train_batch_size'\nMAX_ACCEPTABLE_BATCH_SIZE_DEFAULT = 2000\n\n# Acceptable micro batch sizes, same as train_micro_batch_size_per_gpu\nMICRO_BATCHES = 'micro_batch_sizes'\nMICRO_BATCHES_DEFAULT = [2, 4, 6]\n\n# Min/max of GPUs to search over\nMIN_GPUS = 'min_gpus'\nMIN_GPUS_DEFAULT = 1\nMAX_GPUS = 'max_gpus'\nMAX_GPUS_DEFAULT = 10000\n\nNUM_GPUS_PER_NODE = 'num_gpus_per_node'\nNUM_GPUS_PER_NODE_DEFAULT = 1\n\nMODEL_PARALLEL_SIZE = \"model_parallel_size\"\nMODEL_PARALLEL_SIZE_DEFAULT = 1\n\n# Minimum running time (minutes) before the scheduler will scale us, 0 implies it's unknown\nMIN_TIME = \"min_time\"\nMIN_TIME_DEFAULT = 0\n\n# When finding a suitable batch size, attempt to find one that is closest\n# to the max train batch size given.\nPREFER_LARGER_BATCH = 'prefer_larger_batch'\nPREFER_LARGER_BATCH_DEFAULT = True\n\n# In order to reduce confusion, if elastic mode is enabled we\n# require (via assert) that no batch info is set outside of the\n# elastic config. You can turn off this assert via this config\n# but keep in mind that all batch info defined outside the\n# elastic mode *will be ignored*.\nIGNORE_NON_ELASTIC_BATCH_INFO = 'ignore_non_elastic_batch_info'\nIGNORE_NON_ELASTIC_BATCH_INFO_DEFAULT = False\n\n# Version of elastic logic to use\nVERSION = \"version\"\nVERSION_DEFAULT = LATEST_ELASTICITY_VERSION\n\n# Minimum deepspeed version to use elasticity\nMINIMUM_DEEPSPEED_VERSION = \"0.3.8\"\n\n# Environment variable storing elastic config from resource scheduler\nDEEPSPEED_ELASTICITY_CONFIG = \"DEEPSPEED_ELASTICITY_CONFIG\"\n", "deepspeed/elasticity/elastic_agent.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom torch.distributed.elastic.agent.server.local_elastic_agent import LocalElasticAgent\nfrom typing import Any, Dict, Optional, Tuple\nfrom datetime import datetime\nfrom torch.distributed.elastic.utils.distributed import get_free_port\nfrom torch.distributed.elastic.metrics import put_metric\nfrom torch.distributed.elastic.agent.server.api import (\n    RunResult,\n    WorkerGroup,\n    WorkerSpec,\n    WorkerState,\n)\nfrom torch.distributed import Store\nimport time\nimport os\nfrom torch.distributed.elastic.multiprocessing import start_processes\nfrom torch.distributed.elastic.utils import macros\nimport shutil\nimport copy\nfrom contextlib import closing\nimport subprocess\n\nfrom torch.distributed.elastic.utils.logging import get_logger\n\nlog = get_logger(__name__)\n\n\nclass DSElasticAgent(LocalElasticAgent):\n\n    def __init__(\n        self,\n        spec: WorkerSpec,\n        env: Dict,\n        start_method=\"spawn\",\n        exit_barrier_timeout: float = 300,\n        log_dir: Optional[str] = None,\n    ):\n        super().__init__(spec, start_method, exit_barrier_timeout, log_dir)\n        self.ds_env = env\n\n    @staticmethod\n    def _set_master_addr_port(store: Store,\n                              master_addr: Optional[str],\n                              master_port: Optional[int],\n                              local_addr: Optional[str] = None):\n        if master_port is None:\n            sock = get_free_port()\n            with closing(sock):\n                master_port = sock.getsockname()[1]\n\n        if master_addr is None:\n            # master_addr = _get_fq_hostname()\n            result = subprocess.check_output(\"hostname -I\", shell=True)\n            master_addr = result.decode('utf-8').split()[0]\n\n        store.set(\"MASTER_ADDR\", master_addr.encode(encoding=\"UTF-8\"))\n        store.set(\"MASTER_PORT\", str(master_port).encode(encoding=\"UTF-8\"))\n\n    def _start_workers(self, worker_group: WorkerGroup) -> Dict[int, Any]:\n        spec = worker_group.spec\n        store = worker_group.store\n        assert store is not None\n        master_addr, master_port = super()._get_master_addr_port(store)\n        restart_count = spec.max_restarts - self._remaining_restarts\n\n        use_agent_store = spec.rdzv_handler.get_backend() == \"static\"\n\n        args: Dict[int, Tuple] = {}\n        envs: Dict[int, Dict[str, str]] = {}\n        for worker in worker_group.workers:\n            local_rank = worker.local_rank\n\n            worker_env_ds = copy.deepcopy(self.ds_env)\n            worker_env_elastic = {\n                \"LOCAL_RANK\": str(local_rank),\n                \"RANK\": str(worker.global_rank),\n                \"GROUP_RANK\": str(worker_group.group_rank),\n                \"ROLE_RANK\": str(worker.role_rank),\n                \"ROLE_NAME\": spec.role,\n                \"LOCAL_WORLD_SIZE\": str(spec.local_world_size),\n                \"WORLD_SIZE\": str(worker.world_size),\n                \"GROUP_WORLD_SIZE\": str(worker_group.group_world_size),\n                \"ROLE_WORLD_SIZE\": str(worker.role_world_size),\n                \"MASTER_ADDR\": master_addr,\n                \"MASTER_PORT\": str(master_port),\n                \"TORCHELASTIC_RESTART_COUNT\": str(restart_count),\n                \"TORCHELASTIC_MAX_RESTARTS\": str(spec.max_restarts),\n                \"TORCHELASTIC_RUN_ID\": spec.rdzv_handler.get_run_id(),\n                \"TORCHELASTIC_USE_AGENT_STORE\": str(use_agent_store),\n                \"NCCL_ASYNC_ERROR_HANDLING\": os.getenv(\"NCCL_ASYNC_ERROR_HANDLING\", str(1)),\n            }\n            worker_env_ds.update(worker_env_elastic)\n            if \"OMP_NUM_THREADS\" in os.environ:\n                worker_env_ds[\"OMP_NUM_THREADS\"] = os.environ[\"OMP_NUM_THREADS\"]\n\n            envs[local_rank] = worker_env_ds\n            worker_args = list(spec.args)\n            worker_args = macros.substitute(worker_args, str(local_rank))\n            args[local_rank] = tuple(worker_args)\n\n        # scaling events do not count towards restarts (gets same attempt #)\n        # remove existing log dir if this restart is due to a scaling event\n        attempt_log_dir = os.path.join(self._log_dir, f\"attempt_{restart_count}\")\n        shutil.rmtree(attempt_log_dir, ignore_errors=True)\n        os.makedirs(attempt_log_dir)\n\n        assert spec.entrypoint is not None\n        self._pcontext = start_processes(\n            name=spec.role,\n            entrypoint=spec.entrypoint,\n            args=args,\n            envs=envs,\n            log_dir=attempt_log_dir,\n            start_method=self._start_method,\n            redirects=spec.redirects,\n            tee=spec.tee,\n        )\n\n        return self._pcontext.pids()\n\n    def _invoke_run(self, role: str = \"default\") -> RunResult:\n        # NOTE: currently only works for a single role\n\n        spec = self._worker_group.spec\n        role = spec.role\n\n        log.info(f\"[{role}] starting workers for entrypoint: {spec.get_entrypoint_name()}\")\n\n        self._initialize_workers(self._worker_group)\n        monitor_interval = spec.monitor_interval\n        rdzv_handler = spec.rdzv_handler\n\n        participants = rdzv_handler._state_holder.state.participants\n\n        while True:\n            assert self._worker_group.state != WorkerState.INIT\n            time.sleep(monitor_interval)\n            run_result = self._monitor_workers(self._worker_group)\n            state = run_result.state\n            self._worker_group.state = state\n\n            expire_time = datetime.utcnow() - (rdzv_handler._settings.keep_alive_interval *\n                                               rdzv_handler._settings.keep_alive_max_attempt)\n            _dead_nodes = [\n                node for node, last_heartbeat in rdzv_handler._state_holder.state.last_heartbeats.items()\n                if last_heartbeat < expire_time\n            ]\n\n            put_metric(f\"workers.{role}.remaining_restarts\", self._remaining_restarts)\n            put_metric(f\"workers.{role}.{state.name.lower()}\", 1)\n\n            if state == WorkerState.SUCCEEDED:\n                log.info(f\"[{role}] worker group successfully finished.\"\n                         f\" Waiting {self._exit_barrier_timeout} seconds for other agents to finish.\")\n                self._exit_barrier()\n                return run_result\n            elif state in {WorkerState.UNHEALTHY, WorkerState.FAILED\n                           } or len(participants) > len(rdzv_handler._state_holder.state.participants):\n                if self._remaining_restarts > 0:\n                    log.info(f\"[{role}] Worker group {state.name}. \"\n                             f\"{self._remaining_restarts}/{spec.max_restarts} attempts left;\"\n                             f\" will restart worker group\")\n                    self._remaining_restarts -= 1\n                    # rdzv_handler._state_holder.state.restart = False\n                    self._restart_workers(self._worker_group)\n                    participants = rdzv_handler._state_holder.state.participants\n\n                else:\n                    self._stop_workers(self._worker_group)\n                    self._worker_group.state = WorkerState.FAILED\n                    self._exit_barrier()\n                    return run_result\n            elif state == WorkerState.HEALTHY:\n                # membership changes do not count as retries\n                num_nodes_waiting = rdzv_handler.num_nodes_waiting()\n                group_rank = self._worker_group.group_rank\n                if num_nodes_waiting > 0:\n                    log.info(f\"[{role}] Detected {num_nodes_waiting} \"\n                             f\"new nodes from group_rank={group_rank}; \"\n                             f\"will restart worker group\")\n                    self._restart_workers(self._worker_group)\n                    participants = rdzv_handler._state_holder.state.participants\n            else:\n                raise Exception(f\"[{role}] Worker group in {state.name} state\")\n", "deepspeed/elasticity/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .elasticity import compute_elastic_config, elasticity_enabled, ensure_immutable_elastic_config\nfrom .utils import is_torch_elastic_compatible\nfrom .constants import ENABLED, ENABLED_DEFAULT, ELASTICITY\nif is_torch_elastic_compatible():\n    from .elastic_agent import DSElasticAgent\n", "deepspeed/nebula/config.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom deepspeed.runtime.config_utils import get_scalar_param, DeepSpeedConfigObject\nfrom deepspeed.nebula.constants import *\n\n\nclass DeepSpeedNebulaConfig(DeepSpeedConfigObject):\n\n    def __init__(self, param_dict):\n        super(DeepSpeedNebulaConfig, self).__init__()\n\n        self.enabled = None\n        self.persistent_storage_path = None\n        self.persistent_time_interval = None\n        self.num_of_version_in_retention = None\n        self.enable_nebula_load = None\n\n        if NEBULA in param_dict.keys():\n            nebula_dict = param_dict[NEBULA]\n        else:\n            nebula_dict = {}\n\n        self._initialize(nebula_dict)\n\n    def _initialize(self, nebula_dict):\n        self.enabled = get_scalar_param(nebula_dict, NEBULA_ENABLED, NEBULA_ENABLED_DEFAULT)\n\n        self.load_path = get_scalar_param(nebula_dict, NEBULA_LOAD_PATH, NEBULA_LOAD_PATH_DEFAULT)\n\n        self.enable_nebula_load = get_scalar_param(nebula_dict, NEBULA_ENABLE_NEBULA_LOAD,\n                                                   NEBULA_ENABLE_NEBULA_LOAD_DEFAULT)\n\n        self.persistent_storage_path = get_scalar_param(nebula_dict, NEBULA_PERSISTENT_STORAGE_PATH,\n                                                        NEBULA_PERSISTENT_STORAGE_PATH_DEFAULT)\n\n        self.persistent_time_interval = get_scalar_param(nebula_dict, NEBULA_PERSISTENT_TIME_INTERVAL,\n                                                         NEBULA_PERSISTENT_TIME_INTERVAL_DEFAULT)\n\n        self.num_of_version_in_retention = get_scalar_param(nebula_dict, NEBULA_NUM_OF_VERSION_IN_RETENTION,\n                                                            NEBULA_NUM_OF_VERSION_IN_RETENTION_DEFAULT)\n", "deepspeed/nebula/constants.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\n#########################################\n# nebula\n#########################################\n# Nebula. By default, this feature is not enabled.\n# Users can configure in ds_config.json as below example:\nNEBULA_FORMAT = '''\nnebula should be enabled as:\n\"session_params\": {\n  \"nebula\": {\n        \"enabled\": true,\n        \"persistent_storage_path\": \"/foo/bar\",\n        \"persistent_time_interval\": 100,\n        \"num_of_version_in_retention\": 2,\n        \"enable_nebula_load\": true\n    }\n}\n'''\n\nNEBULA = \"nebula\"\n\nNEBULA_ENABLED = \"enabled\"\nNEBULA_ENABLED_DEFAULT = False\n\n# There is a case where customer want to load the checkpoint saved\n# by raw torch. Because nebula cannot load torch checkpoint directly\n# as they have different folder structures to bring the gap for\n# loading(the data are totally same in bytes for torch and nebula\n# saving).\n# In this case, we must disable nebula load to use raw torch load.\n# Customer can just set NEBULA_ENABLE_NEBULA_LOAD to False. Then use\n# original way of deepspeed to load, i.e. set the value of \"--load\".\nNEBULA_ENABLE_NEBULA_LOAD = \"enable_nebula_load\"\nNEBULA_ENABLE_NEBULA_LOAD_DEFAULT = True\n\n# When you want to resume the previous checkpoint saved by nebula,\n# you can set NEBULA_LOAD_PATH as the parent folder of checkpoint.\n# If NEBULA_LOAD_PATH is None, the NEBULA_PERSISTENT_STORAGE_PATH\n# will be the default path to load.\nNEBULA_LOAD_PATH = \"nebula_load_path\"\nNEBULA_LOAD_PATH_DEFAULT = None\n\n# Nebula will save the checkpoint under NEBULA_LOAD_PATH in the\n# asynchronous way.\nNEBULA_PERSISTENT_STORAGE_PATH = \"persistent_storage_path\"\nNEBULA_PERSISTENT_STORAGE_PATH_DEFAULT = None\n\n# Time interval to trigger the nebula persistence.\nNEBULA_PERSISTENT_TIME_INTERVAL = \"persistent_time_interval\"\nNEBULA_PERSISTENT_TIME_INTERVAL_DEFAULT = 100\n\n# Checkpoint number which will be kept in memory. Let us say,\n# if the value is 2. Then we have checkpoints 1 and 2 are ready\n# now. When it comes to checkpoint 3, the 1 will be removed if\n# 1 has been persisted to disk.\nNEBULA_NUM_OF_VERSION_IN_RETENTION = \"num_of_version_in_retention\"\nNEBULA_NUM_OF_VERSION_IN_RETENTION_DEFAULT = 2\n\n# Nebula envs\nNEBULA_EXPORT_ENVS = [\n    'DLTS_JOB_ID', 'DLTS_NUM_WORKER', 'NEBULA_PERSISTENT_STORAGE_PATH', 'NEBULA_PERSISTENT_TIME_INTERVAL',\n    'AML_RUN_ID', 'AZUREML_RUN_TOKEN', 'AZUREML_WORKSPACE_SCOPE', 'AZUREML_EXPERIMENT_SCOPE',\n    'AZUREML_RUN_HISTORY_SERVICE_ENDPOINT', 'AZUREML_RUN_ID', 'NEBULA_MEMORY_BUFFER_SIZE',\n    'AZUREML_PARAMETER_ITPJOB_NAME', 'FC_TASKROLE_NAME', 'FC_TASK_INDEX', 'MASTER_HOST', 'LOCAL_HOST',\n    'AZUREML_BLOB_ACCOUNT_NAME', 'AZUREML_BLOB_ACCOUNT_KEY'\n]\n\n# ITP env files\nDLTS_POD_ENV_PATH = '/dlts-runtime/env/pod.env'\n", "deepspeed/nebula/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n'''Copyright The Microsoft DeepSpeed Team'''\n", "deepspeed/profiling/config.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom deepspeed.runtime.config_utils import get_scalar_param, DeepSpeedConfigObject\nfrom deepspeed.profiling.constants import *\n\n\nclass DeepSpeedFlopsProfilerConfig(DeepSpeedConfigObject):\n\n    def __init__(self, param_dict):\n        super(DeepSpeedFlopsProfilerConfig, self).__init__()\n\n        self.enabled = None\n        self.recompute_fwd_factor = None\n        self.profile_step = None\n        self.module_depth = None\n        self.top_modules = None\n\n        if FLOPS_PROFILER in param_dict.keys():\n            flops_profiler_dict = param_dict[FLOPS_PROFILER]\n        else:\n            flops_profiler_dict = {}\n\n        self._initialize(flops_profiler_dict)\n\n    def _initialize(self, flops_profiler_dict):\n        self.enabled = get_scalar_param(flops_profiler_dict, FLOPS_PROFILER_ENABLED, FLOPS_PROFILER_ENABLED_DEFAULT)\n\n        self.recompute_fwd_factor = get_scalar_param(flops_profiler_dict, FLOPS_PROFILER_RECOMPUTE_FWD_FACTOR,\n                                                     FLOPS_PROFILER_RECOMPUTE_FWD_FACTOR_DEFAULT)\n\n        self.profile_step = get_scalar_param(flops_profiler_dict, FLOPS_PROFILER_PROFILE_STEP,\n                                             FLOPS_PROFILER_PROFILE_STEP_DEFAULT)\n\n        self.module_depth = get_scalar_param(flops_profiler_dict, FLOPS_PROFILER_MODULE_DEPTH,\n                                             FLOPS_PROFILER_MODULE_DEPTH_DEFAULT)\n\n        self.top_modules = get_scalar_param(flops_profiler_dict, FLOPS_PROFILER_TOP_MODULES,\n                                            FLOPS_PROFILER_TOP_MODULES_DEFAULT)\n\n        self.detailed = get_scalar_param(flops_profiler_dict, FLOPS_PROFILER_DETAILED, FLOPS_PROFILER_DETAILED_DEFAULT)\n\n        self.output_file = get_scalar_param(flops_profiler_dict, FLOPS_PROFILER_OUTPUT_FILE,\n                                            FLOPS_PROFILER_OUTPUT_FILE_DEFAULT)\n", "deepspeed/profiling/constants.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\n#########################################\n# flops profiler\n#########################################\n# Flops profiler. By default, this feature is not enabled.\n# Users can configure in ds_config.json as below example:\nFLOPS_PROFILER_FORMAT = '''\nflops profiler should be enabled as:\n\"session_params\": {\n  \"flops_profiler\": {\n    \"enabled\": true,\n    \"recompute_fwd_factor\": 0.0,\n    \"profile_step\": 1,\n    \"module_depth\": -1,\n    \"top_modules\": 3,\n    \"detailed\": true,\n    \"output_file\": null\n    }\n}\n'''\n\nFLOPS_PROFILER = \"flops_profiler\"\n\nFLOPS_PROFILER_ENABLED = \"enabled\"\nFLOPS_PROFILER_ENABLED_DEFAULT = False\n\nFLOPS_PROFILER_RECOMPUTE_FWD_FACTOR = \"recompute_fwd_factor\"\nFLOPS_PROFILER_RECOMPUTE_FWD_FACTOR_DEFAULT = 0.0\n\nFLOPS_PROFILER_PROFILE_STEP = \"profile_step\"\nFLOPS_PROFILER_PROFILE_STEP_DEFAULT = 1\n\nFLOPS_PROFILER_MODULE_DEPTH = \"module_depth\"\nFLOPS_PROFILER_MODULE_DEPTH_DEFAULT = -1\n\nFLOPS_PROFILER_TOP_MODULES = \"top_modules\"\nFLOPS_PROFILER_TOP_MODULES_DEFAULT = 1\n\nFLOPS_PROFILER_DETAILED = \"detailed\"\nFLOPS_PROFILER_DETAILED_DEFAULT = True\n\nFLOPS_PROFILER_OUTPUT_FILE = \"output_file\"\nFLOPS_PROFILER_OUTPUT_FILE_DEFAULT = None\n", "deepspeed/profiling/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n'''Copyright The Microsoft DeepSpeed Team'''\n", "deepspeed/profiling/flops_profiler/profiler.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom functools import partial\nfrom typing import List, Optional\nfrom collections import OrderedDict\nimport numpy as np\nfrom deepspeed.accelerator import get_accelerator\nfrom deepspeed.utils import logger\nfrom deepspeed.moe.layer import MoE\nfrom deepspeed.utils.timer import FORWARD_GLOBAL_TIMER, BACKWARD_GLOBAL_TIMER, STEP_GLOBAL_TIMER\n\nTensor = torch.Tensor\n\nmodule_flop_count = []\nmodule_mac_count = []\nold_functions = {}\n\nDEFAULT_PRECISION = 2\n\n\nclass FlopsProfiler(object):\n    \"\"\"Measures the latency, number of estimated floating-point operations and parameters of each module in a PyTorch model.\n\n    The flops-profiler profiles the forward pass of a PyTorch model and prints the model graph with the measured profile attached to each module. It shows how latency, flops and parameters are spent in the model and which modules or layers could be the bottleneck. It also outputs the names of the top k modules in terms of aggregated latency, flops, and parameters at depth l with k and l specified by the user. The output profile is computed for each batch of input.\n    The DeepSpeed flops profiler can be used with the DeepSpeed runtime or as a standalone package.\n    When using DeepSpeed for model training, the flops profiler can be configured in the deepspeed_config file and no user code change is required.\n\n    If using the profiler as a standalone package, one imports the flops_profiler package and use the APIs.\n\n    Here is an example for usage in a typical training workflow:\n\n        .. code-block:: python\n\n            model = Model()\n            prof = FlopsProfiler(model)\n\n            for step, batch in enumerate(data_loader):\n                if step == profile_step:\n                    prof.start_profile()\n\n                loss = model(batch)\n\n                if step == profile_step:\n                    flops = prof.get_total_flops(as_string=True)\n                    params = prof.get_total_params(as_string=True)\n                    prof.print_model_profile(profile_step=profile_step)\n                    prof.end_profile()\n\n                loss.backward()\n                optimizer.step()\n\n    To profile a trained model in inference, use the `get_model_profile` API.\n\n    Args:\n        object (torch.nn.Module): The PyTorch model to profile.\n    \"\"\"\n\n    def __init__(self, model, ds_engine=None, recompute_fwd_factor=0.0):\n        self.model = model\n        self.ds_engine = ds_engine\n        self.recompute_fwd_factor = recompute_fwd_factor\n        self.started = False\n        self.func_patched = False\n\n    def start_profile(self, ignore_list=None):\n        \"\"\"Starts profiling.\n\n        Extra attributes are added recursively to all the modules and the profiled torch.nn.functionals are monkey patched.\n\n        Args:\n            ignore_list (list, optional): the list of modules to ignore while profiling. Defaults to None.\n        \"\"\"\n        logger.info(\"Flops profiler started\")\n        self.reset_profile()\n        _patch_functionals()\n        _patch_tensor_methods()\n\n        def register_module_hooks(module, ignore_list):\n            if ignore_list and type(module) in ignore_list:\n                return\n\n            # if computing the flops of a module directly\n            if type(module) in MODULE_HOOK_MAPPING:\n                if not hasattr(module, \"__flops_handle__\"):\n                    module.__flops_handle__ = module.register_forward_hook(MODULE_HOOK_MAPPING[type(module)])\n                return\n\n            # if computing the flops of the functionals in a module\n            def pre_hook(module, input):\n                module_flop_count.append([])\n                module_mac_count.append([])\n\n            if not hasattr(module, \"__pre_hook_handle__\"):\n                module.__pre_hook_handle__ = module.register_forward_pre_hook(pre_hook)\n\n            def post_hook(module, input, output):\n                if module_flop_count:\n                    module.__flops__ += sum([elem[1] for elem in module_flop_count[-1]])\n                    module_flop_count.pop()\n                    module.__macs__ += sum([elem[1] for elem in module_mac_count[-1]])\n                    module_mac_count.pop()\n\n            if not hasattr(module, \"__post_hook_handle__\"):\n                module.__post_hook_handle__ = module.register_forward_hook(post_hook)\n\n            def start_time_hook(module, input):\n                get_accelerator().synchronize()\n                module.__start_time__ = time.time()\n\n            if not hasattr(module, \"__start_time_hook_handle\"):\n                module.__start_time_hook_handle__ = module.register_forward_pre_hook(start_time_hook)\n\n            def end_time_hook(module, input, output):\n                get_accelerator().synchronize()\n                module.__duration__ += time.time() - module.__start_time__\n\n            if not hasattr(module, \"__end_time_hook_handle__\"):\n                module.__end_time_hook_handle__ = module.register_forward_hook(end_time_hook)\n\n        self.model.apply(partial(register_module_hooks, ignore_list=ignore_list))\n        self.started = True\n        self.func_patched = True\n\n    def stop_profile(self):\n        \"\"\"Stop profiling.\n\n        All torch.nn.functionals are restored to their originals.\n        \"\"\"\n        if self.started and self.func_patched:\n            _reload_functionals()\n            _reload_tensor_methods()\n            self.func_patched = False\n\n        def remove_profile_attrs(module):\n            if hasattr(module, \"__pre_hook_handle__\"):\n                module.__pre_hook_handle__.remove()\n                del module.__pre_hook_handle__\n            if hasattr(module, \"__post_hook_handle__\"):\n                module.__post_hook_handle__.remove()\n                del module.__post_hook_handle__\n            if hasattr(module, \"__flops_handle__\"):\n                module.__flops_handle__.remove()\n                del module.__flops_handle__\n            if hasattr(module, \"__start_time_hook_handle__\"):\n                module.__start_time_hook_handle__.remove()\n                del module.__start_time_hook_handle__\n            if hasattr(module, \"__end_time_hook_handle__\"):\n                module.__end_time_hook_handle__.remove()\n                del module.__end_time_hook_handle__\n\n        self.model.apply(remove_profile_attrs)\n\n    def reset_profile(self):\n        \"\"\"Resets the profiling.\n\n        Adds or resets the extra attributes.\n        \"\"\"\n\n        def get_param_count_and_ep(param):\n            \"\"\"\n            Return the number of parameters in the layer, whether the layer is an MoE layer,\n            and its expert parallelism size if so\n            \"\"\"\n            prefix = 'ep_size_'\n            offset = len(prefix)\n            expert_parallelism = 0\n            if getattr(param, \"group_name\", \"\").startswith(prefix):\n                try:\n                    expert_parallelism = int(param.group_name[offset:])\n                except ValueError:\n                    pass\n            return param.numel(), expert_parallelism, param.element_size()\n\n        def add_or_reset_attrs(module):\n            module.__flops__ = 0\n            module.__macs__ = 0\n            module.__params__ = module.__expert_params__ = module.__model_expert_params__ = 0\n            parameters = (get_param_count_and_ep(p) for p in module.parameters())\n            for num_params, expert_parallelism, per_param_size in parameters:\n                params = num_params if not expert_parallelism else 0\n                expert_params = num_params if expert_parallelism else 0\n                # number of expert parameters taking into account other expert parallel groups\n                model_expert_params = num_params * expert_parallelism\n                module.__params__ += params\n                module.__expert_params__ += expert_params\n                module.__model_expert_params__ += model_expert_params\n            module.__start_time__ = 0\n            module.__duration__ = 0\n\n        self.model.apply(add_or_reset_attrs)\n\n    def end_profile(self):\n        \"\"\"Ends profiling.\n\n        The added attributes and handles are removed recursively on all the modules.\n        \"\"\"\n        if not self.started:\n            return\n        self.stop_profile()\n        self.started = False\n\n        def remove_profile_attrs(module):\n            if hasattr(module, \"__flops__\"):\n                del module.__flops__\n            if hasattr(module, \"__macs__\"):\n                del module.__macs__\n            if hasattr(module, \"__params__\"):\n                del module.__params__\n            if hasattr(module, \"__expert_params__\"):\n                del module.__expert_params__\n            if hasattr(module, \"__model_expert_params__\"):\n                del module.__model_expert_params__\n            if hasattr(module, \"__start_time__\"):\n                del module.__start_time__\n            if hasattr(module, \"__duration__\"):\n                del module.__duration__\n\n        self.model.apply(remove_profile_attrs)\n        logger.info(\"Flops profiler finished\")\n\n    def get_total_flops(self, as_string=False):\n        \"\"\"Returns the total flops of the model.\n\n        Args:\n            as_string (bool, optional): whether to output the flops as string. Defaults to False.\n\n        Returns:\n            The number of multiply-accumulate operations of the model forward pass.\n        \"\"\"\n        total_flops = get_module_flops(self.model)\n        return number_to_string(total_flops) if as_string else total_flops\n\n    def get_total_macs(self, as_string=False):\n        \"\"\"Returns the total MACs of the model.\n\n        Args:\n            as_string (bool, optional): whether to output the flops as string. Defaults to False.\n\n        Returns:\n            The number of multiply-accumulate operations of the model forward pass.\n        \"\"\"\n        total_macs = get_module_macs(self.model)\n        return macs_to_string(total_macs) if as_string else total_macs\n\n    def get_total_duration(self, as_string=False):\n        \"\"\"Returns the total duration of the model forward pass.\n\n        Args:\n            as_string (bool, optional): whether to output the duration as string. Defaults to False.\n\n        Returns:\n            The latency of the model forward pass.\n        \"\"\"\n        total_duration = get_module_duration(self.model)\n        return duration_to_string(total_duration) if as_string else total_duration\n\n    def get_total_params(self, as_string=False):\n        \"\"\"Returns the total number of parameters stored per rank.\n\n        Args:\n            as_string (bool, optional): whether to output the parameters as string. Defaults to False.\n\n        Returns:\n            The total number of parameters stored per rank.\n        \"\"\"\n        total_params = self.model.__expert_params__ + self.model.__params__\n        return params_to_string(total_params) if as_string else total_params\n\n    def is_expert_tensor_parallelism_enabled(self):\n        for _, module in self.model.named_modules():\n            if isinstance(module, MoE) and hasattr(module, 'enable_expert_tensor_parallelism'):\n                return module.enable_expert_tensor_parallelism\n        return False\n\n    def print_model_profile(self, profile_step=1, module_depth=-1, top_modules=1, detailed=True, output_file=None):\n        \"\"\"Prints the model graph with the measured profile attached to each module.\n\n        Args:\n            profile_step (int, optional): The global training step at which to profile. Note that warm up steps are needed for accurate time measurement.\n            module_depth (int, optional): The depth of the model to which to print the aggregated module information. When set to -1, it prints information from the top to the innermost modules (the maximum depth).\n            top_modules (int, optional): Limits the aggregated profile output to the number of top modules specified.\n            detailed (bool, optional): Whether to print the detailed model profile.\n            output_file (str, optional): Path to the output file. If None, the profiler prints to stdout.\n        \"\"\"\n        if not self.started:\n            return\n        import sys\n        import os.path\n        original_stdout = None\n        f = None\n        if output_file and output_file != \"\":\n            dir_path = os.path.dirname(os.path.abspath(output_file))\n            if not os.path.exists(dir_path):\n                os.makedirs(dir_path)\n            original_stdout = sys.stdout\n            f = open(output_file, \"w\")\n            sys.stdout = f\n\n        total_flops = self.get_total_flops()\n        total_macs = self.get_total_macs()\n        total_duration = self.get_total_duration()\n        total_params = self.get_total_params()\n        expert_tensor_parallelism = None  # silence the linters\n        total_model_expert_params = total_model_nonexpert_params = 0\n        if self.ds_engine:\n            total_model_nonexpert_params = self.model.__params__ * self.ds_engine.mp_world_size\n            if self.ds_engine.has_moe_layers:\n                expert_tensor_parallelism = self.ds_engine.mp_world_size if self.is_expert_tensor_parallelism_enabled(\n                ) else 1\n                total_model_expert_params = self.model.__model_expert_params__ * expert_tensor_parallelism\n\n        self.flops = total_flops\n        self.macs = total_macs\n        self.params = total_params\n\n        print(\"\\n-------------------------- DeepSpeed Flops Profiler --------------------------\")\n        print(f'Profile Summary at step {profile_step}:')\n        print(\"Notations:\\n\"\n              \"data parallel size (dp_size), model parallel size(mp_size),\\n\"\n              \"number of parameters (params), number of multiply-accumulate operations(MACs),\\n\"\n              \"number of floating-point operations (flops), floating-point operations per second (FLOPS),\\n\"\n              \"fwd latency (forward propagation latency), bwd latency (backward propagation latency),\\n\"\n              \"step (weights update latency), iter latency (sum of fwd, bwd and step latency)\\n\")\n        line_fmt = '{:<70}  {:<8}'\n        if self.ds_engine:\n            print(line_fmt.format('world size: ', self.ds_engine.world_size))\n            print(line_fmt.format('data parallel size: ', self.ds_engine.dp_world_size))\n            print(line_fmt.format('model parallel size: ', self.ds_engine.mp_world_size))\n            print(line_fmt.format('batch size per GPU: ', self.ds_engine.train_micro_batch_size_per_gpu()))\n            if self.ds_engine.has_moe_layers:\n                print(line_fmt.format('expert tensor parallelism enabled: ', expert_tensor_parallelism > 1))\n\n        print(line_fmt.format('params per GPU: ', params_to_string(total_params)))\n        if total_model_expert_params > 0:\n            print(\n                line_fmt.format('params of model: ',\n                                params_to_string(total_model_nonexpert_params + total_model_expert_params)))\n            print(line_fmt.format('   non-expert params of model: ', params_to_string(total_model_nonexpert_params)))\n            print(line_fmt.format('   expert params of model: ', params_to_string(total_model_expert_params)))\n        else:\n            print(\n                line_fmt.format('params of model = params per GPU * mp_size: ',\n                                params_to_string(total_model_nonexpert_params)))\n\n        print(line_fmt.format('fwd MACs per GPU: ', macs_to_string(total_macs)))\n\n        print(line_fmt.format('fwd flops per GPU: ', number_to_string(total_flops)))\n\n        print(\n            line_fmt.format('fwd flops of model = fwd flops per GPU * mp_size: ',\n                            number_to_string(total_flops * (self.ds_engine.mp_world_size if self.ds_engine else 1))))\n\n        fwd_latency = self.get_total_duration()\n        if self.ds_engine and self.ds_engine.wall_clock_breakdown():\n            fwd_latency = self.ds_engine.timers(FORWARD_GLOBAL_TIMER).elapsed(False) / 1000.0\n        print(line_fmt.format('fwd latency: ', duration_to_string(fwd_latency)))\n        print(\n            line_fmt.format('fwd FLOPS per GPU = fwd flops per GPU / fwd latency: ',\n                            flops_to_string(total_flops / fwd_latency)))\n\n        if self.ds_engine and self.ds_engine.wall_clock_breakdown():\n            bwd_factor = 2 + self.recompute_fwd_factor\n            bwd_latency = self.ds_engine.timers(BACKWARD_GLOBAL_TIMER).elapsed(False) / 1000.0\n            step_latency = self.ds_engine.timers(STEP_GLOBAL_TIMER).elapsed(False) / 1000.0\n            print(line_fmt.format('bwd latency: ', duration_to_string(bwd_latency)))\n            print(\n                line_fmt.format(f'bwd FLOPS per GPU = {bwd_factor:g} * fwd flops per GPU / bwd latency: ',\n                                flops_to_string(bwd_factor * total_flops / bwd_latency)))\n            print(\n                line_fmt.format(\n                    f'fwd+bwd FLOPS per GPU = {bwd_factor + 1:g} * fwd flops per GPU / (fwd+bwd latency): ',\n                    flops_to_string((bwd_factor + 1) * total_flops / (fwd_latency + bwd_latency))))\n\n            print(line_fmt.format('step latency: ', duration_to_string(step_latency)))\n\n            iter_latency = fwd_latency + bwd_latency + step_latency\n            print(line_fmt.format('iter latency: ', duration_to_string(iter_latency)))\n            print(\n                line_fmt.format(f'FLOPS per GPU = {bwd_factor + 1:g} * fwd flops per GPU / iter latency: ',\n                                flops_to_string((bwd_factor + 1) * total_flops / iter_latency)))\n\n            samples_per_iter = self.ds_engine.train_micro_batch_size_per_gpu() * self.ds_engine.world_size\n            print(line_fmt.format('samples/second: ', round(samples_per_iter / iter_latency, DEFAULT_PRECISION)))\n\n        def flops_repr(module):\n            params = module.__params__ + module.__expert_params__\n            flops = get_module_flops(module)\n            macs = get_module_macs(module)\n            duration = get_module_duration(module)\n            items = [\n                \"{} = {:g}% Params\".format(\n                    params_to_string(params),\n                    round(100 * params / total_params, DEFAULT_PRECISION) if total_params else 0),\n                \"{} = {:g}% MACs\".format(macs_to_string(macs),\n                                         round(100 * macs / total_macs, DEFAULT_PRECISION) if total_macs else 0),\n                \"{} = {:g}% latency\".format(\n                    duration_to_string(duration),\n                    round(100 * duration / total_duration, DEFAULT_PRECISION) if total_duration else 0),\n                flops_to_string(round(flops / duration, DEFAULT_PRECISION) if duration else 0),\n            ]\n            original_extra_repr = module.original_extra_repr()\n            if original_extra_repr:\n                items.append(original_extra_repr)\n            return \", \".join(items)\n\n        def add_extra_repr(module):\n            flops_extra_repr = flops_repr.__get__(module)\n            if module.extra_repr != flops_extra_repr:\n                module.original_extra_repr = module.extra_repr\n                module.extra_repr = flops_extra_repr\n                assert module.extra_repr != module.original_extra_repr\n\n        def del_extra_repr(module):\n            if hasattr(module, \"original_extra_repr\"):\n                module.extra_repr = module.original_extra_repr\n                del module.original_extra_repr\n\n        self.model.apply(add_extra_repr)\n\n        print(\"\\n----------------------------- Aggregated Profile per GPU -----------------------------\")\n        self.print_model_aggregated_profile(module_depth=module_depth, top_modules=top_modules)\n\n        if detailed:\n            print(\"\\n------------------------------ Detailed Profile per GPU ------------------------------\")\n            print(\n                \"Each module profile is listed after its name in the following order: \\nparams, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS\"\n            )\n            print(\n                \"\\nNote: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.\\n2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\\n3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.\\n\"\n            )\n            print(self.model)\n\n        self.model.apply(del_extra_repr)\n\n        print(\"------------------------------------------------------------------------------\")\n\n        if output_file:\n            sys.stdout = original_stdout\n            f.close()\n\n    def print_model_aggregated_profile(self, module_depth=-1, top_modules=1):\n        \"\"\"Prints the names of the top top_modules modules in terms of aggregated time, flops, and parameters at depth module_depth.\n\n        Args:\n            module_depth (int, optional): the depth of the modules to show. Defaults to -1 (the innermost modules).\n            top_modules (int, optional): the number of top modules to show. Defaults to 1.\n        \"\"\"\n        info = {}\n        if not hasattr(self.model, \"__flops__\"):\n            print(\"no __flops__ attribute in the model, call this function after start_profile and before end_profile\")\n            return\n\n        def walk_module(module, curr_depth, info):\n            if curr_depth not in info:\n                info[curr_depth] = {}\n            if module.__class__.__name__ not in info[curr_depth]:\n                info[curr_depth][module.__class__.__name__] = [\n                    0,\n                    0,\n                    0,\n                ]  # macs, params, time\n            info[curr_depth][module.__class__.__name__][0] += get_module_macs(module)\n            info[curr_depth][module.__class__.__name__][1] += module.__params__ + module.__expert_params__\n            info[curr_depth][module.__class__.__name__][2] += get_module_duration(module)\n            has_children = len(module._modules.items()) != 0\n            if has_children:\n                for child in module.children():\n                    walk_module(child, curr_depth + 1, info)\n\n        walk_module(self.model, 0, info)\n\n        depth = module_depth\n        if module_depth == -1:\n            depth = len(info) - 1\n\n        print(f'Top {top_modules} modules in terms of params, MACs or fwd latency at different model depths:')\n\n        for d in range(depth):\n            num_items = min(top_modules, len(info[d]))\n\n            sort_macs = {\n                k: macs_to_string(v[0])\n                for k, v in sorted(info[d].items(), key=lambda item: item[1][0], reverse=True)[:num_items]\n            }\n            sort_params = {\n                k: params_to_string(v[1])\n                for k, v in sorted(info[d].items(), key=lambda item: item[1][1], reverse=True)[:num_items]\n            }\n            sort_time = {\n                k: duration_to_string(v[2])\n                for k, v in sorted(info[d].items(), key=lambda item: item[1][2], reverse=True)[:num_items]\n            }\n\n            print(f\"depth {d}:\")\n            print(f\"    params      - {sort_params}\")\n            print(f\"    MACs        - {sort_macs}\")\n            print(f\"    fwd latency - {sort_time}\")\n\n\ndef _prod(dims):\n    p = 1\n    for v in dims:\n        p *= v\n    return p\n\n\ndef _linear_flops_compute(input, weight, bias=None):\n    out_features = weight.shape[0]\n    macs = input.numel() * out_features\n    return 2 * macs, macs\n\n\ndef _relu_flops_compute(input, inplace=False):\n    return input.numel(), 0\n\n\ndef _prelu_flops_compute(input: Tensor, weight: Tensor):\n    return input.numel(), 0\n\n\ndef _elu_flops_compute(input: Tensor, alpha: float = 1.0, inplace: bool = False):\n    return input.numel(), 0\n\n\ndef _leaky_relu_flops_compute(input: Tensor, negative_slope: float = 0.01, inplace: bool = False):\n    return input.numel(), 0\n\n\ndef _relu6_flops_compute(input: Tensor, inplace: bool = False):\n    return input.numel(), 0\n\n\ndef _silu_flops_compute(input: Tensor, inplace: bool = False):\n    return input.numel(), 0\n\n\ndef _gelu_flops_compute(input, **kwargs):\n    return input.numel(), 0\n\n\ndef _pool_flops_compute(input,\n                        kernel_size,\n                        stride=None,\n                        padding=0,\n                        dilation=None,\n                        ceil_mode=False,\n                        count_include_pad=True,\n                        divisor_override=None,\n                        return_indices=None):\n    return input.numel(), 0\n\n\ndef _conv_flops_compute(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1):\n    assert weight.shape[1] * groups == input.shape[1]\n\n    batch_size = input.shape[0]\n    in_channels = input.shape[1]\n    out_channels = weight.shape[0]\n    kernel_dims = list(weight.shape[2:])\n    input_dims = list(input.shape[2:])\n\n    length = len(input_dims)\n\n    strides = stride if type(stride) is tuple else (stride, ) * length\n    dilations = dilation if type(dilation) is tuple else (dilation, ) * length\n    if isinstance(padding, str):\n        if padding == 'valid':\n            paddings = (0, ) * length\n        elif padding == 'same':\n            paddings = ()\n            for d, k in zip(dilations, kernel_dims):\n                total_padding = d * (k - 1)\n                paddings += (total_padding // 2, )\n    elif isinstance(padding, tuple):\n        paddings = padding\n    else:\n        paddings = (padding, ) * length\n\n    output_dims = []\n    for idx, input_dim in enumerate(input_dims):\n        output_dim = (input_dim + 2 * paddings[idx] - (dilations[idx] *\n                                                       (kernel_dims[idx] - 1) + 1)) // strides[idx] + 1\n        output_dims.append(output_dim)\n\n    filters_per_channel = out_channels // groups\n    conv_per_position_macs = int(_prod(kernel_dims)) * in_channels * filters_per_channel\n    active_elements_count = batch_size * int(_prod(output_dims))\n    overall_conv_macs = conv_per_position_macs * active_elements_count\n    overall_conv_flops = 2 * overall_conv_macs\n\n    bias_flops = 0\n    if bias is not None:\n        bias_flops = out_channels * active_elements_count\n\n    return int(overall_conv_flops + bias_flops), int(overall_conv_macs)\n\n\ndef _conv_trans_flops_compute(\n    input,\n    weight,\n    bias=None,\n    stride=1,\n    padding=0,\n    output_padding=0,\n    groups=1,\n    dilation=1,\n):\n    batch_size = input.shape[0]\n    in_channels = input.shape[1]\n    out_channels = weight.shape[1]\n    kernel_dims = list(weight.shape[2:])\n    input_dims = list(input.shape[2:])\n\n    length = len(input_dims)\n\n    paddings = padding if type(padding) is tuple else (padding, ) * length\n    strides = stride if type(stride) is tuple else (stride, ) * length\n    dilations = dilation if type(dilation) is tuple else (dilation, ) * length\n\n    output_dims = []\n    for idx, input_dim in enumerate(input_dims):\n\n        output_dim = (input_dim + 2 * paddings[idx] - (dilations[idx] *\n                                                       (kernel_dims[idx] - 1) + 1)) // strides[idx] + 1\n        output_dims.append(output_dim)\n\n    paddings = padding if type(padding) is tuple else (padding, padding)\n    strides = stride if type(stride) is tuple else (stride, stride)\n    dilations = dilation if type(dilation) is tuple else (dilation, dilation)\n\n    filters_per_channel = out_channels // groups\n    conv_per_position_macs = int(_prod(kernel_dims)) * in_channels * filters_per_channel\n    active_elements_count = batch_size * int(_prod(input_dims))\n    overall_conv_macs = conv_per_position_macs * active_elements_count\n    overall_conv_flops = 2 * overall_conv_macs\n\n    bias_flops = 0\n    if bias is not None:\n        bias_flops = out_channels * batch_size * int(_prod(output_dims))\n\n    return int(overall_conv_flops + bias_flops), int(overall_conv_macs)\n\n\ndef _batch_norm_flops_compute(\n    input,\n    running_mean,\n    running_var,\n    weight=None,\n    bias=None,\n    training=False,\n    momentum=0.1,\n    eps=1e-05,\n):\n    has_affine = weight is not None\n    if training:\n        # estimation\n        return input.numel() * (5 if has_affine else 4), 0\n    flops = input.numel() * (2 if has_affine else 1)\n    return flops, 0\n\n\ndef _layer_norm_flops_compute(\n    input: Tensor,\n    normalized_shape: List[int],\n    weight: Optional[Tensor] = None,\n    bias: Optional[Tensor] = None,\n    eps: float = 1e-5,\n):\n    has_affine = weight is not None\n    # estimation\n    return input.numel() * (5 if has_affine else 4), 0\n\n\ndef _group_norm_flops_compute(input: Tensor,\n                              num_groups: int,\n                              weight: Optional[Tensor] = None,\n                              bias: Optional[Tensor] = None,\n                              eps: float = 1e-5):\n    has_affine = weight is not None\n    # estimation\n    return input.numel() * (5 if has_affine else 4), 0\n\n\ndef _instance_norm_flops_compute(\n    input: Tensor,\n    running_mean: Optional[Tensor] = None,\n    running_var: Optional[Tensor] = None,\n    weight: Optional[Tensor] = None,\n    bias: Optional[Tensor] = None,\n    use_input_stats: bool = True,\n    momentum: float = 0.1,\n    eps: float = 1e-5,\n):\n    has_affine = weight is not None\n    # estimation\n    return input.numel() * (5 if has_affine else 4), 0\n\n\ndef _upsample_flops_compute(*args, **kwargs):\n    input = args[0]\n    size = kwargs.get('size', None)\n    if size is None and len(args) > 1:\n        size = args[1]\n\n    if size is not None:\n        if isinstance(size, tuple) or isinstance(size, list):\n            return int(_prod(size)), 0\n        else:\n            return int(size), 0\n\n    scale_factor = kwargs.get('scale_factor', None)\n    if scale_factor is None and len(args) > 2:\n        scale_factor = args[2]\n    assert scale_factor is not None, \"either size or scale_factor should be defined\"\n\n    flops = input.numel()\n    if isinstance(scale_factor, tuple) and len(scale_factor) == len(input):\n        flops *= int(_prod(scale_factor))\n    else:\n        flops *= scale_factor**len(input)\n    return flops, 0\n\n\ndef _softmax_flops_compute(input, dim=None, _stacklevel=3, dtype=None):\n    return input.numel(), 0\n\n\ndef _embedding_flops_compute(\n    input,\n    weight,\n    padding_idx=None,\n    max_norm=None,\n    norm_type=2.0,\n    scale_grad_by_freq=False,\n    sparse=False,\n):\n    return 0, 0\n\n\ndef _dropout_flops_compute(input, p=0.5, training=True, inplace=False):\n    return 0, 0\n\n\ndef _matmul_flops_compute(input, other, *, out=None):\n    \"\"\"\n    Count flops for the matmul operation.\n    \"\"\"\n    macs = _prod(input.shape) * other.shape[-1]\n    return 2 * macs, macs\n\n\ndef _addmm_flops_compute(input, mat1, mat2, *, beta=1, alpha=1, out=None):\n    \"\"\"\n    Count flops for the addmm operation.\n    \"\"\"\n    macs = _prod(mat1.shape) * mat2.shape[-1]\n    return 2 * macs + _prod(input.shape), macs\n\n\ndef _einsum_flops_compute(equation, *operands):\n    \"\"\"\n    Count flops for the einsum operation.\n    \"\"\"\n    equation = equation.replace(\" \", \"\")\n    input_shapes = [o.shape for o in operands]\n\n    # Re-map equation so that same equation with different alphabet\n    # representations will look the same.\n    letter_order = OrderedDict((k, 0) for k in equation if k.isalpha()).keys()\n    mapping = {ord(x): 97 + i for i, x in enumerate(letter_order)}\n    equation = equation.translate(mapping)\n\n    np_arrs = [np.zeros(s) for s in input_shapes]\n    optim = np.einsum_path(equation, *np_arrs, optimize=\"optimal\")[1]\n    for line in optim.split(\"\\n\"):\n        if \"optimized flop\" in line.lower():\n            flop = int(float(line.split(\":\")[-1]))\n            return flop, 0\n    raise NotImplementedError(\"Unsupported einsum operation.\")\n\n\ndef _tensor_addmm_flops_compute(self, mat1, mat2, *, beta=1, alpha=1, out=None):\n    \"\"\"\n    Count flops for the tensor addmm operation.\n    \"\"\"\n    macs = _prod(mat1.shape) * mat2.shape[-1]\n    return 2 * macs + _prod(self.shape), macs\n\n\ndef _mul_flops_compute(input, other, *, out=None):\n    return _elementwise_flops_compute(input, other)\n\n\ndef _add_flops_compute(input, other, *, alpha=1, out=None):\n    return _elementwise_flops_compute(input, other)\n\n\ndef _elementwise_flops_compute(input, other):\n    if not torch.is_tensor(input):\n        if torch.is_tensor(other):\n            return _prod(other.shape), 0\n        else:\n            return 1, 0\n    elif not torch.is_tensor(other):\n        return _prod(input.shape), 0\n    else:\n        dim_input = len(input.shape)\n        dim_other = len(other.shape)\n        max_dim = max(dim_input, dim_other)\n\n        final_shape = []\n        for i in range(max_dim):\n            in_i = input.shape[i] if i < dim_input else 1\n            ot_i = other.shape[i] if i < dim_other else 1\n            if in_i > ot_i:\n                final_shape.append(in_i)\n            else:\n                final_shape.append(ot_i)\n        flops = _prod(final_shape)\n        return flops, 0\n\n\ndef _attn_flops_compute(q, k, v, *args, **kwargs):\n    \"\"\"\n    Count flops for the scaled_dot_product_attention operation.\n    \"\"\"\n    macs = _prod(q.shape) * k.shape[-2]\n    macs += _prod(q.shape[:-1]) * k.shape[-2] * v.shape[-1]\n    return 2 * macs, macs\n\n\ndef wrapFunc(func, funcFlopCompute):\n    oldFunc = func\n    name = func.__str__\n    old_functions[name] = oldFunc\n\n    def newFunc(*args, **kwds):\n        flops, macs = funcFlopCompute(*args, **kwds)\n        if module_flop_count:\n            module_flop_count[-1].append((name, flops))\n        if module_mac_count and macs:\n            module_mac_count[-1].append((name, macs))\n        return oldFunc(*args, **kwds)\n\n    newFunc.__str__ = func.__str__\n\n    return newFunc\n\n\ndef _patch_functionals():\n    # FC\n    F.linear = wrapFunc(F.linear, _linear_flops_compute)\n\n    # convolutions\n    F.conv1d = wrapFunc(F.conv1d, _conv_flops_compute)\n    F.conv2d = wrapFunc(F.conv2d, _conv_flops_compute)\n    F.conv3d = wrapFunc(F.conv3d, _conv_flops_compute)\n\n    # conv transposed\n    F.conv_transpose1d = wrapFunc(F.conv_transpose1d, _conv_trans_flops_compute)\n    F.conv_transpose2d = wrapFunc(F.conv_transpose2d, _conv_trans_flops_compute)\n    F.conv_transpose3d = wrapFunc(F.conv_transpose3d, _conv_trans_flops_compute)\n\n    # activations\n    F.relu = wrapFunc(F.relu, _relu_flops_compute)\n    F.prelu = wrapFunc(F.prelu, _prelu_flops_compute)\n    F.elu = wrapFunc(F.elu, _elu_flops_compute)\n    F.leaky_relu = wrapFunc(F.leaky_relu, _leaky_relu_flops_compute)\n    F.relu6 = wrapFunc(F.relu6, _relu6_flops_compute)\n    if hasattr(F, \"silu\"):\n        F.silu = wrapFunc(F.silu, _silu_flops_compute)\n    F.gelu = wrapFunc(F.gelu, _gelu_flops_compute)\n\n    # Normalizations\n    F.batch_norm = wrapFunc(F.batch_norm, _batch_norm_flops_compute)\n    F.layer_norm = wrapFunc(F.layer_norm, _layer_norm_flops_compute)\n    F.instance_norm = wrapFunc(F.instance_norm, _instance_norm_flops_compute)\n    F.group_norm = wrapFunc(F.group_norm, _group_norm_flops_compute)\n\n    # poolings\n    F.avg_pool1d = wrapFunc(F.avg_pool1d, _pool_flops_compute)\n    F.avg_pool2d = wrapFunc(F.avg_pool2d, _pool_flops_compute)\n    F.avg_pool3d = wrapFunc(F.avg_pool3d, _pool_flops_compute)\n    F.max_pool1d = wrapFunc(F.max_pool1d, _pool_flops_compute)\n    F.max_pool2d = wrapFunc(F.max_pool2d, _pool_flops_compute)\n    F.max_pool3d = wrapFunc(F.max_pool3d, _pool_flops_compute)\n    F.adaptive_avg_pool1d = wrapFunc(F.adaptive_avg_pool1d, _pool_flops_compute)\n    F.adaptive_avg_pool2d = wrapFunc(F.adaptive_avg_pool2d, _pool_flops_compute)\n    F.adaptive_avg_pool3d = wrapFunc(F.adaptive_avg_pool3d, _pool_flops_compute)\n    F.adaptive_max_pool1d = wrapFunc(F.adaptive_max_pool1d, _pool_flops_compute)\n    F.adaptive_max_pool2d = wrapFunc(F.adaptive_max_pool2d, _pool_flops_compute)\n    F.adaptive_max_pool3d = wrapFunc(F.adaptive_max_pool3d, _pool_flops_compute)\n\n    # upsample\n    F.upsample = wrapFunc(F.upsample, _upsample_flops_compute)\n    F.interpolate = wrapFunc(F.interpolate, _upsample_flops_compute)\n\n    # softmax\n    F.softmax = wrapFunc(F.softmax, _softmax_flops_compute)\n\n    # embedding\n    F.embedding = wrapFunc(F.embedding, _embedding_flops_compute)\n\n    # attn\n    F.scaled_dot_product_attention = wrapFunc(F.scaled_dot_product_attention, _attn_flops_compute)\n\n\ndef _patch_tensor_methods():\n    torch.matmul = wrapFunc(torch.matmul, _matmul_flops_compute)\n    torch.Tensor.matmul = wrapFunc(torch.Tensor.matmul, _matmul_flops_compute)\n    torch.Tensor.__matmul__ = wrapFunc(torch.Tensor.__matmul__, _matmul_flops_compute)\n    torch.mm = wrapFunc(torch.mm, _matmul_flops_compute)\n    torch.Tensor.mm = wrapFunc(torch.Tensor.mm, _matmul_flops_compute)\n    torch.bmm = wrapFunc(torch.bmm, _matmul_flops_compute)\n    torch.Tensor.bmm = wrapFunc(torch.Tensor.bmm, _matmul_flops_compute)\n\n    torch.addmm = wrapFunc(torch.addmm, _addmm_flops_compute)\n    torch.Tensor.addmm = wrapFunc(torch.Tensor.addmm, _tensor_addmm_flops_compute)\n\n    torch.mul = wrapFunc(torch.mul, _mul_flops_compute)\n    torch.Tensor.mul = wrapFunc(torch.Tensor.mul, _mul_flops_compute)\n\n    torch.add = wrapFunc(torch.add, _add_flops_compute)\n    torch.Tensor.add = wrapFunc(torch.Tensor.add, _add_flops_compute)\n\n    torch.einsum = wrapFunc(torch.einsum, _einsum_flops_compute)\n\n    torch.baddbmm = wrapFunc(torch.baddbmm, _tensor_addmm_flops_compute)\n\n\ndef _reload_functionals():\n    # torch.nn.functional does not support importlib.reload()\n    F.linear = old_functions[F.linear.__str__]\n    F.conv1d = old_functions[F.conv1d.__str__]\n    F.conv2d = old_functions[F.conv2d.__str__]\n    F.conv3d = old_functions[F.conv3d.__str__]\n    F.conv_transpose1d = old_functions[F.conv_transpose1d.__str__]\n    F.conv_transpose2d = old_functions[F.conv_transpose2d.__str__]\n    F.conv_transpose3d = old_functions[F.conv_transpose3d.__str__]\n    F.relu = old_functions[F.relu.__str__]\n    F.prelu = old_functions[F.prelu.__str__]\n    F.elu = old_functions[F.elu.__str__]\n    F.leaky_relu = old_functions[F.leaky_relu.__str__]\n    F.relu6 = old_functions[F.relu6.__str__]\n    if hasattr(F, \"silu\"):\n        F.silu = old_functions[F.silu.__str__]\n    F.gelu = old_functions[F.gelu.__str__]\n    F.batch_norm = old_functions[F.batch_norm.__str__]\n    F.layer_norm = old_functions[F.layer_norm.__str__]\n    F.instance_norm = old_functions[F.instance_norm.__str__]\n    F.group_norm = old_functions[F.group_norm.__str__]\n    F.avg_pool1d = old_functions[F.avg_pool1d.__str__]\n    F.avg_pool2d = old_functions[F.avg_pool2d.__str__]\n    F.avg_pool3d = old_functions[F.avg_pool3d.__str__]\n    F.max_pool1d = old_functions[F.max_pool1d.__str__]\n    F.max_pool2d = old_functions[F.max_pool2d.__str__]\n    F.max_pool3d = old_functions[F.max_pool3d.__str__]\n    F.adaptive_avg_pool1d = old_functions[F.adaptive_avg_pool1d.__str__]\n    F.adaptive_avg_pool2d = old_functions[F.adaptive_avg_pool2d.__str__]\n    F.adaptive_avg_pool3d = old_functions[F.adaptive_avg_pool3d.__str__]\n    F.adaptive_max_pool1d = old_functions[F.adaptive_max_pool1d.__str__]\n    F.adaptive_max_pool2d = old_functions[F.adaptive_max_pool2d.__str__]\n    F.adaptive_max_pool3d = old_functions[F.adaptive_max_pool3d.__str__]\n    F.upsample = old_functions[F.upsample.__str__]\n    F.interpolate = old_functions[F.interpolate.__str__]\n    F.softmax = old_functions[F.softmax.__str__]\n    F.embedding = old_functions[F.embedding.__str__]\n\n\ndef _reload_tensor_methods():\n    torch.matmul = old_functions[torch.matmul.__str__]\n    torch.Tensor.matmul = old_functions[torch.Tensor.matmul.__str__]\n    torch.mm = old_functions[torch.mm.__str__]\n    torch.Tensor.mm = old_functions[torch.Tensor.mm.__str__]\n    torch.bmm = old_functions[torch.matmul.__str__]\n    torch.Tensor.bmm = old_functions[torch.Tensor.bmm.__str__]\n    torch.addmm = old_functions[torch.addmm.__str__]\n    torch.Tensor.addmm = old_functions[torch.Tensor.addmm.__str__]\n    torch.mul = old_functions[torch.mul.__str__]\n    torch.Tensor.mul = old_functions[torch.Tensor.mul.__str__]\n    torch.add = old_functions[torch.add.__str__]\n    torch.Tensor.add = old_functions[torch.Tensor.add.__str__]\n\n    torch.einsum = old_functions[torch.einsum.__str__]\n\n    torch.baddbmm = old_functions[torch.baddbmm.__str__]\n\n\ndef _rnn_flops(flops, rnn_module, w_ih, w_hh, input_size):\n    gates_size = w_ih.shape[0]\n    # matrix matrix mult ih state and internal state\n    flops += 2 * w_ih.shape[0] * w_ih.shape[1] - gates_size\n    # matrix matrix mult hh state and internal state\n    flops += 2 * w_hh.shape[0] * w_hh.shape[1] - gates_size\n    if isinstance(rnn_module, (nn.RNN, nn.RNNCell)):\n        # add both operations\n        flops += rnn_module.hidden_size\n    elif isinstance(rnn_module, (nn.GRU, nn.GRUCell)):\n        # hadamard of r\n        flops += rnn_module.hidden_size\n        # adding operations from both states\n        flops += rnn_module.hidden_size * 3\n        # last two hadamard _product and add\n        flops += rnn_module.hidden_size * 3\n    elif isinstance(rnn_module, (nn.LSTM, nn.LSTMCell)):\n        # adding operations from both states\n        flops += rnn_module.hidden_size * 4\n        # two hadamard _product and add for C state\n        flops += rnn_module.hidden_size + rnn_module.hidden_size + rnn_module.hidden_size\n        # final hadamard\n        flops += rnn_module.hidden_size + rnn_module.hidden_size + rnn_module.hidden_size\n    return flops\n\n\ndef _rnn_forward_hook(rnn_module, input, output):\n    flops = 0\n    # input is a tuple containing a sequence to process and (optionally) hidden state\n    inp = input[0]\n    batch_size = inp.shape[0]\n    seq_length = inp.shape[1]\n    num_layers = rnn_module.num_layers\n\n    for i in range(num_layers):\n        w_ih = rnn_module.__getattr__(\"weight_ih_l\" + str(i))\n        w_hh = rnn_module.__getattr__(\"weight_hh_l\" + str(i))\n        if i == 0:\n            input_size = rnn_module.input_size\n        else:\n            input_size = rnn_module.hidden_size\n        flops = _rnn_flops(flops, rnn_module, w_ih, w_hh, input_size)\n        if rnn_module.bias:\n            b_ih = rnn_module.__getattr__(\"bias_ih_l\" + str(i))\n            b_hh = rnn_module.__getattr__(\"bias_hh_l\" + str(i))\n            flops += b_ih.shape[0] + b_hh.shape[0]\n\n    flops *= batch_size\n    flops *= seq_length\n    if rnn_module.bidirectional:\n        flops *= 2\n    rnn_module.__flops__ += int(flops)\n\n\ndef _rnn_cell_forward_hook(rnn_cell_module, input, output):\n    flops = 0\n    inp = input[0]\n    batch_size = inp.shape[0]\n    w_ih = rnn_cell_module.__getattr__(\"weight_ih\")\n    w_hh = rnn_cell_module.__getattr__(\"weight_hh\")\n    input_size = inp.shape[1]\n    flops = _rnn_flops(flops, rnn_cell_module, w_ih, w_hh, input_size)\n    if rnn_cell_module.bias:\n        b_ih = rnn_cell_module.__getattr__(\"bias_ih\")\n        b_hh = rnn_cell_module.__getattr__(\"bias_hh\")\n        flops += b_ih.shape[0] + b_hh.shape[0]\n\n    flops *= batch_size\n    rnn_cell_module.__flops__ += int(flops)\n\n\nMODULE_HOOK_MAPPING = {\n    # RNN\n    nn.RNN: _rnn_forward_hook,\n    nn.GRU: _rnn_forward_hook,\n    nn.LSTM: _rnn_forward_hook,\n    nn.RNNCell: _rnn_cell_forward_hook,\n    nn.LSTMCell: _rnn_cell_forward_hook,\n    nn.GRUCell: _rnn_cell_forward_hook,\n}\n\n\ndef macs_to_string(macs, units=None, precision=DEFAULT_PRECISION):\n    return f\"{number_to_string(macs, units=units, precision=precision)}MACs\"\n\n\ndef number_to_string(num, units=None, precision=DEFAULT_PRECISION):\n    if units is None:\n        if num >= 1e12:\n            magnitude, units = 1e12, \"T\"\n        elif num >= 1e9:\n            magnitude, units = 1e9, \"G\"\n        elif num >= 1e6:\n            magnitude, units = 1e6, \"M\"\n        elif num >= 1e3:\n            magnitude, units = 1e3, \"K\"\n        elif num >= 1 or num == 0:\n            magnitude, units = 1, \"\"\n        elif num >= 1e-3:\n            magnitude, units = 1e-3, \"m\"\n        else:\n            magnitude, units = 1e-6, \"u\"\n    else:\n        if units == \"T\":\n            magnitude = 1e12\n        elif units == \"G\":\n            magnitude = 1e9\n        elif units == \"M\":\n            magnitude = 1e6\n        elif units == \"K\":\n            magnitude = 1e3\n        elif units == \"m\":\n            magnitude = 1e-3\n        elif units == \"u\":\n            magnitude = 1e-6\n        else:\n            magnitude = 1\n    return f\"{round(num / magnitude, precision):g} {units}\"\n\n\ndef flops_to_string(flops, units=None, precision=DEFAULT_PRECISION):\n    return f\"{number_to_string(flops, units=units, precision=precision)}FLOPS\"\n\n\ndef bytes_to_string(b, units=None, precision=DEFAULT_PRECISION):\n    return f\"{number_to_string(b, units=units, precision=precision)}B\"\n\n\ndef params_to_string(params_num, units=None, precision=DEFAULT_PRECISION):\n    units = units.replace(\"B\", \"G\") if units else units\n    return number_to_string(params_num, units=units, precision=precision).replace(\"G\", \"B\").strip()\n\n\ndef duration_to_string(duration, units=None, precision=DEFAULT_PRECISION):\n    return f\"{number_to_string(duration, units=units, precision=precision)}s\"\n\n\n    # can not iterate over all submodules using self.model.modules()\n    # since modules() returns duplicate modules only once\ndef get_module_flops(module):\n    sum = module.__flops__\n    # iterate over immediate children modules\n    for child in module.children():\n        sum += get_module_flops(child)\n    return sum\n\n\ndef get_module_macs(module):\n    sum = module.__macs__\n    # iterate over immediate children modules\n    for child in module.children():\n        sum += get_module_macs(child)\n    return sum\n\n\ndef get_module_duration(module):\n    duration = module.__duration__\n    if duration == 0:  # e.g. ModuleList\n        for m in module.children():\n            duration += get_module_duration(m)\n    return duration\n\n\ndef get_model_profile(model,\n                      input_shape=None,\n                      args=[],\n                      kwargs={},\n                      print_profile=True,\n                      detailed=True,\n                      module_depth=-1,\n                      top_modules=1,\n                      warm_up=1,\n                      as_string=True,\n                      output_file=None,\n                      ignore_modules=None,\n                      mode='forward'):\n    \"\"\"Returns the total floating-point operations, MACs, and parameters of a model.\n\n    Example:\n\n    .. code-block:: python\n\n        model = torchvision.models.alexnet()\n        batch_size = 256\n        flops, macs, params = get_model_profile(model=model, input_shape=(batch_size, 3, 224, 224)))\n\n    Args:\n        model ([torch.nn.Module]): the PyTorch model to be profiled.\n        input_shape (tuple): input shape to the model. If specified, the model takes a tensor with this shape as the only positional argument.\n        args (list): list of positional arguments to the model.\n        kwargs (dict): dictionary of keyword arguments to the model.\n        print_profile (bool, optional): whether to print the model profile. Defaults to True.\n        detailed (bool, optional): whether to print the detailed model profile. Defaults to True.\n        module_depth (int, optional): the depth into the nested modules. Defaults to -1 (the inner most modules).\n        top_modules (int, optional): the number of top modules to print in the aggregated profile. Defaults to 3.\n        warm_up (int, optional): the number of warm-up steps before measuring the latency of each module. Defaults to 1.\n        as_string (bool, optional): whether to print the output as string. Defaults to True.\n        output_file (str, optional): path to the output file. If None, the profiler prints to stdout.\n        ignore_modules ([type], optional): the list of modules to ignore during profiling. Defaults to None.\n\n    Returns:\n        The number of floating-point operations, multiply-accumulate operations (MACs), and parameters in the model.\n    \"\"\"\n    assert isinstance(model, nn.Module), \"model must be a PyTorch module\"\n    prof = FlopsProfiler(model)\n    model.eval()\n\n    if input_shape is not None:\n        assert type(input_shape) is tuple, \"input_shape must be a tuple\"\n        assert len(input_shape) >= 1, \"input_shape must have at least one element\"\n        try:\n            input = torch.ones(()).new_empty(\n                (*input_shape, ),\n                dtype=next(model.parameters()).dtype,\n                device=next(model.parameters()).device,\n            )\n        except StopIteration:\n            input = torch.ones(()).new_empty((*input_shape, ))\n\n        args = [input]\n    assert (len(args) > 0) or (len(kwargs) > 0), \"args and/or kwargs must be specified if input_shape is None\"\n\n    logger.info(\"Flops profiler warming-up...\")\n    for _ in range(warm_up):\n        if kwargs:\n            if mode == 'forward':\n                _ = model(*args, **kwargs)\n            if mode == 'generate':\n                _ = model.generate(*args, **kwargs)\n        else:\n            if mode == 'forward':\n                _ = model(*args)\n            if mode == 'generate':\n                _ = model.generate(*args)\n    prof.start_profile(ignore_list=ignore_modules)\n\n    if kwargs:\n        if mode == 'forward':\n            _ = model(*args, **kwargs)\n        if mode == 'generate':\n            _ = model.generate(*args, **kwargs)\n    else:\n        if mode == 'forward':\n            _ = model(*args)\n        if mode == 'generate':\n            _ = model.generate(*args)\n\n    flops = prof.get_total_flops()\n    macs = prof.get_total_macs()\n    params = prof.get_total_params()\n    if print_profile:\n        prof.print_model_profile(profile_step=warm_up,\n                                 module_depth=module_depth,\n                                 top_modules=top_modules,\n                                 detailed=detailed,\n                                 output_file=output_file)\n\n    prof.end_profile()\n    if as_string:\n        return number_to_string(flops), macs_to_string(macs), params_to_string(params)\n\n    return flops, macs, params\n", "deepspeed/profiling/flops_profiler/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .profiler import *\n", "deepspeed/launcher/launch.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\"\"\"\nDeepSpeed launcher, this is similar to torch's distributed.launch but supports\nadditional features such as arbitrary gpu exclusion.\n\ndeepspeed.launcher.launch is intended to be run on a single worker node and\nwill spawn several worker sub-processes depending on how many devices/ranks\nare on the worker.\n\"\"\"\n\nimport sys\nimport subprocess\nimport os\nimport json\nimport base64\nimport time\nimport signal\nimport psutil\nfrom collections import defaultdict\nfrom typing import Dict\nfrom argparse import ArgumentParser, REMAINDER\nfrom deepspeed.accelerator import get_accelerator\nfrom ..constants import TORCH_DISTRIBUTED_DEFAULT_PORT\nfrom ..nebula.constants import DLTS_POD_ENV_PATH\nfrom ..utils import logger, get_numactl_cmd\nfrom ..elasticity import is_torch_elastic_compatible\nfrom .constants import ELASTIC_TRAINING_ID_DEFAULT\n\nPID_FILE_BASEPATH = \"/tmp\"\n\n\ndef parse_args():\n    parser = ArgumentParser(description=\"DeepSpeed distributed training launch\"\n                            \" utility that creates multiple distributed\"\n                            \" processes on a single node\")\n\n    # Optional arguments for the launch helper\n    parser.add_argument(\"--node_rank\",\n                        type=int,\n                        default=0,\n                        help=\"The rank of the node for multi-node distributed \"\n                        \"training\")\n    parser.add_argument(\"--master_addr\",\n                        default=\"127.0.0.1\",\n                        type=str,\n                        help=\"Master node (rank 0)'s address, should be either\"\n                        \" the IP address or the hostname of node 0, for\"\n                        \" single node multi-proc training, the\"\n                        \" --master_addr can simply be 127.0.0.1\")\n    parser.add_argument(\"--master_port\",\n                        default=TORCH_DISTRIBUTED_DEFAULT_PORT,\n                        type=int,\n                        help=\"Master node (rank 0)'s free port that needs to \"\n                        \"be used for communication during distributed \"\n                        \"training\")\n    parser.add_argument(\"--world_info\", default=\"None\", type=str, help=\"world info base64 encoded dictionary\")\n\n    parser.add_argument(\"--module\",\n                        action=\"store_true\",\n                        help=\"Change each process to interpret the launch \"\n                        \"script as a Python module, executing with the same \"\n                        \"behavior as 'python -m'.\")\n\n    parser.add_argument(\"--no_python\",\n                        action=\"store_true\",\n                        help=\"Skip prepending the training script with \"\n                        \"'python' - just execute it directly.\")\n\n    parser.add_argument(\"--enable_elastic_training\", action=\"store_true\", help=\"Enable elastic training support.\")\n\n    parser.add_argument(\"--min_elastic_nodes\", type=int, default=-1, help=\"Min number of nodes in elastic training.\")\n\n    parser.add_argument(\"--max_elastic_nodes\", type=int, default=-1, help=\"Max number of nodes in elastic training.\")\n\n    parser.add_argument(\"--no_local_rank\",\n                        action=\"store_true\",\n                        help=\"Do not pass local_rank as an argument when calling \"\n                        \"the user's training script.\")\n\n    parser.add_argument(\"--save_pid\",\n                        type=int,\n                        default=0,\n                        help=\"main launching process pid, for internal pid tracking\")\n\n    parser.add_argument(\"--enable_each_rank_log\",\n                        default=\"None\",\n                        type=str,\n                        help=\"redirect the stdout and stderr from each rank into different log files\")\n\n    parser.add_argument(\"--bind_cores_to_rank\",\n                        action=\"store_true\",\n                        help=\"Bind each rank to different cores of the host. \"\n                        \"This improves host efficiency especially for CPU backend\")\n\n    parser.add_argument(\"--bind_core_list\",\n                        type=str,\n                        default=None,\n                        help=\"List of cores to bind to with comma separated list of \"\n                        \"numbers and range. i.e. 1,3-5,7 => [1,3,4,5,7].  When not \"\n                        \"specified, all cores on system would be used rank binding\")\n\n    # positional\n    parser.add_argument(\"training_script\",\n                        type=str,\n                        help=\"The full path to the single GPU training \"\n                        \"program/script to be launched in parallel, \"\n                        \"followed by all the arguments for the \"\n                        \"training script\")\n\n    # rest from the training program\n    parser.add_argument('training_script_args', nargs=REMAINDER)\n    return parser.parse_args()\n\n\n# Adapted from https://psutil.readthedocs.io/en/latest/#kill-process-tree\ndef terminate_process_tree(pid):\n    process = psutil.Process(pid)\n    children = process.children(recursive=True)\n    children.append(process)\n    for child in children:\n        try:\n            child.terminate()\n        except psutil.NoSuchProcess:\n            pass\n    gone, alive = psutil.wait_procs(children, timeout=30)\n    for p in alive:\n        p.kill()\n\n\ndef main():\n    args = parse_args()\n    current_env = os.environ.copy()\n\n    for k in current_env.keys():\n        if \"NCCL\" in k:\n            logger.info(f\"{args.node_rank} {k}={current_env[k]}\")\n\n    if args.world_info == \"None\":\n        raise ValueError(\"world_info can not be None\")\n    world_info = base64.urlsafe_b64decode(args.world_info)\n    world_info = json.loads(world_info)\n\n    logger.info(f\"WORLD INFO DICT: {world_info}\")\n    node_list = list(world_info.keys())\n    args.nnodes = len(node_list)\n    local_node = node_list[args.node_rank]\n    local_accelerator_ids = world_info[local_node]\n    num_local_procs = len(local_accelerator_ids)\n    logger.info(f\"nnodes={args.nnodes}, num_local_procs={num_local_procs}, node_rank={args.node_rank}\")\n\n    global_rank_mapping = defaultdict(list)\n    curr_global_rank = 0\n    dist_world_size = 0\n    for node_id in node_list:\n        gids = world_info[node_id]\n        dist_world_size += len(gids)\n        for gid in gids:\n            global_rank_mapping[node_id].append(curr_global_rank)\n            curr_global_rank += 1\n    logger.info(f\"global_rank_mapping={global_rank_mapping}\")\n    logger.info(f\"dist_world_size={dist_world_size}\")\n\n    get_accelerator().set_visible_devices_envs(current_env, local_accelerator_ids)\n    for env in get_accelerator().visible_devices_envs():\n        logger.info(f\"Setting {env}={current_env[env]}\")\n\n    # set PyTorch distributed related environmental variables\n    current_env[\"MASTER_ADDR\"] = args.master_addr\n    current_env[\"MASTER_PORT\"] = str(args.master_port)\n    current_env[\"WORLD_SIZE\"] = str(dist_world_size)\n    current_env[\"CROSS_RANK\"] = str(args.node_rank)\n    current_env[\"CROSS_SIZE\"] = str(args.nnodes)\n    current_env[\"LOCAL_SIZE\"] = str(num_local_procs)\n\n    if args.save_pid:\n        print(f\"launcher pid: {os.getpid()}\")\n\n    pid_file = None\n    if args.save_pid:\n        launcher_pid = os.getpid()\n        pid_file = os.path.join(PID_FILE_BASEPATH, f\"{args.save_pid}.deepspeed\")\n        assert not os.path.isfile(pid_file), \"pid file exists but shouldn't\"\n        with open(pid_file, 'w') as fd:\n            fd.write(f\"{launcher_pid}\")\n\n    if not is_torch_elastic_compatible():\n        if args.enable_elastic_training:\n            logger.info(f\"Disabling elastic training support as \\\n                    PyTorch version should be greater than 1.11.x\")\n            args.enable_elastic_training = False\n\n    if os.path.exists(DLTS_POD_ENV_PATH):\n        with open(DLTS_POD_ENV_PATH) as file:\n            lines = file.readlines()\n            lines = [line.rstrip() for line in lines]\n            for line in lines:\n                if line.startswith('export FC_TASKROLE_NAME') or line.startswith('export FC_TASK_INDEX'):\n                    key_val = line.split()[1]\n                    key, val = key_val.split('=')\n                    current_env[key] = val\n\n    processes = []\n    cmd = []\n\n    if not args.enable_elastic_training:\n        if args.enable_each_rank_log != \"None\":\n            # prepare the log path and the file name prefix\n            if os.path.isfile(args.enable_each_rank_log):\n                raise ValueError(f\"{args.enable_each_rank_log} should not be a file, it should be a directory.\")\n            if not os.path.exists(args.enable_each_rank_log):\n                try:\n                    os.makedirs(args.enable_each_rank_log)\n                except Exception as e:\n                    print(e)\n                    raise ValueError(f\"unable to create directory {args.enable_each_rank_log} for each rank log.\")\n            log_name_prefix = time.strftime(\"%Y%m%d%H%M%S\", time.localtime())\n\n        for local_proc in range(0, num_local_procs):\n            # each process's rank\n            dist_rank = global_rank_mapping[local_node][local_proc]\n            local_rank = dist_rank % num_local_procs\n            current_env[\"RANK\"] = str(dist_rank)\n            current_env[\"LOCAL_RANK\"] = str(local_rank)\n\n            # spawn the processes\n            cmd = []\n            if args.bind_cores_to_rank:\n                cores_per_rank, numactl_cmd = get_numactl_cmd(args.bind_core_list, num_local_procs, local_rank)\n                current_env[\"OMP_NUM_THREADS\"] = f\"{cores_per_rank}\"\n                cmd = cmd + numactl_cmd\n            if not args.no_python:\n                cmd.append(sys.executable)\n                cmd.append(\"-u\")\n                if args.module:\n                    cmd.append(\"-m\")\n            else:\n                if args.module:\n                    raise ValueError(\"Don't use both the '--no_python' flag\"\n                                     \" and the '--module' flag at the same time.\")\n            cmd.append(args.training_script)\n            # A user may not want to pass local_rank as a keyword arg so we make this optional.\n            if not args.no_local_rank:\n                cmd.append(f\"--local_rank={local_rank}\")\n            cmd += args.training_script_args\n\n            if args.enable_each_rank_log != \"None\":\n                log_file = os.path.join(args.enable_each_rank_log, f\"{log_name_prefix}_rank{dist_rank}.log\")\n                log_fd = open(log_file, 'w')\n                process = subprocess.Popen(cmd, env=current_env, stdout=log_fd, stderr=log_fd)\n            else:\n                process = subprocess.Popen(cmd, env=current_env)\n            # logs the command from processes\n            logger.info(f\"process {process.pid} spawned with command: {cmd}\")\n            processes.append(process)\n    else:\n        from ..elasticity import DSElasticAgent\n        from torch.distributed.elastic.rendezvous import RendezvousParameters\n        from torch.distributed.elastic.agent.server.api import WorkerSpec\n        import torch.distributed.elastic.rendezvous.registry as rdzv_registry\n        from torch.distributed.elastic.multiprocessing import Std\n\n        if args.min_elastic_nodes == -1:\n            args.min_elastic_nodes = 1\n        if args.max_elastic_nodes == -1:\n            args.max_elastic_nodes = args.nnodes\n        assert args.max_elastic_nodes > 0 and args.min_elastic_nodes > 0, \"Max and Min nodes should be positive\"\n\n        current_env[\"NCCL_ASYNC_ERROR_HANDLING\"] = str(1)\n\n        # Get config and arguments\n        cmd = []\n        if not args.no_python:\n            cmd = [sys.executable, \"-u\"]\n            if args.module:\n                cmd.append(\"-m\")\n        else:\n            if args.module:\n                raise ValueError(\"Don't use both the '--no_python' flag\"\n                                 \" and the '--module' flag at the same time.\")\n        cmd.append(args.training_script)\n        cmd += args.training_script_args\n        cmd_args = cmd[1:]\n\n        rdzv_configs: Dict[str, str] = {'timeout': 100}\n        run_id = os.environ.get(\"ELASTIC_RUN_ID\", ELASTIC_TRAINING_ID_DEFAULT)\n\n        # Creating config for rendezvous class\n        rdzv_parameters = RendezvousParameters(backend='c10d',\n                                               endpoint=args.master_addr + \":\" + str(args.master_port),\n                                               run_id=run_id,\n                                               min_nodes=args.min_elastic_nodes,\n                                               max_nodes=args.max_elastic_nodes,\n                                               **rdzv_configs)\n\n        spec = WorkerSpec(\n            role='trainer',\n            local_world_size=num_local_procs,\n            entrypoint=cmd[0],\n            args=cmd[1:],\n            rdzv_handler=rdzv_registry.get_rendezvous_handler(rdzv_parameters),\n            max_restarts=100,\n            monitor_interval=5,\n            redirects=Std.from_str(\"0\"),\n            tee=Std.from_str(\"0\"),\n            master_addr=None,\n            master_port=None,\n        )\n        agent = DSElasticAgent(spec, current_env)\n        agent.run()\n\n    sig_names = {2: \"SIGINT\", 15: \"SIGTERM\"}\n    last_return_code = None\n\n    def sigkill_handler(signum, frame):\n        for process in processes:\n            logger.info(f\"Killing subprocess {process.pid}\")\n            try:\n                terminate_process_tree(process.pid)\n            except Exception:\n                pass\n        if last_return_code is not None:\n            logger.error(f\"{cmd} exits with return code = {last_return_code}\")\n            sys.exit(last_return_code)\n        if signum in sig_names:\n            logger.info(f\"Main process received {sig_names[signum]}, exiting\")\n        if args.save_pid:\n            if os.path.isfile(pid_file):\n                os.remove(pid_file)\n        sys.exit(1)\n\n    # pass SIGINT/SIGTERM to children if the parent is being terminated\n    signal.signal(signal.SIGINT, sigkill_handler)\n    signal.signal(signal.SIGTERM, sigkill_handler)\n\n    alive_processes = set(processes)\n    while len(alive_processes):\n        finished_processes = []\n        for process in alive_processes:\n            if process.poll() is None:\n                # the process is still running\n                continue\n            else:\n                if process.returncode != 0:\n                    last_return_code = process.returncode  # for sigkill_handler\n                    sigkill_handler(signal.SIGTERM, None)  # not coming back\n                else:\n                    # exited cleanly\n                    logger.info(f\"Process {process.pid} exits successfully.\")\n                    finished_processes.append(process)\n        alive_processes = set(alive_processes) - set(finished_processes)\n\n        time.sleep(1)\n\n\nif __name__ == \"__main__\":\n    main()\n", "deepspeed/launcher/launcher_helper.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport os\nimport sys\nimport argparse\nimport subprocess\nfrom deepspeed.utils import logger\nfrom deepspeed.launcher.constants import MPICH_LAUNCHER\n\n\ndef parse_args(args=None):\n    parser = argparse.ArgumentParser(description=\"DeepSpeed launcher helper to map environment variables for\"\n                                     \"multi-node/multi-gpu training jobs.\",\n                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument(\"--launcher\",\n                        default=MPICH_LAUNCHER,\n                        type=str,\n                        help=\"(optional) choose launcher backend for multi-node \"\n                        \"training. Options currently include MPICH.\")\n\n    parser.add_argument(\"--module\",\n                        action=\"store_true\",\n                        help=\"Change each process to interpret the launch \"\n                        \"script as a Python module, executing with the same \"\n                        \"behavior as 'python -m'.\")\n\n    parser.add_argument(\"--no_python\",\n                        action=\"store_true\",\n                        help=\"Skip prepending the training script with \"\n                        \"'python' - just execute it directly.\")\n\n    parser.add_argument(\"user_script\", type=str, help=\"User script to launch, followed by any required \"\n                        \"arguments.\")\n\n    parser.add_argument('user_args', nargs=argparse.REMAINDER)\n\n    parser.add_argument(\"--bind_cores_to_rank\",\n                        action=\"store_true\",\n                        help=\"Bind each rank to different cores of the host\")\n\n    parser.add_argument(\"--bind_core_list\",\n                        type=str,\n                        default=None,\n                        help=\"List of cores to bind to with comma separated list of \"\n                        \"numbers and range. i.e. 1,3-5,7 => [1,3,4,5,7].  When not \"\n                        \"specified, all cores on system would be used rank binding\")\n\n    return parser.parse_args(args=args)\n\n\ndef env_mapping(env, rank_name_list=None, local_rank_name_list=None):\n    rank = None\n    for rank_name in rank_name_list:\n        if rank_name in env:\n            if rank == None:\n                rank = env.get(rank_name)\n            elif rank != env.get(rank_name):\n                raise EnvironmentError(f\"rank number doesn't match!\")\n    if rank == None:\n        raise EnvironmentError(f\"rank number is not in current env!\")\n    env['RANK'] = rank\n\n    local_rank = None\n    for local_rank_name in local_rank_name_list:\n        if local_rank_name in env:\n            if local_rank == None:\n                local_rank = env.get(local_rank_name)\n            elif local_rank != env.get(local_rank_name):\n                raise EnvironmentError(f\"local_rank number doesn't match!\")\n    if local_rank == None:\n        raise EnvironmentError(f\"rank number is not in current env!\")\n    env['LOCAL_RANK'] = local_rank\n\n    return env\n\n\ndef main(args=None):\n    args = parse_args(args)\n\n    env = os.environ.copy()\n\n    args.launcher = args.launcher.lower()\n    if args.launcher == MPICH_LAUNCHER:\n        rank_name_list = [\"PMIX_RANK\"] + [\"PMI_RANK\"]\n        local_rank_name_list = [\"PALS_LOCAL_RANKID\"] + [\"MPI_LOCALRANKID\"]\n        env = env_mapping(env, rank_name_list=rank_name_list, local_rank_name_list=local_rank_name_list)\n    else:\n        raise NotImplementedError(f\"Unknown launcher {args.launcher}\")\n\n    python_exec = []\n    if not args.no_python:\n        python_exec += [sys.executable, \"-u\"]\n        if args.module:\n            python_exec.append(\"-m\")\n    cmd = python_exec + [args.user_script] + args.user_args\n\n    logger.info(f\"launcher_helper cmd = {' '.join(cmd)}\")\n\n    result = subprocess.Popen(cmd, env=env, close_fds=False)\n    result.wait()\n\n\nif __name__ == \"__main__\":\n    main()\n", "deepspeed/launcher/constants.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nPDSH_LAUNCHER = 'pdsh'\nPDSH_MAX_FAN_OUT = 1024\n\nOPENMPI_LAUNCHER = 'openmpi'\nMPICH_LAUNCHER = 'mpich'\nIMPI_LAUNCHER = 'impi'\nSLURM_LAUNCHER = 'slurm'\nMVAPICH_LAUNCHER = 'mvapich'\nMVAPICH_TMP_HOSTFILE = '/tmp/deepspeed_mvapich_hostfile'\n\nELASTIC_TRAINING_ID_DEFAULT = \"123456789\"\n", "deepspeed/launcher/runner.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\"\"\"\nDeepSpeed runner is the main front-end to launching multi-worker\ntraining jobs with DeepSpeed. By default this uses pdsh to parallel\nssh into multiple worker nodes and launch all the necessary processes\nper rank for training.\n\"\"\"\n\nimport os\nimport re\nimport sys\nimport json\nimport base64\nimport argparse\nimport subprocess\nimport collections\nfrom copy import deepcopy\nimport signal\nimport time\n\nfrom .multinode_runner import PDSHRunner, OpenMPIRunner, MVAPICHRunner, SlurmRunner, MPICHRunner, IMPIRunner\nfrom .constants import PDSH_LAUNCHER, OPENMPI_LAUNCHER, MVAPICH_LAUNCHER, SLURM_LAUNCHER, MPICH_LAUNCHER, IMPI_LAUNCHER\nfrom ..constants import TORCH_DISTRIBUTED_DEFAULT_PORT\nfrom ..nebula.constants import NEBULA_EXPORT_ENVS\nfrom ..utils import logger\n\nfrom ..autotuning import Autotuner\nfrom deepspeed.accelerator import get_accelerator\n\nDLTS_HOSTFILE = \"/job/hostfile\"\nEXPORT_ENVS = ['MLFLOW', 'PYTHON', 'MV2', 'UCX']\nEXPORT_ENVS += NEBULA_EXPORT_ENVS\nDEEPSPEED_ENVIRONMENT_NAME = os.getenv(\"DS_ENV_FILE\", \".deepspeed_env\")\nDEEPSPEED_ENVIRONMENT_PATHS = [os.path.expanduser(\"~\"), '.']\nPDSH_MAX_FAN_OUT = 1024\n\n# On AISC compute, each node sets environment variables independently, want to prevent\n# exporting rank-0 env variables in case of heterogeneous compute.\nEXCLUDE_ENVS = {'AISC_JOB_NAME': ['NCCL_IB_HCA', 'UCX_NET_DEVICES']}\n\n\ndef parse_args(args=None):\n    parser = argparse.ArgumentParser(description=\"DeepSpeed runner to help launch distributed \"\n                                     \"multi-node/multi-gpu training jobs.\",\n                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument(\"-H\",\n                        \"--hostfile\",\n                        type=str,\n                        default=DLTS_HOSTFILE,\n                        help=\"Hostfile path (in MPI style) that defines the \"\n                        \"resource pool available to the job (e.g., \"\n                        \"worker-0 slots=4)\")\n\n    parser.add_argument(\"-i\",\n                        \"--include\",\n                        type=str,\n                        default=\"\",\n                        help='''Specify hardware resources to use during execution.\n                        String format is\n                                NODE_SPEC[@NODE_SPEC ...],\n                        where\n                                NODE_SPEC=NAME[:SLOT[,SLOT ...]].\n                        If :SLOT is omitted, include all slots on that host.\n                        Example: -i \"worker-0@worker-1:0,2\" will use all slots\n                        on worker-0 and slots [0, 2] on worker-1.\n                        ''')\n\n    parser.add_argument(\"-e\",\n                        \"--exclude\",\n                        type=str,\n                        default=\"\",\n                        help='''Specify hardware resources to NOT use during execution.\n                        Mutually exclusive with --include. Resource formatting\n                        is the same as --include.\n                        Example: -e \"worker-1:0\" will use all available\n                        resources except slot 0 on worker-1.\n                        ''')\n\n    parser.add_argument(\"--num_nodes\",\n                        type=int,\n                        default=-1,\n                        help=\"Total number of worker nodes to run on, this will use \"\n                        \"the top N hosts from the given hostfile.\")\n\n    parser.add_argument(\"--min_elastic_nodes\",\n                        type=int,\n                        default=-1,\n                        help=\"Minimum number of nodes to run elastic training on. \"\n                        \"Default is 1 when elastic training is enabled\")\n\n    parser.add_argument(\"--max_elastic_nodes\",\n                        type=int,\n                        default=-1,\n                        help=\"Maximum number of nodes to run elastic training on. \"\n                        \"Default is num_nodes when elastic training is enabled\")\n\n    parser.add_argument(\"--num_gpus\",\n                        \"--num_accelerators\",\n                        type=int,\n                        default=-1,\n                        help=\"Max number of GPUs to use on each node, will use \"\n                        \"[0:N) GPU ids on each node.\")\n\n    parser.add_argument(\"--master_port\",\n                        default=TORCH_DISTRIBUTED_DEFAULT_PORT,\n                        type=int,\n                        help=\"(optional) Port used by PyTorch distributed for \"\n                        \"communication during training.\")\n\n    parser.add_argument(\"--master_addr\",\n                        default=\"\",\n                        type=str,\n                        help=\"(optional) IP address of node 0, will be \"\n                        \"inferred via 'hostname -I' if not specified.\")\n\n    parser.add_argument(\"--launcher\",\n                        default=PDSH_LAUNCHER,\n                        type=str,\n                        help=\"(optional) choose launcher backend for multi-node \"\n                        \"training. Options currently include PDSH, OpenMPI, MVAPICH, SLURM, MPICH, IMPI.\")\n\n    parser.add_argument(\"--launcher_args\",\n                        default=\"\",\n                        type=str,\n                        help=\"(optional) pass launcher specific arguments as a \"\n                        \"single quoted argument.\")\n\n    parser.add_argument(\"--module\",\n                        action=\"store_true\",\n                        help=\"Change each process to interpret the launch \"\n                        \"script as a Python module, executing with the same \"\n                        \"behavior as 'python -m'.\")\n\n    parser.add_argument(\"--no_python\",\n                        action=\"store_true\",\n                        help=\"Skip prepending the training script with \"\n                        \"'python' - just execute it directly.\")\n\n    parser.add_argument(\"--no_local_rank\",\n                        action=\"store_true\",\n                        help=\"Do not pass local_rank as an argument when calling \"\n                        \"the user's training script.\")\n\n    parser.add_argument(\"--no_ssh_check\",\n                        action=\"store_true\",\n                        help=\"Do not perform ssh check in multi-node launcher model\")\n\n    parser.add_argument(\"--force_multi\",\n                        action=\"store_true\",\n                        help=\"Force multi-node launcher mode, helps in cases where user \"\n                        \"wants to launch on single remote node.\")\n\n    parser.add_argument(\"--save_pid\",\n                        action=\"store_true\",\n                        help=\"Save file containing launcher process id (pid) at /tmp/<main-pid>.ds, \"\n                        \"where <main-pid> is the pid of the first process that invoked `deepspeed`. \"\n                        \"Useful when launching deepspeed processes programmatically.\")\n\n    parser.add_argument(\"--enable_each_rank_log\",\n                        default=\"None\",\n                        type=str,\n                        help=\"redirect the stdout and stderr from each rank into different log files\")\n\n    parser.add_argument(\"--autotuning\",\n                        default=\"\",\n                        choices=[\"tune\", \"run\"],\n                        type=str,\n                        help=\"Run DeepSpeed autotuner to discover optimal configuration parameters \"\n                        \"before running job.\")\n\n    parser.add_argument(\"--elastic_training\",\n                        action=\"store_true\",\n                        help=\"Enable elastic training support in DeepSpeed.\")\n\n    parser.add_argument(\"user_script\", type=str, help=\"User script to launch, followed by any required \"\n                        \"arguments.\")\n\n    parser.add_argument('user_args', nargs=argparse.REMAINDER)\n\n    parser.add_argument(\"--bind_cores_to_rank\",\n                        action=\"store_true\",\n                        help=\"Bind each rank to different cores of the host\")\n\n    parser.add_argument(\"--bind_core_list\",\n                        type=str,\n                        default=None,\n                        help=\"List of cores to bind to with comma separated list of \"\n                        \"numbers and range. i.e. 1,3-5,7 => [1,3,4,5,7].  When not \"\n                        \"specified, all cores on system would be used rank binding\")\n\n    parser.add_argument(\"--ssh_port\", type=int, default=None, help=\"SSH port to use for remote connections\")\n\n    return parser.parse_args(args=args)\n\n\ndef fetch_hostfile(hostfile_path):\n    if not os.path.isfile(hostfile_path):\n        logger.warning(\"Unable to find hostfile, will proceed with training \"\n                       \"with local resources only.\")\n        return None\n\n    # e.g., worker-0 slots=16\n    with open(hostfile_path, 'r') as fd:\n        hostfile_text = fd.readlines()\n\n    return _parse_hostfile(hostfile_text)\n\n\ndef _parse_hostfile(hostfile_lines):\n    # Regex matches one or more non-whitespace characters (\\S+) at the start of\n    # the line, followed by one or more whitespace characters (\\s+), followed\n    # by the string \"slots=\", followed by one or more digits (\\d+).\n    pattern = r'^(\\S+)\\s+slots=(\\d+)'\n\n    resource_pool = collections.OrderedDict()\n\n    for line in hostfile_lines:\n        line = line.strip()\n        match = re.search(pattern, line)\n        if line.startswith(\"#\") or line == \"\":\n            # hostfile comment or empty line, ignore\n            continue\n        elif match:\n            host = match.group(1)\n            num_slots = int(match.group(2))\n            if host in resource_pool:\n                logger.error(f\"Bad hostfile text: {hostfile_lines}\")\n                raise ValueError(f\"Hostfile contains multiple entries for {host}, unable to proceed with launching\")\n            resource_pool[host] = num_slots\n        else:\n            logger.error(f\"Bad hostfile text: {hostfile_lines}\")\n            raise ValueError(f\"Hostfile contains a bad entry: {line}, unable to proceed with launching\")\n\n    if len(resource_pool) == 0:\n        logger.error(f\"Bad hostfile text: {hostfile_lines}\")\n        raise ValueError(\"Hostfile is empty or not formatted correctly, unable to proceed with launching.\")\n\n    return resource_pool\n\n\ndef _stable_remove_duplicates(data):\n    # Create a new list in the same order as original but with duplicates\n    # removed, should never be more than ~16 elements so simple is best\n    new_list = []\n    for x in data:\n        if x not in new_list:\n            new_list.append(x)\n    return new_list\n\n\ndef parse_resource_filter(host_info, include_str=\"\", exclude_str=\"\"):\n    '''Parse an inclusion or exclusion string and filter a hostfile dictionary.\n\n    String format is NODE_SPEC[@NODE_SPEC ...], where\n        NODE_SPEC = NAME[:SLOT[,SLOT ...]].\n    If :SLOT is omitted, include/exclude all slots on that host.\n\n    Examples:\n        include_str=\"worker-0@worker-1:0,2\" will use all slots on worker-0 and\n          slots [0, 2] on worker-1.\n        exclude_str=\"worker-1:0\" will use all available resources except\n          slot 0 on worker-1.\n    '''\n\n    # Constants that define our syntax\n    NODE_SEP = '@'\n    SLOT_LIST_START = ':'\n    SLOT_SEP = ','\n\n    # Ensure include/exclude are mutually exclusive\n    if (include_str != \"\") and (exclude_str != \"\"):\n        raise ValueError('include_str and exclude_str are mutually exclusive.')\n\n    # no-op\n    if (include_str == \"\") and (exclude_str == \"\"):\n        return host_info\n\n    # Either build from scratch or remove items\n    filtered_hosts = dict()\n    if include_str:\n        parse_str = include_str\n    if exclude_str != \"\":\n        filtered_hosts = deepcopy(host_info)\n        parse_str = exclude_str\n\n    # foreach node in the list\n    for node_config in parse_str.split(NODE_SEP):\n        # Node can either be alone or node:slot,slot,slot\n        if SLOT_LIST_START in node_config:\n            hostname, slots = node_config.split(SLOT_LIST_START)\n            slots = [int(x) for x in slots.split(SLOT_SEP)]\n\n            # sanity checks\n            if hostname not in host_info:\n                raise ValueError(f\"Hostname '{hostname}' not found in hostfile\")\n            for slot in slots:\n                if slot not in host_info[hostname]:\n                    raise ValueError(f\"No slot '{slot}' specified on host '{hostname}'\")\n\n            # If include string, build the list from here\n            if include_str:\n                filtered_hosts[hostname] = slots\n            elif exclude_str:\n                for slot in slots:\n                    logger.info(f'removing {slot} from {hostname}')\n                    filtered_hosts[hostname].remove(slot)\n\n        # User just specified the whole node\n        else:\n            hostname = node_config\n            # sanity check hostname\n            if hostname not in host_info:\n                raise ValueError(f\"Hostname '{hostname}' not found in hostfile\")\n\n            if include_str:\n                filtered_hosts[hostname] = host_info[hostname]\n            elif exclude_str:\n                filtered_hosts[hostname] = []\n\n    # Post-processing to remove duplicates and empty nodes\n    del_keys = []\n    for hostname in filtered_hosts:\n        # Remove duplicates\n        filtered_hosts[hostname] = _stable_remove_duplicates(filtered_hosts[hostname])\n        # Remove empty hosts\n        if len(filtered_hosts[hostname]) == 0:\n            del_keys.append(hostname)\n    for name in del_keys:\n        del filtered_hosts[name]\n\n    # Lastly, go over filtered_hosts and convert to a OrderedDict() to ensure\n    # we map ranks to nodes correctly by maintaining host_info ordering.\n    ordered_hosts = collections.OrderedDict()\n    for host in host_info:\n        if host in filtered_hosts:\n            ordered_hosts[host] = filtered_hosts[host]\n\n    return ordered_hosts\n\n\ndef parse_inclusion_exclusion(resource_pool, inclusion, exclusion):\n    active_resources = collections.OrderedDict()\n    for hostname, slots in resource_pool.items():\n        active_resources[hostname] = list(range(slots))\n\n    return parse_resource_filter(active_resources, include_str=inclusion, exclude_str=exclusion)\n\n\ndef encode_world_info(world_info):\n    world_info_json = json.dumps(world_info).encode('utf-8')\n    world_info_base64 = base64.urlsafe_b64encode(world_info_json).decode('utf-8')\n    return world_info_base64\n\n\ndef run_autotuning(args, active_resources):\n    tuner = Autotuner(args, active_resources)\n    logger.info(\"[Start] Running autotuning\")\n\n    tuner.tune()\n    tuner.print_tuning_results()\n\n    logger.info(\"[End] Running autotuning\")\n    tuner.write_optimal_config()\n\n    if args.autotuning == \"run\":\n        tuner.run_after_tuning()\n\n\ndef parse_num_nodes(str_num_nodes: str, elastic_training: bool):\n    node_list = str_num_nodes.split(\":\")\n\n    if len(node_list) == 1:\n        min_nodes, max_nodes = int(node_list[0]), -1\n    elif len(node_list) == 2 and elastic_training:\n        min_nodes, max_nodes = int(node_list[0]), int(node_list[1])\n    elif len(node_list) == 2 and not elastic_training:\n        raise RuntimeError(\"MIN:MAX format is only supported in elastic training\")\n    else:\n        raise RuntimeError(\"num_nodes {} is not in MIN:MAX format\".format(str_num_nodes))\n\n    return min_nodes, max_nodes\n\n\ndef main(args=None):\n    args = parse_args(args)\n\n    if args.elastic_training:\n        assert args.master_addr != \"\", \"Master Addr is required when elastic training is enabled\"\n\n    resource_pool = fetch_hostfile(args.hostfile)\n\n    # respect CUDA_VISIBLE_DEVICES for a single node and no explicit resource filters\n    cuda_visible_devices = os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"\")\n    if not resource_pool and len(cuda_visible_devices):\n        detected_str = f\"Detected CUDA_VISIBLE_DEVICES={cuda_visible_devices}\"\n        if len(args.include) or len(args.exclude) or args.num_nodes > 1 or args.num_gpus > 0:\n            print(\n                f\"{detected_str} but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.\"\n            )\n        else:\n            args.include = f\"localhost:{cuda_visible_devices}\"\n            print(f\"{detected_str}: setting --include={args.include}\")\n        del os.environ[\"CUDA_VISIBLE_DEVICES\"]\n\n    if args.num_nodes >= 0 or args.num_gpus >= 0:\n        if args.include != \"\" or args.exclude != \"\":\n            raise ValueError(\"Cannot specify num_nodes/gpus with include/exclude\")\n\n    multi_node_exec = True\n    if not resource_pool:\n        resource_pool = {}\n        device_count = get_accelerator().device_count()\n        if device_count == 0:\n            raise RuntimeError(\"Unable to proceed, no GPU resources available\")\n        resource_pool['localhost'] = device_count\n        args.master_addr = \"127.0.0.1\"\n        multi_node_exec = False\n\n    if not multi_node_exec and args.num_nodes > 1:\n        raise ValueError(\"Num nodes is >1 but no extra nodes available via hostfile\")\n\n    active_resources = parse_inclusion_exclusion(resource_pool, args.include, args.exclude)\n    env = os.environ.copy()\n\n    # validate that passwordless-ssh is workly properly with this hostfile\n    if multi_node_exec and not args.no_ssh_check:\n        first_host = list(active_resources.keys())[0]\n        try:\n            ssh_check_cmd = \"ssh -o PasswordAuthentication=no \"\n            if args.ssh_port is not None:\n                ssh_check_cmd += f\"-p {args.ssh_port} \"\n            ssh_check_cmd += f\"{first_host} hostname\"\n            subprocess.check_call(ssh_check_cmd, stderr=subprocess.DEVNULL, stdout=subprocess.DEVNULL, shell=True)\n        except subprocess.CalledProcessError:\n            raise RuntimeError(\n                f\"Using hostfile at {args.hostfile} but host={first_host} was not reachable via ssh. If you are running with a single node please remove {args.hostfile} or setup passwordless ssh.\"\n            )\n\n    if not args.master_addr:\n        assert multi_node_exec\n        first_host = list(active_resources.keys())[0]\n        ssh_check_cmd = \"ssh \"\n        if args.ssh_port is not None:\n            ssh_check_cmd += f\" -p {args.ssh_port}\"\n        ssh_check_cmd += f\" {first_host} hostname -I\"\n        hostname_cmd = [ssh_check_cmd]\n        try:\n            result = subprocess.check_output(hostname_cmd, shell=True)\n        except subprocess.CalledProcessError as err:\n            logger.error(\n                \"Unable to detect suitable master address via `hostname -I`, please manually specify one via --master_addr\"\n            )\n            raise err\n        args.master_addr = result.decode('utf-8').split()[0]\n        if not args.master_addr:\n            raise RuntimeError(\n                f\"Unable to detect suitable master address via `hostname -I`, please manually specify one via --master_addr\"\n            )\n        logger.info(f\"Using IP address of {args.master_addr} for node {first_host}\")\n\n    if args.autotuning != \"\":\n        run_autotuning(args, active_resources)\n        return\n\n    if args.num_nodes > 0:\n        updated_active_resources = collections.OrderedDict()\n        for count, hostname in enumerate(active_resources.keys()):\n            if args.num_nodes == count:\n                break\n            updated_active_resources[hostname] = active_resources[hostname]\n        active_resources = updated_active_resources\n\n    if args.num_gpus > 0:\n        updated_active_resources = collections.OrderedDict()\n        for hostname in active_resources.keys():\n            updated_active_resources[hostname] = list(range(args.num_gpus))\n        active_resources = updated_active_resources\n\n    if args.elastic_training:\n        assert not args.no_local_rank, \"--no_local_rank argument is not supported in Elastic training\"\n\n    # encode world info as base64 to make it easier to pass via command line\n    world_info_base64 = encode_world_info(active_resources)\n\n    multi_node_exec = args.force_multi or len(active_resources) > 1\n\n    if not multi_node_exec:\n        deepspeed_launch = [\n            sys.executable, \"-u\", \"-m\", \"deepspeed.launcher.launch\", f\"--world_info={world_info_base64}\",\n            f\"--master_addr={args.master_addr}\", f\"--master_port={args.master_port}\"\n        ]\n        if args.no_python:\n            deepspeed_launch.append(\"--no_python\")\n        if args.module:\n            deepspeed_launch.append(\"--module\")\n        if args.no_local_rank:\n            deepspeed_launch.append(\"--no_local_rank\")\n        if args.save_pid:\n            deepspeed_launch += [\"--save_pid\", f\"{os.getpid()}\"]\n        if args.enable_each_rank_log:\n            deepspeed_launch.append(f\"--enable_each_rank_log={args.enable_each_rank_log}\")\n        if args.elastic_training:\n            deepspeed_launch.append(\"--enable_elastic_training\")\n            deepspeed_launch.append(f\"--max_elastic_nodes={args.max_elastic_nodes}\")\n            deepspeed_launch.append(f\"--min_elastic_nodes={args.min_elastic_nodes}\")\n        if args.bind_cores_to_rank:\n            deepspeed_launch.append(\"--bind_cores_to_rank\")\n        if args.bind_core_list is not None:\n            deepspeed_launch.append(f\"--bind_core_list={args.bind_core_list}\")\n        cmd = deepspeed_launch + [args.user_script] + args.user_args\n    else:\n        args.launcher = args.launcher.lower()\n        if args.launcher == PDSH_LAUNCHER:\n            runner = PDSHRunner(args, world_info_base64)\n        elif args.launcher == OPENMPI_LAUNCHER:\n            runner = OpenMPIRunner(args, world_info_base64, resource_pool)\n        elif args.launcher == MPICH_LAUNCHER:\n            runner = MPICHRunner(args, world_info_base64, resource_pool)\n        elif args.launcher == IMPI_LAUNCHER:\n            runner = IMPIRunner(args, world_info_base64, resource_pool)\n        elif args.launcher == MVAPICH_LAUNCHER:\n            runner = MVAPICHRunner(args, world_info_base64, resource_pool)\n        elif args.launcher == SLURM_LAUNCHER:\n            runner = SlurmRunner(args, world_info_base64, resource_pool)\n        else:\n            raise NotImplementedError(f\"Unknown launcher {args.launcher}\")\n\n        if not runner.backend_exists():\n            raise RuntimeError(f\"launcher '{args.launcher}' not installed.\")\n\n        curr_path = os.path.abspath('.')\n        if 'PYTHONPATH' in env:\n            env['PYTHONPATH'] = curr_path + \":\" + env['PYTHONPATH']\n        else:\n            env['PYTHONPATH'] = curr_path\n\n        excluded_vars = []\n        for exclude_key, var_list in EXCLUDE_ENVS.items():\n            if exclude_key in env.keys():\n                # key exists in launcher env -> var list should be used\n                excluded_vars += var_list\n\n        # load envs from accelerator\n        exports = EXPORT_ENVS + get_accelerator().export_envs()\n        for var in env.keys():\n            if any([var.startswith(name) for name in exports]):\n                if not any([var == name for name in excluded_vars]):\n                    runner.add_export(var, env[var])\n\n        for environ_path in DEEPSPEED_ENVIRONMENT_PATHS:\n            environ_file = os.path.join(environ_path, DEEPSPEED_ENVIRONMENT_NAME)\n            if os.path.isfile(environ_file):\n                logger.info(f\"deepspeed_env file = {environ_file}\")\n                with open(environ_file, 'r') as fd:\n                    for var in fd.readlines():\n                        key, val = var.split('=', maxsplit=1)\n                        runner.add_export(key, val)\n\n        if args.launcher == PDSH_LAUNCHER:\n            cmd, kill_cmd, env = runner.get_cmd(env, active_resources)\n        else:\n            cmd = runner.get_cmd(env, active_resources)\n\n    logger.info(f\"cmd = {' '.join(cmd)}\")\n    result = subprocess.Popen(cmd, env=env)\n\n    def sigkill_handler(signum, frame):\n        result.send_signal(signal.SIGINT)\n        time.sleep(0.1)\n        result.send_signal(signal.SIGTERM)\n        result_kill = subprocess.Popen(kill_cmd, env=env)\n        result_kill.wait()\n        time.sleep(1)\n        sys.exit(1)\n\n    if args.launcher == PDSH_LAUNCHER and multi_node_exec:\n        signal.signal(signal.SIGINT, sigkill_handler)\n        signal.signal(signal.SIGTERM, sigkill_handler)\n\n    result.wait()\n\n    # In case of failure must propagate the error-condition back to the caller (usually shell). The\n    # actual error and traceback should have been printed in the subprocess, so in order to avoid\n    # unnecessary noise we just quietly exit here with the same code as the subprocess\n    if result.returncode > 0:\n        sys.exit(result.returncode)\n\n\nif __name__ == \"__main__\":\n    main()\n", "deepspeed/launcher/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n'''Copyright The Microsoft DeepSpeed Team'''\n", "deepspeed/launcher/multinode_runner.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport os\nimport sys\nimport shutil\nimport subprocess\nimport warnings\nfrom shlex import split\nfrom abc import ABC, abstractmethod\nfrom deepspeed.accelerator import get_accelerator\nfrom ..utils import logger, get_numactl_cmd\nfrom .constants import PDSH_MAX_FAN_OUT, MVAPICH_TMP_HOSTFILE\n\n\nclass MultiNodeRunner(ABC):\n\n    def __init__(self, args, world_info_base64):\n        self.args = args\n        self.validate_args()\n        self.user_arguments = self.parse_user_args()\n        self.user_script = args.user_script\n        self.world_info_base64 = world_info_base64\n        self.exports = {}\n\n    @abstractmethod\n    def backend_exists(self):\n        \"\"\"Return whether the corresponding backend exists\"\"\"\n\n    @abstractmethod\n    def get_cmd(self, environment, active_resources):\n        \"\"\"Return the command to execute on node\"\"\"\n\n    def add_export(self, key, var):\n        self.exports[key.strip()] = var.strip()\n\n    def parse_user_args(self):\n        return self.args.user_args\n\n    @property\n    def name(self):\n        \"\"\"Return the name of the backend\"\"\"\n        return self.__class__.__name__\n\n    def validate_args(self):\n        \"\"\"Validate self.args\"\"\"\n\n\nclass PDSHRunner(MultiNodeRunner):\n\n    def __init__(self, args, world_info_base64):\n        super().__init__(args, world_info_base64)\n\n    def backend_exists(self):\n        return shutil.which('pdsh')\n\n    def parse_user_args(self):\n        processed_args = []\n        for arg in self.args.user_args:\n            # With pdsh, if we are passing a string as an argument, it will get\n            # split on whitespace. To avoid this and support strings that\n            # contain '\"', we do this extra processing step:\n            if \" \" in arg:\n                arg = '\"{}\"'.format(arg.replace('\"', '\\\\\"'))\n            processed_args.append(arg)\n        return processed_args\n\n    @property\n    def name(self):\n        return \"pdsh\"\n\n    def get_cmd(self, environment, active_resources):\n        environment['PDSH_RCMD_TYPE'] = 'ssh'\n        if self.args.ssh_port is not None:  # only specify ssh port if it is specified\n            environment[\"PDSH_SSH_ARGS_APPEND\"] = f\"{environment.get('PDSH_SSH_ARGS_APPEND', '')} \\\n            -p {self.args.ssh_port}\"\n\n        active_workers = \",\".join(active_resources.keys())\n        logger.info(\"Running on the following workers: %s\" % active_workers)\n\n        # PDSH flags for max node fan out and specific hosts to launch on\n        # See https://linux.die.net/man/1/pdsh for flag details\n        pdsh_cmd_args = ['pdsh', '-S', '-f', str(PDSH_MAX_FAN_OUT), '-w', active_workers] + split(\n            self.args.launcher_args)\n\n        exports = \"\"\n        for key, val in self.exports.items():\n            exports += \"export {}={}; \".format(key, val)\n\n        # https://linux.die.net/man/1/pdsh\n        # %n will be replaced by pdsh command\n        deepspeed_launch = [\n            exports, f\"cd {os.path.abspath('.')};\", sys.executable, \"-u\", \"-m\", \"deepspeed.launcher.launch\",\n            f'--world_info={self.world_info_base64}', \"--node_rank=%n\", f\"--master_addr={self.args.master_addr}\",\n            f\"--master_port={self.args.master_port}\"\n        ]\n        if self.args.no_python:\n            deepspeed_launch.append(\"--no_python\")\n        if self.args.module:\n            deepspeed_launch.append(\"--module\")\n        if self.args.no_local_rank:\n            deepspeed_launch.append(\"--no_local_rank\")\n        if self.args.save_pid:\n            deepspeed_launch += [\"--save_pid\", f\"{os.getpid()}\"]\n        if self.args.elastic_training:\n            deepspeed_launch.append(\"--enable_elastic_training\")\n            deepspeed_launch.append(f\"--max_elastic_nodes={self.args.max_elastic_nodes}\")\n            deepspeed_launch.append(f\"--min_elastic_nodes={self.args.min_elastic_nodes}\")\n\n        cmd_to_search = [i + \"\\\\\" for i in deepspeed_launch[2:6]]\n\n        kill_command = pdsh_cmd_args + [\"pkill -f \", \" \".join(cmd_to_search)[:-2]]\n        return pdsh_cmd_args + deepspeed_launch + [self.user_script] + self.user_arguments, kill_command, environment\n\n\nclass OpenMPIRunner(MultiNodeRunner):\n\n    def __init__(self, args, world_info_base64, resource_pool):\n        super().__init__(args, world_info_base64)\n        self.resource_pool = resource_pool\n        self.add_export('UCX_TLS', 'tcp')\n\n    def backend_exists(self):\n        #TODO: if IB is available we should suggestion mvapich\n        return shutil.which('ompi_info')\n\n    @property\n    def name(self):\n        return \"openmpi\"\n\n    def validate_args(self):\n        super().validate_args()\n        #TODO: Allow for include/exclude at node-level but not gpu-level\n        if self.args.include != \"\" or self.args.exclude != \"\":\n            raise ValueError(f\"{self.name} backend does not support worker include/exclusion\")\n        if self.args.num_nodes != -1 or self.args.num_gpus != -1:\n            raise ValueError(f\"{self.name} backend does not support limiting num nodes/gpus\")\n\n    def get_cmd(self, environment, active_resources):\n        total_process_count = sum(self.resource_pool.values())\n\n        mpirun_cmd = [\n            'mpirun',\n            '-n',\n            f'{total_process_count}',\n            '-hostfile',\n            f'{self.args.hostfile}',\n            '--mca',\n            'btl',\n            '^openib',\n            '--mca',\n            'btl_tcp_if_include',\n            'eth0',\n        ] + split(self.args.launcher_args)\n\n        export_cmd = []\n        for k, v in self.exports.items():\n            export_cmd += ['-x', \"{}={}\".format(k, v)]\n\n        python_exec = []\n        if not self.args.no_python:\n            python_exec = [sys.executable, \"-u\"]\n            if self.args.module:\n                python_exec.append(\"-m\")\n\n        return mpirun_cmd + export_cmd + python_exec + [self.user_script] + self.user_arguments\n\n\nclass MPICHRunner(MultiNodeRunner):\n\n    def __init__(self, args, world_info_base64, resource_pool):\n        super().__init__(args, world_info_base64)\n        self.resource_pool = resource_pool\n\n    def backend_exists(self):\n        #TODO: if IB is available we should suggestion mpich\n        return shutil.which('mpirun')  #mpich_info\n\n    @property\n    def name(self):\n        return \"mpich\"\n\n    def validate_args(self):\n        super().validate_args()\n        #TODO: Allow for include/exclude at node-level but not gpu-level\n        if self.args.include != \"\" or self.args.exclude != \"\":\n            raise ValueError(f\"{self.name} backend does not support worker include/exclusion\")\n\n        if self.args.num_nodes != -1 or self.args.num_gpus != -1:\n            raise ValueError(f\"{self.name} backend does not support limiting num nodes/gpus\")\n\n    def get_cmd(self, environment, active_resources):\n        devices_per_node = self.resource_pool.values()\n        total_process_count = sum(devices_per_node)\n        process_per_node = list(devices_per_node)[0]\n        if not all([n == process_per_node for n in devices_per_node]):\n            raise ValueError(\"MPICH requires same number of devices per node\")\n\n        mpirun_cmd = [\n            'mpirun',\n            '-n',\n            f'{total_process_count}',\n            '-ppn',\n            f'{process_per_node}',\n        ] + split(self.args.launcher_args)\n        export_cmd = []\n\n        for k, v in self.exports.items():\n            export_cmd += ['-genv', \"{}={}\".format(k, v)]\n\n        export_cmd += ['-genv', 'MASTER_ADDR', str(self.args.master_addr)]\n        export_cmd += ['-genv', 'MASTER_PORT', str(self.args.master_port)]\n        export_cmd += ['-genv', 'WORLD_SIZE', str(total_process_count)]\n        export_cmd += ['-genv', 'LOCAL_SIZE', str(process_per_node)]\n\n        export_cmd += ['-hosts']\n        hosts = \"\"\n        for i, host in enumerate(self.resource_pool.keys()):\n            if i == 0:\n                hosts = f\"{host}\"\n            else:\n                hosts += f\",{host}\"\n        export_cmd += [hosts]\n\n        helper_args = [\"--launcher\"] + [self.args.launcher]\n        python_exec = []\n        if not self.args.no_python:\n            python_exec += [sys.executable, \"-u\"]\n            if self.args.module:\n                python_exec.append(\"-m\")\n                helper_args.append(\"--module\")\n        else:\n            helper_args.append(\"--no_python\")\n\n        helper_cmd = str(os.path.dirname(os.path.realpath(__file__))) + '/launcher_helper.py'\n        helper_cmd = [helper_cmd] + helper_args + [self.user_script] + self.user_arguments\n\n        return mpirun_cmd + export_cmd + python_exec + helper_cmd\n\n\nclass IMPIRunner(MultiNodeRunner):\n\n    def __init__(self, args, world_info_base64, resource_pool):\n        super().__init__(args, world_info_base64)\n        self.resource_pool = resource_pool\n\n    def backend_exists(self):\n        #TODO: if IB is available we should suggestion mpich\n        return shutil.which('mpirun')  #mpich_info\n\n    @property\n    def name(self):\n        return \"impi\"\n\n    def validate_args(self):\n        super().validate_args()\n        #TODO: Allow for include/exclude at node-level but not gpu-level\n        if self.args.include != \"\" or self.args.exclude != \"\":\n            raise ValueError(f\"{self.name} backend does not support worker include/exclusion\")\n\n        if self.args.num_nodes != -1 or self.args.num_gpus != -1:\n            raise ValueError(f\"{self.name} backend does not support limiting num nodes/gpus\")\n\n    def get_cmd(self, environment, active_resources):\n        devices_per_node = self.resource_pool.values()\n        total_process_count = sum(devices_per_node)\n        process_per_node = list(devices_per_node)[0]\n        if not all([n == process_per_node for n in devices_per_node]):\n            raise ValueError(\"Intel MPI requires same number of devices per node\")\n\n        mpirun_cmd = [\n            'mpirun',\n            '-ppn',\n            f'{process_per_node}',\n        ] + split(self.args.launcher_args)\n        export_cmd = []\n\n        for k, v in self.exports.items():\n            export_cmd += ['-genv', f'{k}', f'{v}']\n\n        if self.args.bind_cores_to_rank:\n            cores_per_rank, _ = get_numactl_cmd(self.args.bind_core_list, process_per_node, 0)\n            export_cmd += ['-genv', 'OMP_NUM_THREADS', str(cores_per_rank)]\n\n        export_cmd += ['-genv', 'MASTER_ADDR', str(self.args.master_addr)]\n        export_cmd += ['-genv', 'MASTER_PORT', str(self.args.master_port)]\n        export_cmd += ['-genv', 'WORLD_SIZE', str(total_process_count)]\n        export_cmd += ['-genv', 'LOCAL_SIZE', str(process_per_node)]\n\n        # turn off IMPI core binding, use deepspeed's own core binding\n        export_cmd += ['-genv', 'I_MPI_PIN', '0']\n\n        export_cmd += ['-hosts']\n        hosts = \"\"\n        for i, host in enumerate(self.resource_pool.keys()):\n            if i == 0:\n                hosts = f\"{host}\"\n            else:\n                hosts += f\",{host}\"\n        export_cmd += [hosts]\n\n        per_host_cmd = []\n\n        for i in range(total_process_count):\n            local_rank = i % process_per_node\n            python_exec = []\n            if self.args.bind_cores_to_rank:\n                _, numactl_cmd = get_numactl_cmd(self.args.bind_core_list, process_per_node, local_rank)\n                python_exec += numactl_cmd\n\n            if not self.args.no_python:\n                python_exec += [sys.executable, \"-u\"]\n                if self.args.module:\n                    python_exec.append(\"-m\")\n            env_mapping = ['-env', 'RANK', str(i)]\n            env_mapping += ['-env', 'LOCAL_RANK', str(local_rank)]\n            if i == 0:\n                per_host_cmd = ['-n', '1'] + env_mapping + python_exec + [self.user_script] + self.user_arguments\n            else:\n                per_host_cmd = per_host_cmd + [':', '-n', '1'] + env_mapping + python_exec + [self.user_script\n                                                                                              ] + self.user_arguments\n        print(mpirun_cmd + export_cmd + per_host_cmd)\n        return mpirun_cmd + export_cmd + per_host_cmd\n\n\nclass SlurmRunner(MultiNodeRunner):\n\n    def __init__(self, args, world_info_base64, resource_pool):\n        super().__init__(args, world_info_base64)\n        self.resource_pool = resource_pool\n\n    def backend_exists(self):\n        return shutil.which('sinfo')\n\n    @property\n    def name(self):\n        return 'slurm'\n\n    def get_cmd(self, environment, active_resources):\n        assert not getattr(self.args, 'detect_nvlink_pairs',\n                           False), \"slurm backend does not support remapping visible devices\"\n        total_process_count = sum(self.resource_pool.values())\n        srun_cmd = [\n            'srun',\n            '-n',\n            f'{total_process_count}',\n        ] + split(self.args.launcher_args)\n\n        if getattr(self.args, 'slurm_comment', ''):\n            srun_cmd += ['--comment', self.args.slurm_comment]\n\n        if self.args.include != \"\":\n            srun_cmd.append('--include')\n            srun_cmd.append(f'{self.args.include}')\n        if self.args.exclude != \"\":\n            srun_cmd.append('--exclude')\n            srun_cmd.append(f'{self.args.exclude}')\n        if self.args.num_nodes > 0:\n            srun_cmd.append('--nodes')\n            srun_cmd.append(f'{self.args.num_nodes}')\n        if self.args.num_gpus > 0:\n            srun_cmd.append('--gpus')\n            srun_cmd.append(f'{self.args.num_gpus}')\n\n        exports = '--export=ALL'\n        for key, val in self.exports.items():\n            exports += f\",{key}={val}\"\n\n        python_exec = [sys.executable, \"-u\"]\n        command = srun_cmd + [exports] + python_exec + [self.user_script] + self.user_arguments\n        return command\n\n\nclass MVAPICHRunner(MultiNodeRunner):\n\n    def __init__(self, args, world_info_base64, resource_pool):\n        super().__init__(args, world_info_base64)\n        self.resource_pool = resource_pool\n\n        # Disable the CMA kernel module, not available on Ubuntu systems\n        self.add_export('MV2_SMP_USE_CMA', '0')\n\n        # If we fail this will output more verbose logging\n        self.add_export('MV2_DEBUG_SHOW_BACKTRACE', '1')\n\n        # Enabled cuda-aware communication\n        if get_accelerator().device_name() == 'cuda':\n            self.add_export('MV2_USE_CUDA', '1')\n\n        # Support deep learning frameworks: http://hidl.cse.ohio-state.edu/userguide/horovod/\n        self.add_export('MV2_SUPPORT_DL', '1')\n\n        # Support MPI_THREAD_MULTIPLE\n        self.add_export('MV2_ENABLE_AFFINITY', '0')\n\n        # Performance tuning flags for allgather\n        self.add_export('MV2_INTER_ALLGATHER_TUNING', '5')\n        self.add_export('MV2_CUDA_USE_NAIVE', '0')\n\n    def backend_exists(self):\n        #TODO: if IB is available we should suggestion mvapich\n        mpiname_exists = shutil.which('mpiname')\n        exists = False\n        if not mpiname_exists:\n            warnings.warn(\"mpiname does not exist, mvapich is not installed properly\")\n        else:\n            results = subprocess.check_output('mpiname', shell=True)\n            mpiname_results = results.decode('utf-8').strip()\n            if \"MVAPICH2-GDR\" in mpiname_results:\n                exists = True\n            else:\n                warnings.warn(f\"Expected MVAPICH2-GDR as return for mpiname but received {mpiname_results}\")\n        return exists\n\n    @property\n    def name(self):\n        return \"mvapich\"\n\n    def validate_args(self):\n        super().validate_args()\n        #TODO: Allow for include/exclude at node-level but not gpu-level\n        if self.args.include != \"\" or self.args.exclude != \"\":\n            raise ValueError(f\"{self.name} backend does not support worker include/exclusion\")\n        if self.args.num_nodes != -1 or self.args.num_gpus != -1:\n            raise ValueError(f\"{self.name} backend does not support limiting num nodes/gpus\")\n\n    def get_cmd(self, environment, active_resources):\n        devices_per_node = self.resource_pool.values()\n        total_process_count = sum(devices_per_node)\n        process_per_node = list(devices_per_node)[0]\n        if not all([n == process_per_node for n in devices_per_node]):\n            raise ValueError(\"mvapich requires same number of devices per node\")\n\n        with open(MVAPICH_TMP_HOSTFILE, 'w') as fd:\n            for host in self.resource_pool.keys():\n                fd.write(f'{host}\\n')\n\n        mpirun_cmd = [\n            'mpirun',\n            '-np',\n            f'{total_process_count}',\n            '-ppn',\n            f'{process_per_node}',\n            '--hostfile',\n            f'{MVAPICH_TMP_HOSTFILE}',\n        ] + split(self.args.launcher_args)\n\n        export_cmd = []\n        for k, v in self.exports.items():\n            export_cmd += ['-env', \"{}={}\".format(k, v)]\n\n        python_exec = []\n        if not self.args.no_python:\n            python_exec = [sys.executable, \"-u\"]\n            if self.args.module:\n                python_exec.append(\"-m\")\n\n        return mpirun_cmd + export_cmd + python_exec + [self.user_script] + self.user_arguments\n", "deepspeed/ops/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom . import adam\nfrom . import adagrad\nfrom . import lamb\nfrom . import lion\nfrom . import sparse_attention\nfrom . import transformer\n\nfrom .transformer import DeepSpeedTransformerLayer, DeepSpeedTransformerConfig\n\nfrom ..git_version_info import compatible_ops as __compatible_ops__\n", "deepspeed/ops/lamb/fused_lamb.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\"\"\"\nCopyright NVIDIA/apex\nThis file is adapted from NVIDIA/apex/optimizer/fused_adam and implements the LAMB optimizer\n\"\"\"\nimport types\nimport torch\nfrom deepspeed.ops.op_builder import FusedLambBuilder\n\n\nclass FusedLamb(torch.optim.Optimizer):\n    \"\"\"Implements the LAMB algorithm. Currently GPU-only.\n\n    LAMB was proposed in `Large Batch Optimization for Deep Learning: Training BERT in 76 minutes.\n    https://arxiv.org/abs/1904.00962\n\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups.\n        lr (float, optional): learning rate. (default: 1e-3)\n        bias_correction (bool, optional): bias correction (default: True)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square. (default: (0.9, 0.999))\n        eps (float, optional): term added to the denominator to improve\n            numerical stability. (default: 1e-8)\n        eps_inside_sqrt (boolean, optional): in the 'update parameters' step,\n            adds eps to the bias-corrected second moment estimate before\n            evaluating square root instead of adding it to the square root of\n            second moment estimate as in the original paper. (default: False)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        max_grad_norm (float, optional): value used to clip global grad norm\n            (default: 0.0)\n        max_coeff(float, optional): maximum value of the lamb coefficient (default: 10.0)\n        min_coeff(float, optional): minimum value of the lamb coefficient (default: 0.01)\n        amsgrad (boolean, optional): NOT SUPPORTED in FusedLamb!\n    \"\"\"\n\n    def __init__(self,\n                 params,\n                 lr=1e-3,\n                 bias_correction=True,\n                 betas=(0.9, 0.999),\n                 eps=1e-8,\n                 eps_inside_sqrt=False,\n                 weight_decay=0.,\n                 max_grad_norm=0.,\n                 max_coeff=10.0,\n                 min_coeff=0.01,\n                 amsgrad=False):\n        self.fused_lamb_cuda = FusedLambBuilder().load()\n\n        if amsgrad:\n            raise RuntimeError('FusedLamb does not support the AMSGrad variant.')\n        defaults = dict(lr=lr,\n                        bias_correction=bias_correction,\n                        betas=betas,\n                        eps=eps,\n                        weight_decay=weight_decay,\n                        max_grad_norm=max_grad_norm,\n                        max_coeff=max_coeff,\n                        min_coeff=min_coeff)\n        super(FusedLamb, self).__init__(params, defaults)\n        self.eps_mode = 0 if eps_inside_sqrt else 1\n        self.lamb_coeffs = []\n\n    def step(self, closure=None, grads=None, output_params=None, scale=1., grad_norms=None):\n        \"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n            grads (list of tensors, optional): weight gradient to use for the\n                optimizer update. If gradients have type torch.half, parameters\n                are expected to be in type torch.float. (default: None)\n            output params (list of tensors, optional): A reduced precision copy\n                of the updated weights written out in addition to the regular\n                updated weights. Have to be of same type as gradients. (default: None)\n            scale (float, optional): factor to divide gradient tensor values\n                by before applying to weights. (default: 1)\n        \"\"\"\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        if grads is None:\n            grads_group = [None] * len(self.param_groups)\n        # backward compatibility\n        # assuming a list/generator of parameter means single group\n        elif isinstance(grads, types.GeneratorType):\n            grads_group = [grads]\n        elif type(grads[0]) != list:\n            grads_group = [grads]\n        else:\n            grads_group = grads\n\n        if output_params is None:\n            output_params_group = [None] * len(self.param_groups)\n        elif isinstance(output_params, types.GeneratorType):\n            output_params_group = [output_params]\n        elif type(output_params[0]) != list:\n            output_params_group = [output_params]\n        else:\n            output_params_group = output_params\n\n        if grad_norms is None:\n            grad_norms = [None] * len(self.param_groups)\n\n        #remove the previous coeffs\n        del self.lamb_coeffs[:]\n\n        for group, grads_this_group, output_params_this_group, grad_norm_group in zip(\n                self.param_groups, grads_group, output_params_group, grad_norms):\n            if grads_this_group is None:\n                grads_this_group = [None] * len(group['params'])\n            if output_params_this_group is None:\n                output_params_this_group = [None] * len(group['params'])\n\n            if grad_norm_group is None:\n                grad_norm_group = [None] * len(group['params'])\n            elif not isinstance(grad_norm_group, list):\n                grad_norm_group = [grad_norm_group]\n\n            bias_correction = 1 if group['bias_correction'] else 0\n\n            for p, grad, output_param, grad_norm in zip(group['params'], grads_this_group, output_params_this_group,\n                                                        grad_norm_group):\n\n                # compute combined scale factor for this group\n                combined_scale = scale\n                if group['max_grad_norm'] > 0:\n                    # norm is in fact norm*scale\n                    clip = ((grad_norm / scale) + 1e-6) / group['max_grad_norm']\n                    if clip > 1:\n                        combined_scale = clip * scale\n\n                #note: p.grad should not ever be set for correct operation of mixed precision optimizer that sometimes sends None gradients\n                if p.grad is None and grad is None:\n                    continue\n                if grad is None:\n                    grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError('FusedLamb does not support sparse gradients')\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state['step'] = 0\n                    # Exponential moving average of gradient values\n                    state['exp_avg'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n                max_coeff = group['max_coeff']\n                min_coeff = group['min_coeff']\n\n                state['step'] += 1\n\n                out_p = torch.tensor([], dtype=torch.float) if output_param is None else output_param\n                lamb_coeff = self.fused_lamb_cuda.lamb(p.data, out_p, exp_avg, exp_avg_sq, grad, group['lr'], beta1,\n                                                       beta2, max_coeff, min_coeff, group['eps'], combined_scale,\n                                                       state['step'], self.eps_mode, bias_correction,\n                                                       group['weight_decay'])\n                self.lamb_coeffs.append(lamb_coeff)\n        return loss\n\n    def get_lamb_coeffs(self):\n        lamb_coeffs = [lamb_coeff.item() for lamb_coeff in self.lamb_coeffs]\n        return lamb_coeffs\n", "deepspeed/ops/lamb/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .fused_lamb import FusedLamb\n", "deepspeed/ops/fp_quantizer/quantize.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nimport abc\nfrom abc import ABC\n\nfrom deepspeed.ops.op_builder import FPQuantizerBuilder\n\nfp_quant_module = None\n\n\nclass Quantizer(ABC):\n    \"\"\"\n    Abstract Quantizer class that implmenents quantize/dequantize methods.\n\n    Arguments:\n        group_size (int, optional): number of values or elements that are grouped\n            together for the quantization process.\n    \"\"\"\n\n    def __init__(self, group_size=512) -> None:\n        self.group_size = group_size\n\n    @abc.abstractmethod\n    def quantize(self,\n                 input,\n                 q_bits=8,\n                 q_mantisa_bits=3,\n                 stochastic_mode=False,\n                 return_meta_tensor=False) -> torch.Tensor:\n        ...\n\n    @abc.abstractmethod\n    def dequantize(self, input_q, fp_out=None, q_bits=8, q_mantisa_bits=3, scale=None) -> torch.Tensor:\n        ...\n\n\nclass FP_Quantize(Quantizer):\n\n    def __init__(self, group_size=512) -> None:\n        global fp_quant_module\n        super().__init__(group_size=group_size)\n        if fp_quant_module is None:\n            fp_quant_module = FPQuantizerBuilder().load()\n        self.orig_dtype = None\n\n    def quantize(self,\n                 input,\n                 q_bits=8,\n                 q_mantisa_bits=3,\n                 stochastic_mode=False,\n                 return_meta_tensor=False) -> torch.Tensor:\n        assert input.dtype == torch.bfloat16, \"only support bf16 for now\"\n        if return_meta_tensor:\n            assert q_bits == 8, \"meta tensor is only supported with q_bit=8\"\n\n        self.orig_dtype = input.dtype\n        self.orig_shape = input.shape\n\n        if q_bits == 8:\n            pass\n        elif q_bits == 12:\n            q_mantisa_bits = 4\n        elif q_bits == 6:\n            q_mantisa_bits = 2\n        elif q_bits == 4:\n            q_mantisa_bits = 1\n        else:\n            assert (0), \\\n                f\"Missing {q_bits}-quantization, please add the template arguments for the kernel to support this precision!\"\n\n        out = fp_quant_module.quantize(input, self.group_size, stochastic_mode, q_bits, q_mantisa_bits)\n\n        if return_meta_tensor:\n            data, scale = out.split(self.group_size, dim=-1)\n            return data.contiguous().reshape(input.shape), scale.contiguous()\n\n        return out\n\n    def dequantize(self, input_q, fp_out=None, q_bits=8, q_mantisa_bits=3, scale=None) -> torch.Tensor:\n        assert (self.orig_dtype is not None), \\\n            \"[De-quantization Error]: you need to call quantize before dequantizing!\"\n        fp_out = torch.empty(self.orig_shape, dtype=self.orig_dtype,\n                             device=input_q.device) if fp_out is None else fp_out\n        if q_bits == 8:\n            pass\n        elif q_bits == 12:\n            q_mantisa_bits = 4\n        elif q_bits == 6:\n            q_mantisa_bits = 2\n        elif q_bits == 4:\n            q_mantisa_bits = 1\n        else:\n            assert (0), \\\n                f\"Missing {q_bits}-dequantization, please add the template arguments for the kernel to support this precision!\"\n\n        if scale is not None:\n            assert input_q.numel() == fp_out.numel(), \\\n            f'[De-quantization Error]: quantized data should have the same size as original tensor when scale is not None!'\n            input_q = torch.cat([input_q.reshape(-1, self.group_size), scale], dim=-1).contiguous()\n\n        fp_quant_module.dequantize(fp_out, input_q, self.group_size, q_mantisa_bits, q_bits - q_mantisa_bits - 1)\n        return fp_out\n\n    def selective_dequantize(self,\n                             input_q,\n                             indexes,\n                             fp_out=None,\n                             q_bits=8,\n                             q_mantisa_bits=3,\n                             scale=None) -> torch.Tensor:\n        assert (not hasattr(self, 'orig_shape') or len(self.orig_shape) == 3), \\\n            \"Selective-Dequantization works on 3d tensor only! Please reshape the tensor before calling dequantize function.\"\n        assert (self.orig_dtype is not None), \\\n            \"[De-quantization Error]: you need to call quantize before dequantizing!\"\n        fp_out = torch.empty(\n            (indexes.shape[0],\n             *self.orig_shape[1:]), dtype=self.orig_dtype, device=input_q.device) if fp_out is None else fp_out\n        if q_bits == 8:\n            pass\n        elif q_bits == 12:\n            q_mantisa_bits = 4\n        elif q_bits == 6:\n            q_mantisa_bits = 2\n        elif q_bits == 4:\n            q_mantisa_bits = 1\n        else:\n            assert (0), \\\n                f\"Missing {q_bits}-dequantization, please add the template arguments for the kernel to support this precision!\"\n\n        if scale is not None:\n            assert input_q.numel() == fp_out.numel(), \\\n            f'[De-quantization Error]: quantized data should have the same size as original tensor when scale is not None!'\n            input_q = torch.cat([input_q.reshape(-1, self.group_size), scale], dim=-1).contiguous()\n\n        fp_quant_module.selective_dequantize(fp_out, input_q, indexes, self.group_size, q_mantisa_bits,\n                                             q_bits - q_mantisa_bits - 1)\n        return fp_out\n", "deepspeed/ops/fp_quantizer/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .quantize import FP_Quantize, Quantizer\n", "deepspeed/ops/sparse_attention/matmul.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\n# DeepSpeed note, code taken & adapted from commit 9aa94789f13ada713af36cfd8cca2fc9a7f6b79a\n# https://github.com/ptillet/torch-blocksparse/blob/master/torch_blocksparse/matmul.py\nimport importlib\nimport torch\n\nimport triton\nimport triton.language as tl\nimport triton._C.libtriton as libtriton\nfrom deepspeed.accelerator import get_accelerator\n\n\n@triton.jit\ndef _kernel(A, B, C, stride_za, stride_ha, stride_ma, stride_ka, stride_zb, stride_hb, stride_kb, stride_nb, stride_zc,\n            stride_hc, stride_mc, stride_nc, DS0, DS1, SDD_K, SDD_off_width, lut, locks, nlocks, **meta):\n    TM = meta['TM']\n    TN = meta['TN']\n    TK = meta['TK']\n    TZ = meta['TZ']\n    BLOCK = meta['BLOCK']\n    #------------#\n    #- Prologue -#\n    #------------#\n    pid0 = tl.program_id(0)\n    pid1 = tl.program_id(1)\n    pidz = tl.program_id(2)\n    if meta['SDD']:\n        pid1 = pid1 + SDD_off_width\n        blockidm = tl.arange(0, TM) // BLOCK\n        blockidn = tl.arange(0, TN) // BLOCK\n        offlutm = blockidm * (TN // BLOCK) * 4\n        offlutn = blockidn * 4\n        header = lut + pid1 * (TM // BLOCK) * (TN // BLOCK) * 4\n        z = tl.load(header + 0)\n        i = tl.load(header + 1 + offlutm)\n        j = tl.load(header + 2 + offlutn)\n        AS1 = SDD_K // TZ\n        lockid = tl.where(TZ > 1, 1, 0)\n        offka = pid0 * AS1\n        offkb = pid0 * AS1\n        offmc = 0\n        offnc = 0\n        offpa = 0\n        offpb = 0\n        maxid = TZ\n        offhc = 0\n        offha = z\n        offhb = z\n        ram = i * BLOCK + (tl.arange(0, TM) % BLOCK)\n        rbn = j * BLOCK + (tl.arange(0, TN) % BLOCK)\n    else:\n        header = lut + pid0 * 6\n        offset = tl.load(header + 0)\n        AS1 = tl.load(header + 1)\n        column = tl.load(header + 2)\n        depth = tl.load(header + 3)\n        lockid = tl.load(header + 4)\n        maxid = tl.load(header + 5)\n        pinc = lut + offset\n        offhc = depth\n        if meta['DSD']:\n            # output offset\n            offnc = pid1 * TN\n            offmc = column * TM\n            offpc = 0\n            # dense input offset\n            offnb = pid1 * TN\n            offkb = tl.load(pinc)\n            offkb = tl.multiple_of(offkb, 8)  # compiler hint\n            offpb = 0\n            # sparse input offset\n            offma = 0\n            offka = 0\n            offpa = tl.load(pinc + 1)\n            offpa = tl.multiple_of(offpa, 8)  # compiler hint\n            offpa = offpa * BLOCK * BLOCK\n            offha = 0\n            offhb = depth\n        else:\n            # output offset\n            offmc = pid1 * TM\n            offnc = column * TN\n            offpc = 0\n            # dense input offset\n            offma = pid1 * TM\n            offka = tl.load(pinc)\n            offka = tl.multiple_of(offka, 8)  # compiler hint\n            offpa = 0\n            # sparse input offset\n            offnb = 0\n            offkb = 0\n            offpb = tl.load(pinc + 1)\n            offpb = tl.multiple_of(offpb, 8)  # compiler hint\n            offpb = offpb * BLOCK * BLOCK\n            offha = depth\n            offhb = 0\n        ram = offma + tl.arange(0, TM)\n        rbn = offnb + tl.arange(0, TN)\n\n    # initialize a, b pointers\n    rka = offka + tl.arange(0, TK)\n    rkb = offkb + tl.arange(0, TK)\n    pa = A + pidz * stride_za + offha * stride_ha + offpa + ram[:, None] * stride_ma + rka[None, :] * stride_ka\n    pb = B + pidz * stride_zb + offhb * stride_hb + offpb + rbn[None, :] * stride_nb + rkb[:, None] * stride_kb\n    if meta['DDS']:\n        checkam = ram[:, None] < DS0\n    else:\n        checkam = AS1 > 0\n    if meta['DSD']:\n        checkbn = rbn[None, :] < DS0\n    else:\n        checkbn = AS1 > 0\n    a = tl.load(pa, mask=checkam, other=0.)\n    b = tl.load(pb, mask=checkbn, other=0.)\n\n    ## ---------------- ##\n    ##    Inner Loop    ##\n    ## ---------------- ##\n    acc = tl.zeros((TM, TN), dtype=tl.float32)\n    for k in range(AS1, 0, -TK):\n        acc += tl.dot(a, b)\n        if meta['SDD']:\n            inc_a = TK * stride_ka\n            inc_b = TK * stride_kb\n        else:\n            pinc += 2\n        if meta['DSD']:\n            inc_b = tl.load(pinc)\n            inc_a = tl.load(pinc + 1)\n            inc_b = tl.multiple_of(inc_b, 8)\n            inc_a = tl.multiple_of(inc_a, 8)\n            inc_b = inc_b * stride_kb\n        if meta['DDS']:\n            inc_a = tl.load(pinc)\n            inc_b = tl.load(pinc + 1)\n            inc_a = tl.multiple_of(inc_a, 8)\n            inc_b = tl.multiple_of(inc_b, 8)\n            inc_a = inc_a * stride_ka\n        pa += inc_a\n        pb += inc_b\n        # pre-fetch\n        checkak = k > TK\n        checkbk = k > TK\n        checka = checkam & checkak\n        checkb = checkbn & checkbk\n        a = tl.load(pa, mask=checka)\n        b = tl.load(pb, mask=checkb)\n    c = acc.to(C.dtype.element_ty)\n\n    if meta['SDD']:\n        checkc = True\n        rr_blockidm = tl.arange(0, TM) // BLOCK\n        rr_blockidn = tl.arange(0, TN) // BLOCK\n        rr_offlutm = rr_blockidm * (TN // BLOCK) * 4\n        rr_offlutn = rr_blockidn * 4\n        off_bkid = 3 + rr_offlutm[:, None] + rr_offlutn[None, :]\n        bkid = tl.load(header + off_bkid)\n        offpc = bkid * BLOCK * BLOCK\n        rcm = tl.arange(0, TM) % BLOCK\n        rcn = tl.arange(0, TN) % BLOCK\n    else:\n        rcm = offmc + tl.arange(0, TM)\n        rcn = offnc + tl.arange(0, TN)\n    if meta['DSD']:\n        checkc = rcn[None, :] < DS0\n    if meta['DDS']:\n        checkc = rcm[:, None] < DS0\n\n    pc = C + offpc + offhc * stride_hc + pidz * stride_zc + rcm[:, None] * stride_mc + rcn[None, :] * stride_nc\n    # write-back directly\n    if lockid == 0:\n        tl.store(pc, c, mask=checkc)\n    # accumulate partial results using spin-locks\n    else:\n        plock = locks + tl.program_id(2) * nlocks * tl.num_programs(1) + tl.program_id(1) * nlocks + lockid - 1\n        pcount = plock + tl.num_programs(2) * tl.num_programs(1) * nlocks\n        while tl.atomic_cas(plock, 0, 1) == 1:\n            pass\n        count = tl.load(pcount)\n        if count == 0:\n            tl.store(pc, c, mask=checkc)\n        else:\n            d = tl.load(pc, mask=checkc)\n            tl.store(pc, d + c, mask=checkc)\n        tl.atomic_xchg(pcount, (count + 1) % maxid)\n        tl.atomic_xchg(plock, 0)\n\n\n##############\n#  MAIN API  #\n##############\nclass _sparse_matmul(torch.autograd.Function):\n\n    sdd_cache = dict()\n    dsd_cache = dict()\n    dds_cache = dict()\n    locks = dict()\n\n    # Given an array sizes representing reduction size for each\n    # column of a block-mode matrix multiplication,\n    # performs load-balancing to achieve more smaller reductions\n    # between `seg_size` elements\n    @staticmethod\n    def load_balance(sizes, block):\n        #global triton\n        #if triton is None:\n        #    triton = importlib.import_module('triton')\n        # segment size\n        # heuristics taken from OpenAI blocksparse code\n        # https://github.com/openai/blocksparse/blob/master/blocksparse/matmul.py#L95\n        max_size = sizes.max()\n        min_size = sizes[sizes != 0].min()\n        #if max_size > min_size * 2.0:\n        #  seg_max = max(triton.cdiv(max_size, 4), min_size*2)\n        #else:\n        #  seg_max = max_size\n        seg_max = max_size\n        seg_min = max(triton.cdiv(seg_max, 4), 4)\n        # split reduction into segments\n        div = sizes // seg_max\n        rem = sizes % seg_max\n        packs = div + (sizes < seg_min).long() + (rem >= seg_min).long()\n        width = packs.sum()\n        segments = torch.empty(width, dtype=sizes.dtype)\n        column = torch.empty_like(segments)\n        lockid = torch.zeros_like(segments)\n        maxid = torch.zeros_like(segments)\n        nlocks = 0\n        current = 0\n        col_idx = 0\n        for i in range(len(sizes)):\n            d, r = div[i], rem[i]\n            isempty = sizes[i] < seg_min\n            last = current + d + (r >= seg_min) + isempty\n            # column id\n            column[current:last] = col_idx\n            # lock id\n            if d > 1 or (d == 1 and r >= seg_min):\n                nlocks += 1\n                lockid[current:last] = nlocks\n                maxid[current:last] = last - current\n            # segment size\n            segments[current:current + d] = seg_max\n            if r < seg_min and not isempty:\n                segments[current + d - 1] += r\n            if r >= seg_min or isempty:\n                segments[current + d] = r\n            current = last\n            col_idx += 1\n        offsets = torch.zeros_like(segments)\n        offsets[1:] = torch.cumsum(segments[:-1], dim=0)\n        return segments, column, lockid, maxid, offsets\n\n    @staticmethod\n    def get_locks(size, dev):\n        if dev not in _sparse_matmul.locks or \\\n            size > _sparse_matmul.locks[dev].size(0):\n            _sparse_matmul.locks[dev] = torch.zeros(size, dtype=torch.int32, device=dev)\n        return _sparse_matmul.locks[dev]\n\n    ##########################\n    # SPARSE = DENSE x DENSE #\n    ##########################\n\n    @staticmethod\n    def make_sdd_lut(layout, block, dtype, device):\n        #_sparse_matmul._load_utils()\n        #start_width = 64 // block\n        #segmented = _sparse_matmul.sdd_segment(layout.type(torch.int32), start_width)\n        start_width = (128 if block > 16 else 32) // block\n        layout = layout.type(torch.int32)\n        segmented = libtriton.superblock(layout.data_ptr(), layout.shape[0], layout.shape[1], layout.shape[2],\n                                         start_width)\n        luts, widths, packs = [], [], []\n        for size, nnz in segmented:\n            \"\"\" width = nnz.shape[0] // (size * size)\n            h = nnz[:, 0]\n            i = nnz[:, 1]\n            j = nnz[:, 2]\n            b = nnz[:, 3]\n            lut = torch.stack((h, i, j, b), dim=1).view(-1).contiguous()\n            luts.append(lut.type(torch.int32).to(device))\n            widths.append(width)\n            packs.append(size) \"\"\"\n            nnz = nnz.reshape(-1, 4)\n            width = nnz.shape[0] // (size * size)\n            luts.append(torch.from_numpy(nnz).type(torch.int32).to(device))\n            widths.append(width)\n            packs.append(size)\n        # create locks\n        return luts, None, widths, packs\n\n    @staticmethod\n    def _sdd_matmul(a, b, trans_a, trans_b, trans_c, spdims, block, luts, num_locks, widths, packs, bench, time):\n        if trans_c:\n            a, b = b, a\n            trans_a, trans_b = not trans_b, not trans_a\n        AS0 = a.size(0)\n        # Shape check\n        a_dim = -2 if trans_a else -1\n        b_dim = -1 if trans_b else -2\n        a_inner, b_inner = a.shape[a_dim], b.shape[b_dim]\n        if a_inner != b_inner:\n            raise ValueError(f\"Size of tensor A along the {a_dim} dim ({a_inner}) must match size \"\n                             f\"of tensor B along the {b_dim} dim ({b_inner})\")\n        if a_inner % 16 != 0:\n            raise ValueError('Reduction size for SDD must be a multiple of 16')\n\n        batch_size = a.size(0)\n        a_outer = a.size(3 if trans_a else 2)\n        dtype = a.dtype\n        is_16_multiple = a_inner % 16 == 0\n        is_32_multiple = a_inner % 32 == 0\n        is_64_multiple = a_inner % 64 == 0\n        if not is_16_multiple:\n            raise ValueError('Reduction size for SDD must be a multiple of 16')\n        device = a.device\n        # create kernel\n        total_width = sum([width * pack * pack for width, pack in zip(widths, packs)])\n        c = torch.empty((batch_size, total_width, block, block), dtype=dtype, device=a.device)\n        for lut, width, pack in zip(luts, widths, packs):\n            F32TK = [8, 16]\n            F16TK = [16]\n            F16TK += [32] if is_32_multiple else []\n            F16TK += [64] if is_64_multiple else []\n            TK = {torch.float32: F32TK, torch.float16: F16TK}[dtype]\n            num_lock = 1\n            meta = {\n                'TM': block * pack,\n                'TN': block * pack,\n                'BLOCK': block,\n                'TK': TK[0],\n                'TZ': 1,\n                'SDD': True,\n                'DSD': False,\n                'DDS': False\n            }\n            # create output\n            locks = _sparse_matmul.get_locks(2 * width * AS0 * num_lock, a.device)\n            # maximum grid size is 65535\n            # so operation might be decomposed into multiple\n            # kernel calls\n            max_width = 49152\n            total = 0 if bench else None\n            for off_width in range(0, width, max_width):\n                grid = lambda meta: [meta['TZ'], min(max_width, width - off_width), batch_size]\n                _kernel[grid](a,\n                              b,\n                              c,\n                              a.stride(0),\n                              a.stride(1),\n                              a.stride(3 if trans_a else 2),\n                              a.stride(2 if trans_a else 3),\n                              b.stride(0),\n                              b.stride(1),\n                              b.stride(3 if trans_b else 2),\n                              b.stride(2 if trans_b else 3),\n                              c.stride(0),\n                              c.stride(0),\n                              c.stride(2),\n                              c.stride(3),\n                              a_outer,\n                              a_outer,\n                              a_inner,\n                              off_width,\n                              lut,\n                              locks,\n                              num_lock,\n                              num_warps=4,\n                              **meta)\n        # save for backward pass\n        return c\n\n    ##########################\n    # DENSE = DENSE x SPARSE #\n    ##########################\n\n    # Given a binary layout of 0s and 1s,\n    # Construct look-up table for efficient execution on GPUs\n    @staticmethod\n    def make_dxx_lut(layout, block, step, trans, device, transform=lambda idx: idx):\n        # load-balancing\n        _empty = torch.tensor([], dtype=torch.int64, device=layout.device)\n        segments = _empty.clone()\n        column = _empty.clone()\n        depth = _empty.clone()\n        lockid = _empty.clone()\n        maxid = _empty.clone()\n        offsets = _empty.clone()\n        current_offset = 0\n        current_maxid = 0\n        for z in range(layout.size(0)):\n            if trans:\n                sizes = torch.sum(layout[z, :, :], 1)\n            else:\n                sizes = torch.sum(layout[z, :, :], 0)\n            z_segments, z_column, z_lockid, z_maxid, z_offsets = _sparse_matmul.load_balance(sizes, block)\n            z_depth = z * torch.ones_like(z_segments)\n            z_lockid[z_lockid > 0] += current_maxid\n            current_maxid = z_lockid.max()\n            # concatenate depth\n            segments = torch.cat((segments, z_segments))\n            column = torch.cat((column, z_column))\n            depth = torch.cat((depth, z_depth))\n            maxid = torch.cat((maxid, z_maxid))\n            offsets = torch.cat((offsets, current_offset + z_offsets))\n            lockid = torch.cat((lockid, z_lockid))\n            current_offset += layout[z, :, :].sum()\n        segments *= step\n        # pointer increments\n        if trans:\n            nnz = layout.nonzero()\n        else:\n            nnz = layout.transpose(1, 2).nonzero()\n        num_blocks = nnz.size(0)\n        offsets = torch.min(offsets, (num_blocks - 1) * torch.ones_like(offsets))\n        idx = transform(nnz[:, 2] * block)\n        xincs = idx.clone()\n        xincs[1:] -= idx[:-1]\n        # divide block into multiple steps\n        div = block // step\n        xincs = xincs.view(-1, 1).repeat(1, div)\n        xincs[:, 1:] = step\n        xincs[:, 0] -= (div - 1) * step\n        # first increment for each reduction is actually the offset\n        xincs[offsets[segments > 0], 0] = idx[offsets[segments > 0]]\n        xincs = xincs.view(-1)\n        # block-mode input increments\n        if trans:\n            widx = torch.arange(num_blocks)\n        else:\n            widx = _empty.clone()\n            current_offset = 0\n            for z in range(layout.size(0)):\n                layoutw = layout[z, :, :].clone()\n                msum = layoutw.sum()\n                layoutw[layoutw > 0] = 1 + torch.arange(msum)\n                widx = torch.cat((widx, current_offset + layoutw.T[layoutw.T > 0] - 1))\n                current_offset += msum\n        widx = widx\n        wincs = widx * block * block\n        wincs[1:] -= widx[:-1] * block * block\n        wincs = wincs.view(-1, 1).repeat(1, div)\n        if trans:\n            wincs[:, 1:] = step\n            wincs[:, 0] -= (div - 1) * step\n        else:\n            wincs[:, 1:] = step * block\n            wincs[:, 0] -= (div - 1) * step * block\n        wincs[offsets[segments > 0], 0] = widx[offsets[segments > 0]]\n        wincs = wincs.view(-1)\n        # adjust offset and segment size\n        offsets *= 2 * div\n        segments *= div\n        # create header\n        width = column.size(0)\n        offsets += 6 * width\n        header = torch.stack((offsets, segments, column, depth, lockid, maxid), dim=1).view(-1).contiguous()\n        incs = torch.stack((xincs, wincs), dim=1).view(-1).contiguous()\n        incs = torch.cat((incs, torch.zeros(2, device=incs.device, dtype=incs.dtype)))\n        # create lut\n        lut = torch.cat((header, incs))\n        lut = lut.type(torch.int32).to(device)\n        # create locks\n        num_locks = max(1, lockid.max())\n        return lut, num_locks, width, None\n\n    @staticmethod\n    def _dds_matmul(a, b, trans_a, trans_b, trans_c, spdims, block, lut, num_locks, width, packs, bench, time):\n        global triton\n        if triton is None:\n            triton = importlib.import_module('triton')\n\n        # shapes / dtypes\n        AS0 = a.size(0)\n        AS1 = a.size(1)\n        AS2 = a.size(3 if trans_a else 2)\n        AS3 = a.size(2 if trans_a else 3)\n        BS0 = spdims[0]\n        BS1 = block * spdims[2 if trans_b else 1]\n        BS2 = block * spdims[1 if trans_b else 2]\n        dtype = a.dtype\n        # kernel\n        meta = {'TN': block, 'TM': 128, 'TK': 16, 'BLOCK': block, 'TZ': 1, 'SDD': False, 'DSD': False, 'DDS': True}\n        # output\n        CS0 = AS0\n        CS1 = AS1\n        CS2 = BS2 if trans_c else AS2\n        CS3 = AS2 if trans_c else BS2\n        locks = _sparse_matmul.get_locks(2 * AS0 * AS2 // 32 * num_locks, a.device)\n        c = torch.empty((CS0, CS1, CS2, CS3), dtype=dtype, device=a.device)\n        grid = lambda meta: [width, triton.cdiv(AS2, meta['TM']), AS0]\n        _kernel[grid](a,\n                      b,\n                      c,\n                      a.stride(0),\n                      a.stride(1),\n                      a.stride(3 if trans_a else 2),\n                      a.stride(2 if trans_a else 3),\n                      b.stride(0),\n                      b.stride(1),\n                      b.stride(3 if trans_b else 2),\n                      b.stride(2 if trans_b else 3),\n                      c.stride(0),\n                      c.stride(1),\n                      c.stride(3 if trans_c else 2),\n                      c.stride(2 if trans_c else 3),\n                      AS2,\n                      BS2,\n                      0,\n                      0,\n                      lut,\n                      locks,\n                      num_locks,\n                      num_warps=4,\n                      **meta)\n        return c\n\n    @staticmethod\n    def _dsd_matmul(a, b, trans_a, trans_b, trans_c, spdims, block, lut, num_locks, width, packs, bench, time):\n        global triton\n        if triton is None:\n            triton = importlib.import_module('triton')\n\n        # shapes / dtypes\n        AS0 = spdims[0]\n        AS1 = block * spdims[2 if trans_a else 1]\n        AS2 = block * spdims[1 if trans_a else 2]\n        BS0 = b.size(0)\n        BS1 = b.size(1)\n        BS2 = b.size(3 if trans_b else 2)\n        BS3 = b.size(2 if trans_b else 3)\n        dtype = a.dtype\n        # kernel\n\n        meta = {'TM': block, 'TN': 128, 'TK': 16, 'BLOCK': block, 'TZ': 1, 'SDD': False, 'DSD': True, 'DDS': False}\n        # output\n        CS0 = BS0\n        CS1 = BS1\n        CS2 = BS3 if trans_c else AS1\n        CS3 = AS1 if trans_c else BS3\n        locks = _sparse_matmul.get_locks(2 * BS0 * BS3 // 32 * num_locks, a.device)\n        c = torch.empty((CS0, CS1, CS2, CS3), dtype=dtype, device=a.device)\n        grid = lambda meta: [width, triton.cdiv(BS3, meta['TN']), BS0]\n        _kernel[grid](a,\n                      b,\n                      c,\n                      a.stride(0),\n                      a.stride(1),\n                      a.stride(3 if trans_a else 2),\n                      a.stride(2 if trans_a else 3),\n                      b.stride(0),\n                      b.stride(1),\n                      b.stride(3 if trans_b else 2),\n                      b.stride(2 if trans_b else 3),\n                      c.stride(0),\n                      c.stride(1),\n                      c.stride(2),\n                      c.stride(3),\n                      BS3,\n                      AS1,\n                      0,\n                      0,\n                      lut,\n                      locks,\n                      num_locks,\n                      num_warps=4,\n                      **meta)\n        return c\n\n    fn = {'sdd': _sdd_matmul.__get__(object), 'dsd': _dsd_matmul.__get__(object), 'dds': _dds_matmul.__get__(object)}\n\n    @staticmethod\n    def forward(ctx, a, b, trans_a, trans_b, trans_c, mode, spdims, block, c_lut, c_num_locks, c_width, c_packs,\n                c_bench, c_time, da_lut, da_num_locks, da_width, da_packs, da_bench, da_time, db_lut, db_num_locks,\n                db_width, db_packs, db_bench, db_time):\n        c = _sparse_matmul.fn[mode](a, b, trans_a, trans_b, trans_c, spdims, block, c_lut, c_num_locks, c_width,\n                                    c_packs, c_bench, c_time)\n        # save for backward\n        ctx.save_for_backward(a, b)\n        ctx.da_num_locks = da_num_locks\n        ctx.da_lut = da_lut\n        ctx.da_width = da_width\n        ctx.da_packs = da_packs\n        ctx.da_bench = da_bench\n        ctx.da_time = da_time\n        ctx.db_lut = db_lut\n        ctx.db_num_locks = db_num_locks\n        ctx.db_width = db_width\n        ctx.db_bench = db_bench\n        ctx.db_packs = db_packs\n        ctx.db_time = db_time\n        ctx.mode = mode\n        ctx.spdims = spdims\n        ctx.block = block\n        ctx.trans_a = trans_a\n        ctx.trans_b = trans_b\n        return c\n\n    @staticmethod\n    def backward(ctx, dc):\n        # saved for backward\n        a, b = ctx.saved_tensors\n        mode = ctx.mode\n        # gradients w.r.t. a\n        if ctx.needs_input_grad[0]:\n            mode_da = mode[1] + mode[0] + mode[2]\n            da = _sparse_matmul.fn[mode_da](dc, b, False, not ctx.trans_b, ctx.trans_a, ctx.spdims, ctx.block,\n                                            ctx.da_lut, ctx.da_num_locks, ctx.da_width, ctx.da_packs, ctx.da_bench,\n                                            ctx.da_time)\n        # gradients w.r.t. b\n        if ctx.needs_input_grad[1]:\n            mode_db = mode[2] + mode[1] + mode[0]\n            db = _sparse_matmul.fn[mode_db](a, dc, not ctx.trans_a, False, ctx.trans_b, ctx.spdims, ctx.block,\n                                            ctx.db_lut, ctx.db_num_locks, ctx.db_width, ctx.db_packs, ctx.db_bench,\n                                            ctx.db_time)\n        return da, db, None, None, None,\\\n               None, None, None, None,\\\n               None, None, None, None, None, None,\\\n               None, None, None, None, None, None,\\\n               None, None, None, None, None, None\n\n\nclass MatMul:\n    \"\"\"Block-Sparse MatMul class; this class handles three types of matrix-multiplication:\n       - sparse = dense X dense\n       - dense = sparse X dense\n       - dense = dense X sparse\n\n    For more details about sparsity config, please see `Generative Modeling with Sparse Transformers`: https://arxiv.org/abs/1904.10509\n    \"\"\"\n\n    def make_lut(self, dtype, device):\n        \"\"\"Generates the sparsity layout/s used in block-sparse matmul\n        \"\"\"\n        key = (dtype, device)\n        if key in self.lut_cache:\n            return self.lut_cache[key]\n        # C look-up table\n        layout, block = self.layout, self.block\n        step = 16\n        if self.mode == 'sdd':\n            c_lut, c_num_locks, c_width, c_packs = _sparse_matmul.make_sdd_lut(layout, block, dtype, device)\n        elif self.mode == 'dsd':\n            c_lut, c_num_locks, c_width, c_packs = _sparse_matmul.make_dxx_lut(layout, block, step, not self.trans_a,\n                                                                               device)\n        elif self.mode == 'dds':\n            c_lut, c_num_locks, c_width, c_packs = _sparse_matmul.make_dxx_lut(layout, block, step, self.trans_b,\n                                                                               device)\n        # DA look-up table\n        if self.mode == 'sdd':\n            da_lut, da_num_locks, da_width, da_packs = _sparse_matmul.make_dxx_lut(layout, block, step, True, device)\n        elif self.mode == 'dsd':\n            da_lut, da_num_locks, da_width, da_packs = _sparse_matmul.make_sdd_lut(layout, block, dtype, device)\n        elif self.mode == 'dds':\n            da_lut, da_num_locks, da_width, da_packs = _sparse_matmul.make_dxx_lut(layout, block, step,\n                                                                                   not self.trans_b, device)\n        # DB look-up table\n        if self.mode == 'sdd':\n            db_lut, db_num_locks, db_width, db_packs = _sparse_matmul.make_dxx_lut(layout, block, step, False, device)\n        elif self.mode == 'dsd':\n            db_lut, db_num_locks, db_width, db_packs = _sparse_matmul.make_dxx_lut(layout, block, step, self.trans_a,\n                                                                                   device)\n        elif self.mode == 'dds':\n            db_lut, db_num_locks, db_width, db_packs = _sparse_matmul.make_sdd_lut(layout, block, dtype, device)\n        self.lut_cache[key] = (c_lut, c_num_locks, c_width, c_packs,\\\n                               da_lut, da_num_locks, da_width, da_packs,\\\n                               db_lut, db_num_locks, db_width, db_packs)\n        return self.lut_cache[key]\n\n    def __init__(self, layout, block, mode, trans_a=False, trans_b=False, bench=False):\n        \"\"\"Initialize the Block-Sparse MatMul class.\n\n        Arguments:\n             layout: required: sparsity layout tensor\n             block: required: an integer determining the block size.\n             mode: required: a string determining type of matmul; ('sdd') sparse = dense X dense, ('dsd') dense = sparse X dense, ('dds') dense = dense X sparse\n             trans_a: optional: a boolean determining if multiplication needs to be applied on transpose of input a; default is false\n             trans_b: optional: a boolean determining if multiplication needs to be applied on transpose of input b; default is false\n             bench: optional: set if you want to do benchmarking\n        \"\"\"\n\n        if mode not in ['sdd', 'dsd', 'dds']:\n            raise NotImplementedError('Supported modes are: sdd, dsd, dds')\n        # look-up table cache\n        self.lut_cache = dict()\n        # attributes\n        self.trans_a = trans_a\n        self.trans_b = trans_b\n        self.mode = mode\n        self.block = block\n        self.layout = layout\n        layout_dim = layout.ndim\n        assert layout_dim in (2, 3), \"Layout should be a 2 or 3 dimensional tensor of 0s and 1s\"\n        if not mode == 'sdd':\n            # Dims to be reduced on the 'inside' of the matmul, either -1 or -2\n            trans_dense, trans_sparse, sparse_inner = (trans_b, trans_a, -1) if mode == 'dsd' else (trans_a, trans_b,\n                                                                                                    -2)\n            self.dense_inner_dim = -((sparse_inner % 2) + 1) if not trans_dense else sparse_inner\n            sparse_inner = sparse_inner if not trans_sparse else -((sparse_inner % 2) + 1)\n\n            # Inner dim of the dense input should be equal to the inner dim of the sparse input\n            self.dense_inner_size = layout.shape[sparse_inner] * block\n            # Expected shape for sparse inputs\n            self.sparse_shape = (layout.sum().item(), block, block)\n\n        # Support using the same layout across attention heads etc.\n        if layout_dim == 2:\n            layout = layout.unsqueeze(0)\n\n        layout = layout.long()  # Above code assumes the layout tensor is an integral type\n\n        self.spdims = layout.shape\n        # timings\n        self.bench = bench\n        self.time_c = None\n        self.time_da = None\n        self.time_db = None\n\n    # pad shapes of a tensor to make it\n    # compatible with kernel calls\n    @staticmethod\n    def _pad_shape(x, is_sparse):\n        max_dim = 3 if is_sparse else 4\n        for i in range(max_dim - x.dim()):\n            x = x.unsqueeze(0)\n        return x\n\n    def __call__(self, a, b):\n        \"\"\"Applies Block-Sparse MatMul.\n\n        For more details about sparsity config, please see `Generative Modeling with Sparse Transformers`: https://arxiv.org/abs/1904.10509\n\n        Arguments:\n             a: required: a dense/block-sparse tensor; first input of mat-mul\n             b: required: a dense/block-sparse tensor; second input of mat-mul\n\n        Return:\n             c: a dense/block-sparse tensor result of a X b\n        \"\"\"\n\n\n        c_lut, c_num_locks, c_width, c_packs,\\\n        da_lut, da_num_locks, da_width, da_packs,\\\n        db_lut, db_num_locks, db_width, db_packs = self.make_lut(a.dtype, a.device)\n        # timings\n        time_c = [None]\n        time_da = [None]\n        time_db = [None]\n\n        original_dims = max(a.ndim, b.ndim)\n        a, b = self._validate_inputs(a, b)\n\n        # pad shapes with ones\n        a = MatMul._pad_shape(a, self.mode == 'dsd')\n        b = MatMul._pad_shape(b, self.mode == 'dds')\n        # execute\n\n        c = _sparse_matmul.apply(a, b, self.trans_a, self.trans_b, False, self.mode, self.spdims, self.block, c_lut,\n                                 c_num_locks, c_width, c_packs, self.bench, time_c, da_lut, da_num_locks, da_width,\n                                 da_packs, self.bench, time_da, db_lut, db_num_locks, db_width, db_packs, self.bench,\n                                 time_db)\n\n        # This removes any leading singleton dimensions we may have added to the tensor that weren't in the input\n        dims_to_trim = c.ndim - original_dims\n        for _ in range(dims_to_trim):\n            c = c.squeeze(0)\n\n        self.time_c = time_c[0]\n        self.time_da = time_da[0]\n        self.time_db = time_db[0]\n        return c\n\n    def _validate_inputs(self, a, b):\n        if a.device != b.device:\n            raise ValueError(f\"Inputs must be on the same device; got {a.device} for tensor A \"\n                             f\"and {b.device} for tensor B\")\n        if not get_accelerator().on_accelerator(a):\n            raise ValueError(\"Only GPU devices are supported for now\")\n\n        # When autocast is enabled, torch.matmul autocasts to float16, so we do the same here\n        if torch.is_autocast_enabled():\n            a, b = a.half(), b.half()\n        elif a.dtype != b.dtype:\n            raise ValueError(f\"Inputs must be the same dtype; got {a.dtype} for A and {b.dtype} for B\")\n\n        mode, trans_a, trans_b = self.mode, self.trans_a, self.trans_b\n        if mode != 'sdd':\n            # One input is sparse\n            dense, dense_name, sparse, sparse_name = (a, 'A', b, 'B') if mode == 'dds' else (b, 'B', a, 'A')\n            dense_inner = dense.shape[self.dense_inner_dim]\n            if dense_inner != self.dense_inner_size:\n                raise ValueError(f\"Expected tensor {dense_name} to have size {self.dense_inner_size} at dim \"\n                                 f\"{self.dense_inner_dim % dense.ndim}, got {dense_inner}.\")\n\n            if sparse.shape[-len(self.sparse_shape):] != self.sparse_shape:\n                raise ValueError(f\"Expected tensor with trailing dimensions of shape {self.sparse_shape} for argument \"\n                                 f\"{sparse_name}, got {sparse.shape}\")\n\n        def add_extra_dims(x):\n            # Add extra leading singleton dimensions if needed\n            dims_needed = 4 - x.ndim\n            if dims_needed > 0:\n                singletons = [1] * dims_needed\n                x = x.view(*singletons, *x.shape)\n            elif dims_needed < 0:\n                raise ValueError(\"Tensors with more than 4 dimensions are not currently supported\")\n\n            return x\n\n        # Pad shapes with leading singleton dimensions\n        a = add_extra_dims(a)\n        b = add_extra_dims(b)\n\n        return a, b\n", "deepspeed/ops/sparse_attention/sparse_self_attention.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch.nn as nn\nimport torch\nfrom torch import distributed as dist\nfrom deepspeed.ops.sparse_attention import SparsityConfig\n\n\nclass SparseSelfAttention(nn.Module):\n    \"\"\"Implements an efficient Sparse Self Attention of Transformer layer based on `Generative Modeling with Sparse Transformers`: https://arxiv.org/abs/1904.10509\n\n    For more information please see, TODO DeepSpeed Sparse Transformer.\n\n    For usage example please see, TODO DeepSpeed Sparse Transformer Tutorial.\n    \"\"\"\n\n    def __init__(\n            self,\n            # SparsityConfig parameters needs to be set accordingly\n            sparsity_config=SparsityConfig(num_heads=4),\n            key_padding_mask_mode='add',\n            attn_mask_mode='mul',\n            max_seq_length=2048):\n        \"\"\"Initialize the sparse self attention layer.\n        Arguments:\n            sparsity_config: optional: this parameter determines sparsity pattern configuration; it is based on SparsityConfig class.\n            key_padding_mask_mode: optional: a string determining if key padding mask needs to be added, `add`, or be multiplied, `mul`.\n            attn_mask_mode: optional: a string determining if attention mask needs to be added, `add`, or be multiplied, `mul`.\n            max_seq_length: optional: the maximum sequence length this sparse attention module will be applied to; it controls the size of the master_layout.\n        \"\"\"\n        super().__init__()\n\n        # sparsity information\n        self.sparsity_config = sparsity_config\n\n        # initialize sparse layout and register as buffer\n        master_layout = self.sparsity_config.make_layout(max_seq_length)\n        self.register_buffer(\"master_layout\", master_layout)\n        self._need_layout_synchronization = True\n\n        # mask modes\n        self.key_padding_mask_mode = key_padding_mask_mode\n        self.attn_mask_mode = attn_mask_mode\n\n    ops = dict()\n\n    def get_layout(self, L):\n        # if layout is never synchronized across GPUs, broadcast the layout from global rank 0\n        if self._need_layout_synchronization and dist.is_initialized():\n            dist.broadcast(self.master_layout, src=0)\n            self._need_layout_synchronization = False\n\n        if (L % self.sparsity_config.block != 0):\n            raise ValueError(\n                f'Sequence Length, {L}, needs to be dividable by Block size {self.sparsity_config.block}!')\n\n        num_blocks = L // self.sparsity_config.block\n        return self.master_layout[..., :num_blocks, :num_blocks].cpu()  # layout needs to be a CPU tensor\n\n    # add to cache\n    def get_ops(self, H, L):\n        from deepspeed.ops.sparse_attention.matmul import MatMul\n        from deepspeed.ops.sparse_attention.softmax import Softmax\n        if L not in SparseSelfAttention.ops:\n            sparsity_layout = self.get_layout(L)\n            sparse_dot_sdd_nt = MatMul(sparsity_layout, self.sparsity_config.block, 'sdd', trans_a=False, trans_b=True)\n\n            sparse_dot_dsd_nn = MatMul(sparsity_layout,\n                                       self.sparsity_config.block,\n                                       'dsd',\n                                       trans_a=False,\n                                       trans_b=False)\n\n            sparse_softmax = Softmax(sparsity_layout, self.sparsity_config.block)\n\n            SparseSelfAttention.ops[L] = (sparse_dot_sdd_nt, sparse_dot_dsd_nn, sparse_softmax)\n        return SparseSelfAttention.ops[L]\n\n    def transpose_key_for_scores(self, x, L):\n        bsz, num_heads, seq_len, head_dim = x.size()\n        if seq_len != L:\n            return x.permute(0, 1, 3, 2)\n        return x\n\n    def transpose_mask_for_sparse(self, qtype, x, is_key_padding_mask=False):\n        x = x.type(qtype)\n        if is_key_padding_mask:\n            xdim = x.dim()\n            for d in range(xdim - 1, 0, -1):\n                x = x.squeeze(dim=d)\n            return x\n        return x.squeeze()\n\n    # forward pass\n    def forward(self, query, key, value, rpe=None, key_padding_mask=None, attn_mask=None):\n        \"\"\"Applies forward phase of sparse self attention\n\n        Arguments:\n            query: required: query tensor\n            key: required: key tensor\n            value: required: value tensor\n            rpe: optional: a tensor same dimension as x that is used as relative position embedding\n            key_padding_mask: optional: a mask tensor of size (BatchSize X SequenceLength)\n            attn_mask: optional: a mask tensor of size (SequenceLength X SequenceLength); currently only 2D is supported\n            key_padding_mask_mode: optional: a boolean determining if key_padding_mask needs to be added or multiplied\n            attn_mask_mode: optional: a boolean determining if attn_mask needs to be added or multiplied\n\n        Return:\n             attn_output: a dense tensor containing attention context\n        \"\"\"\n        assert query.dtype == torch.half, \"sparse attention only supports training in fp16 currently, please file a github issue if you need fp32 support\"\n        bsz, num_heads, tgt_len, head_dim = query.size()\n\n        # transpose back key if it is already transposed\n        key = self.transpose_key_for_scores(key, tgt_len)\n\n        # check that operation is supported\n        if query.shape != key.shape or key.shape != value.shape:\n            raise NotImplementedError('only self-attention is supported for now')\n\n        # squeeze key_padding_mask if it is given\n        if key_padding_mask is not None:\n            key_padding_mask = self.transpose_mask_for_sparse(query.dtype, key_padding_mask, is_key_padding_mask=True)\n\n        # squeeze attn_mask if it is given\n        if attn_mask is not None:\n            attn_mask = self.transpose_mask_for_sparse(query.dtype, attn_mask)\n\n        # cache look-up table computations etc\n        sparse_dot_sdd_nt, sparse_dot_dsd_nn, sparse_softmax = self.get_ops(num_heads, tgt_len)\n\n        scaling = float(head_dim)**-0.5\n\n        # attention scores\n        attn_output_weights = sparse_dot_sdd_nt(query, key)\n        attn_output_weights = sparse_softmax(attn_output_weights,\n                                             scale=scaling,\n                                             rpe=rpe,\n                                             key_padding_mask=key_padding_mask,\n                                             attn_mask=attn_mask,\n                                             key_padding_mask_mode=self.key_padding_mask_mode,\n                                             attn_mask_mode=self.attn_mask_mode)\n\n        # outputs\n        attn_output = sparse_dot_dsd_nn(attn_output_weights, value)\n        return attn_output\n", "deepspeed/ops/sparse_attention/bert_sparse_self_attention.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom torch import nn\nfrom deepspeed.ops.sparse_attention import SparseSelfAttention, FixedSparsityConfig\n\n\nclass BertSparseSelfAttention(nn.Module):\n    \"\"\"Implements Sparse Self Attention layer of Bert model based on https://github.com/microsoft/DeepSpeedExamples/blob/master/bing_bert/nvidia/modelingpreln.py#L373\n\n    For more information please see, TODO DeepSpeed Sparse Transformer.\n\n    For usage example please see, TODO DeepSpeed Sparse Transformer Tutorial.\n    \"\"\"\n\n    def __init__(\n        self,\n        config,\n        # SparsityConfig parameters needs to be set accordingly\n        sparsity_config=FixedSparsityConfig(num_heads=4)):\n        \"\"\"Initialize the bert sparse self attention layer.\n\n        Note) you can use any of the provided sparsity configs or simply add yours!\n\n        Arguments:\n            config: required: Bert model config\n            sparsity_config: optional: this parameter determines sparsity pattern configuration; it is based on FixedSparsityConfig class.\n        \"\"\"\n\n        super(BertSparseSelfAttention, self).__init__()\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\"The hidden size (%d) is not a multiple of the number of attention \"\n                             \"heads (%d)\" % (config.hidden_size, config.num_attention_heads))\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n\n        self.sparse_self_attention = SparseSelfAttention(sparsity_config)\n\n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(self, hidden_states, attention_mask):\n        \"\"\"Applies forward phase of bert sparse self attention\n\n        Arguments:\n            hidden_states: required: hidden_states tensor of the bert model\n            attn_mask: required: a mask tensor of size (SequenceLength X SequenceLength); currently only 2D is supported\n\n        Return:\n             context_layer: a dense tensor containing attention context\n        \"\"\"\n        mixed_query_layer = self.query(hidden_states)\n        mixed_key_layer = self.key(hidden_states)\n        mixed_value_layer = self.value(hidden_states)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n        key_layer = self.transpose_for_scores(mixed_key_layer)\n        value_layer = self.transpose_for_scores(mixed_value_layer)\n\n        context_layer = self.sparse_self_attention(query_layer,\n                                                   key_layer,\n                                                   value_layer,\n                                                   key_padding_mask=attention_mask)\n\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size, )\n        context_layer = context_layer.view(*new_context_layer_shape)\n        return context_layer\n", "deepspeed/ops/sparse_attention/sparsity_config.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nimport random\n\n\nclass SparsityConfig:\n    \"\"\"Abstract Configuration class to store `sparsity configuration of a self attention layer`.\n    It contains shared property of different block-sparse sparsity patterns. However, each class needs to extend it based on required property and functionality.\n    \"\"\"\n\n    def __init__(self, num_heads, block=16, different_layout_per_head=False):\n        \"\"\"Initialize the Sparsity Pattern Config.\n\n        For usage example please see, TODO DeepSpeed Sparse Transformer Tutorial\n\n        Arguments:\n             num_heads: required: an integer determining number of attention heads of the layer.\n             block: optional: an integer determining the block size. Current implementation of sparse self-attention is based on blocked sparse matrices. In which this parameter defines size of such blocks, `Block X Block`.\n             different_layout_per_head: optional: a boolean determining if each head should be assigned a different sparsity layout; default is false and this will be satisfied based on availability.\n        \"\"\"\n\n        self.num_heads = num_heads\n        self.block = block\n        self.different_layout_per_head = different_layout_per_head\n        self.num_layout_heads = num_heads if different_layout_per_head else 1\n\n    def setup_layout(self, seq_len):\n        \"\"\"Create layout tensor for the given sequence length\n\n        Arguments:\n             seq_len: required: an integer determining number of attention heads of the layer.\n\n        Return:\n             layout: a tensor of dimension (num_heads, num_blocks, num_blocks) for sparsity layout of all head; initialized with zero\n        \"\"\"\n\n        if (seq_len % self.block != 0):\n            raise ValueError(f'Sequence Length, {seq_len}, needs to be dividable by Block size {self.block}!')\n        num_blocks = seq_len // self.block\n        # TODO Currently we allocate layout per head; needs to be updated if heads share a single layout.\n        layout = torch.zeros((self.num_heads, num_blocks, num_blocks), dtype=torch.int64)\n        return layout\n\n    def check_and_propagate_first_head_layout(self, layout):\n        \"\"\"If all heads require same sparsity layout, it propagate first head layout to all heads\n\n        Arguments:\n             layout: required: a tensor of dimension (num_heads, num_blocks, num_blocks) containing sparsity layout of all head; may not be completely set at this step\n\n        Return:\n             layout: a tensor of dimension (num_heads, num_blocks, num_blocks) containing sparsity layout of all head\n        \"\"\"\n\n        if not self.different_layout_per_head:\n            layout[1:self.num_heads, :, :] = layout[0, :, :]\n        return layout\n\n\nclass DenseSparsityConfig(SparsityConfig):\n    \"\"\"Configuration class to store `Dense` configuration.\n    In reality, this is not sparse and all blocks are used. We keep it for the sake of comparison and comprehension.\n    \"\"\"\n\n    def __init__(self, num_heads, block=16, different_layout_per_head=False):\n        \"\"\"Initialize the Dense Sparsity Pattern Config.\n        In reality, this is not sparse and all blocks are used. We keep it for the sake of comparison and comprehension.\n\n        Arguments:\n             num_heads: required: an integer determining number of attention heads of the layer.\n             seq_len: required: an integer determining number of attention heads of the layer.\n             different_layout_per_head: optional: this is just for the sake of consistency with other sparsity formats; can ignore it for DenseSparsityConfig\n        \"\"\"\n\n        super().__init__(num_heads, block, different_layout_per_head)\n\n    def make_layout(self, seq_len):\n        \"\"\"Set 1 to all blocks of the layout meaning the pattern is dense; not sparse.\n\n        Arguments:\n             seq_len: required: an integer determining the underling sequence length; must be <= max sequence length\n\n        Return:\n             layout: a tensor of dimension (num_heads, num_blocks, num_blocks) containing sparsity layout of all head; for dense everything is 1\n        \"\"\"\n\n        layout = self.setup_layout(seq_len)\n        layout[:, :, :] = 1\n        return layout\n\n\nclass FixedSparsityConfig(SparsityConfig):\n    \"\"\"Configuration class to store `Fixed` sparsity configuration.\n    For more details about this sparsity config, please see `Generative Modeling with Sparse Transformers`: https://arxiv.org/abs/1904.10509; this has been customized.\n    This class extends parent class of `SparsityConfig` and customizes it for `Fixed` sparsity.\n    \"\"\"\n\n    def __init__(self,\n                 num_heads,\n                 block=16,\n                 different_layout_per_head=False,\n                 num_local_blocks=4,\n                 num_global_blocks=1,\n                 attention='bidirectional',\n                 horizontal_global_attention=False,\n                 num_different_global_patterns=1):\n        \"\"\"Initialize `Fixed` Sparsity Pattern Config.\n\n        For usage example please see, TODO DeepSpeed Sparse Transformer Tutorial\n\n        Arguments:\n             num_heads: required: an integer determining number of attention heads of the layer.\n             block: optional: an integer determining the block size. Current implementation of sparse self-attention is based on blocked sparse matrices. In which this parameter defines size of such blocks, `Block X Block`.\n             different_layout_per_head: optional: a boolean determining if each head should be assigned a different sparsity layout; default is false and this will be satisfied based on availability.\n             num_local_blocks: optional: an integer determining the number of blocks in local attention window.\n             num_global_blocks: optional: an integer determining how many consecutive blocks in a local window is used as the representative of the window for global attention.\n             attention: optional: a string determining attention type. Attention can be `unidirectional`, such as autoregressive models, in which tokens attend only to tokens appear before them in the context. Considering that, the upper triangular of attention matrix is empty as above figure. Or it can be `bidirectional`, such as BERT, in which tokens can attend to any other tokens before or after them. Then, the upper triangular part of the attention matrix is mirror of the lower triangular in the above figure.\n             horizontal_global_attention: optional: a boolean determining if blocks that are global representative of a local window, also attend to all other blocks. This is valid only if attention type is `bidirectional`. Looking at the attention matrix, that means global attention not only includes the vertical blocks, but also horizontal blocks.\n             num_different_global_patterns: optional: an integer determining number of different global attentions layouts. While global attention can be fixed by which block/s are representative of any local window, since there are multi-heads, each head can use a different global representative. For example, with 4 blocks local window and global attention size of 1 block, we can have 4 different versions in which the first, Second, third, or forth block of each local window can be global representative of that window. This parameter determines how many of such patterns we want. Of course, there is a limitation based on num_local_blocks and num_global_blocks.\n        \"\"\"\n\n        super().__init__(num_heads, block, different_layout_per_head)\n\n        self.num_local_blocks = num_local_blocks\n\n        if (num_local_blocks % num_global_blocks != 0):\n            raise ValueError(\n                f'Number of blocks in a local window, {num_local_blocks}, must be dividable by number of global blocks, {num_global_blocks}!'\n            )\n        self.num_global_blocks = num_global_blocks\n\n        if (attention != 'unidirectional' and attention != 'bidirectional'):\n            raise NotImplementedError('only \\\"uni/bi-directional\\\" attentions are supported for now!')\n        self.attention = attention\n\n        if (attention != 'bidirectional' and horizontal_global_attention):\n            raise ValueError('only \\\"bi-directional\\\" attentions can support horizontal global attention!')\n        self.horizontal_global_attention = horizontal_global_attention\n\n        if (num_different_global_patterns > 1 and not different_layout_per_head):\n            raise ValueError(\n                f'Number of different layouts cannot be more than one when you have set a single layout for all heads! Set different_layout_per_head to True.'\n            )\n        if (num_different_global_patterns > (num_local_blocks // num_global_blocks)):\n            raise ValueError(\n                f'Number of layout versions (num_different_global_patterns), {num_different_global_patterns}, cannot be larger than number of local window blocks divided by number of global blocks, {num_local_blocks} / {num_global_blocks} = {num_local_blocks//num_global_blocks}!'\n            )\n        self.num_different_global_patterns = num_different_global_patterns\n\n    def set_local_layout(self, h, layout):\n        \"\"\"Sets local attention layout used by the given head in the sparse attention.\n\n        Arguments:\n             h: required: an integer determining head index\n             layout: required: a tensor of dimension (num_heads, num_blocks, num_blocks) containing sparsity layout of all head; may not be completely set at this step\n\n        Return:\n             layout: a tensor of dimension (num_heads, num_blocks, num_blocks) containing sparsity layout of all head in which local layout is set\n        \"\"\"\n\n        num_blocks = layout.shape[1]\n        for i in range(0, num_blocks, self.num_local_blocks):\n            end = min(i + self.num_local_blocks, num_blocks)\n            for row in range(i, end):\n                for col in range(i, (row + 1 if self.attention == 'unidirectional' else end)):\n                    layout[h, row, col] = 1\n        return layout\n\n    def set_global_layout(self, h, layout):\n        \"\"\"Sets global attention layout used by the given head in the sparse attention.\n\n        Currently we set global blocks starting from the last block of a local window to the first one. That means if a local window consists of 4 blocks and global attention size is one block, we use block #4 in each local window as global. If we have different layout per head, then other heads will get #3, #2, and #1. And if we have more heads (and different layout has set) than num of global attentions, multiple head may have same global attentions.\n        Note) if horizontal_global_attention is set, global blocks will be set both horizontally and vertically.\n\n        Arguments:\n             h: required: an integer determining head index\n             layout: required: a tensor of dimension (num_heads, num_blocks, num_blocks) containing sparsity layout of all head; may not be completely set at this step\n\n        Return:\n             layout: a tensor of dimension (num_heads, num_blocks, num_blocks) containing sparsity layout of all head in which global layout is set\n        \"\"\"\n\n        num_blocks = layout.shape[1]\n        first_global_block_idx = self.num_local_blocks - (\n            1 + h % self.num_different_global_patterns) * self.num_global_blocks\n\n        # set all global blocks except the last one if (in last local window)\n        end = num_blocks - (num_blocks % self.num_local_blocks)\n        for i in range(first_global_block_idx, end, self.num_local_blocks):\n\n            # vertical global attention\n            first_row = 0 if self.attention == 'bidirectional' else i\n            #(((i // self.num_local_blocks) + 1) * self.num_local_blocks)\n            #if (first_row < num_blocks):\n            layout[h, first_row:, i:i + self.num_global_blocks] = 1\n\n            # horizontal global attention; only in bidirectional attention\n            if (self.horizontal_global_attention):\n                layout[h, i:i + self.num_global_blocks, :] = 1\n\n        # set last global blocks; handle possible short last local window\n        if (end < num_blocks):\n            start = min(end + first_global_block_idx, num_blocks - self.num_global_blocks)\n            end = start + self.num_global_blocks\n\n            # vertical global attention\n            first_row = 0 if self.attention == 'bidirectional' else start\n            #(((start // self.num_local_blocks) + 1) * self.num_local_blocks)\n            #if (first_row < num_blocks):\n            layout[h, first_row:, start:end] = 1\n\n            # horizontal global attention\n            if (self.horizontal_global_attention):\n                layout[h, start:end, :] = 1\n        return layout\n\n    def make_layout(self, seq_len):\n        \"\"\"Generates `Fixed` sparsity layout used by each head in the sparse attention.\n\n        Arguments:\n             seq_len: required: an integer determining number of attention heads of the layer.\n\n        Return:\n             layout: a tensor of dimension (num_heads, num_blocks, num_blocks) containing `Fixed` sparsity layout of all head\n        \"\"\"\n\n        layout = self.setup_layout(seq_len)\n        for h in range(0, self.num_layout_heads):\n            layout = self.set_local_layout(h, layout)\n            layout = self.set_global_layout(h, layout)\n\n        layout = self.check_and_propagate_first_head_layout(layout)\n        return layout\n\n\nclass VariableSparsityConfig(SparsityConfig):\n    \"\"\"Configuration class to store `Variable` sparsity configuration.\n    This layout is an extension of FixedSparsityConfig in which:\n      - user can set random layout; default value is zero means no random block\n      - user can provide a list of local block sizes\n      - user can provide a list of global block indices.\n\n    For more details about `Fixed` sparsity config, please see `Generative Modeling with Sparse Transformers`: https://arxiv.org/abs/1904.10509; this has been customized.\n    This class extends parent class of `SparsityConfig` and customizes it for `Fixed` sparsity.\n    \"\"\"\n\n    def __init__(self,\n                 num_heads,\n                 block=16,\n                 different_layout_per_head=False,\n                 num_random_blocks=0,\n                 local_window_blocks=[4],\n                 global_block_indices=[0],\n                 global_block_end_indices=None,\n                 attention='bidirectional',\n                 horizontal_global_attention=False):\n        \"\"\"Initialize `Variable` Sparsity Pattern Config.\n\n        For usage example please see, TODO DeepSpeed Sparse Transformer Tutorial\n\n        Arguments:\n             num_heads: required: an integer determining number of attention heads of the layer.\n             block: optional: an integer determining the block size. Current implementation of sparse self-attention is based on blocked sparse matrices. In which this parameter defines size of such blocks, `Block X Block`.\n             different_layout_per_head: optional: a boolean determining if each head should be assigned a different sparsity layout; default is false and this will be satisfied based on availability. Currently this sparsity config can only assign single layout to all heads; needs to be extended for different layout per head.\n             num_random_blocks: optional: an integer determining the number of random blocks in each block row.\n             local_window_blocks: optional: a list of integers determining the number of blocks in each local attention window. It assumes first number determines # of blocks in the first local window, second the second window, ..., and the last number determines the number of blocks in the remaining local windows.\n             global_block_indices: optional: a list of integers determining which blocks are considered as global attention. Given indices, determine the blocks that all other token blocks attend to and they attend to all other token blocks. Default value is only index 0. Notice that if global_block_end_indices parameter is set, this parameter is used as starting index of each global window.\n             global_block_end_indices: optional: a list of integers determining end indices of global window blocks. By default this is not used. But if it is set, it must have the same size of global_block_indices parameter, and combining this two parameters, for each index i, blocks from global_block_indices[i] to global_block_end_indices[i] (exclusive) are considered as global attention.\n             num_global_blocks: optional: an integer determining how many consecutive blocks in a local window is used as the representative of the window for global attention.\n             attention: optional: a string determining attention type. Attention can be `unidirectional`, such as autoregressive models, in which tokens attend only to tokens appear before them in the context. Considering that, the upper triangular of attention matrix is empty as above figure. Or it can be `bidirectional`, such as BERT, in which tokens can attend to any other tokens before or after them. Then, the upper triangular part of the attention matrix is mirror of the lower triangular in the above figure.\n             horizontal_global_attention: optional: a boolean determining if blocks that are global representative of a local window, also attend to all other blocks. This is valid only if attention type is `bidirectional`. Looking at the attention matrix, that means global attention not only includes the vertical blocks, but also horizontal blocks.\n        \"\"\"\n\n        super().__init__(num_heads, block, different_layout_per_head)\n\n        self.num_random_blocks = num_random_blocks\n        self.local_window_blocks = local_window_blocks\n        self.global_block_indices = global_block_indices\n\n        if (global_block_end_indices is not None):\n            if (len(global_block_indices) != len(global_block_end_indices)):\n                raise ValueError(\n                    f'Global block start indices length, {len(global_block_indices)}, must be same as global block end indices length, {len(global_block_end_indices)}!'\n                )\n            for _, (start_idx, end_idx) in enumerate(zip(global_block_indices, global_block_end_indices)):\n                if start_idx >= end_idx:\n                    raise ValueError(\n                        f'Global block start index, {start_idx}, must be smaller than global block end index, {end_idx}!'\n                    )\n        self.global_block_end_indices = global_block_end_indices\n\n        if (attention != 'unidirectional' and attention != 'bidirectional'):\n            raise NotImplementedError('only \\\"uni/bi-directional\\\" attentions are supported for now!')\n        self.attention = attention\n\n        if (attention != 'bidirectional' and horizontal_global_attention):\n            raise ValueError('only \\\"bi-directional\\\" attentions can support horizontal global attention!')\n        self.horizontal_global_attention = horizontal_global_attention\n\n    def set_random_layout(self, h, layout):\n        \"\"\"Sets random attention layout used by the given head in the sparse attention.\n        Note) By default, it assumes there will be a unique random block layout for all heads; unless `different_layout_per_head` parameter is set in which each head can have a different random layout.\n\n        Arguments:\n             h: required: an integer determining head index\n             layout: required: a tensor of dimension (num_heads, num_blocks, num_blocks) containing sparsity layout of all head; may not be completely set at this step\n\n        Return:\n             layout: a tensor of dimension (num_heads, num_blocks, num_blocks) containing sparsity layout of all head in which random layout is set\n        \"\"\"\n\n        num_blocks = layout.shape[1]\n        if (num_blocks < self.num_random_blocks):\n            raise ValueError(\n                f'Number of random blocks, {self.num_random_blocks}, must be smaller than overall number of blocks in a row, {num_blocks}!'\n            )\n        for row in range(0, num_blocks):\n            rnd_cols = random.sample(range(0, num_blocks), self.num_random_blocks)\n            layout[h, row, rnd_cols] = 1\n        return layout\n\n    def set_local_layout(self, h, layout):\n        \"\"\"Sets local attention layout used by the given head in the sparse attention.\n        Arguments:\n             h: required: an integer determining head index\n             layout: required: a tensor of dimension (num_heads, num_blocks, num_blocks) containing sparsity layout of all head; may not be completely set at this step\n\n        Return:\n             layout: a tensor of dimension (num_heads, num_blocks, num_blocks) containing sparsity layout of all head in which local layout is set\n        \"\"\"\n\n        num_blocks = layout.shape[1]\n        start_block_idx = 0\n        end_block_idx = 0\n        for block_size in self.local_window_blocks:\n            end_block_idx += block_size\n            end_block_idx = min(end_block_idx, num_blocks)\n            for row in range(start_block_idx, end_block_idx):\n                for col in range(start_block_idx, (row + 1 if self.attention == 'unidirectional' else end_block_idx)):\n                    layout[h, row, col] = 1\n            start_block_idx += block_size\n\n        # if there is any remaining not attended part, use the lats local window block size as local window for the remaining applicable local windows\n        for i in range(start_block_idx, num_blocks, block_size):\n            end_block_idx = min(i + block_size, num_blocks)\n            for row in range(i, end_block_idx):\n                for col in range(i, (row + 1 if self.attention == 'unidirectional' else end_block_idx)):\n                    layout[h, row, col] = 1\n        return layout\n\n    def set_global_layout(self, h, layout):\n        \"\"\"Sets global attention layout used by the given head in the sparse attention.\n\n        Arguments:\n             h: required: an integer determining head index\n             layout: required: a tensor of dimension (num_heads, num_blocks, num_blocks) containing sparsity layout of all head; may not be completely set at this step\n\n        Return:\n             layout: a tensor of dimension (num_heads, num_blocks, num_blocks) containing sparsity layout of all head in which global layout is set\n        \"\"\"\n\n        num_blocks = layout.shape[1]\n        if (self.global_block_end_indices is None):\n            for idx in self.global_block_indices:\n                # if global block idx is in the range of the sequence blocks\n                if (idx < num_blocks):\n                    #global rows\n                    if (self.horizontal_global_attention):\n                        layout[h, idx, :] = 1\n\n                    #global columns\n                    first_row = 0 if self.attention == 'bidirectional' else idx\n                    layout[h, first_row:, idx] = 1\n        else:\n            for _, (start_idx, end_idx) in enumerate(zip(self.global_block_indices, self.global_block_end_indices)):\n                # if global block idx is in the range of the sequence blocks\n                if (start_idx < num_blocks):\n                    end_idx = min(end_idx, num_blocks)\n                    #global rows\n                    if (self.horizontal_global_attention):\n                        layout[h, start_idx:end_idx, :] = 1\n\n                    #global columns\n                    first_row = 0 if self.attention == 'bidirectional' else start_idx\n                    layout[h, first_row:, start_idx:end_idx] = 1\n        return layout\n\n    def make_layout(self, seq_len):\n        \"\"\"Generates `Variable` sparsity layout used by each head in the sparse attention.\n\n        Arguments:\n             seq_len: required: an integer determining number of attention heads of the layer.\n\n        Return:\n             layout: a tensor of dimension (num_heads, num_blocks, num_blocks) containing `Variable` sparsity layout of all head\n        \"\"\"\n\n        layout = self.setup_layout(seq_len)\n        for h in range(0, self.num_layout_heads):\n            layout = self.set_random_layout(h, layout)\n            layout = self.set_local_layout(h, layout)\n            layout = self.set_global_layout(h, layout)\n\n        layout = self.check_and_propagate_first_head_layout(layout)\n        return layout\n\n\nclass BigBirdSparsityConfig(SparsityConfig):\n    \"\"\"Configuration class to store `BigBird` sparsity configuration.\n    For more details about this sparsity config, please see `Big Bird: Transformers for Longer Sequences`: https://arxiv.org/pdf/2007.14062.pdf\n    This class extends parent class of `SparsityConfig` and customizes it for `BigBird` sparsity.\n    \"\"\"\n\n    def __init__(self,\n                 num_heads,\n                 block=16,\n                 different_layout_per_head=False,\n                 num_random_blocks=1,\n                 num_sliding_window_blocks=3,\n                 num_global_blocks=1,\n                 attention='bidirectional'):\n        \"\"\"Initialize the BigBird Sparsity Pattern Config.\n\n        For usage example please see, TODO DeepSpeed Sparse Transformer Tutorial\n\n        Arguments:\n             num_heads: required: an integer determining number of attention heads of the layer.\n             block: optional: an integer determining the block size. Current implementation of sparse self-attention is based on blocked sparse matrices. In which this parameter defines size of such blocks, `Block X Block`.\n             different_layout_per_head: optional: a boolean determining if each head should be assigned a different sparsity layout; default is false and this will be satisfied based on availability.\n             num_random_blocks: optional: an integer determining the number of random blocks in each block row.\n             num_sliding_window_blocks: optional: an integer determining the number of blocks in sliding local attention window.\n             num_global_blocks: optional: an integer determining how many consecutive blocks, starting from index 0, are considered as global attention. Global block tokens will be attended by all other block tokens and will attend to all other block tokens as well.\n             attention: optional: a string determining attention type. Attention can be `unidirectional`, such as autoregressive models, in which tokens attend only to tokens appear before them in the context. Considering that, the upper triangular of attention matrix is empty as above figure. Or it can be `bidirectional`, such as BERT, in which tokens can attend to any other tokens before or after them. Then, the upper triangular part of the attention matrix is mirror of the lower triangular in the above figure.\n        \"\"\"\n\n        super().__init__(num_heads, block, different_layout_per_head)\n\n        self.num_random_blocks = num_random_blocks\n        self.num_sliding_window_blocks = num_sliding_window_blocks\n        self.num_global_blocks = num_global_blocks\n\n        if (attention != 'unidirectional' and attention != 'bidirectional'):\n            raise NotImplementedError('only \\\"uni/bi-directional\\\" attentions are supported for now!')\n        self.attention = attention\n\n    def set_random_layout(self, h, layout):\n        \"\"\"Sets random attention layout used by the given head in the sparse attention.\n        Note) By default, it assumes there will be a unique random block layout for all heads; unless `different_layout_per_head` parameter is set in which each head can have a different random layout.\n\n        Arguments:\n             h: required: an integer determining head index\n             layout: required: a tensor of dimension (num_heads, num_blocks, num_blocks) containing sparsity layout of all head; may not be completely set at this step\n\n        Return:\n             layout: a tensor of dimension (num_heads, num_blocks, num_blocks) containing sparsity layout of all head in which random layout is set\n        \"\"\"\n\n        num_blocks = layout.shape[1]\n        if (num_blocks < self.num_random_blocks):\n            raise ValueError(\n                f'Number of random blocks, {self.num_random_blocks}, must be smaller than overall number of blocks in a row, {num_blocks}!'\n            )\n\n        for row in range(0, num_blocks):\n            sample_range = range(0, num_blocks) if self.attention == 'bidirectional' else range(0, row + 1)\n            rnd_cols = random.sample(sample_range, self.num_random_blocks)\n            layout[h, row, rnd_cols] = 1\n        return layout\n\n    def set_sliding_window_layout(self, h, layout):\n        \"\"\"Sets sliding local attention layout used by the given head in the sparse attention.\n\n        Arguments:\n             h: required: an integer determining head index\n             layout: required: a tensor of dimension (num_heads, num_blocks, num_blocks) containing sparsity layout of all head; may not be completely set at this step\n\n        Return:\n             layout: a tensor of dimension (num_heads, num_blocks, num_blocks) containing sparsity layout of all head in which local sliding window layout is set\n        \"\"\"\n\n        num_blocks = layout.shape[1]\n        if (num_blocks < self.num_sliding_window_blocks):\n            raise ValueError(\n                f'Number of sliding window blocks, {self.num_sliding_window_blocks}, must be smaller than overall number of blocks in a row, {num_blocks}!'\n            )\n\n        w = self.num_sliding_window_blocks // 2\n        for row in range(0, num_blocks):\n            start = max(0, row - w)\n            end = min(row + w + 1, num_blocks)\n            layout[h, row, start:end] = 1\n        return layout\n\n    def set_global_layout_itc(self, h, layout):\n        \"\"\"Sets global attention layout used by the given head in the sparse attention.\n\n        Arguments:\n             h: required: an integer determining head index\n             layout: required: a tensor of dimension (num_heads, num_blocks, num_blocks) containing sparsity layout of all head; may not be completely set at this step\n\n        Return:\n             layout: a tensor of dimension (num_heads, num_blocks, num_blocks) containing sparsity layout of all head in which global layout is set\n        \"\"\"\n\n        num_blocks = layout.shape[1]\n        if (num_blocks < self.num_global_blocks):\n            raise ValueError(\n                f'Number of global blocks, {self.num_global_blocks}, must be smaller than overall number of blocks in a row, {num_blocks}!'\n            )\n\n        #global rows\n        layout[h, 0:self.num_global_blocks, :] = 1\n\n        #global columns\n        layout[h, :, 0:self.num_global_blocks] = 1\n\n        if self.attention == 'unidirectional':\n            # zero out anything attending to the future\n            layout = torch.tril(layout)\n\n        return layout\n\n    def make_layout(self, seq_len):\n        \"\"\"Generates `BigBird` sparsity layout used by each head in the sparse attention.\n\n        Arguments:\n             seq_len: required: an integer determining number of attention heads of the layer.\n\n        Return:\n             layout: a tensor of dimension (num_heads, num_blocks, num_blocks) containing `BigBird` sparsity layout of all head\n        \"\"\"\n\n        layout = self.setup_layout(seq_len)\n        for h in range(0, self.num_layout_heads):\n            layout = self.set_random_layout(h, layout)\n            layout = self.set_sliding_window_layout(h, layout)\n            layout = self.set_global_layout_itc(h, layout)\n\n        layout = self.check_and_propagate_first_head_layout(layout)\n        return layout\n\n\nclass BSLongformerSparsityConfig(SparsityConfig):\n    \"\"\"Configuration class to store edited `Longformer` sparsity configuration.\n\n    Note) this is a block-sparse version of the Longformer which is slightly different than original Longformer; which is element-wise sparsity.\n\n    For more details about this sparsity config, please see `Longformer: The Long-Document Transformer`: https://arxiv.org/pdf/2004.05150.pdf\n    This class extends parent class of `SparsityConfig` and customizes it for `Longformer` sparsity.\n    \"\"\"\n\n    def __init__(self,\n                 num_heads,\n                 block=16,\n                 different_layout_per_head=False,\n                 num_sliding_window_blocks=3,\n                 global_block_indices=[0],\n                 global_block_end_indices=None,\n                 attention='bidirectional'):\n        \"\"\"Initialize the edited `Longformer` Sparsity Pattern Config.\n\n        For usage example please see, TODO DeepSpeed Sparse Transformer Tutorial\n\n        Arguments:\n             num_heads: required: an integer determining number of attention heads of the layer.\n             block: optional: an integer determining the block size. Current implementation of sparse self-attention is based on blocked sparse matrices. In which this parameter defines size of such blocks, `Block X Block`.\n             different_layout_per_head: optional: a boolean determining if each head should be assigned a different sparsity layout; default is false and this will be satisfied based on availability.\n\n             num_sliding_window_blocks: optional: an integer determining the number of blocks in sliding local attention window.\n             global_block_indices: optional: a list of integers determining which blocks are considered as global attention. Given indices, determine the blocks that all other token blocks attend to and they attend to all other token blocks. Default value is only index 0. Notice that if global_block_end_indices parameter is set, this parameter is used as starting index of each global window.\n             global_block_end_indices: optional: a list of integers determining end indices of global window blocks. By default this is not used. But if it is set, it must have the same size of global_block_indices parameter, and combining this two parameters, for each index i, blocks from global_block_indices[i] to global_block_end_indices[i] (exclusive) are considered as global attention.\n             attention: optional: a string determining attention type. Attention can be `unidirectional`, such as autoregressive models, in which tokens attend only to tokens appear before them in the context. Considering that, the upper triangular of attention matrix is empty as above figure. Or it can be `bidirectional`, such as BERT, in which tokens can attend to any other tokens before or after them. Then, the upper triangular part of the attention matrix is mirror of the lower triangular in the above figure.\n        \"\"\"\n\n        super().__init__(num_heads, block, different_layout_per_head)\n\n        self.num_sliding_window_blocks = num_sliding_window_blocks\n        self.global_block_indices = global_block_indices\n        self.attention = attention\n\n        if (global_block_end_indices is not None):\n            if (len(global_block_indices) != len(global_block_end_indices)):\n                raise ValueError(\n                    f'Global block start indices length, {len(global_block_indices)}, must be same as global block end indices length, {len(global_block_end_indices)}!'\n                )\n            for _, (start_idx, end_idx) in enumerate(zip(global_block_indices, global_block_end_indices)):\n                if start_idx >= end_idx:\n                    raise ValueError(\n                        f'Global block start index, {start_idx}, must be smaller than global block end index, {end_idx}!'\n                    )\n        self.global_block_end_indices = global_block_end_indices\n\n    def set_sliding_window_layout(self, h, layout):\n        \"\"\"Sets sliding local attention layout used by the given head in the sparse attention.\n\n        Arguments:\n             h: required: an integer determining head index\n             layout: required: a tensor of dimension (num_heads, num_blocks, num_blocks) containing sparsity layout of all head; may not be completely set at this step\n\n        Return:\n             layout: a tensor of dimension (num_heads, num_blocks, num_blocks) containing sparsity layout of all head in which local sliding window layout is set\n        \"\"\"\n\n        num_blocks = layout.shape[1]\n        if (num_blocks < self.num_sliding_window_blocks):\n            raise ValueError(\n                f'Number of sliding window blocks, {self.num_sliding_window_blocks}, must be smaller than overall number of blocks in a row, {num_blocks}!'\n            )\n\n        w = self.num_sliding_window_blocks // 2\n        for row in range(0, num_blocks):\n            start = max(0, row - w)\n            end = min(row + w + 1, num_blocks)\n            layout[h, row, start:end] = 1\n        return layout\n\n    def set_global_layout(self, h, layout):\n        \"\"\"Sets global attention layout used by the given head in the sparse attention.\n\n        Arguments:\n             h: required: an integer determining head index\n             layout: required: a tensor of dimension (num_heads, num_blocks, num_blocks) containing sparsity layout of all head; may not be completely set at this step\n\n        Return:\n             layout: a tensor of dimension (num_heads, num_blocks, num_blocks) containing sparsity layout of all head in which global layout is set\n        \"\"\"\n\n        num_blocks = layout.shape[1]\n        if (self.global_block_end_indices is None):\n            for idx in self.global_block_indices:\n                # if global block idx is in the range of the sequence blocks\n                if (idx < num_blocks):\n                    #global rows\n                    layout[h, idx, :] = 1\n\n                    #global columns\n                    layout[h, :, idx] = 1\n        else:\n            for _, (start_idx, end_idx) in enumerate(zip(self.global_block_indices, self.global_block_end_indices)):\n                # if global block idx is in the range of the sequence blocks\n                if (start_idx < num_blocks):\n                    end_idx = min(end_idx, num_blocks)\n                    #global rows\n                    layout[h, start_idx:end_idx, :] = 1\n\n                    #global columns\n                    layout[h, :, start_idx:end_idx] = 1\n        if self.attention == 'unidirectional':\n            layout = torch.tril(layout)\n        return layout\n\n    def make_layout(self, seq_len):\n        \"\"\"Generates edited `Longformer` sparsity layout used by each head in the sparse attention.\n\n        Arguments:\n             seq_len: required: an integer determining number of attention heads of the layer.\n\n        Return:\n             layout: a tensor of dimension (num_heads, num_blocks, num_blocks) containing `BSLongformer` sparsity layout of all head\n        \"\"\"\n\n        layout = self.setup_layout(seq_len)\n        for h in range(0, self.num_layout_heads):\n            layout = self.set_sliding_window_layout(h, layout)\n            layout = self.set_global_layout(h, layout)\n\n        layout = self.check_and_propagate_first_head_layout(layout)\n        return layout\n\n\nclass LocalSlidingWindowSparsityConfig(SparsityConfig):\n    \"\"\"Configuration class to store `Local Sliding Window` sparsity configuration - a purely-local sliding window attention.\n    This class extends parent class of `SparsityConfig` and customizes it for `Local` sparsity.\n    \"\"\"\n\n    def __init__(self, num_heads, block=16, num_sliding_window_blocks=3, attention='unidirectional'):\n        \"\"\"Initialize the Local Sliding Window Sparsity Pattern Config.\n        For usage example please see, TODO DeepSpeed Sparse Transformer Tutorial\n        Arguments:\n             num_heads: required: an integer determining number of attention heads of the layer.\n             block: optional: an integer determining the block size. Current implementation of sparse self-attention is based on blocked sparse matrices. In which this parameter defines size of such blocks, `Block X Block`.\n             num_sliding_window_blocks: optional: an integer determining the number of blocks in sliding local attention window.\n\t     attention: optional: a string determining attention type. Attention can be `unidirectional`, such as autoregressive models, in which tokens attend only to tokens appear before them in the context. Considering that, the upper triangular of attention matrix is empty as above figure. Or it can be `bidirectional`, such as BERT, in which tokens can attend to any other tokens before or after them. Then, the upper triangular part of the attention matrix is mirror of the lower triangular in the above figure.\n        \"\"\"\n\n        super().__init__(num_heads, block)\n        self.num_sliding_window_blocks = num_sliding_window_blocks\n        self.attention = attention\n\n    def set_sliding_window_layout(self, h, layout):\n        \"\"\"Sets sliding local attention layout used by the given head in the sparse attention.\n        Arguments:\n             h: required: an integer determining head index\n             layout: required: a tensor of dimension (num_heads, num_blocks, num_blocks) containing sparsity layout of all head; may not be completely set at this step\n        Return:\n             layout: a tensor of dimension (num_heads, num_blocks, num_blocks) containing sparsity layout of all head in which local sliding window layout is set\n        \"\"\"\n\n        num_blocks = layout.shape[1]\n        if (num_blocks < self.num_sliding_window_blocks):\n            raise ValueError(\n                f'Number of sliding window blocks, {self.num_sliding_window_blocks}, must be smaller than overall number of blocks in a row, {num_blocks}!'\n            )\n\n        w = self.num_sliding_window_blocks // 2\n        for row in range(0, num_blocks):\n            start = max(0, row - w)\n            end = min(row + w + 1, num_blocks) if self.attention == \"bidirectional\" else row + 1\n            layout[h, row, start:end] = 1\n        return layout\n\n    def make_layout(self, seq_len):\n        \"\"\"Generates `Local Sliding Window` sparsity layout used by each head in the sparse attention.\n        Arguments:\n             seq_len: required: an integer determining number of attention heads of the layer.\n        Return:\n             layout: a tensor of dimension (num_heads, num_blocks, num_blocks) containing `BigBird` sparsity layout of all head\n        \"\"\"\n\n        layout = self.setup_layout(seq_len)\n        for h in range(0, self.num_layout_heads):\n            layout = self.set_sliding_window_layout(h, layout)\n        layout = self.check_and_propagate_first_head_layout(layout)\n        return layout\n", "deepspeed/ops/sparse_attention/sparse_attention_utils.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nfrom torch.nn import functional as F\nfrom deepspeed.ops.sparse_attention import BertSparseSelfAttention, SparsityConfig\n'''\nThis file contains few utility functions to handle adapting pretrained model with sparse self-attention module.\n'''\n\n\nclass SparseAttentionUtils:\n    \"\"\"This class provides some utility functions that are use integrating sparse attention into transformer models.\n    Such utilities include extending position embeddings, replacing current self-attention layer with sparse attention, padding sequences to multiple of block size, etc.\n\n    \"\"\"\n\n    @staticmethod\n    def extend_position_embedding(model, max_position):\n        \"\"\"This function extends the position embedding weights of a model loaded from a checkpoint.\n        It assumes the new max position is bigger than the original max length.\n\n        Arguments:\n            model: required: a transformer model\n            max_position: required: an integer determining new position embedding size\n        Return:\n            model: updated model; in which position embedding weights have been extended based on new size\n        \"\"\"\n\n        if hasattr(model, 'bert'):\n            original_max_position = model.bert.embeddings.position_embeddings.weight.size(0)\n            assert max_position > original_max_position\n            extend_multiples = max(1, max_position // original_max_position)\n            model.bert.embeddings.position_embeddings.weight.data = model.bert.embeddings.position_embeddings.weight.repeat(\n                extend_multiples, 1)\n        elif hasattr(model, 'roberta'):\n            # RoBERTa has positions 0 & 1 reserved, so embedding size is max position + 2\n            original_max_position, embed_size = model.roberta.embeddings.position_embeddings.weight.shape\n            original_max_position -= 2\n            extend_multiples = max(1, max_position // original_max_position)\n            assert max_position > original_max_position\n            max_position += 2\n            extended_position_embedding = model.roberta.embeddings.position_embeddings.weight.new_empty(\n                max_position, embed_size)\n            k = 2\n            for i in range(extend_multiples):\n                extended_position_embedding[k:(\n                    k + original_max_position)] = model.roberta.embeddings.position_embeddings.weight[2:]\n                k += original_max_position\n            model.roberta.embeddings.position_embeddings.weight.data = extended_position_embedding\n        else:\n            raise ValueError(\n                'Please extend \\\"extend_position_embedding\\\" function to support your model type. It currently only supports \\\"bert\\\" & \\\"roberta\\\"!'\n            )\n\n        model.config.max_position_embeddings = max_position\n        print(f'Extended position embeddings to {original_max_position * extend_multiples}')\n\n        return model\n\n    @staticmethod\n    def update_tokenizer_model_max_length(tokenizer, max_position):\n        \"\"\"This function updates the position embedding length of a tokenizer to a new max position.\n\n        Arguments:\n            tokenizer: required: a transformer tokenizer\n            max_position: required: an integer determining new position embedding size\n        Return:\n            tokenizer: updated tokenizer; in which model maximum length has been extended based on new size\n        \"\"\"\n\n        tokenizer.model_max_length = max_position\n        tokenizer.init_kwargs['model_max_length'] = max_position\n        print(f'updated tokenizer model max imum length to {max_position}')\n\n        return tokenizer\n\n    @staticmethod\n    def replace_model_self_attention_with_sparse_self_attention(\n        model,\n        max_position,\n        # SparsityConfig parameters needs to be set accordingly\n        sparsity_config=SparsityConfig(num_heads=4)):\n        \"\"\"This function replaces the self attention layers in model encoder with sparse self attention.\n        It currently supports bert and roberta model and can be easily extended to any other models following similar steps here.\n        For sparsityConfig, refer to the config class.\n\n        Arguments:\n            model: required: a transformer model\n            max_position: required: an integer determining new position embedding size\n            sparsity_config: optional: this parameter determines sparsity pattern configuration; it is based on SparsityConfig class\n\n        Return:\n            model: updated model; in which self attention layer has been replaced with DeepSpeed Sparse Self Attention layer.\n        \"\"\"\n\n        if hasattr(model, 'bert'):\n            model.config.max_position_embeddings = max_position\n            model.replace_self_attention_layer_with_sparse_self_attention_layer(model.config, model.bert.encoder.layer,\n                                                                                sparsity_config)\n        elif hasattr(model, 'roberta'):\n            model.config.max_position_embeddings = max_position + 2\n            model.replace_self_attention_layer_with_sparse_self_attention_layer(model.config,\n                                                                                model.roberta.encoder.layer,\n                                                                                sparsity_config)\n        else:\n            raise ValueError(\n                'Please extend \\\"update_model_self_attention_to_sparse_self_attention\\\" function to support \\\n                                     your model type. It currently only supports \\\"bert\\\" & \\\"roberta\\\"!')\n        return model\n\n    @staticmethod\n    def replace_self_attention_layer_with_sparse_self_attention_layer(\n        config,\n        layers,\n        # SparsityConfig parameters needs to be set accordingly\n        sparsity_config=SparsityConfig(num_heads=4)):\n        \"\"\"This function replaces the self attention layers in attention layer with sparse self attention.\n        For sparsityConfig, refer to the config class.\n\n        Arguments:\n            config: required: transformer model config\n            layers: required: transformer model attention layers\n            sparsity_config: optional: this parameter determines sparsity pattern configuration; it is based on SparsityConfig class\n\n        Return:\n            layers: updated attention layers; in which self attention layers have been replaced with DeepSpeed Sparse Self Attention layer.\n        \"\"\"\n\n        for layer in layers:\n            deepspeed_sparse_self_attn = BertSparseSelfAttention(config, sparsity_config)\n            deepspeed_sparse_self_attn.query = layer.attention.self.query\n            deepspeed_sparse_self_attn.key = layer.attention.self.key\n            deepspeed_sparse_self_attn.value = layer.attention.self.value\n\n            layer.attention.self = deepspeed_sparse_self_attn\n\n        return layers\n\n    @staticmethod\n    def pad_to_block_size(block_size, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds,\n                          pad_token_id, model_embeddings):\n        \"\"\"This function pads input tokens and attention mask on sequence length dimension to be multiple of block size.\n            This is a requirement for Sparse Transformer in which the self attention layer works on sequences of length multiple of block size.\n            It needs to be called in your model, such as BertModel, right before you calculate the embedding outputs.\n            Note)\n            1- instead of passing your embedding layer to this function, you can simply add this function to your model. It can be more simplified if given attention_mask and/or token_type_ids are none.\n            2- you need to call unpad function before returning your model output to unpad the encoder sequence output.\n\n            Arguments:\n                block_size: required: an integer determining the block size of sparsity config.\n                pad_token_id: required: an integer determining the pad token from the model config; such as bert.config.pad_token_id.\n                input_ids: a torch.LongTensor of shape [batch_size, sequence_length] with the word token indices in the vocabulary\n                attention_mask: a torch.LongTensor of shape [batch_size, sequence_length] with indices selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max input sequence length in the current batch. It's the mask that we typically use for attention when a batch has varying length sentences.\n                token_type_ids: a torch.LongTensor of shape [batch_size, sequence_length] with the token types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to a `sentence B` token (see BERT paper for more details).\n                position_ids:  a torch.LongTensor of shape [batch_size, sequence_length] with the indices of positions of each input sequence tokens in the position embeddings.\n                inputs_embeds: an optional torch.FloatTensor of shape [batch_size, sequence_length, hidden_size] that contains embedded representation and can be passed instead of input_ids directly.\n                model_embeddings: an optional object. If inputs_embeds are not none, this will be your model embeddings such as BertEmbeddings from your model such as BertModel. You can move this function inside your model and use self.embeddings instead of passing this parameter.\n\n            Return:\n                pad_len: an integer determining how much inputs have been padded to transfer sequence length dimension to multiple of block size.\n                input_ids: if input_ids are not none padded input_ids otherwise none.\n                attention_mask: if attention_mask is not none padded attention_mask otherwise none.\n                token_type_ids: if token_type_ids are not none padded token_type_ids otherwise none.\n                position_ids: if position_ids are not none padded position_ids otherwise none.\n                inputs_embeds: if inputs_embeds are not none padded inputs_embeds otherwise none.\n        \"\"\"\n\n        batch_size, seq_len = input_ids.shape if input_ids is not None else inputs_embeds.shape[:-1]\n\n        pad_len = (block_size - seq_len % block_size) % block_size\n        if pad_len > 0:\n            if inputs_embeds is not None:\n                pad_input_ids = inputs_embeds.new_full((batch_size, pad_len), pad_token_id, dtype=torch.long)\n                pad_inputs_embeds = model_embeddings(pad_input_ids)\n                inputs_embeds = torch.cat([inputs_embeds, pad_inputs_embeds], dim=-2)\n            # may not be needed as input_ids are not used if inputs_embeds are given\n            if input_ids is not None:\n                input_ids = F.pad(input_ids, (0, pad_len), value=pad_token_id)\n            if position_ids is not None:\n                # pad position_id with pad_token_id\n                position_ids = F.pad(position_ids, (0, pad_len), value=pad_token_id)\n            # pad attention mask without attention on the padding tokens\n            attention_mask = F.pad(attention_mask, (0, pad_len), value=False)\n            # pad token_type_ids with token_type_id = 0\n            token_type_ids = F.pad(token_type_ids, (0, pad_len), value=0)\n\n        return pad_len, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds\n\n    @staticmethod\n    def unpad_sequence_output(pad_len, sequence_output):\n        \"\"\"This function unpads sequence output if inputs of the model were padded.\n           This is a requirement for Sparse Transformer in which the self attention layer works on sequences of length multiple of block size.\n           It needs to be called in your model, such as BertModel, right before you return the model outputs.\n\n           Arguments:\n               pad_len: required: an integer determining how much model inputs have been padded to transfer sequence length dimension to multiple of block size.\n               sequence_output: required: sequence output of the encoder layer.\n\n           Return:\n               sequence_output: unpaded sequence output of the encoder layer.\n        \"\"\"\n\n        if (pad_len > 0):\n            sequence_output = sequence_output[:, :-pad_len]\n        return sequence_output\n", "deepspeed/ops/sparse_attention/softmax.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\n# DeepSpeed note, code taken & adapted from commit 9aa94789f13ada713af36cfd8cca2fc9a7f6b79a\n# https://github.com/ptillet/torch-blocksparse/blob/master/torch_blocksparse/matmul.py\n\nimport torch\n\nimport triton\nimport triton.language as tl\n\n\ndef next_power_of_2(n):\n    n -= 1\n    n |= n >> 1\n    n |= n >> 2\n    n |= n >> 4\n    n |= n >> 8\n    n |= n >> 16\n    n += 1\n    return n\n\n\ndef num_warps(n):\n    if n < 512:\n        return 4\n    if n < 2048:\n        return 8\n    return 16\n\n\n@triton.heuristics({'num_warps': lambda *args, **meta: num_warps(args[6] * meta['BLOCK'])})\n@triton.heuristics({'TN': lambda *args, **meta: next_power_of_2(args[6] * meta['BLOCK'])})\n@triton.jit\ndef _forward(X, scale, LUT, RPE, KP_M, ATTN_M, sizemax, stride_zx, stride_zrpe, stride_hrpe, stride_srpe, stride_zkpm,\n             stride_zattnm, **meta):\n    TN = meta['TN']\n    BLOCK = meta['BLOCK']\n    pidhm = tl.program_id(0)\n    pidz = tl.program_id(1)\n    # create index ranges\n    rxm = pidhm % BLOCK\n    rbm = pidhm // BLOCK\n    rxn = tl.arange(0, TN) % BLOCK\n    rbn = tl.arange(0, TN) // BLOCK\n    # extract information from LUT\n    header = LUT + rbm * 2\n    size = tl.load(header + 0)\n    offset = tl.load(header + 1)\n    check = rbn < size\n    rbmn = tl.where(check, rbn, size - 1)\n    # block id and column id\n    blockid = tl.load(LUT + offset + rbmn * 4 + 0)\n    columnid = tl.load(LUT + offset + rbmn * 4 + 1)\n    rowid = tl.load(LUT + offset + rbmn * 4 + 2)\n    headid = tl.load(LUT + offset + rbmn * 4 + 3)\n    # pointers to X\n    px = X + pidz * stride_zx + blockid * BLOCK * BLOCK + rxm * BLOCK + rxn\n    x = tl.load(px, mask=check, other=-float('inf'))\n    x = x.to(tl.float32)\n    # apply scale\n    if meta['APPLY_SCALE']:\n        x = x * scale\n    # apply RPE\n    if meta['APPLY_RPE']:\n        prpe = RPE + pidz * stride_zrpe + headid * stride_hrpe + columnid * BLOCK + rowid * BLOCK * stride_srpe + rxm * stride_srpe + rxn\n        rpe = tl.load(prpe, mask=check, other=0)\n        x = x + rpe\n    # apply key-padding mask\n    if meta['APPLY_KP_MASK']:\n        pkp_m = KP_M + pidz * stride_zkpm + columnid * BLOCK + rxn\n        kp_m = tl.load(pkp_m, mask=check, other=-float('inf'))\n        if meta['KP_MASK_MUL']:\n            kp_m = tl.where(kp_m == 0, -float('inf'), 0.)\n        x = x + kp_m\n    # apply attention mask\n    if meta['APPLY_ATTN_MASK']:\n        pattn_m = ATTN_M + columnid * BLOCK + rowid * BLOCK * stride_zattnm + rxm * stride_zattnm + rxn\n        attn_m = tl.load(pattn_m, mask=check, other=-float('inf'))\n        if meta['ATTN_MASK_MUL']:\n            attn_m = tl.where(attn_m == 0, -float('inf'), 0.)\n        x = x + attn_m\n    # computation\n    x = tl.softmax(x)\n    tl.store(px, x, mask=check)\n\n\n@triton.heuristics({'num_warps': lambda *args, **meta: num_warps(args[4] * meta['BLOCK'])})\n@triton.heuristics({'TN': lambda *args, **meta: next_power_of_2(args[4]) * meta['BLOCK']})\n@triton.jit\ndef _backward(X, scale, DX, LUT, sizemax, stride_zx, stride_zdx, **meta):\n    pidhm = tl.program_id(0)\n    pidz = tl.program_id(1)\n    TN = meta['TN']\n    BLOCK = meta['BLOCK']\n    # create index ranges\n    rxm = pidhm % BLOCK\n    rbm = pidhm // BLOCK\n    rxn = tl.arange(0, TN) % BLOCK\n    rbn = tl.arange(0, TN) // BLOCK\n    # extract information from look-up table\n    header = LUT + rbm * 2\n    size = tl.load(header + 0)\n    offset = tl.load(header + 1)\n    # bounds checking on lut\n    check = rbn < size\n    rbmn = tl.where(check, rbn, size - 1)\n    # initialize pointers to block-sparse input\n    blockid = tl.load(LUT + offset + rbmn * 4)\n    X = X + pidz * stride_zx + blockid * BLOCK * BLOCK + rxm * BLOCK + rxn\n    DX = DX + pidz * stride_zdx + blockid * BLOCK * BLOCK + rxm * BLOCK + rxn\n    # compute fused softmax backward\n    x = tl.load(X, mask=check, other=0)\n    dx = tl.load(DX, mask=check, other=0)\n    x = x.to(tl.float32)\n    dx = dx.to(tl.float32)\n    y = x * (dx - tl.sum(x * dx, 0)) * scale\n    tl.store(DX, y, mask=check)\n\n\nclass _sparse_softmax(torch.autograd.Function):\n\n    bwd_kernels = dict()\n\n    @staticmethod\n    def make_lut(layout, block, device):\n        _empty = torch.tensor([], dtype=torch.int64, device=layout.device)\n        sizes = _empty.clone()\n        # sizes along rows\n        for h in range(layout.shape[0]):\n            sizes = torch.cat((sizes, layout[h, :, :].sum(-1)))\n        # offsets in block format\n        offsets = torch.zeros_like(sizes)\n        offsets[1:] = torch.cumsum(sizes[:-1], dim=0)\n        # block indices\n        idx = torch.arange(layout.sum())\n        head = layout.nonzero()[:, 0]\n        rows = layout.nonzero()[:, 1]\n        columns = layout.nonzero()[:, 2]\n        core = torch.stack((idx, columns, rows, head), dim=1).view(-1)\n        # construct look-up table\n        offsets = offsets * 4 + 2 * sizes.numel()\n        header = torch.stack((sizes, offsets), dim=1).view(-1)\n        lut = torch.cat((header, core)).type(torch.int32).to(device)\n        return lut, int(sizes.max())\n\n    @staticmethod\n    def forward(ctx, x, scale, rpe, key_padding_mask, attn_mask, kp_mask_mode, attn_mask_mode, spdims, block, lut,\n                num_blocks, maxlut, bench, time):\n\n        apply_scale = False if scale == 1.0 else True\n\n        # handle None rpe\n        if rpe is None:\n            apply_rpe = False\n            stride_zrpe, stride_hrpe, stride_srpe = 0, 0, 0\n            rpe = torch.empty(0, dtype=x.dtype, device=x.device)\n        else:\n            apply_rpe = True\n            stride_zrpe, stride_hrpe, stride_srpe = rpe.stride(0), rpe.stride(1), rpe.stride(2)\n\n        # handle None key_padding_mask\n        if key_padding_mask is None:\n            apply_kp_mask = False\n            stride_zkpm = 0\n            key_padding_mask = torch.empty(0, dtype=x.dtype, device=x.device)\n        else:\n            apply_kp_mask = True\n            stride_zkpm = key_padding_mask.stride(0)\n\n        # handle None attention_mask\n        if attn_mask is None:\n            apply_attn_mask = False\n            stride_zattnm = 0\n            attn_mask = torch.empty(0, dtype=x.dtype, device=x.device)\n        else:\n            apply_attn_mask = True\n            stride_zattnm = attn_mask.stride(0)\n\n        # run kernel\n        M = x.shape[0]\n        meta = {\n            'BLOCK': block,\n            'APPLY_SCALE': apply_scale,\n            'APPLY_RPE': apply_rpe,\n            'APPLY_KP_MASK': apply_kp_mask,\n            'APPLY_ATTN_MASK': apply_attn_mask,\n            'KP_MASK_MUL': kp_mask_mode == 'mul',\n            'ATTN_MASK_MUL': attn_mask_mode == 'mul',\n        }\n        grid = lambda opt: [spdims[0] * spdims[1] * block, M]\n        _forward[grid](x, scale, lut, rpe, key_padding_mask, attn_mask, maxlut, x.stride(0),\\\n                       stride_zrpe, stride_hrpe, stride_srpe, stride_zkpm, stride_zattnm, **meta)\n\n        # save to context\n        ctx.mark_dirty(x)\n        ctx.save_for_backward(x, lut)\n        ctx.spdims = spdims\n        ctx.block = block\n        ctx.maxlut = maxlut\n        ctx.scale = scale\n        ctx.apply_scale = apply_scale\n        ctx.apply_rpe = apply_rpe\n        ctx.apply_kp_mask = apply_kp_mask\n        ctx.apply_attn_mask = apply_attn_mask\n        ctx.kp_mask_mode = kp_mask_mode\n        ctx.attn_mask_mode = attn_mask_mode\n        return x\n\n    @staticmethod\n    def backward(ctx, dx):\n\n        # retrieve from context\n        x, lut = ctx.saved_tensors\n        # run kernel\n        M = x.shape[0]\n        grid = lambda opt: [ctx.spdims[0] * ctx.spdims[1] * ctx.block, M]\n        _backward[grid](x, ctx.scale, dx, lut, ctx.maxlut, x.stride(0), dx.stride(0), BLOCK=ctx.block)\n        return dx, None, None, None, None, None, None, None, None, None, None, None, None, None, None\n\n\nclass Softmax:\n    \"\"\"Block-Sparse Softmax class; this class computes softmax on a block sparse matrix. It is also able to apply either/all of the following masks:\n       - relative position embedding\n       - key padding mask\n       - attention mask\n\n    For more details about sparsity config, please see `Generative Modeling with Sparse Transformers`: https://arxiv.org/abs/1904.10509\n    \"\"\"\n\n    def sparse_softmax(*args, **kwargs):\n        return _sparse_softmax.apply(*args, **kwargs)\n\n    def make_lut(self, device):\n        \"\"\"Generates the sparsity layout used in block-sparse softmax\n        \"\"\"\n        key = (device, )\n        if key not in self.lut_cache:\n            self.lut_cache[key] = _sparse_softmax.make_lut(self.layout, self.block, device)\n        return self.lut_cache[key]\n\n    def __init__(self, layout, block, bench=False):\n        \"\"\"Initialize the Block-Sparse Softmax class.\n\n        Arguments:\n             layout: required: sparsity layout tensor\n             block: required: an integer determining the block size.\n             bench: optional: set if you want to do benchmarking\n        \"\"\"\n\n        self.num_blocks = layout.sum().item()\n        self.spdims = layout.shape\n        self.layout = layout\n        self.block = block\n        self.bench = bench\n        self.lut_cache = dict()\n\n    def __call__(self,\n                 x,\n                 scale=1.,\n                 rpe=None,\n                 key_padding_mask=None,\n                 attn_mask=None,\n                 key_padding_mask_mode='add',\n                 attn_mask_mode='add'):\n        \"\"\"Applies softmax on a Block-Sparse input tensor.\n\n        For more details about sparsity config, please see `Generative Modeling with Sparse Transformers`: https://arxiv.org/abs/1904.10509\n\n        Arguments:\n             x: required: a block-sparse tensor that softmax is applied on it; computation will be in place and result will be returned in the same tensor\n             scale: optional: a float value; x values will be multiplied by this value before normalization. Default value is 1.0.\n             rpe: optional: a tensor same dimension as x that is used as relative position embedding\n             key_padding_mask: optional: a mask tensor of size (BatchSize X SequenceLength)\n             attn_mask: optional: a mask tensor of size (SequenceLength X SequenceLength); currently only 2D is supported\n             key_padding_mask_mode: optional: a boolean determining if key_padding_mask needs to be added or multiplied\n             attn_mask_mode: optional: a boolean determining if attn_mask needs to be added or multiplied\n\n        Return:\n             x: a block-sparse tensor contains normalized input x using softmax; and masks applied if given\n        \"\"\"\n\n        time_y = [None]\n        if rpe is not None and rpe.dtype != x.dtype:\n            raise ValueError('relative position embedding must be %s' % x.dtype)\n        if attn_mask is not None and attn_mask.dtype != x.dtype:\n            raise ValueError('Attention mask must be %s' % x.dtype)\n        if key_padding_mask is not None and key_padding_mask.dtype != x.dtype:\n            raise ValueError('Key padding mask must be %s' % x.dtype)\n        lut, maxlut = self.make_lut(x.device)\n        x = Softmax.sparse_softmax(x, scale, rpe, key_padding_mask, attn_mask, key_padding_mask_mode, attn_mask_mode,\n                                   self.spdims, self.block, lut, self.num_blocks, maxlut, self.bench, time_y)\n        self.time_y = time_y[0]\n        return x\n", "deepspeed/ops/sparse_attention/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .sparsity_config import SparsityConfig, DenseSparsityConfig, FixedSparsityConfig, VariableSparsityConfig, BigBirdSparsityConfig, BSLongformerSparsityConfig, LocalSlidingWindowSparsityConfig\nfrom .sparse_self_attention import SparseSelfAttention\nfrom .bert_sparse_self_attention import BertSparseSelfAttention\nfrom .sparse_attention_utils import SparseAttentionUtils\n", "deepspeed/ops/sparse_attention/trsrc/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport sys\nimport os\n\n\ndef _build_file_index(directory, suffix='.tr'):\n    \"\"\"Build an index of source files and their basenames in a given directory.\n\n    Args:\n        directory (string): the directory to index\n        suffix (string): index files with this suffix\n\n    Returns:\n        list: A list of tuples of the form [(basename, absolute path), ...]\n    \"\"\"\n\n    index = []\n\n    for fname in os.listdir(directory):\n        if fname.endswith(suffix):\n            basename = fname[:fname.rfind(suffix)]  # strip the suffix\n            path = os.path.join(directory, fname)\n            index.append((basename, path))\n\n    return index\n\n\n# Go over all local source files and parse them as strings\n_module = sys.modules[_build_file_index.__module__]\n_directory = os.path.dirname(os.path.realpath(__file__))\nfor name, fname in _build_file_index(_directory):\n    with open(fname, 'r') as fin:\n        setattr(_module, name, fin.read())\n", "deepspeed/ops/quantizer/quantizer.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\n\nfrom deepspeed.ops.op_builder import QuantizerBuilder\n\n# Cuda modules will be imported if needed\nquantizer_cuda_module = None\n\n\ndef ds_quantizer(input, groups=1, bit_num=8, sr=False, asym=False):\n    # Load cuda modules if needed\n    global quantizer_cuda_module\n    if quantizer_cuda_module is None:\n        quantizer_cuda_module = QuantizerBuilder().load()\n    if sr:\n        if asym:\n            quantize_func = quantizer_cuda_module.ds_sr_quantize_asym_fp16 if input.dtype == torch.half else quantizer_cuda_module.ds_sr_quantize_asym_fp32\n        else:\n            quantize_func = quantizer_cuda_module.ds_sr_quantize_fp16 if input.dtype == torch.half else quantizer_cuda_module.ds_sr_quantize_fp32\n    else:\n        if asym:\n            quantize_func = quantizer_cuda_module.ds_quantize_asym_fp16 if input.dtype == torch.half else quantizer_cuda_module.ds_quantize_asym_fp32\n        else:\n            quantize_func = quantizer_cuda_module.ds_quantize_fp16 if input.dtype == torch.half else quantizer_cuda_module.ds_quantize_fp32\n    return quantize_func(input, groups, bit_num)\n", "deepspeed/ops/quantizer/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .quantizer import ds_quantizer\n", "deepspeed/ops/aio/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom ..op_builder import AsyncIOBuilder\n", "deepspeed/ops/adam/fused_adam.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\"\"\"\nCopyright NVIDIA/apex\nThis file is adapted from fused adam in NVIDIA/apex, commit 6bd01c4\n\"\"\"\n\nimport torch\nfrom .multi_tensor_apply import MultiTensorApply\n\nmulti_tensor_applier = MultiTensorApply(2048 * 32)\nfrom deepspeed.accelerator import get_accelerator\nfrom deepspeed.ops.op_builder import FusedAdamBuilder\n\n\nclass FusedAdam(torch.optim.Optimizer):\n    \"\"\"Implements Adam algorithm.\n\n    Currently GPU-only.  Requires Apex to be installed via\n    ``pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./``.\n\n    This version of fused Adam implements 2 fusions.\n\n      * Fusion of the Adam update's elementwise operations\n      * A multi-tensor apply launch that batches the elementwise updates applied to all the model's parameters into one or a few kernel launches.\n\n    :class:`apex.optimizers.FusedAdam` may be used as a drop-in replacement for ``torch.optim.AdamW``,\n    or ``torch.optim.Adam`` with ``adam_w_mode=False``::\n\n        opt = apex.optimizers.FusedAdam(model.parameters(), lr = ....)\n        ...\n        opt.step()\n\n    :class:`apex.optimizers.FusedAdam` may be used with or without Amp.  If you wish to use :class:`FusedAdam` with Amp,\n    you may choose any ``opt_level``::\n\n        opt = apex.optimizers.FusedAdam(model.parameters(), lr = ....)\n        model, opt = amp.initialize(model, opt, opt_level=\"O0\" or \"O1 or \"O2\")\n        ...\n        opt.step()\n\n    In general, ``opt_level=\"O1\"`` is recommended.\n\n\n    .. warning::\n        A previous version of :class:`FusedAdam` allowed a number of additional arguments to ``step``.  These additional arguments\n        are now deprecated and unnecessary.\n\n    Adam was been proposed in `Adam: A Method for Stochastic Optimization`_.\n\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups.\n        lr (float, optional): learning rate. (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square. (default: (0.9, 0.999))\n        eps (float, optional): term added to the denominator to improve\n            numerical stability. (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n            algorithm from the paper `On the Convergence of Adam and Beyond`_\n            (default: False) NOT SUPPORTED in FusedAdam!\n        adam_w_mode (boolean, optional): Apply L2 regularization or weight decay\n            True for decoupled weight decay(also known as AdamW) (default: True)\n        set_grad_none (bool, optional): whether set grad to None when zero_grad()\n            method is called. (default: True)\n\n    .. _Adam - A Method for Stochastic Optimization:\n        https://arxiv.org/abs/1412.6980\n    .. _On the Convergence of Adam and Beyond:\n        https://openreview.net/forum?id=ryQu7f-RZ\n    \"\"\"\n\n    def __init__(self,\n                 params,\n                 lr=1e-3,\n                 bias_correction=True,\n                 betas=(0.9, 0.999),\n                 eps=1e-8,\n                 adam_w_mode=True,\n                 weight_decay=0.,\n                 amsgrad=False,\n                 set_grad_none=True):\n\n        if amsgrad:\n            raise RuntimeError('FusedAdam does not support the AMSGrad variant.')\n        defaults = dict(lr=lr, bias_correction=bias_correction, betas=betas, eps=eps, weight_decay=weight_decay)\n        super(FusedAdam, self).__init__(params, defaults)\n        self.adam_w_mode = 1 if adam_w_mode else 0\n        self.set_grad_none = set_grad_none\n\n        fused_adam_cuda = FusedAdamBuilder().load()\n        # Skip buffer\n        self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n        self.multi_tensor_adam = fused_adam_cuda.multi_tensor_adam\n\n    def zero_grad(self):\n        if self.set_grad_none:\n            for group in self.param_groups:\n                for p in group['params']:\n                    p.grad = None\n        else:\n            super(FusedAdam, self).zero_grad()\n\n    def step(self, closure=None, grads=None, output_params=None, scale=None, grad_norms=None, grad_scaler=None):\n        \"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n\n        The remaining arguments are deprecated, and are only retained (for the moment) for error-checking purposes.\n        \"\"\"\n        if any(p is not None for p in [grads, output_params, scale, grad_norms]):\n            raise RuntimeError(\n                'FusedAdam has been updated.  Simply initialize it identically to torch.optim.Adam, and call step() with no arguments.'\n            )\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            if len(group['params']) == 0:\n                continue\n            bias_correction = 1 if group['bias_correction'] else 0\n            beta1, beta2 = group['betas']\n\n            # assume same step across group now to simplify things\n            # per parameter step can be easily support by making it tensor, or pass list into kernel\n            if 'step' not in group:\n                group['step'] = 0\n\n            # create lists for multi-tensor apply\n            g_16, p_16, m_16, v_16 = [], [], [], []\n            g_bf, p_bf, m_bf, v_bf = [], [], [], []\n            g_32, p_32, m_32, v_32 = [], [], [], []\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                if p.grad.data.is_sparse:\n                    raise RuntimeError(\n                        'FusedAdam does not support sparse gradients, please consider SparseAdam instead')\n\n                state = self.state[p]\n                # State initialization\n                if len(state) == 0:\n                    # DeepSpeed ZeRO 3 processes each subgroup a time, so we need to keep tracking step count for each tensor separately.\n                    # While this is not an issue for ZeRO 1 & 2, since they apply a single optimization step to the whole param group at the same time.\n                    # In order to keep backward compatibility for the existing checkpoints, we use group['state'] to initialize state['step'] if it exists.\n                    state['step'] = group.get('step', 0)\n                    # Exponential moving average of gradient values\n                    state['exp_avg'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n\n                if p.dtype == torch.float16:\n                    g_16.append(p.grad.data)\n                    p_16.append(p.data)\n                    m_16.append(state['exp_avg'])\n                    v_16.append(state['exp_avg_sq'])\n                elif p.dtype == torch.bfloat16:\n                    g_bf.append(p.grad)\n                    p_bf.append(p)\n                    m_bf.append(state['exp_avg'])\n                    v_bf.append(state['exp_avg_sq'])\n                elif p.dtype == torch.float32:\n                    g_32.append(p.grad.data)\n                    p_32.append(p.data)\n                    m_32.append(state['exp_avg'])\n                    v_32.append(state['exp_avg_sq'])\n                else:\n                    raise RuntimeError('FusedAdam only support fp16, bf16 and fp32.')\n\n            if len(g_16) > 0:\n                state['step'] += 1\n                multi_tensor_applier(self.multi_tensor_adam, self._dummy_overflow_buf, [g_16, p_16, m_16, v_16],\n                                     group['lr'], beta1, beta2, group['eps'], state['step'], self.adam_w_mode,\n                                     bias_correction, group['weight_decay'])\n\n            if len(g_bf) > 0:\n                state['step'] += 1\n                multi_tensor_applier(self.multi_tensor_adam, self._dummy_overflow_buf, [g_bf, p_bf, m_bf, v_bf],\n                                     group['lr'], beta1, beta2, group['eps'], state['step'], self.adam_w_mode,\n                                     bias_correction, group['weight_decay'])\n\n            if len(g_32) > 0:\n                state['step'] += 1\n                multi_tensor_applier(self.multi_tensor_adam, self._dummy_overflow_buf, [g_32, p_32, m_32, v_32],\n                                     group['lr'], beta1, beta2, group['eps'], state['step'], self.adam_w_mode,\n                                     bias_correction, group['weight_decay'])\n\n        return loss\n", "deepspeed/ops/adam/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .cpu_adam import DeepSpeedCPUAdam\nfrom .fused_adam import FusedAdam\n", "deepspeed/ops/adam/cpu_adam.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nfrom cpuinfo import get_cpu_info\nfrom deepspeed.utils import logger\nfrom deepspeed.utils.logging import should_log_le\nfrom deepspeed.ops.op_builder import CPUAdamBuilder\n\n\nclass DeepSpeedCPUAdam(torch.optim.Optimizer):\n    optimizer_id = 0\n\n    def __init__(self,\n                 model_params,\n                 lr=1e-3,\n                 bias_correction=True,\n                 betas=(0.9, 0.999),\n                 eps=1e-8,\n                 weight_decay=0,\n                 amsgrad=False,\n                 adamw_mode=True,\n                 fp32_optimizer_states=True):\n        \"\"\"Fast vectorized implementation of two variations of Adam optimizer on CPU:\n\n        * Adam: A Method for Stochastic Optimization: (https://arxiv.org/abs/1412.6980);\n        * AdamW: Fixing Weight Decay Regularization in Adam (https://arxiv.org/abs/1711.05101)\n\n        DeepSpeed CPU Adam(W) provides between 5x to 7x speedup over torch.optim.adam(W).\n        In order to apply this optimizer, the model requires to have its master parameter (in FP32)\n        reside on the CPU memory.\n\n        To train on a heterogeneous system, such as coordinating CPU and GPU, DeepSpeed offers\n        the ZeRO-Offload technology which efficiently offloads the optimizer states into CPU memory,\n        with minimal impact on training throughput. DeepSpeedCPUAdam plays an important role to minimize\n        the overhead of the optimizer's latency on CPU. Please refer to ZeRO-Offload tutorial\n        (https://www.deepspeed.ai/tutorials/zero-offload/) for more information on how to enable this technology.\n\n        For calling step function, there are two options available: (1) update optimizer's states and (2) update\n        optimizer's states and copy the parameters back to GPU at the same time. We have seen that the second\n        option can bring 30% higher throughput than the doing the copy separately using option one.\n\n\n        .. note::\n                We recommend using our `config\n                <https://www.deepspeed.ai/docs/config-json/#optimizer-parameters>`_\n                to allow :meth:`deepspeed.initialize` to build this optimizer\n                for you.\n\n\n        Arguments:\n            model_params (iterable): iterable of parameters to optimize or dicts defining\n                parameter groups.\n            lr (float, optional): learning rate. (default: 1e-3)\n            betas (Tuple[float, float], optional): coefficients used for computing\n                running averages of gradient and its square. (default: (0.9, 0.999))\n            eps (float, optional): term added to the denominator to improve\n                numerical stability. (default: 1e-8)\n            weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n            amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n                algorithm from the paper `On the Convergence of Adam and Beyond`_\n                (default: False) NOT SUPPORTED in DeepSpeed CPUAdam!\n            adamw_mode: select between Adam and AdamW implementations (default: AdamW)\n            fp32_optimizer_states: creates momentum and variance in full precision regardless of\n                        the precision of the parameters (default: True)\n        \"\"\"\n\n        default_args = dict(lr=lr,\n                            betas=betas,\n                            eps=eps,\n                            weight_decay=weight_decay,\n                            bias_correction=bias_correction,\n                            amsgrad=amsgrad)\n        super(DeepSpeedCPUAdam, self).__init__(model_params, default_args)\n\n        cpu_info = get_cpu_info()\n        self.cpu_vendor = cpu_info[\"vendor_id_raw\"].lower() if \"vendor_id_raw\" in cpu_info else \"unknown\"\n        if \"amd\" in self.cpu_vendor:\n            for group_id, group in enumerate(self.param_groups):\n                for param_id, p in enumerate(group['params']):\n                    if p.dtype == torch.half:\n                        logger.warning(\"FP16 params for CPUAdam may not work on AMD CPUs\")\n                        break\n                else:\n                    continue\n                break\n\n        self.opt_id = DeepSpeedCPUAdam.optimizer_id\n        DeepSpeedCPUAdam.optimizer_id = DeepSpeedCPUAdam.optimizer_id + 1\n        self.adam_w_mode = adamw_mode\n        self.fp32_optimizer_states = fp32_optimizer_states\n        self.ds_opt_adam = CPUAdamBuilder().load()\n\n        self.ds_opt_adam.create_adam(self.opt_id, lr, betas[0], betas[1], eps, weight_decay, adamw_mode,\n                                     should_log_le(\"info\"))\n\n    def __del__(self):\n        # need to destroy the C++ object explicitly to avoid a memory leak when deepspeed.initialize\n        # is used multiple times in the same process (notebook or pytest worker)\n        self.ds_opt_adam.destroy_adam(self.opt_id)\n\n    def __setstate__(self, state):\n        super(DeepSpeedCPUAdam, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault('amsgrad', False)\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        \"\"\"Update the model parameters.\n\n        .. note::\n            This method will be called internally by ZeRO-Offload. DeepSpeed\n            users should still use ``engine.step()`` as shown in the\n            `Getting Started\n            <https://www.deepspeed.ai/getting-started/#training>`_ guide.\n\n        Args:\n            closure (callable, optional): closure to compute the loss.\n                Defaults to ``None``.\n\n        Returns:\n            loss: if ``closure`` is provided. Otherwise ``None``.\n        \"\"\"\n\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        # intended device for step\n        device = torch.device('cpu')\n\n        for group_id, group in enumerate(self.param_groups):\n            for param_id, p in enumerate(group['params']):\n\n                if p.grad is None:\n                    continue\n\n                assert p.device == device, f\"CPUAdam param is on {p.device} and must be 'cpu', make \" \\\n                        \"sure you enabled 'offload_optimizer': 'cpu' in your ZeRO config.\"\n\n                state = self.state[p]\n                # State initialization\n                if len(state) == 0:\n                    #print(f'group {group_id} param {param_id} = {p.numel()}')\n                    state['step'] = 0\n\n                    #use full precision by default unless self.fp32_optimizer_states is off\n                    state_dtype = torch.float if self.fp32_optimizer_states else p.dtype\n\n                    # gradient momentums\n                    state['exp_avg'] = torch.zeros_like(p.data, dtype=state_dtype, device=device)\n                    #memory_format=torch.preserve_format)\n                    # gradient variances\n                    state['exp_avg_sq'] = torch.zeros_like(p.data, dtype=state_dtype, device=device)\n                    #memory_format=torch.preserve_format)\n\n                state['step'] += 1\n                beta1, beta2 = group['betas']\n\n                self.ds_opt_adam.adam_update(self.opt_id, state['step'], group['lr'], beta1, beta2, group['eps'],\n                                             group['weight_decay'], group['bias_correction'], p.data, p.grad.data,\n                                             state['exp_avg'], state['exp_avg_sq'])\n        return loss\n", "deepspeed/ops/adam/multi_tensor_apply.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\"\"\"\nCopyright NVIDIA/apex\nThis file is adapted from NVIDIA/apex, commit a109f85\n\"\"\"\n\n\nclass MultiTensorApply(object):\n\n    def __init__(self, chunk_size):\n        self.chunk_size = chunk_size\n\n    def __call__(self, op, noop_flag_buffer, tensor_lists, *args):\n        return op(self.chunk_size, noop_flag_buffer, tensor_lists, *args)\n", "deepspeed/ops/lion/fused_lion.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\"\"\"\nThis file is modified from fused_adam.py\n\"\"\"\n\nimport torch\nfrom .multi_tensor_apply import MultiTensorApply\n\nmulti_tensor_applier = MultiTensorApply(2048 * 32)\nfrom deepspeed.accelerator import get_accelerator\nfrom deepspeed.ops.op_builder import FusedLionBuilder\n\n\nclass FusedLion(torch.optim.Optimizer):\n    \"\"\"Implements Lion algorithm.\n\n    Currently GPU-only.\n\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups.\n        lr (float, optional): learning rate. (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square. (default: (0.9, 0.999))\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        set_grad_none (bool, optional): whether set grad to None when zero_grad()\n            method is called. (default: True)\n\n    .. _Symbolic Discovery of Optimization Algorithms:\n        https://doi.org/10.48550/arXiv.2302.06675\n    \"\"\"\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), weight_decay=0., set_grad_none=True):\n\n        defaults = dict(lr=lr, betas=betas, weight_decay=weight_decay)\n        super(FusedLion, self).__init__(params, defaults)\n        self.set_grad_none = set_grad_none\n\n        fused_lion_cuda = FusedLionBuilder().load()\n        # Skip buffer\n        self._dummy_overflow_buf = get_accelerator().IntTensor([0])\n        self.multi_tensor_lion = fused_lion_cuda.multi_tensor_lion\n\n    def zero_grad(self):\n        if self.set_grad_none:\n            for group in self.param_groups:\n                for p in group['params']:\n                    p.grad = None\n        else:\n            super(FusedLion, self).zero_grad()\n\n    def step(self, closure=None, grads=None, output_params=None, scale=None, grad_norms=None, grad_scaler=None):\n        \"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n\n        The remaining arguments are deprecated, and are only retained (for the moment) for error-checking purposes.\n        \"\"\"\n        if any(p is not None for p in [grads, output_params, scale, grad_norms]):\n            raise RuntimeError('FusedLion has been updated.')\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            if len(group['params']) == 0:\n                continue\n            beta1, beta2 = group['betas']\n\n            # assume same step across group now to simplify things\n            # per parameter step can be easily support by making it tensor, or pass list into kernel\n            if 'step' not in group:\n                group['step'] = 0\n\n            # create lists for multi-tensor apply\n            g_16, p_16, m_16 = [], [], []\n            g_bf, p_bf, m_bf = [], [], []\n            g_32, p_32, m_32 = [], [], []\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                if p.grad.data.is_sparse:\n                    raise NotImplementedError('FusedLion does not support sparse gradients')\n\n                state = self.state[p]\n                # State initialization\n                if len(state) == 0:\n                    # DeepSpeed ZeRO 3 processes each subgroup a time, so we need to keep tracking step count for each tensor separately.\n                    # While this is not an issue for ZeRO 1 & 2, since they apply a single optimization step to the whole param group at the same time.\n                    # In order to keep backward compatibility for the existing checkpoints, we use group['state'] to initialize state['step'] if it exists.\n                    state['step'] = group.get('step', 0)\n                    # Exponential moving average of gradient values\n                    state['exp_avg'] = torch.zeros_like(p.data)\n\n                if p.dtype == torch.float16:\n                    g_16.append(p.grad.data)\n                    p_16.append(p.data)\n                    m_16.append(state['exp_avg'])\n                elif p.dtype == torch.bfloat16:\n                    g_bf.append(p.grad)\n                    p_bf.append(p)\n                    m_bf.append(state['exp_avg'])\n                elif p.dtype == torch.float32:\n                    g_32.append(p.grad.data)\n                    p_32.append(p.data)\n                    m_32.append(state['exp_avg'])\n                else:\n                    raise RuntimeError('FusedLion only support fp16, bf16 and fp32.')\n\n            if len(g_16) > 0:\n                state['step'] += 1\n                multi_tensor_applier(self.multi_tensor_lion, self._dummy_overflow_buf, [g_16, p_16, m_16], group['lr'],\n                                     beta1, beta2, state['step'], group['weight_decay'])\n\n            if len(g_bf) > 0:\n                state['step'] += 1\n                multi_tensor_applier(self.multi_tensor_lion, self._dummy_overflow_buf, [g_bf, p_bf, m_bf], group['lr'],\n                                     beta1, beta2, state['step'], group['weight_decay'])\n\n            if len(g_32) > 0:\n                state['step'] += 1\n                multi_tensor_applier(self.multi_tensor_lion, self._dummy_overflow_buf, [g_32, p_32, m_32], group['lr'],\n                                     beta1, beta2, state['step'], group['weight_decay'])\n\n        return loss\n", "deepspeed/ops/lion/cpu_lion.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nfrom cpuinfo import get_cpu_info\nfrom deepspeed.utils import logger\nfrom deepspeed.utils.logging import should_log_le\nfrom deepspeed.ops.op_builder import CPULionBuilder\n\n\nclass DeepSpeedCPULion(torch.optim.Optimizer):\n    optimizer_id = 0\n\n    def __init__(self, model_params, lr=1e-3, betas=(0.9, 0.999), weight_decay=0, fp32_optimizer_states=True):\n        \"\"\"Fast vectorized implementation of Lion optimizer on CPU:\n\n        See Symbolic Discovery of Optimization Algorithms (https://doi.org/10.48550/arXiv.2302.06675).\n\n        .. note::\n                We recommend using our `config\n                <https://www.deepspeed.ai/docs/config-json/#optimizer-parameters>`_\n                to allow :meth:`deepspeed.initialize` to build this optimizer\n                for you.\n\n\n        Arguments:\n            model_params (iterable): iterable of parameters to optimize or dicts defining\n                parameter groups.\n            lr (float, optional): learning rate. (default: 1e-3)\n            betas (Tuple[float, float], optional): coefficients used for computing\n                running averages of gradient and its square. (default: (0.9, 0.999))\n            weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n            full_precision_optimizer_states: creates momentum and variance in full precision regardless of\n                        the precision of the parameters (default: True)\n        \"\"\"\n\n        default_args = dict(lr=lr, betas=betas, weight_decay=weight_decay)\n        super(DeepSpeedCPULion, self).__init__(model_params, default_args)\n\n        cpu_info = get_cpu_info()\n        self.cpu_vendor = cpu_info[\"vendor_id_raw\"].lower() if \"vendor_id_raw\" in cpu_info else \"unknown\"\n        if \"amd\" in self.cpu_vendor:\n            for group_id, group in enumerate(self.param_groups):\n                for param_id, p in enumerate(group['params']):\n                    if p.dtype == torch.half:\n                        logger.warning(\"FP16 params for CPULion may not work on AMD CPUs\")\n                        break\n                else:\n                    continue\n                break\n\n        self.opt_id = DeepSpeedCPULion.optimizer_id\n        DeepSpeedCPULion.optimizer_id = DeepSpeedCPULion.optimizer_id + 1\n        self.fp32_optimizer_states = fp32_optimizer_states\n        self.ds_opt_lion = CPULionBuilder().load()\n\n        self.ds_opt_lion.create_lion(self.opt_id, lr, betas[0], betas[1], weight_decay, should_log_le(\"info\"))\n\n    def __del__(self):\n        # need to destroy the C++ object explicitly to avoid a memory leak when deepspeed.initialize\n        # is used multiple times in the same process (notebook or pytest worker)\n        self.ds_opt_lion.destroy_lion(self.opt_id)\n\n    def __setstate__(self, state):\n        super(DeepSpeedCPULion, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault('amsgrad', False)\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        \"\"\"Update the model parameters.\n\n        .. note::\n            This method will be called internally by ZeRO-Offload. DeepSpeed\n            users should still use ``engine.step()`` as shown in the\n            `Getting Started\n            <https://www.deepspeed.ai/getting-started/#training>`_ guide.\n\n        Args:\n            closure (callable, optional): closure to compute the loss.\n                Defaults to ``None``.\n\n        Returns:\n            loss: if ``closure`` is provided. Otherwise ``None``.\n        \"\"\"\n\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        # intended device for step\n        device = torch.device('cpu')\n\n        for group_id, group in enumerate(self.param_groups):\n            for param_id, p in enumerate(group['params']):\n\n                if p.grad is None:\n                    continue\n\n                assert p.device == device, f\"CPULion param is on {p.device} and must be 'cpu', make \" \\\n                        \"sure you enabled 'offload_optimizer': 'cpu' in your ZeRO config.\"\n\n                state = self.state[p]\n                # State initialization\n                if len(state) == 0:\n                    #print(f'group {group_id} param {param_id} = {p.numel()}')\n                    state['step'] = 0\n\n                    #use full precision by default unless self.fp32_optimizer_states is off\n                    state_dtype = torch.float if self.fp32_optimizer_states else p.dtype\n\n                    # gradient momentums\n                    state['exp_avg'] = torch.zeros_like(p.data, dtype=state_dtype, device=device)\n                    #memory_format=torch.preserve_format)\n                    # gradient variances\n                    state['exp_avg_sq'] = torch.zeros_like(p.data, dtype=state_dtype, device=device)\n                    #memory_format=torch.preserve_format)\n\n                state['step'] += 1\n                beta1, beta2 = group['betas']\n\n                self.ds_opt_lion.lion_update(self.opt_id, state['step'], group['lr'], beta1, beta2,\n                                             group['weight_decay'], p.data, p.grad.data, state['exp_avg'])\n        return loss\n", "deepspeed/ops/lion/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .cpu_lion import DeepSpeedCPULion\nfrom .fused_lion import FusedLion\n", "deepspeed/ops/lion/multi_tensor_apply.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\"\"\"\nCopyright NVIDIA/apex\nThis file is adapted from NVIDIA/apex, commit a109f85\n\"\"\"\n\n\nclass MultiTensorApply(object):\n\n    def __init__(self, chunk_size):\n        self.chunk_size = chunk_size\n\n    def __call__(self, op, noop_flag_buffer, tensor_lists, *args):\n        return op(self.chunk_size, noop_flag_buffer, tensor_lists, *args)\n", "deepspeed/ops/adagrad/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .cpu_adagrad import DeepSpeedCPUAdagrad\n", "deepspeed/ops/adagrad/cpu_adagrad.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nfrom deepspeed.ops.op_builder import CPUAdagradBuilder\nfrom deepspeed.utils.logging import should_log_le\n\n\nclass DeepSpeedCPUAdagrad(torch.optim.Optimizer):\n    optimizer_id = 0\n\n    def __init__(self, model_params, lr=1e-2, eps=1e-10, weight_decay=0, amsgrad=False, fp32_optimizer_states=True):\n\n        default_args = dict(lr=lr, eps=eps, weight_decay=weight_decay, amsgrad=amsgrad)\n        super(DeepSpeedCPUAdagrad, self).__init__(model_params, default_args)\n\n        self.opt_id = DeepSpeedCPUAdagrad.optimizer_id\n        DeepSpeedCPUAdagrad.optimizer_id = DeepSpeedCPUAdagrad.optimizer_id + 1\n        self.fp32_optimizer_states = fp32_optimizer_states\n        self.ds_opt_adagrad = CPUAdagradBuilder().load()\n\n        self.ds_opt_adagrad.create_adagrad(self.opt_id, lr, eps, weight_decay, should_log_le(\"info\"))\n\n    def __del__(self):\n        # need to destroy the C++ object explicitly to avoid a memory leak when deepspeed.initialize\n        # is used multiple times in the same process (notebook or pytest worker)\n        self.ds_opt_adagrad.destroy_adagrad(self.opt_id)\n\n    def __setstate__(self, state):\n        super(DeepSpeedCPUAdagrad, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault('amsgrad', False)\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        \"\"\"Update the model parameters.\n\n        .. note::\n            This method will be called internally by ZeRO-Offload. DeepSpeed\n            users should still use ``engine.step()`` as shown in the\n            `Getting Started\n            <https://www.deepspeed.ai/getting-started/#training>`_ guide.\n\n        Args:\n            closure (callable, optional): closure to compute the loss.\n                Defaults to ``None``.\n\n        Returns:\n            loss: if ``closure`` is provided. Otherwise ``None``.\n        \"\"\"\n\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        # intended device for step\n        device = torch.device('cpu')\n\n        for group_id, group in enumerate(self.param_groups):\n            for param_id, p in enumerate(group['params']):\n\n                if p.grad is None:\n                    continue\n\n                assert p.device == device, f\"CPUAdagrad param is on {p.device} and must be 'cpu', make \" \\\n                        \"sure you enabled 'offload_optimizer': 'cpu' in your ZeRO config.\"\n\n                state = self.state[p]\n                # State initialization\n                if len(state) == 0:\n                    #print(f'group {group_id} param {param_id} = {p.numel()}')\n                    state['step'] = 0\n\n                    #use full precision by default unless self.fp32_optimizer_states is off\n                    state_dtype = torch.float if self.fp32_optimizer_states else p.dtype\n\n                    #memory_format=torch.preserve_format)\n                    # gradient variances\n                    state['exp_avg_sq'] = torch.zeros_like(p.data, dtype=state_dtype, device='cpu')\n                    #memory_format=torch.preserve_format)\n\n                state['step'] += 1\n\n                if p.grad.is_sparse == True:\n                    sparse_param = p.sparse_mask(p.grad)\n                    sparse_exp_avg_sq = state['exp_avg_sq'].sparse_mask(p.grad)\n                    self.ds_opt_adagrad.adagrad_update(self.opt_id, state['step'], group['lr'], group['eps'],\n                                                       group['weight_decay'], sparse_param.values(), p.grad.values(),\n                                                       sparse_exp_avg_sq.values())\n                    p[sparse_param.indices()] = sparse_param.values()\n                    state['exp_avg_sq'][sparse_exp_avg_sq.indices()] = sparse_exp_avg_sq.values()\n                else:\n                    self.ds_opt_adagrad.adagrad_update(self.opt_id, state['step'], group['lr'], group['eps'],\n                                                       group['weight_decay'], p.data, p.grad.data, state['exp_avg_sq'])\n        return loss\n", "deepspeed/ops/deepspeed4science/evoformer_attn.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nimport numpy as np\nfrom deepspeed.ops.op_builder import EvoformerAttnBuilder\nfrom deepspeed.accelerator import get_accelerator\n\nkernel_ = None\n\n\ndef _attention(Q, K, V, bias1, bias2):\n    assert Q.shape[-3] > 16, \"seq_len must be greater than 16\"\n    O = torch.empty_like(Q, dtype=Q.dtype)\n    assert get_accelerator().on_accelerator(Q), \"Q must be on cuda\"\n    assert get_accelerator().on_accelerator(K), \"K must be on cuda\"\n    assert get_accelerator().on_accelerator(V), \"V must be on cuda\"\n    assert get_accelerator().on_accelerator(bias1), \"bias1 must be on cuda\"\n    assert get_accelerator().on_accelerator(bias2), \"bias2 must be on cuda\"\n    global kernel_\n    if kernel_ is None:\n        kernel_ = EvoformerAttnBuilder().load()\n    nheads = Q.shape[-2]\n    nq = (Q.shape[-3] + 31) // 32 * 32\n    nb = np.prod(Q.shape[:-3])\n    lse = torch.empty((nb, nheads, nq), dtype=torch.float32, device=Q.device)\n    kernel_.attention(Q, K, V, bias1, bias2, O, lse)\n    return O, lse\n\n\ndef attention_bwd(dO, Q, K, V, O, lse, bias1, bias2, bias1_grad, bias2_grad):\n    assert max(Q.shape[-1], V.shape[-1]) <= 64, \"Hidden size is too large. Need to change kMax to a larger value\"\n    dQ = torch.empty_like(Q, dtype=Q.dtype)\n    dK = torch.empty_like(K, dtype=K.dtype)\n    dV = torch.empty_like(V, dtype=V.dtype)\n    assert get_accelerator().on_accelerator(dO), \"dO must be on cuda\"\n    assert get_accelerator().on_accelerator(Q), \"Q must be on cuda\"\n    assert get_accelerator().on_accelerator(K), \"K must be on cuda\"\n    assert get_accelerator().on_accelerator(V), \"V must be on cuda\"\n    assert get_accelerator().on_accelerator(O), \"O must be on cuda\"\n    global kernel_\n    if kernel_ is None:\n        kernel_ = EvoformerAttnBuilder().load()\n    delta = torch.empty_like(lse)\n    if bias1_grad:\n        dB1 = torch.zeros_like(bias1, dtype=torch.float32)\n    else:\n        dB1 = torch.tensor([], dtype=torch.float32, device=bias1.device)\n    if bias2_grad:\n        dB2 = torch.zeros_like(bias2, dtype=torch.float32)\n    else:\n        dB2 = torch.tensor([], dtype=torch.float32, device=bias2.device)\n    kernel_.attention_bwd(dO, Q, K, V, O, lse, delta, bias1, bias2, dQ, dK, dV, dB1, dB2)\n    return dQ, dK, dV, dB1.to(dO.dtype), dB2.to(dO.dtype)\n\n\nclass EvoformerFusedAttention(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx, q, k, v, bias1=None, bias2=None):\n        \"\"\"\n        q, k, v: are in shape [*, L, H, D]\n        \"\"\"\n        bias1_ = bias1.contiguous() if bias1 is not None else torch.tensor([], dtype=q.dtype, device=q.device)\n        bias2_ = bias2.contiguous() if bias2 is not None else torch.tensor([], dtype=q.dtype, device=q.device)\n        q = q.contiguous()\n        k = k.contiguous()\n        v = v.contiguous()\n        o, lse = _attention(q, k, v, bias1_, bias2_)\n        ctx.save_for_backward(q, k, v, o, lse, bias1_, bias2_)\n        return o\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        q, k, v, o, lse, bias1, bias2 = ctx.saved_tensors\n        is_b1_grad = bias1.numel() != 0 and ctx.needs_input_grad[3]\n        is_b2_grad = bias2.numel() != 0 and ctx.needs_input_grad[4]\n        dQ, dK, dV, dB1, dB2 = attention_bwd(grad_output, q, k, v, o, lse, bias1, bias2, is_b1_grad, is_b2_grad)\n        if not is_b1_grad:\n            dB1 = None\n        if not is_b2_grad:\n            dB2 = None\n        return dQ, dK, dV, dB1, dB2\n\n\ndef DS4Sci_EvoformerAttention(Q, K, V, biases):\n    assert len(biases) <= 2\n\n    if (len(biases) == 0):\n        biases.append(None)\n\n    if (len(biases) == 1):\n        biases.append(None)\n\n    bias_1_shape = lambda x: (x.shape[0], x.shape[1], 1, 1, x.shape[2])\n    bias_2_shape = lambda x: (x.shape[0], 1, x.shape[3], x.shape[2], x.shape[2])\n\n    if biases[0] is not None:\n        assert biases[0].shape == bias_1_shape(Q), \"bias1 shape is incorrect\"\n\n    if biases[1] is not None:\n        assert biases[1].shape == bias_2_shape(Q), \"bias2 shape is incorrect\"\n\n    return EvoformerFusedAttention.apply(Q, K, V, biases[0], biases[1])\n", "deepspeed/ops/deepspeed4science/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .evoformer_attn import DS4Sci_EvoformerAttention, EvoformerFusedAttention\n", "deepspeed/ops/transformer/transformer.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport json\nimport math\nimport torch\nfrom torch import nn\nfrom torch.autograd import Function\nfrom deepspeed.accelerator import get_accelerator\nfrom deepspeed.ops.op_builder import TransformerBuilder, StochasticTransformerBuilder\n\n# Cuda modules will be imported if needed\ntransformer_cuda_module = None\nstochastic_transformer_cuda_module = None\n\n\nclass TransformerConfig():\n\n    def __init__(self, batch_size, hidden_size, intermediate_size, heads, attn_dropout_ratio, hidden_dropout_ratio,\n                 num_hidden_layers, initializer_range):\n        self.layer_id = -1\n        self.batch_size = batch_size\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        self.heads = heads\n        self.attn_dropout_ratio = attn_dropout_ratio\n        self.hidden_dropout_ratio = hidden_dropout_ratio\n        self.num_hidden_layers = num_hidden_layers\n        self.initializer_range = initializer_range\n\n\nclass DeepSpeedTransformerConfig(TransformerConfig):\n    \"\"\"Initialize the DeepSpeed Transformer Config.\n\n        Arguments:\n            batch_size: The maximum batch size used for running the kernel on each GPU\n\n            hidden_size: The hidden size of the transformer layer\n\n            intermediate_size: The intermediate size of the feed-forward part of transformer layer\n\n            heads: The number of heads in the self-attention of the transformer layer\n\n            attn_dropout_ratio: The ratio of dropout for the attention's output\n\n            hidden_dropout_ratio: The ratio of dropout for the transformer's output\n\n            num_hidden_layers: The number of transformer layers\n\n            initializer_range: BERT model's initializer range for initializing parameter data\n\n            local_rank: Optional: The rank of GPU running the transformer kernel, it is not required\n                to use if the model already set the current device, otherwise need to set it\n                so that the transformer kernel can work on the right device\n\n            seed: The random seed for the dropout layers\n\n            fp16: Enable half-precision computation\n\n            pre_layer_norm: Select between Pre-LN or Post-LN transformer architecture\n\n            normalize_invertible: Optional: Enable invertible LayerNorm execution (dropping the input activation),\n                default is False\n\n            gelu_checkpoint: Optional: Enable checkpointing of Gelu activation output to save memory,\n                default is False\n\n            adjust_init_range: Optional: Set as True (default) if the model adjusts the weight initial values of\n                its self-attention output and layer output, False keeps the initializer_range no change.\n                See the adjustment below:\n                    output_std = self.config.initializer_range / math.sqrt(2.0 * num_layers)\n\n            attn_dropout_checkpoint: Optional: Enable checkpointing of attention dropout to save memory,\n                default is False\n\n            stochastic_mode:  Enable for high performance, please note that this flag has some level of\n                non-determinism and can produce different results on different runs.  However, we have seen\n                that by enabling it, the pretraining tasks such as BERT are not affected and can obtain\n                a high accuracy level. On the other hand, for the downstream tasks, such as fine-tuning, we recommend\n                to turn it off in order to be able to reproduce the same result through the regular kernel execution.\n\n            return_tuple: Enable if using the return_tuple interface style for sending out the forward results.\n\n            training: Enable for training rather than inference.\n    \"\"\"\n\n    def __init__(self,\n                 batch_size=-1,\n                 hidden_size=-1,\n                 intermediate_size=-1,\n                 heads=-1,\n                 attn_dropout_ratio=-1,\n                 hidden_dropout_ratio=-1,\n                 num_hidden_layers=-1,\n                 initializer_range=-1,\n                 layer_norm_eps=1e-12,\n                 local_rank=-1,\n                 seed=-1,\n                 fp16=False,\n                 pre_layer_norm=True,\n                 normalize_invertible=False,\n                 gelu_checkpoint=False,\n                 adjust_init_range=True,\n                 attn_dropout_checkpoint=False,\n                 stochastic_mode=False,\n                 return_tuple=False,\n                 training=True):\n        super(DeepSpeedTransformerConfig,\n              self).__init__(batch_size, hidden_size,\n                             (intermediate_size if intermediate_size > 0 else 4 * hidden_size), heads,\n                             attn_dropout_ratio, hidden_dropout_ratio, num_hidden_layers, initializer_range)\n        self.fp16 = fp16\n        self.pre_layer_norm = pre_layer_norm\n        self.local_rank = local_rank\n        self.seed = seed\n        self.normalize_invertible = normalize_invertible\n        self.gelu_checkpoint = gelu_checkpoint  # True: if higher batch size is required\n        self.adjust_init_range = adjust_init_range\n        self.test_gemm = False\n        self.layer_norm_eps = layer_norm_eps\n        self.training = training\n        self.is_grad_enabled = True\n        self.attn_dropout_checkpoint = attn_dropout_checkpoint\n        self.stochastic_mode = stochastic_mode\n        self.return_tuple = return_tuple\n\n    @classmethod\n    def from_dict(cls, json_object):\n        config = DeepSpeedTransformerConfig()\n        for key, value in json_object.items():\n            config.__dict__[key] = value\n        return config\n\n    @classmethod\n    def from_json_file(cls, json_file):\n        with open(json_file, \"r\", encoding='utf-16') as reader:\n            text = reader.read()\n        return cls.from_dict(json.loads(text))\n\n\nclass DeepSpeedTransformerFunction(Function):\n\n    @staticmethod\n    def forward(ctx, input, input_mask, self, grads, layer_id, attn_qkvw, attn_qkvb, attn_ow, attn_ob, attn_nw,\n                attn_nb, inter_w, inter_b, output_w, output_b, norm_w, norm_b, config):\n\n        cuda_module = stochastic_transformer_cuda_module if config.stochastic_mode else transformer_cuda_module\n        forward_func = cuda_module.forward_fp16 if config.fp16 else cuda_module.forward_fp32\n\n        inp_size = input.size()\n        if inp_size[1] % 16 != 0:\n            input = torch.cat(\n                (input,\n                 torch.randn(\n                     (inp_size[0], (16 - (inp_size[1] % 16)), inp_size[2]), device=input.device, dtype=input.dtype)),\n                1)\n            input_mask = torch.cat((input_mask, torch.ones((inp_size[0], input_mask.shape[1], input_mask.shape[2], \\\n                                            (16 - (inp_size[1] % 16))), device=input_mask.device, dtype=input_mask.dtype) * -10000), 3)\n\n        (output, inp_norm, qkv_tf, soft_inp, ctx_bufB, attn_o_inp, add_res, ff1_inp, gelu_inp, ff2_inp,\n         attn_prob_dropout_mask, attn_output_dropout_mask, layer_output_dropout_mask, attn_layer_norm_var,\n         attn_layer_norm_mean, layer_norm_var, layer_norm_mean) = forward_func(\n             config.layer_id, input, input_mask, attn_qkvw, attn_qkvb, attn_ow, attn_ob, attn_nw, attn_nb, inter_w,\n             inter_b, output_w, output_b, norm_w, norm_b, config.training and config.is_grad_enabled,\n             config.pre_layer_norm, config.attn_dropout_checkpoint, config.normalize_invertible,\n             config.gelu_checkpoint)\n\n        # For testing only.\n        if grads is not None:\n            for i in [2]:\n                attn_qkvw.register_hook(lambda x, i=i, self=self: grads.append([\n                    x[i * attn_ow.size(0):(i + 1) * attn_ow.size(0)], (\"Q_W\" if i == 0 else \"K_W\" if i == 1 else \"V_W\")\n                ]))\n            for i in [2]:\n                attn_qkvb.register_hook(lambda x, i=i, self=self: grads.append([\n                    x[i * attn_ow.size(0):(i + 1) * attn_ow.size(0)], (\"Q_B\" if i == 0 else \"K_B\" if i == 1 else \"V_B\")\n                ]))\n\n            attn_ow.register_hook(lambda x, self=self: grads.append([x, \"O_W\"]))\n            attn_ob.register_hook(lambda x, self=self: grads.append([x, \"O_B\"]))\n            attn_nw.register_hook(lambda x, self=self: grads.append([x, \"N2_W\"]))\n            attn_nb.register_hook(lambda x, self=self: grads.append([x, \"N2_B\"]))\n            inter_w.register_hook(lambda x, self=self: grads.append([x, \"int_W\"]))\n            inter_b.register_hook(lambda x, self=self: grads.append([x, \"int_B\"]))\n            output_w.register_hook(lambda x, self=self: grads.append([x, \"out_W\"]))\n            output_b.register_hook(lambda x, self=self: grads.append([x, \"out_B\"]))\n            norm_w.register_hook(lambda x, self=self: grads.append([x, \"norm_W\"]))\n            norm_b.register_hook(lambda x, self=self: grads.append([x, \"norm_B\"]))\n\n        if config.is_grad_enabled and config.training:\n            if (config.pre_layer_norm and config.normalize_invertible):\n                ctx.save_for_backward(input_mask, attn_qkvw, attn_qkvb, attn_ow, attn_ob, attn_nw, attn_nb, inter_w,\n                                      inter_b, output_w, output_b, norm_w, norm_b)\n            else:\n                ctx.save_for_backward(output, input, input_mask, attn_qkvw, attn_qkvb, attn_ow, attn_ob, attn_nw,\n                                      attn_nb, inter_w, inter_b, output_w, output_b, norm_w, norm_b)\n\n            ctx.config = config\n            if (config.pre_layer_norm or not config.normalize_invertible):\n                ctx.inp_norm = inp_norm\n\n            ctx.qkv_tf = qkv_tf\n            ctx.soft_inp = soft_inp\n            if not config.attn_dropout_checkpoint:\n                ctx.ctx_bufB = ctx_bufB\n\n            ctx.attn_o_inp = attn_o_inp\n            if not config.normalize_invertible:\n                ctx.add_res = add_res\n\n            ctx.attn_layer_norm_mean = attn_layer_norm_mean\n            ctx.layer_norm_mean = layer_norm_mean\n\n            ctx.ff1_inp = ff1_inp\n            if not config.gelu_checkpoint:\n                ctx.gelu_inp = gelu_inp\n\n            ctx.ff2_inp = ff2_inp\n            ctx.attn_prob_dropout_mask = attn_prob_dropout_mask\n            ctx.attn_output_dropout_mask = attn_output_dropout_mask\n            ctx.layer_output_dropout_mask = layer_output_dropout_mask\n            ctx.attn_layer_norm_var = attn_layer_norm_var\n            ctx.layer_norm_var = layer_norm_var\n\n        if inp_size[1] % 16 != 0:\n            output = torch.narrow(output, 1, 0, inp_size[1])\n\n        if config.return_tuple:\n            return (output, )  # outputs -> (output) : outputs[0] = output\n        else:\n            return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        bsz = grad_output.shape[0]\n        grad_output_shape = grad_output.size()\n        if grad_output_shape[1] % 16 != 0:\n            grad_output = torch.cat((grad_output, torch.zeros((bsz, (16 - (grad_output_shape[1] % 16)), \\\n                                        grad_output_shape[2]), device=grad_output.device, dtype=grad_output.dtype)), 1)\n\n        assert ctx.config.training\n\n        if (ctx.config.pre_layer_norm and ctx.config.normalize_invertible):\n            (input_mask, attn_qkvw, attn_qkvb, attn_ow, attn_ob, attn_nw, attn_nb, inter_w, inter_b, output_w,\n             output_b, norm_w, norm_b) = ctx.saved_tensors\n        else:\n            (output, input, input_mask, attn_qkvw, attn_qkvb, attn_ow, attn_ob, attn_nw, attn_nb, inter_w, inter_b,\n             output_w, output_b, norm_w, norm_b) = ctx.saved_tensors\n\n        cuda_module = stochastic_transformer_cuda_module if ctx.config.stochastic_mode else transformer_cuda_module\n        backward_func = cuda_module.backward_fp16 if ctx.config.fp16 else cuda_module.backward_fp32\n\n        (grad_input, grad_attn_qkvw, grad_attn_qkvb, grad_attn_ow, grad_attn_ob, grad_attn_nw, grad_attn_nb,\n         grad_inter_w, grad_inter_b, grad_output_w, grad_output_b, grad_norm_w, grad_norm_b) = backward_func(\n             ctx.config.layer_id, grad_output,\n             (ctx.inp_norm if (ctx.config.pre_layer_norm and ctx.config.normalize_invertible) else output),\n             (ctx.inp_norm if (ctx.config.pre_layer_norm or not ctx.config.normalize_invertible) else input),\n             ctx.qkv_tf, ctx.soft_inp, (ctx.soft_inp if ctx.config.attn_dropout_checkpoint else ctx.ctx_bufB),\n             ctx.attn_o_inp, (ctx.ff1_inp if ctx.config.normalize_invertible else ctx.add_res), ctx.ff1_inp,\n             (ctx.ff2_inp if ctx.config.gelu_checkpoint else ctx.gelu_inp), ctx.ff2_inp, ctx.attn_prob_dropout_mask,\n             ctx.attn_output_dropout_mask, ctx.layer_output_dropout_mask, ctx.attn_layer_norm_var,\n             ctx.attn_layer_norm_mean, ctx.layer_norm_var, ctx.layer_norm_mean,\n             (ctx.inp_norm if\n              (ctx.config.pre_layer_norm and ctx.config.normalize_invertible) else input), input_mask, attn_qkvw,\n             attn_qkvb, attn_ow, attn_ob, attn_nw, attn_nb, inter_w, inter_b, output_w, output_b, norm_w, norm_b)\n\n        # This appears to be an effective way to release context memory\n        ctx.qkv_tf = None\n        ctx.soft_inp = None\n        ctx.ctx_bufB = None\n        ctx.gelu_inp = None\n        ctx.ff2_inp = None\n        ctx.attn_o_inp = None\n        ctx.ff1_inp = None\n        ctx.add_res = None\n        ctx.inp_norm = None\n        ctx.config = None\n        ctx.attn_layer_norm_mean = None\n        ctx.layer_norm_mean = None\n        ctx.attn_prob_dropout_mask = None\n        ctx.attn_output_dropout_mask = None\n        ctx.layer_output_dropout_mask = None\n        ctx.attn_layer_norm_var = None\n        ctx.layer_norm_var = None\n\n        if grad_output_shape[1] % 16 != 0:\n            grad_input = torch.narrow(grad_input, 1, 0, grad_output_shape[1])\n\n        return (grad_input, None, None, None, None, grad_attn_qkvw, grad_attn_qkvb, grad_attn_ow, grad_attn_ob,\n                grad_attn_nw, grad_attn_nb, grad_inter_w, grad_inter_b, grad_output_w, grad_output_b, grad_norm_w,\n                grad_norm_b, None)\n\n\nclass DeepSpeedTransformerLayer(nn.Module):\n    \"\"\"Initialize the DeepSpeed Transformer Layer.\n\n        Static variable:\n            layer_id: The layer-index counter starting from 0 and incrementing by 1 every time a layer object is instantiated,\n            e.g. if a model has 24 transformer layers, layer_id goes from 0 to 23.\n        Arguments:\n            config: An object of DeepSpeedTransformerConfig\n\n            initial_weights: Optional: Only used for unit test\n\n            initial_biases: Optional: Only used for unit test\n    \"\"\"\n    layer_id = 0\n\n    def __init__(self, config, initial_weights=None, initial_biases=None):\n        super(DeepSpeedTransformerLayer, self).__init__()\n\n        self.config = config\n        self.config.layer_id = DeepSpeedTransformerLayer.layer_id\n        DeepSpeedTransformerLayer.layer_id = DeepSpeedTransformerLayer.layer_id + 1\n\n        print(\"DeepSpeed Transformer config is \", self.config.__dict__)\n\n        if self.config.local_rank >= 0:\n            get_accelerator().set_device(self.config.local_rank)\n\n        if initial_weights is None and initial_biases is None:\n            self.attn_qkvw = nn.Parameter(torch.Tensor(self.config.hidden_size * 3, self.config.hidden_size))\n            self.attn_qkvb = nn.Parameter(torch.Tensor(self.config.hidden_size * 3))\n            self.attn_ow = nn.Parameter(torch.Tensor(self.config.hidden_size, self.config.hidden_size))\n            self.attn_ob = nn.Parameter(torch.Tensor(self.config.hidden_size))\n            self.attn_nw = nn.Parameter(torch.Tensor(self.config.hidden_size))\n            self.attn_nb = nn.Parameter(torch.Tensor(self.config.hidden_size))\n            self.inter_w = nn.Parameter(torch.Tensor(self.config.intermediate_size, self.config.hidden_size))\n            self.inter_b = nn.Parameter(torch.Tensor(self.config.intermediate_size))\n            self.output_w = nn.Parameter(torch.Tensor(self.config.hidden_size, self.config.intermediate_size))\n            self.output_b = nn.Parameter(torch.Tensor(self.config.hidden_size))\n            self.norm_w = nn.Parameter(torch.Tensor(self.config.hidden_size))\n            self.norm_b = nn.Parameter(torch.Tensor(self.config.hidden_size))\n            self.init_transformer_weights(self.config.adjust_init_range)\n        else:\n            # For testing only.\n            q = initial_weights[0].data\n            k = initial_weights[1].data\n            v = initial_weights[2].data\n\n            self.attn_qkvw = nn.Parameter(torch.cat((q, k, v)))\n            #self.attn_qkvw[i * self.config.hidden_size:(i + 1) * self.config.hidden_size] = \\\n            #    initial_weights[i].clone()\n            #torch.empty_like(initial_weights[i]).data.copy_(initial_weights[i].data)\n            self.attn_qkvb = nn.Parameter(torch.Tensor(self.config.hidden_size * 3))\n            self.attn_qkvb.data.zero_()\n            self.attn_ow = initial_weights[3]\n            self.attn_ob = initial_biases[3]\n            self.attn_nw = initial_weights[4]\n            self.attn_nb = initial_biases[4]\n            self.inter_w = initial_weights[5]\n            self.inter_b = initial_biases[5]\n            self.output_w = initial_weights[6]\n            self.output_b = initial_biases[6]\n            self.norm_w = initial_weights[7]\n            self.norm_b = initial_biases[7]\n\n        # Load cuda modules if needed\n        global transformer_cuda_module, stochastic_transformer_cuda_module\n        if transformer_cuda_module is None and not self.config.stochastic_mode:\n            transformer_cuda_module = TransformerBuilder().load()\n        if stochastic_transformer_cuda_module is None and self.config.stochastic_mode:\n            stochastic_transformer_cuda_module = StochasticTransformerBuilder().load()\n\n        # create the layer in cuda kernels.\n        cuda_module = stochastic_transformer_cuda_module if self.config.stochastic_mode else transformer_cuda_module\n        create_layer_func = cuda_module.create_transformer_layer_fp16 if self.config.fp16 else cuda_module.create_transformer_layer_fp32\n\n        create_layer_func(self.config.layer_id, self.config.batch_size, self.config.hidden_size, self.config.heads,\n                          self.config.intermediate_size, self.config.attn_dropout_ratio,\n                          self.config.hidden_dropout_ratio, self.config.layer_norm_eps, self.config.seed,\n                          self.config.pre_layer_norm, self.config.test_gemm, self.config.attn_dropout_checkpoint,\n                          self.config.normalize_invertible, self.config.gelu_checkpoint, self.config.stochastic_mode)\n\n    def init_transformer_weights(self, adjust_init_range=False):\n        num_layers = self.config.num_hidden_layers\n        output_std = self.config.initializer_range\n        if adjust_init_range and self.config.local_rank == 0:\n            print(\"Accounting for accumulation on the residual path\")\n            output_std = self.config.initializer_range / math.sqrt(2.0 * num_layers)\n\n        self.attn_qkvw.data.normal_(mean=0.0, std=self.config.initializer_range)\n        self.attn_qkvb.data.zero_()\n        self.attn_ow.data.normal_(mean=0.0, std=output_std)\n        self.attn_ob.data.zero_()\n        self.attn_nw.data.fill_(1.0)\n        self.attn_nb.data.zero_()\n        self.inter_w.data.normal_(mean=0.0, std=self.config.initializer_range)\n        self.inter_b.data.zero_()\n        self.output_w.data.normal_(mean=0.0, std=output_std)\n        self.output_b.data.zero_()\n        self.norm_w.data.fill_(1.0)\n        self.norm_b.data.zero_()\n\n    def forward(self,\n                hidden_states,\n                attention_mask=None,\n                head_mask=None,\n                layer_head_mask=None,\n                encoder_hidden_states=None,\n                encoder_attention_mask=None,\n                past_key_value=None,\n                output_attentions=False,\n                grads=None):\n        self.config.is_grad_enabled = torch.is_grad_enabled()\n        self.config.training = self.training\n        return DeepSpeedTransformerFunction.apply(hidden_states, attention_mask, self, grads, self.config.layer_id,\n                                                  self.attn_qkvw, self.attn_qkvb, self.attn_ow, self.attn_ob,\n                                                  self.attn_nw, self.attn_nb, self.inter_w, self.inter_b,\n                                                  self.output_w, self.output_b, self.norm_w, self.norm_b, self.config)\n", "deepspeed/ops/transformer/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .transformer import DeepSpeedTransformerLayer, DeepSpeedTransformerConfig\nfrom .inference.config import DeepSpeedInferenceConfig\nfrom ...model_implementations.transformers.ds_transformer import DeepSpeedTransformerInference\nfrom .inference.moe_inference import DeepSpeedMoEInferenceConfig, DeepSpeedMoEInference\n", "deepspeed/ops/transformer/inference/config.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport json\nimport torch\nfrom deepspeed.utils.types import ActivationFuncType, NormType\n\n\nclass TransformerConfig():\n\n    def __init__(self, hidden_size, intermediate_size, heads, num_hidden_layers):\n        self.layer_id = -1\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        self.heads = heads\n        self.num_hidden_layers = num_hidden_layers\n\n\nclass DeepSpeedInferenceConfig(TransformerConfig):\n    \"\"\"Initialize the DeepSpeed Transformer Config.\n        Arguments:\n            hidden_size: The hidden size of the transformer layer\n            intermediate_size: The intermediate size of the feed-forward part of transformer layer\n            heads: The number of heads in the self-attention of the transformer layer\n            num_hidden_layers: The number of transformer layers\n            layer_norm_eps: The epsilon value for the layer norm\n            local_rank: Optional: The rank of GPU running the transformer kernel, it is not required\n                to use if the model already set the current device, otherwise need to set it\n                so that the transformer kernel can work on the right device\n            mp_size (optional): This argument is mainly used to create the parameters on the kernel side\n                using model-parallel architecture. If the client model already takes care of this, there is no\n                need to pass this argument.\n            pre_layer_norm: Select between Pre-LN or Post-LN transformer architecture\n            stochastic_mode:  Enable for high performance, please note that this flag has some level of\n                non-determinism and can produce different results on different runs.  However, we have seen\n                that by enabling it, the pretraining tasks such as BERT are not affected and can obtain\n                a high accuracy level. On the other hand, for the downstream tasks, such as fine-tuning, we recommend\n                to turn it off in order to be able to reproduce the same result through the regular kernel execution.\n\n            scale_attention: If true, both q and k are scaled by 1/sqrt(attention_heads) before attention computation.\n            return_tuple: if True, returns the transformer output as a tuple, otherwise returns as a tensor\n            bigscience_bloom: This flag is added temporarily for supporting the BLOOM-176B model architecture.\n            use_triton: This flag is to enable triton kernels in inference or not.\n            invert_mask: If True, the attention mask is inverted when passed to attention block.\n    \"\"\"\n\n    def __init__(self,\n                 hidden_size=-1,\n                 intermediate_size=-1,\n                 heads=-1,\n                 num_hidden_layers=-1,\n                 layer_norm_eps=1e-12,\n                 local_rank=-1,\n                 mp_size=1,\n                 dtype=torch.float16,\n                 pre_layer_norm=True,\n                 norm_type=NormType.LayerNorm,\n                 stochastic_mode=False,\n                 scale_attention=True,\n                 triangular_masking=True,\n                 local_attention=False,\n                 window_size=256,\n                 rotary_dim=-1,\n                 rotate_half=False,\n                 rotate_every_two=True,\n                 return_tuple=True,\n                 mlp_after_attn=True,\n                 mlp_act_func_type=ActivationFuncType.GELU,\n                 training_mp_size=1,\n                 bigscience_bloom=False,\n                 max_out_tokens=1024,\n                 min_out_tokens=1,\n                 enable_qkv_quantization=False,\n                 use_mup=False,\n                 scale_attn_by_inverse_layer_idx=False,\n                 return_single_tuple=False,\n                 set_empty_params=False,\n                 transposed_mode=False,\n                 use_triton=False,\n                 triton_autotune=False,\n                 num_kv=-1,\n                 rope_theta=10000,\n                 invert_mask=True):\n        super(DeepSpeedInferenceConfig,\n              self).__init__(hidden_size, (intermediate_size if intermediate_size > 0 else 4 * hidden_size), heads,\n                             num_hidden_layers)\n        self.dtype = dtype\n        self.pre_layer_norm = pre_layer_norm\n        self.norm_type = norm_type\n        self.local_rank = local_rank\n        self.stochastic_mode = stochastic_mode\n        self.epsilon = layer_norm_eps\n        self.mp_size = mp_size\n        self.scale_attention = scale_attention\n        self.triangular_masking = triangular_masking\n        self.local_attention = local_attention\n        self.window_size = window_size\n        self.rotary_dim = rotary_dim\n        self.rotate_half = rotate_half\n        self.rotate_every_two = rotate_every_two\n        self.return_tuple = return_tuple\n        self.mlp_after_attn = mlp_after_attn\n        self.mlp_act_func_type = mlp_act_func_type\n        self.specialized_mode = False\n        self.training_mp_size = training_mp_size\n        self.bigscience_bloom = bigscience_bloom\n        self.max_out_tokens = max_out_tokens\n        self.min_out_tokens = min_out_tokens\n        self.scale_attn_by_inverse_layer_idx = scale_attn_by_inverse_layer_idx\n        self.enable_qkv_quantization = enable_qkv_quantization\n        self.use_mup = use_mup\n        self.return_single_tuple = return_single_tuple\n        self.set_empty_params = set_empty_params\n        self.transposed_mode = transposed_mode\n        self.use_triton = use_triton\n        self.triton_autotune = triton_autotune\n        self.num_kv = num_kv\n        self.rope_theta = rope_theta\n        self.invert_mask = invert_mask\n\n    @classmethod\n    def from_dict(cls, json_object):\n        config = DeepSpeedInferenceConfig()\n        for key, value in json_object.items():\n            config.__dict__[key] = value\n        return config\n\n    @classmethod\n    def from_json_file(cls, json_file):\n        with open(json_file, \"r\", encoding='utf-8') as reader:\n            text = reader.read()\n        return cls.from_dict(json.loads(text))\n", "deepspeed/ops/transformer/inference/ds_attention.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport math\nimport torch\nimport torch.nn as nn\nfrom deepspeed import comm as dist\nfrom deepspeed.accelerator import get_accelerator\nfrom .op_binding import LinearOp, VectorMatMulOp, SoftmaxContextOp, QKVGemmOp, SoftmaxOp\n\nminus_inf = -10000.0\n\n\nclass DeepSpeedSelfAttention(nn.Module):\n    num_layers = 0\n    _qkv_buffers = []\n\n    def __init__(self, config, mp_group=None, q_scales=None, q_groups=1, merge_count=1):\n        super(DeepSpeedSelfAttention, self).__init__()\n        self.config = config\n        data_type = self.config.dtype\n        data_type_fp = torch.half if self.config.dtype == torch.int8 else self.config.dtype\n        self.config.layer_id = DeepSpeedSelfAttention.num_layers\n        DeepSpeedSelfAttention.num_layers = DeepSpeedSelfAttention.num_layers + 1\n        device = get_accelerator().current_device_name()  #if config.bigscience_bloom else 'cpu'\n        if self.config.set_empty_params:\n            self.attn_qw = None\n            self.attn_qb = None\n            self.attn_kw = None\n            self.attn_kb = None\n            self.attn_vw = None\n            self.attn_vb = None\n            self.attn_qkvw = None\n            self.attn_qkvb = None\n            self.attn_ow = None\n            self.attn_ob = None\n        else:\n            qkv_size_per_partition = (self.config.hidden_size // self.config.mp_size) * 3 if config.num_kv < 0 else \\\n                                     ((self.config.heads + self.config.num_kv * 2) // self.config.mp_size) * (self.config.hidden_size // self.config.heads)\n            self.attn_qkvw = nn.Parameter(torch.empty(self.config.hidden_size,\n                                                      qkv_size_per_partition,\n                                                      dtype=data_type,\n                                                      device=device),\n                                          requires_grad=False)\n            self.attn_qkvb = nn.Parameter(torch.empty(qkv_size_per_partition, dtype=data_type_fp, device=device),\n                                          requires_grad=False)\n            out_size_per_partition = self.config.hidden_size // self.config.mp_size\n            self.attn_ow = nn.Parameter(torch.empty(out_size_per_partition,\n                                                    self.config.hidden_size,\n                                                    dtype=data_type,\n                                                    device=device),\n                                        requires_grad=False)\n\n            self.attn_ob = nn.Parameter(torch.empty(self.config.hidden_size, dtype=data_type_fp, device=device),\n                                        requires_grad=False)\n\n        self.num_attention_heads_per_partition = self.config.heads // self.config.mp_size\n        self.num_kv_partition = self.config.num_kv // self.config.mp_size\n        self.hidden_size_per_partition = self.config.hidden_size // self.config.mp_size\n        self.hidden_size_per_attention_head = self.config.hidden_size // self.config.heads\n\n        self.mp_group = mp_group\n\n        # used for quantization\n        self.q_scales = q_scales\n        self.q_groups = q_groups\n        self.merge_count = int(math.log2(merge_count))\n\n        self.norm_factor = math.sqrt(self.config.hidden_size // self.config.heads)\n        if not config.use_mup:\n            self.norm_factor = math.sqrt(self.norm_factor)\n\n        if self.config.scale_attn_by_inverse_layer_idx is True:\n            self.norm_factor *= math.sqrt(self.config.layer_id + 1)\n            # https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/gpt2/modeling_gpt2.py#L191\n\n        self.qkv_func = QKVGemmOp(config)\n        self.score_context_func = SoftmaxContextOp(config)\n        self.linear_func = LinearOp(config)\n        self.vector_matmul_func = VectorMatMulOp(config)\n        if len(DeepSpeedSelfAttention._qkv_buffers) == 0:\n            DeepSpeedSelfAttention._qkv_buffers = [\n                torch.empty(self.hidden_size_per_partition * 3,\n                            self.config.hidden_size,\n                            dtype=data_type_fp,\n                            device=device),\n                torch.empty(self.hidden_size_per_partition * 3, dtype=data_type_fp, device=device)\n            ]\n\n    def compute_attention(self, qkv_out, input_mask, layer_past, alibi):\n        if isinstance(qkv_out, list) or isinstance(qkv_out, tuple):\n            qkv_out = qkv_out[0]\n\n        no_masking = input_mask is None or input_mask is False\n\n        if no_masking:\n            input_mask = torch.empty(1)\n\n        attn_key_value = self.score_context_func(\n            query_key_value=qkv_out,\n            attn_mask=((1 - input_mask).to(qkv_out.dtype) *\n                       minus_inf) if input_mask.dtype == torch.int64 else input_mask,\n            heads=self.num_attention_heads_per_partition,\n            num_kv=self.num_kv_partition,\n            norm_factor=(1 / self.norm_factor if self.config.scale_attention else 1.0),\n            no_masking=no_masking,\n            layer_id=self.config.layer_id,\n            num_layers=DeepSpeedSelfAttention.num_layers,\n            alibi=alibi)\n\n        context_layer, key_layer, value_layer = attn_key_value\n        return context_layer, key_layer, value_layer\n\n    def _merge_qkv(self):\n        qvkw = DeepSpeedSelfAttention._qkv_buffers[0]\n        qvkw[:self.hidden_size_per_partition, :] = self.attn_qw  # type: ignore\n        qvkw[self.hidden_size_per_partition:2 * self.hidden_size_per_partition, :] = self.attn_kw  # type: ignore\n        qvkw[2 * self.hidden_size_per_partition:, :] = self.attn_vw  # type: ignore\n        if self.attn_qb is not None:\n            qvkb = DeepSpeedSelfAttention._qkv_buffers[1]\n            qvkb[:self.hidden_size_per_partition] = self.attn_qb\n            qvkb[self.hidden_size_per_partition:2 * self.hidden_size_per_partition] = self.attn_kb  # type: ignore\n            qvkb[2 * self.hidden_size_per_partition:] = self.attn_vb  # type: ignore\n        return DeepSpeedSelfAttention._qkv_buffers\n\n    def forward(self,\n                input,\n                input_mask,\n                head_mask=None,\n                layer_past=None,\n                get_present=False,\n                encoder_hidden_states=None,\n                encoder_attention_mask=None,\n                output_attentions=False,\n                norm_w=None,\n                norm_b=None,\n                alibi=None):\n        if self.attn_qkvw is None:\n            self._attn_qkvw, self._attn_qkvb = self._merge_qkv()\n        else:\n            self._attn_qkvw = self.attn_qkvw\n            self._attn_qkvb = self.attn_qkvb\n        if not self.config.pre_layer_norm:\n            qkv_out = self.linear_func(input=input,\n                                       weight=self._attn_qkvw,\n                                       bias=self._attn_qkvb,\n                                       add_bias=self.attn_qkvb is not None,\n                                       do_flash_attn=False,\n                                       num_heads=self.num_attention_heads_per_partition,\n                                       num_layers=DeepSpeedSelfAttention.num_layers)\n        else:\n            qkv_out = self.qkv_func(input=input,\n                                    weight=self._attn_qkvw,\n                                    bias=self._attn_qkvb,\n                                    gamma=norm_w,\n                                    beta=norm_b)\n\n        context_layer, key_layer, value_layer = self.compute_attention(qkv_out=qkv_out,\n                                                                       input_mask=input_mask,\n                                                                       layer_past=layer_past,\n                                                                       alibi=alibi)\n\n        output = self.vector_matmul_func(input=context_layer, weight=self.attn_ow)\n        inp_norm = qkv_out[-1]\n\n        if self.config.mlp_after_attn and self.mp_group is not None and dist.get_world_size(group=self.mp_group) > 1:\n            dist.all_reduce(output, group=self.mp_group)\n        return (output, key_layer, value_layer, context_layer, inp_norm)\n\n\nclass BloomSelfAttention(DeepSpeedSelfAttention):\n\n    def __init__(self, *args, **kwargs):\n        super(BloomSelfAttention, self).__init__(*args, **kwargs)\n        self.softmax_func = SoftmaxOp(self.config)\n\n    ########### This part is taken/modified form the HF modeling_bloom.py ################\n    # Reference: https://github.com/huggingface/transformers/blob/main/src/transformers/models/bloom/modeling_bloom.py\n\n    def _transpose_for_context(self, x):\n        x = x.permute(0, 2, 1, 3).contiguous()\n        new_x_layer_shape = x.size()[:-2] + \\\n                                    (self.hidden_size_per_partition,)\n        return x.view(*new_x_layer_shape).contiguous()\n\n    def _split_tensor_along_last_dim(self, tensor, num_partitions, contiguous_split_chunks=True):\n        \"\"\"Split a tensor along its last dimension.\n\n        Args:\n            tensor: ([`torch.tensor`], *required*):\n                input tensor to split\n            num_partitions ([`int`], *required*):\n                number of partitions to split the tensor\n            contiguous_split_chunks ([`bool`], *optional*, default=`False`)::\n                If True, make each chunk contiguous in memory.\n        \"\"\"\n        # Get the size and dimension.\n        last_dim = tensor.dim() - 1\n        numerator, denominator = tensor.size()[last_dim], num_partitions\n        if not (numerator % denominator == 0):\n            raise ValueError(f\"{numerator} is not divisible by {denominator}\")\n        last_dim_size = numerator // denominator\n        # Split.\n        tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)\n        # Note: torch.split does not create contiguous tensors by default.\n        if contiguous_split_chunks:\n            return tuple(chunk.contiguous() for chunk in tensor_list)\n\n        return tensor_list\n\n    def compute_attention(self, qkv_out, input_mask, layer_past, alibi):\n        if isinstance(qkv_out, list) or isinstance(qkv_out, tuple):\n            qkv_out = qkv_out[0]\n\n        no_masking = input_mask is None\n\n        if no_masking:\n            input_mask = torch.empty(1)\n\n        mixed_x_layer = qkv_out\n        alibi = alibi.to(get_accelerator().current_device_name())\n        head_dim = self.hidden_size_per_partition // self.num_attention_heads_per_partition\n        new_tensor_shape = mixed_x_layer.size()[:-1] + (self.num_attention_heads_per_partition, 3 * head_dim)\n        mixed_x_layer = mixed_x_layer.view(*new_tensor_shape)\n\n        query_layer, key_layer, value_layer = self._split_tensor_along_last_dim(mixed_x_layer, 3)\n\n        # [batch_size, head_dim, q_length, k_length]\n        output_size = (query_layer.size(0), query_layer.size(2), query_layer.size(1), key_layer.size(1))\n        # [batch_size, q_length, num_heads, head_dim] -> [q_length, batch_size * num_heads, head_dim]\n        query_layer = query_layer.transpose(1, 2).reshape(output_size[0] * output_size[1], output_size[2], -1)\n        # [batch_size, k_length, num_heads, head_dim] -> [k_length, batch_size * num_heads, head_dim]\n        key_layer = key_layer.transpose(1, 2).reshape(output_size[0] * output_size[1], output_size[3],\n                                                      -1).transpose(-1, -2)\n        value_layer = value_layer.transpose(1, 2).reshape(output_size[0] * output_size[1], output_size[3], -1)\n        if layer_past is not None:\n            past_key, past_value = layer_past\n            # concatenate along seq_length dimension -> [batch_size, qk_length, num_heads, head_dim]\n            key_layer = torch.cat((past_key.type_as(key_layer), key_layer), dim=-1)\n            value_layer = torch.cat((past_value.type_as(value_layer), value_layer), dim=-2)\n\n        presents = (key_layer, value_layer)\n        # Raw attention scores. [batch_size * num_heads, q_length, k_length]\n        matmul_result = torch.matmul(query_layer, key_layer)\n        # change view to [batch_size, num_heads, q_length, k_length]\n        attention_scores = matmul_result.view(output_size[0], output_size[1], output_size[2], -1)\n\n        offset = dist.get_rank() * self.num_attention_heads_per_partition if dist.is_initialized() else 0\n        target_dtype = torch.float16 if self.config.dtype == torch.int8 else self.config.dtype\n\n        # When using the hybrid engine with BLOOM, input_mask needs to be converted from torch.bool -> torch.int64\n        if input_mask.dtype == torch.bool:\n            input_mask = input_mask.long()\n\n        # Invert input_mask per transformer implementation (eg, in BLOOM, it's already inverted)\n        if self.config.invert_mask:\n            input_mask = 1 - input_mask\n\n        attention_probs = self.softmax_func(attn_scores=attention_scores,\n                                            attn_mask=input_mask.to(target_dtype) * minus_inf,\n                                            alibi=alibi,\n                                            triangular=(self.config.triangular_masking\n                                                        and (attention_scores.shape[-2] > 1)),\n                                            recompute=False,\n                                            local_attention=False,\n                                            window_size=1,\n                                            async_op=False,\n                                            layer_scale=1 / (self.norm_factor * self.norm_factor),\n                                            head_offset=offset)\n\n        # change view [batch_size x num_heads, q_length, k_length]\n        attention_probs_reshaped = attention_probs.view(*matmul_result.shape)\n\n        # matmul: [batch_size * num_heads, q_length, head_dim]\n        context_layer = torch.bmm(attention_probs_reshaped, value_layer)\n\n        # change view [batch_size, num_heads, q_length, head_dim]\n        context_layer = context_layer.view(\n            context_layer.size(0) // self.num_attention_heads_per_partition, self.num_attention_heads_per_partition,\n            context_layer.size(1), context_layer.shape[-1])\n\n        context_layer = self._transpose_for_context(context_layer)\n        key_layer = presents[0]\n        value_layer = presents[1]\n\n        return context_layer, key_layer, value_layer\n\n    ###################### End of HF modeling_bloom addition ########################\n", "deepspeed/ops/transformer/inference/triton_ops.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\"\"\"\nInspired by original Triton implementation:\nhttps://github.com/openai/triton/blob/release/2.1.x/python/tutorials/06-fused-attention.py\n\"\"\"\n\nimport torch\nimport triton\nimport triton.language as tl\n\n\n@triton.jit\ndef _fwd_kernel(\n    Q,\n    K,\n    V,\n    sm_scale,\n    Out,\n    stride_qz,\n    stride_qh,\n    stride_qm,\n    stride_qk,\n    stride_kz,\n    stride_kh,\n    stride_kn,\n    stride_kk,\n    stride_vz,\n    stride_vh,\n    stride_vk,\n    stride_vn,\n    stride_oz,\n    stride_oh,\n    stride_om,\n    stride_on,\n    Z,\n    H,\n    N_CTX,\n    BLOCK_M: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    qvk_offset = off_hz * stride_qh\n    Q_block_ptr = tl.make_block_ptr(base=Q + qvk_offset,\n                                    shape=(N_CTX, BLOCK_DMODEL),\n                                    strides=(stride_qm, stride_qk),\n                                    offsets=(start_m * BLOCK_M, 0),\n                                    block_shape=(BLOCK_M, BLOCK_DMODEL),\n                                    order=(1, 0))\n    K_block_ptr = tl.make_block_ptr(base=K + qvk_offset,\n                                    shape=(BLOCK_DMODEL, N_CTX),\n                                    strides=(stride_kk, stride_kn),\n                                    offsets=(0, 0),\n                                    block_shape=(BLOCK_DMODEL, BLOCK_N),\n                                    order=(0, 1))\n    V_block_ptr = tl.make_block_ptr(base=V + qvk_offset,\n                                    shape=(N_CTX, BLOCK_DMODEL),\n                                    strides=(stride_vk, stride_vn),\n                                    offsets=(0, 0),\n                                    block_shape=(BLOCK_N, BLOCK_DMODEL),\n                                    order=(1, 0))\n    # initialize offsets\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    # initialize pointer to m and l\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    # scale sm_scale by log_2(e) and use\n    # 2^x instead of exp in the loop because CSE and LICM\n    # don't work as expected with `exp` in the loop\n    qk_scale = sm_scale * 1.44269504\n    # load q: it will stay in SRAM throughout\n    q = tl.load(Q_block_ptr)\n    q = (q * qk_scale).to(tl.float16)\n    # loop over k, v and update accumulator\n    lo = 0\n    #hi = (start_m + 1) * BLOCK_M if IS_CAUSAL else N_CTX\n    hi = N_CTX\n    #hi = (start_m + 1) * BLOCK_M\n    for start_n in range(lo, hi, BLOCK_N):\n        # -- load k, v --\n        k = tl.load(K_block_ptr)\n        v = tl.load(V_block_ptr)\n        # -- compute qk ---\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        #if IS_CAUSAL:\n        #qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n        qk += tl.dot(q, k)\n        # -- compute scaling constant ---\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n        alpha = tl.math.exp2(m_i - m_i_new)\n        p = tl.math.exp2(qk - m_i_new[:, None])\n        # -- scale and update acc --\n        acc_scale = l_i * 0 + alpha  # workaround some compiler bug\n        acc *= acc_scale[:, None]\n        acc += tl.dot(p.to(tl.float16), v)\n        # -- update m_i and l_i --\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n        # update pointers\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n    # write back l and m\n    acc = acc / l_i[:, None]\n    #l_ptrs = L + off_hz * N_CTX + offs_m\n    #tl.store(l_ptrs, m_i + tl.math.log2(l_i))\n    # write back O\n    O_block_ptr = tl.make_block_ptr(base=Out + qvk_offset,\n                                    shape=(N_CTX, BLOCK_DMODEL),\n                                    strides=(stride_om, stride_on),\n                                    offsets=(start_m * BLOCK_M, 0),\n                                    block_shape=(BLOCK_M, BLOCK_DMODEL),\n                                    order=(1, 0))\n    tl.store(O_block_ptr, acc.to(tl.float16))\n\n\nclass triton_flash_attn(torch.nn.Module):\n\n    def __init__(self, ):\n        super(triton_flash_attn, self).__init__()\n\n    def forward(self, q, k, v, sm_scale, block_128=True):\n        BLOCK = 128 if block_128 else 64\n        # shape constraints\n        Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n        o = torch.empty_like(q)\n        grid = (triton.cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1])\n        num_warps = 4 if Lk <= 64 else 8\n\n        _fwd_kernel[grid](\n            q,\n            k,\n            v,\n            sm_scale,\n            o,\n            q.stride(0),\n            q.stride(1),\n            q.stride(2),\n            q.stride(3),\n            k.stride(0),\n            k.stride(1),\n            k.stride(2),\n            k.stride(3),\n            v.stride(0),\n            v.stride(1),\n            v.stride(2),\n            v.stride(3),\n            o.stride(0),\n            o.stride(1),\n            o.stride(2),\n            o.stride(3),\n            k.shape[0],\n            k.shape[1],\n            k.shape[2],\n            BLOCK_M=BLOCK,\n            BLOCK_N=BLOCK,\n            BLOCK_DMODEL=Lk,\n            num_warps=num_warps,\n            num_stages=1,\n        )\n        return o\n", "deepspeed/ops/transformer/inference/diffusers_2d_transformer.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\n\nclass Diffusers2DTransformerConfig():\n\n    def __init__(self, int8_quantization=False):\n        self.int8_quantization = int8_quantization\n", "deepspeed/ops/transformer/inference/diffusers_transformer_block.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nimport torch.nn as nn\n\nfrom deepspeed import module_inject\nfrom .diffusers_attention import DeepSpeedDiffusersAttention\nfrom .bias_add import nhwc_bias_add\nfrom .diffusers_2d_transformer import Diffusers2DTransformerConfig\nfrom deepspeed.ops.op_builder import InferenceBuilder, SpatialInferenceBuilder\nfrom deepspeed.utils.types import ActivationFuncType\n\n# Ops will be loaded on demand\ntransformer_cuda_module = None\nspatial_cuda_module = None\n\n\ndef load_transformer_module():\n    global transformer_cuda_module\n    if transformer_cuda_module is None:\n        transformer_cuda_module = InferenceBuilder().load()\n    return transformer_cuda_module\n\n\ndef load_spatial_module():\n    global spatial_cuda_module\n    if spatial_cuda_module is None:\n        spatial_cuda_module = SpatialInferenceBuilder().load()\n    return spatial_cuda_module\n\n\nclass DeepSpeedDiffusersTransformerBlock(nn.Module):\n\n    def __init__(self, equivalent_module: nn.Module, config: Diffusers2DTransformerConfig):\n        super(DeepSpeedDiffusersTransformerBlock, self).__init__()\n        self.quantizer = module_inject.GroupQuantizer(q_int8=config.int8_quantization)\n        # Ensure ops are built by the time we start running\n        self.config = config\n\n        self.ff1_w = self.quantizer.quantize(\n            nn.Parameter(equivalent_module.ff.net[0].proj.weight.data, requires_grad=False))\n        self.ff1_b = nn.Parameter(equivalent_module.ff.net[0].proj.bias.data, requires_grad=False)\n        self.ff2_w = self.quantizer.quantize(nn.Parameter(equivalent_module.ff.net[2].weight.data,\n                                                          requires_grad=False))\n        self.ff2_b = nn.Parameter(equivalent_module.ff.net[2].bias.data, requires_grad=False)\n\n        self.norm1_g = nn.Parameter(equivalent_module.norm1.weight.data, requires_grad=False)\n        self.norm1_b = nn.Parameter(equivalent_module.norm1.bias.data, requires_grad=False)\n        self.norm1_eps = equivalent_module.norm1.eps\n\n        self.norm2_g = nn.Parameter(equivalent_module.norm2.weight.data, requires_grad=False)\n        self.norm2_b = nn.Parameter(equivalent_module.norm2.bias.data, requires_grad=False)\n        self.norm2_eps = equivalent_module.norm2.eps\n\n        self.norm3_g = nn.Parameter(equivalent_module.norm3.weight.data, requires_grad=False)\n        self.norm3_b = nn.Parameter(equivalent_module.norm3.bias.data, requires_grad=False)\n        self.norm3_eps = equivalent_module.norm3.eps\n\n        self.attn_1 = equivalent_module.attn1\n        self.attn_2 = equivalent_module.attn2\n\n        # Pull the bias in if we can\n        if isinstance(self.attn_1, DeepSpeedDiffusersAttention):\n            self.attn_1.do_out_bias = False\n            self.attn_1_bias = self.attn_1.attn_ob\n        else:\n            self.attn_1_bias = nn.Parameter(torch.zeros_like(self.norm2_g), requires_grad=False)\n\n        # Pull the bias in if we can\n        if isinstance(self.attn_2, DeepSpeedDiffusersAttention):\n            self.attn_2.do_out_bias = False\n            self.attn_2_bias = self.attn_2.attn_ob\n        else:\n            self.attn_2_bias = nn.Paramaeter(torch.zeros_like(self.norm3_g), requires_grad=False)\n\n        self.transformer_cuda_module = load_transformer_module()\n        load_spatial_module()\n\n    def forward(self, hidden_states, context=None, timestep=None, **kwargs):\n        # In v0.12.0 of diffuser, several new kwargs were added. Capturing\n        # those with kwargs to maintain backward compatibility\n\n        # In v0.11.0 of diffusers, the kwarg was changed from 'context' to 'encoder_hidden_states'\n        # This is so we can support older and newer versions of diffusers\n        if \"encoder_hidden_states\" in kwargs and kwargs[\"encoder_hidden_states\"] is not None:\n            context = kwargs[\"encoder_hidden_states\"]\n\n        out_norm_1 = self.transformer_cuda_module.layer_norm(hidden_states, self.norm1_g, self.norm1_b, self.norm1_eps)\n        out_attn_1 = self.attn_1(out_norm_1)\n\n        out_norm_2, out_attn_1 = self.transformer_cuda_module.layer_norm_residual_store_pre_ln_res(\n            out_attn_1, self.attn_1_bias, hidden_states, self.norm2_g, self.norm2_b, self.norm2_eps)\n        out_attn_2 = self.attn_2(out_norm_2, context=context)\n        out_norm_3, out_attn_2 = self.transformer_cuda_module.layer_norm_residual_store_pre_ln_res(\n            out_attn_2, self.attn_2_bias, out_attn_1, self.norm3_g, self.norm3_b, self.norm3_eps)\n\n        out_ff1 = nn.functional.linear(out_norm_3, self.ff1_w)\n        out_geglu = self.transformer_cuda_module.gated_activation(out_ff1, self.ff1_b, ActivationFuncType.GATED_GELU)\n\n        out_ff2 = nn.functional.linear(out_geglu, self.ff2_w)\n        return nhwc_bias_add(out_ff2, self.ff2_b, other=out_attn_2)\n", "deepspeed/ops/transformer/inference/ds_mlp.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport math\nimport torch\nimport torch.nn as nn\nfrom deepspeed import comm as dist\nfrom deepspeed.utils.types import GATED_ACTIVATION_TYPES\nfrom deepspeed.accelerator import get_accelerator\nfrom .op_binding import MLPGemmOp, VectorMatMulOp, GELUGemmOp, ResidualAddOp\n\n\nclass DeepSpeedMLP(nn.Module):\n    _inter_w_buffers = []\n\n    def __init__(self, config, mp_group=None, q_scales=None, q_groups=1, merge_count=1, mlp_extra_grouping=False):\n        super(DeepSpeedMLP, self).__init__()\n\n        self.config = config\n\n        data_type = torch.int8 if self.config.dtype == torch.int8 else self.config.dtype\n        data_type_fp = torch.half if self.config.dtype == torch.int8 else self.config.dtype\n        device = get_accelerator().current_device_name()\n\n        proj_factor = 2 if self.config.mlp_act_func_type in GATED_ACTIVATION_TYPES else 1\n        self.config.intermediate_size = self.config.intermediate_size if self.config.intermediate_size > 0 else 4 * self.config.hidden_size\n        self.intm_w_sz_per_partition = self.config.intermediate_size * proj_factor // self.config.mp_size\n        self.intm_o_sz_per_partition = self.config.intermediate_size // self.config.mp_size\n\n        if self.config.set_empty_params:\n            self.attn_nw = None\n            self.attn_nb = None\n            self.inter_w = None\n            self.inter_b = None\n            self.inter_up_w = None\n            self.inter_up_b = None\n            self.inter_gate_w = None\n            self.inter_gate_b = None\n            self.output_w = None\n            self.output_b = None\n        else:\n            self.attn_nw = nn.Parameter(torch.empty(self.config.hidden_size, dtype=data_type_fp, device=device),\n                                        requires_grad=False)\n            self.attn_nb = nn.Parameter(torch.empty(self.config.hidden_size, dtype=data_type_fp, device=device),\n                                        requires_grad=False)\n\n            self.inter_w = nn.Parameter(torch.empty(self.config.hidden_size,\n                                                    self.intm_w_sz_per_partition,\n                                                    dtype=data_type,\n                                                    device=device),\n                                        requires_grad=False)\n            self.inter_b = nn.Parameter(torch.empty(self.intm_w_sz_per_partition, dtype=data_type_fp, device=device),\n                                        requires_grad=False)\n            self.output_w = nn.Parameter(torch.empty(self.intm_o_sz_per_partition,\n                                                     self.config.hidden_size,\n                                                     dtype=data_type,\n                                                     device=device),\n                                         requires_grad=False)\n            self.output_b = nn.Parameter(torch.empty(self.config.hidden_size, dtype=data_type_fp, device=device),\n                                         requires_grad=False)\n\n        # used for quantization\n        self.q_scales = q_scales\n        self.q_groups = q_groups * 2 if mlp_extra_grouping else q_groups\n        self.merge_count = int(math.log2(merge_count))\n        self.mp_group = mp_group\n\n        self.mlp_gemm_func = MLPGemmOp(config)\n        self.vector_matmul_func = VectorMatMulOp(config)\n        self.fused_gemm_gelu = GELUGemmOp(config)\n        self.residual_add_func = ResidualAddOp(config)\n\n        if len(DeepSpeedMLP._inter_w_buffers) == 0:\n            DeepSpeedMLP._inter_w_buffers = [\n                torch.empty(self.intm_w_sz_per_partition, self.config.hidden_size, dtype=data_type, device=device),\n                torch.empty(self.intm_w_sz_per_partition, dtype=data_type_fp, device=device)\n            ]\n\n    def _merge_inter_w(self):\n        inter_w = DeepSpeedMLP._inter_w_buffers[0]\n        inter_w[:self.intm_w_sz_per_partition // 2, :] = self.inter_up_w  # type: ignore\n        inter_w[self.intm_w_sz_per_partition // 2:, :] = self.inter_gate_w  # type: ignore\n        if self.inter_up_b is not None:\n            inter_b = DeepSpeedMLP._inter_w_buffers[1]\n            inter_b[:self.intm_w_sz_per_partition // 2] = self.inter_up_b  # type: ignore\n            inter_b[self.intm_w_sz_per_partition // 2:] = self.inter_gate_b  # type: ignore\n        return DeepSpeedMLP._inter_w_buffers\n\n    def forward(self, input, residual, residual_norm, bias):\n        if self.inter_w is None:\n            self._inter_w, self._inter_b = self._merge_inter_w()\n        else:\n            self._inter_w = self.inter_w\n            self._inter_b = self.inter_b\n\n        residual_add = None\n        if self.attn_nw is None:\n            output = self.fused_gemm_gelu(input=residual_norm,\n                                          weight=self._inter_w,\n                                          bias=self._inter_b,\n                                          weight_out=self.output_w)\n        else:\n            output, residual_add = self.mlp_gemm_func(input=input,\n                                                      residual=residual,\n                                                      weight_interm=self._inter_w,\n                                                      weight_out=self.output_w,\n                                                      input_bias=bias,\n                                                      bias=self._inter_b,\n                                                      gamma=self.attn_nw,\n                                                      beta=self.attn_nb)\n\n        residual = self.residual_add_func(hidden_state=output,\n                                          residual=residual,\n                                          add_bias=bias is not None,\n                                          attention_output=input,\n                                          attention_bias=bias if bias is not None else self.output_b,\n                                          final_bias=self.output_b,\n                                          residual_add=residual_add)\n        if self.mp_group is not None and dist.get_world_size(group=self.mp_group) > 1:\n            dist.all_reduce(residual, group=self.mp_group)\n\n        return residual\n", "deepspeed/ops/transformer/inference/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .config import DeepSpeedInferenceConfig\nfrom ....model_implementations.transformers.ds_transformer import DeepSpeedTransformerInference\nfrom .moe_inference import DeepSpeedMoEInferenceConfig, DeepSpeedMoEInference\n", "deepspeed/ops/transformer/inference/moe_inference.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport json\nimport math\nimport torch\nfrom torch.autograd import Function\n# accelerator modules will be imported if needed\ninference_module = None\nspecialized_mode = None\nimport torch.nn as nn\nfrom .ds_attention import DeepSpeedSelfAttention\nfrom .config import DeepSpeedInferenceConfig\nfrom ....moe.sharded_moe import TopKGate\nfrom deepspeed import comm as dist\nfrom deepspeed.accelerator import get_accelerator\nfrom deepspeed.ops.op_builder import InferenceBuilder\n\n\nclass DeepSpeedMoEInferenceConfig(DeepSpeedInferenceConfig):\n    \"\"\"Initialize the DeepSpeed Transformer Config.\n        Arguments:\n            hidden_size: The hidden size of the transformer layer\n            intermediate_size: The intermediate size of the feed-forward part of transformer layer\n            heads: The number of heads in the self-attention of the transformer layer\n            num_hidden_layers: The number of transformer layers\n            layer_norm_eps: The epsilon value for the layer norm\n            local_rank: Optional: The rank of GPU running the transformer kernel, it is not required\n                to use if the model already set the current device, otherwise need to set it\n                so that the transformer kernel can work on the right device\n            mp_size (optional): This argument is mainly used to create the parameters on the kernel side\n                using model-parallel architecture. If the client model already takes care of this, there is no\n                need to pass this argument.\n            fp16: Enable half-precision computation\n            bf16: Enable bf16 floating point computation\n            pre_layer_norm: Select between Pre-LN or Post-LN transformer architecture\n            stochastic_mode:  Enable for high performance, please note that this flag has some level of\n                non-determinism and can produce different results on different runs.  However, we have seen\n                that by enabling it, the pretraining tasks such as BERT are not affected and can obtain\n                a high accuracy level. On the other hand, for the downstream tasks, such as fine-tuning, we recommend\n                to turn it off in order to be able to reproduce the same result through the regular kernel execution.\n\n            scale_attention: If true, both q and k are scaled by 1/sqrt(attention_heads) before attention computation.\n            return_tuple: if True, returns the transformer output as a tuple, otherwise returns as a tensor\n    \"\"\"\n\n    def __init__(self,\n                 hidden_size=-1,\n                 intermediate_size=-1,\n                 heads=-1,\n                 num_hidden_layers=-1,\n                 layer_norm_eps=1e-12,\n                 local_rank=-1,\n                 mp_size=1,\n                 fp16=False,\n                 bf16=False,\n                 q_int8=False,\n                 pre_layer_norm=True,\n                 stochastic_mode=False,\n                 scale_attention=True,\n                 triangular_masking=True,\n                 local_attention=False,\n                 window_size=256,\n                 return_tuple=True,\n                 moe_experts=1,\n                 global_experts=1,\n                 k=1,\n                 capacity_factor=1.,\n                 eval_capacity_factor=1.,\n                 min_capacity=1,\n                 noisy_gate_policy=None,\n                 drop_tokens=True,\n                 use_rts=False,\n                 mlp_type='standard',\n                 scale_attn_by_inverse_layer_idx=False):\n        super(DeepSpeedMoEInferenceConfig,\n              self).__init__(hidden_size, (intermediate_size if intermediate_size > 0 else 4 * hidden_size), heads,\n                             num_hidden_layers, layer_norm_eps, local_rank, mp_size, fp16, bf16, q_int8,\n                             pre_layer_norm, stochastic_mode, scale_attention, triangular_masking, local_attention,\n                             window_size, return_tuple)\n        self.moe_experts = moe_experts\n        self.k = k\n        self.capacity_factor = capacity_factor\n        self.eval_capacity_factor = eval_capacity_factor\n        self.min_capacity = min_capacity\n        self.noisy_gate_policy = noisy_gate_policy\n        self.drop_tokens = drop_tokens\n        self.use_rts = use_rts\n        self.global_experts = global_experts\n        self.mlp_type = mlp_type\n        self.scale_attn_by_inverse_layer_idx = scale_attn_by_inverse_layer_idx\n\n    @classmethod\n    def from_dict(cls, json_object):\n        config = DeepSpeedInferenceConfig()\n        for key, value in json_object.items():\n            config.__dict__[key] = value\n        return config\n\n    @classmethod\n    def from_json_file(cls, json_file):\n        with open(json_file, \"r\", encoding='utf-8') as reader:\n            text = reader.read()\n        return cls.from_dict(json.loads(text))\n\n\nclass DeepSpeedMLPFunction(Function):\n\n    @staticmethod\n    def forward(ctx, input, inter_w, inter_b, config, output_b, output_w, q_scales, q_groups, merge_count, mp_group,\n                async_op):\n        if config.q_int8:\n            intermediate = inference_module.fused_gemm_gelu_int8(input, inter_w, inter_b, config.epsilon, q_scales[2],\n                                                                 (q_groups * (2**merge_count)), config.pre_layer_norm)\n            output = inference_module.vector_matmul_int8(intermediate, output_w, q_scales[3], q_groups, (merge_count))\n        else:\n            mlp_gemm_func = inference_module.fused_gemm_gelu_fp16 if config.fp16 else \\\n                                    inference_module.fused_gemm_gelu_fp32\n\n            output = mlp_gemm_func(input, inter_w, inter_b, output_w, config.epsilon, config.pre_layer_norm, async_op)\n        if mp_group is not None and dist.get_world_size(group=mp_group) > 1:\n            dist.all_reduce(output, group=mp_group, async_op=async_op)\n\n        return output + output_b\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        raise RuntimeError('You are running with DeepSpeed Inference mode. \\\n                            Please switch to Training mode for running backward!')\n\n\nclass DeepSpeedMoEMLP(nn.Module):\n\n    def __init__(self, config, q_scales=None, q_groups=1, merge_count=1, mlp_extra_grouping=False, mp_group=None):\n        super(DeepSpeedMoEMLP, self).__init__()\n\n        self.config = config\n        self.attn_nw = nn.Parameter(torch.Tensor(self.config.hidden_size))\n        self.attn_nb = nn.Parameter(torch.Tensor(self.config.hidden_size))\n        interm_size = self.config.intermediate_size // (1 if mp_group is None else dist.get_world_size(group=mp_group))\n        self.inter_w = nn.Parameter(torch.Tensor(self.config.hidden_size, interm_size))\n        self.inter_b = nn.Parameter(torch.Tensor(interm_size))\n        self.output_w = nn.Parameter(torch.Tensor((interm_size), self.config.hidden_size))\n        self.output_b = nn.Parameter(torch.Tensor(self.config.hidden_size))\n\n        # used for quantization\n        self.q_scales = q_scales\n        self.q_groups = q_groups * 2 if mlp_extra_grouping else q_groups\n        self.merge_count = int(math.log2(merge_count))\n        self.mp_group = mp_group\n\n    def forward(self, input, async_op=False):\n        return DeepSpeedMLPFunction.apply(input, self.inter_w, self.inter_b, self.config, self.output_b, self.output_w,\n                                          self.q_scales, self.q_groups, self.merge_count, self.mp_group, async_op)\n\n\nclass DeepSpeedMoEInference(nn.Module):\n    \"\"\"Initialize the DeepSpeed MoE Transformer Layer.\n        Arguments:\n            layer_id: The layer index starting from 0, e.g. if model has 24 transformer layers,\n                layer_id will be 0,1,2...23 when each layer object is instantiated\n            config: An object of DeepSpeedInferenceConfig\n            mp_group: Model parallelism group initialized on the modeling side.\n            quantize_scales: This argument groups all the layers' scales used for quantization\n            quantize_groups: Number of groups used for quantizing the model\n            merge_count: Shows the number of model-parallel checkpoints merged before running inference.\n                We use this argument to control the quantization scale for the model parameters if a bigger\n                quantize-grouping than 1 is used.\n            mlp_extra_grouping: This flag is used to show a 2x higher number of groups used for the MLP part\n                of a Transformer layer. We use this feature for quantization to reduce the convergence impact\n                for specific downstream tasks.\n    \"\"\"\n    layer_id = 0\n\n    def __init__(self,\n                 config,\n                 mp_group=None,\n                 ep_group=None,\n                 expert_mp_group=None,\n                 quantize_scales=None,\n                 quantize_groups=1,\n                 merge_count=1,\n                 mlp_extra_grouping=False):\n        super(DeepSpeedMoEInference, self).__init__()\n\n        self.config = config\n        self.config.layer_id = DeepSpeedMoEInference.layer_id\n        global inference_module\n        global specialized_mode\n        if inference_module is None:\n            specialized_mode = False\n            # InferenceSpecializedBuilder is not among DeepSpeed provided builder yet, so we infer by builder name string\n            builder = get_accelerator().create_op_builder(\"InferenceSpecializedBuilder\")\n            if builder is not None and builder.is_compatible():\n                inference_module = builder.load()\n                specialized_mode = True\n            else:\n                inference_module = InferenceBuilder().load()\n        self.config.specialized_mode = specialized_mode\n        assert self.config.dtype != torch.bfloat16, \"DeepSpeed MoE Transformer Inference not yet tested for bfloat support\"\n\n        DeepSpeedMoEInference.layer_id += 1\n        self.attention = DeepSpeedSelfAttention(self.config, mp_group, quantize_scales, quantize_groups, merge_count)\n        self.attn_nw = nn.Parameter(torch.Tensor(self.config.hidden_size))\n        self.attn_nb = nn.Parameter(torch.Tensor(self.config.hidden_size))\n\n        self.norm_w = nn.Parameter(torch.Tensor(self.config.hidden_size))\n        self.norm_b = nn.Parameter(torch.Tensor(self.config.hidden_size))\n\n        if config.mlp_type == 'residual':\n            self.res_mlp = DeepSpeedMoEMLP(config, quantize_scales, quantize_groups, merge_count, mlp_extra_grouping,\n                                           mp_group)\n            self.res_coef = nn.Parameter(torch.Tensor(self.config.hidden_size, 2))\n            self.coef_func = inference_module.softmax_fp16 if self.config.dtype in [torch.float16, torch.int8] else \\\n                                        inference_module.softmax_fp32\n            self.vector_matmul_func = inference_module.vector_matmul_fp16 if self.config.dtype == torch.float16 else \\\n                                    inference_module.vector_matmul_fp32\n\n        config.mp_size = 1\n        self.mlp = nn.ModuleList(\n            DeepSpeedMoEMLP(config, quantize_scales, quantize_groups, merge_count, mlp_extra_grouping, expert_mp_group)\n            for i in range(self.config.moe_experts))\n\n        self.moe_gate = TopKGate(self.config.hidden_size, self.config.global_experts, self.config.k,\n                                 self.config.capacity_factor, self.config.eval_capacity_factor,\n                                 self.config.min_capacity, self.config.noisy_gate_policy, self.config.drop_tokens,\n                                 self.config.use_rts, self.ep_group)\n\n        self.ep_group = ep_group\n        self.mp_group = mp_group\n        self.expert_mp_group = expert_mp_group\n\n        print(\"DeepSpeed MoE Transformer Inference config is \", self.config.__dict__)\n\n        self.bias_residual_func = inference_module.bias_residual_fp16 if self.config.dtype in [torch.float16, torch.int8] else \\\n                                        inference_module.bias_residual_fp32\n        self.ds_layernorm = inference_module.layer_norm_fp16 if self.config.dtype in [torch.float16, torch.int8] else \\\n                                        inference_module.layer_norm_fp32\n        self.einsum_sec_sm_ecm = inference_module.einsum_sec_sm_ecm_fp16 if self.config.dtype in [torch.float16, torch.int8] else \\\n                                        inference_module.einsum_sec_sm_ecm_fp32\n\n    def res_coef_func(self, inp, async_op):\n        inp = self.vector_matmul_func(inp, self.res_coef, async_op)\n        return self.coef_func(inp, torch.empty(1), False, False, False, 256, async_op)\n\n    def moe_gate_einsum(self, attention_output):\n        _, combined_weights, dispatch_mask, _ = self.moe_gate(\n            attention_output.view(-1, self.config.hidden_size),\n            None,\n        )\n        dispatched_attention = self.einsum_sec_sm_ecm(dispatch_mask.type_as(attention_output),\n                                                      attention_output.view(-1, self.config.hidden_size))\n        return dispatched_attention, combined_weights\n\n    def expert_exec(self, dispatched_input):\n        dispatched_input = dispatched_input.reshape(self.config.global_experts // self.config.moe_experts,\n                                                    self.config.moe_experts, -1, self.config.hidden_size)\n\n        chunks = dispatched_input.chunk(self.config.moe_experts, dim=1)\n        expert_outputs = torch.empty((\n            self.config.moe_experts,\n            chunks[0].shape[0],\n        ) + chunks[0].shape[2:],\n                                     dtype=dispatched_input.dtype,\n                                     device=dispatched_input.device)\n        for chunk, expert in zip(chunks, range(len(self.mlp))):\n            expert_outputs[expert] = self.mlp[expert](chunk.view(-1, dispatched_input.shape[-2],\n                                                                 dispatched_input.shape[-1]))\n        return expert_outputs\n\n    def _alltoall(self, dispatched_attention):\n        if dist.get_world_size(group=self.ep_group) > 1:\n            dispatched_input = torch.empty_like(dispatched_attention)\n            dist.all_to_all_single(dispatched_input, dispatched_attention, group=self.ep_group)\n            return dispatched_input\n        else:\n            return dispatched_attention\n\n    def scale_expert_output(self, attention_output, expert_output, combined_weights):\n        combined_output = torch.matmul(\n            combined_weights.type_as(attention_output).reshape(combined_weights.shape[0], -1),\n            expert_output.reshape(-1, expert_output.shape[-1]))\n        return combined_output.reshape(attention_output.shape)\n\n    def forward(self,\n                input,\n                input_mask=None,\n                attention_mask=None,\n                head_mask=None,\n                layer_past=None,\n                get_key_value=False,\n                get_present=False,\n                encoder_output=None,\n                enc_dec_attn_mask=None,\n                encoder_hidden_states=None,\n                encoder_attention_mask=None,\n                use_cache=False,\n                output_attentions=False):\n        get_present = (get_present or get_key_value or use_cache)\n        input_mask = input_mask if attention_mask is None else attention_mask\n        input_type = input.dtype\n\n        if (self.config.dtype in [torch.float16, torch.int8]) and input_type == torch.float:\n            input = input.half()\n\n        with torch.no_grad():\n            attention_output = self.attention(input, input_mask, head_mask, layer_past, get_present,\n                                              encoder_hidden_states, encoder_attention_mask, output_attentions,\n                                              self.norm_w, self.norm_b)\n\n            if get_present:\n                attention_output, p_key, p_value = attention_output[0:3]\n                presents = (p_key, p_value)\n            elif output_attentions:\n                attention_output, _, _, context_output = attention_output[0:4]\n            else:\n                attention_output = attention_output[0]\n\n            residual_add = attention_output + self.attention.attn_ob\n            attention_output = self.ds_layernorm(residual_add, self.attn_nw, self.attn_nb, self.config.epsilon)\n\n            if self.config.mlp_type == 'residual':\n                res_mlp_out = self.res_mlp(attention_output, async_op=True)\n                res_coef_out = self.res_coef_func(attention_output, async_op=True)\n\n            if self.expert_mp_group is not None:\n                world_size = dist.get_world_size(group=self.expert_mp_group)\n                gather_buffer = torch.zeros(world_size * attention_output.numel(),\n                                            dtype=attention_output.dtype,\n                                            device=attention_output.device)\n                dist.all_gather_into_tensor(gather_buffer, attention_output, group=self.expert_mp_group)\n                attention_output = gather_buffer.view(-1, *attention_output.size()[1:])\n\n            ############## MoE Gating + Experts ###############\n            dispatched_attention, combined_weights = self.moe_gate_einsum(attention_output)\n            dispatched_input = self._alltoall(dispatched_attention)\n            expert_outputs = self.expert_exec(dispatched_input)\n            expert_output = self._alltoall(expert_outputs)\n            output = self.scale_expert_output(attention_output, expert_output, combined_weights)\n            ################################################\n\n            if self.expert_mp_group is not None:\n                output = output.split(output.shape[0] // dist.get_world_size(group=self.expert_mp_group),\n                                      dim=0)[dist.get_rank(group=self.expert_mp_group)]\n\n            if self.config.mlp_type == 'residual':\n                inference_module.moe_res_matmul(res_mlp_out, res_coef_out, output)\n\n            output = self.bias_residual_func(output, residual_add, torch.empty(1))\n\n            if not self.config.pre_layer_norm:\n                output = self.ds_layernorm(output, self.norm_w, self.norm_b, self.config.epsilon)\n\n            if input_type != output.dtype:\n                output = output.to(input_type)\n\n        if get_present:\n            output = (output, presents)\n\n        if self.config.return_tuple:\n            return output if type(output) is tuple else (output, )\n        else:\n            return output\n", "deepspeed/ops/transformer/inference/diffusers_attention.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport math\nimport torch\nfrom torch.autograd import Function\nimport torch.nn as nn\nfrom packaging import version as pkg_version\nfrom deepspeed.utils.logging import log_dist\nfrom deepspeed.accelerator import get_accelerator\nfrom deepspeed.ops.op_builder import InferenceBuilder\n\n# Cuda modules will be imported if needed\ninference_module = None\nminus_inf = -10000.0\ntriton_flash_attn = None\n\n\ndef load_triton_flash_attn():\n    global triton_flash_attn\n    try:\n        import triton\n    except ImportError:\n        raise ImportError(\"Please install triton 2.0+ or `pip install deepspeed[sd]`\")\n\n    if pkg_version.parse(triton.__version__) < pkg_version.parse(\"2.0\"):\n        raise ImportError(\"Please install triton 2.0+ or `pip install deepspeed[sd]`\")\n\n    from .triton_ops import triton_flash_attn\n\n\nclass DeepSpeedDiffusersAttentionFunction(Function):\n\n    @staticmethod\n    def forward(ctx, input, context, input_mask, config, attn_qkvw, attn_qw, attn_kw, attn_vw, attn_qkvb,\n                num_attention_heads_per_partition, norm_factor, hidden_size_per_partition, attn_ow, attn_ob,\n                do_out_bias, score_context_func, linear_func, triton_flash_attn_kernel, rope_theta):\n\n        def _transpose_for_context(x):\n            x = x.permute(0, 2, 1, 3)\n            new_x_layer_shape = x.size()[:-2] + \\\n                                      (hidden_size_per_partition,)\n            return x.reshape(*new_x_layer_shape)\n\n        def _transpose_for_scores(x):\n            attention_head_size = x.shape[-1] // num_attention_heads_per_partition\n            new_x_shape = x.size()[:-1] + (num_attention_heads_per_partition, attention_head_size)\n            x = x.reshape(*new_x_shape)\n            x = x.permute(0, 2, 1, 3)\n            return x.contiguous()\n\n        def selfAttention_fp(input, context, input_mask):\n            if config.dtype in [torch.half, torch.float16] and input.dtype == torch.float32:\n                input = input.half()\n            head_size = input.shape[-1] // config.heads\n            do_flash_attn = (head_size <= 128)\n            scale = (1 / norm_factor) * (1 / norm_factor)\n            if do_flash_attn and context is None:\n                qkv_out = linear_func(input, attn_qkvw, attn_qkvb if attn_qkvb is not None else attn_qkvw, attn_qkvb\n                                      is not None, do_flash_attn, config.heads, False, rope_theta)\n\n                context_layer = triton_flash_attn_kernel(qkv_out[0], qkv_out[1], qkv_out[2], scale,\n                                                         input.shape[-2] % 128 == 0)\n                context_layer = _transpose_for_context(context_layer[:, :, :, :head_size])\n\n            else:\n                do_flash_attn = False\n                if context is not None:\n                    query = torch.matmul(input, attn_qw)\n                    key = torch.matmul(context, attn_kw)\n                    value = torch.matmul(context, attn_vw)\n                else:\n                    qkv = torch.matmul(input, attn_qkvw)\n                    query, key, value = qkv.chunk(3, dim=-1)\n                    query = query.contiguous()\n                    key = key.contiguous()\n                    value = value.contiguous()\n                query, key, value = inference_module.pad_transform_fp16(query, key, value, config.heads, do_flash_attn)\n                attention_scores = (torch.matmul(query, key.transpose(-1, -2)) * scale).softmax(dim=-1)\n                context_layer = _transpose_for_context(torch.matmul(attention_scores, value))\n\n            output = linear_func(context_layer, attn_ow, attn_ob, do_out_bias, False, config.heads, False, rope_theta)\n            return output\n\n        output = selfAttention_fp(input, context, input_mask)\n\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output, grad_output1, grad_output2, grad_output3):\n        raise RuntimeError('You are running with DeepSpeed Inference mode. \\\n                            Please switch to Training mode for running backward!')\n\n\nclass DeepSpeedDiffusersAttention(nn.Module):\n    \"\"\"Initialize the DeepSpeed Transformer Layer.\n        Arguments:\n            layer_id: The layer index starting from 0, e.g. if model has 24 transformer layers,\n                layer_id will be 0,1,2...23 when each layer object is instantiated\n            config: An object of DeepSpeedInferenceConfig\n    \"\"\"\n    layer_id = 0\n\n    def __init__(\n        self,\n        config,\n    ):\n        super(DeepSpeedDiffusersAttention, self).__init__()\n\n        self.config = config\n        self.config.layer_id = DeepSpeedDiffusersAttention.layer_id\n        DeepSpeedDiffusersAttention.layer_id += 1\n        device = get_accelerator().current_device_name() if config.bigscience_bloom else 'cpu'\n        qkv_size_per_partition = (self.config.hidden_size // self.config.mp_size) * 3\n\n        data_type = self.config.dtype\n        data_type_fp = torch.half if self.config.dtype == torch.int8 else self.config.dtype\n        global inference_module\n        if inference_module is None:\n            builder = InferenceBuilder()\n            inference_module = builder.load()\n\n        if DeepSpeedDiffusersAttention.layer_id == 1:\n            log_dist(f\"DeepSpeed-Attention config: {self.config.__dict__}\", [0])\n\n        self.attn_qkvw = nn.Parameter(torch.empty(self.config.hidden_size,\n                                                  qkv_size_per_partition,\n                                                  dtype=data_type,\n                                                  device=device),\n                                      requires_grad=False)\n        self.attn_kw = nn.Parameter(torch.empty(self.config.hidden_size,\n                                                self.config.hidden_size,\n                                                dtype=data_type,\n                                                device=device),\n                                    requires_grad=False)\n        self.attn_vw = nn.Parameter(torch.empty(self.config.hidden_size,\n                                                self.config.hidden_size,\n                                                dtype=data_type,\n                                                device=device),\n                                    requires_grad=False)\n        self.attn_qw = nn.Parameter(torch.empty(self.config.hidden_size,\n                                                self.config.hidden_size,\n                                                dtype=data_type,\n                                                device=device),\n                                    requires_grad=False)\n        self.attn_qkvb = nn.Parameter(torch.empty(qkv_size_per_partition, dtype=data_type_fp, device=device),\n                                      requires_grad=False)\n        out_size_per_partition = self.config.hidden_size // self.config.mp_size\n        self.attn_ow = nn.Parameter(torch.empty(out_size_per_partition,\n                                                self.config.hidden_size,\n                                                dtype=data_type,\n                                                device=device),\n                                    requires_grad=False)\n\n        self.attn_ob = nn.Parameter(torch.empty(self.config.hidden_size, dtype=data_type_fp, device=device),\n                                    requires_grad=False)\n        self.do_out_bias = True\n\n        if triton_flash_attn is None:\n            load_triton_flash_attn()\n        self.triton_flash_attn_kernel = triton_flash_attn()\n        self.num_attention_heads_per_partition = self.config.heads // self.config.mp_size\n        self.hidden_size_per_partition = self.config.hidden_size // self.config.mp_size\n        self.hidden_size_per_attention_head = self.config.hidden_size // self.config.heads\n\n        self.norm_factor = math.sqrt(math.sqrt(self.config.hidden_size // self.config.heads))\n\n        if self.config.scale_attn_by_inverse_layer_idx is True:\n            self.norm_factor *= math.sqrt(self.config.layer_id + 1)\n            # https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/gpt2/modeling_gpt2.py#L191\n\n        if self.config.dtype in [torch.float16, torch.int8]:\n            self.score_context_func = inference_module.softmax_context_fp16\n            self.linear_func = inference_module.linear_layer_fp16\n            self.allocate_workspace = inference_module.allocate_workspace_fp16\n        else:\n            self.score_context_func = inference_module.softmax_context_fp32\n            self.linear_func = inference_module.linear_layer_fp32\n            self.allocate_workspace = inference_module.allocate_workspace_fp32\n\n    def forward(self, input, context=None, input_mask=None):\n        if self.config.layer_id == 0:\n            self.allocate_workspace(self.config.hidden_size, self.config.heads,\n                                    input.size()[1],\n                                    input.size()[0], DeepSpeedDiffusersAttention.layer_id, self.config.mp_size, False,\n                                    0, self.config.max_out_tokens, self.config.min_out_tokens)\n        output = DeepSpeedDiffusersAttentionFunction.apply(input, context, input_mask, self.config, self.attn_qkvw,\n                                                           self.attn_qw, self.attn_kw, self.attn_vw, self.attn_qkvb,\n                                                           self.num_attention_heads_per_partition, self.norm_factor,\n                                                           self.hidden_size_per_partition, self.attn_ow, self.attn_ob,\n                                                           self.do_out_bias, self.score_context_func, self.linear_func,\n                                                           self.triton_flash_attn_kernel, self.config.rope_theta)\n\n        return output\n", "deepspeed/ops/transformer/inference/bias_add.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Optional\nimport torch\nfrom deepspeed.ops.op_builder import SpatialInferenceBuilder\n\nspatial_cuda_module = None\n\n\ndef nhwc_bias_add(activation: torch.Tensor,\n                  bias: torch.Tensor,\n                  other: Optional[torch.Tensor] = None,\n                  other_bias: Optional[torch.Tensor] = None) -> torch.Tensor:\n    global spatial_cuda_module\n    if spatial_cuda_module is None:\n        spatial_cuda_module = SpatialInferenceBuilder().load()\n\n    if other is None:\n        return spatial_cuda_module.nhwc_bias_add(activation, bias)\n    elif other_bias is None:\n        return spatial_cuda_module.nhwc_bias_add_add(activation, bias, other)\n    else:\n        return spatial_cuda_module.nhwc_bias_add_bias_add(activation, bias, other, other_bias)\n", "deepspeed/ops/transformer/inference/op_binding/mlp_gemm.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Optional\n\nimport os\nimport torch\nimport torch.nn.functional as F\nfrom ..config import DeepSpeedInferenceConfig\nfrom .base import BaseOp\nfrom deepspeed.utils.types import NormType\n\n\nclass MLPGemmOp(BaseOp):\n\n    def __init__(self, config: DeepSpeedInferenceConfig):\n        super(MLPGemmOp, self).__init__(config)\n        try:\n            if self.config.norm_type == NormType.LayerNorm:\n                if self.config.dtype in [\n                        torch.float16, torch.int8\n                ]:  # non-triton cuda kernel has a higher performance in MLP than mlp_gemm_func in triton.ops\n                    self.mlp_gemm_func = self.inference_module.mlp_gemm_fp16  # type: ignore\n                elif self.config.dtype == torch.bfloat16:\n                    self.mlp_gemm_func = self.inference_module.mlp_gemm_bf16\n                else:\n                    self.mlp_gemm_func = self.inference_module.mlp_gemm_fp32  # type: ignore\n            elif self.config.norm_type == NormType.RMSNorm:\n                if self.config.dtype in [torch.float16, torch.int8]:\n                    self.mlp_gemm_func = self.inference_module.rms_mlp_gemm_fp16  # type: ignore\n                elif self.config.dtype == torch.bfloat16:\n                    self.mlp_gemm_func = self.inference_module.rms_mlp_gemm_bf16\n                else:\n                    self.mlp_gemm_func = self.inference_module.rms_mlp_gemm_fp32  # type: ignore\n        except AttributeError:\n            if self.config.norm_type == NormType.LayerNorm:\n                self.mlp_gemm_func = self.mlp_gemm_fallback\n            elif self.config.norm_type == NormType.RMSNorm:\n                self.mlp_gemm_func = self.rms_mlp_gemm_fallback\n\n    def mlp_gemm_fallback(self, input, residual, input_bias, weight_interm, weight_out, bias, gamma, beta, eps,\n                          pre_layer_norm, mlp_after_attn, interm_scale, out_scale, dtype, mlp_act_func_type,\n                          transpose):\n        if os.environ.get('DS_KI_FALLBACK') == 'True' and mlp_after_attn and not transpose:\n            residual_add = F.layer_norm(input + residual + input_bias, (input.shape[2], ), gamma, beta,\n                                        self.config.epsilon)\n            tmp = torch.matmul(residual_add, weight_interm)\n            tmp = F.gelu(tmp + bias)\n            output = torch.matmul(tmp, weight_out)\n            return (output, residual_add)\n        else:\n            raise NotImplementedError\n\n    def rms_mlp_gemm_fallback(self, input, residual, weight_interm, weight_out, gamma, eps, interm_scale, out_scale,\n                              dtype, mlp_act_func_type, transpose):\n        raise NotImplementedError\n\n    def forward(self,\n                input: torch.Tensor,\n                residual: torch.Tensor,\n                weight_interm: torch.Tensor,\n                weight_out: torch.Tensor,\n                input_bias: Optional[torch.Tensor] = None,\n                bias: Optional[torch.Tensor] = None,\n                gamma: Optional[torch.Tensor] = None,\n                beta: Optional[torch.Tensor] = None):\n        if self.config.norm_type == NormType.LayerNorm:\n            output, residual_add = self.mlp_gemm_func(\n                input,\n                residual,\n                input_bias,\n                weight_interm,\n                weight_out,\n                bias,\n                gamma,\n                beta,\n                self.config.epsilon,\n                self.config.pre_layer_norm,\n                self.config.mlp_after_attn,\n                weight_interm.scale if hasattr(weight_interm, 'scale') else torch.empty(1),  # type: ignore\n                weight_out.scale if hasattr(weight_out, 'scale') else torch.empty(1),  # type: ignore\n                self.config.dtype == torch.int8,\n                self.config.mlp_act_func_type,\n                self.config.transposed_mode)\n        else:\n            if input_bias is not None:\n                input += input_bias\n            output, residual_add = self.mlp_gemm_func(\n                input,\n                residual,\n                weight_interm,\n                weight_out,\n                gamma,\n                self.config.epsilon,\n                weight_interm.scale if hasattr(weight_interm, 'scale') else torch.empty(1),  # type: ignore\n                weight_out.scale if hasattr(weight_out, 'scale') else torch.empty(1),  # type: ignore\n                self.config.dtype == torch.int8,\n                self.config.mlp_act_func_type,\n                self.config.transposed_mode)\n        return output, residual_add\n", "deepspeed/ops/transformer/inference/op_binding/softmax_context.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nfrom deepspeed import comm as dist\nfrom ..config import DeepSpeedInferenceConfig\nfrom .base import BaseOp\n\n\nclass SoftmaxContextOp(BaseOp):\n\n    def __init__(self, config: DeepSpeedInferenceConfig):\n        super(SoftmaxContextOp, self).__init__(config)\n        try:\n            if self.config.dtype in [torch.float16, torch.int8]:\n                self.softmax_context_func = self.inference_module.softmax_context_fp16\n            elif self.config.dtype == torch.bfloat16:\n                self.softmax_context_func = self.inference_module.softmax_context_bf16\n            else:\n                self.softmax_context_func = self.inference_module.softmax_context_fp32\n        except AttributeError:\n            self.softmax_context_func = self.softmax_context_fallback\n\n    def softmax_context_fallback(self, query_key_value, attn_mask, rotary_dim, rotate_half, rotate_every_two, heads,\n                                 num_kv, norm_factor, triangular_masking, local_attention, window_size, no_masking,\n                                 layer_id, num_layers, alibi, rope_theta):\n        raise NotImplementedError\n\n    def forward(self, query_key_value: torch.Tensor, attn_mask: torch.Tensor, heads: int, num_kv: int,\n                norm_factor: float, no_masking: bool, layer_id: int, num_layers: int, alibi: torch.Tensor):\n\n        if alibi is not None:\n            batch_heads = query_key_value.shape[0] * heads\n            offset = dist.get_rank() * batch_heads if dist.is_initialized() else 0\n            alibi = alibi[offset:batch_heads + offset, :, :]\n        else:\n            alibi = torch.empty(1)\n\n        output = self.softmax_context_func(query_key_value, attn_mask, self.config.rotary_dim, self.config.rotate_half,\n                                           self.config.rotate_every_two, heads, num_kv, norm_factor,\n                                           self.config.triangular_masking, self.config.local_attention,\n                                           self.config.window_size, no_masking, layer_id, num_layers, alibi,\n                                           self.config.rope_theta)\n\n        return output\n", "deepspeed/ops/transformer/inference/op_binding/qkv_gemm.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport os\nimport torch\nimport torch.nn.functional as F\nfrom ..config import DeepSpeedInferenceConfig\nfrom .base import BaseOp\nimport deepspeed\nfrom deepspeed.utils.types import NormType\n\n\nclass QKVGemmOp(BaseOp):\n\n    def __init__(self, config: DeepSpeedInferenceConfig):\n        super(QKVGemmOp, self).__init__(config)\n        try:\n            if self.config.norm_type == NormType.LayerNorm:\n                if self.config.dtype in [torch.float16, torch.int8]:\n                    if deepspeed.HAS_TRITON and self.config.use_triton and self.config.dtype == torch.float16:\n                        from deepspeed.ops.transformer.inference.triton.ops import qkv_gemm_func as _triton_qkv_gemm_func\n                        self.qkv_gemm_func = _triton_qkv_gemm_func\n                        triton_autotune = config.triton_autotune and config.layer_id == 0\n                        if triton_autotune:\n                            __class__._triton_autotune(2, self.config.max_out_tokens, self.config.hidden_size)\n                    else:\n                        self.qkv_gemm_func = self.inference_module.qkv_gemm_fp16  # type: ignore\n                elif self.config.dtype == torch.bfloat16:\n                    self.qkv_gemm_func = self.inference_module.qkv_gemm_bf16\n                else:\n                    self.qkv_gemm_func = self.inference_module.qkv_gemm_fp32  # type: ignore\n            elif self.config.norm_type == NormType.RMSNorm:\n                if self.config.dtype in [torch.float16, torch.int8]:\n                    self.qkv_gemm_func = self.inference_module.rms_qkv_gemm_fp16  # type: ignore\n                elif self.config.dtype == torch.bfloat16:\n                    self.qkv_gemm_func = self.inference_module.rms_qkv_gemm_bf16\n                else:\n                    self.qkv_gemm_func = self.inference_module.rms_qkv_gemm_fp32  # type: ignore\n        except AttributeError:\n            if self.config.norm_type == NormType.LayerNorm:\n                self.qkv_gemm_func = self.qkv_gemm_fallback\n            elif self.config.norm_type == NormType.RMSNorm:\n                self.qkv_gemm_func = self.rms_qkv_gemm_fallback\n\n    @staticmethod\n    def _triton_autotune(min_seqlen, max_seqlen, hidden_size, dtype=torch.float16):\n        from deepspeed.ops.transformer.inference.triton.matmul_ext import Fp16Matmul, matmul\n        seqlen = [(min_seqlen + i)\n                  for i in range(0, max_seqlen - min_seqlen + Fp16Matmul._cache_stride + 1, Fp16Matmul._cache_stride)]\n        Fp16Matmul._read_autotune_table()\n        for N in seqlen:\n            A = torch.randn((N, hidden_size), dtype=dtype, device='cuda')\n            B = torch.randn((hidden_size, 3 * hidden_size), dtype=dtype, device='cuda')\n            matmul(A, B)\n        Fp16Matmul._update_autotune_table()\n\n    def qkv_gemm_fallback(self, input, weight, q_scale, bias, gamma, beta, eps, add_bias, q_int8, transpose):\n        if os.environ.get('DS_KI_FALLBACK') == 'True' and not transpose:\n            inp_norm = F.layer_norm(input, (input.shape[2], ), gamma, beta, eps)\n            tmp = torch.matmul(inp_norm, weight)\n            if add_bias:\n                tmp += bias\n            output = [tmp, inp_norm]\n            return output\n        else:\n            raise NotImplementedError\n\n    def rms_qkv_gemm_fallback(self, input, weight, q_scale, gamma, eps, q_int8, transpose):\n        raise NotImplementedError\n\n    def forward(self, input: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor, gamma: torch.Tensor,\n                beta: torch.Tensor):\n\n        add_bias = bias is not None\n        bias = bias if add_bias else torch.empty(1)  # type: ignore\n        q_scale = weight.scale if hasattr(weight, 'scale') else torch.empty(1)  # type: ignore\n        q_int8 = self.config.dtype == torch.int8\n\n        if self.config.norm_type == NormType.LayerNorm:\n            output, norm = self.qkv_gemm_func(input, weight, q_scale, bias, gamma, beta, self.config.epsilon, add_bias,\n                                              q_int8, self.config.transposed_mode)\n        else:\n            output, norm = self.qkv_gemm_func(input, weight, q_scale, gamma, self.config.epsilon, q_int8,\n                                              self.config.transposed_mode)\n            if add_bias:\n                output += bias\n\n        return output, norm\n", "deepspeed/ops/transformer/inference/op_binding/linear.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nfrom ..config import DeepSpeedInferenceConfig\nfrom .base import BaseOp\nimport deepspeed\n\n\nclass LinearOp(BaseOp):\n\n    def __init__(self, config: DeepSpeedInferenceConfig):\n        super(LinearOp, self).__init__(config)\n        try:\n            if self.config.dtype in [torch.float16, torch.int8]:\n                if deepspeed.HAS_TRITON and self.config.use_triton and self.config.dtype == torch.float16:\n                    from deepspeed.ops.transformer.inference.triton.ops import linear_func as _triton_linear_func\n                    self.linear_func = _triton_linear_func\n                    triton_autotune = config.triton_autotune and config.layer_id == 0\n                    if triton_autotune:\n                        __class__._triton_autotune(2, self.config.max_out_tokens, self.config.hidden_size)\n                else:\n                    self.linear_func = self.inference_module.linear_layer_fp16\n                self.linear_func = self.inference_module.linear_layer_fp16\n            elif self.config.dtype == torch.bfloat16:\n                self.linear_func = self.inference_module.linear_layer_bf16\n            else:\n                self.linear_func = self.inference_module.linear_layer_fp32\n        except AttributeError:\n            self.linear_func = self.linear_fallback\n\n    def linear_fallback(self, input, weight, bias, add_bias, do_flash_attn, num_heads, transpose, rope_theta):\n        raise NotImplementedError\n\n    def forward(self,\n                input: torch.Tensor,\n                weight: torch.Tensor,\n                bias: torch.Tensor,\n                add_bias: bool,\n                do_flash_attn: bool,\n                num_heads: int,\n                external_cache: bool = None,\n                num_layers: int = None):\n        qkv_out = self.linear_func(input, weight, bias, add_bias, do_flash_attn, num_heads,\n                                   self.config.transposed_mode, self.config.rope_theta)\n        return qkv_out\n\n    @staticmethod\n    def _triton_autotune(min_seqlen, max_seqlen, hidden_size, dtype=torch.float16):\n        from deepspeed.ops.transformer.inference.triton.matmul_ext import Fp16Matmul, matmul\n        seqlen = [(min_seqlen + i)\n                  for i in range(0, max_seqlen - min_seqlen + Fp16Matmul._cache_stride + 1, Fp16Matmul._cache_stride)]\n        Fp16Matmul._read_autotune_table()\n        for N in seqlen:\n            A = torch.randn((N, hidden_size), dtype=dtype, device='cuda')\n            B = torch.randn((hidden_size, 3 * hidden_size), dtype=dtype, device='cuda')\n            matmul(A, B)\n        Fp16Matmul._update_autotune_table()\n", "deepspeed/ops/transformer/inference/op_binding/base.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nfrom ..config import DeepSpeedInferenceConfig\n\nfrom deepspeed.ops.op_builder import InferenceBuilder\n\n\nclass BaseOp(torch.nn.Module):\n    inference_module = None\n\n    def __init__(self, config: DeepSpeedInferenceConfig):\n        super(BaseOp, self).__init__()\n        self.config = config\n        if BaseOp.inference_module is None:\n            builder = InferenceBuilder()\n            BaseOp.inference_module = builder.load()\n", "deepspeed/ops/transformer/inference/op_binding/vector_matmul.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport os\nimport torch\nfrom ..config import DeepSpeedInferenceConfig\nfrom .base import BaseOp\nimport deepspeed\n\n\nclass VectorMatMulOp(BaseOp):\n\n    def __init__(self, config: DeepSpeedInferenceConfig):\n        super(VectorMatMulOp, self).__init__(config)\n        try:\n            if self.config.dtype == torch.float16:\n                if deepspeed.HAS_TRITON and config.use_triton:\n                    from deepspeed.ops.transformer.inference.triton.ops import vector_matmul_func as _triton_vector_matmul_func\n                    self.vector_matmul_func = _triton_vector_matmul_func\n                    triton_autotune = config.triton_autotune and config.layer_id == 0\n                    if triton_autotune:\n                        __class__._triton_autotune(2, self.config.max_out_tokens, self.config.hidden_size)\n                else:\n                    self.vector_matmul_func = self.inference_module.vector_matmul_fp16\n            elif self.config.dtype == torch.int8:\n                self.vector_matmul_func = self.inference_module.vector_matmul_fp16\n            elif self.config.dtype == torch.bfloat16:\n                self.vector_matmul_func = self.inference_module.vector_matmul_bf16\n            else:\n                self.vector_matmul_func = self.inference_module.vector_matmul_fp32\n        except AttributeError:\n            self.vector_matmul_func = self.vector_matmul_fallback\n\n    def vector_matmul_fallback(self, input, weight, async_op, q_scale, q_int8, transpose):\n        if os.environ.get('DS_KI_FALLBACK') == 'True' and not transpose:\n            return torch.matmul(input, weight)\n        else:\n            raise NotImplementedError\n\n    def forward(self, input: torch.Tensor, weight: torch.Tensor, async_op: bool = False):\n        q_scale = weight.scale if hasattr(weight, 'scale') else torch.empty(1)\n        q_int8 = self.config.dtype == torch.int8\n        output = self.vector_matmul_func(input, weight, async_op, q_scale, q_int8, self.config.transposed_mode)\n        return output\n\n    @staticmethod\n    def _triton_autotune(min_seqlen, max_seqlen, hidden_size, dtype=torch.float16):\n        from deepspeed.ops.transformer.inference.triton.matmul_ext import Fp16Matmul, matmul\n        seqlen = [(min_seqlen + i)\n                  for i in range(0, max_seqlen - min_seqlen + Fp16Matmul._cache_stride + 1, Fp16Matmul._cache_stride)]\n        Fp16Matmul._read_autotune_table()\n        for N in seqlen:\n            A = torch.randn((N, hidden_size), dtype=dtype, device='cuda')\n            B = torch.randn((hidden_size, hidden_size), dtype=dtype, device='cuda')\n            matmul(A, B)\n        Fp16Matmul._update_autotune_table()\n", "deepspeed/ops/transformer/inference/op_binding/softmax.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport os\nimport torch\nimport torch.nn.functional as F\nfrom ..config import DeepSpeedInferenceConfig\nfrom .base import BaseOp\n\n\nclass SoftmaxOp(BaseOp):\n\n    def __init__(self, config: DeepSpeedInferenceConfig):\n        super(SoftmaxOp, self).__init__(config)\n        self.num_attention_heads_per_partition = config.heads // config.mp_size\n        try:\n            if self.config.dtype in [torch.float16, torch.int8]:\n                self.softmax_func = self.inference_module.softmax_fp16\n            elif self.config.dtype == torch.bfloat16:\n                self.softmax_func = self.inference_module.softmax_bf16\n            else:\n                self.softmax_func = self.inference_module.softmax_fp32\n        except AttributeError:\n            self.softmax_func = self.softmax_fallback\n\n    def softmax_fallback(self, attn_scores, attn_mask, alibi, triangular, recompute, local_attention, window_size,\n                         async_op, layer_scale, head_offset, mp_size):\n        if os.environ.get('DS_KI_FALLBACK') == 'True':\n            alibi = alibi[head_offset:head_offset + self.num_attention_heads_per_partition]\n            input_dtype = attn_scores.dtype\n            if (triangular):\n                tri = ~torch.tril(torch.ones(attn_scores.size(), device=attn_scores.device)).to(bool)\n                attn_scores = torch.masked_fill(attn_scores * layer_scale, tri, torch.finfo(input_dtype).min)\n            if alibi is not None:\n                attn_scores += alibi\n            if attn_mask is not None:\n                # expand atten_mask from two dim into 4 dim, insert two dims in the middle\n                attn_mask = attn_mask[:, None, None, :]\n                attn_scores += attn_mask\n            output = F.softmax(attn_scores, dim=-1, dtype=torch.float32).to(input_dtype)\n            return output\n        else:\n            raise NotImplementedError\n\n    def forward(self, attn_scores: torch.Tensor, attn_mask: torch.Tensor, alibi: torch.Tensor, triangular: bool,\n                recompute: bool, local_attention: bool, window_size: int, async_op: bool, layer_scale: float,\n                head_offset: int):\n        output = self.softmax_func(attn_scores, attn_mask, alibi, triangular, recompute, local_attention, window_size,\n                                   async_op, layer_scale, head_offset, self.config.mp_size)\n\n        return output\n", "deepspeed/ops/transformer/inference/op_binding/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .linear import LinearOp\nfrom .vector_matmul import VectorMatMulOp\nfrom .softmax_context import SoftmaxContextOp\nfrom .qkv_gemm import QKVGemmOp\nfrom .softmax import SoftmaxOp\nfrom .mlp_gemm import MLPGemmOp\nfrom .gelu_gemm import GELUGemmOp\nfrom .residual_add import ResidualAddOp\n", "deepspeed/ops/transformer/inference/op_binding/gelu_gemm.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nfrom ..config import DeepSpeedInferenceConfig\nfrom .base import BaseOp\nimport deepspeed\n\n\nclass GELUGemmOp(BaseOp):\n\n    def __init__(self, config: DeepSpeedInferenceConfig):\n        super(GELUGemmOp, self).__init__(config)\n        try:\n            if self.config.dtype in [torch.float16, torch.int8]:\n                if deepspeed.HAS_TRITON and self.config.use_triton and self.config.dtype == torch.float16:\n                    from deepspeed.ops.transformer.inference.triton.ops import fused_gemm_gelu as _triton_fused_gemm_gelu\n                    self.fused_gemm_gelu = _triton_fused_gemm_gelu  # type: ignore\n                else:\n                    self.fused_gemm_gelu = self.inference_module.fused_gemm_gelu_fp16  # type: ignore\n            elif self.config.dtype == torch.bfloat16:\n                self.fused_gemm_gelu = self.inference_module.fused_gemm_gelu_bf16  # type: ignore\n            else:\n                self.fused_gemm_gelu = self.inference_module.fused_gemm_gelu_fp32  # type: ignore\n        except AttributeError:\n            self.fused_gemm_gelu = self.gelu_gemm_fallback\n\n    def gelu_gemm_fallback(self, input, weight, scale, bias, out, out_scale, dtype, transpose):\n        raise NotImplementedError\n\n    def forward(self, input: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor, weight_out: torch.Tensor):\n\n        output = self.fused_gemm_gelu(\n            input,\n            weight,\n            weight.scale if hasattr(weight, 'scale') else torch.empty(1),  # type: ignore\n            bias,\n            weight_out,\n            weight_out.scale if hasattr(weight_out, 'scale') else torch.empty(1),  # type: ignore\n            self.config.dtype == torch.int8,\n            self.config.transposed_mode)\n\n        return output\n", "deepspeed/ops/transformer/inference/op_binding/residual_add.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport os\nimport torch\nfrom typing import Optional\nfrom ..config import DeepSpeedInferenceConfig\nfrom .base import BaseOp\n\n\nclass ResidualAddOp(BaseOp):\n\n    def __init__(self, config: DeepSpeedInferenceConfig):\n        super(ResidualAddOp, self).__init__(config)\n        try:\n            if self.config.dtype in [torch.float16, torch.int8]:\n                self.residual_add_func = self.inference_module.residual_add_bias_fp16\n            elif self.config.dtype == torch.bfloat16:\n                self.residual_add_func = self.inference_module.residual_add_bias_bf16\n            else:\n                self.residual_add_func = self.inference_module.residual_add_bias_fp32\n        except AttributeError:\n            self.residual_add_func = None\n        try:\n            self._vector_add = self.inference_module._vector_add\n        except AttributeError:\n            self._vector_add = None\n\n    def forward(self,\n                hidden_state: torch.Tensor,\n                residual: torch.Tensor,\n                add_bias: bool,\n                attention_output: Optional[torch.Tensor] = None,\n                residual_add: Optional[torch.Tensor] = None,\n                attention_bias: Optional[torch.Tensor] = None,\n                final_bias: Optional[torch.Tensor] = None):\n\n        if self.residual_add_func is not None:\n            if final_bias is None:\n                residual = self._vector_add(residual, hidden_state, 1.0 / self.config.mp_size)\n            else:\n                if not self.config.pre_layer_norm and residual_add is not None:\n                    # only use residual add if its set and we are not pre layer norm\n                    residual = residual_add\n\n                self.residual_add_func(hidden_state, residual, attention_output, attention_bias, final_bias,\n                                       self.config.mp_size, self.config.mlp_after_attn, add_bias,\n                                       self.config.pre_layer_norm)\n        else:\n            # fallback\n            if os.environ.get('DS_KI_FALLBACK') == 'True' and self.config.mlp_after_attn:\n                if self.config.pre_layer_norm:\n                    tmp = (residual.float() + attention_output.float() + attention_bias.float() +\n                           final_bias.float()) / self.config.mp_size + hidden_state.float()\n                else:\n                    tmp = residual.float() + hidden_state.float() + final_bias.float()\n\n                input_dtype = hidden_state.dtype\n                residual = tmp.to(input_dtype)\n            else:\n                raise NotImplementedError\n        return residual\n", "deepspeed/ops/transformer/inference/triton/attention.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport math\nimport torch\nimport torch.nn as nn\nimport triton\nimport triton.language as tl\nfrom deepspeed.accelerator import get_accelerator\nfrom deepspeed import comm as dist\nfrom deepspeed.ops.transformer.inference.op_binding import LinearOp, VectorMatMulOp, SoftmaxContextOp, QKVGemmOp\nfrom deepspeed.ops.transformer.inference.triton import (\n    softmax,\n    score_4d_matmul,\n    context_4d_matmul,\n)\n\nminus_inf = -10000.0\n\n\nclass TritonSelfAttention(nn.Module):\n    num_layers = 0\n\n    def __init__(self, config, mp_group=None, q_scales=None, q_groups=1, merge_count=1, qkv_merging=False):\n        super(TritonSelfAttention, self).__init__()\n        self.config = config\n        data_type = self.config.dtype\n        data_type_fp = torch.half if self.config.dtype == torch.int8 else self.config.dtype\n        assert data_type_fp == torch.half, \"triton supports fp16 data_type_fp\"\n\n        self.config.layer_id = TritonSelfAttention.num_layers\n        TritonSelfAttention.num_layers = TritonSelfAttention.num_layers + 1\n        device = get_accelerator().current_device_name()  #if config.bigscience_bloom else 'cpu'\n\n        assert config.mp_size == 1, \"mp_size has to be 1 with triton attention yet\"\n        if self.config.set_empty_params:\n            self.attn_qw = None\n            self.attn_qb = None\n            self.attn_kw = None\n            self.attn_kb = None\n            self.attn_vw = None\n            self.attn_vb = None\n            self.attn_qkvw = None\n            self.attn_qkvb = None\n            self.attn_ow = None\n            self.attn_ob = None\n        else:\n            qkv_size_per_partition = (self.config.hidden_size // self.config.mp_size) * 3\n            self.attn_qkvw = nn.Parameter(torch.empty(self.config.hidden_size,\n                                                      qkv_size_per_partition,\n                                                      dtype=data_type,\n                                                      device=device),\n                                          requires_grad=False)\n            self.attn_qkvb = nn.Parameter(torch.empty(qkv_size_per_partition, dtype=data_type_fp, device=device),\n                                          requires_grad=False)\n            # self-ouput weights\n            out_size_per_partition = self.config.hidden_size // self.config.mp_size\n            self.attn_ow = nn.Parameter(torch.empty(out_size_per_partition,\n                                                    self.config.hidden_size,\n                                                    dtype=data_type,\n                                                    device=device),\n                                        requires_grad=False)\n\n            self.attn_ob = nn.Parameter(torch.empty(self.config.hidden_size, dtype=data_type_fp, device=device),\n                                        requires_grad=False)\n\n        self.num_attention_heads_per_partition = self.config.heads // self.config.mp_size\n        self.hidden_size_per_partition = self.config.hidden_size // self.config.mp_size\n        self.hidden_size_per_attention_head = self.config.hidden_size // self.config.heads\n\n        self.mp_group = mp_group\n        self.use_flash = False\n        # triton flash attention is enabled when the compute capability >= 8.0\n        if get_accelerator().is_triton_supported():\n            self.use_flash = True\n\n        # used for quantization\n        self.q_scales = q_scales\n        self.q_groups = q_groups\n        self.merge_count = int(math.log2(merge_count))\n\n        self.norm_factor = math.sqrt(self.config.hidden_size // self.config.heads)\n        if not config.use_mup:\n            self.norm_factor = math.sqrt(self.norm_factor)\n\n        if self.config.scale_attn_by_inverse_layer_idx is True:\n            self.norm_factor *= math.sqrt(self.config.layer_id + 1)\n            # https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/gpt2/modeling_gpt2.py#L191\n\n        triton_autotune = self.config.triton_autotune and self.config.layer_id == 0\n        self.qkv_func = QKVGemmOp(config)\n        self.score_context_func = SoftmaxContextOp(config)\n        self.linear_func = LinearOp(config)\n        self.vector_matmul_func = VectorMatMulOp(config)\n\n        self.hidden_size = config.hidden_size\n        self.head_size = config.hidden_size // config.heads\n        self.scale = (1 / self.norm_factor / self.norm_factor if self.config.scale_attention else 1.0\n                      )  # making it back to 1/sqrt(head_size)\n        self.triangular_masking = self.config.triangular_masking\n\n        # triton autotune table update for score/context matmul\n        if triton_autotune:\n            print(f\"running triton autotune for regular attention kernel\")\n            __class__._triton_autotune(2, self.config.max_out_tokens, self.head_size, self.config.hidden_size,\n                                       self.triangular_masking, self.scale)\n\n    @staticmethod\n    def _triton_autotune(min_seqlen,\n                         max_seqlen,\n                         head_size,\n                         hidden_size,\n                         triangular_masking,\n                         scale,\n                         dtype=torch.float16):\n        from deepspeed.ops.transformer.inference.triton.matmul_ext import Fp16Matmul, score_4d_matmul, context_4d_matmul\n        seqlen = [(min_seqlen + i)\n                  for i in range(0, max_seqlen - min_seqlen + Fp16Matmul._cache_stride + 1, Fp16Matmul._cache_stride)]\n        Fp16Matmul._read_autotune_table()\n        for N in seqlen:\n            qkv = torch.randn((1, N, 3 * hidden_size), dtype=dtype, device='cuda')\n            output = score_4d_matmul(qkv, head_size, triangular_masking, scale)\n            context_4d_matmul(output, qkv, head_size)\n        Fp16Matmul._update_autotune_table()\n\n    def ds_compute_attention(self, qkv_out, input_mask, layer_past, alibi):\n        if isinstance(qkv_out, list):\n            qkv_out = qkv_out[0]\n\n        no_masking = input_mask is None\n\n        if no_masking:\n            input_mask = torch.empty(1)\n\n        attn_key_value = self.score_context_func(\n            query_key_value=qkv_out,\n            attn_mask=((1 - input_mask).to(qkv_out.dtype) *\n                       minus_inf) if input_mask.dtype == torch.int64 else input_mask,\n            heads=self.num_attention_heads_per_partition,\n            norm_factor=(1 / self.norm_factor if self.config.scale_attention else 1.0),\n            no_masking=no_masking,\n            layer_id=self.config.layer_id,\n            num_layers=TritonSelfAttention.num_layers,\n            alibi=alibi)\n\n        context_layer, key_layer, value_layer = attn_key_value\n        return context_layer, key_layer, value_layer\n\n    def forward(\n            self,\n            input,\n            input_mask,\n            head_mask=None,\n            layer_past=None,\n            get_present=False,  # not used\n            encoder_hidden_states=None,  # not used\n            encoder_attention_mask=None,  # not used\n            triangularutput_attentions=False,  # not used\n            norm_w=None,\n            norm_b=None,\n            alibi=None,\n            use_triton_attention=True):\n\n        if not self.config.pre_layer_norm:\n            qkv_out = self.linear_func(input=input,\n                                       weight=self.attn_qkvw,\n                                       bias=self.attn_qkvb,\n                                       add_bias=self.attn_qkvb is not None,\n                                       do_flash_attn=False,\n                                       num_heads=self.num_attention_heads_per_partition,\n                                       num_layers=TritonSelfAttention.num_layers)\n            qkv = qkv_out\n        else:\n            qkv_out = self.qkv_func(input=input,\n                                    weight=self.attn_qkvw,\n                                    bias=(self.attn_qkvb if self.attn_qkvb is not None else norm_b),\n                                    gamma=norm_w,\n                                    beta=norm_b)\n            qkv = qkv_out[0]\n\n        if use_triton_attention and (alibi is None):\n            context_layer = _triton_attention(qkv=qkv,\n                                              input_mask=input_mask,\n                                              scale=self.scale,\n                                              layer_past=layer_past,\n                                              alibi=alibi,\n                                              head_size=self.head_size,\n                                              use_triton_flash=self.use_flash,\n                                              use_cuda_flash=False,\n                                              triangular=self.triangular_masking)\n            key_layer, value_layer = qkv[:, :, self.hidden_size:2 * self.hidden_size], qkv[:, :, 2 * self.hidden_size:]\n        else:\n            context_layer, key_layer, value_layer = self.ds_compute_attention(qkv_out=qkv_out,\n                                                                              input_mask=input_mask,\n                                                                              layer_past=layer_past,\n                                                                              alibi=alibi)\n        output = self.vector_matmul_func(input=context_layer, weight=self.attn_ow)\n\n        inp_norm = qkv_out[-1]\n\n        if self.config.mlp_after_attn and self.mp_group is not None and dist.get_world_size(group=self.mp_group) > 1:\n            dist.all_reduce(output, group=self.mp_group)\n\n        return (output, key_layer, value_layer, context_layer, inp_norm)\n\n\nglobal inference_module\n\n\ndef _triton_attention(qkv,\n                      input_mask,\n                      layer_past,\n                      alibi,\n                      scale,\n                      head_size,\n                      triangular=False,\n                      use_cuda_flash=False,\n                      use_triton_flash=False,\n                      use_ds_attention=False):\n    if isinstance(qkv, list):\n        qkv = qkv[0]\n\n    assert alibi is None, \"layer_past not supported in alibi yet\"\n\n    if use_triton_flash:\n        output = _triton_packed_flash(qkv,\n                                      head_size,\n                                      input_mask,\n                                      scale,\n                                      causal=triangular,\n                                      add_mask=(not triangular and input_mask is not None))\n    else:\n        output = score_4d_matmul(qkv, head_size, triangular, scale)\n        if triangular:\n            output = softmax(output)\n        else:\n            output = softmax(output, input_mask)\n        output = context_4d_matmul(output, qkv, head_size)\n\n    return output\n\n\n'''\nflash attention 2\nmodified the triton kernel in\nhttps://github.com/openai/triton/blob/08c16589573621fcb8cd5a9c3b8a0537077f876d/python/tutorials/06-fused-attention.py\n'''\n\n\n@triton.jit\ndef _flash_packed_kernel(\n    QKV,\n    mask,\n    ADD_MASK: tl.constexpr,\n    IS_CAUSAL: tl.constexpr,\n    sm_scale,\n    Out,\n    stride_qz,\n    stride_qn,\n    stride_qm,\n    stride_mz,\n    stride_oz,\n    stride_on,\n    Z,\n    H,\n    N_CTX,\n    P_SEQ,\n    hidden_size,\n    BLOCK_M: tl.constexpr,\n    BLOCK_DMODEL: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    batch = off_hz // H\n    head = off_hz % H\n\n    q_offset = batch * stride_qz + head * BLOCK_DMODEL\n    k_offset = q_offset + hidden_size\n    v_offset = k_offset + hidden_size\n\n    # initialize offsets\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n\n    q_ptrs = QKV + q_offset + offs_m[:, None] * stride_qn + offs_d[None, :]\n    k_ptrs = QKV + hidden_size + q_offset + offs_n[:, None] * stride_qn + offs_d[None, :]\n    v_ptrs = QKV + 2 * hidden_size + q_offset + offs_n[:, None] * stride_qn + offs_d[None, :]\n\n    # mask\n    off_mask = batch * stride_mz + offs_n[None, :]\n    mask_ptrs = mask + off_mask\n\n    # initialize pointer to m and l\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    # scale sm_scale by log_2(e) and use\n    # 2^x instead of exp in the loop because CSE and LICM\n    # don't work as expected with `exp` in the loop\n    qk_scale = sm_scale * 1.44269504\n    # load q: it will stay in SRAM throughout\n    q = tl.load(q_ptrs, mask=offs_m[:, None] < N_CTX, other=0.0)\n    q = (q * qk_scale).to(tl.float16)\n    # loop over k, v and update accumulator\n    lo = 0\n    hi = P_SEQ + (start_m + 1) * BLOCK_M if IS_CAUSAL else N_CTX + P_SEQ\n    for start_n in range(lo, hi, BLOCK_N):\n        # -- load k, v --\n        k = tl.load(k_ptrs + start_n * stride_qn, mask=(start_n + offs_n)[:, None] < N_CTX, other=0.0)\n        v = tl.load(v_ptrs + start_n * stride_qn, mask=(start_n + offs_n)[:, None] < N_CTX, other=0.0)\n        # -- compute qk ---\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float16)\n\n        if ADD_MASK:\n            mask_val = tl.load(mask_ptrs)\n            mask_ptrs += BLOCK_N\n            qk = qk + mask_val.to(tl.float32)\n\n        if IS_CAUSAL:\n            qk = tl.where(P_SEQ + offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n\n        qk += tl.dot(q, tl.trans(k), out_dtype=tl.float16)\n        qk += tl.where((start_n + offs_n)[None, :] < N_CTX, 0, minus_inf)\n        # -- compute scaling constant ---\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n        alpha = tl.math.exp2(m_i - m_i_new)\n        p = tl.math.exp2(qk - m_i_new[:, None])\n        # -- scale and update acc --\n        acc_scale = l_i * 0 + alpha  # workaround some compiler bug\n        acc *= acc_scale[:, None]\n        acc += tl.dot(p.to(tl.float16), v.to(tl.float16))\n        # -- update m_i and l_i --\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n\n    # write back l and m\n    acc = acc / l_i[:, None]\n    o_offset = batch * stride_oz + head * BLOCK_DMODEL\n    out_ptrs = Out + o_offset + (offs_m[:, None] * stride_on + offs_d[None, :])\n    tl.store(out_ptrs, acc.to(tl.float16), mask=offs_m[:, None] < N_CTX)\n\n\ndef _triton_packed_flash(qkv, head_size, mask, sm_scale, causal=False, add_mask=True):\n    heads = qkv.shape[-1] // 3 // head_size\n    hidden_size = qkv.shape[-1] // 3\n\n    BLOCK_M = 128\n    BLOCK_N = 64 if head_size <= 64 else 32\n\n    o = torch.empty((qkv.shape[0], qkv.shape[1], hidden_size), device=qkv.device, dtype=torch.half)\n    if mask is None:\n        mask = torch.empty(0)\n        add_mask = False\n\n    grid = (triton.cdiv(qkv.shape[1], BLOCK_M), qkv.shape[0] * heads, 1)\n    num_stages = 4 if head_size <= 64 else 3\n    num_warps = 4\n    P_SEQ = 0\n\n    _flash_packed_kernel[grid](qkv,\n                               mask,\n                               add_mask,\n                               causal,\n                               sm_scale,\n                               o,\n                               qkv.stride(0),\n                               qkv.stride(1),\n                               qkv.stride(2),\n                               mask.stride(1) if add_mask else 0,\n                               o.stride(0),\n                               o.stride(1),\n                               qkv.shape[0],\n                               heads,\n                               qkv.shape[1],\n                               P_SEQ,\n                               hidden_size,\n                               BLOCK_M=BLOCK_M,\n                               BLOCK_N=BLOCK_N,\n                               BLOCK_DMODEL=head_size,\n                               num_warps=num_warps,\n                               num_stages=num_stages)\n\n    return o\n", "deepspeed/ops/transformer/inference/triton/gelu.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nimport triton\nimport triton.language as tl\nfrom deepspeed.accelerator import get_accelerator\n\n\n@triton.jit\ndef gelu_functor(x):\n    # Using approximation introduces greater parity errors.\n    # return tl.sigmoid(1.702 * x) * x\n    return x * 0.5 * (1.0 + tl.math.erf(x / 1.41421356237))\n\n\n@triton.jit\ndef gelu_kernel(x_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    output = gelu_functor(x)\n    tl.store(output_ptr + offsets, output, mask=mask)\n\n\ndef gelu(activations: torch.Tensor) -> torch.Tensor:\n    assert activations.is_contiguous()\n    assert get_accelerator().on_accelerator(activations)\n\n    output = torch.empty_like(activations)\n    n_elements = output.numel()\n    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']), )\n    gelu_kernel[grid](activations, output, n_elements, BLOCK_SIZE=1024)\n    return output\n", "deepspeed/ops/transformer/inference/triton/mlp.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nimport math\nimport torch.nn as nn\nfrom deepspeed.accelerator import get_accelerator\nfrom deepspeed import comm as dist\nfrom ..op_binding import MLPGemmOp, VectorMatMulOp, GELUGemmOp, ResidualAddOp\n\n\nclass TritonMLP(nn.Module):\n\n    def __init__(self, config, mp_group=None, q_scales=None, q_groups=1, merge_count=1, mlp_extra_grouping=False):\n        super(TritonMLP, self).__init__()\n\n        self.config = config\n        data_type = self.config.dtype\n        data_type_fp = torch.half if self.config.dtype == torch.int8 else self.config.dtype\n        device = get_accelerator().current_device_name()\n        self.attn_nw = nn.Parameter(torch.empty(self.config.hidden_size, dtype=data_type_fp, device=device),\n                                    requires_grad=False)\n        self.attn_nb = nn.Parameter(torch.empty(self.config.hidden_size, dtype=data_type_fp, device=device),\n                                    requires_grad=False)\n        intm_size_per_partition = self.config.intermediate_size // self.config.mp_size\n        self.inter_w = nn.Parameter(torch.empty(self.config.hidden_size,\n                                                intm_size_per_partition,\n                                                dtype=data_type,\n                                                device=device),\n                                    requires_grad=False)\n        self.inter_b = nn.Parameter(torch.empty(intm_size_per_partition, dtype=data_type_fp, device=device),\n                                    requires_grad=False)\n        self.output_w = nn.Parameter(torch.empty(intm_size_per_partition,\n                                                 self.config.hidden_size,\n                                                 dtype=data_type,\n                                                 device=device),\n                                     requires_grad=False)\n        self.output_b = nn.Parameter(torch.empty(self.config.hidden_size, dtype=data_type_fp, device=device),\n                                     requires_grad=False)\n\n        # used for quantization\n        self.q_scales = q_scales\n        self.q_groups = q_groups * 2 if mlp_extra_grouping else q_groups\n        self.merge_count = int(math.log2(merge_count))\n        self.mp_group = mp_group\n\n        self.mlp_gemm_func = MLPGemmOp(config)\n        self.vector_matmul_func = VectorMatMulOp(config)\n        self.fused_gemm_gelu = GELUGemmOp(config)\n        self.residual_add_func = ResidualAddOp(config)\n\n    def forward(self, input, residual, residual_norm, bias):\n        residual_add = None\n        if self.attn_nw is None:\n            output = self.fused_gemm_gelu(input=residual_norm,\n                                          weight=self.inter_w,\n                                          bias=self.inter_b,\n                                          weight_out=self.output_w)\n        else:\n            output, residual_add = self.mlp_gemm_func(input=input,\n                                                      residual=residual,\n                                                      input_bias=bias,\n                                                      weight_interm=self.inter_w,\n                                                      weight_out=self.output_w,\n                                                      bias=self.inter_b,\n                                                      gamma=self.attn_nw,\n                                                      beta=self.attn_nb)\n        residual = self.residual_add_func(hidden_state=output,\n                                          residual=residual,\n                                          attention_output=input,\n                                          attention_bias=bias if bias is not None else self.output_b,\n                                          final_bias=self.output_b,\n                                          add_bias=bias is not None,\n                                          residual_add=residual_add)\n\n        if self.mp_group is not None and dist.get_world_size(group=self.mp_group) > 1:\n            dist.all_reduce(residual, group=self.mp_group)\n\n        return residual\n", "deepspeed/ops/transformer/inference/triton/triton_matmul_kernel.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport triton\nimport triton.language as tl\nfrom .gelu import gelu_functor\nimport torch\n\nAUTOTUNE_TOP_K = 10\nSKIP_AUTOTUNE = False\n\n\ndef _triton_ops_matmul_early_config_prune(configs, named_args):\n    device = torch.cuda.current_device()  #ignore-cuda\n    capability = torch.cuda.get_device_capability()  #ignore-cuda\n    # BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, num_warps, num_stages\n    dtsize = named_args['A'].element_size()\n    dtype = named_args['A'].dtype\n\n    # 1. make sure we have enough smem\n    pruned_configs = []\n    for config in configs:\n        kw = config.kwargs\n        BLOCK_M, BLOCK_N, BLOCK_K, num_stages = \\\n            kw['BLOCK_M'], kw['BLOCK_N'], kw['BLOCK_K'], config.num_stages\n\n        max_shared_memory = triton.runtime.driver.utils.get_device_properties(device)[\"max_shared_mem\"]\n        required_shared_memory = (BLOCK_M + BLOCK_N) * BLOCK_K * num_stages * dtsize\n        if required_shared_memory <= max_shared_memory:\n            pruned_configs.append(config)\n\n    return pruned_configs\n\n\ndef _fp16_matmul_prune_config(configs, named_args, skip_autotune=SKIP_AUTOTUNE):\n    if skip_autotune:\n        configs = [configs[0]]\n    else:\n        configs = _triton_ops_matmul_early_config_prune(configs, named_args)\n    return configs\n\n\n\"\"\"\nfp16 matmul implementation is adapted from triton matmul:\nhttps://github.com/openai/triton/blob/34817ecc954a6f4ca7b4dfb352fdde1f8bd49ca5/python/triton/ops/matmul.py\n\"\"\"\n\n\n@triton.autotune(\n    configs=[\n        # basic configs for compute-bound matmuls\n        triton.Config({\n            'BLOCK_M': 128,\n            'BLOCK_N': 256,\n            'BLOCK_K': 32,\n            'SPLIT_K': 1\n        }, num_stages=3, num_warps=8),\n        triton.Config({\n            'BLOCK_M': 256,\n            'BLOCK_N': 128,\n            'BLOCK_K': 32,\n            'SPLIT_K': 1\n        }, num_stages=3, num_warps=8),\n        triton.Config({\n            'BLOCK_M': 256,\n            'BLOCK_N': 64,\n            'BLOCK_K': 32,\n            'SPLIT_K': 1\n        }, num_stages=4, num_warps=4),\n        triton.Config({\n            'BLOCK_M': 64,\n            'BLOCK_N': 256,\n            'BLOCK_K': 32,\n            'SPLIT_K': 1\n        }, num_stages=4, num_warps=4),\n        triton.Config({\n            'BLOCK_M': 128,\n            'BLOCK_N': 128,\n            'BLOCK_K': 32,\n            'SPLIT_K': 1\n        }, num_stages=4, num_warps=4),\n        triton.Config({\n            'BLOCK_M': 128,\n            'BLOCK_N': 64,\n            'BLOCK_K': 32,\n            'SPLIT_K': 1\n        }, num_stages=4, num_warps=4),\n        triton.Config({\n            'BLOCK_M': 64,\n            'BLOCK_N': 128,\n            'BLOCK_K': 32,\n            'SPLIT_K': 1\n        }, num_stages=4, num_warps=4),\n        triton.Config({\n            'BLOCK_M': 128,\n            'BLOCK_N': 32,\n            'BLOCK_K': 32,\n            'SPLIT_K': 1\n        }, num_stages=4, num_warps=4),\n        triton.Config({\n            'BLOCK_M': 64,\n            'BLOCK_N': 32,\n            'BLOCK_K': 32,\n            'SPLIT_K': 1\n        }, num_stages=5, num_warps=2),\n    ],\n    key=['CACHE_M', 'CACHE_N', 'CACHE_K'],\n    prune_configs_by={\n        'early_config_prune': _fp16_matmul_prune_config,\n        'perf_model': None,\n        'top_k': AUTOTUNE_TOP_K\n    },\n)\n@triton.heuristics({\n    'EVEN_K': lambda args: args['K'] % (args['BLOCK_K'] * args['SPLIT_K']) == 0,\n})\n@triton.jit\ndef _fp_matmul(\n    A,\n    B,\n    C,\n    M,\n    N,\n    K,\n    bias,\n    stride_am,\n    stride_ak,\n    stride_bk,\n    stride_bn,\n    stride_cm,\n    stride_cn,\n    CACHE_M,\n    CACHE_N,\n    CACHE_K,\n    BLOCK_M: tl.constexpr,\n    BLOCK_N: tl.constexpr,\n    BLOCK_K: tl.constexpr,\n    GROUP_M: tl.constexpr,\n    SPLIT_K: tl.constexpr,\n    EVEN_K: tl.constexpr,\n    ACC_TYPE: tl.constexpr,\n    BIAS_ADD: tl.constexpr,\n    ACTIVATION: tl.constexpr,\n):\n    # matrix multiplication\n    pid = tl.program_id(0)\n    pid_z = tl.program_id(1)\n    grid_m = (M + BLOCK_M - 1) // BLOCK_M\n    grid_n = (N + BLOCK_N - 1) // BLOCK_N\n    # re-order program ID for better L2 performance\n    width = GROUP_M * grid_n\n    group_id = pid // width\n    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + (pid % group_size)\n    pid_n = (pid % width) // (group_size)\n    # do matrix multiplication\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n    rk = pid_z * BLOCK_K + tl.arange(0, BLOCK_K)\n    # pointers\n    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)\n    for k in range(K, 0, -BLOCK_K * SPLIT_K):\n        if EVEN_K:\n            a = tl.load(A)\n            b = tl.load(B)\n        else:\n            a = tl.load(A, mask=rk[None, :] < k, other=0.)\n            b = tl.load(B, mask=rk[:, None] < k, other=0.)\n        acc += tl.dot(a, b)\n        A += BLOCK_K * SPLIT_K * stride_ak\n        B += BLOCK_K * SPLIT_K * stride_bk\n    # bias addition\n    if BIAS_ADD:\n        bias_offset = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n        bias_ptr = bias + bias_offset\n        b = tl.load(bias_ptr, mask=bias_offset < N)\n        acc = acc + b[None, :]\n    # activation\n    if ACTIVATION == \"relu\":\n        acc = tl.where(acc >= 0, acc, 0)\n    elif ACTIVATION == \"leaky_relu\":\n        acc = tl.where(acc >= 0, acc, 0.01 * acc)\n    elif ACTIVATION == \"gelu\":\n        #acc = tl.sigmoid(1.702 * acc) * acc\n        acc = gelu_functor(acc)\n    elif ACTIVATION == \"sigmoid\":\n        acc = tl.sigmoid(acc)  # sigmoid\n    acc = acc.to(C.dtype.element_ty)\n    # rematerialize rm and rn to save registers\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    C = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)\n    mask = (rm < M)[:, None] & (rn < N)[None, :]\n    # handles write-back with reduction-splitting\n    if SPLIT_K == 1:\n        tl.store(C, acc, mask=mask)\n    else:\n        tl.atomic_add(C, acc, mask=mask)\n\n\ndef matmul_4d_prune_config(configs, named_args, skip_autotune=SKIP_AUTOTUNE):\n    if skip_autotune:\n        configs = [configs[0]]\n    else:\n        device = torch.cuda.current_device()  #ignore-cuda\n        capability = torch.cuda.get_device_capability()  #ignore-cuda\n        # BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, num_warps, num_stages\n        dtsize = named_args['a_ptr'].element_size()\n        dtype = named_args['a_ptr'].dtype\n\n        # make sure we have enough smem\n        pruned_configs = []\n        for config in configs:\n            kw = config.kwargs\n            BLOCK_M, BLOCK_N, BLOCK_K, num_stages = \\\n                kw['BLOCK_SIZE_M'], kw['BLOCK_SIZE_N'], kw['BLOCK_SIZE_K'], config.num_stages\n\n            max_shared_memory = triton.runtime.driver.utils.get_device_properties(device)[\"max_shared_mem\"]\n            required_shared_memory = (BLOCK_M + BLOCK_N) * BLOCK_K * num_stages * dtsize\n            if required_shared_memory <= max_shared_memory:\n                pruned_configs.append(config)\n        configs = pruned_configs\n    return configs\n\n\n@triton.autotune(\n    configs=[\n        triton.Config(\n            {\n                \"BLOCK_SIZE_M\": 64,\n                \"BLOCK_SIZE_N\": 64,\n                \"BLOCK_SIZE_K\": 64,\n                \"GROUP_SIZE_M\": 8\n            },\n            num_stages=1,  # this is mainly for unit test, to minimize the share memory usage\n            num_warps=8),\n        triton.Config(\n            {\n                \"BLOCK_SIZE_M\": 128,\n                \"BLOCK_SIZE_N\": 128,\n                \"BLOCK_SIZE_K\": 32,\n                \"GROUP_SIZE_M\": 8,\n            },\n            num_stages=4,\n            num_warps=4,\n        ),\n        triton.Config(\n            {\n                \"BLOCK_SIZE_M\": 128,\n                \"BLOCK_SIZE_N\": 64,\n                \"BLOCK_SIZE_K\": 32,\n                \"GROUP_SIZE_M\": 8,\n            },\n            num_stages=4,\n            num_warps=4,\n        ),\n        triton.Config(\n            {\n                \"BLOCK_SIZE_M\": 64,\n                \"BLOCK_SIZE_N\": 128,\n                \"BLOCK_SIZE_K\": 32,\n                \"GROUP_SIZE_M\": 8,\n            },\n            num_stages=4,\n            num_warps=4,\n        ),\n        triton.Config(\n            {\n                \"BLOCK_SIZE_M\": 128,\n                \"BLOCK_SIZE_N\": 32,\n                \"BLOCK_SIZE_K\": 32,\n                \"GROUP_SIZE_M\": 8,\n            },\n            num_stages=4,\n            num_warps=4,\n        ),\n        triton.Config(\n            {\n                \"BLOCK_SIZE_M\": 64,\n                \"BLOCK_SIZE_N\": 32,\n                \"BLOCK_SIZE_K\": 32,\n                \"GROUP_SIZE_M\": 8,\n            },\n            num_stages=5,\n            num_warps=2,\n        ),\n        triton.Config(\n            {\n                \"BLOCK_SIZE_M\": 32,\n                \"BLOCK_SIZE_N\": 64,\n                \"BLOCK_SIZE_K\": 32,\n                \"GROUP_SIZE_M\": 8,\n            },\n            num_stages=5,\n            num_warps=2,\n        ),\n    ],\n    key=['CACHE_M', 'CACHE_N', 'CACHE_K'],\n    prune_configs_by={\n        'early_config_prune': matmul_4d_prune_config,\n        'perf_model': None,\n        'top_k': AUTOTUNE_TOP_K\n    },\n)\n@triton.jit\ndef matmul_4d_kernel(\n    # Pointers to matrices\n    a_ptr,\n    b_ptr,\n    c_ptr,\n    # Matrix dimensions\n    M,\n    N,\n    K,\n    CACHE_M,\n    CACHE_N,\n    CACHE_K,\n    stride_ab,\n    stride_ah,\n    stride_am,\n    stride_ak,\n    stride_bb,\n    stride_bh,\n    stride_bk,\n    stride_bn,\n    stride_cb,\n    stride_ch,\n    stride_cm,\n    stride_cn,\n    scale,\n    # Meta-parameters\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n    MASK: tl.constexpr,\n):\n    \"\"\"Kernel for computing the matmul C = A x B.\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    head = tl.program_id(axis=1)\n    batch = tl.program_id(axis=2)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    if MASK:\n        if (pid_m + 1) * BLOCK_SIZE_M - 1 < pid_n * BLOCK_SIZE_N:\n            c = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=c_ptr.dtype.element_ty) - float(\"inf\")\n            offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n            offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n            c_ptrs = (c_ptr + batch * stride_cb + head * stride_ch + stride_cm * offs_cm[:, None] +\n                      stride_cn * offs_cn[None, :])\n            tl.store(c_ptrs, c)\n            return\n\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = (a_ptr + batch * stride_ab + head * stride_ah +\n              (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak))\n    b_ptrs = (b_ptr + batch * stride_bb + head * stride_bh +\n              (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn))\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        a_mask = (offs_am[:, None] < M) & (offs_k[None, :] + k < K)\n        b_mask = (offs_k[:, None] + k < K) & (offs_bn[None, :] < N)\n        a = tl.load(a_ptrs, mask=a_mask, other=0.)\n        b = tl.load(b_ptrs, mask=b_mask, other=0.)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    c = accumulator.to(c_ptr.dtype.element_ty)\n    if scale > 0:\n        c = c * scale.to(c_ptr.dtype.element_ty)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    if MASK:\n        c += tl.where(offs_cm[:, None] >= offs_cn[None, :], 0, float(\"-inf\"))\n    c_ptrs = (c_ptr + batch * stride_cb + head * stride_ch + stride_cm * offs_cm[:, None] +\n              stride_cn * offs_cn[None, :])\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n", "deepspeed/ops/transformer/inference/triton/layer_norm.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nimport triton\nimport triton.language as tl\n'''\nlayer-normalization\nmodified the triton kernel in\nhttps://github.com/openai/triton/blob/34817ecc954a6f4ca7b4dfb352fdde1f8bd49ca5/python/tutorials/05-layer-norm.py\n'''\n\n\n@triton.jit\ndef layer_norm_kernel(\n    Out,\n    A,\n    Weight,\n    Bias,\n    stride,\n    N,\n    eps,\n    BLOCK_SIZE: tl.constexpr,\n):\n    # position of elements processed by this program\n    row = tl.program_id(0)\n    Out += row * stride\n    A += row * stride\n    # compute mean\n    mean = 0\n    _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        a = tl.load(A + cols, mask=cols < N, other=0.0).to(tl.float32)\n        _mean += a\n    mean = tl.sum(_mean, axis=0) / N\n    # compute variance\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        a = tl.load(A + cols, mask=cols < N, other=0.0).to(tl.float32)\n        a = tl.where(cols < N, a - mean, 0.0)\n        _var += a * a\n    var = tl.sum(_var, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    # multiply by weight and add bias\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        weight = tl.load(Weight + cols, mask=mask)\n        bias = tl.load(Bias + cols, mask=mask)\n        a = tl.load(A + cols, mask=mask, other=0.0).to(tl.float32)\n        a_hat = (a - mean) * rstd\n        out = a_hat * weight + bias\n        # # write-back\n        tl.store(Out + cols, out, mask=mask)\n\n\n@triton.jit\ndef layer_norm_residual_kernel(\n    Out,\n    A,\n    Residual,\n    ln_input,\n    Weight,\n    Bias,\n    stride,\n    N,\n    eps,\n    BLOCK_SIZE: tl.constexpr,\n):\n    # position of elements processed by this program\n    row = tl.program_id(0)\n    Out += row * stride\n    A += row * stride\n    Residual += row * stride\n    ln_input += row * stride\n    # compute mean\n    mean = 0\n    _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        a = tl.load(A + cols, mask=cols < N, other=0.0).to(tl.float32)\n        res = tl.load(Residual + cols, mask=cols < N, other=0.0).to(tl.float32)\n        a = a + res\n        tl.store(ln_input + cols, a, mask=cols < N)\n        _mean += a\n    mean = tl.sum(_mean, axis=0) / N\n    # compute variance\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        a = tl.load(ln_input + cols, mask=cols < N, other=0.0).to(tl.float32)\n        a = tl.where(cols < N, a - mean, 0.0)\n        _var += a * a\n    var = tl.sum(_var, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    # multiply by weight and add bias\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        weight = tl.load(Weight + cols, mask=mask)\n        bias = tl.load(Bias + cols, mask=mask)\n        a = tl.load(ln_input + cols, mask=mask, other=0.0).to(tl.float32)\n        a_hat = (a - mean) * rstd\n        out = a_hat * weight + bias\n        # write-back\n        tl.store(Out + cols, out, mask=mask)\n\n\n@triton.jit\ndef layer_norm_residual_bias_kernel(\n    Out,\n    A,\n    Residual,\n    InputBias,\n    ln_input,\n    Weight,\n    Bias,\n    stride,\n    N,\n    eps,\n    BLOCK_SIZE: tl.constexpr,\n):\n    # position of elements processed by this program\n    row = tl.program_id(0)\n    Out += row * stride\n    A += row * stride\n    Residual += row * stride\n    ln_input += row * stride\n    # compute mean\n    mean = 0\n    _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        a = tl.load(A + cols, mask=cols < N, other=0.0).to(tl.float32)\n        res = tl.load(Residual + cols, mask=cols < N, other=0.0).to(tl.float32)\n        b = tl.load(InputBias + cols, mask=cols < N, other=0.0).to(tl.float32)\n        a = a + b + res\n        tl.store(ln_input + cols, a, mask=cols < N)\n        _mean += a\n    mean = tl.sum(_mean, axis=0) / N\n    # compute variance\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        a = tl.load(ln_input + cols, mask=cols < N, other=0.0).to(tl.float32)\n        a = tl.where(cols < N, a - mean, 0.0)\n        _var += a * a\n    var = tl.sum(_var, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    # multiply by weight and add bias\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        weight = tl.load(Weight + cols, mask=mask)\n        bias = tl.load(Bias + cols, mask=mask)\n        a = tl.load(ln_input + cols, mask=mask, other=0.0).to(tl.float32)\n        a_hat = (a - mean) * rstd\n        out = a_hat * weight + bias\n        # write-back\n        tl.store(Out + cols, out, mask=mask)\n\n\ndef layer_norm(a, weight, bias, eps):\n    assert a.is_contiguous()\n    assert weight.is_contiguous()\n    assert bias.is_contiguous()\n\n    # allocate output\n    out = torch.empty_like(a)\n    # reshape input data into 2D tensor\n    a_arg = a.view(-1, a.shape[-1])\n    M, N = a_arg.shape\n    # Less than 64KB per feature: enqueue fused kernel\n    MAX_FUSED_SIZE = 65536 // a.element_size()\n    BLOCK_SIZE = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\n    BLOCK_SIZE = max(BLOCK_SIZE, 128)\n    BLOCK_SIZE = min(BLOCK_SIZE, 4096)\n    BLOCK_SIZE = BLOCK_SIZE if N <= 4096 else 8192\n    # heuristics for number of warps\n    num_warps = min(max(BLOCK_SIZE // 256, 1), 8)\n    layer_norm_kernel[(M, )](\n        out,\n        a_arg,\n        weight,\n        bias,\n        a_arg.stride(0),\n        N,\n        eps,\n        BLOCK_SIZE=BLOCK_SIZE,\n        num_warps=num_warps,\n    )\n    return out\n\n\ndef layer_norm_residual(a, input_bias, residual, weight, bias, eps):\n    assert a.is_contiguous()\n    assert weight.is_contiguous()\n    assert bias.is_contiguous()\n    assert residual.is_contiguous()\n\n    # allocate output and scratch-pad for residual addition\n    out = torch.empty_like(a)\n    ln_input = torch.empty_like(a)\n    # reshape input data into 2D tensor\n    a_arg = a.view(-1, a.shape[-1])\n    residual = residual.view(-1, residual.shape[-1])\n    M, N = a_arg.shape\n    # Less than 64KB per feature: enqueue fused kernel\n    MAX_FUSED_SIZE = 65536 // a.element_size()\n    BLOCK_SIZE = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\n    BLOCK_SIZE = max(BLOCK_SIZE, 128)\n    BLOCK_SIZE = min(BLOCK_SIZE, 4096)\n    BLOCK_SIZE = BLOCK_SIZE if N <= 4096 else 8192\n    # heuristics for number of warps\n    num_warps = min(max(BLOCK_SIZE // 256, 1), 8)\n    if input_bias is None:\n        layer_norm_residual_kernel[(M, )](\n            out,\n            a_arg,\n            residual,\n            ln_input,\n            weight,\n            bias,\n            a_arg.stride(0),\n            N,\n            eps,\n            BLOCK_SIZE=BLOCK_SIZE,\n            num_warps=num_warps,\n        )\n    else:\n        layer_norm_residual_bias_kernel[(M, )](\n            out,\n            a_arg,\n            residual,\n            input_bias,\n            ln_input,\n            weight,\n            bias,\n            a_arg.stride(0),\n            N,\n            eps,\n            BLOCK_SIZE=BLOCK_SIZE,\n            num_warps=num_warps,\n        )\n    return out\n", "deepspeed/ops/transformer/inference/triton/ops.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport deepspeed\nfrom deepspeed.ops.op_builder import InferenceBuilder\nimport deepspeed.ops.transformer.inference.triton.matmul_ext as matmul_ext\nfrom deepspeed.ops.transformer.inference.triton.layer_norm import layer_norm, layer_norm_residual\n\ninference_module = None\n\n\ndef vector_matmul_func(input, weight, async_op, q_scale, q_int8, transposed_mode):\n    assert not transposed_mode and not async_op and not q_int8\n    return matmul_ext.matmul(input, weight, bias=None, activation=\"\", use_triton=True)\n\n\ndef fused_gemm_gelu(input,\n                    weight,\n                    weight_scale,\n                    bias,\n                    weight_out,\n                    weight_out_scale,\n                    epsilon,\n                    pre_layer_norm,\n                    q_int8,\n                    async_op,\n                    transposed_mode,\n                    use_triton_ln=True):\n    assert not transposed_mode\n\n    # activation\n    activation = \"gelu\"\n\n    # intermediate fc in FF\n    intm_out = matmul_ext.matmul(input, weight, bias=bias, activation=activation, use_triton=True)\n\n    # output fc in FF\n    ff_out = matmul_ext.matmul(\n        intm_out,\n        weight_out,\n        bias=None,\n        activation=\"\",  # bias added layer with residual_add + bias + layerNorm layer\n        use_triton=True)\n    return ff_out\n\n\ndef linear_func(input, weight, bias, add_bias, do_flash_attn, num_heads, transposed_mode=False):\n    assert not transposed_mode and not do_flash_attn\n    qkv_out = matmul_ext.matmul(input, weight, bias=(bias if add_bias else None), activation=\"\", use_triton=True)\n\n    return qkv_out\n\n\ndef mlp_gemm_func(input,\n                  residual,\n                  input_bias,\n                  weight_interm,\n                  weight_out,\n                  bias,\n                  gamma,\n                  beta,\n                  epsilon,\n                  pre_layer_norm,\n                  mlp_after_attn,\n                  weight_interm_scale,\n                  weight_out_scale,\n                  q_int8,\n                  mlp_act_func_type,\n                  transposed_mode,\n                  use_triton_ln=True):\n    assert not transposed_mode\n\n    # residual add and layerNorm after attention\n    if use_triton_ln:\n        mlp_input = layer_norm_residual(input, input_bias, residual, gamma, beta, epsilon)\n    else:\n        global inference_module\n        if inference_module is None:\n            inference_module = InferenceBuilder().load()\n        mlp_input = inference_module._layer_norm_residual(input, input_bias, residual, gamma, beta, epsilon)\n\n    # activation\n    if deepspeed.utils.types.ActivationFuncType(mlp_act_func_type) == deepspeed.utils.types.ActivationFuncType.GELU:\n        activation = \"gelu\"\n    elif deepspeed.utils.types.ActivationFuncType(mlp_act_func_type) == deepspeed.utils.types.ActivationFuncType.ReLU:\n        activation = \"relu\"\n    else:\n        activation = \"\"\n\n    # intermediate fc in FF\n    intm_out = matmul_ext.matmul(mlp_input, weight_interm, bias=bias, activation=activation, use_triton=True)\n    # output fc in FF\n    ff_out = matmul_ext.matmul(\n        intm_out,\n        weight_out,\n        bias=None,\n        activation=\"\",  # bias added layer with residual_add + bias + layerNorm layer\n        use_triton=True)\n\n    return ff_out, mlp_input\n\n\ndef qkv_gemm_func(\n    input,\n    weight,\n    q_scale,\n    bias,\n    gamma,\n    beta,\n    epsilon,\n    add_bias,\n    q_int8,\n    transposed_mode=False,\n    use_triton_ln=True,\n):\n\n    assert not transposed_mode\n    # residual add and layerNorm after attention\n    if use_triton_ln:\n        qkv_input = layer_norm(input, gamma, beta, epsilon)\n    else:\n        global inference_module\n        if inference_module is None:\n            inference_module = InferenceBuilder().load()\n        qkv_input = inference_module.layer_norm(input, gamma, beta, epsilon)\n\n    qkv_out = matmul_ext.matmul(qkv_input, weight, bias=(bias if add_bias else None), activation=\"\", use_triton=True)\n\n    return qkv_out, qkv_input\n", "deepspeed/ops/transformer/inference/triton/matmul_ext.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nimport triton\nimport os\nfrom filelock import FileLock\nimport deepspeed.ops.transformer.inference.triton.triton_matmul_kernel as triton_matmul_kernel\nimport pickle\nfrom io import open\nimport deepspeed\nfrom pathlib import Path\nimport atexit\nimport subprocess\n\n\n# -----------------------------------------------------------------------------\n# util class/functions for triton\ndef is_nfs_path(path):\n    # Normalize the path to get the absolute path\n    path = os.path.abspath(path)\n\n    # Use the 'df' command to find the file system type for the given path\n    try:\n        output = subprocess.check_output(['df', '-T', path], encoding='utf-8')\n    except subprocess.CalledProcessError:\n        return False  # Command failed\n\n    # Process the output of 'df -T' to check for 'nfs' in the filesystem type column\n    lines = output.strip().split('\\n')\n    if len(lines) > 1:  # The first line is headers\n        fs_type = lines[1].split()[1].lower()  # File system type is the second column\n        return 'nfs' in fs_type\n    return False\n\n\nclass TritonCacheDir:\n    _warning_printed = False\n\n    @staticmethod\n    def default_cache_dir():\n        tmp_path = os.path.join(Path.home(), \".triton\", \"autotune\")\n        if is_nfs_path(tmp_path) and not TritonCacheDir._warning_printed:\n            print(\n                f\"Warning: The default cache directory for DeepSpeed Triton autotune, {tmp_path}, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\"\n            )\n            TritonCacheDir._warning_printed = True\n        return tmp_path\n\n\ndef bias_add_activation(C, bias=None, activation=\"\"):\n    if bias is not None:\n        C += bias\n    # activation\n    if activation == \"relu\":\n        relu = torch.nn.Relu()\n        C = relu(C)\n    elif activation == \"leaky_relu\":\n        leaky_relu = torch.nn.LeakyReLU(0.01)\n        C = leaky_relu(C)\n    elif activation == \"gelu\":\n        sigmoid = torch.nn.Sigmoid()\n        C = sigmoid(1.702 * C) * C\n    elif activation == \"sigmoid\":\n        sigmoid = torch.nn.Sigmoid()\n        C = sigmoid(C)\n    return C\n\n\nclass AutotuneCacheManager:\n    \"\"\"\n        Cache manager for autotune\n    \"\"\"\n\n    def __init__(self, key):\n        self.key = key\n        self.file_path = None\n        self.lock_path = None\n        # if caching is enabled, get the lock and bin path\n        self.cache_dir = os.environ.get('TRITON_CACHE_DIR', TritonCacheDir.default_cache_dir())\n        if self.cache_dir:\n            os.makedirs(self.cache_dir, exist_ok=True)\n        if self.cache_dir:\n            self.file_path = os.path.join(self.cache_dir, self.key + \".pickle\")\n            self.lock_path = self.file_path + \".lock\"\n\n    def has_file(self):\n        return self.file_path and os.path.exists(self.file_path)\n\n    def put(self, table):\n        if self.file_path:\n            assert self.lock_path is not None\n            with FileLock(self.lock_path):\n                with open(self.file_path + \".tmp\", 'wb') as handle:\n                    pickle.dump(table, handle)\n                os.rename(self.file_path + \".tmp\", self.file_path)\n\n    def load(self):\n        if os.path.exists(self.file_path):\n            with open(self.file_path, 'rb') as handle:\n                loaded_dict = pickle.load(handle)\n            return loaded_dict\n        else:\n            return None\n\n\n# -----------------------------------------------------------------------------\n# triton matmul class\n\n\nclass MatmulExt(torch.autograd.Function):\n    \"\"\"\n        a wrapper class that can call different triton matmul kernels depending on the input parameters\n    \"\"\"\n\n    @staticmethod\n    def forward(A, B, bias=None, activation=\"\", use_triton=True, update_autotune_table=False):\n        \"\"\"\n            A: input, activation matrix A\n            B: input, weight matrix B\n        \"\"\"\n        matmul = None\n        quantize_activation = False\n        Batch = 0\n\n        if len(A.shape) == 3:  # if A is 3d-tensor where batch index is given as 0-axis\n            assert A.is_contiguous(), \"matrix A must be contiguous\"\n            Batch, M, K = A.shape\n            A = A.view(-1, K)\n\n        # fp16 activation and fp16 weight matmul into fp16 output\n        matmul = fp16_matmul\n        C = matmul.forward(A, B, use_triton=use_triton, bias=bias, activation=activation)\n\n        if matmul and update_autotune_table:\n            matmul._update_autotune_table()\n\n        if Batch > 0:\n            C = C.view(Batch, M, -1)\n\n        return C\n\n\nclass TritonMatmul(torch.autograd.Function):\n    \"\"\"\n        triton matmul kernel superclass\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def _ref_forward(A, B, ref_dtype=torch.float32):\n        C = torch.matmul(A.type(ref_dtype), B.type(ref_dtype))\n        return C\n\n    @staticmethod\n    def _read_autotune_table(cache_key, triton_kernel):\n        cache_manager = AutotuneCacheManager(cache_key)\n        table = cache_manager.load()\n        if table:\n            triton_kernel.cache = table\n\n    @staticmethod\n    def _write_autotune_table(cache_key, triton_kernel):\n        cache_manager = AutotuneCacheManager(cache_key)\n        cache_manager.put(triton_kernel.cache)\n\n    @staticmethod\n    def _update_autotune_table(cache_key, triton_kernel):\n        cache_manager = AutotuneCacheManager(cache_key)\n        autotune_table = cache_manager.load()\n        if autotune_table is None:\n            autotune_table = dict()\n        autotune_table.update(triton_kernel.cache)  # always overwrite with the new autotune results\n        cache_manager = AutotuneCacheManager(cache_key)\n        cache_manager.put(autotune_table)\n\n    @staticmethod\n    def forward(\n            A,\n            B,\n            ref_dtype=torch.float32,  # fp32 only\n            bias=None,\n            activation=\"\"):\n        C = torch.matmul(A.type(ref_dtype), B.type(ref_dtype))\n        C = bias_add_activation(C, bias, activation)\n        return C\n\n\nclass Fp16Matmul(TritonMatmul):\n    \"\"\"\n        fp16 matrix multiplication kernel\n        dtypes: fp16 x fp16 = fp16\n    \"\"\"\n\n    _2d_kernel = triton_matmul_kernel._fp_matmul\n    _4d_kernel = triton_matmul_kernel.matmul_4d_kernel\n    _cache_stride = 32\n\n    def __init__(self, read_cache=True):\n        super().__init__()\n        if read_cache:\n            __class__._read_autotune_table()\n\n    def skip_autotune(self):\n        __class__._2d_kernel.configs = [__class__._2d_kernel.configs[0]]\n        __class__._4d_kernel.configs = [__class__._4d_kernel.configs[0]]\n\n    @staticmethod\n    def forward(A, B, use_triton=True, bias=None, activation=\"\"):\n        if use_triton:\n            device = A.device\n            # handle non-contiguous inputs if necessary\n            if A.stride(0) > 1 and A.stride(1) > 1:\n                A = A.contiguous()\n            if B.stride(0) > 1 and B.stride(1) > 1:\n                B = B.contiguous()\n            # checks constraints\n            assert A.shape[1] == B.shape[0], \"incompatible dimensions\"\n            M, K = A.shape\n            _, N = B.shape\n            # allocates output\n            C = torch.empty((M, N), device=device, dtype=A.dtype)\n            # accumulator types\n            ACC_TYPE = triton.language.float32 if A.dtype in [torch.float16, torch.bfloat16, torch.float32\n                                                              ] else triton.language.int32\n            # launch kernel\n            grid = lambda META: (triton.cdiv(M, META['BLOCK_M']) * triton.cdiv(N, META['BLOCK_N']), META['SPLIT_K'])\n            __class__._2d_kernel[grid](A,\n                                       B,\n                                       C,\n                                       M,\n                                       N,\n                                       K,\n                                       bias,\n                                       A.stride(0),\n                                       A.stride(1),\n                                       B.stride(0),\n                                       B.stride(1),\n                                       C.stride(0),\n                                       C.stride(1),\n                                       M // __class__._cache_stride,\n                                       N // __class__._cache_stride,\n                                       K // __class__._cache_stride,\n                                       GROUP_M=8,\n                                       ACC_TYPE=ACC_TYPE,\n                                       BIAS_ADD=(0 if bias is None else 1),\n                                       ACTIVATION=activation)\n        else:\n            C = torch.matmul(A, B)\n        return C\n\n    @staticmethod\n    def _matmul_4d(a, b):\n        assert a.shape[-1] == b.shape[-2], \"incompatible dimensions\"\n        assert a.is_contiguous(), \"matrix A must be contiguous\"\n        assert b.is_contiguous(), \"matrix B must be contiguous\"\n\n        B, H, M, K = a.shape\n        B, H, K, N = b.shape\n\n        assert K > 1, \"inner-product dimension K should be larger than 1\"\n\n        c = torch.empty((B, H, M, N), device=a.device, dtype=a.dtype)\n\n        grid = lambda META: (\n            triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\n            H,\n            B,\n        )\n\n        __class__._4d_kernel[grid](\n            a,\n            b,\n            c,\n            M,\n            N,\n            K,\n            M // __class__._cache_stride,\n            N // __class__._cache_stride,\n            K // __class__._cache_stride,\n            a.stride(0),\n            a.stride(1),\n            a.stride(2),\n            a.stride(3),\n            b.stride(0),\n            b.stride(1),\n            b.stride(2),\n            b.stride(3),\n            c.stride(0),\n            c.stride(1),\n            c.stride(2),\n            c.stride(3),\n            scale=-1.0,\n            MASK=False,\n        )\n        return c\n\n    @staticmethod\n    def _score_4d_matmul(input, head_size, input_mask, scale=-1.0):\n        assert input.is_contiguous(), \"matrix input must be contiguous\"\n\n        batches = input.shape[0]\n        d_model = input.shape[-1] // 3\n        num_of_heads = d_model // head_size\n\n        q = input[:, :, :d_model]\n        k = input[:, :, d_model:d_model * 2]\n\n        q = q.view(batches, -1, num_of_heads, head_size)\n        k = k.view(batches, -1, num_of_heads, head_size)\n\n        # checks constraints\n        assert q.shape == k.shape, \"incompatible dimensions\"\n        B, M, H, K = q.shape\n        B, N, H, K = k.shape\n\n        assert K > 1, \"inner-product dimension K should be larger than 1\"\n\n        # allocates output\n        output = torch.empty((B, H, M, N), device=q.device, dtype=q.dtype)\n        grid = lambda META: (\n            triton.cdiv(M, META[\"BLOCK_SIZE_M\"]) * triton.cdiv(N, META[\"BLOCK_SIZE_N\"]),\n            H,\n            B,\n        )\n        __class__._4d_kernel[grid](\n            q,\n            k,\n            output,\n            M,\n            N,\n            K,\n            M // __class__._cache_stride,\n            N // __class__._cache_stride,\n            K // __class__._cache_stride,\n            q.stride(0),\n            q.stride(2),\n            q.stride(1),\n            q.stride(3),\n            k.stride(0),\n            k.stride(2),\n            k.stride(3),\n            k.stride(1),\n            output.stride(0),\n            output.stride(1),\n            output.stride(2),\n            output.stride(3),\n            scale=scale,\n            MASK=False,\n        )\n        return output\n\n    @staticmethod\n    def _context_4d_matmul(prob, input, head_size):\n        assert prob.is_contiguous(), \"matrix prob must be contiguous\"\n        assert input.is_contiguous(), \"matrix input must be contiguous\"\n\n        batches = input.shape[0]\n        d_model = input.shape[-1] // 3\n        num_of_heads = d_model // head_size\n\n        v = input[:, :, d_model * 2:]\n\n        v = v.view(batches, -1, num_of_heads, head_size)\n\n        # checks constraints\n        assert (prob.shape[0] == v.shape[0] and prob.shape[1] == v.shape[2] and prob.shape[2] == v.shape[1]\n                and prob.shape[3] == v.shape[1]), \"incompatible dimensions\"\n        B, H, M, K = prob.shape\n        B, K, H, N = v.shape\n\n        assert K > 1, \"inner-product dimension K should be larger than 1\"\n\n        # allocates output\n        output = torch.empty((B, M, H, N), device=v.device, dtype=v.dtype)\n        grid = lambda META: (\n            triton.cdiv(M, META[\"BLOCK_SIZE_M\"]) * triton.cdiv(N, META[\"BLOCK_SIZE_N\"]),\n            H,\n            B,\n        )\n\n        __class__._4d_kernel[grid](\n            prob,\n            v,\n            output,\n            M,\n            N,\n            K,\n            M // __class__._cache_stride,\n            N // __class__._cache_stride,\n            K // __class__._cache_stride,\n            prob.stride(0),\n            prob.stride(1),\n            prob.stride(2),\n            prob.stride(3),\n            v.stride(0),\n            v.stride(2),\n            v.stride(1),\n            v.stride(3),\n            # Here we also transpose the output when writing to memory.\n            output.stride(0),\n            output.stride(2),\n            output.stride(1),\n            output.stride(3),\n            scale=-1,\n            MASK=False,\n        )\n        return output.view(batches, -1, d_model)\n\n    @staticmethod\n    def _ref_forward(A, B, ref_dtype=torch.float32, bias=None, activation=\"\"):\n        C = torch.matmul(A.type(ref_dtype), B.type(ref_dtype))\n        C = bias_add_activation(C, bias, activation)\n        return C\n\n    @staticmethod\n    def _check_parity(A,\n                      B,\n                      output_dtype,\n                      SA=None,\n                      SB=None,\n                      qblock_size=None,\n                      ref_dtype=torch.float32,\n                      tol=0.01,\n                      use_triton=True,\n                      bias=None,\n                      activation=\"\"):\n        torch_output = __class__._ref_forward(A, B, ref_dtype=ref_dtype, bias=bias, activation=activation)\n        triton_output = __class__.forward(A, B, use_triton=use_triton, bias=bias, activation=activation)\n        assert torch.allclose(triton_output.cpu().type(torch_output.dtype), torch_output.cpu(), rtol=tol)\n        print(f\"{__class__.__name__}: PASSed the parity check\")\n        return triton_output, torch_output\n\n    @staticmethod\n    def _read_autotune_table():\n        TritonMatmul._read_autotune_table(__class__.__name__ + \"_2d_kernel\", __class__._2d_kernel)\n        TritonMatmul._read_autotune_table(__class__.__name__ + \"_4d_kernel\", __class__._4d_kernel)\n\n    @staticmethod\n    def _write_autotune_table():\n        TritonMatmul._write_autotune_table(__class__.__name__ + \"_2d_kernel\", __class__._2d_kernel)\n        TritonMatmul._write_autotune_table(__class__.__name__ + \"_4d_kernel\", __class__._4d_kernel)\n\n    @staticmethod\n    def _update_autotune_table():\n        TritonMatmul._update_autotune_table(__class__.__name__ + \"_2d_kernel\", __class__._2d_kernel)\n        TritonMatmul._update_autotune_table(__class__.__name__ + \"_4d_kernel\", __class__._4d_kernel)\n\n\n# -----------------------------------------------------------------------------\n# mapping\nif deepspeed.HAS_TRITON:\n    fp16_matmul = Fp16Matmul()\n    matmul = MatmulExt.forward\n    matmul_4d = fp16_matmul._matmul_4d\n    score_4d_matmul = fp16_matmul._score_4d_matmul\n    context_4d_matmul = fp16_matmul._context_4d_matmul\nelse:\n    fp16_matmul = None\n    matmul = None\n    matmul_4d = None\n    score_4d_matmul = None\n    context_4d_matmul = None\n\n\n@atexit.register\ndef matmul_ext_update_autotune_table():\n    if deepspeed.HAS_TRITON:\n        fp16_matmul._update_autotune_table()\n", "deepspeed/ops/transformer/inference/triton/softmax.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nimport triton\nimport triton.language as tl\n'''\nsoftmax\nmodified the triton kernel in\nhttps://github.com/openai/triton/blob/34817ecc954a6f4ca7b4dfb352fdde1f8bd49ca5/python/tutorials/02-fused-softmax.py\n'''\n\n\n@triton.jit\ndef softmax_kernel(output_ptr, input_ptr, stride, n_cols, BLOCK_SIZE: tl.constexpr):\n    row_idx = tl.program_id(0)\n    row_start_ptr = input_ptr + row_idx * stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    input_ptrs = row_start_ptr + col_offsets\n    row = tl.load(input_ptrs, mask=col_offsets < n_cols, other=-float('inf')).to(tl.float32)\n    row_minus_max = row - tl.max(row, axis=0)\n    numerator = tl.exp(row_minus_max)\n    denominator = tl.sum(numerator, axis=0)\n    softmax_output = numerator / denominator\n    output_row_start_ptr = output_ptr + row_idx * stride\n    output_ptrs = output_row_start_ptr + col_offsets\n    tl.store(output_ptrs, softmax_output, mask=col_offsets < n_cols)\n\n\n@triton.jit\ndef masked_softmax_kernel(output_ptr, input_ptr, stride, mask_ptr, mask_stride, n_cols, BLOCK_SIZE: tl.constexpr):\n    row_idx = tl.program_id(0)\n    row_start_ptr = input_ptr + row_idx * stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    input_ptrs = row_start_ptr + col_offsets\n    mask_ptrs = mask_ptr + col_offsets + row_idx * mask_stride  # mask_stride is 0 for 1d mask\n    row = tl.load(input_ptrs, mask=col_offsets < n_cols, other=-float('inf')).to(tl.float32)\n    mask = tl.load(mask_ptrs, mask=col_offsets < n_cols, other=0).to(tl.float32)\n    row_minus_max = row - tl.max(row, axis=0)\n    row_minus_max = row_minus_max + mask\n    numerator = tl.exp(row_minus_max)\n    denominator = tl.sum(numerator, axis=0)\n    softmax_output = numerator / denominator\n    output_row_start_ptr = output_ptr + row_idx * stride\n    output_ptrs = output_row_start_ptr + col_offsets\n    tl.store(output_ptrs, softmax_output, mask=col_offsets < n_cols)\n\n\ndef softmax(input: torch.Tensor, mask: torch.Tensor = None, dim=-1) -> torch.Tensor:\n    assert input.is_contiguous()\n    assert (dim == -1) or (dim == len(input.shape) - 1), \"Only dim=-1 is supported\"\n\n    use_mask = False if mask is None else True\n    input_arg = input.view(-1, input.shape[-1])\n    n_rows, n_cols = input_arg.shape\n    BLOCK_SIZE = max(triton.next_power_of_2(n_cols), 2)\n    num_warps = 4\n    if BLOCK_SIZE >= 2048:\n        num_warps = 8\n    if BLOCK_SIZE >= 4096:\n        num_warps = 16\n    # Allocate output\n    output = torch.empty_like(input)\n    if use_mask:\n        assert mask.is_contiguous()\n        mask = mask.view(-1, mask.shape[-1])\n        mask_stride = mask.shape[-1] if mask.shape[-2] > 1 else 0\n        masked_softmax_kernel[(n_rows, )](\n            output,\n            input,\n            input_arg.stride(0),\n            mask,\n            mask_stride,\n            n_cols,\n            num_warps=num_warps,\n            BLOCK_SIZE=BLOCK_SIZE,\n        )\n    else:\n        softmax_kernel[(n_rows, )](\n            output,\n            input,\n            input_arg.stride(0),\n            n_cols,\n            num_warps=num_warps,\n            BLOCK_SIZE=BLOCK_SIZE,\n        )\n    return output\n", "deepspeed/ops/transformer/inference/triton/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .residual_add import residual_add_bias\nfrom .layer_norm import layer_norm, layer_norm_residual\nfrom .gelu import gelu\nfrom .softmax import softmax\nfrom .ops import *\nfrom .matmul_ext import fp16_matmul, matmul_4d, score_4d_matmul, context_4d_matmul\n", "deepspeed/ops/transformer/inference/triton/residual_add.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nimport triton\nimport triton.language as tl\nfrom deepspeed.accelerator import get_accelerator\n\n\n@triton.jit\ndef residual_add_bias_kernel(\n    hidden_state_ptr,\n    residual_ptr,\n    attn_output_ptr,\n    hidden_state_size,\n    attn_bias_ptr,\n    final_bias_ptr,\n    bias_size,\n    output_ptr,\n    mp_size: tl.constexpr,\n    mlp_after_attn: tl.constexpr,\n    pre_attn_norm: tl.constexpr,\n    add_attn_bias: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    pid = tl.program_id(axis=0)\n\n    block_start = pid * BLOCK_SIZE\n\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < hidden_state_size\n\n    bias_offsets = offsets % bias_size\n    bias_mask = bias_offsets < bias_size\n\n    tl_hidden_state = tl.load(hidden_state_ptr + offsets, mask=mask)\n    tl_residual = tl.load(residual_ptr + offsets, mask=mask)\n    tl_attn_output = tl.load(attn_output_ptr + offsets, mask=mask)\n    tl_attn_bias = tl.load(attn_bias_ptr + bias_offsets, mask=bias_mask)\n    tl_final_bias = tl.load(final_bias_ptr + bias_offsets, mask=bias_mask)\n\n    if mlp_after_attn:\n        if pre_attn_norm:\n            output = tl_hidden_state + (tl_residual + tl_final_bias + tl_attn_output + tl_attn_bias) / mp_size\n        else:\n            output = tl_hidden_state + tl_residual + tl_final_bias\n    else:\n        output = tl_hidden_state + tl_attn_output + (tl_residual + tl_final_bias) / mp_size\n        if add_attn_bias:\n            output += tl_attn_bias / mp_size\n\n    tl.store(output_ptr + offsets, output, mask=mask)\n\n\ndef residual_add_bias(hidden_state: torch.Tensor, residual: torch.Tensor, attn_output: torch.Tensor,\n                      attn_bias: torch.Tensor, final_bias: torch.Tensor, mp_size: int, mlp_after_attn: bool,\n                      add_attn_bias: bool, pre_attn_norm: bool):\n    # check that all tensors are on the same device\n    assert get_accelerator().on_accelerator(hidden_state) \\\n        and get_accelerator().on_accelerator(residual) \\\n        and get_accelerator().on_accelerator(attn_output) \\\n        and get_accelerator().on_accelerator(attn_bias) \\\n        and get_accelerator().on_accelerator(final_bias)\n\n    # check that all tensors have the same dtype\n    assert hidden_state.dtype == residual.dtype == attn_output.dtype \\\n        == attn_bias.dtype == final_bias.dtype\n\n    # check that all tensors have the right shape\n    assert hidden_state.shape == residual.shape == attn_output.shape\n    assert attn_bias.shape == final_bias.shape\n    assert attn_bias.shape[0] == hidden_state.shape[2]\n\n    output = torch.empty_like(hidden_state)\n\n    hidden_state_size = output.numel()\n    bias_size = attn_bias.numel()\n\n    grid = lambda meta: (triton.cdiv(hidden_state_size, meta['BLOCK_SIZE']), )\n\n    residual_add_bias_kernel[grid](hidden_state, residual, attn_output, hidden_state_size,\\\n                    attn_bias, final_bias, bias_size, output, mp_size, mlp_after_attn, pre_attn_norm, \\\n                    add_attn_bias, \\\n                    BLOCK_SIZE=1024)\n\n    return output\n", "deepspeed/ops/random_ltd/dropping_utils.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\n\nfrom deepspeed.ops.op_builder import RandomLTDBuilder\n\"\"\"\nReturns:\n    sampled_indices: [layers, batch_size, reserved_length]\n    new_mask: [batch_size, 1, reserved_length, reserved_length]\n\"\"\"\n\nrandom_ltd_module = None\n\n\ndef gpt_sample_tokens(reserved_length: int,\n                      seq_length: int,\n                      batch_size: int,\n                      layers: int = 1,\n                      device: str = 'cpu',\n                      attn_mask: torch.Tensor = None):\n\n    prob_dist = torch.ones((layers * batch_size, seq_length), device=device)\n    sampled_indices = torch.multinomial(prob_dist, reserved_length)\n\n    sampled_indices = sampled_indices.reshape(layers, batch_size, reserved_length).to(torch.int32)\n    global random_ltd_module\n    if random_ltd_module is None:\n        random_ltd_module = RandomLTDBuilder().load()\n    sampled_indices = random_ltd_module.token_sort_(sampled_indices, seq_length)\n\n    # Not certain the optimized kernel is actually better here, cause it kind of screws\n    # with alignment right if the sequence length is not divisible by like 16\n    # new_mask = random_ltd_module.mask_gather_gpt(attn_mask, reserved_length)\n    if attn_mask is not None:\n        new_mask = attn_mask[:, :, :reserved_length, :reserved_length]\n    else:\n        new_mask = None\n\n    return sampled_indices, new_mask\n\n\n\"\"\"\nReturns:\n    sampled_indices: [layers, batch_size, reserved_length]\n    new_mask: [layers, batch_size, 1, reserved_length, reserved_length]\n\"\"\"\n\n\ndef bert_sample_tokens(reserved_length: int,\n                       seq_length: int,\n                       batch_size: int,\n                       layers: int = 1,\n                       device: str = 'cpu',\n                       attn_mask: torch.Tensor = None):\n    assert attn_mask is not None\n    prob_dist = torch.ones((layers * batch_size, seq_length), device=device)\n    sampled_indices = torch.multinomial(prob_dist, reserved_length)\n\n    sampled_indices = sampled_indices.reshape(layers, batch_size, reserved_length).to(torch.int32)\n    global random_ltd_module\n    if random_ltd_module is None:\n        random_ltd_module = RandomLTDBuilder().load()\n\n    sampled_indices = random_ltd_module.token_sort_(sampled_indices, seq_length)\n    dtype = sampled_indices.dtype\n\n    sampled_indices = sampled_indices.to(torch.long)\n    new_mask = []\n    for l in range(layers):\n        tmp_mask_list = []\n        for i in range(batch_size):\n            mask_tmp = attn_mask[i:i + 1, :, sampled_indices[l][i], :]\n            tmp_mask_list.append(mask_tmp[:, :, :, sampled_indices[l][i]])\n        new_mask.append(torch.cat(tmp_mask_list, dim=0))\n\n    return sampled_indices.to(dtype), new_mask\n\n\nclass GatherTokens(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx, activations: torch.Tensor, sorted_indices: torch.Tensor, batch_first: bool):\n        global random_ltd_module\n        if random_ltd_module is None:\n            random_ltd_module = RandomLTDBuilder().load()\n        ctx.save_for_backward(activations, sorted_indices)\n        ctx.batch_first = batch_first\n        return activations, random_ltd_module.token_gather(activations, sorted_indices, batch_first)\n\n    @staticmethod\n    def backward(ctx, a_gradients: torch.Tensor, g_gradients: torch.Tensor):\n\n        g_gradients = g_gradients.contiguous()\n        global random_ltd_module\n        if random_ltd_module is None:\n            random_ltd_module = RandomLTDBuilder().load()\n        activations, sorted_indices = ctx.saved_tensors\n        batch_first = ctx.batch_first\n\n        return random_ltd_module.token_scatter_(a_gradients, g_gradients, sorted_indices, batch_first), None, None\n\n\nclass ScatterTokens(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx, all_activations: torch.Tensor, layer_activations: torch.Tensor, sorted_indices: torch.Tensor,\n                batch_first: bool):\n        global random_ltd_module\n        if random_ltd_module is None:\n            random_ltd_module = RandomLTDBuilder().load()\n        scatter_results = random_ltd_module.token_scatter_(all_activations.clone(), layer_activations, sorted_indices,\n                                                           batch_first)\n\n        ctx.save_for_backward(sorted_indices)\n        ctx.batch_first = batch_first\n        return scatter_results\n\n    @staticmethod\n    def backward(ctx, out_gradients: torch.Tensor):\n\n        out_gradients = out_gradients.contiguous()\n        global random_ltd_module\n        if random_ltd_module is None:\n            random_ltd_module = RandomLTDBuilder().load()\n        sorted_indices, = ctx.saved_tensors\n        batch_first = ctx.batch_first\n\n        ret_val = random_ltd_module.token_gather(out_gradients, sorted_indices, batch_first)\n        return out_gradients, ret_val, None, None\n", "deepspeed/ops/random_ltd/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .dropping_utils import gpt_sample_tokens, bert_sample_tokens, GatherTokens, ScatterTokens\n", "deepspeed/runtime/lr_schedules.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\"\"\"\nImplementation of learning rate schedules.\n\nTaken and modified from PyTorch v1.0.1 source\nhttps://github.com/pytorch/pytorch/blob/v1.1.0/torch/optim/lr_scheduler.py\n\"\"\"\n\nimport argparse\nfrom torch.optim import Optimizer\nimport math\nfrom deepspeed.utils import logger\n\nLR_SCHEDULE = 'lr_schedule'\nLR_RANGE_TEST = 'LRRangeTest'\nONE_CYCLE = 'OneCycle'\nWARMUP_LR = 'WarmupLR'\nWARMUP_DECAY_LR = 'WarmupDecayLR'\nWARMUP_COSINE_LR = 'WarmupCosineLR'\nVALID_LR_SCHEDULES = [LR_RANGE_TEST, ONE_CYCLE, WARMUP_LR, WARMUP_DECAY_LR, WARMUP_COSINE_LR]\n\nLR_RANGE_TEST_MIN_LR = 'lr_range_test_min_lr'\nLR_RANGE_TEST_STEP_RATE = 'lr_range_test_step_rate'\nLR_RANGE_TEST_STEP_SIZE = 'lr_range_test_step_size'\nLR_RANGE_TEST_STAIRCASE = 'lr_range_test_staircase'\n\nEDGE_VALUE = 'edge_value'\nMID_VALUE = 'mid_value'\n\nCYCLE_FIRST_STEP_SIZE = 'cycle_first_step_size'\nCYCLE_FIRST_STAIR_COUNT = 'cycle_first_stair_count'\nCYCLE_SECOND_STEP_SIZE = 'cycle_second_step_size'\nCYCLE_SECOND_STAIR_COUNT = 'cycle_second_stair_count'\nDECAY_STEP_SIZE = 'decay_step_size'\n\nCYCLE_MIN_LR = 'cycle_min_lr'\nCYCLE_MAX_LR = 'cycle_max_lr'\nDECAY_LR_RATE = 'decay_lr_rate'\n\nCYCLE_MIN_MOM = 'cycle_min_mom'\nCYCLE_MAX_MOM = 'cycle_max_mom'\nDECAY_MOM_RATE = 'decay_mom_rate'\n\nWARMUP_MIN_LR = 'warmup_min_lr'\nWARMUP_MAX_LR = 'warmup_max_lr'\nWARMUP_NUM_STEPS = 'warmup_num_steps'\nWARMUP_TYPE = 'warmup_type'\nWARMUP_LOG_RATE = 'log'\nWARMUP_LINEAR_RATE = 'linear'\n\nWARMUP_MIN_RATIO = 'warmup_min_ratio'\nCOS_MIN_RATIO = 'cos_min_ratio'\n\nTOTAL_NUM_STEPS = 'total_num_steps'\n\n\ndef add_tuning_arguments(parser):\n    group = parser.add_argument_group('Convergence Tuning', 'Convergence tuning configurations')\n\n    # LR scheduler\n    group.add_argument('--lr_schedule', type=str, default=None, help='LR schedule for training.')\n\n    # Learning rate range test\n    group.add_argument(\"--lr_range_test_min_lr\", type=float, default=0.001, help='Starting lr value.')\n    group.add_argument(\"--lr_range_test_step_rate\", type=float, default=1.0, help='scaling rate for LR range test.')\n    group.add_argument(\"--lr_range_test_step_size\", type=int, default=1000, help='training steps per LR change.')\n    group.add_argument(\"--lr_range_test_staircase\",\n                       type=bool,\n                       default=False,\n                       help='use staircase scaling for LR range test.')\n\n    # OneCycle schedule\n    group.add_argument(\"--cycle_first_step_size\",\n                       type=int,\n                       default=1000,\n                       help='size of first step of 1Cycle schedule (training steps).')\n    group.add_argument(\"--cycle_first_stair_count\",\n                       type=int,\n                       default=-1,\n                       help='first stair count for 1Cycle schedule.')\n    group.add_argument(\"--cycle_second_step_size\",\n                       type=int,\n                       default=-1,\n                       help='size of second step of 1Cycle schedule (default first_step_size).')\n    group.add_argument(\"--cycle_second_stair_count\",\n                       type=int,\n                       default=-1,\n                       help='second stair count for 1Cycle schedule.')\n    group.add_argument(\"--decay_step_size\",\n                       type=int,\n                       default=1000,\n                       help='size of intervals for applying post cycle decay (training steps).')\n\n    # 1Cycle LR\n    group.add_argument(\"--cycle_min_lr\", type=float, default=0.01, help='1Cycle LR lower bound.')\n    group.add_argument(\"--cycle_max_lr\", type=float, default=0.1, help='1Cycle LR upper bound.')\n    group.add_argument(\"--decay_lr_rate\", type=float, default=0.0, help='post cycle LR decay rate.')\n\n    # 1Cycle Momentum\n    group.add_argument('--cycle_momentum', default=False, action='store_true', help='Enable 1Cycle momentum schedule.')\n    group.add_argument(\"--cycle_min_mom\", type=float, default=0.8, help='1Cycle momentum lower bound.')\n    group.add_argument(\"--cycle_max_mom\", type=float, default=0.9, help='1Cycle momentum upper bound.')\n    group.add_argument(\"--decay_mom_rate\", type=float, default=0.0, help='post cycle momentum decay rate.')\n\n    # Warmup LR\n    group.add_argument('--warmup_min_lr', type=float, default=0, help='WarmupLR minimum/initial LR value')\n    group.add_argument('--warmup_max_lr', type=float, default=0.001, help='WarmupLR maximum LR value.')\n    group.add_argument('--warmup_num_steps', type=int, default=1000, help='WarmupLR step count for LR warmup.')\n    group.add_argument('--warmup_type',\n                       type=str,\n                       default=WARMUP_LOG_RATE,\n                       help='WarmupLR increasing function during warmup')\n\n    # WarmUP cos LR\n    group.add_argument(\"--warmup_min_ratio\", type=float, default=0.01, help='Cosine LR lower bound.')\n    group.add_argument(\"--cos_min_ratio\", type=float, default=0.01, help='Cosine LR lower bound.')\n\n    return parser\n\n\ndef parse_arguments():\n    parser = argparse.ArgumentParser()\n    parser = add_tuning_arguments(parser)\n\n    lr_sched_args, unknown_args = parser.parse_known_args()\n    return lr_sched_args, unknown_args\n\n\ndef override_lr_range_test_params(args, params):\n    if hasattr(args, LR_RANGE_TEST_MIN_LR) and args.lr_range_test_min_lr is not None:\n        params[LR_RANGE_TEST_MIN_LR] = args.lr_range_test_min_lr\n\n    if hasattr(args, LR_RANGE_TEST_STEP_RATE) and args.lr_range_test_step_rate is not None:\n        params[LR_RANGE_TEST_STEP_RATE] = args.lr_range_test_step_rate\n\n    if hasattr(args, LR_RANGE_TEST_STEP_SIZE) and args.lr_range_test_step_size is not None:\n        params[LR_RANGE_TEST_STEP_SIZE] = args.lr_range_test_step_size\n\n    if hasattr(args, LR_RANGE_TEST_STAIRCASE) and args.lr_range_test_staircase is not None:\n        params[LR_RANGE_TEST_STAIRCASE] = args.lr_range_test_staircase\n\n\ndef override_1cycle_params(args, params):\n    if hasattr(args, CYCLE_FIRST_STEP_SIZE) and args.cycle_first_step_size is not None:\n        params[CYCLE_FIRST_STEP_SIZE] = args.cycle_first_step_size\n\n    if hasattr(args, CYCLE_FIRST_STAIR_COUNT) and args.cycle_first_stair_count is not None:\n        params[CYCLE_FIRST_STAIR_COUNT] = args.cycle_first_stair_count\n\n    if hasattr(args, CYCLE_SECOND_STEP_SIZE) and args.cycle_second_step_size is not None:\n        params[CYCLE_SECOND_STEP_SIZE] = args.cycle_second_step_size\n\n    if hasattr(args, CYCLE_SECOND_STAIR_COUNT) and args.cycle_second_stair_count is not None:\n        params[CYCLE_SECOND_STAIR_COUNT] = args.cycle_second_stair_count\n\n    if hasattr(args, DECAY_STEP_SIZE) and args.decay_step_size is not None:\n        params[DECAY_STEP_SIZE] = args.decay_step_size\n\n    # 1Cycle LR params\n    if hasattr(args, CYCLE_MIN_LR) and args.cycle_min_lr is not None:\n        params[CYCLE_MIN_LR] = args.cycle_min_lr\n\n    if hasattr(args, CYCLE_MAX_LR) and args.cycle_max_lr is not None:\n        params[CYCLE_MAX_LR] = args.cycle_max_lr\n\n    if hasattr(args, DECAY_LR_RATE) and args.decay_lr_rate is not None:\n        params[DECAY_LR_RATE] = args.decay_lr_rate\n\n    # 1Cycle MOM params\n    if hasattr(args, CYCLE_MIN_MOM) and args.cycle_min_mom is not None:\n        params[CYCLE_MIN_MOM] = args.cycle_min_mom\n\n    if hasattr(args, CYCLE_MAX_MOM) and args.cycle_max_mom is not None:\n        params[CYCLE_MAX_MOM] = args.cycle_max_mom\n\n    if hasattr(args, DECAY_MOM_RATE) and args.decay_mom_rate is not None:\n        params[DECAY_MOM_RATE] = args.decay_mom_rate\n\n\ndef override_warmupLR_params(args, params):\n    if hasattr(args, WARMUP_MIN_LR) and args.warmup_min_lr is not None:\n        params[WARMUP_MIN_LR] = args.warmup_min_lr\n\n    if hasattr(args, WARMUP_MAX_LR) and args.warmup_max_lr is not None:\n        params[WARMUP_MAX_LR] = args.warmup_max_lr\n\n    if hasattr(args, WARMUP_NUM_STEPS) and args.warmup_num_steps is not None:\n        params[WARMUP_NUM_STEPS] = args.warmup_num_steps\n\n    if hasattr(args, WARMUP_TYPE) and args.warmup_type is not None:\n        params[WARMUP_TYPE] = args.warmup_type\n\n\ndef override_params(args, params):\n    # LR range test params\n    override_lr_range_test_params(args, params)\n\n    # 1Cycle params\n    override_1cycle_params(args, params)\n\n    # WarmupLR params\n    override_warmupLR_params(args, params)\n\n\ndef get_config_from_args(args):\n    if not hasattr(args, LR_SCHEDULE) or args.lr_schedule is None:\n        return None, '--{} not specified on command line'.format(LR_SCHEDULE)\n\n    if not args.lr_schedule in VALID_LR_SCHEDULES:\n        return None, '{} is not supported LR schedule'.format(args.lr_schedule)\n\n    config = {}\n    config['type'] = args.lr_schedule\n    config['params'] = {}\n\n    if args.lr_schedule == LR_RANGE_TEST:\n        override_lr_range_test_params(args, config['params'])\n    elif args.lr_schedule == ONE_CYCLE:\n        override_1cycle_params(args, config['params'])\n    else:\n        override_warmupLR_params(args, config['params'])\n\n    return config, None\n\n\ndef get_lr_from_config(config):\n    if not 'type' in config:\n        return None, 'LR schedule type not defined in config'\n\n    if not 'params' in config:\n        return None, 'LR schedule params not defined in config'\n\n    lr_schedule = config['type']\n    lr_params = config['params']\n\n    if not lr_schedule in VALID_LR_SCHEDULES:\n        return None, '{} is not a valid LR schedule'.format(lr_schedule)\n\n    if lr_schedule == LR_RANGE_TEST:\n        return lr_params[LR_RANGE_TEST_MIN_LR], ''\n    if lr_schedule == ONE_CYCLE:\n        return lr_params[CYCLE_MAX_LR], ''\n    # Warmup LR\n    return lr_params[WARMUP_MAX_LR], ''\n\n\n\"\"\"\nOnly optimizers that are subclass of torch.optim.Optimizer are supported. So check the passed optimizer and wrapped\noptimizer to see if requirement is satisfied.\nTODO: Looking under the hood to examine the wrapped optimizer is a hack that requires a better long-term fix.\n\"\"\"\n\n\ndef get_torch_optimizer(optimizer):\n    if isinstance(optimizer, Optimizer):\n        return optimizer\n\n    if hasattr(optimizer, 'optimizer') and isinstance(optimizer.optimizer, Optimizer):\n        return optimizer.optimizer\n\n    raise TypeError('{} is not a subclass of torch.optim.Optimizer'.format(type(optimizer).__name__))\n\n\nclass LRRangeTest(object):\n    \"\"\"Sets the learning rate of each parameter group according to\n    learning rate range test (LRRT) policy. The policy increases learning\n    rate starting from a base value with a constant frequency, as detailed in\n    the paper `A disciplined approach to neural network hyper-parameters: Part1`_.\n\n    LRRT policy is used for finding maximum LR that trains a model without divergence, and can be used to\n    configure the LR boundaries for Cyclic LR schedules.\n\n    LRRT changes the learning rate after every batch.\n    `step` should be called after a batch has been used for training.\n\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        lr_range_test_min_lr (float or list): Initial learning rate which is the\n            lower boundary in the range test for each parameter group.\n        lr_range_test_step_size (int): Interval of training steps to increase learning rate. Default: 2000\n        lr_range_test_step_rate (float): Scaling rate for range test. Default: 1.0\n        lr_range_test_staircase (bool): Scale in staircase fashion, rather than continuous. Default: False.\n        last_batch_iteration (int): The index of the last batch. This parameter is used when\n            resuming a training job. Since `step()` should be invoked after each\n            batch instead of after each epoch, this number represents the total\n            number of *batches* computed, not the total number of epochs computed.\n            When last_batch_iteration=-1, the schedule is started from the beginning.\n            Default: -1\n\n    Example:\n        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n        >>> scheduler = LRRangeTest(optimizer)\n        >>> data_loader = torch.utils.data.DataLoader(...)\n        >>> for epoch in range(10):\n        >>>     for batch in data_loader:\n        >>>         train_batch(...)\n        >>>         scheduler.step()\n\n        _A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay:\n        https://arxiv.org/abs/1803.09820\n\"\"\"\n\n    def __init__(self,\n                 optimizer: Optimizer,\n                 lr_range_test_min_lr: float = 1e-3,\n                 lr_range_test_step_size: int = 2000,\n                 lr_range_test_step_rate: float = 1.0,\n                 lr_range_test_staircase: bool = False,\n                 last_batch_iteration: int = -1):\n\n        self.optimizer = get_torch_optimizer(optimizer)\n\n        if isinstance(lr_range_test_min_lr, list) or isinstance(lr_range_test_min_lr, tuple):\n            if len(lr_range_test_min_lr) != len(self.optimizer.param_groups):\n                raise ValueError(\"expected {} lr_range_test_min_lr, got {}\".format(len(self.optimizer.param_groups),\n                                                                                   len(lr_range_test_min_lr)))\n            self.min_lr = list(lr_range_test_min_lr)\n        else:\n            self.min_lr = [lr_range_test_min_lr] * len(self.optimizer.param_groups)\n\n        self.step_size = lr_range_test_step_size\n        self.step_rate = lr_range_test_step_rate\n        self.last_batch_iteration = last_batch_iteration\n        self.staircase = lr_range_test_staircase\n        self.interval_fn = self._staircase_interval if lr_range_test_staircase else self._continuous_interval\n\n        if last_batch_iteration == -1:\n            self._update_optimizer(self.min_lr)\n\n    def _staircase_interval(self):\n        return math.floor(float(self.last_batch_iteration + 1) / self.step_size)\n\n    def _continuous_interval(self):\n        return float(self.last_batch_iteration + 1) / self.step_size\n\n    def _get_increase(self):\n        return (1 + self.step_rate * self.interval_fn())\n\n    def get_lr(self):\n        lr_increase = self._get_increase()\n        return [lr_range_test_min_lr * lr_increase for lr_range_test_min_lr in self.min_lr]\n\n    def get_last_lr(self):\n        \"\"\" Return last computed learning rate by current scheduler.\n        \"\"\"\n        assert getattr(self, '_last_lr', None) is not None, \"need to call step() first\"\n        return self._last_lr\n\n    def _update_optimizer(self, group_lrs):\n        for param_group, lr in zip(self.optimizer.param_groups, group_lrs):\n            param_group['lr'] = lr\n\n    def step(self, batch_iteration=None):\n        if batch_iteration is None:\n            batch_iteration = self.last_batch_iteration + 1\n        self.last_batch_iteration = batch_iteration\n        self._update_optimizer(self.get_lr())\n        self._last_lr = [group['lr'] for group in self.optimizer.param_groups]\n\n    def state_dict(self):\n        return {'last_batch_iteration': self.last_batch_iteration}\n\n    def load_state_dict(self, sd):\n        self.last_batch_iteration = sd['last_batch_iteration']\n\n\nclass OneCycle(object):\n    \"\"\"Sets the learning rate of each parameter group according to\n    1Cycle learning rate policy (1CLR). 1CLR is a variation of the\n    Cyclical Learning Rate (CLR) policy that involves one cycle followed by\n    decay. The policy simultaneously cycles the learning rate (and momentum)\n    between two boundaries with a constant frequency, as detailed in\n    the paper `A disciplined approach to neural network hyper-parameters`_.\n\n    1CLR policy changes the learning rate after every batch.\n    `step` should be called after a batch has been used for training.\n\n    This implementation was adapted from the github repo: `pytorch/pytorch`_\n\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        cycle_min_lr (float or list): Initial learning rate which is the\n            lower boundary in the cycle for each parameter group.\n        cycle_max_lr (float or list): Upper learning rate boundaries in the cycle\n            for each parameter group. Functionally,\n            it defines the cycle amplitude (cycle_max_lr - cycle_min_lr).\n            The lr at any cycle is the sum of cycle_min_lr\n            and some scaling of the amplitude; therefore\n            cycle_max_lr may not actually be reached depending on\n            scaling function.\n        decay_lr_rate(float): Decay rate for learning rate. Default: 0.\n        cycle_first_step_size (int): Number of training iterations in the\n            increasing half of a cycle. Default: 2000\n        cycle_second_step_size (int): Number of training iterations in the\n            decreasing half of a cycle. If cycle_second_step_size is None,\n            it is set to cycle_first_step_size. Default: None\n        cycle_first_stair_count(int): Number of stairs in first half of cycle phase. This means\n        lr/mom are changed in staircase fashion. Default 0, means staircase disabled.\n        cycle_second_stair_count(int): Number of stairs in second half of cycle phase. This means\n        lr/mom are changed in staircase fashion. Default 0, means staircase disabled.\n        decay_step_size (int): Intervals for applying decay in decay phase. Default: 0, means no decay.\n        cycle_momentum (bool): If ``True``, momentum is cycled inversely\n            to learning rate between 'cycle_min_mom' and 'cycle_max_mom'.\n            Default: True\n        cycle_min_mom (float or list): Initial momentum which is the\n            lower boundary in the cycle for each parameter group.\n            Default: 0.8\n        cycle_max_mom (float or list): Upper momentum boundaries in the cycle\n            for each parameter group. Functionally,\n            it defines the cycle amplitude (cycle_max_mom - cycle_min_mom).\n            The momentum at any cycle is the difference of cycle_max_mom\n            and some scaling of the amplitude; therefore\n            cycle_min_mom may not actually be reached depending on\n            scaling function. Default: 0.9\n        decay_mom_rate (float): Decay rate for momentum. Default: 0.\n        last_batch_iteration (int): The index of the last batch. This parameter is used when\n            resuming a training job. Since `step()` should be invoked after each\n            batch instead of after each epoch, this number represents the total\n            number of *batches* computed, not the total number of epochs computed.\n            When last_batch_iteration=-1, the schedule is started from the beginning.\n            Default: -1\n\n    Example:\n        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n        >>> scheduler = OneCycle(optimizer, 0.0001, 0.0010)\n        >>> data_loader = torch.utils.data.DataLoader(...)\n        >>> for epoch in range(10):\n        >>>     for batch in data_loader:\n        >>>         train_batch(...)\n        >>>         scheduler.step()\n\n\n    .. _A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay: https://arxiv.org/abs/1803.09820\n    \"\"\"\n\n    def __init__(self,\n                 optimizer,\n                 cycle_min_lr,\n                 cycle_max_lr,\n                 decay_lr_rate=0.,\n                 cycle_first_step_size=2000,\n                 cycle_second_step_size=None,\n                 cycle_first_stair_count=0,\n                 cycle_second_stair_count=None,\n                 decay_step_size=0,\n                 cycle_momentum=True,\n                 cycle_min_mom=0.8,\n                 cycle_max_mom=0.9,\n                 decay_mom_rate=0.,\n                 last_batch_iteration=-1):\n\n        self.optimizer = get_torch_optimizer(optimizer)\n\n        # Initialize cycle shape\n        self._initialize_cycle(cycle_first_step_size, cycle_second_step_size, cycle_first_stair_count,\n                               cycle_second_stair_count, decay_step_size)\n\n        # Initialize cycle lr\n        self._initialize_lr(self.optimizer, cycle_min_lr, cycle_max_lr, decay_lr_rate, last_batch_iteration)\n\n        # Initialize cyclic momentum\n        self.cycle_momentum = cycle_momentum\n        if cycle_momentum:\n            self._initialize_momentum(self.optimizer, cycle_min_mom, cycle_max_mom, decay_mom_rate,\n                                      last_batch_iteration)\n        # Initialize batch iteration tracker\n        self.last_batch_iteration = last_batch_iteration\n\n    # Configure cycle shape\n\n    def _initialize_cycle(self, cycle_first_step_size, cycle_second_step_size, cycle_first_stair_count,\n                          cycle_second_stair_count, decay_step_size):\n        cycle_first_step_size = float(cycle_first_step_size)\n        cycle_second_step_size = float(\n            cycle_second_step_size) if cycle_second_step_size is not None else cycle_first_step_size\n\n        self.total_size = cycle_first_step_size + cycle_second_step_size\n        self.step_ratio = cycle_first_step_size / self.total_size\n        self.first_stair_count = cycle_first_stair_count\n        self.second_stair_count = cycle_first_stair_count if cycle_second_stair_count is None else cycle_second_stair_count\n        self.decay_step_size = decay_step_size\n\n        if math.isclose(self.decay_step_size, 0):\n            self.skip_lr_decay = True\n            self.skip_mom_decay = True\n        else:\n            self.skip_lr_decay = False\n            self.skip_mom_decay = False\n\n    # Configure lr schedule\n    def _initialize_lr(self, optimizer, cycle_min_lr, cycle_max_lr, decay_lr_rate, last_batch_iteration):\n        self.min_lrs = [cycle_min_lr] * len(optimizer.param_groups)\n        if last_batch_iteration == -1:\n            for lr, group in zip(self.min_lrs, optimizer.param_groups):\n                group['lr'] = lr\n\n        self.max_lrs = [cycle_max_lr] * len(optimizer.param_groups)\n        self.decay_lr_rate = decay_lr_rate\n\n        if math.isclose(self.decay_lr_rate, 0):\n            self.skip_lr_decay = True\n\n    # Configure momentum schedule\n    def _initialize_momentum(self, optimizer, cycle_min_mom, cycle_max_mom, decay_mom_rate, last_batch_iteration):\n        if 'betas' not in optimizer.defaults:\n            optimizer_name = type(optimizer).__name__\n            logger.warn(\n                f\"cycle_momentum is disabled because optimizer {optimizer_name} does not support momentum, no betas attribute in defaults\"\n            )\n            self.cycle_momentum = False\n            return\n\n        self.decay_mom_rate = decay_mom_rate\n        self.min_moms = [(cycle_min_mom, 0.99)] * len(optimizer.param_groups)\n        self.max_moms = [(cycle_max_mom, 0.99)] * len(optimizer.param_groups)\n\n        if last_batch_iteration == -1:\n            for momentum, group in zip(self.min_moms, optimizer.param_groups):\n                group['betas'] = momentum\n\n        if math.isclose(self.decay_mom_rate, 0):\n            self.skip_mom_decay = True\n\n    def _get_scale_factor(self):\n        batch_iteration = (self.last_batch_iteration + 1)\n        cycle = math.floor(1 + batch_iteration / self.total_size)\n        x = 1. + batch_iteration / self.total_size - cycle\n        if x <= self.step_ratio:\n            scale_factor = x / self.step_ratio\n        else:\n            scale_factor = (x - 1) / (self.step_ratio - 1)\n\n        return scale_factor\n\n    def _get_cycle_mom(self):\n        scale_factor = self._get_scale_factor()\n        momentums = []\n        for base_betas, max_betas in zip(self.min_moms, self.max_moms):\n            cycle_min_mom = base_betas[0]\n            cycle_max_mom = max_betas[0]\n            base_height = (cycle_max_mom - cycle_min_mom) * scale_factor\n            momentum = cycle_max_mom - base_height\n            momentums.append((momentum, base_betas[1]))\n        return momentums\n\n    def _get_cycle_lr(self):\n        scale_factor = self._get_scale_factor()\n        lrs = []\n        for cycle_min_lr, cycle_max_lr in zip(self.min_lrs, self.max_lrs):\n            base_height = (cycle_max_lr - cycle_min_lr) * scale_factor\n            lr = cycle_min_lr + base_height\n            lrs.append(lr)\n\n        return lrs\n\n    def _get_decay_mom(self, decay_batch_iteration):\n        if self.skip_mom_decay:\n            return self.max_moms\n\n        decay_interval = decay_batch_iteration / self.decay_step_size\n        mom_decay_factor = (1 + self.decay_mom_rate * decay_interval)\n        momentums = [(beta0 * mom_decay_factor, beta1) for beta0, beta1 in self.max_moms]\n\n        return momentums\n\n    def _get_decay_lr(self, decay_batch_iteration):\n        \"\"\"Calculates the learning rate at batch index. This function is used\n        after the cycle completes and post cycle decaying of lr/mom is enabled.\n        This function treats `self.last_batch_iteration` as the last batch index.\n        \"\"\"\n        if self.skip_lr_decay:\n            return self.min_lrs\n\n        decay_interval = decay_batch_iteration / self.decay_step_size\n        lr_decay_factor = (1 + self.decay_lr_rate * decay_interval)\n        lrs = [cycle_min_lr / lr_decay_factor for cycle_min_lr in self.min_lrs]\n\n        return lrs\n\n    def get_lr(self):\n        \"\"\"Calculates the learning rate at batch index. This function treats\n        `self.last_batch_iteration` as the last batch index.\n        \"\"\"\n        if self.last_batch_iteration < self.total_size:\n            return self._get_cycle_lr()\n        return self._get_decay_lr(self.last_batch_iteration - self.total_size + 1)\n\n    def get_mom(self):\n        \"\"\"Calculates the momentum at batch index. This function treats\n        `self.last_batch_iteration` as the last batch index.\n        \"\"\"\n        if not self.cycle_momentum:\n            return None\n\n        if self.last_batch_iteration < self.total_size:\n            return self._get_cycle_mom()\n        return self._get_decay_mom(self.last_batch_iteration - self.total_size + 1)\n\n    def get_last_lr(self):\n        \"\"\" Return last computed learning rate by current scheduler.\n        \"\"\"\n        assert getattr(self, '_last_lr', None) is not None, \"need to call step() first\"\n        return self._last_lr\n\n    def step(self, batch_iteration=None):\n        \"\"\" Updates the optimizer with the learning rate for the last batch index.\n        `self.last_batch_iteration` is treated as the last batch index.\n\n        If self.cycle_momentum is true, also updates optimizer momentum.\n        \"\"\"\n        if batch_iteration is None:\n            batch_iteration = self.last_batch_iteration + 1\n\n        self.last_batch_iteration = batch_iteration\n        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n            param_group['lr'] = lr\n        self._last_lr = [group['lr'] for group in self.optimizer.param_groups]\n\n        if self.cycle_momentum:\n            momentums = self.get_mom()\n            for param_group, momentum in zip(self.optimizer.param_groups, momentums):\n                param_group['betas'] = momentum\n\n    def state_dict(self):\n        return {'last_batch_iteration': self.last_batch_iteration}\n\n    def load_state_dict(self, sd):\n        self.last_batch_iteration = sd['last_batch_iteration']\n\n\nclass WarmupLR(object):\n    \"\"\"Increase the learning rate of each parameter group from min lr to max lr\n        over warmup_num_steps steps, and then fix at max lr.\n\n        Args:\n            optimizer (Optimizer): Wrapped optimizer.\n            warmup_min_lr (float or list): minimum learning rate. Default: 0\n            warmup_max_lr (float or list): maximum learning rate. Default: 0.001\n            warmup_num_steps (int): number of steps to warm up from min_lr to max_lr. Default: 1000\n            warmup_type {\u2018log\u2019, \u2018linear\u2019}: increasing function from min_lr to max_lr during warmup. Default: log\n            last_batch_iteration (int): The index of the last batch. Default: -1.\n        Example:\n            >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n            >>> scheduler = WarmupLR(optimizer)\n            >>> data_loader = torch.utils.data.DataLoader(...)\n            >>> for epoch in range(10):\n            >>>     for batch in data_loader:\n            >>>         train_batch(...)\n            >>>         scheduler.step()\n\n    \"\"\"\n\n    def __init__(self,\n                 optimizer: Optimizer,\n                 warmup_min_lr: float = 0.0,\n                 warmup_max_lr: float = 0.001,\n                 warmup_num_steps: int = 1000,\n                 warmup_type: str = WARMUP_LOG_RATE,\n                 last_batch_iteration: int = -1):\n\n        self.optimizer = get_torch_optimizer(optimizer)\n\n        self.min_lrs = self._format_param(self.optimizer, warmup_min_lr, \"min_lr\")\n        self.max_lrs = self._format_param(self.optimizer, warmup_max_lr, \"max_lr\")\n        self.delta_lrs = [big - small for big, small in zip(self.max_lrs, self.min_lrs)]\n        self.warmup_num_steps = max(2, warmup_num_steps)\n        # Currently only support linear and log function\n        if warmup_type not in {WARMUP_LOG_RATE, WARMUP_LINEAR_RATE}:\n            logger.warning(f\"Using unknown warmup_type: {warmup_type}. The increasing function \"\n                           f\"is set to default (log)\")\n            warmup_type = WARMUP_LOG_RATE\n        self.warmup_type = warmup_type\n        self.inverse_log_warm_up = 1.0 / math.log(self.warmup_num_steps)\n        self.last_batch_iteration = last_batch_iteration\n\n    def get_lr(self):\n        if self.last_batch_iteration < 0:\n            logger.warning(\"Attempting to get learning rate from scheduler before it has started\")\n            return [0.0]\n        gamma = self._get_gamma()\n        return [min_lr + (delta_lr * gamma) for min_lr, delta_lr in zip(self.min_lrs, self.delta_lrs)]\n\n    def get_last_lr(self):\n        \"\"\" Return last computed learning rate by current scheduler.\n        \"\"\"\n        assert getattr(self, '_last_lr', None) is not None, \"need to call step() first\"\n        return self._last_lr\n\n    def step(self, last_batch_iteration=None):\n        if last_batch_iteration is None:\n            last_batch_iteration = self.last_batch_iteration + 1\n        self.last_batch_iteration = last_batch_iteration\n        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n            param_group['lr'] = lr\n        self._last_lr = [group['lr'] for group in self.optimizer.param_groups]\n\n    def state_dict(self):\n        return {'last_batch_iteration': self.last_batch_iteration}\n\n    def load_state_dict(self, sd):\n        self.last_batch_iteration = sd['last_batch_iteration']\n\n    def _get_gamma(self):\n        if self.last_batch_iteration < self.warmup_num_steps:\n            if self.warmup_type == WARMUP_LOG_RATE:\n                return self.inverse_log_warm_up * math.log(self.last_batch_iteration + 1)\n            elif self.warmup_type == WARMUP_LINEAR_RATE:\n                return self.last_batch_iteration / self.warmup_num_steps\n        return 1.0\n\n    def _format_param(self, optimizer, param_value, param_name):\n        if isinstance(param_value, list) or isinstance(param_value, tuple):\n            if len(param_value) != len(optimizer.param_groups):\n                raise ValueError(\"expected {} value for {}, got {}\".format(len(optimizer.param_groups), param_name,\n                                                                           FileNotFoundError(param_value)))\n            return list(param_value)\n        return [param_value] * len(optimizer.param_groups)\n\n\nclass WarmupDecayLR(WarmupLR):\n    \"\"\"Increase the learning rate of each parameter group from min lr to max lr\n        over warmup_num_steps steps, and then decay at linear rate over the remaining training steps.\n\n        Args:\n            optimizer (Optimizer): Wrapped optimizer.\n            total_num_steps (int): total number of training steps\n            warmup_min_lr (float or list): minimum learning rate. Default: 0\n            warmup_max_lr (float or list): maximum learning rate. Default: 0.001\n            warmup_num_steps (int): number of steps to warm up from min_lr to max_lr. Default: 1000\n            warmup_type {\u2018log\u2019, \u2018linear\u2019}: increasing function from min_lr to max_lr during warmup. Default: log\n            last_batch_iteration (int): The index of the last batch. Default: -1.\n        Example:\n            >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n            >>> scheduler = WarmupDecayLR(optimizer, 1000000)\n            >>> data_loader = torch.utils.data.DataLoader(...)\n            >>> for epoch in range(10):\n            >>>     for batch in data_loader:\n            >>>         train_batch(...)\n            >>>         scheduler.step()\n\n    \"\"\"\n\n    def __init__(self,\n                 optimizer: Optimizer,\n                 total_num_steps: int,\n                 warmup_min_lr: float = 0.0,\n                 warmup_max_lr: float = 0.001,\n                 warmup_num_steps: int = 1000,\n                 warmup_type: str = WARMUP_LOG_RATE,\n                 last_batch_iteration: int = -1):\n\n        self.total_num_steps = total_num_steps\n        super(WarmupDecayLR, self).__init__(optimizer, warmup_min_lr, warmup_max_lr, warmup_num_steps, warmup_type,\n                                            last_batch_iteration)\n        if self.total_num_steps < self.warmup_num_steps:\n            logger.warning('total_num_steps {} is less than warmup_num_steps {}'.format(\n                total_num_steps, warmup_num_steps))\n\n    def _get_gamma(self):\n        if self.last_batch_iteration < self.warmup_num_steps:\n            if self.warmup_type == WARMUP_LOG_RATE:\n                return self.inverse_log_warm_up * math.log(self.last_batch_iteration + 1)\n            elif self.warmup_type == WARMUP_LINEAR_RATE:\n                return self.last_batch_iteration / self.warmup_num_steps\n        return max(\n            0.0,\n            float(self.total_num_steps - self.last_batch_iteration) /\n            float(max(1.0, self.total_num_steps - self.warmup_num_steps)))\n\n\nclass WarmupCosineLR(object):\n    \"\"\"Increase the learning rate of each parameter group from min lr ratio to max lr ratio\n        over warmup_num_steps steps, and then decay at cosine rate over the remaining training steps to min cosine ratio.\n\n        Args:\n            optimizer (Optimizer): Wrapped optimizer.\n            total_num_steps (int): total number of training steps\n            warmup_min_ratio (float or list): warmup start learning rate ratio. Default: 0\n            warmup_num_steps (int): number of steps to warm up from warmup_min_ratio to 1.0. Default: 1000\n            warmup_type {\u2018log\u2019, \u2018linear\u2019}: increasing function from min_lr to max_lr during warmup. Default: log\n            cos_min_ratio (float): cosine end learning rate ratio. Default: 0.0001\n            last_batch_iteration (int): The index of the last batch. Default: -1.\n        Example:\n            >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n            >>> scheduler = WarmupCosineLR(optimizer, 1000000)\n            >>> data_loader = torch.utils.data.DataLoader(...)\n            >>> for epoch in range(10):\n            >>>     for batch in data_loader:\n            >>>         train_batch(...)\n            >>>         scheduler.step()\n\n    \"\"\"\n\n    def __init__(self,\n                 optimizer: Optimizer,\n                 total_num_steps: int,\n                 warmup_min_ratio: float = 0.0,\n                 warmup_num_steps: int = 1000,\n                 cos_min_ratio: float = 0.0001,\n                 warmup_type: str = WARMUP_LOG_RATE,\n                 last_batch_iteration: int = -1):\n\n        self.optimizer = get_torch_optimizer(optimizer)\n\n        self.total_num_steps = total_num_steps\n        self.last_batch_iteration = last_batch_iteration\n        self.cos_min_ratio = cos_min_ratio\n\n        self.warmup_type = warmup_type\n        self.warmup_min_ratio = warmup_min_ratio\n        self.warmup_num_steps = max(2, warmup_num_steps)\n        self.inverse_log_warm_up = 1.0 / math.log(self.warmup_num_steps)\n\n        if self.total_num_steps < self.warmup_num_steps:\n            logger.warning('total_num_steps {} is less than warmup_num_steps {}'.format(\n                total_num_steps, warmup_num_steps))\n        self.org_lrs = [group['lr'] for group in self.optimizer.param_groups]\n\n    def get_lr_ratio(self):\n        if self.last_batch_iteration < 0:\n            logger.warning(\"Attempting to get learning rate from scheduler before it has started\")\n            return [0.0]\n\n        if self.last_batch_iteration < self.warmup_num_steps:\n            if self.warmup_type == WARMUP_LOG_RATE:\n                ratio = self.inverse_log_warm_up * math.log(self.last_batch_iteration + 1)\n            elif self.warmup_type == WARMUP_LINEAR_RATE:\n                ratio = self.last_batch_iteration / self.warmup_num_steps\n            ratio_delta = 1. - self.warmup_min_ratio\n            ratio = self.warmup_min_ratio + ratio * ratio_delta\n            return ratio\n\n        real_last_step = self.last_batch_iteration - self.warmup_num_steps + 1\n        real_total_steps = self.total_num_steps - self.warmup_num_steps\n        ratio_delta = 1. - self.cos_min_ratio\n        ratio = (1 + math.cos(math.pi * real_last_step / real_total_steps)) / 2\n        ratio = max(0.0, self.cos_min_ratio + ratio_delta * ratio)\n        return ratio\n\n    def step(self, last_batch_iteration=None):\n        if last_batch_iteration is None:\n            last_batch_iteration = self.last_batch_iteration + 1\n        self.last_batch_iteration = last_batch_iteration\n\n        lrs = self.get_lr()\n        for param_group, lr in zip(self.optimizer.param_groups, lrs):\n            param_group['lr'] = lr\n        self._last_lr = [group['lr'] for group in self.optimizer.param_groups]\n\n    def get_lr(self):\n        if self.last_batch_iteration < 0:\n            logger.warning(\"Attempting to get learning rate from scheduler before it has started\")\n            return [0.0]\n        lr_ratio = self.get_lr_ratio()\n        return [org_lr * lr_ratio for org_lr in self.org_lrs]\n\n    def get_last_lr(self):\n        \"\"\" Return last computed learning rate by current scheduler.\n        \"\"\"\n        assert getattr(self, '_last_lr', None) is not None, \"need to call step() first\"\n        return self._last_lr\n\n    def state_dict(self):\n        return {'last_batch_iteration': self.last_batch_iteration}\n\n    def load_state_dict(self, sd):\n        self.last_batch_iteration = sd['last_batch_iteration']\n\n    def _format_param(self, optimizer, param_value, param_name):\n        if isinstance(param_value, list) or isinstance(param_value, tuple):\n            if len(param_value) != len(optimizer.param_groups):\n                raise ValueError(\"expected {} value for {}, got {}\".format(len(optimizer.param_groups), param_name,\n                                                                           FileNotFoundError(param_value)))\n            return list(param_value)\n        return [param_value] * len(optimizer.param_groups)\n", "deepspeed/runtime/engine.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport os\nimport re\nimport stat\nimport torch\nimport hashlib\nfrom collections import defaultdict, OrderedDict, deque\nfrom shutil import copyfile\nimport gc\n\nfrom torch.nn.modules import Module\nfrom torch.nn.parameter import Parameter\nfrom torch.optim import Optimizer\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom torch._utils import _flatten_dense_tensors, _unflatten_dense_tensors\n\nfrom typing import Callable, Dict, Union, Iterable\n\nimport deepspeed\n\nfrom deepspeed import comm as dist\nfrom deepspeed.runtime.utils import see_memory_usage, DummyOptim\nfrom .zero.offload_config import OffloadDeviceEnum\nfrom deepspeed.runtime.zero.stage_1_and_2 import DeepSpeedZeroOptimizer\nfrom deepspeed.runtime.zero.partition_parameters import ZeroParamStatus\nfrom deepspeed.runtime.zero.utils import is_zero_supported_optimizer, ZeRORuntimeException\nfrom deepspeed.runtime.zero.parameter_offload import DeepSpeedZeRoOffload\nfrom deepspeed.runtime.zero.config import ZERO_OPTIMIZATION\n\nfrom deepspeed.runtime.fp16.fused_optimizer import FP16_Optimizer\nfrom deepspeed.runtime.fp16.unfused_optimizer import FP16_UnfusedOptimizer\nfrom deepspeed.runtime.bf16_optimizer import BF16_Optimizer\n\nfrom deepspeed.runtime.config import DEEPSPEED_OPTIMIZERS, \\\n    ADAGRAD_OPTIMIZER, ADAM_OPTIMIZER, ADAMW_OPTIMIZER, LAMB_OPTIMIZER, ONEBIT_ADAM_OPTIMIZER, ONEBIT_LAMB_OPTIMIZER, \\\n    TORCH_ADAM_PARAM, ADAM_W_MODE, ADAM_W_MODE_DEFAULT, ZERO_ONE_ADAM_OPTIMIZER, MUADAM_OPTIMIZER, MUADAMW_OPTIMIZER, \\\n    MUSGD_OPTIMIZER, LION_OPTIMIZER\n\nfrom deepspeed.runtime.dataloader import DeepSpeedDataLoader\nfrom deepspeed.runtime.constants import \\\n    ROUTE_TRAIN, ROUTE_PREDICT, ROUTE_EVAL, \\\n    PLD_THETA, PLD_GAMMA, BFLOAT16, FP16, AMP, GRADIENT_ACCUMULATION_STEPS, \\\n    DATA_PARALLEL_GROUP, GLOBAL_RANK\nfrom deepspeed.runtime.zero.config import ZeroStageEnum\nfrom deepspeed.compression import compression_scheduler\nfrom deepspeed.compression.constants import \\\n    WEIGHT_QUANTIZE_IN_FORWARD_ENABLED, \\\n    WEIGHT_QUANTIZATION, SHARED_PARAMETERS, \\\n    WEIGHT_QUANTIZE_ENABLED, \\\n    WEIGHT_QUANTIZE_GROUPS, \\\n    WEIGHT_QUANTIZE_FP16_MIXED_QUANTIZE, \\\n    WEIGHT_QUANTIZE_CHANGE_RATIO, \\\n    WEIGHT_QUANTIZE_TYPE, \\\n    WEIGHT_QUANTIZE_ROUNDING, \\\n    WEIGHT_QUANTIZE_VERBOSE, \\\n    WEIGHT_QUANTIZE_KERNEL\nfrom deepspeed.checkpoint.constants import OPTIMIZER_STATE_DICT, FROZEN_PARAM_FRAGMENTS\nfrom deepspeed.runtime.sparse_tensor import SparseTensor\n\nfrom deepspeed.runtime import lr_schedules\nfrom deepspeed.utils import groups\nfrom deepspeed.utils import logger, log_dist, instrument_w_nvtx\nfrom deepspeed.utils.timer import NoopTimer, ThroughputTimer, SynchronizedWallClockTimer, \\\n    FORWARD_MICRO_TIMER, BACKWARD_MICRO_TIMER, BACKWARD_INNER_MICRO_TIMER, BACKWARD_REDUCE_MICRO_TIMER, \\\n    STEP_MICRO_TIMER, \\\n    FORWARD_GLOBAL_TIMER, BACKWARD_GLOBAL_TIMER, BACKWARD_INNER_GLOBAL_TIMER, BACKWARD_REDUCE_GLOBAL_TIMER, \\\n    STEP_GLOBAL_TIMER\nfrom deepspeed.utils.debug import debug_extract_module_and_param_names, debug_clear_module_and_param_names\nfrom deepspeed.monitor.monitor import MonitorMaster\nfrom deepspeed.runtime.progressive_layer_drop import ProgressiveLayerDrop\nfrom deepspeed.runtime.utils import clip_grad_norm_\nfrom deepspeed.runtime.eigenvalue import Eigenvalue\nfrom deepspeed.runtime.data_pipeline.constants import DATA_SAMPLING, \\\n    DATA_ROUTING, DATA_SAMPLING_ENABLED, CURRICULUM_LEARNING, \\\n    CURRICULUM_LEARNING_ENABLED, DATA_SAMPLING_NUM_WORKERS, RANDOM_LTD, \\\n    RANDOM_LTD_ENABLED, RANDOM_LTD_LAYER_ID, RANDOM_LTD_LAYER_NUM, \\\n    RANDOM_LTD_LAYER_TOKEN_LR_SCHEDULE, RANDOM_LTD_LAYER_TOKEN_LR_ENABLED, \\\n    RANDOM_LTD_GLOBAL_BATCH_SIZE, RANDOM_LTD_MICRO_BATCH_SIZE, DATA_EFFICIENCY\nfrom deepspeed.runtime.data_pipeline.curriculum_scheduler import CurriculumScheduler\nfrom deepspeed.runtime.data_pipeline.data_routing.scheduler import RandomLTDScheduler\nfrom deepspeed.runtime.data_pipeline.data_routing.helper import remove_random_ltd_state_dict\nfrom deepspeed.runtime.data_pipeline.data_routing.basic_layer import RandomLayerTokenDrop\n\nfrom deepspeed.runtime.checkpoint_engine.torch_checkpoint_engine import TorchCheckpointEngine\nfrom deepspeed.utils.zero_to_fp32 import get_fp32_state_dict_from_zero_checkpoint\n\nfrom .pipe.module import PipelineModule\nfrom .utils import get_ma_status\nfrom .compiler import is_compile_supported\nfrom ..ops.adam import FusedAdam\nfrom ..moe.sharded_moe import TopKGate, MOELayer\nfrom ..moe.layer import MoE\nfrom ..moe.utils import is_moe_param, configure_moe_param_groups\nfrom ..git_version_info import version\n\nfrom deepspeed.profiling.flops_profiler.profiler import FlopsProfiler\nfrom deepspeed.utils.logging import print_json_dist, print_configuration\n\nfrom deepspeed.accelerator import get_accelerator\n\nfrom deepspeed.runtime.config import DtypeEnum\n\nMEMORY_OPT_ALLREDUCE_SIZE = 500000000\n\nDeepSpeedOptimizerCallable = \\\n    Callable[[Union[Iterable[Parameter], Dict[str, Iterable]]], Optimizer]\nDeepSpeedSchedulerCallable = Callable[[Optimizer], _LRScheduler]\n\ntry:\n    import apex\n    from apex import amp\n    APEX_INSTALLED = True\nexcept ImportError:\n    # Fail silently so we don't spam logs unnecessarily if user isn't using amp\n    APEX_INSTALLED = False\n\n\ndef split_half_float_double_sparse(tensors):\n    device_type = get_accelerator().device_name()\n    supported_types = get_accelerator().supported_dtypes()\n\n    for t in tensors:\n        assert t.dtype in supported_types, f\"attempting to reduce an unsupported grad type: {t.dtype}\"\n\n    sparse_tensor_buckets, dense_tensor_buckets = [], []\n    for i, dtype in enumerate(supported_types):\n        sparse_bucket, dense_bucket = [], []\n        for t in tensors:\n            if t.dtype == dtype:\n                if isinstance(t, SparseTensor):\n                    sparse_bucket.append(t)\n                else:\n                    dense_bucket.append(t)\n        if sparse_bucket:\n            sparse_tensor_buckets.append((dtype, sparse_bucket))\n        if dense_bucket:\n            dense_tensor_buckets.append((dtype, dense_bucket))\n    return sparse_tensor_buckets, dense_tensor_buckets\n\n\nclass EngineTimers(object):\n    r\"\"\"Wallclock timers for DeepSpeedEngine\"\"\"\n\n    def __init__(self, enable_micro_timers, enable_global_timers):\n        self.forward_timers = []\n        self.backward_timers = []\n        self.backward_inner_timers = []\n        self.backward_reduce_timers = []\n        self.step_timers = []\n        self.global_timers = []\n        self.micro_timers = []\n\n        if enable_micro_timers:\n            self.forward_timers += [FORWARD_MICRO_TIMER]\n            self.backward_timers += [BACKWARD_MICRO_TIMER]\n            self.backward_inner_timers += [BACKWARD_INNER_MICRO_TIMER]\n            self.backward_reduce_timers += [BACKWARD_REDUCE_MICRO_TIMER]\n            self.step_timers += [STEP_MICRO_TIMER]\n            self.micro_timers += [\n                FORWARD_MICRO_TIMER, BACKWARD_MICRO_TIMER, BACKWARD_INNER_MICRO_TIMER, BACKWARD_REDUCE_MICRO_TIMER,\n                STEP_MICRO_TIMER\n            ]\n\n        if enable_global_timers:\n            self.forward_timers += [FORWARD_GLOBAL_TIMER]\n            self.backward_timers += [BACKWARD_GLOBAL_TIMER]\n            self.backward_inner_timers += [BACKWARD_INNER_GLOBAL_TIMER]\n            self.backward_reduce_timers += [BACKWARD_REDUCE_GLOBAL_TIMER]\n            self.step_timers += [STEP_GLOBAL_TIMER]\n            self.global_timers += [\n                FORWARD_GLOBAL_TIMER, BACKWARD_GLOBAL_TIMER, BACKWARD_INNER_GLOBAL_TIMER, BACKWARD_REDUCE_GLOBAL_TIMER,\n                STEP_GLOBAL_TIMER\n            ]\n\n\nclass DeepSpeedEngine(Module):\n    r\"\"\"DeepSpeed engine for training.\"\"\"\n\n    def __init__(self,\n                 args,\n                 model,\n                 optimizer=None,\n                 model_parameters=None,\n                 training_data=None,\n                 lr_scheduler=None,\n                 mpu=None,\n                 dist_init_required=None,\n                 collate_fn=None,\n                 config=None,\n                 config_class=None,\n                 dont_change_device=False):\n        super(DeepSpeedEngine, self).__init__()\n        self.dont_change_device = dont_change_device\n        self.client_optimizer = optimizer\n        self.client_lr_scheduler = lr_scheduler\n        self.training_data = training_data\n        self.collate_fn = collate_fn\n        self.mpu = mpu\n        self.all_to_all_group = None\n        self.data_parallel_group = None\n        self.global_steps = 0\n        self.global_samples = 0\n        self.micro_steps = 0\n        self.skipped_steps = 0\n        self.gradient_average = True\n        self.warn_unscaled_loss = True\n        self.config = config\n        self._config = config_class\n        self.loaded_checkpoint_mp_world_size = None\n        self.loaded_checkpoint_dp_world_size = None\n        self.enable_backward_allreduce = True\n        self.progressive_layer_drop = None\n        self.eigenvalue = None\n        self.block_eigenvalue = None\n        self.gas_boundary_ctr = 0\n        self.dist_backend = get_accelerator().communication_backend_name()\n        self.has_moe_layers = False\n        self.num_experts = []\n        self.gate_modules = []\n        self.moe_layers = []\n        self._step_applied = False\n        self._global_grad_norm = None\n        self.use_ds_comm = False  # False --> Use torch.dist, True --> Use ds.comm backend.\n\n        self.checkpoint_engine = None\n\n        self._is_gradient_accumulation_boundary = None\n        self.scale_wrt_gas = None\n        self.losses = None\n\n        # for debug purposes - can then debug print: debug_get_module_name(module)\n        debug_extract_module_and_param_names(model)\n\n        self._do_args_sanity_check(args)\n        self._configure_with_arguments(args, mpu)\n        self._do_sanity_check()\n        see_memory_usage(f\"DeepSpeed Engine: After args sanity test\", force=self.memory_breakdown())\n        if mpu is not None:\n            if self.elasticity_enabled():\n                if not self.is_elastic_model_parallel_supported():\n                    assert not self.elasticity_enabled(), (\"Elasticity is not currently supported\"\n                                                           \" with model parallelism.\")\n\n        self._set_distributed_vars(args)\n\n        dist.configure(self._config)\n\n        self.monitor = MonitorMaster(self._config.monitor_config)\n\n        see_memory_usage(\n            f\"DeepSpeed Engine: Before configure distributed model\",\n            force=self.memory_breakdown(),\n        )\n\n        self.pipeline_parallelism = isinstance(model, PipelineModule)\n\n        # Configure distributed model\n        self._configure_distributed_model(model)\n\n        # needed for zero_to_fp32 weights reconstruction to remap nameless data to state_dict\n        self.param_names = {param: name for name, param in model.named_parameters()}\n\n        self._get_model_parameters()\n\n        see_memory_usage(f\"DeepSpeed Engine: After configure distributed model\")\n\n        # Configure wall clock timers\n        self.timers = SynchronizedWallClockTimer()\n        # Throughput timer\n        self.tput_timer = ThroughputTimer(self._config.timers_config,\n                                          batch_size=self.train_batch_size(),\n                                          steps_per_output=self.steps_per_print(),\n                                          monitor_memory=False)\n\n        log_dist(f\"DeepSpeed Flops Profiler Enabled: {self.flops_profiler_enabled()}\", ranks=[0])\n\n        if self.flops_profiler_enabled():\n            self.flops_profiler = FlopsProfiler(self.module, self, self.flops_profiler_recompute_fwd_factor())\n\n        if training_data:\n            self.training_dataloader = self.deepspeed_io(training_data)\n        else:\n            self.training_dataloader = None\n\n        # Configure optimizer and scheduler\n        self.optimizer = None\n        self.basic_optimizer = None\n        self.lr_scheduler = None\n        has_optimizer = False\n\n        if optimizer or self.optimizer_name():\n            has_optimizer = True\n        # If no parameters given by init default to module parameters\n        if model_parameters is None:\n            model_parameters = self.module.parameters()\n\n        # Convert model parameters from generator to list\n        if not isinstance(model_parameters, list):\n            model_parameters = list(model_parameters)\n\n        if has_optimizer:\n            self._configure_optimizer(optimizer, model_parameters)\n            self._configure_lr_scheduler(lr_scheduler)\n            self._report_progress(0)\n        elif self.zero_optimization():\n            # no optim selected but zero is enabled\n            self.optimizer = self._configure_zero_optimizer(optimizer=None)\n        elif self.bfloat16_enabled():\n            self.optimizer = self._configure_bf16_optimizer(optimizer=None)\n\n        # Hook optimizer for snip_momentum pruning\n        if hasattr(model, 'pruners'):\n            from ..compression.helper import rewrite_optimizer_step\n            self.optimizer.pruners = model.pruners\n            rewrite_optimizer_step(self.optimizer)\n\n        # Bookkeeping for sparse support\n        self.sparse_tensor_module_names = set()\n        # if self.sparse_gradients_enabled():\n        for name, module in self.module.named_modules():\n            if isinstance(module, (torch.nn.Embedding, torch.nn.EmbeddingBag)) and self.sparse_gradients_enabled():\n                self.sparse_tensor_module_names.add(name + \".weight\")\n                logger.info(\"Will convert {} to sparse tensor during training\".format(name))\n\n        self.save_non_zero_checkpoint = False\n        self.save_zero_checkpoint = False\n        if not isinstance(self.optimizer, DeepSpeedZeRoOffload):\n            self._configure_checkpointing(dist_init_required)\n\n        if self.eigenvalue_enabled():\n            self.eigenvalue = self._configure_eigenvalue()\n\n        if self.pld_enabled():\n            self.progressive_layer_drop = self._configure_progressive_layer_drop()\n\n        if self.curriculum_enabled_legacy():\n            self.curriculum_scheduler_legacy = self._configure_curriculum_scheduler_legacy()\n\n        if self.random_ltd_enabled():\n            random_ltd_config = self.random_ltd_config()\n            random_ltd_config[RANDOM_LTD_GLOBAL_BATCH_SIZE] = self.train_batch_size()\n            random_ltd_config[RANDOM_LTD_MICRO_BATCH_SIZE] = self.train_micro_batch_size_per_gpu()\n            self.random_ltd_scheduler = self._configure_random_ltd_scheduler(random_ltd_config)\n\n        # Engine timers\n\n        self.engine_timers = EngineTimers(enable_micro_timers=self.wall_clock_breakdown(),\n                                          enable_global_timers=self.wall_clock_breakdown()\n                                          or self.flops_profiler_enabled())\n\n        if self.global_rank == 0:\n            self._config.print(\"DeepSpeedEngine configuration\")\n            if self.dump_state():\n                print_configuration(self, \"DeepSpeedEngine\")\n\n        # Use torch (un)flatten ops\n        self.flatten = _flatten_dense_tensors\n        self.unflatten = _unflatten_dense_tensors\n\n        self._is_compiled = False\n\n    def destroy(self):\n        if self.optimizer is not None and hasattr(self.optimizer, 'destroy'):\n            self.optimizer.destroy()\n        debug_clear_module_and_param_names()\n\n    def _get_model_parameters(self):\n        if self.autotuning_profile_model_info():\n            self.autotuning_model_info = {}\n            num_params = 0\n            trainable_num_params = 0\n\n            for p in self.module.parameters():\n                # since user code might call deepspeed.zero.Init() before deepspeed.initialize(), need to check the attribute to check if the parameter is partitioned in zero 3 already or not\n                n = 0\n                if hasattr(p, \"ds_tensor\"):  # if the parameter is partitioned in zero 3\n                    n += p.ds_numel\n                else:  # if the parameter is not partitioned in zero 3 yet\n                    n += p.numel()\n                num_params += n\n                if p.requires_grad:\n                    trainable_num_params += n\n            if self.global_rank == 0:\n                self.autotuning_model_info[\"num_params\"] = num_params * self.mp_world_size\n                self.autotuning_model_info[\"trainable_num_params\"] = trainable_num_params * self.mp_world_size\n\n            logger.info(f\"model parameter = {num_params}\")\n\n    def get_batch_info(self):\n        \"\"\"Get all training batch related settings.\n        Returns:\n            train_batch_size (int): The effective training batch size. This is the amount of data\n                samples that leads to one step of model update.\n            train_micro_batch_size_per_gpu (int): Batch size to be processed by one GPU in one\n                step (without gradient accumulation).\n            gradient_accumulation_steps (int): Number of training steps to accumulate gradients\n                before averaging and applying them.\n        \"\"\"\n        return (\n            self.train_batch_size,\n            self.train_micro_batch_size_per_gpu,\n            self.gradient_accumulation_steps,\n        )\n\n    def set_train_batch_size(self, train_batch_size):\n        \"\"\"Adjust the global batch size by increasing or decreasing the number of\n        micro-batches (i.e., gradient accumulation steps). The size of each micro-batch\n        (i.e., ``train_micro_batch_size_per_gpu``) is not changed.\n        Args:\n            train_batch_size (int): The new global batch size for training.\n        Raises:\n            ValueError: if ``train_batch_size`` is not divisible by the\n                configured micro-batch size and data parallelism.\n        \"\"\"\n        if train_batch_size % (self.train_micro_batch_size_per_gpu() * self.dp_world_size) != 0:\n            #print(f'{train_batch_size=} {self.train_micro_batch_size_per_gpu()=} {self.dp_world_size=}')\n            raise ValueError(f'Train batch size must be divisible by micro-batch data parallelism')\n        new_gas = train_batch_size // (self.train_micro_batch_size_per_gpu() * self.dp_world_size)\n        # overwrite config\n        self._config.train_batch_size = train_batch_size\n        self._config.gradient_accumulation_steps = new_gas\n\n    def set_train_micro_batch_size(self, micro_batch_size):\n        \"\"\"Adjust the micro batch size(i.e., the micro batch size in every data parallel group),\n        while keep the gradient accumulation steps the same.\n        Args:\n            micro_batch_size (int): The new micro batch size for training.\n        \"\"\"\n        # overwrite config\n        new_global_batch_size = micro_batch_size * self._config.gradient_accumulation_steps * self.dp_world_size\n        self._config.train_batch_size = new_global_batch_size\n        self._config.train_micro_batch_size_per_gpu = micro_batch_size\n\n    def set_data_post_process_func(self, post_process_func):\n        if self.training_dataloader is not None:\n            self.training_dataloader.post_process_func = post_process_func\n\n    def set_custom_curriculum_learning_schedule(self, schedule_func_dict):\n        if self.training_dataloader is not None and self.curriculum_learning_enabled():\n            self.training_dataloader.data_sampler.set_custom_curriculum_learning_schedule(schedule_func_dict)\n\n    def get_global_grad_norm(self) -> float:\n        \"\"\"Return the 2-norm of all gradients. If there is model parallelism,\n        the norm will be global.\n        The computed norm will be cached and reused until the next step() pass.\n        .. note::\n            In the presence of model parallelism, this is a collective call\n            and acts as a barrier among ``mpu.get_model_parallel_group()``.\n        Returns:\n            float: norm\n        \"\"\"\n        return self._global_grad_norm\n\n    def __getattr__(self, name):\n        \"\"\"\n        Pass through attributes defined in the model if they are not overridden by ds-engine.\n        \"\"\"\n\n        _module = {}\n        if \"module\" in self.__dict__:\n            _module = self.__dict__['module']\n        if name in dir(self):\n            return getattr(self, name)\n        elif name in dir(_module):\n            return getattr(_module, name)\n        else:\n            raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")\n\n    def checkpoint_tag_validation_enabled(self):\n        return self._config.checkpoint_tag_validation_enabled\n\n    def checkpoint_tag_validation_fail(self):\n        return self._config.checkpoint_tag_validation_fail\n\n    def elasticity_enabled(self):\n        return self._config.elasticity_enabled\n\n    def is_elastic_model_parallel_supported(self):\n        if self.elasticity_enabled():\n            # Add code for finding number of GPUs per node automatically\n            if self._config.num_gpus_per_node % self._config.elastic_model_parallel_size == 0:\n                return True\n            else:\n                return False\n\n    def pld_enabled(self):\n        return self._config.pld_enabled\n\n    def pld_params(self):\n        return self._config.pld_params\n\n    def pld_theta(self):\n        return self.pld_params()[PLD_THETA]\n\n    def pld_gamma(self):\n        return self.pld_params()[PLD_GAMMA]\n\n    def eigenvalue_enabled(self):\n        return self._config.eigenvalue_enabled\n\n    def eigenvalue_verbose(self):\n        return self._config.eigenvalue_verbose\n\n    def eigenvalue_max_iter(self):\n        return self._config.eigenvalue_max_iter\n\n    def eigenvalue_tol(self):\n        return self._config.eigenvalue_tol\n\n    def eigenvalue_stability(self):\n        return self._config.eigenvalue_stability\n\n    def eigenvalue_gas_boundary_resolution(self):\n        return self._config.eigenvalue_gas_boundary_resolution\n\n    def eigenvalue_layer_name(self):\n        return self._config.eigenvalue_layer_name\n\n    def eigenvalue_layer_num(self):\n        return self._config.eigenvalue_layer_num\n\n    def curriculum_enabled_legacy(self):\n        return self._config.curriculum_enabled_legacy\n\n    def curriculum_params_legacy(self):\n        return self._config.curriculum_params_legacy\n\n    def data_efficiency_enabled(self):\n        return self._config.data_efficiency_enabled\n\n    def data_efficiency_config(self):\n        return self._config.data_efficiency_config\n\n    def data_sampling_enabled(self):\n        return self._config.data_efficiency_config[DATA_SAMPLING][DATA_SAMPLING_ENABLED]\n\n    def data_sampling_config(self):\n        return self._config.data_efficiency_config[DATA_SAMPLING]\n\n    def curriculum_learning_enabled(self):\n        return self._config.data_efficiency_config[DATA_SAMPLING][CURRICULUM_LEARNING][CURRICULUM_LEARNING_ENABLED]\n\n    def curriculum_learning_config(self):\n        return self._config.data_efficiency_config[DATA_SAMPLING][CURRICULUM_LEARNING]\n\n    def random_ltd_enabled(self):\n        return self._config.data_efficiency_config[DATA_ROUTING][RANDOM_LTD][RANDOM_LTD_ENABLED]\n\n    def random_ltd_config(self):\n        return self._config.data_efficiency_config[DATA_ROUTING][RANDOM_LTD]\n\n    def random_ltd_initialize(self):\n        assert self.random_ltd_enabled()\n        random_ltd_config = self.random_ltd_config()\n        random_ltd_queue = deque([x for x in sorted(random_ltd_config[RANDOM_LTD_LAYER_ID])])\n        count = 0\n        for name, layer in self.module.named_modules():\n            if isinstance(layer, RandomLayerTokenDrop):\n                if len(random_ltd_queue) != 0 and str(random_ltd_queue[0]) in name:  ###[1,2,3]\n                    layer.init_config(random_ltd_config, self.random_ltd_scheduler, count)\n                    random_ltd_queue.popleft()\n                    count += 1\n\n        if random_ltd_config[RANDOM_LTD_LAYER_NUM] != count:\n            raise ValueError(f'random_ltd_layer_num {random_ltd_config[RANDOM_LTD_LAYER_NUM]} must be \\\n                equivalent to the len of random_ltd_layer_id {count}')\n\n        if random_ltd_config[RANDOM_LTD_LAYER_TOKEN_LR_SCHEDULE][RANDOM_LTD_LAYER_TOKEN_LR_ENABLED]:\n            assert self.client_lr_scheduler is None\n            raise ValueError(f'not yet support')\n            #self.lr_scheduler = lr_schedules.WarmupLayerTokenDecayLR(self.optimizer, self.random_ltd_scheduler)\n\n    def wall_clock_breakdown(self):\n        return self._config.wall_clock_breakdown\n\n    def flops_profiler_enabled(self):\n        return self._config.flops_profiler_config.enabled or self.autotuning_enabled()\n\n    def flops_profiler_recompute_fwd_factor(self):\n        return self._config.flops_profiler_config.recompute_fwd_factor\n\n    def flops_profiler_profile_step(self):\n        step = self._config.flops_profiler_config.profile_step\n        if self._config.autotuning_config.enabled:\n            step = self.autotuning_start_profile_step()\n        return step\n\n    def flops_profiler_module_depth(self):\n        return self._config.flops_profiler_config.module_depth\n\n    def flops_profiler_top_modules(self):\n        return self._config.flops_profiler_config.top_modules\n\n    def flops_profiler_detailed(self):\n        if self._config.autotuning_config.enabled:\n            return False\n        return self._config.flops_profiler_config.detailed\n\n    def flops_profiler_output_file(self):\n        return self._config.flops_profiler_config.output_file\n\n    def memory_breakdown(self):\n        return self._config.memory_breakdown\n\n    def autotuning_enabled(self):\n        return self._config.autotuning_config.enabled\n\n    def autotuning_start_profile_step(self):\n        return self._config.autotuning_config.start_profile_step\n\n    def autotuning_end_profile_step(self):\n        return self._config.autotuning_config.end_profile_step\n\n    def autotuning_metric_path(self):\n        path = self._config.autotuning_config.metric_path\n        if not path:\n            path = os.path.join(os.getcwd(), \"autotuning_metric.json\")\n        return path\n\n    def autotuning_model_info_path(self):\n        path = self._config.autotuning_config.model_info_path\n        if not path:\n            path = os.path.join(os.getcwd(), \"autotuning_model_info.json\")\n        return path\n\n    def autotuning_metric(self):\n        return self._config.autotuning_config.metric\n\n    def autotuning_profile_model_info(self):\n        return self.autotuning_enabled(\n        ) and self._config.autotuning_config.model_info and self._config.autotuning_config.model_info.get(\n            \"profile\", False)\n\n    def sparse_gradients_enabled(self):\n        return self._config.sparse_gradients_enabled\n\n    def train_batch_size(self):\n        return self._config.train_batch_size\n\n    def train_micro_batch_size_per_gpu(self):\n        return self._config.train_micro_batch_size_per_gpu\n\n    def optimizer_name(self):\n        return (self.client_optimizer.__class__.__name__ if self.client_optimizer else self._config.optimizer_name)\n\n    def optimizer_params(self):\n        return self._config.optimizer_params\n\n    def optimizer_legacy_fusion(self):\n        return self._config.optimizer_legacy_fusion\n\n    def scheduler_name(self):\n        return self._config.scheduler_name\n\n    def scheduler_params(self):\n        return self._config.scheduler_params\n\n    def quantize_training(self):\n        return (\n            self._config.compression_config[WEIGHT_QUANTIZATION][SHARED_PARAMETERS]\n            [WEIGHT_QUANTIZE_IN_FORWARD_ENABLED],\n            self._config.compression_config[WEIGHT_QUANTIZATION][SHARED_PARAMETERS][WEIGHT_QUANTIZE_ENABLED],\n            self._config.compression_config[WEIGHT_QUANTIZATION][SHARED_PARAMETERS][WEIGHT_QUANTIZE_GROUPS],\n            self._config.compression_config[WEIGHT_QUANTIZATION][SHARED_PARAMETERS]\n            [WEIGHT_QUANTIZE_FP16_MIXED_QUANTIZE],\n            self._config.compression_config[WEIGHT_QUANTIZATION][SHARED_PARAMETERS][WEIGHT_QUANTIZE_CHANGE_RATIO],\n            self._config.compression_config[WEIGHT_QUANTIZATION][SHARED_PARAMETERS][WEIGHT_QUANTIZE_TYPE],\n            self._config.compression_config[WEIGHT_QUANTIZATION][SHARED_PARAMETERS][WEIGHT_QUANTIZE_ROUNDING],\n            self._config.compression_config[WEIGHT_QUANTIZATION][SHARED_PARAMETERS][WEIGHT_QUANTIZE_VERBOSE],\n            self._config.compression_config[WEIGHT_QUANTIZATION][SHARED_PARAMETERS][WEIGHT_QUANTIZE_KERNEL],\n        )\n\n    def zero_optimization(self):\n        return self._config.zero_enabled\n\n    def zero_allow_untested_optimizer(self):\n        return self._config.zero_allow_untested_optimizer\n\n    def zero_force_ds_cpu_optimizer(self):\n        return self._config.zero_force_ds_cpu_optimizer\n\n    def zero_reduce_scatter(self):\n        return self._config.zero_config.reduce_scatter\n\n    def zero_overlap_comm(self):\n        return self._config.zero_config.overlap_comm\n\n    def zero_offload_optimizer(self):\n        return self._config.zero_config.offload_optimizer\n\n    def zero_offload_param(self):\n        return self._config.zero_config.offload_param\n\n    def zero_use_cpu_optimizer(self):\n        if self._config.zero_config.offload_optimizer is not None:\n            return self._config.zero_config.offload_optimizer.device in [OffloadDeviceEnum.cpu, OffloadDeviceEnum.nvme]\n        return False\n\n    def zero_cpu_offload(self):\n        if self._config.zero_config.offload_optimizer is not None:\n            return self._config.zero_config.offload_optimizer.device == OffloadDeviceEnum.cpu\n        return False\n\n    def zero_partial_offload(self):\n        return getattr(self._config.zero_config.offload_optimizer, \"ratio\", 1.0)\n\n    def zero_sub_group_size(self):\n        return self._config.zero_config.sub_group_size\n\n    def zero_optimization_stage(self):\n        return self._config.zero_optimization_stage\n\n    def mics_shard_size(self):\n        return self._config.mics_shard_size\n\n    def zero_reduce_bucket_size(self):\n        return self._config.zero_config.reduce_bucket_size\n\n    def zero_multi_rank_bucket_allreduce(self):\n        return self._config.zero_config.use_multi_rank_bucket_allreduce\n\n    def zero_allgather_bucket_size(self):\n        return self._config.zero_config.allgather_bucket_size\n\n    def zero_optimization_partition_gradients(self):\n        return self.zero_optimization_stage() >= ZeroStageEnum.gradients\n\n    def zero_optimization_partition_weights(self):\n        return self.zero_optimization_stage() >= ZeroStageEnum.weights\n\n    def is_first_weights_partition_group(self):\n        ret = True if self.mics_shard_size() < 0 \\\n            and self.zero_optimization_partition_weights() else False\n        if self.mics_shard_size() > 0 and self.global_rank < self.mics_shard_size():\n            ret = True\n        return ret\n\n    def zero_contiguous_gradients(self):\n        return self._config.zero_config.contiguous_gradients\n\n    def zero_load_from_fp32_weights(self):\n        return self._config.zero_config.load_from_fp32_weights\n\n    def zero_elastic_checkpoint(self):\n        return self._config.zero_config.elastic_checkpoint\n\n    def zero_has_nvme_offload(self):\n        if not hasattr(self.optimizer, \"swap_optimizer\"):\n            return False\n        return self.optimizer.swap_optimizer or self.optimizer.params_in_nvme_and_cpu\n\n    def zero_max_live_parameters(self):\n        return self._config.zero_config.max_live_parameters\n\n    def zero_max_reuse_distance(self):\n        return self._config.zero_config.max_reuse_distance\n\n    def zero_prefetch_bucket_size(self):\n        return self._config.zero_config.prefetch_bucket_size\n\n    def zero_param_persistence_threshold(self):\n        return self._config.zero_config.param_persistence_threshold\n\n    def zero_model_persistence_threshold(self):\n        return self._config.zero_config.model_persistence_threshold\n\n    def zero_gather_16bit_weights_on_model_save(self):\n        return self._config.zero_config.gather_16bit_weights_on_model_save\n\n    def zero_grad_hooks(self):\n        return self._config.zero_config.grad_hooks\n\n    def zero_legacy_stage1(self):\n        return self._config.zero_config.legacy_stage1\n\n    def zero_ignore_unused_parameters(self):\n        return self._config.zero_config.ignore_unused_parameters\n\n    def graph_harvesting(self):\n        return self._config.graph_harvesting\n\n    def fp16_enabled(self):\n        return self._config.fp16_enabled\n\n    def bfloat16_enabled(self):\n        return self._config.bfloat16_enabled\n\n    def fp16_master_weights_and_gradients(self):\n        return self._config.fp16_master_weights_and_gradients\n\n    def amp_enabled(self):\n        return self._config.amp_enabled\n\n    def amp_params(self):\n        return self._config.amp_params\n\n    def fp16_auto_cast(self):\n        return self._config.fp16_auto_cast\n\n    def loss_scale(self):\n        return self._config.loss_scale\n\n    def gradient_accumulation_steps(self):\n        return self._config.gradient_accumulation_steps\n\n    def use_node_local_storage(self):\n        return self._config.use_node_local_storage\n\n    def load_universal_checkpoint(self):\n        return self._config.load_universal_checkpoint\n\n    @property\n    def communication_data_type(self):\n        res = self._config.communication_data_type\n        if res is not None:\n            return res\n\n        if self.fp16_enabled():\n            return torch.float16\n\n        if self.bfloat16_enabled():\n            return torch.bfloat16\n\n        return torch.float32\n\n    @communication_data_type.setter\n    def communication_data_type(self, value):\n        self._config.communication_data_type = value\n\n    def postscale_gradients(self):\n        return not self._config.prescale_gradients\n\n    def gradient_predivide_factor(self):\n        return self._config.gradient_predivide_factor\n\n    def steps_per_print(self):\n        return self._config.steps_per_print\n\n    def zero_allgather_partitions(self):\n        return self._config.zero_config.allgather_partitions\n\n    def zero_round_robin_gradients(self):\n        return self._config.zero_config.round_robin_gradients\n\n    def zero_hpz_partition_size(self):\n        return self._config.zero_config.zero_hpz_partition_size\n\n    def zero_quantized_weights(self):\n        return self._config.zero_config.zero_quantized_weights\n\n    def zero_quantized_nontrainable_weights(self):\n        return self._config.zero_config.zero_quantized_nontrainable_weights\n\n    def zero_quantized_gradients(self):\n        return self._config.zero_config.zero_quantized_gradients\n\n    def dump_state(self):\n        return self._config.dump_state\n\n    def gradient_clipping(self):\n        return self._config.gradient_clipping\n\n    def dynamic_loss_scale(self):\n        return self._config.loss_scale == 0\n\n    def initial_dynamic_scale(self):\n        return self._config.initial_dynamic_scale\n\n    def dynamic_loss_scale_args(self):\n        return self._config.dynamic_loss_scale_args\n\n    def swap_tensor_config(self):\n        return self._config.swap_tensor_config\n\n    def aio_config(self):\n        return self._config.aio_config\n\n    def get_data_types(self):\n        model_dtype = torch.float32\n        if self.fp16_enabled():\n            model_dtype = torch.float16\n        elif self.bfloat16_enabled():\n            model_dtype = torch.bfloat16\n\n        if self._config.grad_accum_dtype is None:\n            if model_dtype == torch.bfloat16 and not self.zero_optimization():\n                grad_accum_dtype = torch.float32\n            else:\n                grad_accum_dtype = model_dtype\n        else:\n            grad_accum_dtype = DtypeEnum(self._config.grad_accum_dtype).value\n\n        return (model_dtype, grad_accum_dtype)\n\n    def _optimizer_has_ckpt_event_prologue(self):\n        return self.optimizer is not None and hasattr(self.optimizer, 'checkpoint_event_prologue')\n\n    def _optimizer_has_ckpt_event_epilogue(self):\n        return self.optimizer is not None and hasattr(self.optimizer, 'checkpoint_event_epilogue')\n\n    def _configure_lr_scheduler(self, client_lr_scheduler):\n        # First check for scheduler in json configuration\n        lr_scheduler = self._scheduler_from_config(self.optimizer)\n        if lr_scheduler:\n            log_dist(f\"DeepSpeed using configured LR scheduler = {self.scheduler_name()}\", ranks=[0])\n            self.lr_scheduler = lr_scheduler\n        else:\n            if isinstance(client_lr_scheduler, Callable):\n                log_dist('DeepSpeed using client callable to create LR scheduler', ranks=[0])\n                self.lr_scheduler = client_lr_scheduler(self.basic_optimizer)\n            else:\n                log_dist('DeepSpeed using client LR scheduler', ranks=[0])\n                self.lr_scheduler = client_lr_scheduler\n\n        log_dist(f'DeepSpeed LR Scheduler = {self.lr_scheduler}', ranks=[0])\n\n    def _configure_checkpointing(self, dist_init_required):\n        self.checkpoint_engine = TorchCheckpointEngine()\n\n        if self._config is not None and self._config.nebula_config.enabled:\n            try:\n                from deepspeed.runtime.checkpoint_engine.nebula_checkpoint_engine import \\\n                    NebulaCheckpointEngine\n                self.checkpoint_engine = NebulaCheckpointEngine(config_params=self._config.nebula_config)\n            except ImportError as err:\n                logger.error(f\"No torch_nebula was found! Will fall back to torch.save. Details: {err}\")\n                self.checkpoint_engine = TorchCheckpointEngine()\n\n        dp_rank = groups._get_sequence_data_parallel_rank()\n\n        rank = self.local_rank if self.use_node_local_storage() else dp_rank\n\n        # only the first data parallel process needs to store the model checkpoint\n        # if you want to use node local storage this must be done by rank 0 on each\n        # node\n        self.save_non_zero_checkpoint = (rank == 0) or (self.zero_optimization_partition_weights()\n                                                        and self.is_first_weights_partition_group())\n\n        if self.zero_optimization() or self.bfloat16_enabled():\n            param_rank = dist.get_rank(group=self.optimizer.dp_process_group)\n\n            # Only the first parameter parallel process needs to store the\n            # optimizer state checkpoints for zero\n            self.save_zero_checkpoint = param_rank == dp_rank\n\n    def _scheduler_from_config(self, optimizer):\n        scheduler_name = self.scheduler_name()\n        if scheduler_name is not None:\n            if hasattr(lr_schedules, scheduler_name):\n                scheduler = getattr(lr_schedules, scheduler_name)\n            else:\n                assert hasattr(torch.optim.lr_scheduler,\n                               scheduler_name), f\"DeepSpeed does not recognize LR scheduler {scheduler_name}\"\n\n                scheduler = getattr(torch.optim.lr_scheduler, scheduler_name)\n\n            scheduler_params = self.scheduler_params()\n            instantiated_scheduler = scheduler(optimizer, **scheduler_params)\n            return instantiated_scheduler\n        else:\n            return None\n\n    def _set_distributed_vars(self, args):\n        device_rank = args.device_rank if args is not None and hasattr(args, 'device_rank') else self.local_rank\n        if device_rank >= 0:\n            get_accelerator().set_device(device_rank)\n            self.device = torch.device(get_accelerator().device_name(), device_rank)\n            self.world_size = dist.get_world_size()\n            self.global_rank = dist.get_rank()\n        else:\n            self.world_size = 1\n            self.global_rank = 0\n            self.device = torch.device(get_accelerator().device_name())\n\n    # Configure based on command line arguments\n    def _configure_with_arguments(self, args, mpu):\n        # After the distributed backend is initialized we are guaranteed the LOCAL_RANK\n        # environment variable is set. We must align args.local_rank to this value for\n        # backwards compatibility with scripts relying on [args|self].local_rank containing\n        # the correct local rank info. _do_args_sanity_check will ensure this is the case.\n\n        if \"OMPI_COMM_WORLD_LOCAL_RANK\" in os.environ:\n            ompi_local_rank = os.environ.get(\"OMPI_COMM_WORLD_LOCAL_RANK\")\n            local_rank = os.environ.get('LOCAL_RANK', ompi_local_rank)\n            assert ompi_local_rank == local_rank, f\"LOCAL_RANK ({local_rank}) != OMPI_COMM_WORLD_LOCAL_RANK ({ompi_local_rank}), \" \\\n                \"not sure how to proceed as we're seeing conflicting local rank info.\"\n            os.environ['LOCAL_RANK'] = local_rank\n\n        self.local_rank = int(os.environ['LOCAL_RANK'])\n        if hasattr(args, 'local_rank'):\n            args.local_rank = self.local_rank\n\n    # Validate command line arguments\n    def _do_args_sanity_check(self, args):\n        assert \"LOCAL_RANK\" in os.environ or \"OMPI_COMM_WORLD_LOCAL_RANK\" in os.environ, \"DeepSpeed requires the LOCAL_RANK environment \" \\\n            \"variable, it is set by the deepspeed launcher, deepspeed.init_distributed, or the torch's launcher. If using a \" \\\n            \"different launcher please ensure LOCAL_RANK is set prior to initializing deepspeed.\"\n\n        if hasattr(args, 'local_rank') and args.local_rank is not None:\n            assert isinstance(args.local_rank,\n                              int), f\"args.local_rank of {args.local_rank} is an unknown type {type(args.local_rank)}\"\n            if args.local_rank >= 0:\n                env_local_rank = int(os.environ.get(\"LOCAL_RANK\"))\n                assert (\n                    env_local_rank == args.local_rank\n                ), f\"Mismatch in local rank setting, args.local_rank={args.local_rank} but env['LOCAL_RANK']={env_local_rank}.\"\n\n    def _is_supported_optimizer(self, optimizer_name):\n        return (optimizer_name in DEEPSPEED_OPTIMIZERS or getattr(torch.optim, optimizer_name, None) is not None)\n\n    def _supported_optims(self):\n        FairseqOptimizer = None\n        try:\n            from fairseq.optim.fairseq_optimizer import FairseqOptimizer\n        except ImportError:\n            pass\n\n        expected_optim_types = [Optimizer]\n        if FairseqOptimizer:\n            # fairseq optims are not torch.optim objects\n            expected_optim_types.append(FairseqOptimizer)\n        return expected_optim_types\n\n    # Validate configuration based on command line arguments\n    def _do_sanity_check(self):\n        if self.fp16_enabled() and not get_accelerator().is_fp16_supported():\n            raise ValueError(\"Type fp16 is not supported.\")\n\n        expected_optim_types = self._supported_optims()\n        expected_optim_types += [type(None), Callable]\n        assert isinstance(self.client_optimizer, tuple(expected_optim_types)), \\\n            f'Client Optimizer is of unexpected type {type(self.client_optimizer)}'\n\n        if not self.client_optimizer:\n            if self.optimizer_name() is not None:\n                assert self._is_supported_optimizer(\n                    self.optimizer_name()), \"{} is not a supported DeepSpeed Optimizer\".format(self.optimizer_name())\n\n        if (self.optimizer_name() == LAMB_OPTIMIZER or self.optimizer_name() == ONEBIT_LAMB_OPTIMIZER):\n            assert (self.dynamic_loss_scale()), \"DeepSpeed {} optimizer requires dynamic loss scaling\".format(\n                self.optimizer_name())\n\n        # Detect invalid combinations of client optimizer and client scheduler\n        if isinstance(self.client_lr_scheduler, _LRScheduler):\n            assert isinstance(self.client_optimizer, Optimizer), \\\n                f'Client Optimizer (type = {type(self.client_optimizer)} is not instantiated but Client LR Scheduler is instantiated'\n\n    def _broadcast_model(self):\n\n        def is_replicated(p):\n            if hasattr(p, \"ds_status\") and p.ds_status is not ZeroParamStatus.AVAILABLE:\n                return False\n            return True\n\n        for p in self.module.parameters():\n            # Broadcast the model for different parameters\n            if is_moe_param(p):\n                if torch.is_tensor(p) and is_replicated(p):\n                    dist.broadcast(p.data,\n                                   groups._get_expert_broadcast_src_rank(p.group_name),\n                                   group=self.expert_data_parallel_group[p.group_name])\n            else:\n                if torch.is_tensor(p) and is_replicated(p):\n                    dist.broadcast(p.data, groups._get_broadcast_src_rank(), group=self.seq_data_parallel_group)\n\n    @staticmethod\n    def __check_params(model: Module, dtype: torch.dtype) -> None:\n        return\n        if not all(param.dtype == dtype for param in model.parameters()) and dist.get_rank() == 0:\n            raise ValueError(f\"{dtype} is enabled but the following parameters have dtype that is \"\n                             f\"not {dtype}: \"\n                             f\"{[(n, p.dtype) for n, p in model.named_parameters() if p.dtype != dtype]}\")\n\n    def _set_client_model(self, model):\n        # register client model in _modules so that nn.module methods work correctly\n        modules = self.__dict__.get('_modules')\n        modules['module'] = model\n        # register module attribute in engine but avoid getattr\n        self.__dict__['module'] = model\n\n    def _configure_distributed_model(self, model):\n        self._set_client_model(model)\n        is_zero_init_model = self.zero_optimization_partition_weights() and any(\n            [hasattr(param, \"ds_id\") for param in self.module.parameters()])\n\n        if self.fp16_enabled():\n            if is_zero_init_model:\n                self.__check_params(self.module, torch.half)\n            self.module.half()\n        elif self.bfloat16_enabled():\n            if is_zero_init_model:\n                self.__check_params(self.module, torch.bfloat16)\n            self.module.bfloat16()\n        else:\n            self.__check_params(self.module, torch.float)\n\n        # zero.Init() handles device placement of model\n        if not (self.dont_change_device or is_zero_init_model):\n            self.module.to(self.device)\n\n        # MoE related initialization\n        for _, module in self.module.named_modules():\n            if isinstance(module, MoE):\n                self.has_moe_layers = True\n                self.num_experts.append(module.num_experts)\n\n        if self.has_moe_layers:\n            for _, module in self.module.named_modules():\n                if isinstance(module, TopKGate):\n                    self.gate_modules.append(module)\n                    if self.wall_clock_breakdown():\n                        module.wall_clock_breakdown = True\n                if isinstance(module, MOELayer):\n                    self.moe_layers.append(module)\n                    if self.wall_clock_breakdown():\n                        module.wall_clock_breakdown = True\n\n        # Pass the mpu from here to groups. For subsequent use, just query groups\n        if self.mpu is not None:\n            groups.mpu = self.mpu\n\n        # Set deepspeed parallelism spec. for the model including expert parallelism\n        for _, module in self.module.named_modules():\n            if hasattr(module, 'set_deepspeed_parallelism'):\n                module.set_deepspeed_parallelism(self._config.use_data_before_expert_parallel_)\n\n        # Query the groups module to get information about various parallel groups\n        self.local_all_to_all_group = None\n        if self.zero_quantized_gradients():\n            log_dist(\"Using quantized gradients\", ranks=[0])\n            self.local_all_to_all_group = groups._get_local_all_to_all_group()\n        self.data_parallel_group = groups._get_data_parallel_group()\n        self.dp_world_size = groups._get_data_parallel_world_size()\n        self.seq_data_parallel_group = groups._get_sequence_data_parallel_group()\n        self.seq_dp_world_size = groups._get_sequence_data_parallel_world_size()\n        self.mp_world_size = groups._get_model_parallel_world_size()\n        self.expert_parallel_group = groups._get_expert_parallel_group_dict()\n        self.expert_data_parallel_group = groups._get_expert_data_parallel_group_dict()\n        self.sequence_parallel_size = groups._get_sequence_parallel_world_size()\n        if self.sequence_parallel_size > 1:\n            self.communication_data_type = self._config.seq_parallel_communication_data_type\n\n        if not (self.amp_enabled() or is_zero_init_model):\n            self._broadcast_model()\n\n    # check if parameters are duplicated in optimizer param_groups\n    def _check_for_duplicates(self, optimizer):\n        for name, param in self.module.named_parameters():\n            param_id = id(param)\n\n            def ids_list(group):\n                return [id(param) for param in group]\n\n            occurrence = sum([\n                ids_list(group['params']).count(param_id) if param_id in ids_list(group['params']) else 0\n                for group in optimizer.param_groups\n            ])\n            assert occurrence <= 1, f\"Parameter with name: {name} occurs multiple times in optimizer.param_groups. Make sure it only appears once to prevent undefined behavior.\"\n\n    def _do_optimizer_sanity_check(self, basic_optimizer):\n        model_dtype, grad_accum_dtype = self.get_data_types()\n        zero_enabled = self.zero_optimization()\n        amp_enabled = self.amp_enabled()\n        # config based assertions\n        assert (\n            not (amp_enabled and zero_enabled)\n        ), \"Amp and ZeRO are not currently compatible, please use (legacy) fp16 mode which performs similar to amp opt_mode=O2\"\n        if zero_enabled:\n            if not is_zero_supported_optimizer(basic_optimizer):\n                assert (\n                    self.zero_allow_untested_optimizer()\n                ), 'You are using an untested ZeRO Optimizer. Please add <\"zero_allow_untested_optimizer\": true> in the configuration file to use it.'\n\n                if self.global_rank == 0:\n                    logger.warning(\"**** You are using ZeRO with an untested optimizer, proceed with caution *****\")\n            if model_dtype == torch.bfloat16 and grad_accum_dtype == torch.float32 and self.zero_optimization_stage(\n            ) == 1 and not self.zero_cpu_offload():\n                return BFLOAT16\n            return ZERO_OPTIMIZATION\n        elif amp_enabled:\n            if model_dtype != grad_accum_dtype:\n                raise NotImplementedError(\n                    \"Model data type and gradient accumulation data type must be equal to use Amp\")\n            if model_dtype == torch.bfloat16 or model_dtype == torch.float16:\n                raise NotImplementedError(\"Cannot enable both amp with (legacy) fp16 or bfloat16 mode\")\n            try:\n                logger.info(\"Initializing Apex amp from: {}\".format(amp.__path__))\n            except NameError:\n                # If apex/amp is available it will be imported above\n                raise RuntimeError(\"Unable to import apex/amp, please make sure it is installed\")\n            return AMP\n        # data type checks\n        elif model_dtype == grad_accum_dtype:\n            if model_dtype == torch.bfloat16:\n                if self.pipeline_parallelism:\n                    logger.warning(\n                        \"**** BF16 gradient accumulation is not safe numerically with large number of accumulation steps, proceed with caution *****\"\n                    )\n                    return BFLOAT16\n                else:\n                    raise NotImplementedError(\n                        \"Bfloat16 wrapper must use a gradient accumulation type of fp32, enable ZeRO to use Bfloat16 gradient accumulation\"\n                    )\n            if model_dtype == torch.float16:\n                return FP16\n            # else optimizer_wrapper = None\n        elif model_dtype == torch.bfloat16 and grad_accum_dtype == torch.float32:\n            return BFLOAT16\n        else:\n            raise NotImplementedError(\"unsupported mix of model dtype and gradient accumulation type\")\n\n        return None\n\n    # Configure optimizer\n    def _configure_optimizer(self, client_optimizer, model_parameters):\n        if client_optimizer is None:\n            if self.has_moe_layers:\n                model_parameters = configure_moe_param_groups(model_parameters)\n            basic_optimizer = self._configure_basic_optimizer(model_parameters)\n            log_dist(f\"Using DeepSpeed Optimizer param name {self.optimizer_name()} as basic optimizer\", ranks=[0])\n        else:\n            if isinstance(client_optimizer, tuple(self._supported_optims())):\n                basic_optimizer = client_optimizer\n                log_dist('Using client Optimizer as basic optimizer', ranks=[0])\n            else:\n                basic_optimizer = client_optimizer(model_parameters)\n                log_dist('Using client callable to create basic optimizer', ranks=[0])\n\n            if self.zero_use_cpu_optimizer() and not isinstance(basic_optimizer, deepspeed.ops.adam.DeepSpeedCPUAdam):\n                if self.zero_force_ds_cpu_optimizer():\n                    msg = f'You are using ZeRO-Offload with a client provided optimizer ({type(basic_optimizer)}) which in most cases will yield poor performance. Please either use deepspeed.ops.adam.DeepSpeedCPUAdam or set an optimizer in your ds-config (https://www.deepspeed.ai/docs/config-json/#optimizer-parameters). If you really want to use a custom optimizer w. ZeRO-Offload and understand the performance impacts you can also set <\"zero_force_ds_cpu_optimizer\": false> in your configuration file.'\n                    raise ZeRORuntimeException(msg)\n\n        basic_optimizer.param_groups[:] = [pg for pg in basic_optimizer.param_groups if len(pg[\"params\"]) != 0]\n        log_dist(\"Removing param_group that has no 'params' in the basic Optimizer\", ranks=[0])\n\n        self._check_for_duplicates(basic_optimizer)\n\n        self.basic_optimizer = basic_optimizer\n        log_dist(\"DeepSpeed Basic Optimizer = {}\".format(basic_optimizer.__class__.__name__), ranks=[0])\n\n        optimizer_wrapper = self._do_optimizer_sanity_check(basic_optimizer)\n\n        if optimizer_wrapper == ZERO_OPTIMIZATION:\n            self.optimizer = self._configure_zero_optimizer(basic_optimizer)\n        elif optimizer_wrapper == AMP:\n            amp_params = self.amp_params()\n            log_dist(f\"Initializing AMP with these params: {amp_params}\", ranks=[0])\n            model, self.optimizer = amp.initialize(self.module, basic_optimizer, **amp_params)\n            self._set_client_model(model)\n            self._broadcast_model()\n            # TODO: maybe need to broadcast experts differently?\n        elif optimizer_wrapper == FP16:\n            self.optimizer = self._configure_fp16_optimizer(basic_optimizer)\n        elif optimizer_wrapper == BFLOAT16:\n            self.optimizer = self._configure_bf16_optimizer(basic_optimizer)\n        else:\n            self.optimizer = basic_optimizer\n\n        log_dist(\"DeepSpeed Final Optimizer = {}\".format(self.optimizer.__class__.__name__), ranks=[0])\n\n        self.compression_scheduler = self._configure_compression_scheduler()\n        self.quantizer = self._configure_quantization()\n\n    def _configure_basic_optimizer(self, model_parameters):\n        optimizer_parameters = self.optimizer_params()\n        if optimizer_parameters is None:\n            optimizer_parameters = {}\n        # print(optimizer_parameters.keys())\n        if \"max_grad_norm\" in optimizer_parameters.keys():\n            raise ValueError(\n                \"'max_grad_norm' is not supported as an optimizer parameter, please switch to using the deepspeed parameter 'gradient_clipping' see: https://www.deepspeed.ai/docs/config-json/#gradient-clipping for more details\"\n            )\n\n        if self.optimizer_name() in [ADAM_OPTIMIZER, ADAMW_OPTIMIZER]:\n            torch_adam = optimizer_parameters.pop(TORCH_ADAM_PARAM, False)\n            adam_w_mode = optimizer_parameters.pop(ADAM_W_MODE, ADAM_W_MODE_DEFAULT)\n\n            # Optimizer name of Adam forces AdamW logic unless adam_w_mode is explicitly set\n            effective_adam_w_mode = self.optimizer_name() == ADAMW_OPTIMIZER or adam_w_mode\n\n            if torch_adam:\n                if not effective_adam_w_mode:\n                    optimizer = torch.optim.Adam(model_parameters, **optimizer_parameters)\n                else:\n                    optimizer = torch.optim.AdamW(model_parameters, **optimizer_parameters)\n            else:\n                if self.zero_use_cpu_optimizer():\n                    from deepspeed.ops.adam import DeepSpeedCPUAdam\n                    optimizer = DeepSpeedCPUAdam(model_parameters,\n                                                 **optimizer_parameters,\n                                                 adamw_mode=effective_adam_w_mode)\n                else:\n                    from deepspeed.ops.adam import FusedAdam\n\n                    optimizer = FusedAdam(\n                        model_parameters,\n                        **optimizer_parameters,\n                        adam_w_mode=effective_adam_w_mode,\n                    )\n\n        elif self.optimizer_name() == ADAGRAD_OPTIMIZER:\n            if self.zero_use_cpu_optimizer():\n                from deepspeed.ops.adagrad import DeepSpeedCPUAdagrad\n                optimizer = DeepSpeedCPUAdagrad(model_parameters, **optimizer_parameters)\n            else:\n                optimizer = torch.optim.Adagrad(model_parameters, **optimizer_parameters)\n        elif self.optimizer_name() == LAMB_OPTIMIZER:\n            from deepspeed.ops.lamb import FusedLamb\n\n            optimizer = FusedLamb(model_parameters, **optimizer_parameters)\n        elif self.optimizer_name() == ONEBIT_ADAM_OPTIMIZER:\n            assert not self.zero_optimization(), \"1bit-Adam is not compatible with ZeRO\"\n            from deepspeed.runtime.fp16.onebit.adam import OnebitAdam\n\n            optimizer = OnebitAdam(model_parameters, self, **optimizer_parameters)\n            if not self.fp16_enabled():\n                logger.warning(f\"Currently the convergence of 1-bit Adam is only verified under FP16\")\n        elif self.optimizer_name() == ZERO_ONE_ADAM_OPTIMIZER:\n            assert not self.zero_optimization(), \"0/1 Adam is not compatible with ZeRO\"\n            from deepspeed.runtime.fp16.onebit.zoadam import ZeroOneAdam\n\n            optimizer = ZeroOneAdam(model_parameters, self, **optimizer_parameters)\n            if not self.fp16_enabled():\n                logger.warning(f'Currently the convergence of 0/1 Adam is only verified under FP16')\n        elif self.optimizer_name() == ONEBIT_LAMB_OPTIMIZER:\n            assert not self.zero_optimization(), \"1bit-Lamb is not compatible with ZeRO\"\n            from deepspeed.runtime.fp16.onebit.lamb import OnebitLamb\n\n            optimizer = OnebitLamb(model_parameters, self, **optimizer_parameters)\n            if not self.fp16_enabled():\n                logger.warning(f\"Currently the convergence of 1-bit Lamb is only verified under FP16\")\n        elif self.optimizer_name() == LION_OPTIMIZER:\n            if self.zero_use_cpu_optimizer():\n                from deepspeed.ops.lion import DeepSpeedCPULion\n                optimizer = DeepSpeedCPULion(model_parameters, **optimizer_parameters)\n            else:\n                from deepspeed.ops.lion import FusedLion\n                optimizer = FusedLion(model_parameters, **optimizer_parameters)\n        elif self.optimizer_name() == MUADAM_OPTIMIZER:\n            try:\n                from mup import MuAdam\n            except ImportError:\n                logger.error(f\"Install mup to use MuAdam optimizer\")\n            optimizer = MuAdam(model_parameters, **optimizer_parameters)\n        elif self.optimizer_name() == MUADAMW_OPTIMIZER:\n            try:\n                from mup import MuAdamW\n            except ImportError:\n                logger.error(f\"Install mup to use MuAdamW optimizer\")\n            optimizer = MuAdamW(model_parameters, **optimizer_parameters)\n        elif self.optimizer_name() == MUSGD_OPTIMIZER:\n            try:\n                from mup import MuSGD\n            except ImportError:\n                logger.error(f\"Install mup to use MuSGD optimizer\")\n            optimizer = MuSGD(model_parameters, **optimizer_parameters)\n        else:\n            torch_optimizer = getattr(torch.optim, self.optimizer_name())\n            optimizer = torch_optimizer(model_parameters, **optimizer_parameters)\n        return optimizer\n\n    def _configure_compression_scheduler(self):\n        return compression_scheduler(self.module, self._config.compression_config)\n\n    def _configure_random_ltd_scheduler(self, configs):\n        return RandomLTDScheduler(configs)\n\n    def _configure_quantization(self):\n        (\n            quantize_weight_in_forward,\n            quantize_enabled,\n            q_groups,\n            q_mixed_fp16,\n            q_change_ratio,\n            q_type,\n            q_rounding,\n            q_verbose,\n            use_quantizer_kernel,\n        ) = self.quantize_training()\n        if quantize_enabled and not quantize_weight_in_forward:\n            assert self.fp16_enabled(\n            ), \"MoQ (quantize in optimization step) weight quantization is only supported for FP16\"\n        quantizer = None\n        if quantize_enabled and not quantize_weight_in_forward:\n            from deepspeed.runtime.quantize import Quantizer\n\n            quantizer = Quantizer(\n                q_groups,\n                q_mixed_fp16,\n                q_change_ratio,\n                q_type,\n                q_rounding,\n                q_verbose,\n                self.eigenvalue_enabled(),\n                use_quantizer_kernel,\n                self.eigenvalue_layer_num() if self.eigenvalue_enabled() else 0,\n            )\n        return quantizer\n\n    def _configure_fp16_optimizer(self, optimizer):\n        initial_dynamic_scale = self.initial_dynamic_scale()\n        dynamic_loss_args = self.dynamic_loss_scale_args()\n        clip_grad = self.gradient_clipping()\n        if APEX_INSTALLED:\n            fused_opts = (apex.optimizers.FusedAdam, FusedAdam)\n        else:\n            fused_opts = FusedAdam\n        if isinstance(optimizer, fused_opts) \\\n                or self.optimizer_name() in [ONEBIT_ADAM_OPTIMIZER, ZERO_ONE_ADAM_OPTIMIZER]:\n            if self.dynamic_loss_scale():\n                log_dist(f'Creating fp16 optimizer with dynamic loss scale', ranks=[0])\n                timers = self.timers if self.wall_clock_breakdown() else NoopTimer()\n                optimizer = FP16_Optimizer(\n                    optimizer,\n                    deepspeed=self,\n                    dynamic_loss_scale=True,\n                    initial_dynamic_scale=initial_dynamic_scale,\n                    dynamic_loss_args=dynamic_loss_args,\n                    mpu=self.mpu,\n                    clip_grad=clip_grad,\n                    fused_adam_legacy=self.optimizer_legacy_fusion(),\n                    timers=timers,\n                    has_moe_layers=self.has_moe_layers,\n                )\n            else:\n                log_dist(f'Creating fp16 optimizer with static loss scale: {self.loss_scale()}', ranks=[0])\n                optimizer = FP16_Optimizer(\n                    optimizer,\n                    deepspeed=self,\n                    static_loss_scale=self.loss_scale(),\n                    mpu=self.mpu,\n                    clip_grad=clip_grad,\n                    fused_adam_legacy=self.optimizer_legacy_fusion(),\n                    has_moe_layers=self.has_moe_layers,\n                )\n        else:\n            log_dist(f'Creating fp16 unfused optimizer with dynamic loss scale', ranks=[0])\n            optimizer = FP16_UnfusedOptimizer(\n                optimizer,\n                deepspeed=self,\n                static_loss_scale=self.loss_scale(),\n                dynamic_loss_scale=self.dynamic_loss_scale(),\n                dynamic_loss_args=dynamic_loss_args,\n                mpu=self.mpu,\n                clip_grad=clip_grad,\n                fused_lamb_legacy=self.optimizer_name() == LAMB_OPTIMIZER,\n            )\n\n        return optimizer\n\n    def _configure_bf16_optimizer(self, optimizer):\n        clip_grad = self.gradient_clipping()\n\n        if optimizer is None:\n            optimizer = DummyOptim(list(self.module.parameters()))\n\n        log_dist('Creating BF16 optimizer', ranks=[0])\n\n        timers = self.timers if self.wall_clock_breakdown() else NoopTimer()\n        optimizer = BF16_Optimizer(optimizer,\n                                   self.param_names,\n                                   mpu=self.mpu,\n                                   clip_grad=clip_grad,\n                                   allgather_bucket_size=self.zero_allgather_bucket_size(),\n                                   dp_process_group=self.seq_data_parallel_group,\n                                   timers=timers,\n                                   grad_acc_dtype=self.get_data_types()[1],\n                                   graph_harvesting=self.graph_harvesting(),\n                                   immediate_grad_update=self._config.bfloat16_immediate_grad_update,\n                                   has_moe_layers=self.has_moe_layers)\n\n        return optimizer\n\n    def _configure_zero_optimizer(self, optimizer):\n        zero_stage = self.zero_optimization_stage()\n\n        mics_shard_size = self.mics_shard_size()\n        model_dtype, gradient_accumulation_dtype = self.get_data_types()\n\n        timers = self.timers if self.wall_clock_breakdown() else NoopTimer()\n\n        if optimizer is None:\n            optimizer = DummyOptim(list(self.module.parameters()))\n\n        if self.zero_legacy_stage1():\n            raise Exception(\n                \"The deprecated version of ZeRO Stage 1 is not supported in deepspeed >= 0.5.9. Please downgrade to a version less than 0.5.9 if you need to use this deprecated version of ZeRO.\"\n            )\n\n        if zero_stage <= ZeroStageEnum.gradients:\n            overlap_comm = self.zero_overlap_comm()\n            contiguous_gradients = self.zero_contiguous_gradients()\n            round_robin_gradients = self.zero_round_robin_gradients()\n            assert not isinstance(optimizer, DummyOptim), \"zero stage {} requires an optimizer\".format(zero_stage)\n\n            log_dist(f'Creating {model_dtype} ZeRO stage {zero_stage} optimizer', ranks=[0])\n\n            if isinstance(self.module, PipelineModule):\n                if overlap_comm:\n                    logger.warning(\"Pipeline parallelism does not support overlapped communication, will be disabled.\")\n                    overlap_comm = False\n            optimizer = DeepSpeedZeroOptimizer(\n                optimizer,\n                self.param_names,\n                timers=timers,\n                static_loss_scale=self.loss_scale(),\n                dynamic_loss_scale=self.dynamic_loss_scale(),\n                dynamic_loss_args=self.dynamic_loss_scale_args(),\n                clip_grad=self.gradient_clipping(),\n                contiguous_gradients=contiguous_gradients,\n                reduce_bucket_size=self.zero_reduce_bucket_size(),\n                use_multi_rank_bucket_allreduce=self.zero_multi_rank_bucket_allreduce(),\n                allgather_bucket_size=self.zero_allgather_bucket_size(),\n                dp_process_group=self.seq_data_parallel_group,\n                expert_parallel_group=self.expert_parallel_group if self.has_moe_layers else None,\n                expert_data_parallel_group=self.expert_data_parallel_group if self.has_moe_layers else None,\n                reduce_scatter=self.zero_reduce_scatter(),\n                overlap_comm=overlap_comm,\n                offload_optimizer_config=self.zero_offload_optimizer(),\n                mpu=self.mpu,\n                postscale_gradients=self.postscale_gradients(),\n                gradient_predivide_factor=self.gradient_predivide_factor(),\n                gradient_accumulation_steps=self.gradient_accumulation_steps(),\n                ignore_unused_parameters=self.zero_ignore_unused_parameters(),\n                partition_grads=zero_stage == ZeroStageEnum.gradients,\n                round_robin_gradients=round_robin_gradients,\n                has_moe_layers=self.has_moe_layers,\n                fp16_master_weights_and_gradients=self.fp16_master_weights_and_gradients(),\n                gradient_accumulation_dtype=gradient_accumulation_dtype,\n                communication_data_type=self.communication_data_type,\n                elastic_checkpoint=self.zero_elastic_checkpoint())\n\n        elif zero_stage == ZeroStageEnum.weights:\n            assert not self.has_moe_layers, \"MoE not supported with Stage 3\"\n            if isinstance(optimizer, DummyOptim):\n                log_dist(\"Creating ZeRO Offload\", ranks=[0])\n                zero_param_parallel_group = groups._get_zero_param_intra_parallel_group()\n                if self.zero_hpz_partition_size() > 1 and zero_param_parallel_group is None:\n                    self._set_zero_group_parallelism()\n                    zero_param_parallel_group = groups._get_zero_param_intra_parallel_group()\n                optimizer = DeepSpeedZeRoOffload(\n                    self.module,\n                    timers=timers,\n                    ds_config=self.config,\n                    overlap_comm=self.zero_overlap_comm(),\n                    prefetch_bucket_size=self.zero_prefetch_bucket_size(),\n                    max_reuse_distance=self.zero_max_reuse_distance(),\n                    max_live_parameters=self.zero_max_live_parameters(),\n                    param_persistence_threshold=self.zero_param_persistence_threshold(),\n                    model_persistence_threshold=self.zero_model_persistence_threshold(),\n                    offload_param_config=self.zero_offload_param(),\n                    mpu=self.mpu,\n                    zero_param_parallel_group=zero_param_parallel_group,\n                    zero_quantized_weights=self.zero_quantized_weights(),\n                    zero_quantized_nontrainable_weights=self.zero_quantized_nontrainable_weights(),\n                )\n            else:\n                log_dist(\n                    f'Creating fp16 ZeRO stage {zero_stage} optimizer,'\n                    f' MiCS is enabled {mics_shard_size>0},'\n                    f' Hierarchical params gather {self._config.mics_hierarchial_params_gather}',\n                    ranks=[0])\n                if mics_shard_size > 0:\n                    return self._return_mics_optimizer(optimizer, timers)\n\n                log_dist(f'Creating {model_dtype} ZeRO stage {zero_stage} optimizer', ranks=[0])\n                from deepspeed.runtime.zero.stage3 import DeepSpeedZeroOptimizer_Stage3\n                optimizer = DeepSpeedZeroOptimizer_Stage3(\n                    self.module,\n                    optimizer,\n                    timers=timers,\n                    ds_config=self.config,\n                    static_loss_scale=self.loss_scale(),\n                    dynamic_loss_scale=self.dynamic_loss_scale(),\n                    dynamic_loss_args=self.dynamic_loss_scale_args(),\n                    clip_grad=self.gradient_clipping(),\n                    contiguous_gradients=self.zero_contiguous_gradients(),\n                    reduce_bucket_size=self.zero_reduce_bucket_size(),\n                    prefetch_bucket_size=self.zero_prefetch_bucket_size(),\n                    max_reuse_distance=self.zero_max_reuse_distance(),\n                    max_live_parameters=self.zero_max_live_parameters(),\n                    param_persistence_threshold=self.zero_param_persistence_threshold(),\n                    model_persistence_threshold=self.zero_model_persistence_threshold(),\n                    dp_process_group=self.seq_data_parallel_group,\n                    all2all_process_group=self.local_all_to_all_group,\n                    reduce_scatter=self.zero_reduce_scatter(),\n                    overlap_comm=self.zero_overlap_comm(),\n                    offload_optimizer_config=self.zero_offload_optimizer(),\n                    offload_param_config=self.zero_offload_param(),\n                    sub_group_size=self.zero_sub_group_size(),\n                    offload_ratio=self.zero_partial_offload(),\n                    mpu=self.mpu,\n                    postscale_gradients=self.postscale_gradients(),\n                    gradient_predivide_factor=self.gradient_predivide_factor(),\n                    gradient_accumulation_steps=self.gradient_accumulation_steps(),\n                    aio_config=self.aio_config(),\n                    gradient_accumulation_dtype=gradient_accumulation_dtype,\n                    communication_data_type=self.communication_data_type,\n                    zero_hpz_partition_size=self.zero_hpz_partition_size(),\n                    zero_quantized_weights=self.zero_quantized_weights(),\n                    zero_quantized_nontrainable_weights=self.zero_quantized_nontrainable_weights(),\n                )\n\n        else:\n            raise NotImplementedError(\"ZeRO stage {} not implemented\".format(zero_stage))\n\n        return optimizer\n\n    def _return_mics_optimizer(self, basic_optimizer, timers):\n        from deepspeed.runtime.zero.mics import MiCS_Optimizer\n        model_dtype, gradient_accumulation_dtype = self.get_data_types()\n        optimizer = MiCS_Optimizer(self.module,\n                                   basic_optimizer,\n                                   timers=timers,\n                                   ds_config=self.config,\n                                   static_loss_scale=self.loss_scale(),\n                                   dynamic_loss_scale=self.dynamic_loss_scale(),\n                                   dynamic_loss_args=self.dynamic_loss_scale_args(),\n                                   clip_grad=self.gradient_clipping(),\n                                   contiguous_gradients=self.zero_contiguous_gradients(),\n                                   reduce_bucket_size=self.zero_reduce_bucket_size(),\n                                   prefetch_bucket_size=self.zero_prefetch_bucket_size(),\n                                   max_reuse_distance=self.zero_max_reuse_distance(),\n                                   max_live_parameters=self.zero_max_live_parameters(),\n                                   param_persistence_threshold=self.zero_param_persistence_threshold(),\n                                   model_persistence_threshold=self.zero_model_persistence_threshold(),\n                                   dp_process_group=self.seq_data_parallel_group,\n                                   reduce_scatter=self.zero_reduce_scatter(),\n                                   overlap_comm=self.zero_overlap_comm(),\n                                   offload_optimizer_config=self.zero_offload_optimizer(),\n                                   offload_param_config=self.zero_offload_param(),\n                                   sub_group_size=self.zero_sub_group_size(),\n                                   mpu=self.mpu,\n                                   postscale_gradients=self.postscale_gradients(),\n                                   gradient_predivide_factor=self.gradient_predivide_factor(),\n                                   gradient_accumulation_steps=self.gradient_accumulation_steps(),\n                                   aio_config=self.aio_config(),\n                                   gradient_accumulation_dtype=gradient_accumulation_dtype,\n                                   communication_data_type=self.communication_data_type)\n        return optimizer\n\n    def _configure_eigenvalue(self):\n        eigenvalue = Eigenvalue(\n            verbose=self.eigenvalue_verbose(),\n            max_iter=self.eigenvalue_max_iter(),\n            tol=self.eigenvalue_tol(),\n            stability=self.eigenvalue_stability(),\n            gas_boundary_resolution=self.eigenvalue_gas_boundary_resolution(),\n            layer_name=self.eigenvalue_layer_name(),\n            layer_num=self.eigenvalue_layer_num(),\n        )\n\n        return eigenvalue\n\n    def _configure_progressive_layer_drop(self):\n        pld = ProgressiveLayerDrop(theta=self.pld_theta(), gamma=self.pld_gamma())\n\n        return pld\n\n    def _configure_curriculum_scheduler_legacy(self):\n        scheduler = CurriculumScheduler(self.curriculum_params_legacy())\n        return scheduler\n\n    @staticmethod\n    def is_map_style_dataset(obj):\n        return hasattr(obj, \"__getitem__\") and hasattr(obj, \"__len__\")\n\n    @staticmethod\n    def is_iterable_style_dataset(obj):\n        return isinstance(obj, torch.utils.data.IterableDataset)  # hasattr(obj, \"__iter__\") should work as well\n\n    def dataloader_drop_last(self):\n        return self._config.dataloader_drop_last\n\n    def was_step_applied(self) -> bool:\n        \"\"\"Returns True if the latest ``step()`` produced in parameter updates.\n        Note that a ``False`` return is not an error condition. Steps are frequently\n        no-ops, such as between gradient accumulation boundaries or when overflows\n        occur.\n        Returns:\n            bool: Whether the latest ``step()`` modified model parameters.\n        \"\"\"\n        return self._step_applied\n\n    def deepspeed_io(self,\n                     dataset,\n                     batch_size=None,\n                     route=ROUTE_TRAIN,\n                     pin_memory=True,\n                     data_sampler=None,\n                     collate_fn=None,\n                     num_local_io_workers=None):\n        if not (self.is_map_style_dataset(dataset) or self.is_iterable_style_dataset(dataset)):\n            raise ValueError(\"Training data must be a torch Dataset\")\n\n        if batch_size is None:\n            batch_size = self.train_micro_batch_size_per_gpu()\n\n        if collate_fn is None:\n            collate_fn = self.collate_fn\n\n        # Currently we only use timer in train route\n        deepspeed_io_timer = None\n        if route == ROUTE_TRAIN:\n            deepspeed_io_timer = self.tput_timer\n\n        # If mpu is provided, forward world size and parallel rank to sampler.\n        data_parallel_world_size = self.dp_world_size\n        data_parallel_rank = self.global_rank\n        if self.mpu is not None:\n            data_parallel_world_size = self.mpu.get_data_parallel_world_size()\n            data_parallel_rank = self.mpu.get_data_parallel_rank()\n\n        if data_sampler is None and (route == ROUTE_PREDICT or route == ROUTE_EVAL):\n            data_sampler = torch.utils.data.DistributedSampler(\n                dataset,\n                num_replicas=data_parallel_world_size,\n                rank=data_parallel_rank,\n                shuffle=False,\n            )\n\n        deepspeed_dataloader_config = {}\n        if self.curriculum_learning_enabled():\n            deepspeed_dataloader_config = {\n                CURRICULUM_LEARNING: self.curriculum_learning_enabled(),\n                DATA_EFFICIENCY: self.data_efficiency_config(),\n                DATA_PARALLEL_GROUP: self.data_parallel_group,\n                GRADIENT_ACCUMULATION_STEPS: self.gradient_accumulation_steps(),\n                GLOBAL_RANK: self.global_rank,\n                DATA_SAMPLING_NUM_WORKERS: self.data_sampling_config()[DATA_SAMPLING_NUM_WORKERS]\n            }\n\n        return DeepSpeedDataLoader(dataset=dataset,\n                                   batch_size=batch_size,\n                                   pin_memory=pin_memory,\n                                   collate_fn=collate_fn,\n                                   local_rank=self.local_rank,\n                                   tput_timer=deepspeed_io_timer,\n                                   num_local_io_workers=num_local_io_workers,\n                                   data_sampler=data_sampler,\n                                   data_parallel_world_size=data_parallel_world_size,\n                                   data_parallel_rank=data_parallel_rank,\n                                   dataloader_drop_last=self.dataloader_drop_last(),\n                                   deepspeed_dataloader_config=deepspeed_dataloader_config)\n\n    def train(self, mode=True):\n        r\"\"\"\"\"\"\n\n        self.warn_unscaled_loss = True\n        self.module.train(mode)\n\n    def eval(self):\n        r\"\"\"\"\"\"\n\n        self.warn_unscaled_loss = True\n        self.module.train(False)\n\n    def _scale_loss_by_gas(self, prescaled_loss, eval_micro_batches=None):\n        # In pipeline evaluation, there is an option to use different micro-bs, which creates different number of\n        # micro batches, thus the training gas, is not valid in this case. need to use the number of eval_micro_batches\n        scaling_factor = self.gradient_accumulation_steps() if eval_micro_batches is None else eval_micro_batches\n        if isinstance(prescaled_loss, torch.Tensor):\n            scaled_loss = prescaled_loss / scaling_factor\n        elif isinstance(prescaled_loss, tuple) or isinstance(prescaled_loss, list):\n            scaled_loss = []\n            for l in prescaled_loss:\n                if isinstance(l, torch.Tensor):\n                    scaled_loss.append(l / scaling_factor)\n                else:\n                    scaled_loss.append(l)\n        else:\n            scaled_loss = prescaled_loss\n            if self.warn_unscaled_loss:\n                logger.warning(f\"DeepSpeed unable to scale loss because of type: {type(prescaled_loss)}\")\n                self.warn_unscaled_loss = False\n\n        return scaled_loss\n\n    @instrument_w_nvtx\n    def forward(self, *inputs, **kwargs):\n        r\"\"\"Execute forward propagation\n        Arguments:\n            *inputs: Variable length input list\n            **kwargs: variable length keyword arguments\n        \"\"\"\n\n        if self.autotuning_profile_model_info():\n            ma = get_ma_status()\n        else:\n            see_memory_usage(\"Engine before forward\", force=self.memory_breakdown())\n\n        flops_profiler_active = (self.flops_profiler_enabled()\n                                 and self.global_steps == self.flops_profiler_profile_step() and self.global_rank == 0)\n\n        # used to check quantization happens at step 0!\n        if self.global_steps == 0 and hasattr(self, \"compression_scheduler\"):\n            self.compression_scheduler.step(step_zero_check=True)\n            if self.quantizer:\n                tensor_to_quantize = self.optimizer.bit16_groups if self.zero_optimization_stage(\n                ) == 2 else self.optimizer.fp16_groups\n                if self.compression_scheduler.weight_quantization_enabled:\n                    self.quantizer.quantize(\n                        tensor_to_quantize,\n                        (self.optimizer.overflow if self.fp16_enabled() else False),\n                        self.eigenvalue_enabled(),\n                        None,\n                    )\n\n        if flops_profiler_active:\n            self.flops_profiler.start_profile(ignore_list=None)\n\n        if self.module.training:\n            if self.progressive_layer_drop:\n                kwargs.update(self.progressive_layer_drop.get_state())\n\n        if self.__class__.__name__ != \"PipelineEngine\":\n            # TODO: The above if condition is a HACK since for PipelineEngine\n            # it's difficult to inject argument in forward pass.\n            if self.module.training and self.curriculum_enabled_legacy():\n                self.curriculum_scheduler_legacy.update_difficulty(self.global_steps + 1)\n                if self.curriculum_params_legacy()[\"curriculum_type\"] == \"seqlen\":\n                    kwargs.update({\"curriculum_seqlen\": self.curriculum_scheduler_legacy.get_current_difficulty()})\n\n        if self.module.training and self.random_ltd_enabled():\n            self.random_ltd_scheduler.update_seq(self.global_steps)\n\n        if self.zero_optimization_partition_weights():\n            # Enable automated discovery of external parameters by indicating that\n            # we are in a forward pass.\n            for module in self.module.modules():\n                module._parameters._in_forward = True\n\n        self._start_timers(self.engine_timers.forward_timers)\n\n        if self.training_dataloader is None:\n            self.tput_timer.start()\n\n        if self.fp16_auto_cast():\n            inputs = self._cast_inputs_half(inputs)\n\n        loss = self.module(*inputs, **kwargs)\n\n        if self.zero_optimization_partition_weights():\n            # Disable automated discovery of external parameters\n            for module in self.module.modules():\n                module._parameters._in_forward = False\n\n        self._stop_timers(self.engine_timers.forward_timers)\n\n        if flops_profiler_active:\n            self.flops_profiler.stop_profile()\n\n        if self.autotuning_profile_model_info():\n            activation_mem = get_ma_status() - ma\n            self.autotuning_model_info[\"activation_mem_per_gpu\"] = activation_mem\n            print_json_dist(self.autotuning_model_info, [0], path=self.autotuning_model_info_path())\n            exit()\n        else:\n            see_memory_usage(\"Engine after forward\", force=self.memory_breakdown())\n        return loss\n\n    def _cast_inputs_half(self, inputs):\n        if isinstance(inputs, (list, tuple)):\n            new_inputs = []\n            for v in inputs:\n                new_inputs.append(self._cast_inputs_half(v))\n            return inputs.__class__(new_inputs)\n        elif isinstance(inputs, dict):\n            new_inputs = {}\n            for k, v in inputs.items():\n                new_inputs[k] = self._cast_inputs_half(v)\n            return new_inputs\n        elif hasattr(inputs, 'half'):\n            return inputs.half()\n        else:\n            return inputs\n\n    def print_forward_breakdown(self, fwd_time):\n        gate_time = 0.0\n        moe_time = 0.0\n        falltoall = 0.0\n        salltoall = 0.0\n\n        for gate in self.gate_modules:\n            #logger.info(f\"Individual TopK gate time: {gate.gate_time:.2f} ms\")\n            gate_time += gate.gate_time\n\n        for l in self.moe_layers:\n            #logger.info(f\"MoE layer; total: {l.time_moe:.2f} ms, first alltoall: {l.time_falltoall:.2f}, second alltoall: {l.time_salltoall:.2f}\")\n            moe_time += l.time_moe\n            falltoall += l.time_falltoall\n            salltoall += l.time_salltoall\n\n        # TODO: Allreduce/average them across ranks for more accurate timing.\n\n        # if deepspeed.comm.get_rank() == 0:\n        log_dist(\n            f\"time (ms) | fwd: {fwd_time:.2f} (fwd_moe: {moe_time:.2f}, 1st_a2a: {falltoall:.2f}, 2nd_a2a: {salltoall:.2f}, top_k: {gate_time:.2f})\",\n            ranks=[0])\n\n    @instrument_w_nvtx\n    def allreduce_gradients(self, bucket_size=MEMORY_OPT_ALLREDUCE_SIZE):\n        # Pass (PP) gas boundary flag to optimizer (required for zero)\n        self.optimizer.is_gradient_accumulation_boundary = self.is_gradient_accumulation_boundary()\n        # ZeRO stage >= 2 communicates during non gradient accumulation boundaries as well\n        if self.zero_optimization_partition_gradients():\n            self.optimizer.overlapping_partition_gradients_reduce_epilogue()\n\n        # Communicate only at gradient accumulation boundaries\n        elif self.is_gradient_accumulation_boundary():\n            if self.zero_optimization_stage() == ZeroStageEnum.optimizer_states and hasattr(\n                    self.optimizer, 'reduce_gradients'):\n                self.optimizer.reduce_gradients(pipeline_parallel=self.pipeline_parallelism)\n            else:\n                grads = None\n                self.buffered_allreduce_fallback(grads=grads, elements_per_buffer=bucket_size)\n\n    @instrument_w_nvtx\n    def backward(self, loss, allreduce_gradients=True, release_loss=False, retain_graph=False, scale_wrt_gas=True):\n        r\"\"\"Execute backward pass on the loss\n        Arguments:\n            loss: Torch tensor on which to execute backward propagation\n            allreduce_gradients: is deprecated, ignored, and will soon be removed'\n            retain_graph: bool, default: false\n                forward on user defined choice of retain_graph\n        \"\"\"\n\n        see_memory_usage(\"Engine before backward\", force=self.memory_breakdown())\n\n        if self.scale_wrt_gas is not None:\n            scale_wrt_gas = self.scale_wrt_gas\n\n        if not allreduce_gradients:\n            logger.warning(f\"Argument `allreduce_gradients` is deprecated, ignored, and will soon be removed\")\n\n        # scale loss w.r.t. gradient accumulation if needed\n        if self.gradient_accumulation_steps() > 1 and scale_wrt_gas:\n            loss = self._scale_loss_by_gas(loss.float())\n\n        # Log training loss\n        mean_loss = loss.mean().detach()\n        self.losses = mean_loss if self.losses is None else self.losses + mean_loss\n        if self.monitor.enabled:\n            if self.is_gradient_accumulation_boundary():\n                if self.global_rank == 0:\n                    self.summary_events = [(\n                        f\"Train/Samples/train_loss\",\n                        self.losses.item(),\n                        self.global_samples,\n                    )]\n                    self.monitor.write_events(self.summary_events)\n\n        self._start_timers(self.engine_timers.backward_timers)\n\n        assert self.optimizer is not None and not isinstance(self.optimizer, DummyOptim), \\\n            \"must provide optimizer during init in order to use backward\"\n\n        self._start_timers(self.engine_timers.backward_inner_timers)\n\n        if self.zero_optimization():\n            self.optimizer.is_gradient_accumulation_boundary = self.is_gradient_accumulation_boundary()\n            self.optimizer.backward(loss, retain_graph=retain_graph)\n        elif self.amp_enabled():\n            # AMP requires delaying unscale when inside gradient accumulation boundaries\n            # https://nvidia.github.io/apex/advanced.html#gradient-accumulation-across-iterations\n            delay_unscale = not self.is_gradient_accumulation_boundary()\n            with amp.scale_loss(loss, self.optimizer, delay_unscale=delay_unscale) as scaled_loss:\n                scaled_loss.backward(retain_graph=retain_graph)\n        elif self.fp16_enabled():\n            if self.eigenvalue_enabled():\n                self.optimizer.backward(loss, create_graph=True, retain_graph=True)\n            else:\n                self.optimizer.backward(loss, retain_graph=retain_graph)\n        elif self.bfloat16_enabled():\n            self.optimizer.backward(loss)\n        else:\n            if self.eigenvalue_enabled():\n                loss.backward(create_graph=True, retain_graph=True)\n            else:\n                loss.backward(retain_graph=retain_graph)\n\n        self._stop_timers(self.engine_timers.backward_inner_timers)\n\n        self._start_timers(self.engine_timers.backward_reduce_timers)\n\n        if allreduce_gradients and self.enable_backward_allreduce:\n            # Traditional code path that allreduces the module parameter grads\n            self.allreduce_gradients()\n\n        self._stop_timers(self.engine_timers.backward_reduce_timers)\n\n        self._stop_timers(self.engine_timers.backward_timers)\n\n        if release_loss:\n            # loss.data = None\n            pass\n\n        see_memory_usage(\"Engine after backward\", force=self.memory_breakdown())\n\n        return loss\n\n    def is_gradient_accumulation_boundary(self):\n        \"\"\"\n        Query whether the current micro-batch is at the boundary of\n        gradient accumulation, and thus will trigger gradient reductions and\n        an optimizer step.\n\n        Returns:\n            bool: if the current step is a gradient accumulation boundary.\n\n        \"\"\"\n        if self._is_gradient_accumulation_boundary is None:\n            return (self.micro_steps + 1) % \\\n                self.gradient_accumulation_steps() == 0\n        else:\n            return self._is_gradient_accumulation_boundary\n\n    def set_gradient_accumulation_boundary(self, is_boundary):\n        \"\"\"\n        Manually overrides the DeepSpeed engine's gradient accumulation boundary state, this is an optional\n        feature and should be used with care. The state should be set before to the intended\n        value before each forward/backward. The final forward/backward should have the\n        boundary state set to True. This style allows client code to only call engine.step() once after all\n        the gradient accumulation passes are complete. See example below:\n        .. code-block:: python\n        engine.set_gradient_accumulation_boundary(False)\n        for _ in range(gradient_accumulation_steps - 1):\n            micro_batch = next(data_loader)\n            loss = engine(micro_batch)\n            engine.backward(loss)\n        engine.set_gradient_accumulation_boundary(True)\n        micro_batch = next(data_loader)\n        loss = engine(micro_batch)\n        engine.backward(loss)\n        engine.step()\n        Arguments:\n            is_boundary (bool): are we at a gradient accumulation boundary or not?\n        \"\"\"\n        self._is_gradient_accumulation_boundary = is_boundary\n        self.optimizer.is_gradient_accumulation_boundary = is_boundary\n\n    def zero_grad(self):\n        \"\"\"\n        Zero parameter grads.\n        \"\"\"\n        for param_name, param in self.module.named_parameters():\n            param.grad = None\n\n    def clip_fp32_gradients(self):\n        clip_grad_norm_(parameters=self.module.parameters(), max_norm=self.gradient_clipping(), mpu=self.mpu)\n\n    def _take_model_step(self, lr_kwargs, block_eigenvalue={}):\n        if self.gradient_clipping() > 0.0:\n            if not (self.fp16_enabled() or self.bfloat16_enabled() or self.amp_enabled() or self.zero_optimization()):\n                self.clip_fp32_gradients()\n            elif self.amp_enabled():\n                # AMP's recommended way of doing clipping\n                # https://nvidia.github.io/apex/advanced.html#gradient-clipping\n                master_params = amp.master_params(self.optimizer)\n                clip_grad_norm_(parameters=master_params, max_norm=self.gradient_clipping(), mpu=self.mpu)\n        self.optimizer.step()\n\n        if hasattr(self.optimizer, '_global_grad_norm'):\n            self._global_grad_norm = self.optimizer._global_grad_norm\n\n        # Quantize the updated parameter if there is no overflow\n        if self.quantizer:\n            tensor_to_quantize = self.optimizer.bit16_groups if self.zero_optimization_stage(\n            ) == 2 else self.optimizer.fp16_groups\n            if self.compression_scheduler.weight_quantization_enabled:\n                self.quantizer.quantize(\n                    tensor_to_quantize,\n                    (self.optimizer.overflow if self.fp16_enabled() else False),\n                    self.eigenvalue_enabled(),\n                    block_eigenvalue,\n                )\n        # zero grad in basic optimizer could be unreliable and may not exhibit\n        # the behavior that we want\n        if self.bfloat16_enabled():\n            # TODO: Temporary until bf16_optimizer and zero_optimizer are integrated\n            if self.zero_optimization() and hasattr(self.optimizer, \"zero_grad\"):\n                self.optimizer.zero_grad()\n            else:\n                pass\n        elif self.zero_optimization() or self.fp16_enabled() or self.amp_enabled():\n            self.optimizer.zero_grad()\n        else:\n            self.zero_grad()\n\n        report_progress = self.global_rank == 0 if self.global_rank else True\n\n        # Check overflow here since in DS fp16 optimizer, the overflow is updated in above step() function.\n        overflow = False\n        if hasattr(self.optimizer, \"overflow\"):\n            overflow = self.optimizer.overflow\n        self._step_applied = not overflow\n\n        if overflow:\n            self.skipped_steps += 1\n        else:\n            self.compression_scheduler.step()\n            if self.lr_scheduler is not None:\n                try:\n                    self.lr_scheduler.step(**(lr_kwargs or {}))\n                except TypeError:\n                    # XXX Hack to work with Megatron 2.0 and DeepSpeed pipelines.\n                    # We don't currently have a way to specify lr_kwargs from\n                    # pipe_engine.train_batch()\n                    self.lr_scheduler.step(self.train_batch_size())\n\n        if report_progress and (self.global_steps + 1) % self.steps_per_print() == 0:\n            self._report_progress(self.global_steps + 1)\n\n        self.losses = None\n        self.global_steps += 1\n        self.global_samples += self.train_batch_size()\n\n    def step(self, lr_kwargs=None):\n        r\"\"\"Execute the weight update step after forward and backward propagation\n        on effective_train_batch.\n        \"\"\"\n        see_memory_usage(\"Engine before step\", force=self.memory_breakdown())\n\n        # Check early because self.global_steps is incremented at some point here.\n        # TODO: Delay self.global_steps increment until very end of this function.\n        flops_profiler_active = self.flops_profiler_enabled(\n        ) and self.global_steps == self.flops_profiler_profile_step() and self.global_rank == 0\n\n        self._start_timers(self.engine_timers.step_timers)\n\n        assert self.optimizer is not None and not isinstance(self.optimizer, DummyOptim), \\\n            \"must provide optimizer during init in order to use step\"\n\n        report_progress = False\n\n        self._step_applied = False  # assume False, will flip to True\n\n        # Update the model when we reach gradient accumulation boundaries\n        if self.is_gradient_accumulation_boundary():\n            self.gas_boundary_ctr += 1\n\n            if (self.eigenvalue_enabled() and (self.gas_boundary_ctr % self.eigenvalue_gas_boundary_resolution() == 0)\n                    and self.quantizer.any_precision_switch()):\n                log_dist(f\"computing eigenvalue...\", ranks=[0])\n                self.block_eigenvalue = self.eigenvalue.compute_eigenvalue(self.module, self.device,\n                                                                           self.optimizer.cur_scale)\n\n            if self.progressive_layer_drop:\n                self.progressive_layer_drop.update_state(self.global_steps)\n\n            if (self.eigenvalue_enabled() and not self.gas_boundary_ctr % self.eigenvalue_gas_boundary_resolution()\n                    and self.quantizer.any_precision_switch()):\n                self._take_model_step(lr_kwargs, self.block_eigenvalue)\n            else:\n                self._take_model_step(lr_kwargs)\n\n            report_progress = self.global_rank == 0 if self.global_rank else True\n\n        self.tput_timer.stop(global_step=self.is_gradient_accumulation_boundary(), report_speed=report_progress)\n\n        self._stop_timers(self.engine_timers.step_timers)\n\n        # Log learning rate\n        if self.monitor.enabled:\n            if self.is_gradient_accumulation_boundary():\n                if self.global_rank == 0:\n                    self.summary_events = [(f\"Train/Samples/lr\", self.get_lr()[0], self.global_samples)]\n\n                    if self.fp16_enabled() and hasattr(self.optimizer, \"cur_scale\"):\n                        self.summary_events.append((\n                            f\"Train/Samples/loss_scale\",\n                            self.optimizer.cur_scale,\n                            self.global_samples,\n                        ))\n\n                    if (self.eigenvalue_enabled()\n                            and not self.gas_boundary_ctr % self.eigenvalue_gas_boundary_resolution()):\n                        ev_values = self.block_eigenvalue.values()\n                        for i in range(len(ev_values)):\n                            self.summary_events.append((\n                                f\"Train/Eigenvalues/ModelBlockParam_{i}\",\n                                self.ev_values[i][0],\n                                self.global_samples,\n                            ))\n                    self.monitor.write_events(self.summary_events)\n\n        # Check flops profiling\n        if flops_profiler_active:\n            if self.autotuning_enabled():\n                self.flops = self.flops_profiler.get_total_flops() * 3\n                self.fwd_duration = self.flops_profiler.get_total_duration()\n            else:\n                self.flops_profiler.print_model_profile(\n                    profile_step=self.global_steps,\n                    module_depth=self.flops_profiler_module_depth(),\n                    top_modules=self.flops_profiler_top_modules(),\n                    detailed=self.flops_profiler_detailed(),\n                    output_file=self.flops_profiler_output_file(),\n                )\n            self.flops_profiler.end_profile()\n\n        if self.autotuning_enabled() and self.global_steps == (self.autotuning_end_profile_step() + 1):\n            self._autotuning_exit()\n\n        if self.wall_clock_breakdown():\n            # Log micro timing and reset\n            self.timers.log(names=self.engine_timers.micro_timers, memory_breakdown=self.memory_breakdown())\n\n        if self.wall_clock_breakdown() or self.flops_profiler_enabled():\n            # Log global timing and reset\n            if self.is_gradient_accumulation_boundary():\n                if self.monitor.enabled:\n                    self._write_monitor()\n\n                if self.has_moe_layers:\n                    fwd_time = self.timers(FORWARD_GLOBAL_TIMER).elapsed(reset=False)\n                    self.print_forward_breakdown(fwd_time=fwd_time)\n\n                self.timers.log(self.engine_timers.global_timers)\n\n        self.micro_steps += 1\n        see_memory_usage(\"Engine after step\", force=self.memory_breakdown())\n\n    def _start_timers(self, timer_names):\n        for name in timer_names:\n            self.timers(name).start()\n\n    def _stop_timers(self, timer_names):\n        record = self.is_gradient_accumulation_boundary() and \\\n            self.flops_profiler_enabled() and \\\n                (self.global_steps >= self.flops_profiler_profile_step())\n        for name in timer_names:\n            self.timers(name).stop(record=record)\n\n    def _autotuning_exit(self):\n        if self.global_rank == 0:\n            msg = self.timers.get_mean([\n                FORWARD_GLOBAL_TIMER,\n                BACKWARD_GLOBAL_TIMER,\n                STEP_GLOBAL_TIMER,\n            ], reset=False)\n            titer = 0.0\n            titer += msg[FORWARD_GLOBAL_TIMER] if FORWARD_GLOBAL_TIMER in msg else 0\n            titer += msg[BACKWARD_GLOBAL_TIMER] if BACKWARD_GLOBAL_TIMER in msg else 0\n            titer += msg[STEP_GLOBAL_TIMER] if STEP_GLOBAL_TIMER in msg else 0\n            titer *= self.gradient_accumulation_steps()\n            msg[\"latency\"] = titer\n            msg[\"FLOPS_per_gpu\"] = self.flops * 1_000_000 * self.gradient_accumulation_steps() / titer\n            msg[\"throughput\"] = self.train_batch_size() * 1_000_000 / \\\n                msg[\"latency\"]\n            print_json_dist(msg, [0], path=self.autotuning_metric_path())\n            log_dist(\n                f\"Wrote metrics to {self.autotuning_metric_path()}, {os.path.abspath(self.autotuning_metric_path())}\",\n                ranks=[0])\n            import atexit\n            atexit.register(print, \"Autotuning: done with running current ds config.\")\n        exit()\n\n    def _write_monitor(self):\n        if self.global_rank == 0:\n            self.summary_events = [\n                (\n                    f\"Train/Samples/elapsed_time_ms_forward\",\n                    self.timers(FORWARD_GLOBAL_TIMER).elapsed(reset=False),\n                    self.global_samples,\n                ),\n                (\n                    f\"Train/Samples/elapsed_time_ms_backward\",\n                    self.timers(BACKWARD_GLOBAL_TIMER).elapsed(reset=False),\n                    self.global_samples,\n                ),\n                (\n                    f\"Train/Samples/elapsed_time_ms_backward_inner\",\n                    self.timers(BACKWARD_INNER_GLOBAL_TIMER).elapsed(reset=False),\n                    self.global_samples,\n                ),\n                (\n                    f\"Train/Samples/elapsed_time_ms_backward_allreduce\",\n                    self.timers(BACKWARD_REDUCE_GLOBAL_TIMER).elapsed(reset=False),\n                    self.global_samples,\n                ),\n                (\n                    f\"Train/Samples/elapsed_time_ms_step\",\n                    self.timers(STEP_GLOBAL_TIMER).elapsed(reset=False),\n                    self.global_samples,\n                ),\n            ]\n            self.monitor.write_events(self.summary_events)\n\n    def _get_optimizer_param(self, param_name):\n        result = []\n        if not self.optimizer:\n            return result\n        for group in self.optimizer.param_groups:\n            if param_name in group:\n                result.append(group[param_name])\n            else:\n                result.append(0.0)\n        return result\n\n    def get_lr(self):\n        return self._get_optimizer_param(\"lr\")\n\n    def get_type(self):\n        return self._get_optimizer_param(\"type\")\n\n    def get_mom(self):\n        if self.optimizer_name() in [\"SGD\", \"RMSprop\"]:\n            return self._get_optimizer_param(\"momentum\")\n        else:\n            return self._get_optimizer_param(\"betas\")\n\n    def get_pld_theta(self):\n        if self.progressive_layer_drop:\n            return self.progressive_layer_drop.get_theta()\n        else:\n            return None\n\n    def _report_progress(self, step):\n        lr = self.get_lr()\n        mom = self.get_mom()\n        log_dist(f\"step={step}, skipped={self.skipped_steps}, lr={lr}, mom={mom}\", ranks=[0])\n\n    def allreduce_bucket(self, bucket, dp_group, dp_world_size=None):\n        tensor = self.flatten(bucket)\n\n        tensor_to_allreduce = tensor\n\n        if self.communication_data_type != tensor.dtype:\n            tensor_to_allreduce = tensor.to(self.communication_data_type)\n\n        if dp_world_size is None:\n            dp_world_size = dist.get_world_size(group=dp_group)\n        if self.postscale_gradients():\n            if self.gradient_predivide_factor() != 1.0:\n                tensor_to_allreduce.mul_(1.0 / self.gradient_predivide_factor())\n\n            dist.all_reduce(tensor_to_allreduce, group=dp_group)\n            if self.gradient_average:\n                if self.gradient_predivide_factor() != dp_world_size:\n                    tensor_to_allreduce.mul_(self.gradient_predivide_factor() / dp_world_size)\n        else:\n            tensor_to_allreduce.mul_(1. / dp_world_size)\n            dist.all_reduce(tensor_to_allreduce, group=dp_group)\n\n        if self.communication_data_type != tensor.dtype and tensor is not tensor_to_allreduce:\n            tensor.copy_(tensor_to_allreduce)\n\n        return tensor\n\n    def allreduce_and_copy(self, small_bucket, dp_group, dp_world_size=None):\n        allreduced = self.allreduce_bucket(small_bucket, dp_group, dp_world_size)\n        for buf, synced in zip(small_bucket, self.unflatten(allreduced, small_bucket)):\n            buf.copy_(synced)\n\n    def allreduce_no_retain(self, bucket, dp_group, numel_per_bucket=500000000, dp_world_size=None):\n        small_bucket = []\n        numel = 0\n        for tensor in bucket:\n            small_bucket.append(tensor)\n            numel = numel + tensor.numel()\n            if numel > numel_per_bucket:\n                self.allreduce_and_copy(small_bucket, dp_group, dp_world_size)\n                small_bucket = []\n                numel = 0\n        if len(small_bucket) > 0:\n            self.allreduce_and_copy(small_bucket, dp_group, dp_world_size)\n\n    def _get_gradients_for_reduction(self):\n        non_expert_grads = []\n        expert_grads = {}\n        if self.has_moe_layers:\n            for key in self.expert_data_parallel_group.keys():\n                expert_grads[key] = []\n\n        for param_name, param in self.module.named_parameters():\n            if not param.requires_grad:\n                continue\n\n            if param.grad is None:\n                # In cases where there is an imbalance of empty grads across\n                # ranks we must create empty grads, this will ensure that every\n                # rank is reducing the same size. In some cases it may make\n                # sense in the future to support the ability to average not\n                # w.r.t. world size but with a different value.\n                param.grad = torch.zeros(param.size(), dtype=param.dtype, device=param.device)\n\n            grad_data = param.grad.data\n            if param_name in self.sparse_tensor_module_names or grad_data.is_sparse:\n                # Call param.grad without data to avoid problem with setting of updated grads\n                grad_data = SparseTensor(param.grad)\n\n            if is_moe_param(param):\n                expert_grads[param.group_name].append(grad_data)\n            else:\n                non_expert_grads.append(grad_data)\n\n        return non_expert_grads, expert_grads\n\n    def _reduce_non_expert_gradients(self, grads, elements_per_buffer):\n        split_sparse_tensor_buckets, split_dense_tensor_buckets = split_half_float_double_sparse(grads)\n        if self.pipeline_parallelism:\n            dp_group = self.mpu.get_data_parallel_group()\n            dp_world_size = dist.get_world_size(dp_group)\n        else:\n            dp_group = groups._get_sequence_data_parallel_group()\n            dp_world_size = dist.get_world_size(dp_group) / float(self.sequence_parallel_size)\n        for _, sparse_bucket_tuple in enumerate(split_sparse_tensor_buckets):\n            if sparse_bucket_tuple:\n                bucket_type, sparse_bucket = sparse_bucket_tuple\n                self.sparse_allreduce_no_retain(sparse_bucket, dp_group=dp_group, dp_world_size=dp_world_size)\n\n        for _, dense_bucket_tuple in enumerate(split_dense_tensor_buckets):\n            if dense_bucket_tuple:\n                bucket_type, dense_bucket = dense_bucket_tuple\n                self.allreduce_no_retain(dense_bucket,\n                                         dp_group=dp_group,\n                                         numel_per_bucket=elements_per_buffer,\n                                         dp_world_size=dp_world_size)\n\n    def _reduce_expert_gradients(self, expert_grads, elements_per_buffer):\n        # to maintain the gradients value unaffected by ep_size setting,\n        # utilize dp_world_size for allreduce average\n        dp_world_size = dist.get_world_size(groups._get_data_parallel_group())\n        for ep_name, expert_grads_group in expert_grads.items():\n            ep_dp_group = groups._get_expert_data_parallel_group(ep_name)\n            split_sparse_tensor_buckets, split_dense_tensor_buckets = split_half_float_double_sparse(\n                expert_grads_group)\n\n            for _, sparse_bucket_tuple in enumerate(split_sparse_tensor_buckets):\n                if sparse_bucket_tuple:\n                    bucket_type, sparse_bucket = sparse_bucket_tuple\n                    self.sparse_allreduce_no_retain(sparse_bucket, dp_group=ep_dp_group, dp_world_size=dp_world_size)\n\n            for _, dense_bucket_tuple in enumerate(split_dense_tensor_buckets):\n                if dense_bucket_tuple:\n                    bucket_type, dense_bucket = dense_bucket_tuple\n                    # Separate between diff groups\n                    self.allreduce_no_retain(dense_bucket,\n                                             dp_group=ep_dp_group,\n                                             numel_per_bucket=elements_per_buffer,\n                                             dp_world_size=dp_world_size)\n\n    def buffered_allreduce_fallback(self, grads=None, elements_per_buffer=500000000):\n        if grads is None:\n            if hasattr(self.optimizer, \"get_grads_for_reduction\"):\n                # This is currently for BF16 optimizer\n                non_expert_grads, expert_grads = self.optimizer.get_grads_for_reduction()\n            else:\n                non_expert_grads, expert_grads = self._get_gradients_for_reduction()\n        else:\n            assert not self.has_moe_layers, \"attempting to reduce grads in unsupported way w.r.t. MoE\"\n            non_expert_grads = grads\n\n        self._reduce_non_expert_gradients(non_expert_grads, elements_per_buffer)\n\n        if self.has_moe_layers:\n            self._reduce_expert_gradients(expert_grads, elements_per_buffer)\n\n    def sparse_allreduce_no_retain(self, bucket, dp_group, dp_world_size=None):\n        allreduced_sparses = self.sparse_allreduce_bucket(bucket, dp_group, dp_world_size)\n        # Densify sparse tensor and copy back to original location\n        for tensor in allreduced_sparses:\n            if tensor.is_sparse:\n                tensor.orig_dense_tensor.data = tensor.to_coo_tensor()\n            else:\n                tensor.orig_dense_tensor.copy_(tensor.to_dense())\n\n    def sparse_allreduce_bucket(self, bucket, dp_group, dp_world_size=None):\n        sparse_list = []\n        for sparse in bucket:\n            sparse_list.append(self.sparse_allreduce(sparse, dp_group, dp_world_size))\n        return sparse_list\n\n    def sparse_allreduce(self, sparse, dp_group, dp_world_size=None):\n        original_data_type = sparse.values.dtype\n        if self.communication_data_type != sparse.values.dtype:\n            if self.communication_data_type in (torch.float16, torch.bfloat16):\n                indices = sparse.indices.to(torch.int32)\n            else:\n                indices = sparse.indices\n            values = sparse.values.to(self.communication_data_type)\n        else:\n            indices = sparse.indices\n            values = sparse.values\n\n        if dp_world_size is None:\n            dp_world_size = dist.get_world_size(group=dp_group)\n        if self.postscale_gradients():\n            if self.gradient_average:\n                values.mul_(self.gradient_predivide_factor() / (dp_world_size))\n        else:\n            values.mul_(1. / (dp_world_size))\n\n        indices_device_list = self.sparse_all_gather(indices, dp_group)\n        values_device_list = self.sparse_all_gather(values, dp_group)\n\n        sparse.indices = torch.cat(indices_device_list).to(torch.long)\n        sparse.values = torch.cat(values_device_list).to(original_data_type)\n        return sparse\n\n    def sparse_all_gather(self, value, dp_group):\n        my_size = torch.LongTensor([value.size()[0]]).to(self.device)\n        all_sizes = self.all_gather_scalar(my_size, dp_group)\n        max_size = torch.cat(all_sizes).max()\n        fill_size = max_size - my_size\n\n        assert value.dim() in [1, 2]\n        if value.dim() == 1:\n            if fill_size > 0:\n                value = torch.cat([value, value.new_empty(fill_size)])\n            tensor_list = [value.new_empty(max_size) for _ in range(dist.get_world_size(group=dp_group))]\n        else:\n            if fill_size > 0:\n                value = torch.cat([value, value.new_empty(fill_size, value.size()[1])])\n            tensor_list = [\n                value.new_empty(max_size,\n                                value.size()[1]) for _ in range(dist.get_world_size(group=dp_group))\n            ]\n\n        dist.all_gather(tensor_list, value, group=dp_group)\n        tensors = []\n        for dev_idx, t in enumerate(tensor_list):\n            size = all_sizes[dev_idx][0]\n            tensors.append(t.index_select(0, torch.arange(size, dtype=torch.long, device=self.device)))\n\n        return tensors\n\n    def all_gather_scalar(self, value, dp_group):\n        tensor_list = [value.new_zeros(value.size()) for _ in range(dist.get_world_size(group=dp_group))]\n        dist.all_gather(tensor_list, value, group=dp_group)\n        return tensor_list\n\n    def module_state_dict(self, destination=None, prefix=\"\", keep_vars=False, exclude_frozen_parameters=False):\n        sd = self.module.state_dict(destination=destination, prefix=prefix, keep_vars=keep_vars)\n\n        # Remove frozen parameter weights from state_dict if specified\n        if exclude_frozen_parameters:\n            for n, p in self.module.named_parameters():\n                if not p.requires_grad and n in sd:\n                    del sd[n]\n\n        if self.random_ltd_enabled():\n            sd = remove_random_ltd_state_dict(sd)\n        return sd\n\n    @staticmethod\n    def load_moe_state_dict(checkpoint_path,\n                            tag,\n                            state_dict,\n                            old_moe_load,\n                            model=None,\n                            mpu=None,\n                            num_experts=1,\n                            checkpoint_engine=TorchCheckpointEngine()):\n        if old_moe_load:\n            expp_rank = groups._get_expert_data_parallel_rank(groups._get_max_expert_size_name())\n\n            num_local_experts = max(num_experts) // groups._get_expert_parallel_world_size(\n                groups._get_max_expert_size_name())\n            for local_expert_id in range(num_local_experts):\n                global_expert_id = expp_rank * num_local_experts + local_expert_id\n                expert_state_dict = checkpoint_engine.load(\n                    DeepSpeedEngine._get_expert_ckpt_name(\n                        checkpoint_path,\n                        -1,  # -1 means ignore layer_id\n                        global_expert_id,\n                        tag,\n                        mpu),\n                    map_location=torch.device('cpu'))\n\n                # Updating global -> local expert ids\n                moe_str_prefix = '.deepspeed_moe.experts.deepspeed_experts.'\n                for key in list(expert_state_dict.keys()):\n                    local_key = key.replace(f'{moe_str_prefix}{global_expert_id}',\n                                            f'{moe_str_prefix}{local_expert_id}')\n                    expert_state_dict[local_key] = expert_state_dict.pop(key)\n                state_dict.update(expert_state_dict)\n\n        else:\n            moe_layer_id = 0\n            for n_module, module in model.named_modules():\n                if isinstance(module, MoE):  # and deepspeed.comm.get_rank() == 0:\n                    group_name = module.expert_group_name\n                    num_local_experts = module.num_local_experts\n                    expp_rank = groups._get_expert_parallel_rank(group_name)\n                    # loop all local_experts\n                    for local_expert_id in range(num_local_experts):\n                        global_expert_id = expp_rank * num_local_experts + local_expert_id\n                        expert_state_dict = checkpoint_engine.load(DeepSpeedEngine._get_expert_ckpt_name(\n                            checkpoint_path, moe_layer_id, global_expert_id, tag, mpu),\n                                                                   map_location=torch.device('cpu'))\n                        # print(expert_state_dict.keys())\n                        # Updating global -> local expert ids\n                        moe_str_prefix = '.deepspeed_moe.experts.deepspeed_experts.'\n                        for key in list(expert_state_dict.keys()):\n                            local_key = key.replace(f'{moe_str_prefix}{global_expert_id}',\n                                                    f'{moe_str_prefix}{local_expert_id}')\n                            expert_state_dict[local_key] = expert_state_dict.pop(key)\n                        state_dict.update(expert_state_dict)\n                    moe_layer_id += 1\n\n    def load_module_state_dict(self, checkpoint, strict=True, custom_load_fn=None, fetch_z3_params=False):\n        if fetch_z3_params:\n            params_to_fetch = [\n                p for p in self.module.parameters()\n                if hasattr(p, 'ds_id') and p.ds_status == ZeroParamStatus.NOT_AVAILABLE\n            ]\n        else:\n            params_to_fetch = []\n\n        with deepspeed.zero.GatheredParameters(params_to_fetch, modifier_rank=0):\n            module_state_dict = checkpoint['module']\n            if custom_load_fn:\n                custom_load_fn(src=module_state_dict, dst=self.module)\n            else:\n                self.module.load_state_dict(\n                    module_state_dict,  # TODO\n                    strict=strict)\n\n        if checkpoint.get(FROZEN_PARAM_FRAGMENTS, None) is not None:\n            saved_frozen_params = checkpoint[FROZEN_PARAM_FRAGMENTS]\n            for param in self.module.parameters():\n                if param.requires_grad:\n                    continue\n                if param not in self.param_names:\n                    raise ValueError(f\"failed to find frozen {param} in named params\")\n                name = self.param_names[param]\n                if hasattr(param, 'ds_id'):\n                    param.ds_tensor.data.copy_(saved_frozen_params[name].data)\n                else:\n                    param.data.copy_(saved_frozen_params[name].data)\n\n    def _get_zero_ckpt_prefix(self, dp_rank, bf16_mode):\n        return f'{\"bf16_\" if bf16_mode else \"\"}zero_pp_rank_{dp_rank}'\n\n    def _get_rank_zero_ckpt_name(self, checkpoints_path, tag, mp_rank, dp_rank, bf16_mode):\n        file_prefix = self._get_zero_ckpt_prefix(dp_rank, bf16_mode=bf16_mode)\n        zero_ckpt_name = os.path.join(\n            checkpoints_path,\n            str(tag),\n            f\"{file_prefix}_mp_rank_{mp_rank:02d}_optim_states.pt\",\n        )\n        return zero_ckpt_name\n\n    def _get_zero_ckpt_name(self, checkpoints_path, tag):\n        mp_rank = 0 if self.mpu is None else self.mpu.get_model_parallel_rank()\n        pp_rank = dist.get_rank(group=self.optimizer.dp_process_group)\n        bf16_mode = self.bfloat16_enabled()\n        return self._get_rank_zero_ckpt_name(checkpoints_path, tag, mp_rank, pp_rank, bf16_mode)\n\n    def _get_ckpt_name(self, checkpoints_path, tag, mp_placeholder=None):\n        if mp_placeholder is not None:\n            mp_rank_str = mp_placeholder\n        else:\n            mp_rank = 0 if self.mpu is None else self.mpu.get_model_parallel_rank()\n            mp_rank_str = f\"{mp_rank:02d}\"\n\n        if self.zero_optimization_partition_weights():\n            filename = \"zero_pp_rank_{}\".format(dist.get_rank(group=self.optimizer.dp_process_group))\n            ckpt_name = os.path.join(\n                checkpoints_path,\n                str(tag),\n                f\"{filename}_mp_rank_{mp_rank_str}_model_states.pt\",\n            )\n        else:\n            ckpt_name = os.path.join(\n                checkpoints_path,\n                str(tag),\n                \"mp_rank_\" + mp_rank_str + \"_model_states.pt\",\n            )\n        return ckpt_name\n\n    def _get_optimizer_ckpt_name(self, checkpoints_path, tag, expp_rank):\n        mp_rank = 0 if self.mpu is None else self.mpu.get_model_parallel_rank()\n        ckpt_name = os.path.join(checkpoints_path, str(tag),\n                                 f'expp_rank_{expp_rank}_mp_rank_{mp_rank:02d}_optim_states.pt')\n        return ckpt_name\n\n    @staticmethod\n    def _get_expert_ckpt_name(checkpoints_path, layer_id, expert_id, tag, mpu=None):\n        mp_rank = 0 if mpu is None else mpu.get_model_parallel_rank()\n        if layer_id <= -1:\n            # Used to support old checkpoint loading\n            ckpt_name = os.path.join(checkpoints_path, '' if tag is None else str(tag),\n                                     f'expert_{expert_id}_mp_rank_{mp_rank:02d}_model_states.pt')\n        else:\n            # Used to support new checkpoint loading\n            ckpt_name = os.path.join(checkpoints_path, '' if tag is None else str(tag),\n                                     f'layer_{layer_id}_expert_{expert_id}_mp_rank_{mp_rank:02d}_model_states.pt')\n        return ckpt_name\n\n    def _get_all_ckpt_names(self, checkpoints_path, tag):\n        # It is required that (checkpoints_path, tag) are consistent among all ranks.\n        ckpt_file_pattern = self._get_ckpt_name(checkpoints_path, tag, mp_placeholder=\"*\")\n        import glob\n\n        ckpt_files = glob.glob(ckpt_file_pattern)\n        ckpt_files.sort()\n        return ckpt_files\n\n    def load_checkpoint(self,\n                        load_dir,\n                        tag=None,\n                        load_module_strict=True,\n                        load_optimizer_states=True,\n                        load_lr_scheduler_states=True,\n                        load_module_only=False,\n                        custom_load_fn=None):\n        \"\"\"\n        Load training checkpoint\n\n        Arguments:\n            load_dir: Required. Directory to load the checkpoint from\n            tag: Checkpoint tag used as a unique identifier for checkpoint, if not provided will attempt to load tag in 'latest' file\n            load_module_strict: Optional. Boolean to strictly enforce that the keys in state_dict of module and checkpoint match.\n            load_optimizer_states: Optional. Boolean to load the training optimizer states from Checkpoint. Ex. ADAM's momentum and variance\n            load_lr_scheduler_states: Optional. Boolean to add the learning rate scheduler states from Checkpoint.\n            load_module_only: Optional. Boolean to load only the model weights from the checkpoint. Ex. warmstarting.\n            custom_load_fn: Optional. Custom model load function.\n\n        Returns:\n            A tuple of ``load_path`` and ``client_state``.\n            *``load_path``: Path of the loaded checkpoint. ``None`` if loading the checkpoint failed.\n            *``client_state``: State dictionary used for loading required training states in the client code.\n\n        Important: under ZeRO3, one cannot load checkpoint with ``engine.load_checkpoint()`` right\n        after ``engine.save_checkpoint()``. It is because ``engine.module`` is partitioned, and\n        ``load_checkpoint()`` wants a pristine model. If insisting to do so, please reinitialize engine\n        before ``load_checkpoint()``.\n\n        \"\"\"\n\n        if tag is None:\n            latest_tag = \"latest_universal\" if self.load_universal_checkpoint() else \"latest\"\n            latest_path = os.path.join(load_dir, latest_tag)\n            if os.path.isfile(latest_path):\n                with open(latest_path, \"r\") as fd:\n                    tag = fd.read().strip()\n            else:\n                if self.load_universal_checkpoint():\n                    raise ValueError(f'Invalid for universal checkpoint: {latest_path} does not exist')\n                else:\n                    logger.warning(\n                        f\"Unable to find latest file at {latest_path}, if trying to load latest \"\n                        \"checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.\"\n                    )\n                    return None, None\n\n        if self._optimizer_has_ckpt_event_prologue():\n            # Prepare for checkpoint load by ensuring all parameters are partitioned\n            self.optimizer.checkpoint_event_prologue()\n\n        load_path, client_states = self._load_checkpoint(load_dir,\n                                                         tag,\n                                                         load_module_strict=load_module_strict,\n                                                         load_optimizer_states=load_optimizer_states,\n                                                         load_lr_scheduler_states=load_lr_scheduler_states,\n                                                         load_module_only=load_module_only,\n                                                         custom_load_fn=custom_load_fn)\n\n        load_zero_checkpoint = load_path is not None and (self.zero_optimization() or self.bfloat16_enabled())\n        if load_zero_checkpoint:\n            if (load_optimizer_states and not load_module_only) or self.load_universal_checkpoint():\n                success = self._load_zero_checkpoint(load_dir, tag, load_optimizer_states=load_optimizer_states)\n            else:\n                success = False\n            if not success:\n                self.optimizer._restore_from_bit16_weights()\n\n        if self.zero_has_nvme_offload():\n            from shutil import copytree, disk_usage\n            offload_dir = self.optimizer.optimizer_swapper.swap_folder\n            offload_ckpt_dir = os.path.join(load_dir, tag, \"offloaded_tensors\")\n            _, _, free = disk_usage(offload_dir)\n            logger.info(\n                f\"Copying NVMe offload checkpoint from {offload_ckpt_dir} to {offload_dir}, {free / 1e9:,.2f} GB free on target filesystem...\"\n            )\n            copytree(offload_ckpt_dir, offload_dir, dirs_exist_ok=True)\n            _, _, free = disk_usage(offload_dir)\n            logger.info(f\"Copying complete! {free / 1e9:,.2f} GB free on target filesystem\")\n            self.optimizer.reset_swap_buffers()\n\n        if self._optimizer_has_ckpt_event_epilogue():\n            self.optimizer.checkpoint_event_epilogue()\n\n        if self.load_universal_checkpoint():\n            self.optimizer.update_lp_params()\n\n        return load_path, client_states\n\n    def _load_checkpoint(self,\n                         load_dir,\n                         tag,\n                         load_module_strict=True,\n                         load_optimizer_states=True,\n                         load_lr_scheduler_states=True,\n                         load_module_only=False,\n                         custom_load_fn=None):\n\n        from deepspeed.runtime.state_dict_factory import SDLoaderFactory\n\n        ckpt_list = self._get_all_ckpt_names(load_dir, tag)\n        sd_loader = SDLoaderFactory.get_sd_loader(ckpt_list, checkpoint_engine=self.checkpoint_engine)\n\n        is_pipe_parallel = isinstance(self.module, PipelineModule)\n\n        mp_rank = 0 if self.mpu is None else self.mpu.get_model_parallel_rank()\n        load_path, checkpoint, _ = sd_loader.load(self.mp_world_size, mp_rank, is_pipe_parallel=is_pipe_parallel)\n\n        if checkpoint is None:\n            return None, None\n\n        fetch_z3_params = False\n        if self.zero_optimization_partition_weights() and not load_optimizer_states:\n            checkpoint['module'] = get_fp32_state_dict_from_zero_checkpoint(load_dir)\n            fetch_z3_params = True\n\n        if is_pipe_parallel:\n            # Pipeline parallelism uses this to load its own checkpoint files.\n            self._curr_ckpt_path = os.path.join(load_dir, tag)\n\n        if self.has_moe_layers:\n            # print(checkpoint.keys())\n            old_moe_load = False\n            if not isinstance(checkpoint['num_experts'], list):\n                old_moe_load = True\n            DeepSpeedEngine.load_moe_state_dict(load_dir,\n                                                tag,\n                                                state_dict=checkpoint['module'],\n                                                old_moe_load=old_moe_load,\n                                                model=self.module,\n                                                mpu=self.mpu,\n                                                num_experts=self.num_experts,\n                                                checkpoint_engine=self.checkpoint_engine)\n        if not self.load_universal_checkpoint():\n            self.load_module_state_dict(checkpoint=checkpoint,\n                                        strict=load_module_strict,\n                                        custom_load_fn=custom_load_fn,\n                                        fetch_z3_params=fetch_z3_params)\n\n        self.loaded_checkpoint_dp_world_size = checkpoint['dp_world_size']\n\n        optim_checkpoint = None\n        if load_module_only:\n            deepspeed_states = ['module']\n            if self.optimizer is not None:\n                self.optimizer.refresh_fp32_params()\n        else:\n            has_zero_optimizer_state = self.zero_optimization() or self.bfloat16_enabled()\n            if load_optimizer_states and self.optimizer is not None and not has_zero_optimizer_state:\n                if self.has_moe_layers:\n                    largest_group_name = groups._get_max_expert_size_name()\n                    expp_rank = groups._get_expert_parallel_rank(largest_group_name)\n                    optim_load_path = self._get_optimizer_ckpt_name(load_dir, tag, expp_rank)\n                    optim_checkpoint = self.checkpoint_engine.load(optim_load_path, map_location=torch.device('cpu'))\n                else:\n                    optim_checkpoint = checkpoint\n\n                if self.fp16_enabled() or self.bfloat16_enabled():\n                    self.optimizer.load_state_dict(optim_checkpoint['optimizer'],\n                                                   load_optimizer_states=load_optimizer_states)\n                else:\n                    optim_checkpoint = checkpoint\n\n                self.optimizer.load_state_dict(optim_checkpoint['optimizer'])\n\n            if load_lr_scheduler_states and self.lr_scheduler is not None:\n                self.lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n\n            if self.random_ltd_enabled() and self.random_ltd_scheduler is not None and 'random_ltd' in checkpoint:\n                self.random_ltd_scheduler.load_state_dict(checkpoint['random_ltd'])\n\n            if self.training_dataloader is not None and self.curriculum_learning_enabled(\n            ) and 'data_sampler' in checkpoint:\n                self.training_dataloader.data_sampler.load_state_dict(checkpoint['data_sampler'])\n\n            def get_sparse_tensor_module_names(original_set, loaded_set, original_parameters, loaded_parameters):\n                result = set()\n\n                for name in original_set:\n                    if name in loaded_parameters and name not in loaded_set:\n                        continue  # parameter existed in previous model and was not sparse\n                    result.add(name)\n\n                for name in loaded_set:\n                    if name in original_parameters:\n                        result.add(name)  # parameter exists in both configs and it was sparse\n\n                return result\n\n            if 'sparse_tensor_module_names' in checkpoint:\n                sparse_tensor_module_names = checkpoint['sparse_tensor_module_names']\n            elif 'csr_tensor_module_names' in checkpoint:\n                sparse_tensor_module_names = checkpoint['csr_tensor_module_names']\n            else:\n                sparse_tensor_module_names = None\n            if sparse_tensor_module_names is not None:\n                if load_module_strict:\n                    self.sparse_tensor_module_names = sparse_tensor_module_names\n                else:\n                    self.sparse_tensor_module_names = get_sparse_tensor_module_names(\n                        self.sparse_tensor_module_names, sparse_tensor_module_names,\n                        dict(self.module.named_parameters()), checkpoint[\"module\"])\n\n            self.global_steps = checkpoint['global_steps']\n            self.global_samples = checkpoint.get('global_samples', self.global_steps * self.train_batch_size())\n            self.skipped_steps = checkpoint['skipped_steps']\n            self.loaded_checkpoint_mp_world_size = checkpoint['mp_world_size']\n            deepspeed_states = [\n                'module', 'sparse_tensor_module_names', 'skipped_steps', 'global_steps', 'dp_world_size',\n                'mp_world_size', 'data_sampler', 'random_ltd'\n            ]\n        client_state = {}\n\n        if load_lr_scheduler_states:\n            deepspeed_states.append('lr_scheduler')\n        if load_optimizer_states:\n            deepspeed_states.append('optimizer')\n\n        client_state = {key: value for key, value in checkpoint.items() if not key in deepspeed_states}\n\n        if optim_checkpoint is not None:\n            client_state['optimizer'] = optim_checkpoint['optimizer']\n\n        return load_path, client_state\n\n    def _load_zero_checkpoint(self, load_dir, tag, load_optimizer_states=True):\n\n        load_serial = None\n        # When use loading checkpoint serial, checkpoint loading start from local rank 0,\n        # all other local rank would be paused, waiting for its rank-1 peer ready and its notification.\n        if self._config.zero_config.pipeline_loading_checkpoint:\n            assert self.zero_optimization_stage(\n            ) == ZeroStageEnum.weights, \"Only stage3 support for pipeline checkpoint loading\"\n            load_serial = torch.zeros(1).to(self.device)\n            if dist.get_local_rank() != 0:\n                dist.recv(tensor=load_serial, src=dist.get_rank() - 1)\n        if self.load_universal_checkpoint():\n            zero_sd_list = None\n            checkpoint_folder = f'{os.path.join(load_dir, tag)}'\n        else:\n            if load_optimizer_states and self.seq_dp_world_size != self.loaded_checkpoint_dp_world_size:\n                raise ZeRORuntimeException(\"The checkpoint being loaded used a DP \" \\\n                    f\"world size of {self.loaded_checkpoint_dp_world_size} but the \" \\\n                    f\"current world size is {self.seq_dp_world_size}. Automatic adjustment \" \\\n                    \"of ZeRO's optimizer state partitioning with a new world size is not \" \\\n                    \"currently supported.\")\n            checkpoint_folder = None\n            zero_sd_list = self._get_all_zero_checkpoints(load_dir, tag)\n            if zero_sd_list is None:\n                return False\n\n        self.optimizer.load_state_dict(state_dict_list=zero_sd_list,\n                                       load_optimizer_states=load_optimizer_states,\n                                       load_from_fp32_weights=self.zero_load_from_fp32_weights(),\n                                       checkpoint_folder=checkpoint_folder,\n                                       load_serial=load_serial)\n\n        if self.load_universal_checkpoint():\n            logger.info(f'loaded universal zero checkpoints from {checkpoint_folder} for rank {self.global_rank}')\n        else:\n            logger.info(f\"loading {len(zero_sd_list)} zero partition checkpoints for rank {self.global_rank}\")\n        return True\n\n    def _get_mp_rank_zero_checkpoint_names(self, load_dir, tag, mp_rank, dp_world_size, bf16_mode):\n        zero_ckpt_names = []\n        for dp_rank in range(dp_world_size):\n            ckpt_name = self._get_rank_zero_ckpt_name(checkpoints_path=load_dir,\n                                                      tag=tag,\n                                                      mp_rank=mp_rank,\n                                                      dp_rank=dp_rank,\n                                                      bf16_mode=bf16_mode)\n            zero_ckpt_names.append(ckpt_name)\n\n        return zero_ckpt_names\n\n    def _get_all_zero_checkpoint_names(self, load_dir, tag, bf16_mode):\n        mp_rank = 0 if self.mpu is None else self.mpu.get_model_parallel_rank()\n        zero_ckpt_names = self._get_mp_rank_zero_checkpoint_names(load_dir=load_dir,\n                                                                  tag=tag,\n                                                                  mp_rank=mp_rank,\n                                                                  dp_world_size=self.loaded_checkpoint_dp_world_size,\n                                                                  bf16_mode=bf16_mode)\n        for i, ckpt_name in enumerate(zero_ckpt_names):\n            if not os.path.exists(ckpt_name):\n                # transparently handle the old file pattern for optim_states\n                if \"optim_states.pt\" in ckpt_name:\n                    ckpt_name_try = ckpt_name.replace(\"_optim_states.pt\", \"optim_states.pt\")\n                    if os.path.exists(ckpt_name_try):\n                        zero_ckpt_names[i] = ckpt_name_try\n                        continue\n\n        return zero_ckpt_names\n\n    def _get_all_zero_checkpoint_state_dicts(self, zero_ckpt_names):\n        zero_sd_list = []\n        for i, ckpt_name in enumerate(zero_ckpt_names):\n            _state = None\n            if ckpt_name is None:\n                _state = {OPTIMIZER_STATE_DICT: None}\n            # Fully load state for current rank\n            elif self.zero_elastic_checkpoint() or dist.get_rank(group=self.optimizer.dp_process_group) == i:\n                _state = self.checkpoint_engine.load(\n                    ckpt_name,\n                    map_location='cpu',\n                )\n            else:\n                _state = {OPTIMIZER_STATE_DICT: None}\n            zero_sd_list.append(_state)\n\n        zero_optimizer_sd = [sd[OPTIMIZER_STATE_DICT] for sd in zero_sd_list]\n        logger.info(f\"successfully read {len(zero_optimizer_sd)} ZeRO state_dicts for rank {self.global_rank}\")\n        return zero_optimizer_sd\n\n    def _get_all_zero_checkpoints(self, load_dir, tag):\n        for bf16_mode in [self.bfloat16_enabled(), not self.bfloat16_enabled()]:\n            zero_ckpt_names = self._get_all_zero_checkpoint_names(load_dir, tag, bf16_mode)\n            if zero_ckpt_names is not None:\n                # Warn if loading checkpoint of different bit16 type\n                if bf16_mode is not self.bfloat16_enabled():\n                    checkpoint_bit16 = BFLOAT16 if bf16_mode else FP16\n                    engine_bit16 = BFLOAT16 if self.bfloat16_enabled() else FP16\n                    logger.warn(f'Loading {checkpoint_bit16} zero checkpoints into {engine_bit16} training engine')\n                return self._get_all_zero_checkpoint_state_dicts(zero_ckpt_names)\n\n        return None\n\n    def _checkpoint_tag_validation(self, tag):\n        if self.checkpoint_tag_validation_enabled():\n            s_hash = hashlib.sha1(tag.encode())\n            bhash = torch.ByteTensor([s_hash.digest()]).flatten().to(self.device)\n            max_bhash = bhash.clone()\n            min_bhash = bhash.clone()\n            dist.all_reduce(max_bhash, op=dist.ReduceOp.MAX)\n            dist.all_reduce(min_bhash, op=dist.ReduceOp.MIN)\n            valid = all(min_bhash == bhash) and all(max_bhash == bhash)\n            msg = (f\"[rank={dist.get_rank()}] The checkpoint tag name '{tag}' is not consistent across \"\n                   \"all ranks. Including rank unique information in checkpoint tag could cause issues when \"\n                   \"restoring with different world sizes.\")\n            if self.checkpoint_tag_validation_fail():\n                assert valid, msg\n            elif not valid:\n                logger.warning(msg)\n\n    def save_checkpoint(self, save_dir, tag=None, client_state={}, save_latest=True, exclude_frozen_parameters=False):\n        \"\"\"Save training checkpoint\n\n        Arguments:\n            save_dir: Required. Directory for saving the checkpoint\n            tag: Optional. Checkpoint tag used as a unique identifier for the checkpoint, global step is\n                used if not provided. Tag name must be the same across all ranks.\n            client_state: Optional. State dictionary used for saving required training states in the client code.\n            save_latest: Optional. Save a file 'latest' pointing to the latest saved checkpoint.\n            exclude_frozen_parameters: Optional. Exclude frozen parameters from checkpointed state.\n        Important: all processes must call this method and not just the process with rank 0. It is\n        because each process needs to save its master weights and scheduler+optimizer states. This\n        method will hang waiting to synchronize with other processes if it's called just for the\n        process with rank 0.\n\n        \"\"\"\n        if self._optimizer_has_ckpt_event_prologue():\n            # Custom preparation for checkpoint save, if applicable\n            self.optimizer.checkpoint_event_prologue()\n\n        rank = self.local_rank if self.use_node_local_storage() else self.global_rank\n\n        # This is to make sure the checkpoint names are created without collision\n        # There seems to be issue creating them in parallel\n\n        # Ensure save_dir directory exists\n        if rank == 0:\n            self.checkpoint_engine.makedirs(save_dir, exist_ok=True)\n        dist.barrier()\n\n        if tag is None:\n            tag = f\"global_step{self.global_steps}\"\n\n        # Ensure tag is a string\n        tag = str(tag)\n        self.checkpoint_engine.create(tag)\n\n        # Ensure checkpoint tag is consistent across ranks\n        self._checkpoint_tag_validation(tag)\n\n        if self.has_moe_layers:\n            self.save_non_zero_checkpoint = False\n            self._create_checkpoint_file(save_dir, tag, False)\n            self._save_moe_checkpoint(save_dir,\n                                      tag,\n                                      client_state=client_state,\n                                      exclude_frozen_parameters=exclude_frozen_parameters)\n\n        # We distribute the task of saving layer checkpoint files among\n        # data parallel instances, so all procs should call _save_checkpoint.\n        # All procs then call module_state_dict(), but only procs of data\n        # parallel rank 0 save the general model params.\n        if not self.has_moe_layers:\n            self._create_checkpoint_file(save_dir, tag, False)\n            self._save_checkpoint(save_dir,\n                                  tag,\n                                  client_state=client_state,\n                                  exclude_frozen_parameters=exclude_frozen_parameters)\n\n        if self.save_zero_checkpoint:\n            self._create_zero_checkpoint_files(save_dir, tag)\n            self._save_zero_checkpoint(save_dir, tag)\n\n        if self.zero_has_nvme_offload():\n            from shutil import copytree, disk_usage\n            offload_dir = self.optimizer.optimizer_swapper.swap_folder\n            offload_ckpt_dir = os.path.join(save_dir, tag, \"offloaded_tensors\")\n            _, _, free = disk_usage(save_dir)\n            logger.info(\n                f\"Copying NVMe offload files from {offload_dir} to {offload_ckpt_dir}, {free / 1e9:,.2f} GB free on target filesystem...\"\n            )\n            copytree(offload_dir,\n                     offload_ckpt_dir,\n                     ignore=lambda _, dir_list: list(filter(lambda x: 'gradient' in x, dir_list)),\n                     dirs_exist_ok=False)\n            _, _, free = disk_usage(save_dir)\n            logger.info(f\"Copying complete! {free / 1e9:,.2f} GB free on target filesystem\")\n\n        if self._optimizer_has_ckpt_event_epilogue():\n            self.optimizer.checkpoint_event_epilogue()\n\n        # Save latest checkpoint tag\n        self.checkpoint_engine.commit(tag)\n        if save_latest and rank == 0:\n            with open(os.path.join(save_dir, 'latest'), 'w') as fd:\n                fd.write(tag)\n\n        dist.barrier()\n\n        return True\n\n    def _get_non_moe_state_dict(self, full_state_dict):\n        \"\"\"\n            Get the state dict of the non-moe layers\n        \"\"\"\n        for key in list(full_state_dict.keys()):\n            if 'expert' in key and 'moe.gate.wg.weight' not in key:\n                full_state_dict.pop(key)\n\n        return full_state_dict\n\n    def _save_moe_checkpoint(self, save_dir, tag, client_state={}, exclude_frozen_parameters=False):\n        save_path = self._get_ckpt_name(save_dir, tag)\n        # A hack to save the checkpointing directory. Pipeline parallelism overrides\n        # module_state_dict() and uses this path to save the model. module_state_dict()\n        # then instead just returns None.\n\n        # Using layer_#_export_# to save the model's expert state_dict\n        moe_layer_id = 0\n        for n_module, module in self.module.named_modules():\n            if isinstance(module, MoE):  # and deepspeed.comm.get_rank() == 0:\n                group_name = module.expert_group_name\n                num_local_experts = module.num_local_experts\n                expp_rank = groups._get_expert_parallel_rank(group_name)\n                exp_dp_rank = groups._get_expert_data_parallel_rank(group_name)\n                # print(expp_rank, exp_dp_rank)\n                if exp_dp_rank != 0:\n                    moe_layer_id += 1\n                    continue\n\n                # get all moe parameters\n                moe_state_dict = {}\n                for n, p in module.state_dict().items():\n                    if 'expert' in n and 'moe.gate.wg.weight' not in n:\n                        moe_state_dict[n_module + '.' + n] = p\n                moe_str_prefix = '.deepspeed_moe.experts.deepspeed_experts.'\n                # print(moe_state_dict.keys()) # until now, everything is fine. So the bug happens at next few lines\n                # Reorder the moe name rank, so that each checkpoint only has one expert\n                experts_state_dict = defaultdict(dict)\n                for key in list(moe_state_dict.keys()):\n                    m = re.match(f\".*{moe_str_prefix}([0-9]+).*\", key)\n\n                    local_expert_id = None\n                    if not m:\n                        logger.warn(f'No expert found in key {key}.')\n                    else:\n                        local_expert_id = m.group(1)\n\n                    global_expert_id = expp_rank * \\\n                        num_local_experts + int(local_expert_id)\n                    expert_key = key.replace(f'{moe_str_prefix}{local_expert_id}',\n                                             f'{moe_str_prefix}{global_expert_id}')\n                    # truncating extra tensor (shared) storage\n                    truncated = moe_state_dict.pop(key).clone().detach()\n                    experts_state_dict[str(global_expert_id)][expert_key] = truncated\n\n                # let save the moe parameters\n                for global_expert_id, expert_state_dict in experts_state_dict.items():\n                    # save the moe parameters\n                    moe_save_path = self._get_expert_ckpt_name(save_dir, moe_layer_id, global_expert_id, tag, self.mpu)\n                    if self.random_ltd_enabled():\n                        expert_state_dict = remove_random_ltd_state_dict(expert_state_dict)\n                    self.checkpoint_engine.save(expert_state_dict, moe_save_path)\n                moe_layer_id += 1\n\n        self._curr_ckpt_path = os.path.join(save_dir, tag)\n\n        largest_group_name = groups._get_max_expert_size_name()\n        expp_rank = groups._get_expert_parallel_rank(largest_group_name)\n        exp_dp_rank = groups._get_expert_data_parallel_rank(largest_group_name)\n\n        # In the case of E + D parallelism, only the\n        # first expert parallel group should save the expert weights\n        # since each expert parallel group is a copy of the model's experts\n        if exp_dp_rank == 0:\n            # Save optimizer states. They are different across each exp parallel rank.\n            optimizer_state = {\n                'optimizer': self.optimizer.state_dict() if self.optimizer and not self.zero_optimization() else None\n            }\n            # TODO: why use BufferedWriter not the path\n            file_path = self._get_optimizer_ckpt_name(save_dir, tag, expp_rank)\n            self.checkpoint_engine.save(optimizer_state, file_path)\n\n        # Load flow uses below saved file for model parameters, RNG and more\n        if groups._get_data_parallel_rank() == 0:\n            # Get non-moe parameters\n            # Classes DeepSpeedEngine and PipelineEngine have different behavior for method module_state_dict.\n            # DeepSpeedEngine returns the state dict, where PipelineEngine saves the state dict and returns None.\n            # We need to get the state dict, therefore, call to DeepSpeedEngine (base class for PipelineEngine)\n            model_state_dict = self._get_non_moe_state_dict(\n                DeepSpeedEngine.module_state_dict(self, exclude_frozen_parameters=exclude_frozen_parameters))\n\n            # TODO: update num experts info,.. in checkpoint\n            state = {\n                'module':\n                model_state_dict,\n                'lr_scheduler':\n                self.lr_scheduler.state_dict() if self.lr_scheduler is not None else None,\n                'data_sampler':\n                self.training_dataloader.data_sampler.state_dict() if\n                (self.training_dataloader is not None and self.curriculum_learning_enabled()) else None,\n                'random_ltd':\n                self.random_ltd_scheduler.state_dict() if self.random_ltd_enabled() else None,\n                'sparse_tensor_module_names':\n                self.sparse_tensor_module_names,\n                'skipped_steps':\n                self.skipped_steps,\n                'global_steps':\n                self.global_steps,\n                'global_samples':\n                self.global_samples,\n                'dp_world_size':\n                self.dp_world_size,\n                'mp_world_size':\n                self.mp_world_size,\n                'num_experts':\n                self.num_experts\n            }\n            state.update(client_state)\n            logger.info(f'Saving model checkpoint: {save_path}')\n            self.checkpoint_engine.save(state, save_path)\n\n    def _create_checkpoint_file(self, save_dir, tag, zero_checkpoint):\n        name_function = (self._get_zero_ckpt_name if zero_checkpoint else self._get_ckpt_name)\n        try:\n            checkpoint_name = name_function(save_dir, tag)\n            path = os.path.dirname(checkpoint_name)\n            self.checkpoint_engine.makedirs(path, exist_ok=True)\n        except:\n            logger.error(f\"Failed saving model checkpoint to {save_dir} with tag {tag}\")\n            return False\n\n        return True\n\n    def _create_zero_checkpoint_files(self, save_dir, tag):\n        success = True\n        # zero checkpoint files are created sequentially\n        for rank in range(dist.get_world_size(self.optimizer.dp_process_group)):\n            if rank == self.global_rank:\n                success = self._create_checkpoint_file(save_dir, tag, True)\n\n            dist.barrier(group=self.optimizer.dp_process_group)\n\n        return success\n\n    def _save_checkpoint(self, save_dir, tag, client_state={}, exclude_frozen_parameters=False):\n\n        save_path = self._get_ckpt_name(save_dir, tag)\n\n        zero_optimizer_state = self.zero_optimization() or self.bfloat16_enabled()\n\n        save_frozen_param = self.zero_optimization_partition_gradients() and not exclude_frozen_parameters\n\n        # A hack to save the checkpointing directory. Pipeline parallelism overrides\n        # module_state_dict() and uses this path to save the model. module_state_dict()\n        # then instead just returns None.  The module_state_dict() implementation in\n        # PipelineEngine expects the save path to be set in self._curr_ckpt_path.\n        self._curr_ckpt_path = os.path.join(save_dir, tag)\n        module = self.module_state_dict(exclude_frozen_parameters=exclude_frozen_parameters)\n        self._curr_ckpt_path = None\n\n        state = dict(module=module,\n                     buffer_names=self._get_buffer_names(),\n                     optimizer=self.optimizer.state_dict() if self.optimizer and not zero_optimizer_state else None,\n                     param_shapes=self._get_zero_param_shapes() if self.optimizer and zero_optimizer_state else None,\n                     frozen_param_shapes=self._get_zero_frozen_param_attributes(self._get_param_shape_func)\n                     if save_frozen_param else None,\n                     shared_params=self._get_shared_params() if self.optimizer and zero_optimizer_state else None,\n                     frozen_param_fragments=self._get_zero_frozen_param_attributes(self._get_param_fragment_func)\n                     if save_frozen_param else None,\n                     lr_scheduler=self.lr_scheduler.state_dict() if self.lr_scheduler is not None else None,\n                     data_sampler=self.training_dataloader.data_sampler.state_dict() if\n                     (self.training_dataloader is not None and self.curriculum_learning_enabled()) else None,\n                     random_ltd=self.random_ltd_scheduler.state_dict() if self.random_ltd_enabled() else None,\n                     sparse_tensor_module_names=self.sparse_tensor_module_names,\n                     skipped_steps=self.skipped_steps,\n                     global_steps=self.global_steps,\n                     global_samples=self.global_samples,\n                     dp_world_size=self.seq_dp_world_size,\n                     mp_world_size=self.mp_world_size,\n                     ds_config=self.config,\n                     ds_version=version)\n        state.update(client_state)\n\n        if self.save_non_zero_checkpoint:\n            log_dist(message=f'Saving model checkpoint: {save_path}', ranks=[0, 1])\n            self.checkpoint_engine.save(state, save_path)\n\n    def _get_buffer_names(self):\n        buffer_names = []\n\n        # we save buffer names so that we could extract later the real buffers from the saved\n        # state_dict[\"module\"] in the non-zero checkpoint - the buffers are already there but they\n        # are intermixed with param placeholders\n\n        # have to traverse the tree to be able to skip non-persistent buffers\n        def get_layer_named_buffers(module, prefix=\"\"):\n            for name, buf in module.named_buffers(recurse=False):\n                if buf is not None and name not in module._non_persistent_buffers_set:\n                    buffer_names.append(prefix + name)\n\n            for name, child in module.named_children():\n                if child is not None:\n                    get_layer_named_buffers(child, prefix + name + \".\")\n\n        get_layer_named_buffers(self.module, prefix=\"\")\n\n        return buffer_names\n\n    def _get_param_shape_func(self, param):\n        return param.ds_shape if hasattr(param, 'ds_id') else param.shape\n\n    def _get_param_fragment_func(self, param):\n        return param.ds_tensor.detach().cpu() if hasattr(param, 'ds_id') else param.detach().cpu()\n\n    def _get_zero_frozen_param_attributes(self, attr_func):\n        frozen_param_fragments = OrderedDict()\n\n        for param in self.module.parameters():\n            if param.requires_grad:\n                continue\n            if param not in self.param_names:\n                raise ValueError(f\"failed to find frozen {param} in named params\")\n            name = self.param_names[param]\n            frozen_param_fragments[name] = attr_func(param)\n\n        return frozen_param_fragments\n\n    def _get_zero_param_shapes(self):\n        \"\"\"Returns a dict of name to shape mapping, only for the flattened fp32 weights saved by the\n        optimizer. the names are exactly as in state_dict. The order is absolutely important, since\n        the saved data is just flattened data with no identifiers and requires reconstruction in the\n        same order it was saved.\n        We can't rely on self.module.named_parameters() to get the saved tensors, as some params\n        will be missing and others unsaved and then it'd be impossible to reconstruct state_dict\n        from the flattened weights.\n        optimizer.bit16_groups seems to be the easiest to use as it's in all zeroX versions.\n        \"\"\"\n        param_group_shapes = []\n        cnt = 0\n        numel = 0\n\n        # zero2 started using a round_robin_bit16_groups which is a shuffled version of bit16_groups -\n        # if we don't use it, we get parameters ordered incorrectly\n        if hasattr(self.optimizer, \"round_robin_bit16_groups\"):\n            bit16_groups = self.optimizer.round_robin_bit16_groups\n        elif self.bfloat16_enabled() and hasattr(self.optimizer, \"bf16_groups\"):\n            bit16_groups = self.optimizer.bf16_groups\n        else:\n            bit16_groups = self.optimizer.bit16_groups if self.zero_optimization_stage(\n            ) == 2 else self.optimizer.fp16_groups\n\n        for bit16_group in bit16_groups:\n            param_shapes = OrderedDict()\n            for param in bit16_group:\n                cnt += 1\n                numel += param.ds_numel if hasattr(param, \"ds_numel\") else param.numel()\n                shape = param.ds_shape if hasattr(param, \"ds_shape\") else param.shape\n                if param not in self.param_names:\n                    raise ValueError(f\"failed to find optimizer param in named params\")\n                name = self.param_names[param]\n                param_shapes[name] = shape\n\n                # uncomment to debug zero_to_fp32.py problems\n                # if self.global_rank == 0: print(f\"saving param {name} {shape} (numel={shape.numel()})\")\n            param_group_shapes.append(param_shapes)\n        # if self.global_rank == 0: print(f\"Total saved {numel} numels in {cnt} params\")\n\n        return param_group_shapes\n\n    def _get_shared_params(self):\n        \"\"\"\n        Returns a dict of shared params, which can later be used to reconstruct the original state dict,\n        e.g. in `zero_to_fp32`. Each dict entry is a pair of param names, where the key is the name\n        of the variable that isn't stored and the value is the actual param holding data.\n        \"\"\"\n        shared_index = {}\n        shared_params_by_full_name = {}\n\n        is_zero3_model = (self.zero_optimization_partition_weights()\n                          and any(hasattr(param, \"ds_id\") for param in self.module.parameters()))\n\n        def get_layer_state_dict(module, prefix=\"\"):\n            # handle params\n            for name, param in module.named_parameters(recurse=False):\n                if param is None or (is_zero3_model and not hasattr(param, \"ds_id\")):\n                    continue\n                key = prefix + name\n\n                # When weights are manged by stage 3, we can't rely on param.data_ptr() as it will be reused\n                # as weights get gathered and reduced, but param.ds_id is unique across all zero weights\n                # (and shared params will have the same param.ds_id)\n                param_id = param.ds_id if is_zero3_model else param.data_ptr()\n\n                if param_id in shared_index:\n                    # shared weights\n                    #print(f\"`{key}` is shared with `{shared_index[param_id]}`\")\n                    shared_params_by_full_name[key] = shared_index[param_id]\n                else:\n                    shared_index[param_id] = key\n\n            for name, child in module.named_children():\n                if child is not None:\n                    get_layer_state_dict(child, prefix + name + \".\")\n\n        if dist.get_rank() == 0:\n            get_layer_state_dict(self.module, prefix=\"\")\n\n        return shared_params_by_full_name\n\n    def _copy_recovery_script(self, save_path):\n        base_dir = os.path.dirname(os.path.dirname(__file__))\n        script = \"zero_to_fp32.py\"\n        src = os.path.join(base_dir, \"utils\", script)\n        dst = os.path.join(save_path, script)\n        #logger.info(f\"creating recovery script {dst}\")\n        copyfile(src, dst)\n        self._change_recovery_script_permissions(dst)\n\n    def _change_recovery_script_permissions(self, dst):\n        # make executable (safeguard for file shares - Azure as example)\n        try:\n            os.chmod(dst, os.stat(dst).st_mode | stat.S_IEXEC)\n        except (FileNotFoundError, PermissionError) as e:\n            #this message is used in unit test TestZeRONonDistributed\n            logger.info(\n                f'Warning: Could not change permissions for {dst} due to error: {e}. Continuing without changing permissions.'\n            )\n\n    def _save_zero_checkpoint(self, save_path, tag):\n        zero_checkpoint_name = self._get_zero_ckpt_name(save_path, tag)\n        zero_sd = dict(optimizer_state_dict=self.optimizer.state_dict(), ds_config=self.config, ds_version=version)\n        self.checkpoint_engine.save(zero_sd, zero_checkpoint_name)\n\n        if self.global_rank == 0:\n            self._copy_recovery_script(save_path)\n        ckpt_type = 'zero' if self.zero_optimization() else 'bf16_zero'\n        logger.info(f'{ckpt_type} checkpoint saved {zero_checkpoint_name}')\n\n    def _zero3_consolidated_16bit_state_dict(self, exclude_frozen_parameters=False):\n        \"\"\"\n        Get a full non-partitioned state_dict with fp16 weights on cpu.\n        Important: this function must be called on all ranks and not just rank 0.\n        This is similar to nn.Module.state_dict (modelled after _save_to_state_dict), but:\n        1. consolidates the weights from different partitions on gpu0\n        2. works on one layer at a time to require as little gpu0 memory as possible, by\n        moving the already consolidated weights to cpu\n        3. takes care to keep the shared params shared when gradually copying the params to cpu\n        Returns:\n            a consolidated fp16 ``state_dict`` on cpu on rank 0, ``None`` on other ranks\n        \"\"\"\n        if not self.zero_optimization_partition_weights():\n            raise ValueError(\"this function requires ZeRO-3 mode\")\n\n        state_dict = OrderedDict() if dist.get_rank() == 0 else None\n        shared_params = {}\n\n        def get_layer_state_dict(module, prefix=\"\"):\n            # gather one layer at a time to be memory-efficient\n            # must use modifier_rank=0 to release GPU memory after each layer gathered\n            #see_memory_usage(\"before GatheredParameters\", force=True)\n            with deepspeed.zero.GatheredParameters(list(module.parameters(recurse=False)), modifier_rank=0):\n                if dist.get_rank() == 0:\n                    # handle params\n                    for name, param in module.named_parameters(recurse=False):\n                        if param is None or (exclude_frozen_parameters and not param.requires_grad):\n                            continue\n                        key = prefix + name\n                        # can't rely on param.data_ptr() as it will be reused as weights gets\n                        # gathered and reduced, but param.ds_id is unique across all zero weights\n                        # (and shared params will have the same param.ds_id)\n                        if param.ds_id in shared_params:\n                            # shared weights\n                            #print(f\"`{key}` is shared with `{shared_params[param.ds_id]}`\")\n                            state_dict[key] = state_dict[shared_params[param.ds_id]]\n                        else:\n                            state_dict[key] = param.detach().cpu()\n                            shared_params[param.ds_id] = key\n                        #print(f\"param {param.ds_id} {param.shape} {key} \")\n\n                    # now buffers - not sure if need to take care of potentially shared weights here\n                    for name, buf in module.named_buffers(recurse=False):\n                        if (buf is not None and name not in module._non_persistent_buffers_set):\n                            state_dict[prefix + name] = buf.detach().cpu()\n            #see_memory_usage(\"after GatheredParameters\", force=True)\n\n            for name, child in module.named_children():\n                if child is not None:\n                    get_layer_state_dict(child, prefix + name + \".\")\n\n        # Prepare for checkpoint save by ensuring all parameters are partitioned\n        if self._optimizer_has_ckpt_event_prologue():\n            self.optimizer.checkpoint_event_prologue()\n\n        see_memory_usage(\"before get_layer_state_dict\", force=False)\n        get_layer_state_dict(self.module, prefix=\"\")\n        see_memory_usage(\"after get_layer_state_dict\", force=False)\n\n        if self._optimizer_has_ckpt_event_epilogue():\n            self.optimizer.checkpoint_event_epilogue()\n\n        return state_dict\n\n    def save_fp16_model(self, save_dir, save_filename=\"pytorch_model.bin\"):\n        \"\"\"has been renamed to save_16bit_model, keeping this around for backwards\n        compatibility\"\"\"\n        return self.save_16bit_model(save_dir, save_filename)\n\n    def save_16bit_model(self, save_dir, save_filename=\"pytorch_model.bin\", exclude_frozen_parameters=False):\n        \"\"\"\n        Save 16bit model weights\n\n        This method saves the 16bit model weights at the desired destination.\n\n        Arguments:\n            save_dir: Required. Directory for saving the model\n            save_filename: Optional. Filename to save to. Defaults to ``pytorch_model.bin``\n            exclude_frozen_parameters: Optional. Exclude frozen parameters from checkpointed state.\n\n        Returns:\n            ``True`` when a model has been saved, ``False`` otherwise. It will not be saved if\n            stage3_gather_16bit_weights_on_model_save is ``False``.\n\n        Important: all processes must call this method and not just the process with rank 0. It is\n        because the processes need to work in sync to gather the weights. This method will hang\n        waiting to synchronize with other processes if it's called just for the process with rank 0.\n\n        \"\"\"\n\n        path = os.path.join(save_dir, save_filename)\n\n        if self.zero_optimization_partition_weights():\n            if self.zero_gather_16bit_weights_on_model_save():\n                # consolidation is expensive in time and memory and therefore isn't a default\n                state_dict = self._zero3_consolidated_16bit_state_dict(\n                    exclude_frozen_parameters=exclude_frozen_parameters)\n            else:\n                # the model will be bogus if not consolidated so don't confuse the user by saving it\n                logger.info(\n                    f\"Did not save the model {path} because `stage3_gather_16bit_weights_on_model_save` is False\")\n                return False\n        else:\n            state_dict = self.module_state_dict(exclude_frozen_parameters=exclude_frozen_parameters)\n\n        tag = f\"global_step{self.global_steps}\"\n        tag = str(tag)\n        self.checkpoint_engine.create(tag)\n\n        if dist.get_rank() == 0:\n            self.checkpoint_engine.makedirs(save_dir, exist_ok=True)\n            logger.info(f\"Saving model weights to {path}, tag: {tag}\")\n            self.checkpoint_engine.save(state_dict, path)\n\n        self.checkpoint_engine.commit(tag)\n\n        return True\n\n    def empty_partition_cache(self):\n        \"\"\"\n        Release GPU memory consumed by offloaded model parameters.\n        \"\"\"\n        if hasattr(self.optimizer, 'empty_partition_cache'):\n            self.optimizer.empty_partition_cache()\n            gc.collect()\n            get_accelerator().empty_cache()\n\n    def compile(self, backend=get_accelerator().get_compile_backend(), compile_kwargs={}) -> None:\n        \"\"\"Compile the module using the specified backend and kwargs.\n        If a compiler_fn is set, it will be used instead of torch.compile().\n        \"\"\"\n        if not is_compile_supported():\n            raise RuntimeError(\"compile is not supported in your version of PyTorch.\")\n\n        if self.is_compiled:\n            return\n\n        self.module.compile(backend=backend, **compile_kwargs)\n        self._is_compiled = True\n\n    @property\n    def is_compiled(self) -> bool:\n        return self._is_compiled\n", "deepspeed/runtime/compiler.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\n\n\ndef is_compile_supported():\n    return hasattr(torch, \"compiler\") and hasattr(torch.nn.Module, \"compile\")\n\n\ndef disable(func):\n    if is_compile_supported():\n        return torch.compiler.disable(func)\n    return func\n", "deepspeed/runtime/config.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport os\nfrom typing import Union\nfrom enum import Enum\n\nimport torch\nimport json\nimport hjson\nimport copy\nimport base64\n\nfrom .constants import *\nfrom .fp16.loss_scaler import (\n    INITIAL_LOSS_SCALE,\n    SCALE_WINDOW,\n    DELAYED_SHIFT,\n    CONSECUTIVE_HYSTERESIS,\n    MIN_LOSS_SCALE,\n)\nfrom .config_utils import (\n    get_scalar_param,\n    dict_raise_error_on_duplicate_keys,\n    ScientificNotationEncoder,\n)\nfrom .zero.config import get_zero_config, ZeroStageEnum\nfrom .activation_checkpointing.config import DeepSpeedActivationCheckpointingConfig\nfrom ..comm.config import DeepSpeedCommsConfig\nfrom ..monitor.config import get_monitor_config\nfrom ..inference.config import WeightQuantConfig\n\nfrom deepspeed import comm as dist\nfrom deepspeed.runtime.config_utils import DeepSpeedConfigModel\n\nfrom ..git_version_info import version as __version__\nfrom ..utils import logger\n\nfrom ..elasticity import (\n    elasticity_enabled,\n    compute_elastic_config,\n    ensure_immutable_elastic_config,\n)\nfrom ..elasticity.config import ElasticityConfigError\nfrom ..elasticity.constants import (\n    ELASTICITY,\n    IGNORE_NON_ELASTIC_BATCH_INFO,\n    IGNORE_NON_ELASTIC_BATCH_INFO_DEFAULT,\n    MODEL_PARALLEL_SIZE,\n    MODEL_PARALLEL_SIZE_DEFAULT,\n    NUM_GPUS_PER_NODE,\n    NUM_GPUS_PER_NODE_DEFAULT,\n)\n\nfrom ..profiling.config import DeepSpeedFlopsProfilerConfig\nfrom ..autotuning.config import DeepSpeedAutotuningConfig\nfrom ..nebula.config import DeepSpeedNebulaConfig\n\nfrom ..compression.config import get_compression_config, get_quantize_enabled\nfrom ..compression.constants import *\nfrom .swap_tensor.aio_config import get_aio_config\n\nfrom .data_pipeline.config import get_data_efficiency_enabled, get_data_efficiency_config, get_curriculum_enabled_legacy, get_curriculum_params_legacy\nfrom .data_pipeline.constants import *\n\nfrom ..utils.config import get_timers_config\n\nTENSOR_CORE_ALIGN_SIZE = 8\n\nADAGRAD_OPTIMIZER = 'adagrad'\nADAM_OPTIMIZER = 'adam'\nADAMW_OPTIMIZER = 'adamw'\nLAMB_OPTIMIZER = 'lamb'\nONEBIT_ADAM_OPTIMIZER = 'onebitadam'\nZERO_ONE_ADAM_OPTIMIZER = 'zerooneadam'\nONEBIT_LAMB_OPTIMIZER = 'onebitlamb'\nMUADAM_OPTIMIZER = 'muadam'\nMUADAMW_OPTIMIZER = 'muadamw'\nMUSGD_OPTIMIZER = 'musgd'\nLION_OPTIMIZER = 'lion'\nDEEPSPEED_OPTIMIZERS = [\n    ADAGRAD_OPTIMIZER, ADAM_OPTIMIZER, ADAMW_OPTIMIZER, LAMB_OPTIMIZER, ONEBIT_ADAM_OPTIMIZER, ONEBIT_LAMB_OPTIMIZER,\n    ZERO_ONE_ADAM_OPTIMIZER, MUADAM_OPTIMIZER, MUADAMW_OPTIMIZER, MUSGD_OPTIMIZER, LION_OPTIMIZER\n]\n\n# extra optimizer parameters for adam/adamw\nTORCH_ADAM_PARAM = \"torch_adam\"\n\n# default to adamw logic for adam/adamw optimizers unless user explicitly opts out\nADAM_W_MODE = \"adam_w_mode\"\nADAM_W_MODE_DEFAULT = True\n\n\nclass DeepSpeedConfigError(Exception):\n    pass\n\n\nclass DtypeEnum(Enum):\n    # The torch dtype must always be the first value (so we return torch.dtype)\n    fp16 = torch.float16, \"torch.float16\", \"fp16\", \"float16\", \"half\"\n    fp32 = torch.float32, \"torch.float32\", \"fp32\", \"float32\", \"float\"\n    int8 = torch.int8, \"torch.int8\", \"int8\"\n    bf16 = torch.bfloat16, \"torch.bfloat16\", \"bf16\", \"bfloat16\"\n\n    # Copied from https://stackoverflow.com/a/43210118\n    # Allows us to use multiple values for each Enum index and returns first\n    # listed value when Enum is called\n    def __new__(cls, *values):\n        obj = object.__new__(cls)\n        # first value is canonical value\n        obj._value_ = values[0]\n        for other_value in values[1:]:\n            cls._value2member_map_[other_value] = obj\n        obj._all_values = values\n        return obj\n\n    def __repr__(self):\n        return \"<%s.%s: %s>\" % (\n            self.__class__.__name__,\n            self._name_,\n            \", \".join([repr(v) for v in self._all_values]),\n        )\n\n\ndef get_pld_enabled(param_dict):\n    if PROGRESSIVE_LAYER_DROP in param_dict.keys():\n        return get_scalar_param(param_dict[PROGRESSIVE_LAYER_DROP], PLD_ENABLED, PLD_ENABLED_DEFAULT)\n    else:\n        return False\n\n\ndef get_pld_params(param_dict):\n    if PROGRESSIVE_LAYER_DROP in param_dict.keys():\n        pld_params = copy.copy(param_dict[PROGRESSIVE_LAYER_DROP])\n        pld_params.pop(PLD_ENABLED)\n        return pld_params\n    else:\n        return False\n\n\ndef get_amp_enabled(param_dict):\n    if AMP in param_dict.keys():\n        return get_scalar_param(param_dict[AMP], AMP_ENABLED, AMP_ENABLED_DEFAULT)\n    else:\n        return False\n\n\ndef get_amp_params(param_dict):\n    if AMP in param_dict.keys():\n        amp_params = copy.copy(param_dict[AMP])\n        amp_params.pop(AMP_ENABLED)\n        return amp_params\n    else:\n        return False\n\n\ndef get_fp16_enabled(param_dict):\n    if FP16 in param_dict.keys():\n        return get_scalar_param(param_dict[FP16], FP16_ENABLED, FP16_ENABLED_DEFAULT)\n    else:\n        return False\n\n\ndef get_bfloat16_enabled(param_dict):\n    for key in [BFLOAT16, BFLOAT16_OLD]:\n        if key in param_dict.keys():\n            return get_scalar_param(param_dict[key], BFLOAT16_ENABLED, BFLOAT16_ENABLED_DEFAULT)\n    return False\n\n\ndef get_bfloat16_immediate_grad_update(param_dict):\n    for key in [BFLOAT16, BFLOAT16_OLD]:\n        if key in param_dict.keys():\n            return get_scalar_param(param_dict[key], BFLOAT16_IMMEDIATE_GRAD_UPDATE,\n                                    BFLOAT16_IMMEDIATE_GRAD_UPDATE_DEFAULT)\n    return False\n\n\ndef get_fp16_master_weights_and_grads_enabled(param_dict):\n    if get_fp16_enabled(param_dict):\n        return get_scalar_param(param_dict[FP16], FP16_MASTER_WEIGHTS_AND_GRADS, FP16_MASTER_WEIGHTS_AND_GRADS_DEFAULT)\n    else:\n        return False\n\n\ndef get_fp16_auto_cast(param_dict):\n    if get_fp16_enabled(param_dict):\n        return get_scalar_param(param_dict[FP16], FP16_AUTO_CAST, FP16_AUTO_CAST_DEFAULT)\n\n\ndef get_loss_scale(param_dict):\n    if get_fp16_enabled(param_dict):\n        return get_scalar_param(param_dict[FP16], FP16_LOSS_SCALE, FP16_LOSS_SCALE_DEFAULT)\n    elif get_bfloat16_enabled(param_dict):\n        return 1.0\n    else:\n        return FP16_LOSS_SCALE_DEFAULT\n\n\ndef get_initial_dynamic_scale(param_dict):\n    if get_fp16_enabled(param_dict):\n        initial_scale_power = get_scalar_param(param_dict[FP16], FP16_INITIAL_SCALE_POWER,\n                                               FP16_INITIAL_SCALE_POWER_DEFAULT)\n    elif get_bfloat16_enabled(param_dict):\n        initial_scale_power = 0\n    else:\n        initial_scale_power = FP16_INITIAL_SCALE_POWER_DEFAULT\n\n    return 2**initial_scale_power\n\n\ndef get_dynamic_loss_scale_args(param_dict):\n    loss_scale_args = None\n    if get_fp16_enabled(param_dict):\n        fp16_dict = param_dict[FP16]\n        dynamic_loss_args = [\n            FP16_INITIAL_SCALE_POWER,\n            FP16_LOSS_SCALE_WINDOW,\n            FP16_MIN_LOSS_SCALE,\n            FP16_HYSTERESIS,\n            FP16_CONSECUTIVE_HYSTERESIS,\n        ]\n        if any(arg in list(fp16_dict.keys()) for arg in dynamic_loss_args):\n            init_scale = get_scalar_param(fp16_dict, FP16_INITIAL_SCALE_POWER, FP16_INITIAL_SCALE_POWER_DEFAULT)\n            scale_window = get_scalar_param(fp16_dict, FP16_LOSS_SCALE_WINDOW, FP16_LOSS_SCALE_WINDOW_DEFAULT)\n            delayed_shift = get_scalar_param(fp16_dict, FP16_HYSTERESIS, FP16_HYSTERESIS_DEFAULT)\n            consecutive_hysteresis = get_scalar_param(fp16_dict, FP16_CONSECUTIVE_HYSTERESIS,\n                                                      FP16_CONSECUTIVE_HYSTERESIS_DEFAULT)\n            min_loss_scale = get_scalar_param(fp16_dict, FP16_MIN_LOSS_SCALE, FP16_MIN_LOSS_SCALE_DEFAULT)\n            loss_scale_args = {\n                INITIAL_LOSS_SCALE: 2**init_scale,\n                SCALE_WINDOW: scale_window,\n                DELAYED_SHIFT: delayed_shift,\n                CONSECUTIVE_HYSTERESIS: consecutive_hysteresis,\n                MIN_LOSS_SCALE: min_loss_scale,\n            }\n\n    return loss_scale_args\n\n\ndef get_gradient_accumulation_steps(param_dict):\n    return get_scalar_param(param_dict, GRADIENT_ACCUMULATION_STEPS, GRADIENT_ACCUMULATION_STEPS_DEFAULT)\n\n\ndef get_sparse_gradients_enabled(param_dict):\n    return get_scalar_param(param_dict, SPARSE_GRADIENTS, SPARSE_GRADIENTS_DEFAULT)\n\n\ndef get_communication_data_type(param_dict,\n                                comm_type=COMMUNICATION_DATA_TYPE,\n                                comm_data_type_default=COMMUNICATION_DATA_TYPE_DEFAULT):\n    val = get_scalar_param(param_dict, comm_type, comm_data_type_default)\n    val = val.lower() if val is not None else val\n    if val is None:\n        return val  # we must determine it by other parameters\n    elif val == \"fp32\":\n        return torch.float32\n    elif val == \"fp16\":\n        return torch.float16\n    elif val == \"bf16\":\n        return torch.bfloat16\n\n    raise ValueError(f\"Invalid communication_data_type. Supported data types: ['fp16', 'bf16', 'fp32']. Got: {val}\")\n\n\ndef get_prescale_gradients(param_dict):\n    return get_scalar_param(param_dict, PRESCALE_GRADIENTS, PRESCALE_GRADIENTS_DEFAULT)\n\n\ndef get_gradient_predivide_factor(param_dict):\n    return get_scalar_param(param_dict, GRADIENT_PREDIVIDE_FACTOR, GRADIENT_PREDIVIDE_FACTOR_DEFAULT)\n\n\ndef get_steps_per_print(param_dict):\n    return get_scalar_param(param_dict, STEPS_PER_PRINT, STEPS_PER_PRINT_DEFAULT)\n\n\ndef get_disable_allgather(param_dict):\n    return get_scalar_param(param_dict, DISABLE_ALLGATHER, DISABLE_ALLGATHER_DEFAULT)\n\n\ndef get_dump_state(param_dict):\n    return get_scalar_param(param_dict, DUMP_STATE, DUMP_STATE_DEFAULT)\n\n\ndef get_gradient_clipping(param_dict):\n    return get_scalar_param(param_dict, GRADIENT_CLIPPING, GRADIENT_CLIPPING_DEFAULT)\n\n\ndef get_graph_harvesting(param_dict):\n    return get_scalar_param(param_dict, GRAPH_HARVESTING, GRAPH_HARVESTING_DEFAULT)\n\n\ndef get_sparse_attention(param_dict):\n    if SPARSE_ATTENTION in param_dict.keys():\n        sparsity = param_dict[SPARSE_ATTENTION]\n        mode = get_sparse_attention_mode(sparsity)\n\n        if mode == SPARSE_DENSE_MODE:\n            return get_sparse_dense_config(sparsity)\n        elif mode == SPARSE_FIXED_MODE:\n            return get_sparse_fixed_config(sparsity)\n        elif mode == SPARSE_VARIABLE_MODE:\n            return get_sparse_variable_config(sparsity)\n        elif mode == SPARSE_BIGBIRD_MODE:\n            return get_sparse_bigbird_config(sparsity)\n        elif mode == SPARSE_BSLONGFORMER_MODE:\n            return get_sparse_bslongformer_config(sparsity)\n        else:\n            raise NotImplementedError(f\"Given sparsity mode, {mode}, has not been implemented yet!\")\n\n    else:\n        return None\n\n\ndef get_sparse_dense_config(sparsity):\n    block = get_scalar_param(sparsity, SPARSE_BLOCK, SPARSE_BLOCK_DEFAULT)\n    return {SPARSE_MODE: SPARSE_DENSE_MODE, SPARSE_BLOCK: block}\n\n\ndef get_sparse_fixed_config(sparsity):\n    block = get_scalar_param(sparsity, SPARSE_BLOCK, SPARSE_BLOCK_DEFAULT)\n    different_layout_per_head = get_scalar_param(\n        sparsity,\n        SPARSE_DIFFERENT_LAYOUT_PER_HEAD,\n        SPARSE_DIFFERENT_LAYOUT_PER_HEAD_DEFAULT,\n    )\n    num_local_blocks = get_scalar_param(sparsity, SPARSE_NUM_LOCAL_BLOCKS, SPARSE_NUM_LOCAL_BLOCKS_DEFAULT)\n    num_global_blocks = get_scalar_param(sparsity, SPARSE_NUM_GLOBAL_BLOCKS, SPARSE_NUM_GLOBAL_BLOCKS_DEFAULT)\n    attention = get_scalar_param(sparsity, SPARSE_ATTENTION_TYPE, SPARSE_ATTENTION_TYPE_DEFAULT)\n    horizontal_global_attention = get_scalar_param(\n        sparsity,\n        SPARSE_HORIZONTAL_GLOBAL_ATTENTION,\n        SPARSE_HORIZONTAL_GLOBAL_ATTENTION_DEFAULT,\n    )\n    num_different_global_patterns = get_scalar_param(\n        sparsity,\n        SPARSE_NUM_DIFFERENT_GLOBAL_PATTERNS,\n        SPARSE_NUM_DIFFERENT_GLOBAL_PATTERNS_DEFAULT,\n    )\n\n    return {\n        SPARSE_MODE: SPARSE_FIXED_MODE,\n        SPARSE_BLOCK: block,\n        SPARSE_DIFFERENT_LAYOUT_PER_HEAD: different_layout_per_head,\n        SPARSE_NUM_LOCAL_BLOCKS: num_local_blocks,\n        SPARSE_NUM_GLOBAL_BLOCKS: num_global_blocks,\n        SPARSE_ATTENTION_TYPE: attention,\n        SPARSE_HORIZONTAL_GLOBAL_ATTENTION: horizontal_global_attention,\n        SPARSE_NUM_DIFFERENT_GLOBAL_PATTERNS: num_different_global_patterns,\n    }\n\n\ndef get_sparse_variable_config(sparsity):\n    block = get_scalar_param(sparsity, SPARSE_BLOCK, SPARSE_BLOCK_DEFAULT)\n    different_layout_per_head = get_scalar_param(\n        sparsity,\n        SPARSE_DIFFERENT_LAYOUT_PER_HEAD,\n        SPARSE_DIFFERENT_LAYOUT_PER_HEAD_DEFAULT,\n    )\n    num_random_blocks = get_scalar_param(sparsity, SPARSE_NUM_RANDOM_BLOCKS, SPARSE_NUM_RANDOM_BLOCKS_DEFAULT)\n    local_window_blocks = get_scalar_param(sparsity, SPARSE_LOCAL_WINDOW_BLOCKS, SPARSE_LOCAL_WINDOW_BLOCKS_DEFAULT)\n    global_block_indices = get_scalar_param(sparsity, SPARSE_GLOBAL_BLOCK_INDICES, SPARSE_GLOBAL_BLOCK_INDICES_DEFAULT)\n    global_block_end_indices = get_scalar_param(\n        sparsity,\n        SPARSE_GLOBAL_BLOCK_END_INDICES,\n        SPARSE_GLOBAL_BLOCK_END_INDICES_DEFAULT,\n    )\n    attention = get_scalar_param(sparsity, SPARSE_ATTENTION_TYPE, SPARSE_ATTENTION_TYPE_DEFAULT)\n    horizontal_global_attention = get_scalar_param(\n        sparsity,\n        SPARSE_HORIZONTAL_GLOBAL_ATTENTION,\n        SPARSE_HORIZONTAL_GLOBAL_ATTENTION_DEFAULT,\n    )\n\n    return {\n        SPARSE_MODE: SPARSE_VARIABLE_MODE,\n        SPARSE_BLOCK: block,\n        SPARSE_DIFFERENT_LAYOUT_PER_HEAD: different_layout_per_head,\n        SPARSE_NUM_RANDOM_BLOCKS: num_random_blocks,\n        SPARSE_LOCAL_WINDOW_BLOCKS: local_window_blocks,\n        SPARSE_GLOBAL_BLOCK_INDICES: global_block_indices,\n        SPARSE_GLOBAL_BLOCK_END_INDICES: global_block_end_indices,\n        SPARSE_ATTENTION_TYPE: attention,\n        SPARSE_HORIZONTAL_GLOBAL_ATTENTION: horizontal_global_attention,\n    }\n\n\ndef get_sparse_bigbird_config(sparsity):\n    block = get_scalar_param(sparsity, SPARSE_BLOCK, SPARSE_BLOCK_DEFAULT)\n    different_layout_per_head = get_scalar_param(\n        sparsity,\n        SPARSE_DIFFERENT_LAYOUT_PER_HEAD,\n        SPARSE_DIFFERENT_LAYOUT_PER_HEAD_DEFAULT,\n    )\n    num_random_blocks = get_scalar_param(sparsity, SPARSE_NUM_RANDOM_BLOCKS, SPARSE_NUM_RANDOM_BLOCKS_DEFAULT)\n    num_sliding_window_blocks = get_scalar_param(\n        sparsity,\n        SPARSE_NUM_SLIDING_WINDOW_BLOCKS,\n        SPARSE_NUM_SLIDING_WINDOW_BLOCKS_DEFAULT,\n    )\n    num_global_blocks = get_scalar_param(sparsity, SPARSE_NUM_GLOBAL_BLOCKS, SPARSE_NUM_GLOBAL_BLOCKS_DEFAULT)\n\n    return {\n        SPARSE_MODE: SPARSE_BIGBIRD_MODE,\n        SPARSE_BLOCK: block,\n        SPARSE_DIFFERENT_LAYOUT_PER_HEAD: different_layout_per_head,\n        SPARSE_NUM_RANDOM_BLOCKS: num_random_blocks,\n        SPARSE_NUM_SLIDING_WINDOW_BLOCKS: num_sliding_window_blocks,\n        SPARSE_NUM_GLOBAL_BLOCKS: num_global_blocks,\n    }\n\n\ndef get_sparse_bslongformer_config(sparsity):\n    block = get_scalar_param(sparsity, SPARSE_BLOCK, SPARSE_BLOCK_DEFAULT)\n    different_layout_per_head = get_scalar_param(\n        sparsity,\n        SPARSE_DIFFERENT_LAYOUT_PER_HEAD,\n        SPARSE_DIFFERENT_LAYOUT_PER_HEAD_DEFAULT,\n    )\n    num_sliding_window_blocks = get_scalar_param(\n        sparsity,\n        SPARSE_NUM_SLIDING_WINDOW_BLOCKS,\n        SPARSE_NUM_SLIDING_WINDOW_BLOCKS_DEFAULT,\n    )\n    global_block_indices = get_scalar_param(sparsity, SPARSE_GLOBAL_BLOCK_INDICES, SPARSE_GLOBAL_BLOCK_INDICES_DEFAULT)\n    global_block_end_indices = get_scalar_param(\n        sparsity,\n        SPARSE_GLOBAL_BLOCK_END_INDICES,\n        SPARSE_GLOBAL_BLOCK_END_INDICES_DEFAULT,\n    )\n\n    return {\n        SPARSE_MODE: SPARSE_BSLONGFORMER_MODE,\n        SPARSE_BLOCK: block,\n        SPARSE_DIFFERENT_LAYOUT_PER_HEAD: different_layout_per_head,\n        SPARSE_NUM_SLIDING_WINDOW_BLOCKS: num_sliding_window_blocks,\n        SPARSE_GLOBAL_BLOCK_INDICES: global_block_indices,\n        SPARSE_GLOBAL_BLOCK_END_INDICES: global_block_end_indices,\n    }\n\n\ndef get_sparse_attention_mode(param_dict):\n    if SPARSE_MODE in param_dict.keys():\n        return param_dict[SPARSE_MODE]\n    else:\n        return SPARSE_MODE_DEFAULT\n\n\ndef get_sparse_attention_type(param_dict):\n    if SPARSE_ATTENTION_TYPE in param_dict.keys():\n        return param_dict[SPARSE_ATTENTION_TYPE]\n    else:\n        return SPARSE_ATTENTION_TYPE_DEFAULT\n\n\ndef get_pipeline_config(param_dict):\n    \"\"\"Parses pipeline engine configuration. \"\"\"\n    default_pipeline = {\n        \"stages\": \"auto\",\n        \"partition\": \"best\",\n        \"seed_layers\": False,\n        \"activation_checkpoint_interval\": 0,\n        \"pipe_partitioned\": True,\n        \"grad_partitioned\": True,\n    }\n    config = default_pipeline\n    for key, val in param_dict.get(\"pipeline\", {}).items():\n        config[key] = val\n    return config\n\n\ndef get_optimizer_name(param_dict):\n    if OPTIMIZER in param_dict.keys() and TYPE in param_dict[OPTIMIZER].keys():\n        return param_dict[OPTIMIZER][TYPE]\n    else:\n        return OPTIMIZER_TYPE_DEFAULT\n\n\ndef get_optimizer_params(param_dict):\n    if (get_optimizer_name(param_dict) is not None and OPTIMIZER_PARAMS in param_dict[OPTIMIZER].keys()):\n        return param_dict[OPTIMIZER][OPTIMIZER_PARAMS]\n    else:\n        return None\n\n\ndef get_optimizer_gradient_clipping(param_dict):\n    optimizer_params = get_optimizer_params(param_dict)\n    if optimizer_params is not None and MAX_GRAD_NORM in optimizer_params.keys():\n        return optimizer_params[MAX_GRAD_NORM]\n    else:\n        return None\n\n\ndef get_optimizer_legacy_fusion(param_dict):\n    if OPTIMIZER in param_dict.keys() and LEGACY_FUSION in param_dict[OPTIMIZER].keys():\n        return param_dict[OPTIMIZER][LEGACY_FUSION]\n    else:\n        return LEGACY_FUSION_DEFAULT\n\n\ndef get_zero_allow_untested_optimizer(param_dict):\n    return get_scalar_param(param_dict, ZERO_ALLOW_UNTESTED_OPTIMIZER, ZERO_ALLOW_UNTESTED_OPTIMIZER_DEFAULT)\n\n\ndef get_zero_force_ds_cpu_optimizer(param_dict):\n    return get_scalar_param(param_dict, ZERO_FORCE_DS_CPU_OPTIMIZER, ZERO_FORCE_DS_CPU_OPTIMIZER_DEFAULT)\n\n\ndef get_scheduler_name(param_dict):\n    if SCHEDULER in param_dict.keys() and TYPE in param_dict[SCHEDULER].keys():\n        return param_dict[SCHEDULER][TYPE]\n    else:\n        return SCHEDULER_TYPE_DEFAULT\n\n\ndef get_scheduler_params(param_dict):\n    if (get_scheduler_name(param_dict) is not None and SCHEDULER_PARAMS in param_dict[SCHEDULER].keys()):\n        return param_dict[SCHEDULER][SCHEDULER_PARAMS]\n    else:\n        return None\n\n\ndef get_train_batch_size(param_dict):\n    return get_scalar_param(param_dict, TRAIN_BATCH_SIZE, TRAIN_BATCH_SIZE_DEFAULT)\n\n\ndef get_train_micro_batch_size_per_gpu(param_dict):\n    return get_scalar_param(\n        param_dict,\n        TRAIN_MICRO_BATCH_SIZE_PER_GPU,\n        TRAIN_MICRO_BATCH_SIZE_PER_GPU_DEFAULT,\n    )\n\n\ndef get_wall_clock_breakdown(param_dict):\n    return get_scalar_param(param_dict, WALL_CLOCK_BREAKDOWN, WALL_CLOCK_BREAKDOWN_DEFAULT)\n\n\ndef get_memory_breakdown(param_dict):\n    return get_scalar_param(param_dict, MEMORY_BREAKDOWN, MEMORY_BREAKDOWN_DEFAULT)\n\n\nclass HybridEngineConfig(DeepSpeedConfigModel):\n    enabled: bool = False\n    max_out_tokens: int = 512\n    inference_tp_size: int = 1\n    release_inference_cache: bool = False\n    pin_parameters: bool = True\n    tp_gather_partition_size: int = 8\n\n\ndef get_hybrid_engine_config(param_dict):\n    hybrid_engine_config_dict = param_dict.get(\"hybrid_engine\", {})\n    hybrid_engine_config = HybridEngineConfig(**hybrid_engine_config_dict)\n    return hybrid_engine_config\n\n\ndef get_expert_data_topo_config(param_dict):\n    return get_scalar_param(param_dict, USE_DATA_BEFORE_EXPERT_PARALLEL, USE_DATA_BEFORE_EXPERT_PARALLEL_DEFAULT)\n\n\ndef get_eigenvalue_config(param_dict):\n    if get_quantize_enabled(param_dict):\n        param_dict = param_dict[QUANTIZE_TRAINING]\n        assert not get_eigenvalue_enabled(param_dict), \"Eigenvalue based MoQ is temporarily disabled\"\n        return (\n            get_eigenvalue_enabled(param_dict),\n            get_eigenvalue_verbose(param_dict),\n            get_eigenvalue_max_iter(param_dict),\n            get_eigenvalue_tol(param_dict),\n            get_eigenvalue_stability(param_dict),\n            get_eigenvalue_gas_boundary_resolution(param_dict),\n            get_eigenvalue_layer_name(param_dict),\n            get_eigenvalue_layer_num(param_dict),\n        )\n    else:\n        return (\n            EIGENVALUE_ENABLED_DEFAULT,\n            EIGENVALUE_VERBOSE_DEFAULT,\n            EIGENVALUE_MAX_ITER_DEFAULT,\n            EIGENVALUE_TOL_DEFAULT,\n            EIGENVALUE_STABILITY_DEFAULT,\n            EIGENVALUE_GAS_BOUNDARY_RESOLUTION_DEFAULT,\n            EIGENVALUE_LAYER_NAME_DEFAULT,\n            EIGENVALUE_LAYER_NUM_DEFAULT,\n        )\n\n\ndef get_eigenvalue_enabled(param_dict):\n    if EIGENVALUE in param_dict.keys():\n        return get_scalar_param(param_dict[EIGENVALUE], EIGENVALUE_ENABLED, EIGENVALUE_ENABLED_DEFAULT)\n    else:\n        return EIGENVALUE_ENABLED_DEFAULT\n\n\ndef get_eigenvalue_verbose(param_dict):\n    if EIGENVALUE in param_dict.keys():\n        return get_scalar_param(param_dict[EIGENVALUE], EIGENVALUE_VERBOSE, EIGENVALUE_VERBOSE_DEFAULT)\n    else:\n        return EIGENVALUE_VERBOSE_DEFAULT\n\n\ndef get_eigenvalue_max_iter(param_dict):\n    if EIGENVALUE in param_dict.keys():\n        return get_scalar_param(param_dict[EIGENVALUE], EIGENVALUE_MAX_ITER, EIGENVALUE_MAX_ITER_DEFAULT)\n    else:\n        return EIGENVALUE_MAX_ITER_DEFAULT\n\n\ndef get_eigenvalue_tol(param_dict):\n    if EIGENVALUE in param_dict.keys():\n        return get_scalar_param(param_dict[EIGENVALUE], EIGENVALUE_TOL, EIGENVALUE_TOL_DEFAULT)\n    else:\n        return EIGENVALUE_TOL_DEFAULT\n\n\ndef get_eigenvalue_stability(param_dict):\n    if EIGENVALUE in param_dict.keys():\n        return get_scalar_param(param_dict[EIGENVALUE], EIGENVALUE_STABILITY, EIGENVALUE_STABILITY_DEFAULT)\n    else:\n        return EIGENVALUE_STABILITY_DEFAULT\n\n\ndef get_eigenvalue_gas_boundary_resolution(param_dict):\n    if EIGENVALUE in param_dict.keys():\n        return get_scalar_param(\n            param_dict[EIGENVALUE],\n            EIGENVALUE_GAS_BOUNDARY_RESOLUTION,\n            EIGENVALUE_GAS_BOUNDARY_RESOLUTION_DEFAULT,\n        )\n    else:\n        return EIGENVALUE_GAS_BOUNDARY_RESOLUTION_DEFAULT\n\n\ndef get_eigenvalue_layer_name(param_dict):\n    if EIGENVALUE in param_dict.keys():\n        return get_scalar_param(param_dict[EIGENVALUE], EIGENVALUE_LAYER_NAME, EIGENVALUE_LAYER_NAME_DEFAULT)\n    else:\n        return EIGENVALUE_LAYER_NAME_DEFAULT\n\n\ndef get_eigenvalue_layer_num(param_dict):\n    if EIGENVALUE in param_dict.keys():\n        return get_scalar_param(param_dict[EIGENVALUE], EIGENVALUE_LAYER_NUM, EIGENVALUE_LAYER_NUM_DEFAULT)\n    else:\n        return EIGENVALUE_LAYER_NUM_DEFAULT\n\n\ndef get_checkpoint_params(param_dict):\n    return param_dict.get(CHECKPOINT, {})\n\n\ndef get_data_types_params(param_dict):\n    return param_dict.get(DATA_TYPES, {})\n\n\ndef get_checkpoint_tag_validation_mode(checkpoint_params):\n    tag_validation_mode = checkpoint_params.get(CHECKPOINT_TAG_VALIDATION, CHECKPOINT_TAG_VALIDATION_DEFAULT)\n    tag_validation_mode = tag_validation_mode.upper()\n    if tag_validation_mode in CHECKPOINT_TAG_VALIDATION_MODES:\n        return tag_validation_mode\n    else:\n        raise DeepSpeedConfigError(\n            \"Checkpoint config contains invalid tag_validation \"\n            f\"value of {tag_validation_mode}, expecting one of {CHECKPOINT_TAG_VALIDATION_MODES}\")\n\n\ndef get_checkpoint_parallel_write_pipeline(checkpoint_params):\n    par_write_params = checkpoint_params.get(CHECKPOINT_PARALLEL_WRITE, {})\n    par_write_pipeline = par_write_params.get(CHECKPOINT_PARALLEL_WRITE_PIPELINE_STAGE,\n                                              CHECKPOINT_PARALLEL_WRITE_PIPELINE_STAGE_DEFAULT)\n    if par_write_pipeline in [True, False]:\n        return par_write_pipeline\n    else:\n        raise DeepSpeedConfigError(\"checkpoint::parallel_write::pipeline_stage \"\n                                   f\"value of '{par_write_pipeline}' is invalid, expecting: true or false\")\n\n\ndef get_dataloader_drop_last(param_dict):\n    return get_scalar_param(param_dict, DATALOADER_DROP_LAST, DATALOADER_DROP_LAST_DEFAULT)\n\n\n'''Write deepspeed config files by modifying basic templates.\nCan be used for quickly changing parameters via command line parameters.'''\n\n\nclass DeepSpeedConfigWriter:\n\n    def __init__(self, data=None):\n        self.data = data if data is not None else {}\n\n    def add_config(self, key, value):\n        self.data[key] = value\n\n    def load_config(self, filename):\n        self.data = json.load(open(filename, \"r\"), object_pairs_hook=dict_raise_error_on_duplicate_keys)\n\n    def write_config(self, filename):\n        with open(filename, \"w\") as outfile:\n            json.dump(self.data, outfile)\n\n\nclass DeepSpeedConfig(object):\n\n    def __init__(self, config: Union[str, dict], mpu=None):\n        super(DeepSpeedConfig, self).__init__()\n        if isinstance(config, dict):\n            self._param_dict = config\n        elif os.path.exists(config):\n            self._param_dict = hjson.load(open(config, \"r\"), object_pairs_hook=dict_raise_error_on_duplicate_keys)\n        else:\n            try:\n                config_decoded = base64.urlsafe_b64decode(config).decode('utf-8')\n                self._param_dict = hjson.loads(config_decoded)\n            except (UnicodeDecodeError, AttributeError):\n                raise ValueError(\n                    f\"Expected a string path to an existing deepspeed config, or a dictionary or a valid base64. Received: {config}\"\n                )\n        try:\n            self.global_rank = dist.get_rank()\n            if mpu is None:\n                self.world_size = dist.get_world_size()\n            else:\n                self.world_size = mpu.get_data_parallel_world_size()\n        except:\n            self.global_rank = 0\n            self.world_size = 1\n\n        # If elastic-mode enabled, update compute + update _param_dict\n        self.elasticity_enabled = elasticity_enabled(self._param_dict)\n        if self.elasticity_enabled:\n            logger.info(\"DeepSpeed elasticity support enabled\")\n            final_batch_size, valid_gpus, micro_batch_size = compute_elastic_config(\n                ds_config=self._param_dict,\n                target_deepspeed_version=__version__,\n                world_size=self.world_size,\n            )\n\n            elastic_dict = self._param_dict[ELASTICITY]\n\n            # Ensure the resource scheduler saw the same elastic config we are using at runtime\n            ensure_immutable_elastic_config(runtime_elastic_config_dict=elastic_dict)\n\n            self.elastic_model_parallel_size = elastic_dict.get(MODEL_PARALLEL_SIZE, MODEL_PARALLEL_SIZE_DEFAULT)\n            if self.elastic_model_parallel_size < 1:\n                raise ElasticityConfigError(\"Model-Parallel size cannot be less than 1, \"\n                                            f\"given model-parallel size: {self.elastic_model_parallel_size}\")\n\n            self.num_gpus_per_node = elastic_dict.get(NUM_GPUS_PER_NODE, NUM_GPUS_PER_NODE_DEFAULT)\n            if self.num_gpus_per_node < 1:\n                raise ElasticityConfigError(\"NUmber of GPUs per node cannot be less than 1, \"\n                                            f\"given number of GPUs per node: {self.num_gpus_per_node}\")\n\n            ignore_non_elastic_batch_info = elastic_dict.get(IGNORE_NON_ELASTIC_BATCH_INFO,\n                                                             IGNORE_NON_ELASTIC_BATCH_INFO_DEFAULT)\n\n            if not ignore_non_elastic_batch_info:\n                batch_params = [\n                    TRAIN_BATCH_SIZE,\n                    TRAIN_MICRO_BATCH_SIZE_PER_GPU,\n                    GRADIENT_ACCUMULATION_STEPS,\n                ]\n                if any(map(lambda t: t in self._param_dict, batch_params)):\n                    raise ElasticityConfigError(\"One or more batch related parameters were found in your \" \\\n                        f\"ds_config ({TRAIN_BATCH_SIZE}, {TRAIN_MICRO_BATCH_SIZE_PER_GPU}, and/or \" \\\n                        f\"{GRADIENT_ACCUMULATION_STEPS}). These parameters *will not be used* since \" \\\n                        \"elastic training is enabled, which takes control of these parameters. \" \\\n                        \"If you want to suppress this error (the parameters will be silently ignored) \" \\\n                        f\"please set {IGNORE_NON_ELASTIC_BATCH_INFO}':true in your elasticity config.\")\n\n            # micro_bsz * world_size * gas = total_batch_size\n            # gas = total_batch_size // (micro_bsz * world_size)\n            gradient_accu_steps = final_batch_size // (micro_batch_size * self.world_size)\n\n            if TRAIN_BATCH_SIZE in self._param_dict:\n                logger.warning(\"[Elasticity] overriding training_batch_size: \"\n                               f\"{self._param_dict[TRAIN_BATCH_SIZE]} -> {final_batch_size}\")\n            if TRAIN_MICRO_BATCH_SIZE_PER_GPU in self._param_dict:\n                logger.warning(\"[Elasticity] overriding train_micro_batch_size_per_gpu: \"\n                               f\"{self._param_dict[TRAIN_MICRO_BATCH_SIZE_PER_GPU]} -> {micro_batch_size}\")\n            if GRADIENT_ACCUMULATION_STEPS in self._param_dict:\n                logger.warning(\"[Elasticity] overriding gradient_accumulation_steps: \"\n                               f\"{self._param_dict[GRADIENT_ACCUMULATION_STEPS]} -> {gradient_accu_steps}\")\n\n            logger.info(f\"[Elasticity] valid GPU counts: {valid_gpus}\")\n\n            self._param_dict[TRAIN_BATCH_SIZE] = final_batch_size\n            self._param_dict[TRAIN_MICRO_BATCH_SIZE_PER_GPU] = micro_batch_size\n            self._param_dict[GRADIENT_ACCUMULATION_STEPS] = gradient_accu_steps\n\n        # Pass a copy so that user json is unmodified, e.g. for logging\n        self._initialize_params(copy.copy(self._param_dict))\n        self._configure_train_batch_size()\n        self._do_sanity_check()\n\n    def _initialize_params(self, param_dict):\n        self.train_batch_size = get_train_batch_size(param_dict)\n        #print(f\"beginning get_train_batch_size = {get_train_batch_size}\")\n        self.train_micro_batch_size_per_gpu = get_train_micro_batch_size_per_gpu(param_dict)\n        self.gradient_accumulation_steps = get_gradient_accumulation_steps(param_dict)\n        self.steps_per_print = get_steps_per_print(param_dict)\n        self.dump_state = get_dump_state(param_dict)\n\n        self.disable_allgather = get_disable_allgather(param_dict)\n        self.communication_data_type = get_communication_data_type(param_dict)\n        self.seq_parallel_communication_data_type = get_communication_data_type(\n            param_dict, SEQ_PARALLEL_COMMUNICATION_DATA_TYPE, SEQ_PARALLEL_COMMUNICATION_DATA_TYPE_DEFAULT)\n        self.prescale_gradients = get_prescale_gradients(param_dict)\n        self.gradient_predivide_factor = get_gradient_predivide_factor(param_dict)\n        self.sparse_gradients_enabled = get_sparse_gradients_enabled(param_dict)\n\n        self.zero_config = get_zero_config(param_dict)\n        self.mics_shard_size = self.zero_config.mics_shard_size\n        self.mics_hierarchial_params_gather = self.zero_config.mics_hierarchical_params_gather\n        self.zero_optimization_stage = self.zero_config.stage\n        self.zero_enabled = self.zero_optimization_stage > 0\n\n        self.activation_checkpointing_config = DeepSpeedActivationCheckpointingConfig(param_dict)\n\n        self.comms_config = DeepSpeedCommsConfig(param_dict)\n        self.monitor_config = get_monitor_config(param_dict)\n\n        self.gradient_clipping = get_gradient_clipping(param_dict)\n        self.fp16_enabled = get_fp16_enabled(param_dict)\n        self.fp16_auto_cast = get_fp16_auto_cast(param_dict)\n        self.bfloat16_enabled = get_bfloat16_enabled(param_dict)\n        self.bfloat16_immediate_grad_update = get_bfloat16_immediate_grad_update(param_dict)\n        assert not (self.fp16_enabled\n                    and self.bfloat16_enabled), 'bfloat16 and fp16 modes cannot be simultaneously enabled'\n        self.fp16_master_weights_and_gradients = get_fp16_master_weights_and_grads_enabled(param_dict)\n        self.amp_enabled = get_amp_enabled(param_dict)\n        self.amp_params = get_amp_params(param_dict)\n        self.loss_scale = get_loss_scale(param_dict)\n        self.initial_dynamic_scale = get_initial_dynamic_scale(param_dict)\n        self.dynamic_loss_scale_args = get_dynamic_loss_scale_args(param_dict)\n\n        self.compression_config = get_compression_config(param_dict)\n        self.graph_harvesting = get_graph_harvesting(param_dict)\n\n        self.optimizer_name = get_optimizer_name(param_dict)\n        if (self.optimizer_name is not None and self.optimizer_name.lower() in DEEPSPEED_OPTIMIZERS):\n            self.optimizer_name = self.optimizer_name.lower()\n\n        self.optimizer_params = get_optimizer_params(param_dict)\n        self.optimizer_legacy_fusion = get_optimizer_legacy_fusion(param_dict)\n\n        self.zero_allow_untested_optimizer = get_zero_allow_untested_optimizer(param_dict)\n\n        self.zero_force_ds_cpu_optimizer = get_zero_force_ds_cpu_optimizer(param_dict)\n\n        self.scheduler_name = get_scheduler_name(param_dict)\n        self.scheduler_params = get_scheduler_params(param_dict)\n\n        self.flops_profiler_config = DeepSpeedFlopsProfilerConfig(param_dict)\n        self.wall_clock_breakdown = (get_wall_clock_breakdown(param_dict) | self.flops_profiler_config.enabled)\n        self.memory_breakdown = get_memory_breakdown(param_dict)\n        self.autotuning_config = DeepSpeedAutotuningConfig(param_dict)\n\n        (\n            self.eigenvalue_enabled,\n            self.eigenvalue_verbose,\n            self.eigenvalue_max_iter,\n            self.eigenvalue_tol,\n            self.eigenvalue_stability,\n            self.eigenvalue_gas_boundary_resolution,\n            self.eigenvalue_layer_name,\n            self.eigenvalue_layer_num,\n        ) = get_eigenvalue_config(param_dict)\n\n        self.use_data_before_expert_parallel_ = get_expert_data_topo_config(param_dict)\n        self.hybrid_engine = get_hybrid_engine_config(param_dict)\n\n        self.sparse_attention = get_sparse_attention(param_dict)\n        self.pipeline = get_pipeline_config(param_dict)\n\n        self.pld_enabled = get_pld_enabled(param_dict)\n        self.pld_params = get_pld_params(param_dict)\n\n        self.curriculum_enabled_legacy = get_curriculum_enabled_legacy(param_dict)\n        self.curriculum_params_legacy = get_curriculum_params_legacy(param_dict)\n\n        self.data_efficiency_enabled = get_data_efficiency_enabled(param_dict)\n        self.data_efficiency_config = get_data_efficiency_config(param_dict)\n\n        checkpoint_params = get_checkpoint_params(param_dict)\n        validation_mode = get_checkpoint_tag_validation_mode(checkpoint_params)\n        self.checkpoint_tag_validation_enabled = (validation_mode != ValidationMode.IGNORE)\n        self.checkpoint_tag_validation_fail = validation_mode == ValidationMode.FAIL\n        self.load_universal_checkpoint = checkpoint_params.get(LOAD_UNIVERSAL_CHECKPOINT,\n                                                               LOAD_UNIVERSAL_CHECKPOINT_DEFAULT)\n\n        self.use_node_local_storage = checkpoint_params.get(USE_NODE_LOCAL_STORAGE_CHECKPOINT,\n                                                            USE_NODE_LOCAL_STORAGE_CHECKPOINT_DEFAULT)\n\n        data_types_params = get_data_types_params(param_dict)\n        self.grad_accum_dtype = data_types_params.get(GRAD_ACCUM_DTYPE, GRAD_ACCUM_DTYPE_DEFAULT)\n\n        par_write_pipe = get_checkpoint_parallel_write_pipeline(checkpoint_params)\n        self.checkpoint_parallel_write_pipeline = par_write_pipe\n\n        self.aio_config = get_aio_config(param_dict)\n\n        self.dataloader_drop_last = get_dataloader_drop_last(param_dict)\n\n        self.nebula_config = DeepSpeedNebulaConfig(param_dict)\n\n        self.weight_quantization_config = WeightQuantConfig(\n            **param_dict['weight_quantization']) if 'weight_quantization' in param_dict else None\n\n        self.timers_config = get_timers_config(param_dict)\n\n    def _batch_assertion(self):\n\n        train_batch = self.train_batch_size\n        micro_batch = self.train_micro_batch_size_per_gpu\n        grad_acc = self.gradient_accumulation_steps\n\n        assert (train_batch > 0), f\"Train batch size: {train_batch} has to be greater than 0\"\n\n        assert (micro_batch > 0), f\"Micro batch size per gpu: {micro_batch} has to be greater than 0\"\n\n        assert (grad_acc > 0), f\"Gradient accumulation steps: {grad_acc} has to be greater than 0\"\n\n        assert train_batch == micro_batch * grad_acc * self.world_size, (\n            f\"Check batch related parameters. train_batch_size is not equal \"\n            \"to micro_batch_per_gpu * gradient_acc_step * world_size \"\n            f\"{train_batch} != {micro_batch} * {grad_acc} * {self.world_size}\")\n\n    def _set_batch_related_parameters(self):\n\n        train_batch = self.train_batch_size\n        micro_batch = self.train_micro_batch_size_per_gpu\n        grad_acc = self.gradient_accumulation_steps\n\n        #print(f\"train_batch = {train_batch}, micro_batch={micro_batch}\")\n\n        # all values are provided nothing needs to be set\n        if train_batch is not None and micro_batch is not None and grad_acc is not None:\n            return\n\n        # global_accumulation_steps needs to be set\n        elif train_batch is not None and micro_batch is not None:\n            grad_acc = train_batch // micro_batch\n            grad_acc //= self.world_size\n            self.gradient_accumulation_steps = grad_acc\n\n        # micro_batch_per_gpu needs to be set\n        elif train_batch is not None and grad_acc is not None:\n            micro_batch = train_batch // self.world_size\n            micro_batch //= grad_acc\n            self.train_micro_batch_size_per_gpu = micro_batch\n\n        # train_batch_size needs to be set\n        elif micro_batch is not None and grad_acc is not None:\n            train_batch_size = micro_batch * grad_acc\n            train_batch_size *= self.world_size\n            self.train_batch_size = train_batch_size\n\n        # gradient_accumulation_steps and micro_batch_per_gpus is set\n        elif train_batch is not None:\n            self.gradient_accumulation_steps = 1\n            self.train_micro_batch_size_per_gpu = train_batch // self.world_size\n\n        # train_batch_size and gradient_accumulation_step is set\n        elif micro_batch is not None:\n            self.train_batch_size = micro_batch * self.world_size\n            self.gradient_accumulation_steps = 1\n\n        # either none of the three parameters are provided or just gradient_accumulation_step is provided\n        else:\n            assert False, \\\n                'Either train_batch_size or train_micro_batch_size_per_gpu needs to be provided'\n\n    def _configure_train_batch_size(self):\n        self._set_batch_related_parameters()\n        self._batch_assertion()\n\n    def _do_sanity_check(self):\n        self._do_error_check()\n\n        self._do_warning_check()\n\n    def print_user_config(self):\n        logger.info(\"  json = {}\".format(\n            json.dumps(\n                self._param_dict,\n                sort_keys=True,\n                indent=4,\n                cls=ScientificNotationEncoder,\n                separators=(\",\", \":\"),\n            )))\n\n    def print(self, name):\n        logger.info(\"{}:\".format(name))\n        for arg in sorted(vars(self)):\n            if arg != \"_param_dict\":\n                dots = \".\" * (29 - len(arg))\n                logger.info(\"  {} {} {}\".format(arg, dots, getattr(self, arg)))\n\n        self.print_user_config()\n\n    def _do_error_check(self):\n        assert (self.train_micro_batch_size_per_gpu\n                ), \"DeepSpeedConfig: {} is not defined\".format(TRAIN_MICRO_BATCH_SIZE_PER_GPU)\n\n        assert (\n            self.gradient_accumulation_steps), \"DeepSpeedConfig: {} is not defined\".format(GRADIENT_ACCUMULATION_STEPS)\n\n        if self.zero_enabled:\n            assert (self.zero_optimization_stage <=\n                    ZeroStageEnum.max_stage), \"DeepSpeedConfig: Maximum supported ZeRO stage is {}\".format(\n                        ZeroStageEnum.max_stage)\n\n        if self.fp16_master_weights_and_gradients:\n            assert self.zero_enabled and self.zero_optimization_stage == ZeroStageEnum.gradients, \"Fp16_master_weights_and_grads is only supported with ZeRO Stage 2 for now.\"\n\n    def _do_warning_check(self):\n        fp16_enabled = self.fp16_enabled\n\n        vocabulary_size = self._param_dict.get(VOCABULARY_SIZE, VOCABULARY_SIZE_DEFAULT)\n        if vocabulary_size and vocabulary_size % TENSOR_CORE_ALIGN_SIZE != 0:\n            logger.warning(\n                \"DeepSpeedConfig: vocabulary size {} is not aligned to {}, may import tensor core utilization.\".format(\n                    vocabulary_size, TENSOR_CORE_ALIGN_SIZE))\n\n        if (self.optimizer_params is not None and MAX_GRAD_NORM in self.optimizer_params.keys()\n                and self.optimizer_params[MAX_GRAD_NORM] > 0):\n            if fp16_enabled:\n                if self.global_rank == 0:\n                    logger.warning(\"DeepSpeedConfig: In FP16 mode, DeepSpeed will pass {}:{} to FP16 wrapper\".format(\n                        MAX_GRAD_NORM, self.optimizer_params[MAX_GRAD_NORM]))\n            else:\n                if self.global_rank == 0:\n                    logger.warning(\n                        \"DeepSpeedConfig: In FP32 mode, DeepSpeed does not permit MAX_GRAD_NORM ({}) > 0, setting to zero\"\n                        .format(self.optimizer_params[MAX_GRAD_NORM]))\n                self.optimizer_params[MAX_GRAD_NORM] = 0.0\n", "deepspeed/runtime/utils.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\"\"\"\nCopyright NVIDIA/Megatron\n\nHelper functions and classes from multiple sources.\n\"\"\"\n\nfrom collections.abc import Iterable\nfrom deepspeed.moe.utils import is_moe_param\nimport os\nimport psutil\nimport gc\nfrom math import sqrt\n\nimport torch\nfrom deepspeed import comm as dist\ntry:\n    from torch._six import inf\nexcept ModuleNotFoundError:\n    from torch import inf\n\nfrom deepspeed.utils import groups, logger\nfrom deepspeed.utils.bwc import (bwc_tensor_model_parallel_rank, bwc_pipeline_parallel_world_size,\n                                 bwc_pipeline_parallel_group)\nfrom deepspeed.runtime.constants import PIPE_REPLICATED\nfrom numpy import prod\nfrom deepspeed.accelerator import get_accelerator\n\nfrom deepspeed.module_inject.policy import transpose\nfrom torch.nn import functional as F\n\ntorch_memory_reserved = get_accelerator().memory_reserved\ntorch_max_memory_reserved = get_accelerator().max_memory_reserved\n\n\nclass DummyOptim():\n    \"\"\"\n    Dummy optimizer presents model parameters as a param group, this is\n    primarily used to allow ZeRO-3 without an optimizer\n    \"\"\"\n\n    def __init__(self, params):\n        self.param_groups = []\n        self.param_groups.append({'params': params})\n\n\ngraph_cache = {}\n\n\ndef graph_process(replay_first_step, func, *args, **kwargs):\n    # `func` should only contain operations on the GPU\n    # Please ensure that the memory address of the data required by 'func' remains constant\n    if func.__name__ not in graph_cache:\n        cuda_stream = get_accelerator().Stream()\n        cuda_stream.wait_stream(get_accelerator().current_stream())\n        with get_accelerator().stream(cuda_stream):\n            func(*args, **kwargs)\n        get_accelerator().current_stream().wait_stream(cuda_stream)\n        graph_cache[func.__name__] = get_accelerator().create_graph()\n        with get_accelerator().capture_to_graph(graph_cache[func.__name__]):\n            func(*args, **kwargs)\n        if replay_first_step:\n            get_accelerator().replay_graph(graph_cache[func.__name__])\n    else:\n        get_accelerator().replay_graph(graph_cache[func.__name__])\n\n\ndef noop_decorator(func):\n    return func\n\n\nclass noop_context(object):\n\n    def __init__(self):\n        pass\n\n    def __enter__(self):\n        pass\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        pass\n\n\ndef ensure_directory_exists(filename):\n    \"\"\"Create the directory path to ``filename`` if it does not already exist.\n\n    Args:\n        filename (str): A file path.\n    \"\"\"\n    dirname = os.path.dirname(filename)\n    os.makedirs(dirname, exist_ok=True)\n\n\ndef set_random_seed(seed):\n    \"\"\"Set the random seed for common PRNGs used during training: random, numpy, and torch.\n\n    Args:\n        seed (int): the seed to use\n    \"\"\"\n    import numpy\n    import random\n    random.seed(seed)\n    numpy.random.seed(seed)\n    torch.manual_seed(seed)\n\n\ndef is_model_parallel_parameter(p) -> bool:\n    if hasattr(p, 'model_parallel') and p.model_parallel:\n        return True\n\n    if hasattr(p, 'tensor_model_parallel') and p.tensor_model_parallel:\n        return True\n\n    return False\n\n\ndef copy_to_device(item, device, criterion_func):\n    \"\"\"\n    Return a copy of tensor on specified device.\n    Works on individual tensors, and tensors contained/nested in lists, tuples, and dicts.\n    Parameters:\n        item: tensor to copy or (possibly nested) container of tensors to copy.\n        device: target device\n        criterion_func: Function to restrict copy operation to items meet criterion\n\n    Returns:\n        None\n    \"\"\"\n    if criterion_func(item):\n        return item.to(device)\n    elif isinstance(item, list):\n        return [copy_to_device(v, device, criterion_func) for v in item]\n    elif isinstance(item, tuple):\n        return tuple([copy_to_device(v, device, criterion_func) for v in item])\n    elif isinstance(item, dict):\n        return {k: copy_to_device(v, device, criterion_func) for k, v in item.items()}\n    else:\n        return item\n\n\ndef move_to_device(item, device, criterion_func):\n    \"\"\"\n    Move tensor on to specified device by changing the storage.\n    Works on individual tensors, and tensors contained/nested in lists, tuples, and dicts.\n    Parameters:\n        item: tensor to move or (possibly nested) container of tensors to move.\n        device: target device\n        criterion_func: Function to restrict move operation to items meet criterion\n\n    Returns:\n        None\n    \"\"\"\n    if criterion_func(item):\n        device_copy = item.to(device)\n        item.data = device_copy.data\n        return item\n    elif isinstance(item, list):\n        return [move_to_device(v, device, criterion_func) for v in item]\n    elif isinstance(item, tuple):\n        return tuple([move_to_device(v, device, criterion_func) for v in item])\n    elif isinstance(item, dict):\n        return {k: move_to_device(v, device, criterion_func) for k, v in item.items()}\n    else:\n        return item\n\n\ndef get_norm_with_moe_layers_fast(all_groups_norm, group):\n    # This implementation standardizes the grad_norm across ranks. A more precise implementation can be found in 'get_norm_with_moe_layers'.\n    # Need to allreduce (avg) the norms across different ranks because moe params will not be synced during allreduce\n    scaled_norm = all_groups_norm * 1.0 / float(dist.get_world_size(group=group))\n    scaled_norm_tensor = torch.tensor(scaled_norm, device=get_accelerator().current_device_name(), dtype=torch.float)\n    dist.all_reduce(scaled_norm_tensor, group=group)\n    all_groups_norm = scaled_norm_tensor.item()\n    #print(f\"old = {all_groups_norm_old} and new = {all_groups_norm} at rank: {deepspeed.comm.get_rank()}\")\n    return all_groups_norm\n\n\nclass CheckOverflow(object):\n    '''Checks for overflow in gradient across parallel process'''\n\n    def __init__(self, param_groups=None, mpu=None, zero_reduce_scatter=False, deepspeed=None):\n        self.mpu = mpu\n        self.params = [] if param_groups else None\n        self.zero_reduce_scatter = zero_reduce_scatter\n        self.deepspeed = deepspeed\n        self.has_moe_params = False\n        if param_groups:\n            for group in param_groups:\n                for param in group:\n                    self.params.append(param)\n                    if is_moe_param(param):\n                        self.has_moe_params = True\n\n    def check_using_norm(self, norm_group, reduce_overflow=True):\n        # TODO: I don't think reduce_overflow is needed if mpu is None\n        overflow = -1 in norm_group\n        overflow_gpu = get_accelerator().FloatTensor([overflow])\n        if self.has_moe_params:\n            # In this case, we need to do an all_reduce across\n            # the expert_parallel_group, so that if there was\n            # an overflow due to expert weights, we detect it\n\n            # Only need to check groups.get_largest_expert_parallel_group()\n            dist.all_reduce(overflow_gpu, op=dist.ReduceOp.MAX, group=groups._get_max_expert_parallel_group())\n        if self.mpu is not None:\n            dist.all_reduce(overflow_gpu, op=dist.ReduceOp.MAX, group=self.mpu.get_model_parallel_group())\n        elif reduce_overflow:\n            dist.all_reduce(overflow_gpu, op=dist.ReduceOp.MAX)\n            dist.barrier()\n        overflow = overflow_gpu[0].item()\n        return bool(overflow)\n\n    def check(self, param_groups=None):\n        params = []\n        has_moe_params = False\n        if param_groups is None:\n            params = self.params\n            has_moe_params = self.has_moe_params\n        else:\n            assert param_groups is not None, \\\n                \"self.params and param_groups both cannot be none\"\n\n            for group in param_groups:\n                for param in group:\n                    params.append(param)\n                    if is_moe_param(param):\n                        has_moe_params = True\n\n        return self.has_overflow(params, has_moe_params=has_moe_params)\n\n    # `params` is a list / generator of torch.Variable\n    def has_overflow_serial(self, params):\n        for i, p in enumerate(params):\n            if p.grad is not None and self._has_inf_or_nan(p.grad.data, i):\n                return True\n        return False\n\n    def has_overflow(self, params, has_moe_params=None):\n        if has_moe_params is None:\n            has_moe_params = self.has_moe_params\n        overflow = self.has_overflow_serial(params)\n        # Since each model parallel GPU carries only part of the model,\n        # make sure overflow flag is synced across all the model parallel GPUs\n        overflow_gpu = get_accelerator().ByteTensor([overflow])\n        # deepspeed.comm.all_reduce(overflow_gpu,\n        #                             op=deepspeed.comm.ReduceOp.MAX,\n        #                             group=mpu.get_model_parallel_group())\n        if has_moe_params:\n            # All reduce this across expert_parallel_group, so that if an expert\n            # overflows, we detect it here\n            dist.all_reduce(overflow_gpu, op=dist.ReduceOp.MAX, group=groups._get_max_expert_parallel_group())\n        if self.zero_reduce_scatter:\n            dist.all_reduce(overflow_gpu, op=dist.ReduceOp.MAX, group=dist.get_world_group())\n        elif self.mpu is not None:\n            if self.deepspeed is not None:\n                using_pipeline = hasattr(self.deepspeed, 'pipeline_enable_backward_allreduce')\n                if (using_pipeline and self.deepspeed.pipeline_enable_backward_allreduce is False) or (\n                        not using_pipeline and self.deepspeed.enable_backward_allreduce is False):\n                    dist.all_reduce(overflow_gpu, op=dist.ReduceOp.MAX, group=self.mpu.get_data_parallel_group())\n            dist.all_reduce(overflow_gpu, op=dist.ReduceOp.MAX, group=self.mpu.get_model_parallel_group())\n        elif self.deepspeed is not None and self.deepspeed.enable_backward_allreduce is False:\n            dist.all_reduce(overflow_gpu, op=dist.ReduceOp.MAX, group=dist.get_world_group())\n\n        overflow = overflow_gpu[0].item()\n        return bool(overflow)\n\n    # `x` is a torch.Tensor\n    @staticmethod\n    def _has_inf_or_nan(x, i):\n        try:\n            # if x is half, the .float() incurs an additional deep copy, but it's necessary if\n            # Pytorch's .sum() creates a one-element tensor of the same type as x\n            # (which is true for some recent version of pytorch).\n            cpu_sum = float(x.float().sum())\n            # More efficient version that can be used if .sum() returns a Python scalar\n            # cpu_sum = float(x.sum())\n        except RuntimeError as instance:\n            # We want to check if inst is actually an overflow exception.\n            # RuntimeError could come from a different error.\n            # If so, we still want the exception to propagate.\n            if \"value cannot be converted\" not in instance.args[0]:\n                raise\n            return True\n        else:\n            if cpu_sum == float('inf') or cpu_sum == -float('inf') or cpu_sum != cpu_sum:\n                return True\n            return False\n\n\ndef _handle_overflow(cpu_sum, x, i):\n    import math\n    rank = dist.get_rank()\n    if rank == 0:\n        t_i = -1\n        for v_i, v in enumerate(x.data.contiguous().view(-1)):\n            if not math.isfinite(float(v)):\n                t_i = v_i\n                break\n        logger.info(f\"rank {rank} detected overflow {cpu_sum} in tensor {i}:{t_i} shape {x.shape}\")\n\n\ndef get_global_norm(norm_list):\n    \"\"\" Compute total from a list of norms\n    \"\"\"\n    total_norm = 0.0\n    for norm in norm_list:\n        total_norm += norm**2.0\n    # logger.info(f'norm_list = {norm_list} global = {sqrt(total_norm)}')\n    return sqrt(total_norm)\n\n\ndef clip_grad_norm_(parameters, max_norm, norm_type=2, mpu=None):\n    \"\"\"Clips gradient norm of an iterable of parameters.\n\n    This has been adapted from Nvidia megatron. We add norm averaging\n    to consider MoE params when calculating norm as they will result\n    in different norms across different ranks.\n\n    This is adapted from torch.nn.utils.clip_grad.clip_grad_norm_ and\n    added functionality to handle model parallel parameters. Note that\n    the gradients are modified in place.\n\n    Arguments:\n        parameters (Iterable[Tensor] or Tensor): an iterable of Tensors or a\n            single Tensor that will have gradients normalized\n        max_norm (float or int): max norm of the gradients\n        norm_type (float or int): type of the used p-norm. Can be ``'inf'`` for\n            infinity norm.\n\n    Returns:\n        Total norm of the parameters (viewed as a single vector).\n    \"\"\"\n    if isinstance(parameters, torch.Tensor):\n        parameters = [parameters]\n    parameters = list(filter(lambda p: p.grad is not None, parameters))\n    norm_type = float(norm_type)\n    all_norms = []\n    if norm_type == inf:\n        for p in parameters:\n            all_norms.append(p.grad.data.abs().max().float())\n        total_norm = torch.stack(all_norms).max()\n        total_norm = total_norm.to(get_accelerator().current_device_name())\n        # Take max across all GPUs.\n        if mpu is not None:\n            dist.all_reduce(total_norm, op=dist.ReduceOp.MAX, group=mpu.get_model_parallel_group())\n    else:\n        total_norm = 0\n        for p in parameters:\n            if mpu is not None:\n                if (mpu.get_model_parallel_rank() == 0) or is_model_parallel_parameter(p):\n                    param_norm = p.grad.data.detach().float().norm(norm_type)\n                    all_norms.append(param_norm)\n            else:\n                param_norm = p.grad.data.detach().float().norm(norm_type)\n                all_norms.append(param_norm)\n        if len(all_norms) > 0:\n            total_norm = torch.stack(all_norms).square().sum().float()\n        else:\n            total_norm = get_accelerator().FloatTensor([0.0])\n        total_norm = total_norm.to(get_accelerator().current_device_name())\n        # Sum across all model parallel GPUs.\n        if mpu is not None:\n            dist.all_reduce(total_norm, op=dist.ReduceOp.SUM, group=mpu.get_model_parallel_group())\n        total_norm = total_norm.pow(1. / norm_type)\n\n    # Need to average total_norm across different GPUs due to the presence of moe params\n    pg = groups._get_data_parallel_group()\n    scaled_norm = total_norm * 1.0 / float(dist.get_world_size(group=pg))\n    scaled_norm_tensor = scaled_norm\n\n    dist.all_reduce(scaled_norm_tensor, group=pg)\n    total_norm = scaled_norm_tensor\n    total_norm = total_norm.to(parameters[0].device)\n\n    max_norm = torch.tensor([float(max_norm)], device=total_norm.device)\n    clip_coef = max_norm / (total_norm + 1e-6)\n    tmp_tensor = torch.tensor([1.0], device=clip_coef.device)\n    clip_coef = torch.min(tmp_tensor, clip_coef)\n    for p in parameters:\n        p.grad.data.mul_(clip_coef)\n    return total_norm\n\n\ndef get_flattened_grad_norm(parameters, norm_type=2, mpu=None, grad_norm_mask=None):\n    \"\"\"Get grad norm of an iterable of parameters.\n\n    This is adapted from torch.nn.utils.clip_grad.clip_grad_norm_ and\n    added functionality to handle model parallel parameters. Note that\n    the gradients are modified in place. Taken from Nvidia Megatron.\n\n    Arguments:\n        parameters (Iterable[Tensor] or Tensor): an iterable of Tensors or a\n            single Tensor that will have gradients normalized\n        norm_type (float or int): type of the used p-norm. Can be ``'inf'`` for\n            infinity norm.\n        grad_norm_mask (List[Tensor]): A list of Tensor, where\n            each Tensor is a 2D Tensor containing ranges of [start_index, end_index].\n    Returns:\n        Total norm of the parameters (viewed as a single vector).\n    \"\"\"\n    if isinstance(parameters, torch.Tensor):\n        parameters = [parameters]\n    parameters = list(filter(lambda p: p.grad is not None, parameters))\n\n    norm_type = float(norm_type)\n    if norm_type == inf:\n        total_norm = max(p.grad.data.abs().max() for p in parameters)\n        total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])\n        # Take max across all GPUs.\n        if mpu is not None:\n            dist.all_reduce(total_norm_cuda, op=dist.ReduceOp.MAX, group=mpu.get_model_parallel_group())\n        total_norm = total_norm_cuda[0].item()\n    else:\n        total_norm = 0.\n        for idx, p in enumerate(parameters):\n            # Use grad_norm_mask to avoid redundant computation of flattened gradient norm\n            if grad_norm_mask is not None and len(grad_norm_mask[idx]) > 0:\n\n                # A loop-free implementation to create a mask tensor based on a range list\n                # which is logically equivalent to the following implementation.\n                # # mask_tensor_ = torch.zeros_like(p, device=p.device, dtype=bool)\n                # # for mask_idx in grad_norm_mask[idx]:\n                # #   mask_tensor_[mask_idx[0]:mask_idx[1]] = True\n                cum_sum_pairs = torch.tensor([1, -1], device=get_accelerator().current_device_name(),\n                                             dtype=p.dtype).repeat(grad_norm_mask[idx].shape[0], 1)\n                mask_tensor = torch.zeros(p.shape[0] + 1,\n                                          device=get_accelerator().current_device_name(),\n                                          dtype=p.dtype)\n                mask_tensor = mask_tensor.scatter_(0, grad_norm_mask[idx].view(-1),\n                                                   cum_sum_pairs.view(-1)).cumsum(0).bool()[:-1]\n\n                param_norm = torch.masked_fill(p.grad.data, mask_tensor, 0).float().norm(norm_type)\n\n            else:\n                param_norm = p.grad.data.float().norm(norm_type)\n            total_norm += param_norm.item()**norm_type\n\n        # Sum across all model parallel GPUs.\n        total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])\n        if mpu is not None:\n            dist.all_reduce(total_norm_cuda, op=dist.ReduceOp.SUM, group=mpu.get_model_parallel_group())\n        total_norm = total_norm_cuda[0].item()**(1. / norm_type)\n\n    if total_norm == float('inf') or total_norm == -float('inf') or total_norm != total_norm:\n        total_norm = -1\n\n    return total_norm\n\n\ndef get_grad_zeros(parameters, mpu=None):\n    \"\"\"Compute the number of grads with zero values.\n\n    This is adapted from get_grad_norm\n\n    Arguments:\n        parameters (Iterable[Tensor] or Tensor): an iterable of Tensors or a\n            single Tensor that will have gradients normalized\n\n    Returns:\n        Total number of params with zero values (viewed as a single vector).\n    \"\"\"\n    if isinstance(parameters, torch.Tensor):\n        parameters = [parameters]\n    parameters = list(filter(lambda p: p.grad is not None, parameters))\n\n    total_zeros = 0.\n    tensor_mp_rank = bwc_tensor_model_parallel_rank(mpu=mpu)\n    for p in parameters:\n        # Pipeline parallelism may replicate parameters. Avoid multi-counting.\n        if hasattr(p, PIPE_REPLICATED) and p.ds_pipe_replicated:\n            continue\n\n        # Filter to avoid over-counting replicated tensors from tensor\n        # model parallelism\n        if (tensor_mp_rank > 0) and not is_model_parallel_parameter(p):\n            continue\n\n        count_zeros = p.grad.numel() - torch.count_nonzero(p.grad)\n        total_zeros += count_zeros.item()\n\n    # Sum across all model parallel GPUs.\n    total_zeros_cuda = get_accelerator().FloatTensor([float(total_zeros)])\n    if mpu is not None:\n        dist.all_reduce(total_zeros_cuda, op=dist.ReduceOp.SUM, group=mpu.get_model_parallel_group())\n    total_zeros = total_zeros_cuda[0].item()\n\n    return total_zeros\n\n\ndef get_weight_norm(parameters, norm_type=2, mpu=None):\n    \"\"\"Get norm of an iterable of parameters.\n\n    This is adapted from torch.nn.utils.clip_grad.clip_grad_norm_ and\n    added functionality to handle model parallel parameters. Note that\n    the gradients are modified in place. Taken from Nvidia Megatron.\n\n    Arguments:\n        parameters (Iterable[Tensor] or Tensor): an iterable of Tensors or a\n            single Tensor that will have gradients normalized\n        norm_type (float or int): type of the used p-norm. Can be ``'inf'`` for\n            infinity norm.\n\n    Returns:\n        Total norm of the parameters (viewed as a single vector).\n        -1 if the norm value is NaN or Inf.\n    \"\"\"\n    if isinstance(parameters, torch.Tensor):\n        parameters = [parameters]\n\n    norm_type = float(norm_type)\n    if norm_type == inf:\n        total_norm = max(p.data.abs().max() for p in parameters)\n        total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])\n        # Take max across all GPUs.\n        if mpu is not None:\n            dist.all_reduce(total_norm_cuda, op=dist.ReduceOp.MAX, group=mpu.get_model_parallel_group())\n        total_norm = total_norm_cuda[0].item()\n    else:\n        total_norm = 0.\n        tensor_mp_rank = bwc_tensor_model_parallel_rank(mpu=mpu)\n        for p in parameters:\n            # Pipeline parallelism may replicate parameters. Avoid multi-counting.\n            if hasattr(p, PIPE_REPLICATED) and p.ds_pipe_replicated:\n                continue\n\n            # Filter to avoid over-counting replicated tensors from tensor\n            # model parallelism\n            if (tensor_mp_rank > 0) and not is_model_parallel_parameter(p):\n                continue\n\n            param_norm = p.data.float().norm(norm_type)\n            total_norm += param_norm**norm_type\n\n        # Sum across all model parallel GPUs.\n        total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])\n        if mpu is not None:\n            dist.all_reduce(total_norm_cuda, op=dist.ReduceOp.SUM, group=mpu.get_model_parallel_group())\n        total_norm = total_norm_cuda[0].item()**(1. / norm_type)\n\n    if total_norm == float('inf') or total_norm == -float('inf') or total_norm != total_norm:\n        total_norm = -1\n\n    return total_norm\n\n\ndef prefix_sum_inc(weights):\n    \"\"\" Compute an inclusive prefix sum.\n\n    Example:\n        >>> prefix_sum_inc([3,4,5])\n        [3, 7, 12]\n    \"\"\"\n    weights_ = [w for w in weights]\n    for x in range(1, len(weights_)):\n        weights_[x] += weights_[x - 1]\n    return weights_\n\n\ndef partition_uniform(num_items, num_parts):\n    import numpy\n    parts = [0] * (num_parts + 1)\n    # First check for the trivial edge case\n    if num_items <= num_parts:\n        for p in range(num_parts + 1):\n            parts[p] = min(p, num_items)\n        return parts\n\n    chunksize = num_items // num_parts\n    residual = num_items - (chunksize * num_parts)\n\n    parts = numpy.arange(0, (num_parts + 1) * chunksize, chunksize)\n\n    for i in range(residual):\n        parts[i + 1:] += 1\n    parts = parts.tolist()\n\n    return parts\n\n\ndef partition_balanced(weights, num_parts):\n    \"\"\"\n    use dynamic programming solve `The Linear Partition Problem`.\n    see https://www8.cs.umu.se/kurser/TDBAfl/VT06/algorithms/BOOK/BOOK2/NODE45.HTM\n    \"\"\"\n    import numpy as np\n    n = len(weights)\n    m = num_parts\n\n    if n <= m:\n        return partition_uniform(n, m)\n\n    dp_max = np.full((n + 1, m + 1), np.inf)\n    dp_min = np.full((n + 1, m + 1), np.inf)\n    dp_cost = np.full((n + 1, m + 1), np.inf)\n    position = np.zeros((n + 1, m + 1), dtype=int)\n    prefix_sum = np.zeros((n + 1))\n    prefix_sum[1:] = np.cumsum(weights)\n\n    dp_max[0, 0] = 0\n    dp_cost[0, 0] = 0\n    for i in range(1, n + 1):\n        for j in range(1, min(i, m) + 1):\n            for k in range(i):\n                max_sum = max(dp_max[k, j - 1], prefix_sum[i] - prefix_sum[k])\n                min_sum = min(dp_min[k, j - 1], prefix_sum[i] - prefix_sum[k])\n                cost = max_sum - min_sum\n                if dp_cost[i, j] >= cost:\n                    dp_cost[i, j] = cost\n                    dp_max[i, j] = max_sum\n                    dp_min[i, j] = min_sum\n                    position[i, j] = k\n\n    parts = [n]\n    for i in reversed(range(1, m + 1)):\n        parts.append(position[parts[-1], i])\n    parts.reverse()\n\n    return parts\n\n\nclass PartitionedTensor:\n\n    def __init__(self, tensor, group, partition_meta=None):\n        super().__init__()\n\n        self.group = group\n        self.num_parts = dist.get_world_size(group=self.group)\n        self.rank = dist.get_rank(group=self.group)\n        self.orig_size = list(tensor.size())\n        self.orig_device = tensor.device\n        self.local_data, self.partition = self._partition_tensor(tensor)\n        self.even_split = tensor.numel() % self.num_parts == 0\n\n    @classmethod\n    def from_meta(cls, meta, local_part, group, device=get_accelerator().device_name()):\n        assert meta.dtype == torch.long\n        dummy = torch.ones(dist.get_world_size(group=group))\n        part_obj = cls(tensor=dummy, group=group)\n\n        meta = meta.tolist()\n\n        # [N, list0, ..., listN-1]\n        part_obj.orig_size = meta[1:(1 + meta[0])]\n        meta = meta[1 + meta[0]:]\n\n        part_obj.orig_device = device\n        part_obj.local_data = local_part.detach()\n\n        part_obj.group = group\n\n        # Partition is encoded like the rowptr of a CSR matrix:\n        # [num_parts, rank, 0, part_1, ..., part_num_parts]\n        # TODO: support shuffle between different partition granularities\n        assert part_obj.num_parts == meta[0]\n        assert part_obj.rank == meta[1]\n        part_obj.partition = meta[2:]  # length num_parts+1\n\n        return part_obj\n\n    def _partition_tensor(self, tensor):\n        partition = partition_uniform(num_items=tensor.numel(), num_parts=self.num_parts)\n        start = partition[self.rank]\n        length = partition[self.rank + 1] - start\n        tensor_part = tensor.detach().contiguous().view(-1).narrow(0, start=start, length=length).clone()\n\n        return tensor_part, partition\n\n    def full(self, device=None):\n        if device is None:\n            device = self.orig_device\n\n        # Allocate the full tensor as a flat buffer.\n        full_numel = prod(self.full_size())\n        flat_tensor = torch.zeros([full_numel], dtype=self.local_data.dtype, device=device)\n        if self.even_split:\n            # Collect the full tensor\n            dist.all_gather_into_tensor(flat_tensor, self.local_data, group=self.group)\n        else:\n            for part_id in range(self.num_parts):\n                part_size = self.partition[part_id + 1] - self.partition[part_id]\n                buf = flat_tensor.narrow(0, start=self.partition[part_id], length=part_size)\n                if part_id == self.rank:\n                    buf.copy_(self.local_data)\n                dist.broadcast(buf, part_id, self.group)\n        return flat_tensor.view(self.full_size()).clone().detach()\n\n    def to_meta(self):\n        \"\"\"Returns a torch.LongTensor that encodes partitioning information.\n\n        Can be used along with ``data()`` to serialize a ``PartitionedTensor`` for\n        communication.\n\n        Returns:\n            torch.LongTensor: a tensor encoding the meta-information for the partitioning\n        \"\"\"\n        meta = []\n        meta.append(len(self.orig_size))\n        meta += list(self.orig_size)\n        meta.append(self.num_parts)\n        meta.append(self.rank)\n        meta += self.partition\n        return torch.LongTensor(data=meta).to(self.orig_device)\n\n    def data(self):\n        return self.local_data\n\n    def local_size(self):\n        return self.local_data.size()\n\n    def full_size(self):\n        return self.orig_size\n\n\nmem_alloced = 0\nmem_cached = 0\n\n\ndef memory_status(msg, print_rank=-1, reset_max=False):\n    global mem_alloced, mem_cached\n\n    rank = dist.get_rank()\n    if print_rank != -1 and rank != print_rank:\n        return\n\n    get_accelerator().synchronize()\n\n    if reset_max:\n        get_accelerator().reset_max_memory_cached()\n        get_accelerator().reset_max_memory_allocated()\n\n    new_alloced = get_accelerator().memory_allocated()\n    new_cached = get_accelerator().memory_cached()\n\n    delta_alloced = new_alloced - mem_alloced\n    delta_cached = new_cached - mem_cached\n\n    mem_cached = new_cached\n    mem_alloced = new_alloced\n\n    max_alloced = get_accelerator().max_memory_allocated()\n    max_cached = get_accelerator().max_memory_cached()\n\n    # convert to GB for printing\n    new_alloced /= 1024**3\n    new_cached /= 1024**3\n    delta_alloced /= 1024**3\n    delta_cached /= 1024**3\n    max_alloced /= 1024**3\n    max_cached /= 1024**3\n\n    print(\n        f'RANK={rank} MEMSTATS', msg, f'device={get_accelerator().current_device_name()} '\n        f'current alloc={new_alloced:0.4f}GB (delta={delta_alloced:0.4f}GB max={max_alloced:0.4f}GB) '\n        f'current cache={new_cached:0.4f}GB (delta={delta_cached:0.4f}GB max={max_cached:0.4f}GB)')\n\n\ndef get_ma_status():\n    if dist.is_initialized() and not dist.get_rank() == 0:\n        return 0\n    return get_accelerator().memory_allocated()\n\n\ndef empty_cache():\n    get_accelerator().empty_cache()\n    get_accelerator().reset_peak_memory_stats()\n\n\ndef see_memory_usage(message, force=False):\n    if not force:\n        return\n    if dist.is_initialized() and not dist.get_rank() == 0:\n        return\n\n    # python doesn't do real-time garbage collection so do it explicitly to get the correct RAM reports\n    gc.collect()\n\n    # Print message except when distributed but not rank 0\n    logger.info(message)\n    logger.info(f\"MA {round(get_accelerator().memory_allocated() / (1024 * 1024 * 1024),2 )} GB \\\n        Max_MA {round(get_accelerator().max_memory_allocated() / (1024 * 1024 * 1024),2)} GB \\\n        CA {round(torch_memory_reserved() / (1024 * 1024 * 1024),2)} GB \\\n        Max_CA {round(torch_max_memory_reserved() / (1024 * 1024 * 1024))} GB \")\n\n    vm_stats = psutil.virtual_memory()\n    used_GB = round(((vm_stats.total - vm_stats.available) / (1024**3)), 2)\n    logger.info(f'CPU Virtual Memory:  used = {used_GB} GB, percent = {vm_stats.percent}%')\n\n    # get the peak memory to report correct data, so reset the counter for the next call\n    get_accelerator().reset_peak_memory_stats()\n\n\ndef call_to_str(base, *args, **kwargs):\n    \"\"\"Construct a string representation of a call.\n\n    Args:\n        base (str): name of the call\n        args (tuple, optional): args to ``base``\n        kwargs (dict, optional): kwargs supplied to ``base``\n\n    Returns:\n        str: A string representation of base(*args, **kwargs)\n    \"\"\"\n    name = f'{base}('\n    if args:\n        name += ', '.join(repr(arg) for arg in args)\n        if kwargs:\n            name += ', '\n    if kwargs:\n        name += ', '.join(f'{key}={repr(arg)}' for key, arg in kwargs.items())\n    name += ')'\n    return name\n\n\ndef get_only_unique_item(items):\n    item_set = set(items)\n    if len(item_set) != 1:\n        raise RuntimeError(f\"expected there to be only one unique element in {items}\")\n    unique_item, = item_set\n\n    return unique_item\n\n\ndef get_global_norm_of_tensors(input_tensors, norm_type=2, mpu=None, use_graph=False, moe_ep_group=None):\n    \"\"\"Get norm of an iterable of tensors.\n\n    This is adapted from torch.nn.utils.clip_grad.clip_grad_norm_ and\n    added functionality to handle model parallel parameters. Taken from Nvidia Megatron.\n\n    Arguments:\n        input_tensors (Iterable[Tensor]): an iterable of Tensors will have norm computed\n        norm_type (float or int): type of the used p-norm. Can be ``'inf'`` for\n            infinity norm.\n\n    Returns:\n        Total norm of the tensors (viewed as a single vector).\n    \"\"\"\n    assert isinstance(input_tensors, Iterable), f'expected Iterable type not {type(input_tensors)}'\n    assert all([torch.is_tensor(t) for t in input_tensors]), f'expected list of only tensors'\n\n    norm_type = float(norm_type)\n    all_norms = []\n    if norm_type == inf:\n        for t in input_tensors:\n            all_norms.append(t.data.abs().max().float())\n        total_norm = torch.stack(all_norms).max()\n        device_total_norm = total_norm.to(get_accelerator().current_device_name())\n        # Max across model parallel\n        if mpu is not None:\n            # For MoE grads, max over model parallel only if MoE-TP is enabled\n            if moe_ep_group is None or groups._get_expert_model_parallel_world_size() > 1:\n                dist.all_reduce(device_total_norm, op=dist.ReduceOp.MAX, group=mpu.get_model_parallel_group())\n            # If MoE grads and MoE-TP disabled, max over pipeline parallel\n            elif bwc_pipeline_parallel_world_size(mpu) > 1:\n                dist.all_reduce(device_total_norm, op=dist.ReduceOp.MAX, group=bwc_pipeline_parallel_group(mpu))\n\n        # MoE grads: max across expert parallel group\n        if moe_ep_group is not None:\n            dist.all_reduce(device_total_norm, op=dist.ReduceOp.MAX, group=moe_ep_group)\n        total_norm = device_total_norm.to(input_tensors[0].device)\n    else:\n\n        if 'norm_tensors_compute_buffer' not in graph_cache or len(\n                graph_cache['norm_tensors_compute_buffer']) != len(input_tensors):\n            graph_cache['norm_tensors_compute_buffer'] = [\n                torch.empty([], dtype=torch.float, device=get_accelerator().current_device_name())\n                for t in input_tensors\n            ]\n        compute_buffer = graph_cache['norm_tensors_compute_buffer']\n\n        def _norm_tensors(tensor_list, _compute_buffer, _norm_type):\n            for i, t in enumerate(tensor_list):\n                _compute_buffer[i].data.copy_(t.data.float().norm(_norm_type)**_norm_type)\n                if i != 0:\n                    _compute_buffer[0].data.add_(_compute_buffer[i].data)\n\n        if use_graph:\n            graph_process(False, _norm_tensors, input_tensors, compute_buffer, norm_type)\n        else:\n            _norm_tensors(input_tensors, compute_buffer, norm_type)\n\n        device_total_norm = compute_buffer[0].float().detach()\n\n        # Sum across model parallel\n        if mpu is not None:\n            # For MoE grads, sum over model parallel only if MoE-TP is enabled\n            if moe_ep_group is None or groups._get_expert_model_parallel_world_size() > 1:\n                dist.all_reduce(device_total_norm, op=dist.ReduceOp.SUM, group=mpu.get_model_parallel_group())\n            # If MoE grads and MoE-TP disabled, sum over pipeline parallel\n            elif bwc_pipeline_parallel_world_size(mpu) > 1:\n                dist.all_reduce(device_total_norm, op=dist.ReduceOp.SUM, group=bwc_pipeline_parallel_group(mpu))\n\n        # MoE grads: sum across expert parallel group\n        if moe_ep_group is not None:\n            dist.all_reduce(device_total_norm, op=dist.ReduceOp.SUM, group=moe_ep_group)\n        total_norm = device_total_norm.to(input_tensors[0].device).pow(1. / norm_type)\n\n    inf_or_nan = total_norm.isinf().logical_or(total_norm.isnan())\n    total_norm.masked_fill_(inf_or_nan, -1)\n\n    return total_norm\n\n\ndef clip_tensors_by_global_norm(input_tensors, max_norm=1.0, global_norm=None, mpu=None, eps=1e-6, use_graph=False):\n    \"\"\"Clip list of tensors by global norm.\n    Args:\n        input_tensors: List of tensors to be clipped\n        global_norm (float, optional): Precomputed norm. Defaults to None.\n        mpu (optional): model parallelism unit. Defaults to None.\n        eps (float, optional): epsilon value added to grad norm. Defaults to 1e-6\n    Returns:\n        float: the global norm\n    \"\"\"\n    if global_norm is None:\n        global_norm = get_global_norm_of_tensors(input_tensors, mpu=mpu, use_graph=use_graph)\n    clip_coef = max_norm / (global_norm + eps)\n    if clip_coef < 1:\n        if use_graph:\n\n            def clip_tensors(_tensor_list, _clip_coef_tensor):\n                for t in _tensor_list:\n                    t.detach().mul_(_clip_coef_tensor)\n\n            if 'clip_coef_tensor' not in graph_cache:\n                # Alloc memory\n                graph_cache['clip_coef_tensor'] = torch.tensor(clip_coef,\n                                                               dtype=torch.float32).to(get_accelerator().device_name())\n            clip_coef_tensor = graph_cache['clip_coef_tensor']\n            clip_coef_tensor.copy_(torch.tensor(clip_coef, dtype=torch.float32))\n            graph_process(False, clip_tensors, input_tensors, clip_coef_tensor)\n\n        else:\n            for t in input_tensors:\n                t.detach().mul_(clip_coef)\n    return global_norm\n\n\ndef align_dense_tensors(tensor_list, alignment):\n    num_elements = sum(t.numel() for t in tensor_list)\n    remaining = num_elements % alignment\n\n    if remaining:\n        elements_to_add = alignment - remaining\n        pad_tensor = torch.zeros(elements_to_add, device=tensor_list[0].device, dtype=tensor_list[0].dtype)\n        padded_tensor_list = tensor_list + [pad_tensor]\n    else:\n        padded_tensor_list = tensor_list\n\n    return padded_tensor_list\n\n\ndef all_gather_into_tensor_dp_groups(groups_flat, partitioned_param_groups, dp_process_group):\n    for group_id, (group_flat, partitioned_params) in enumerate(zip(groups_flat, partitioned_param_groups)):\n        partition_id = dist.get_rank(group=dp_process_group[group_id])\n        dp_world_size = dist.get_world_size(group=dp_process_group[group_id])\n        if dp_world_size == 1:\n            # no groups share optimizer states\n            # pipeline parallel with bf16 will default call this even if dp size = 1.\n            continue\n        dist.all_gather_into_tensor(group_flat, partitioned_params[partition_id], dp_process_group[group_id])\n\n\ndef all_gather_dp_groups(groups_flat, partitioned_param_groups, dp_process_group, start_alignment_factor,\n                         allgather_bucket_size):\n    if dist.has_all_gather_into_tensor():\n        return all_gather_into_tensor_dp_groups(groups_flat, partitioned_param_groups, dp_process_group)\n\n    for group_id, partitioned_params in enumerate(partitioned_param_groups):\n        # Sequential AllGather Best of both worlds\n        partition_id = dist.get_rank(group=dp_process_group[group_id])\n        dp_world_size = dist.get_world_size(group=dp_process_group[group_id])\n\n        if dp_world_size == 1:\n            # no groups share optimizer states\n            # pipeline parallel with bf16 will default call this even if dp size = 1.\n            continue\n        num_shards = max(1, partitioned_params[partition_id].numel() * dp_world_size // allgather_bucket_size)\n\n        shard_size = partitioned_params[partition_id].numel() // num_shards\n\n        # Enforce nccl/rccl alignment of start location of each shard\n        shard_size = shard_size - (shard_size % start_alignment_factor)\n\n        num_elements = shard_size\n\n        assert shard_size * num_shards <= partitioned_params[partition_id].numel()\n\n        for shard_id in range(num_shards):\n\n            if shard_id == (num_shards - 1):\n                num_elements = partitioned_params[partition_id].numel() - shard_id * shard_size\n\n            shard_list = []\n            for dp_id in range(dp_world_size):\n                curr_shard = partitioned_params[dp_id].narrow(0, shard_id * shard_size, num_elements).detach()\n                shard_list.append(curr_shard)\n\n            dist.all_gather(shard_list, shard_list[partition_id], dp_process_group[group_id])\n\n\nclass TLinear(torch.nn.Linear):\n\n    def __init__(self, orig_layer, name=\"\"):\n        self.name = name\n        super().__init__(orig_layer.weight.shape[1], orig_layer.weight.shape[0], bias=(orig_layer.bias is not None))\n        self.weight.data = transpose(orig_layer.weight.data)\n        self.bias = orig_layer.bias\n        self._fwd_func = self._fwd_bias_add if self.bias is not None else self._fwd\n\n    def _fwd(self, input):\n        return F.linear(input, self.weight)\n\n    def _fwd_bias_add(self, input):\n        return F.linear(input, self.weight, bias=self.bias)\n\n    def forward(self, input):\n        return self._fwd_func(input)\n\n\ndef get_inactive_params(param_list):\n    from deepspeed.runtime.zero.partition_parameters import ZeroParamStatus\n    return [param for param in param_list if (hasattr(param, 'ds_id') and \\\n                            param.ds_status == ZeroParamStatus.NOT_AVAILABLE)]\n\n\ndef get_norm_with_moe_layers(non_expert_norm, mpu, expert_tensors, norm_type=2):\n    \"\"\" Compute the global norm with MoE experts\n\n    Inputs:\n    non_expert_norm (float) : the calculated norm of the non-expert params\n    expert_tensors (Dict[ep_name, List[Tensor]): Dictionary of expert group name to list of grad tensors\n    norm_type (int): the norm to use\n\n    Returns:\n        if norm is (-/+) inf, returns -1\n        otherwise the global norm (float)\n    \"\"\"\n\n    def to_tensor(v):\n        return get_accelerator().FloatTensor(float(v)).detach()\n\n    group_norms = [non_expert_norm]\n    for exp_name, tensors in expert_tensors.items():\n        group_norm = get_global_norm_of_tensors(input_tensors=tensors,\n                                                mpu=mpu,\n                                                norm_type=norm_type,\n                                                use_graph=False,\n                                                moe_ep_group=groups._get_expert_parallel_group(exp_name))\n        group_norms.append(group_norm)\n\n    # check if all norms are valid\n    group_norms = torch.stack([to_tensor(norm) for norm in group_norms])\n    if group_norms.eq(-1).any():\n        return -1\n\n    # combine norms\n    if norm_type == inf:\n        total_norm = group_norms.max().item()\n    else:\n        total_norm = group_norms.pow(norm_type).sum()\n        total_norm = total_norm.item()**(1. / norm_type)\n        if total_norm == float('inf') or total_norm == -float('inf'):\n            total_norm = -1\n\n    return total_norm\n", "deepspeed/runtime/eigenvalue.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nfrom deepspeed.utils import log_dist\nimport numpy as np\nimport logging\n\n\nclass Eigenvalue(object):\n\n    def __init__(self,\n                 verbose=False,\n                 max_iter=100,\n                 tol=1e-2,\n                 stability=0,\n                 gas_boundary_resolution=1,\n                 layer_name='',\n                 layer_num=0):\n        super().__init__()\n\n        self.verbose = verbose\n        self.max_iter = max_iter\n        self.tol = tol\n        self.stability = stability\n        self.gas_boundary_resolution = gas_boundary_resolution\n        self.layer_name = layer_name\n        self.layer_num = layer_num\n\n        assert len(self.layer_name) > 0 and layer_num > 0\n\n        log_dist(\n            f'enabled eigenvalue with verbose={verbose}, max_iter={max_iter}, tol={tol}, stability={stability}, gas_boundary_resolution={gas_boundary_resolution}, layer_name={layer_name}, layer_num={layer_num}',\n            ranks=[0])\n\n    # Replace all nan/pos-inf/neg-inf to zero\n    # TODO: Pytorch new version may add this function, replace this one by then.\n    def nan_to_num(self, x):\n        device = x.device\n        x = x.cpu().numpy()\n        x = np.nan_to_num(x=x, copy=False, nan=0.0, posinf=0.0, neginf=0.0)\n        return torch.from_numpy(x).to(device)\n\n    def normalize(self, v):\n        norm_squared = self.inner_product(v, v)\n        norm = norm_squared**0.5 + self.stability\n        normalized_vectors = [vector / norm for vector in v]\n        normalized_vectors = [self.nan_to_num(vector) for vector in normalized_vectors]\n        return normalized_vectors\n\n    def inner_product(self, xs, ys):\n        return sum([torch.sum(x * y) for (x, y) in zip(xs, ys)])\n\n    def get_layers(self, module):\n        scope_names = self.layer_name.split('.')\n        assert len(scope_names) > 0\n\n        m = module\n        for name in scope_names:\n            assert hasattr(m, name), \"layer_name configuration is invalid.\"\n            m = getattr(m, name)\n\n        return m\n\n    def compute_eigenvalue(self, module, device=None, scale=1.0):\n        block_eigenvalue = []\n        param_keys = []\n        layers = self.get_layers(module)\n\n        for block in range(self.layer_num):\n            model_block = layers[block]\n\n            # We found this randn() has obvious accuracy impact in some cases, save/recover random state here.\n            rng_state = torch.random.get_rng_state()\n            if device is None:\n                v = [\n                    torch.randn(p.size()) for p in model_block.parameters()\n                    if p.grad is not None and p.grad.grad_fn is not None\n                ]\n            else:\n                v = [\n                    torch.randn(p.size(), device=device) for p in model_block.parameters()\n                    if p.grad is not None and p.grad.grad_fn is not None\n                ]\n            torch.random.set_rng_state(rng_state)\n\n            grads = [\n                param.grad for param in model_block.parameters()\n                if param.grad is not None and param.grad.grad_fn is not None\n            ]\n            params = [\n                param for param in model_block.parameters()\n                if param.grad is not None and param.grad.grad_fn is not None\n            ]\n\n            layer_keys = [id(p) for p in model_block.parameters()]\n            param_keys.append(layer_keys)\n\n            v = self.normalize(v)\n\n            # Disable eigenvalue if the model doesn't support second order gradients computation,\n            # e.g. when enabling DS transformer kernel.\n            if len(grads) == 0 or len(params) == 0:\n                log_dist(f'The model does NOT support eigenvalue computation.', ranks=[0], level=logging.WARNING)\n                return []\n\n            i = 0\n            eigenvalue_current, eigenvalue_previous = 1., 0.\n\n            while (i < self.max_iter) and abs(eigenvalue_current) > 0 and (abs(\n                (eigenvalue_current - eigenvalue_previous) / eigenvalue_current) >=\n                                                                           self.tol):  # test convergence criteria\n                eigenvalue_previous = eigenvalue_current\n\n                Hv = torch.autograd.grad(grads, params, grad_outputs=v, only_inputs=True, retain_graph=True)\n                #Hv = [hv.float() for hv in Hv]\n                Hv = [self.nan_to_num(hv).float() for hv in Hv]\n\n                eigenvalue_current = self.inner_product(Hv, v).item()\n\n                v = self.normalize(Hv)\n                v = [x / scale for x in v]\n                i += 1\n\n            eigenvalue_current *= scale\n            block_eigenvalue.append(eigenvalue_current)\n\n            if self.verbose:\n                log_dist(f'block: {block}, power iteration: {i}, eigenvalue: {eigenvalue_current}', ranks=[0])\n\n        block_eigenvalue = self.post_process(block_eigenvalue)\n\n        if self.verbose:\n            log_dist(f'post processed block_eigenvalue: {block_eigenvalue}', ranks=[0])\n\n        # {param_id: (eigenvalue, layer_id)}\n        ev_dict = {}\n        for i, (layer_keys, value) in enumerate(zip(param_keys, block_eigenvalue)):\n            ev_dict.update(dict.fromkeys(layer_keys, (value, i)))\n\n        return ev_dict\n\n    # 1. Map all eigenvalues to [0, 1.0].\n    # 2. Some layers can't generate valid eigenvalues on fp16 precision, use 1.0 instead.\n    def post_process(self, value_list):\n        max_value = abs(max(value_list, key=abs))\n        return [abs(v) / max_value if v != 0.0 else 1.0 for v in value_list]\n", "deepspeed/runtime/config_utils.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\"\"\"\nCollection of DeepSpeed configuration utilities\n\"\"\"\nimport json\nimport collections\nimport collections.abc\nfrom functools import reduce\nfrom deepspeed.pydantic_v1 import BaseModel\nfrom deepspeed.utils import logger\n\n\nclass DeepSpeedConfigModel(BaseModel):\n    \"\"\"\n    This class should be used as a base for all DeepSpeed configs. It extends\n    pydantic.BaseModel to allow for deprecated fields. To enable this feature,\n    add deprecated=True to pydantic.Field:\n\n    my_dep_field: int = Field(0, deprecated=True)\n\n    Deprecated Field kwargs:\n    - deprecated: [True|False], default False\n        Enables / Disables deprecated fields\n    - deprecated_msg: str, default \"\"\n        Message to include with deprecation warning\n    - new_param: str, default \"\"\n        Name of the field replacing the deprecated field\n    - set_new_param: [True|False], default True\n        If new_param is provided, enables setting the value of that param with\n        deprecated field value\n    - new_param_fn: callable, default (lambda x: x)\n        If new_param is provided and set_new_param is True, this function will\n        modify the value of the deprecated field before placing that value in\n        the new_param field\n\n    Example:\n        my_new_field is replacing a deprecated my_old_field. The expected type\n        for my_new_field is int while the expected type for my_old_field is\n        str. We want to maintain backward compatibility with our configs, so we\n        define the fields with:\n\n        class MyExampleConfig(DeepSpeedConfigModel):\n            my_new_field: int = 0\n            my_old_field: str = Field('0',\n                                      deprecated=True,\n                                      new_param='my_new_field',\n                                      new_param_fn=(lambda x: int(x)))\n    \"\"\"\n\n    def __init__(self, strict=False, **data):\n        if (not strict):  # This is temporary until we refactor all DS configs, allows HF to load models\n            data = {k: v for k, v in data.items() if (v != \"auto\" or k == \"replace_method\")}\n        super().__init__(**data)\n        self._deprecated_fields_check(self)\n\n    def _process_deprecated_field(self, pydantic_config, field):\n        # Get information about the deprecated field\n        fields_set = pydantic_config.__fields_set__\n        dep_param = field.name\n        kwargs = field.field_info.extra\n        new_param_fn = kwargs.get(\"new_param_fn\", lambda x: x)\n        param_value = new_param_fn(getattr(pydantic_config, dep_param))\n        new_param = kwargs.get(\"new_param\", \"\")\n        dep_msg = kwargs.get(\"deprecated_msg\", \"\")\n        if dep_param in fields_set:\n            logger.warning(f\"Config parameter {dep_param} is deprecated\" +\n                           (f\" use {new_param} instead\" if new_param else \"\") + (f\". {dep_msg}\" if dep_msg else \"\"))\n            # Check if there is a new param and if it should be set with a value\n            if new_param and kwargs.get(\"set_new_param\", True):\n                # Remove the deprecate field if there is a replacing field\n                try:\n                    delattr(pydantic_config, dep_param)\n                except Exception as e:\n                    logger.error(f\"Tried removing deprecated '{dep_param}' from config\")\n                    raise e\n\n                # Set new param value\n                new_param_nested = new_param.split(\".\")\n                if len(new_param_nested) > 1:\n                    # If the new param exists in a subconfig, we need to get\n                    # the fields set for that subconfig\n                    pydantic_config = reduce(getattr, new_param_nested[:-1], pydantic_config)\n                    fields_set = pydantic_config.__fields_set__\n                new_param_name = new_param_nested[-1]\n                assert (\n                    new_param_name not in fields_set\n                ), f\"Cannot provide deprecated parameter '{dep_param}' and replacing parameter '{new_param}' together\"\n                # A custom function for converting the old param value to new param value can be provided\n                try:\n                    setattr(pydantic_config, new_param_name, param_value)\n                except Exception as e:\n                    logger.error(f\"Tried setting value for '{new_param}' with value from deprecated '{dep_param}'\")\n                    raise e\n\n    def _deprecated_fields_check(self, pydantic_config):\n        fields = pydantic_config.__fields__\n        for field in fields.values():\n            if field.field_info.extra.get(\"deprecated\", False):\n                self._process_deprecated_field(pydantic_config, field)\n\n    class Config:\n        validate_all = True\n        validate_assignment = True\n        use_enum_values = True\n        allow_population_by_field_name = True\n        extra = \"forbid\"\n        arbitrary_types_allowed = True\n\n\ndef get_config_default(config, field_name):\n    assert field_name in config.__fields__, f\"'{field_name}' is not a field in {config}\"\n    assert not config.__fields__.get(\n        field_name).required, f\"'{field_name}' is a required field and does not have a default value\"\n    return config.__fields__.get(field_name).default\n\n\nclass pp_int(int):\n    \"\"\"\n    A wrapper for integers that will return a custom string or comma-formatted\n    string of the integer. For example, print(pp_int(1e5)) will return\n    \"10,000\". This is useful mainly for auto-generated documentation purposes.\n    \"\"\"\n\n    def __new__(cls, val, custom_print_str=None):\n        inst = super().__new__(cls, val)\n        inst.custom_print_str = custom_print_str\n        return inst\n\n    def __repr__(self):\n        if self.custom_print_str:\n            return self.custom_print_str\n        return f\"{self.real:,}\"\n\n\n# adapted from https://stackoverflow.com/a/50701137/9201239\nclass ScientificNotationEncoder(json.JSONEncoder):\n    \"\"\"\n    This class overrides ``json.dumps`` default formatter.\n\n    This version keeps everything as normal except formats numbers bigger than 1e3 using scientific notation.\n\n    Just pass ``cls=ScientificNotationEncoder`` to ``json.dumps`` to activate it\n\n    \"\"\"\n\n    def iterencode(self, o, _one_shot=False, level=0):\n        indent = self.indent if self.indent is not None else 4\n        prefix_close = \" \" * level * indent\n        level += 1\n        prefix = \" \" * level * indent\n        if isinstance(o, bool):\n            return \"true\" if o else \"false\"\n        elif isinstance(o, float) or isinstance(o, int):\n            if o > 1e3:\n                return f\"{o:e}\"\n            else:\n                return f\"{o}\"\n        elif isinstance(o, collections.abc.Mapping):\n            x = [f'\\n{prefix}\"{k}\": {self.iterencode(v, level=level)}' for k, v in o.items()]\n            return \"{\" + \", \".join(x) + f\"\\n{prefix_close}\" + \"}\"\n        elif isinstance(o, collections.abc.Sequence) and not isinstance(o, str):\n            return f\"[{ f', '.join(map(self.iterencode, o)) }]\"\n        return \"\\n, \".join(super().iterencode(o, _one_shot))\n\n\nclass DeepSpeedConfigObject(object):\n    \"\"\"\n    For json serialization\n    \"\"\"\n\n    def repr(self):\n        return self.__dict__\n\n    def __repr__(self):\n        return json.dumps(\n            self.__dict__,\n            sort_keys=True,\n            indent=4,\n            cls=ScientificNotationEncoder,\n        )\n\n\ndef get_scalar_param(param_dict, param_name, param_default_value):\n    return param_dict.get(param_name, param_default_value)\n\n\ndef get_list_param(param_dict, param_name, param_default_value):\n    return param_dict.get(param_name, param_default_value)\n\n\ndef get_dict_param(param_dict, param_name, param_default_value):\n    return param_dict.get(param_name, param_default_value)\n\n\ndef dict_raise_error_on_duplicate_keys(ordered_pairs):\n    \"\"\"Reject duplicate keys.\"\"\"\n    d = dict((k, v) for k, v in ordered_pairs)\n    if len(d) != len(ordered_pairs):\n        counter = collections.Counter([pair[0] for pair in ordered_pairs])\n        keys = [key for key, value in counter.items() if value > 1]\n        raise ValueError(\"Duplicate keys in DeepSpeed config: {}\".format(keys))\n    return d\n", "deepspeed/runtime/sparse_tensor.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\"\"\"\nImplementation of a compressed sparse tensor. Similar in\nfunctionality to TensorFlow's IndexedSlices implementation.\n\"\"\"\n\nimport torch\n\n\nclass SparseTensor(object):\n    \"\"\" Compressed Sparse Tensor \"\"\"\n\n    def __init__(self, dense_tensor=None):\n        self.orig_dense_tensor = dense_tensor\n        self.dtype = self.orig_dense_tensor.dtype\n        self.is_sparse = dense_tensor.is_sparse\n        if dense_tensor is not None:\n            if dense_tensor.is_sparse:\n                dense_tensor = dense_tensor.coalesce()\n                self.indices = dense_tensor.indices().flatten()\n                self.values = dense_tensor.values()\n            else:\n                result = torch.sum(dense_tensor, dim=1)\n                self.indices = result.nonzero().flatten()\n                self.values = dense_tensor[self.indices]\n            self.dense_size = list(dense_tensor.size())\n        else:\n            self.indices = None\n            self.values = None\n            self.dense_size = None\n\n    def to_coo_tensor(self):\n        return torch.sparse_coo_tensor(self.indices.unsqueeze(0), self.values, self.dense_size)\n\n    @staticmethod\n    def type():\n        return \"deepspeed.SparseTensor\"\n\n    def to_dense(self):\n        it = self.indices.unsqueeze(1)\n        full_indices = torch.cat([it for _ in range(self.dense_size[1])], dim=1)\n        return self.values.new_zeros(self.dense_size).scatter_add_(0, full_indices, self.values)\n\n    def sparse_size(self):\n        index_size = list(self.indices.size())\n        index_size = index_size[0]\n        value_size = list(self.values.size())\n        value_size = value_size[0] * value_size[1]\n        dense_size = self.dense_size[0] * self.dense_size[1]\n        return index_size + value_size, dense_size\n\n    def add(self, b):\n        assert self.dense_size == b.dense_size\n        self.indices = torch.cat([self.indices, b.indices])\n        self.values = torch.cat([self.values, b.values])\n\n    def __str__(self):\n        sparse_size, dense_size = self.sparse_size()\n        return \"DeepSpeed.SparseTensor(indices_size={}, values_size={}, \" \\\n               \"dense_size={}, device={}, reduction_factor={})\".format(\n            self.indices.size(), self.values.size(), self.dense_size,\n            self.indices.get_device(), dense_size / sparse_size\n        )\n\n    def __repr__(self):\n        return self.__str__()\n", "deepspeed/runtime/base_optimizer.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport os\nimport torch\n\nfrom deepspeed.utils import logger\nfrom deepspeed.utils.tensor_fragment import map_to_flat_opt_states\nfrom deepspeed.runtime.utils import bwc_tensor_model_parallel_rank\n\n\nclass DeepSpeedOptimizer(object):\n    pass\n\n\nclass ZeROOptimizer(DeepSpeedOptimizer):\n\n    def load_hp_checkpoint_state_from_checkpoint_dir(self, lp_groups_name: str, checkpoint_dir: str) -> None:\n        checkpoint_dir = os.path.join(checkpoint_dir, \"zero\")\n        optim_state_path = os.path.join(checkpoint_dir, \"optimizer_state.pt\")\n        assert os.path.isfile(\n            optim_state_path), f'{optim_state_path} containing optimizer global state is missing! Cannot proceed.'\n        optim_sd = torch.load(optim_state_path)\n\n        self._load_global_state(optim_sd)\n\n        tp_rank = bwc_tensor_model_parallel_rank(mpu=self.mpu)\n        if self.mpu is None:\n            logger.warn(\"MPU is not provided, setting tp size to 1 in checkpoint loading.\")\n            tp_world_size = 1\n        else:\n            tp_world_size = self.mpu.get_slice_parallel_world_size() if hasattr(self.mpu, \"get_slice_parallel_world_size\") \\\n                else self.mpu.get_tensor_model_parallel_world_size()\n\n        for i, (param_group,\n                loaded_param_group) in enumerate(zip(self.optimizer.param_groups, optim_sd['param_groups'])):\n            # We have an assumption that all params in the same param_group have the same keys\n            opt_keys = set()\n            steps = []\n\n            lp_groups = getattr(self, lp_groups_name)\n            for lp in lp_groups[i]:\n                if lp._hp_mapping is not None:\n                    #print(f\"Loading {self.param_names[lp]} {tp_rank=} {tp_world_size=}\")\n                    step = lp.load_hp_checkpoint_state(os.path.join(checkpoint_dir, self.param_names[lp]), tp_rank,\n                                                       tp_world_size)\n                    for key in lp._hp_mapping.get_optim_state_keys():\n                        opt_keys.add(key)\n                    steps.append(step)\n\n            hp_param = param_group['params'][0]\n            assert all(step == steps[0] for step in steps), f\"Steps {steps} are not equal\"\n            if steps[0] is not None:\n                self.optimizer.state[hp_param]['step'] = steps[0]\n\n            map_to_flat_opt_states(hp_param, lp_groups[i], self.optimizer.state, opt_keys)\n\n            for key, value in loaded_param_group.items():\n                if key == 'params':\n                    continue\n                param_group[key] = value\n", "deepspeed/runtime/constants.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\n#############################################\n# Routes\n#############################################\nROUTE_TRAIN = \"train\"\nROUTE_EVAL = \"eval\"\nROUTE_PREDICT = \"predict\"\nROUTE_ENCODE = \"encode\"\n\n#############################################\n# Batch size\n#############################################\nTRAIN_BATCH_SIZE = \"train_batch_size\"\nTRAIN_BATCH_SIZE_DEFAULT = None\n\n#############################################\n# Sparse attention\n#############################################\nSPARSE_ATTENTION = \"sparse_attention\"\nSPARSE_DENSE_MODE = \"dense\"\nSPARSE_FIXED_MODE = \"fixed\"\nSPARSE_VARIABLE_MODE = \"variable\"\nSPARSE_BIGBIRD_MODE = \"bigbird\"\nSPARSE_BSLONGFORMER_MODE = \"bslongformer\"\nSPARSE_MODE = \"mode\"\nSPARSE_MODE_DEFAULT = SPARSE_FIXED_MODE\nSPARSE_BLOCK = \"block\"\nSPARSE_BLOCK_DEFAULT = 16\nSPARSE_DIFFERENT_LAYOUT_PER_HEAD = \"different_layout_per_head\"\nSPARSE_DIFFERENT_LAYOUT_PER_HEAD_DEFAULT = False\nSPARSE_NUM_LOCAL_BLOCKS = \"num_local_blocks\"\nSPARSE_NUM_LOCAL_BLOCKS_DEFAULT = 4\nSPARSE_NUM_GLOBAL_BLOCKS = \"num_global_blocks\"\nSPARSE_NUM_GLOBAL_BLOCKS_DEFAULT = 1\nSPARSE_ATTENTION_TYPE = \"attention\"\nSPARSE_ATTENTION_TYPE_DEFAULT = \"bidirectional\"\nSPARSE_HORIZONTAL_GLOBAL_ATTENTION = \"horizontal_global_attention\"\nSPARSE_HORIZONTAL_GLOBAL_ATTENTION_DEFAULT = False\nSPARSE_NUM_DIFFERENT_GLOBAL_PATTERNS = \"num_different_global_patterns\"\nSPARSE_NUM_DIFFERENT_GLOBAL_PATTERNS_DEFAULT = 1\nSPARSE_NUM_RANDOM_BLOCKS = \"num_random_blocks\"\nSPARSE_NUM_RANDOM_BLOCKS_DEFAULT = 0\nSPARSE_LOCAL_WINDOW_BLOCKS = \"local_window_blocks\"\nSPARSE_LOCAL_WINDOW_BLOCKS_DEFAULT = [4]\nSPARSE_GLOBAL_BLOCK_INDICES = \"global_block_indices\"\nSPARSE_GLOBAL_BLOCK_INDICES_DEFAULT = [0]\nSPARSE_GLOBAL_BLOCK_END_INDICES = \"global_block_end_indices\"\nSPARSE_GLOBAL_BLOCK_END_INDICES_DEFAULT = None\nSPARSE_NUM_SLIDING_WINDOW_BLOCKS = \"num_sliding_window_blocks\"\nSPARSE_NUM_SLIDING_WINDOW_BLOCKS_DEFAULT = 3\n\n#############################################\n# Optimizer and lr scheduler\n#############################################\nOPTIMIZER = \"optimizer\"\nOPTIMIZER_TYPE_DEFAULT = None\nOPTIMIZER_PARAMS = \"params\"\nTYPE = \"type\"\nLEGACY_FUSION = \"legacy_fusion\"\nLEGACY_FUSION_DEFAULT = False\nSCHEDULER = \"scheduler\"\nSCHEDULER_TYPE_DEFAULT = None\nSCHEDULER_PARAMS = \"params\"\nMAX_GRAD_NORM = 'max_grad_norm'\n\n#############################################\n# Optimizer and lr scheduler\n#############################################\nZERO_ALLOW_UNTESTED_OPTIMIZER = \"zero_allow_untested_optimizer\"\nZERO_ALLOW_UNTESTED_OPTIMIZER_DEFAULT = False\nZERO_FORCE_DS_CPU_OPTIMIZER = \"zero_force_ds_cpu_optimizer\"\nZERO_FORCE_DS_CPU_OPTIMIZER_DEFAULT = True\n\n# Steps\nSTEPS_PER_PRINT = \"steps_per_print\"\nSTEPS_PER_PRINT_DEFAULT = 10\n\n#########################################\n# Training micro batch size per GPU\n#########################################\n# Batch size for one training step. This is used when the\n# TRAIN_BATCH_SIZE cannot fit in GPU memory to determine\n# the number of gradient accumulation steps. By default, this\n# is set to None. Users can configure in ds_config.json as below example:\nTRAIN_MICRO_BATCH_SIZE_PER_GPU = '''\nTRAIN_MICRO_BATCH_SIZE_PER_GPU is defined in this format:\n\"train_micro_batch_size_per_gpu\": 1\n'''\nTRAIN_MICRO_BATCH_SIZE_PER_GPU = \"train_micro_batch_size_per_gpu\"\nTRAIN_MICRO_BATCH_SIZE_PER_GPU_DEFAULT = None\n\n#########################################\n# Gradient Accumulation\n#########################################\n# Gradient accumulation feature. By default, this feature is not enabled.\n# Users can configure in ds_config.json as below example:\nGRADIENT_ACCUMULATION_FORMAT = '''\nGradient Accumulation should be of the format:\n\"gradient_accumulation_steps\": 1\n'''\nGRADIENT_ACCUMULATION_STEPS = \"gradient_accumulation_steps\"\nGRADIENT_ACCUMULATION_STEPS_DEFAULT = None\n\n# DeepSpeed CSR gradient sparsity\nSPARSE_GRADIENTS = \"sparse_gradients\"\nSPARSE_GRADIENTS_DEFAULT = False\n\n#########################################\n# BFLOAT16 support\n#########################################\n# BFLOAT16 feature. By default, this feature is not enabled.\n# Users can configure in ds_config.json as below example:\nBFLOAT16_FORMAT = '''\nBFLOAT16 parameters should be of the format:\n\"bf16\": {\n  \"enabled\": true\n}\n'''\nBFLOAT16 = \"bf16\"\nBFLOAT16_OLD = \"bfloat16\"  # keeping for backwards compatibility\n\nBFLOAT16_ENABLED = \"enabled\"\nBFLOAT16_ENABLED_DEFAULT = False\n\n# BFLOAT16 optimizer immediate gradient update\nBFLOAT16_IMMEDIATE_GRAD_UPDATE = \"immediate_grad_update\"\nBFLOAT16_IMMEDIATE_GRAD_UPDATE_DEFAULT = False\n\n#########################################\n# FP16 support\n#########################################\n# FP16 feature. By default, this feature is not enabled.\n# Users can configure in ds_config.json as below example:\nFP16_FORMAT = '''\nFP16 parameters should be of the format:\n\"fp16\": {\n  \"enabled\": true,\n  \"auto_cast\": false,\n  \"loss_scale\": 0,\n  \"initial_scale_power\": 16,\n  \"loss_scale_window\": 1000,\n  \"hysteresis\": 2,\n  \"consecutive_hysteresis\": false,\n  \"min_loss_scale\": 1\n}\n'''\nFP16 = \"fp16\"\n\nFP16_ENABLED = \"enabled\"\nFP16_ENABLED_DEFAULT = False\n\n# FP16 loss scale, zero means using dynamic scaling\nFP16_LOSS_SCALE = \"loss_scale\"\nFP16_LOSS_SCALE_DEFAULT = 0\n\nFP16_AUTO_CAST = \"auto_cast\"\nFP16_AUTO_CAST_DEFAULT = False\n\n# FP16 initial dynamic scale loss power\nFP16_INITIAL_SCALE_POWER = \"initial_scale_power\"\nFP16_INITIAL_SCALE_POWER_DEFAULT = 16\n\n# FP16 loss scale window\nFP16_LOSS_SCALE_WINDOW = \"loss_scale_window\"\nFP16_LOSS_SCALE_WINDOW_DEFAULT = 1000\n\n# FP16 hysteresis\nFP16_HYSTERESIS = \"hysteresis\"\nFP16_HYSTERESIS_DEFAULT = 2\n\n# FP16 consecutive hysteresis\nFP16_CONSECUTIVE_HYSTERESIS = \"consecutive_hysteresis\"\nFP16_CONSECUTIVE_HYSTERESIS_DEFAULT = False\n\n# FP16 min loss scale\nFP16_MIN_LOSS_SCALE = \"min_loss_scale\"\nFP16_MIN_LOSS_SCALE_DEFAULT = 1\n\n# FP16 master and grads\nFP16_MASTER_WEIGHTS_AND_GRADS = \"fp16_master_weights_and_grads\"\nFP16_MASTER_WEIGHTS_AND_GRADS_DEFAULT = False\n\n#########################################\n# Apex AMP support\n#########################################\n# Use Apex AMP for mixed precision support, all parameters (other than 'enabled') will be passed to\n# amp.initialize(model, optimizer, **amp_params)\n# See apex documentation for supported parameters/features: https://nvidia.github.io/apex/amp.html#apex.amp.initialize\nAMP_FORMAT = '''\n\"amp\" {\n  \"enabled: true,\n  \"opt_level\": \"O1\",\n  ...\n}\n'''\nAMP = \"amp\"\n\nAMP_ENABLED = \"enabled\"\nAMP_ENABLED_DEFAULT = False\n\n#########################################\n# Gradient clipping\n#########################################\n# Gradient clipping. By default, this feature is not enabled.\n# Users can configure in ds_config.json as below example:\nGRADIENT_CLIPPING_FORMAT = '''\nGradient clipping should be enabled as:\n\"gradient_clipping\": 1.0\n'''\nGRADIENT_CLIPPING = 'gradient_clipping'\nGRADIENT_CLIPPING_DEFAULT = 0.\n\n#########################################\n# Capture graph for short kernels sequences\n#########################################\n# Graph harvesting. By default, this feature is not enabled.\n# Users can configure in ds_config.json as below example:\nGRAPH_HARVESTING_FORMAT = '''\nGraph harvesting should be enabled as:\n\"graph_harvesting\": true\n'''\nGRAPH_HARVESTING = 'graph_harvesting'\nGRAPH_HARVESTING_DEFAULT = False\n\n#########################################\n# Communication data type\n#########################################\n# Supported types: ['none', 'fp16', 'fp32']\n# By default, this feature is not enabled ('none' value)\n# Users can configure in ds_config.json as below example:\nCOMMUNICATION_DATA_TYPE_FORMAT = '''\nCommunication data type should be set as:\n\"communication_data_type\": \"fp32\"\n'''\nCOMMUNICATION_DATA_TYPE = \"communication_data_type\"\nCOMMUNICATION_DATA_TYPE_DEFAULT = None\n\n###########################################################\n# Gradient communication data type for sequence parallelism\n###########################################################\n# Supported types: ['fp16', 'bf16','fp32']\n# Default value is fp32\n# Users can configure in ds_config.json as below example:\nSEQ_PARALLEL_COMMUNICATION_DATA_TYPE_FORMAT = '''\nOptional comm data type for seq paralleism should be set as:\n\"seq_parallel_communication_data_type\": \"fp32\"\n'''\nSEQ_PARALLEL_COMMUNICATION_DATA_TYPE = \"seq_parallel_comm_data_type\"\nSEQ_PARALLEL_COMMUNICATION_DATA_TYPE_DEFAULT = \"fp32\"\n\n#########################################\n# Scale/predivide gradients before allreduce\n#########################################\n# Prescale gradients. By default, this feature is not enabled.\n# Users can configure in ds_config.json as below example:\nPRESCALE_GRADIENTS_FORMAT = '''\nGradient prescaling should be enabled as:\n\"prescale_gradients\": true\n'''\nPRESCALE_GRADIENTS = \"prescale_gradients\"\nPRESCALE_GRADIENTS_DEFAULT = False\n\nGRADIENT_PREDIVIDE_FACTOR_FORMAT = '''\nGradient predivide factor should be enabled as:\n\"gradient_predivide_factor\": 1.0\n'''\nGRADIENT_PREDIVIDE_FACTOR = \"gradient_predivide_factor\"\nGRADIENT_PREDIVIDE_FACTOR_DEFAULT = 1.0\n\n#########################################\n# Disable AllGather\n#########################################\n# Disable AllGather. By default, this feature is not enabled.\n# Users can configure in ds_config.json as below example:\nDISABLE_ALLGATHER_FORMAT = '''\nDisable AllGather should be enabled as:\n\"disable_allgather\": true\n'''\nDISABLE_ALLGATHER = \"disable_allgather\"\nDISABLE_ALLGATHER_DEFAULT = False\n\n#########################################\n# Dump DeepSpeed state\n#########################################\n# Dump State. By default, this feature is not enabled.\n# Users can configure in ds_config.json as below example:\nDUMP_STATE_FORMAT = '''\nDump state should be enabled as:\n\"dump_state\": true\n'''\nDUMP_STATE = 'dump_state'\nDUMP_STATE_DEFAULT = False\n\n#########################################\n# Vocabulary size\n#########################################\n# Vocabulary size.\n# Users can configure in ds_config.json as below example:\nVOCABULARY_SIZE_FORMAT = '''\nVocabulary size can be specified as:\n\"vocabulary_size\": 1024\n'''\nVOCABULARY_SIZE = 'vocabulary_size'\nVOCABULARY_SIZE_DEFAULT = None\n\n#########################################\n# Wall block breakdown\n#########################################\n# Wall clock breakdown. By default, this feature is not enabled.\n# Users can configure in ds_config.json as below example:\nWALL_CLOCK_BREAKDOWN_FORMAT = '''\nWall block breakdown should be enabled as:\n\"wall_clock_breakdown\": true\n'''\nWALL_CLOCK_BREAKDOWN = 'wall_clock_breakdown'\nWALL_CLOCK_BREAKDOWN_DEFAULT = False\n\nMEMORY_BREAKDOWN = 'memory_breakdown'\nMEMORY_BREAKDOWN_DEFAULT = False\n\n#########################################\n# Eigenvalue\n#########################################\n# Eigenvalue computation. By default, this feature is not enabled.\n# Users can configure in ds_config.json as below example:\nEIGENVALUE_FORMAT = '''\nTensorboard can be specified as:\n\"eigenvalue\": {\n  \"enabled\": true,\n  \"verbose\": true,\n  \"max_iter\": 100,\n  \"tol\": 1e-2,\n  \"stability\": 1e-6\n}\n'''\nEIGENVALUE = \"eigenvalue\"\n\n# Tensorboard enable signal\nEIGENVALUE_ENABLED = \"enabled\"\nEIGENVALUE_ENABLED_DEFAULT = False\n\nEIGENVALUE_VERBOSE = \"verbose\"\nEIGENVALUE_VERBOSE_DEFAULT = False\n\nEIGENVALUE_MAX_ITER = \"max_iter\"\nEIGENVALUE_MAX_ITER_DEFAULT = 100\n\nEIGENVALUE_TOL = \"tol\"\nEIGENVALUE_TOL_DEFAULT = 1e-2\n\nEIGENVALUE_STABILITY = \"stability\"\nEIGENVALUE_STABILITY_DEFAULT = 1e-6\n\nEIGENVALUE_GAS_BOUNDARY_RESOLUTION = \"gas_boundary_resolution\"\nEIGENVALUE_GAS_BOUNDARY_RESOLUTION_DEFAULT = 1\n\nEIGENVALUE_LAYER_NAME = \"layer_name\"\nEIGENVALUE_LAYER_NAME_DEFAULT = \"bert.encoder.layer\"\n\nEIGENVALUE_LAYER_NUM = \"layer_num\"\nEIGENVALUE_LAYER_NUM_DEFAULT = 0\n\n#########################################\n# Progressive Layer Drop (PLD)\n#########################################\nPROGRESSIVE_LAYER_DROP = \"progressive_layer_drop\"\n\n# PLD enable signal\nPLD_ENABLED = \"enabled\"\nPLD_ENABLED_DEFAULT = False\n\nPLD_THETA = \"theta\"\nPLD_THETA_DEFAULT = 1.0\n\nPLD_GAMMA = \"gamma\"\nPLD_GAMMA_DEFAULT = 0.001\n\n\n#########################################\n# Validation modes\n#########################################\nclass ValidationMode:\n    WARN = \"WARN\"\n    IGNORE = \"IGNORE\"\n    FAIL = \"FAIL\"\n\n\n#########################################\n# Checkpoint config params\n#########################################\n# \"checkpoint\": {\n#   tag_validation=[\"Ignore\"|\"Warn\"|\"Fail\"]\n#   load_universal=false\n#   use_node_local_storage=false\n#   parallel_write: {\n#     pipeline_stage: [True|False]\n#   }\n# }\nCHECKPOINT = \"checkpoint\"\nCHECKPOINT_TAG_VALIDATION = \"tag_validation\"\nCHECKPOINT_TAG_VALIDATION_DEFAULT = ValidationMode.WARN\nCHECKPOINT_TAG_VALIDATION_MODES = [ValidationMode.WARN, ValidationMode.IGNORE, ValidationMode.FAIL]\n\nLOAD_UNIVERSAL_CHECKPOINT = \"load_universal\"\nLOAD_UNIVERSAL_CHECKPOINT_DEFAULT = False\n\nUSE_NODE_LOCAL_STORAGE_CHECKPOINT = \"use_node_local_storage\"\nUSE_NODE_LOCAL_STORAGE_CHECKPOINT_DEFAULT = False\n\nCHECKPOINT_PARALLEL_WRITE = \"parallel_write\"\nCHECKPOINT_PARALLEL_WRITE_PIPELINE_STAGE = \"pipeline_stage\"\nCHECKPOINT_PARALLEL_WRITE_PIPELINE_STAGE_DEFAULT = False\n\n#########################################\n# Data types config params\n#########################################\n# \"data_types\": {\n#   grad_accum_dtype=[\"bf16\"|\"fp16\"|\"fp32\"]\n#   }\n# }\n\nDATA_TYPES = \"data_types\"\nGRAD_ACCUM_DTYPE = \"grad_accum_dtype\"\nGRAD_ACCUM_DTYPE_DEFAULT = None\n\n#########################################\n# Drop the last incomplete Batch\n# #########################################\n# dataloader_drop_last. By default, this feature is not enabled.\n# Users can configure in ds_config.json as below example:\nDATALOADER_DROP_LAST_FORMAT = '''\nThe last incomplete batch can be dropped by setting:\n\"dataloader_drop_last\": True\n'''\nDATALOADER_DROP_LAST = \"dataloader_drop_last\"\nDATALOADER_DROP_LAST_DEFAULT = False\n\n#########################################\n# PIPELINE PARALLELISM\n#########################################\nPIPE_REPLICATED = 'ds_pipe_replicated'\n\n#########################################\n# DATA PARALLELISM\n#########################################\nDATA_PARALLEL_GROUP = \"data_parallel_group\"\nGLOBAL_RANK = \"global_rank\"\n\n#########################################\n# EXPERT-DATA PARALLELISM TOPO Config\n#########################################\nUSE_DATA_BEFORE_EXPERT_PARALLEL = \"use_data_before_expert_parallelism\"\nUSE_DATA_BEFORE_EXPERT_PARALLEL_DEFAULT = False\n", "deepspeed/runtime/hybrid_engine.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nfrom deepspeed.inference.config import DeepSpeedInferenceConfig\nfrom deepspeed.module_inject.replace_policy import replace_policies\nfrom deepspeed.module_inject.utils import policy_to_ds_container\nfrom .engine import DeepSpeedEngine\nfrom .utils import TLinear, get_inactive_params\nfrom deepspeed.runtime.zero import GatheredParameters\nimport time\nimport gc\nimport math\nfrom deepspeed import comm as dist\nfrom deepspeed.accelerator import get_accelerator\nfrom torch import nn\nfrom deepspeed.utils import logger\n\nfrom deepspeed.ops.op_builder import InferenceBuilder\n\nfrom deepspeed.module_inject.layers import LinearLayer, Normalize, EmbeddingLayer, OPTEmbedding\ntry:\n    import transformers\n    OPTLearnedPositionalEmbedding = transformers.models.opt.modeling_opt.OPTLearnedPositionalEmbedding\nexcept:\n    OPTLearnedPositionalEmbedding = None\ninference_cuda_module = None\n\n\nclass DeepSpeedHybridEngine(DeepSpeedEngine):\n    r\"\"\"DeepSpeed engine for training and inference.\"\"\"\n    inference_mp_group = None\n\n    def __init__(self, args, model, **kwargs):\n\n        super().__init__(args, model, **kwargs)\n\n        # synch seed between all GPUs\n        _rng_state = get_accelerator().get_rng_state().to(get_accelerator().current_device_name())\n        dist.broadcast(_rng_state, 0)\n        get_accelerator().set_rng_state(_rng_state.cpu())\n\n        self.Z3_enabled = (self._config.zero_config.stage == 3)\n        self.gather_all_layers = self._config.hybrid_engine.pin_parameters\n\n        # inference containers / fwds\n        self._inference_containers = []\n        self._orig_modules = []\n        self._orig_fwds = []\n        self.create_inference_module()\n\n        # Performance stats\n        self._t_start = None\n        self._total_latency = 0\n        self._iters = 0\n        self._training_start_time = None\n        self._generate_latency = 0\n        self._training_latency = 0\n        self._total_batch_size = None\n        self._gather_latency = 0\n\n        global inference_cuda_module\n        if inference_cuda_module is None:\n            builder = InferenceBuilder()\n            inference_cuda_module = builder.load()\n\n        self.is_lora_fused = False\n\n    def convert_to_linear_transposed(self, model):\n\n        def _replace_linear_layer(r_module, parent_type=None, prev_type=None):\n            for name, child in r_module.named_children():\n                if child.__class__ in [torch.nn.Linear] and \\\n                    (parent_type is torch.nn.ModuleList or prev_type is torch.nn.ModuleList):\n                    setattr(r_module, name, TLinear(child, name))\n                else:\n                    _replace_linear_layer(child, type(r_module), prev_type=parent_type)\n            return r_module\n\n        _replace_linear_layer(model)\n\n    def new_inference_container(self, orig_layer, policy_cls, layer_id):\n        policy = policy_cls(orig_layer, inference=True)\n\n        if self._config.fp16_enabled:\n            inference_dtype = torch.float16\n        elif self._config.bfloat16_enabled:\n            inference_dtype = torch.bfloat16\n        else:\n            inference_dtype = torch.float32\n\n        _container = policy_to_ds_container(\n            policy=policy,\n            config=DeepSpeedInferenceConfig(\n                set_empty_params=True,\n                dtype=inference_dtype,\n                max_out_tokens=self._config.hybrid_engine.max_out_tokens,\n                min_out_tokens=self._config.hybrid_engine.max_out_tokens,\n                transposed_mode=True,\n            ),\n            model_config=self.module.config if hasattr(self.module, 'config') else None,\n            layer_id=layer_id,\n            child=orig_layer)\n\n        if self.mpu is not None:\n            if hasattr(self.mpu, 'get_model_parallel_world_size'):\n                _container.set_tensor_parallel_config(self.mpu.get_model_parallel_world_size(),\n                                                      self.mpu.get_model_parallel_group())\n            else:\n                _container.set_tensor_parallel_config(self.mpu.get_tensor_model_parallel_world_size(),\n                                                      self.mpu.get_tensor_model_parallel_group())\n        else:\n            _container.set_tensor_parallel_config(self._config.hybrid_engine.inference_tp_size, self.mp_group)\n        _container.initialize_tensors(enable_training=True)\n        _container.create_ds_model_config()\n        _container.create_module()\n        _container.set_params_wo_copy(Z3_enabled=self.Z3_enabled)\n        return _container\n\n    def populate_all_inference_policies(self):\n        self.inference_policies = {}\n        for plcy in replace_policies:\n            _ = plcy(None)\n            if isinstance(plcy._orig_layer_class, list):\n                for orig_layer_class in plcy._orig_layer_class:\n                    self.inference_policies.update({orig_layer_class: (self.new_inference_container, plcy)})\n            elif plcy._orig_layer_class is not None:\n                self.inference_policies.update({plcy._orig_layer_class: (self.new_inference_container, plcy)})\n        self.inference_policies.update({\n            nn.Linear: (LinearLayer, ),\n            nn.Embedding: (EmbeddingLayer, ),\n            nn.LayerNorm: (Normalize, ),\n            OPTLearnedPositionalEmbedding: (OPTEmbedding, )\n        })\n\n    def _fuse_lora_layer(self, layer_id):\n        self._inference_containers[layer_id].fuse_lora()\n\n    def fuse_lora_weight(self):\n        for layer_id in range(len(self.layer_params)):\n            self._fuse_lora_layer(layer_id)\n\n    def _unfuse_lora_layer(self, layer_id):\n        self._inference_containers[layer_id].unfuse_lora()\n\n    def unfuse_lora_weight(self):\n        for layer_id in range(len(self.layer_params)):\n            self._unfuse_lora_layer(layer_id)\n\n    def unfuse_lora_weight_non_pinned(self):\n        for layer_id in range(len(self.layer_params)):\n            non_active_params = get_inactive_params(self.layer_params[layer_id])\n            non_active_lora_params = get_inactive_params(self.layer_lora_params[layer_id])\n            non_active_params.extend(non_active_lora_params)\n\n            with GatheredParameters(non_active_params):\n                self._unfuse_lora_layer(layer_id)\n\n    def retake_inference_cache(self):\n        if self._config.hybrid_engine.release_inference_cache:\n            retake_success = inference_cuda_module.retake_workspace()\n\n            if not retake_success:\n                logger.warning(\"Unable to acquire workspace on first attempt, emptying cache and retrying.\")\n                gc.collect()\n                get_accelerator().empty_cache()\n                retake_success = inference_cuda_module.retake_workspace()\n\n                if not retake_success:\n                    raise RuntimeError(\"Unable to retake inference workspace.\")\n\n    def generate(self, *inputs, **kwargs):\n        if self._total_batch_size is None:\n            bsz = inputs[0].shape[0] if len(inputs) > 0 else \\\n                kwargs['input_ids'].shape[0]\n            self._total_batch_size = bsz * dist.get_world_size()\n\n        self._t0 = time.time()\n\n        if self.Z3_enabled and self.gather_all_layers:\n            if self._config.hybrid_engine.inference_tp_size > 1:\n                non_tp_params = []\n                for other_layer in self._other_layers:\n                    non_tp_params.extend(list(other_layer.parameters()))\n\n                partition_size = self._config.hybrid_engine.tp_gather_partition_size\n\n                layer_groups = math.ceil(len(self.layer_params) / partition_size)\n                for lg in range(layer_groups):\n                    non_active_params = []\n                    non_active_lora_params = []\n                    for layer_id in range(lg * partition_size, min(len(self.layer_params), (lg + 1) * partition_size),\n                                          1):\n                        non_tp_params.extend(self.layer_params[layer_id][:4])\n                        non_active_params.extend(get_inactive_params(self.layer_params[layer_id]))\n                        non_active_params.extend(get_inactive_params(self.layer_lora_params[layer_id]))\n                    with GatheredParameters(non_active_params):\n                        for layer_id in range(lg * partition_size,\n                                              min(len(self.layer_params), (lg + 1) * partition_size), 1):\n                            if len(self.all_lora_params) > 0:\n                                self._fuse_lora_layer(layer_id)\n\n                            if self.mpu is not None:\n                                self._inference_containers[layer_id].apply_tensor_parallelism(self.mp_replace,\n                                                                                              reversed_dim=True)\n\n                # TODO(cmikeh2) Evaluate if this can be deferred when release_inference_cache\n                # is enabled.\n                gc.collect()\n                get_accelerator().empty_cache()\n\n                self._gather_latency = time.time() - self._t0\n\n                input_shape = inputs[0].shape if len(inputs) > 0 else \\\n                                kwargs['input_ids'].shape\n                output = torch.zeros(\n                    (input_shape[0] * self._config.hybrid_engine.inference_tp_size, ) + input_shape[1:],\n                    dtype=inputs[0].dtype if len(inputs) > 0 else kwargs['input_ids'].dtype,\n                    device=inputs[0].device if len(inputs) > 0 else kwargs['input_ids'].device)\n                input_cont = inputs[0].contiguous() if len(inputs) > 0 else kwargs['input_ids'].contiguous()\n                dist.all_gather_into_tensor(output, input_cont, group=self.mp_group)\n\n                if len(inputs) > 0:\n                    inputs = (output, *inputs[1:])\n                else:\n                    kwargs['input_ids'] = output\n\n                self.retake_inference_cache()\n\n                non_active_params = get_inactive_params(non_tp_params)\n                with GatheredParameters(non_active_params):\n                    generate_ret_vals = self._generate(*inputs, **kwargs)\n\n                for layer_id in range(len(self.layer_params)):\n                    self._inference_containers[layer_id].release_memory()\n\n                rank = dist.get_rank(group=self.mp_group)\n                generate_ret_vals = generate_ret_vals[input_shape[0] * rank:input_shape[0] * (rank + 1)]\n\n            else:\n                non_active_layers = get_inactive_params(self.all_layers_params)\n                non_active_lora_params = get_inactive_params(self.all_lora_params)\n                non_active_layers.extend(non_active_lora_params)\n                with GatheredParameters(non_active_layers):\n                    self._gather_latency = time.time() - self._t0\n\n                    if len(self.all_lora_params) > 0:\n                        self.fuse_lora_weight()\n\n                    self.retake_inference_cache()\n                    generate_ret_vals = self._generate(*inputs, **kwargs)\n\n                    if len(self.all_lora_params) > 0:\n                        self.unfuse_lora_weight()\n        else:\n            if len(self.all_lora_params) > 0 and (not self.Z3_enabled):\n                self.fuse_lora_weight()\n\n            self.retake_inference_cache()\n            generate_ret_vals = self._generate(*inputs, **kwargs)\n\n            if len(self.all_lora_params) > 0:\n                if (not self.Z3_enabled):\n                    self.unfuse_lora_weight()\n                else:\n                    self.unfuse_lora_weight_non_pinned()\n                self.is_lora_fused = False\n\n        if self._config.hybrid_engine.release_inference_cache:\n            inference_cuda_module.release_workspace()\n            gc.collect()\n            get_accelerator().empty_cache()\n\n        self._generate_latency = time.time() - self._t0 - self._gather_latency\n\n        return generate_ret_vals\n\n    def create_inference_containers(self, module, layer_id=0):\n        for name, child in module.named_children():\n            if child.__class__ in self.inference_policies:\n                if self.inference_policies[child.__class__][0] == self.new_inference_container:\n                    self._inference_containers.append(self.inference_policies[child.__class__][0](\n                        child, self.inference_policies[child.__class__][-1], layer_id))\n                    self._orig_modules.append(child)\n                    self._orig_fwds.append(child.forward)\n\n                    self.layer_params.append(self._inference_containers[layer_id].get_all_params())\n\n                    self.lora_params.append(self._inference_containers[layer_id].get_lora_params())\n                    self.layer_lora_params.append([])\n                    for lora_param in self.lora_params[layer_id]:\n                        self.layer_lora_params[layer_id].extend(lora_param[:-1])\n                        self.all_lora_params.extend(lora_param[:-1])\n\n                    layer_id += 1\n                else:\n                    self._other_layers.append(self.inference_policies[child.__class__][0](\n                        weight=child.weight, bias=child.bias if hasattr(child, 'bias') else None))\n                    self._orig_modules_others.append(child)\n                    self._orig_fwds_others.append(child.forward)\n            else:\n                self.create_inference_containers(child, layer_id=layer_id)\n\n    def create_inference_module(self):\n        self.layer_params = []\n        self.layer_lora_params = []\n        self.lora_params = []\n        self.all_lora_params = []\n\n        self._other_layers = []\n        self._orig_modules_others = []\n        self._orig_fwds_others = []\n\n        if self._config.hybrid_engine.inference_tp_size > 1:\n            if self.mpu is None:\n                global_rank = dist.get_rank()\n                world_size = dist.get_world_size()\n                mp_group_id = global_rank // self._config.hybrid_engine.inference_tp_size\n                num_mp_groups = world_size // self._config.hybrid_engine.inference_tp_size\n                for mp_group_id in range(num_mp_groups):\n                    ranks = list(\n                        range(mp_group_id * self._config.hybrid_engine.inference_tp_size, \\\n                            (mp_group_id + 1) * self._config.hybrid_engine.inference_tp_size, \\\n                            1)\n                    )\n                    mp_group = dist.new_group(ranks)\n                    if global_rank in ranks:\n                        # mp_group is used for broader collective\n                        self.mp_group = mp_group\n\n                        # mp_replace is used for container tensor slicing\n                        from deepspeed.module_inject import ReplaceWithTensorSlicing\n                        self.mp_replace = ReplaceWithTensorSlicing(\n                            mp_group=self.mp_group,\n                            mp_size=self._config.hybrid_engine.inference_tp_size,\n                            out_dim=0,\n                            in_dim=1)\n\n            else:\n                self.mp_group = self.mpu.get_model_parallel_group() if hasattr(self.mpu, 'get_model_parallel_group') else \\\n                    self.mpu.get_tensor_model_parallel_group()\n\n                from deepspeed.module_inject import ReplaceWithTensorSlicing\n                self.mp_replace = ReplaceWithTensorSlicing(mp_group=self.mp_group,\n                                                           mp_size=self._config.hybrid_engine.inference_tp_size,\n                                                           out_dim=0,\n                                                           in_dim=1)\n        else:\n            self.mp_group = None\n            self.mp_replace = None\n        self.populate_all_inference_policies()\n        self.all_layers_params = list(self.module.parameters())\n        self.create_inference_containers(self.module)\n\n        if len(self._inference_containers) > 0:\n            self._generate = self.module.generate\n            self.module.generate = self.generate\n\n        self._t0 = time.time()\n\n    def _zero3_forward(self, layer_id):\n\n        def run_forward(*inputs, **kwargs):\n            non_active_params = get_inactive_params(self.layer_params[layer_id])\n            non_active_lora_params = get_inactive_params(self.layer_lora_params[layer_id])\n            non_active_params.extend(non_active_lora_params)\n\n            with GatheredParameters(non_active_params):\n                if len(self.all_lora_params) > 0:\n                    # Use the is_lora_fused flag to prevent multiple fusion in Z3 with non-pinned memory\n                    if not self.is_lora_fused:\n                        self._fuse_lora_layer(layer_id)\n                    # Set the is_lora_fused to true when reaching the last layer\n                    if layer_id == len(self.layer_params) - 1:\n                        self.is_lora_fused = True\n                return self._inference_containers[layer_id].module.forward(*inputs, **kwargs)\n\n        return run_forward\n\n    def eval(self):\n        if self._t_start is not None:\n            latency = time.time() - self._t_start\n            self._total_latency = self._total_latency + latency\n            self._iters = self._iters + 1\n            if not dist.is_initialized() or dist.get_rank() == 0:\n                if self._total_batch_size is not None:\n                    cur_samples_p_sec = f'|CurSamplesPerSec={(1 / latency * self._total_batch_size):.2f} '\n                    avg_samples_p_sec = f'|AvgSamplesPerSec={(1 / (self._total_latency / self._iters) * self._total_batch_size):.2f}'\n                else:\n                    cur_samples_p_sec = ''\n                    avg_samples_p_sec = ''\n                others = latency - (self._generate_latency + self._training_latency)\n                print(f'|E2E latency={(latency):.2f}s ' + \\\n                      f'|Gather latency={self._gather_latency:.2f}s ({(self._gather_latency / latency * 100):.2f}%) '\n                      f'|Generate time={(self._generate_latency):.2f}s ({(self._generate_latency / latency * 100):.2f}%) ' + \\\n                      f'|Training time={(self._training_latency):.2f}s ({(self._training_latency / latency * 100):.2f}%) ' + \\\n                      f'|Others={others:.2f} ({(others / latency * 100):.2f}%)' + \\\n                      cur_samples_p_sec + \\\n                      avg_samples_p_sec)\n            self._t_start = time.time()\n        self._training_latency = 0\n        super().eval()\n        if len(self._inference_containers) > 0:\n            for i, (orig_module, inference_container) in enumerate(zip(self._orig_modules,\n                                                                       self._inference_containers)):\n                if self.Z3_enabled and not self.gather_all_layers:\n                    orig_module.forward = self._zero3_forward(i)\n                else:\n                    orig_module.forward = inference_container.module.forward\n\n                inference_container.transform_for_inference()\n\n            if not self.Z3_enabled or self.gather_all_layers:\n                for orig_module, inference_layer in zip(self._orig_modules_others, self._other_layers):\n                    orig_module.forward = inference_layer.forward\n        if self.Z3_enabled:\n            gc.collect()\n            get_accelerator().empty_cache()\n        if self._t_start is None:\n            self._t_start = time.time()\n\n    def train(self, mode=True):\n        if mode and len(self._orig_modules) > 0:\n            for inference_container, orig_module, orig_fwd in zip(self._inference_containers, self._orig_modules,\n                                                                  self._orig_fwds):\n                inference_container.transform_for_training()\n                orig_module.forward = orig_fwd\n            for orig_module, orig_fwd in zip(self._orig_modules_others, self._orig_fwds_others):\n                orig_module.forward = orig_fwd\n        super().train(mode)\n        if mode:\n            self._training_start_time = time.time()\n\n    def step(self, lr_kwargs=None):\n        super().step(lr_kwargs=lr_kwargs)\n\n        if len(self._inference_containers) > 0:\n            if not self.Z3_enabled:\n                for inference_container in self._inference_containers:\n                    inference_container.reset_params()\n\n        if self._training_start_time is not None:\n            self._training_latency += (time.time() - self._training_start_time)\n            self._training_start_time = time.time()\n", "deepspeed/runtime/progressive_layer_drop.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport numpy as np\nfrom deepspeed.utils import log_dist\n\n\nclass ProgressiveLayerDrop(object):\n    r\"\"\" Progressive Layer Dropping (PLD) for model training.\n        This implements the PLD technique for compressed model training\n        from this paper: https://arxiv.org/pdf/2010.13369.pdf\n    Args:\n        theta (float): a hyper-parameter that controls the trade-off between training time and robustness.\n        The lower the theta value, the faster the training speed. Default value: 0.5.\n        gamma (float): a hyper-parameter that controls how fast the drop ratio increases. Default value: 0.001.\n    \"\"\"\n\n    def __init__(self, theta=0.5, gamma=0.001):\n        super().__init__()\n\n        self.theta = theta\n        self.gamma = gamma\n        self.current_theta = 1.0\n        log_dist(f'Enabled progressive layer dropping (theta = {self.theta})', ranks=[0])\n\n    def get_state(self):\n        kwargs = {'progressive_layer_drop': True, 'pld_theta': self.get_theta()}\n        return kwargs\n\n    def get_theta(self):\n        return self.current_theta\n\n    def update_state(self, global_step):\n\n        def _prob(x, gamma, p):\n            return (1. - p) * np.exp(-gamma * x) + p\n\n        self.current_theta = _prob(global_step, self.gamma, self.theta)\n", "deepspeed/runtime/state_dict_factory.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nimport os\nimport copy\nimport collections\nimport json\nfrom abc import ABC, abstractmethod\n\nfrom deepspeed.utils import logger\nfrom deepspeed.runtime.checkpoint_engine.torch_checkpoint_engine import TorchCheckpointEngine\n\nfrom .weight_quantizer import WeightQuantization\n\nAUTO_MODULE_KEY = 'auto'\n\n\nclass SDLoaderFactory:\n\n    @staticmethod\n    def get_sd_loader_json(json_file, checkpoint_engine):\n        if isinstance(json_file, str):\n            with open(json_file) as f:\n                data = json.load(f)\n        else:\n            assert isinstance(json_file, dict)\n            data = json_file\n        sd_type = data['type']\n        ckpt_list = data['checkpoints']\n        version = data['version']\n        ckpt_type = data.get('parallelization', 'pp')\n        mp_size = data.get('mp_size', 0)\n        if sd_type.lower() in ['bloom', 'ds_model']:\n            return data\n        return SDLoaderFactory.get_sd_loader(ckpt_list, checkpoint_engine, sd_type, version)\n\n    @staticmethod\n    def get_sd_loader(ckpt_list, checkpoint_engine, sd_type='Megatron', version=None):\n        if sd_type == 'Megatron':\n            return MegatronSDLoader(ckpt_list, version, checkpoint_engine)\n        else:\n            assert False, '{} checkpoint type is not supported'.format(sd_type)\n\n\nclass SDLoaderBase(ABC):\n\n    def __init__(self, ckpt_list, version, checkpoint_engine):\n        self.module_key = None\n        self.ckpt_list = ckpt_list\n        self.version = version\n        self.checkpoint_engine = TorchCheckpointEngine() if checkpoint_engine is None else checkpoint_engine\n        self.check_ckpt_list()\n\n    def load(self,\n             mp_world_size,\n             mp_rank,\n             module_key=AUTO_MODULE_KEY,\n             is_pipe_parallel=False,\n             quantize=False,\n             quantize_bits=8,\n             quantize_groups=64,\n             mlp_extra_grouping=True):\n        self.module_key = module_key\n        num_ckpt = len(self.ckpt_list)\n        idx = mp_rank * num_ckpt // mp_world_size\n        \"\"\" We have multiple cases to handle here for both training and inference:\n            1. PipeModule loading mp_rank_*.pt files, is_pipe_parallel=True, module_key is not None\n                a. if no mp_size/pp_size resizing occurs, for both training & inference, loading\n                   the mp_rank related checkpoint directly.\n                b. if has mp_size/pp_size resizing, only Megatron model inference is supported,\n                   in this case each mp_rank_*.pt have same content, we will load the first checkpoint\n                   file (idx=0), to avoid idx exceeding file list boundary.\n\n            2. PipeModule loading layer_*.pt files, is_pipe_parallel=True, module_key is None\n                a. if no mp_size resizing occurs, for both training & inference, loading\n                   the mp_rank related checkpoint directly.\n                b. if has mp_size resizing, only Megatron model inference is supported,\n                   checkpoint file(s) will be merged/split according to mp_rank, mp_world_size and\n                   checkpoint file list.\n\n            3. Non-PipeModule loading mp_rank_*.pt files, is_pipe_parallel=False\n                Same with case (2).\n        \"\"\"\n        if is_pipe_parallel and module_key is not None and mp_world_size != num_ckpt:\n            mp_world_size = num_ckpt\n            idx = 0\n\n        load_path = self.ckpt_list[idx]\n\n        merge_count = 1\n        if num_ckpt == mp_world_size:\n            assert os.path.exists(load_path)\n            #logger.info(f'rank: {mp_rank} loading checkpoint: {load_path}')\n            sd = self.checkpoint_engine.load(load_path, map_location=lambda storage, \\\n                loc: storage)\n\n            if quantize:\n                quantizer = WeightQuantization(mlp_extra_grouping=mlp_extra_grouping, mp_size=mp_world_size)\n                sd_module, all_scales = quantizer.sd_quantize_megatron(self.get_module(sd), quantize_bits,\n                                                                       quantize_groups)\n                self.set_module(sd, sd_module)\n            else:\n                all_scales = None\n        elif num_ckpt > mp_world_size:\n            sd, all_scales, merge_count = self.merge_state_dict(mp_world_size, mp_rank, quantize, \\\n                quantize_bits, quantize_groups, mlp_extra_grouping)\n        else:\n            sd, all_scales = self.split_state_dict(mp_world_size, mp_rank, quantize, quantize_bits, \\\n                quantize_groups, mlp_extra_grouping)\n        return load_path, sd, (all_scales, merge_count)\n\n    def get_merge_state_dicts(self, mp_world_size, mp_rank):\n        num_ckpt = len(self.ckpt_list)\n        assert num_ckpt % mp_world_size == 0, 'Invalid checkpoints and world size for sd merge'\n\n        num_to_merge = num_ckpt // mp_world_size\n        ckpt_list = [self.ckpt_list[i] for i in range(num_to_merge * mp_rank, num_to_merge * (mp_rank + 1))]\n\n        logger.info(f\"mp_rank: {mp_rank}, ckpt_list: {ckpt_list}\")\n        sd_list = [self.checkpoint_engine.load(ckpt, map_location=lambda storage, loc: storage) for ckpt in ckpt_list]\n        return sd_list\n\n    def get_split_state_dict(self, mp_world_size, mp_rank):\n        num_ckpt = len(self.ckpt_list)\n        assert mp_world_size % num_ckpt == 0, 'Invalid checkpoints and world size for sd split'\n\n        num_to_split = mp_world_size // num_ckpt\n        ckpt_index = mp_rank // num_to_split\n        ckpt_offset = mp_rank % num_to_split\n\n        logger.info(f\"mp_rank: {mp_rank}, ckpt_list: {self.ckpt_list[ckpt_index]}, offset: {ckpt_offset}\")\n\n        sd = self.checkpoint_engine.load(self.ckpt_list[ckpt_index], map_location=lambda storage, loc: storage)\n\n        return sd, num_to_split, ckpt_offset\n\n    def _choose_module_key(self, sd):\n        assert not ('module' in sd\n                    and 'model' in sd), \"checkpoint has both 'model' and 'module' keys, not sure how to proceed\"\n        assert 'module' in sd or 'model' in sd, \"checkpoint contains neither 'model' or 'module' keys, not sure how to proceed\"\n        if 'module' in sd:\n            return 'module'\n        elif 'model' in sd:\n            return 'model'\n\n    def get_module(self, sd):\n        if self.module_key is None:\n            return sd\n        elif self.module_key == AUTO_MODULE_KEY:\n            return sd[self._choose_module_key(sd)]\n        else:\n            return sd[self.module_key]\n\n    def set_module(self, sd, module):\n        if self.module_key is None:\n            sd = module\n        elif self.module_key == AUTO_MODULE_KEY:\n            sd[self._choose_module_key(sd)] = module\n        else:\n            sd[self.module_key] = module\n        return sd\n\n    def check_ckpt_list(self):\n        #logger.info(f'checkpoint file list: {self.ckpt_list}')\n        assert len(self.ckpt_list) > 0\n\n        sd = self.checkpoint_engine.load(self.ckpt_list[0], map_location=lambda storage, loc: storage)\n\n        # check checkpoint count is same with saved mp_world_size\n        if 'mp_world_size' in sd.keys():\n            assert len(self.ckpt_list) == sd[\n                'mp_world_size'], f\"checkpoint count {len(self.ckpt_list)} is different from saved mp_world_size {sd['mp_world_size']}\"\n\n    @abstractmethod\n    def merge_state_dict(self, mp_world_size, mp_rank, quantize, quantize_bits, groups, mlp_extra_grouping):\n        pass\n\n    @abstractmethod\n    def split_state_dict(self, mp_world_size, mp_rank, quantize, quantize_bits, groups, mlp_extra_grouping):\n        pass\n\n    @abstractmethod\n    def sanity_check(self, ckpt_file_name):\n        pass\n\n\nclass MegatronSDLoader(SDLoaderBase):\n\n    def __init__(self, ckpt_list, version, checkpoint_engine):\n        super().__init__(ckpt_list, version, checkpoint_engine)\n        \"\"\"\n        ## Q/K/V data need special processing\n        key: transformer.layers.0.attention.query_key_value.weight, shape: torch.Size([3192, 4256])\n        key: transformer.layers.0.attention.query_key_value.bias, shape: torch.Size([3192])\n\n        ## merge or split on axis=0\n        key: word_embeddings.weight, shape: torch.Size([12672, 4256])\n        key: transformer.layers.0.mlp.dense_h_to_4h.bias, shape: torch.Size([4256])\n        key: transformer.layers.0.mlp.dense_h_to_4h.weight, shape: torch.Size([4256, 4256])\n\n        ## merge or split on axis=1\n        key: transformer.layers.0.attention.dense.weight, shape: torch.Size([4256, 1064])\n        key: transformer.layers.0.mlp.dense_4h_to_h.weight, shape: torch.Size([4256, 4256])\n\n        ## no change required\n        key: transformer.layers.0.mlp.dense_4h_to_h.bias, shape: torch.Size([4256])\n        key: transformer.final_layernorm.weight, shape: torch.Size([4256])\n        key: transformer.final_layernorm.bias, shape: torch.Size([4256])\n        key: transformer.layers.0.attention.dense.bias, shape: torch.Size([4256])\n        key: transformer.layers.0.post_attention_layernorm.weight, shape: torch.Size([4256])\n        key: transformer.layers.0.post_attention_layernorm.bias, shape: torch.Size([4256])\n        key: transformer.layers.0.input_layernorm.weight, shape: torch.Size([4256])\n        key: transformer.layers.0.input_layernorm.bias, shape: torch.Size([4256])\n        key: position_embeddings.weight, shape: torch.Size([1024, 4256])\n        \"\"\"\n\n    def merge_query_key_value(self, param_list, ckpt_ver):\n        \"\"\"\n        Up to now we found 3 Q/K/V parameter formats in different Megatron checkpoint versions:\n\n        1. version 0, there is no version information saved in checkpoint.\n            format: [(3 * np * hn), h]\n        2. version 1.0\n            format: [(np * hn * 3), h]\n        3. version 2.0\n            format: [(np * 3 * hn), h]\n\n        h: hidden size\n        n: number of attention heads\n        p: number of model parallel partitions\n        np: n/p\n        hn: h/n\n        \"\"\"\n\n        new_qkv = None\n        if ckpt_ver == 0:\n            # [(3 * np * hn), h]\n            assert param_list[0].shape[0] % 3 == 0\n            size_qkv = param_list[0].shape[0] // 3\n            split_tensors = [torch.split(param, size_qkv, dim=0) for param in param_list]\n\n            tensors = []\n            for i in range(3):\n                tensor_tuple = [t[i] for t in split_tensors]\n                tensors.append(torch.cat(tensor_tuple, axis=0))\n            new_qkv = torch.cat(tensors, axis=0)\n        elif ckpt_ver == 1.0 or ckpt_ver == 2.0:\n            # [(np * hn * 3), h] or [(np * 3 * hn), h]\n            new_qkv = torch.cat(param_list, axis=0)\n        else:\n            assert False, f'checkpoint version: {ckpt_ver} is not supported'\n\n        return new_qkv\n\n    def split_query_key_value(self, param, num_to_split, offset, ckpt_ver):\n        \"\"\"\n        Up to now we found 3 Q/K/V parameter formats in different Megatron checkpoint versions:\n\n        1. version 0, there is no version information saved in checkpoint.\n            format: [(3 * np * hn), h]\n        2. version 1.0\n            format: [(np * hn * 3), h]\n        3. version 2.0\n            format: [(np * 3 * hn), h]\n\n        h: hidden size\n        n: number of attention heads\n        p: number of model parallel partitions\n        np: n/p\n        hn: h/n\n        \"\"\"\n\n        new_qkv = None\n        if ckpt_ver == 0:\n            # [(3 * np * hn), h]\n            assert param.shape[0] % 3 == 0\n            size_qkv = param.shape[0] // 3\n            split_tensors = torch.split(param, size_qkv, dim=0)\n\n            assert split_tensors[0].shape[0] % num_to_split == 0\n            split_size = split_tensors[0].shape[0] // num_to_split\n\n            tensors = []\n            for i in range(3):\n                tensors.append(torch.split(split_tensors[i], split_size, dim=0)[offset])\n            new_qkv = torch.cat(tensors, axis=0)\n        elif ckpt_ver == 1.0 or ckpt_ver == 2.0:\n            # [(np * hn * 3), h] or [(np * 3 * hn), h]\n            assert param.shape[0] % num_to_split == 0\n            size_qkv = param.shape[0] // num_to_split\n            split_tensors = torch.split(param, size_qkv, dim=0)\n            new_qkv = split_tensors[offset]\n        else:\n            assert False, f'checkpoint version: {ckpt_ver} is not supported'\n\n        return new_qkv\n\n    def merge_state_dict(self,\n                         mp_world_size,\n                         mp_rank,\n                         quantize=False,\n                         quantize_bits=8,\n                         groups=64,\n                         mlp_extra_grouping=True):\n        self.sanity_check(self.ckpt_list[0])\n\n        sd_list = self.get_merge_state_dicts(mp_world_size, mp_rank)\n        ds_sd = copy.deepcopy(sd_list[0])\n        new_client_sd = collections.OrderedDict()\n\n        client_sd_list = [self.get_module(sd) for sd in sd_list]\n        keys = client_sd_list[0].keys()\n\n        ckpt_ver = self.get_checkpoint_version(ds_sd)\n        logger.info(f\"checkpoint version: {ckpt_ver}\")\n        if quantize:\n            quantizer = WeightQuantization(mlp_extra_grouping=mlp_extra_grouping, mp_size=mp_world_size)\n\n        for key in keys:\n            value_list = [sd[key] for sd in client_sd_list]\n\n            if \"attention.dense.weight\" in key or \"mlp.dense_4h_to_h.weight\" in key:\n                if quantize:\n                    value_list = quantizer.Quantize(value_list, quantize_bits, groups, key=key, merge_dim=1)\n                new_client_sd[key] = torch.cat(value_list, axis=1)\n            elif \"attention.query_key_value\" in key:\n                if quantize and \"attention.query_key_value.weight\" in key:\n                    value_list = quantizer.Quantize(value_list, quantize_bits, groups, key=key)\n                    new_client_sd[key] = torch.cat(value_list, axis=0)\n                else:\n                    if quantize:\n                        new_client_sd[key] = torch.cat(value_list, axis=0)\n                    else:\n                        new_client_sd[key] = self.merge_query_key_value(value_list, ckpt_ver)\n            elif \"mlp.dense_h_to_4h.weight\" in key or \"word_embeddings.weight\" in key or \"mlp.dense_h_to_4h.bias\" in key:\n                if quantize and \"mlp.dense_h_to_4h.weight\" in key:\n                    value_list = quantizer.Quantize(value_list, quantize_bits, groups, key=key)\n                new_client_sd[key] = torch.cat(value_list, axis=0)\n            else:\n                new_client_sd[key] = value_list[0]\n        if quantize:\n            all_scales = quantizer.merge_scales()\n        ds_sd = self.set_module(ds_sd, new_client_sd)\n\n        return ds_sd, (all_scales if quantize else None), len(client_sd_list)\n\n    def split_state_dict(self,\n                         mp_world_size,\n                         mp_rank,\n                         quantize=False,\n                         quantize_bits=8,\n                         groups=64,\n                         mlp_extra_grouping=True):\n        #self.sanity_check(self.ckpt_list[0])\n\n        sd, num_to_split, ckpt_offset = self.get_split_state_dict(mp_world_size, mp_rank)\n        ds_sd = copy.deepcopy(sd)\n        new_client_sd = collections.OrderedDict()\n\n        client_sd = self.get_module(sd)\n\n        ckpt_ver = self.get_checkpoint_version(ds_sd)\n        logger.info(f\"checkpoint version: {ckpt_ver}\")\n\n        if quantize:\n            quantizer = WeightQuantization(mlp_extra_grouping=mlp_extra_grouping, mp_size=mp_world_size)\n\n        for key in client_sd.keys():\n            value = client_sd[key]\n\n            if \"attention.dense.weight\" in key or \"mlp.dense_4h_to_h.weight\" in key:\n                assert value.shape[1] % num_to_split == 0\n                split_size = value.shape[1] // num_to_split\n                if quantize:\n                    q_vals = quantizer.Quantize([value], quantize_bits, groups, key)\n                    value = q_vals[0]\n                new_client_sd[key] = torch.split(value, split_size, dim=1)[ckpt_offset]\n            elif \"attention.query_key_value\" in key:\n                if quantize and \"attention.query_key_value.weight\" in key:\n                    q_vals = quantizer.Quantize([value], quantize_bits, groups, key)\n                    value = q_vals[0]\n                new_client_sd[key] = self.split_query_key_value(value, num_to_split, ckpt_offset, ckpt_ver)\n            elif \"mlp.dense_h_to_4h.weight\" in key or \"word_embeddings.weight\" in key or \"mlp.dense_h_to_4h.bias\" in key or \"final_linear.weight\" in key:\n                assert value.shape[0] % num_to_split == 0\n                split_size = value.shape[0] // num_to_split\n                if quantize and \"mlp.dense_h_to_4h.weight\" in key:\n                    q_vals = quantizer.Quantize([value], quantize_bits, groups, key)\n                    value = q_vals[0]\n                new_client_sd[key] = torch.split(value, split_size, dim=0)[ckpt_offset]\n            else:\n                new_client_sd[key] = value\n\n        if quantize:\n            all_scales = quantizer.merge_scales_split(num_to_split)\n\n        ds_sd = self.set_module(ds_sd, new_client_sd)\n\n        return ds_sd, (all_scales if quantize else None)\n\n    def sanity_check(self, ckpt_file_name):\n        keys_to_check = [\n            \"attention.dense.weight\", \"mlp.dense_4h_to_h.weight\", \"attention.query_key_value\",\n            \"mlp.dense_h_to_4h.weight\", \"mlp.dense_h_to_4h.bias\"\n        ]\n\n        sd = self.checkpoint_engine.load(ckpt_file_name, map_location=lambda storage, loc: storage)\n\n        # partial_key is a sub-string of one key in the sd\n        def check_key_exist(partial_key, sd):\n            keys = sd.keys()\n            found = False\n            for k in keys:\n                if partial_key in k:\n                    found = True\n                    break\n            return found\n\n        for key in keys_to_check:\n            assert check_key_exist(key,\n                                   self.get_module(sd)), f'key: {key} is not found in the checkpoint {ckpt_file_name}'\n\n    def get_checkpoint_version(self, state_dict):\n        # Use 0 if version info doesn't exist\n        return self.version if self.version is not None else state_dict.get('checkpoint_version', 0)\n", "deepspeed/runtime/weight_quantizer.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nfrom ..module_inject.replace_policy import HFBertLayerPolicy, replace_policies\nfrom deepspeed.accelerator import get_accelerator\n\n\nclass WeightQuantization(object):\n\n    def __init__(self, mlp_extra_grouping=True, mp_size=1):\n        self.dense_scales = []\n        self.qkv_scales = []\n        self.mlp4hh_scales = []\n        self.mlph4h_scales = []\n        self.mlp_extra_grouping = mlp_extra_grouping\n        self.mp_size = mp_size\n\n    def quantize_data(self, data, quantize_bits, groups, key=None):\n        data_groups = torch.split(data.float().view(-1), data.numel() // groups)\n        max_d = [max(g.max(), g.min().abs()) for g in data_groups]\n        data_scale = [float(1 << quantize_bits) / (2 * mx + 1e-5) for mx in max_d]\n        data_int = [(g * s) for g, s in zip(data_groups, data_scale)]\n        data_int = [\n            di.round().clamp(-(1 << (quantize_bits - 1)), (((1 << (quantize_bits - 1)) - 1))) for di in data_int\n        ]\n        data_int = torch.cat(data_int).reshape(data.shape)\n        data_int = data_int.to(torch.int8)\n        data_scale = torch.cat([s.unsqueeze(0).unsqueeze(0) for s in data_scale])\n        return data_int, data_scale\n\n    def is_mlp(self, data, merge_count=1):\n        return ((self.mp_size *data.shape[0] * merge_count) / data.shape[1] == 4 or \\\n                (self.mp_size *data.shape[1] * merge_count) / data.shape[0] == 4)\n\n    def is_qkv(self, data):\n        return ((self.mp_size * data.shape[0]) / data.shape[1] == 3 or \\\n                (self.mp_size * data.shape[1]) / data.shape[0] == 3)\n\n    def Quantize(self, value_list, quantize_bits, groups, key, merge_dim=0):\n        if self.mlp_extra_grouping and self.is_mlp(value_list[0], merge_count=len(value_list)):\n            groups *= 2\n        q_scale = []\n        index = 0\n        for data in value_list:\n            data_int, data_scale = self.quantize_data(data, quantize_bits, groups, key)\n            q_scale.append(data_scale)\n            value_list[index] = data_int\n            index += 1\n        q_scale = (1 /\n                   torch.cat(q_scale, dim=merge_dim).to(get_accelerator().current_device_name()).view(-1).unsqueeze(0))\n        if \"mlp.dense_4h_to_h.weight\" in key:\n            self.mlp4hh_scales.append(q_scale)\n        elif \"mlp.dense_h_to_4h.weight\" in key:\n            self.mlph4h_scales.append(q_scale)\n        elif \"attention.query_key_value.weight\" in key:\n            self.qkv_scales.append(q_scale)\n        else:\n            self.dense_scales.append(q_scale)\n        return value_list\n\n    def merge_layer_scales(self, layer_scales):\n        max_dim = max([s.shape[-1] for s in layer_scales])\n        layer_scales = [\n            torch.cat((s, torch.zeros((1, max_dim - s.shape[-1]), device=get_accelerator().current_device_name())),\n                      dim=-1) if s.shape[-1] < max_dim else s for s in layer_scales\n        ]\n        return torch.cat(layer_scales).unsqueeze(0)\n\n    def merge_scales(self):\n        all_scales = []\n        for dense_scale, qkv_scale, m4hh_scale, mh4h_scale in \\\n            zip(self.dense_scales, self.qkv_scales, self.mlp4hh_scales, self.mlph4h_scales):\n            all_scales.append(self.merge_layer_scales([qkv_scale, dense_scale, mh4h_scale, m4hh_scale]))\n        return torch.cat(all_scales)\n\n    def merge_scales_split(self, split_count):\n        all_scales = [[] for _ in range(split_count)]\n        for dense_scale, qkv_scale, m4hh_scale, mh4h_scale in \\\n            zip(self.dense_scales, self.qkv_scales, self.mlp4hh_scales, self.mlph4h_scales):\n            dense_scale = torch.split(dense_scale, dense_scale.numel() // split_count)\n            qkv_scale = torch.split(qkv_scale, qkv_scale.numel() // split_count)\n            m4hh_scale = torch.split(m4hh_scale, m4hh_scale.numel() // split_count)\n            mh4h_scale = torch.split(mh4h_scale, mh4h_scale.numel() // split_count)\n            for s in range(split_count):\n                all_scales[s].append(\n                    torch.cat([\n                        torch.cat((qkv_scale[s], torch.zeros_like(qkv_scale[s])), dim=1),\n                        torch.cat((dense_scale[s], torch.zeros_like(dense_scale[s])), dim=1), mh4h_scale[s],\n                        m4hh_scale[s]\n                    ]).unsqueeze(0))\n            for scales_a in all_scales:\n                torch.cat(scales_a)\n        return all_scales\n\n    def sd_quantize_megatron(self, sd, quantize_bits, groups):\n        keys = sd.keys()\n        for key in keys:\n            value_list = [sd[key]]\n            if \"attention.dense.weight\" in key or \"mlp.dense_4h_to_h.weight\" in key or \\\n                \"mlp.dense_h_to_4h.weight\" in key or \"attention.query_key_value.weight\" in key:\n                value_list = self.Quantize(value_list, quantize_bits, groups, key=key)\n            sd[key] = value_list[0]\n\n        all_scales = self.merge_scales()\n        return sd, all_scales\n\n    def model_quantize(self, model, quantize_policy, quantize_bits, groups):\n        all_scales = []\n\n        def quantize_fn(layer, policy_cls):\n            policy = policy_cls(layer)\n\n            _, qkvw, _, dense_w, _, _ = policy.attention()\n            _, _h4h_w, _, _4hh_w, _ = policy.mlp()\n            keys = [qkvw, dense_w, _h4h_w, _4hh_w]\n            layer_scales = []\n\n            for key in range(len(keys)):\n                if self.mlp_extra_grouping and self.is_mlp(keys[key]):\n                    data_quantized, data_scale = self.quantize_data(keys[key], quantize_bits, groups * 2)\n                elif policy_cls is HFBertLayerPolicy and self.is_qkv(keys[key]):\n                    data_quantized, data_scale = self.quantize_data(keys[key], quantize_bits, groups * 3)\n                else:\n                    data_quantized, data_scale = self.quantize_data(keys[key], quantize_bits, groups)\n                keys[key].copy_(data_quantized)\n                layer_scales.append((1 / data_scale.to(get_accelerator().current_device_name()).view(-1).unsqueeze(0)))\n            all_scales.append(self.merge_layer_scales(layer_scales))\n            return layer\n\n        def _quantize_module(model, policies):\n            for name, child in model.named_children():\n                if child.__class__ in policies:\n                    quantize_fn, replace_policy = policies[child.__class__]\n                    setattr(model, name, quantize_fn(child, replace_policy))\n                else:\n                    _quantize_module(child, policies)\n\n            return model\n\n        policy = {}\n        if quantize_policy is not None:\n            for layer_name, replace_policy in quantize_policy.items():\n                policy.update({layer_name: (quantize_fn, replace_policy)})\n        else:\n            for plcy in replace_policies:\n                policy.update({plcy._orig_layer_class: (quantize_fn, plcy)})\n\n        quantized_module = _quantize_module(model, policy)\n\n        return quantized_module, torch.cat(all_scales)\n", "deepspeed/runtime/bf16_optimizer.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom collections import OrderedDict\nimport torch\nimport sys\nfrom torch._utils import _flatten_dense_tensors, _unflatten_dense_tensors\nfrom deepspeed import comm as dist\nfrom deepspeed.runtime.constants import PIPE_REPLICATED\nfrom deepspeed.runtime.base_optimizer import ZeROOptimizer\nfrom packaging import version as pkg_version\nfrom deepspeed.git_version_info import version\nfrom deepspeed.runtime.utils import (get_global_norm_of_tensors, clip_tensors_by_global_norm, DummyOptim,\n                                     align_dense_tensors, all_gather_dp_groups, is_model_parallel_parameter,\n                                     see_memory_usage, graph_process, get_norm_with_moe_layers)\nfrom deepspeed.utils import link_hp_params, lazy_init_hp_params_optimizer_state, fragment_address, groups\nfrom deepspeed.moe.utils import is_moe_param, is_moe_param_group\nfrom deepspeed.utils.bwc import bwc_tensor_model_parallel_rank\nfrom deepspeed.checkpoint import enable_universal_checkpoint\nfrom deepspeed.checkpoint.constants import (DS_VERSION, PARTITION_COUNT, BASE_OPTIMIZER_STATE,\n                                            SINGLE_PARTITION_OF_FP32_GROUPS, CLIP_GRAD, GROUP_PADDINGS,\n                                            PARAM_SLICE_MAPPINGS)\n\nsetattr(sys.modules[__name__], 'fragment_address', fragment_address)\n\n\nclass BF16_Optimizer(ZeROOptimizer):\n\n    def __init__(self,\n                 init_optimizer,\n                 param_names,\n                 mpu=None,\n                 clip_grad=0.0,\n                 norm_type=2,\n                 allgather_bucket_size=5000000000,\n                 dp_process_group=None,\n                 timers=None,\n                 grad_acc_dtype=None,\n                 graph_harvesting=False,\n                 immediate_grad_update=False,\n                 has_moe_layers=False):\n        super().__init__()\n        see_memory_usage('begin bf16_optimizer', force=True)\n        self.timers = timers\n        self.optimizer = init_optimizer\n        self.param_names = param_names\n        self.using_real_optimizer = not isinstance(self.optimizer, DummyOptim)\n\n        assert grad_acc_dtype in [torch.float32, torch.bfloat16\n                                  ], f\"BF16Optimizer: Unsupported gradient accumulation data type: {grad_acc_dtype}\"\n        self.grad_acc_dtype = grad_acc_dtype\n        self.immediate_grad_update = immediate_grad_update\n\n        self.clip_grad = clip_grad\n        self.norm_type = norm_type\n        self.mpu = mpu\n        self.allgather_bucket_size = int(allgather_bucket_size)\n        self.dp_process_group = dp_process_group\n        self.dp_rank = dist.get_rank(group=self.dp_process_group)\n        self.has_moe_layers = has_moe_layers\n        self.non_expert_gradients = []\n        self.real_dp_process_group = [dp_process_group for i in range(len(self.optimizer.param_groups))]\n        if self.has_moe_layers:\n            self._configure_moe_settings()\n\n        # Use torch (un)flatten ops\n        self.flatten = _flatten_dense_tensors\n        self.unflatten = _unflatten_dense_tensors\n\n        #align nccl all-gather send buffers to 4-bye boundary\n        self.nccl_start_alignment_factor = 2  # 4-byte alignment/sizeof(fp16) = 2\n\n        # Build BF16/FP32 groups\n        self.bf16_groups = []\n        self.bf16_groups_flat = []\n        self.bf16_partitioned_groups = []\n\n        self.fp32_groups_flat_partition = []\n\n        # Maintain different fp32 gradients views for convenience\n        self.fp32_groups_gradients = []\n        self.fp32_groups_gradient_dict = {}\n        self.fp32_groups_gradients_flat = []\n        self.fp32_groups_actual_gradients_flat = []\n        self.fp32_groups_gradient_flat_partition = []\n        self.fp32_groups_has_gradients = []\n\n        self.group_paddings = []\n        self.graph_harvesting = graph_harvesting\n        if self.using_real_optimizer:\n            self._setup_for_real_optimizer()\n\n        see_memory_usage('end bf16_optimizer', force=True)\n\n    def _configure_moe_settings(self):\n        assert any(\n            [is_moe_param_group(group) for group in self.optimizer.param_groups]\n        ), \"The model has moe layers, but None of the param groups are marked as MoE. Create a param group with 'moe' key set to True before creating optimizer\"\n\n        for i, group in enumerate(self.optimizer.param_groups):\n            if is_moe_param_group(group):\n                assert all([is_moe_param(param)\n                            for param in group['params']]), \"All params in MoE group must be MoE params\"\n                self.real_dp_process_group[i] = groups._get_expert_data_parallel_group(group['name'])\n        self.expert_gradients = {}\n        if self.has_moe_layers:\n            for key in groups._get_expert_data_parallel_group_dict().keys():\n                self.expert_gradients[key] = []\n\n    def _setup_for_real_optimizer(self):\n        self.partition_count = [dist.get_world_size(group=pg) for pg in self.real_dp_process_group]\n\n        for i, param_group in enumerate(self.optimizer.param_groups):\n            real_dp_world_size = dist.get_world_size(group=self.real_dp_process_group[i])\n            see_memory_usage(f'before initializing group {i}', force=True)\n\n            partition_id = dist.get_rank(group=self.real_dp_process_group[i])\n\n            # grab the original list\n            trainable_parameters = [param for param in param_group['params'] if param.requires_grad]\n            self.bf16_groups.append(trainable_parameters)\n\n            # create flat bf16 params\n            self.bf16_groups_flat.append(\n                self._flatten_dense_tensors_aligned(self.bf16_groups[i],\n                                                    self.nccl_start_alignment_factor * real_dp_world_size))\n            # Make bf16 params point to flat tensor storage\n            self._update_storage_to_flattened_tensor(tensor_list=self.bf16_groups[i],\n                                                     flat_tensor=self.bf16_groups_flat[i])\n\n            # divide flat weights into equal sized partitions\n            partition_size = self.bf16_groups_flat[i].numel() // real_dp_world_size\n            bf16_dp_partitions = [\n                self.bf16_groups_flat[i].narrow(0, dp_index * partition_size, partition_size)\n                for dp_index in range(real_dp_world_size)\n            ]\n            self.bf16_partitioned_groups.append(bf16_dp_partitions)\n\n            # create fp32 params partition\n            self.fp32_groups_flat_partition.append(bf16_dp_partitions[partition_id].clone().float().detach())\n            self.fp32_groups_flat_partition[i].requires_grad = True\n\n            num_elem_list = [t.numel() for t in self.bf16_groups[i]]\n\n            # create fp32 gradients\n            fp32_flat_buffer = torch.zeros_like(self.bf16_groups_flat[i], dtype=self.grad_acc_dtype)\n            self.fp32_groups_gradients_flat.append(fp32_flat_buffer)\n            if self.has_moe_layers and is_moe_param_group(param_group):\n                self.expert_gradients[param_group['name']].append(fp32_flat_buffer)\n            else:\n                self.non_expert_gradients.append(fp32_flat_buffer)\n\n            # track individual fp32 gradients for entire model\n            fp32_gradients = self._split_flat_tensor(flat_tensor=self.fp32_groups_gradients_flat[i],\n                                                     num_elem_list=num_elem_list)\n            self.fp32_groups_gradients.append(fp32_gradients)\n            self.fp32_groups_gradient_dict[i] = fp32_gradients\n\n            # flat tensor corresponding to actual fp32 gradients (i.e., minus alignment padding)\n            length_without_padding = sum(num_elem_list)\n            self.fp32_groups_actual_gradients_flat.append(\n                torch.narrow(self.fp32_groups_gradients_flat[i], 0, 0, length_without_padding))\n\n            # flat tensor corresponding to gradient partition\n            self.fp32_groups_gradient_flat_partition.append(\n                torch.narrow(self.fp32_groups_gradients_flat[i], 0, partition_id * partition_size, partition_size))\n\n            # track fp32 gradient updates\n            self.fp32_groups_has_gradients.append([False] * len(self.bf16_groups[i]))\n\n            # Record padding required for alignment\n            if partition_id == dist.get_world_size(group=self.real_dp_process_group[i]) - 1:\n                padding = self.bf16_groups_flat[i].numel() - length_without_padding\n            else:\n                padding = 0\n\n            self.group_paddings.append(padding)\n\n            # update optimizer param groups to reference fp32 params partition\n            param_group['params'] = [self.fp32_groups_flat_partition[i]]\n\n            see_memory_usage(f'after initializing group {i}', force=True)\n\n        see_memory_usage('before initialize_optimizer', force=True)\n        self.initialize_optimizer_states()\n        see_memory_usage('end initialize_optimizer', force=True)\n\n        if self.immediate_grad_update:\n            self.create_grad_acc_hooks()\n\n        # Need optimizer states initialized before linking lp to optimizer state\n        self._link_all_hp_params()\n        self._hp_optimizer_states_linked = False\n        self._enable_universal_checkpoint()\n        self._param_slice_mappings = self._create_param_mapping()\n\n    def _enable_universal_checkpoint(self):\n        for lp_param_group in self.bf16_groups:\n            enable_universal_checkpoint(param_list=lp_param_group)\n\n    def _create_param_mapping(self):\n        param_mapping = []\n        for i, _ in enumerate(self.optimizer.param_groups):\n            param_mapping_per_group = OrderedDict()\n            for lp in self.bf16_groups[i]:\n                if lp._hp_mapping is not None:\n                    lp_name = self.param_names[lp]\n                    param_mapping_per_group[lp_name] = lp._hp_mapping.get_hp_fragment_address()\n            param_mapping.append(param_mapping_per_group)\n\n        return param_mapping\n\n    def _link_all_hp_params(self):\n        for i, _ in enumerate(self.optimizer.param_groups):\n            real_dp_world_size = dist.get_world_size(group=self.real_dp_process_group[i])\n\n            # Link bf16 and fp32 params in partition\n            partition_id = dist.get_rank(group=self.real_dp_process_group[i])\n            partition_size = self.bf16_groups_flat[i].numel() // real_dp_world_size\n            flat_hp_partition = self.fp32_groups_flat_partition[i]\n            link_hp_params(lp_param_list=self.bf16_groups[i],\n                           flat_hp_partition=flat_hp_partition,\n                           gradient_dict=self.fp32_groups_gradient_dict,\n                           offload_gradient_dict=None,\n                           use_offload=False,\n                           param_group_index=i,\n                           partition_start=partition_id * partition_size,\n                           partition_size=partition_size,\n                           dp_group=self.real_dp_process_group[i])\n\n    def _lazy_init_hp_params_optimizer_state(self):\n        if not self._hp_optimizer_states_linked:\n            for i, _ in enumerate(self.optimizer.param_groups):\n                lazy_init_hp_params_optimizer_state(self.bf16_groups[i], self.fp32_groups_flat_partition[i],\n                                                    self.optimizer.state)\n            self._hp_optimizer_states_linked = True\n\n    def initialize_optimizer_states(self):\n        \"\"\"Take an optimizer step with zero-valued gradients to allocate internal\n        optimizer state.\n\n        This helps prevent memory fragmentation by allocating optimizer state at the\n        beginning of training instead of after activations have been allocated.\n        \"\"\"\n        for param_partition, grad_partition in zip(self.fp32_groups_flat_partition,\n                                                   self.fp32_groups_gradient_flat_partition):\n            # In case of grad acc dtype different than FP32, need to cast to high precision.\n            param_partition.grad = grad_partition.to(\n                param_partition.dtype) if grad_partition.dtype != param_partition.dtype else grad_partition\n\n        if self.grad_acc_dtype is not torch.float32:\n            for param_partition in self.fp32_groups_flat_partition:\n                param_partition.grad = None\n\n        self.clear_hp_grads()\n\n    def _split_flat_tensor(self, flat_tensor, num_elem_list):\n        assert sum(num_elem_list) <= flat_tensor.numel()\n        tensor_list = []\n        offset = 0\n        for num_elem in num_elem_list:\n            dense_tensor = torch.narrow(flat_tensor, 0, offset, num_elem)\n            tensor_list.append(dense_tensor)\n            offset += num_elem\n\n        return tensor_list\n\n    def _update_storage_to_flattened_tensor(self, tensor_list, flat_tensor):\n        updated_params = self.unflatten(flat_tensor, tensor_list)\n        for p, q in zip(tensor_list, updated_params):\n            p.data = q.data\n\n    def _flatten_dense_tensors_aligned(self, tensor_list, alignment):\n        return self.flatten(align_dense_tensors(tensor_list, alignment))\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        if closure is not None:\n            raise NotImplementedError(f'{self.__class__} does not support closure.')\n\n        non_expert_grads_for_norm, expert_grads_for_norm = self.get_grads_for_norm()\n        non_expert_groups_norm = get_global_norm_of_tensors(input_tensors=non_expert_grads_for_norm,\n                                                            mpu=self.mpu,\n                                                            norm_type=self.norm_type,\n                                                            use_graph=self.graph_harvesting)\n        all_groups_norm = non_expert_groups_norm\n        if self.has_moe_layers:\n            all_groups_norm = get_norm_with_moe_layers(non_expert_groups_norm,\n                                                       mpu=self.mpu,\n                                                       expert_tensors=expert_grads_for_norm,\n                                                       norm_type=self.norm_type)\n\n        self._global_grad_norm = all_groups_norm\n\n        assert all_groups_norm > 0.\n        if self.clip_grad > 0.:\n            clip_tensors_by_global_norm(input_tensors=self.get_grads_for_norm(for_clipping=True),\n                                        max_norm=self.clip_grad,\n                                        global_norm=all_groups_norm,\n                                        mpu=self.mpu,\n                                        use_graph=self.graph_harvesting)\n\n        self.optimizer.step()\n\n        # We need to link optimizer state after the first step() call\n        self._lazy_init_hp_params_optimizer_state()\n\n        self.update_lp_params()\n\n        self.clear_hp_grads()\n\n    def backward(self, loss, update_hp_grads=True, clear_lp_grads=False, **bwd_kwargs):\n        \"\"\"Perform a backward pass and copy the low-precision gradients to the\n        high-precision copy.\n\n        We copy/accumulate to the high-precision grads now to prevent accumulating in the\n        bf16 grads after successive backward() calls (i.e., grad accumulation steps > 1)\n\n        The low-precision grads are deallocated during this procedure.\n        \"\"\"\n        self.clear_lp_grads()\n        loss.backward(**bwd_kwargs)\n\n        if update_hp_grads:\n            self.update_hp_grads(clear_lp_grads=clear_lp_grads)\n\n    @torch.no_grad()\n    def _update_hp_grad(self, lp, group_idx, param_idx, clear_lp_grads):\n        if lp.grad is None:\n            return\n\n        hp_grad = self.fp32_groups_gradients[group_idx][param_idx]\n        assert hp_grad is not None, \\\n            f'high precision param has no gradient, lp param_id = {id(lp)} group_info = [{group_idx}][{param_idx}]'\n\n        hp_grad.data.add_(lp.grad.data.to(hp_grad.dtype).view(hp_grad.shape))\n        lp._hp_grad = hp_grad\n        self.fp32_groups_has_gradients[group_idx][param_idx] = True\n\n        # clear gradients\n        if clear_lp_grads:\n            lp.grad.zero_()\n\n    @torch.no_grad()\n    def _update_hp_grads_func(self, clear_lp_grads=False):\n        for i, group in enumerate(self.bf16_groups):\n            for j, lp in enumerate(group):\n                self._update_hp_grad(lp, i, j, clear_lp_grads)\n\n    @torch.no_grad()\n    def update_hp_grads(self, clear_lp_grads=False):\n        if self.immediate_grad_update:\n            return\n\n        if self.graph_harvesting:\n            graph_process(False, self._update_hp_grads_func, clear_lp_grads)\n        else:\n            self._update_hp_grads_func(clear_lp_grads)\n        #cpu op\n        for i, group in enumerate(self.bf16_groups):\n            for j, lp in enumerate(group):\n                if lp.grad is None:\n                    continue\n                self.fp32_groups_has_gradients[i][j] = True\n\n    @torch.no_grad()\n    def get_grads_for_reduction(self):\n        if self.has_moe_layers:\n            return self.non_expert_gradients, self.expert_gradients\n        return self.non_expert_gradients, {}\n\n    @torch.no_grad()\n    def get_grads_for_norm(self, for_clipping=False):\n        \"\"\"\n        Returns:\n            tuple[list[Tensor], dict[ep_name, List[Tensor]] | list:\n            If for_clipping, return all gradients.\n            Otherwise, separate and return dict of expert_grad and list of non_expert_grad\n        \"\"\"\n        # (grads, expert_group_name)\n        expert_grads_for_norm = {}\n\n        # grads\n        non_expert_grads_for_norm = []\n        all_grads_for_clip = []\n\n        tensor_mp_rank = bwc_tensor_model_parallel_rank(mpu=self.mpu)\n        assert len(self.bf16_groups) == len(self.optimizer.param_groups)\n        for i, group in enumerate(self.bf16_groups):\n            for j, lp in enumerate(group):\n                if not for_clipping:\n                    if hasattr(lp, PIPE_REPLICATED) and lp.ds_pipe_replicated:\n                        continue\n\n                    # skip duplicated parameters. perform norm only on cards with tp_rank=0.\n                    # non-duplicated parameters include:\n                    # - Parameters with tp: Use allreducesum of mp_group.\n                    # - Moe Parameters with ep: Use allreducesum of ep_group.\n                    if not (tensor_mp_rank == 0 or is_model_parallel_parameter(lp) or is_moe_param(lp)):\n                        continue\n\n                if not self.fp32_groups_has_gradients[i][j]:\n                    continue\n                if not for_clipping:\n                    param_group = self.optimizer.param_groups[i]\n                    if self.has_moe_layers and is_moe_param_group(param_group):\n                        if param_group['name'] not in expert_grads_for_norm:\n                            expert_grads_for_norm[param_group['name']] = []\n                        expert_grads_for_norm[param_group['name']].append(self.fp32_groups_gradients[i][j])\n                    else:\n                        non_expert_grads_for_norm.append(self.fp32_groups_gradients[i][j])\n                else:\n                    all_grads_for_clip.append(self.fp32_groups_gradients[i][j])\n        if not for_clipping:\n            return non_expert_grads_for_norm, expert_grads_for_norm\n        return all_grads_for_clip\n\n    @torch.no_grad()\n    def update_lp_params(self):\n        for i, (bf16_partitions,\n                fp32_partition) in enumerate(zip(self.bf16_partitioned_groups, self.fp32_groups_flat_partition)):\n            partition_id = dist.get_rank(group=self.real_dp_process_group[i])\n            bf16_partitions[partition_id].data.copy_(fp32_partition.data)\n            # print_rank_0(f'update_lp_params {i=} {partition_id=}', force=True)\n            # if i == 0:\n            #     print_rank_0(f'{fp32_partition[:10]=}', force=True)\n\n        all_gather_dp_groups(groups_flat=self.bf16_groups_flat,\n                             partitioned_param_groups=self.bf16_partitioned_groups,\n                             dp_process_group=self.real_dp_process_group,\n                             start_alignment_factor=self.nccl_start_alignment_factor,\n                             allgather_bucket_size=self.allgather_bucket_size)\n\n    def clear_hp_grads(self):\n        for flat_gradients in self.fp32_groups_gradients_flat:\n            flat_gradients.zero_()\n\n        for i, group in enumerate(self.fp32_groups_gradients):\n            self.fp32_groups_has_gradients[i] = [False] * len(group)\n\n    def clear_lp_grads(self):\n\n        # using zero_() fixed memory address for graph replay\n        set_to_none = False if self.graph_harvesting else True\n        zero_grads_list = []\n        for group in self.bf16_groups:\n            for param in group:\n                if set_to_none:\n                    param.grad = None\n                elif param.grad is not None:\n                    if param.grad.grad_fn is not None:\n                        param.grad.detach_()\n                    zero_grads_list.append(param.grad)\n        if not set_to_none and len(zero_grads_list) > 0:\n            torch._foreach_zero_(zero_grads_list)\n\n    def state_dict(self):\n        state_dict = {}\n        state_dict[CLIP_GRAD] = self.clip_grad\n        state_dict[BASE_OPTIMIZER_STATE] = self.optimizer.state_dict()\n        state_dict[SINGLE_PARTITION_OF_FP32_GROUPS] = self.fp32_groups_flat_partition\n        state_dict[GROUP_PADDINGS] = self.group_paddings\n        state_dict[PARTITION_COUNT] = self.partition_count\n        state_dict[DS_VERSION] = version\n        state_dict[PARAM_SLICE_MAPPINGS] = self._param_slice_mappings\n\n        return state_dict\n\n    # Restore base optimizer fp32 weights bfloat16 weights\n    def _restore_from_bit16_weights(self):\n        for i, group in enumerate(self.bf16_groups):\n            partition_id = dist.get_rank(group=self.real_dp_process_group[i])\n            for bf16_partitions, fp32_partition in zip(self.bf16_partitioned_groups, self.fp32_groups_flat_partition):\n                fp32_partition.data.copy_(bf16_partitions[partition_id].data)\n\n    def refresh_fp32_params(self):\n        self._restore_from_bit16_weights()\n\n    def load_state_dict(self,\n                        state_dict_list,\n                        checkpoint_folder,\n                        load_optimizer_states=True,\n                        load_from_fp32_weights=False,\n                        load_serial=None):\n        if checkpoint_folder:\n            self._load_universal_checkpoint(checkpoint_folder, load_optimizer_states, load_from_fp32_weights)\n        else:\n            self._load_legacy_checkpoint(state_dict_list, load_optimizer_states, load_from_fp32_weights)\n\n    def _load_legacy_checkpoint(self, state_dict_list, load_optimizer_states=True, load_from_fp32_weights=False):\n\n        dp_rank = dist.get_rank(group=self.dp_process_group)\n        current_rank_sd = state_dict_list[dp_rank]\n\n        ckpt_version = current_rank_sd.get(DS_VERSION, False)\n        assert ckpt_version, f\"Empty ds_version in checkpoint, not clear how to proceed\"\n        ckpt_version = pkg_version.parse(ckpt_version)\n\n        self.clip_grad = current_rank_sd.get(CLIP_GRAD, self.clip_grad)\n\n        if load_optimizer_states:\n            print(f\"_load_legacy_checkpoint current_rank_sd[BASE_OPTIMIZER_STATE]\")\n            self.optimizer.load_state_dict(current_rank_sd[BASE_OPTIMIZER_STATE])\n\n        if load_from_fp32_weights:\n            for current, saved in zip(self.fp32_groups_flat_partition,\n                                      current_rank_sd[SINGLE_PARTITION_OF_FP32_GROUPS]):\n                src_tensor = _get_padded_tensor(saved, current.numel())\n                current.data.copy_(src_tensor.data)\n\n        if load_optimizer_states:\n            self._link_all_hp_params()\n\n    def _load_universal_checkpoint(self, checkpoint_folder, load_optimizer_states, load_from_fp32_weights):\n        self.load_hp_checkpoint_state_from_checkpoint_dir(\"bf16_groups\", checkpoint_folder)\n\n    def _load_global_state(self, sd):\n        pass\n\n    @property\n    def param_groups(self):\n        \"\"\"Forward the wrapped optimizer's parameters.\"\"\"\n        return self.optimizer.param_groups\n\n    def accumulate_hp_grads_and_remove_lp(self, lp_param, group_idx, param_idx):\n        assert self.immediate_grad_update\n        self._update_hp_grad(lp_param, group_idx, param_idx, clear_lp_grads=True)\n\n    def create_grad_acc_hooks(self):\n        self.grad_accs = []\n        for i, param_group in enumerate(self.bf16_groups):\n            for j, param in enumerate(param_group):\n                if param.requires_grad:\n\n                    def wrapper(param, i, j):\n                        param_tmp = param.expand_as(param)\n                        grad_acc = param_tmp.grad_fn.next_functions[0][0]\n\n                        def accumulate_hp_grads_and_remove_lp(*notneeded):\n                            self.accumulate_hp_grads_and_remove_lp(param, i, j)\n\n                        grad_acc.register_hook(accumulate_hp_grads_and_remove_lp)\n                        self.grad_accs.append(grad_acc)\n\n                    wrapper(param, i, j)\n\n\ndef _get_padded_tensor(src_tensor, size):\n    if src_tensor.numel() >= size:\n        return src_tensor\n    padded_tensor = torch.zeros(size, dtype=src_tensor.dtype, device=src_tensor.device)\n    slice_tensor = torch.narrow(padded_tensor, 0, 0, src_tensor.numel())\n    slice_tensor.data.copy_(src_tensor.data)\n    return padded_tensor\n", "deepspeed/runtime/quantize.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nimport math\nfrom deepspeed.utils import logger\nfrom deepspeed.ops.quantizer import ds_quantizer\n\nTWO_D_PARAMS = 6\n\n\nclass Quantizer(object):\n\n    def __init__(self,\n                 q_groups=1,\n                 q_mixed_fp16=False,\n                 q_change_ratio=0.01,\n                 q_type=0,\n                 q_rounding=0,\n                 q_verbose=False,\n                 q_eigenvalue=False,\n                 use_quantizer_kernel=False,\n                 layer_num=0):\n\n        self.q_groups = q_groups\n        self.q_mixed_fp16 = q_mixed_fp16\n        self.q_change_ratio = q_change_ratio\n        self.q_type = q_type\n        self.qsteps = 0\n        self.quantize_real_ratio = 1.000\n        self.q_verbose = q_verbose\n        self.q_eigenvalue = q_eigenvalue\n        self.use_quantizer_kernel = use_quantizer_kernel\n        self.q_rounding = q_rounding\n        self.layer_num = layer_num\n\n    def any_precision_switch(self):\n        # Temporary disabled functionality\n        if self.layer_num == 0:\n            return True\n        result = False\n        for index in range(self.layer_num):\n            if self.q_start_bits[index] != self.q_target_bits:\n                next_step = self.qsteps + (TWO_D_PARAMS * (self.layer_num if self.layer_num != 0 else 1))\n                if next_step >= self.q_period[index]:\n                    result = True\n        return result\n\n    def quantize(self, parameter_group, overflow, eigenvalue_enabled, block_eigenvalue={}):\n\n        if overflow and not eigenvalue_enabled:\n            return\n\n        self.step()\n\n        self.update_fp16_ratio()\n\n        for i in range(len(parameter_group)):\n            for p in parameter_group[i]:\n                if len(p.size()) > 1 and hasattr(p, \"start_bits\") and p.start_bits:\n                    param_id = id(p)\n                    if block_eigenvalue is None:\n                        eigenvalue, layer_id = None, 0\n                    else:\n                        eigenvalue, layer_id = block_eigenvalue[param_id] if param_id in block_eigenvalue else (None,\n                                                                                                                0)\n                    if eigenvalue is not None:\n                        factor = 1 + math.floor(eigenvalue * 4)\n                        p.data = self.compute_quantization(p.data, layer_id, factor)\n                    else:\n                        p.data = self.compute_quantization(p, layer_id)\n\n    def step(self):\n        self.qsteps += 1\n\n    def quantize_highbit(self, inputs, num_bits):\n\n        q_range = 2**num_bits\n        input_flat = inputs.reshape(self.q_groups, -1)\n        g_min = input_flat.amin(dim=-1, keepdim=True)\n        g_max = input_flat.amax(dim=-1, keepdim=True)\n\n        # Random number generator (Uniform)\n        if self.q_rounding == 'nearest':\n            p = 0.\n        else:\n            p = input_flat.new(input_flat.shape).uniform_(-0.5, 0.5)\n\n        if self.q_type == 'symmetric':\n            scale = 2 * torch.max(torch.abs(g_min), torch.abs(g_max)) / q_range\n            zero_point = 0.\n            input_flat = (input_flat / scale + p).round().clamp(-(q_range >> 1), (q_range >> 1) - 1) * scale\n        elif self.q_type == 'asymmetric':\n            scale = (g_max - g_min) / q_range\n            zero_point = (g_min / scale).round() * scale\n            input_flat = ((input_flat - zero_point) / scale + p).round().clamp(0, (q_range - 1)) * scale + zero_point\n        output = input_flat.reshape(inputs.shape).contiguous()\n        return output\n\n    def quantize_tenary(self, inputs):\n        input_flat = inputs.reshape(self.q_groups, -1)\n        n = input_flat.shape[1]\n        m = input_flat.norm(p=1, dim=1).div(n)\n        thres = (0.7 * m).view(-1, 1)  #.expand_as(input_flat)\n        pos = (input_flat > thres).type(inputs.type())\n        neg = (input_flat < -thres).type(inputs.type())\n        mask = (input_flat.abs() > thres).type(inputs.type())\n        alpha = ((mask * input_flat).abs().sum(dim=1) / mask.sum(dim=1)).view(-1, 1)\n        output = alpha * pos - alpha * neg\n        output = output.reshape(inputs.shape).contiguous()\n        return output\n\n    def quantize_binary(self, inputs):\n        input_flat = inputs.reshape(self.q_groups, -1)\n        n = input_flat.shape[1]\n        m = input_flat.norm(p=1, dim=1, keepdim=True).div(n)\n        output = input_flat.sign().mul(m)\n        output = output.reshape(inputs.shape).contiguous()\n        return output\n\n    def mixed_fp16_quantize(self, input, input_q, index):\n        if self.q_mixed_fp16 and self.q_start_bits[index] >= (self.q_target_bits - 1):\n            input_q = input * self.quantize_real_ratio + (1 - self.quantize_real_ratio) * input_q\n            return input_q\n        return input_q\n\n    def compute_quantization(self, input, index=0, factor=1):\n        # fixing the quantization bits based on the training steps\n        # when reducing 1 bit at each period, we increase the period\n        # to go slowly toward the target quantization bits\n        # the period and starting bit can be configured\n\n        if input.start_bits != input.target_bits:\n            if self.qsteps >= input.q_period:\n                self.quantize_real_ratio = 1.0\n                input.q_period <<= 1\n                input.q_period *= factor\n                input.start_bits -= 1\n                if self.q_verbose:\n                    logger.info(\n                        f'Quantization settings: current bit-precision = {input.start_bits}, step = {self.qsteps}, quantization period = {input.q_period}, index = {index}'\n                    )\n        assert (input.start_bits >= input.target_bits), \\\n            'Quantization bit is lower than target precision bits!'\n\n        if self.use_quantizer_kernel:\n            if input.start_bits <= 2:\n                raise ValueError('Quantization bit is too low, please do it without quantization kernel!')\n            input_q = ds_quantizer(input.data.clone(),\n                                   self.q_groups,\n                                   input.start_bits,\n                                   asym=False if self.q_type == 'symmetric' else True,\n                                   sr=False if self.q_rounding == 'nearest_neighbor' else True)\n        else:\n            if input.start_bits >= 3:\n                input_flat = self.quantize_highbit(input.data, input.start_bits)\n            elif input.start_bits == 2:\n                assert self.q_type == 'symmetric', 'Quantization type is not symmetric!'\n                assert self.q_rounding == 'nearest', 'Quantization rounding is not nearest_neighbor!'\n                input_flat = self.quantize_tenary(input.data)\n            elif input.start_bits == 1:\n                assert self.q_type == 'symmetric', 'Quantization type is not symmetric!'\n                assert self.q_rounding == 'nearest', 'Quantization rounding is not nearest_neighbor!'\n                input_flat = self.quantize_binary(input.data)\n        if self.use_quantizer_kernel:\n            return self.mixed_fp16_quantize(input.data, input_q, index)\n        else:\n            if self.q_mixed_fp16 and input.start_bits >= input.target_bits - 1:\n                input_flat = self.quantize_real_ratio * input.data + \\\n                              (1 - self.quantize_real_ratio) * input_flat\n            return input_flat\n\n    def update_fp16_ratio(self):\n        if self.q_mixed_fp16:\n            if self.quantize_real_ratio > 0:\n                self.quantize_real_ratio -= self.q_change_ratio\n            else:\n                self.quantize_real_ratio = 0.000\n", "deepspeed/runtime/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n", "deepspeed/runtime/compression/cupy.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport cupy\nfrom torch.utils.dlpack import to_dlpack\nfrom torch.utils.dlpack import from_dlpack\n\n\nclass CupyBackend(object):\n\n    def __init__(self):\n        pass\n\n    def torch2cupy(self, tensor):\n        return cupy.fromDlpack(to_dlpack(tensor))\n\n    def cupy2torch(self, cupy_tensor):\n        return from_dlpack(cupy_tensor.toDlpack())\n\n    def compress_by_chunk(self, cupy_bool_tensor, num_chunks):\n        packed_sign = cupy.packbits(cupy_bool_tensor)\n        sign_list_packed = cupy.split(packed_sign, num_chunks)\n        cupy.cuda.get_current_stream().synchronize()\n        return sign_list_packed\n", "deepspeed/runtime/compression/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n'''Copyright The Microsoft DeepSpeed Team'''\n", "deepspeed/runtime/comm/coalesced_collectives.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\"\"\"\nbatched collective operations for overhead amortization and better\nbandwidth utilization\n\"\"\"\n\nimport math\nfrom typing import List\nimport torch\nfrom torch import Tensor\nfrom deepspeed import comm as dist\nfrom deepspeed.comm import ProcessGroup, all_to_all_single\nfrom deepspeed.accelerator import get_accelerator\nfrom deepspeed.utils import instrument_w_nvtx\nfrom deepspeed.ops import op_builder\nfrom deepspeed.utils import logger\n\n\ndef _torch_reduce_scatter_fn(input_tensor: Tensor, output_tensor: Tensor, group=None, async_op=False, prof=False):\n    return instrument_w_nvtx(dist.reduce_scatter_fn)(output_tensor, input_tensor, group=group, async_op=False)\n\n\nquantizer_module = None\n\n\n@instrument_w_nvtx\n@torch.no_grad()\ndef all_to_all_quant_reduce(tensors: List[Tensor], groups: {}) -> List[Tensor]:\n    global quantizer_module\n    if quantizer_module is None:\n        quantizer_module = op_builder.QuantizerBuilder().load()\n    local_world_size = get_accelerator().device_count()\n    global_world_size = dist.get_world_size()\n    num_nodes = global_world_size // local_world_size\n    this_rank = dist.get_rank()\n    intra_idx = int(this_rank / local_world_size)\n    inter_idx = this_rank % local_world_size\n    output_lst: List[Tensor] = [None] * len(tensors)\n    for idx, tensor in enumerate(tensors):\n        if tensor.dim() == 1:\n            output_lst[idx] = reduce_scatter_coalesced([tensor])[0]\n        elif tensor.numel() % (2 * global_world_size) != 0:\n            # Due to the constraint of 2-stage all-to-all, the input tensor must be divisible by 2 * global_world_size\n            # Otherwise, all-to-all cannot be performed because of shape mismatch.\n            # See more at https://github.com/microsoft/DeepSpeed/pull/5056\n            logger.warning(\n                f\"qgZ falls back to reduce_scatter because tensor size = {tensor.numel()} is not divisible by (2 * global_world_size) = {2 * global_world_size}. Please consider allocating a new world to enable qgZ\"\n            )\n            output_lst[idx] = reduce_scatter_coalesced([tensor])[0]\n        else:\n            intra_quant_group = max(tensor.shape[0], tensor.shape[1], global_world_size)\n\n            inter_quant_group = intra_quant_group // local_world_size\n            intra_quant_int4, intra_q_scales = quantizer_module.swizzle_quant(tensor, intra_quant_group, 4,\n                                                                              quantizer_module.Symmetric, 1, num_nodes,\n                                                                              local_world_size)\n            local_output = torch.empty_like(intra_quant_int4)\n            scale_output = torch.empty_like(intra_q_scales)\n            all_to_all_single(local_output, intra_quant_int4, group=groups[f'local_{intra_idx}'])\n            all_to_all_single(scale_output, intra_q_scales, group=groups[f'local_{intra_idx}'])\n            global_input_tensor, global_scales = quantizer_module.quantized_reduction(\n                local_output, scale_output, intra_quant_group, inter_quant_group, 4, quantizer_module.Symmetric,\n                local_world_size)\n            global_output = torch.empty_like(global_input_tensor)\n            global_scale_output = torch.empty_like(global_scales)\n            all_to_all_single(global_output, global_input_tensor, group=groups[f'global_{inter_idx}'])\n            all_to_all_single(global_scale_output, global_scales, group=groups[f'global_{inter_idx}'])\n            final_output = quantizer_module.dequantize(global_output, global_scale_output, global_scale_output.numel(),\n                                                       4, quantizer_module.Symmetric)\n            assert final_output.numel(\n            ) % num_nodes == 0, f\"final_output.numel()={final_output.numel()} is not divisible by num_nodes={num_nodes}\"\n            output_lst[idx] = (sum(list(final_output.chunk(num_nodes))) / num_nodes).view(-1)\n    return output_lst\n\n\n@instrument_w_nvtx\n@torch.no_grad()\ndef reduce_scatter_coalesced(\n    tensors: List[Tensor],\n    group: ProcessGroup = None,\n) -> List[Tensor]:\n    \"\"\"simultaneously reduce-scatter a list of tensors - this can be done more\n    efficiently than individual reduce scatter calls\n    TODO. see if PyTorch team wants a c++ version of this for ProcessGroupNCCL\n    \"\"\"\n    this_rank = dist.get_rank(group)\n    world_sz = dist.get_world_size(group)\n\n    partition_lst_for_each_tensor = [None] * len(tensors)\n    for tensor_idx, tensor in enumerate(tensors):\n        flattened_tensor = tensor.view(-1)\n        chunk_sz = math.ceil(tensor.numel() / world_sz)\n        partition_lst_for_each_tensor[tensor_idx] = [\n            flattened_tensor[rank * chunk_sz:rank * chunk_sz + chunk_sz] for rank in range(0, world_sz)\n        ]\n\n    padded_partition_sz_for_each_tensor = tuple(math.ceil(t.numel() / world_sz) for t in tensors)\n\n    if len(tensors) == 1 and tensors[0].numel() % world_sz == 0:\n        # if there's only one tensor being reduced and we don't need to pad\n        # we have an opportunity to avoid a memory allocation\n        tensor_partition_flat_buffer = tensors[0].view(-1)\n    else:\n        # interleave tensor partitions such that the correct reduced partitions of each tensor\n        # end up at each rank\n        tensor_partitions_lst_with_padding = []\n        for rank in range(world_sz):\n            for tensor_idx in range(len(tensors)):\n                # add tensor content\n                tensor_chunk = partition_lst_for_each_tensor[tensor_idx][rank]\n                tensor_partitions_lst_with_padding.append(tensor_chunk)\n\n                # add padding if necessary\n                padding_sz = padded_partition_sz_for_each_tensor[tensor_idx] - tensor_chunk.numel()\n                if padding_sz > 0:\n                    tensor_partitions_lst_with_padding.append(\n                        torch.empty(padding_sz, dtype=tensor_chunk.dtype, device=tensor_chunk.device))\n\n        tensor_partition_flat_buffer = instrument_w_nvtx(torch.cat)(tensor_partitions_lst_with_padding)\n\n    tensor_partition_flat_buffer.div_(world_sz)  # pre-divide\n    tensor_partition_buffer_for_each_rank: List[Tensor] = torch.chunk(tensor_partition_flat_buffer, world_sz)\n\n    # batched reduce-scatter call\n    _torch_reduce_scatter_fn(tensor_partition_flat_buffer,\n                             tensor_partition_buffer_for_each_rank[this_rank],\n                             group=group)\n\n    # reverse procedure of the interleaving done previously, done on the\n    # result of the batched reduce-scatter\n    output_lst: List[Tensor] = [None] * len(tensors)\n    offset = 0\n    for tensor_idx in range(len(tensors)):\n        output_lst[tensor_idx] = tensor_partition_buffer_for_each_rank[this_rank].narrow(\n            0, offset, partition_lst_for_each_tensor[tensor_idx][this_rank].numel())\n\n        offset += padded_partition_sz_for_each_tensor[tensor_idx]\n    return output_lst\n", "deepspeed/runtime/comm/mpi.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nimport cupy\nimport time\nimport numpy as np\nfrom mpi4py import MPI\n\nfrom deepspeed.runtime.compression.cupy import CupyBackend\n\n\nclass MpiBackend(object):\n\n    def __init__(self, cuda_aware):\n        self.comm = MPI.COMM_WORLD\n        self.rank = self.comm.Get_rank()\n        self.size = self.comm.Get_size()\n        self.cuda_aware = cuda_aware\n        self.compression_backend = CupyBackend()\n\n    def my_igather(self, rank, size, comm, sendbuf, recbuf, root):\n        req = []\n        if rank == root:\n            for idx in range(size):\n                if idx != rank:\n                    req.append(comm.Irecv(recbuf[idx], source=idx))\n                else:\n                    recbuf[rank] = sendbuf\n        else:\n            req.append(comm.Isend(sendbuf, dest=root))\n        return req\n\n    def gather_cuda(self, rank, world_size, comm, cupy_sign_list_packed, cupy_recvbuf_sign, cupy_worker_scale,\n                    cupy_recvbuf_scale):\n        # We do in-place operations on cupy buffers so we do not return any buffers\n        requests = []\n        for idx in range(world_size):\n            req_sign = self.my_igather(rank, world_size, comm, cupy_sign_list_packed[idx], cupy_recvbuf_sign, root=idx)\n            requests += req_sign\n\n        for idx in range(world_size):\n            req_scale = self.my_igather(rank, world_size, comm, cupy_worker_scale, cupy_recvbuf_scale, root=idx)\n            requests += req_scale\n\n        MPI.Request.Waitall(requests)\n\n    def gather_host(self, rank, world_size, comm, cupy_sign_list_packed, cupy_recvbuf_sign, cupy_worker_scale,\n                    cupy_recvbuf_scale):\n\n        # In-place operations are not possible for newly created cupy arrays\n        # so we need to return the new buffers\n        numpy_recvbuf_sign = np.zeros([world_size, cupy_sign_list_packed[rank].size],\n                                      dtype=cupy_sign_list_packed[0].dtype)\n        numpy_recvbuf_scale = np.zeros([world_size, 1], dtype=cupy_worker_scale.dtype)\n\n        # 1. convert from cupy to numpy\n        numpy_sign_list_packed = cupy_sign_list_packed\n\n        for idx in range(world_size):\n            numpy_sign_list_packed[idx] = cupy.asnumpy(cupy_sign_list_packed[idx])\n\n        numpy_worker_scale = cupy.asnumpy(cupy_worker_scale)\n        numpy_recvbuf_scale = cupy.asnumpy(cupy_recvbuf_scale)\n\n        cupy.cuda.get_current_stream().synchronize()\n\n        # 2. use numpy buffers for communication\n        requests = []\n\n        for idx in range(world_size):\n            req_sign = self.my_igather(rank,\n                                       world_size,\n                                       comm,\n                                       numpy_sign_list_packed[idx],\n                                       numpy_recvbuf_sign,\n                                       root=idx)\n            requests += req_sign\n\n        for idx in range(world_size):\n            req_scale = self.my_igather(rank, world_size, comm, numpy_worker_scale, numpy_recvbuf_scale, root=idx)\n            requests += req_scale\n\n        MPI.Request.Waitall(requests)\n\n        # 3. Convert back from numpy to cupy\n        cupy_recvbuf_sign = cupy.asarray(numpy_recvbuf_sign)\n        for idx in range(world_size):\n            cupy_sign_list_packed[idx] = cupy.asarray(numpy_sign_list_packed[idx])\n\n        cupy_worker_scale = cupy.asarray(numpy_worker_scale)\n        cupy_recvbuf_scale = cupy.asarray(numpy_recvbuf_scale)\n        cupy.cuda.get_current_stream().synchronize()\n\n        return cupy_sign_list_packed, cupy_recvbuf_sign, cupy_worker_scale, cupy_recvbuf_scale\n\n    def allgather_cuda(self, comm, cupy_server_sign_packed, cupy_recvbuf_sign_server, cupy_server_scale,\n                       cupy_recvbuf_scale_server):\n        comm.Allgather(cupy_server_sign_packed, cupy_recvbuf_sign_server)\n        comm.Allgather(cupy_server_scale, cupy_recvbuf_scale_server)\n\n    def allgather_host(self, comm, cupy_server_sign_packed, cupy_recvbuf_sign_server, cupy_server_scale,\n                       cupy_recvbuf_scale_server):\n\n        # 1. Convert cupy to numpy\n        numpy_recvbuf_sign_server = np.zeros([comm.Get_size(), cupy_server_sign_packed.size],\n                                             dtype=cupy_server_sign_packed.dtype)\n        numpy_recvbuf_scale_server = np.zeros([comm.Get_size(), 1], dtype=cupy_server_scale.dtype)\n\n        numpy_server_sign_packed = cupy.asnumpy(cupy_server_sign_packed)\n        numpy_recvbuf_sign_server = cupy.asnumpy(cupy_recvbuf_sign_server)\n        numpy_server_scale = cupy.asnumpy(cupy_server_scale)\n        numpy_recvbuf_scale_server = cupy.asnumpy(cupy_recvbuf_scale_server)\n        cupy.cuda.get_current_stream().synchronize()\n\n        # 2. Communicate numpy buffers\n        comm.Allgather(numpy_server_sign_packed, numpy_recvbuf_sign_server)\n        comm.Allgather(numpy_server_scale, numpy_recvbuf_scale_server)\n        comm.Barrier()\n\n        # 3. Convert numpy back to cupy\n        cupy_server_sign_packed = cupy.asarray(numpy_server_sign_packed)\n        cupy_recvbuf_sign_server = cupy.asarray(numpy_recvbuf_sign_server)\n        cupy_server_scale = cupy.asarray(numpy_server_scale)\n        cupy_recvbuf_scale_server = cupy.asarray(numpy_recvbuf_scale_server)\n        cupy.cuda.get_current_stream().synchronize()\n\n        return cupy_server_sign_packed, cupy_recvbuf_sign_server, cupy_server_scale, cupy_recvbuf_scale_server\n\n    def compressed_allreduce(self, buffer_m: torch.tensor, worker_error, server_error, local_rank):\n\n        all_start_time = time.time()\n        original_shape = buffer_m.size()\n        if len(original_shape) > 1:\n            buffer_m = torch.flatten(buffer_m)\n        original_size = buffer_m.numel()\n        worker_error_size = worker_error.numel()\n        cupy.cuda.Device(local_rank).use()\n\n        if original_size != worker_error_size:\n            empty_tensor = torch.zeros(worker_error_size - original_size, device=buffer_m.device)\n            buffer_m = torch.cat([buffer_m, empty_tensor])\n\n        buffer_m.add_(worker_error)\n        worker_scale = torch.linalg.norm(buffer_m) / np.sqrt(torch.numel(buffer_m))\n        worker_error.set_(buffer_m - worker_scale * buffer_m.sign().add_(1).bool().float().add_(-0.5).mul_(2.0))\n\n        cupy_sign_list_packed = self.compression_backend.compress_by_chunk(\n            self.compression_backend.torch2cupy(buffer_m.sign_().add_(1).bool()), self.size)\n        cupy_worker_scale = self.compression_backend.torch2cupy(worker_scale)\n\n        cupy_recvbuf_sign = cupy.zeros([self.size, cupy_sign_list_packed[self.rank].size],\n                                       dtype=cupy_sign_list_packed[0].dtype)\n        cupy_recvbuf_scale = cupy.zeros([self.size, 1], dtype=cupy_worker_scale.dtype)\n\n        # Communication Phase 1\n        gather_start = time.time()\n        if self.cuda_aware:\n            self.gather_cuda(self.rank, self.size, self.comm, cupy_sign_list_packed, cupy_recvbuf_sign,\n                             cupy_worker_scale, cupy_recvbuf_scale)\n        else:\n            _, cupy_recvbuf_sign, _, cupy_recvbuf_scale = self.gather_host(self.rank, self.size, self.comm,\n                                                                           cupy_sign_list_packed, cupy_recvbuf_sign,\n                                                                           cupy_worker_scale, cupy_recvbuf_scale)\n        gather_end = time.time()\n\n        # cupy_sign_list_packed, cupy_worker_scale, worker_scale = None, None, None\n        cupy_sign_list_packed = None\n\n        compensated_server_m = self.compression_backend.cupy2torch(\n            (cupy.unpackbits(cupy_recvbuf_sign.flatten())).reshape(self.size, -1)).float().add_(-0.5).mul_(2.0).mul_(\n                self.compression_backend.cupy2torch(cupy_recvbuf_scale).mul_(1 / self.size)).sum(0)\n        compensated_server_m.add_(server_error)\n        server_scale = torch.linalg.norm(compensated_server_m) / np.sqrt(compensated_server_m.numel())\n        server_error.set_(compensated_server_m -\n                          server_scale * compensated_server_m.sign().add_(1).bool().float().add_(-0.5).mul_(2.0))\n\n        cupy_server_scale = self.compression_backend.torch2cupy(server_scale)\n\n        cupy_server_sign_packed = self.compression_backend.compress_by_chunk(\n            self.compression_backend.torch2cupy(compensated_server_m.sign_().add_(1).bool()), 1)\n        compensated_server_m = None\n\n        cupy_recvbuf_sign_server = cupy.zeros([self.size, cupy_server_sign_packed[0].size],\n                                              dtype=cupy_recvbuf_sign.dtype)\n        cupy_recvbuf_scale_server = cupy.zeros([self.size, 1], dtype=cupy_recvbuf_scale.dtype)\n        # cupy_recvbuf_sign, cupy_recvbuf_scale = None, None\n        cupy_recvbuf_sign = None\n\n        # Communication Phase 2\n        if self.cuda_aware:\n            self.allgather_cuda(self.comm, cupy_server_sign_packed[0], cupy_recvbuf_sign_server, cupy_server_scale,\n                                cupy_recvbuf_scale_server)\n        else:\n            _, cupy_recvbuf_sign_server, _, cupy_recvbuf_scale_server = self.allgather_host(\n                self.comm, cupy_server_sign_packed[0], cupy_recvbuf_sign_server, cupy_server_scale,\n                cupy_recvbuf_scale_server)\n\n        # cupy_server_sign_packed, cupy_server_scale, server_scale = None, None, None\n        cupy_server_sign_packed = None\n\n        buffer_m.data.copy_(\n            self.compression_backend.cupy2torch((cupy.unpackbits(cupy_recvbuf_sign_server.flatten())).reshape(\n                self.size, -1)).float().add_(-0.5).mul_(2.0).mul_(\n                    self.compression_backend.cupy2torch(cupy_recvbuf_scale_server)).flatten().data)\n        if original_size != worker_error_size:\n            buffer_m = buffer_m[0:original_size]\n        if len(original_shape) > 1:\n            buffer_m = buffer_m.reshape(original_shape)\n\n        # cupy_recvbuf_sign_server, cupy_recvbuf_scale_server = None, None\n\n        return buffer_m\n", "deepspeed/runtime/comm/hccl.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport numpy as np\nimport torch\nimport torch_npu\nimport deepspeed.comm as dist\n\n\nclass HcclBackend(object):\n\n    def __init__(self, mpu=None):\n        if mpu is None:\n            self.world_group = dist.new_group(ranks=range(dist.get_world_size()))\n        else:\n            self.mpu = mpu\n            self.world_group = self.mpu.get_data_parallel_group()\n        self.size = dist.get_world_size(group=self.world_group)\n        self.rank = dist.get_rank(group=self.world_group)\n\n    def my_igather(self, rank, size, group, sendbuf, recvbuf, root):\n        req = []\n        if rank == root:\n            for idx in range(size):\n                if idx != rank:\n                    req.append(dist.irecv(recvbuf[idx], src=idx, group=group))\n                else:\n                    recvbuf[rank] = sendbuf\n        else:\n            req.append(dist.isend(sendbuf, group=group, dst=root))\n        return req\n\n    def my_gather(self, rank, size, group, sendbuf, recvbuf, root):\n        if rank == root:\n            for idx in range(size):\n                if idx != rank:\n                    dist.recv(recvbuf[idx], src=idx, group=group)\n                else:\n                    recvbuf[rank] = sendbuf\n        else:\n            dist.send(sendbuf, group=group, dst=root)\n\n    def compressed_allreduce(self, buffer_m: torch.tensor, worker_error, server_error, local_rank):\n        original_shape = buffer_m.size()\n        if len(original_shape) > 1:\n            buffer_m = torch.flatten(buffer_m)\n\n        # align size of original_buffer and error\n        original_size = buffer_m.numel()\n        worker_error_size = worker_error.numel()\n        if original_size != worker_error_size:\n            empty_tensor = torch.zeros(worker_error_size - original_size, device=buffer_m.device)\n            buffer_m = torch.cat([buffer_m, empty_tensor])\n\n        buffer_m.add_(worker_error)\n        worker_scale = torch.linalg.norm(buffer_m) / np.sqrt(torch.numel(buffer_m))\n\n        worker_error.set_(buffer_m - worker_scale * buffer_m.sign().add_(1).bool().float().add_(-0.5).mul_(2.0))\n\n        sign_list_packed_tmp = torch_npu.npu_sign_bits_pack(buffer_m, self.size).type(torch.int8)\n\n        recvbuf_sign = torch.zeros([self.size, len(sign_list_packed_tmp[self.rank])],\n                                   dtype=sign_list_packed_tmp[0].dtype,\n                                   device=sign_list_packed_tmp.device)\n\n        sign_list_packed = [sign_list_packed_tmp[idx] for idx in range(self.size)]\n\n        recvbuf_scale = [\n            torch.zeros(1, dtype=worker_scale.dtype, device=torch.device(local_rank)) for _ in range(self.size)\n        ]\n\n        # communication phase 1\n        # all to all for sign\n        dist.all_to_all_single(recvbuf_sign, torch.stack(sign_list_packed), group=self.world_group)\n        # all gather for scale\n        dist.all_gather(recvbuf_scale, worker_scale, group=self.world_group)\n\n        flattened_recvbuf_sign = recvbuf_sign.type(torch.uint8).flatten()\n        compensated_server_m = torch_npu.npu_sign_bits_unpack(flattened_recvbuf_sign, self.size, torch.float32) \\\n            .mul_(torch.stack(recvbuf_scale).mul_(1 / self.size)).sum(0)\n\n        compensated_server_m.add_(server_error)\n\n        server_scale = torch.norm(compensated_server_m) / np.sqrt(compensated_server_m.numel())\n\n        server_error.set_(compensated_server_m -\n                          server_scale * compensated_server_m.sign().add_(1).bool().float().add_(-0.5).mul_(2.0))\n\n        server_sign_packed = torch_npu.npu_sign_bits_pack(compensated_server_m, 1).type(torch.int8)\n\n        # recvbuf_sign_server\n        recvbuf_sign_server_tmp = torch.zeros([self.size, len(server_sign_packed[0])],\n                                              dtype=recvbuf_sign.dtype,\n                                              device=server_sign_packed.device)\n\n        recvbuf_sign_server = [recvbuf_sign_server_tmp[idx] for idx in range(self.size)]\n\n        # recvbuf_scale_server\n        recvbuf_scale_server_tmp = torch.zeros([self.size, 1],\n                                               dtype=worker_scale.dtype,\n                                               device=server_sign_packed.device)\n\n        recvbuf_scale_server = [recvbuf_scale_server_tmp[idx] for idx in range(self.size)]\n\n        # communication Phase 2\n        dist.all_gather(recvbuf_sign_server, server_sign_packed[0], group=self.world_group)\n        dist.all_gather(recvbuf_scale_server, server_scale, group=self.world_group)\n\n        recvbuf_sign_server = torch.stack(recvbuf_sign_server)\n\n        flattened_recvbuf_sign_server = recvbuf_sign_server.type(torch.uint8).flatten()\n\n        buffer_m.data.copy_(\n            torch_npu.npu_sign_bits_unpack(flattened_recvbuf_sign_server, self.size,\n                                           torch.float32).mul_(recvbuf_scale_server_tmp).flatten().data)\n\n        if original_size != worker_error_size:\n            buffer_m = buffer_m[0:original_size]\n        if len(original_shape) > 1:\n            buffer_m = buffer_m.reshape(original_shape)\n\n        return buffer_m\n", "deepspeed/runtime/comm/compressed.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport numpy as np\nimport torch\nimport deepspeed.comm as dist\nfrom deepspeed.accelerator import get_accelerator\nfrom deepspeed.ops.op_builder import PackbitsBuilder\n\n\nclass CompressedBackend(object):\n\n    def __init__(self, mpu=None):\n        if mpu is None:\n            self.world_group = dist.new_group(ranks=range(dist.get_world_size()))\n        else:\n            self.mpu = mpu\n            self.world_group = self.mpu.get_data_parallel_group()\n        self.size = dist.get_world_size(group=self.world_group)\n        self.rank = dist.get_rank(group=self.world_group)\n        self.packer = PackbitsBuilder().load()\n\n    def my_igather(self, rank, size, group, sendbuf, recvbuf, root):\n        req = []\n        if rank == root:\n            for idx in range(size):\n                if idx != rank:\n                    req.append(dist.irecv(recvbuf[idx], src=idx, group=group))\n                else:\n                    recvbuf[rank] = sendbuf\n        else:\n            req.append(dist.isend(sendbuf, group=group, dst=root))\n        return req\n\n    def my_gather(self, rank, size, group, sendbuf, recvbuf, root):\n        if rank == root:\n            for idx in range(size):\n                if idx != rank:\n                    dist.recv(recvbuf[idx], src=idx, group=group)\n                else:\n                    recvbuf[rank] = sendbuf\n        else:\n            dist.send(sendbuf, group=group, dst=root)\n\n    def pack(self, buffer, size):\n        # pack float tensor into uint8 tensor\n        packed = self.packer.packbits(buffer.float(), buffer.numel(), self.rank)\n        return packed.reshape(size, -1)\n\n    def unpack(self, buffer, size, dtype):\n        # unpack uint8 to float tensor\n        unpacked = self.packer.unpackbits(buffer, buffer.numel(), self.rank)\n        return unpacked.reshape(size, -1).to(dtype)\n\n    def compressed_allreduce(self, buffer_m: torch.tensor, worker_error, server_error, local_rank):\n        original_shape = buffer_m.size()\n        if len(original_shape) > 1:\n            buffer_m = torch.flatten(buffer_m)\n\n        # align size of original_buffer and error\n        original_size = buffer_m.numel()\n        worker_error_size = worker_error.numel()\n        if original_size != worker_error_size:\n            empty_tensor = torch.zeros(worker_error_size - original_size, device=buffer_m.device)\n            buffer_m = torch.cat([buffer_m, empty_tensor])\n\n        buffer_m.add_(worker_error)\n        worker_scale = torch.linalg.norm(buffer_m) / np.sqrt(torch.numel(buffer_m))\n\n        worker_error.set_(buffer_m - worker_scale * buffer_m.sign().add_(1).bool().float().add_(-0.5).mul_(2.0))\n\n        sign_list_packed_tmp = self.pack(buffer_m, self.size).type(torch.int8)\n\n        recvbuf_sign = torch.zeros([self.size, len(sign_list_packed_tmp[self.rank])],\n                                   dtype=sign_list_packed_tmp[0].dtype,\n                                   device=sign_list_packed_tmp.device)\n\n        sign_list_packed = [sign_list_packed_tmp[idx] for idx in range(self.size)]\n\n        recvbuf_scale = [\n            torch.zeros(1, dtype=worker_scale.dtype, device=get_accelerator().current_device_name())\n            for _ in range(self.size)\n        ]\n\n        # communication phase 1\n        # all to all for sign\n        dist.all_to_all_single(recvbuf_sign, torch.stack(sign_list_packed), group=self.world_group)\n        # all gather for scale\n        dist.all_gather(recvbuf_scale, worker_scale, group=self.world_group)\n\n        flattened_recvbuf_sign = recvbuf_sign.type(torch.uint8).flatten()\n        compensated_server_m = self.unpack(flattened_recvbuf_sign, self.size, torch.float32) \\\n            .mul_(torch.stack(recvbuf_scale).mul_(1 / self.size)).sum(0)\n\n        compensated_server_m.add_(server_error)\n\n        server_scale = torch.norm(compensated_server_m) / np.sqrt(compensated_server_m.numel())\n\n        server_error.set_(compensated_server_m -\n                          server_scale * compensated_server_m.sign().add_(1).bool().float().add_(-0.5).mul_(2.0))\n\n        server_sign_packed = self.pack(compensated_server_m, 1).type(torch.int8)\n\n        # recvbuf_sign_server\n        recvbuf_sign_server_tmp = torch.zeros([self.size, len(server_sign_packed[0])],\n                                              dtype=recvbuf_sign.dtype,\n                                              device=server_sign_packed.device)\n\n        recvbuf_sign_server = [recvbuf_sign_server_tmp[idx] for idx in range(self.size)]\n\n        # recvbuf_scale_server\n        recvbuf_scale_server_tmp = torch.zeros([self.size, 1],\n                                               dtype=worker_scale.dtype,\n                                               device=server_sign_packed.device)\n\n        recvbuf_scale_server = [recvbuf_scale_server_tmp[idx] for idx in range(self.size)]\n\n        # communication Phase 2\n        dist.all_gather(recvbuf_sign_server, server_sign_packed[0], group=self.world_group)\n        dist.all_gather(recvbuf_scale_server, server_scale, group=self.world_group)\n\n        recvbuf_sign_server = torch.stack(recvbuf_sign_server)\n\n        flattened_recvbuf_sign_server = recvbuf_sign_server.type(torch.uint8).flatten()\n\n        buffer_m.data.copy_(\n            self.unpack(flattened_recvbuf_sign_server, self.size,\n                        torch.float32).mul_(recvbuf_scale_server_tmp).flatten().data)\n\n        if original_size != worker_error_size:\n            buffer_m = buffer_m[0:original_size]\n        if len(original_shape) > 1:\n            buffer_m = buffer_m.reshape(original_shape)\n\n        return buffer_m\n", "deepspeed/runtime/comm/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n'''Copyright The Microsoft DeepSpeed Team'''\n", "deepspeed/runtime/comm/nccl.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nfrom deepspeed import comm as dist\nimport cupy\nimport numpy as np\n\nfrom deepspeed.runtime.compression.cupy import CupyBackend\nfrom deepspeed.utils.torch import required_torch_version\nfrom deepspeed.accelerator import get_accelerator\n\n\nclass NcclBackend(object):\n\n    def __init__(self, mpu=None):\n        if mpu is None:\n            self.world_group = dist.new_group(ranks=range(dist.get_world_size()))\n        else:\n            self.mpu = mpu\n            self.world_group = self.mpu.get_data_parallel_group()\n        self.rank = dist.get_rank(group=self.world_group)\n        self.size = dist.get_world_size(group=self.world_group)\n        self.compression_backend = CupyBackend()\n        self.bool_not_supported = required_torch_version(min_version=1.10)\n\n    def my_igather(self, rank, size, group, sendbuf, recvbuf, root):\n        req = []\n        if rank == root:\n            for idx in range(size):\n                if idx != rank:\n                    req.append(dist.irecv(recvbuf[idx], src=idx, group=group))\n                else:\n                    recvbuf[rank] = sendbuf\n        else:\n            req.append(dist.isend(sendbuf, group=group, dst=root))\n        return req\n\n    def my_gather(self, rank, size, group, sendbuf, recvbuf, root):\n        if rank == root:\n            for idx in range(size):\n                if idx != rank:\n                    dist.recv(recvbuf[idx], src=idx, group=group)\n                else:\n                    recvbuf[rank] = sendbuf\n        else:\n            dist.send(sendbuf, group=group, dst=root)\n\n    def compressed_allreduce(self, buffer_m: torch.tensor, worker_error, server_error, local_rank):\n\n        # all_start_time = time.time()\n        original_shape = buffer_m.size()\n        if len(original_shape) > 1:\n            buffer_m = torch.flatten(buffer_m)\n        original_size = buffer_m.numel()\n        worker_error_size = worker_error.numel()\n        cupy.cuda.Device(local_rank).use()\n\n        if original_size != worker_error_size:\n            empty_tensor = torch.zeros(worker_error_size - original_size, device=buffer_m.device)\n            buffer_m = torch.cat([buffer_m, empty_tensor])\n\n        buffer_m.add_(worker_error)\n        worker_scale = torch.linalg.norm(buffer_m) / np.sqrt(buffer_m.numel())\n        worker_error.set_(buffer_m - worker_scale * buffer_m.sign().add_(1).bool().float().add_(-0.5).mul_(2.0))\n\n        if self.bool_not_supported:\n            cupy_sign_list_packed = self.compression_backend.compress_by_chunk(\n                self.compression_backend.torch2cupy(buffer_m.sign_().add_(1).bool().to(dtype=torch.uint8)), self.size)\n        else:\n            cupy_sign_list_packed = self.compression_backend.compress_by_chunk(\n                self.compression_backend.torch2cupy(buffer_m.sign_().add_(1).bool()), self.size)\n        cupy_worker_scale = self.compression_backend.torch2cupy(worker_scale)\n\n        cupy_recvbuf_sign = cupy.zeros([self.size, cupy_sign_list_packed[self.rank].size],\n                                       dtype=cupy_sign_list_packed[0].dtype)\n        # cupy_recvbuf_scale = cupy.zeros([self.size, 1], dtype=cupy_worker_scale.dtype)\n\n        sign_list_packed = [\n            self.compression_backend.cupy2torch(cupy_sign_list_packed[idx]) for idx in range(self.size)\n        ]\n\n        # worker_scale = self.compression_backend.cupy2torch(cupy_worker_scale)\n        recvbuf_sign = self.compression_backend.cupy2torch(cupy_recvbuf_sign)\n        #recvbuf_scale = self.compression_backend.cupy2torch(cupy_recvbuf_scale)\n        recvbuf_scale = [\n            torch.zeros(1, dtype=worker_scale.dtype, device=torch.device(get_accelerator().device_name(local_rank)))\n            for i in range(self.size)\n        ]\n\n        # communication phase 1\n        # gather_start = time.time()\n        # Alltoall for sign\n        dist.all_to_all_single(recvbuf_sign, torch.stack(sign_list_packed), group=self.world_group)\n        # Allgather for scale\n        dist.all_gather(recvbuf_scale, worker_scale, group=self.world_group)\n\n        # gather_end = time.time()\n\n        # cupy_sign_list_packed, sign_list_packed, cupy_worker_scale, worker_scale = None, None, None, None\n        cupy_sign_list_packed = None\n\n        cupy_recvbuf_sign = self.compression_backend.torch2cupy(recvbuf_sign)\n        #cupy_recvbuf_scale = self.compression_backend.torch2cupy(torch.stack(recvbuf_scale))\n\n        compensated_server_m = self.compression_backend.cupy2torch(\n            (cupy.unpackbits(cupy_recvbuf_sign.flatten())).reshape(self.size, -1)).float().add_(-0.5).mul_(2.0).mul_(\n                torch.stack(recvbuf_scale).mul_(1 / self.size)).sum(0)\n        compensated_server_m.add_(server_error)\n        server_scale = torch.linalg.norm(compensated_server_m) / np.sqrt(compensated_server_m.numel())\n        server_error.set_(compensated_server_m -\n                          server_scale * compensated_server_m.sign().add_(1).bool().float().add_(-0.5).mul_(2.0))\n\n        # cupy_server_scale = self.compression_backend.torch2cupy(server_scale)\n\n        if self.bool_not_supported:\n            cupy_server_sign_packed = self.compression_backend.compress_by_chunk(\n                self.compression_backend.torch2cupy(compensated_server_m.sign_().add_(1).bool().to(dtype=torch.uint8)),\n                1)\n        else:\n            cupy_server_sign_packed = self.compression_backend.compress_by_chunk(\n                self.compression_backend.torch2cupy(compensated_server_m.sign_().add_(1).bool()), 1)\n        compensated_server_m = None\n\n        cupy_recvbuf_sign_server = cupy.zeros([self.size, cupy_server_sign_packed[0].size],\n                                              dtype=cupy_recvbuf_sign.dtype)\n        # cupy_recvbuf_sign, recvbuf_sign = None, None\n        cupy_recvbuf_sign = None\n\n        server_sign_packed = [self.compression_backend.cupy2torch(cupy_server_sign_packed[0])]\n        recvbuf_sign_server = [\n            self.compression_backend.cupy2torch(cupy_recvbuf_sign_server[idx]) for idx in range(self.size)\n        ]\n\n        # server_scale = self.compression_backend.cupy2torch(cupy_server_scale)\n        cupy_recvbuf_scale_server = cupy.zeros([self.size, 1], dtype=cupy_worker_scale.dtype)\n        # cupy_recvbuf_scale, recvbuf_scale = None, None\n\n        recvbuf_scale_server = [\n            self.compression_backend.cupy2torch(cupy_recvbuf_scale_server[idx]) for idx in range(self.size)\n        ]\n\n        # Communication Phase 2\n        dist.all_gather(recvbuf_sign_server, server_sign_packed[0], group=self.world_group)\n        dist.all_gather(recvbuf_scale_server, server_scale, group=self.world_group)\n\n        cupy_server_sign_packed = None\n\n        # need to convert from a tensor list to a single tensor\n        # dist.all_gather only provides a tensor list as the recv/output buffer\n        recvbuf_sign_server = torch.stack(recvbuf_sign_server)\n\n        cupy_recvbuf_sign_server = self.compression_backend.torch2cupy(recvbuf_sign_server)\n\n        buffer_m.data.copy_(\n            self.compression_backend.cupy2torch((cupy.unpackbits(cupy_recvbuf_sign_server.flatten())).reshape(\n                self.size, -1)).float().add_(-0.5).mul_(2.0).mul_(\n                    self.compression_backend.cupy2torch(cupy_recvbuf_scale_server)).flatten().data)\n        if original_size != worker_error_size:\n            buffer_m = buffer_m[0:original_size]\n        if len(original_shape) > 1:\n            buffer_m = buffer_m.reshape(original_shape)\n\n        return buffer_m\n", "deepspeed/runtime/pipe/engine.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom types import MethodType\nfrom collections import OrderedDict\n\nimport torch\nfrom deepspeed import comm as dist\n\nfrom deepspeed.utils import logger\nfrom deepspeed.utils.timer import ThroughputTimer\nfrom deepspeed.accelerator import get_accelerator\nfrom deepspeed.runtime.bf16_optimizer import BF16_Optimizer\n\nfrom ..engine import DeepSpeedEngine, MEMORY_OPT_ALLREDUCE_SIZE\nfrom deepspeed.utils.timer import FORWARD_MICRO_TIMER, FORWARD_GLOBAL_TIMER, BACKWARD_MICRO_TIMER, \\\n    BACKWARD_GLOBAL_TIMER, BACKWARD_INNER_MICRO_TIMER, BACKWARD_INNER_GLOBAL_TIMER, \\\n    BACKWARD_REDUCE_MICRO_TIMER, BACKWARD_REDUCE_GLOBAL_TIMER, \\\n    STEP_MICRO_TIMER, STEP_GLOBAL_TIMER\n\nfrom ..utils import PartitionedTensor\nfrom ..dataloader import RepeatingLoader\nfrom ..zero.config import ZeroStageEnum\nfrom ..activation_checkpointing import checkpointing as ds_checkpointing\n\nfrom .module import PipelineModule, PipelineError\nfrom . import p2p\nfrom . import schedule\n\nTARGET_ID = -2\nLOG_STAGE = -2\nDATA_PARALLEL_ID = -2\n\nBATCH_INPUT_TIMER = 'batch_input'\nTRAIN_BATCH_TIMER = 'train_batch'\nPIPE_SEND_OUTPUT_TIMER = 'pipe_send_output'\nPIPE_SEND_GRAD_TIMER = 'pipe_send_grad'\nPIPE_RECV_INPUT_TIMER = 'pipe_recv_input'\nPIPE_RECV_GRAD_TIMER = 'pipe_recv_grad'\n\n\ndef is_even(number):\n    return number % 2 == 0\n\n\nmem_alloced = 0\nmem_cached = 0\n\n\ndef _tensor_bytes(tensor):\n    return tensor.numel() * tensor.element_size()\n\n\nclass PipelineEngine(DeepSpeedEngine):\n    \"\"\" A training engine hybrid pipeline, data, and model parallel training.\n\n    This engine is created by ``deepspeed.initialize()`` when a :class:`PipelineModule`\n    is provided.\n    \"\"\"\n    ID_TO_DTYPE = [\n        torch.float32, torch.float64, torch.complex64, torch.complex128, torch.float16, torch.bfloat16, torch.uint8,\n        torch.int8, torch.int16, torch.int32, torch.int64, torch.bool\n    ]\n    DTYPE_TO_ID = {dtype: id_ for id_, dtype in enumerate(ID_TO_DTYPE)}\n\n    def __init__(self, has_bool_tensors=False, *super_args, **super_kwargs):\n        super().__init__(*super_args, **super_kwargs)\n        assert isinstance(self.module, PipelineModule), \"model must base PipelineModule\"\n\n        assert self.zero_optimization_stage(\n        ) < ZeroStageEnum.gradients, \"ZeRO-2 and ZeRO-3 are incompatible with pipeline parallelism\"\n\n        # We schedule the all-reduces, so disable it in super().backward()\n        self.enable_backward_allreduce = False\n        self.has_bool_tensors = has_bool_tensors\n        self.eval_return_logits = False\n        self.outputs = None\n        # BF16 Optimizer is hardcoded for fp32 gradient accumulation\n        self.using_bf16_optimizer = type(self.optimizer) == BF16_Optimizer\n\n        # used to disable the pipeline all-reduce when used with 1-bit Adam/1-bit LAMB\n        self.pipeline_enable_backward_allreduce = True\n\n        if self.elasticity_enabled():\n            if not self.is_elastic_model_parallel_supported():\n                assert not self.elasticity_enabled(), \"Elasticity is not currently supported\" \\\n                \" with pipeline parallelism.\"\n\n        # pipeline step for logging\n        self.log_batch_step_id = -1\n\n        self.micro_batch_size = self.train_micro_batch_size_per_gpu()\n        self.micro_batches = self.gradient_accumulation_steps()\n\n        # Set Grid and Communication Groups\n        self.grid = self.module._grid\n        if self.grid.get_global_rank() == 0:\n            logger.info(f'CONFIG: micro_batches={self.micro_batches} '\n                        f'micro_batch_size={self.micro_batch_size}')\n\n        self.global_rank = self.grid.get_global_rank()\n\n        assert self.dp_world_size == self.grid.data_parallel_size\n        assert self.train_batch_size() == \\\n            self.micro_batch_size * self.micro_batches * self.grid.data_parallel_size\n\n        #  Set Stage Inf\n        self.num_stages = self.grid.pipe_parallel_size\n        self.stage_id = self.grid.get_stage_id()\n        self.prev_stage = self.stage_id - 1\n        self.next_stage = self.stage_id + 1\n\n        self.data_iterator = None\n        self.batch_fn = None\n\n        self._force_grad_boundary = False\n\n        self.batch_timer = ThroughputTimer(self._config.timers_config,\n                                           batch_size=self.train_batch_size(),\n                                           logging_fn=self.tput_log,\n                                           monitor_memory=False,\n                                           steps_per_output=self.steps_per_print())\n\n        # PipelineEngine needs to handle data loading specially due to only the first\n        # and last stages loading inputs/labels. We construct a sampler that uses\n        if self.training_data:\n            self._build_data_iter(self.training_data)\n\n        self.is_pipe_parallel = self.grid.pipe_parallel_size > 1\n        self.is_data_parallel = self.grid.data_parallel_size > 1\n        self.is_model_parallel = self.grid.model_parallel_size > 1\n\n        # Partition input/output buffers\n        # XXX temporarily disable while I revert some partition hacks.\n        assert isinstance(self._config.pipeline['pipe_partitioned'], bool)\n        assert isinstance(self._config.pipeline['grad_partitioned'], bool)\n        self.is_pipe_partitioned = self.is_model_parallel and self._config.pipeline['pipe_partitioned']\n        self.is_grad_partitioned = self.is_model_parallel and self._config.pipeline['grad_partitioned']\n        logger.info(f'is_pipe_partitioned= {self.is_pipe_partitioned} '\n                    f'is_grad_partitioned= {self.is_grad_partitioned}')\n\n        model_parameters = filter(lambda p: p.requires_grad, self.module.parameters())\n        num_params = sum([p.numel() for p in model_parameters])\n        unique_params = num_params\n        # Subtract tied parameters if we don't own them\n        if self.module.tied_comms:\n            tied_params = 0\n            for key, d in self.module.tied_comms.items():\n                if self.global_rank != min(d['ranks']):\n                    tied_params += sum(p.numel() for p in d['module'].parameters())\n            unique_params -= tied_params\n        params_tensor = torch.LongTensor(data=[num_params, unique_params]).to(self.device)\n        dist.all_reduce(params_tensor, group=self.grid.get_model_parallel_group())\n        params_tensor = params_tensor.tolist()\n        total_params = params_tensor[0]\n        unique_params = params_tensor[1]\n        if self.grid.data_parallel_id == 0:\n            logger.info(f'RANK={self.global_rank} '\n                        f'STAGE={self.stage_id} '\n                        f'LAYERS={self.module._local_stop - self.module._local_start} '\n                        f'[{self.module._local_start}, {self.module._local_stop}) '\n                        f'STAGE_PARAMS={num_params} ({num_params/1e6:0.3f}M) '\n                        f'TOTAL_PARAMS={total_params} ({total_params/1e6:0.3f}M) '\n                        f'UNIQUE_PARAMS={unique_params} ({unique_params/1e6:0.3f}M)')\n\n        #initialize peer-2-peer communication and allreduce groups\n        if self.is_pipe_parallel:\n            p2p.init_process_groups(self.grid)\n\n        # Pipeline buffers\n        self.num_pipe_buffers = 0\n        self.pipe_buffers = {\n            'inputs': [],  # batch input and received activations\n            'labels': [],  # labels from batch input\n            'outputs': [],  # activations\n            'output_tensors': [],  # tensor object to preserve backward graph\n        }\n        self.pipe_recv_buf = None\n        self.grad_layer = None\n\n        self.meta_buffer = None\n\n        self.first_output_send = True\n        self.first_gradient_send = True\n        self.pipe_partition_input_meta_cache = None\n        self.pipe_partition_output_meta_cache = None\n        self.pipe_partition_grad_meta_cache = None\n        self.grad_partition_grad_layer_meta_cache = None\n\n        #stores the loss for the current micro batch being processed\n        self.loss = torch.tensor(0.0).to(self.device)\n\n        #stores the loss for the entire batch\n        self.total_loss = None\n        self.total_additional_losses = None\n        self.agg_loss = torch.tensor(0.0, requires_grad=False).to(self.device)\n        self.dp_group_loss = torch.tensor(0.0, requires_grad=False).to(self.device)\n\n        # stores aggregated-DP train final loss and aggregated-DP additional losses, if any\n        # additional losses are stored as dict: {loss-name: agg-loss}\n        self.agg_train_loss = None\n        self.agg_additional_losses = None\n\n        if self._config.pipeline['activation_checkpoint_interval'] > 0:\n            self.module.activation_checkpoint_interval = self._config.pipeline['activation_checkpoint_interval']\n            # set use_reentrant default to True.\n            if self._config.pipeline.get('use_reentrant') is None:\n                self._config.pipeline['use_reentrant'] = True\n            if self._config.pipeline['use_reentrant'] is False:\n                # set activation_checkpoint_func to non_reentrant_checkpoint func.\n                self.module.activation_checkpoint_func = ds_checkpointing.non_reentrant_checkpoint\n                if self.grid.get_global_rank() == 0:\n                    logger.info(f'CONFIG: activation_checkpoint_func=non_reentrant_checkpoint')\n\n        self.module.checkpoint_parallel_write_pipeline = self._config.checkpoint_parallel_write_pipeline\n\n        if self.is_last_stage():\n            self.loss_model = self.module.loss_fn\n\n        self.has_attention_mask = self.module.__class__.__name__ == 'GPT2ModelPipe'\n        # Initialize pipeline communicators. Just send a 0.\n        if is_even(self.stage_id):\n            if not self.is_last_stage():\n                p2p.send(self.loss, self.next_stage)\n            if not self.is_first_stage():\n                p2p.recv(self.loss, self.prev_stage)\n        else:\n            if not self.is_first_stage():\n                p2p.recv(self.loss, self.prev_stage)\n            if not self.is_last_stage():\n                p2p.send(self.loss, self.next_stage)\n\n        # XXX look into timer reporting timing\n        # Initialize some timers because of early weirdness.\n        if self.wall_clock_breakdown():\n            self.timers(FORWARD_MICRO_TIMER).start()\n            self.timers(FORWARD_MICRO_TIMER).stop()\n            self.timers(BACKWARD_MICRO_TIMER).start()\n            self.timers(BACKWARD_MICRO_TIMER).stop()\n            self.timers(BACKWARD_INNER_MICRO_TIMER).start()\n            self.timers(BACKWARD_INNER_MICRO_TIMER).stop()\n            self.timers(BACKWARD_REDUCE_MICRO_TIMER).start()\n            self.timers(BACKWARD_REDUCE_MICRO_TIMER).stop()\n            self.timers(BACKWARD_REDUCE_GLOBAL_TIMER).start()\n            self.timers(BACKWARD_REDUCE_GLOBAL_TIMER).stop()\n            self.timers(STEP_MICRO_TIMER).start()\n            self.timers(STEP_MICRO_TIMER).stop()\n\n    def set_has_attention_mask(self, value):\n        assert isinstance(value, bool)\n        self.has_attention_mask = value\n\n    def _build_data_iter(self, dataset):\n        sampler = torch.utils.data.distributed.DistributedSampler(dataset,\n                                                                  num_replicas=self.dp_world_size,\n                                                                  rank=self.mpu.get_data_parallel_rank(),\n                                                                  shuffle=False)\n        # Build a loader and make it repeating.\n        pipe_dataloader = self.deepspeed_io(dataset, data_sampler=sampler)\n        pipe_dataloader = RepeatingLoader(pipe_dataloader)\n        self.set_dataloader(pipe_dataloader)\n\n    def _exec_reduce_tied_grads(self):\n        # We need to run this first to write to self.averaged_gradients;\n        # since this class turns `enable_backward_allreduce` off,\n        # `self.overlapping_partition_gradients_reduce_epilogue()` defined in the DeepSpeedEngine\n        # never actually runs. I suspect this is because of efficiency problems; get_flat_partition in\n        # stage2.py might do something expensive; someone will have to look into that later. But\n        # in the meantime, this fixes ZeRO2 + Pipelining enough to run a demo. Further profiling\n        # needed to decide if it actually breaks everything.\n        # (see https://github.com/EleutherAI/gpt-neox/issues/62#issuecomment-761471944)\n        if self.zero_optimization_partition_gradients():\n            self.optimizer.overlapping_partition_gradients_reduce_epilogue()\n\n        weight_group_list = self.module.get_tied_weights_and_groups()\n        for weight, group in weight_group_list:\n            grad = weight._hp_grad if self.using_bf16_optimizer else weight.grad\n            dist.all_reduce(grad, group=group)\n\n    def _exec_reduce_grads(self):\n        self._force_grad_boundary = True\n        if self.pipeline_enable_backward_allreduce:\n            if self.using_bf16_optimizer:\n                # PP+BF16 work for ZeRO Stage 1\n                self._bf16_reduce_grads()\n            else:\n                self.allreduce_gradients(bucket_size=MEMORY_OPT_ALLREDUCE_SIZE)\n        self._force_grad_boundary = False\n\n    def _bf16_reduce_grads(self):\n        self.buffered_allreduce_fallback(grads=None, elements_per_buffer=MEMORY_OPT_ALLREDUCE_SIZE)\n\n    def _reserve_pipe_buffers(self, num_buffers):\n        \"\"\"Ensure that each pipeline buffer has at least ``num_buffers`` slots.\n\n        This method only reserves slots and does not allocate tensors.\n\n        Args:\n            num_buffers (int): The number of buffers to reserve.\n        \"\"\"\n        if self.num_pipe_buffers >= num_buffers:\n            return\n\n        num_added = num_buffers - self.num_pipe_buffers\n        for key in self.pipe_buffers:\n            self.pipe_buffers[key].extend([None] * num_added)\n        self.num_pipe_buffers = num_buffers\n\n    def reset_activation_shape(self):\n        \"\"\"Reset the buffers when the shape of activation and gradient change.\n        For example, for curriculum learning that changes the seqlen of each\n        sample, we need to call this whenever the seqlen is going to change.\n        \"\"\"\n        self.first_output_send = True\n        self.pipe_recv_buf = None\n        self.grad_layer = None\n        self.meta_buffer = None\n\n        self.pipe_partition_input_meta_cache = None\n        self.pipe_partition_output_meta_cache = None\n        self.pipe_partition_grad_meta_cache = None\n        self.grad_partition_grad_layer_meta_cache = None\n\n    def train_batch(self, data_iter=None):\n        \"\"\"Progress the pipeline to train the next batch of data. The engine will ingest\n        ``self.train_batch_size()`` total samples collectively across all workers.\n\n\n        An iterator that over training data should be provided as an argument\n        unless ``deepspeed.initialize()`` was provided a training set. In that event,\n        the training data will automatically be read.\n\n\n        .. warning::\n            A total of ``self.gradient_accumulation_steps()`` entries will be pulled\n            from ``data_iter`` by each pipeline. There must be sufficient\n            data left in ``data_iter`` or else a ``StopIteration`` will halt training.\n\n            DeepSpeed provides a convenience class :class:`deepspeed.utils.RepeatingLoader`\n            that wraps data loaders to automatically restart upon a ``StopIteration``.\n\n        Args:\n            data_iter (Iterator, optional): Iterator of training data.\n\n        Returns:\n            The arithmetic mean of the losses computed this batch.\n        \"\"\"\n        if not torch._C.is_grad_enabled():\n            raise RuntimeError(f'train_batch() requires gradients enabled. Use eval_batch() instead.')\n\n        # Curriculum learning could change activation shape\n        if self.curriculum_enabled_legacy():\n            new_difficulty = self.curriculum_scheduler_legacy.update_difficulty( \\\n                self.global_steps + 1)\n            if self.global_steps == 0 or self.curriculum_scheduler_legacy.first_step:\n                self.reset_activation_shape()\n                self.curriculum_scheduler_legacy.first_step = False\n            elif new_difficulty != self.curriculum_scheduler_legacy.get_difficulty( \\\n                self.global_steps):\n                self.reset_activation_shape()\n\n        if data_iter is not None:\n            self.set_dataiterator(data_iter)\n\n        self.module.train()\n        self.total_loss = None\n        self.total_additional_losses = None\n        self._compute_loss = True\n\n        # Do the work\n        self.timers(TRAIN_BATCH_TIMER).start()\n        sched = schedule.TrainSchedule(micro_batches=self.micro_batches,\n                                       stages=self.num_stages,\n                                       stage_id=self.stage_id)\n        self._exec_schedule(sched)\n\n        with torch.no_grad():\n            self.agg_train_loss = self._aggregate_total_loss()\n\n        self.timers(TRAIN_BATCH_TIMER).stop()\n\n        if self.global_steps % self.steps_per_print() == 0:\n            if self.global_rank == 0:\n                elapsed = self.timers(TRAIN_BATCH_TIMER).elapsed(reset=True) / 1000.0\n                iter_time = elapsed / self.steps_per_print()\n                tput = self.train_batch_size() / iter_time\n                log_str = f'steps: {self.global_steps} loss: {self.agg_train_loss:0.4f} '\n                if self.agg_additional_losses is not None:\n                    for loss_name, loss_value in self.agg_additional_losses.items():\n                        log_str += f'{loss_name}: {loss_value.item():0.4f} '\n                log_str += f'iter time (s): {iter_time:0.3f} samples/sec: {tput:0.3f}'\n                print(log_str)\n            else:\n                self.timers(TRAIN_BATCH_TIMER).elapsed(reset=True)\n\n        # Monitoring\n        if self.global_rank == 0 and self.monitor.enabled:\n            self.summary_events = [(f'Train/Samples/train_loss', self.agg_train_loss.mean().item(),\n                                    self.global_samples)]\n            self.monitor.write_events(self.summary_events)\n\n        if self.wall_clock_breakdown() and self.global_steps % self.steps_per_print() == 0:\n            self.timers.log([\n                PIPE_SEND_OUTPUT_TIMER,\n                PIPE_SEND_GRAD_TIMER,\n                PIPE_RECV_INPUT_TIMER,\n                PIPE_RECV_GRAD_TIMER,\n            ])\n\n        # TODO: should return precisely what loss returned and allow others to be queried?\n        return self.agg_train_loss\n\n    def eval_batch(self,\n                   data_iter,\n                   return_logits=False,\n                   compute_loss=True,\n                   reduce_output='avg',\n                   bcast_loss=True,\n                   num_micro_batches=None):\n        \"\"\"Evaluate the pipeline on a batch of data from ``data_iter``. The\n        engine will evaluate ``self.train_batch_size()`` total samples\n        collectively across all workers.\n\n        This method is equivalent to:\n\n        .. code-block:: python\n\n            module.eval()\n            with torch.no_grad():\n                output = module(batch)\n\n        .. warning::\n            A total of ``self.gradient_accumulation_steps()`` entries will be pulled\n            from ``data_iter`` by each pipeline. There must be sufficient\n            data left in ``data_iter`` or else a ``StopIteration`` will halt training.\n\n            DeepSpeed provides a convenience class :class:`deepspeed.utils.RepeatingLoader`\n            that wraps data loaders to automatically restart upon a ``StopIteration``.\n\n        Args:\n            data_iter (Iterator): Iterator of data to evaluate.\n\n        Returns:\n            The arithmetic mean of the losses computed this batch.\n        \"\"\"\n        self.eval_return_logits = return_logits\n        self.module.eval()\n\n        # Curriculum learning could change activation shape\n        if self.curriculum_enabled_legacy():\n            new_difficulty = self.curriculum_scheduler_legacy.update_difficulty( \\\n                self.global_steps + 1)\n            if self.global_steps == 0 or self.curriculum_scheduler_legacy.first_step:\n                self.reset_activation_shape()\n                self.curriculum_scheduler_legacy.first_step = False\n            elif new_difficulty != self.curriculum_scheduler_legacy.get_difficulty( \\\n                self.global_steps):\n                self.reset_activation_shape()\n\n        eval_output = None\n\n        self._compute_loss = compute_loss\n\n        # Use the provided data iterator\n        train_iterator = self.data_iterator\n        self.set_dataiterator(data_iter)\n\n        # set the number micro batches in case the user chose value than training\n        micro_batches = self.micro_batches if num_micro_batches is None else num_micro_batches\n\n        # Do the work\n        sched = schedule.InferenceSchedule(micro_batches=self.micro_batches,\n                                           stages=self.num_stages,\n                                           stage_id=self.stage_id)\n\n        # prevent dead-lock with multiple evals sequence\n        dist.barrier()\n\n        with torch.no_grad():\n            self._exec_schedule(sched)\n\n        if self.is_last_stage():\n            eval_output = self._reduce_outputs(self.fwd_outputs, reduce=reduce_output, micro_batches=micro_batches)\n\n        if compute_loss and (bcast_loss or self.monitor.enabled):\n            eval_output = self._bcast_pipe_scalar(eval_output)\n\n        if self.global_rank == 0 and self.monitor.enabled:\n            self.summary_events = [(f'Train/Samples/eval_loss', eval_output.mean().item(), self.global_samples)]\n            self.monitor.write_events(self.summary_events)\n\n        # Restore the training iterator\n        self.set_dataiterator(train_iterator)\n\n        # Reset any buffers that may have been populated during the forward passes.\n        #ds_checkpointing.reset()\n        self.eval_return_logits = False\n        if return_logits:\n            outputs = self.outputs\n            self.outputs = None\n            return eval_output, outputs\n        return eval_output\n\n    def set_train_batch_size(self, train_batch_size):\n        \"\"\"Adjust the global batch size by increasing or decreasing the number of\n        micro-batches (i.e., gradient accumulation steps). The size of each micro-batch\n        (i.e., ``train_micro_batch_size_per_gpu``) is not changed.\n        Args:\n            train_batch_size (int): The new global batch size for training.\n        Raises:\n            ValueError: if ``train_batch_size`` is not divisible by the\n                configured micro-batch size and data parallelism.\n        \"\"\"\n        super().set_train_batch_size(train_batch_size)\n        self.micro_batches = self.gradient_accumulation_steps()\n\n    def is_first_stage(self):\n        \"\"\"True if this process is in the first stage in the pipeline.\"\"\"\n        return self.stage_id == 0\n\n    def is_last_stage(self):\n        \"\"\"True if this process is in the last stage in the pipeline.\"\"\"\n        return self.stage_id == self.num_stages - 1\n\n    def _reduce_outputs(self, outputs, reduce='avg', reduce_dp=True, micro_batches=None):\n        if reduce is None:\n            return outputs\n\n        if reduce.lower() == 'avg':\n            # first sum over all microbatches\n            if torch.is_tensor(outputs[0]):\n                reduced = sum(outputs)\n            else:\n                assert isinstance(outputs, (list, tuple))\n                reduced = [torch.zeros_like(o) for o in outputs[0]]\n                for idx, out in outputs:\n                    reduced[idx] += out\n\n            # Average over the microbatches\n            reduced = self._scale_loss_by_gas(reduced, eval_micro_batches=micro_batches)\n\n            # Average over DP groups\n            if reduce_dp and self.is_data_parallel:\n                if torch.is_tensor(reduced):\n                    dist.all_reduce(reduced, group=self.mpu.get_data_parallel_group())\n                    reduced /= self.dp_world_size\n                else:\n                    for idx in range(len(reduced)):\n                        dist.all_reduce(reduced[idx], group=self.mpu.get_data_parallel_group())\n                        reduced[idx] /= self.dp_world_size\n\n            return reduced\n        else:\n            raise NotImplementedError(f'reduction type {reduce} not supported.')\n\n    def _bcast_pipe_scalar(self, data, src_rank=None, dtype=torch.float32):\n        # Default to last stage (e.g., for broadcasting loss)\n        if src_rank is None:\n            src_rank = self.grid.stage_to_global(self.num_stages - 1)\n        assert src_rank in self.grid.pp_group\n\n        if self.global_rank == src_rank:\n            result = data.clone().detach().type(dtype).to(self.device)\n        else:\n            result = torch.Tensor([0.]).type(dtype).to(self.device)\n\n        dist.broadcast(tensor=result, src=src_rank, group=self.mpu.get_pipe_parallel_group())\n\n        return result\n\n    def _aggregate_total_loss(self):\n        # Scale loss, average among DP ranks, and bcast loss to the rest of my DP group\n        if self.is_last_stage():\n            # Scale loss and additional losses, if any\n            loss = self._scale_loss_by_gas(self.total_loss)\n            self.agg_additional_losses = self.total_additional_losses\n            if self.agg_additional_losses is not None:\n                self.agg_additional_losses = OrderedDict({\n                    loss_name: self._scale_loss_by_gas(_loss.clone().detach())\n                    for loss_name, _loss in self.agg_additional_losses.items()\n                })\n\n            self.dp_group_loss = loss.clone().detach()\n            agg_loss = self.dp_group_loss.clone().detach()\n            #print(f'RANK={self.global_rank} bcast SENDER src={self.global_rank} group={self.grid.pp_group}', flush=True)\n\n            # Average loss across all data-parallel groups\n            if self.is_data_parallel:\n                if self.agg_additional_losses is None:\n                    dist.all_reduce(agg_loss, group=self.mpu.get_data_parallel_group())\n                    agg_loss /= self.dp_world_size\n                else:\n                    # use a single reduce op for agg_loss and additional losses, if any\n                    assert '__train_loss__' not in self.agg_additional_losses.keys()\n                    tensors = OrderedDict({'__train_loss__': agg_loss})\n                    tensors.update(self.agg_additional_losses.items())\n                    flat_tensor = torch.cat([t.clone().reshape(-1).detach() for t in tensors.values()])\n                    dist.all_reduce(flat_tensor, group=self.mpu.get_data_parallel_group())\n                    flat_tensor /= self.dp_world_size\n                    offset = 0\n                    reduced_tensor = {}\n                    for name, t in tensors.items():\n                        n_elem = t.numel()\n                        reduced_tensor[name] = flat_tensor[offset:offset + n_elem].clone().detach().reshape(t.shape)\n                        offset += n_elem\n                    agg_loss = reduced_tensor['__train_loss__']\n                    self.agg_additional_losses = OrderedDict(\n                        {name: reduced_tensor[name]\n                         for name in self.agg_additional_losses.keys()})\n\n            assert self.global_rank in self.grid.pp_group\n            losses = [self.dp_group_loss, agg_loss]\n            if self.agg_additional_losses is not None:\n                losses += list(self.agg_additional_losses.values())\n            losses = torch.stack(losses).float()\n            if self.is_pipe_parallel:\n                dist.broadcast(tensor=losses, src=self.global_rank, group=self.mpu.get_pipe_parallel_group())\n        else:\n            # Get loss from last stage\n            src_rank = self.grid.stage_to_global(self.num_stages - 1)\n            assert src_rank in self.grid.pp_group\n            # losses to reduce are: dp_group_loss, agg_loss, model additional losses\n            # therefore: 2 + n_additional_losses\n            additional_losses = self.module.get_additional_losses()\n            n_additional_losses = 0 if additional_losses is None else len(additional_losses)\n            losses = torch.Tensor([0.] * (2 + n_additional_losses)).to(self.device)\n            dist.broadcast(tensor=losses, src=src_rank, group=self.grid.get_pipe_parallel_group())\n            self.dp_group_loss = losses[0].clone().detach()\n            agg_loss = losses[1].clone().detach()\n            if additional_losses is not None:\n                self.agg_additional_losses = OrderedDict(\n                    {name: losses[2 + i].clone().detach()\n                     for i, name in enumerate(additional_losses.keys())})\n        return agg_loss\n\n    def set_dataloader(self, loader):\n        \"\"\"\"\"\"\n        if self.is_first_stage() or self.is_last_stage():\n            self.training_dataloader = loader\n            self.data_iterator = iter(self.training_dataloader)\n\n    def set_dataiterator(self, iterator):\n        \"\"\" Store an iterator to sample for training data. \"\"\"\n        if self.is_first_stage() or self.is_last_stage():\n            self.training_dataloader = None\n            self.data_iterator = iterator\n\n    def set_batch_fn(self, fn):\n        \"\"\"Execute a post-processing function on input data.\n\n        Args:\n            fn (function): The function to run.\n        \"\"\"\n        self.batch_fn = fn\n\n    def is_gradient_accumulation_boundary(self):\n        \"\"\"True if the engine is executing a gradient reduction or optimizer step instruction.\n\n        This is overridden from :class:`DeepSpeedEngine` to force reductions\n        and steps when the pipeline engine is instructed to do so.\n\n        Returns:\n            bool: whether reductions and optimizer steps should occur.\n        \"\"\"\n        return self._force_grad_boundary\n\n    def log_for_device(self, *msg):\n        if LOG_STAGE == self.stage_id or LOG_STAGE == -1:\n            if DATA_PARALLEL_ID == self.grid.data_parallel_id or DATA_PARALLEL_ID == -1:\n                print(\n                    f'RANK={dist.get_rank()} '\n                    f'PIPE-ID={self.stage_id} '\n                    f'DATA-ID={self.grid.data_parallel_id} '\n                    f'MBATCH-ID={self.microbatch_id} '\n                    f'STEP-ID={self.log_batch_step_id} '\n                    '::',\n                    *msg,\n                    flush=True)\n\n    def tput_log(self, *msg):\n        if self.global_rank == 0 and self.global_steps % self.steps_per_print() == 0:\n            print(*msg)\n\n    def _next_batch(self):\n        # If using 3D parallelism, only some first-stage ranks may do IO\n        batch = None\n        if self.data_iterator is not None:\n            batch = next(self.data_iterator)\n\n        # Any post-processing, like broadcasting across a slice-parallel group.\n        if self.batch_fn:\n            batch = self.batch_fn(batch)\n\n        return batch\n\n    def _exec_forward_pass(self, buffer_id):\n        self.tput_timer.start()\n        self.mem_status('BEFORE FWD', reset_max=True)\n\n        if isinstance(self.pipe_buffers['inputs'][buffer_id], tuple):\n            inputs = tuple(t.clone() for t in self.pipe_buffers['inputs'][buffer_id])\n        else:\n            inputs = self.pipe_buffers['inputs'][buffer_id].clone()\n\n        # collect the partitioned input from the previous stage\n        if self.is_pipe_partitioned and not self.is_first_stage():\n            if self.pipe_partition_input_meta_cache is None:\n                self.pipe_partition_input_meta_cache = inputs[0].to('cpu')\n            part_input = PartitionedTensor.from_meta(meta=self.pipe_partition_input_meta_cache,\n                                                     local_part=inputs[1],\n                                                     group=self.grid.get_slice_parallel_group())\n\n            inputs = (part_input.full(), *inputs[2:])\n            inputs[0].requires_grad = True\n            # skip mask\n            #inputs[1].requires_grad = True\n            part_input = None\n            inputs = inputs[0] if len(inputs) == 1 else inputs\n            self.pipe_buffers['inputs'][buffer_id] = inputs\n\n        # inputs has no gradient because it is from a cloned tensor\n        outputs = super().forward(inputs)\n\n        # Reset activation checkpointing buffers.\n        # Need to call this between evaluation iterations\n        if not self.module.training:\n            ds_checkpointing.reset()\n\n        # Partition the outputs if we are not the last stage\n        if self.is_pipe_partitioned and not self.is_last_stage():\n            if isinstance(outputs, tuple):\n                first_output = outputs[0]\n                # TODO: Improve pipe partitioning to pass multiple tensors that require grads\n                assert all([torch.is_tensor(elt) and elt.requires_grad is False for elt in outputs[1:]])\n                outputs_tail = outputs[1:]\n            elif torch.is_tensor(outputs):\n                first_output = outputs\n                outputs_tail = []\n            else:\n                raise ValueError(\"expecting a tensor or a tuple of tensors\")\n            part = PartitionedTensor(tensor=first_output, group=self.grid.get_slice_parallel_group())\n            # Clear the large output data, but save the computation graph\n            first_output.data = torch.zeros(1, device=first_output.data.device)\n            self.pipe_buffers['output_tensors'][buffer_id] = first_output\n            # Inject the partitioned tensor into the output before sending\n            outputs = (part.to_meta(), part.data(), *outputs_tail)\n            part = None\n\n        self.pipe_buffers['outputs'][buffer_id] = outputs\n\n        # Optionally compute loss on the last device\n        if self.is_last_stage():\n            if self._compute_loss and self.module.loss_fn is not None:\n                labels = self.pipe_buffers['labels'][buffer_id]\n                self.loss = self.module.loss_fn(outputs, labels)\n            else:\n                # Some models just return loss from forward()\n                self.loss = outputs\n            if self.eval_return_logits:\n                self.outputs = outputs\n\n            if isinstance(self.loss, torch.Tensor):\n                self.fwd_outputs.append(self.loss.detach())\n            else:\n                self.fwd_outputs.append([l.detach() for l in self.loss])\n\n            def add_to_total_loss(_total_loss, _loss):\n                if isinstance(_loss, torch.Tensor):\n                    if _total_loss is None:\n                        _total_loss = torch.zeros_like(_loss)\n                    _total_loss += _loss.detach()\n                else:\n                    if _total_loss is None:\n                        _total_loss = [torch.zeros_like(_l) for _l in _loss]\n                    for _idx, _l in enumerate(_loss):\n                        _total_loss[_idx] += _l.detach()\n                return _total_loss\n\n            self.total_loss = add_to_total_loss(self.total_loss, self.loss)\n\n            # aggregate additional losses across gradient accumulation steps\n            additional_losses = self.module.get_additional_losses()\n            if additional_losses is not None:\n                if self.total_additional_losses is None:\n                    self.total_additional_losses = OrderedDict()\n                for name, loss in additional_losses.items():\n                    total = self.total_additional_losses[name] if name in self.total_additional_losses else None\n                    self.total_additional_losses[name] = add_to_total_loss(total, loss)\n\n    def _exec_backward_pass(self, buffer_id):\n        assert self.optimizer is not None, \"must provide optimizer during \" \\\n                                           \"init in order to use backward\"\n\n        self.mem_status('BEFORE BWD', reset_max=True)\n\n        # The last stage just runs backward on the loss using DeepSpeed's typical\n        # mechanisms.\n        if self.is_last_stage():\n            super().backward(self.loss)\n            self.mem_status('AFTER BWD')\n            return\n\n        outputs = self.pipe_buffers['outputs'][buffer_id]\n\n        if self.wall_clock_breakdown():\n            self.timers(BACKWARD_MICRO_TIMER).start()\n            self.timers(BACKWARD_GLOBAL_TIMER).start()\n            self.timers(BACKWARD_INNER_MICRO_TIMER).start()\n            self.timers(BACKWARD_INNER_GLOBAL_TIMER).start()\n\n        # Reconstruct if we previously partitioned the output. We must be\n        # careful to also restore the computational graph of the tensors we partitioned.\n        if self.is_pipe_partitioned:\n            if self.is_grad_partitioned:\n                if self.pipe_partition_output_meta_cache is None:\n                    self.pipe_partition_output_meta_cache = outputs[0].to('cpu')\n                part_output = PartitionedTensor.from_meta(meta=self.pipe_partition_output_meta_cache,\n                                                          local_part=outputs[1],\n                                                          group=self.grid.get_slice_parallel_group())\n                self.pipe_buffers['output_tensors'][buffer_id].data = part_output.full()\n                outputs = (self.pipe_buffers['output_tensors'][buffer_id], *outputs[2:])\n            else:\n                # Already restored from partition\n                self.pipe_buffers['output_tensors'][buffer_id].data = outputs[0]\n                outputs = (self.pipe_buffers['output_tensors'][buffer_id], *outputs[1:])\n\n        grad_tensors = self.grad_layer\n        if self.is_grad_partitioned:\n            #print(f'RANK={self.global_rank} BEFORE-BWD restoring grad={self.grad_layer[0].size()} {self.grad_layer[1].size()}')\n            if self.grad_partition_grad_layer_meta_cache is None:\n                self.grad_partition_grad_layer_meta_cache = self.grad_layer[0].to('cpu')\n            part_grad = PartitionedTensor.from_meta(meta=self.grad_partition_grad_layer_meta_cache,\n                                                    local_part=self.grad_layer[1],\n                                                    group=self.grid.get_slice_parallel_group())\n            grad_tensors = (part_grad.full(), *grad_tensors[2:])\n            part_grad = None\n            #print(f'RANK={self.global_rank} BEFORE-BWD restored grad={self.grad_layer[0].size()} {self.grad_layer[1].size()}')\n\n        if self.using_bf16_optimizer and not self.is_last_stage():\n            # manually call because we don't call optimizer.backward()\n            self.optimizer.clear_lp_grads()\n\n        # This handles either a single tensor or tuple of tensors.\n        if isinstance(outputs, tuple):\n            out_tensors = [t for t in outputs if t.is_floating_point()]\n            assert len(out_tensors) == len(grad_tensors)\n            torch.autograd.backward(tensors=out_tensors, grad_tensors=grad_tensors)\n        else:\n            torch.autograd.backward(tensors=(outputs, ), grad_tensors=(grad_tensors, ))\n\n        if self.using_bf16_optimizer and not self.is_last_stage():\n            # manually call because we don't call optimizer.backward()\n            if not self._config.bfloat16_immediate_grad_update:\n                self.optimizer.update_hp_grads(clear_lp_grads=False)\n\n        # Free up the memory from the output of forward()\n        self.pipe_buffers['output_tensors'][buffer_id] = None\n        self.pipe_buffers['outputs'][buffer_id] = None\n        grad_tensors = None\n\n        if self.wall_clock_breakdown():\n            self.timers(BACKWARD_INNER_MICRO_TIMER).stop()\n            self.timers(BACKWARD_INNER_GLOBAL_TIMER).stop()\n            self.timers(BACKWARD_MICRO_TIMER).stop()\n            self.timers(BACKWARD_GLOBAL_TIMER).stop()\n\n        self.mem_status('AFTER BWD')\n\n    def _exec_load_micro_batch(self, buffer_id):\n        if self.wall_clock_breakdown():\n            self.timers(BATCH_INPUT_TIMER).start()\n\n        batch = self._next_batch()\n\n        if self.is_first_stage():\n            loaded = None\n            if torch.is_tensor(batch[0]):\n                loaded = batch[0].clone().to(self.device).detach()\n                if self._config.pipeline['activation_checkpoint_interval'] > 0 and self._config.pipeline[\n                        'use_reentrant']:\n                    loaded.requires_grad = loaded.is_floating_point()\n            else:\n                assert isinstance(batch[0], (tuple, list))\n                # Assume list or tuple\n                loaded = []\n                for x in batch[0]:\n                    assert torch.is_tensor(x)\n                    mine = x.clone().detach().to(self.device)\n                    if self._config.pipeline['activation_checkpoint_interval'] > 0 and self._config.pipeline[\n                            'use_reentrant']:\n                        mine.requires_grad = mine.is_floating_point()\n                    loaded.append(mine)\n                loaded = tuple(loaded)\n\n            self.pipe_buffers['inputs'][buffer_id] = loaded\n\n        if self.is_last_stage():\n            loaded = batch[1]\n            if torch.is_tensor(batch[1]):\n                loaded = batch[1].to(self.device)\n            # XXX: torch 1.6.0 DataLoader will auto convert tuple to list\n            elif isinstance(batch[1], (tuple, list)):\n                loaded = []\n                for x in batch[1]:\n                    assert torch.is_tensor(x)\n                    x = x.to(self.device).detach()\n                    loaded.append(x)\n                loaded = tuple(loaded)\n\n            self.pipe_buffers['labels'][buffer_id] = loaded\n\n        if self.wall_clock_breakdown():\n            self.timers(BATCH_INPUT_TIMER).stop()\n\n    def _send_tensor_meta(self, buffer, recv_stage):\n        \"\"\" Communicate metadata about upcoming p2p transfers.\n\n        Metadata is communicated in this order:\n            * type (0: tensor, 1: list)\n            * num_tensors if type=list\n            foreach tensor in buffer:\n                * ndims\n                * shape\n        \"\"\"\n        send_bytes = 0\n        if isinstance(buffer, torch.Tensor):\n            type_tensor = torch.LongTensor(data=[0]).to(self.device)\n            p2p.send(type_tensor, recv_stage)\n            send_shape = torch.LongTensor(data=buffer.size()).to(self.device)\n            send_ndims = torch.LongTensor(data=[len(buffer.size())]).to(self.device)\n            p2p.send(send_ndims, recv_stage)\n            p2p.send(send_shape, recv_stage)\n            send_bytes += _tensor_bytes(buffer)\n        elif isinstance(buffer, list):\n            assert (False)\n            type_tensor = torch.LongTensor(data=[1]).to(self.device)\n            p2p.send(type_tensor, recv_stage)\n            count_tensor = torch.LongTensor(data=[len(buffer)]).to(self.device)\n            p2p.send(count_tensor, recv_stage)\n            for tensor in buffer:\n                assert isinstance(tensor, torch.Tensor)\n                send_shape = torch.LongTensor(data=tensor.size()).to(self.device)\n                send_ndims = torch.LongTensor(data=[len(tensor.size())]).to(self.device)\n                p2p.send(send_ndims, recv_stage)\n                p2p.send(send_shape, recv_stage)\n                send_bytes += _tensor_bytes(tensor)\n        elif isinstance(buffer, tuple):\n            type_tensor = torch.LongTensor(data=[2]).to(self.device)\n            p2p.send(type_tensor, recv_stage)\n            count_tensor = torch.LongTensor(data=[len(buffer)]).to(self.device)\n            p2p.send(count_tensor, recv_stage)\n            for idx, tensor in enumerate(buffer):\n                assert isinstance(tensor, torch.Tensor)\n                send_shape = torch.LongTensor(data=tensor.size()).to(self.device)\n                send_ndims = torch.LongTensor(data=[len(tensor.size())]).to(self.device)\n                send_dtype = torch.LongTensor(data=[self.DTYPE_TO_ID[tensor.dtype]]).to(self.device)\n                p2p.send(send_dtype, recv_stage)\n                p2p.send(send_ndims, recv_stage)\n                p2p.send(send_shape, recv_stage)\n                # Useful for performance debugging.\n                '''\n                new_bytes = _tensor_bytes(tensor)\n                send_bytes += _tensor_bytes(tensor)\n                # Useful for performance debugging.\n                if self.grid.data_parallel_id == 0:\n                    print(\n                        f'STAGE={self.stage_id} pipe-send-volume[{idx}]: shape={send_shape} {new_bytes/1024**2:0.2f}MB'\n                    )\n                '''\n        else:\n            raise NotImplementedError(f'Could not send meta type {type(buffer)}')\n\n        # Useful for performance debugging.\n        '''\n        if self.grid.data_parallel_id == 0:\n            print(f'STAGE={self.stage_id} pipe-send-volume: {send_bytes/1024**2:0.2f}MB')\n        '''\n\n    def _recv_tensor_meta(self, send_stage):\n        \"\"\"Receive metadata about upcoming p2p transfers and return allocated buffers.\n\n        Metadata is communicated in this order:\n            * type (0: tensor, 1: list)\n            * num_tensors if type=list\n            foreach tensor in buffer:\n                * ndims\n                * shape\n\n        Returns:\n            Allocated buffer for receiving from send_stage.\n        \"\"\"\n\n        type_tensor = torch.LongTensor(data=[0]).to(self.device)\n        p2p.recv(type_tensor, send_stage)\n        recv_type = type_tensor.item()\n\n        # A single tensor will be sent.\n        if recv_type == 0:\n            recv_ndims = torch.LongTensor(data=[0]).to(self.device)\n            p2p.recv(recv_ndims, send_stage)\n            recv_ndims = recv_ndims.item()\n            recv_shape = torch.LongTensor([1] * recv_ndims).to(self.device)\n            p2p.recv(recv_shape, send_stage)\n            recv_shape = recv_shape.tolist()\n            return self._allocate_buffer(recv_shape, num_buffers=1)[0]\n\n        # List or tuple of tensors\n        elif recv_type == 1 or recv_type == 2:\n            count_tensor = torch.LongTensor(data=[0]).to(self.device)\n            p2p.recv(count_tensor, send_stage)\n            num_tensors = count_tensor.item()\n            recv_shapes_and_dtypes = []\n            for idx in range(num_tensors):\n                recv_dtype = torch.LongTensor(data=[0]).to(self.device)\n                p2p.recv(recv_dtype, send_stage)\n                recv_dtype = self.ID_TO_DTYPE[recv_dtype.item()]\n                recv_ndims = torch.LongTensor(data=[0]).to(self.device)\n                p2p.recv(recv_ndims, send_stage)\n                recv_ndims = recv_ndims.item()\n                recv_shape = torch.LongTensor([1] * recv_ndims).to(self.device)\n                p2p.recv(recv_shape, send_stage)\n                recv_shapes_and_dtypes.append((recv_shape.tolist(), recv_dtype))\n\n            buffers = self._allocate_buffers(recv_shapes_and_dtypes, num_buffers=1)[0]\n            # Convert to tuples if requested.\n            if recv_type == 2:\n                buffers = tuple(buffers)\n            return buffers\n\n        else:\n            raise NotImplementedError(f'Could not receive type {type(recv_type)}')\n\n    def _exec_send_activations(self, buffer_id):\n        if self.wall_clock_breakdown():\n            self.timers(PIPE_SEND_OUTPUT_TIMER).start()\n\n        outputs = self.pipe_buffers['outputs'][buffer_id]\n\n        # NCCL does not like to send torch.BoolTensor types, so cast the mask to half().\n        # We could do char, but with half() we can eventually flatten with other fp16\n        # messages (TODO)\n        if self.has_attention_mask or self.has_bool_tensors:\n            outputs = list(outputs)\n            outputs[-1] = outputs[-1].half()\n            outputs = tuple(outputs)\n\n        if self.first_output_send:\n            self.first_output_send = False\n            self._send_tensor_meta(outputs, self.next_stage)\n\n        if isinstance(outputs, torch.Tensor):\n            p2p.send(outputs, self.next_stage)\n        elif isinstance(outputs, tuple):\n            for idx, buffer in enumerate(outputs):\n                p2p.send(buffer, self.next_stage)\n        else:\n            raise NotImplementedError('Could not send output of type '\n                                      f'{type(outputs)}')\n\n        # Restore the boolean tensor\n        if self.has_attention_mask or self.has_bool_tensors:\n            outputs = list(outputs)\n            outputs[-1] = outputs[-1].bool()\n            outputs = tuple(outputs)\n\n        if self.wall_clock_breakdown():\n            self.timers(PIPE_SEND_OUTPUT_TIMER).stop()\n\n    def _exec_send_grads(self, buffer_id):\n        if self.wall_clock_breakdown():\n            self.timers(PIPE_SEND_GRAD_TIMER).start()\n\n        inputs = self.pipe_buffers['inputs'][buffer_id]\n\n        # Partition the gradient\n        if self.is_grad_partitioned:\n            if isinstance(inputs, tuple):\n                first_input = inputs[0]\n                assert all([torch.is_tensor(elt) for elt in inputs[1:]])\n                inputs_grad_tail = [elt.grad for elt in inputs[1:]]\n            elif torch.is_tensor(inputs):\n                first_input = inputs\n                inputs_grad_tail = []\n            else:\n                raise ValueError(\"expecting a tensor or a tuple of tensors\")\n            assert torch.is_tensor(first_input)\n            part = PartitionedTensor(tensor=first_input.grad, group=self.grid.get_slice_parallel_group())\n\n            inputs = (part.to_meta(), part.data(), *inputs_grad_tail)\n\n        # XXX Terrible hack\n        # Drop the attention mask from the input buffer here. It does not have\n        # a grad that needs to be communicated. We free the buffer immediately\n        # after, so no need to restore it. The receiver also has a hack that skips\n        # the recv. This is because NCCL does not let us send torch.BoolTensor :-(.\n        if self.has_attention_mask or self.has_bool_tensors:\n            inputs = list(inputs)\n            inputs.pop()\n            inputs = tuple(inputs)\n\n        if isinstance(inputs, torch.Tensor):\n            assert inputs.grad is not None\n            p2p.send(inputs.grad, self.prev_stage)\n        else:\n            # XXX terrible hacky branch\n            if self.is_grad_partitioned:\n                # First two sends are partitioned gradient\n                p2p.send(inputs[0], self.prev_stage)\n                p2p.send(inputs[1], self.prev_stage)\n            else:\n                for idx, buffer in enumerate(inputs):\n                    # Skip tensors that will not produce a grad\n                    if not buffer.is_floating_point():\n                        assert buffer.grad is None\n                        continue\n                    assert buffer.grad is not None\n                    p2p.send(buffer.grad, self.prev_stage)\n\n        # We can free up the input buffer now\n        self.pipe_buffers['inputs'][buffer_id] = None\n\n        if self.wall_clock_breakdown():\n            self.timers(PIPE_SEND_GRAD_TIMER).stop()\n\n    def _exec_recv_activations(self, buffer_id):\n        if self.wall_clock_breakdown():\n            self.timers(PIPE_RECV_INPUT_TIMER).start()\n\n        recvd = None\n\n        # Allocate the buffer if necessary\n        if self.pipe_recv_buf is None:\n            self.pipe_recv_buf = self._recv_tensor_meta(self.prev_stage)\n\n        if isinstance(self.pipe_recv_buf, torch.Tensor):\n            p2p.recv(self.pipe_recv_buf, self.prev_stage)\n            recvd = self.pipe_recv_buf.clone().detach()\n            recvd.requires_grad = recvd.is_floating_point()\n        else:\n            assert isinstance(self.pipe_recv_buf, tuple)\n            recvd = [None] * len(self.pipe_recv_buf)\n            for idx, buffer in enumerate(self.pipe_recv_buf):\n                assert torch.is_tensor(buffer)\n                # XXX hardcode meta type\n                if self.is_pipe_partitioned and idx == 0 and buffer.dtype != torch.long:\n                    if self.meta_buffer is None:\n                        self.meta_buffer = torch.zeros(buffer.size(), dtype=torch.long, device=self.device)\n                    buffer = self.meta_buffer\n\n                p2p.recv(buffer, self.prev_stage)\n                recvd[idx] = buffer.clone().detach()\n\n            # NCCL does not like to send torch.BoolTensor types, so un-cast the\n            # attention mask\n            if self.has_attention_mask or self.has_bool_tensors:\n                recvd[-1] = recvd[-1].bool()\n\n            recvd = tuple(recvd)\n\n            for buffer in recvd:\n                buffer.requires_grad = buffer.is_floating_point()\n\n        self.pipe_buffers['inputs'][buffer_id] = recvd\n\n        if self.wall_clock_breakdown():\n            self.timers(PIPE_RECV_INPUT_TIMER).stop()\n\n    def _exec_recv_grads(self, buffer_id):\n        if self.wall_clock_breakdown():\n            self.timers(PIPE_RECV_GRAD_TIMER).start()\n\n        outputs = self.pipe_buffers['outputs'][buffer_id]\n        # XXX these shapes are hardcoded for Megatron\n        # Restore partitioned output if it was partitioned and we are sending full gradients\n        if self.is_pipe_partitioned and not self.is_grad_partitioned:\n            if self.pipe_partition_grad_meta_cache is None:\n                self.pipe_partition_grad_meta_cache = outputs[0].to('cpu')\n            part_output = PartitionedTensor.from_meta(meta=self.pipe_partition_grad_meta_cache,\n                                                      local_part=outputs[1],\n                                                      group=self.grid.get_slice_parallel_group())\n            outputs[0].data = part_output.full()\n            outputs = (outputs[0], *outputs[2:])\n            # save for backward\n            self.pipe_buffers['outputs'][buffer_id] = outputs\n\n        # Allocate gradient if necessary\n        if self.grad_layer is None:\n            if isinstance(outputs, torch.Tensor):\n                s = list(outputs.size())\n                self.grad_layer = self._allocate_buffer(s, dtype=outputs.dtype, num_buffers=1)[0]\n            else:\n                # XXX This is a HACK\n                # When we exchange activations/gradients, the two pipe stages\n                # need to issue the send/recv with the same buffer sizes or\n                # else there is a deadlock. The is_floating_point() filter is\n                # used to avoid sending gradients for tensors that do not\n                # produce gradients. When TP>1, we partition the first\n                # activations/gradients across TP ranks to save communication\n                # volume and memory. That partitioned tensor is represented as\n                # two tensors: a 1/TPth chunk of the original data and also a\n                # small LongTensor storing the metadata used to reconstruct on\n                # the other side. When combined, the floating point filter also\n                # filtered out the metadata tensor. This quick (hacky) fix just\n                # branches on is_grad_partitioned so we don't filter out the\n                # metadata tensor.\n                if self.is_grad_partitioned:\n                    sizes_and_dtypes = [(list(t.size()), t.dtype)\n                                        for t in outputs[:2]] + [(list(t.size()), t.dtype)\n                                                                 for t in outputs[2:] if t.is_floating_point()]\n                else:\n                    sizes_and_dtypes = [(list(t.size()), t.dtype) for t in outputs if t.is_floating_point()]\n                self.grad_layer = self._allocate_buffers(sizes_and_dtypes, num_buffers=1)[0]\n\n        if isinstance(self.grad_layer, torch.Tensor):\n            p2p.recv(self.grad_layer, self.next_stage)\n        else:\n            assert isinstance(outputs, tuple)\n            for idx, buffer in enumerate(self.grad_layer):\n                # XXX GPT-2 hack\n                if self.is_grad_partitioned and idx == 0 and buffer.dtype != torch.long:\n                    buffer.data = torch.zeros(buffer.size(), dtype=torch.long, device=self.device)\n                p2p.recv(buffer, self.next_stage)\n\n        if self.wall_clock_breakdown():\n            self.timers(PIPE_RECV_GRAD_TIMER).stop()\n\n    def _exec_optimizer_step(self, lr_kwargs=None):\n        if self.wall_clock_breakdown():\n            self.timers(STEP_MICRO_TIMER).start()\n            self.timers(STEP_GLOBAL_TIMER).start()\n        self.mem_status('BEFORE STEP', reset_max=True)\n\n        self._force_grad_boundary = True\n        self._take_model_step(lr_kwargs)\n        self._force_grad_boundary = False\n\n        self.mem_status('AFTER STEP')\n\n        if self.global_rank == 0 and self.monitor.enabled:\n            self.summary_events = [(f'Train/Samples/lr', self.get_lr()[0], self.global_samples)]\n            if self.fp16_enabled() and hasattr(self.optimizer, 'cur_scale'):\n                self.summary_events.append(\n                    (f'Train/Samples/loss_scale', self.optimizer.cur_scale, self.global_samples))\n            self.monitor.write_events(self.summary_events)\n\n        if self.wall_clock_breakdown():\n            self.timers(STEP_MICRO_TIMER).stop()\n            self.timers(STEP_GLOBAL_TIMER).stop()\n            if self.global_steps % self.steps_per_print() == 0:\n                self.timers.log([\n                    BATCH_INPUT_TIMER,\n                    FORWARD_MICRO_TIMER,\n                    BACKWARD_MICRO_TIMER,\n                    BACKWARD_INNER_MICRO_TIMER,\n                    BACKWARD_REDUCE_MICRO_TIMER,\n                    STEP_MICRO_TIMER,\n                ])\n            if self.global_steps % self.steps_per_print() == 0:\n                self.timers.log([\n                    FORWARD_GLOBAL_TIMER,\n                    BACKWARD_GLOBAL_TIMER,\n                    BACKWARD_INNER_GLOBAL_TIMER,\n                    BACKWARD_REDUCE_GLOBAL_TIMER,\n                    STEP_GLOBAL_TIMER,\n                ])\n\n    def _allocate_zeros(self, shape, **kwargs):\n        \"\"\" Allocate a tensor of zeros on the engine's device.\n\n        Arguments:\n            shape: the shape of the tensor to allocate\n            kwargs: passed to torch.zeros()\n\n        Returns:\n            A tensor from torch.zeros() allocated on self.device.\n        \"\"\"\n        if \"dtype\" not in kwargs:\n            if self.fp16_enabled():\n                kwargs[\"dtype\"] = torch.half\n            if self.bfloat16_enabled():\n                kwargs[\"dtype\"] = torch.bfloat16\n\n        return torch.zeros(shape, device=self.device, **kwargs)\n\n    def _allocate_buffer(self, shape, num_buffers=-1, **kwargs):\n        buffers = []\n        if num_buffers == -1:\n            num_buffers = self.num_pipe_buffers\n        for count in range(num_buffers):\n            buffers.append(self._allocate_zeros(shape, **kwargs))\n        return buffers\n\n    def _allocate_buffers(self, shapes_and_dtypes, requires_grad=False, num_buffers=-1):\n        buffers = []\n        if num_buffers == -1:\n            num_buffers = self.num_pipe_buffers\n        for count in range(num_buffers):\n            buffer = []\n            for shape, dtype in shapes_and_dtypes:\n                buffer.append(self._allocate_zeros(shape, dtype=dtype, requires_grad=requires_grad))\n            buffers.append(buffer)\n        return buffers\n\n    def forward(self, *args, **kwargs):\n        \"\"\"Disabled for pipeline parallel training. See ``train_batch()``. \"\"\"\n        raise PipelineError(\"Only train_batch() is accessible in pipeline mode.\")\n\n    def backward(self, *args, **kwargs):\n        \"\"\"Disabled for pipeline parallel training. See ``train_batch()``. \"\"\"\n        raise PipelineError(\"Only train_batch() is accessible in pipeline mode.\")\n\n    def step(self, *args, **kwargs):\n        \"\"\"Disabled for pipeline parallel training. See ``train_batch()``. \"\"\"\n        raise PipelineError(\"Only train_batch() is accessible in pipeline mode.\")\n\n    def mem_status(self, msg, print_rank=-1, reset_max=False):\n        return\n        global mem_alloced, mem_cached\n        if not self.global_steps == 0 or not self.global_steps == 9:\n            #return\n            pass\n        if self.mpu.get_data_parallel_rank() != 0:\n            return\n\n        if self.global_rank != 0:\n            return\n\n        rank = self.global_rank\n        if print_rank != -1 and rank != print_rank:\n            return\n\n        get_accelerator().synchronize()\n\n        if reset_max:\n            get_accelerator().reset_max_memory_cached()\n            get_accelerator().reset_max_memory_allocated()\n\n        new_alloced = get_accelerator().memory_allocated()\n        new_cached = get_accelerator().memory_cached()\n\n        delta_alloced = new_alloced - mem_alloced\n        delta_cached = new_cached - mem_cached\n\n        mem_cached = new_cached\n        mem_alloced = new_alloced\n\n        max_alloced = get_accelerator().max_memory_allocated()\n        max_cached = get_accelerator().max_memory_cached()\n\n        # convert to GB for printing\n        new_alloced /= 1024**3\n        new_cached /= 1024**3\n        delta_alloced /= 1024**3\n        delta_cached /= 1024**3\n        max_alloced /= 1024**3\n        max_cached /= 1024**3\n\n        print(\n            f'RANK={rank} STAGE={self.stage_id} STEP={self.global_steps} MEMSTATS', msg,\n            f'current alloc={new_alloced:0.4f}GB (delta={delta_alloced:0.4f}GB max={max_alloced:0.4f}GB) '\n            f'current cache={new_cached:0.4f}GB (delta={delta_cached:0.4f}GB max={max_cached:0.4f}GB)')\n\n    def module_state_dict(self, exclude_frozen_parameters=False):\n        \"\"\"Override hack to save a pipe model and return the directory path of the save.\n\n        This method should only be called by DeepSpeed's ``save_checkpoint()``. The\n        recommended way of saving a ``PipelineModule`` outside of ``save_checkpoint()``\n        is ``save_state_dict()``.\n\n        Returns:\n            None\n        \"\"\"\n        assert isinstance(self.module, PipelineModule)\n        assert self._curr_ckpt_path is not None, \\\n            \"PipelineEngine expects module_state_dict() to be called from save_checkpoint()\"\n\n        self.module.save_state_dict(self._curr_ckpt_path,\n                                    checkpoint_engine=self.checkpoint_engine,\n                                    exclude_frozen_params=exclude_frozen_parameters)\n        return None\n\n    def load_module_state_dict(self, checkpoint, strict=True, custom_load_fn=None, fetch_z3_params=False):\n        \"\"\"Override hack to instead use a directory path.\n\n        This is important because pipeline models checkpoint by layer instead of rank.\n\n        If ``state_dict`` is not ``None`` or a ``str``, we revert to ``super()`` expecting a ``dict``.\n\n        Args:\n            state_dict (str, None): unused\n            strict (bool, optional): Strict state loading. Defaults to True.\n        \"\"\"\n        assert custom_load_fn is None, \"custom_load_fn not supported w. pipeline parallelism\"\n        state_dict = checkpoint if self.has_moe_layers else checkpoint['module']\n        if (state_dict is not None) and (not isinstance(state_dict, str)):\n            super().load_module_state_dict(state_dict, strict)\n            return\n\n        self.module.load_state_dir(load_dir=self._curr_ckpt_path,\n                                   strict=strict,\n                                   checkpoint_engine=self.checkpoint_engine)\n\n    # A map of PipeInstruction types to methods. Each method will be executed with the\n    # kwargs provided to the PipeInstruction from the scheduler.\n    _INSTRUCTION_MAP = {\n        schedule.OptimizerStep: _exec_optimizer_step,\n        schedule.ReduceGrads: _exec_reduce_grads,\n        schedule.ReduceTiedGrads: _exec_reduce_tied_grads,\n        schedule.LoadMicroBatch: _exec_load_micro_batch,\n        schedule.ForwardPass: _exec_forward_pass,\n        schedule.BackwardPass: _exec_backward_pass,\n        schedule.SendActivation: _exec_send_activations,\n        schedule.RecvActivation: _exec_recv_activations,\n        schedule.SendGrad: _exec_send_grads,\n        schedule.RecvGrad: _exec_recv_grads,\n    }\n\n    def _exec_schedule(self, pipe_schedule):\n        # Reserve and reset buffers.\n        self._reserve_pipe_buffers(pipe_schedule.num_pipe_buffers())\n        self.fwd_outputs = []\n\n        # For each step in the schedule\n        for step_cmds in pipe_schedule:\n            # For each instruction in the step\n            for cmd in step_cmds:\n                if type(cmd) not in self._INSTRUCTION_MAP:\n                    raise RuntimeError(f'{self.__class__.__name__} does not understand instruction {repr(cmd)}')\n\n                # Equivalent to: self._exec_forward_pass(buffer_id=0)\n                self._exec_instr = MethodType(self._INSTRUCTION_MAP[type(cmd)], self)\n                self._exec_instr(**cmd.kwargs)\n\n    def get_additional_losses(self):\n        return self.agg_additional_losses\n", "deepspeed/runtime/pipe/topology.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom deepspeed import comm as dist\n\nfrom collections import namedtuple\nfrom itertools import product as cartesian_product\n\n\nclass ProcessTopology:\n    \"\"\" Manages the mapping of n-dimensional Cartesian coordinates to linear\n    indices. This mapping is used to map the rank of processes to the grid\n    for various forms of parallelism.\n\n    Each axis of the tensor is accessed by its name. The provided ordering\n    of the axes defines the layout of the topology. ProcessTopology uses a \"row-major\"\n    layout of the tensor axes, and so axes=['x', 'y'] would map coordinates (x,y) and\n    (x,y+1) to adjacent linear indices. If instead axes=['y', 'x'] was used, coordinates\n    (x,y) and (x+1,y) would be adjacent.\n\n    Some methods return ProcessCoord namedtuples.\n    \"\"\"\n\n    def __init__(self, axes, dims):\n        \"\"\"Create a mapping of n-dimensional tensor coordinates to linear indices.\n\n        Arguments:\n            axes (list): the names of the tensor axes\n            dims (list): the dimension (length) of each axis of the topology tensor\n        \"\"\"\n\n        self.axes = axes  # names of each topology axis\n        self.dims = dims  # length of each topology axis\n\n        # This is actually a class that lets us hash {'row':3, 'col':2} mappings\n        self.ProcessCoord = namedtuple('ProcessCoord', axes)\n\n        self.mapping = {}\n        ranges = [range(d) for d in dims]\n        # example: 1, (0,0,1)\n        for global_rank, coord in enumerate(cartesian_product(*ranges)):\n            key = {axis: coord[self.axes.index(axis)] for axis in self.axes}\n            key = self.ProcessCoord(**key)\n            # for example, {ProcessCoord(row=0, col=1) : 1}\n            self.mapping[key] = global_rank\n\n    def get_rank(self, **coord_kwargs):\n        \"\"\"Return the global rank of a process via its coordinates.\n\n        Coordinates are specified as kwargs. For example:\n\n            >>> X = ProcessTopology(axes=['x', 'y'], dims=[2,3])\n            >>> X.get_rank(x=0, y=1)\n            1\n        \"\"\"\n        if len(coord_kwargs) != len(self.axes):\n            raise ValueError('get_rank() does not support slices. Use filter_match())')\n\n        key = self.ProcessCoord(**coord_kwargs)\n        assert key in self.mapping, f'key {coord_kwargs} invalid'\n        return self.mapping[key]\n\n    def get_axis_names(self):\n        \"\"\"Return a list of the axis names in the ordering of the topology. \"\"\"\n        return self.axes\n\n    def get_rank_repr(self, rank, omit_axes=['data', 'pipe'], inner_sep='_', outer_sep='-'):\n        \"\"\"Return a string representation of a rank.\n\n        This method is primarily used for checkpointing model data.\n\n        For example:\n            >>> topo = Topo(axes=['a', 'b'], dims=[2, 2])\n            >>> topo.get_rank_repr(rank=3)\n            'a_01-b_01'\n            >>> topo.get_rank_repr(rank=3, omit_axes=['a'])\n            'b_01'\n\n        Args:\n            rank (int): A rank in the topology.\n            omit_axes (list, optional): Axes that should not be in the representation. Defaults to ['data', 'pipe'].\n            inner_sep (str, optional): [description]. Defaults to '_'.\n            outer_sep (str, optional): [description]. Defaults to '-'.\n\n        Returns:\n            str: A string representation of the coordinate owned by ``rank``.\n        \"\"\"\n        omit_axes = frozenset(omit_axes)\n        axes = [a for a in self.get_axis_names() if a not in omit_axes]\n        names = []\n        for ax in axes:\n            ax_rank = getattr(self.get_coord(rank=rank), ax)\n            names.append(f'{ax}{inner_sep}{ax_rank:02d}')\n        return outer_sep.join(names)\n\n    def get_dim(self, axis):\n        \"\"\"Return the number of processes along the given axis.\n\n        For example:\n            >>> X = ProcessTopology(axes=['x', 'y'], dims=[2,3])\n            >>> X.get_dim('y')\n            3\n        \"\"\"\n        if axis not in self.axes:\n            return 0\n        return self.dims[self.axes.index(axis)]\n\n    def get_coord(self, rank):\n        \"\"\"Return the coordinate owned by a process rank.\n\n        The axes of the returned namedtuple can be directly accessed as members. For\n        example:\n            >>> X = ProcessTopology(axes=['x', 'y'], dims=[2,3])\n            >>> coord = X.get_coord(rank=1)\n            >>> coord.x\n            0\n            >>> coord.y\n            1\n        \"\"\"\n        for coord, idx in self.mapping.items():\n            if idx == rank:\n                return coord\n        raise ValueError(f'rank {rank} not found in topology.')\n\n    def get_axis_comm_lists(self, axis):\n        \"\"\" Construct lists suitable for a communicator group along axis ``axis``.\n\n        Example:\n            >>> topo = Topo(axes=['pipe', 'data', 'model'], dims=[2, 2, 2])\n            >>> topo.get_axis_comm_lists('pipe')\n            [\n                [0, 4], # data=0, model=0\n                [1, 5], # data=0, model=1\n                [2, 6], # data=1, model=0\n                [3, 7], # data=1, model=1\n            ]\n\n        Returns:\n            A list of lists whose coordinates match in all axes *except* ``axis``.\n        \"\"\"\n\n        # We don't want to RuntimeError because it allows us to write more generalized\n        # code for hybrid parallelisms.\n        if axis not in self.axes:\n            return []\n\n        # Grab all axes but `axis`\n        other_axes = [a for a in self.axes if a != axis]\n\n        lists = []\n\n        # Construct all combinations of coords with other_axes\n        ranges = [range(self.get_dim(a)) for a in other_axes]\n        for coord in cartesian_product(*ranges):\n            other_keys = {a: coord[other_axes.index(a)] for a in other_axes}\n            # now go over all ranks in `axis`.\n            sub_list = []\n            for axis_key in range(self.get_dim(axis)):\n                key = self.ProcessCoord(**other_keys, **{axis: axis_key})\n                sub_list.append(self.mapping[key])\n            lists.append(sub_list)\n\n        return lists\n\n    def filter_match(self, **filter_kwargs):\n        \"\"\"Return the list of ranks whose coordinates match the provided criteria.\n\n        Example:\n            >>> X = ProcessTopology(axes=['pipe', 'data', 'model'], dims=[2, 2, 2])\n            >>> X.filter_match(pipe=0, data=1)\n            [2, 3]\n            >>> [X.get_coord(rank) for rank in X.filter_match(pipe=0, data=1)]\n            [ProcessCoord(pipe=0, data=1, model=0), ProcessCoord(pipe=0, data=1, model=1)]\n\n        Arguments:\n            **filter_kwargs (dict): criteria used to select coordinates.\n\n        Returns:\n            The list of ranks whose coordinates match filter_kwargs.\n        \"\"\"\n\n        def _filter_helper(x):\n            for key, val in filter_kwargs.items():\n                if getattr(x, key) != val:\n                    return False\n            return True\n\n        coords = filter(_filter_helper, self.mapping.keys())\n        return [self.mapping[coord] for coord in coords]\n\n    def get_axis_list(self, axis, idx):\n        \"\"\"Returns the list of global ranks whose coordinate in an axis is idx.\n\n        For example:\n            >>> X = ProcessTopology(axes=['x', 'y'], dims=[2,3])\n            >>> X.get_axis_list(axis='x', idx=0)\n            [0, 1, 2]\n            >>> X.get_axis_list(axis='y', idx=0)\n            [0, 3]\n        \"\"\"\n\n        # This could be faster by generating the desired keys directly instead of\n        # filtering.\n        axis_num = self.axes.index(axis)\n        ranks = [self.mapping[k] for k in self.mapping.keys() if k[axis_num] == idx]\n        return ranks\n\n    def world_size(self):\n        return len(self.mapping)\n\n    def __str__(self):\n        return str(self.mapping)\n\n\ndef _prime_factors(N):\n    \"\"\" Returns the prime factorization of positive integer N. \"\"\"\n    if N <= 0:\n        raise ValueError(\"Values must be strictly positive.\")\n\n    primes = []\n    while N != 1:\n        for candidate in range(2, N + 1):\n            if N % candidate == 0:\n                primes.append(candidate)\n                N //= candidate\n                break\n    return primes\n\n\nclass PipeDataParallelTopology(ProcessTopology):\n    \"\"\" A topology specialization for hybrid data and pipeline parallelism.\n\n        Uses data parallelism on the last dimension to encourage gradient\n        reductions to use high-bandwidth intra-node links and lower-volume\n        pipeline communications to use low-bandwidth inter-node links.\n    \"\"\"\n\n    def __init__(self, num_pp, num_dp):\n        super().__init__(axes=['pipe', 'data'], dims=[num_pp, num_dp])\n\n\nclass PipeModelDataParallelTopology(ProcessTopology):\n    \"\"\" A topology for hybrid pipeline, model, and data parallelism. \"\"\"\n\n    def __init__(self, num_pp, num_mp, num_dp):\n        super().__init__(axes=['pipe', 'data', 'model'], dims=[num_pp, num_dp, num_mp])\n\n\nclass PipelineParallelGrid:\n    \"\"\"Implements a grid object that stores the data parallel ranks\n    corresponding to each of the model parallel stages\n\n    The grid object organizes the processes in a distributed pytorch job\n    into a 2D grid, of stage_id and data_parallel_id.\n\n    self.stage_id and self.data_parallel_id stores the stage id\n    and the data parallel id of current process.\n\n    self.dp_group groups the processes by stage_id.\n    self.dp_group[i], is a list containing all process ranks whose\n    stage_id is i.\n\n    self.p2p_groups stores a list of tuple, where each tuple\n    stores process ranks of adjacent stages for a given data_parallel_id.\n    For example if num_stage is 5 then a tuple [7,8] represents stages [3, 4],\n    with data_parallel id = 1. A stage wrap around will appear as non-adjacent ranks,\n    for example tuple [4,0] with representing wrap-around stage 4 and 0, for\n    data_parallel_id = 0, or similarly [9,5] represents wrapped around stages [4,0]\n    for data_parallel_id = 1.\n    \"\"\"\n\n    def __init__(self, topology=None, process_group=None):\n        # TODO use process_group if provided\n        self.global_rank = dist.get_rank()\n        self.world_size = dist.get_world_size()\n        if topology is not None:\n            if self.global_rank == 0:\n                print('Using topology:', topology)\n            self._topo = topology\n        else:\n            num_pp = 1\n            num_dp = 1\n            for idx, prime in enumerate(_prime_factors(self.world_size)):\n                if idx % 2 == 0:\n                    num_pp *= prime\n                else:\n                    num_dp *= prime\n            self._topo = PipeDataParallelTopology(num_dp=num_dp, num_pp=num_pp)\n        self.data_parallel_size = max(self._topo.get_dim('data'), 1)\n        self.pipe_parallel_size = max(self._topo.get_dim('pipe'), 1)\n        self.model_parallel_size = max(self._topo.get_dim('model'), 1)\n        self.slice_parallel_size = self.model_parallel_size\n        assert self._is_grid_valid(), \"Invalid Grid\"\n\n        self.stage_id = self.get_stage_id()\n        self.data_parallel_id = self.get_data_parallel_id()\n\n        # Create new ProcessGroups for all model parallelism. DeepSpeedLight uses these\n        # to detect overflow, etc.\n        self.ds_model_proc_group = None\n        self.ds_model_rank = -1\n        for dp in range(self.data_parallel_size):\n            ranks = sorted(self._topo.get_axis_list(axis='data', idx=dp))\n            if self.global_rank == 0:\n                #print(f'RANK={self.global_rank} building DeepSpeed model group: {ranks}')\n                pass\n            proc_group = dist.new_group(ranks=ranks)\n            if self.global_rank in ranks:\n                self.ds_model_proc_group = proc_group\n                self.ds_model_world_size = len(ranks)\n                self.ds_model_rank = ranks.index(self.global_rank)\n        assert self.ds_model_rank > -1\n        assert self.ds_model_proc_group is not None\n\n        # Create new ProcessGroup for gradient all-reduces - these are the data parallel groups\n        self.dp_group = []\n        self.dp_groups = self._topo.get_axis_comm_lists('data')\n        for g in self.dp_groups:\n            proc_group = dist.new_group(ranks=g)\n            if self.global_rank in g:\n                self.dp_group = g\n                self.dp_proc_group = proc_group\n\n        self.is_first_stage = (self.stage_id == 0)\n        self.is_last_stage = (self.stage_id == (self.pipe_parallel_size - 1))\n\n        self.p2p_groups = self._build_p2p_groups()\n\n        # Create new ProcessGroup for pipeline collectives - these are pipe parallel groups\n        self.pp_group = []\n        self.pp_proc_group = None\n        self.pipe_groups = self._topo.get_axis_comm_lists('pipe')\n        for ranks in self.pipe_groups:\n            if self.global_rank == 0:\n                #print(f'RANK={self.global_rank} building pipeline group: {ranks}')\n                pass\n            proc_group = dist.new_group(ranks=ranks)\n            if self.global_rank in ranks:\n                self.pp_group = ranks\n                self.pp_proc_group = proc_group\n        assert self.pp_proc_group is not None\n\n        # Create new ProcessGroup for model (tensor-slicing) collectives\n\n        # Short circuit case without model parallelism.\n        # TODO: it would be nice if topology had bcast semantics to avoid this branching\n        # case?\n        if self.model_parallel_size == 1:\n            for group_rank in range(self.world_size):\n                group_rank = [group_rank]\n                group = dist.new_group(ranks=group_rank)\n                if group_rank[0] == self.global_rank:\n                    self.slice_group = group_rank\n                    self.slice_proc_group = group\n            return\n        else:\n            self.mp_group = []\n            self.model_groups = self._topo.get_axis_comm_lists('model')\n            for g in self.model_groups:\n                proc_group = dist.new_group(ranks=g)\n                if self.global_rank in g:\n                    self.slice_group = g\n                    self.slice_proc_group = proc_group\n\n    def get_stage_id(self):\n        return self._topo.get_coord(rank=self.global_rank).pipe\n\n    def get_data_parallel_id(self):\n        return self._topo.get_coord(rank=self.global_rank).data\n\n    def _build_p2p_groups(self):\n        \"\"\"Groups for sending and receiving activations and gradients across model\n        parallel stages.\n        \"\"\"\n        comm_lists = self._topo.get_axis_comm_lists('pipe')\n        p2p_lists = []\n        for rank in range(self.world_size):\n            for l in comm_lists:\n                assert len(l) == self.pipe_parallel_size\n                if rank in l:\n                    idx = l.index(rank)\n                    buddy_rank = l[(idx + 1) % self.pipe_parallel_size]\n                    p2p_lists.append([rank, buddy_rank])\n                    break  # next global rank\n        assert len(p2p_lists) == self.world_size\n        return p2p_lists\n\n    def _is_grid_valid(self):\n        ranks = 1\n        for ax in self._topo.get_axis_names():\n            ranks *= self._topo.get_dim(ax)\n        return ranks == dist.get_world_size()\n\n    #returns the global rank of the process with the provided stage id\n    #which has the same data_parallel_id as caller process\n    def stage_to_global(self, stage_id, **kwargs):\n        me = self._topo.get_coord(self.global_rank)\n        transform = me._replace(pipe=stage_id, **kwargs)._asdict()\n        return self._topo.get_rank(**transform)\n\n    def topology(self):\n        return self._topo\n\n    # MPU functions for DeepSpeed integration\n    def get_global_rank(self):\n        return self.global_rank\n\n    def get_pipe_parallel_rank(self):\n        \"\"\" The stage of the pipeline this rank resides in. \"\"\"\n        return self.get_stage_id()\n\n    def get_pipe_parallel_world_size(self):\n        \"\"\" The number of stages in the pipeline. \"\"\"\n        return self.pipe_parallel_size\n\n    def get_pipe_parallel_group(self):\n        \"\"\" The group of ranks within the same pipeline. \"\"\"\n        return self.pp_proc_group\n\n    def get_data_parallel_rank(self):\n        \"\"\" Which pipeline this rank resides in. \"\"\"\n        return self.data_parallel_id\n\n    def get_data_parallel_world_size(self):\n        \"\"\" The number of pipelines. \"\"\"\n        return self.data_parallel_size\n\n    def get_data_parallel_group(self):\n        \"\"\" The group of ranks within the same stage of all pipelines. \"\"\"\n        return self.dp_proc_group\n\n    # These are model parallel groups across all types of model parallelism.\n    # Deepspeed uses them to detect overflow, etc.\n    def get_model_parallel_rank(self):\n        return self.ds_model_rank\n\n    def get_model_parallel_world_size(self):\n        return self.ds_model_world_size\n\n    def get_model_parallel_group(self):\n        return self.ds_model_proc_group\n\n    # For Megatron-style tensor slicing\n    def get_slice_parallel_rank(self):\n        if 'model' in self._topo.get_axis_names():\n            return self._topo.get_coord(rank=self.global_rank).model\n        else:\n            return 0\n\n    def get_slice_parallel_world_size(self):\n        return self.slice_parallel_size\n\n    def get_slice_parallel_group(self):\n        return self.slice_proc_group\n", "deepspeed/runtime/pipe/module.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport os\nimport glob\n\nimport re as regex\n\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\nfrom deepspeed import comm as dist\n\nfrom deepspeed.utils import logger\nfrom .. import utils as ds_utils\nfrom ..activation_checkpointing import checkpointing\nfrom .topology import PipeDataParallelTopology, PipelineParallelGrid\nfrom deepspeed.runtime.state_dict_factory import SDLoaderFactory\nfrom deepspeed.accelerator import get_accelerator\nfrom deepspeed.checkpoint.utils import clone_tensors_for_torch_save\n\n\nclass PipelineError(Exception):\n    \"\"\"Errors related to the use of deepspeed.PipelineModule \"\"\"\n\n\nclass LayerSpec:\n    \"\"\"Building block for specifying pipeline-parallel modules.\n\n    LayerSpec stores the type information and parameters for each stage in a\n    PipelineModule. For example:\n\n    .. code-block:: python\n\n        nn.Sequence(\n            torch.nn.Linear(self.in_dim, self.hidden_dim, bias=False),\n            torch.nn.Linear(self.hidden_hidden, self.out_dim)\n        )\n\n    becomes\n\n    .. code-block:: python\n\n        layer_specs = [\n            LayerSpec(torch.nn.Linear, self.in_dim, self.hidden_dim, bias=False),\n            LayerSpec(torch.nn.Linear, self.hidden_hidden, self.out_dim)]\n        ]\n    \"\"\"\n\n    def __init__(self, typename, *module_args, **module_kwargs):\n        self.typename = typename\n        self.module_args = module_args\n        self.module_kwargs = module_kwargs\n\n        if not issubclass(typename, nn.Module):\n            raise RuntimeError('LayerSpec only supports torch.nn.Module types.')\n\n        if dist.is_initialized():\n            self.global_rank = dist.get_rank()\n        else:\n            self.global_rank = -1\n\n    def __repr__(self):\n        return ds_utils.call_to_str(self.typename.__name__, self.module_args, self.module_kwargs)\n\n    def build(self, log=False):\n        \"\"\"Build the stored specification.\"\"\"\n        if log:\n            logger.info(f'RANK={self.global_rank} building {repr(self)}')\n\n        return self.typename(*self.module_args, **self.module_kwargs)\n\n\nclass TiedLayerSpec(LayerSpec):\n\n    def __init__(self, key, typename, *module_args, forward_fn=None, tied_weight_attr=['weight'], **module_kwargs):\n        super().__init__(typename, *module_args, **module_kwargs)\n        self.key = key\n        self.forward_fn = forward_fn\n        self.tied_weight_attr = [tied_weight_attr] if type(tied_weight_attr) == str else tied_weight_attr\n\n\nclass PipelineModule(nn.Module):\n    \"\"\"Modules to be parallelized with pipeline parallelism.\n\n    The key constraint that enables pipeline parallelism is the\n    representation of the forward pass as a sequence of layers\n    and the enforcement of a simple interface between them. The\n    forward pass is implicitly defined by the module ``layers``. The key\n    assumption is that the output of each layer can be directly fed as\n    input to the next, like a ``torch.nn.Sequence``. The forward pass is\n    implicitly:\n\n    .. code-block:: python\n\n        def forward(self, inputs):\n            x = inputs\n            for layer in self.layers:\n                x = layer(x)\n            return x\n\n    .. note::\n        Pipeline parallelism is not compatible with ZeRO-2 and ZeRO-3.\n\n    Args:\n        layers (Iterable): A sequence of layers defining pipeline structure. Can be a ``torch.nn.Sequential`` module.\n        num_stages (int, optional): The degree of pipeline parallelism. If not specified, ``topology`` must be provided.\n        topology (``deepspeed.runtime.pipe.ProcessTopology``, optional): Defines the axes of parallelism axes for training. Must be provided if ``num_stages`` is ``None``.\n        loss_fn (callable, optional): Loss is computed ``loss = loss_fn(outputs, label)``\n        seed_layers(bool, optional): Use a different seed for each layer. Defaults to False.\n        seed_fn(type, optional): The custom seed generating function. Defaults to random seed generator.\n        base_seed (int, optional): The starting seed. Defaults to 1234.\n        partition_method (str, optional): The method upon which the layers are partitioned. Defaults to 'parameters'.\n        activation_checkpoint_interval (int, optional): The granularity activation checkpointing in terms of number of layers. 0 disables activation checkpointing.\n        activation_checkpoint_func (callable, optional): The function to use for activation checkpointing. Defaults to ``deepspeed.checkpointing.checkpoint``.\n        checkpointable_layers(list, optional): Checkpointable layers may not be checkpointed. Defaults to None which does not additional filtering.\n    \"\"\"\n\n    def __init__(self,\n                 layers,\n                 num_stages=None,\n                 topology=None,\n                 loss_fn=None,\n                 seed_layers=False,\n                 seed_fn=None,\n                 base_seed=1234,\n                 partition_method='parameters',\n                 activation_checkpoint_interval=0,\n                 activation_checkpoint_func=checkpointing.checkpoint,\n                 checkpointable_layers=None):\n\n        super().__init__()\n\n        if num_stages is None and topology is None:\n            raise RuntimeError('must provide num_stages or topology')\n\n        self.micro_offset = 0\n\n        self.loss_fn = loss_fn\n\n        self.checkpointable_layers = checkpointable_layers\n        if checkpointable_layers is not None:\n            assert isinstance(checkpointable_layers, list), \"param `checkpointable_layers` must be type of list.\"\n\n        self.seed_layers = seed_layers\n        self.seed_fn = seed_fn\n        self.base_seed = base_seed\n        if dist.get_rank() == 0:\n            try:\n                seed_str = self.seed_fn.__name__\n            except AttributeError:\n                seed_str = None\n            print(f'SEED_LAYERS={self.seed_layers} BASE_SEED={self.base_seed} SEED_FN={seed_str}')\n\n        # Setup world info\n        self.world_group = dist.new_group(ranks=range(dist.get_world_size()))\n        self.global_rank = dist.get_rank(group=self.world_group)\n        self.world_size = dist.get_world_size(group=self.world_group)\n        self.local_rank = int(os.environ.get(\"LOCAL_RANK\", None))\n        assert self.local_rank is not None\n\n        if topology:\n            self._topo = topology\n            self.num_stages = self._topo.get_dim('pipe')\n        else:\n            self.num_stages = num_stages\n            if topology is None:\n                if self.world_size % self.num_stages != 0:\n                    raise RuntimeError(\n                        f'num_stages ({self.num_stages}) must divide distributed world size ({self.world_size})')\n                dp = self.world_size // num_stages\n                topology = PipeDataParallelTopology(num_pp=num_stages, num_dp=dp)\n                self._topo = topology\n\n        # Construct communicators for pipeline topology\n        self._grid = PipelineParallelGrid(process_group=self.world_group, topology=self._topo)\n\n        self.stage_id = self._topo.get_coord(self.global_rank).pipe\n\n        # Initialize partition information\n        self._layer_specs = list(layers)\n        self._num_layers = len(self._layer_specs)\n        self._local_start = 0\n        self._local_stop = None\n        self._partition_layers(method=partition_method)\n\n        self.forward_funcs = []\n        self.fwd_map = {}\n        self.tied_modules = nn.ModuleDict()\n        self.tied_weight_attrs = {}\n\n        # Offset the random seed by the stage ID.\n        #newseed = get_accelerator().initial_seed() + self._grid.get_stage_id()\n        #ds_utils.set_random_seed(newseed)\n\n        #with torch.random.fork_rng(devices=[get_accelerator().current_device_name()]):\n        self._build()\n        self.to(get_accelerator().device_name(self.local_rank))\n\n        self.tied_comms = self._index_tied_modules()\n        self._synchronize_tied_weights()\n\n        self.activation_checkpoint_interval = activation_checkpoint_interval\n\n        self.activation_checkpoint_func = activation_checkpoint_func\n        # if configuration use_reentrant = False, self.activation_checkpoint_func will be set to ``checkpointing.non_reentrant_checkpoint``\n\n    def _build(self):\n        specs = self._layer_specs\n\n        for local_idx, layer in enumerate(specs[self._local_start:self._local_stop]):\n            layer_idx = local_idx + self._local_start\n            if self.seed_layers:\n                if self.seed_fn:\n                    self.seed_fn(self.base_seed + layer_idx)\n                else:\n                    ds_utils.set_random_seed(self.base_seed + layer_idx)\n\n            # Recursively build PipelineModule objects\n            if isinstance(layer, PipelineModule):\n                raise NotImplementedError('RECURSIVE BUILD NOT YET IMPLEMENTED')\n\n            # LayerSpec objects contain an nn.Module that should be allocated now.\n            elif isinstance(layer, nn.Module):\n                name = str(layer_idx)\n                self.forward_funcs.append(layer)\n                self.fwd_map.update({name: len(self.forward_funcs) - 1})\n                self.add_module(name, layer)\n\n            # TiedLayerSpec objects contain an nn.Module that should be allocated now.\n            elif isinstance(layer, TiedLayerSpec):\n                # Build and register the module if we haven't seen it before.\n                if layer.key not in self.tied_modules:\n                    self.tied_modules[layer.key] = layer.build()\n                    self.tied_weight_attrs[layer.key] = layer.tied_weight_attr\n\n                if layer.forward_fn is None:\n                    # Just use forward()\n                    self.forward_funcs.append(self.tied_modules[layer.key])\n                else:\n                    # User specified fn with args (module, input)\n                    self.forward_funcs.append(partial(layer.forward_fn, self.tied_modules[layer.key]))\n\n            # LayerSpec objects contain an nn.Module that should be allocated now.\n            elif isinstance(layer, LayerSpec):\n                module = layer.build()\n                name = str(layer_idx)\n                self.forward_funcs.append(module)\n                self.fwd_map.update({name: len(self.forward_funcs) - 1})\n                self.add_module(name, module)\n\n            # Last option: layer may be a functional (e.g., lambda). We do nothing in\n            # that case and just use it in forward()\n            else:\n                self.forward_funcs.append(layer)\n\n        # All pipeline parameters should be considered as model parallel in the context\n        # of our FP16 optimizer\n        for p in self.parameters():\n            p.ds_pipe_replicated = False\n\n    def _get_frozen_parameter_names(self, layer):\n        \"\"\" Get names of frozen parameters in the layer.\n\n            Returns:\n                A list of frozen parameter names\n        \"\"\"\n        if isinstance(layer, LayerSpec):\n            l = layer.build()\n            return [n for n, p in l.named_parameters() if not p.requires_grad]\n        elif isinstance(layer, nn.Module):\n            return [n for n, p in layer.named_parameters() if not p.requires_grad]\n\n        return []\n\n    def _count_layer_params(self):\n        \"\"\"Count the trainable parameters in individual layers.\n\n        This routine will only build one layer at a time.\n\n        Returns:\n            A list of the number of parameters in each layer.\n        \"\"\"\n        param_counts = [0] * len(self._layer_specs)\n        for idx, layer in enumerate(self._layer_specs):\n            if isinstance(layer, LayerSpec):\n                l = layer.build()\n                params = filter(lambda p: p.requires_grad, l.parameters())\n                param_counts[idx] = sum(p.numel() for p in params)\n            elif isinstance(layer, nn.Module):\n                params = filter(lambda p: p.requires_grad, layer.parameters())\n                param_counts[idx] = sum(p.numel() for p in params)\n        return param_counts\n\n    def _find_layer_type(self, layername):\n        idxs = []\n        typeregex = regex.compile(layername, regex.IGNORECASE)\n        for idx, layer in enumerate(self._layer_specs):\n            name = None\n            if isinstance(layer, LayerSpec):\n                name = layer.typename.__name__\n            elif isinstance(layer, nn.Module):\n                name = layer.__class__.__name__\n            else:\n                try:\n                    name = layer.__name__\n                except AttributeError:\n                    continue\n            if typeregex.search(name):\n                idxs.append(idx)\n\n        if len(idxs) == 0:\n            raise RuntimeError(f\"Partitioning '{layername}' found no valid layers to partition.\")\n        return idxs\n\n    def forward(self, forward_input):\n        # We need to offset the seed by the microbatch ID. Save it in a local var to\n        # ensure it is preserved in the closure. Otherwise checkpointed forward funcs\n        # will see a different offset.\n        self.micro_offset += 1\n\n        def exec_range_func(start, end):\n            ''' Helper function to be used with checkpoint()\n            Adapted from torch.utils.checkpoint:checkpoint_sequential()\n            '''\n            local_micro_offset = self.micro_offset + 1\n\n            def exec_func(*inputs):\n                # Single tensor inputs need to be unwrapped\n                if len(inputs) == 1:\n                    inputs = inputs[0]\n                for idx, layer in enumerate(self.forward_funcs[start:end]):\n                    self.curr_layer = idx + self._local_start\n                    if self.seed_layers:\n                        new_seed = (self.base_seed * local_micro_offset) + self.curr_layer\n                        if self.seed_fn:\n                            self.seed_fn(new_seed)\n                        else:\n                            ds_utils.set_random_seed(new_seed)\n\n                    inputs = layer(inputs)\n                return inputs\n\n            return exec_func\n\n        if self.activation_checkpoint_interval == 0:\n            func = exec_range_func(0, len(self.forward_funcs))\n            x = func(forward_input)\n        else:\n            num_layers = len(self.forward_funcs)\n            x = forward_input\n            for start_idx in range(0, num_layers, self.activation_checkpoint_interval):\n                end_idx = min(start_idx + self.activation_checkpoint_interval, num_layers)\n\n                funcs = self.forward_funcs[start_idx:end_idx]\n                # Since we either pass tensors or tuples of tensors without unpacking, we\n                # need to be careful not to double-wrap tensors with tuple.\n                if not isinstance(x, tuple):\n                    x = (x, )\n\n                if self._is_checkpointable(funcs):\n                    x = self.activation_checkpoint_func(exec_range_func(start_idx, end_idx), *x)\n                else:\n                    x = exec_range_func(start_idx, end_idx)(*x)\n        return x\n\n    def _partition_layers(self, method='uniform'):\n        num_stages = self._topo.get_dim('pipe')\n        stage_id = self._topo.get_coord(self.global_rank).pipe\n\n        if self.global_rank == 0:\n            logger.info(f'Partitioning pipeline stages with method {method}')\n\n        method = method.lower()\n\n        # Each stage gets a simple uniform number of layers.\n        if method == 'uniform':\n            num_layers = len(self._layer_specs)\n            self.parts = ds_utils.partition_uniform(num_items=num_layers, num_parts=num_stages)\n        elif method == 'parameters':\n            param_counts = self._count_layer_params()\n            self.parts = ds_utils.partition_balanced(weights=param_counts, num_parts=num_stages)\n        elif method.startswith('type:'):\n            layertype = method.split(':')[1]\n            binary_weights = [0] * len(self._layer_specs)\n            for idx in self._find_layer_type(layertype):\n                binary_weights[idx] = 1\n            self.parts = ds_utils.partition_balanced(weights=binary_weights, num_parts=num_stages)\n        elif method == 'profile':\n            raise NotImplementedError(f'Partitioning method {method} not implemented.')\n        else:\n            raise NotImplementedError(f'Partitioning method {method} not implemented.')\n\n        # Print some information on the partitioning.\n        if self.global_rank == 0:\n            for stage in range(num_stages):\n                start = self.parts[stage]\n                stop = self.parts[stage + 1]\n                print(f'stage={stage} layers={stop - start}')\n                for idx, layer in enumerate(self._layer_specs[start:stop]):\n                    name = str(layer)\n                    if isinstance(layer, LayerSpec):\n                        name = layer.typename.__name__\n                    if isinstance(layer, nn.Module):\n                        name = layer.__class__.__name__\n                    else:\n                        try:\n                            name = layer.__name__\n                        except AttributeError:\n                            pass\n                    print(f'    {idx+start:2d}: {name}')\n            if self.loss_fn:\n                try:\n                    print(f'  loss: {self.loss_fn.__name__}')\n                except AttributeError:\n                    print(f'  loss: {self.loss_fn.__class__.__name__}')\n\n        self._set_bounds(start=self.parts[stage_id], stop=self.parts[stage_id + 1])\n\n    def allreduce_tied_weight_gradients(self):\n        '''All reduce the gradients of the tied weights between tied stages'''\n        for key, comm in self.tied_comms.items():\n            for attr_name in comm['weight_attr']:\n                weight = getattr(self.tied_modules[key], attr_name)\n                dist.all_reduce(weight.grad, group=comm['group'])\n\n    def get_tied_weights_and_groups(self):\n        weight_group_list = []\n        for key, comm in self.tied_comms.items():\n            for attr_name in comm['weight_attr']:\n                weight = getattr(self.tied_modules[key], attr_name)\n                weight_group_list.append((weight, comm['group']))\n        return weight_group_list\n\n    def _synchronize_tied_weights(self):\n        for key, comm in self.tied_comms.items():\n            for attr_name in comm['weight_attr']:\n                dist.broadcast(\n                    getattr(comm['module'], attr_name),\n                    src=min(comm['ranks']),\n                    group=comm['group'],\n                )\n\n    def _index_tied_modules(self):\n        ''' Build communication structures for tied modules. '''\n        tied_comms = {}\n        if self._topo.get_dim('pipe') == 1:\n            return tied_comms\n\n        specs = self._layer_specs\n        tie_keys = set(s.key for s in specs if isinstance(s, TiedLayerSpec))\n        for key in tie_keys:\n            # Find the layers that the tied module appears in\n            tied_layers = []\n            for idx, layer in enumerate(specs):\n                if isinstance(layer, TiedLayerSpec) and layer.key == key:\n                    tied_layers.append(idx)\n            # Find all stages with this tied module\n            # TODO: Would be nice to remove the nested data/model parallelism loops and\n            # TODO: instead generalize in some way, since we really just care about the\n            # TODO: stage that owns the tied layer. Then loop over each (dp, mp, ...)\n            # TODO: fiber to generate process groups.\n            tied_stages = set(self.stage_owner(idx) for idx in tied_layers)\n            for dp in range(self._grid.data_parallel_size):\n                for mp in range(self._grid.get_slice_parallel_world_size()):\n                    tied_ranks = []\n                    for s in sorted(tied_stages):\n                        if self._grid.get_slice_parallel_world_size() > 1:\n                            tied_ranks.append(self._grid.stage_to_global(stage_id=s, data=dp, model=mp))\n                        else:\n                            tied_ranks.append(self._grid.stage_to_global(stage_id=s, data=dp))\n                    group = dist.new_group(ranks=tied_ranks)\n\n                    # Record this tied module if we own a local copy of it.\n                    if self.global_rank in tied_ranks:\n                        assert key in self.tied_modules\n                        if key in self.tied_modules:\n                            tied_comms[key] = {\n                                'ranks': tied_ranks,\n                                'group': group,\n                                'weight_attr': self.tied_weight_attrs[key],\n                                'module': self.tied_modules[key],\n                            }\n                            # Only count the tied module once in the eyes of the FP16 optimizer\n                            if self.global_rank != tied_ranks[0]:\n                                for p in self.tied_modules[key].parameters():\n                                    p.ds_pipe_replicated = True\n        '''\n        if len(tied_comms) > 0:\n            print(f'RANK={self.global_rank} tied_comms={tied_comms}')\n        '''\n\n        return tied_comms\n\n    def partitions(self):\n        return self.parts\n\n    def stage_owner(self, layer_idx):\n        assert 0 <= layer_idx < self._num_layers\n        for stage in range(self._topo.get_dim('pipe')):\n            if self.parts[stage] <= layer_idx < self.parts[stage + 1]:\n                return stage\n        raise RuntimeError(f'Layer {layer_idx} not owned? parts={self.parts}')\n\n    def _set_bounds(self, start=None, stop=None):\n        \"\"\"Manually define the range of layers that will be built on this process.\n\n        These boundaries are treated as list slices and so start is inclusive and stop is\n        exclusive. The default of None for both results in all layers being built\n        locally.\n        \"\"\"\n        self._local_start = start\n        self._local_stop = stop\n\n    def set_checkpoint_interval(self, interval):\n        assert interval >= 0\n        self.checkpoint_interval = interval\n\n    def topology(self):\n        \"\"\" ProcessTopology object to query process mappings. \"\"\"\n        return self._topo\n\n    def mpu(self):\n        return self._grid\n\n    def num_pipeline_stages(self):\n        return self._topo.get_dim('pipe')\n\n    def ckpt_prefix(self, checkpoints_path, tag):\n        \"\"\"Build a prefix for all checkpoint files written by this module. \"\"\"\n        # All checkpoint files start with this\n        rank_name = 'module'\n\n        # Data parallelism is omitted from the naming convention because we are agnostic\n        # to this in the checkpoint.\n        omit_dims = frozenset(['data'])\n        axes = [a for a in self._grid._topo.get_axis_names() if a not in omit_dims]\n        for dim in axes:\n            rank = getattr(self._grid._topo.get_coord(rank=self.global_rank), dim)\n            rank_name += f'-{dim}_{rank:02d}'\n\n        ckpt_name = os.path.join(checkpoints_path, str(tag), rank_name)\n        return ckpt_name\n\n    def ckpt_layer_path(self, ckpt_dir, local_layer_idx):\n        \"\"\"Customize a prefix for a specific pipeline module layer. \"\"\"\n        idx = local_layer_idx + self._local_start\n        layer_ckpt_path = os.path.join(ckpt_dir, f'layer_{idx:02d}')\n        rank_repr = self._grid._topo.get_rank_repr(rank=self.global_rank)\n        if rank_repr != '':\n            layer_ckpt_path += f'-{rank_repr}'\n        layer_ckpt_path += '-model_states.pt'\n        return layer_ckpt_path\n\n    def ckpt_layer_path_list(self, ckpt_dir, local_layer_idx):\n        \"\"\"Get all ckpt file list for a specific pipeline module layer. \"\"\"\n        idx = local_layer_idx + self._local_start\n        layer_ckpt_path = os.path.join(ckpt_dir, f'layer_{idx:02d}-')\n        layer_ckpt_path += \"*model_states.pt\"\n        ckpt_files = glob.glob(layer_ckpt_path)\n        ckpt_files.sort()\n        return ckpt_files\n\n    def save_state_dict(self, save_dir, checkpoint_engine, exclude_frozen_params=False):\n        # Processes having the same model parallel rank on different data parallel instances\n        # have identical layer weights.  We can distribute the task of saving the layer weights\n        # among the data parallel ranks.  For example, if a pipeline stage has 9 layers and\n        # if there are 2 data parallel instances, rank 0 will save the first 5 layers and\n        # rank 1 will save the last 4.\n        dp_rank = self._grid.data_parallel_id\n        dp_size = self._grid.data_parallel_size\n        num_layers = len(self.forward_funcs)\n        if self.checkpoint_parallel_write_pipeline:\n            # spread layers evenly across data parallel ranks\n            offsets = ds_utils.partition_uniform(num_layers, dp_size)\n            start, end = offsets[dp_rank], offsets[dp_rank + 1]\n        else:\n            # data parallel rank 0 writes all layers\n            if dp_rank != 0:\n                return\n            start, end = 0, num_layers\n        layer_list = self.forward_funcs[start:end]\n\n        checkpoint_engine.makedirs(save_dir, exist_ok=True)\n        for idx, layer in enumerate(layer_list):\n            model_ckpt_path = self.ckpt_layer_path(save_dir, start + idx)\n            if not hasattr(layer, 'state_dict'):\n                continue\n\n            orig_state_dict = layer.state_dict()\n            if exclude_frozen_params:\n                for n in self._get_frozen_parameter_names(layer):\n                    del orig_state_dict[n]\n            final_state_dict = clone_tensors_for_torch_save(orig_state_dict)\n            checkpoint_engine.save(final_state_dict, model_ckpt_path)\n\n    def load_state_dir(self, load_dir, checkpoint_engine, strict=True):\n        for idx, layer in enumerate(self.forward_funcs):\n            # Functions, etc. will not have state_dicts\n            if not hasattr(layer, 'load_state_dict'):\n                continue\n\n            # get all checkpoint files for the layer.\n            model_ckpt_list = self.ckpt_layer_path_list(load_dir, idx)\n            mp_rank = self._grid.get_slice_parallel_rank()\n            mp_world_size = self._grid.get_slice_parallel_world_size()\n\n            sd_loader = SDLoaderFactory.get_sd_loader(model_ckpt_list,\n                                                      version=2.0,\n                                                      checkpoint_engine=checkpoint_engine)\n            load_path, checkpoint, _ = sd_loader.load(mp_world_size, mp_rank, module_key=None, is_pipe_parallel=True)\n\n            layer.load_state_dict(checkpoint, strict=strict)\n\n            # if self._grid.data_parallel_id == 0:\n            #     logger.info(\n            #         f'RANK={self.global_rank} Loaded layer={idx+self._local_start} file={load_path}'\n            #     )\n\n        self._synchronize_tied_weights()\n\n    def _is_checkpointable(self, funcs):\n\n        if self.activation_checkpoint_func is not checkpointing.non_reentrant_checkpoint:\n            # This hook excludes the embedding layer\n            # because only non_reentrant_checkpoint can accept inputs with requires_grad=False\n            # otherwise, the backward of the embedding layer won't receive gradients.\n            if self.__class__.__name__ in ('GPTModelPipe', 'GPT2ModelPipe'):\n                return all('ParallelTransformerLayerPipe' in f.__class__.__name__ for f in funcs)\n        if self.checkpointable_layers is not None:\n            return all(f.__class__.__name__ in self.checkpointable_layers for f in funcs)\n        params = [f.parameters() for f in funcs if isinstance(f, torch.nn.Module)]\n        return any(len(list(p)) > 0 for p in params)\n\n    def get_additional_losses(self):\n        \"\"\" Returns model specific additional losses for reporting\n\n         Return a dictionary of {\"loss name\": loss_value} or None if no additional losses.\n        \"\"\"\n        return None\n", "deepspeed/runtime/pipe/schedule.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom ..utils import call_to_str\n\nfrom abc import ABC, abstractmethod\n\n\nclass PipeSchedule(ABC):\n    \"\"\"Directs the execution of a pipeline engine by generating sequences of\n    :class:`PipeInstruction`.\n\n    Schedules are generators that yield sequences of\n    :class:`PipeInstruction` to process the micro-batches in one batch.\n    Each yielded step is atomic in the sense that a barrier\n    synchronization can be placed between successive steps without\n    deadlock.\n\n    Below is an example schedule that implements data parallelism with gradient accumulation:\n\n    .. code-block:: python\n\n        class DataParallelSchedule(PipeSchedule):\n            def steps(self):\n                for step_id in range(self.micro_batches):\n                    cmds = [\n                        LoadMicroBatch(buffer_id=0),\n                        ForwardPass(buffer_id=0),\n                        BackwardPass(buffer_id=0),\n                    ]\n                    if step_id == self.micro_batches - 1:\n                        cmds.extend([\n                            ReduceGrads(),\n                            OptimizerStep(),\n                        ])\n                    yield cmds\n\n            def num_pipe_buffers(self):\n                return 1\n\n    Args:\n        micro_batches (int): The number of micro-batches that comprise a batch.\n        stages (int): The number of pipeline stages.\n        stage_id (int): The pipe stage that will execute the generated schedule.\n    \"\"\"\n\n    def __init__(self, micro_batches, stages, stage_id):\n        super().__init__()\n        self.micro_batches = micro_batches\n        self.stages = stages\n        self.stage_id = stage_id\n        self.prev_stage = self.stage_id - 1\n        self.next_stage = self.stage_id + 1\n\n    @abstractmethod\n    def steps(self):\n        \"\"\"Yield a list of :class:`PipeInstruction` for each step in the schedule.\n\n        .. note::\n            Schedules must implement ``steps()`` to define the schedule.\n\n        Returns:\n            Instructions to be executed as one step of the pipeline\n        \"\"\"\n        pass\n\n    def num_pipe_buffers(self):\n        \"\"\"The number of pipeline buffers that will be used by this stage.\n\n        .. note::\n            Schedules should specialize ``num_pipe_buffers()`` for memory savings at scale.\n\n        Returns:\n            The number of buffers for the engine to allocate.\n        \"\"\"\n        return self.micro_batches\n\n    def _valid_micro_batch(self, micro_batch_id):\n        return 0 <= micro_batch_id < self.micro_batches\n\n    def _valid_stage(self, stage_id):\n        return 0 <= stage_id < self.stages\n\n    @property\n    def stage(self):\n        \"\"\"Stage index used to configure this schedule.\"\"\"\n        return self.stage_id\n\n    @property\n    def num_stages(self):\n        \"\"\"The number of total pipeline stages used to configure this schedule.\"\"\"\n        return self.stages\n\n    @property\n    def num_micro_batches(self):\n        \"\"\"The number of total micro_batches used to configure this schedule.\"\"\"\n        return self.micro_batches\n\n    @property\n    def is_first_stage(self):\n        \"\"\"True if the configured ``stage_id`` is the first stage in the pipeline.\"\"\"\n        return self.stage_id == 0\n\n    @property\n    def is_last_stage(self):\n        \"\"\"True if the configured ``stage_id`` is the last stage in the pipeline.\"\"\"\n        return self.stage_id == self.stages - 1\n\n    def _buffer_idx(self, micro_batch_id):\n        \"\"\"Map a micro-batch index to a pipeline buffer index.\n\n        This method uses a cyclic allocation strategy.\n\n        Args:\n            micro_batch_id (int): The micro-batch index relative to the beginning of the schedule.\n\n        Returns:\n            int: The index of the buffer that should store data.\n        \"\"\"\n        assert self._valid_micro_batch(micro_batch_id)\n        return micro_batch_id % self.num_pipe_buffers()\n\n    def __iter__(self):\n        self.it = None\n        return self\n\n    def __next__(self):\n        if self.it is None:\n            self.it = self.steps()\n        return next(self.it)\n\n\nclass InferenceSchedule(PipeSchedule):\n    \"\"\"A schedule for inferencing batches using pipeline parallelism.\n    \"\"\"\n\n    def steps(self):\n        \"\"\"\"\"\"\n        prev_micro_batch_id = -1\n        total_steps = self.micro_batches + self.stages - 1\n        for step_id in range(total_steps):\n            cmds = []\n            micro_batch_id = step_id - self.stage_id\n\n            # Alternate send/recv buffers\n            if _is_even(self.stage_id):\n                recv_buf = step_id % 2\n                send_buf = (step_id + 1) % 2\n            else:\n                recv_buf = (step_id + 1) % 2\n                send_buf = step_id % 2\n\n            if self.is_first_stage or self.is_last_stage:\n                if self._valid_micro_batch(micro_batch_id):\n                    cmds.append(LoadMicroBatch(recv_buf))\n\n            if _is_even(self.stage_id):\n                if self._valid_stage(self.next_stage):\n                    if self._valid_micro_batch(micro_batch_id - 1):\n                        cmds.append(SendActivation(send_buf))\n                if self._valid_stage(self.prev_stage):\n                    if self._valid_micro_batch(micro_batch_id):\n                        cmds.append(RecvActivation(recv_buf))\n            else:\n                if self._valid_stage(self.prev_stage):\n                    if self._valid_micro_batch(micro_batch_id):\n                        cmds.append(RecvActivation(recv_buf))\n\n                if self._valid_stage(self.next_stage):\n                    if self._valid_micro_batch(micro_batch_id - 1):\n                        cmds.append(SendActivation(send_buf))\n\n            if self._valid_micro_batch(micro_batch_id):\n                cmds.append(ForwardPass(recv_buf))\n\n            yield cmds\n\n    def num_pipe_buffers(self):\n        \"\"\"Only two pipeline buffers are required for inferencing.\n\n        Returns:\n            ``2``\n        \"\"\"\n        return 2\n\n\nclass TrainSchedule(PipeSchedule):\n    \"\"\"A schedule for training a batch using hybrid parallelism.\n\n    Pipeline parallelism is extracted through gradient accumulation and thus\n    convergence follows that of a data parallel approach with the same batch\n    size.\n    \"\"\"\n\n    def steps(self):\n        \"\"\"\"\"\"\n        prev_micro_batch_id = -1\n        total_steps = 2 * (self.micro_batches + self.stages - 1)\n        for step_id in range(total_steps):\n            # Map the step of the pipeline to the micro-batch id and also whether it is a\n            # forward or backward pass step.\n            micro_batch_id, is_forward = self._step_to_micro_batch(step_id)\n\n            if self._valid_micro_batch(prev_micro_batch_id):\n                prev_buffer = self._buffer_idx(prev_micro_batch_id)\n            if self._valid_micro_batch(micro_batch_id):\n                curr_buffer = self._buffer_idx(micro_batch_id)\n\n            cmds = []\n\n            # Exchange activations\n            if is_forward:\n                if self._valid_micro_batch(prev_micro_batch_id) and self._valid_stage(self.prev_stage):\n                    cmds.append(SendGrad(prev_buffer))\n                if self._valid_micro_batch(micro_batch_id) and self._valid_stage(self.prev_stage):\n                    cmds.append(RecvActivation(curr_buffer))\n            else:\n                if self._valid_micro_batch(micro_batch_id) and self._valid_stage(self.next_stage):\n                    cmds.append(RecvGrad(curr_buffer))\n                if self._valid_micro_batch(prev_micro_batch_id) and self._valid_stage(self.next_stage):\n                    cmds.append(SendActivation(prev_buffer))\n\n            # First/last stage loads\n            if self.stage_id == 0 or self.stage_id == self.stages - 1:\n                if is_forward and self._valid_micro_batch(micro_batch_id):\n                    cmds.append(LoadMicroBatch(curr_buffer))\n\n            # Computation\n            if self._valid_micro_batch(micro_batch_id):\n                if is_forward:\n                    cmds.append(ForwardPass(curr_buffer))\n                else:\n                    cmds.append(BackwardPass(curr_buffer))\n\n            # Model step at the end of the batch\n            if step_id == total_steps - 1:\n                cmds.append(ReduceTiedGrads())\n                cmds.append(ReduceGrads())\n                cmds.append(OptimizerStep())\n\n            # Prepare state for next time\n            prev_micro_batch_id = micro_batch_id\n            yield cmds\n\n    def num_pipe_buffers(self):\n        \"\"\"Return the number of pipeline buffers required for this stage.\n\n        This is equivalent to the maximum number of in-flight forward passes,\n        since we need to remember the activations of forward passes in order\n        to run backpropagation. For synchronous 1F1B, this is equivalent to\n        the index difference between this stage and the last stage.\n        \"\"\"\n        buffers = min(self.stages - self.stage_id, self.micro_batches)\n        return max(2, buffers)\n\n    def _step_to_micro_batch(self, step_id):\n        if _is_even(step_id) and _is_even(self.stage_id):\n            micro_batch_id = self._even_step_forward_id(step_id)\n            is_forward = True\n\n        elif _is_odd(step_id) and _is_odd(self.stage_id):\n            micro_batch_id = self._odd_step_forward_id(step_id)\n            is_forward = True\n\n        elif _is_even(step_id) and _is_odd(self.stage_id):\n            micro_batch_id = self._even_step_backward_id(step_id)\n            is_forward = False\n\n        elif _is_odd(step_id) and _is_even(self.stage_id):\n            micro_batch_id = self._odd_step_backward_id(step_id)\n            is_forward = False\n\n        else:\n            assert False\n\n        return micro_batch_id, is_forward\n\n    def _even_step_forward_id(self, step_id):\n        base = step_id // 2\n        micro_batch_id = int(base - self.stage_id // 2)\n        return micro_batch_id\n\n    def _odd_step_forward_id(self, step_id):\n        base = (step_id - 1) // 2\n        micro_batch_id = int(base - self.stage_id // 2)\n        return micro_batch_id\n\n    def _even_step_backward_id(self, step_id):\n        base = step_id // 2\n        micro_batch_id = int(base - self.stages + (self.stage_id + 1) // 2)\n        return micro_batch_id\n\n    def _odd_step_backward_id(self, step_id):\n        base = ((step_id - 1) // 2) - self.stages + 1\n        micro_batch_id = int(base + self.stage_id // 2)\n        return micro_batch_id\n\n\nclass DataParallelSchedule(PipeSchedule):\n    \"\"\"An example schedule that trains using traditional data parallelism with gradient\n    accumulation.\n    \"\"\"\n\n    def steps(self):\n        \"\"\"\"\"\"\n        for step_id in range(self.micro_batches):\n            cmds = [\n                LoadMicroBatch(buffer_id=0),\n                ForwardPass(buffer_id=0),\n                BackwardPass(buffer_id=0),\n            ]\n            if step_id == self.micro_batches - 1:\n                cmds.extend([\n                    ReduceGrads(),\n                    OptimizerStep(),\n                ])\n            yield cmds\n\n    def num_pipe_buffers(self):\n        \"\"\"Only one pipeline buffer needed.\n        \"\"\"\n        return 1\n\n\nclass PipeInstruction:\n    \"\"\"Base class for all instructions to be executed by the pipeline engine.\n\n    All keyword arguments are stored as members similar to a ``namedtuple``. These are\n    then accessible to the :class:`PipeEngine` during execution.\n\n    Args:\n        kwargs (optional): keyword arguments to store as members\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        self.name = self.__class__.__name__\n        self.kwargs = kwargs\n        for key, val in kwargs.items():\n            setattr(self, key, val)\n\n    def __repr__(self):\n        return call_to_str(self.name, **self.kwargs)\n\n\nclass OptimizerStep(PipeInstruction):\n    \"\"\"Performs one step with the optimizer and zeros gradients.\n\n    .. note:: Should be issued after :class:`ReduceGrads` and :class:`ReduceTiedGrads`.\n\n    .. note:: Can be a synchronization point among data-parallel ranks.\n    \"\"\"\n    pass\n\n\nclass ReduceGrads(PipeInstruction):\n    \"\"\"Reduce the computed gradients among data-parallel processes within the stage.\n    \"\"\"\n    pass\n\n\nclass ReduceTiedGrads(PipeInstruction):\n    \"\"\"Reduce the computed gradients of tied modules within a pipeline-parallel group.\n\n    .. warning::\n        The stages included in this synchronization point are not known until\n        the model is partitioned among pipeline stages. In the worst case, it\n        includes all pipeline stages. This instruction should be scheduled\n        carefully to avoid deadlocks.\n    \"\"\"\n    pass\n\n\nclass BufferOpInstruction(PipeInstruction):\n    \"\"\"A pipeline instruction that operates on pipeline buffer(s).\n\n    Args:\n        buffer_id (int): the index of the pipeline buffer() to modify.\n    \"\"\"\n\n    def __init__(self, buffer_id, **kwargs):\n        super().__init__(buffer_id=buffer_id, **kwargs)\n\n\n# IO\nclass LoadMicroBatch(BufferOpInstruction):\n    \"\"\"Load a micro-batch into a buffer.\n\n    Roughly:\n\n    .. code-block:: python\n\n        buffers['inputs'][buffer_id] = next(data_iter)\n    \"\"\"\n    pass\n\n\n# Compute\nclass ForwardPass(BufferOpInstruction):\n    \"\"\"Compute a forward pass.\n\n    Roughly:\n\n    .. code-block:: python\n\n        buffers['outputs'][buffer_id] = forward(buffers['inputs'][buffer_id])\n    \"\"\"\n    pass\n\n\nclass BackwardPass(BufferOpInstruction):\n    \"\"\"Compute a backward pass and accumulate gradients.\n\n    Roughly:\n\n    .. code-block:: python\n\n        outputs = buffers['outputs'][buffer_id]\n        gradients = buffers['gradients'][buffer_id]\n        torch.autograd.backward(tensors=outputs,\n                                grad_tensors=gradients)\n    \"\"\"\n    pass\n\n\n# Communication\nclass SendActivation(BufferOpInstruction):\n    \"\"\"Send activations to the next stage in the pipeline.\n\n    Roughly:\n\n    .. code-block:: python\n\n        send(buffers['outputs'][buffer_id])\n\n    .. note::\n        The communication is blocking and must be paired with a :class:`RecvActivation`\n        on the next pipeline stage to avoid deadlock.\n    \"\"\"\n    pass\n\n\nclass RecvActivation(BufferOpInstruction):\n    \"\"\"Receive activations from the previous stage in the pipeline.\n\n    Roughly:\n\n    .. code-block:: python\n\n        buffers['inputs'][buffer_id] = recv()\n\n    .. note::\n        The communication is blocking and must be paired with a :class:`SendActivation`\n        on the previous pipeline stage to avoid deadlock.\n    \"\"\"\n    pass\n\n\nclass SendGrad(BufferOpInstruction):\n    \"\"\"Send computed gradients to the previous pipeline stage.\n    with respect to the received activations\n\n    .. note::\n        Only received tensors with ``requires_grad==True`` will produce gradients.\n        Missing gradients will be replaced with ``None`` on the receiving stage.\n\n    .. note::\n        The communication is blocking and must be paired with a :class:`RecvGrad`\n        on the previous pipeline stage to avoid deadlock.\n    \"\"\"\n    pass\n\n\nclass RecvGrad(BufferOpInstruction):\n    \"\"\"Receive computed gradients the next pipeline stage.\n\n    .. note::\n        Only activations with ``requires_grad==True`` will produce gradients.\n        Missing gradients will be replaced with ``None``.\n\n    .. note::\n        The communication is blocking and must be paired with a :class:`SendGrad`\n        on the next pipeline stage to avoid deadlock.\n    \"\"\"\n    pass\n\n\ndef _is_even(x):\n    return x % 2 == 0\n\n\ndef _is_odd(x):\n    return x % 2 != 0\n", "deepspeed/runtime/pipe/p2p.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport pickle\nimport typing\n\nimport torch\nfrom deepspeed import comm as dist\n\nfrom deepspeed.utils.torch import required_torch_version\nfrom deepspeed.accelerator import get_accelerator\n\n_groups = None\n_grid = None\n\n_async = []\n\n\ndef can_send_recv() -> bool:\n    return required_torch_version(min_version=1.8)\n\n\n#initializes adjacent process groups\n#run this only after deepspeed.init_distributed() has been called\ndef init_process_groups(grid):\n    global _groups, _grid\n    _grid = grid\n\n    assert _grid.pipe_parallel_size > 1, \"There is no pipeline parallelism\"\n\n    if not can_send_recv():\n        _groups = [dist.new_group(ranks=group) for group in _grid.p2p_groups]\n\n\ndef _is_valid_send_recv(src_stage, dest_stage):\n    first_stage = 0\n    last_stage = _grid.pipe_parallel_size - 1\n    assert abs(src_stage-dest_stage) == 1 or \\\n        (src_stage == first_stage and dest_stage == last_stage) or \\\n        (src_stage == last_stage and dest_stage == first_stage), \\\n    \"Functionality currently limited to send and receive between adjacent ranks only\"\n\n\ndef send(tensor, dest_stage, async_op=False):\n    global _groups\n    assert async_op == False, \"Doesn't support async_op true\"\n    src_stage = _grid.get_stage_id()\n    _is_valid_send_recv(src_stage, dest_stage)\n\n    dest_rank = _grid.stage_to_global(stage_id=dest_stage)\n    if async_op:\n        global _async\n        op = dist.isend(tensor, dest_rank)\n        _async.append(op)\n    else:\n\n        if can_send_recv():\n            return dist.send(tensor, dest_rank)\n        else:\n            group = _get_send_recv_group(src_stage, dest_stage)\n            src_rank = _grid.stage_to_global(stage_id=src_stage)\n            return dist.broadcast(tensor, src_rank, group=group, async_op=async_op)\n\n\ndef recv(tensor, src_stage, async_op=False):\n    global _groups\n    assert async_op == False, \"Doesn't support async_op true\"\n    dest_stage = _grid.get_stage_id()\n    _is_valid_send_recv(src_stage, dest_stage)\n\n    src_rank = _grid.stage_to_global(stage_id=src_stage)\n\n    if async_op:\n        global _async\n        op = dist.irecv(tensor, src_rank)\n        _async.append(op)\n    else:\n        if can_send_recv():\n            return dist.recv(tensor, src_rank)\n        else:\n            group = _get_send_recv_group(src_stage, dest_stage)\n            return dist.broadcast(tensor, src_rank, group=group, async_op=async_op)\n\n\ndef wait():\n    global _async\n    for op in _async:\n        op.wait()\n    _async = []\n\n    get_accelerator().synchronize()\n\n\ndef send_obj(msg: typing.Any, dest: int):\n    \"\"\"Send an arbitrary python object to ``dest``.\n\n    Note: ``msg`` must be pickleable.\n\n    WARN: This incurs a CPU -> GPU transfer and should be used sparingly\n    for performance reasons.\n\n    Args:\n        msg (typing.Any): The object to send.\n        dest (int): Destination rank.\n    \"\"\"\n    # serialize the message\n    msg = pickle.dumps(msg)\n    # construct a tensor to send\n    msg = torch.ByteTensor(torch.ByteStorage.from_buffer(msg)).to(get_accelerator().device_name())\n\n    # Send meta and message\n    length_tensor = torch.tensor([len(msg)], dtype=torch.long).to(get_accelerator().device_name())\n    dist.send(length_tensor, dst=dest)\n    dist.send(msg, dst=dest)\n\n\ndef recv_obj(sender: int) -> typing.Any:\n    \"\"\"Receive an arbitrary python object from ``sender``.\n\n    WARN: This incur a CPU <-> GPU transfers and should be used sparingly\n    for performance reasons.\n\n    Args:\n        sender (int): The rank sending the message.\n    \"\"\"\n    # Get message meta\n    length = torch.tensor([0], dtype=torch.long).to(get_accelerator().device_name())\n    dist.recv(length, src=sender)\n\n    # Receive and deserialize\n    msg = torch.empty(length.item(), dtype=torch.uint8).to(get_accelerator().device_name())\n    dist.recv(msg, src=sender)\n\n    msg = pickle.loads(msg.cpu().numpy().tobytes())\n\n    def _to(x):\n        \"\"\"Recursively move to the current device.\"\"\"\n        if torch.is_tensor(x):\n            return x.to(get_accelerator().device_name())\n        if isinstance(x, (tuple, list)):\n            ret = [_to(x_) for x_ in x]\n            if isinstance(x, tuple):\n                ret = tuple(ret)\n            return ret\n        # handle kwargs\n        if isinstance(x, dict):\n            ret = dict()\n            for key, val in x.items():\n                ret[_to(key)] = _to(val)\n            return ret\n\n        # Anything else is a no-op\n        return x\n\n    msg = _to(msg)\n    return msg\n\n\ndef _get_send_recv_group(src_stage, dest_stage):\n    '''the group id is always the smaller rank unless its a wrap around'''\n\n    stage_id = None\n\n    first_stage = 0\n    last_stage = _grid.pipe_parallel_size - 1\n\n    if (src_stage == first_stage and dest_stage == last_stage\n            or dest_stage == first_stage and src_stage == last_stage):\n        stage_id = last_stage\n    elif src_stage > dest_stage:\n        stage_id = dest_stage\n    else:\n        stage_id = src_stage\n    '''group_id corresponds to group of [group_id, group_id+1]\n     unless group_id is the rank of the last stage\n     in which case group_id corresponds to group[group_id-num_stages+1, group_id]\n     '''\n    group_id = _grid.stage_to_global(stage_id=stage_id)\n\n    return _groups[group_id]\n", "deepspeed/runtime/pipe/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .module import PipelineModule, LayerSpec, TiedLayerSpec\nfrom .topology import ProcessTopology\n", "deepspeed/runtime/swap_tensor/aio_config.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom deepspeed.runtime.config_utils import get_scalar_param\nfrom deepspeed.runtime.swap_tensor.constants import *\n\nAIO_DEFAULT_DICT = {\n    AIO_BLOCK_SIZE: AIO_BLOCK_SIZE_DEFAULT,\n    AIO_QUEUE_DEPTH: AIO_QUEUE_DEPTH_DEFAULT,\n    AIO_THREAD_COUNT: AIO_THREAD_COUNT_DEFAULT,\n    AIO_SINGLE_SUBMIT: AIO_SINGLE_SUBMIT_DEFAULT,\n    AIO_OVERLAP_EVENTS: AIO_OVERLAP_EVENTS_DEFAULT\n}\n\n\ndef get_aio_config(param_dict):\n    if AIO in param_dict.keys() and param_dict[AIO] is not None:\n        aio_dict = param_dict[AIO]\n        return {\n            AIO_BLOCK_SIZE: get_scalar_param(aio_dict, AIO_BLOCK_SIZE, AIO_BLOCK_SIZE_DEFAULT),\n            AIO_QUEUE_DEPTH: get_scalar_param(aio_dict, AIO_QUEUE_DEPTH, AIO_QUEUE_DEPTH_DEFAULT),\n            AIO_THREAD_COUNT: get_scalar_param(aio_dict, AIO_THREAD_COUNT, AIO_THREAD_COUNT_DEFAULT),\n            AIO_SINGLE_SUBMIT: get_scalar_param(aio_dict, AIO_SINGLE_SUBMIT, AIO_SINGLE_SUBMIT_DEFAULT),\n            AIO_OVERLAP_EVENTS: get_scalar_param(aio_dict, AIO_OVERLAP_EVENTS, AIO_OVERLAP_EVENTS_DEFAULT)\n        }\n\n    return AIO_DEFAULT_DICT\n", "deepspeed/runtime/swap_tensor/partitioned_optimizer_swapper.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\"\"\"\nFunctionality of swapping optimizer tensors to/from (NVMe) storage devices.\n\"\"\"\n\nimport torch\n\nfrom deepspeed.utils.logging import logger\nfrom deepspeed.ops.op_builder import AsyncIOBuilder\nfrom deepspeed import comm as dist\n\nfrom deepspeed.runtime.swap_tensor.constants import *\nfrom deepspeed.runtime.swap_tensor.utils import swap_in_tensors, swap_out_tensors, print_object, \\\n    get_sized_buffers\nfrom deepspeed.runtime.swap_tensor.async_swapper import AsyncTensorSwapper\nfrom deepspeed.runtime.swap_tensor.optimizer_utils import OptimizerSwapper\nfrom deepspeed.accelerator import get_accelerator\n\nDEBUG_MODE = False\n\nSWAP_IN_PARAM_TIMER = 'swap_in_param'\nSWAP_OUT_PARAM_TIMER = 'swap_out_param'\nSWAP_IN_GRADIENT_TIMER = 'swap_in_gradient'\n\n\nclass PartitionedOptimizerSwapper(OptimizerSwapper):\n\n    def __init__(self, swap_config, aio_config, base_folder, optimizer, largest_numel, device, dtype, timers):\n        super(PartitionedOptimizerSwapper, self).__init__(swap_config, aio_config, base_folder, optimizer,\n                                                          largest_numel, device, dtype, timers)\n\n        aio_op = AsyncIOBuilder().load()\n        self.aio_handle = aio_op.aio_handle(aio_config[AIO_BLOCK_SIZE], aio_config[AIO_QUEUE_DEPTH],\n                                            aio_config[AIO_SINGLE_SUBMIT], aio_config[AIO_OVERLAP_EVENTS],\n                                            aio_config[AIO_THREAD_COUNT])\n\n        # Overlap swapping out\n        self.gradient_swapper = AsyncTensorSwapper(aio_handle=self.aio_handle,\n                                                   numel_alignment=self.numel_alignment,\n                                                   timers=self.timers)\n\n        self.print_exclude_list += ['aio_handle', 'gradient_swapper', 'print_exclude_list']\n\n        if dist.get_rank() == 0:\n            print_object(obj=self, name='PartitionedOptimizerSwapper', exclude_list=self.print_exclude_list)\n\n    def initialize_parameters(self, parameters, src_tensors):\n        self._initialize_parameters(parameters=parameters, src_tensors=src_tensors, aio_handle=self.aio_handle)\n\n    def initialize_from_swapped_fp16_params(self, fp16_partitions_info, fp16_num_elems, fp16_pinned_buffers,\n                                            fp32_parameters):\n        self._initialize_from_swapped_fp16_params(aio_handle=self.aio_handle,\n                                                  fp16_partitions_info=fp16_partitions_info,\n                                                  fp16_num_elems=fp16_num_elems,\n                                                  fp16_pinned_buffers=fp16_pinned_buffers,\n                                                  fp32_parameters=fp32_parameters)\n\n    def flush_gradients(self):\n        self._flush_gradient_swapper(self.gradient_swapper)\n\n    def swap_in_optimizer_state(self, parameter, async_parameter=None):\n        swap_info = self._get_param_swap_info(parameter)\n        if swap_info is None:\n            return\n\n        self._flush_gradient_swapper(self.gradient_swapper)\n\n        required_buffer_count = len(swap_info.tensors) + (1 if swap_info.has_gradients() else 0)\n        aligned_numel = self._io_aligned_numel(swap_info.numel())\n        pinned_buffers = self.swap_buffer_manager.allocate(num_elems=aligned_numel,\n                                                           count=required_buffer_count,\n                                                           dtype=parameter.dtype)\n        assert pinned_buffers is not None\n        self.allocated_swap_buffers = pinned_buffers.copy()\n\n        self._start_timer(SWAP_IN_PARAM_TIMER)\n        self._swap_in_parameter(aio_handle=self.aio_handle,\n                                parameter=parameter,\n                                dest_buffers=pinned_buffers[:required_buffer_count])\n        self._stop_timer(SWAP_IN_PARAM_TIMER)\n        self.timer_names.add(SWAP_IN_PARAM_TIMER)\n\n        self._start_timer(SWAP_IN_GRADIENT_TIMER)\n        self._swap_in_gradients(aio_handle=self.aio_handle, parameter=parameter, dest_buffer=pinned_buffers[-1])\n        self._stop_timer(SWAP_IN_GRADIENT_TIMER)\n        self.timer_names.add(SWAP_IN_GRADIENT_TIMER)\n\n    def swap_out_optimizer_state(self, parameter, async_swap=False):\n        swap_info = self._get_param_swap_info(parameter=parameter)\n\n        if swap_info is None:\n            return\n\n        self._start_timer(SWAP_OUT_PARAM_TIMER)\n        pinned_tensors, pinned_paths, unpinned_tensors, unpinned_paths = self._separate_pinned_tensors(swap_info)\n        swap_bytes = sum([self._io_aligned_numel(t.numel()) * t.element_size() for t in swap_info.tensors])\n\n        WRITE_TIMER = 'swap_submit_write'\n        self._start_timer(WRITE_TIMER)\n\n        swap_out_tensors(self.aio_handle, pinned_tensors, pinned_paths)\n        assert self.aio_handle.wait() == len(pinned_tensors)\n        for t in pinned_tensors:\n            t.data = torch.Tensor()\n\n        if len(unpinned_tensors) > 0:\n            pinned_buffers = self.swap_buffer_manager.allocate_all(num_elems=self.largest_numel, dtype=self.dtype)\n            self._swap_out_unpinned_tensors(aio_handle=self.aio_handle,\n                                            unpinned_tensors=unpinned_tensors,\n                                            dest_paths=unpinned_paths,\n                                            pinned_buffers=pinned_buffers)\n            self.allocated_swap_buffers += pinned_buffers\n\n            for t in unpinned_tensors:\n                t.data = torch.Tensor()\n        self._stop_timer(WRITE_TIMER)\n\n        self.swap_buffer_manager.free(self.allocated_swap_buffers)\n        self.allocated_swap_buffers = []\n\n        self._stop_timer(SWAP_OUT_PARAM_TIMER)\n        self.timer_names.add(SWAP_OUT_PARAM_TIMER)\n\n        self._log_timers([WRITE_TIMER])\n\n        if DEBUG_MODE and dist.get_rank() == 0:\n            logger.info(f'optimizer_param_swap_out: {(swap_bytes/(1024**3)):5.2f} GB')\n\n    def swap_out_gradients(self, parameter, gradient_offsets, gradient_tensors):\n        self._swap_out_gradients(parameter=parameter,\n                                 gradient_offsets=gradient_offsets,\n                                 gradient_tensors=gradient_tensors,\n                                 gradient_swapper=self.gradient_swapper)\n\n    def _swap_in_parameter(self, aio_handle, parameter, dest_buffers):\n        swap_info = self._get_param_swap_info(parameter)\n        if swap_info is None:\n            return\n\n        assert len(swap_info.tensors) <= len(dest_buffers)\n\n        swap_lengths = [self._io_aligned_numel(swap_info.numel())] * len(swap_info.tensors)\n        swap_buffers = get_sized_buffers(dest_buffers, swap_lengths)\n\n        READ_TIMER = 'swap_submit_read_param'\n        WAIT_TIMER = 'swap_wait_read_param'\n\n        self._start_timer(READ_TIMER)\n        swap_in_tensors(aio_handle, swap_buffers, swap_info.swap_paths)\n        self._stop_timer(READ_TIMER)\n\n        swap_bytes = sum([buffer.numel() * buffer.element_size() for buffer in swap_buffers])\n\n        self._start_timer(WAIT_TIMER)\n        aio_handle.wait()\n        self._stop_timer(WAIT_TIMER)\n\n        compute_lengths = [swap_info.numel()] * len(swap_info.tensors)\n        compute_buffers = get_sized_buffers(dest_buffers, compute_lengths)\n        for t, buffer in zip(swap_info.tensors, compute_buffers):\n            t.data = buffer.data\n\n        self._log_timers([READ_TIMER, WAIT_TIMER])\n        if DEBUG_MODE and dist.get_rank() == 0:\n            logger.info(f'optimizer_param_swap_in: {(swap_bytes/(1024**3)):5.2f} GB')\n\n    def _separate_pinned_tensors(self, swap_info):\n        pinned_tensors = []\n        pinned_paths = []\n\n        unpinned_tensors = []\n        unpinned_paths = []\n\n        for tensor, path in zip(swap_info.tensors, swap_info.swap_paths):\n            if get_accelerator().is_pinned(tensor):\n                pinned_tensors.append(tensor)\n                pinned_paths.append(path)\n            else:\n                unpinned_tensors.append(tensor)\n                unpinned_paths.append(path)\n\n        return pinned_tensors, pinned_paths, unpinned_tensors, unpinned_paths\n\n    def _swap_in_pinned_gradients(self, aio_handle, parameter, gradient_tensor):\n        swap_info = self.swap_params_info[OptimizerSwapper.parameter_id(parameter)]\n        param_gradients = swap_info.swapped_gradients.values()\n        swap_buffers = [gradient_tensor.narrow(0, grad.offset, grad.length) for grad in param_gradients]\n        swap_paths = [grad.path for grad in param_gradients]\n        SWAP_READ_GRADIENTS = 'swap_submit_read_gradient'\n        SWAP_WAIT_GRADIENTS = 'swap_submit_wait_gradient'\n\n        self._start_timer(SWAP_READ_GRADIENTS)\n        swap_in_tensors(aio_handle, swap_buffers, swap_paths)\n        self._stop_timer(SWAP_READ_GRADIENTS)\n\n        self._start_timer(SWAP_WAIT_GRADIENTS)\n        assert len(swap_buffers) == aio_handle.wait()\n        self._stop_timer(SWAP_WAIT_GRADIENTS)\n\n        self._log_timers([SWAP_READ_GRADIENTS, SWAP_WAIT_GRADIENTS])\n\n    def _swap_in_gradients(self, aio_handle, parameter, dest_buffer):\n        swap_info = self.swap_params_info.get(OptimizerSwapper.parameter_id(parameter), None)\n        if not (swap_info and swap_info.has_gradients()):\n            return\n\n        assert get_accelerator().is_pinned(dest_buffer)\n        assert parameter.numel() <= dest_buffer.numel()\n\n        parameter.grad = dest_buffer.narrow(0, 0, parameter.numel())\n\n        if swap_info.swapped_gradients:\n            self._swap_in_pinned_gradients(aio_handle, parameter, parameter.grad)\n\n        if swap_info.unswapped_gradients:\n            self._retrieve_unswapped_grad_partitions(swap_info=swap_info, dest_buffer=parameter.grad)\n", "deepspeed/runtime/swap_tensor/partitioned_param_swapper.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\"\"\"\nFunctionality of swapping tensors to/from (NVMe) storage devices.\n\"\"\"\n\nimport os\nimport shutil\nfrom enum import Enum\nimport torch\nfrom deepspeed import comm as dist\nfrom deepspeed.accelerator import get_accelerator\nfrom deepspeed.ops.op_builder import AsyncIOBuilder\nfrom .constants import *\nfrom .utils import swap_in_tensors, swap_out_tensors, MIN_AIO_BYTES, AIO_ALIGNED_BYTES, print_object, SwapBufferPool\n\n\ndef print_rank_0(message, debug=False, force=False):\n    if dist.get_rank() == 0 and (debug or force):\n        print(message)\n\n\nclass PartitionedParamStatus(Enum):\n    # Partitioned parameters are present and ready for use\n    AVAILABLE = 1\n\n    # partitioned params are in some non-memory device\n    NOT_AVAILABLE = 2\n\n    # partitioned params are being read from some non-memory device.\n    INFLIGHT = 3\n\n\nclass AsyncPartitionedParameterSwapper(object):\n\n    def __init__(self, ds_config, model_dtype):\n\n        aio_op = AsyncIOBuilder().load(verbose=False)\n        self.aio_handle = aio_op.aio_handle\n        self.dtype = model_dtype\n\n        #set swap buffers, create aio handles\n        self._configure_aio(ds_config)\n\n        #mapping from param id to path\n        self.id_to_path = {}\n\n        #mapping from pram_id to buffer id\n        self.param_id_to_buffer_id = {}\n\n        # mapping from param_id to swap buffer\n        self.param_id_to_swap_buffer = {}\n\n        #number of elements in the param\n        self.param_id_to_numel = {}\n\n        self.pending_writes = 0\n        self.pending_reads = 0\n\n        #keep track of async swap in params and buffers\n        self.inflight_params = []\n        self.inflight_swap_in_buffers = []\n        self.inflight_numel = 0\n\n        #keep track of available params\n        self.available_params = set()\n        self.available_numel = 0\n\n        # for swapping out from partitioned fp32 params\n        self.partitioned_swap_buffer = None\n        self.partitioned_swap_pool = None\n\n        self.invalid_buffer = torch.tensor(1).half()\n\n        if dist.get_rank() == 0:\n            exclude_list = ['aio_read_handle', 'aio_write_handle', 'buffers']\n            print_object(obj=self, name='AsyncPartitionedParameterSwapper', exclude_list=exclude_list)\n\n    def available_swap_in_buffers(self):\n        return len(self.available_buffer_ids)\n\n    def _configure_aio(self, ds_config):\n        self.swap_config = ds_config.zero_config.offload_param\n        torch_dtype_string = str(self.dtype).split(\".\")[1]\n        self.swap_folder = os.path.join(self.swap_config.nvme_path, 'zero_stage_3', f'{torch_dtype_string}params',\n                                        f'rank{dist.get_rank()}')\n        shutil.rmtree(self.swap_folder, ignore_errors=True)\n        os.makedirs(self.swap_folder, exist_ok=True)\n\n        self.swap_element_size = torch.tensor([], dtype=self.dtype).element_size()\n\n        self.aio_config = ds_config.aio_config\n\n        # Read/Write alignment for each thread during Intra-request parallelism\n        self.min_aio_bytes = max(MIN_AIO_BYTES, self.aio_config[AIO_BLOCK_SIZE])\n        self.aligned_bytes = AIO_ALIGNED_BYTES * self.aio_config[AIO_THREAD_COUNT]\n        self.numel_alignment = self.aligned_bytes // self.swap_element_size\n\n        self.elements_per_buffer = self.swap_config.buffer_size\n        self.aligned_elements_per_buffer = self._io_aligned_numel(self.elements_per_buffer)\n        self.param_buffer_count = self.swap_config.buffer_count\n\n        self.available_buffer_ids = [i for i in range(self.param_buffer_count)]\n        self.reserved_buffer_ids = []\n        self.buffers = get_accelerator().pin_memory(torch.empty(int(self.aligned_elements_per_buffer *\n                                                                    self.param_buffer_count),\n                                                                dtype=self.dtype,\n                                                                requires_grad=False),\n                                                    align_bytes=0)\n\n        self.aio_read_handle = self.aio_handle(self.aio_config[AIO_BLOCK_SIZE], self.aio_config[AIO_QUEUE_DEPTH],\n                                               self.aio_config[AIO_SINGLE_SUBMIT], self.aio_config[AIO_OVERLAP_EVENTS],\n                                               self.aio_config[AIO_THREAD_COUNT])\n\n        self.aio_write_handle = self.aio_handle(self.aio_config[AIO_BLOCK_SIZE], self.aio_config[AIO_QUEUE_DEPTH],\n                                                self.aio_config[AIO_SINGLE_SUBMIT],\n                                                self.aio_config[AIO_OVERLAP_EVENTS], self.aio_config[AIO_THREAD_COUNT])\n\n        self.swap_out_params = []\n\n    #Check if partitioned param or numel in a tensor is swappable or not\n    def swappable_tensor(self, param=None, numel=None):\n        if param is not None:\n            assert numel is None, \"Both parma and numel cannot be provided\"\n            numel = param.ds_tensor.ds_numel\n        if numel is not None:\n            return self.min_aio_bytes <= numel * self.swap_element_size\n        assert False, \"Either param or numel must be provided\"\n\n    def get_path(self, param, must_exist=False):\n        paths = self._get_swap_paths([param], must_exist=must_exist)\n        return paths[0]\n\n    def _get_swap_paths(self, params, must_exist=False):\n        paths = []\n        for param in params:\n            param_id = param.ds_id\n            if param_id in self.id_to_path.keys():\n                param_path = self.id_to_path[param_id]\n            else:\n                assert not must_exist, f\"Path for param id {param_id} does not exist\"\n                param_path = os.path.join(self.swap_folder, f'{param_id}_param.tensor.swp')\n\n                self.id_to_path[param_id] = param_path\n            paths.append(param_path)\n\n        return paths\n\n    def _get_swap_buffers(self, params):\n        buffers = []\n        for param in params:\n            param_id = param.ds_id\n            assert param_id in self.param_id_to_swap_buffer.keys(), \\\n            f'param {param_id} has not been assigned a swap buffer'\n            buffers.append(self.param_id_to_swap_buffer[param_id])\n\n        return buffers\n\n    def _track_numel(self, params):\n        for param in params:\n            assert param.ds_tensor is not None, \"Partitioned tensor is None\"\n            self.param_id_to_numel[param.ds_id] = param.ds_tensor.ds_numel\n\n    def _allocate_and_return_buffers_for_swap_in(self, params):\n        compute_buffers = []\n        swap_buffers = []\n\n        for param in params:\n            param_id = param.ds_id\n            assert param_id in self.param_id_to_numel.keys(), f\" Number of elements in param {param_id} is unknown\"\n            assert param_id not in self.param_id_to_buffer_id.keys(\n            ), f\"param {param_id} already assigned swap buffer id {self.param_id_to_buffer_id[param_id]}\"\n            assert param_id not in self.param_id_to_swap_buffer.keys(\n            ), f\"param {param_id} has already been assigned a swap buffer\"\n\n            buffer_id = self.available_buffer_ids.pop()\n            print_rank_0(f\"param {param.ds_id} is assigned swap in buffer id {buffer_id}  \")\n            self.param_id_to_buffer_id[param_id] = buffer_id\n            aligned_swap_numel = self._io_aligned_numel(self.param_id_to_numel[param_id])\n            swap_buffer = self.buffers.narrow(0, int(buffer_id * self.aligned_elements_per_buffer), aligned_swap_numel)\n\n            self.param_id_to_swap_buffer[param_id] = swap_buffer\n            compute_buffer = swap_buffer.narrow(0, 0, self.param_id_to_numel[param_id])\n            compute_buffers.append(compute_buffer)\n            swap_buffers.append(swap_buffer)\n\n        return compute_buffers, swap_buffers\n\n    #waits for inflight nvme write to complete\n    def synchronize_writes(self):\n        if self.pending_writes == 0:\n            return\n        assert self.pending_writes == self.aio_write_handle.wait()\n        self.pending_writes = 0\n        self.remove_partition_and_release_buffers(self.swap_out_params)\n        self.swap_out_params = []\n\n    #waits for inflight nvme reads to complete\n    def synchronize_reads(self):\n        if self.pending_reads == 0:\n            return\n\n        assert self.pending_reads == self.aio_read_handle.wait()\n\n        self.pending_reads = 0\n\n        for param, swap_in_buffer in zip(self.inflight_params, self.inflight_swap_in_buffers):\n            param_id = param.ds_id\n            compute_buffer = swap_in_buffer.narrow(0, 0, self.param_id_to_numel[param_id])\n            param.ds_tensor.data = compute_buffer.data\n            param.ds_tensor.status = PartitionedParamStatus.AVAILABLE\n\n        self.available_params.update([param.ds_id for param in self.inflight_params])\n        self.available_numel += self.inflight_numel\n\n        self.inflight_params = []\n        self.inflight_swap_in_buffers = []\n        self.inflight_numel = 0\n\n    #Removes the memory assignment and releases the buffers\n    #Should only be executed after swapping out the tensors\n    def remove_partition_and_release_buffers(self, params):\n        for param in params:\n            param_id = param.ds_id\n\n            if param_id in self.param_id_to_buffer_id.keys():\n\n                buffer_id = self.param_id_to_buffer_id[param_id]\n\n                assert buffer_id is not None, \"Missing buffer id for releasing\"\n\n                self.available_buffer_ids.append(buffer_id)\n                del self.param_id_to_buffer_id[param_id]\n                del self.param_id_to_swap_buffer[param_id]\n                print_rank_0(f\"param {param.ds_id} releases buffer id {buffer_id}  \")\n\n                if param_id in self.available_params:\n                    self.available_params.remove(param_id)\n                    self.available_numel -= self.param_id_to_numel[param_id]\n\n            param.ds_tensor.data = self.invalid_buffer.data\n            param.ds_tensor.status = PartitionedParamStatus.NOT_AVAILABLE\n\n    #writes from in memory to nvme. Does not release the buffers\n    def _swap_out(self, params, async_op=True):\n\n        swap_out_paths = self._get_swap_paths(params)\n        swap_out_params = self._get_swap_buffers(params)\n        self._track_numel(params)\n\n        swap_out_tensors(self.aio_write_handle, swap_out_params, swap_out_paths)\n\n        self.pending_writes += len(swap_out_params)\n        self.swap_out_params += params\n\n        if not async_op:\n            self.synchronize_writes()\n\n    #blocking swap out followed by releasing the memory buffers\n    def swap_out_and_release(self, params, async_op=False, force_buffer_release=False):\n        if async_op:\n            assert force_buffer_release, \"Should not release preallocated buffers without completing the swap out. Set force_buffer_release to True to do it anyways\"\n        self._swap_out(params, async_op=async_op)\n\n    # book keeping function for inflight swap in\n    def _update_inflight_swap_in(self, params, swap_in_buffers, inflight_numel):\n        self.inflight_params.extend(params)\n        self.inflight_swap_in_buffers.extend(swap_in_buffers)\n        self.inflight_numel += inflight_numel\n\n        for param in params:\n            param.ds_tensor.status = PartitionedParamStatus.INFLIGHT\n\n        self.pending_reads += len(params)\n\n    #assigns an in memory buffer and swaps in from nvme\n    def swap_in(self, params, async_op=True, swap_in_buffers=None):\n\n        assert all([param.ds_tensor.status == PartitionedParamStatus.NOT_AVAILABLE\n                    for param in params]), \"Some params are already available or in flight\"\n        swap_in_paths = self._get_swap_paths(params)\n\n        if swap_in_buffers is None:\n            if len(self.available_buffer_ids) < len(swap_in_paths):\n                ids = [p.ds_id for p in params]\n                print_rank_0(\n                    f'Not enough swap in buffers {len(self.available_buffer_ids)} for {len(swap_in_paths)} params, ids = {ids}',\n                    force=True)\n                print_rank_0(\n                    f'Num inflight: params {len(self.inflight_params)}, buffers {len(self.inflight_swap_in_buffers)}, numel = {self.inflight_numel}',\n                    force=True)\n                print_rank_0(\n                    f'Num available params: count = {len(self.available_params)}, ids = {self.available_params}, numel = {self.available_numel}',\n                    force=True)\n\n            assert len(swap_in_paths) <= len(\n                self.available_buffer_ids\n            ), f\"Not enough buffers {len(self.available_buffer_ids)} for swapping {len(swap_in_paths)}\"\n            compute_buffers, swap_in_buffers = self._allocate_and_return_buffers_for_swap_in(params)\n            inflight_numel = sum([t.numel() for t in compute_buffers])\n        else:\n            inflight_numel = sum([t.numel() for t in swap_in_buffers])\n\n        swap_in_tensors(self.aio_read_handle, swap_in_buffers, swap_in_paths)\n\n        self._update_inflight_swap_in(params, swap_in_buffers, inflight_numel)\n\n        if not async_op:\n            self.synchronize_reads()\n\n    # Enables swapping into buffer that is out the control of swapper. This is always synchronous\n    def swap_into_buffer(self, param, dest_buffer):\n        assert param.ds_tensor.status == PartitionedParamStatus.NOT_AVAILABLE, f\"param {param.ds_id} is already available or inflight\"\n\n        require_swap_buffer = not (get_accelerator().is_pinned(dest_buffer)\n                                   and self._is_io_aligned(dest_buffer.numel()))\n\n        if require_swap_buffer:\n            assert len(self.available_buffer_ids) > 0, f\"No buffer available to swap param {param.ds_id}.\"\n            compute_buffers, swap_in_buffers = self._allocate_and_return_buffers_for_swap_in([param])\n            inflight_numel = compute_buffers[0].numel()\n        else:\n            swap_in_buffers = [dest_buffer]\n            inflight_numel = dest_buffer.numel()\n\n        swap_in_paths = self._get_swap_paths([param])\n\n        swap_in_tensors(self.aio_read_handle, swap_in_buffers, swap_in_paths)\n        self._update_inflight_swap_in([param], swap_in_buffers, inflight_numel)\n        self.synchronize_reads()\n\n        if require_swap_buffer:\n            dest_buffer.data.copy_(param.ds_tensor.data)\n            # Release swap buffer memory assignment. Note, this will mark the parameter not available.\n            self.remove_partition_and_release_buffers([param])\n\n    #assign a buffer to a param and return the buffer\n    def get_buffer(self, param, numel):\n        param_id = param.ds_id\n\n        assert self.available_swap_in_buffers(\n        ) > 0, f\"No swap buffers to allocate for fp16 param {param_id} of numel = {numel}\"\n        assert numel < self.elements_per_buffer, f\"More elements {numel} than buffer size {self.elements_per_buffer}\"\n\n        self.param_id_to_numel[param_id] = numel\n        buffer_id = self.available_buffer_ids.pop()\n        self.param_id_to_buffer_id[param_id] = buffer_id\n        aligned_swap_numel = self._io_aligned_numel(self.param_id_to_numel[param_id])\n        swap_buffer = self.buffers.narrow(0, int(buffer_id * self.aligned_elements_per_buffer), aligned_swap_numel)\n\n        self.param_id_to_swap_buffer[param_id] = swap_buffer\n        compute_buffer = swap_buffer.narrow(0, 0, self.param_id_to_numel[param_id])\n        print_rank_0(f\"param {param.ds_id} is assigned swap in buffer id {buffer_id}\")\n        return compute_buffer\n\n    def reserve_available_buffers(self):\n        buffers = []\n        for id in self.available_buffer_ids:\n            buffers.append(\n                self.buffers.narrow(0, int(id * self.aligned_elements_per_buffer),\n                                    int(self.aligned_elements_per_buffer)))\n            self.reserved_buffer_ids.append(id)\n\n        self.available_buffer_ids = []\n        return buffers\n\n    def release_reserved_buffers(self):\n        for id in self.reserved_buffer_ids:\n            self.available_buffer_ids.append(id)\n        self.reserved_buffer_ids = []\n\n    def _io_aligned_numel(self, numel):\n        remainder = numel % self.numel_alignment\n        return numel if remainder == 0 else (numel + self.numel_alignment - remainder)\n\n    def _is_io_aligned(self, numel):\n        return (numel % self.numel_alignment) == 0\n\n    def reserve_partitioned_swap_space(self, partition_num_elems):\n        aligned_numel = sum([self._io_aligned_numel(numel) for numel in partition_num_elems])\n        self.partitioned_swap_buffer = get_accelerator().pin_memory(torch.zeros(aligned_numel,\n                                                                                device='cpu',\n                                                                                dtype=self.dtype),\n                                                                    align_bytes=0)\n        self.partitioned_swap_pool = SwapBufferPool([self.partitioned_swap_buffer])\n\n    def swap_out_partitioned_params(self, dst_fp16_params, src_fp32_params):\n        assert self.partitioned_swap_buffer is not None, f'partitioned swap buffers for fp16 params not initialized'\n        assert self.partitioned_swap_pool is not None, f'partitioned swap pool for fp16 params not initialized'\n        assert len(dst_fp16_params) == len(src_fp32_params), \\\n        f'mismatch in number of fp16 params {len(dst_fp16_params)} and fp32 params {len(src_fp32_params)}'\n\n        fp16_swap_paths = self._get_swap_paths(dst_fp16_params, must_exist=True)\n        self.synchronize_writes()\n        self.partitioned_swap_pool.reset()\n        for i, fp32_tensor in enumerate(src_fp32_params):\n            swap_tensor, _ = self.partitioned_swap_pool.insert_tensor(fp32_tensor, fp16_swap_paths[i],\n                                                                      self._io_aligned_numel(fp32_tensor.numel()))\n            assert swap_tensor is not None\n            dst_fp16_params[i].ds_tensor.status = PartitionedParamStatus.AVAILABLE\n\n        self.partitioned_swap_pool.swap_out(self.aio_write_handle)\n\n        for param in dst_fp16_params:\n            param.ds_tensor.status = PartitionedParamStatus.NOT_AVAILABLE\n", "deepspeed/runtime/swap_tensor/utils.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\"\"\"\nFunctionality of swapping tensors to/from (NVMe) storage devices.\n\"\"\"\n\nimport torch\nfrom deepspeed.utils.logging import logger\nfrom deepspeed.accelerator import get_accelerator\n\nfrom deepspeed import comm as dist\n\nMIN_AIO_BYTES = 1024**2\nAIO_ALIGNED_BYTES = 1024\n\n\ndef swap_in_tensors(swap_handle, tensor_buffers, swap_paths):\n    for buffer, path in zip(tensor_buffers, swap_paths):\n        assert (swap_handle.async_pread(buffer, path) == 0)\n\n\ndef swap_out_tensors(swap_handle, tensor_buffers, swap_paths):\n    for buffer, path in zip(tensor_buffers, swap_paths):\n        assert (swap_handle.async_pwrite(buffer, path) == 0)\n\n\ndef print_object(obj, name, exclude_list=[]):\n    logger.info('{}:'.format(name))\n    for arg in sorted(vars(obj)):\n        if not arg in exclude_list:\n            dots = '.' * (29 - len(arg))\n            logger.info('  {} {} {}'.format(arg, dots, getattr(obj, arg)))\n\n\nclass SwapBuffer(object):\n\n    def __init__(self, buffer):\n        self.buffer = buffer\n        self.reset()\n\n    def reset(self):\n        self.offset = 0\n        self.swap_tensors = {}\n        self.compute_tensors = {}\n        self.swap_paths = {}\n        self.num_elem = 0\n\n    def insert_tensor(self, tensor, swap_path, aligned_numel):\n        swap_tensor, compute_tensor = self.allocate_tensor(swap_path, tensor.numel(), aligned_numel)\n        compute_tensor.data.copy_(tensor.data)\n        return swap_tensor, compute_tensor\n\n    def allocate_tensor(self, swap_path, numel, aligned_numel):\n        assert self.has_space(aligned_numel)\n        assert not self.offset in self.swap_tensors\n\n        allocate_offset = self.offset\n        swap_tensor = self.buffer.narrow(0, allocate_offset, aligned_numel)\n        dest_tensor = swap_tensor.narrow(0, 0, numel)\n\n        self.swap_tensors[allocate_offset] = swap_tensor\n        self.compute_tensors[allocate_offset] = dest_tensor\n        self.swap_paths[allocate_offset] = swap_path\n        self.offset += aligned_numel\n        self.num_elem += numel\n\n        return self.swap_tensors[allocate_offset], self.compute_tensors[allocate_offset]\n\n    def has_space(self, numel):\n        return (self.offset + numel) <= self.buffer.numel()\n\n    def get_swap_tensors(self):\n        return [tensor for tensor in self.swap_tensors.values()]\n\n    def get_swap_paths(self):\n        return [path for path in self.swap_paths.values()]\n\n    def get_compute_tensors(self):\n        return [tensor for tensor in self.compute_tensors.values()]\n\n    def get_num_elem(self):\n        return self.num_elem\n\n    def get_swap_tensor(self, offset):\n        return self.swap_tensors.get(offset, None)\n\n    def get_compute_tensor(self, offset):\n        return self.compute_tensors.get(offset, None)\n\n    def get_swap_path(self, offset):\n        return self.swap_paths(offset, None)\n\n\nclass SwapBufferPool(object):\n\n    def __init__(self, buffers):\n        assert all([get_accelerator().is_pinned(buf) for buf in buffers])\n        self.buffers = [SwapBuffer(buf) for buf in buffers]\n        self.current_index = 0\n\n    def reset(self):\n        self.current_index = 0\n        for buffer in self.buffers:\n            buffer.reset()\n\n    def allocate_tensor(self, numel, swap_path, aligned_numel):\n        if self.has_space(aligned_numel):\n            swap_tensor, compute_tensor = self._get_current_buffer().allocate_tensor(swap_path, numel, aligned_numel)\n            return swap_tensor, compute_tensor\n\n        return None, None\n\n    def insert_tensor(self, tensor, swap_path, aligned_numel):\n        if self.has_space(aligned_numel):\n            swap_tensor, compute_tensor = self._get_current_buffer().insert_tensor(tensor, swap_path, aligned_numel)\n            return swap_tensor, compute_tensor\n\n        return None, None\n\n    def get_swap_tensors(self):\n        swap_tensors = []\n        for buffer in self._get_used_buffers():\n            swap_tensors += buffer.get_swap_tensors()\n\n        return swap_tensors\n\n    def get_swap_paths(self):\n        swap_paths = []\n        for buffer in self._get_used_buffers():\n            swap_paths += buffer.get_swap_paths()\n\n        return swap_paths\n\n    def get_compute_tensors(self):\n        compute_tensors = []\n        for buffer in self._get_used_buffers():\n            compute_tensors += buffer.get_compute_tensors()\n\n        return compute_tensors\n\n    def has_space(self, numel):\n        if self._get_current_buffer().has_space(numel):\n            return True\n\n        if self.current_index == len(self.buffers) - 1:\n            return False\n\n        self.current_index += 1\n        return self._get_current_buffer().has_space(numel)\n\n    def swap_out(self, aio_handle, async_op=False):\n        swap_tensors = self.get_swap_tensors()\n        swap_paths = self.get_swap_paths()\n        assert all([p is not None for p in swap_paths])\n\n        swap_out_tensors(aio_handle, swap_tensors, swap_paths)\n\n        if not async_op:\n            assert len(swap_tensors) == aio_handle.wait()\n\n    def swap_in(self, aio_handle, async_op=False):\n        swap_tensors = self.get_swap_tensors()\n        swap_paths = self.get_swap_paths()\n        assert all([p is not None for p in swap_paths])\n\n        swap_in_tensors(aio_handle, swap_tensors, swap_paths)\n\n        if not async_op:\n            assert len(swap_tensors) == aio_handle.wait()\n\n    def _get_current_buffer(self):\n        return self.buffers[self.current_index]\n\n    def _get_used_buffers(self):\n        return self.buffers[:self.current_index + 1]\n\n\nclass SwapBufferManager(object):\n\n    def __init__(self, num_elems, count, dtype):\n        self.num_elems = num_elems\n        self.count = count\n        self.dtype = dtype\n        self.all_buffers = [\n            get_accelerator().pin_memory(torch.zeros(num_elems, device='cpu', dtype=dtype), align_bytes=0)\n            for _ in range(count)\n        ]\n        self.free_buffer_index = [i for i in range(count)]\n        self.used_buffer_index = {}\n        self.gigabytes = (self.all_buffers[0].element_size() * num_elems * count) / (1024**3)\n\n        if dist.get_rank() == 0:\n            exclude_list = ['all_buffers']\n            print_object(obj=self, name='SwapBufferManager', exclude_list=exclude_list)\n\n    def allocate(self, num_elems, count, dtype):\n        assert dtype == self.dtype\n        assert num_elems <= self.num_elems\n        if count > len(self.free_buffer_index):\n            return None\n\n        used_indices = self.free_buffer_index[-count:]\n        self.free_buffer_index = self.free_buffer_index[:-count]\n\n        buffers = []\n        for i in used_indices:\n            tmp_buffer = self.all_buffers[i].narrow(0, 0, num_elems)\n            buffers.append(tmp_buffer)\n            self.used_buffer_index[id(tmp_buffer)] = i\n        return buffers\n\n    def allocate_all(self, num_elems, dtype):\n        return self.allocate(num_elems=num_elems, count=len(self.free_buffer_index), dtype=dtype)\n\n    def free(self, buffers):\n        buffer_ids = []\n        for buf in buffers:\n            buffer_ids.append(id(buf))\n\n        assert all([b_id in self.used_buffer_index for b_id in buffer_ids])\n\n        for b_id in buffer_ids:\n            self.free_buffer_index.append(self.used_buffer_index[b_id])\n            del (self.used_buffer_index[b_id])\n\n\ndef get_sized_buffer(buffer, num_elems):\n    assert num_elems <= buffer.numel(), \\\n        f'num_elems {num_elems} > buffer {buffer.numel()}'\n    return buffer.narrow(0, 0, num_elems) if num_elems < buffer.numel() else buffer\n\n\ndef get_sized_buffers(buffer_list, num_elems_list):\n    swap_buffers = [\n        get_sized_buffer(buffer, num_elems) \\\n        for buffer, num_elems in zip(buffer_list, num_elems_list)\n    ]\n    return swap_buffers\n", "deepspeed/runtime/swap_tensor/constants.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\"\"\"\nAIO\n\"\"\"\nAIO_FORMAT = '''\n\"aio\": {\n  \"block_size\": 1048576,\n  \"queue_depth\": 8,\n  \"thread_count\": 1,\n  \"single_submit\": false,\n  \"overlap_events\": true\n}\n'''\nAIO = \"aio\"\nAIO_BLOCK_SIZE = \"block_size\"\nAIO_BLOCK_SIZE_DEFAULT = 1048576\nAIO_QUEUE_DEPTH = \"queue_depth\"\nAIO_QUEUE_DEPTH_DEFAULT = 8\nAIO_THREAD_COUNT = \"thread_count\"\nAIO_THREAD_COUNT_DEFAULT = 1\nAIO_SINGLE_SUBMIT = \"single_submit\"\nAIO_SINGLE_SUBMIT_DEFAULT = False\nAIO_OVERLAP_EVENTS = \"overlap_events\"\nAIO_OVERLAP_EVENTS_DEFAULT = True\n", "deepspeed/runtime/swap_tensor/pipelined_optimizer_swapper.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\"\"\"\nFunctionality of swapping optimizer tensors to/from (NVMe) storage devices.\n\"\"\"\n\nfrom deepspeed.ops.op_builder import AsyncIOBuilder\nfrom deepspeed import comm as dist\n\nfrom deepspeed.runtime.swap_tensor.constants import *\nfrom deepspeed.runtime.swap_tensor.utils import swap_in_tensors, swap_out_tensors, print_object\nfrom deepspeed.runtime.swap_tensor.async_swapper import AsyncTensorSwapper\nfrom deepspeed.runtime.swap_tensor.utils import get_sized_buffer\nfrom deepspeed.runtime.swap_tensor.optimizer_utils import OptimizerSwapper\n\n\nclass OptimizerSwapOp(object):\n\n    def __init__(self, aio_handle, read_op, param_info, allocated_buffers, state_buffers, num_ops):\n        self.aio_handle = aio_handle\n        self.read_op = read_op\n        self.param_info = param_info\n        self.allocated_buffers = allocated_buffers\n        self.state_buffers = state_buffers\n        self.wait_required = True\n        self.num_ops = num_ops\n\n    def is_parameter(self, parameter):\n        return OptimizerSwapper.parameter_id(parameter) == self.param_info.param_id\n\n    def wait(self):\n        assert self.wait_required\n        assert self.aio_handle.wait() == self.num_ops\n        self.wait_required = False\n\n\nSYNC_SWAP_IN = 'sync_swap_in'\nASYNC_SWAP_IN = 'async_swap_in'\nSYNC_SWAP_OUT = 'sync_swap_out'\nASYNC_SWAP_OUT = 'async_swap_out'\n\nSWAP_IN_STATE_TIMER = 'swap_in_state'\nSWAP_OUT_STATE_TIMER = 'swap_out_state'\nSWAP_OUT_GRADIENT_TIMER = 'swap_out_gradient'\nASYNC_SWAP_IN_STATE_TIMER = \"async_swap_in_state\"\nASYNC_SWAP_OUT_STATE_TIMER = 'async_swap_out_state'\n\n\nclass PipelinedOptimizerSwapper(OptimizerSwapper):\n\n    def __init__(self, swap_config, aio_config, base_folder, optimizer, largest_numel, device, dtype, timers):\n        super(PipelinedOptimizerSwapper, self).__init__(swap_config, aio_config, base_folder, optimizer, largest_numel,\n                                                        device, dtype, timers)\n\n        aio_op = AsyncIOBuilder().load()\n        self.write_aio_handle = aio_op.aio_handle(aio_config[AIO_BLOCK_SIZE], aio_config[AIO_QUEUE_DEPTH],\n                                                  aio_config[AIO_SINGLE_SUBMIT], aio_config[AIO_OVERLAP_EVENTS],\n                                                  aio_config[AIO_THREAD_COUNT])\n\n        self.read_aio_handle = aio_op.aio_handle(aio_config[AIO_BLOCK_SIZE], aio_config[AIO_QUEUE_DEPTH],\n                                                 aio_config[AIO_SINGLE_SUBMIT], aio_config[AIO_OVERLAP_EVENTS],\n                                                 aio_config[AIO_THREAD_COUNT])\n\n        # Overlap gradient swap out\n        self.gradient_swapper = AsyncTensorSwapper(aio_handle=self.write_aio_handle,\n                                                   numel_alignment=self.numel_alignment,\n                                                   timers=self.timers)\n\n        self.async_swap_in = swap_config.pipeline_read\n        self.async_swap_out = swap_config.pipeline_write\n\n        self.swap_ops = {SYNC_SWAP_IN: None, ASYNC_SWAP_IN: None, SYNC_SWAP_OUT: None, ASYNC_SWAP_OUT: None}\n\n        self.print_exclude_list += [\n            'gradient_swapper', 'read_aio_handle', 'write_aio_handle', 'swap_ops', 'print_exclude_list'\n        ]\n\n        if dist.get_rank() == 0:\n            print_object(obj=self, name='PipelinedOptimizerSwapper', exclude_list=self.print_exclude_list)\n\n    def initialize_parameters(self, parameters, src_tensors):\n        self._initialize_parameters(parameters=parameters, src_tensors=src_tensors, aio_handle=self.write_aio_handle)\n\n    def initialize_from_swapped_fp16_params(self, fp16_partitions_info, fp16_num_elems, fp16_pinned_buffers,\n                                            fp32_parameters):\n        self._initialize_from_swapped_fp16_params(aio_handle=self.write_aio_handle,\n                                                  fp16_partitions_info=fp16_partitions_info,\n                                                  fp16_num_elems=fp16_num_elems,\n                                                  fp16_pinned_buffers=fp16_pinned_buffers,\n                                                  fp32_parameters=fp32_parameters)\n\n    def flush_gradients(self):\n        self._flush_gradient_swapper(self.gradient_swapper)\n\n    def swap_in_optimizer_state(self, parameter, async_parameter):\n        assert parameter is not None\n        assert self.swap_ops[SYNC_SWAP_IN] is None\n\n        self._flush_gradient_swapper(self.gradient_swapper)\n\n        self._start_timer(SWAP_IN_STATE_TIMER)\n\n        if self.swap_ops[ASYNC_SWAP_IN]:\n            assert self.swap_ops[ASYNC_SWAP_IN].is_parameter(parameter)\n            self.swap_ops[SYNC_SWAP_IN] = self.swap_ops[ASYNC_SWAP_IN]\n            self.swap_ops[ASYNC_SWAP_IN] = None\n        else:\n            self.swap_ops[SYNC_SWAP_IN] = self._swap_in_optimizer_state(aio_handle=self.read_aio_handle,\n                                                                        parameter=parameter)\n\n        if self.swap_ops[SYNC_SWAP_IN]:\n            self.swap_ops[SYNC_SWAP_IN].wait()\n\n        if self.async_swap_in and async_parameter is not None:\n            assert self.swap_ops[ASYNC_SWAP_IN] is None\n            self.swap_ops[ASYNC_SWAP_IN] = self._swap_in_optimizer_state(aio_handle=self.read_aio_handle,\n                                                                         parameter=async_parameter)\n\n        self._stop_timer(SWAP_IN_STATE_TIMER)\n        self.timer_names.add(SWAP_IN_STATE_TIMER)\n\n    def swap_out_optimizer_state(self, parameter, async_swap):\n        self._start_timer(SWAP_OUT_STATE_TIMER)\n\n        if self.swap_ops[ASYNC_SWAP_OUT]:\n            self._start_timer(ASYNC_SWAP_OUT_STATE_TIMER)\n            self._complete_swap_out(ASYNC_SWAP_OUT)\n            self._stop_timer(ASYNC_SWAP_OUT_STATE_TIMER)\n            self.timer_names.add(ASYNC_SWAP_OUT_STATE_TIMER)\n\n        assert self.swap_ops[SYNC_SWAP_IN] is not None\n        assert not self.swap_ops[SYNC_SWAP_IN].wait_required\n        swap_op = self._swap_out_optimizer_state(aio_handle=self.write_aio_handle,\n                                                 parameter=parameter,\n                                                 swap_in_op=self.swap_ops[SYNC_SWAP_IN])\n        self.swap_ops[SYNC_SWAP_IN] = None\n\n        if self.async_swap_out and async_swap:\n            self.swap_ops[ASYNC_SWAP_OUT] = swap_op\n        else:\n            self.swap_ops[SYNC_SWAP_OUT] = swap_op\n            self._complete_swap_out(SYNC_SWAP_OUT)\n\n        self._stop_timer(SWAP_OUT_STATE_TIMER)\n        self.timer_names.add(SWAP_OUT_STATE_TIMER)\n\n    def swap_out_gradients(self, parameter, gradient_offsets, gradient_tensors):\n        self._swap_out_gradients(parameter=parameter,\n                                 gradient_offsets=gradient_offsets,\n                                 gradient_tensors=gradient_tensors,\n                                 gradient_swapper=self.gradient_swapper)\n\n    def _complete_swap_out(self, swap_out_type):\n        self.swap_ops[swap_out_type].wait()\n        self.swap_buffer_manager.free(self.swap_ops[swap_out_type].allocated_buffers)\n        self.swap_ops[swap_out_type] = None\n\n    def _swap_out_optimizer_state(self, aio_handle, parameter, swap_in_op):\n        assert swap_in_op.is_parameter(parameter)\n\n        allocated_buffers = swap_in_op.allocated_buffers.copy()\n        swap_buffers = swap_in_op.state_buffers.copy()\n\n        param_info = swap_in_op.param_info\n        self._update_param_state_info(param_info, parameter)\n        unpinned_tensors = param_info.get_unpinned_state_tensors()\n\n        if len(unpinned_tensors) > 0:\n            new_alloc_buffers = self.swap_buffer_manager.allocate(num_elems=self._io_aligned_numel(param_info.numel()),\n                                                                  count=len(unpinned_tensors),\n                                                                  dtype=param_info.dtype())\n            assert new_alloc_buffers is not None\n\n            allocated_buffers += new_alloc_buffers\n            swap_buffers += new_alloc_buffers\n\n            for pinned_dst, unpinned_src in zip(new_alloc_buffers, unpinned_tensors):\n                dst = get_sized_buffer(pinned_dst, unpinned_src.numel())\n                dst.data.copy_(unpinned_src.data)\n\n        swap_paths = param_info.swap_paths.copy()\n        assert len(swap_paths) == len(swap_buffers)\n\n        swap_out_tensors(aio_handle, swap_buffers, swap_paths)\n\n        swap_out_op = OptimizerSwapOp(aio_handle=aio_handle,\n                                      param_info=param_info,\n                                      read_op=False,\n                                      allocated_buffers=allocated_buffers,\n                                      state_buffers=swap_buffers,\n                                      num_ops=len(swap_buffers))\n\n        return swap_out_op\n\n    def _swap_in_optimizer_state(self, aio_handle, parameter):\n        param_info = self._get_param_swap_info(parameter)\n        if param_info is None:\n            return None\n\n        required_buffer_count = len(param_info.tensors) + (1 if param_info.has_gradients() else 0)\n        aligned_numel = self._io_aligned_numel(param_info.numel())\n        allocated_buffers = self.swap_buffer_manager.allocate(num_elems=aligned_numel,\n                                                              count=required_buffer_count,\n                                                              dtype=parameter.dtype)\n        assert allocated_buffers is not None, \\\n        f\"PipelinedOptimizerSwapper ran out of swap buffers, try increasing 'buffer_count'\"\n\n        state_buffers = allocated_buffers[:len(param_info.tensors)]\n        param_info.set_swap_buffers(state_buffers)\n\n        swap_buffers = state_buffers.copy()\n        swap_paths = param_info.swap_paths.copy()\n\n        if param_info.has_gradients():\n            parameter.grad = allocated_buffers[-1].narrow(0, 0, param_info.numel())\n            if param_info.swapped_gradients:\n                swap_buffers += param_info.get_swap_gradient_buffers(parameter.grad)\n                swap_paths += param_info.get_swap_gradient_paths()\n\n        swap_in_tensors(aio_handle, swap_buffers, swap_paths)\n\n        if param_info.unswapped_gradients:\n            self._retrieve_unswapped_grad_partitions(swap_info=param_info, dest_buffer=parameter.grad)\n\n        swap_in_op = OptimizerSwapOp(aio_handle=aio_handle,\n                                     param_info=param_info,\n                                     read_op=True,\n                                     allocated_buffers=allocated_buffers,\n                                     state_buffers=state_buffers,\n                                     num_ops=len(swap_buffers))\n\n        return swap_in_op\n", "deepspeed/runtime/swap_tensor/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n", "deepspeed/runtime/swap_tensor/async_swapper.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\"\"\"\nFunctionality of swapping tensors to/from (NVMe) storage devices.\n\"\"\"\nimport torch\n\nfrom deepspeed import comm as dist\nfrom deepspeed.utils.logging import logger\nfrom deepspeed.runtime.swap_tensor.utils import swap_out_tensors, SwapBuffer\nfrom deepspeed.accelerator import get_accelerator\n\nINVALID_BUFFER_INDEX = -1\nASYNC_SWAPPER_WAIT_TIMER = 'async_swap_gradient_wait'\n\n\nclass AsyncTensorSwapper(object):\n\n    def __init__(self, aio_handle, numel_alignment, timers):\n        self.free_buffer_index = []\n        self.swapping_buffer_index = []\n        self.ready_buffer_index = []\n        self.current_buffer_index = INVALID_BUFFER_INDEX\n        self.all_buffers = []\n        self.aio_handle = aio_handle\n        self.numel_alignment = numel_alignment\n        self.max_numel = 0\n        self.num_pending_swaps = 0\n        self.timers = timers\n        self.timer_names = set()\n        self.num_elements_swapped = 0\n        self.dtype = None\n\n    def has_buffers(self):\n        return len(self.all_buffers) > 0\n\n    def add_buffers(self, buffer_list):\n        assert len(self.all_buffers) == 0\n        assert all([get_accelerator().is_pinned(buffer) for buffer in buffer_list])\n        dtype = buffer_list[0].dtype\n        assert all([buffer.dtype == dtype for buffer in buffer_list])\n\n        self.dtype = dtype\n        self.all_buffers = [SwapBuffer(buffer) for buffer in buffer_list]\n        self.free_buffer_index += [i for i in range(len(self.all_buffers))]\n        self.max_numel = max([buffer.numel() for buffer in buffer_list])\n        self.timer_names = set()\n\n    def get_timer_names(self):\n        return list(self.timer_names)\n\n    def release_buffers(self):\n        self._report_statistics('Swapped out[Before flush]')\n        self._flush_buffers_until_complete()\n        self._report_statistics('Swapped out[After flush]')\n\n        pinned_buffers = [buf.buffer for buf in self.all_buffers]\n        self.all_buffers = []\n        self.free_buffer_index = []\n        self.current_buffer_index = INVALID_BUFFER_INDEX\n        self.num_elements_swapped = 0\n        self.dtype = None\n\n        return pinned_buffers\n\n    def swap_out_tensors(self, tensor_list, path_list):\n        for tensor, swap_path in zip(tensor_list, path_list):\n            self._swap_out_tensor(tensor, swap_path)\n\n    def _report_statistics(self, message):\n        if dist.get_rank() == 0:\n            element_size = torch.tensor([], dtype=self.dtype).element_size()\n            swapped_GB = (self.num_elements_swapped * element_size) / (1024**3)\n            logger.debug(f'{message} num_elems = {self.num_elements_swapped}, {swapped_GB:5.2f} GB')\n\n    def _swap_out_tensor(self, tensor, swap_path):\n        assert len(self.all_buffers) > 0\n\n        aligned_numel = self._io_aligned_numel(tensor.numel())\n        assert aligned_numel <= self.max_numel\n\n        self._make_swap_space(aligned_numel)\n        assert self.current_buffer_index != INVALID_BUFFER_INDEX\n\n        swap_buffer = self._get_current_buffer()\n        swap_buffer.insert_tensor(tensor, swap_path, aligned_numel)\n\n    def _make_swap_space(self, numel):\n        if self.current_buffer_index == INVALID_BUFFER_INDEX:\n            self._allocate_buffer()\n            return\n\n        if not self._get_current_buffer().has_space(numel):\n            if len(self.free_buffer_index) > 0:\n                self._flush_ready_buffers()\n            else:\n                self._flush_buffers_until_complete()\n            self._allocate_buffer()\n\n    def _io_aligned_numel(self, numel):\n        remainder = numel % self.numel_alignment\n        return numel if remainder == 0 else (numel + self.numel_alignment - remainder)\n\n    def _allocate_buffer(self):\n        assert self.current_buffer_index == INVALID_BUFFER_INDEX\n        assert len(self.all_buffers) > 0\n        assert len(self.free_buffer_index) > 0\n        self.current_buffer_index = self.free_buffer_index[-1]\n        self.free_buffer_index = self.free_buffer_index[:-1]\n\n    def _flush_ready_buffers(self):\n        if self.current_buffer_index != INVALID_BUFFER_INDEX:\n            self.ready_buffer_index.append(self.current_buffer_index)\n            self.current_buffer_index = INVALID_BUFFER_INDEX\n\n        self._swap_out_ready_buffers()\n\n    def _flush_buffers_until_complete(self):\n        self._flush_ready_buffers()\n        assert len(self.ready_buffer_index) == 0\n\n        self._wait_for_swap_complete()\n        assert len(self.swapping_buffer_index) == 0\n        assert len(self.free_buffer_index) == len(self.all_buffers)\n\n    def _swap_out_ready_buffers(self):\n        for buffer_index in self.ready_buffer_index:\n            buffer = self._get_buffer(buffer_index)\n            swap_tensors = buffer.get_swap_tensors()\n            swap_paths = buffer.get_swap_paths()\n            self.num_pending_swaps += len(swap_tensors)\n            swap_out_tensors(self.aio_handle, swap_tensors, swap_paths)\n\n        self.swapping_buffer_index += self.ready_buffer_index\n        self.ready_buffer_index = []\n\n    def _wait_for_swap_complete(self):\n        assert len(self.swapping_buffer_index) > 0\n\n        self._start_timer(ASYNC_SWAPPER_WAIT_TIMER)\n        assert self.aio_handle.wait() == self.num_pending_swaps\n        self._stop_timer(ASYNC_SWAPPER_WAIT_TIMER)\n        self.timer_names.add(ASYNC_SWAPPER_WAIT_TIMER)\n\n        self.num_pending_swaps = 0\n\n        for buffer_index in self.swapping_buffer_index:\n            buffer = self._get_buffer(buffer_index)\n            self.num_elements_swapped += buffer.get_num_elem()\n            buffer.reset()\n\n        self.free_buffer_index += self.swapping_buffer_index\n        assert len(self.free_buffer_index) <= len(self.all_buffers)\n        self.swapping_buffer_index = []\n\n    def _get_buffer(self, index):\n        assert index != INVALID_BUFFER_INDEX\n        return self.all_buffers[index]\n\n    def _get_current_buffer(self):\n        return self._get_buffer(self.current_buffer_index)\n\n    def _start_timer(self, name):\n        if self.timers:\n            self.timers(name).start()\n\n    def _stop_timer(self, name):\n        if self.timers:\n            self.timers(name).stop()\n\n    def _log_timers(self, name_list, force=False):\n        if self.timers and force:\n            self.timers.log(name_list)\n", "deepspeed/runtime/swap_tensor/optimizer_utils.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\"\"\"\nFunctionality of swapping tensors to/from (NVMe) storage devices.\n\"\"\"\n\nimport os\nimport torch\n\nfrom deepspeed import comm as dist\nfrom deepspeed.utils.logging import logger\nfrom deepspeed.runtime.swap_tensor.constants import *\nfrom deepspeed.runtime.swap_tensor.utils import swap_in_tensors, swap_out_tensors, \\\n    MIN_AIO_BYTES, AIO_ALIGNED_BYTES, get_sized_buffers\nfrom deepspeed.runtime.swap_tensor.utils import SwapBufferManager, SwapBufferPool\nfrom deepspeed.accelerator import get_accelerator\n\n\nclass FlattenedTensorSwapInfo(object):\n\n    def __init__(self, path, length, offset):\n        self.path = path\n        self.offset = offset\n        self.length = length\n\n\nclass OptimizerStateSwapInfo(object):\n\n    def __init__(self, parameter, numel, base_folder):\n        self.tensors = []\n        self.param_id = OptimizerSwapper.parameter_id(parameter)\n        self.swap_folder = base_folder\n        self.swap_paths = []\n        self.swapped_gradients = {}\n        self.unswapped_gradients = {}\n        self.tensor_numel = numel\n        self.tensor_dtype = parameter.dtype\n        self.tensor_device = parameter.device\n        self.has_state_tensors = False\n        self._add_tensors([parameter])\n\n    def numel(self):\n        return self.tensor_numel\n\n    def has_gradients(self):\n        return self.swapped_gradients or self.unswapped_gradients\n\n    def _add_tensors(self, tensor_list):\n        for t in tensor_list:\n            self.tensors.append(t)\n            self.swap_paths.append(os.path.join(self.swap_folder, f'{OptimizerSwapper.parameter_id(t)}.tensor.swp'))\n\n    def add_state_tensors(self, tensor_list):\n        self.has_state_tensors = True\n        self._add_tensors(tensor_list)\n\n    def device(self):\n        return self.tensor_device\n\n    def dtype(self):\n        return self.tensor_dtype\n\n    def release_memory(self):\n        for tensor in self.tensors:\n            tensor.data = torch.Tensor()\n\n    def get_or_create_gradient_paths(self, offsets, lengths):\n        gradient_paths = []\n        for offset, length in zip(offsets, lengths):\n            if not offset in self.swapped_gradients.keys():\n                path = os.path.join(self.swap_folder, f'{self.param_id}_gradient_{offset}_{length}.tensor.swp')\n                self.swapped_gradients[offset] = FlattenedTensorSwapInfo(path, length, offset)\n\n            gradient_paths.append(self.swapped_gradients[offset].path)\n\n        return gradient_paths\n\n    def set_swap_buffers(self, buffers):\n        compute_lengths = [self.numel()] * len(self.tensors)\n        compute_buffers = get_sized_buffers(buffers, compute_lengths)\n        for t, buffer in zip(self.tensors, compute_buffers):\n            t.data = buffer.data\n\n    def get_swap_gradient_buffers(self, swap_buffer):\n        assert self.numel() <= swap_buffer.numel()\n        return [swap_buffer.narrow(0, grad.offset, grad.length) for grad in self.swapped_gradients.values()]\n\n    def get_swap_gradient_paths(self):\n        return [grad.path for grad in self.swapped_gradients.values()]\n\n    def get_unpinned_state_tensors(self):\n        return [t for t in self.tensors if not get_accelerator().is_pinned(t)]\n\n    def read_unswapped_gradients(self, dest_buffer):\n        num_elem_count = 0\n        for offset, grad_partition in self.unswapped_gradients.items():\n            dst_tensor = dest_buffer.narrow(0, offset, grad_partition.numel())\n            dst_tensor.data.copy_(grad_partition.data)\n            num_elem_count += grad_partition.numel()\n\n        return num_elem_count\n\n    def release_unswapped_gradients(self):\n        self.unswapped_gradients = {}\n\n\nSWAPPER_DEBUG_MODE = False\nSWAP_OUT_GRADIENT_TIMER = 'swap_out_gradient'\n\n\nclass OptimizerSwapper(object):\n\n    @staticmethod\n    def parameter_id(param):\n        return param.ds_id\n\n    def __init__(self, swap_config, aio_config, base_folder, optimizer, largest_numel, device, dtype, timers):\n        self.swap_config = swap_config\n        self.aio_config = aio_config\n\n        # NVMe swap management\n        self.swap_params_info = {}\n        self.swap_element_size = torch.tensor([], dtype=dtype).element_size()\n        self.swap_folder = os.path.join(base_folder, 'optimizer', f'rank{dist.get_rank()}')\n        os.makedirs(self.swap_folder, exist_ok=True)\n\n        self.optimizer = optimizer\n\n        # Read/Write alignment for each thread during Intra-request parallelism\n        self.min_aio_bytes = max(MIN_AIO_BYTES, aio_config[AIO_BLOCK_SIZE])\n        self.aligned_bytes = AIO_ALIGNED_BYTES * aio_config[AIO_THREAD_COUNT]\n        self.numel_alignment = self.aligned_bytes // self.swap_element_size\n\n        # Swap buffer management\n        self.largest_numel = self._io_aligned_numel(largest_numel)\n        self.dtype = dtype\n        self.swap_buffer_manager = SwapBufferManager(num_elems=self.largest_numel,\n                                                     count=swap_config.buffer_count,\n                                                     dtype=dtype)\n\n        # Timers\n        self.timers = timers\n        self.timer_names = set()\n\n        # Print exclusion list\n        self.print_exclude_list = [\n            'optimizer',\n            'swap_buffer_manager',\n            'swap_params_info',\n            'timers',\n            'timer_names',\n        ]\n\n    def swappable_tensor(self, param=None, numel=None):\n        assert param is not None or numel is not None, \"Either param or numel must be provided\"\n        if param is not None:\n            return self.min_aio_bytes <= (param.numel() * self.swap_element_size)\n        return self.min_aio_bytes <= (numel * self.swap_element_size)\n\n    def init_timers(self):\n        self.timer_names = set()\n\n    def log_timers(self):\n        if self.timer_names:\n            self._log_timers(list(self.timer_names), force=True)\n\n    def pre_backward(self):\n        self.init_timers()\n\n    def post_backward(self):\n        pass\n\n    def _flush_gradient_swapper(self, gradient_swapper):\n        if gradient_swapper.has_buffers():\n            self._start_timer(SWAP_OUT_GRADIENT_TIMER)\n            pinned_buffers = gradient_swapper.release_buffers()\n            self.swap_buffer_manager.free(pinned_buffers)\n            self._stop_timer(SWAP_OUT_GRADIENT_TIMER)\n            self.timer_names.add(SWAP_OUT_GRADIENT_TIMER)\n            self.timer_names.update(gradient_swapper.get_timer_names())\n\n    def _swap_out_gradients(self, parameter, gradient_offsets, gradient_tensors, gradient_swapper):\n        if not OptimizerSwapper.parameter_id(parameter) in self.swap_params_info.keys():\n            return\n\n        swap_info = self.swap_params_info[OptimizerSwapper.parameter_id(parameter)]\n\n        swappable_tensors = []\n        swappable_offsets = []\n        swappable_lengths = []\n\n        aligned_gradients, aligned_offsets = self._adjust_for_misaligned_lengths(tensors=gradient_tensors,\n                                                                                 offsets=gradient_offsets)\n\n        self._start_timer(SWAP_OUT_GRADIENT_TIMER)\n        for tensor, offset in zip(aligned_gradients, aligned_offsets):\n            if not self.swappable_tensor(param=tensor):\n                swap_info.unswapped_gradients[offset] = tensor\n                continue\n\n            swappable_tensors.append(tensor)\n            swappable_offsets.append(offset)\n            swappable_lengths.append(tensor.numel())\n\n        if len(swappable_tensors) > 0:\n            if not gradient_swapper.has_buffers():\n                pinned_buffers = self.swap_buffer_manager.allocate_all(num_elems=self.largest_numel, dtype=self.dtype)\n\n                gradient_swapper.add_buffers(pinned_buffers)\n\n            swappable_paths = swap_info.get_or_create_gradient_paths(swappable_offsets, swappable_lengths)\n\n            gradient_swapper.swap_out_tensors(tensor_list=swappable_tensors, path_list=swappable_paths)\n\n        self._stop_timer(SWAP_OUT_GRADIENT_TIMER)\n        self.timer_names.add(SWAP_OUT_GRADIENT_TIMER)\n\n    def _initialize_from_swapped_fp16_params(self, aio_handle, fp16_partitions_info, fp16_num_elems,\n                                             fp16_pinned_buffers, fp32_parameters):\n        assert len(fp32_parameters) == len(fp16_partitions_info)\n        assert len(fp32_parameters) == len(fp16_num_elems)\n        assert all([get_accelerator().is_pinned(buffer) for buffer in fp16_pinned_buffers])\n\n        fp32_swap_paths = self._get_swap_paths(parameters=fp32_parameters, num_elems=fp16_num_elems)\n\n        fp32_pinned_buffers = self.swap_buffer_manager.allocate_all(num_elems=self.largest_numel, dtype=self.dtype)\n\n        fp16_buffer_numel = [buf.numel() for buf in fp16_pinned_buffers]\n        assert all([numel >= self.largest_numel for numel in fp16_buffer_numel]), \\\n        f\"numel of fp16 buffers {fp16_buffer_numel} is too small for initializing fp32 params {self.largest_numel}\"\n\n        fp32_swap_buffers = SwapBufferPool(fp32_pinned_buffers)\n        fp16_swap_buffers = SwapBufferPool(fp16_pinned_buffers)\n\n        curr_index = 0\n        while curr_index < len(fp32_parameters):\n            fp16_pinned_tensors = self._swap_in_fp16_params(aio_handle=aio_handle,\n                                                            fp16_num_elems=fp16_num_elems[curr_index:],\n                                                            fp16_partitions_info=fp16_partitions_info[curr_index:],\n                                                            fp16_swap_buffers=fp16_swap_buffers)\n\n            if dist.get_rank() == 0 and SWAPPER_DEBUG_MODE:\n                for i, tensor in enumerate(fp16_pinned_tensors):\n                    true_index = curr_index + i\n                    logger.info(\n                        f'swap_in_fp16_param: fp32_id = {OptimizerSwapper.parameter_id(fp32_parameters[true_index])} index = {true_index} orig_num_elem = {fp16_num_elems[true_index]}, swap_num_elem = {fp16_pinned_tensors[i].numel()}'\n                    )\n\n            swap_out_count = self._swap_out_fp16_params(aio_handle=aio_handle,\n                                                        fp32_swap_paths=fp32_swap_paths[curr_index:],\n                                                        fp32_swap_buffers=fp32_swap_buffers,\n                                                        fp16_pinned_tensors=fp16_pinned_tensors)\n            assert swap_out_count == len(fp16_pinned_tensors), \\\n            f\"{swap_out_count} does not match {len(fp16_pinned_tensors)}\"\n\n            fp16_swap_buffers.reset()\n            fp32_swap_buffers.reset()\n            curr_index += swap_out_count\n\n        self.swap_buffer_manager.free(fp32_pinned_buffers)\n\n    def _swap_in_fp16_params(self, aio_handle, fp16_num_elems, fp16_partitions_info, fp16_swap_buffers):\n        assert len(fp16_num_elems) > 0\n\n        swapped_fp16_tensors = []\n        swap_tensors = []\n        swap_paths = []\n        unswapped_srcs = []\n        unswapped_dsts = []\n\n        for i, numel in enumerate(fp16_num_elems):\n            pinned_tensor, _ = fp16_swap_buffers.allocate_tensor(numel, None, numel)\n            if pinned_tensor is None:\n                break\n\n            swapped_fp16_tensors.append(pinned_tensor)\n            offset = 0\n            for tensor, partition_numel, partition_path in fp16_partitions_info[i]:\n                dst_tensor = pinned_tensor.narrow(0, offset, partition_numel)\n                if partition_path is None:\n                    unswapped_srcs.append(tensor)\n                    unswapped_dsts.append(dst_tensor)\n                else:\n                    swap_paths.append(partition_path)\n                    swap_tensors.append(dst_tensor)\n                offset += partition_numel\n\n        assert len(swapped_fp16_tensors) + len(unswapped_srcs) > 0\n        ret = swap_in_tensors(aio_handle, swap_tensors, swap_paths)\n        for src, dst in zip(unswapped_srcs, unswapped_dsts):\n            dst.data.copy_(src.data)\n\n        assert len(swap_tensors) == aio_handle.wait()\n\n        return swapped_fp16_tensors\n\n    def _swap_out_fp16_params(self, aio_handle, fp32_swap_paths, fp32_swap_buffers, fp16_pinned_tensors):\n\n        assert len(fp16_pinned_tensors) <= len(fp32_swap_paths)\n        swap_out_count = 0\n        for i, fp16_tensor in enumerate(fp16_pinned_tensors):\n            if not fp32_swap_buffers.has_space(fp16_tensor.numel()):\n                fp32_swap_buffers.swap_out(aio_handle)\n                fp32_swap_buffers.reset()\n\n            pinned_tensor, _ = fp32_swap_buffers.insert_tensor(fp16_tensor, fp32_swap_paths[i],\n                                                               self._io_aligned_numel(fp16_tensor.numel()))\n            assert pinned_tensor is not None\n            swap_out_count += 1\n\n        if len(fp32_swap_buffers.get_swap_tensors()) > 0:\n            fp32_swap_buffers.swap_out(aio_handle)\n\n        return swap_out_count\n\n    def _initialize_parameters(self, parameters, src_tensors, aio_handle):\n        assert len(parameters) == len(src_tensors)\n\n        swap_paths = self._get_swap_paths(parameters=parameters, num_elems=[src.numel() for src in src_tensors])\n\n        SWAP_INIT_TIMER = \"swap_init_write\"\n        self._start_timer(SWAP_INIT_TIMER)\n\n        pinned_buffers = self.swap_buffer_manager.allocate_all(num_elems=self.largest_numel, dtype=self.dtype)\n        assert pinned_buffers is not None\n\n        self._swap_out_unpinned_tensors(aio_handle=aio_handle,\n                                        unpinned_tensors=src_tensors,\n                                        dest_paths=swap_paths,\n                                        pinned_buffers=pinned_buffers)\n\n        if dist.get_rank() == 0 and SWAPPER_DEBUG_MODE:\n            for i, tensor in enumerate(src_tensors):\n                logger.info(\n                    f'copy_in_fp16_param: fp32_id = {OptimizerSwapper.parameter_id(parameters[i])} index = {i}, swap_num_elem = {src_tensors[i].numel()}'\n                )\n\n        self.swap_buffer_manager.free(pinned_buffers)\n\n        self._stop_timer(SWAP_INIT_TIMER)\n        self._log_timers([SWAP_INIT_TIMER])\n\n    def _get_swap_paths(self, parameters, num_elems):\n        swap_info_list = [\n            self._create_param_swap_info(parameter=p,\n                                         numel=numel) \\\n            for p, numel in zip(parameters, num_elems)\n        ]\n        assert len(swap_info_list) == len(num_elems)\n\n        swap_paths = [info.swap_paths[0] for info in swap_info_list]\n        return swap_paths\n\n    def _swap_out_unpinned_tensors(self, aio_handle, unpinned_tensors, dest_paths, pinned_buffers):\n\n        swap_buffer_count = len(pinned_buffers)\n        unpinned_tensor_count = len(unpinned_tensors)\n\n        for i in range(0, unpinned_tensor_count, swap_buffer_count):\n            swap_tensor_count = min((unpinned_tensor_count - i), swap_buffer_count)\n\n            src_tensors = unpinned_tensors[i:(i + swap_tensor_count)]\n            compute_lengths = [t.numel() for t in src_tensors]\n            compute_buffers = get_sized_buffers(pinned_buffers, compute_lengths)\n\n            for dst, src in zip(compute_buffers, src_tensors):\n                dst.data.copy_(src.data)\n\n            swap_lengths = [self._io_aligned_numel(t.numel()) for t in src_tensors]\n            swap_buffers = get_sized_buffers(pinned_buffers, swap_lengths)\n\n            swap_paths = dest_paths[i:(i + swap_tensor_count)]\n            swap_out_tensors(aio_handle, swap_buffers, swap_paths)\n\n            assert aio_handle.wait() == swap_tensor_count\n\n    def _adjust_for_misaligned_lengths(self, tensors, offsets):\n        new_tensors = []\n        new_offsets = []\n\n        for orig_tensor, orig_offset in zip(tensors, offsets):\n            if not self.swappable_tensor(param=orig_tensor):\n                new_tensors.append(orig_tensor)\n                new_offsets.append(orig_offset)\n                continue\n\n            remainder = orig_tensor.numel() % self.numel_alignment\n            if remainder == 0:\n                new_tensors.append(orig_tensor)\n                new_offsets.append(orig_offset)\n                continue\n\n            # Split into two by making remainder a tensor\n            aligned_length = (orig_tensor.numel() // self.numel_alignment) * self.numel_alignment\n            new_tensors.append(orig_tensor.narrow(0, 0, aligned_length))\n            new_offsets.append(orig_offset)\n\n            # remainder tensor\n            new_tensors.append(orig_tensor.narrow(0, aligned_length, remainder))\n            new_offsets.append(orig_offset + aligned_length)\n\n        return new_tensors, new_offsets\n\n    def _retrieve_unswapped_grad_partitions(self, swap_info, dest_buffer):\n        UNSWAPPED_READ_GRADIENTS = 'unswapped_read_gradients'\n        self._start_timer(UNSWAPPED_READ_GRADIENTS)\n        tensor_count = len(swap_info.unswapped_gradients)\n        num_elem_count = swap_info.read_unswapped_gradients(dest_buffer)\n        self._stop_timer(UNSWAPPED_READ_GRADIENTS)\n        self._log_timers([UNSWAPPED_READ_GRADIENTS])\n\n        # It should be safe to discard unswapped gradient partitions\n        swap_info.release_unswapped_gradients()\n\n        if SWAPPER_DEBUG_MODE:\n            logger.info(\n                f'optimizer_retrieve_unswapped_gradients: param={swap_info.param_id} tensor_count={tensor_count} elem_count={num_elem_count}'\n            )\n\n    def _get_state_tensors(self, parameter):\n        if not parameter in self.optimizer.state:\n            return []\n\n        tensor_list = []\n        for state_name, value in self.optimizer.state[parameter].items():\n            if torch.is_tensor(value):\n                value.ds_id = state_name + '-' + parameter.ds_id\n                tensor_list.append(value)\n\n        return tensor_list\n\n    def _update_param_state_info(self, swap_info, parameter):\n        if not swap_info.has_state_tensors:\n            state_tensors = self._get_state_tensors(parameter)\n            if state_tensors:\n                swap_info.add_state_tensors(state_tensors)\n\n    def _create_param_swap_info(self, parameter, numel):\n        param_id = OptimizerSwapper.parameter_id(parameter)\n        assert not param_id in self.swap_params_info\n\n        self.swap_params_info[param_id] = OptimizerStateSwapInfo(parameter=parameter,\n                                                                 numel=numel,\n                                                                 base_folder=self.swap_folder)\n        swap_info = self.swap_params_info[param_id]\n\n        self._update_param_state_info(swap_info, parameter)\n\n        return swap_info\n\n    def _get_param_swap_info(self, parameter):\n        param_id = OptimizerSwapper.parameter_id(parameter)\n        swap_info = self.swap_params_info.get(param_id, None)\n\n        if swap_info is not None:\n            self._update_param_state_info(swap_info, parameter)\n\n        return swap_info\n\n    def _start_timer(self, name):\n        if self.timers:\n            self.timers(name).start()\n\n    def _stop_timer(self, name):\n        if self.timers:\n            self.timers(name).stop()\n\n    def _log_timers(self, name_list, force=False):\n        if self.timers and (SWAPPER_DEBUG_MODE or force):\n            self.timers.log(name_list)\n\n    def _io_aligned_numel(self, numel):\n        remainder = numel % self.numel_alignment\n        return numel if remainder == 0 else (numel + self.numel_alignment - remainder)\n", "deepspeed/runtime/activation_checkpointing/config.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom deepspeed.runtime.config_utils import get_scalar_param, DeepSpeedConfigObject\n\n#########################################\n#  DeepSpeed Activation Checkpointing\n#########################################\n# Activation Checkpointing Allows to save memory by only keeping a select few\n#activations for the backpropagation.\nACTIVATION_CHKPT_FORMAT = '''\nActivation Checkpointing should be configured as:\n\"session_params\": {\n  \"activation_checkpointing\": {\n    \"partitioned_activations\": [true|false],\n    \"number_checkpoints\": 100,\n    \"contiguous_memory_optimization\": [true|false],\n    \"cpu_checkpointing\": [true|false],\n    \"profile\": [true|false],\n    \"synchronize_checkpoint_boundary\": [true|false],\n    }\n}\n'''\n\nACT_CHKPT_PARTITION_ACTIVATIONS = 'partition_activations'\nACT_CHKPT_PARTITION_ACTIVATIONS_DEFAULT = False\n\nACT_CHKPT_NUMBER_CHECKPOINTS = 'number_checkpoints'\nACT_CHKPT_NUMBER_CHECKPOINTS_DEFAULT = None\n\nACT_CHKPT_CONTIGUOUS_MEMORY_OPTIMIZATION = 'contiguous_memory_optimization'\nACT_CHKPT_CONTIGUOUS_MEMORY_OPTIMIZATION_DEFAULT = False\n\nACT_CHKPT_SYNCHRONIZE_CHECKPOINT_BOUNDARY = 'synchronize_checkpoint_boundary'\nACT_CHKPT_SYNCHRONIZE_CHECKPOINT_BOUNDARY_DEFAULT = False\n\nACT_CHKPT_PROFILE = 'profile'\nACT_CHKPT_PROFILE_DEFAULT = False\n\nACT_CHKPT_CPU_CHECKPOINTING = 'cpu_checkpointing'\nACT_CHKPT_CPU_CHECKPOINTING_DEFAULT = False\n\nACT_CHKPT = 'activation_checkpointing'\n\nACT_CHKPT_DEFAULT = {\n    ACT_CHKPT_PARTITION_ACTIVATIONS: ACT_CHKPT_PARTITION_ACTIVATIONS_DEFAULT,\n    ACT_CHKPT_NUMBER_CHECKPOINTS: ACT_CHKPT_NUMBER_CHECKPOINTS_DEFAULT,\n    ACT_CHKPT_CONTIGUOUS_MEMORY_OPTIMIZATION: ACT_CHKPT_CONTIGUOUS_MEMORY_OPTIMIZATION_DEFAULT,\n    ACT_CHKPT_SYNCHRONIZE_CHECKPOINT_BOUNDARY: ACT_CHKPT_SYNCHRONIZE_CHECKPOINT_BOUNDARY_DEFAULT,\n    ACT_CHKPT_PROFILE: ACT_CHKPT_PROFILE_DEFAULT,\n    ACT_CHKPT_CPU_CHECKPOINTING: ACT_CHKPT_CPU_CHECKPOINTING_DEFAULT\n}\n\n\nclass DeepSpeedActivationCheckpointingConfig(DeepSpeedConfigObject):\n\n    def __init__(self, param_dict):\n        super(DeepSpeedActivationCheckpointingConfig, self).__init__()\n\n        self.partition_activations = None\n        self.contiguous_memory_optimization = None\n        self.cpu_checkpointing = None\n        self.number_checkpoints = None\n        self.synchronize_checkpoint_boundary = None\n        self.profile = None\n\n        if ACT_CHKPT in param_dict.keys():\n            act_chkpt_config_dict = param_dict[ACT_CHKPT]\n        else:\n            act_chkpt_config_dict = ACT_CHKPT_DEFAULT\n\n        self._initialize(act_chkpt_config_dict)\n\n    def _initialize(self, act_chkpt_config_dict):\n        self.partition_activations = get_scalar_param(act_chkpt_config_dict, ACT_CHKPT_PARTITION_ACTIVATIONS,\n                                                      ACT_CHKPT_PARTITION_ACTIVATIONS_DEFAULT)\n\n        self.contiguous_memory_optimization = get_scalar_param(act_chkpt_config_dict,\n                                                               ACT_CHKPT_CONTIGUOUS_MEMORY_OPTIMIZATION,\n                                                               ACT_CHKPT_CONTIGUOUS_MEMORY_OPTIMIZATION_DEFAULT)\n\n        self.cpu_checkpointing = get_scalar_param(act_chkpt_config_dict, ACT_CHKPT_CPU_CHECKPOINTING,\n                                                  ACT_CHKPT_CPU_CHECKPOINTING_DEFAULT)\n\n        self.number_checkpoints = get_scalar_param(act_chkpt_config_dict, ACT_CHKPT_NUMBER_CHECKPOINTS,\n                                                   ACT_CHKPT_NUMBER_CHECKPOINTS_DEFAULT)\n\n        self.profile = get_scalar_param(act_chkpt_config_dict, ACT_CHKPT_PROFILE, ACT_CHKPT_PROFILE_DEFAULT)\n\n        self.synchronize_checkpoint_boundary = get_scalar_param(act_chkpt_config_dict,\n                                                                ACT_CHKPT_SYNCHRONIZE_CHECKPOINT_BOUNDARY,\n                                                                ACT_CHKPT_SYNCHRONIZE_CHECKPOINT_BOUNDARY_DEFAULT)\n", "deepspeed/runtime/activation_checkpointing/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n'''Copyright The Microsoft DeepSpeed Team'''\n", "deepspeed/runtime/activation_checkpointing/checkpointing.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\"\"\"\nUse to partition the activations stored for backward propagation\nTherefore reduces the memory consumption\nAlso implements CPU checkpointing and contiguous memory checkpointing\nReduces memory consumption and memory fragmentation\n\nCode for rng checkpointing taken from NVIDIA Megatron-LM mpu/random.py\nb886b7bb972afe72bac0f5de4f42a4a7bae8ebef\n\"\"\"\n\n# Parts of the code here are adapted from PyTorch\n# repo: https://github.com/pytorch/pytorch\nimport copy\nimport torch\nimport contextlib\nfrom deepspeed import comm as dist\nimport weakref\n\nimport mmap\nfrom torch import _C\n\nfrom deepspeed.runtime.config import DeepSpeedConfig\nfrom deepspeed.utils import logger\nfrom deepspeed.runtime.utils import copy_to_device, move_to_device, see_memory_usage\nfrom deepspeed.utils.timer import SynchronizedWallClockTimer as Timers, FORWARD_GLOBAL_TIMER\nfrom deepspeed.utils.bwc import bwc_tensor_model_parallel_rank\nfrom deepspeed.accelerator import get_accelerator\nfrom deepspeed.runtime import compiler\n\n# DeepSpeed Checkpointing Enabled or Disabled\ndeepspeed_checkpointing_enabled = False\n\n# MP parameters\nmpu = None\nmp_rank = None\nmp_size = None\nmp_group = None\n\n# Model Parameters\nnum_layers = None\n\n# Checkpointing buffers\ncontiguous_data_buffers = []\ndata_offsets = []\n\ncontiguous_size_buffers = []\nsize_offsets = []\n\ntimers = None\n\n# optimization flags\nPARTITION_ACTIVATIONS = False\nCPU_CHECKPOINT = False\nCONTIGUOUS_CHECKPOINTING = False\nSYNCHRONIZE = False\nPROFILE_TIME = False\n\n# Default name for the model parallel rng tracker.\n_MODEL_PARALLEL_RNG_TRACKER_NAME = 'model-parallel-rng'\ntransport_stream = None\ncuda_device = None\n\n\ndef detach_variable(inputs, device=None):\n    if isinstance(inputs, tuple):\n        out = []\n        for inp in inputs:\n            if not isinstance(inp, torch.Tensor):\n                out.append(inp)\n                continue\n\n            requires_grad = inp.requires_grad\n\n            if device is not None:\n                x = inp.to(device=device)\n            else:\n                x = inp\n\n            x = x.detach()\n            x.requires_grad = requires_grad\n            out.append(x)\n        return tuple(out)\n    else:\n        raise RuntimeError(\"Only tuple of tensors is supported. Got Unsupported input type: \", type(inputs).__name__)\n\n\ndef _set_cuda_rng_state(new_state, device=-1):\n    \"\"\"Sets the random number generator state of the current GPU.\n\n    Arguments:\n        new_state (torch.ByteTensor): The desired state\n    This function is adapted from PyTorch repo (torch.cuda.set_rng_state) #ignore-cuda\n    with a single change: the input state is not cloned. Cloning caused\n    major performance issues for +4 GPU cases.\n    \"\"\"\n    if hasattr(_C, '_cuda_setRNGState') and callable(_C._cuda_setRNGState):\n        # older PyTorch\n        def cb():\n            with get_accelerator().device(device):\n                _C._cuda_setRNGState(new_state)\n    else:\n        # newer PyTorch\n        if device == -1:\n            device = torch.device(get_accelerator().device_name())\n        elif isinstance(device, str):\n            device = torch.device(device)\n        elif isinstance(device, int):\n            device = torch.device(get_accelerator().device_name(), device)\n\n        def cb():\n            idx = device.index\n            if idx is None:\n                idx = get_accelerator().current_device()\n            default_generator = get_accelerator().default_generator(idx)\n            default_generator.set_state(new_state)\n\n    get_accelerator().lazy_call(cb)\n\n\nclass CudaRNGStatesTracker:\n    \"\"\"Tracker for the cuda RNG states.\n\n    Using the `add` method, a cuda rng state is initialized based on\n    the input `seed` and is assigned to `name`. Later, by forking the\n    rng state, we can perform operations and return to our starting\n    cuda state.\n    \"\"\"\n\n    def __init__(self):\n        # Map from a string name to the cuda rng state.\n        self.states_ = {}\n        # Seeds are just for book keeping and ensure no seed is set twice.\n        self.seeds_ = set()\n\n    def reset(self):\n        \"\"\"Set to the initial state (no tracker).\"\"\"\n        self.states_ = {}\n        self.seeds_ = set()\n\n    def get_states(self):\n        \"\"\"Get rng states. Copy the dictionary so we have direct\n        pointers to the states, not just a pointer to the dictionary.\"\"\"\n        return copy.copy(self.states_)\n\n    def set_states(self, states):\n        \"\"\"Set the rng states. For efficiency purposes, we do not check\n        the size of seed for compatibility.\"\"\"\n        self.states_ = states\n\n    def add(self, name, seed):\n        \"\"\"Track the rng state.\"\"\"\n        # Check seed is not already used.\n        if seed in self.seeds_:\n            raise Exception('seed {} already exists'.format(seed))\n        self.seeds_.add(seed)\n        # Check that state is not already defined.\n        if name in self.states_:\n            raise Exception('cuda rng state {} already exists'.format(name))\n        # Get the current rng state.\n        orig_rng_state = get_accelerator().get_rng_state()\n        # Set the new state and store it.\n        get_accelerator().manual_seed(seed)\n        self.states_[name] = get_accelerator().get_rng_state()\n        # Reset rng state to what it was.\n        _set_cuda_rng_state(orig_rng_state)\n\n    @contextlib.contextmanager\n    def fork(self, name=_MODEL_PARALLEL_RNG_TRACKER_NAME):\n        \"\"\"Fork the cuda rng state, perform operations, and exit with\n        the original state.\"\"\"\n        # Check if we have added the state\n        if name not in self.states_:\n            raise Exception('cuda rng state {} is not added'.format(name))\n        # Store current rng state.\n        orig_cuda_rng_state = get_accelerator().get_rng_state()\n        # Set rng state to the desired one\n        _set_cuda_rng_state(self.states_[name])\n        # Do the stuff we wanted to do.\n        try:\n            yield\n        finally:\n            # Update the current rng state for later use.\n            self.states_[name] = get_accelerator().get_rng_state()\n            # And set the state to the original state we started with.\n            _set_cuda_rng_state(orig_cuda_rng_state)\n\n\n# RNG tracker object.\n_CUDA_RNG_STATE_TRACKER = CudaRNGStatesTracker()\n\n\ndef get_cuda_rng_tracker():\n    \"\"\"Get cuda rng tracker.\"\"\"\n    return _CUDA_RNG_STATE_TRACKER\n\n\ndef model_parallel_cuda_manual_seed(seed):\n    \"\"\"Initialize model parallel cuda seed.\n\n    This function should be called after the model parallel is\n    initialized. Also, no get_accelerator().manual_seed should be called\n    after this function. Basically, this is replacement for that\n    function.\n    Two set of RNG states are tracked:\n        default state: This is for data parallelism and is the same among a\n                       set of model parallel GPUs but different across\n                       different model parallel groups. This is used for\n                       example for dropout in the non-model-parallel regions.\n        model-parallel state: This state is different among a set of model\n                              parallel GPUs, but the same across data parallel\n                              groups. This is used for example for dropout in\n                              model parallel regions.\n    \"\"\"\n    global mpu\n\n    tp_rank = bwc_tensor_model_parallel_rank(mpu)\n\n    # 2718 is just for fun and any POSITIVE value will work.\n    offset = seed + 2718\n    model_parallel_seed = offset + tp_rank\n    # Data parallel gets the original seed.\n    data_parallel_seed = seed\n\n    if dist.get_rank() == 0:\n        logger.info(\n            '> initializing model parallel cuda seeds on global rank {}, '\n            'model parallel rank {}, and data parallel rank {} with '\n            'model parallel seed: {} and data parallel seed: {}'.format(dist.get_rank(), tp_rank,\n                                                                        mpu.get_data_parallel_rank(),\n                                                                        model_parallel_seed, data_parallel_seed), )\n    _CUDA_RNG_STATE_TRACKER.reset()\n    # Set the default state.\n    get_accelerator().manual_seed(data_parallel_seed)\n    # and model parallel state.\n    _CUDA_RNG_STATE_TRACKER.add(_MODEL_PARALLEL_RNG_TRACKER_NAME, model_parallel_seed)\n\n\ndef model_parallel_reconfigure_tp_seed(seed):\n    global mpu\n    tp_rank = bwc_tensor_model_parallel_rank(mpu)\n    model_parallel_seed = seed + 2718 + tp_rank\n    with _CUDA_RNG_STATE_TRACKER.fork():\n        get_accelerator().manual_seed(model_parallel_seed)\n\n\ndef get_partition_start(item):\n    global mp_rank, mp_size, mp_group\n    size = item.numel()\n    partition_size = size / mp_size\n    start = partition_size * mp_rank\n    return int(start)\n\n\ndef get_partition_size(item):\n    global mp_rank, mp_size, mp_group\n    size = item.numel()\n    assert size % mp_size == 0, \"Doesn't handle if partition activation if item is not divisible by mp size\"\n    partition_size = size / mp_size\n    return int(partition_size)\n\n\ndef gather_partitioned_activations(tensors, device=None):\n    global mp_rank, mp_size, mp_group\n    assert len(tensors) % 2 == 0, f'Expected even count of tensors, instead got {len(tensors)}'\n    inputs = []\n    num_args = int(len(tensors) / 2)\n    for i in range(num_args):\n\n        item = tensors[2 * i]\n        size = tensors[2 * i + 1]\n\n        if not is_activation_to_checkpoint(item):\n            inputs.append(item)\n            continue\n\n        # don't need to do all_gather if model parallel is not enabled\n        if mp_group is None or mp_size == 1:\n            item = item.view(list(size.numpy()))\n            if device is not None:\n                item = item.to(device)\n            inputs.append(item)\n            continue\n\n        partition_size = item.numel()\n        tensor_size = partition_size * mp_size\n        if device is not None:\n            flat_tensor = torch.zeros([tensor_size], dtype=item.dtype, device=device)\n        else:\n            flat_tensor = torch.zeros([tensor_size], dtype=item.dtype, device=item.device)\n        part = flat_tensor.narrow(0, partition_size * mp_rank, partition_size)\n        part.copy_(item)\n        dist.all_gather_into_tensor(flat_tensor, part, group=mp_group)\n        input_tensor = flat_tensor.view(list(size.numpy()))\n        item.data = input_tensor.data\n\n        inputs.append(item)\n\n    return tuple(inputs)\n\n\ndef extract_tensors(all_objects):\n    \"\"\"\n    Separate objects in list/tuple into tensors and non-tensors and create a mapping to enable re-aggregation.\n    The order of tensors and non-tensors is preserved in their respective output groups.\n\n    Parameters:\n        all_objects (list/tuple): Objects containing tensors and non-tensors to be split.\n\n    Returns:\n        tuple: Containing tensors, non-tensors, and bools of whether each position in original list/tuple was a tensor.\n\n    \"\"\"\n    tensor_objects = [v for v in all_objects if torch.is_tensor(v)]\n    non_tensor_objects = [v for v in all_objects if not torch.is_tensor(v)]\n    tensor_flags = [torch.is_tensor(v) for v in all_objects]\n    if type(all_objects) is tuple:\n        return tuple(tensor_objects), tuple(non_tensor_objects), tuple(tensor_flags)\n    return tensor_objects, non_tensor_objects, tensor_flags\n\n\ndef merge_tensors(tensor_objects, non_tensor_objects, tensor_flags):\n    \"\"\"\n    Merge two lists (or tuples) of tensors and non-tensors using a mapping of positions in merged list (or tuple).\n\n    Parameters:\n        tensor_objects (list/tuple): Tensors to merge.\n        non_tensor_objects (list/tuple): Non-tensors to merge.\n        tensor_flags (list/tuple): Indicates whether each position in output is a tensor.\n\n    Returns:\n        tuple: Merge of tensors and non-tensors\n    \"\"\"\n    merged_objects = []\n    tensor_idx = 0\n    non_tensor_idx = 0\n\n    real_tensor_flags = None\n\n    # remove the flags that are assigned to the size of the flattened tensors\n    if PARTITION_ACTIVATIONS:\n        real_tensor_flags = []\n        previous_flag = False\n        for flag in tensor_flags:\n            if previous_flag:\n                previous_flag = False\n                continue\n            previous_flag = flag\n            real_tensor_flags.append(flag)\n    else:\n        real_tensor_flags = tensor_flags\n\n    for is_tensor in real_tensor_flags:\n        if is_tensor:\n            merged_objects.append(tensor_objects[tensor_idx])\n            tensor_idx += 1\n        else:\n            merged_objects.append(non_tensor_objects[non_tensor_idx])\n            non_tensor_idx += 1\n\n    return tuple(merged_objects)\n\n\ndef is_activation_to_checkpoint(item):\n    \"\"\"\n        Is an activation to be checkpointed\n    \"\"\"\n    global mp_size\n    return torch.is_tensor(item) and item.is_floating_point() and item.numel() >= mp_size\n\n\ndef partition_activations(args, cpu_checkpoint, contiguous_checkpoint):\n    global contiguous_data_buffers, data_offsets\n\n    inputs = []\n    num_non_fp_tensors = 0\n\n    for arg_index, item in enumerate(args):\n        if not is_activation_to_checkpoint(item):\n            inputs.append(item)\n            num_non_fp_tensors += 1\n            continue\n\n        i = arg_index - num_non_fp_tensors\n        partition_size = get_partition_size(item)\n        partition = item.detach().contiguous().view(-1).narrow(0, get_partition_start(item), partition_size).clone()\n\n        buffer_device = torch.device('cpu') if cpu_checkpoint else partition.device\n\n        if contiguous_checkpoint:\n            if i >= len(contiguous_data_buffers):\n                tensor_list = [\n                    torch.tensor(()).new_empty([partition_size], dtype=partition.dtype, device=buffer_device)\n                    for _ in range(num_layers)\n                ]\n                contiguous_data_buffers.append(tensor_list)\n                data_offsets.append(0)\n            elif contiguous_data_buffers[i] is None:\n                tensor_list = [\n                    torch.tensor(()).new_empty([partition_size], dtype=partition.dtype, device=buffer_device)\n                    for _ in range(num_layers)\n                ]\n                contiguous_data_buffers[i] = tensor_list\n                data_offsets[i] = 0\n\n            # Because the 'new_empty' returns uninitialized pages,\n            # the pages need to be populated during the cudaMemcpy time\n            # which increases the data copy time. To avoid this, we\n            # pre-populate these pages by simply writing 0 ahead of\n            # the actual cudaMemcpy operation time. Due to the\n            # previously launched GPU kernels, there is a small\n            # window of time here for CPUs to populate pages asynchronously.\n            contiguous_data_buffers[i][data_offsets[i]].data[range(\n                0, contiguous_data_buffers[i][data_offsets[i]].data.shape[0],\n                int(mmap.PAGESIZE / contiguous_data_buffers[i][data_offsets[i]].data.element_size()))] = 0\n\n            contiguous_partition = contiguous_data_buffers[i][data_offsets[i]].data.copy_(partition.data)\n            data_offsets[i] = data_offsets[i] + 1\n            inputs.append(contiguous_partition)\n        else:\n            partition = partition.cpu() if CPU_CHECKPOINT else partition\n            inputs.append(partition)\n\n    return inputs\n\n\ndef get_partitioned_activations_for_backward(args, inputs, contiguous_checkpoint):\n    global contiguous_size_buffers, size_offsets\n\n    new_args = []\n    num_non_fp_tensors = 0\n\n    for arg_index, (arg, inp) in enumerate(zip(args, inputs)):\n        size = torch.tensor(arg.size()) if torch.is_tensor(arg) else None\n        if not is_activation_to_checkpoint(arg):\n            new_args.append(arg)\n            new_args.append(size)\n            num_non_fp_tensors += 1\n            continue\n\n        arg.data = torch.empty([], device=arg.device).data\n        arg.saved_data = inp.data\n\n        new_args.append(arg)\n        i = arg_index - num_non_fp_tensors\n\n        if contiguous_checkpoint:\n            numel = size.numel()\n            if i >= len(contiguous_size_buffers):\n                tmp = torch.tensor(())\n                contiguous_size_buffers.append(\n                    tmp.new_empty([numel * num_layers], dtype=size.dtype, device=size.device))\n                size_offsets.append(0)\n            elif contiguous_size_buffers[i] is None:\n                tmp = torch.tensor(())\n                contiguous_size_buffers[i] = tmp.new_empty([numel * num_layers], dtype=size.dtype, device=size.device)\n                size_offsets[i] = 0\n\n            contiguous_size = contiguous_size_buffers[i].narrow(0, size_offsets[i], numel).data.copy_(size.data)\n            contiguous_size = contiguous_size.view_as(size)\n            size_offsets[i] = size_offsets[i] + numel\n            new_args.append(contiguous_size)\n        else:\n            new_args.append(size)\n\n    return new_args\n\n\ndef get_cpu_activations_for_backward(args, inputs):\n    new_args = []\n    for i, (arg, inp) in enumerate(zip(args, inputs)):\n        if not is_activation_to_checkpoint(arg):\n            new_args.append(arg)\n            continue\n\n        arg.data = torch.empty([], device=arg.device).data\n        arg.saved_data = inp.data\n        new_args.append(arg)\n\n    return new_args\n\n\nclass CheckpointFunction(torch.autograd.Function):\n    \"\"\"This function is adapted from torch.utils.checkpoint with\n       two main changes:\n           1) torch.cuda.set_rng_state is replaced with `_set_cuda_rng_state`  #ignore-cuda\n           2) the states in the model parallel tracker are also properly\n              tracked/set/reset.\n           3) Performance activation partitioning, contiguous memory optimization\n           4) CPU Checkpointing\n           5) Profile forward and backward functions\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, run_function, all_outputs, *args):\n        global mpu, timers, SYNCHRONIZE, PROFILE_TIME\n\n        def save_args_for_backward(*all_args):\n            tensor_args, non_tensor_args, tensor_flags = extract_tensors(all_objects=all_args)\n            ctx.deepspeed_saved_tensors = tensor_args\n            ctx.non_tensor_args = non_tensor_args\n            ctx.tensor_flags = tensor_flags\n\n        if SYNCHRONIZE:\n            get_accelerator().synchronize()\n\n        if timers is None and PROFILE_TIME:\n            timers = Timers()\n\n        if PROFILE_TIME:\n            timers(FORWARD_GLOBAL_TIMER).start()\n\n        ctx.run_function = run_function\n        global num_layers\n        global mp_rank, mp_size, mp_group\n        global contiguous_data_buffers, contiguous_size_buffers\n        global data_offsets, size_offsets\n        if mp_rank is None:\n            if mpu is not None:\n                if hasattr(mpu, 'get_tensor_model_parallel_rank'):\n                    mp_rank = mpu.get_tensor_model_parallel_rank()\n                    mp_size = mpu.get_tensor_model_parallel_world_size()\n                    mp_group = mpu.get_tensor_model_parallel_group()\n                else:\n                    mp_rank = mpu.get_model_parallel_rank()\n                    mp_size = mpu.get_model_parallel_world_size()\n                    mp_group = mpu.get_model_parallel_group()\n            else:\n                mp_rank = 0\n                mp_size = 1\n                mp_group = None\n\n        global cuda_device, transport_stream, PARTITION_ACTIVATIONS, buffer_0, buffer_1, buffer_0_offset, buffer_1_offset\n\n        if cuda_device is None:\n            see_memory_usage(\"First Forward Beginning\", force=False)\n            if dist.get_rank() == 0:\n                logger.info(f\"Activation Checkpointing Information\")\n                logger.info(f\"----Partition Activations {PARTITION_ACTIVATIONS}, CPU CHECKPOINTING {CPU_CHECKPOINT}\")\n                logger.info(\n                    f\"----contiguous Memory Checkpointing {CONTIGUOUS_CHECKPOINTING} with {num_layers} total layers\")\n                logger.info(f\"----Synchronization {SYNCHRONIZE}\")\n                logger.info(f\"----Profiling time in checkpointing {PROFILE_TIME}\")\n\n            cuda_device = get_accelerator().current_device_name()\n            transport_stream = get_accelerator().Stream(device=cuda_device)\n\n        if PARTITION_ACTIVATIONS:\n            inputs = partition_activations(args, CPU_CHECKPOINT, CONTIGUOUS_CHECKPOINTING)\n        elif CPU_CHECKPOINT:\n            inputs = copy_to_device(args, device=torch.device('cpu'), criterion_func=is_activation_to_checkpoint)\n\n        # just in case something funky is happening such as reuse of inputs\n        inputs_cuda = copy_to_device(args, device=cuda_device, criterion_func=is_activation_to_checkpoint)\n\n        # Copy the rng states.\n        ctx.fwd_cpu_rng_state = torch.get_rng_state()\n        ctx.fwd_cuda_rng_state = get_accelerator().get_rng_state()\n        ctx.fwd_cuda_rng_state_tracker = get_cuda_rng_tracker().get_states()\n\n        see_memory_usage(\"Before running forward on the layer\", force=False)\n        # ctx.save_for_backward(*args)\n        with torch.no_grad():\n            outputs = run_function(*inputs_cuda)\n\n        see_memory_usage(\"After running forward on the layer\", force=False)\n        del inputs_cuda\n\n        if PARTITION_ACTIVATIONS:\n            new_args = get_partitioned_activations_for_backward(args, inputs, CONTIGUOUS_CHECKPOINTING)\n            assert len(new_args) % 2 == 0, f'save_for_backward called with odd number of args, {len(new_args)}'\n            save_args_for_backward(*new_args)\n        elif CPU_CHECKPOINT:\n            new_args = get_cpu_activations_for_backward(args, inputs)\n            save_args_for_backward(*new_args)\n        else:\n            save_args_for_backward(*args)\n\n        if PROFILE_TIME:\n            timers(FORWARD_GLOBAL_TIMER).stop()\n            timers.log([FORWARD_GLOBAL_TIMER])\n        if SYNCHRONIZE:\n            get_accelerator().synchronize()\n\n        # Tensors returned from forward() may not be differentiable.\n        if torch.is_tensor(outputs):\n            non_grad_outputs = [outputs] if not outputs.is_floating_point() else []\n        else:\n            non_grad_outputs = [o for o in outputs if torch.is_tensor(o) and not o.is_floating_point()]\n        ctx.mark_non_differentiable(*non_grad_outputs)\n\n        if torch.is_tensor(outputs):\n            all_outputs += [outputs]\n            return outputs\n        else:\n            all_outputs += outputs\n            outputs, _, _ = extract_tensors(all_objects=outputs)\n            return tuple(outputs)\n\n    @staticmethod\n    def backward(ctx, *grads):\n        global timers\n        see_memory_usage(\"In backward\", force=False)\n        # removing pointers to the contiguous buffer memory\n        # so that they can be garbage collected once the checkpoints\n        # have been used\n        if SYNCHRONIZE:\n            get_accelerator().synchronize()\n        if PROFILE_TIME:\n            timers('backward').start()\n\n        if CONTIGUOUS_CHECKPOINTING:\n            global data_offsets, size_offsets\n            global contiguous_data_buffers, contiguous_size_buffers\n\n            for buffers in contiguous_data_buffers:\n                buffers = []\n\n            # frees up all the pointers to the checkpoints except for the ones\n            # stored by save for backward\n            contiguous_data_buffers = []\n            contiguous_size_buffers = []\n            data_offsets = []\n            size_offsets = []\n\n        see_memory_usage(\"In backward checkpointing code\", force=False)\n        if not torch.autograd._is_checkpoint_valid():\n            raise RuntimeError(\"Checkpointing is not compatible with .grad(), \"\n                               \"please use .backward() if possible\")\n\n        global cuda_device, transport_stream, PARTITION_ACTIVATIONS\n\n        # Rebuild deepspeed_saved_tensors\n        for t in ctx.deepspeed_saved_tensors:\n            if t is not None and hasattr(t, 'saved_data') and t.saved_data is not None:\n                t.data = t.saved_data.to(t.device)\n                t.saved_data = None\n\n        if PARTITION_ACTIVATIONS:\n            # with get_accelerator().stream(transport_stream):\n            inputs = gather_partitioned_activations(ctx.deepspeed_saved_tensors,\n                                                    device=cuda_device if CPU_CHECKPOINT else None)\n            detached_inputs = detach_variable(inputs)\n        elif CPU_CHECKPOINT:\n            inputs = move_to_device(ctx.deepspeed_saved_tensors, cuda_device, is_activation_to_checkpoint)\n            detached_inputs = detach_variable(inputs)\n        else:\n            inputs = ctx.deepspeed_saved_tensors\n            detached_inputs = detach_variable(inputs)\n\n        # Add non tensor input args\n        detached_inputs = merge_tensors(tensor_objects=detached_inputs,\n                                        non_tensor_objects=ctx.non_tensor_args,\n                                        tensor_flags=ctx.tensor_flags)\n\n        # Store the current states.\n        bwd_cpu_rng_state = torch.get_rng_state()\n        bwd_cuda_rng_state = get_accelerator().get_rng_state()\n        bwd_cuda_rng_state_tracker = get_cuda_rng_tracker().get_states()\n\n        # Set the states to what it used to be before the forward pass.\n        torch.set_rng_state(ctx.fwd_cpu_rng_state)\n        _set_cuda_rng_state(ctx.fwd_cuda_rng_state)\n        get_cuda_rng_tracker().set_states(ctx.fwd_cuda_rng_state_tracker)\n\n        # if PARTITION_ACTIVATIONS:\n        #     current_stream=get_accelerator().current_stream()\n        #     current_stream.wait_stream(transport_stream)\n\n        see_memory_usage(\"In backward checkpointing code before forward\", force=False)\n\n        with torch.enable_grad():\n            outputs = ctx.run_function(*detached_inputs)\n\n        see_memory_usage(\"In backward checkpointing code after forward\", force=False)\n        # Set the states back to what it was at the start of this function.\n        torch.set_rng_state(bwd_cpu_rng_state)\n        _set_cuda_rng_state(bwd_cuda_rng_state)\n        get_cuda_rng_tracker().set_states(bwd_cuda_rng_state_tracker)\n\n        if isinstance(outputs, torch.Tensor):\n            outputs = (outputs, )\n\n        # Filter out non tensor outputs\n        outputs, _, _ = extract_tensors(all_objects=outputs)\n\n        # Construct arguments to autograd.backward().\n        # This is usually just outputs and grads, but forward() can return tensors that\n        # are not differentiable.\n        output_tensors = []\n        grad_tensors = []\n        for out, grad in zip(outputs, grads):\n            if out.requires_grad:\n                output_tensors.append(out)\n                grad_tensors.append(grad)\n\n        see_memory_usage(\"In backward checkpointing code before backward\", force=False)\n\n        torch.autograd.backward(output_tensors, grad_tensors)\n\n        # Force clear our stashed tensors to prevent a memory leak in certain scenarios\n        ctx.deepspeed_saved_tensors = None\n        ctx.non_tensor_args = None\n        ctx.tensor_flags = None\n\n        see_memory_usage(\"After backward checkpointing code after backward\", force=False)\n\n        if PROFILE_TIME:\n            timers('backward').stop()\n            timers.log(['backward'])\n        if SYNCHRONIZE:\n            get_accelerator().synchronize()\n        ret_list = [None, None]  # first None for ctx\n        for inp in detached_inputs:\n            if torch.is_tensor(inp):\n                ret_list.append(inp.grad)\n            else:\n                ret_list.append(None)\n\n        return tuple(ret_list)\n\n\ndef non_reentrant_checkpoint(function, *args):\n    \"\"\"This function is union of `torch.utils.checkpoint._checkpoint_without_reentrant` and `CheckpointFunction` in this module\n\n    This function is aim to solve the back probagation error raised from all input requires no grad.\n    * has already been implemented in pytorch for a while, the solution is stable at most time except for jit module mode.\n    * can help to solve the issue which is hacked by `deepspeed.runtime.pipe.module.PipelineModule._is_checkpointable`\n\n    Main modifications compared to the implementation of torch:\n    1. adapt to the signature of `checkpoint` function in this module\n    2. solve the non-deterministic by random state management consistent with deepspeed `CheckpointFunction`\n    3. when there is partition or cpu checkpointing, gather them in the unpack_hook during back probagation\n    4. make all after backward blocks in the hook which will executed after all leaf nodes backward execution.\n    5. above 4. is inspired by `torch.autograd.graph.register_multi_grad_hook`, which is only implemented after 2.0.0\n    \"\"\"\n    global mpu, timers, SYNCHRONIZE, PROFILE_TIME\n\n    deepspeed_saved_tensors = None\n    non_tensor_args = None\n    tensor_flags = None\n\n    def save_args_for_backward(*all_args):\n        \"\"\"keep this function to reduce the modification from original implementation\"\"\"\n        nonlocal deepspeed_saved_tensors, non_tensor_args, tensor_flags\n        tensor_args, non_tensor_args, tensor_flags = extract_tensors(all_objects=all_args)\n        deepspeed_saved_tensors = tensor_args\n        non_tensor_args = non_tensor_args\n        tensor_flags = tensor_flags\n\n    if SYNCHRONIZE:\n        get_accelerator().synchronize()\n\n    if timers is None and PROFILE_TIME:\n        timers = Timers()\n\n    if PROFILE_TIME:\n        timers(FORWARD_GLOBAL_TIMER).start()\n\n    global num_layers\n    global mp_rank, mp_size, mp_group\n    global contiguous_data_buffers, contiguous_size_buffers\n    global data_offsets, size_offsets\n    if mp_rank is None:\n        if mpu is not None:\n            if hasattr(mpu, 'get_tensor_model_parallel_rank'):\n                mp_rank = mpu.get_tensor_model_parallel_rank()\n                mp_size = mpu.get_tensor_model_parallel_world_size()\n                mp_group = mpu.get_tensor_model_parallel_group()\n            else:\n                mp_rank = mpu.get_model_parallel_rank()\n                mp_size = mpu.get_model_parallel_world_size()\n                mp_group = mpu.get_model_parallel_group()\n        else:\n            mp_rank = 0\n            mp_size = 1\n            mp_group = None\n\n    global cuda_device, transport_stream, PARTITION_ACTIVATIONS, buffer_0, buffer_1, buffer_0_offset, buffer_1_offset\n\n    if cuda_device is None:\n        see_memory_usage(\"First Forward Beginning\", force=False)\n        if dist.get_rank() == 0:\n            logger.info(f\"Activation Checkpointing Information\")\n            logger.info(f\"----Partition Activations {PARTITION_ACTIVATIONS}, CPU CHECKPOINTING {CPU_CHECKPOINT}\")\n            logger.info(\n                f\"----contiguous Memory Checkpointing {CONTIGUOUS_CHECKPOINTING} with {num_layers} total layers\")\n            logger.info(f\"----Synchronization {SYNCHRONIZE}\")\n            logger.info(f\"----Profiling time in checkpointing {PROFILE_TIME}\")\n\n        cuda_device = get_accelerator().current_device_name()\n        transport_stream = get_accelerator().Stream(device=cuda_device)\n\n    if PARTITION_ACTIVATIONS:\n        inputs = partition_activations(args, CPU_CHECKPOINT, CONTIGUOUS_CHECKPOINTING)\n    elif CPU_CHECKPOINT:\n        inputs = copy_to_device(args, device=torch.device('cpu'), criterion_func=is_activation_to_checkpoint)\n\n    # just in case something funky is happening such as reuse of inputs\n    inputs_cuda = copy_to_device(args, device=cuda_device, criterion_func=is_activation_to_checkpoint)\n\n    # Copy the rng states.\n    fwd_cpu_rng_state = torch.get_rng_state()\n    fwd_cuda_rng_state = get_accelerator().get_rng_state()\n    fwd_cuda_rng_state_tracker = get_cuda_rng_tracker().get_states()\n\n    if PARTITION_ACTIVATIONS:\n        new_args = get_partitioned_activations_for_backward(args, inputs, CONTIGUOUS_CHECKPOINTING)\n        assert len(new_args) % 2 == 0, f'save_for_backward called with odd number of args, {len(new_args)}'\n        save_args_for_backward(*new_args)\n    elif CPU_CHECKPOINT:\n        new_args = get_cpu_activations_for_backward(args, inputs)\n        save_args_for_backward(*new_args)\n    else:\n        save_args_for_backward(*args)\n\n    class Holder():\n        \"\"\"the place holder object used as activations to save memory\"\"\"\n        pass\n\n    # weakref seems utilized to discover the tensor deletion before a whole\n    # forward backward pair loop finished\n    storage: weakref.WeakKeyDictionary = weakref.WeakKeyDictionary()\n    weak_holder_list = []\n    leaf_tensors = []\n    backward_visited_leaf_nodes = 0\n\n    def checkpoint_pack(tensor_from_forward):\n        \"\"\"used to record the activation order in the `weak_holder_list`\n\n        the activation order in holder list is consistent between the first forward and recomputing forward.\n        * the jit compiled forward will break the order consistency *\n        \"\"\"\n        res = Holder()\n        weak_holder_list.append(weakref.ref(res))\n\n        # if this is a leaf tensor, save it for backward progression trace\n        # leaf tensor used to be input or parameters, which is not activations and\n        # has no memory overhead\n        if tensor_from_forward.requires_grad and tensor_from_forward.is_leaf:\n            leaf_tensors.append(tensor_from_forward)\n        return res\n\n    def checkpoint_unpack(holder_from_backward):\n        \"\"\"retrieve the activations from recompute\"\"\"\n        nonlocal deepspeed_saved_tensors, non_tensor_args, tensor_flags\n\n        # if this is the first step of backward probagation, recompute the graph and save\n        # all the activations with the same order as `checkpoint_pack` does\n        if len(storage) == 0:\n            unpack_counter = 0\n\n            def replay_pack(tensor_from_replay):\n                \"\"\"save recompute activations\"\"\"\n                nonlocal unpack_counter\n                unpack_counter += 1\n\n                if weak_holder_list[unpack_counter - 1]() is None:\n                    return\n\n                detached_activations = tensor_from_replay.detach()\n                storage[weak_holder_list[unpack_counter - 1]()] = detached_activations\n\n                return\n\n            def replay_unpack(none_value):\n                \"\"\"recompute graph need not to backward\"\"\"\n                raise RuntimeError(\"You are calling backwards on a tensor that is never exposed.\")\n\n            global timers\n            see_memory_usage(\"In backward\", force=False)\n            # removing pointers to the contiguous buffer memory\n            # so that they can be garbage collected once the checkpoints\n            # have been used\n            if SYNCHRONIZE:\n                get_accelerator().synchronize()\n            if PROFILE_TIME:\n                timers('backward').start()\n\n            if CONTIGUOUS_CHECKPOINTING:\n                global data_offsets, size_offsets\n                global contiguous_data_buffers, contiguous_size_buffers\n\n                for buffers in contiguous_data_buffers:\n                    buffers = []\n\n                # frees up all the pointers to the checkpoints except for the ones\n                # stored by save for backward\n                contiguous_data_buffers = []\n                contiguous_size_buffers = []\n                data_offsets = []\n                size_offsets = []\n\n            see_memory_usage(\"In backward checkpointing code\", force=False)\n            if not torch.autograd._is_checkpoint_valid():\n                raise RuntimeError(\"Checkpointing is not compatible with .grad(), \"\n                                   \"please use .backward() if possible\")\n\n            global cuda_device, transport_stream, PARTITION_ACTIVATIONS\n\n            # gather inputs which is partitioned or checkpointed before first forward\n            if PARTITION_ACTIVATIONS:\n                # with get_accelerator().stream(transport_stream):\n                inputs = gather_partitioned_activations(deepspeed_saved_tensors,\n                                                        device=cuda_device if CPU_CHECKPOINT else None)\n                detached_inputs = detach_variable(inputs)\n            elif CPU_CHECKPOINT:\n                inputs = move_to_device(deepspeed_saved_tensors, cuda_device, is_activation_to_checkpoint)\n                detached_inputs = detach_variable(inputs)\n            else:\n                inputs = deepspeed_saved_tensors\n                detached_inputs = detach_variable(inputs)\n\n            # Add non tensor input args\n            detached_inputs = merge_tensors(tensor_objects=detached_inputs,\n                                            non_tensor_objects=non_tensor_args,\n                                            tensor_flags=tensor_flags)\n\n            # Store the current states.\n            bwd_cpu_rng_state = torch.get_rng_state()\n            bwd_cuda_rng_state = get_accelerator().get_rng_state()\n            bwd_cuda_rng_state_tracker = get_cuda_rng_tracker().get_states()\n\n            # Set the states to what it used to be before the forward pass.\n            torch.set_rng_state(fwd_cpu_rng_state)\n            _set_cuda_rng_state(fwd_cuda_rng_state)\n            get_cuda_rng_tracker().set_states(fwd_cuda_rng_state_tracker)\n\n            see_memory_usage(\"In backward checkpointing code before forward\", force=False)\n            with torch.enable_grad(), torch.autograd.graph.saved_tensors_hooks(replay_pack, replay_unpack):\n                _unused = function(*detached_inputs)\n\n            see_memory_usage(\"In backward checkpointing code after forward\", force=False)\n            # Set the states back to what it was at the start of this function.\n            torch.set_rng_state(bwd_cpu_rng_state)\n            _set_cuda_rng_state(bwd_cuda_rng_state)\n            get_cuda_rng_tracker().set_states(bwd_cuda_rng_state_tracker)\n\n            deepspeed_saved_tensors = None\n            non_tensor_args = None\n            tensor_flags = None\n\n        if holder_from_backward not in storage:\n            raise RuntimeError(\"Attempt to retrieve a tensor saved by autograd multiple times without checkpoint\"\n                               \" recomputation being triggered in between, this is not currently supported.\")\n\n        return storage[holder_from_backward]\n\n    def after_backward_hook(_nonuse_grads):\n        \"\"\"the hook registered to all leaf tensors\"\"\"\n        nonlocal leaf_tensors, backward_visited_leaf_nodes\n        backward_visited_leaf_nodes += 1\n\n        if backward_visited_leaf_nodes == len(leaf_tensors):\n            see_memory_usage(\"After backward checkpointing code after backward\", force=False)\n\n            if PROFILE_TIME:\n                timers('backward').stop()\n                timers.log(['backward'])\n            if SYNCHRONIZE:\n                get_accelerator().synchronize()\n\n    with torch.autograd.graph.saved_tensors_hooks(checkpoint_pack, checkpoint_unpack):\n        outputs = function(*inputs_cuda)\n    for leaf_tensor in leaf_tensors:\n        leaf_tensor.register_hook(after_backward_hook)\n\n    see_memory_usage(\"After running forward on the layer\", force=False)\n\n    if PROFILE_TIME:\n        timers(FORWARD_GLOBAL_TIMER).stop()\n        timers.log([FORWARD_GLOBAL_TIMER])\n    if SYNCHRONIZE:\n        get_accelerator().synchronize()\n\n    all_outputs = []\n    if torch.is_tensor(outputs):\n        all_outputs += [outputs]\n    else:\n        all_outputs += outputs\n\n    if len(all_outputs) == 1:\n        return all_outputs[0]\n    else:\n        return tuple(all_outputs)\n\n\n@compiler.disable  # WA from Pytorch repo for compile + zero 3 accuracy issue\ndef checkpoint(function, *args):\n    \"\"\"Checkpoint a model or part of the model.\n    This has been directly copied from torch.utils.checkpoint. \"\"\"\n\n    all_outputs = []\n    CheckpointFunction.apply(function, all_outputs, *args)\n    if len(all_outputs) == 1:\n        return all_outputs[0]\n    else:\n        return tuple(all_outputs)\n\n\ndef partition_activations_in_checkpoint(partition_activation):\n    global PARTITION_ACTIVATIONS\n    PARTITION_ACTIVATIONS = partition_activation\n    if dist.get_rank() == 0:\n        logger.info(f\"**************Partition Activations {PARTITION_ACTIVATIONS}************\")\n\n\ndef set_num_layers(nlayers):\n    global num_layers\n    num_layers = nlayers\n\n\ndef reset():\n    \"\"\"Resets memory buffers related to contiguous memory optimizations.\n    Should be called during eval when multiple forward propagations are\n    computed without any backward propagation that usually clears these\n    buffers.\n    Arguments:\n        None\n\n    Return:\n        None\n    \"\"\"\n    if CONTIGUOUS_CHECKPOINTING:\n        global data_offsets, size_offsets\n        global contiguous_data_buffers, contiguous_size_buffers\n\n        for buffers in contiguous_data_buffers:\n            buffers = []\n\n        # frees up all the pointers to the checkpoints except for the ones\n        # stored by save for backward\n        contiguous_data_buffers = []\n        contiguous_size_buffers = []\n        data_offsets = []\n        size_offsets = []\n\n\ndef _configure_using_config_file(config, mpu=None):\n    global num_layers, PARTITION_ACTIVATIONS, CONTIGUOUS_CHECKPOINTING, \\\n        CPU_CHECKPOINT, SYNCHRONIZE, PROFILE_TIME\n\n    config = DeepSpeedConfig(config, mpu=mpu).activation_checkpointing_config\n    if dist.get_rank() == 0:\n        logger.info(config.repr())\n    PARTITION_ACTIVATIONS = config.partition_activations\n    CONTIGUOUS_CHECKPOINTING = config.contiguous_memory_optimization\n    num_layers = config.number_checkpoints\n    CPU_CHECKPOINT = config.cpu_checkpointing\n    SYNCHRONIZE = config.synchronize_checkpoint_boundary\n    PROFILE_TIME = config.profile\n\n\ndef _configure_defaults():\n\n    global mpu, num_layers, deepspeed_checkpointing_enabled\n\n    global PARTITION_ACTIVATIONS, CONTIGUOUS_CHECKPOINTING, \\\n        CPU_CHECKPOINT, SYNCHRONIZE, PROFILE_TIME\n\n    PARTITION_ACTIVATIONS = False\n    CONTIGUOUS_CHECKPOINTING = False\n    num_layers = False\n    CPU_CHECKPOINT = False\n    SYNCHRONIZE = False\n    PROFILE_TIME = False\n    deepspeed_checkpointing_enabled = True\n\n\ndef configure(\n    mpu_,\n    deepspeed_config=None,\n    partition_activations=None,\n    contiguous_checkpointing=None,\n    num_checkpoints=None,\n    checkpoint_in_cpu=None,\n    synchronize=None,\n    profile=None,\n):\n    \"\"\"Configure DeepSpeed Activation Checkpointing.\n\n    Arguments:\n        mpu_: Optional: An object that implements the following methods\n            get_model_parallel_rank/group/world_size, and get_data_parallel_rank/group/world_size\n\n        deepspeed_config: Optional: DeepSpeed Config json file when provided will be used to\n            configure DeepSpeed Activation Checkpointing\n\n        partition_activations: Optional: Partitions activation checkpoint across model parallel\n            GPUs when enabled. By default False. Will overwrite deepspeed_config if provided\n\n        contiguous_checkpointing: Optional: Copies activation checkpoints to a contiguous memory\n            buffer. Works only with homogeneous checkpoints when partition_activations is enabled.\n            Must provide num_checkpoints. By default False. Will overwrite deepspeed_config if\n            provided\n\n        num_checkpoints: Optional: Number of activation checkpoints stored during the forward\n            propagation of the model. Used to calculate the buffer size for contiguous_checkpointing\n            Will overwrite deepspeed_config if provided\n\n        checkpoint_in_cpu: Optional: Moves the activation checkpoint to CPU. Only works with\n            partition_activation. Default is false. Will overwrite deepspeed_config if provided\n\n        synchronize: Optional: Performs get_accelerator().synchronize() at the beginning and end of\n            each call to deepspeed.checkpointing.checkpoint for both forward and backward pass.\n            By default false. Will overwrite deepspeed_config if provided\n\n        profile: Optional: Logs the forward and backward time for each\n            deepspeed.checkpointing.checkpoint invocation. Will overwrite deepspeed_config\n            if provided\n\n    Returns:\n        None\n    \"\"\"\n    global mpu, num_layers, deepspeed_checkpointing_enabled\n\n    global PARTITION_ACTIVATIONS, CONTIGUOUS_CHECKPOINTING, \\\n        CPU_CHECKPOINT, SYNCHRONIZE, PROFILE_TIME\n\n    _configure_defaults()\n\n    if mpu_ is not None:\n        mpu = mpu_\n\n    if deepspeed_config is not None:\n        _configure_using_config_file(deepspeed_config, mpu=mpu)\n\n    if partition_activations is not None:\n        PARTITION_ACTIVATIONS = partition_activations\n\n    if contiguous_checkpointing is not None:\n        CONTIGUOUS_CHECKPOINTING = contiguous_checkpointing\n\n    if num_checkpoints is not None:\n        num_layers = num_checkpoints\n\n    if checkpoint_in_cpu is not None:\n        CPU_CHECKPOINT = checkpoint_in_cpu\n\n    if synchronize is not None:\n        SYNCHRONIZE = synchronize\n\n    if profile is not None:\n        PROFILE_TIME = profile\n\n    if CONTIGUOUS_CHECKPOINTING:\n        assert PARTITION_ACTIVATIONS, \"Contiguous Checkpointing is only available with partitioned activations. Set partitioned activations to true in deepspeed config\"\n    if CONTIGUOUS_CHECKPOINTING:\n        assert num_layers is not None, \"Must specify the number of layers with contiguous memory checkpointing\"\n\n\ndef is_configured():\n    \"\"\"True if deepspeed activation checkpointing has been configured\n        by calling deepspeed.checkpointing.configure, else returns false\n\n    Arguments:\n        None\n\n    Return:\n        True of configured, else False\n    \"\"\"\n    return deepspeed_checkpointing_enabled\n", "deepspeed/runtime/zero/config.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport sys\nfrom typing import Optional\nfrom enum import Enum\nfrom deepspeed.pydantic_v1 import Field, validator, root_validator\nfrom deepspeed.runtime.config_utils import get_scalar_param, pp_int, DeepSpeedConfigModel\nfrom deepspeed.utils import logger\nfrom .offload_config import DeepSpeedZeroOffloadParamConfig, DeepSpeedZeroOffloadOptimizerConfig, OffloadDeviceEnum\n\n# ZeRO optimization. By default, this optimization is not enabled.\n# Users have to configure the desired optimization (0 means disabled) in params.json as below example:\nZERO_FORMAT = \"\"\"\nZeRO optimization should be enabled as:\n\"session_params\": {\n  \"zero_optimization\": {\n    \"stage\": [0|1|2],\n    \"stage3_max_live_parameters\" : 1000000000,\n    \"stage3_max_reuse_distance\" : 1000000000,\n    \"stage3_use_all_reduce_for_fetch_params\": [true|false],\n    \"allgather_partitions\": [true|false],\n    \"use_multi_rank_bucket_allreduce\": [true|false],\n    \"allgather_bucket_size\": 500000000,\n    \"reduce_scatter\": [true|false],\n    \"contiguous_gradients\" : [true|false]\n    \"overlap_comm\": [true|false],\n    \"reduce_bucket_size\": 500000000,\n    \"load_from_fp32_weights\": [true|false],\n    \"cpu_offload\": [true|false] (deprecated),\n    \"cpu_offload_params\" : [true|false] (deprecated),\n    \"cpu_offload_use_pin_memory\": [true|false] (deprecated),\n    \"sub_group_size\" : 1000000000000,\n    \"offload_param\": {...},\n    \"offload_optimizer\": {...},\n    \"ignore_unused_parameters\": [true|false],\n    \"round_robin_gradients\": [true|false],\n    \"zero_hpz_partition_size\": 1,\n    \"zero_quantized_weights\": [true|false],\n    \"zero_quantized_nontrainable_weights\": [true|false],\n    \"zero_quantized_gradients\": [true|false],\n    \"memory_efficient_linear\": [true|false],\n    \"override_module_apply\": [true|false],\n    }\n}\n\"\"\"\n\nZERO_OPTIMIZATION = \"zero_optimization\"\n\n\ndef read_zero_config_deprecated(param_dict):\n    zero_config_dict = {}\n    zero_config_dict[\"stage\"] = 1 if param_dict[ZERO_OPTIMIZATION] else 0\n    if zero_config_dict[\"stage\"] > 0:\n        zero_config_dict[\"allgather_bucket_size\"] = get_scalar_param(param_dict, \"allgather_size\", 5e8)\n    logger.warning(\n        \"DeepSpeedConfig: this format of ZeRO optimization setup is deprecated. Please use the following format: {}\".\n        format(ZERO_FORMAT))\n    return zero_config_dict\n\n\ndef get_zero_config(param_dict):\n    if ZERO_OPTIMIZATION in param_dict:\n        zero_config_dict = param_dict[ZERO_OPTIMIZATION]\n        if isinstance(zero_config_dict, bool):\n            zero_config_dict = read_zero_config_deprecated(param_dict)\n    else:\n        zero_config_dict = {}\n    return DeepSpeedZeroConfig(**zero_config_dict)\n\n\nclass ZeroStageEnum(int, Enum):\n    \"\"\" Enum class for possible zero stages \"\"\"\n    disabled = 0\n    optimizer_states = 1\n    gradients = 2\n    weights = 3\n    max_stage = 3\n\n\nclass DeepSpeedZeroConfig(DeepSpeedConfigModel):\n    \"\"\"\n    Sets parameters for ZeRO optimizations.\n    \"\"\"\n\n    stage: ZeroStageEnum = 0\n    \"\"\"\n    Chooses different stages of ZeRO Optimizer. Stage 0, 1, 2, and 3 refer\n    to disabled, optimizer state partitioning, and optimizer+gradient state\n    partitioning, and optimizer+gradient+parameter partitioning, respectively.\n    \"\"\"\n\n    contiguous_gradients: bool = True\n    \"\"\"\n    Copies the gradients to a contiguous buffer as they are produced. Avoids\n    memory fragmentation during backward pass.\n    \"\"\"\n\n    reduce_scatter: bool = True\n    \"\"\"\n    Uses reduce or reduce scatter instead of allreduce to average gradients\n    \"\"\"\n\n    reduce_bucket_size: int = Field(pp_int(5e8), ge=0)\n    \"\"\"\n    Number of elements reduced/allreduced at a time. Limits the memory required\n    for the allgather for large model sizes\n    \"\"\"\n\n    use_multi_rank_bucket_allreduce: bool = True\n    \"\"\"\n    Combine the reduce buckets of the different ranks and do an All-Reduce instead of multiple Reduce ops.\n    This feature is useful when the model is small and we want to scale it on too many GPUs which therefore\n    reduces the message sizes of each packet.\n    \"\"\"\n\n    allgather_partitions: bool = True\n    \"\"\"\n    Chooses between allgather collective or a series of broadcast collectives\n    to gather updated parameters from all the GPUs at the end of each step\n    \"\"\"\n\n    allgather_bucket_size: int = Field(pp_int(5e8), ge=0)\n    \"\"\"\n    Number of elements allgathered at a time. Limits the memory required for\n    the allgather for large model sizes\n    \"\"\"\n\n    overlap_comm: bool = None  # None for dynamic default value (see validator `overlap_comm_valid` below)\n    \"\"\"\n    Attempts to overlap the reduction of the gradients with backward computation\n    \"\"\"\n\n    load_from_fp32_weights: bool = True\n    \"\"\"\n    Boolean indicating whether to initialize fp32 master weights from fp32\n    copies in checkpoint (no precision loss) or from model's fp16 copies (with\n    precision loss). This can be used to initialize optimizer state even when\n    checkpoint is missing optimizer state.\n    \"\"\"\n\n    elastic_checkpoint: bool = False\n    \"\"\"\n    Enable loading checkpoint that was saved by job with different GPU count.\n    No longer supported.\n    \"\"\"\n\n    offload_param: Optional[DeepSpeedZeroOffloadParamConfig] = None\n    \"\"\"\n    Enable offloading of model parameters to CPU or NVMe. This frees up GPU\n    memory for larger models or batch sizes. Valid only with stage 3. Expects a\n    dictionary containing values for :any:`DeepSpeedZeroOffloadParamConfig`.\n    \"\"\"\n\n    offload_optimizer: Optional[DeepSpeedZeroOffloadOptimizerConfig] = None\n    \"\"\"\n    Enable offloading of optimizer state to CPU or NVMe, and optimizer\n    computation to CPU. This frees up GPU memory for larger models or batch\n    sizes. Valid for ZeRO stage 1, 2, 3. Expects a dictionary containing values\n    for :any:`DeepSpeedZeroOffloadOptimizerConfig`.\n    \"\"\"\n\n    sub_group_size: int = Field(pp_int(1e9), ge=0)\n    \"\"\"\n    Tile size for parameter processing to fit massive models (with trillions of\n    parameters). Used by ZeRO3-Offload and ZeRO-Infinity\n    \"\"\"\n\n    cpu_offload_param: bool = Field(\n        None,\n        deprecated=True,\n        new_param=\"offload_param\",\n        new_param_fn=(lambda val: DeepSpeedZeroOffloadParamConfig(device=OffloadDeviceEnum.cpu) if val else None),\n    )\n    \"\"\" Deprecated, please use ``offload_param`` \"\"\"\n\n    cpu_offload_use_pin_memory: bool = Field(\n        None,\n        deprecated=True,\n        new_param=\"offload_param or offload_optimizer\",\n        set_new_param=False,\n    )\n    \"\"\" Deprecated, please use ``offload_param`` or ``offload_optimizer`` \"\"\"\n\n    cpu_offload: bool = Field(\n        None,\n        deprecated=True,\n        new_param=\"offload_optimizer\",\n        new_param_fn=(lambda val: DeepSpeedZeroOffloadOptimizerConfig(device=OffloadDeviceEnum.cpu) if val else None),\n    )\n    \"\"\" Deprecated, please use ``offload_optimizer`` \"\"\"\n\n    prefetch_bucket_size: int = Field(pp_int(5e7), ge=0, alias=\"stage3_prefetch_bucket_size\")\n    \"\"\"\n    Maximum number of parameter elements to fetch ahead of use. Used by ZeRO3,\n    ZeRO3-Offload, ZeRO-Infinity, and ZeRO-Inference.\n    \"\"\"\n\n    param_persistence_threshold: int = Field(pp_int(1e5), ge=0, alias=\"stage3_param_persistence_threshold\")\n    \"\"\"\n    Do not partition parameters smaller than this threshold. Smaller values use\n    less memory, but can greatly increase communication (especially\n    latency-bound messages).\n    \"\"\"\n\n    model_persistence_threshold: int = Field(pp_int(sys.maxsize, \"sys.maxsize\"),\n                                             ge=0,\n                                             alias=\"stage3_model_persistence_threshold\")\n    \"\"\"\n    Maximum number of parameter elements that can be persisted in GPU and not\n    partitioned. This imposes an upper bound on the number of unpartitioned\n    parameters resulting from param_persistence_threshold setting. Used by\n    ZeRO3-Offload, ZeRO-Infinity and ZeRO-Inference.\n    \"\"\"\n\n    max_live_parameters: int = Field(pp_int(1e9), ge=0, alias=\"stage3_max_live_parameters\")\n    \"\"\"\n    The maximum number of parameters resident per GPU before releasing. Smaller\n    values use less memory, but perform more communication.\n    \"\"\"\n\n    max_reuse_distance: int = Field(pp_int(1e9), ge=0, alias=\"stage3_max_reuse_distance\")\n    \"\"\"\n    Do not release a parameter if it will be reused within this threshold of\n    parameters. Smaller values use less memory, but perform more communication.\n    \"\"\"\n\n    gather_16bit_weights_on_model_save: bool = Field(False, alias=\"stage3_gather_16bit_weights_on_model_save\")\n    \"\"\"\n    Consolidate the weights before saving the model by ``save_16bit_model()``.\n    Since the weights are partitioned across GPUs, they aren\u2019t part of\n    ``state_dict``, so this function automatically gathers the weights when\n    this option is enabled and then saves the fp16 model weights.\n    \"\"\"\n\n    use_all_reduce_for_fetch_params: bool = Field(False, alias=\"stage3_use_all_reduce_for_fetch_params\")\n    \"\"\"\n    Use all_reduce op when fetching module parameters at stage3. This improves performance by reducing\n    the overhead of concatenation and slicing on the host.\n    \"\"\"\n\n    stage3_gather_fp16_weights_on_model_save: bool = Field(False,\n                                                           deprecated=True,\n                                                           new_param=\"gather_16bit_weights_on_model_save\")\n    \"\"\" Deprecated, please use ``gather_16bit_weights_on_model_save`` \"\"\"\n\n    ignore_unused_parameters: bool = True\n    \"\"\"\n    Unused parameters in modules may be unexpected in static networks, but\n    could be normal in dynamic networks. This controls whether or not training\n    should terminate with an error message when unused parameters are detected.\n    This is set to ``True`` by default, which means unused parameters are\n    ignored and training continues. Now is just used in stage 2.\n    \"\"\"\n\n    legacy_stage1: bool = False\n    \"\"\"\n    For backward-compatibility enable old ZeRO stage 1 implementation. Use at\n    your own risk, will be deprecated soon.\n    \"\"\"\n\n    round_robin_gradients: bool = False\n    \"\"\"\n    Stage 1 and 2 optimization for CPU offloading that parallelizes gradient\n    copying to CPU memory among ranks by fine-grained gradient partitioning.\n    Performance benefit grows with gradient accumulation steps (more copying\n    between optimizer steps) or GPU count (increased parallelism).\n    \"\"\"\n    zero_hpz_partition_size: int = Field(1, ge=0)\n    \"\"\"\n    Number of ranks in zero parameters partitioning secondary group\n    \"\"\"\n    zero_quantized_weights: bool = False\n    \"\"\"\n    Boolean indicating whether to quantize zero parameters (weights)\n    for efficient all_gather comm\n    \"\"\"\n    zero_quantized_nontrainable_weights: bool = False\n    \"\"\"\n    Boolean indicating whether to quantize non-trainable zero parameters (weights)\n    for efficient memory usage and communication. Different from zero_quantized_weights\n    that stores the weights in original precision and only perform quantization during communication,\n    this flag will store the weights in quantized precision. This is useful for LoRA training.\n    \"\"\"\n    zero_quantized_gradients: bool = False\n    \"\"\"\n    Boolean indicating whether to use quantized zero gradients\n    for efficient all_2_all_reduce comm\n    \"\"\"\n\n    mics_shard_size: int = Field(-1, new_param=\"mics_shard_size\")\n\n    mics_hierarchical_params_gather: bool = False\n\n    memory_efficient_linear: bool = True\n    \"\"\"\n    Use memory efficient linear implementation, for Stage 3.\n    \"\"\"\n    \"\"\"\n    Whether force load checkpoint in pipeline mode, current only for Stage 3.\n    \"\"\"\n    pipeline_loading_checkpoint: bool = False\n\n    override_module_apply: bool = True\n    \"\"\"\n    Override nn.Module apply function, for Stage 3.\n    \"\"\"\n\n    # Validators\n    @validator(\"overlap_comm\")\n    def overlap_comm_valid(cls, field_value, values):\n        if field_value is None:\n            assert (\"stage\" in values), \"DeepSpeedZeroConfig: 'stage' must be defined before 'overlap_comm'\"\n            field_value = values[\"stage\"] == ZeroStageEnum.weights\n        return field_value\n\n    @root_validator\n    def offload_ratio_check(cls, values):\n        offload_config = getattr(values, \"offload_optimizer\", {})\n        if offload_config and offload_config.ratio < 1.0:\n            assert values.get(\"stage\") == ZeroStageEnum.weights, \"Partial offloading only supported for ZeRO Stage 3.\"\n        return values\n", "deepspeed/runtime/zero/contiguous_memory_allocator.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\n\nfrom deepspeed import comm as dist\n\n\ndef print_rank_0(message):\n    if dist.get_rank() == 0:\n        print(message)\n\n\nclass ContiguousMemoryAllocator(object):\n\n    def __init__(self, size, dtype, device):\n        self.buffer = torch.zeros(size, dtype=dtype, device=device)\n\n        #address to contiguous size available\n        self.contiguous_sizes = {}\n\n        self.contiguous_sizes[0] = size\n\n        #tensor id to its address\n        self.tensor_addresses = {}\n\n        #tensor address to its size\n        self.tensor_sizes = {}\n\n        #tensor address to ids\n        self.tensor_ids = {}\n\n        #id to tensors\n        self.tensor_map = {}\n\n        #id to params. Maps each tensor buffer to list of parameters that uses it\n        self.id_to_params = {}\n\n        self.total_size = size\n        self.total_free = size\n        self.largest_contiguous = size\n        self.max_allocated = 0\n\n        self.count = 0\n\n    #create a tensor of size from the pre-allocated buffer\n    #if not enough free space will fail\n    #if not enough contiguous space, will defragment and allocate\n    def allocate_tensor(self, size):\n        free_before = self.total_free\n\n        assert size <= self.total_free, \"Not enough memory in buffer. Allocation failed\"\n        if self.largest_contiguous < size:\n            print_rank_0(\"Needs defragmentation to allocate. Before Defragmentation:\")\n            self.print_allocation(resolution=100)\n            self._defragment_memory()\n            #set the param data to the new tensor buffer locations\n            self._reset_param_data()\n            print_rank_0(\"After defragmentation:\")\n            self.print_allocation(resolution=100)\n\n        self.total_free = self.total_free - size\n\n        allocated = self.total_size - self.total_free\n        if allocated > self.max_allocated:\n            self.max_allocated = allocated\n\n        tensor_address = self._get_new_tensor_address(size)\n\n        ret_tensor = self._get_new_tensor(tensor_address, size)\n        print_rank_0(\n            f\"Free before allocation {free_before}. Allocating {size}. Free after allocation {self.total_free}. Max allocated {self.max_allocated}\"\n        )\n        assert self.total_free + size == free_before, \"Allocation bookkeeping error\"\n\n        return ret_tensor\n\n    #assigns the tensor data to the param data and keeps track of the assignment\n    #any change the underlying buffer from defragmentation will cause a\n    #reassignment of the param data\n    def assign_to_param(self, tensor, param, numel, shape):\n        tensor_id = id(tensor)\n\n        assert tensor_id in self.tensor_map.keys(), \"No such tensor allocated by the allocator.\"\n        assert tensor.numel() >= numel, \"Assert tensor buffer does is not large enough\"\n        assert not tensor_id in self.id_to_params.keys(), \"This tensor has already been assigned to a param\"\n\n        self.id_to_params[tensor_id] = [param]\n\n        replicated_tensor = tensor.narrow(0, 0, numel).view(shape)\n        param.data = replicated_tensor.data\n        param.contiguous_tensor_id = tensor_id\n\n    #deletes the tensor and frees up the underlying buffer\n    def release_tensor(self, tensor):\n        free_before = self.total_free\n        tensor_id = id(tensor)\n        tensor_size = tensor.numel()\n        self._release_tensor(tensor_id)\n        self._unassign_params(tensor_id)\n        self.total_free += tensor_size\n        print_rank_0(\n            f\"Free before release {free_before}. Released {tensor.numel()}. Total free after {self.total_free}.\")\n        assert self.total_free - tensor_size == free_before, \"Release bookkeeping error\"\n\n    def release_tensor_with_id(self, tensor_id):\n        free_before = self.total_free\n        assert tensor_id in self.tensor_map.keys(), \"Invalid tensor id\"\n        tensor = self.tensor_map[tensor_id]\n        tensor_size = tensor.numel()\n        self._release_tensor(tensor_id)\n        self._unassign_params(tensor_id)\n        self.total_free += tensor_size\n        print_rank_0(\n            f\"Free before release {free_before}. Released {tensor.numel()}. Total free after {self.total_free}.\")\n        assert self.total_free - tensor_size == free_before, \"Release bookkeeping error\"\n\n    #shows the current memory allocation at specified resolution\n    def print_allocation(self, resolution=200):\n        total_size = self.buffer.numel() * 1.0\n        empty = []\n        for addr, size in self.contiguous_sizes.items():\n            start = int(addr * resolution / total_size)\n            end = int((addr + size) * resolution / total_size)\n            empty.extend(range(start, end))\n        s = ''\n        for i in range(resolution):\n            s += '.' if i in empty else '|'\n        print_rank_0(s)\n\n    def max_allocated(self):\n        return self.max_allocated\n\n    #to be called after defragmentation that moves the tensor buffers\n    #this call reassigns the data of all the parameters using the tensor buffers\n    def _reset_param_data(self):\n        for id, tensor in self.tensor_map.items():\n            for param in self.id_to_params[id]:\n                param.data = tensor.narrow(0, 0, param.numel()).view(param.data.shape).data\n\n    def _unassign_params(self, tensor_id):\n        if tensor_id in self.id_to_params.keys():\n            del self.id_to_params[tensor_id]\n\n    def _release_tensor(self, tensor_id):\n        assert tensor_id in self.tensor_addresses, f\"Tensor id {tensor_id} not found\"\n\n        address = self.tensor_addresses[tensor_id]\n        contiguous_size = self.tensor_map[tensor_id].numel()\n\n        del self.tensor_addresses[tensor_id]\n        del self.tensor_ids[address]\n        del self.tensor_map[tensor_id]\n        del self.tensor_sizes[address]\n\n        self._consolidate_address(address, contiguous_size)\n        self.largest_contiguous = self._largest_contiguous()\n\n    def _consolidate_address(self, address, contiguous_size):\n\n        #consolidate next buffer\n        end_address = address + contiguous_size\n        if end_address in self.contiguous_sizes:\n            contiguous_size += self.contiguous_sizes[end_address]\n            del self.contiguous_sizes[end_address]\n\n        #consolidate previous buffer\n        for addr, size in self.contiguous_sizes.items():\n            if addr + size == address:\n                del self.contiguous_sizes[addr]\n                contiguous_size += size\n                address = addr\n                break\n\n        self.contiguous_sizes[address] = contiguous_size\n\n    def _defragment_memory(self):\n        empty_addresses = sorted(self.contiguous_sizes.keys())\n        tensor_addresses = sorted(self.tensor_addresses.values())\n\n        tensor_index = 0\n\n        while tensor_index < len(tensor_addresses):\n\n            empty_addr = empty_addresses[0]\n            empty_size = self.contiguous_sizes[empty_addr]\n\n            tensor_addr = tensor_addresses[tensor_index]\n            tensor_size = self.tensor_sizes[tensor_addr]\n            tensor_id = self.tensor_ids[tensor_addr]\n            tensor = self.tensor_map[self.tensor_ids[tensor_addr]]\n\n            assert tensor_size == tensor.numel(), \\\n                f\"Size mismatch. {tensor_size} is allocated at addr {tensor_addr} but tensor size is {tensor.numel()} \"\n\n            assert empty_addr != tensor_addr, \\\n                f\"Cannot have same empty address {empty_addr} and tensor address {tensor_addr}\"\n\n            if empty_addr < tensor_addr:\n\n                if empty_size >= tensor_size:\n                    dest_buffer = self.buffer.narrow(0, empty_addr, tensor_size)\n                    src_buffer = self.buffer.narrow(0, tensor_addr, tensor_size)\n                    dest_buffer.data.copy_(src_buffer.data)\n                else:\n\n                    #print_rank_0(f'empty addr : {empty_addr}, empty size {empty_size} tensor addr {tensor_addr} tensor size {tensor_size}')\n                    src_addr = tensor_addr\n                    dest_addr = empty_addr\n                    while src_addr < (tensor_addr + tensor_size):\n                        copy_size = min(empty_size, tensor_addr + tensor_size - src_addr)\n\n                        dest_buffer = self.buffer.narrow(0, dest_addr, copy_size)\n                        src_buffer = self.buffer.narrow(0, src_addr, copy_size)\n\n                        dest_buffer.data.copy_(src_buffer.data)\n\n                        src_addr += copy_size\n                        dest_addr += copy_size\n\n                self._replace_old_address_with_new(tensor_id, empty_addr)\n\n                tensor_index += 1\n\n            else:\n                tensor_index += 1\n\n            empty_addresses = sorted(self.contiguous_sizes.keys())\n\n    def _replace_old_address_with_new(self, tensor_id, new_address):\n\n        tensor = self.tensor_map[tensor_id]\n        tensor_size = tensor.numel()\n        tensor.data = self.buffer.narrow(0, new_address, tensor_size).data\n\n        self._release_tensor(tensor_id)\n        self._mark_as_occupied(new_address, tensor_size)\n\n        self.tensor_ids[new_address] = tensor_id\n        self.tensor_map[tensor_id] = tensor\n        self.tensor_addresses[tensor_id] = new_address\n        self.tensor_sizes[new_address] = tensor_size\n\n    def _get_new_tensor_address(self, size):\n        tensor_address = None\n        for address, contiguous_size in self.contiguous_sizes.items():\n            if contiguous_size >= size and \\\n                    (tensor_address is None or \\\n                    contiguous_size < self.contiguous_sizes[tensor_address]):\n                tensor_address = address\n        assert tensor_address is not None, \"address cannot be None\"\n        return tensor_address\n\n    def _get_new_tensor(self, address, size):\n        available_contiguous_size = self.contiguous_sizes[address]\n\n        assert size <= available_contiguous_size, \\\n            f\"Tensor numel {size} is large than available contiguous size {available_contiguous_size}\"\n        self.count += 1\n        new_tensor = self.buffer.narrow(0, address, size)\n        tensor_id = id(new_tensor)\n        self.tensor_addresses[tensor_id] = address\n        self.tensor_sizes[address] = size\n\n        self.tensor_ids[address] = tensor_id\n        self.tensor_map[tensor_id] = new_tensor\n\n        self._mark_as_occupied(address, size)\n\n        return new_tensor\n\n    def _largest_contiguous(self):\n        if len(self.contiguous_sizes) > 0:\n            return max([size for _, size in self.contiguous_sizes.items()])\n        else:\n            return 0\n\n    def _mark_as_occupied(self, address, size):\n        available_contiguous_size = self.contiguous_sizes[address]\n        del self.contiguous_sizes[address]\n\n        if available_contiguous_size != size:\n            self.contiguous_sizes[address + size] = available_contiguous_size - size\n\n        self.largest_contiguous = self._largest_contiguous()\n", "deepspeed/runtime/zero/mics_utils.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\n# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n# SPDX-License-Identifier: Apache-2.0\n\nimport os\nfrom dataclasses import dataclass\nfrom typing import List\n\nimport numpy as np\nimport torch\nfrom torch import Tensor\n\nfrom deepspeed import comm as dist\nfrom deepspeed.accelerator import get_accelerator\nfrom deepspeed.utils import logger\n\n\ndef _log_rank0(msg):\n    if dist.get_rank() == 0:\n        logger.info(msg)\n\n\n@torch.jit.script\ndef scale_tensors(tensors: List[Tensor], scale: int):\n    for t in tensors:\n        t.div_(scale)\n\n\n@dataclass\nclass MiCS_CommGroups:\n    \"\"\"\"\"\"\n    param_shard_group = None\n    param_shard_size = -1\n    param_shard_rank = -1\n\n    param_repli_group = None\n    param_repli_size = -1\n    param_repli_rank = -1\n\n    param_intra_node_group = None\n    param_inter_node_shard_group = None\n\n\ndef create_mics_comm_groups(\n    shard_size,\n    dp_group,\n    hierarchical_allgather=False,\n    mpu=None,\n):\n    \"\"\"\n    create shard-group, replicate-group from config_file\n    TODO: consider broadcast the config from rank0\n\n    Returns:\n        MiCS_CommGroups\n    \"\"\"\n    # env var for debugging purpose\n    ndevices_per_node = int(os.environ.get(\"NDEV_PER_NODE\", get_accelerator().device_count()))\n    _log_rank0(f'creating MiCS communication groups with per node device size {ndevices_per_node}')\n    groups = MiCS_CommGroups()\n\n    if mpu is not None:\n        assert dp_group == mpu.get_data_parallel_group()\n\n    # full size of the world\n    world_size = dist.get_world_size()\n    # global rank\n    global_rank = dist.get_rank()\n\n    config = _generate_mics_config(world_size, ndevices_per_node, shard_size, 1)\n    ranks_of_shard_group = config['shard_groups']\n    ranks_of_repli_group = config['replicate_groups']\n    if len(ranks_of_repli_group) == 0:\n        assert len(ranks_of_shard_group) == 1, \"replicate groups are empty only for single shard group\"\n        for r in ranks_of_shard_group[0]:\n            ranks_of_repli_group.append([r])\n\n    # for simplicity\n    assert _sizes_all_same(ranks_of_repli_group), \"replicate groups must have the same size\"\n    assert _sizes_all_same(ranks_of_shard_group), \"shard groups must have the same size\"\n\n    assert sum([len(g) for g in ranks_of_shard_group]) == dist.get_world_size(), \"all sharded ranks \"\n    if len(ranks_of_shard_group) > 1:  # if only shard on one group then no need for replicate groups\n        assert len(ranks_of_shard_group) == len(\n            ranks_of_repli_group[0]), \"number of shard groups must equal to the size of each replicate group\"\n\n    global_rank = dist.get_rank()\n    # create shard groups\n    for shard_ranks in ranks_of_shard_group:\n        _group = dist.new_group(shard_ranks)\n        if global_rank in shard_ranks:\n            groups.param_shard_group = _group\n            groups.param_shard_size = len(shard_ranks)\n            groups.param_shard_rank = dist.get_rank(_group)\n            logger.info(f'rank {global_rank}, shard group'\n                        f' {groups.param_shard_rank}/{dist.get_world_size(group=_group)}')\n\n    # create replicate groups\n    for repli_ranks in ranks_of_repli_group:\n        if len(repli_ranks) > 1:\n            _group = dist.new_group(repli_ranks)\n            if global_rank in repli_ranks:\n                groups.param_repli_group = _group\n                groups.param_repli_size = len(repli_ranks)\n                groups.param_repli_rank = dist.get_rank(group=_group)\n                logger.info(f'rank {global_rank} '\n                            f'replicate group {groups.param_repli_rank}/{dist.get_world_size(group=_group)}')\n        else:\n            groups.param_repli_group = None\n            groups.param_repli_size = 1\n            groups.param_repli_rank = 0\n            logger.info(f'rank {global_rank} replicate group 0/1')\n\n    # assign shard group size as world size\n    assert groups.param_shard_size == len(ranks_of_shard_group[0])\n\n    if hierarchical_allgather:\n        # create hierarchy inter-node, intra-node groups\n        # n_span_nodes = config['shard_span']\n        n_span_nodes = config['span_nodes']\n        assert n_span_nodes > 1, \"sharding spans on single node, no need for hierarchy allgather\"\n        assert len(ranks_of_shard_group[0]) % n_span_nodes == 0\n\n        n_gpu_per_node = len(ranks_of_shard_group[0]) // n_span_nodes\n        intra_node_ranks_group = []\n        inter_node_ranks_group = []\n        for shard_group in ranks_of_shard_group:\n            _intra_node_ranks = []\n            for i in range(0, len(shard_group), n_gpu_per_node):\n                _intra_node_ranks.append(shard_group[i:i + n_gpu_per_node])\n            _inter_node_ranks = []\n            for i in range(n_gpu_per_node):\n                _ranks = [_g[i] for _g in _intra_node_ranks]\n                _inter_node_ranks.append(_ranks)\n\n            intra_node_ranks_group.append(_intra_node_ranks)\n            inter_node_ranks_group.append(_inter_node_ranks)\n\n        _log_rank0(f\"create for hierarchy all-gather groups: intra nodes {intra_node_ranks_group}\")\n        _log_rank0(f\"create for hierarchy all-gather groups: inter nodes {inter_node_ranks_group}\")\n\n        # create communicators\n        for shard_group in intra_node_ranks_group:\n            for intra_node_ranks in shard_group:\n                _group = dist.new_group(intra_node_ranks)\n                if global_rank in intra_node_ranks:\n                    groups.param_intra_node_group = _group\n                _log_rank0(f'create group for intra node ranks {intra_node_ranks}')\n\n        for shard_group in inter_node_ranks_group:\n            for inter_node_ranks in shard_group:\n                _group = dist.new_group(inter_node_ranks)\n                if global_rank in inter_node_ranks:\n                    groups.param_inter_node_shard_group = _group\n                _log_rank0(f'create group for inter node ranks {inter_node_ranks}')\n    return groups\n\n\ndef _generate_mics_config(world_size, ndev_per_node, shard_size, pp_size=1):\n    \"\"\"Generating the configuration for sharding This shard config generation assume\n    that the pipeline stages are partitioned in order, i.e., first ranks\n    hold the stage0, etc.\n\n    Args:\n\n        shard_size (int): zero3 data-parallel shard size, FIXME:\n        change the name later\n\n        pp_size (int): pipeline parallel size, currently, only work with\n        pipeline parallelism + zero\n\n    \"\"\"\n    assert world_size % pp_size == 0\n    assert (world_size // pp_size) % shard_size == 0, \\\n        f\"dp group size is not dividable by dp_shard_size, \"\\\n        f\" (world_size {world_size}, pp_size {pp_size}, dp_shard_size {shard_size})\"\n\n    config = {}\n    shard_groups = np.arange(world_size).reshape(-1, shard_size)\n    replicate_groups = []\n    for i in range(shard_size):\n        same_shard_ranks = shard_groups[:, i].tolist()\n        n_ranks = len(same_shard_ranks)\n        replicate_size = n_ranks // pp_size\n        replicate_groups.extend([same_shard_ranks[j:j + replicate_size] for j in range(0, n_ranks, replicate_size)])\n\n    config['replicate_groups'] = replicate_groups\n    config['shard_groups'] = shard_groups.tolist()\n    config[\"span_nodes\"] = len(shard_groups[0]) // ndev_per_node\n    return config\n\n\ndef _sizes_all_same(groups):\n    \"\"\"all groups have same length\"\"\"\n    all_same = True\n    for g in groups:\n        if len(g) != len(groups[0]):\n            return False\n    return all_same\n", "deepspeed/runtime/zero/linear.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\n#Linear Module to use with ZeRO Stage 3 to allow for parameter memory release\n#after the module execution during forward\n#Instead of saving variables using save_for_backward, we save variable ids\n#Allowing us to retrieve the variable without creating pointer to it\n#Which allows for underlying tensor to be garbage collected\n#When partitioned as needed by the Zero Stage 3 optimizer\n#TODO instead of patching Linear module, we could patch the ctx.save_for_backward\n#ctx.saved_tensors so that this approach works for all nn modules that are built upon\n#torch.nn.function. However the issue is that many modules uses C++ implementations\n#which does not have pytorch implementation. Eg torch.addmm which acts as a functional\n#when implemented outside of torch.autograd.Function\n\nimport math\n\nimport torch\nfrom torch import Tensor\nfrom torch.nn.parameter import Parameter\nfrom torch.nn import init\nfrom torch.nn.modules.module import Module\nfrom deepspeed.runtime.utils import noop_decorator\nfrom deepspeed import comm as dist\nfrom deepspeed.accelerator import get_accelerator\n\n\ndef print_rank_0(message, debug=False, force=False):\n    if dist.get_rank() == 0 and (debug or force):\n        print(message)\n\n\ntry:\n    autocast_custom_fwd = get_accelerator().amp().custom_fwd\n    autocast_custom_bwd = get_accelerator().amp().custom_bwd\nexcept (ImportError, AttributeError) as exp:\n    autocast_custom_fwd = noop_decorator\n    autocast_custom_bwd = noop_decorator\n\n\nclass LinearFunctionForZeroStage3(torch.autograd.Function):\n\n    # Note that both forward and backward are @staticmethods\n    @staticmethod\n    @autocast_custom_fwd\n    # bias is an optional argument\n    def forward(ctx, input, weight, bias=None):\n\n        ctx.save_for_backward(input, weight, bias)\n\n        if input.dim() == 2 and bias is not None:\n            # fused op is marginally faster\n            ret = torch.addmm(bias, input, weight.t())\n        else:\n            output = input.matmul(weight.t())\n            if bias is not None:\n                output += bias\n            ret = output\n\n        return ret\n\n    # This function has only a single output, so it gets only one gradient\n    @staticmethod\n    @autocast_custom_bwd\n    def backward(ctx, grad_output):\n        # This is a pattern that is very convenient - at the top of backward\n        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n        # None. Thanks to the fact that additional trailing Nones are\n        # ignored, the return statement is simple even when the function has\n        # optional inputs.\n        input, weight, bias = ctx.saved_tensors\n\n        grad_input = grad_weight = grad_bias = None\n\n        #print(f\"backward shaped grad_output {grad_output.shape}, input {input.shape}, weight {weight.shape} and bias {bias.shape if bias is not None else None}\")\n        # These needs_input_grad checks are optional and there only to\n        # improve efficiency. If you want to make your code simpler, you can\n        # skip them. Returning gradients for inputs that don't require it is\n        # not an error.\n        if ctx.needs_input_grad[0]:\n            #print(f\"Computing grad input weight {weight.shape} grad_output {grad_output.shape}\")\n            grad_input = grad_output.matmul(weight)\n            #print(f\"Computed grad input {grad_input.shape}\")\n        if ctx.needs_input_grad[1]:\n            #print(\"Computing grad weight\")\n            dim = grad_output.dim()\n            if dim > 2:\n                grad_weight = grad_output.reshape(-1,\n                                                  grad_output.shape[-1]).t().matmul(input.reshape(-1, input.shape[-1]))\n            else:\n                grad_weight = grad_output.t().matmul(input)\n            #print(f\"Computed grad weight grad_weight {grad_weight.shape}\")\n        if bias is not None and ctx.needs_input_grad[2]:\n            #print(\"Computing grad bias\")\n            if dim > 2:\n                grad_bias = grad_output.sum([i for i in range(dim - 1)])\n            else:\n                grad_bias = grad_output.sum(0)\n            #print(\"Done computing grad bias\")\n            #print(\"needs bias\")\n        #print(f\"backward shaped grad_input {grad_input.shape}, grad_weight {grad_weight.shape}, grad_bias {grad_bias.shape if grad_bias is not None else None}\")\n        return grad_input, grad_weight, grad_bias\n\n\ndef zero3_linear_wrap(input, weight, bias=None):\n    if bias is None:\n        return LinearFunctionForZeroStage3.apply(input, weight)\n    else:\n        return LinearFunctionForZeroStage3.apply(input, weight, bias)\n\n\nclass LinearModuleForZeroStage3(Module):\n    r\"\"\"Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.\n    The weights are pre-transposed and stored as A^T instead of transposing during each\n    forward. Memory savings proportional to the parameter size.\n\n    Args:\n        in_features: size of each input sample\n        out_features: size of each output sample\n        bias: If set to ``False``, the layer will not learn an additive bias.\n            Default: ``True``\n\n    Shape:\n        - Input: :math:`(N, *, H_{in})` where :math:`*` means any number of\n          additional dimensions and :math:`H_{in} = \\text{in\\_features}`\n        - Output: :math:`(N, *, H_{out})` where all but the last dimension\n          are the same shape as the input and :math:`H_{out} = \\text{out\\_features}`.\n\n    Attributes:\n        weight: the learnable weights of the module of shape\n            :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n            initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n            :math:`k = \\frac{1}{\\text{in\\_features}}`\n        bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n                If :attr:`bias` is ``True``, the values are initialized from\n                :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n                :math:`k = \\frac{1}{\\text{in\\_features}}`\n\n    Examples::\n\n        >>> m = nn.Linear(20, 30)\n        >>> input = torch.randn(128, 20)\n        >>> output = m(input)\n        >>> print(output.size())\n        torch.Size([128, 30])\n    \"\"\"\n    __constants__ = ['in_features', 'out_features']\n    in_features: int\n    out_features: int\n    weight: Tensor\n\n    def __init__(self, in_features: int, out_features: int, bias: bool = True) -> None:\n        super(LinearModuleForZeroStage3, self).__init__()\n        print(\"Building ZeRO module\")\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = Parameter(torch.Tensor(out_features, in_features))\n        if bias:\n            self.bias = Parameter(torch.Tensor(out_features))\n        else:\n            self.register_parameter('bias', None)\n        self.reset_parameters()\n\n    def reset_parameters(self) -> None:\n        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        if self.bias is not None:\n            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / math.sqrt(fan_in)\n            init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, input: Tensor) -> Tensor:\n        return LinearFunctionForZeroStage3.apply(input, self.weight, self.bias)\n\n    def extra_repr(self) -> str:\n        return 'in_features={}, out_features={}, bias={}'.format(self.in_features, self.out_features, self.bias\n                                                                 is not None)\n", "deepspeed/runtime/zero/utils.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport os\nfrom typing import List\n\nimport torch\nfrom deepspeed import comm as dist\nfrom deepspeed.utils import logger\nfrom deepspeed.ops.adam import DeepSpeedCPUAdam\nfrom deepspeed.ops.adagrad import DeepSpeedCPUAdagrad\nfrom deepspeed.ops.adam import FusedAdam\nfrom deepspeed.ops.lion import DeepSpeedCPULion, FusedLion\nfrom deepspeed.utils.nvtx import instrument_w_nvtx\nfrom deepspeed.accelerator import get_accelerator\n\n\ndef _initialize_parameter_parallel_groups(parameter_parallel_size=None):\n    data_parallel_size = int(dist.get_world_size())\n    parameter_parallel_size = parameter_parallel_size or data_parallel_size\n    logger.info(\"data_parallel_size: %s, parameter_parallel_size: %s\", data_parallel_size, parameter_parallel_size)\n    assert data_parallel_size % parameter_parallel_size == 0, \\\n        'world size should be divisible by parameter parallel size'\n    rank = dist.get_rank()\n    my_group = None\n    for i in range(data_parallel_size // parameter_parallel_size):\n        ranks = range(i * parameter_parallel_size, (i + 1) * parameter_parallel_size)\n        group = dist.new_group(ranks)\n        if rank in ranks:\n            my_group = group\n    return my_group\n\n\nclass ZeRORuntimeException(Exception):\n    pass\n\n\nZERO_SUPPORTED_OPTIMIZERS = [\n    torch.optim.Adam, torch.optim.AdamW, FusedAdam, DeepSpeedCPUAdam, torch.optim.Adagrad, DeepSpeedCPUAdagrad,\n    DeepSpeedCPULion, FusedLion\n]\n\n# Add apex FusedAdam to supported list if apex is installed\ntry:\n    import apex\n    if hasattr(apex, 'optimizers') and hasattr(apex.optimizers, 'FusedAdam'):\n        ZERO_SUPPORTED_OPTIMIZERS.append(apex.optimizers.FusedAdam)\nexcept ImportError:\n    pass\n\n\ndef is_zero_supported_optimizer(optimizer):\n    if dist.get_rank() == 0:\n        logger.info(f'Checking ZeRO support for optimizer={optimizer.__class__.__name__} type={type(optimizer)}')\n    return type(optimizer) in ZERO_SUPPORTED_OPTIMIZERS\n\n\ndef get_lst_from_rank0(lst: List[int]) -> None:\n    \"\"\"\n    NOTE: creates both communication and synchronization overhead so should be used\n    sparingly\n    \"\"\"\n    lst_tensor = torch.tensor(\n        lst if dist.get_rank() == 0 else [-1] * len(lst),\n        dtype=int,\n        # device=get_accelerator().current_device_name(),\n        device=torch.device(get_accelerator().device_name(os.environ[\"LOCAL_RANK\"])),\n        requires_grad=False,\n    )\n    dist.broadcast(lst_tensor, src=0, async_op=False)\n\n    return list(lst_tensor.cpu().numpy())\n\n\n@instrument_w_nvtx\ndef assert_ints_same_as_other_ranks(ints: List[int]) -> None:\n    \"\"\"\n    NOTE: creates both communication and synchronization overhead so should be\n    used sparingly\n\n    takes a list of ints from each rank and ensures that they are the same\n    across ranks, throwing an exception if they are not.\n    \"\"\"\n    rank0_ints = get_lst_from_rank0(ints)\n    if ints != rank0_ints:\n        raise RuntimeError(f\"disagreement between rank0 and rank{dist.get_rank()}: \"\n                           f\"rank0: {rank0_ints}, rank{dist.get_rank()}: {ints}\")\n\n\ndef is_builtin_type(obj):\n    # https://stackoverflow.com/a/17795199\n    return obj.__class__.__module__ == '__builtin__' or obj.__class__.__module__ == \"builtins\"\n\n\ndef isinstance_namedtuple(obj: object) -> bool:\n    \"\"\"\n    Is this an instance of namedtuple/NamedTuple?\n    From: https://stackoverflow.com/a/62692640\n\n    Args:\n        obj (object): An object.\n\n    Returns:\n        bool: True if namedtuple/NamedTuple else False.\n    \"\"\"\n    return isinstance(obj, tuple) and hasattr(obj, '_asdict') and hasattr(obj, '_fields')\n\n\ndef is_zero_param(parameter):\n    if not torch.is_tensor(parameter):\n        return False\n    return hasattr(parameter, 'ds_id')\n\n\ndef apply_to_tensors_only(function, value, warning_msg_fn=None):\n    \"\"\"\n    Apply `function` to every Tensor in `value`.\n\n    Args:\n        functional: The function class to apply.\n        value (Any): Target object to apply `function` to.\n\n    Returns:\n        Any: Output of `function`.\n    \"\"\"\n    if isinstance(value, (tuple, list)):\n        touched_outputs = []\n        for elem in value:\n            touched_output = apply_to_tensors_only(function, elem)\n            touched_outputs.append(touched_output)\n\n        if isinstance_namedtuple(value):\n            # namedtuples require a slightly different syntax.\n            return value.__class__(*touched_outputs)\n\n        return value.__class__(touched_outputs)\n    elif isinstance(value, dict):\n        # apply inplace to avoid recreating dict inherited objects\n        for key in value.keys():\n            value[key] = apply_to_tensors_only(function, value[key])\n        return value\n\n    elif isinstance(value, torch.Tensor):\n        # this also applies to torch.Tensor's subclasses like torch.nn.parameter.Parameter\n        touched_output = function(value)\n\n        # restore zero param attributes if those get stripped by `backward_function`\n        if not is_zero_param(touched_output) and is_zero_param(value):\n            touched_output.ds_param_alias = value\n\n        return touched_output\n    else:\n        if not is_builtin_type(value):\n            global warned\n            if warning_msg_fn and not warned and dist.get_rank() == 0:\n                logger.warning(warning_msg_fn(value))\n                warned = True\n        return value\n", "deepspeed/runtime/zero/mics.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\n# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n# SPDX-License-Identifier: Apache-2.0\n\nimport sys\nfrom typing import List\n\nimport deepspeed\nimport torch\nfrom deepspeed import comm as dist\nfrom deepspeed.runtime.zero.utils import is_zero_param\nfrom deepspeed.runtime.zero.mics_utils import (MiCS_CommGroups, create_mics_comm_groups, scale_tensors)\nfrom deepspeed.runtime.zero.parameter_offload import DeepSpeedZeRoOffload\nfrom deepspeed.runtime.zero.partition_parameters import Init, AllGatherCoalescedHandle, ZeroParamStatus\nfrom deepspeed.runtime.zero.stage3 import DeepSpeedZeroOptimizer_Stage3\nfrom deepspeed.utils import instrument_w_nvtx, log_dist\nfrom deepspeed.accelerator import get_accelerator\nfrom torch import Tensor\nfrom torch.nn import Parameter\n\n\ndef has_hierarchical_all_gather_groups(comm_groups: MiCS_CommGroups):\n    result = False\n    if comm_groups.param_intra_node_group is not None and comm_groups.param_inter_node_shard_group is not None:\n        result = True\n    return result\n\n\nclass MiCS_AllGatherCoalescedHandle(AllGatherCoalescedHandle):\n    \"\"\" This handle assumes that no need to\n    copy data out from a contiguous tensor\n    \"\"\"\n\n    def __init__(self, allgather_handle, params: List[Parameter], partitions: List[Tensor], world_size: int) -> None:\n        super().__init__(allgather_handle, params, partitions, world_size)\n\n    def wait(self) -> None:\n        \"\"\"\n        \"\"\"\n        # let the current stream to op\n        try:\n            # print(\"HANDLE\", self.allgather_handle)\n            instrument_w_nvtx(self.allgather_handle.wait)()\n        except (ValueError, RuntimeError) as e:\n            log_dist(\n                f\"WARNING: Runtime Error while waiting the collective all-gather, possibly due to the _IllegalWork\",\n                ranks=[0])\n            log_dist(f\"Error message: {e}\", ranks=[0])\n\n        if self.complete:\n            return\n\n        for _, param in enumerate(self.params):\n            assert param.ds_status == ZeroParamStatus.INFLIGHT, f\"expected param {param.ds_summary()} to be inflight\"\n            param.ds_status = ZeroParamStatus.AVAILABLE\n\n        self.complete = True\n\n\nclass MiCS_Init(Init):\n\n    def __init__(self,\n                 module=None,\n                 data_parallel_group=None,\n                 sequence_data_parallel_group=None,\n                 mem_efficient_linear=True,\n                 remote_device=None,\n                 pin_memory=False,\n                 config_dict_or_path=None,\n                 config=None,\n                 enabled=True,\n                 dtype=None,\n                 mpu=None):\n        \"\"\"A context manager to partition the model parameters during the model\n        construction with MiCS partition strategy. Model states are partitioned\n        to the number of devices specified via ``mics_shard_size`` field in the\n        deepspeed config json file. The context manager also introduces\n        hierarchical communication method to reduce the cost of inter-node\n        communications, which can be enabled with\n        ``mics_hierarchical_params_gather`` field in deepspeed config.\n\n        Args:\n            module (``torch.nn.Module``, optional): If provided, partition the model as\n                if it was constructed in the context.\n            data_parallel_group (``deepspeed.comm`` process group, optional):\n                The group of processes to partition among. Defaults to all processes.\n            mem_efficient_linear (bool, optional): Replace\n                torch.nn.functional.linear with an implementation that allows\n                DeepSpeed to partition parameters. Defaults to ``True``.\n            remote_device (string, optional): The initial device to store model\n                weights e.g., ``cpu``, ``nvme``. Passing ``\"cpu\"`` will create the model in CPU\n                memory. The model may still be moved to GPU based on the\n                offload settings for training. Defaults to param offload device if a config is\n                defined, otherwise GPU.\n            pin_memory (bool, optional): Potentially increase performance by\n                using pinned memory for model weights. ``remote_device`` must be\n                ``\"cpu\"``. Defaults to pin_memory value in config, otherwise ``False``.\n            config_dict_or_path (dict or ``json file``, optional): If provided, provides configuration\n                for swapping fp16 params to NVMe.\n            config (dict or ``json file``, optional): Deprecated, use config_dict_or_path instead.\n            enabled (bool, optional): If ``False``, this context has no\n                effect. Defaults to ``True``.\n            dtype (``dtype``, optional): Can be used to change the data type of the parameters.\n                Supported options are ``torch.half`` and ``torch.float``. Defaults to ``None``\n            mpu (``object``, optional): A model parallelism unit object that implements get_{model,data}_parallel_{rank,group,world_size}.\n\n        This context follows the same logic as ``deepspeed.zero.Init()``, but\n        with the modification for partition size of each parameter.\n\n        Examples\n        --------\n\n        #. Allocate a model and partition it among all processes:\n\n            .. code-block:: python\n                # the config_dict_or_path is required to let the context manager know\n                # how partition the parameters.\n                # The configuration has to include the field ``mics_shard_size``\n                with deepspeed.zero.MiCS_Init(config_dict_or_path=ds_config):\n                    model = MyLargeModel()\n\n\n        #. Allocate a model in pinned CPU memory and partition it among a subgroup of processes:\n\n            .. code-block:: python\n\n                with deepspeed.zero.MiCS_Init(data_parallel_group=mpu.get_data_parallel_group(),\n                                              remote_device=\"cpu\",\n                                              pin_memory=True\n                                              config_dict_or_path=ds_config):\n                    model = MyLargeModel()\n\n\n        #. Partition an already-allocated model in CPU memory:\n\n            .. code-block:: python\n\n                model = deepspeed.zero.MiCS_Init(module=model,\n                                                 config_dict_or_path=ds_config)\n        \"\"\"\n\n        assert config_dict_or_path is not None, \"Must provide configuration for MiCS Initialization\"\n        _ds_config = deepspeed.runtime.config.DeepSpeedConfig(config_dict_or_path, mpu)\n        if not dist.is_initialized():\n            dist.init_distributed()\n            assert dist.is_initialized(), \"Parameters cannot be scattered without initializing deepspeed.comm\"\n\n        if data_parallel_group is None and sequence_data_parallel_group is None:\n            ds_process_group = dist.get_world_group()\n        elif sequence_data_parallel_group is not None:\n            ds_process_group = sequence_data_parallel_group\n        elif data_parallel_group is not None:\n            ds_process_group = data_parallel_group\n        else:  # both given\n            raise ValueError(\n                \"Both 'data_parallel_group' and 'sequence_data_parallel_group' were specified. Please provide only one of these arguments.\"\n            )\n\n        self.mics_comm_groups = create_mics_comm_groups(\n            _ds_config.mics_shard_size,\n            ds_process_group,\n            hierarchical_allgather=_ds_config.mics_hierarchial_params_gather,\n            mpu=mpu)\n\n        super().__init__(module, data_parallel_group, mem_efficient_linear, remote_device, pin_memory,\n                         config_dict_or_path, config, enabled, dtype, mpu)\n\n    def _convert_to_deepspeed_param(self, param):\n        super()._convert_to_deepspeed_param(param)\n        # attach communication groups to every param\n        param.comm = self.mics_comm_groups\n\n        # record existing all_gather_coalesced implementation\n        # so that we can fallback later\n        old_all_gather_coalesced = param.all_gather_coalesced\n\n        def _param_all_gather_coalesced(params, param_buffers=None, **kwargs):\n            \"\"\"\"\"\"\n            mics_comm_groups: MiCS_CommGroups = params[0].comm\n            hierarchical_all_gather = has_hierarchical_all_gather_groups(mics_comm_groups)\n            if dist.has_coalescing_manager() and hierarchical_all_gather:\n                return self._hierarchical_all_gather_params(params, param_buffers)\n            elif dist.has_coalescing_manager():\n                return self._flat_all_gather_with_coalescing_manager(params, param_buffers)\n            else:\n                return old_all_gather_coalesced(params, **kwargs)\n\n        # change the all_gather_coalesced method\n        param.all_gather_coalesced = _param_all_gather_coalesced\n\n    def _pre_all_gather(self, params, params_buffers=None):\n        # fetches from nvme if the partition is not available and in nvme\n        self._ensure_availability_of_partitioned_params(params)\n\n        for param in params:\n            if param.ds_status != ZeroParamStatus.NOT_AVAILABLE:\n                raise RuntimeError(param.ds_summary())\n            param.ds_status = ZeroParamStatus.INFLIGHT\n\n        # ensure that each rank has params in same order. the allgather\n        # is done by flattening the parameter list into a single tensor that\n        # can be allgathered in a single call - this means that if each rank\n        # gives a list of the same parameters in a different order we will\n        # silently get incorrect parameter values, and have very difficult\n        # to debug correctness issues.\n        params = sorted(params, key=lambda p: p.ds_id)\n        return params, params_buffers\n\n    def _flat_all_gather_with_coalescing_manager(self, params, params_buffers=None):\n        \"\"\"\"\"\"\n        # must have to change the status of the param\n        # and ensure they are on the device\n        params, params_buffers = self._pre_all_gather(params, params_buffers)\n\n        mics_comm_groups: MiCS_CommGroups = params[0].comm\n        param_shard_size = mics_comm_groups.param_shard_size\n\n        output_tensors = []\n        input_tensors = []\n        for i, p in enumerate(params):\n            t_size = p.ds_tensor.ds_numel * param_shard_size\n            if params_buffers is not None and params_buffers[i] is not None:\n                assert params_buffers[i].numel(\n                ) == t_size, f'params_to_gather_buffers[{i}] size {params_buffers[i].numel()} does not match with t_size {t_size}'\n                flat_out = params_buffers[i]\n            else:\n                flat_out = torch.empty(t_size, dtype=p.dtype, device=self.local_device, requires_grad=False).view(-1)\n            output_tensors.append(flat_out)\n            _flat_input = p.ds_tensor.data.view(-1)\n            input_tensors.append(_flat_input)\n\n        all_gather_handle = dist.all_gather_coalesced(output_tensors,\n                                                      input_tensors,\n                                                      group=mics_comm_groups.param_shard_group,\n                                                      async_op=True)\n\n        for idx, param in enumerate(params):\n            param.data = output_tensors[idx].narrow(0, 0, param.ds_numel).view(param.ds_shape).data\n\n        return MiCS_AllGatherCoalescedHandle(allgather_handle=all_gather_handle,\n                                             params=params,\n                                             partitions=[],\n                                             world_size=param_shard_size)\n\n    def _hierarchical_all_gather_params(self, params, params_buffers=None):\n        \"\"\"\"\"\"\n        params, params_buffers = self._pre_all_gather(params, params_buffers)\n\n        mics_comm_groups: MiCS_CommGroups = params[0].comm\n        local_rank = dist.get_rank(group=mics_comm_groups.param_intra_node_group)\n        inter_node_comm_group = mics_comm_groups.param_inter_node_shard_group\n        intra_node_comm_group = mics_comm_groups.param_intra_node_group\n        param_shard_size = mics_comm_groups.param_shard_size\n\n        inter_node_size = dist.get_world_size(group=inter_node_comm_group)\n        intra_node_size = dist.get_world_size(group=intra_node_comm_group)\n        param_tensors = []\n        for i, p in enumerate(params):\n            param_size = p.ds_tensor.ds_numel * param_shard_size\n            if params_buffers is not None and params_buffers[i] is not None:\n                assert params_buffers[i].numel(\n                ) == param_size, f'param_buffers[{i}] size {params_buffers[i].numel()} does not match with param_size {param_size}'\n                param_tensor = params_buffers[i]\n            else:\n                param_tensor = torch.empty(param_size, dtype=p.dtype, device=self.local_device,\n                                           requires_grad=False).view(-1)\n            param_tensors.append(param_tensor)\n\n        # inter node all-gather\n        inter_outputs = []\n        inter_inputs = []\n        for i, p in enumerate(params):\n            inter_size = p.ds_tensor.ds_numel * inter_node_size\n            _out = param_tensors[i].narrow(0, local_rank * inter_size, inter_size)\n            inter_outputs.append(_out)\n            inter_inputs.append(p.ds_tensor.data.view(-1).to(self.local_device))\n        # sync enqueue\n        dist.all_gather_coalesced(inter_outputs, inter_inputs, group=inter_node_comm_group, async_op=False)\n\n        # intra node all-gather\n        intra_outputs = []\n        intra_inputs = []\n        for i, p in enumerate(params):\n            # partition param into multiple chunks for allgather\n            # because inter-node all-gather outputs are in a continues memory\n            # while in param memory, those inter-node data are placed in different\n            # location.\n            # each chunk is an intra-node output\n            param_chunk = param_tensors[i].view(\n                (inter_node_size, intra_node_size, p.ds_tensor.ds_numel)).narrow(1, local_rank, 1)\n            param_chunk.copy_(inter_outputs[i].detach().clone().view(param_chunk.size()))\n            output_chunks = torch.chunk(param_tensors[i], inter_node_size)\n            for j, _out in enumerate(output_chunks):\n                intra_chunk_size = intra_node_size * p.ds_tensor.ds_numel\n                local_offset = local_rank * p.ds_tensor.ds_numel\n                _in = param_tensors[i].narrow(0, j * intra_chunk_size + local_offset, p.ds_tensor.ds_numel)\n                intra_outputs.append(_out)\n                intra_inputs.append(_in)\n\n        all_gather_handle = dist.all_gather_coalesced(intra_outputs,\n                                                      intra_inputs,\n                                                      group=intra_node_comm_group,\n                                                      async_op=True)\n        for i, param in enumerate(params):\n            param.data = param_tensors[i].narrow(0, 0, param.ds_numel).view(param.ds_shape).data\n\n        return MiCS_AllGatherCoalescedHandle(\n            allgather_handle=all_gather_handle,\n            params=params,\n            partitions=[],\n            world_size=param_shard_size,\n        )\n\n    def get_partition_dp_group(self, param):\n        return param.comm.param_shard_group\n\n    def get_partition_rank(self):\n        return self.mics_comm_groups.param_shard_rank\n\n    @property\n    def num_partitions(self):\n        return self.mics_comm_groups.param_shard_size\n\n\nclass MiCS_Offload(DeepSpeedZeRoOffload):\n    \"\"\" Wrapper to change the behavior for parameter sharding\n    \"\"\"\n\n    def _convert_to_zero_parameters(self, ds_config, module, mpu):\n        \"\"\" overload the parent class function for convert the parameters\n\n        \"\"\"\n        log_dist(f'Convert to zero parameters from MiCS Offload manager', ranks=[0])\n        non_zero_params = [p for p in module.parameters() if not is_zero_param(p)]\n        if non_zero_params:\n            zero_params = [p for p in module.parameters() if is_zero_param(p)]\n            if zero_params:\n                zero_params[0].convert_to_zero_parameters(param_list=non_zero_params)\n            else:\n                group = None\n                if mpu:\n                    group = mpu.get_data_parallel_group()\n\n                MiCS_Init(module=module,\n                          data_parallel_group=group,\n                          dtype=self.dtype,\n                          config_dict_or_path=ds_config,\n                          remote_device=self.offload_device,\n                          pin_memory=self.offload_param_pin_memory,\n                          mpu=mpu)\n\n\nclass MiCS_Optimizer(DeepSpeedZeroOptimizer_Stage3):\n    \"\"\"\n    MiCS Optimizer\n    \"\"\"\n\n    def __init__(self,\n                 module,\n                 init_optimizer,\n                 timers,\n                 ds_config,\n                 static_loss_scale=1,\n                 dynamic_loss_scale=False,\n                 dynamic_loss_args=None,\n                 verbose=True,\n                 contiguous_gradients=True,\n                 reduce_bucket_size=500000000,\n                 prefetch_bucket_size=50000000,\n                 max_reuse_distance=1000000000,\n                 max_live_parameters=1000000000,\n                 param_persistence_threshold=100000,\n                 model_persistence_threshold=sys.maxsize,\n                 dp_process_group=None,\n                 reduce_scatter=True,\n                 overlap_comm=False,\n                 offload_optimizer_config=None,\n                 offload_param_config=None,\n                 sub_group_size=1000000000000,\n                 offload_ratio=0.0,\n                 mpu=None,\n                 clip_grad=0,\n                 gradient_accumulation_dtype=torch.float16,\n                 communication_data_type=torch.float16,\n                 postscale_gradients=True,\n                 gradient_predivide_factor=1,\n                 gradient_accumulation_steps=1,\n                 elastic_checkpoint=False,\n                 aio_config=None):\n\n        log_dist(\"Init MiCS optimizer\", ranks=[0])\n        super().__init__(module, init_optimizer, timers, ds_config, static_loss_scale, dynamic_loss_scale,\n                         dynamic_loss_args, verbose, contiguous_gradients, reduce_bucket_size, prefetch_bucket_size,\n                         max_reuse_distance, max_live_parameters, param_persistence_threshold,\n                         model_persistence_threshold, dp_process_group, reduce_scatter, overlap_comm,\n                         offload_optimizer_config, offload_param_config, sub_group_size, offload_ratio, mpu, clip_grad,\n                         gradient_accumulation_dtype, communication_data_type, postscale_gradients,\n                         gradient_predivide_factor, gradient_accumulation_steps, elastic_checkpoint, aio_config)\n        first_param = next(module.parameters())\n        # overload the dp_process_group and partition_count\n        assert hasattr(first_param, \"comm\"), \" \".join([\n            \"Sharded parameters don't have the MiCS_CommGroups attached.\",\n            \"Might due to the use of deepspeed.zero.Init context for initializing the weights.\",\n            \"To use MiCS sharding, please use deepspeed.zero.MiCS_Init instead for initializing parameter.\"\n        ])\n        self.dp_process_group = first_param.comm.param_shard_group\n        self.partition_count = first_param.comm.param_shard_size\n\n    def initialize_ds_offload(\n        self,\n        *args,\n        **kwargs,\n    ):\n        return MiCS_Offload(*args, **kwargs)\n\n    def partition_grads(self, params_to_release: List[Parameter], grad_partitions: List[Tensor]) -> None:\n        grad_buffers = super().partition_grads(params_to_release, grad_partitions)\n        # perform all-reduce among replication groups\n        # the function will perform accumulation boundary check\n        self.allreduce_mics_shard_grads(params_to_release, grad_buffers)\n\n    @instrument_w_nvtx\n    def allreduce_mics_shard_grads(self, params, partitioned_grads_buffers: List[Tensor]):\n        \"\"\"\n        \"\"\"\n        # TODO: improve the condition check\n        if not self.is_gradient_accumulation_boundary or \\\n            len(partitioned_grads_buffers) == 0:\n            return\n\n        mics_comm_groups: MiCS_CommGroups = params[0].comm\n        param_repli_group = mics_comm_groups.param_repli_group\n        param_repli_size = mics_comm_groups.param_repli_size\n\n        if param_repli_size is None or param_repli_size <= 1:\n            return\n        if not get_accelerator().on_accelerator(partitioned_grads_buffers[0]):\n            raise RuntimeError(\"Local sharding has no support for CPU offloading\")\n\n        if dist.has_all_reduce_coalesced():\n            scale_tensors(partitioned_grads_buffers, param_repli_size)\n            dist.all_reduce_coalesced(tensors=partitioned_grads_buffers, group=param_repli_group)\n        else:\n            # manually coalescing all-reduce\n            aggregated_buffer: Tensor = torch.cat(partitioned_grads_buffers)\n            aggregated_buffer.div_(param_repli_size)\n            dist.all_reduce(aggregated_buffer, group=param_repli_group)\n            offset = 0\n            for grad_buff in partitioned_grads_buffers:\n                grad_buff.view(-1).copy_(aggregated_buffer.narrow(0, offset, grad_buff.numel()))\n                offset += grad_buff.numel()\n\n    def load_state_dict(self,\n                        state_dict_list,\n                        load_optimizer_states=True,\n                        load_from_fp32_weights=False,\n                        checkpoint_folder=None,\n                        load_serial=None):\n        r\"\"\" Loading the ZeRO-3/MiCS partitioned checkpoints\n        Because the self.dp_process_group is replaced with the communicator for\n        partition group we can call the load_state_dict logic from ZeRO-3.\n        \"\"\"\n        super().load_state_dict(state_dict_list, load_optimizer_states, load_from_fp32_weights, checkpoint_folder)\n", "deepspeed/runtime/zero/tiling.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nimport deepspeed\nfrom deepspeed.runtime.utils import partition_uniform as partition\n\n\ndef split_tensor_along_last_dim(tensor, partitions, contiguous_split_chunks=False):\n    \"\"\"Split a tensor along its last dimension. Adapted from Megatron-LM.\n\n    Arguments:\n        tensor: input tensor.\n        partitions: list of partition sizes to supply to torch.split\n        contiguous_split_chunks: If True, make each chunk contiguous\n                                 in memory.\n    \"\"\"\n    # Get the size and dimension.\n    last_dim = tensor.dim() - 1\n\n    # Split.\n    tensor_list = torch.split(tensor, partitions, dim=last_dim)\n    # Note: torch.split does not create contiguous tensors by default.\n    if contiguous_split_chunks:\n        return tuple(chunk.contiguous() for chunk in tensor_list)\n\n    return tensor_list\n\n\nclass TiledLinear(torch.nn.Module):\n\n    def __init__(self,\n                 in_features,\n                 out_features,\n                 bias=True,\n                 in_splits=1,\n                 out_splits=1,\n                 input_is_already_split=False,\n                 combine_out_splits=True,\n                 linear_cls=torch.nn.Linear,\n                 init_linear=None,\n                 **kwargs):\n        \"\"\"A replacement for ``torch.nn.Linear`` that works with ZeRO-3 to reduce\n        memory requirements via tiling.\n\n        TiledLinear breaks the input and output dimensions of a linear layer\n        into tiles that are processed in sequence. This class enables huge\n        linear layers when combined with ZeRO-3 because inactive tiles can be\n        partitioned and offloaded.\n\n        .. note::\n            We recommend using as few tiles as necessary. Tiling\n            significantly reduces memory usage, but can reduce throughput\n            for inexpensive layers. This due to the smaller kernels having\n            less parallelism and lower arithmetic intensity, while\n            introducing more frequent synchronization and communication.\n\n        Args:\n            in_features (int): See ``torch.nn.Linear``\n            out_features (int): See ``torch.nn.Linear``\n            bias (bool, optional): See ``torch.nn.Linear``\n            in_splits (int, optional): The number of tiles along the input dimension. Defaults to 1.\n            out_splits (int, optional): The number of tiles along the output dimension. Defaults to 1.\n            input_is_already_split (bool, optional): If set to ``True``, assume that the ``input_`` in\n                to ``forward()`` is already split into ``in_splits`` chunks. Defaults to ``False``.\n            combine_out_splits (bool, optional): If set to ``False``, do not combine the ``out_splits`` outputs\n                into a single tensor. Defaults to ``True``.\n            linear_cls (class, optional): The underlying class to build individual tiles.\n                Defaults to ``torch.nn.Linear``.\n            init_linear (``torch.nn.Linear``, optional): If set, copy the parameters of\n                ``init_linear``. Useful for debugging. Defaults to ``None``.\n            kwargs (dict, optional): additional keyword arguments to provide to ``linear_cls()``.\n\n        Raises:\n            RuntimeError: ``in_splits`` must be within the range [1, in_features).\n            RuntimeError: ``out_splits`` must be within the range of [1, out_features).\n        \"\"\"\n\n        super().__init__()\n\n        if (in_splits < 1) or (in_splits > in_features):\n            raise RuntimeError('in splits must be in range [1, in_features].')\n        if (out_splits < 1) or (out_splits > out_features):\n            raise RuntimeError('out splits must be in range [1, out_features].')\n\n        # global, not necessarily local\n        self.in_features = in_features\n        self.out_features = out_features\n        self.use_bias = bias\n\n        self.out_splits = out_splits\n        self.in_splits = in_splits\n        self.input_is_already_split = input_is_already_split\n        self.combine_out_splits = combine_out_splits\n\n        # Build partition-lists. These are CSR-style splits [0, part0, part1, ..., features]\n        # For example, row_parts[p] gives the start of partition p and row_parts[p+1]\n        # is the exclusive end.\n        self.in_parts = partition(num_items=in_features, num_parts=in_splits)\n        self.out_parts = partition(num_items=out_features, num_parts=out_splits)\n\n        assert len(self.out_parts) == out_splits + 1\n        assert len(self.in_parts) == in_splits + 1\n        assert self.out_parts[0] == 0\n        assert self.out_parts[out_splits] == out_features\n        assert self.in_parts[in_splits] == in_features\n\n        self.linears = torch.nn.ModuleList()\n        for out_id in range(out_splits):\n            self.linears.append(torch.nn.ModuleList())\n\n            local_out_dim = self.out_parts[out_id + 1] - self.out_parts[out_id]\n\n            for in_id in range(in_splits):\n                #if input_size is split, we only need one bias\n                local_bias = bias if in_id == (in_splits - 1) else False\n\n                local_in_dim = self.in_parts[in_id + 1] - self.in_parts[in_id]\n                local = linear_cls(local_in_dim, local_out_dim, bias=local_bias, **kwargs)\n                self.linears[out_id].append(local)\n\n        # Optionally initialize with a known tensor\n        if init_linear is not None:\n            self.copy_params_from(init_linear)\n\n    def forward(self, input_):\n        if self.in_splits > 1 and not self.input_is_already_split:\n            input_parts = partition(input_.shape[-1], self.in_splits)\n            split_sizes = [input_parts[p + 1] - input_parts[p] for p in range(self.in_splits)]\n            inputs = self._split_global_input(input_, split_sizes)\n        elif self.in_splits > 1:\n            inputs = input_\n            assert len(\n                inputs) == self.in_splits, f\"Col splits {self.in_splits} does not match input splits {len(inputs)}\"\n        else:\n            # no splits\n            inputs = [input_]\n\n        outputs = [None] * self.out_splits\n        for out_id in range(self.out_splits):\n            for in_id in range(self.in_splits):\n                local_output = self.linears[out_id][in_id](inputs[in_id])\n\n                outputs[out_id] = self._reduce_local_output(in_id=in_id,\n                                                            out_id=out_id,\n                                                            current_out=outputs[out_id],\n                                                            new_out=local_output)\n\n        if self.combine_out_splits:\n            return self._combine_output_splits(outputs)\n\n        return outputs\n\n    def _split_global_input(self, input, split_sizes):\n        \"\"\"Partition an input tensor along the last dimension, aligned with given splits.\n\n        Subclasses should override this method to account for new input types.\n\n        Args:\n            input (List[Tensor]): The tensor to partition along the last dimension.\n            split_sizes (List[int]): The size of each partition.\n\n        Returns:\n            List[Any]: A list of the chunks of ``input``.\n        \"\"\"\n        return split_tensor_along_last_dim(input, split_sizes)\n\n    def _reduce_local_output(self, in_id, out_id, current_out, new_out):\n        \"\"\"Reduce (sum) a new local result into the existing local results.\n\n        Subclasses should override this method.\n\n        For a given ``out_id``, this method is called ``in_id-1`` times. The first input\n        split is a simple assignment.\n\n        Args:\n            in_id (int): The input split that produced ``new_out``.\n            out_id (int): The output split that produced ``new_out``.\n            current_out (Any): The reduced form of all previous ``out_id`` results.\n            new_out (Any): The local result from forward (``in_id``, ``out_id``)e\n\n        Returns:\n            Any: The combined result of ``current_out`` and ``new_out``.\n        \"\"\"\n\n        if current_out is None:\n            #this clone is necessary to preserve auto grad\n            #there is some issue with inplace update for outputs that are views\n            return new_out.clone()\n        else:\n            return current_out + new_out\n\n    def _combine_output_splits(self, outputs):\n        \"\"\"Join the splits of the output into a single result.\n\n        Args:\n            outputs (List[Any]): The reduced outputs for each output split.\n\n        Returns:\n            Any: The combined outputs.\n        \"\"\"\n        assert len(outputs) == self.out_splits\n        return torch.cat(outputs, dim=-1)\n\n    @torch.no_grad()\n    def copy_params_from(self, other):\n        \"\"\"Copy the weight and bias data from ``other``.\n\n        This is especially useful for reproducible initialization and testing.\n\n        Equivalent to:\n\n        .. code-block:: python\n\n            with torch.no_grad():\n                self.weight.copy_(other.weight)\n                if self.bias is not None:\n                    self.bias.copy_(other.bias)\n\n        .. note::\n            If ZeRO-3 is enabled, this is a collective operation and the\n            updated parameters of data-parallel rank 0 will be visible on all\n            ranks. See :class:`deepspeed.zero.GatheredParameters` for more\n            information.\n\n\n        Args:\n            other (``torch.nn.Linear``): the linear layer to copy from.\n        \"\"\"\n        assert hasattr(other, 'weight')\n        assert other.weight.size() == (self.out_features, self.in_features)\n        if self.use_bias:\n            assert hasattr(other, 'bias')\n            assert other.bias is not None\n            assert other.bias.size() == (self.out_features, )\n        else:\n            assert other.bias is None\n\n        for row in range(self.out_splits):\n            rstart = self.out_parts[row]\n            rstop = self.out_parts[row + 1]\n\n            for col in range(self.in_splits):\n                cstart = self.in_parts[col]\n                cstop = self.in_parts[col + 1]\n\n                local = self.linears[row][col]\n                global_weight = other.weight[rstart:rstop, cstart:cstop]\n                with deepspeed.zero.GatheredParameters(local.weight, modifier_rank=0):\n                    local.weight.copy_(global_weight)\n\n            if local.bias is not None:\n                with deepspeed.zero.GatheredParameters(local.bias, modifier_rank=0):\n                    local.bias.data.copy_(other.bias[rstart:rstop].data)\n\n\nclass TiledLinearReturnBias(TiledLinear):\n    \"\"\"Wrapper for a Linear class that returns its own bias parameter, such as\n    used by Megatron-LM.\n    \"\"\"\n\n    def _reduce_local_output(self, in_id, out_id, current_out, new_out):\n        \"\"\"Reduces output tensors, but not the returned bias. \"\"\"\n        if current_out is not None:\n            old_tensor, old_bias = current_out\n        else:\n            old_tensor, old_bias = None, None\n\n        assert isinstance(new_out, tuple)\n        assert len(new_out) == 2\n\n        tensor, bias = new_out\n        assert tensor is not None\n\n        tensor = super()._reduce_local_output(in_id=in_id, out_id=out_id, current_out=old_tensor, new_out=tensor)\n\n        if bias is None:\n            bias = old_bias\n\n        return tensor, bias\n\n    def _combine_output_splits(self, outputs):\n        # stack output tensors\n        tensors = [o[0] for o in outputs]\n        tensor = super()._combine_output_splits(tensors)\n\n        # stack biases if applicable\n        biases = [o[1] for o in outputs if o[1] is not None]\n        if len(biases) > 0:\n            bias = super()._combine_output_splits(biases)\n        else:\n            bias = None\n\n        return tensor, bias\n", "deepspeed/runtime/zero/partitioned_param_coordinator.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom dataclasses import dataclass\nimport collections\nfrom collections import UserDict\nfrom typing import Deque, Set\n\nfrom deepspeed import comm as dist\nfrom deepspeed.utils import z3_leaf_module\nfrom deepspeed.utils.logging import logger\nfrom deepspeed.runtime.zero.offload_config import OffloadDeviceEnum\nfrom deepspeed.runtime.zero.partition_parameters import *\nfrom deepspeed.runtime.zero.partitioned_param_profiler import PartitionedParameterProfiler\nfrom deepspeed.runtime.swap_tensor.partitioned_param_swapper import PartitionedParamStatus\nfrom deepspeed.utils.debug import debug_module2name_id, debug_param2name_id\nfrom deepspeed.accelerator import get_accelerator\nimport deepspeed.runtime.compiler as compiler\n\nimport logging\n\nENABLE_PROFILER = False\n\n\ndef debug_rank0(message: str) -> None:\n    if dist.get_rank() == 0:\n        logger.debug(message)\n\n\n@instrument_w_nvtx\ndef get_all_parameters(sub_module, recurse=False):\n    return itertools.chain(sub_module.named_parameters(recurse=recurse), sub_module.ds_external_parameters())\n\n\n@compiler.disable\ndef iter_params(module: Module, recurse=False) -> Iterable[Parameter]:\n    return map(lambda pair: pair[1], get_all_parameters(module, recurse))\n\n\nclass ZeRoTraceMode(Enum):\n    # Record trace of the network during a single forward+backward (for training) or forward (for inference)\n    RECORD = 1\n    # Use recorded network trace to optimize current forward+backward or forward\n    COMPLETE = 2\n    # Recorded trace does not match current forward+backward or forward pass.\n    INVALID = 3\n\n\nclass InflightParamRegistry(UserDict):\n    \"\"\"registry for parameters in flight\"\"\"\n\n    def __setitem__(self, param: Parameter, handle: AllGatherCoalescedHandle) -> None:\n        if param in self.data:\n            raise RuntimeError(f\"{param.ds_summary()} already in registry\")\n        if param.ds_status != ZeroParamStatus.INFLIGHT:\n            raise RuntimeError(f\"attempted to add non-inflight parameter to registry {param.ds_summary()}\")\n        self.data[param] = handle\n\n\nclass PartitionedParameterCoordinator:\n    FORWARD_FETCH_SUBMIT = 'forward_fetch_submit'\n    FORWARD_FETCH_WAIT = 'forward_fetch_wait'\n    FORWARD_PREFETCH_SUBMIT = 'forward_prefetch_submit'\n    BACKWARD_FETCH_SUBMIT = 'backward_fetch_submit'\n    BACKWARD_FETCH_WAIT = 'backward_fetch_wait'\n    BACKWARD_PREFETCH_SUBMIT = 'backward_prefetch_wait'\n    FORWARD_ALL_GATHER = 'forward_all_gather'\n    BACKWARD_ALL_GATHER = 'backward_all_gather'\n    \"\"\"Handles partitioning and gathering of parameters.\"\"\"\n\n    @dataclass\n    class __ParamInTrace:\n        param: Parameter\n        step_id_last_used_at: int\n\n    def __init__(\n        self,\n        prefetch_bucket_sz: int,\n        max_reuse_distance_in_numel: int,\n        max_available_parameters_in_numel: int,\n        allgather_stream: get_accelerator().Stream,\n        inflight_param_registry: InflightParamRegistry,\n        prefetch_nvme: bool = False,\n        timers=None,\n        zero_quantized_weights=False,\n        zero_quantized_nontrainable_weights=False,\n    ) -> None:\n        # mapping of param -> handle for each param that is currently in flight\n        self.__inflight_param_registry = inflight_param_registry\n        # keeps track of the number of submodules invoked so far.\n        self.__step_id: int = 0\n        # network tracing mode\n        self.__trace_mode: ZeRoTraceMode = ZeRoTraceMode.RECORD\n        # sequence of submodules/parameters in forward pass + backward pass\n        self.__submodule_order: Iterable[Module] = []\n        self.__param_order: Iterable[__class__.__ParamInTrace] = []\n        self.__most_recent_step_id_param_fetched_for = collections.defaultdict(lambda: int(-1e10))\n        self.__step_id_module_fetched_for = collections.defaultdict(lambda: collections.deque())\n        # number of available params, and max number of available params\n        self.__n_available_params: int = 0\n        self.__max_n_available_params: int = max_available_parameters_in_numel\n        # max distance between two use of the module beyond which module is released\n        self.__max_reuse_dist_in_numel: int = max_reuse_distance_in_numel\n        # queue for parameters to fetch. parameters will be popped off the left\n        # side of the dequeue as they are fetched\n        self.__param_queue: Deque[__class__.__ParamInTrace] = None\n        self.__prefetch_bucket_sz: int = prefetch_bucket_sz\n        self.__prefetch_nvme: bool = prefetch_nvme\n        self.hierarchy: int = 0\n        self.zero_quantized_weights = zero_quantized_weights\n        self.zero_quantized_nontrainable_weights = zero_quantized_nontrainable_weights\n\n        # stream that will be used for allgather operations\n        self.__allgather_stream: get_accelerator().Stream = allgather_stream\n\n        # limit the number of fetch events that can be queued at once\n        # otherwise, what happens is memory is allocated by the host thread at the\n        # time of the call, but not used until later by the asynchronous cuda stream.\n        # allowing an infinite number of these to queue up causes a lot of memory\n        # pressure that then becomes detrimental to performance.\n        # this is a much less elegant way of fixing this vs something like using\n        # cudaMallocAsync/cudaFreeAsync. Choosing to not expose this to the user now\n        # because ideally in the future its replaced by an async allocation\n        # mechanism which doesn't require any configuration by the user.\n        self.__ongoing_fetch_events: Deque[get_accelerator().Event] = collections.deque()\n        # TODO. make this configurable via JSON\n        self.__max_ongoing_fetch_events: int = 2\n        self.__profiler = PartitionedParameterProfiler(timers if ENABLE_PROFILER else None)\n\n    \"\"\"Tracing and Tracking\n    TODO. consider performing trace before initializing PartitionedParameterCoordinator\n    and passing trace results into constructor. This way all the code in here can\n    just assume that the trace is complete and the results can be entirely\n    immutable.\n\n    Bookkeeping operations used to track where we are in the forward/backward pass\n    \"\"\"\n\n    def _clear_trace_structures(self) -> None:\n        self.__submodule_order = []\n        self.__param_order = []\n        self.__most_recent_step_id_param_fetched_for = collections.defaultdict(lambda: int(-1e10))\n        self.__param_queue = None\n\n    def is_complete_trace(self) -> bool:\n        return self.__trace_mode == ZeRoTraceMode.COMPLETE\n\n    def is_invalid_trace(self) -> bool:\n        return self.__trace_mode == ZeRoTraceMode.INVALID\n\n    def is_record_trace(self) -> bool:\n        return self.__trace_mode == ZeRoTraceMode.RECORD\n\n    def _invalidate_trace(self) -> None:\n        if self.is_invalid_trace():\n            raise RuntimeError(\"attempted to invalidate already invalid trace\")\n        self.__trace_mode = ZeRoTraceMode.INVALID\n        self._clear_trace_structures()\n\n    def trace_prologue(self, sub_module: Module) -> None:\n        if self.is_complete_trace():\n            # sub_module must match expectation else invalidate trace cache\n            if len(self.__submodule_order) <= self.__step_id:\n                print_rank_0(\n                    f\"Invalidate trace cache @ step {self.__step_id} and module {sub_module.id}: \"\n                    f\"cache has only {len(self.__submodule_order)} modules\",\n                    force=True)\n                self._invalidate_trace()\n                return\n\n            if sub_module != self.__submodule_order[self.__step_id]:\n                expected_module_id = self.__submodule_order[self.__step_id].id\n                print_rank_0(\n                    f\"Invalidate trace cache @ step {self.__step_id}: \"\n                    f\"expected module {expected_module_id}, but got module {sub_module.id}\",\n                    force=True)\n                self._invalidate_trace()\n\n    @compiler.disable\n    def record_module(self, sub_module: Module) -> None:\n        \"\"\"adds sub module to trace\"\"\"\n        if not self.is_record_trace():\n            raise RuntimeError(f\"attempted to record trace when status = {self.__trace_mode}\")\n\n        self.__submodule_order.append(sub_module)\n        self.__step_id_module_fetched_for[sub_module.id].append(self.__step_id)\n\n    def record_parameters(self, sub_module: Module) -> None:\n        \"\"\"adds sub module to trace\"\"\"\n        if not self.is_record_trace():\n            raise RuntimeError(f\"attempted to record trace when status = {self.__trace_mode}\")\n\n        step_id = self.__step_id_module_fetched_for[sub_module.id].popleft()\n        for param in sorted(set(iter_params(sub_module, recurse=z3_leaf_module(sub_module))), key=lambda p: p.ds_id):\n            self.__param_order.append(__class__.__ParamInTrace(param=param, step_id_last_used_at=step_id))\n\n    def construct_parameter_trace_from_module_trace(self):\n        \"\"\"use module trace to construct parameter trace\"\"\"\n        self.__param_order = []\n        for sub_module in self.__submodule_order:\n            self.record_parameters(sub_module)\n\n    def reset_step(self) -> None:\n        \"\"\"indicate that we have completed one fwd+bwd for the model\"\"\"\n        if self.__inflight_param_registry:\n            raise RuntimeError(f\"still have inflight params \"\n                               f\"{[p.ds_summary() for p in self.__inflight_param_registry.keys()]}\")\n\n        if not self.is_complete_trace():  # not self.trace_complete:\n            # Make sure that recorded submodule orders are identical across ranks\n            assert_ints_same_as_other_ranks([m.id for m in self.__submodule_order])\n\n            if self.is_record_trace():\n                # Successfully recorded a trace\n                self.construct_parameter_trace_from_module_trace()\n                # Make sure that recorded parameter orders are identical across ranks\n                assert_ints_same_as_other_ranks([p.param.ds_id for p in self.__param_order])\n                assert_ints_same_as_other_ranks([p.step_id_last_used_at for p in self.__param_order])\n\n                self.__submodule_order = tuple(self.__submodule_order)  # freeze\n                self.__param_order = tuple(self.__param_order)  # freeze\n                self.__trace_mode = ZeRoTraceMode.COMPLETE\n                print_rank_0(\n                    f\"completed record trace of {len(self.__submodule_order)} sub modules: {[m.id for m in self.__submodule_order]}\",\n                    force=False)\n            else:\n                # Enable trace recording for next forward/backward pass\n                self.__trace_mode = ZeRoTraceMode.RECORD\n\n        else:\n            if self.__profiler is not None:\n                self.__profiler.log_events()\n\n        self.__param_queue = collections.deque(self.__param_order)  # reset fetch queue\n        self.__most_recent_step_id_param_fetched_for = collections.defaultdict(lambda: int(-1e10))\n        self.__step_id_module_fetched_for = collections.defaultdict(lambda: collections.deque())\n        self.__step_id = 0\n        self.__n_available_params = 0\n        self.__profiler.reset_events()\n\n    def _dump_params(self, tag, sub_module, params, step_id=None):\n        if step_id is None:\n            step_id = self.__step_id\n        param_names = [debug_param2name_id(p) for p in params]\n        print_rank_0(f'{tag} step = {step_id} mod = {debug_module2name_id(sub_module)} p_names = {param_names}',\n                     force=False)\n\n    def _dump_param_ids(self, tag, mod_id, p_ids, step_id=None):\n        if step_id is None:\n            step_id = self.__step_id\n        print_rank_0(f'{tag} mod = {mod_id}, step = {step_id}, p_ids = {p_ids}', force=False)\n\n    \"\"\"Fetch and Release\n    Fetching, prefetching, and releasing parameters\n    \"\"\"\n\n    @compiler.disable\n    @instrument_w_nvtx\n    @torch.no_grad()\n    def fetch_sub_module(self, current_submodule: Module, forward: bool) -> None:\n        \"\"\"This method does the following (in order):\n        1. kick off fetch for parameters in immediately required sub module\n        2. kick off fetch for next few parameters we will need later (prefetch)\n        3. block on parameters in immediately required sub module\n        \"\"\"\n        if logger.isEnabledFor(logging.DEBUG):\n            debug_rank0(\n                f\"{self.__step_id}: M{current_submodule.id}({type(current_submodule).__name__}) P{[p.ds_id for p in iter_params(current_submodule, recurse=z3_leaf_module(current_submodule))]} \"\n                + str({\n                    \"avail\": f\"{self.__n_available_params:.1e}\",\n                    \"queue_sz\": f\"{len(self.__param_queue or [])}\",\n                    \"inflight\": [p.ds_id for p in self.__inflight_param_registry],\n                }))\n\n        params_to_fetch = frozenset(iter_params(current_submodule, recurse=z3_leaf_module(current_submodule)))\n        fetch_numel = sum(\n            [p.partition_numel() for p in params_to_fetch if p.ds_status == ZeroParamStatus.NOT_AVAILABLE])\n\n        if fetch_numel > 0:\n            event_name = __class__.FORWARD_FETCH_SUBMIT if forward else __class__.BACKWARD_FETCH_SUBMIT\n            self._dump_param_ids(event_name, current_submodule.id,\n                                 [p.ds_id for p in params_to_fetch if p.ds_status == ZeroParamStatus.NOT_AVAILABLE])\n            self.__profiler.start_event(event_name)\n            # kick off all gather for params in the immediately required submodule\n            #for param in params_to_fetch:\n            if logger.isEnabledFor(logging.DEBUG):\n                for param in params_to_fetch:\n                    debug_rank0(f\"-fetch: {param.ds_summary()}\")\n            self.__all_gather_params(params_to_fetch, forward)\n            self.__profiler.stop_event(event_name, fetch_numel)\n\n        wait_numel = 0\n        wait_event_name = __class__.FORWARD_FETCH_WAIT if forward else __class__.BACKWARD_FETCH_WAIT\n        self.__profiler.start_event(wait_event_name)\n        # wait for parameters in the immediately needed submodule to become available\n        for param in params_to_fetch:\n            param.ds_active_sub_modules.add(current_submodule.id)\n            if logger.isEnabledFor(logging.DEBUG):\n                debug_rank0(f\"-wait: {param.ds_summary()}\")\n            if param in self.__inflight_param_registry:\n                wait_numel += param.partition_numel()\n                with get_accelerator().stream(self.__allgather_stream):\n                    while self.__ongoing_fetch_events and self.__ongoing_fetch_events[0].query():\n                        self.__ongoing_fetch_events.popleft()\n                    if len(self.__ongoing_fetch_events) > self.__max_ongoing_fetch_events:\n                        self.__ongoing_fetch_events.popleft().synchronize()\n\n                    self.__inflight_param_registry.pop(param).wait()\n\n                    if not get_accelerator().handles_memory_backpressure():\n                        event = get_accelerator().Event()\n                        event.record()\n                        self.__ongoing_fetch_events.append(event)\n\n            assert param.ds_status == ZeroParamStatus.AVAILABLE, param.ds_summary()\n        if not get_accelerator().resolves_data_dependency():\n            get_accelerator().current_stream().wait_stream(self.__allgather_stream)\n        self.__profiler.stop_event(wait_event_name, wait_numel)\n\n        # kick off parameter prefetches for upcoming modules\n        # don't prefetch if we dont have a completed model trace\n        if self.is_complete_trace():\n            # go through the parameters we need for the current module and pop them\n            # off the fetch queue so that they aren't prefetched later.\n            # if params have already been popped off the fetch queue by earlier\n            # prefetches we won't look for them here\n            discarded_from_prefetch_queue = set()\n            params_not_already_fetched = set(\n                filter(lambda p: self.__most_recent_step_id_param_fetched_for[p] < self.__step_id, params_to_fetch))\n            while self.__param_queue and len(discarded_from_prefetch_queue) < len(params_not_already_fetched):\n                param_in_trace = self.__param_queue.popleft()\n                self.__most_recent_step_id_param_fetched_for[\n                    param_in_trace.param] = param_in_trace.step_id_last_used_at\n                discarded_from_prefetch_queue.add(param_in_trace.param)\n\n            if discarded_from_prefetch_queue != params_not_already_fetched:\n                raise RuntimeError(\n                    f\"tracing error at step {self.__step_id}: \\n\"\n                    f\"module id: {current_submodule.id}, training: {current_submodule.training}\\n\"\n                    f\"expected the next {len(params_not_already_fetched)} parameters in the \"\n                    f\"parameter fetch queue to be {tuple(p.ds_summary(use_debug_name=True) for p in params_not_already_fetched)} \\n\"\n                    f\"but got \\n {tuple(p.ds_summary(use_debug_name=True) for p in discarded_from_prefetch_queue)}.\")\n\n            def _is_currently_on_nvme(param):\n                if param.nvme_swapper is None:\n                    return False\n\n                return param.ds_tensor.final_location == OffloadDeviceEnum.nvme \\\n                    and param.ds_tensor.status == PartitionedParamStatus.NOT_AVAILABLE\n\n            # kick off all gather for params in the next few submodules (prefetch)\n            if self.__prefetch_bucket_sz > 0:\n                max_params_to_prefetch = min(self.__max_n_available_params - self.__n_available_params,\n                                             self.__prefetch_bucket_sz)\n                params_to_prefetch = set()\n                numel_prefetching = 0\n                while self.__param_queue and numel_prefetching < max_params_to_prefetch:\n                    param_in_trace: __class__.__ParamInTrace = self.__param_queue.popleft()\n\n                    if _is_currently_on_nvme(param_in_trace.param):\n                        # nvme prefetch is handled elsewhere. Need to break here to preserve fetch order\n                        self.__param_queue.appendleft(param_in_trace)\n                        break\n\n                    do_prefetch = param_in_trace.param.ds_status == ZeroParamStatus.NOT_AVAILABLE\n                    if param_in_trace.param in params_to_prefetch:\n                        # Avoid duplicates\n                        do_prefetch = False\n\n                    self.__most_recent_step_id_param_fetched_for[param_in_trace.param] = \\\n                        max(self.__most_recent_step_id_param_fetched_for[param_in_trace.param],\n                            param_in_trace.step_id_last_used_at)\n\n                    if do_prefetch:\n                        params_to_prefetch.add(param_in_trace.param)\n                        numel_prefetching += param_in_trace.param.ds_numel\n\n                if numel_prefetching > 0:\n                    event_name = __class__.FORWARD_PREFETCH_SUBMIT if forward else __class__.BACKWARD_PREFETCH_SUBMIT\n                    self.__profiler.start_event(event_name)\n                    if logger.isEnabledFor(logging.DEBUG):\n                        for param in params_to_prefetch:\n                            debug_rank0(f\"-prefetch: {param.ds_summary()}\")\n                    self.__all_gather_params(params_to_prefetch, forward)\n                    self.__profiler.stop_event(event_name, numel_prefetching)\n\n                if self.__prefetch_nvme:\n                    self.__prefetch_nvme_param_partitions()\n\n        self.__step_id += 1\n\n    @instrument_w_nvtx\n    @torch.no_grad()\n    def release_sub_module(self, submodule: Module) -> None:\n        \"\"\"release the parameters of a sub module, assuming they meet conditions to\n        be released.\"\"\"\n        params_to_release = (self.__params_to_release(submodule, self.__step_id) if self.is_complete_trace() else set(\n            p.ds_id for p in iter_params(submodule, recurse=z3_leaf_module(submodule))))\n        for param in iter_params(submodule, recurse=z3_leaf_module(submodule)):\n            param.ds_active_sub_modules.discard(submodule.id)\n            if param.ds_id in params_to_release and not param.is_external_param:\n                self.__release_param(param)\n\n    @instrument_w_nvtx\n    @torch.no_grad()\n    def release_and_reset_all(self, module: Module) -> None:\n        \"\"\"release all module parameters\"\"\"\n        for param in iter_params(module, recurse=True):\n            if param in self.__inflight_param_registry:\n                raise RuntimeError(f\"param {param.ds_summary()} still in flight\")\n\n            # TODO. make this throw if if there are still active submodules. currently\n            # there's a hook execution issue\n            param.ds_active_sub_modules.clear()\n            self.__release_param(param)\n\n        for param in iter_params(module, recurse=True):\n            if param.ds_status != ZeroParamStatus.NOT_AVAILABLE:\n                raise RuntimeError(f\"{param.ds_summary()} expected to be released\")\n\n    @instrument_w_nvtx\n    def __all_gather_params(self, params: Set[Parameter], forward: bool) -> None:\n        quantized_params = []\n        nonquantized_params = []\n        for param in params:\n            if hasattr(param.ds_tensor, 'ds_quant_scale'):\n                quantized_params.append(param)\n            else:\n                nonquantized_params.append(param)\n        if quantized_params:\n            self.__all_gather_params_(quantized_params, forward, quantize=True)\n        if nonquantized_params:\n            self.__all_gather_params_(nonquantized_params, forward, quantize=self.zero_quantized_weights)\n\n    def __all_gather_params_(self, params: Set[Parameter], forward: bool, quantize: bool = False) -> None:\n        \"\"\"for each partitioned parameter, kick off an async allgather and store\n        the work handle for the in flight parameters.\"\"\"\n        partitioned_params = []\n        all_gather_numel = 0  # numel = num of elements\n        for param in params:\n            if param.ds_status == ZeroParamStatus.NOT_AVAILABLE:\n                partitioned_params.append(param)\n                all_gather_numel += param.ds_numel\n\n        if partitioned_params:\n            self.__n_available_params += all_gather_numel\n            # here we need to handle a special case where some of the parameters have a valid hpz secondary tensor (e.g. they are not trainable so their secondary tensor never expire) but others do not.\n            partitioned_params_with_secondary_tensors = [\n                p for p in partitioned_params if p.ds_secondary_tensor is not None\n            ]\n            partitioned_params_without_secondary_tensors = [\n                p for p in partitioned_params if p.ds_secondary_tensor is None\n            ]\n            for param_group in [\n                    partitioned_params_with_secondary_tensors, partitioned_params_without_secondary_tensors\n            ]:\n                if not param_group:\n                    continue\n                with get_accelerator().stream(self.__allgather_stream):\n                    event_name = __class__.FORWARD_ALL_GATHER if forward else __class__.BACKWARD_ALL_GATHER\n                    self.__profiler.start_event(event_name)\n                    handle = param_group[0].all_gather_coalesced(param_group, quantize=quantize)\n                    self.__profiler.stop_event(event_name, all_gather_numel)\n                for param in param_group:\n                    assert param.ds_status == ZeroParamStatus.INFLIGHT, param.ds_summary()\n                    self.__inflight_param_registry[param] = handle\n\n            # Release swap buffers for persisted params on nvme since they will never be partitioned or evicted from GPU\n            swap_persisted_params = [\n                p for p in partitioned_params if p.ds_persist and p.ds_tensor.final_location == OffloadDeviceEnum.nvme\n            ]\n            if swap_persisted_params:\n                swap_persisted_params[0].nvme_swapper.remove_partition_and_release_buffers(swap_persisted_params)\n\n    @compiler.disable\n    @instrument_w_nvtx\n    def __release_param(self, param: Parameter) -> None:\n        if param.ds_status == ZeroParamStatus.AVAILABLE and not param.ds_active_sub_modules:\n            if logger.isEnabledFor(logging.DEBUG):\n                debug_rank0(f\"-release: {param.ds_summary()}\")\n            param.partition()\n            self.__n_available_params -= param.ds_numel\n\n    @instrument_w_nvtx\n    @functools.lru_cache(maxsize=None)\n    def __params_to_release(self, submodule_to_release: Module, step_id: int) -> Set[int]:\n        if not self.is_complete_trace():\n            raise RuntimeError(\"expected trace to be complete\")\n\n        params_to_release = set(\n            p.ds_id for p in iter_params(submodule_to_release, recurse=z3_leaf_module(submodule_to_release))\n            if not p.ds_persist)\n\n        # Problem: When prefetcher scans the param trace, it skips AVAILABLE params.\n        # This creates issues if those params are released before the skipped uses:\n        # 1) It hurts performance as the skipped uses are never prefetched.\n        # 2) For nvme params, we run out of swap buffers because the prefetch order\n        # diverges from the trace.\n        # Solution: Don't release params whose reuse was skipped by prefetch. This is\n        # possible because we detect such skips during prefetch and mark those params.\n        for param in iter_params(submodule_to_release, recurse=z3_leaf_module(submodule_to_release)):\n            if self.__most_recent_step_id_param_fetched_for[param] > step_id:\n                params_to_release.discard(param.ds_id)\n\n        # examine all modules within `max_reuse_dist_in_numel` of the current step,\n        # if we see any of the candidate parameters to be released reoccur while\n        # doing this, remove them from the set of parameters to release.\n        params_traversed = 0\n        for module in self.__submodule_order[step_id:]:\n            if params_traversed >= self.__max_reuse_dist_in_numel:\n                break\n            for param in iter_params(module, recurse=z3_leaf_module(submodule_to_release)):\n                params_to_release.discard(param.ds_id)\n                params_traversed += param.ds_numel\n\n        return params_to_release\n\n    @instrument_w_nvtx\n    def __prefetch_nvme_param_partitions(self) -> None:\n        \"\"\"swap in parameter partitions from nvme for those parameters that will be used\n        after the ones that are already being prefetched into full parameters\n        \"\"\"\n        if not self.is_complete_trace():\n            return\n\n        numel_in_flight = sum(param.ds_numel for param in self.__inflight_param_registry)\n\n        numel_considered = 0\n        swap_in_params = []\n        for param_in_trace in self.__param_queue:\n            param = param_in_trace.param\n            if param.nvme_swapper is None:\n                continue\n            if (numel_considered > 2 * numel_in_flight\n                    or len(swap_in_params) >= param.nvme_swapper.available_swap_in_buffers()):\n                break\n            if param.ds_tensor.status == PartitionedParamStatus.NOT_AVAILABLE:\n                swap_in_params.append(param)\n            numel_considered += param.ds_numel\n\n        if swap_in_params:\n            swap_in_params[0].nvme_swapper.swap_in(swap_in_params, async_op=True)\n", "deepspeed/runtime/zero/stage3.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport sys\nimport gc\nimport collections\nfrom typing import Deque, Dict, Tuple\nfrom deepspeed import comm as dist\nfrom deepspeed.utils import groups\n\nfrom torch._utils import _flatten_dense_tensors, _unflatten_dense_tensors\nfrom deepspeed.runtime.base_optimizer import ZeROOptimizer\nfrom deepspeed.utils import logger\nfrom deepspeed.runtime.fp16.loss_scaler import CreateLossScaler\nfrom deepspeed.runtime.comm.coalesced_collectives import reduce_scatter_coalesced, all_to_all_quant_reduce\nfrom deepspeed.runtime.utils import inf, get_global_norm, is_model_parallel_parameter, get_only_unique_item\nfrom deepspeed.runtime.zero.partition_parameters import *\nfrom deepspeed.runtime.zero.config import ZeroStageEnum\nfrom deepspeed.runtime.zero.offload_config import OffloadDeviceEnum\nfrom deepspeed.runtime.zero.parameter_offload import DeepSpeedZeRoOffload\nfrom deepspeed.runtime.zero.utils import apply_to_tensors_only\nfrom deepspeed.ops.adam import DeepSpeedCPUAdam\nfrom deepspeed.runtime.swap_tensor.partitioned_param_swapper import PartitionedParamStatus\nfrom deepspeed.runtime.swap_tensor.optimizer_utils import OptimizerSwapper\nfrom deepspeed.runtime.swap_tensor.partitioned_optimizer_swapper import PartitionedOptimizerSwapper\nfrom deepspeed.runtime.swap_tensor.pipelined_optimizer_swapper import PipelinedOptimizerSwapper\nfrom deepspeed.checkpoint.constants import OPTIMIZER_STATE_DICT, FP32_FLAT_GROUPS, PARTITION_COUNT, ZERO_STAGE, LOSS_SCALER\nfrom deepspeed.accelerator import get_accelerator\nfrom deepspeed.utils import z3_leaf_parameter\n\n# Toggle this to true to enable correctness test\n# with gradient partitioning and without\npg_correctness_test = False\n\nOPTIMIZER_SWAP_IN_STATE_TIMER = 'optimizer_swap_in_state'\nINIT_OPTIMIZER_TIMER = 'init_optimizer_state'\nOPTIMIZER_SWAP_OUT_STATE_TIMER = 'optimizer_swap_out_state'\nOPTIMIZER_STEP_TIMER = 'optimizer_step'\n\n\ndef print_rank_0(message, debug=False, force=False):\n    rank = dist.get_rank()\n    if rank == 0 and (debug or force):\n        logger.info(message)\n    # other variations\n    # - print for all ranks w/o interleaving\n    # printflock(f\"[{rank}] {message}\")\n    # - print to log file per rank\n    # log_rank_file(rank, message)\n\n\ndef input(msg):\n    return\n\n\ndef isclose(a, b, rtol=1e-09, atol=0.0):\n    return abs(a - b) <= max(rtol * max(abs(a), abs(b)), atol)\n\n\ndef lcm(x, y):\n    from fractions import gcd  # or can import gcd from `math` in Python 3\n    return x * y // gcd(x, y)\n\n\ndef move_to_cpu(tensor_list):\n    for tensor in tensor_list:\n        tensor.data = tensor.data.cpu()\n\n\nINITIAL_MICRO_STEP_ID = -1\n\n\nclass DeepSpeedZeroOptimizer_Stage3(ZeROOptimizer):\n    \"\"\"\n    DeepSpeedZeroOptimizer designed to reduce the memory footprint\n    required for training large deep learning models.\n\n    For more details please see ZeRO: Memory Optimization Towards Training A Trillion Parameter Models\n    https://arxiv.org/abs/1910.02054\n\n    For usage examples, refer to TODO: DeepSpeed Tutorial\n\n    \"\"\"\n\n    def __init__(\n        self,\n        module,\n        init_optimizer,\n        timers,\n        ds_config,\n        static_loss_scale=1.0,\n        dynamic_loss_scale=False,\n        dynamic_loss_args=None,\n        verbose=True,\n        contiguous_gradients=True,\n        reduce_bucket_size=500000000,\n        prefetch_bucket_size=50000000,\n        max_reuse_distance=1000000000,\n        max_live_parameters=1000000000,\n        param_persistence_threshold=100000,\n        model_persistence_threshold=sys.maxsize,\n        dp_process_group=None,\n        reduce_scatter=True,\n        overlap_comm=False,\n        offload_optimizer_config=None,\n        offload_param_config=None,\n        sub_group_size=1000000000000,\n        offload_ratio=0.0,\n        mpu=None,\n        clip_grad=0.0,\n        gradient_accumulation_dtype=torch.float32,\n        communication_data_type=torch.float16,\n        postscale_gradients=True,\n        gradient_predivide_factor=1.0,\n        gradient_accumulation_steps=1,\n        elastic_checkpoint=False,\n        aio_config=None,\n        all2all_process_group=None,\n        zero_hpz_partition_size=1,\n        zero_quantized_weights=False,\n        zero_quantized_nontrainable_weights=False,\n    ):\n        see_memory_usage(\"Stage 3 initialize beginning\", force=True)\n\n        print_rank_0(f\"initialized {__class__.__name__} with args: {locals()}\", force=False)\n\n        if dist.get_rank() == 0:\n            logger.info(f\"Reduce bucket size {reduce_bucket_size}\")\n            logger.info(f\"Prefetch bucket size {prefetch_bucket_size}\")\n        # The fused optimizer does all the work. We need this layer for two reason:\n        # 1. maintain same user API from apex.fp16_utils\n        # 2. keep common stuff here in case we need to add ne552w fused optimizer later\n\n        # differences from apex.fp16_utils:\n        # - assume all model params in fp16\n        # - assume all params requires grad\n        # - flat by groups, not keeping state. TODO: remove state explicitly?\n        # - master grad and unflat master weight never exist. TODO: a way to save out unflat master?\n        if not get_accelerator().is_available():\n            raise SystemError(\"Cannot use fp16 without accelerator.\")\n\n        self.optimizer = init_optimizer\n\n        # Use torch (un)flatten ops\n        self.flatten = _flatten_dense_tensors\n        self.unflatten = _unflatten_dense_tensors\n        self.dtype = self.optimizer.param_groups[0]['params'][0].dtype\n        self.gradient_accumulation_dtype = gradient_accumulation_dtype\n        self._global_grad_norm = 0.\n\n        self.custom_loss_scaler = False\n        self.external_loss_scale = None\n\n        self.optimizer_swapper = None\n        self.swap_optimizer = False\n\n        self.offload_optimizer = False\n        self.offload_optimizer_pin_memory = False\n        self.offload_optimizer_fast_init = False\n        self.offload_param = False\n        self.offload_param_pin_memory = False\n        self.params_in_nvme_and_cpu = False\n        self.max_params_in_cpu = 0\n        self.partial_offload = offload_ratio\n\n        #num of ranks in a ZeRO param partitioning group\n        self.zero_hpz_partition_size = zero_hpz_partition_size\n\n        zero_param_parallel_group = groups._get_zero_param_intra_parallel_group()\n        print_rank_0(\n            f\"ZeRO Stage 3 param partitioning group {self.zero_hpz_partition_size} {zero_param_parallel_group}\",\n            force=False)\n        if self.zero_hpz_partition_size > 1 and zero_param_parallel_group is None:\n            self._set_zero_group_parallelism()\n            zero_param_parallel_group = groups._get_zero_param_intra_parallel_group()\n\n        self.parameter_offload = self.initialize_ds_offload(\n            module=module,\n            timers=timers,\n            ds_config=ds_config,\n            overlap_comm=overlap_comm,\n            prefetch_bucket_size=prefetch_bucket_size,\n            max_reuse_distance=max_reuse_distance,\n            max_live_parameters=max_live_parameters,\n            param_persistence_threshold=param_persistence_threshold,\n            model_persistence_threshold=model_persistence_threshold,\n            dp_process_group=dp_process_group,\n            offload_param_config=offload_param_config,\n            mpu=mpu,\n            zero_param_parallel_group=zero_param_parallel_group,\n            zero_quantized_weights=zero_quantized_weights,\n            zero_quantized_nontrainable_weights=zero_quantized_nontrainable_weights)\n\n        self.persistent_parameters = self.parameter_offload.persistent_parameters\n        self._configure_offloading(offload_optimizer_config, offload_param_config)\n\n        # backup fused_adam optimizer init\n        if self.offload_optimizer and self.partial_offload != 1.0:\n            backup_gpu_tensor = torch.randn(1, device=get_accelerator().device_name()).to(self.dtype)\n            backup_gpu_param = torch.nn.Parameter(backup_gpu_tensor)\n            assert type(init_optimizer) == DeepSpeedCPUAdam, 'Hybrid Optimizer Only Supports DeepSpeedCPUAdam'\n            self.backup_optimizer = torch.optim.AdamW([backup_gpu_param],\n                                                      lr=self.optimizer.param_groups[0][\"lr\"],\n                                                      betas=self.optimizer.param_groups[0][\"betas\"],\n                                                      eps=self.optimizer.param_groups[0][\"eps\"],\n                                                      weight_decay=self.optimizer.param_groups[0][\"weight_decay\"],\n                                                      amsgrad=self.optimizer.param_groups[0][\"amsgrad\"])\n            # Multiple param_groups configs for back-up optimizer\n            if len(self.optimizer.param_groups) > 1:\n                for i in range(1, len(self.optimizer.param_groups)):\n                    self.backup_optimizer.add_param_group(self.optimizer.param_groups[i])\n\n        self.module = module\n        self.elastic_checkpoint = elastic_checkpoint\n\n        self.inf_or_nan_tracker: Tensor = torch.zeros(1,\n                                                      dtype=torch.bool,\n                                                      device=get_accelerator().current_device_name(),\n                                                      requires_grad=False)\n\n        self.deepspeed_adam_offload = (self.offload_optimizer and type(init_optimizer) == DeepSpeedCPUAdam)\n\n        self.device = get_accelerator().current_device_name() if not self.offload_optimizer else OffloadDeviceEnum.cpu\n        ### streams used for overlapping computation with communication\n        self.reduce_and_partition_stream = None if get_accelerator().is_synchronized_device() else get_accelerator(\n        ).Stream() if overlap_comm else get_accelerator().default_stream()\n\n        ############################################################################\n\n        self.n_caching_allocator_flushes = 0\n\n        #-------------Stage 3 Setup-------------------#\n\n        self.timers = timers\n\n        self.all2all_process_group = all2all_process_group\n\n        self.reduce_scatter = reduce_scatter\n\n        self.dp_process_group = self.parameter_offload.dp_process_group\n        self.sequence_parallel_size = groups._get_sequence_parallel_world_size()\n\n        self.all2all_process_group = all2all_process_group\n\n        self.zero_quantized_nontrainable_weights = zero_quantized_nontrainable_weights\n\n        self.partition_count = dist.get_world_size(group=self.dp_process_group)\n\n        if mpu is None:\n            self.model_parallel_group = None\n            self.model_parallel_rank = 0\n        else:\n            self.model_parallel_group = mpu.get_model_parallel_group()\n            self.model_parallel_rank = mpu.get_model_parallel_rank()\n\n        self.overflow = False\n        self.clip_grad = clip_grad\n        self.communication_data_type = communication_data_type\n        self.gradient_predivide_factor = gradient_predivide_factor\n        self.postscale_gradients = postscale_gradients\n        self.gradient_accumulation_steps = gradient_accumulation_steps\n        self.micro_step_id = 0\n        self.reduce_bucket_size = int(reduce_bucket_size)\n\n        if self.all2all_process_group is not None:\n            assert self.all2all_process_group is not None and self.reduce_scatter == True, \"when enable all_to_all_reduce, reduce_scatter should also be enabled for data type checks.\"\n\n        if self.reduce_scatter:\n            valid_reduce_scatter_dtypes = (torch.float16, torch.bfloat16, torch.float32)\n            assert self.communication_data_type in valid_reduce_scatter_dtypes, f\"ZeRO-3 supports {valid_reduce_scatter_dtypes} communication_data_type with reduce scatter enabled. Got: '{self.communication_data_type}'\"\n            assert self.gradient_predivide_factor == 1.0, \"gradient_predivide_factor != 1.0 is not yet supported with ZeRO-3 with reduce scatter enabled\"\n            assert self.postscale_gradients, \"pre-scale gradients is not yet supported with ZeRO-3 with reduce scatter enabled\"\n\n        # Holds the mode parameter\n        # The param.data may not hold any meaningful data\n        # when param's status is NOT_AVAILABLE or IN_FLGHT\n        self.fp16_groups = []\n\n        # Hold partitioned parameters\n        self.fp16_partitioned_groups = []\n\n        # Holds a fused and flattened copy of the parameters\n        self.fp16_partitioned_groups_flat = []\n        self.fp16_partitioned_groups_flat_numel = []\n        self.fp16_partitioned_groups_flat_id = []\n\n        #defragmented pinned memory\n        self.param_groups_fp16_flat_cpu_memory = []\n\n        #a single 32-bit partition of the parallel partitioned parameters\n        #that this process will update\n        self.fp32_partitioned_groups_flat = []\n        self.next_swappable_fp32_partitioned_groups = []\n\n        # number of elements per partition in each group\n        self.partition_size = []\n\n        self.all_reduce_print = False\n\n        self.prefetch_elements = int(prefetch_bucket_size)\n\n        self.contiguous_gradients = contiguous_gradients\n\n        # padding on each partition for alignment purposes\n        self.groups_padding = []\n\n        self.sub_group_size = sub_group_size\n\n        self.sub_group_to_group_id = {}\n\n        # Trainable parameters\n        self.trainable_param_groups = self._get_trainable_parameter_groups()\n\n        see_memory_usage(\"Before creating fp16 partitions\", force=True)\n        self._create_fp16_partitions_with_defragmentation(self.trainable_param_groups)\n        num_fp16_subgroups = len(self.fp16_partitioned_groups_flat)\n        see_memory_usage(f\"After creating fp16 partitions: {num_fp16_subgroups}\", force=True)\n\n        # Optimizer tensor swapping\n        if self.swap_optimizer:\n            self._configure_tensor_swapping(offload_optimizer_config, aio_config)\n\n        self.is_gradient_accumulation_boundary: bool = True\n\n        self.param_reduce_events: Deque[get_accelerator().Event] = collections.deque()\n        # TODO. make this configurable via JSON\n        self.max_param_reduce_events: int = 2\n\n        self.param_dict = {}\n\n        # map between param_id and bool to specify if a param is in this partition\n        self.is_param_in_current_partition = {}\n\n        self.extra_large_param_to_reduce = None\n        self.grads_in_ipg_bucket = []\n        self.params_in_ipg_bucket = []\n\n        self.params_already_reduced = {}\n        self.is_gradient_accumulation_boundary = True\n        self._release_ipg_buffers()\n        self.previous_reduced_grads = None\n\n        # model parameter traversal-based param id that's stable across runs\n        for params_group in self.fp16_groups:\n            for param in params_group:\n                param_id = self.get_param_id(param)\n                self.param_dict[param_id] = param\n                self.params_already_reduced[param_id] = False\n\n        #Largest partitioned param\n        largest_partitioned_param_numel = 0\n        for fp16_partitioned_group in self.fp16_partitioned_groups:\n            if len(fp16_partitioned_group) > 0:\n                largest_partitioned_param_numel = max(\n                    largest_partitioned_param_numel,\n                    max([max(tensor.numel(), tensor.ds_numel) for tensor in fp16_partitioned_group]))\n\n        print_rank_0(f'Largest partitioned param numel = {largest_partitioned_param_numel}', force=False)\n\n        self._setup_for_real_optimizer()\n        self.grad_position = {}\n        self.set_grad_positions()\n\n        if self.offload_optimizer:\n            self.norm_for_param_grads = {}\n\n        # stores if a partition has been reduced in this step\n        self.is_partition_reduced = {}\n\n        # stores if a grad in a partition has been computed or not\n        self.is_grad_computed = {}\n\n        # will store the averaged gradients required by this partition\n        self.averaged_gradients = {}\n\n        #creates backward hooks for gradient partitioning\n        ###Calls all gather param\n        self._grad_acc_hooks = []\n        self._leaf_module_hooks = []\n        self.create_reduce_and_remove_grad_hooks()\n\n        #exit(0)\n\n        # we may have a way of fusing dynamic scale. Do not support for now\n        self.loss_scaler = CreateLossScaler(dtype=self.dtype,\n                                            static_loss_scale=static_loss_scale,\n                                            dynamic_scaling=dynamic_loss_scale,\n                                            dynamic_loss_args=dynamic_loss_args)\n        self.dynamic_loss_scale = self.loss_scaler.dynamic\n\n        self.debug_fp16_grads = [{} for _ in self.fp16_groups]\n\n        self._link_all_hp_params()\n\n        if dist.get_rank(group=self.dp_process_group) == 0:\n            see_memory_usage(f\"After initializing ZeRO optimizer\", force=True)\n\n    def destroy(self):\n        self.parameter_offload.destroy()\n        for hook in self._grad_acc_hooks:\n            hook.remove()\n        for hook in self._leaf_module_hooks:\n            hook.remove()\n        print_rank_0(\"Removed grad acc hooks\", force=False)\n        del self.__ipg_bucket_flat_buffer\n\n    def initialize_ds_offload(\n        self,\n        module,\n        timers,\n        ds_config,\n        overlap_comm,\n        prefetch_bucket_size,\n        max_reuse_distance,\n        max_live_parameters,\n        param_persistence_threshold,\n        model_persistence_threshold,\n        dp_process_group,\n        offload_param_config,\n        mpu,\n        zero_param_parallel_group,\n        zero_quantized_weights,\n        zero_quantized_nontrainable_weights,\n    ):\n        return DeepSpeedZeRoOffload(module=module,\n                                    timers=timers,\n                                    ds_config=ds_config,\n                                    overlap_comm=overlap_comm,\n                                    prefetch_bucket_size=prefetch_bucket_size,\n                                    max_reuse_distance=max_reuse_distance,\n                                    max_live_parameters=max_live_parameters,\n                                    param_persistence_threshold=param_persistence_threshold,\n                                    model_persistence_threshold=model_persistence_threshold,\n                                    dp_process_group=dp_process_group,\n                                    offload_param_config=offload_param_config,\n                                    mpu=mpu,\n                                    zero_param_parallel_group=zero_param_parallel_group,\n                                    zero_quantized_weights=zero_quantized_weights,\n                                    zero_quantized_nontrainable_weights=zero_quantized_nontrainable_weights)\n\n    def _get_trainable_parameter_groups(self):\n        param_groups = []\n        PARAMS_KEY = \"params\"\n        for param_group in self.optimizer.param_groups:\n            trainable_params = [p for p in param_group[PARAMS_KEY] if p.requires_grad]\n            if len(trainable_params) == 0:\n                continue\n\n            trainable_param_group = {}\n            for key in param_group.keys():\n                if key == PARAMS_KEY:\n                    trainable_param_group[PARAMS_KEY] = trainable_params\n                else:\n                    trainable_param_group[key] = param_group[key]\n            param_groups.append(trainable_param_group)\n\n        return param_groups\n\n    def _set_zero_group_parallelism(self):\n        groups._create_zero_param_parallel_group(self.zero_hpz_partition_size)\n\n    def invalidate_secondary_tensor(self):\n        for fpg in self.fp16_groups:\n            for param in fpg:\n                if param.ds_secondary_tensor is not None:\n                    param.ds_secondary_tensor = None\n\n    def _setup_for_real_optimizer(self):\n        see_memory_usage(\"Before creating fp32 partitions\", force=True)\n        self._create_fp32_partitions()\n        see_memory_usage(\"After creating fp32 partitions\", force=True)\n        dist.barrier()\n\n        # To support pipelined optimizer swapping\n        self._create_next_swappable_fp32_groups()\n\n        see_memory_usage(\"Before initializing optimizer states\", force=True)\n\n        self.initialize_optimizer_states()\n        see_memory_usage(\"After initializing optimizer states\", force=True)\n        dist.barrier()\n\n        if dist.get_rank() == 0:\n            logger.info(f\"optimizer state initialized\")\n\n        # IPG\n        if self.contiguous_gradients:\n            self.__ipg_bucket_flat_buffer: Tensor = torch.empty(self.reduce_bucket_size,\n                                                                dtype=self.dtype,\n                                                                device=get_accelerator().current_device_name())\n\n        self.grad_partitions_flat_buffer = None\n        self.__param_id_to_grad_partition: Dict[int, Tensor] = {}\n\n        all_params = list(itertools.chain.from_iterable(self.fp16_groups))\n\n        self.grad_partitions_flat_buffer: Tensor = torch.zeros(sum(p.partition_numel() for p in all_params),\n                                                               dtype=self.gradient_accumulation_dtype,\n                                                               device=self.device)\n        if self.offload_optimizer_pin_memory:\n            self.grad_partitions_flat_buffer = get_accelerator().pin_memory(self.grad_partitions_flat_buffer)\n\n        offset = 0\n        for param in all_params:\n            self.__param_id_to_grad_partition[param.ds_id] = self.grad_partitions_flat_buffer.narrow(\n                0, offset, param.partition_numel())\n            offset += param.partition_numel()\n\n    def _link_all_hp_params(self):\n        for p in self.module.parameters():\n            p._z3_optimizer = self\n\n    def set_lr(self, lr):\n        \"\"\"Set the learning rate.\"\"\"\n        for param_group in self.optimizer.param_groups:\n            param_group[\"lr\"] = lr\n\n    def get_lr(self):\n        \"\"\"Return the current learning rate.\"\"\"\n        return self.optimizer.param_groups[0][\"lr\"]\n\n    # TODO. factor out to a utility outside of stage3\n    @staticmethod\n    def defragment(tensors: List[Tensor]) -> Tensor:\n        \"\"\"move provided tensors into a contiguous flat buffer, with some additional\n        measures taken to reduce memory fragmentation\"\"\"\n        assert len(set(t.dtype for t in tensors)) == 1\n        assert len(set(t.device for t in tensors)) == 1\n\n        cpu_buffer = torch.empty(sum(p.numel() for p in tensors),\n                                 dtype=get_only_unique_item(t.dtype for t in tensors),\n                                 device=\"cpu\")\n        tensor_infos: List[Tuple[Tensor, int, int]] = []\n        orig_device = get_only_unique_item(t.device for t in tensors)\n\n        offset = 0\n        for tensor in tensors:\n            tensor_numel = tensor.numel()\n            # move the tensor from device memory to host memory\n            cpu_buffer.narrow(0, offset, tensor_numel).copy_(tensor)\n            tensor.data = torch.empty(0, dtype=tensor.dtype, device=tensor.device)\n\n            # record some data so we can restore the device tensor later\n            tensor_infos.append((tensor, offset, tensor_numel))\n\n            offset += tensor_numel\n\n        gc.collect()\n        get_accelerator().empty_cache()\n\n        # copy tensors (now flattened and contiguous) back to GPU\n        device_buffer = cpu_buffer.to(orig_device)\n\n        # restore device tensors\n        for tensor, offset, tensor_numel in tensor_infos:\n            tensor.data = device_buffer.narrow(0, offset, tensor_numel)\n\n        return device_buffer\n\n    def _get_param_coordinator(self, training):\n        return self.parameter_offload.get_param_coordinator(training)\n\n    def _configure_offloading(self, offload_optimizer_config, offload_param_config):\n        ###################### offload optimizer setup ##################################\n        if offload_optimizer_config is not None and offload_optimizer_config.device != OffloadDeviceEnum.none:\n            self.offload_optimizer = True\n            self.offload_optimizer_pin_memory = offload_optimizer_config.pin_memory\n            self.swap_optimizer = offload_optimizer_config.device == OffloadDeviceEnum.nvme\n            self.offload_optimizer_fast_init = offload_optimizer_config.fast_init\n\n        ###################### offload param setup ##################################\n        if offload_param_config is not None and offload_param_config.device != OffloadDeviceEnum.none:\n            self.offload_param = True\n            self.offload_param_pin_memory = offload_param_config.pin_memory\n            self.params_in_nvme_and_cpu = offload_param_config.device == OffloadDeviceEnum.nvme\n            self.max_params_in_cpu = offload_param_config.max_in_cpu\n            print_rank_0(\n                f\"FP16 params swapping is {self.params_in_nvme_and_cpu}, Max params in CPU is {self.max_params_in_cpu}\",\n                force=False)\n\n    def _configure_tensor_swapping(self, offload_optimizer_config, aio_config):\n        nvme_swap_folder = os.path.join(offload_optimizer_config.nvme_path, 'zero_stage_3')\n        os.makedirs(nvme_swap_folder, exist_ok=True)\n        if dist.get_rank() == 0:\n            logger.info(f'Tensor Swapping: Adding optimizer tensors')\n\n        swapper_type = PipelinedOptimizerSwapper if offload_optimizer_config.pipeline else PartitionedOptimizerSwapper\n\n        self.optimizer_swapper = swapper_type(swap_config=offload_optimizer_config,\n                                              aio_config=aio_config,\n                                              base_folder=nvme_swap_folder,\n                                              optimizer=self.optimizer,\n                                              largest_numel=max(self.fp16_partitioned_groups_flat_numel),\n                                              device=self.device,\n                                              dtype=torch.float32,\n                                              timers=self.timers)\n\n    @property\n    def elements_in_ipg_bucket(self):\n        return sum(p.ds_numel for p in self.params_in_ipg_bucket)\n\n    def _move_to_flat_buffer(self, param_list, flat_buffer, avoid_copy=False):\n        '''If flat buffer is None then the parameters in the param_list are\n        not copied to the flat buffer. This is because they exceed the number of max_params_in_cpu\n        Some of these parameters may already be in CPU in unflattened buffers\n        or they maybe in GPU, or they maybe in NVME. If they are in NVME, then\n        they will be marked as NOT_AVAILABLE, and will be moved to CPU when they are\n        needed during training.'''\n        if flat_buffer is None:\n            # this dst buffer is on NVMe, so skip this\n            return\n\n        start = 0\n        for param in param_list:\n            src = param.ds_tensor\n            dest = flat_buffer.narrow(0, start, src.ds_numel)\n            start = start + src.ds_numel\n            '''if the parameter was initialized in nvme then bring it to the destination buffer directly'''\n            if src.status == PartitionedParamStatus.NOT_AVAILABLE:\n                print_rank_0(\n                    f\"Swapping in {param.ds_id} with partition size {param.partition_numel()} permanently to CPU\")\n                param.nvme_swapper.swap_into_buffer(param, dest)\n                src.data = dest.data\n                src.status = PartitionedParamStatus.AVAILABLE\n            else:\n                assert src.status == PartitionedParamStatus.AVAILABLE, \"Partitioned Param must be available here\"\n                if not avoid_copy:\n                    dest.data.copy_(src.data)\n                src.data = dest.data\n\n            # Final location must be gpu/cpu in this case\n            param.ds_tensor.final_location = 'not-nvme'\n\n    def _create_param_groups_fp16_flat_cpu_memory(self):\n\n        aggregate_params_count = 0\n\n        for j, param_group in enumerate(self.trainable_param_groups):\n            params_in_group = sum([p.partition_numel() for p in param_group['params']])\n\n            flat_buffer_size = params_in_group\n\n            if self.params_in_nvme_and_cpu and \\\n                aggregate_params_count + params_in_group > self.max_params_in_cpu:\n\n                flat_buffer_size = max(0, self.max_params_in_cpu - aggregate_params_count)\n\n            aggregate_params_count += params_in_group\n\n            if flat_buffer_size > 0:\n                print_rank_0(f\"group {j} flat buffer size {flat_buffer_size}\", force=False)\n                self.param_groups_fp16_flat_cpu_memory.append(get_accelerator().pin_memory(\n                    torch.empty(int(flat_buffer_size), dtype=self.dtype)))\n            else:\n                print_rank_0(f\"No flat buffer size. Param group size was  {params_in_group}\", force=False)\n\n                self.param_groups_fp16_flat_cpu_memory.append(torch.empty(1, dtype=self.dtype))\n\n    def _create_fp16_partitions_with_defragmentation(self, fp16_param_groups):\n        dist.barrier()\n\n        param_groups: List[List[Parameter]] = tuple(\n            self._create_fp16_sub_groups(param_group[\"params\"]) for param_group in fp16_param_groups)\n\n        # bookkeeping related to param groups\n        for param_group_idx, param_group in enumerate(param_groups):\n            for sub_group in param_group:\n                sub_group_idx = len(self.fp16_groups)\n\n                # record sub group and partitions\n                self.fp16_groups.append(sub_group)\n                self.fp16_partitioned_groups.append([param.ds_tensor for param in sub_group])\n\n                # record sub group -> group mapping\n                self.sub_group_to_group_id[sub_group_idx] = param_group_idx\n\n                # record total elements of parameter partitions in sub group\n                self.fp16_partitioned_groups_flat_numel.append(sum(p.partition_numel() for p in sub_group))\n\n                # record ds_ids of parameter partitions in sub group\n                self.fp16_partitioned_groups_flat_id.append([p.ds_id for p in sub_group])\n\n                # record padding required to align group to world size (only applies to last rank)\n                rank_requires_padding = dist.get_rank(\n                    self.dp_process_group) == dist.get_world_size(self.dp_process_group) - 1\n                self.groups_padding.append([p.padding_size() if rank_requires_padding else 0 for p in sub_group])\n\n        # move parameters to flattened buffer\n        if not self.offload_param:  # partitioned params remain in GPU during training\n            # move parameter partitions into a single contiguous flat buffer\n            parameter_partitions: List[Tensor] = []\n            for sub_group in self.fp16_groups:\n                for param in sub_group:\n                    parameter_partitions.append(param.ds_tensor)\n            device_buffer = __class__.defragment(parameter_partitions)\n\n            # setup flat buffers per subgroup, these are each just sections of the\n            # contiguous flat buffer for all parameters that we created earlier\n            offset = 0\n            for sub_group in self.fp16_groups:\n                sub_group_numel = sum(param.partition_numel() for param in sub_group)\n                self.fp16_partitioned_groups_flat.append(device_buffer.narrow(0, offset, sub_group_numel))\n                offset += sub_group_numel\n        else:  # partitioned params offloaded to CPU when not in use\n            # create a flat CPU memory allocation for each param group\n            self._create_param_groups_fp16_flat_cpu_memory()\n            for param_group_idx, param_group in enumerate(param_groups):\n                flat_offset = 0\n                for i, sub_group in enumerate(param_group):\n                    total_elements = sum(p.partition_numel() for p in sub_group)\n                    print_rank_0(f\"Params in nvme and cpu {self.params_in_nvme_and_cpu}\")\n                    #Flat buffer may not be available for parameters that reside in NVME\n                    if not self.params_in_nvme_and_cpu or flat_offset + total_elements <= self.param_groups_fp16_flat_cpu_memory[\n                            param_group_idx].numel():\n                        fp16_partitioned_group_flat = self.param_groups_fp16_flat_cpu_memory[param_group_idx].narrow(\n                            0, flat_offset, total_elements)\n                        print_rank_0(\n                            f\"Creating a flat buffer for subgroup {i} requiring {total_elements} elements, and cumulative CPU elements {flat_offset + total_elements}\",\n                            force=False)\n\n                    elif self.params_in_nvme_and_cpu:\n                        fp16_partitioned_group_flat = None\n                        print_rank_0(f\"No flat buffer for sub group {i} of {total_elements} elements\", force=False)\n                    else:\n                        assert False, \"Either params are in nvme, or they are in CPU memory. This code path should not be triggered. Please see you max_params_in_cpu and params_in_nvme configs\"\n\n                    self.fp16_partitioned_groups_flat.append(fp16_partitioned_group_flat)\n                    flat_offset += total_elements\n\n                    self._move_to_flat_buffer(sub_group,\n                                              fp16_partitioned_group_flat,\n                                              avoid_copy=not self.offload_param)\n\n        # if necessary, create a pinned memory buffer to be used for swapping out\n        # params to NVME after optimizer step\n        should_create_fp16_flat_reuse_buffer = any(flattened_partition_group is None\n                                                   for flattened_partition_group in self.fp16_partitioned_groups_flat)\n        if should_create_fp16_flat_reuse_buffer:\n            max_partition_numel, largest_partition_numel = 0, None\n            for sub_group in self.fp16_groups:\n                total_elements = sum(t.partition_numel() for t in sub_group)\n                if total_elements > max_partition_numel:\n                    largest_partition_numel = [t.ds_numel for t in sub_group]\n                    max_partition_numel = total_elements\n\n            assert len(largest_partition_numel) > 0, f'Unexpected that largest partition is empty'\n            self.fp16_groups[0][0].nvme_swapper.reserve_partitioned_swap_space(largest_partition_numel)\n\n    def _swap_in_sub_group_to_flat_buffer(self, flat_buffer, sub_group_id):\n        offset = 0\n        elements_in_sub_group = sum([t.ds_numel for t in self.fp16_partitioned_groups[sub_group_id]])\n        assert (flat_buffer.numel() == elements_in_sub_group)\n        for param, partitioned_param in zip(self.fp16_groups[sub_group_id],\n                                            self.fp16_partitioned_groups[sub_group_id]):\n            dest = flat_buffer.narrow(0, offset, partitioned_param.ds_numel)\n            if partitioned_param.status == PartitionedParamStatus.NOT_AVAILABLE:\n                print_rank_0(\n                    f\"Swapping in {param.ds_id} with elements {param.ds_numel} and partition {param.partition_numel()}\"\n                )\n                param.nvme_swapper.swap_in([param], async_op=False)\n                dest.data.copy_(partitioned_param.data)\n                param.nvme_swapper.remove_partition_and_release_buffers([param])\n                print_rank_0(f\"Swapping in {param.ds_id} done\")\n            else:\n                dest.data.copy_(partitioned_param.data)\n            offset += partitioned_param.ds_numel\n\n    def _create_next_swappable_fp32_groups(self):\n        reverse_order_indices = [i for i in range(len(self.fp32_partitioned_groups_flat))]\n        reverse_order_indices.reverse()\n\n        next_group = None\n        for i in reverse_order_indices:\n            self.next_swappable_fp32_partitioned_groups.append(next_group)\n            if self._swappable_optimizer_subgroup(i):\n                next_group = self.fp32_partitioned_groups_flat[i]\n\n        self.next_swappable_fp32_partitioned_groups.reverse()\n\n    def _get_sub_group_partitions(self, sub_group_id):\n        sub_group_partitions = []\n        for param, partitioned_param in zip(self.fp16_groups[sub_group_id],\n                                            self.fp16_partitioned_groups[sub_group_id]):\n            if partitioned_param.status == PartitionedParamStatus.NOT_AVAILABLE:\n                swap_path = param.nvme_swapper.get_path(param, True)\n                sub_group_partitions.append((partitioned_param, param.partition_numel(), swap_path))\n            else:\n                sub_group_partitions.append((partitioned_param, partitioned_param.ds_numel, None))\n\n        return sub_group_partitions\n\n    def _create_fp32_partitions(self):\n        cpu_memory_usage = 0\n        cpu_memory_sub_groups = 0\n        nvme_memory_usage = 0\n        num_swappable_partitions = 0\n        num_swap_from_nvme_partitions = 0\n        num_swap_from_cpu_partitions = 0\n        swap_from_nvme_memory_usage = 0\n        swap_from_cpu_memory_usage = 0\n        GIGA_BYTES = (1024**3)\n\n        swappable_fp32_tensors = []\n        swappable_fp16_src_tensors = []\n        nvme_fp16_partitions_info = []\n        nvme_fp16_num_elems = []\n        nvme_fp32_dest_tensors = []\n        fp32_element_size = torch.tensor([], dtype=torch.float32).element_size()\n\n        # Assign portion of subgroup to cpu, the other to gpu.\n        if self.offload_optimizer:\n            self.subgroup_to_device = {}\n            sub_group_size = len(self.fp16_partitioned_groups_flat)\n            # print(f\"Partial offload sub_group_size is {sub_group_size}, ratio is {self.partial_offload}\\n\")\n            for i in range(sub_group_size):\n                if i < int(self.partial_offload * sub_group_size):\n                    self.subgroup_to_device[i] = 'cpu'\n                else:\n                    self.subgroup_to_device[i] = get_accelerator()._name\n\n        for i, tensor in enumerate(self.fp16_partitioned_groups_flat):\n            num_elements = self.fp16_partitioned_groups_flat_numel[i]\n\n            # a partition of the fp32 master weights that will be updated by this process\n            if self._swappable_optimizer_subgroup(i):\n                self.fp32_partitioned_groups_flat.append(torch.Tensor())\n                nvme_memory_usage += (fp32_element_size * num_elements)\n                num_swappable_partitions += 1\n\n                if self.params_in_nvme_and_cpu and tensor is None:\n                    num_swap_from_nvme_partitions += 1\n                    swap_from_nvme_memory_usage += (fp32_element_size * num_elements)\n                    if self.offload_optimizer_fast_init:\n                        sub_group_partitions = self._get_sub_group_partitions(i)\n                        nvme_fp16_partitions_info.append(sub_group_partitions)\n                        nvme_fp16_num_elems.append(num_elements)\n                        nvme_fp32_dest_tensors.append(self.fp32_partitioned_groups_flat[i])\n                    else:\n                        unpinned_fp32_buffer = torch.empty(num_elements, device=self.device, dtype=torch.float)\n                        self._swap_in_sub_group_to_flat_buffer(unpinned_fp32_buffer, i)\n                        self.optimizer_swapper.initialize_parameters(parameters=[self.fp32_partitioned_groups_flat[i]],\n                                                                     src_tensors=[unpinned_fp32_buffer])\n                else:\n                    num_swap_from_cpu_partitions += 1\n                    swap_from_cpu_memory_usage += (fp32_element_size * num_elements)\n                    swappable_fp32_tensors.append(self.fp32_partitioned_groups_flat[i])\n                    swappable_fp16_src_tensors.append(self.fp16_partitioned_groups_flat[i])\n            else:\n                cpu_memory_usage += (fp32_element_size * num_elements)\n                cpu_memory_sub_groups += 1\n\n                if self.params_in_nvme_and_cpu and tensor is None:\n                    unpinned_fp32_buffer = torch.empty(num_elements, device=self.device, dtype=torch.float)\n                    self._swap_in_sub_group_to_flat_buffer(unpinned_fp32_buffer, i)\n                    self.fp32_partitioned_groups_flat.append(unpinned_fp32_buffer)\n                else:\n                    if self.offload_optimizer:\n                        self.fp32_partitioned_groups_flat.append(self.fp16_partitioned_groups_flat[i].to(\n                            self.subgroup_to_device[i]).clone().float().detach())\n                    else:\n                        self.fp32_partitioned_groups_flat.append(self.fp16_partitioned_groups_flat[i].to(\n                            self.device).clone().float().detach())\n\n            self.fp32_partitioned_groups_flat[i].requires_grad = True  # keep this in case internal optimizer uses it\n            ds_id_begin = str(self.fp16_partitioned_groups_flat_id[i][0])\n            ds_id_end = str(self.fp16_partitioned_groups_flat_id[i][-1])\n            self.fp32_partitioned_groups_flat[i].ds_id = ds_id_begin + '_' + ds_id_end\n\n        if len(swappable_fp32_tensors) > 0:\n            self.optimizer_swapper.initialize_parameters(parameters=swappable_fp32_tensors,\n                                                         src_tensors=swappable_fp16_src_tensors)\n\n        if len(nvme_fp32_dest_tensors) > 0:\n            fp16_pinned_buffers = self.fp16_groups[0][0].nvme_swapper.reserve_available_buffers()\n            assert len(fp16_pinned_buffers) > 0\n            self.optimizer_swapper.initialize_from_swapped_fp16_params(fp16_partitions_info=nvme_fp16_partitions_info,\n                                                                       fp16_num_elems=nvme_fp16_num_elems,\n                                                                       fp16_pinned_buffers=fp16_pinned_buffers,\n                                                                       fp32_parameters=nvme_fp32_dest_tensors)\n            self.fp16_groups[0][0].nvme_swapper.release_reserved_buffers()\n\n        nvme_gigabytes = nvme_memory_usage / GIGA_BYTES\n        print_rank_0(f'Swappable FP32 Partitions: count={num_swappable_partitions} size={nvme_gigabytes:5.2f} GB',\n                     force=False)\n        if self.params_in_nvme_and_cpu:\n            print_rank_0(\n                f'Swap from NVMe Partitions: count = {num_swap_from_nvme_partitions}, size = {swap_from_nvme_memory_usage/GIGA_BYTES:5.2f}GB',\n                force=False)\n            print_rank_0(\n                f'Swap from CPU Partitions: count = {num_swap_from_cpu_partitions}, size = {swap_from_cpu_memory_usage/GIGA_BYTES:5.2f}GB',\n                force=False)\n\n        cpu_memory_gigabytes = cpu_memory_usage / GIGA_BYTES\n        print_rank_0(f'In-Memory FP32 Partitions: count={cpu_memory_sub_groups} size={cpu_memory_gigabytes:5.2f} GB',\n                     force=False)\n\n        # Clear for on-the-fly population before the optimizer step\n        for param_group in self.optimizer.param_groups:\n            param_group['params'] = []\n\n    def _create_fp16_sub_groups(self, params_group):\n\n        params_group_numel = sum([param.partition_numel() for param in params_group])\n        sub_group_size = self.sub_group_size\n\n        if sub_group_size is None or sub_group_size >= params_group_numel:\n            return [params_group]\n\n        sub_groups = []\n        sub_group = []\n        local_sub_group_size = 0\n        for param in params_group:\n\n            sub_group.append(param)\n            local_sub_group_size += param.partition_numel()\n\n            if local_sub_group_size >= sub_group_size or id(param) == id(params_group[-1]):\n\n                sub_groups.append(sub_group)\n\n                sub_group = []\n                local_sub_group_size = 0\n\n        return sub_groups\n\n    def _release_ipg_buffers(self):\n        if self.contiguous_gradients:\n            self.ipg_buffer = None\n\n    def _optimizer_step(self, sub_group_id):\n        param_group_id = self.sub_group_to_group_id[sub_group_id]\n        fp32_param = self.fp32_partitioned_groups_flat[sub_group_id]\n        if self.offload_optimizer:\n            cur_device = self.subgroup_to_device[sub_group_id]\n            if cur_device == 'cpu':\n                self.optimizer.param_groups[param_group_id]['params'] = [fp32_param]\n                cpu_loss = self.optimizer.step()\n                self.optimizer.param_groups[param_group_id]['params'] = []\n            else:\n                self.backup_optimizer.param_groups[param_group_id]['params'] = [fp32_param]\n                gpu_loss = self.backup_optimizer.step()\n                self.backup_optimizer.param_groups[param_group_id]['params'] = []\n        else:\n            self.optimizer.param_groups[param_group_id]['params'] = [fp32_param]\n            self.optimizer.step()\n            self.optimizer.param_groups[param_group_id]['params'] = []\n\n    def _swappable_optimizer_subgroup(self, sub_group_id):\n        if not self.swap_optimizer:\n            return False\n\n        return self.optimizer_swapper.swappable_tensor(None,\n                                                       numel=self.fp16_partitioned_groups_flat_numel[sub_group_id])\n\n    def _partitioned_params_swap_out(self, i):\n        offset = 0\n        fp32_param = self.fp32_partitioned_groups_flat[i]\n        assert fp32_param is not None, \\\n        f'fp32 parameters of sub_group {i} is None'\n\n        swap_fp16_params = []\n        swap_fp32_params = []\n        for param, partitioned_param in zip(self.fp16_groups[i], self.fp16_partitioned_groups[i]):\n            src = fp32_param.narrow(0, offset, partitioned_param.ds_numel)\n            if partitioned_param.status == PartitionedParamStatus.AVAILABLE:\n                partitioned_param.data.copy_(src.data)\n            else:\n                swap_fp32_params.append(src)\n                swap_fp16_params.append(param)\n            offset += partitioned_param.ds_numel\n\n        if len(swap_fp16_params):\n            swap_fp16_params[0].nvme_swapper.swap_out_partitioned_params(dst_fp16_params=swap_fp16_params,\n                                                                         src_fp32_params=swap_fp32_params)\n\n    def initialize_optimizer_states(self):\n        num_subgroups = len(self.fp16_groups)\n\n        largest_numel = max([sum([p.ds_numel for p in psg]) for psg in self.fp16_partitioned_groups])\n        gradient_dtype = self.fp32_partitioned_groups_flat[0].dtype\n        gradient_buffer = torch.zeros(int(largest_numel), dtype=gradient_dtype, device=self.device)\n\n        timer_names = set()\n\n        # State initialization for the Adagrad optimizer occurs at construction as opposed to other optimizers\n        # which do lazy initialization of the state at the first call to step.\n        is_adagrad = isinstance(self.optimizer, torch.optim.Adagrad)\n\n        if self.swap_optimizer:\n            self.optimizer_swapper.init_timers()\n\n        timer_names.add(INIT_OPTIMIZER_TIMER)\n        self.timers(INIT_OPTIMIZER_TIMER).start()\n\n        for i, group in enumerate(self.fp16_groups):\n            swappable_optimizer_subgroup = self._swappable_optimizer_subgroup(i)\n            swappable_param_subgroup = self.fp16_partitioned_groups_flat[i] is None\n\n            num_elements = int(self.fp16_partitioned_groups_flat_numel[i])\n\n            see_memory_usage(\n                f'[Begin] Initialize optimizer states {i} / {num_subgroups} subgroups, num_elems: {num_elements}, swappable opt/param:{swappable_optimizer_subgroup}/{swappable_param_subgroup}',\n                force=False)\n\n            if swappable_optimizer_subgroup:\n                self._optimizer_states_and_gradient_swap_in(i, timer_names)\n\n            if self.offload_optimizer and not swappable_optimizer_subgroup:\n                subgroup_gradient_buffer = torch.zeros(num_elements, dtype=gradient_dtype, device=self.device)\n                if self.offload_optimizer_pin_memory:\n                    subgroup_gradient_buffer = get_accelerator().pin_memory(subgroup_gradient_buffer)\n\n                self.fp32_partitioned_groups_flat[i].grad = subgroup_gradient_buffer.to(self.subgroup_to_device[i])\n            else:\n                self.fp32_partitioned_groups_flat[i].grad = gradient_buffer.narrow(0, 0, num_elements)\n\n            if swappable_param_subgroup:\n                self._partitioned_params_swap_out(i)\n\n            if swappable_optimizer_subgroup:\n                self._optimizer_states_and_gradient_swap_out(i, timer_names)\n\n            see_memory_usage(\n                f'[End] Initialize optimizer states {i} / {num_subgroups} subgroups, num_elems: {num_elements}, swappable opt/param:{swappable_optimizer_subgroup}/{swappable_param_subgroup}',\n                force=False)\n\n        # Initialize the optimizer states with the flattened fp32 partition.\n        if is_adagrad:\n            self.optimizer = torch.optim.Adagrad(self.fp32_partitioned_groups_flat, **self.optimizer.defaults)\n\n        self.timers(INIT_OPTIMIZER_TIMER).stop()\n        self.timers.log(timer_names)\n\n        if self.swap_optimizer:\n            self.optimizer_swapper.log_timers()\n\n        if not self.offload_optimizer:\n            for group in self.fp32_partitioned_groups_flat:\n                group.grad = None\n\n        # Reset steps\n        return\n\n    #########################################################################\n    #########################ZeRO Partition Gradients########################\n    #########################################################################\n\n    def get_first_param_index(self, group_id, param_group, partition_id):\n        for index, param in enumerate(param_group):\n            param_id = self.get_param_id(param)\n            if partition_id in self.param_to_partition_ids[group_id][param_id]:\n                return index\n        return None\n\n    def initialize_gradient_partitioning_data_structures(self):\n\n        total_partitions = dist.get_world_size(group=self.dp_process_group)\n\n        for i, param_group in enumerate(self.fp16_groups):\n\n            self.param_to_partition_ids[i] = {}\n            self.is_partition_reduced[i] = {}\n            self.total_grads_in_partition[i] = {}\n            self.remaining_grads_in_partition[i] = {}\n            self.is_grad_computed[i] = {}\n            self.grad_partition_insertion_offset[i] = {}\n            self.grad_start_offset[i] = {}\n            self.first_param_index_in_partition[i] = {}\n\n            for partition_id in range(total_partitions):\n                self.is_grad_computed[i][partition_id] = {}\n                self.grad_partition_insertion_offset[i][partition_id] = {}\n                self.grad_start_offset[i][partition_id] = {}\n                self.initialize_gradient_partition(i, param_group, partition_id)\n                self.is_partition_reduced[i][partition_id] = False\n                self.first_param_index_in_partition[i][partition_id] = self.get_first_param_index(\n                    i, param_group, partition_id)\n\n    @instrument_w_nvtx\n    def independent_gradient_partition_epilogue(self):\n        self.report_ipg_memory_usage(f\"In ipg_epilogue before reduce_ipg_grads\", 0)\n        self.__reduce_and_partition_ipg_grads()\n        self.report_ipg_memory_usage(f\"In ipg_epilogue after reduce_ipg_grads\", 0)\n\n        if not get_accelerator().resolves_data_dependency():\n            self.reduce_and_partition_stream.synchronize()\n\n        for param_id in self.params_already_reduced.keys():\n            self.params_already_reduced[param_id] = False\n\n        #in case of cpu offload, averaged gradients are already in fp32_partitioned_groups_flat.grad\n        #TODO: use a similar code path for both cpu_offload and non-cpu offload\n        if not self.offload_optimizer:\n            for i, sub_group in enumerate(self.fp16_groups):\n                #TODO: This is redundant\n                self.averaged_gradients[i] = [\n                    self.__param_id_to_grad_partition[param.ds_id]\n                    if param.requires_grad else torch.zeros_like(param.ds_tensor) for param in sub_group\n                ]\n        # this method gets called after every backward. need to increment\n        # here because if it gets incremented in backward() the micro step\n        # id will be off by one when we do the reduce and partition at the.\n        # start of this method.\n        # TODO. make this less error prone\n        self.micro_step_id += 1\n\n    def overlapping_partition_gradients_reduce_epilogue(self):\n        self.independent_gradient_partition_epilogue()\n\n    def create_reduce_and_remove_grad_hooks(self):\n        print_rank_0(f'[Begin] Create gradient reduction hooks')\n        self.grad_accs = []\n        self.leaf_parameters = defaultdict(list)\n        for i, param_group in enumerate(self.fp16_groups):\n            for param in param_group:\n                if param.requires_grad:\n                    #print_rank_0(f\" Before all gather {param.device}, {param.shape}\")\n                    print_rank_0(f\"Before all gather {param.device}, {param.shape}\", force=False)\n\n                    # The hook must be created in un-partitioned parameter\n                    param.all_gather()\n\n                    #print(f\"After all gather {param.device}, {param.shape}\")\n                    def wrapper(param):\n                        param_tmp = param.expand_as(param)\n                        grad_acc = param_tmp.grad_fn.next_functions[0][0]\n\n                        @instrument_w_nvtx\n                        def reduce_partition_and_remove_grads(*notneeded):\n                            self.reduce_ready_partitions_and_remove_grads(param)\n\n                        self._grad_acc_hooks.append(grad_acc.register_hook(reduce_partition_and_remove_grads))\n                        self.grad_accs.append(grad_acc)\n\n                    #print(f\"param grad fn {param.expand_as(param).grad_fn}\")\n                    if z3_leaf_parameter(param):\n                        self.leaf_parameters[param.ds_z3_leaf_module].append(param)\n                    else:\n                        wrapper(param)\n\n                    # Partition the parameter after creating the hook\n                    param.partition()\n\n        # We delay reduce-scatter for all gradients in the leaf modules until the backward pass of the leaf module is done\n        for leaf_module, leaf_parameters in self.leaf_parameters.items():\n\n            def wrapper_pre_hook(params):\n\n                def forward_pre_hook(module, input):\n                    \"\"\"Pre-forward hook to set backward hook on input tensors to the leaf module\"\"\"\n                    module._leaf_module_inputs_remaining = 0\n\n                    @instrument_w_nvtx\n                    def reduce_leaf_module_grads(grad):\n                        module._leaf_module_inputs_remaining -= 1\n                        # Make sure everything is done in the leaf module\n                        if module._leaf_module_inputs_remaining == 0:\n                            for param in params:\n                                if param.grad is None:\n                                    param.grad = torch.zeros_like(param)\n                                self.reduce_ready_partitions_and_remove_grads(param)\n\n                    def set_module_bwd_hook(tensor):\n                        if tensor.requires_grad:\n                            module._leaf_module_inputs_remaining += 1\n                            tensor.register_hook(reduce_leaf_module_grads)\n                        return tensor\n\n                    output = apply_to_tensors_only(set_module_bwd_hook, input)\n\n                    return output\n\n                return forward_pre_hook\n\n            def wrapper_post_hook():\n\n                def forward_post_hook(module, input, output):\n                    \"\"\"Pre-forward hook to set backward hook on input tensors to the leaf module\"\"\"\n                    module._leaf_output_required_grad_num = 0\n\n                    def increment_rg_count_bwd_hook(tensor):\n                        if tensor.requires_grad:\n                            module._leaf_output_required_grad_num += 1\n                        return tensor\n\n                    apply_to_tensors_only(increment_rg_count_bwd_hook, output)\n\n                    if module._leaf_module_inputs_remaining == 0 and module._leaf_output_required_grad_num > 0:\n                        raise RuntimeError(\n                            \"A module cannot be set as a leaf module when it does not have any input tensors that require gradients and has output tensors that require gradients. This is because the gradient reduction hook will not be called in this case.\"\n                        )\n\n                return forward_post_hook\n\n            self._leaf_module_hooks.append(leaf_module.register_forward_pre_hook(wrapper_pre_hook(leaf_parameters)))\n            self._leaf_module_hooks.append(leaf_module.register_forward_hook(wrapper_post_hook()))\n\n        print_rank_0(f'[End] Create gradient reduction hooks')\n\n    def get_param_id(self, param):\n        return OptimizerSwapper.parameter_id(param)\n\n    def report_ipg_memory_usage(self, tag, param_elems):\n        elem_count = self.elements_in_ipg_bucket + param_elems\n        percent_of_bucket_size = (100.0 * elem_count) // self.reduce_bucket_size\n        see_memory_usage(\n            f\"{tag}: elems in_bucket {self.elements_in_ipg_bucket} param {param_elems} max_percent {percent_of_bucket_size}\",\n            force=False)\n\n    ###############Independent Partition Gradient ########################\n    def reduce_independent_p_g_buckets_and_remove_grads(self, param):\n        #print_rank_0(f\"Inside reduce ipg buckets. {debug_param2name_id_shape(param)}, ipg elements {self.elements_in_ipg_bucket}, reduce bucket size {self.reduce_bucket_size}\", force=True)\n\n        # Because the ipg bucket is initialized with a random place holder tensor, we must\n        # explicitly check that the bucket has any real data in it (self.elements_in_ipg_bucket >\n        # 0). Otherwise if the incoming param.ds_numel is large, this branch may get triggered on a\n        # garbage data and `self.average_tensor()` will crash because its params_to_reduce will be\n        # empty, while reduction_list will have that garbage data.\n        if self.elements_in_ipg_bucket + param.ds_numel > self.reduce_bucket_size and self.elements_in_ipg_bucket > 0:\n            self.report_ipg_memory_usage(\"In ipg_remove_grads before reduce_ipg_grads\", param.ds_numel)\n\n            self.__reduce_and_partition_ipg_grads()\n\n        self.__add_grad_to_ipg_bucket(param)\n\n    @instrument_w_nvtx\n    @torch.no_grad()\n    def __add_grad_to_ipg_bucket(self, param: Parameter) -> None:\n        if not get_accelerator().resolves_data_dependency():\n            self.reduce_and_partition_stream.wait_stream(get_accelerator().default_stream())\n\n        if self.contiguous_gradients and self.elements_in_ipg_bucket + param.grad.numel() <= self.reduce_bucket_size:\n            # move the gradient to a contiguous buffer\n            with get_accelerator().stream(self.reduce_and_partition_stream):\n                # move the parameter's gradient to the contiguous flat buffer\n                new_grad_tensor = self.__ipg_bucket_flat_buffer.narrow(0, self.elements_in_ipg_bucket,\n                                                                       param.grad.numel()).view_as(param.grad)\n                new_grad_tensor.copy_(param.grad, non_blocking=True)\n                if not get_accelerator().is_synchronized_device():\n                    param.grad.record_stream(get_accelerator().current_stream())\n                param.grad.data = new_grad_tensor\n\n        self.params_in_ipg_bucket.append(param)\n\n    @instrument_w_nvtx\n    @torch.no_grad()\n    def __reduce_and_partition_ipg_grads(self, safe_mode: bool = False) -> None:\n        if not self.params_in_ipg_bucket:\n            return\n\n        for param in self.params_in_ipg_bucket:\n            if param.grad.numel() != param.ds_numel:\n                raise RuntimeError(f\"{param.grad.numel()} != {param.ds_numel} Cannot reduce scatter \"\n                                   f\"gradients whose size is not same as the params\")\n\n        assert len(set(p.ds_id for p in self.params_in_ipg_bucket)) == len(self.params_in_ipg_bucket)\n\n        while self.param_reduce_events and self.param_reduce_events[0].query():\n            self.param_reduce_events.popleft()\n        if len(self.param_reduce_events) > self.max_param_reduce_events:\n            self.param_reduce_events.popleft().synchronize()\n\n        with get_accelerator().stream(self.reduce_and_partition_stream):\n            if safe_mode:\n                assert_ints_same_as_other_ranks([p.ds_id for p in self.params_in_ipg_bucket])\n\n            if self.contiguous_gradients and self.elements_in_ipg_bucket <= self.reduce_bucket_size and not self.reduce_scatter:\n                grad_bucket = self.__ipg_bucket_flat_buffer.narrow(0, 0, self.elements_in_ipg_bucket)\n                grad_partitions = self.__avg_scatter_contiguous_grads(grad_bucket)\n            else:\n                self.params_in_ipg_bucket.sort(key=lambda p: p.ds_id)\n                grad_partitions = self.__avg_scatter_grads(self.params_in_ipg_bucket)\n\n            self.partition_grads(self.params_in_ipg_bucket, grad_partitions)\n\n            self.params_in_ipg_bucket.clear()\n\n            if not get_accelerator().handles_memory_backpressure():\n                event = get_accelerator().Event()\n                event.record()\n                self.param_reduce_events.append(event)\n\n    @instrument_w_nvtx\n    def __avg_scatter_contiguous_grads(self, buffer_to_reduce: Tensor) -> List[Tensor]:\n        dtype = buffer_to_reduce.dtype\n        if self.communication_data_type != dtype:\n            buffer_to_reduce = buffer_to_reduce.to(self.communication_data_type)\n        if self.postscale_gradients and self.gradient_predivide_factor != 1.0:\n            buffer_to_reduce = buffer_to_reduce.div_(self.gradient_predivide_factor)\n\n        world_sz = dist.get_world_size(self.dp_process_group)\n        rank = dist.get_rank(self.dp_process_group)\n        buffer_to_reduce.div_(world_sz / float(self.sequence_parallel_size))\n\n        dist.all_reduce(buffer_to_reduce, group=self.dp_process_group)\n\n        if self.postscale_gradients and self.gradient_predivide_factor != world_sz:\n            buffer_to_reduce = buffer_to_reduce.mul(self.gradient_predivide_factor)\n\n        if self.communication_data_type != self.dtype:\n            buffer_to_reduce = buffer_to_reduce.to(self.dtype)\n\n        grad_partitions = []\n        grad_offset_in_buffer = 0\n        for param in self.params_in_ipg_bucket:\n            grad = param.grad\n            chunk_sz = math.ceil(grad.numel() / world_sz)\n\n            start_offset = grad_offset_in_buffer + min(rank * chunk_sz, grad.numel())\n            end_offset = grad_offset_in_buffer + min(rank * chunk_sz + chunk_sz, grad.numel())\n\n            partition = buffer_to_reduce[start_offset:end_offset]\n            if param.partition_numel() != partition.numel():\n                padded_partition = torch.zeros(param.partition_numel(), device=grad.device, dtype=grad.dtype)\n                if partition.numel() > 0:\n                    padded_partition[:partition.numel()] = partition\n                grad_partitions.append(padded_partition)\n            else:\n                grad_partitions.append(partition)\n            grad_offset_in_buffer += grad.numel()\n\n        return grad_partitions\n\n    @instrument_w_nvtx\n    def __avg_scatter_grads(self, params_to_reduce: List[Parameter]) -> List[Tensor]:\n        \"\"\"average gradients and scatter partitions across ranks\"\"\"\n\n        full_grads_for_rank = [p.grad for p in params_to_reduce]\n        if self.communication_data_type != self.dtype:\n            full_grads_for_rank = [g.to(self.communication_data_type) for g in full_grads_for_rank]\n\n        if self.postscale_gradients and self.gradient_predivide_factor != 1.0:\n            full_grads_for_rank = [g.div(self.gradient_predivide_factor) for g in full_grads_for_rank]\n\n        local_world_size = get_accelerator().device_count()\n        global_world_size = dist.get_world_size()\n        num_nodes = global_world_size // local_world_size\n        if self.all2all_process_group is not None and num_nodes > 1:\n            grad_partitions_for_rank = all_to_all_quant_reduce(full_grads_for_rank, self.all2all_process_group)\n        else:\n            grad_partitions_for_rank = reduce_scatter_coalesced(full_grads_for_rank, self.dp_process_group)\n\n        if self.postscale_gradients and self.gradient_predivide_factor != 1.0 and self.gradient_predivide_factor != dist.get_world_size(\n                self.dp_process_group):\n            grad_partitions_for_rank = [g.mul(self.gradient_predivide_factor) for g in grad_partitions_for_rank]\n\n        if self.communication_data_type != self.dtype:\n            grad_partitions_for_rank = [g.to(self.dtype) for g in grad_partitions_for_rank]\n\n        return grad_partitions_for_rank\n\n    def set_grad_positions(self):\n        for i, group in enumerate(self.fp16_groups):\n            current_offset = 0\n            for param in group:\n                param_id = self.get_param_id(param)\n                num_elements = param.partition_numel()\n\n                self.grad_position[param_id] = [int(i), int(current_offset), int(num_elements)]\n                #print(f\"param id {param_id} i:{i}, ds_tensor {num_elements} numel {param.numel()}\")\n                current_offset += num_elements\n        see_memory_usage(f\"After Set Grad positions\", force=False)\n\n    def _constant_buffered_norm2(self, input, buffer_size=250000000):\n        norm = None\n        for part in input.view(-1).split(buffer_size):\n            if norm is None:\n                norm = part.data.double().norm(2)**2.0\n            else:\n                norm += part.data.double().norm(2)**2.0\n        return norm**0.5\n\n    def set_norm_for_param_grad_in_gpu(self, param):\n        param_id = self.get_param_id(param)\n        #self.norm_for_param_grads[param_id] = param.grad.data.double().norm(2)\n        #Using a more memory efficient version\n        self.norm_for_param_grads[param_id] = self._constant_buffered_norm2(param.grad)\n\n    def async_inplace_copy_grad_to_fp32_buffer_from_gpu(self, param, fp32_grad_tensor):\n        with get_accelerator().stream(self.copy_grad_stream):\n            param_id = self.get_param_id(param)\n            src_tensor = param.grad.view(-1).float()\n            #print(f\"src_tensor {src_tensor.size()} and fp32 grad {fp32_grad_tensor.size()}\")\n            fp32_grad_tensor.copy_(src_tensor, non_blocking=True)\n            param.grad = None\n\n    def complete_grad_norm_calculation_for_cpu_offload(self, params):\n        total_norm = 0.0\n        norm_type = 2.0\n        for p in params:\n            if is_model_parallel_parameter(p) or (self.model_parallel_rank == 0):\n                param_id = self.get_param_id(p)\n                if param_id in self.norm_for_param_grads.keys():\n                    param_norm = self.norm_for_param_grads[param_id]\n                    total_norm += param_norm**2\n\n        # Sum across all model parallel GPUs.\n        total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])\n\n        dist.all_reduce(total_norm_cuda, op=dist.ReduceOp.SUM, group=self.dp_process_group)\n\n        self._model_parallel_all_reduce(tensor=total_norm_cuda, op=dist.ReduceOp.SUM)\n\n        total_norm = total_norm_cuda[0]**(1. / norm_type)\n\n        norm_is_inf = total_norm.isinf()\n        norm_is_nan = total_norm.isnan()\n        inf_or_nan = norm_is_nan.logical_or(norm_is_inf)\n\n        err = torch.tensor(-1.0, device=inf_or_nan.device, dtype=torch.float)\n        total_norm = inf_or_nan * err + inf_or_nan.logical_not() * total_norm\n\n        return total_norm\n\n    @instrument_w_nvtx\n    def partition_grads(self, params_to_release: List[Parameter], grad_partitions: List[Tensor]) -> None:\n        offload_fp32_gradients = {}\n        offload_fp32_offsets = {}\n        buffers = []\n        for param, grad_partition in zip(params_to_release, grad_partitions):\n\n            contains_real_data = param.partition_numel() * dist.get_rank(self.dp_process_group) < param.ds_numel\n            if not contains_real_data:\n                # this grad partition is empty - don't need to do anything\n                param.grad = None\n                continue\n\n            # move or accumulate gradient partition to target buffer\n            grad_buffer = self.__param_id_to_grad_partition[param.ds_id].narrow(0, 0, grad_partition.numel())\n            buffers.append(grad_buffer)\n            if self.micro_step_id == 0:  # don't accumulate\n                grad_buffer.copy_(grad_partition, non_blocking=True)\n                # ensure grad buffer is a CUDA buffer to speed up the next few\n                # operations and so it can be used asynchronously\n                grad_buffer = grad_buffer.to(grad_partition.device, non_blocking=True)\n            elif get_accelerator().on_accelerator(grad_buffer):\n                grad_buffer.add_(grad_partition.to(self.gradient_accumulation_dtype).view(grad_buffer.shape))\n            else:\n                # if dst is CPU, copy first to src device, do the addition\n                # there, then move back to dst. adding directly to cpu is very slow\n                cuda_grad_buffer = grad_buffer.to(grad_partition.device, non_blocking=True)\n                cuda_grad_buffer.add_(grad_partition.to(self.gradient_accumulation_dtype).view(cuda_grad_buffer.shape))\n                grad_buffer.copy_(cuda_grad_buffer, non_blocking=True)\n                # ensure grad buffer is a CUDA buffer to speed up the next few\n                # operations and so it can be used asynchronously\n                grad_buffer = cuda_grad_buffer\n\n            # offload the gradient partition if applicable\n            if self.offload_optimizer:\n                i, dest_offset, _ = self.grad_position[self.get_param_id(param)]\n\n                if self.is_gradient_accumulation_boundary:\n                    self.norm_for_param_grads[self.get_param_id(param)] = self._constant_buffered_norm2(grad_buffer)\n\n                    if self._swappable_optimizer_subgroup(i):\n                        if not i in offload_fp32_gradients.keys():\n                            offload_fp32_gradients[i] = []\n                            offload_fp32_offsets[i] = []\n\n                        offload_fp32_gradients[i].append(grad_buffer.float())\n                        offload_fp32_offsets[i].append(dest_offset)\n                    else:\n                        fp32_grad_tensor = self.fp32_partitioned_groups_flat[i].grad.narrow(\n                            0, dest_offset, grad_buffer.numel())\n                        fp32_grad_tensor.copy_(grad_buffer)\n\n            # free the gradient\n            if not get_accelerator().is_synchronized_device():\n                param.grad.record_stream(get_accelerator().current_stream())\n            param.grad = None\n\n        if self.offload_optimizer and self.swap_optimizer:\n            for i in offload_fp32_gradients.keys():\n                self.optimizer_swapper.swap_out_gradients(parameter=self.fp32_partitioned_groups_flat[i],\n                                                          gradient_offsets=offload_fp32_offsets[i],\n                                                          gradient_tensors=offload_fp32_gradients[i])\n        return buffers\n\n    def reduce_ready_partitions_and_remove_grads(self, param):\n        #print_rank_0(f\"Backward {debug_param2name_id_shape(param)}\", force=True)\n        self.reduce_independent_p_g_buckets_and_remove_grads(param)\n\n    def zero_reduced_gradients(self, partition_id, i):\n\n        def are_all_related_partitions_reduced(params_id):\n            for partition_id in self.param_to_partition_ids[i][params_id]:\n                if not self.is_partition_reduced[i][partition_id]:\n                    return False\n            return True\n\n        for params_id in self.is_grad_computed[i][partition_id]:\n            if are_all_related_partitions_reduced(params_id):\n                self.param_dict[params_id].grad = None\n\n    def quantize_nontrainable_params(self):\n        \"\"\" In ZeRO-3, when the zero_quantized_nontrainable_weights flag is set, we quantize the non-trainable weights and also store them in quantized format. However, this check for trainable/non-trainable is done when deepspeed initializes the partitioning. So, if the user changes the trainable/non-trainable status of a parameter after the partitioning is done (e.g. LoRA), the user needs to re-quantize the non-trainable weights by calling this function.\n        \"\"\"\n        if not self.zero_quantized_nontrainable_weights:\n            print_rank_0(\n                f\"Warning: quantize_nontrainable_params() called with zero_quantized_nontrainable_weights disabled, return without doing anything\",\n                force=True)\n            return\n        quantizer_module = CUDAQuantizer()\n\n        def quantize_dstensor(tensor):\n            assert tensor.dtype == torch.float16, f\"quantize_dstensor() expects tensor.dtype == torch.float16, got {tensor.dtype}\"\n            partition_size = tensor.ds_numel\n            ds_status = tensor.status\n            final_location = tensor.final_location\n            tensor, tensor.ds_quant_scale = quantizer_module.quantize(tensor)\n            tensor.ds_numel = partition_size\n            tensor.status = ds_status\n            tensor.final_location = final_location\n            tensor.requires_grad = False\n            return tensor\n\n        for param in self.module.parameters():\n            if hasattr(param, \"ds_tensor\") and (param.ds_tensor.numel() <= 2048 or param.ds_numel <= 500000):\n                # skip small parameters\n                continue\n            if hasattr(param,\n                       \"ds_tensor\") and not param.requires_grad and not hasattr(param.ds_tensor, \"ds_quant_scale\"):\n                param.ds_tensor = quantize_dstensor(param.ds_tensor)\n            if hasattr(param, \"ds_secondary_tensor\") and not param.requires_grad and not hasattr(\n                    param.ds_secondary_tensor, \"ds_quant_scale\") and param.ds_secondary_tensor is not None:\n                param.ds_secondary_tensor = quantize_dstensor(param.ds_secondary_tensor)\n        get_accelerator().synchronize()\n\n    def flatten_and_print(self, message, tensors, start=0, n=5):\n        flatten_tensor = self.flatten(tensors)\n\n        def print_func():\n            logger.info(flatten_tensor.contiguous().view(-1).narrow(0, start, n))\n\n        self.sequential_execution(print_func, message)\n\n    def get_grads_to_reduce(self, i, partition_id):\n\n        def get_reducible_portion(key):\n            grad = self.param_dict[key].grad\n            total_elements = grad.numel()\n            start = self.grad_start_offset[i][partition_id][key]\n            num_elements = min(total_elements - start,\n                               self.partition_size[i] - self.grad_partition_insertion_offset[i][partition_id][key])\n            if not pg_correctness_test:\n                if num_elements == total_elements:\n                    return grad\n                else:\n                    return grad.contiguous().view(-1).narrow(0, int(start), int(num_elements))\n            else:\n                if num_elements == total_elements:\n                    return grad.clone()\n                else:\n                    return grad.clone().contiguous().view(-1).narrow(0, int(start), int(num_elements))\n\n        grads_to_reduce = []\n        for key in self.is_grad_computed[i][partition_id]:\n            grad = get_reducible_portion(key)\n            grads_to_reduce.append(grad)\n        return grads_to_reduce\n\n    def sequential_execution(self, function, message, group=None):\n        if group is None:\n            group = self.dp_process_group\n        if dist.get_rank(group=group) == 0:\n            logger.info(message)\n        for id in range(dist.get_world_size(group=group)):\n            if id == dist.get_rank(group=group):\n                function()\n            dist.barrier(group=group)\n\n    def set_none_gradients_to_zero(self, i, partition_id):\n        for param_id in self.is_grad_computed[i][partition_id]:\n            param = self.param_dict[param_id]\n            if param.grad is None:\n                param.grad = torch.zeros_like(param)\n\n    ######################Reduction Related Methods##############################\n\n    def allreduce_bucket(self, bucket, rank=None, log=None):\n        rank = None\n        tensor = self.flatten(bucket)\n\n        tensor_to_allreduce = tensor\n\n        if pg_correctness_test:\n            communication_data_type = torch.float32\n        else:\n            communication_data_type = self.communication_data_type\n\n        if communication_data_type != tensor.dtype:\n            tensor_to_allreduce = tensor.to(communication_data_type)\n\n        tensor_to_allreduce.div_(dist.get_world_size(group=self.dp_process_group) / float(self.sequence_parallel_size))\n\n        if rank is None:\n            #    \"All Reducing\"\n            dist.all_reduce(tensor_to_allreduce, group=self.dp_process_group)\n        else:\n            global_rank = dist.get_global_rank(self.dp_process_group, rank)\n            dist.reduce(tensor_to_allreduce, global_rank, group=self.dp_process_group)\n\n        if communication_data_type != tensor.dtype and tensor is not tensor_to_allreduce:\n            if rank is None or rank == dist.get_rank(group=self.dp_process_group):\n                tensor.copy_(tensor_to_allreduce)\n\n        return tensor\n\n    # if rank is specified do a reduction instead of an allreduce\n    def allreduce_and_copy(self, small_bucket, rank=None, log=None):\n        with get_accelerator().stream(self.reduction_stream):\n            allreduced = self.allreduce_bucket(small_bucket, rank=rank, log=log)\n            if rank is None or rank == dist.get_rank(group=self.dp_process_group):\n                for buf, synced in zip(small_bucket, self.unflatten(allreduced, small_bucket)):\n                    buf.copy_(synced)\n\n    def allreduce_no_retain(self, bucket, numel_per_bucket=500000000, rank=None, log=None):\n        small_bucket = []\n        numel = 0\n        for tensor in bucket:\n            small_bucket.append(tensor)\n            numel = numel + tensor.numel()\n            if numel > numel_per_bucket:\n                self.allreduce_and_copy(small_bucket, rank=rank, log=None)\n                small_bucket = []\n        if len(small_bucket) > 0:\n            self.allreduce_and_copy(small_bucket, rank=rank, log=log)\n\n    #############################################################################\n    #############################################################################\n    #############################################################################\n\n    # views the tensor as multiple partitions and returns\n    # those partitions\n    def get_data_parallel_partitions(self, tensor):\n        partitions = []\n\n        dp = dist.get_world_size(group=self.dp_process_group)\n        dp_id = dist.get_rank(group=self.dp_process_group)\n\n        total_num_elements = tensor.numel()\n\n        base_size = total_num_elements // dp\n        remaining = total_num_elements % dp\n\n        start = 0\n        for id in range(dp):\n            partition_size = base_size\n            if id < remaining:\n                partition_size = partition_size + 1\n            partitions.append(tensor.narrow(0, start, partition_size))\n            start = start + partition_size\n        return partitions\n\n    def get_partition_info(self, tensor_list, partition_size, partition_id):\n        params_in_partition = []\n        params_not_in_partition = []\n\n        start_index = partition_size * partition_id\n        end_index = partition_size * (partition_id + 1)\n\n        current_index = 0\n        first_offset = 0\n\n        for tensor in tensor_list:\n\n            tensor_size = tensor.numel()\n\n            if start_index <= current_index < end_index:\n                params_in_partition.append(tensor)\n\n            elif current_index < start_index < (current_index + tensor_size):\n                params_in_partition.append(tensor)\n\n                assert (first_offset == 0\n                        ), \"This can happen either zero or only once as this must be the first tensor in the partition\"\n                first_offset = start_index - current_index\n\n            else:\n                params_not_in_partition.append(tensor)\n\n            current_index = current_index + tensor_size\n\n        return params_in_partition, params_not_in_partition, first_offset\n\n    @instrument_w_nvtx\n    def zero_grad(self, set_to_none=True):\n        \"\"\"\n        Zero FP16 parameter grads.\n        \"\"\"\n        self.micro_step_id = 0\n\n        # FP32 grad should never exist.\n        # For speed, set model fp16 grad to None by default\n        for group in self.fp16_groups:\n            for p in group:\n                if set_to_none:\n                    if p.grad is not None and get_accelerator().on_accelerator(p.grad):\n                        p.grad.record_stream(get_accelerator().current_stream())\n                    p.grad = None\n                else:\n                    if p.grad is not None:\n                        p.grad.detach_()\n                        p.grad.zero_()\n\n    def _model_parallel_all_reduce(self, tensor, op):\n        \"\"\" Perform all reduce within model parallel group, if any.\n        \"\"\"\n        if self.model_parallel_group is None:\n            pass\n        else:\n            dist.all_reduce(tensor=tensor, op=op, group=self.model_parallel_group)\n\n    @instrument_w_nvtx\n    def get_grad_norm_direct(self, gradients, params, norm_type=2):\n        \"\"\"Clips gradient norm of an iterable of parameters.\n\n        This is adapted from torch.nn.utils.clip_grad.clip_grad_norm_ and\n        added functionality to handle model parallel parameters. Note that\n        the gradients are modified in place.\n\n        Arguments:\n            parameters (Iterable[Tensor] or Tensor): an iterable of Tensors or a\n                single Tensor that will have gradients normalized\n            max_norm (float or int): max norm of the gradients\n            norm_type (float or int): type of the used p-norm. Can be ``'inf'`` for\n                infinity norm.\n\n        Returns:\n            Total norm of the parameters (viewed as a single vector).\n        \"\"\"\n        norm_type = float(norm_type)\n        if norm_type == inf:\n            total_norm = max(g.data.abs().max() for g in gradients)\n            total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])\n            dist.all_reduce(total_norm_cuda, op=dist.ReduceOp.MAX, group=self.dp_process_group)\n\n            # Take max across all GPUs.\n            self._model_parallel_all_reduce(tensor=total_norm_cuda, op=dist.ReduceOp.MAX)\n            total_norm = total_norm_cuda[0]\n        else:\n            # if dist.get_rank() == 0:\n            #    logger.info(f\"Total Norm beginning {total_norm}\")\n            grad_norms = []\n            for g, p in zip(gradients, params):\n                if is_model_parallel_parameter(p) or (self.model_parallel_rank == 0):\n                    grad_norms.append(g.to(get_accelerator().device_name(), non_blocking=True).double().norm(2))\n\n            # Sum across all model parallel GPUs.\n            if len(grad_norms) == 0:\n                # FIX https://github.com/microsoft/DeepSpeed/issues/3564\n                total_norm_cuda = torch.tensor(0,\n                                               dtype=gradients[0].dtype).to(get_accelerator().device_name()).double()\n            else:\n                total_norm_cuda = torch.sum(torch.pow(torch.stack(grad_norms), 2))\n\n            dist.all_reduce(total_norm_cuda, op=dist.ReduceOp.SUM, group=self.dp_process_group)\n\n            self._model_parallel_all_reduce(tensor=total_norm_cuda, op=dist.ReduceOp.SUM)\n\n            total_norm = total_norm_cuda**(1. / norm_type)\n\n        norm_is_inf = total_norm.isinf()\n        norm_is_nan = total_norm.isnan()\n        inf_or_nan = norm_is_nan.logical_or(norm_is_inf)\n\n        err = torch.tensor(-1.0, device=self.device, dtype=torch.float)\n        total_norm = inf_or_nan * err + inf_or_nan.logical_not() * total_norm\n\n        return total_norm\n\n    # creates a flat fused tensor from the tensor list starting at the first_offset\n    # in the first tensor of the list. If there are not enough elements in the tensor\n    # list then the flat tensor will be padded with zeros\n    def get_flat_partition(self, tensor_list, first_offset, partition_size, return_tensor_list=False):\n        flat_tensor_list = []\n        current_size = 0\n        for i, tensor in enumerate(tensor_list):\n            if tensor.grad is None:\n                tensor.grad = torch.zeros_like(tensor)\n\n            tensor = tensor.grad\n            num_elements = tensor.numel()\n            tensor_offset = 0\n\n            # we need to offset to get to the right element\n            if i == 0 and first_offset > 0:\n                tensor_offset = first_offset\n                num_elements = num_elements - tensor_offset\n\n            # we dont need all elements of the tensor\n            if num_elements > (partition_size - current_size):\n                num_elements = partition_size - current_size\n\n            # we need a narrow view of the tensor based on the tensor offset and number of elements that\n            # we need from this tensor\n            if tensor_offset > 0 or num_elements < tensor.numel():\n                flat_tensor_list.append(tensor.contiguous().view(-1).narrow(0, int(tensor_offset), int(num_elements)))\n            else:\n                flat_tensor_list.append(tensor)\n\n            current_size = current_size + num_elements\n\n        # this means its the last partition and does not align with the dp boundary. We need to pad before flattening\n        if current_size < partition_size:\n            flat_tensor_list.append(\n                torch.zeros(int(partition_size - current_size),\n                            dtype=tensor_list[0].dtype,\n                            device=tensor_list[0].device))\n\n        if return_tensor_list:\n            return flat_tensor_list\n\n        return self.flatten(flat_tensor_list)\n\n    def free_grad_in_param_list(self, param_list):\n        for p in param_list:\n            p.grad = None\n\n    def reset_cpu_buffers(self):\n        self.norm_for_param_grads = {}\n\n    def _pre_step(self):\n        self.micro_step_id = 0\n\n        print_rank_0(f\"Inside Step function\")\n        see_memory_usage(f\"In step before checking overflow\", force=False)\n\n        print_rank_0(\"Finished Tracing at Beginning of Step\")\n        self._get_param_coordinator(training=True).hierarchy = 0\n\n        print_rank_0(\"Finished Tracing at Beginning of Step\")\n\n    @instrument_w_nvtx\n    def _get_norm_groups(self):\n        norm_groups = []\n        for i, group in enumerate(self.fp16_groups):\n            if self.offload_optimizer:\n                norm_groups.append(self.complete_grad_norm_calculation_for_cpu_offload(self.fp16_groups[i]))\n            else:\n                norm_groups.append(self.get_grad_norm_direct(self.averaged_gradients[i], self.fp16_groups[i]))\n        return norm_groups\n\n    @instrument_w_nvtx\n    def _prepare_fp32_grad_for_sub_group(self, sub_group_id):\n        partition_id = dist.get_rank(group=self.dp_process_group)\n\n        single_grad_partition = self.flatten(self.averaged_gradients[sub_group_id]).to(\n            self.fp32_partitioned_groups_flat[sub_group_id].dtype)\n\n        assert single_grad_partition.numel() == self.fp32_partitioned_groups_flat[sub_group_id].numel(), \\\n            \"averaged gradients have different number of elements that partition size {} {} {} {}\".format(\n                single_grad_partition.numel(), self.fp32_partitioned_groups_flat[sub_group_id].numel(), sub_group_id, partition_id)\n\n        self.fp32_partitioned_groups_flat[sub_group_id].grad = single_grad_partition\n\n        # release all the gradient since we have already created a necessary copy in dp_grad_partition\n        self.zero_grad(set_to_none=True)\n\n        if not get_accelerator().is_synchronized_device():\n            for grad in filter(lambda g: get_accelerator().on_accelerator(g), self.averaged_gradients[sub_group_id]):\n                grad.record_stream(get_accelerator().current_stream())\n\n        self.averaged_gradients[sub_group_id] = None\n\n    @instrument_w_nvtx\n    def _prepare_sub_group(self, sub_group_id, timer_names):\n        see_memory_usage(f'Before prepare optimizer sub group {sub_group_id}', force=False)\n        if self._swappable_optimizer_subgroup(sub_group_id):\n            self._optimizer_states_and_gradient_swap_in(sub_group_id, timer_names)\n        elif not self.offload_optimizer:\n            self._prepare_fp32_grad_for_sub_group(sub_group_id)\n        see_memory_usage(f'After prepare optimizer sub group {sub_group_id}', force=False)\n\n    def _optimizer_states_and_gradient_swap_in(self, sub_group_id, timer_names):\n        param_length = self.fp16_partitioned_groups_flat_numel[sub_group_id]\n        fp32_param_id = self.get_param_id(self.fp32_partitioned_groups_flat[sub_group_id])\n        assert self._swappable_optimizer_subgroup(sub_group_id), \\\n            f'Parameter {fp32_param_id} of numel={param_length} is not swappable'\n\n        see_memory_usage(f'pre-step Before swapping in optimizer tensors {sub_group_id}', force=False)\n        timer_names.add(OPTIMIZER_SWAP_IN_STATE_TIMER)\n        self.timers(OPTIMIZER_SWAP_IN_STATE_TIMER).start()\n\n        self.optimizer_swapper.swap_in_optimizer_state(\n            parameter=self.fp32_partitioned_groups_flat[sub_group_id],\n            async_parameter=self.next_swappable_fp32_partitioned_groups[sub_group_id])\n\n        self.timers(OPTIMIZER_SWAP_IN_STATE_TIMER).stop()\n        see_memory_usage(f'pre-step After swapping in optimizer tensors {sub_group_id}', force=False)\n\n    @instrument_w_nvtx\n    def _release_sub_group(self, sub_group_id, timer_names):\n        see_memory_usage(f'Before release optimizer sub group {sub_group_id}', force=False)\n        # get rid of the fp32 gradients. Not needed anymore\n        if not self.offload_optimizer:\n            self.fp32_partitioned_groups_flat[sub_group_id].grad = None\n\n        if self._swappable_optimizer_subgroup(sub_group_id):\n            self._optimizer_states_and_gradient_swap_out(sub_group_id, timer_names)\n        see_memory_usage(f'After release optimizer sub group {sub_group_id}', force=False)\n\n    # create a flat tensor aligned at the alignment boundary\n    @instrument_w_nvtx\n    def flatten_dense_tensors_aligned(self, tensor_list, alignment):\n        num_elements = 0\n        for tens in tensor_list:\n            num_elements = num_elements + tens.numel()\n\n        remaining = num_elements % alignment\n\n        if remaining:\n            elements_to_add = alignment - remaining\n            pad_tensor = torch.zeros(elements_to_add, device=tensor_list[0].device, dtype=tensor_list[0].dtype)\n            padded_tensor_list = tensor_list + [pad_tensor]\n\n            num_elements = num_elements + elements_to_add\n        else:\n            padded_tensor_list = tensor_list\n\n        return self.flatten(padded_tensor_list)\n\n    def _optimizer_states_and_gradient_swap_out(self, sub_group_id, timer_names):\n        param_length = self.fp16_partitioned_groups_flat_numel[sub_group_id]\n        fp32_param_id = self.get_param_id(self.fp32_partitioned_groups_flat[sub_group_id])\n        assert self._swappable_optimizer_subgroup(sub_group_id), \\\n            f'Parameter {fp32_param_id} of numel={param_length} is not swappable'\n\n        see_memory_usage(f'post-step Before swapping out optimizer tensors {sub_group_id}', force=False)\n        timer_names.add(OPTIMIZER_SWAP_OUT_STATE_TIMER)\n        self.timers(OPTIMIZER_SWAP_OUT_STATE_TIMER).start()\n\n        self.optimizer_swapper.swap_out_optimizer_state(\n            parameter=self.fp32_partitioned_groups_flat[sub_group_id],\n            async_swap=self.next_swappable_fp32_partitioned_groups[sub_group_id] is not None)\n\n        self.timers(OPTIMIZER_SWAP_OUT_STATE_TIMER).stop()\n        see_memory_usage(f'post-step After swapping out optimizer tensors {sub_group_id}', force=False)\n\n        # get rid of the fp32 gradients. Not needed anymore\n        self.fp32_partitioned_groups_flat[sub_group_id].grad = None\n\n    def _unflatten_partitioned_parameters(self, sub_group_id):\n        updated_params = self.unflatten(self.fp16_partitioned_groups_flat[sub_group_id],\n                                        self.fp16_partitioned_groups[sub_group_id])\n\n        for partitioned_param, q in zip(self.fp16_partitioned_groups[sub_group_id], updated_params):\n            partitioned_param.data = q.data\n\n    def _overflow_clean_up(self, prev_scale):\n        see_memory_usage('After overflow before clearing gradients', force=False)\n        self.zero_grad(set_to_none=True)\n\n        if self.offload_optimizer:\n            self.reset_cpu_buffers()\n        else:\n            self.averaged_gradients = {}\n\n        see_memory_usage('After overflow after clearing gradients', force=False)\n\n    @instrument_w_nvtx\n    def _overflow_check_and_loss_scale_update(self):\n\n        # First compute norm for all group so we know if there is overflow\n        if self.dtype == torch.float16:\n            self.check_overflow()\n\n        #loss scaling related computation\n        prev_scale = self.loss_scale\n        self._update_scale(self.overflow)\n\n        if self.overflow:\n            self._overflow_clean_up(prev_scale)\n\n        return self.overflow\n\n    @instrument_w_nvtx\n    def _post_step(self, timer_names):\n        if self.offload_optimizer:\n            self.reset_cpu_buffers()\n\n        #Gathering persisting parameters\n        if len(self.persistent_parameters) > 0:\n            self.persistent_parameters[0].all_gather(self.persistent_parameters)\n\n        if self.swap_optimizer:\n            self.optimizer_swapper.log_timers()\n\n        self.invalidate_secondary_tensor()\n\n        self.timers.log(timer_names)\n\n        see_memory_usage('After zero_optimizer step', force=False)\n        print_rank_0(f\"------------------Finishing Step-----------------------\")\n\n    @instrument_w_nvtx\n    def _reassign_or_swap_out_partitioned_parameters(self, sub_group_id):\n        if self.fp16_partitioned_groups_flat[sub_group_id] is not None:\n            self.fp16_partitioned_groups_flat[sub_group_id].data.copy_(\n                self.fp32_partitioned_groups_flat[sub_group_id].data)\n\n            #unflatten fp16 parameter subgroup\n            self._unflatten_partitioned_parameters(sub_group_id)\n        else:\n            self._partitioned_params_swap_out(sub_group_id)\n\n    def override_loss_scale(self, loss_scale):\n        if loss_scale != self.external_loss_scale:\n            logger.info(f'[deepspeed] setting loss scale from {self.external_loss_scale} -> {loss_scale}')\n        self.custom_loss_scaler = True\n        self.external_loss_scale = loss_scale\n\n    @instrument_w_nvtx\n    def step(self, closure=None):\n        \"\"\"\n            Not supporting closure.\n        \"\"\"\n        self._pre_step()\n        self._partition_all_parameters()\n\n        #checks for overflow, adjust the loss scale accordingly\n        if self._overflow_check_and_loss_scale_update():\n            if self.swap_optimizer:\n                self.optimizer_swapper.log_timers()\n            return\n\n        norm_groups = self._get_norm_groups()\n        scaled_global_grad_norm = get_global_norm(norm_list=norm_groups)\n\n        # Stash unscaled gradient norm\n        self._global_grad_norm = scaled_global_grad_norm / self.loss_scale\n\n        timer_names = set()\n\n        timer_names.add(OPTIMIZER_STEP_TIMER)\n        self.timers(OPTIMIZER_STEP_TIMER).start()\n\n        #update parameters one sub group at a time\n        for sub_group_id, group in enumerate(self.fp16_groups):\n\n            #prepare optimizer states, gradients and fp32 parameters for update\n            self._prepare_sub_group(sub_group_id, timer_names)\n\n            #scale the fp32 gradients\n            self.unscale_and_clip_grads(sub_group_id, scaled_global_grad_norm)\n\n            #apply the optimizer step on the sub group and copy fp32 parameters to fp16\n            self._optimizer_step(sub_group_id)\n\n            #put fp16 parameters in appropriate location\n            self._reassign_or_swap_out_partitioned_parameters(sub_group_id)\n\n            #release memory or swap out optimizer states of fp32 parameters\n            self._release_sub_group(sub_group_id, timer_names)\n\n        self.timers(OPTIMIZER_STEP_TIMER).stop()\n\n        self._post_step(timer_names)\n\n        # warn user about caching allocator flushes\n        memory_stats = get_accelerator().memory_stats()\n        alloc_retries = memory_stats.get(\"num_alloc_retries\")\n        if alloc_retries is None:\n            alloc_retries = 0\n        if alloc_retries > self.n_caching_allocator_flushes:\n            if dist.get_rank() == 0:\n                logger.warning(\n                    \"%d pytorch allocator cache flushes since last step. this happens \"\n                    \"when there is high memory pressure and is detrimental to \"\n                    \"performance. if this is happening frequently consider adjusting \"\n                    \"settings to reduce memory consumption. If you are unable to \"\n                    \"make the cache flushes go away consider adding \"\n                    \"get_accelerator().empty_cache() calls in your training loop to ensure \"\n                    \"that all ranks flush their caches at the same time\",\n                    alloc_retries - self.n_caching_allocator_flushes)\n            self.n_caching_allocator_flushes = alloc_retries\n\n    def dump_pre_step_gradients(self, debug_fp32_grads):\n        # Dump gradient norms for debugging\n        for i, _ in enumerate(self.fp16_groups):\n            print(f'Pre-Step Dump Norms for Group {i} FP16P, FP16G, FP32G, FP32GUC')\n            for fp16_param, fp32_grad in zip(self.fp16_groups[i], debug_fp32_grads[i]):\n                param_id = self.get_param_id(fp16_param)\n                fp16_grad_norm = self.debug_fp16_grads[i][param_id]\n\n                fp32_grad_norm = [float(t.data.float().norm(2)) for t in fp32_grad]\n                norm_list = [fp16_grad_norm, fp32_grad_norm]\n                print(f'Pre-Step Norms {i} {param_id} = {norm_list}')\n\n    def dump_post_step_gradients(self):\n        # Dump gradient norms for debugging\n        for i, group in enumerate(self.fp16_groups):\n            print(f'Post-Step Dump Norms for Group {i} FP16P, FP16DS, FP16FLAT, FP32FLAT')\n            unflat_fp16 = self.unflatten(self.fp16_groups_flat[i], self.fp16_groups[i])\n            unflat_fp32 = self.unflatten(self.fp32_partitioned_groups_flat[i], self.fp16_groups[i])\n            for j, p in enumerate(self.fp16_groups[i]):\n                param_id = self.get_param_id(p)\n                param_norm = float(p.data.float().norm(2))\n                ds_norm = float(p.ds_tensor.data.float().norm(2))\n\n                unflat_norm = [float(t.data.float().norm(2)) for t in [unflat_fp16[j], unflat_fp32[j]]]\n                norm_list = [param_norm, ds_norm] + unflat_norm\n                print(f'Post-Step Norms {i} {param_id} = {norm_list}')\n\n    @instrument_w_nvtx\n    def unscale_and_clip_grads(self, sub_group_id, total_norm):\n        # compute combined scale factor for this group\n        combined_scale = self.loss_scale\n        if self.clip_grad > 0.:\n            # norm is in fact norm*scale\n            clip = ((total_norm / self.loss_scale) + 1e-6) / self.clip_grad\n            if clip > 1:\n                combined_scale = clip * self.loss_scale\n\n        self.fp32_partitioned_groups_flat[sub_group_id].grad.mul_(1. / combined_scale)\n\n    def _check_overflow(self, partition_gradients=True):\n        self.overflow = self.has_overflow(partition_gradients)\n\n    # `params` is a list / generator of torch.Variable\n    def has_overflow_serial(self, params, is_grad_list=False):\n        for p in params:\n            if p.grad is not None and self._has_inf_or_nan(p.grad.data):\n                return True\n\n        return False\n\n    def has_overflow_partitioned_grads_serial(self):\n        for i in range(len(self.fp16_groups)):\n            for j, grad in enumerate(self.averaged_gradients[i]):\n                if grad is not None and self._has_inf_or_nan(grad.data, j):\n                    return True\n        return False\n\n    @instrument_w_nvtx\n    def has_overflow(self, partition_gradients=True):\n        if partition_gradients:\n            with get_accelerator().stream(self.reduce_and_partition_stream):\n                if hasattr(self.inf_or_nan_tracker, \"logical_or_\"):\n                    self.inf_or_nan_tracker.logical_or_(torch.isinf(self.grad_partitions_flat_buffer).any())\n                    self.inf_or_nan_tracker.logical_or_(torch.isnan(self.grad_partitions_flat_buffer).any())\n                else:\n                    # logical_or_ not available in older versions of pytorch\n                    self.inf_or_nan_tracker += torch.isinf(self.grad_partitions_flat_buffer).any()\n                    self.inf_or_nan_tracker += torch.isnan(self.grad_partitions_flat_buffer).any()\n                    self.inf_or_nan_tracker = self.inf_or_nan_tracker > 0\n\n                overflow_gpu = self.inf_or_nan_tracker.clone().to(torch.uint8)\n                self.inf_or_nan_tracker.zero_()\n\n            if not get_accelerator().resolves_data_dependency():\n                get_accelerator().default_stream().wait_stream(self.reduce_and_partition_stream)\n            dist.all_reduce(overflow_gpu, op=dist.ReduceOp.MAX, group=self.dp_process_group)\n\n        else:\n            params = []\n            for group in self.fp16_groups:\n                for param in group:\n                    params.append(param)\n\n            overflow = self.has_overflow_serial(params, is_grad_list=partition_gradients)\n            overflow_gpu = get_accelerator().ByteTensor([overflow])\n\n        # Since each model parallel GPU carries only part of the model,\n        # make sure overflow flag is synced across all the model parallel GPUs\n        self._model_parallel_all_reduce(tensor=overflow_gpu, op=dist.ReduceOp.MAX)\n\n        overflow = overflow_gpu[0].item()\n        return bool(overflow)\n\n    # `x` is a torch.Tensor\n    @staticmethod\n    def _has_inf_or_nan(x, j=None):\n        try:\n            # if x is half, the .float() incurs an additional deep copy, but it's necessary if\n            # Pytorch's .sum() creates a one-element tensor of the same type as x\n            # (which is true for some recent version of pytorch).\n            cpu_sum = float(x.float().sum())\n            # More efficient version that can be used if .sum() returns a Python scalar\n            # cpu_sum = float(x.sum())\n        except RuntimeError as instance:\n            # We want to check if inst is actually an overflow exception.\n            # RuntimeError could come from a different error.\n            # If so, we still want the exception to propagate.\n            if \"value cannot be converted\" not in instance.args[0]:\n                raise\n            return True\n        else:\n            if cpu_sum == float('inf') or cpu_sum == -float('inf') or cpu_sum != cpu_sum:\n                return True\n            return False\n\n    @instrument_w_nvtx\n    def backward(self, loss, retain_graph=False):\n        \"\"\"\n        :attr:`backward` performs the following steps:\n\n        1. fp32_loss = loss.float()\n        2. scaled_loss = fp32_loss*loss_scale\n        3. scaled_loss.backward(), which accumulates scaled gradients into the ``.grad`` attributes of the model's fp16 leaves\n        \"\"\"\n        if self.swap_optimizer:\n            self.optimizer_swapper.pre_backward()\n\n        see_memory_usage(f\"Before backward\", force=False)\n\n        if self.custom_loss_scaler:\n            scaled_loss = self.external_loss_scale * loss\n            scaled_loss.backward()\n        else:\n            self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)\n\n        self._get_param_coordinator(training=True).reset_step()\n\n        if self.swap_optimizer:\n            self.optimizer_swapper.post_backward()\n\n    def get_fp32_grad_partitions(self) -> Dict[int, Dict[int, Tensor]]:\n        \"\"\"get fp32 gradient partition dictionary\n        accessed as grad_dict[parameter_group_index][parameter_index]\n        \"\"\"\n        if not get_accelerator().resolves_data_dependency():\n            self.reduce_and_partition_stream.synchronize()\n        grad_dict = collections.defaultdict(dict)\n        if self.offload_optimizer:\n            for group in self.fp16_groups:\n                for param_idx, param in enumerate(group):\n                    group_idx, dest_offset, num_elements = self.grad_position[self.get_param_id(param)]\n                    fp32_grad = self.fp32_partitioned_groups_flat[group_idx].grad.narrow(0, dest_offset, num_elements)\n                    grad_dict[group_idx][param_idx] = fp32_grad\n        else:\n            for group_idx, group in self.averaged_gradients.items():\n                for param_idx, gradient in enumerate(group):\n                    grad_dict[group_idx][param_idx] = gradient.float()\n\n        return grad_dict\n\n    def _fp32_state_allgather(self, param, fp32_state_partition):\n        reduce_buffer = torch.zeros(self.partition_count * fp32_state_partition.numel(),\n                                    dtype=torch.float32,\n                                    device=param.device)\n        my_rank = dist.get_rank(group=self.dp_process_group)\n        partition = reduce_buffer.narrow(0, fp32_state_partition.numel() * my_rank, fp32_state_partition.numel())\n        partition.data.copy_(fp32_state_partition.data, non_blocking=False)\n        dist.all_gather_into_tensor(reduce_buffer, partition, group=self.dp_process_group)\n        return reduce_buffer.narrow(0, 0, param.ds_numel).view(param.ds_shape)\n\n    def get_fp32_grad_for_param(self, param) -> Tensor:\n        if not param.requires_grad:\n            return None\n\n        if not get_accelerator().resolves_data_dependency():\n            self.reduce_and_partition_stream.synchronize()\n\n        if self.offload_optimizer:\n            group_idx, dest_offset, num_elements = self.grad_position[self.get_param_id(param)]\n            fp32_grad = self.fp32_partitioned_groups_flat[group_idx].grad.narrow(0, dest_offset, num_elements)\n        else:\n            fp32_grad = self.__param_id_to_grad_partition[param.ds_id].float()\n\n        return self._fp32_state_allgather(param, fp32_grad)\n\n    def _get_fp32_opt_state_partition(self, param, optim_state_key=None):\n        if not get_accelerator().resolves_data_dependency():\n            self.reduce_and_partition_stream.synchronize()\n\n        group_idx, dest_offset, num_elements = self.grad_position[self.get_param_id(param)]\n\n        if self._swappable_optimizer_subgroup(group_idx):\n            self._optimizer_states_and_gradient_swap_in(group_idx)\n\n        fp32_param = self.fp32_partitioned_groups_flat[group_idx]\n        if optim_state_key is None:\n            fp32_opt_state = fp32_param.narrow(0, dest_offset, num_elements)\n        else:\n            fp32_opt_state = self.optimizer.state[fp32_param][optim_state_key].narrow(0, dest_offset, num_elements)\n\n        return fp32_opt_state, group_idx\n\n    def get_full_hp_param(self, param, optim_state_key=None) -> Tensor:\n        if not param.requires_grad:\n            return None\n\n        fp32_opt_state, group_idx = self._get_fp32_opt_state_partition(param, optim_state_key)\n        hp_param = self._fp32_state_allgather(param, fp32_opt_state)\n\n        if self._swappable_optimizer_subgroup(group_idx):\n            self._optimizer_states_and_gradient_swap_out(group_idx)\n\n        return hp_param\n\n    def set_full_hp_param(self, value, param, optim_state_key=None):\n        if not param.requires_grad:\n            return\n\n        assert value.numel(\n        ) == param.ds_numel, f\" Number of elements do not match: {value.numel()} != {param.ds_numel}\"\n\n        fp32_opt_state_partition, group_idx = self._get_fp32_opt_state_partition(param, optim_state_key)\n        my_rank = dist.get_rank(group=self.dp_process_group)\n        value_partition = value.flatten().narrow(0,\n                                                 fp32_opt_state_partition.numel() * my_rank,\n                                                 fp32_opt_state_partition.numel())\n        fp32_opt_state_partition.data.copy_(value_partition.data)\n\n        if self._swappable_optimizer_subgroup(group_idx):\n            self._optimizer_states_and_gradient_swap_out(group_idx)\n\n    ### Local API START ###\n\n    def get_local_fp32_param(self, param, optim_state_key=None) -> Tensor:\n        if not param.requires_grad:\n            return None\n        fp32_opt_state, group_idx = self._get_fp32_opt_state_partition(param, optim_state_key)\n        return fp32_opt_state\n\n    def get_local_fp32_grad_for_param(self, param) -> Tensor:\n        if not param.requires_grad:\n            return None\n\n        if not get_accelerator().resolves_data_dependency():\n            self.reduce_and_partition_stream.synchronize()\n\n        if self.offload_optimizer:\n            group_idx, dest_offset, num_elements = self.grad_position[self.get_param_id(param)]\n            fp32_grad = self.fp32_partitioned_groups_flat[group_idx].grad.narrow(0, dest_offset, num_elements)\n        else:\n            fp32_grad = self.__param_id_to_grad_partition[param.ds_id].float()\n        return fp32_grad\n\n    def set_local_hp_param(self, value, param, optim_state_key=None):\n        if not param.requires_grad:\n            return\n\n        assert hasattr(param, \"ds_tensor\"), f\" The parameter does not contain the partitioned copy of the tensor.\"\n        assert value.numel() == param.ds_tensor.numel(\n        ), f\" Number of elements do not match: {value.numel()} != {param.ds_tensor.ds_numel}\"\n\n        fp32_opt_state_partition, group_idx = self._get_fp32_opt_state_partition(param, optim_state_key)\n        value_partition = value.flatten()\n        fp32_opt_state_partition.data.copy_(value_partition.data)\n\n        if self._swappable_optimizer_subgroup(group_idx):\n            self._optimizer_states_and_gradient_swap_out(group_idx)\n        logger.info(f\"[set_local_hp_param][update the params' value successfully]\")\n\n    ### Local API END ###\n\n    @instrument_w_nvtx\n    def _partition_all_parameters(self):\n        self.parameter_offload.partition_all_parameters()\n\n    def check_overflow(self, partition_gradients=True):\n        self._check_overflow(partition_gradients)\n\n    def _update_scale(self, has_overflow=False):\n        self.loss_scaler.update_scale(has_overflow)\n\n    # Promote state so it can be retrieved or set via \"fp16_optimizer_instance.state\"\n    def _get_state(self):\n        return self.optimizer.state\n\n    def _set_state(self, value):\n        self.optimizer.state = value\n\n    state = property(_get_state, _set_state)\n\n    # Promote param_groups so it can be retrieved or set via \"fp16_optimizer_instance.param_groups\"\n    # (for example, to adjust the learning rate)\n    def _get_param_groups(self):\n        return self.optimizer.param_groups\n\n    def _set_param_groups(self, value):\n        self.optimizer.param_groups = value\n        self.trainable_param_groups = self._get_trainable_parameter_groups()\n\n    param_groups = property(_get_param_groups, _set_param_groups)\n\n    # Promote loss scale so it can be retrieved or set via \"fp16_optimizer_instance.loss_scale\"\n    def _get_loss_scale(self):\n        if self.custom_loss_scaler:\n            return self.external_loss_scale\n        else:\n            return self.loss_scaler.cur_scale\n\n    def _set_loss_scale(self, value):\n        self.loss_scaler.cur_scale = value\n\n    loss_scale = property(_get_loss_scale, _set_loss_scale)\n    cur_scale = property(_get_loss_scale, _set_loss_scale)\n\n    def _get_lean_tensors(self, padded_flattened_tensor, group_tensors, paddings):\n        # Remove paddings from flattened tensor\n        individual_tensors = self.unflatten(padded_flattened_tensor, group_tensors)\n        lean_lengths = [t.numel() - pad for t, pad in zip(group_tensors, paddings)]\n        lean_tensors = [t[:len] for t, len in zip(individual_tensors, lean_lengths)]\n        #logger.info(f'rank {dist.get_rank()}: lean_tensors = {[t.numel() for t in lean_tensors]}')\n        return lean_tensors\n\n    #TODO REVISIT this for stage 3\n    def get_lean_optimizer_state(self):\n        # Return optimizer states after removing paddings.\n        # This method assumes that each param group contains a single flattened tensor.\n        optimizer_groups_state = []\n\n        for i, group in enumerate(self.optimizer.param_groups):\n            p = group['params'][0]\n            lean_state = {}\n            for key, value in self.optimizer.state[p].items():\n                if torch.is_tensor(value):\n                    padded_lens = [t.numel() for t in self.fp16_partitioned_groups[i]]\n                    lean_state[key] = self._get_lean_tensors(value, self.fp16_partitioned_groups[i],\n                                                             self.groups_padding[i])\n                    lean_flat_len = sum([t.numel() for t in lean_state[key]])\n                else:\n                    lean_state[key] = value\n\n            optimizer_groups_state.append(lean_state)\n\n        return optimizer_groups_state\n\n    def get_groups_without_padding(self, groups_with_padding):\n        # Return group tensor after removing paddings added for alignment to DP world size.\n        groups_without_padding = []\n        for i, group in enumerate(groups_with_padding):\n            lean_group = self._get_lean_tensors(group, self.fp16_partitioned_groups[i], self.groups_padding[i])\n            groups_without_padding.append(lean_group)\n\n        return groups_without_padding\n\n    def _set_fp32_optimizer_param_groups(self):\n        for sub_group_id, _ in enumerate(self.fp16_groups):\n            param_group_id = self.sub_group_to_group_id[sub_group_id]\n            self.optimizer.param_groups[param_group_id]['params'].append(\n                self.fp32_partitioned_groups_flat[sub_group_id])\n\n    def _clear_fp32_optimizer_param_groups(self):\n        for param_group in self.optimizer.param_groups:\n            param_group['params'] = []\n\n    def _rigid_state_dict(self):\n        state_dict = {}\n        state_dict[ZERO_STAGE] = ZeroStageEnum.weights\n        state_dict[LOSS_SCALER] = self.loss_scaler\n        state_dict['dynamic_loss_scale'] = self.dynamic_loss_scale\n        state_dict['overflow'] = self.overflow\n        state_dict[PARTITION_COUNT] = self.partition_count\n\n        self._set_fp32_optimizer_param_groups()\n        state_dict[OPTIMIZER_STATE_DICT] = self.optimizer.state_dict()\n        state_dict[FP32_FLAT_GROUPS] = self.fp32_partitioned_groups_flat\n        self._clear_fp32_optimizer_param_groups()\n\n        return state_dict\n\n    def state_dict(self):\n        \"\"\"\n        Returns a dict containing the current state of this :class:`FP16_Optimizer` instance.\n        This dict contains attributes of :class:`FP16_Optimizer`, as well as the state_dict\n        of the contained Pytorch optimizer.\n        Example::\n            checkpoint = {}\n            checkpoint['model'] = model.state_dict()\n            checkpoint['optimizer'] = optimizer.state_dict()\n            torch.save(checkpoint, \"saved.pth\")\n        \"\"\"\n        if self.elastic_checkpoint:\n            raise NotImplementedError(\"ZeRO-3 does not yet support elastic checkpointing, please disable for now.\")\n\n        return self._rigid_state_dict()\n\n\n# Restore base optimizer fp32 weights from checkpoint by:\n# 1) Merging fp32 weights from checkpoints of all partitions\n# 2) Extracting fp32 weights for current partition from merged weights\n# 3) Using extracted weights to update base optimizer weights directly.\n\n    def _restore_from_fp32_weights(self, all_state_dict):\n\n        flat_local_partition = []\n        for i in range(len(self.fp32_partitioned_groups_flat)):\n            merged_partitions = [sd['fp32_groups'][i] for sd in all_state_dict]\n            flat_local_partition.append(self._get_flattened_partition(merged_partitions))\n\n        for current, saved in zip(self.fp32_partitioned_groups_flat, flat_local_partition):\n            current.data.copy_(saved.data)\n\n    # Restore base optimizer fp32 weights from ZeRO fp16 weights\n    def _restore_from_bit16_weights(self):\n        for fp16_partitions, fp32_partition in zip(self.fp16_partitioned_groups_flat,\n                                                   self.fp32_partitioned_groups_flat):\n            fp32_partition.data.copy_(fp16_partitions.data)\n\n    # Refresh the fp32 master params from the fp16 copies.\n    def refresh_fp32_params(self):\n        self._restore_from_bit16_weights()\n\n    # Extract flattened partition for current rank from all partitions\n    def _get_flattened_partition(self, all_partition_states):\n        partition_id = dist.get_rank(group=self.dp_process_group)\n        alignment = dist.get_world_size(group=self.dp_process_group)\n\n        param_partitions = [[] for _ in range(len(all_partition_states[0]))]\n        for i, partition in enumerate(all_partition_states):\n            for j, param in enumerate(partition):\n                param_partitions[j].append(param)\n\n        local_state_partitions = []\n        for param_index, param_slices in enumerate(param_partitions):\n            flattened_merged_tensor = self.flatten_dense_tensors_aligned(param_slices, alignment)\n            new_partitions = self.get_data_parallel_partitions(flattened_merged_tensor)\n            local_state_partitions.append(new_partitions[partition_id])\n\n        if torch.is_tensor(local_state_partitions[0]):\n            return self.flatten_dense_tensors_aligned(local_state_partitions, alignment)\n\n        # Assume non-tensor states are not partitioned and equal across ranks, so return first one\n        return local_state_partitions[0]\n\n    # Restore base optimizer state from checkpoint by\n    # 1) Merging optimizer state from checkpoints of all partitions\n    # 2) Extracting optimizer state for current partition from the merged state\n    # 3) Using the extracted value to directly update the base optimizer.\n    def _restore_base_optimizer_state(self, all_state_dict):\n        base_optimizer_group_states = []\n        for i in range(len(self.optimizer.param_groups)):\n            partition_states = {}\n            all_partition_group_states = [sd['base_optimizer_state'][i] for sd in all_state_dict]\n            for key in all_partition_group_states[0].keys():\n                all_partition_states = [all_states[key] for all_states in all_partition_group_states]\n                partition_states[key] = self._get_flattened_partition(all_partition_states)\n            base_optimizer_group_states.append(partition_states)\n\n        for i, group in enumerate(self.optimizer.param_groups):\n            p = group['params'][0]\n            for key, saved in base_optimizer_group_states[i].items():\n                if torch.is_tensor(self.optimizer.state[p][key]):\n                    self.optimizer.state[p][key].data.copy_(saved.data)\n                else:\n                    self.optimizer.state[p][key] = saved\n\n    def _rigid_load_state_dict(self, state_dict, load_optimizer_states=True):\n        # I think it should actually be ok to reload the optimizer before the model.\n        self.loss_scaler = state_dict[LOSS_SCALER]\n        self.dynamic_loss_scale = state_dict['dynamic_loss_scale']\n        self.overflow = state_dict['overflow']\n\n        if load_optimizer_states:\n            self._set_fp32_optimizer_param_groups()\n            self.optimizer.load_state_dict(state_dict[OPTIMIZER_STATE_DICT])\n            self._clear_fp32_optimizer_param_groups()\n\n        if self.swap_optimizer or self.params_in_nvme_and_cpu:\n            # Purge the swapped optimizer state, it was initialized to the freshly created model and not the checkpoint\n            for swap_info in self.optimizer_swapper.swap_params_info.values():\n                swap_info.tensors = [swap_info.tensors[0]]\n                swap_info.has_state_tensors = False\n\n        if self.swap_optimizer:\n            # Touch all parameters to synchronize all buffers\n            timer_names = set()\n            self._partition_all_parameters()\n            for sub_group_id, group in enumerate(self.fp16_groups):\n                self._prepare_sub_group(sub_group_id, timer_names)\n                self._reassign_or_swap_out_partitioned_parameters(sub_group_id)\n                self._release_sub_group(sub_group_id, timer_names)\n            self._post_step(timer_names)\n\n        # restore fp32 partitions\n        for curr_param, saved_param in zip(self.fp32_partitioned_groups_flat, state_dict[FP32_FLAT_GROUPS]):\n            curr_param.data.copy_(saved_param.data)\n\n        # restore fp16 partitions from fp32\n        for sub_group_id in range(len(self.fp32_partitioned_groups_flat)):\n            fp32_param = self.fp32_partitioned_groups_flat[sub_group_id]\n            if sum(fp32_param.size()) > 0:\n                fp16_param = self.fp16_partitioned_groups_flat[sub_group_id]\n                fp16_param.data.copy_(fp32_param.data)\n\n        # update fp16 unflattened params\n        for sub_group_id in range(len(self.fp16_partitioned_groups_flat)):\n            updated_params = self.unflatten(self.fp16_partitioned_groups_flat[sub_group_id],\n                                            self.fp16_partitioned_groups[sub_group_id])\n\n            for partitioned_param, q in zip(self.fp16_partitioned_groups[sub_group_id], updated_params):\n                partitioned_param.data = q.data\n\n    # TODO: Support different/changing load/save DP degree.\n    def load_state_dict(self,\n                        state_dict_list,\n                        load_optimizer_states=True,\n                        load_from_fp32_weights=False,\n                        checkpoint_folder=None,\n                        load_serial=None):\n        r\"\"\"Loading a ZeRO checkpoint\n        Arguments:\n            state_dict_list: List of all saved ZeRO checkpoints, one for each saved partition.\n                Note that the number of saved partitions may differ from number of loading partitions to support\n                changing GPU count, specifically DP world size, between saving and loading checkpoints.\n            load_optimizer_states: Boolean indicating whether or not to load base optimizer states\n            load_from_fp32_weights: Boolean indicating whether to initialize fp32 master weights from fp32\n            copies in checkpoints (no precision loss) or from model's fp16 copies (with precision loss).\n        \"\"\"\n        \"\"\"\n        Loads a state_dict created by an earlier call to state_dict().\n        If ``fp16_optimizer_instance`` was constructed from some ``init_optimizer``,\n        whose parameters in turn came from ``model``, it is expected that the user\n        will call ``model.load_state_dict()`` before\n        ``fp16_optimizer_instance.load_state_dict()`` is called.\n        Example::\n            model = torch.nn.Linear(D_in, D_out).to(get_accelerator().device_name()).half()\n            optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n            optimizer = FP16_Optimizer(optimizer, static_loss_scale = 128.0)\n            ...\n            checkpoint = torch.load(\"saved.pth\")\n            model.load_state_dict(checkpoint['model'])\n            optimizer.load_state_dict(checkpoint['optimizer'])\n        \"\"\"\n\n        if self.elastic_checkpoint:\n            raise NotImplementedError(\"ZeRO-3 does not yet support elastic checkpointing, please disable for now.\")\n\n        self._rigid_load_state_dict(state_dict_list[dist.get_rank(group=self.dp_process_group)],\n                                    load_optimizer_states=load_optimizer_states)\n\n        # when use loading checkpoint serial, after finish loading, we need to\n        # delete the temp state_dict_list variable to save memory, then trigger\n        # the next rank's loading\n        if load_serial is not None:\n            load_serial += 1\n            rank = dist.get_rank(group=self.dp_process_group)\n            local_rank = dist.get_local_rank()\n            del state_dict_list[rank]\n            rank_end = dist.get_world_size() - 1\n            if local_rank != rank_end:\n                dist.send(tensor=load_serial, dst=rank + 1)\n\n        if len(self.persistent_parameters) > 0:\n            self.persistent_parameters[0].partition(self.persistent_parameters)\n            # self.persistent_parameters[0].all_gather(self.persistent_parameters) # this will be done in checkpoint_event_epilogue() so remove it to prevent double all_gather\n\n    def reset_swap_buffers(self):\n        timer_names = set()\n        for sub_group_id, group in enumerate(self.fp16_groups):\n            self._prepare_sub_group(sub_group_id, timer_names)\n            self._reassign_or_swap_out_partitioned_parameters(sub_group_id)\n            self._release_sub_group(sub_group_id, timer_names)\n\n    def checkpoint_event_prologue(self):\n        self._partition_all_parameters()\n\n    def checkpoint_event_epilogue(self):\n        if len(self.persistent_parameters) > 0:\n            self.persistent_parameters[0].all_gather(self.persistent_parameters)\n\n    def empty_partition_cache(self):\n        self.parameter_offload.empty_partition_cache()\n\n\ndef _handle_overflow(cpu_sum, x, i):\n    import math\n    rank = dist.get_rank()\n    if rank == 0:\n        t_i = -1\n        for v_i, v in enumerate(x.data.contiguous().view(-1)):\n            if not math.isfinite(float(v)):\n                t_i = v_i\n                break\n        logger.info(f\"rank {rank} detected overflow {cpu_sum} in tensor {i}:{t_i} shape {x.shape}\")\n\n\ndef estimate_zero3_model_states_mem_needs(total_params,\n                                          largest_layer_params,\n                                          num_gpus_per_node=1,\n                                          num_nodes=1,\n                                          cpu_offload=True,\n                                          cpu_offload_params=True,\n                                          zero_init=True,\n                                          additional_buffer_factor=1.5):\n\n    total_gpus = num_nodes * num_gpus_per_node\n    gpus_factor = 1 / num_nodes\n    largest_layer_memory = (4 * largest_layer_params)\n\n    if cpu_offload:\n        if cpu_offload_params:\n            gpu_mem = largest_layer_memory\n\n            if zero_init:\n                cpu_mem = total_params * 18 * gpus_factor * additional_buffer_factor\n            else:\n                cpu_mem = total_params * max(4 * num_gpus_per_node, 18 * gpus_factor) * additional_buffer_factor\n\n        else:\n            gpu_mem = largest_layer_memory + int(2 * total_params / total_gpus)\n\n            if zero_init:\n                cpu_mem = total_params * 16 * gpus_factor * additional_buffer_factor\n            else:\n                cpu_mem = total_params * max(4 * num_gpus_per_node, 16 * gpus_factor) * additional_buffer_factor\n    else:\n        gpu_mem = largest_layer_memory + int(18 * total_params / total_gpus)\n        if zero_init:\n            cpu_mem = largest_layer_params * 4 * num_gpus_per_node * additional_buffer_factor\n        else:\n            cpu_mem = total_params * 4 * num_gpus_per_node * additional_buffer_factor\n\n    return int(cpu_mem), int(gpu_mem), largest_layer_memory\n\n\ndef model_to_params(model):\n    # shared params calculated only once\n    total_params = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters()).values())\n\n    largest_layer_params = 0\n    for m in model.modules():\n        # assuming no shared params within a single layer\n        layer_params = sum(p.numel() for p in m.parameters(recurse=False))\n        largest_layer_params = max(largest_layer_params, layer_params)\n\n    return total_params, largest_layer_params\n\n\ndef estimate_zero3_model_states_mem_needs_all_live(model,\n                                                   num_gpus_per_node=1,\n                                                   num_nodes=1,\n                                                   additional_buffer_factor=1.5):\n    \"\"\"\n    Print out estimates on memory usage requirements for ZeRO 3 params, optim states and gradients\n    for a given ``model`` and hardware setup.\n\n    If you have an actual model object, use this function and everything will be derived\n    automatically.\n\n    If it's a hypothetical model, use ``estimate_zero3_model_states_mem_needs_all_cold`` where you have to pass\n    the ``total_params`` and ``largest_layer_params`` explicitly.\n\n    Args:\n        - ``model``: ``nn.Module`` object\n        - ``num_gpus_per_node``: how many gpus per node (defaults to 1)\n        - ``num_nodes``: how many nodes (defaults to 1),\n        - ``additional_buffer_factor``: estimation factor (defaults to 1.5):\n\n    \"\"\"\n\n    total_params, largest_layer_params = model_to_params(model)\n\n    estimate_zero3_model_states_mem_needs_all_cold(total_params=total_params,\n                                                   largest_layer_params=largest_layer_params,\n                                                   num_gpus_per_node=num_gpus_per_node,\n                                                   num_nodes=num_nodes,\n                                                   additional_buffer_factor=additional_buffer_factor)\n\n\ndef estimate_zero3_model_states_mem_needs_all_cold(total_params,\n                                                   largest_layer_params,\n                                                   num_gpus_per_node=1,\n                                                   num_nodes=1,\n                                                   additional_buffer_factor=1.5):\n    \"\"\"\n    Print out estimates on memory usage requirements for ZeRO 3 params, optim states and gradients\n    for a given ``model`` and hardware setup.\n\n    If it's a hypothetical model, use this function where you have to pass\n    the ``total_params`` and ``largest_layer_params`` explicitly.\n\n    If you have an actual model object, use ``estimate_zero3_model_states_mem_needs_all_live`` and everything\n    will be derived automatically.\n\n    Args:\n        - ``total_params``: total  model params\n        - ``largest_layer_params``: largest layer's params\n        - ``num_gpus_per_node``: how many gpus per node (defaults to 1)\n        - ``num_nodes``: how many nodes (defaults to 1),\n        - ``additional_buffer_factor``: estimation factor (defaults to 1.5):\n\n    \"\"\"\n\n    def format_options(cpu_offload, cpu_offload_params, zero_init):\n        enabled = []\n        padded_cpu_str = f'{OffloadDeviceEnum.cpu:4}'\n        param_device = padded_cpu_str if cpu_offload_params else \"none\"\n        enabled.append(f\"offload_param={param_device}\")\n        optimizer_device = padded_cpu_str if cpu_offload else \"none\"\n        enabled.append(f\"offload_optimizer={optimizer_device}\")\n        enabled.append(f\"zero_init={1 if zero_init else 0}\")\n        return \", \".join(enabled)\n\n    nodes_str = \"nodes\" if num_nodes > 1 else \"node\"\n    gpus_str = \"GPUs\" if num_gpus_per_node > 1 else \"GPU\"\n    print(\n        \"Estimated memory needed for params, optim states and gradients for a:\\n\"\n        f\"HW: Setup with {num_nodes} {nodes_str}, {num_gpus_per_node} {gpus_str} per node.\\n\"\n        f\"SW: Model with {int(total_params/1e6)}M total params, {int(largest_layer_params/1e6)}M largest layer params.\"\n    )\n    print(\"  per CPU  |  per GPU |   Options\")\n    for cpu_offload in [True, False]:\n        for cpu_offload_params in [True, False]:\n            if not cpu_offload and cpu_offload_params:\n                continue\n            for zero_init in [True, False]:\n                cpu_mem, gpu_mem, largest_layer_memory = estimate_zero3_model_states_mem_needs(\n                    total_params=total_params,\n                    largest_layer_params=largest_layer_params,\n                    num_gpus_per_node=num_gpus_per_node,\n                    num_nodes=num_nodes,\n                    cpu_offload=cpu_offload,\n                    cpu_offload_params=cpu_offload_params,\n                    zero_init=zero_init,\n                    additional_buffer_factor=additional_buffer_factor)\n\n                options_str = format_options(cpu_offload=cpu_offload,\n                                             cpu_offload_params=cpu_offload_params,\n                                             zero_init=zero_init)\n                print(f\" {cpu_mem/2**30:7.2f}GB | {gpu_mem/2**30:6.2f}GB | {options_str}\")\n", "deepspeed/runtime/zero/stage_1_and_2.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nfrom deepspeed import comm as dist\nfrom packaging import version as pkg_version\nfrom collections import OrderedDict\nfrom torch._utils import _flatten_dense_tensors, _unflatten_dense_tensors\n\nfrom deepspeed.runtime.base_optimizer import ZeROOptimizer\nfrom deepspeed.runtime.fp16.loss_scaler import CreateLossScaler\nfrom deepspeed.runtime.utils import (empty_cache, see_memory_usage, inf, is_model_parallel_parameter,\n                                     align_dense_tensors, all_gather_dp_groups)\nfrom deepspeed.runtime.zero.config import ZeroStageEnum\nfrom deepspeed.runtime.zero.offload_config import OffloadDeviceEnum\nfrom deepspeed.ops.adam import DeepSpeedCPUAdam\nfrom deepspeed.utils import logger\nfrom deepspeed.utils.bwc import bwc_tensor_model_parallel_rank\nfrom deepspeed.moe.utils import is_moe_param\nfrom deepspeed.git_version_info import version\n\nfrom deepspeed.runtime.constants import PIPE_REPLICATED\nfrom deepspeed.accelerator import get_accelerator\n\nfrom deepspeed.checkpoint.constants import (DS_VERSION, GROUP_PADDINGS, PARTITION_COUNT, LOSS_SCALER,\n                                            SINGLE_PARTITION_OF_FP32_GROUPS, BASE_OPTIMIZER_STATE,\n                                            BASE_OPTIMIZER_STATE_STEP, CLIP_GRAD, ZERO_STAGE, PARAM_SLICE_MAPPINGS)\nfrom deepspeed.utils import link_hp_params, lazy_init_hp_params_optimizer_state\nfrom deepspeed.checkpoint import enable_universal_checkpoint\n\nfrom deepspeed.utils import groups\n# Toggle this to true to enable correctness test\n# with gradient partitioning and without\npg_correctness_test = False\n\nOPTIMIZER_ALLGATHER_TIMER = 'optimizer_allgather'\nOPTIMIZER_GRADIENTS_TIMER = 'optimizer_gradients'\nOPTIMIZER_STEP_TIMER = 'optimizer_step'\nOPTIMIZER_TIMERS = [OPTIMIZER_ALLGATHER_TIMER, OPTIMIZER_GRADIENTS_TIMER, OPTIMIZER_STEP_TIMER]\n\n\ndef input(msg):\n    return\n\n\ndef split_half_float_double(tensors):\n    device_type = get_accelerator().device_name()\n    dtypes = [\n        \"torch.{}.HalfTensor\".format(device_type), \"torch.{}.FloatTensor\".format(device_type),\n        \"torch.{}.DoubleTensor\".format(device_type), \"torch.{}.BFloat16Tensor\".format(device_type)\n    ]\n    buckets = []\n    for i, dtype in enumerate(dtypes):\n        bucket = [t for t in tensors if t.type() == dtype]\n        if bucket:\n            buckets.append(bucket)\n    return buckets\n\n\ndef isclose(a, b, rtol=1e-09, atol=0.0):\n    return abs(a - b) <= max(rtol * max(abs(a), abs(b)), atol)\n\n\ndef lcm(x, y):\n    from fractions import gcd  # or can import gcd from `math` in Python 3\n    return x * y // gcd(x, y)\n\n\ndef get_alignment_padding(tensor_list, alignment):\n    num_elements = sum([tensor.numel() for tensor in tensor_list])\n    remainder = num_elements % alignment\n    return (alignment - remainder) if remainder else remainder\n\n\ndef print_rank_msg(msg):\n    print(f\"rank {dist.get_rank()} - {msg}\")\n\n\ndef _get_padded_tensor(src_tensor, size):\n    if src_tensor.numel() >= size:\n        return src_tensor\n    padded_tensor = torch.zeros(size, dtype=src_tensor.dtype, device=src_tensor.device)\n    slice_tensor = torch.narrow(padded_tensor, 0, 0, src_tensor.numel())\n    slice_tensor.data.copy_(src_tensor.data)\n    return padded_tensor\n\n\ndef _pad_tensor_by_size(src_tensor, pad_size, dtype, device):\n    padded_tensor = torch.zeros(src_tensor.numel() + pad_size, dtype=dtype, device=device)\n    padded_tensor.data[:src_tensor.numel()].copy_(src_tensor.data)\n    return padded_tensor\n\n\nclass DeepSpeedZeroOptimizer(ZeROOptimizer):\n    \"\"\"\n    DeepSpeedZeroOptimizer designed to reduce the memory footprint\n    required for training large deep learning models.\n\n    For more details please see ZeRO: Memory Optimization Towards Training A Trillion Parameter Models\n    https://arxiv.org/abs/1910.02054\n\n    For usage examples, refer to TODO: DeepSpeed Tutorial\n\n    \"\"\"\n\n    def __init__(self,\n                 init_optimizer,\n                 param_names,\n                 timers,\n                 static_loss_scale=1.0,\n                 dynamic_loss_scale=False,\n                 dynamic_loss_args=None,\n                 verbose=True,\n                 contiguous_gradients=True,\n                 reduce_bucket_size=500000000,\n                 use_multi_rank_bucket_allreduce=True,\n                 allgather_bucket_size=5000000000,\n                 dp_process_group=None,\n                 expert_parallel_group=None,\n                 expert_data_parallel_group=None,\n                 reduce_scatter=True,\n                 overlap_comm=False,\n                 offload_optimizer_config=None,\n                 mpu=None,\n                 clip_grad=0.0,\n                 gradient_accumulation_dtype=torch.float32,\n                 communication_data_type=torch.float16,\n                 postscale_gradients=True,\n                 gradient_predivide_factor=1.0,\n                 gradient_accumulation_steps=1,\n                 ignore_unused_parameters=True,\n                 partition_grads=True,\n                 round_robin_gradients=False,\n                 has_moe_layers=False,\n                 fp16_master_weights_and_gradients=False,\n                 elastic_checkpoint=False):\n\n        if offload_optimizer_config is not None and offload_optimizer_config.device != OffloadDeviceEnum.none:\n            self.cpu_offload = True\n            self.cpu_offload_pin_memory = offload_optimizer_config.pin_memory\n        else:\n            self.cpu_offload = False\n            self.cpu_offload_pin_memory = False\n\n        if dist.get_rank() == 0:\n            logger.info(f\"Reduce bucket size {reduce_bucket_size}\")\n            logger.info(f\"Allgather bucket size {allgather_bucket_size}\")\n            logger.info(f\"CPU Offload: {self.cpu_offload}\")\n            logger.info(f'Round robin gradient partitioning: {round_robin_gradients}')\n        # The fused optimizer does all the work. We need this layer for two reason:\n        # 1. maintain same user API from apex.fp16_utils\n        # 2. keep common stuff here in case we need to add ne552w fused optimizer later\n\n        self.elastic_checkpoint = elastic_checkpoint\n        self.param_names = param_names\n        self.mpu = mpu\n        # differences from apex.fp16_utils:\n        # - assume all model params in fp16\n        # - assume all params requires grad\n        # - flat by groups, not keeping state. TODO: remove state explicitly?\n        # - master grad and unflat master weight never exist. TODO: a way to save out unflat master?\n        if not get_accelerator().is_available():\n            raise SystemError(\"Accelerator is not detected, cannot perform low precision training (e.g., fp16, bf16).\")\n        self.optimizer = init_optimizer\n\n        # Use torch (un)flatten ops\n        self.flatten = _flatten_dense_tensors\n        self.unflatten = _unflatten_dense_tensors\n\n        # ZeRO stage 1 (False) or 2 (True)\n        self.partition_gradients = partition_grads\n        self.zero_stage_string = \"ZeRO-2\" if partition_grads else \"ZeRO-1\"\n\n        self.timers = timers\n\n        self.reduce_scatter = reduce_scatter\n\n        self.overlap_comm = overlap_comm\n\n        self.deepspeed_adam_offload = self.cpu_offload\n\n        self.device = get_accelerator().current_device_name() if not self.cpu_offload else 'cpu'\n\n        self.dp_process_group = dp_process_group\n        self.sequence_parallel_size = groups._get_sequence_parallel_world_size()\n        #expert parallel group\n        self.ep_process_group = expert_parallel_group\n\n        #data parallel group for experts\n        self.expert_dp_process_group = expert_data_parallel_group\n\n        #data parallel size for non-experts\n        dp_size = dist.get_world_size(group=self.dp_process_group)\n\n        #For MoE models this maybe different for different param group\n        #It will be modified during MoE setup later in the init\n        self.real_dp_process_group = [dp_process_group for i in range(len(self.optimizer.param_groups))]\n        self.partition_count = [dp_size for i in range(len(self.optimizer.param_groups))]\n\n        self.is_gradient_accumulation_boundary = True\n\n        # CPU-Offload requires contiguous gradients\n        self.contiguous_gradients = contiguous_gradients or self.cpu_offload\n\n        self.has_moe_layers = has_moe_layers\n        if self.has_moe_layers:\n            self._configure_moe_settings()\n        self._global_grad_norm = 0.\n\n        if mpu is None:\n            self.model_parallel_group = None\n            self.model_parallel_world_size = 1\n            self.model_parallel_rank = 0\n        else:\n            self.model_parallel_group = mpu.get_model_parallel_group()\n            self.model_parallel_world_size = mpu.get_model_parallel_world_size()\n            self.model_parallel_rank = bwc_tensor_model_parallel_rank(mpu)\n\n        self.overflow = False\n        self.clip_grad = clip_grad\n        self.communication_data_type = communication_data_type\n        self.gradient_predivide_factor = gradient_predivide_factor\n        self.postscale_gradients = postscale_gradients\n        self.gradient_accumulation_steps = gradient_accumulation_steps\n        self.micro_step_id = 0\n        self.ignore_unused_parameters = ignore_unused_parameters\n        self.round_robin_gradients = round_robin_gradients\n\n        self.extra_large_param_to_reduce = None\n        self.fp16_master_weights_and_gradients = fp16_master_weights_and_gradients\n\n        if self.fp16_master_weights_and_gradients:\n            assert self.cpu_offload and type(self.optimizer) in [DeepSpeedCPUAdam], \\\n            f\"fp16_master_and_gradients requires optimizer to support keeping fp16 master and gradients while keeping the optimizer states in fp32.\"\\\n            f\"Currently only supported using ZeRO-Offload with DeepSpeedCPUAdam. But current setting is ZeRO-Offload:{self.cpu_offload} and optimizer type {type(self.optimizer)}.\" \\\n            f\"Either disable fp16_master_weights_and_gradients or enable {self.zero_stage_string} Offload with DeepSpeedCPUAdam.\"\n\n        if self.reduce_scatter and self.partition_gradients:\n            valid_reduce_scatter_dtypes = (torch.float16, torch.bfloat16, torch.float32)\n            assert self.communication_data_type in valid_reduce_scatter_dtypes, f\"{self.zero_stage_string} supports {valid_reduce_scatter_dtypes} communication_data_type with reduce scatter enabled. Got: '{self.communication_data_type}'\"\n            assert self.gradient_predivide_factor == 1.0, f\"gradient_predivide_factor != 1.0 is not yet supported with {self.zero_stage_string} with reduce scatter enabled\"\n            assert self.postscale_gradients, f\"pre-scale gradients is not yet supported with {self.zero_stage_string} with reduce scatter enabled\"\n\n        # param flattened by groups\n        self.bit16_groups = []\n        self.bit16_groups_flat = []\n\n        # param partitioned by data parallel degree\n        # this will contain a list of equal sized tensors\n        # each of which will be updated by a different process\n        self.parallel_partitioned_bit16_groups = []\n\n        # a single 32-bit partition of the parallel partitioned parameters\n        # that this process will update\n        self.single_partition_of_fp32_groups = []\n\n        # param partition info\n\n        # These are the parameters in each group that will not be updated by this process directly\n        self.params_not_in_partition = []\n\n        # These are the parameters that will be updated by this process directly\n        self.params_in_partition = []\n\n        # Offset from the first parameter in the self.params_in_partition\n        # the parameter boundaries may not align with partition boundaries\n        # so we need to keep track of the offset\n        self.first_offset = []\n\n        # number of elements per partition in each group\n        self.partition_size = []\n\n        # align nccl all-gather send buffers to 4-byte boundary\n        self.nccl_start_alignment_factor = 2  # 4-byte alignment/sizeof(fp16) = 2\n\n        assert (\n            allgather_bucket_size % self.nccl_start_alignment_factor == 0\n        ), f\"allgather_bucket_size must be a multiple of nccl_start_alignment_factor, {self.nccl_start_alignment_factor} \"\n\n        self.all_reduce_print = False\n        self.dtype = self.optimizer.param_groups[0]['params'][0].dtype\n        self.gradient_accumulation_dtype = gradient_accumulation_dtype\n\n        if self.dtype != self.gradient_accumulation_dtype:\n            self.use_separate_grad_accum = True\n        else:\n            self.use_separate_grad_accum = False\n        if self.use_separate_grad_accum and not self.partition_gradients:\n            self.use_grad_accum_attribute = True\n        else:\n            self.use_grad_accum_attribute = False\n\n        self.round_robin_bit16_groups = []\n        self.round_robin_bit16_indices = []\n        self.round_robin_bit16_meta = []\n\n        # Use different parallel to do all_to_all_reduce related things\n        # padding on each partition for alignment purposes\n        self.groups_padding = []\n        # loop to deal with groups\n        for i, param_group in enumerate(self.optimizer.param_groups):\n            partition_id = dist.get_rank(group=self.real_dp_process_group[i])\n\n            # push this group to list before modify\n            # TODO: Explore simplification that avoids the extra book-keeping by pushing the reordered group\n            trainable_parameters = []\n            for param in param_group['params']:\n                if param.requires_grad:\n                    param.grad_accum = None\n                    trainable_parameters.append(param)\n            self.bit16_groups.append(trainable_parameters)\n\n            # not sure why apex was cloning the weights before flattening\n            # removing cloning here\n\n            see_memory_usage(f\"Before moving param group {i} to CPU\")\n            # move all the parameters to cpu to free up GPU space for creating flat buffer\n\n            # Create temp CPU param copies, free accelerator tensors\n            orig_group_numel = 0\n            for param in self.bit16_groups[i]:\n                orig_group_numel += param.numel()\n                param.cpu_data = param.data.cpu()\n                param.data = torch.empty(1).to(param.device)\n\n            empty_cache()\n            see_memory_usage(f\"After moving param group {i} to CPU\", force=False)\n\n            # Reorder group parameters for load balancing of gradient partitioning during backward among ranks.\n            # This ensures that gradients are reduced in a fashion such that ownership round robins among the ranks.\n            # For example, rather than 3 gradients (g_n+2, g_n+1, g_n) that are reduced consecutively belonging\n            # to the same rank, instead they will belong to 3 ranks (r_m+2, r_m+1, r_m).\n            if self.round_robin_gradients:\n                round_robin_tensors, round_robin_indices = self._round_robin_reorder(\n                    self.bit16_groups[i], dist.get_world_size(group=self.real_dp_process_group[i]))\n            else:\n                round_robin_tensors = self.bit16_groups[i]\n                round_robin_indices = list(range(len(self.bit16_groups[i])))\n\n            self.round_robin_bit16_groups.append(round_robin_tensors)\n            self.round_robin_bit16_indices.append(round_robin_indices)\n\n            # Create meta tensors list, ordered according to round_robin_tensors\n            meta_tensors = []\n            for param in round_robin_tensors:\n                meta_tensors.append(torch.zeros_like(param.cpu_data, device=\"meta\"))\n            self.round_robin_bit16_meta.append(meta_tensors)\n\n            # create flat buffer in CPU\n            flattened_buffer = self.flatten_dense_tensors_aligned(\n                self.round_robin_bit16_groups[i],\n                self.nccl_start_alignment_factor * dist.get_world_size(group=self.real_dp_process_group[i]),\n                use_cpu_data=True)\n\n            # free temp CPU params\n            for param in self.bit16_groups[i]:\n                del param.cpu_data\n\n            # Move CPU flat tensor to the accelerator memory.\n            self.bit16_groups_flat.append(flattened_buffer.to(get_accelerator().current_device_name()))\n            del flattened_buffer\n\n            see_memory_usage(f\"After flattening and moving param group {i} to GPU\", force=False)\n\n            # Record padding required for alignment\n            if partition_id == dist.get_world_size(group=self.real_dp_process_group[i]) - 1:\n                padding = self.bit16_groups_flat[i].numel() - orig_group_numel\n            else:\n                padding = 0\n            self.groups_padding.append(padding)\n\n            if dist.get_rank(group=self.real_dp_process_group[i]) == 0:\n                see_memory_usage(f\"After Flattening and after emptying param group {i} cache\", force=False)\n\n            # set model bit16 weight to slices of flattened buffer\n            self._update_model_bit16_weights(i)\n\n            # divide the flat weights into near equal partition equal to the data parallel degree\n            # each process will compute on a different part of the partition\n            data_parallel_partitions = self.get_data_parallel_partitions(self.bit16_groups_flat[i], i)\n            self.parallel_partitioned_bit16_groups.append(data_parallel_partitions)\n\n            # verify that data partition start locations are 4-byte aligned\n            for partitioned_data in data_parallel_partitions:\n                assert (partitioned_data.data_ptr() % (2 * self.nccl_start_alignment_factor) == 0)\n\n            # A partition of the fp32 master weights that will be updated by this process.\n            # Note that the params in single_partition_of_fp32_groups is cloned and detached\n            # from the origin params of the model.\n            if not fp16_master_weights_and_gradients:\n                weights_partition = self.parallel_partitioned_bit16_groups[i][partition_id].to(\n                    self.device).clone().float().detach()\n            else:\n                weights_partition = self.parallel_partitioned_bit16_groups[i][partition_id].to(\n                    self.device).clone().half().detach()\n\n            if self.cpu_offload:\n                weights_partition = get_accelerator().pin_memory(weights_partition)\n\n            self.single_partition_of_fp32_groups.append(weights_partition)\n\n            # Set local optimizer to have flat params of its own partition.\n            # After this, the local optimizer will only contain its own partition of params.\n            # In that case, the local optimizer only saves the states(momentum, variance, etc.) related to its partition's params(zero stage1).\n            self.single_partition_of_fp32_groups[\n                i].requires_grad = True  # keep this in case internal optimizer uses it\n            param_group['params'] = [self.single_partition_of_fp32_groups[i]]\n\n            partition_size = len(self.bit16_groups_flat[i]) / dist.get_world_size(group=self.real_dp_process_group[i])\n            params_in_partition, params_not_in_partition, first_offset = self.get_partition_info(\n                self.round_robin_bit16_groups[i], partition_size, partition_id)\n\n            self.partition_size.append(partition_size)\n            self.params_in_partition.append(params_in_partition)\n            self.params_not_in_partition.append(params_not_in_partition)\n            self.first_offset.append(first_offset)\n\n        self.reduce_bucket_size = int(reduce_bucket_size)\n        self.use_multi_rank_bucket_allreduce = use_multi_rank_bucket_allreduce\n        self.allgather_bucket_size = int(allgather_bucket_size)\n\n        self.reduction_stream = None if get_accelerator().is_synchronized_device() else get_accelerator().Stream()\n        #self.copy_grad_stream = get_accelerator().Stream()\n        self.callback_queued = False\n\n        self.param_dict = {}\n\n        # map between param_id and bool to specify if a param is in this partition\n        self.is_param_in_current_partition = {}\n\n        self.grads_in_ipg_bucket = []\n        self.params_in_ipg_bucket = []\n        self.elements_in_ipg_bucket = 0\n        self.params_already_reduced = []\n        self._release_ipg_buffers()\n        self.previous_reduced_grads = None\n        self.ipg_bucket_has_moe_params = False\n\n        # simplified param id\n        self.param_id = {}\n\n        #interesting code: unique ids being assigned to individual parameters\n        largest_param_numel = 0\n        count = 0\n        for i, params_group in enumerate(self.bit16_groups):\n            for param in params_group:\n                unique_id = id(param)\n                self.param_id[unique_id] = count\n                self.param_dict[count] = param\n                self.params_already_reduced.append(False)\n                if param.numel() > largest_param_numel:\n                    largest_param_numel = param.numel()\n                count = count + 1\n\n        for param_group in self.params_in_partition:\n            for param in param_group:\n                self.is_param_in_current_partition[self.get_param_id(param)] = True\n\n        for param_group in self.params_not_in_partition:\n            for param in param_group:\n                self.is_param_in_current_partition[self.get_param_id(param)] = False\n\n        if self.cpu_offload:\n            self.accumulated_grads_in_cpu = {}\n            self.norm_for_param_grads = {}\n            self.local_overflow = False\n            self.grad_position = {}\n            self.temp_grad_buffer_for_cpu_offload = torch.zeros(largest_param_numel,\n                                                                device=self.device,\n                                                                dtype=self.dtype)\n            if self.cpu_offload_pin_memory:\n                self.temp_grad_buffer_for_cpu_offload = get_accelerator().pin_memory(\n                    self.temp_grad_buffer_for_cpu_offload)\n            self.temp_grad_buffer_for_gpu_offload = torch.zeros(largest_param_numel,\n                                                                device=get_accelerator().current_device_name(),\n                                                                dtype=self.dtype)\n            for i, params_group in enumerate(self.bit16_groups):\n                self.get_grad_position(i, self.params_in_partition[i], self.first_offset[i], self.partition_size[i])\n\n        # mapping from parameter to partition that it belongs to\n        self.param_to_partition_ids = {}\n\n        # stores if a partition has been reduced in this step\n        self.is_partition_reduced = {}\n\n        # number of grads in partition that still need to be computed\n        self.remaining_grads_in_partition = {}\n\n        # total number of grads in partition\n        self.total_grads_in_partition = {}\n\n        # stores if a grad in a partition has been computed or not\n        self.is_grad_computed = {}\n\n        # stores the offset at which a parameter gradient needs to be inserted in a partition\n        self.grad_partition_insertion_offset = {}\n\n        # the offset in the gradient at which it must be inserted at the beginning of the partition\n        self.grad_start_offset = {}\n\n        # will store the averaged gradients required by this partition\n        self.averaged_gradients = {}\n\n        # For cpu_offload, will store the averaged gradients required by this partition\n        self.offload_gradient_dict = {}\n\n        # store index of first parameter in each partition\n        self.first_param_index_in_partition = {}\n\n        # initializes all data structures for implementing gradient partitioning\n        self.initialize_gradient_partitioning_data_structures()\n\n        # resets the data structure value for the next backward propagation\n        self.reset_partition_gradient_structures()\n\n        # creates backward hooks for gradient partitioning\n        self._grad_acc_hooks = []\n        if self.partition_gradients or self.overlap_comm:\n            self.create_reduce_and_remove_grad_hooks()\n\n        self.custom_loss_scaler = False\n        self.external_loss_scale = None\n\n        # we may have a way of fusing dynamic scale. Do not support for now\n        self.loss_scaler = CreateLossScaler(dtype=self.dtype,\n                                            static_loss_scale=static_loss_scale,\n                                            dynamic_scaling=dynamic_loss_scale,\n                                            dynamic_loss_args=dynamic_loss_args)\n        self.dynamic_loss_scale = self.loss_scaler.dynamic\n\n        if self.dtype != torch.float16:\n            # Only fp16 should use dynamic loss scaling\n            assert self.loss_scaler.cur_scale == 1.0\n            assert not self.dynamic_loss_scale\n\n        see_memory_usage(\"Before initializing optimizer states\", force=True)\n        self.initialize_optimizer_states()\n        see_memory_usage(\"After initializing optimizer states\", force=True)\n\n        if dist.get_rank() == 0:\n            logger.info(f\"optimizer state initialized\")\n\n        if dist.get_rank(group=self.dp_process_group) == 0:\n            see_memory_usage(f\"After initializing ZeRO optimizer\", force=True)\n\n        self._link_all_hp_params()\n        self._hp_optimizer_states_linked = False\n\n        self._enable_universal_checkpoint()\n        self._param_slice_mappings = self._create_param_mapping()\n\n    def destroy(self):\n        for hook in self._grad_acc_hooks:\n            hook.remove()\n        self.print_rank_0(\"Removed grad acc hooks\")\n\n    def _enable_universal_checkpoint(self):\n        for lp_param_group in self.bit16_groups:\n            enable_universal_checkpoint(param_list=lp_param_group)\n\n    def _create_param_mapping(self):\n        param_mapping = []\n        for i, _ in enumerate(self.optimizer.param_groups):\n            param_mapping_per_group = OrderedDict()\n            for lp in self.bit16_groups[i]:\n                if lp._hp_mapping is not None:\n                    lp_name = self.param_names[lp]\n                    param_mapping_per_group[lp_name] = lp._hp_mapping.get_hp_fragment_address()\n            param_mapping.append(param_mapping_per_group)\n\n        return param_mapping\n\n    def _link_all_hp_params(self):\n        dp_world_size = dist.get_world_size(group=self.dp_process_group)\n        if self.cpu_offload:\n            self._get_offload_gradient_dict()\n\n        for i, _ in enumerate(self.optimizer.param_groups):\n            # Link bit16 and fp32 params in partition\n            partition_id = dist.get_rank(group=self.real_dp_process_group[i])\n            partition_size = self.bit16_groups_flat[i].numel() // dp_world_size\n            flat_hp_partition = self.single_partition_of_fp32_groups[i]\n            link_hp_params(lp_param_list=self.bit16_groups[i],\n                           flat_hp_partition=flat_hp_partition,\n                           gradient_dict=self.averaged_gradients,\n                           offload_gradient_dict=self.offload_gradient_dict,\n                           use_offload=self.cpu_offload,\n                           param_group_index=i,\n                           partition_start=partition_id * partition_size,\n                           partition_size=partition_size,\n                           dp_group=self.real_dp_process_group[i])\n\n    def _lazy_init_hp_params_optimizer_state(self):\n        if not self._hp_optimizer_states_linked:\n            for i, _ in enumerate(self.optimizer.param_groups):\n                lazy_init_hp_params_optimizer_state(self.bit16_groups[i], self.single_partition_of_fp32_groups[i],\n                                                    self.optimizer.state)\n            self._hp_optimizer_states_linked = True\n\n    def is_moe_group(self, group):\n        return 'moe' in group and group['moe']\n\n    def _configure_moe_settings(self):\n        # if we're using ZeRO stage 2, ensure contiguous gradients are used\n        if self.partition_gradients:\n            assert self.contiguous_gradients, \"Contiguous Gradients in ZeRO Stage 2 must be set to True for MoE. Other code paths are not tested with MoE\"\n        # NOTE: To run ZeRO stage 1 with MoE, we need to set self.contiguous_gradients to True or ignore the assertion\n        if not self.partition_gradients and not self.contiguous_gradients:\n            logger.warn(\n                \"ZeRO Stage 1 has not been thoroughly tested with MoE. This configuration is still experimental.\")\n        assert self.reduce_scatter, \"Reduce Scatter in ZeRO Stage 2 must be set to True for MoE. Other code paths are not tested with MoE\"\n\n        assert any(\n            [self.is_moe_group(group) for group in self.optimizer.param_groups]\n        ), \"The model has moe layers, but None of the param groups are marked as MoE. Create a param group with 'moe' key set to True before creating optimizer\"\n        self.is_moe_param_group = []\n        for i, group in enumerate(self.optimizer.param_groups):\n            if self.is_moe_group(group):\n                assert all([is_moe_param(param)\n                            for param in group['params']]), \"All params in MoE group must be MoE params\"\n                self.real_dp_process_group[i] = self.expert_dp_process_group[group['name']]\n                self.partition_count[i] = dist.get_world_size(group=self.expert_dp_process_group[group['name']])\n                self.is_moe_param_group.append(True)\n            else:\n                self.is_moe_param_group.append(False)\n\n        assert self.expert_dp_process_group is not None, \"Expert data parallel group should be configured with MoE\"\n        assert self.ep_process_group is not None, \"Expert parallel group should be configured with MoE\"\n\n    def _update_model_bit16_weights(self, group_index):\n        updated_params = self.unflatten(self.bit16_groups_flat[group_index], self.round_robin_bit16_meta[group_index])\n        for p, q in zip(self.round_robin_bit16_groups[group_index], updated_params):\n            p.data = q.data\n\n        # set model fp16 weight to slices of reordered flattened buffer\n        for param_index, param in enumerate(self.bit16_groups[group_index]):\n            new_index = self.round_robin_bit16_indices[group_index][param_index]\n            param.data = self.round_robin_bit16_groups[group_index][new_index].data\n\n    def _round_robin_reorder(self, tensor_list, num_partitions):\n\n        # disable round robin if need to debug something\n        # return tensor_list, list(range(len(tensor_list)))\n\n        partition_tensors = {}\n\n        for i, tensor in enumerate(tensor_list):\n            j = i % num_partitions\n            if not j in partition_tensors:\n                partition_tensors[j] = []\n            partition_tensors[j].append((i, tensor))\n\n        reordered_tensors = []\n        reordered_indices = {}\n\n        for partition_index in partition_tensors.keys():\n            for i, (original_index, tensor) in enumerate(partition_tensors[partition_index]):\n                reordered_indices[original_index] = len(reordered_tensors)\n                reordered_tensors.append(tensor)\n\n        return reordered_tensors, reordered_indices\n\n    def _release_ipg_buffers(self):\n        if self.contiguous_gradients:\n            self.ipg_buffer = None\n            self.grads_in_partition = None\n            self.grads_in_partition_offset = 0\n\n    def initialize_optimizer_states(self):\n\n        for i, group in enumerate(self.bit16_groups):\n            single_grad_partition = torch.zeros(int(self.partition_size[i]),\n                                                dtype=self.single_partition_of_fp32_groups[i].dtype,\n                                                device=self.device)\n            self.single_partition_of_fp32_groups[i].grad = get_accelerator().pin_memory(\n                single_grad_partition) if self.cpu_offload_pin_memory else single_grad_partition\n\n        # Initialize the optimizer states with the flattened fp32 partition.\n        # State initialization for the Adagrad optimizer occurs at construction as opposed to other optimizers\n        # which do lazy initialization of the state at the first call to step.\n        if isinstance(self.optimizer, torch.optim.Adagrad):\n            self.optimizer = torch.optim.Adagrad(self.single_partition_of_fp32_groups, **self.optimizer.defaults)\n\n        if not self.cpu_offload:\n            for group in self.single_partition_of_fp32_groups:\n                group.grad = None  #class init\n\n        return\n\n    #########################################################################\n    #################### ZeRO Stage 1 - reduce gradients ####################\n    #########################################################################\n    def reduce_gradients(self, pipeline_parallel=False):\n        world_size = dist.get_world_size(self.dp_process_group)\n        my_rank = dist.get_rank(self.dp_process_group)\n\n        # with PP we must create ipg buffer, since backward is handled outside zero\n        if pipeline_parallel and self.contiguous_gradients:\n            self.ipg_buffer = []\n            buf_0 = torch.empty(int(self.reduce_bucket_size),\n                                dtype=self.dtype,\n                                device=get_accelerator().current_device_name())\n            self.ipg_buffer.append(buf_0)\n            self.ipg_index = 0\n\n        if not self.overlap_comm:\n            for i, group in enumerate(self.bit16_groups):\n                for param in group:\n                    grad_reduc = self.get_gradient_for_reduction(param)\n                    if grad_reduc is not None:\n                        self.reduce_ready_partitions_and_remove_grads(param, i)\n        # reduce any pending grads in either hook/non-hook case\n        self.overlapping_partition_gradients_reduce_epilogue()\n\n    #########################################################################\n    #########################ZeRO Partition Gradients########################\n    #########################################################################\n\n    def get_first_param_index(self, group_id, param_group, partition_id):\n        for index, param in enumerate(param_group):\n            param_id = self.get_param_id(param)\n            if partition_id in self.param_to_partition_ids[group_id][param_id]:\n                return index\n        return None\n\n    def initialize_gradient_partitioning_data_structures(self):\n\n        for i, param_group in enumerate(self.round_robin_bit16_groups):\n            total_partitions = dist.get_world_size(group=self.real_dp_process_group[i])\n\n            self.param_to_partition_ids[i] = {}\n            self.is_partition_reduced[i] = {}\n            self.total_grads_in_partition[i] = {}\n            self.remaining_grads_in_partition[i] = {}\n            self.is_grad_computed[i] = {}\n            self.grad_partition_insertion_offset[i] = {}\n            self.grad_start_offset[i] = {}\n            self.first_param_index_in_partition[i] = {}\n\n            for partition_id in range(total_partitions):\n                self.is_grad_computed[i][partition_id] = {}\n                self.grad_partition_insertion_offset[i][partition_id] = {}\n                self.grad_start_offset[i][partition_id] = {}\n                self.total_grads_in_partition[i][partition_id] = 0\n                self.initialize_gradient_partition(i, param_group, partition_id)\n                self.is_partition_reduced[i][partition_id] = False\n                self.first_param_index_in_partition[i][partition_id] = self.get_first_param_index(\n                    i, param_group, partition_id)\n\n    def independent_gradient_partition_epilogue(self):\n        self.report_ipg_memory_usage(f\"In ipg_epilogue before reduce_ipg_grads\", 0)\n        self.reduce_ipg_grads()\n        self.report_ipg_memory_usage(f\"In ipg_epilogue after reduce_ipg_grads\", 0)\n\n        # if dist.get_rank() == 0:\n        #    logger.info(\"Params already reduced %s\", self.params_already_reduced)\n        for i in range(len(self.params_already_reduced)):\n            self.params_already_reduced[i] = False\n\n        if self.overlap_comm:\n            if not get_accelerator().resolves_data_dependency():\n                get_accelerator().synchronize()\n            # It is safe to clear previously reduced grads of other partitions\n            self._clear_previous_reduced_grads()\n\n        if self.cpu_offload is False:\n            for i, _ in enumerate(self.bit16_groups):\n\n                if not i in self.averaged_gradients or self.averaged_gradients[i] is None:\n                    self.averaged_gradients[i] = self.get_flat_partition(\n                        self.params_in_partition[i],\n                        self.first_offset[i],\n                        self.partition_size[i],\n                        dtype=self.gradient_accumulation_dtype,\n                        device=get_accelerator().current_device_name(),\n                        return_tensor_list=True)\n                else:\n                    avg_new = self.get_flat_partition(self.params_in_partition[i],\n                                                      self.first_offset[i],\n                                                      self.partition_size[i],\n                                                      dtype=self.gradient_accumulation_dtype,\n                                                      device=get_accelerator().current_device_name(),\n                                                      return_tensor_list=True)\n\n                    for accumulated_grad, new_avg_grad in zip(self.averaged_gradients[i], avg_new):\n                        accumulated_grad.add_(new_avg_grad)\n\n        self._release_ipg_buffers()\n\n        # No need to keep the gradients anymore.\n        # All gradients required by the step\n        # are in self.averaged_gradients\n        self.zero_grad(set_to_none=True)\n        see_memory_usage(f\"End ipg_epilogue\")\n\n    # resets all partition to no reduced\n    # sets remaining grads to the total number of grads in each partition\n    # set is grad computed to false for all grads in partition\n    def reset_partition_gradient_structures(self):\n        for i, _ in enumerate(self.bit16_groups):\n            total_partitions = dist.get_world_size(group=self.real_dp_process_group[i])\n            for partition_id in range(total_partitions):\n                self.is_partition_reduced[i][partition_id] = False\n                self.remaining_grads_in_partition[i][partition_id] = self.total_grads_in_partition[i][partition_id]\n\n                for param_id in self.is_grad_computed[i][partition_id]:\n                    self.is_grad_computed[i][partition_id][param_id] = False\n\n    def initialize_gradient_partition(self, i, param_group, partition_id):\n\n        def set_key_value_list(dictionary, key, value):\n            if key in dictionary:\n                dictionary[key].append(value)\n            else:\n                dictionary[key] = [value]\n\n        def increment_value(dictionary, key):\n            if key in dictionary:\n                dictionary[key] += 1\n            else:\n                dictionary[key] = 1\n\n        partition_size = self.partition_size[i]\n\n        start_index = partition_size * partition_id\n        end_index = partition_size * (partition_id + 1)\n\n        current_index = 0\n        first_offset = 0\n\n        for param in param_group:\n\n            param_size = param.numel()\n            param_id = self.get_param_id(param)\n\n            if start_index <= current_index < end_index:\n                set_key_value_list(self.param_to_partition_ids[i], param_id, partition_id)\n                increment_value(self.total_grads_in_partition[i], partition_id)\n\n                self.is_grad_computed[i][partition_id][param_id] = False\n\n                self.grad_partition_insertion_offset[i][partition_id][param_id] = current_index - start_index\n                self.grad_start_offset[i][partition_id][param_id] = 0\n\n            elif current_index < start_index < (current_index + param_size):\n                assert (first_offset == 0\n                        ), \"This can happen either zero or only once as this must be the first tensor in the partition\"\n                first_offset = start_index - current_index\n\n                set_key_value_list(self.param_to_partition_ids[i], param_id, partition_id)\n                increment_value(self.total_grads_in_partition[i], partition_id)\n\n                self.is_grad_computed[i][partition_id][param_id] = False\n\n                self.grad_partition_insertion_offset[i][partition_id][param_id] = 0\n                self.grad_start_offset[i][partition_id][param_id] = first_offset\n\n            current_index = current_index + param_size\n\n    def overlapping_partition_gradients_reduce_epilogue(self):\n        self.independent_gradient_partition_epilogue()\n\n    def fill_grad_accum_attribute(self):\n        for group in self.bit16_groups:\n            for param in group:\n                if param.grad is not None:\n                    if param.grad_accum is None:\n                        param.grad_accum = param.grad.to(self.gradient_accumulation_dtype)\n                    else:\n                        param.grad_accum.add_(\n                            param.grad.to(self.gradient_accumulation_dtype).view(param.grad_accum.shape))\n                    param.grad = None\n\n    def get_gradient_for_reduction(self, param):\n        if self.use_grad_accum_attribute:\n            return param.grad_accum.to(self.dtype) if param.grad_accum is not None else None\n        else:\n            return param.grad\n\n    def get_param_gradient_attribute(self, param):\n        return param.grad_accum if self.use_grad_accum_attribute else param.grad\n\n    # Clear the tensor the reduction gradient attribute is pointing to\n    def clear_grad_attribute(self, param):\n        if self.use_grad_accum_attribute:\n            param.grad_accum = None\n        else:\n            param.grad = None\n\n    def create_reduce_and_remove_grad_hooks(self):\n        self.grad_accs = []\n        for i, param_group in enumerate(self.bit16_groups):\n            for param in param_group:\n                if param.requires_grad:\n\n                    def wrapper(param, i):\n                        param_tmp = param.expand_as(param)\n                        grad_acc = param_tmp.grad_fn.next_functions[0][0]\n\n                        def reduce_partition_and_remove_grads(*notneeded):\n                            self.reduce_ready_partitions_and_remove_grads(param, i)\n\n                        self._grad_acc_hooks.append(grad_acc.register_hook(reduce_partition_and_remove_grads))\n                        self.grad_accs.append(grad_acc)\n\n                    wrapper(param, i)\n\n    def get_param_id(self, param):\n        unique_id = id(param)\n        return self.param_id[unique_id]\n\n    def report_ipg_memory_usage(self, tag, param_elems):\n        elem_count = self.elements_in_ipg_bucket + param_elems\n        percent_of_bucket_size = (100.0 * elem_count) // self.reduce_bucket_size\n        see_memory_usage(\n            f\"{tag}: elems in_bucket {self.elements_in_ipg_bucket} param {param_elems} max_percent {percent_of_bucket_size}\"\n        )\n\n    # create a flat tensor aligned at the alignment boundary\n    def flatten_dense_tensors_aligned(self, tensor_list, alignment, use_cpu_data=False):\n        tensor_list = [param.cpu_data for param in tensor_list] if use_cpu_data else tensor_list\n        return self.flatten(align_dense_tensors(tensor_list, alignment))\n\n    ############### Independent Partition Gradient ########################\n    def reduce_independent_p_g_buckets_and_remove_grads(self, param, i):\n\n        grad_reduc = self.get_gradient_for_reduction(param)\n        if self.elements_in_ipg_bucket + param.numel() > self.reduce_bucket_size:\n            self.report_ipg_memory_usage(\"In ipg_remove_grads before reduce_ipg_grads\", param.numel())\n            self.reduce_ipg_grads()\n            if self.contiguous_gradients and self.overlap_comm:\n                # Swap ipg_index between 0 and 1\n                self.ipg_index = 1 - self.ipg_index\n            self.report_ipg_memory_usage(\"In ipg_remove_grads after reduce_ipg_grads\", param.numel())\n\n        param_id = self.get_param_id(param)\n        assert self.params_already_reduced[param_id] == False, \\\n            f\"The parameter {param_id} has already been reduced. \\\n            Gradient computed twice for this partition. \\\n            Multiple gradient reduction is currently not supported\"\n\n        if self.contiguous_gradients:\n            if param.numel() > self.reduce_bucket_size:\n                self.extra_large_param_to_reduce = param\n            else:\n                # keeping the gradients contiguous to prevent memory fragmentation, and avoid flattening\n                new_grad_tensor = self.ipg_buffer[self.ipg_index].narrow(0, self.elements_in_ipg_bucket, param.numel())\n                new_grad_tensor.copy_(grad_reduc.view(-1))\n                grad_reduc.data = new_grad_tensor.data.view_as(grad_reduc)\n\n        self.elements_in_ipg_bucket += param.numel()\n\n        assert grad_reduc is not None, f\"rank {dist.get_rank()} - Invalid to reduce Param {param_id} with None gradient\"\n\n        self.grads_in_ipg_bucket.append(grad_reduc)\n        self.params_in_ipg_bucket.append((i, param, param_id))\n\n        #make sure the average tensor function knows how to average the gradients\n        if is_moe_param(param):\n            self.ipg_bucket_has_moe_params = True\n\n        self.report_ipg_memory_usage(\"End ipg_remove_grads\", 0)\n\n    def print_rank_0(self, message):\n        if dist.get_rank() == 0:\n            logger.info(message)\n\n    def gradient_reduction_w_predivide(self, tensor):\n\n        dp_world_size = dist.get_world_size(group=self.dp_process_group)\n\n        tensor_to_allreduce = tensor\n\n        if self.communication_data_type != tensor.dtype:\n            tensor_to_allreduce = tensor.to(self.communication_data_type)\n\n        if self.postscale_gradients:\n            if self.gradient_predivide_factor != 1.0:\n                tensor_to_allreduce.mul_(1. / self.gradient_predivide_factor)\n\n            dist.all_reduce(tensor_to_allreduce, group=self.dp_process_group)\n\n            if self.gradient_predivide_factor != dp_world_size:\n                tensor_to_allreduce.mul_(self.gradient_predivide_factor /\n                                         (dp_world_size / float(self.sequence_parallel_size)))\n        else:\n            tensor_to_allreduce.div_(dp_world_size / float(self.sequence_parallel_size))\n            dist.all_reduce(tensor_to_allreduce, group=self.dp_process_group)\n\n        if self.communication_data_type != tensor.dtype and tensor is not tensor_to_allreduce:\n            tensor.copy_(tensor_to_allreduce)\n\n        return tensor\n\n    def allreduce_and_copy_with_multiple_ranks(self,\n                                               small_bucket,\n                                               log=None,\n                                               divide=True,\n                                               process_group=None,\n                                               bucket_ranks=None):\n        process_group = self.dp_process_group if process_group is None else process_group\n        allreduced = self.allreduce_bucket(small_bucket, log=log, divide=divide, process_group=process_group)\n        for buf, synced, bucket_rank in zip(small_bucket, self.unflatten(allreduced, small_bucket), bucket_ranks):\n            if dist.get_rank(group=process_group) == bucket_rank:\n                buf.copy_(synced)\n\n    def allreduce_and_scatter(self, bucket, numel_per_bucket=500000000, log=None, divide=True, process_group=None):\n        small_bucket = []\n        small_bucket_ranks = []\n        numel = 0\n        allreduce_sizes = []\n\n        for i, bucket_elem in enumerate(bucket):\n            rank, tensor = bucket_elem\n            small_bucket.append(tensor)\n            small_bucket_ranks.append(rank)\n            numel = numel + tensor.numel()\n            if numel > numel_per_bucket:\n                self.allreduce_and_copy_with_multiple_ranks(small_bucket,\n                                                            log=None,\n                                                            divide=divide,\n                                                            process_group=process_group,\n                                                            bucket_ranks=small_bucket_ranks)\n                small_bucket = []\n                small_bucket_ranks = []\n                numel = 0\n\n        if len(small_bucket) > 0:\n            self.allreduce_and_copy_with_multiple_ranks(small_bucket,\n                                                        log=None,\n                                                        divide=divide,\n                                                        process_group=process_group,\n                                                        bucket_ranks=small_bucket_ranks)\n\n    def average_tensor(self, tensor):\n        if self.overlap_comm:\n            stream = self.reduction_stream\n            if not get_accelerator().resolves_data_dependency():\n                stream.wait_stream(get_accelerator().current_stream())\n                get_accelerator().current_stream().wait_stream(stream)\n        else:\n            stream = get_accelerator().current_stream()\n\n        with get_accelerator().stream(stream):\n            if not self.reduce_scatter:\n                self.gradient_reduction_w_predivide(tensor)\n                return\n\n            # Accumulate destination ranks and bucket offsets for each gradient slice.\n            # Note: potential future optimization, record access pattern of parameters\n            # in backward pass and partition gradients w.r.t. access pattern so that our\n            # bucket is guaranteed to be contiguous w.r.t. ranks\n            rank_and_offsets = []\n            real_dp_process_group = []\n            curr_size = 0\n            prev_id, prev_process_group = -1, None\n\n            process_group = self.dp_process_group\n            # count = 0\n            for i, param, param_id in self.params_in_ipg_bucket:\n\n                process_group = self.dp_process_group\n                grad_reduc = self.get_gradient_for_reduction(param)\n                #Averages gradients at parameter level if ipg has a moe param\n                #Otherwise averaging is done at the entire buffer level at the end of the loop\n                # MoE param have different groups\n                if self.ipg_bucket_has_moe_params:\n                    process_group = self.expert_dp_process_group[param.group_name] if is_moe_param(\n                        param) else self.dp_process_group\n                    grad_reduc.data.div_(dist.get_world_size(group=process_group) / float(self.sequence_parallel_size))\n\n                partition_ids = self.param_to_partition_ids[i][param_id]\n                assert all([p_id < dist.get_world_size(group=process_group) for p_id in partition_ids\n                            ]), f\"world size {dist.get_world_size(group=process_group)} and p_ids: {partition_ids}\"\n                partition_size = self.partition_size[i]\n                # Get all partition ids + their offsets\n                partition_ids_w_offsets = []\n                for partition_id in partition_ids:\n                    offset = self.grad_start_offset[i][partition_id][param_id]\n                    partition_ids_w_offsets.append((partition_id, offset))\n                partition_ids_w_offsets.sort(key=lambda t: t[1])\n\n                # Calculate rank and offsets for grad slices\n                for idx in range(len(partition_ids_w_offsets)):\n                    partition_id, offset = partition_ids_w_offsets[idx]\n\n                    # if dist.get_rank() == 0 and count < 100:\n                    #     print(f\"Rank {dist.get_rank()} rank offset id {idx} calculated dp size {dist.get_world_size(group=process_group)} real dp size {dist.get_world_size(self.real_dp_process_group[i])} and dst: {partition_id}\")\n                    # count += 1\n\n                    # Calculate numel for grad slice depending on partition location\n                    if idx == len(partition_ids_w_offsets) - 1:\n                        # Last partition_id uses its own offset\n                        numel = param.numel() - offset\n                    else:\n                        # Set numel to next partition's offset\n                        numel = partition_ids_w_offsets[idx + 1][1] - offset\n\n                    # Merge bucket ranges if they belong to the same rank\n                    if partition_id == prev_id and process_group == prev_process_group:\n                        prev_pid, prev_size, prev_numel = rank_and_offsets[-1]\n                        rank_and_offsets[-1] = (prev_pid, prev_size, prev_numel + numel)\n                    else:\n                        rank_and_offsets.append((partition_id, curr_size, numel))\n                        real_dp_process_group.append(process_group)\n                    curr_size += numel\n                    prev_id, prev_process_group = partition_id, process_group\n\n            if not self.ipg_bucket_has_moe_params:\n                tensor.div_(dist.get_world_size(group=self.dp_process_group) / float(self.sequence_parallel_size))\n\n            buckets = {}\n            for i, (dst, bucket_offset, numel) in enumerate(rank_and_offsets):\n                grad_slice = tensor.narrow(0, int(bucket_offset), int(numel))\n                bucket_key = real_dp_process_group[i] if self.use_multi_rank_bucket_allreduce else (\n                    dst, real_dp_process_group[i])\n                if bucket_key not in buckets:\n                    buckets[bucket_key] = []\n                if self.use_multi_rank_bucket_allreduce:\n                    buckets[bucket_key].append((dst, grad_slice))\n                else:\n                    buckets[bucket_key].append(grad_slice)\n\n            for bucket_key in buckets:\n                if self.use_multi_rank_bucket_allreduce:\n                    self.allreduce_and_scatter(buckets[bucket_key],\n                                               numel_per_bucket=self.reduce_bucket_size,\n                                               divide=False,\n                                               process_group=bucket_key)\n                else:\n                    dst, process_group = bucket_key\n                    self.allreduce_no_retain(buckets[bucket_key],\n                                             numel_per_bucket=self.reduce_bucket_size,\n                                             rank=dst,\n                                             divide=False,\n                                             process_group=process_group)\n\n    ##############################################################################\n    ############################# CPU Offload Methods#############################\n    ##############################################################################\n    def get_grad_position(self, group_id, tensor_list, first_offset, partition_size):\n        current_offset = 0\n\n        for i, tensor in enumerate(tensor_list):\n            param_id = self.get_param_id(tensor)\n            param_start_offset = 0\n\n            num_elements = tensor.numel()\n\n            # we need to offset to get to the right element\n            if i == 0 and first_offset > 0:\n                tensor_offset = first_offset\n                num_elements = num_elements - tensor_offset\n                param_start_offset = first_offset\n\n            # we dont need all elements of the tensor\n            if num_elements > (partition_size - current_offset):\n                num_elements = partition_size - current_offset\n\n            self.grad_position[param_id] = [\n                int(group_id), int(param_start_offset),\n                int(current_offset), int(num_elements)\n            ]\n            current_offset += num_elements\n\n    def update_overflow_tracker_for_param_grad(self, param):\n        grad_accum = self.get_param_gradient_attribute(param)\n        if grad_accum is not None and self._has_inf_or_nan(grad_accum.data):\n            self.local_overflow = True\n\n    def _get_offload_gradient_dict(self):\n        for param_group_index, _ in enumerate(self.optimizer.param_groups):\n            self.offload_gradient_dict[param_group_index] = []\n            for lp_param in self.params_in_partition[param_group_index]:\n                param_id = self.get_param_id(lp_param)\n                [_, _, dest_offset, num_elements] = self.grad_position[param_id]\n                dest_tensor = self.single_partition_of_fp32_groups[param_group_index].grad.view(-1).narrow(\n                    0, dest_offset, num_elements)\n                self.offload_gradient_dict[param_group_index].append(dest_tensor)\n\n    def async_accumulate_grad_in_cpu_via_gpu(self, param):\n        param_id = self.get_param_id(param)\n\n        [i, source_offset, dest_offset, num_elements] = self.grad_position[param_id]\n\n        # copy to a preexisiting buffer to avoid memory allocation penalty\n        dest_buffer = self.temp_grad_buffer_for_gpu_offload.view(-1).narrow(0, 0, param.numel())\n\n        #buffer for storing gradients for this parameter in CPU\n        def buffer_to_accumulate_to_in_cpu():\n            if not self.fp16_master_weights_and_gradients:\n                buffer = torch.zeros(param.numel(), dtype=param.dtype, device=self.device)\n                return get_accelerator().pin_memory(buffer) if self.cpu_offload_pin_memory else buffer\n            else:\n                return self.single_partition_of_fp32_groups[i].grad.view(-1).narrow(0, dest_offset, num_elements)\n\n        #accumulate gradients into param.grad_accum or parts of it that belongs to this partition\n        def accumulate_gradients():\n            grad_accum = self.get_param_gradient_attribute(param)\n            if not self.fp16_master_weights_and_gradients:\n                dest_buffer.copy_(self.accumulated_grads_in_cpu[param_id].view(-1), non_blocking=True)\n                grad_accum.data.view(-1).add_(dest_buffer)\n            else:\n                dest_buffer.narrow(0, source_offset,\n                                   num_elements).copy_(self.accumulated_grads_in_cpu[param_id].view(-1),\n                                                       non_blocking=True)\n                grad_accum.data.view(-1).narrow(0, source_offset,\n                                                num_elements).add_(dest_buffer.narrow(0, source_offset, num_elements))\n\n        #move accumulated gradients back to CPU\n        def copy_gradients_to_cpu():\n            grad_accum = self.get_param_gradient_attribute(param)\n            if not self.fp16_master_weights_and_gradients:\n                self.accumulated_grads_in_cpu[param_id].data.copy_(grad_accum.data.view(-1), non_blocking=True)\n            else:\n                self.accumulated_grads_in_cpu[param_id].data.copy_(grad_accum.data.view(-1).narrow(\n                    0, source_offset, num_elements),\n                                                                   non_blocking=True)\n\n        if param_id not in self.accumulated_grads_in_cpu:\n            self.accumulated_grads_in_cpu[param_id] = buffer_to_accumulate_to_in_cpu()\n\n        if self.micro_step_id > 0:\n            accumulate_gradients()\n\n        # at the boundary we will send 32bit directly\n        if not self.is_gradient_accumulation_boundary:\n            copy_gradients_to_cpu()\n\n    def set_norm_for_param_grad(self, param):\n        param_id = self.get_param_id(param)\n        grad_accum = self.get_param_gradient_attribute(param)\n        accumulated_grad = self.accumulated_grads_in_cpu[\n            param_id] if self.gradient_accumulation_steps > 1 else grad_accum\n\n        [i, source_offset, dest_offset, num_elements] = self.grad_position[param_id]\n\n        start = source_offset\n        accumulated_grad = accumulated_grad.view(-1).narrow(0, start, num_elements)\n\n        self.norm_for_param_grads[param_id] = accumulated_grad.data.double().norm(2)\n\n    def set_norm_for_param_grad_in_gpu(self, param):\n        param_id = self.get_param_id(param)\n        grad_accum = self.get_param_gradient_attribute(param)\n        if grad_accum is None:\n            accumulated_grad = param.grad\n        else:\n            accumulated_grad = grad_accum\n\n        [i, source_offset, dest_offset, num_elements] = self.grad_position[param_id]\n\n        start = source_offset\n        accumulated_grad = accumulated_grad.view(-1).narrow(0, start, num_elements)\n\n        self.norm_for_param_grads[param_id] = accumulated_grad.data.double().norm(2)\n\n    def async_inplace_copy_grad_to_fp32_buffer_from_gpu(self, param):\n        param_id = self.get_param_id(param)\n\n        [i, source_offset, dest_offset, num_elements] = self.grad_position[param_id]\n\n        dest_tensor = self.single_partition_of_fp32_groups[i].grad.view(-1).narrow(0, dest_offset, num_elements)\n\n        grad_accum = self.get_param_gradient_attribute(param)\n        if grad_accum is None:\n            src_tensor = grad_accum.view(-1).narrow(0, source_offset, num_elements)\n        else:\n            src_tensor = grad_accum.view(-1).narrow(0, source_offset, num_elements)\n        if not self.fp16_master_weights_and_gradients:\n            src_tensor = src_tensor.float()\n\n        dest_tensor.copy_(src_tensor, non_blocking=True)\n        param.grad = None  #offload only\n\n    def complete_grad_norm_calculation_for_cpu_offload(self, params):\n        total_norm = 0.0\n        norm_type = 2.0\n        for p in params:\n            # Pipeline parallelism may replicate parameters. Avoid multi-counting.\n            if hasattr(p, PIPE_REPLICATED) and p.ds_pipe_replicated:\n                continue\n\n            if is_model_parallel_parameter(p) or (self.model_parallel_rank == 0):\n                param_id = self.get_param_id(p)\n                # as some model have trainable parameters but skipped in training,\n                # their backward hooks in self.create_reduce_and_remove_grad_hooks() will not run,\n                # so they have no norm_for_param_grads\n                if param_id in self.norm_for_param_grads:\n                    param_norm = self.norm_for_param_grads[param_id]\n                    total_norm += param_norm.item()**2\n                else:\n                    # As unused parameters in modules may not be expected sometimes,\n                    # add an explicit error msg when it occurred and an option to\n                    # avoid the error\n                    assert self.ignore_unused_parameters, \"\"\"\n                        This assert indicates that your module has parameters that\n                        were not used in producing loss.\n                        You can avoid this assert by\n                        (1) enable ignore_unused_parameters option in zero_optimization config;\n                        (2) making sure all trainable parameters and `forward` function\n                            outputs participate in calculating loss.\n                    \"\"\"\n\n        # Sum across all model parallel GPUs.\n        total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])\n        dist.all_reduce(total_norm_cuda, op=dist.ReduceOp.SUM, group=self.dp_process_group)\n\n        self._model_parallel_all_reduce(tensor=total_norm_cuda, op=dist.ReduceOp.SUM)\n\n        total_norm = total_norm_cuda[0].item()**(1. / norm_type)\n\n        if total_norm == float('inf') or total_norm == -float('inf') or total_norm != total_norm:\n            total_norm = -1\n\n        return total_norm\n\n    ############################################################################################\n    def copy_grads_in_partition(self, param):\n        if self.cpu_offload:\n\n            if self.gradient_accumulation_steps > 1:\n                self.async_accumulate_grad_in_cpu_via_gpu(param)\n\n            if self.is_gradient_accumulation_boundary:\n                self.set_norm_for_param_grad_in_gpu(param)\n\n                self.update_overflow_tracker_for_param_grad(param)\n\n                self.async_inplace_copy_grad_to_fp32_buffer_from_gpu(param)\n\n            return\n        #print(f\"ID {self.get_param_id(param)} grad norm {param.grad.norm()}\")\n        if self.grads_in_partition is None:\n            self.grads_in_partition_offset = 0\n            total_size = 0\n            for group in self.params_in_partition:\n                for param_in_partition in group:\n                    total_size += param_in_partition.numel()\n\n            see_memory_usage(f\"before copying {total_size} gradients into partition\")\n            self.grads_in_partition = torch.empty(int(total_size),\n                                                  dtype=self.dtype,\n                                                  device=get_accelerator().current_device_name())\n            see_memory_usage(f\"after copying {total_size} gradients into partition\")\n\n        grad_reduc = self.get_gradient_for_reduction(param)\n        # The allreduce buffer will be rewritten. Copy the gradients in partition to a new buffer\n        new_grad_tensor = self.grads_in_partition.view(-1).narrow(0, self.grads_in_partition_offset, param.numel())\n        new_grad_tensor.copy_(grad_reduc.view(-1))\n        grad_reduc.data = new_grad_tensor.data.view_as(grad_reduc)\n        #print(f\"Grad norm after copy to contiguous_buffer {param.grad.data.norm()}\")\n        self.grads_in_partition_offset += param.numel()\n\n    def reduce_ipg_grads(self):\n        if self.contiguous_gradients:\n            if self.extra_large_param_to_reduce is not None:\n                assert len(self.params_in_ipg_bucket) == 1, \"more than 1 param in ipg bucket, this shouldn't happen\"\n                _, _, param_id = self.params_in_ipg_bucket[0]\n                assert self.get_param_id(self.extra_large_param_to_reduce\n                                         ) == param_id, \"param in ipg bucket does not match extra-large param\"\n                extra_large_grad_reduc = self.get_gradient_for_reduction(self.extra_large_param_to_reduce)\n                self.average_tensor(extra_large_grad_reduc.view(-1))\n                self.extra_large_param_to_reduce = None\n            else:\n                self.average_tensor(self.ipg_buffer[self.ipg_index].narrow(0, 0, self.elements_in_ipg_bucket))\n        else:\n            self.buffered_reduce_fallback(None,\n                                          self.grads_in_ipg_bucket,\n                                          elements_per_buffer=self.elements_in_ipg_bucket)\n\n        if self.overlap_comm:\n            stream = self.reduction_stream\n        elif self.cpu_offload:\n            # TODO: copy_grad_stream is disabled because of race with reduce. This hurts perf and should be fixed.\n            #            get_accelerator().synchronize()\n            #            stream = self.copy_grad_stream\n            stream = get_accelerator().current_stream()\n        else:\n            stream = get_accelerator().current_stream()\n\n        with get_accelerator().stream(stream):\n            for _, param, param_id in self.params_in_ipg_bucket:\n\n                assert self.params_already_reduced[param_id] == False, \\\n                    f\"The parameter {param_id} has already been reduced. \\\n                    Gradient computed twice for this partition. \\\n                    Multiple gradient reduction is currently not supported\"\n\n                self.params_already_reduced[param_id] = True\n                if self.partition_gradients:\n                    if not self.is_param_in_current_partition[param_id]:\n                        if self.overlap_comm and self.contiguous_gradients is False:\n                            # Clear grads of other partitions during the next reduction\n                            # to avoid clearing them before the reduction is complete.\n                            if self.previous_reduced_grads is None:\n                                self.previous_reduced_grads = []\n                            self.previous_reduced_grads.append(param)\n                        else:\n                            self.clear_grad_attribute(param)\n                    elif self.contiguous_gradients:\n                        self.copy_grads_in_partition(param)\n                else:  # zero stage 1 - partition only optimizer state\n                    if self.contiguous_gradients and self.is_param_in_current_partition[param_id]:\n                        self.copy_grads_in_partition(param)\n\n        self.grads_in_ipg_bucket = []\n        self.params_in_ipg_bucket = []\n        self.ipg_bucket_has_moe_params = False\n        self.elements_in_ipg_bucket = 0\n        #####################################################################\n\n    def reduce_ready_partitions_and_remove_grads(self, param, i):\n        if self.partition_gradients or self.is_gradient_accumulation_boundary:\n            self.reduce_independent_p_g_buckets_and_remove_grads(param, i)\n\n    def zero_reduced_gradients(self, partition_id, i):\n\n        def are_all_related_partitions_reduced(params_id):\n            for partition_id in self.param_to_partition_ids[i][params_id]:\n                if not self.is_partition_reduced[i][partition_id]:\n                    return False\n            return True\n\n        for params_id in self.is_grad_computed[i][partition_id]:\n            if are_all_related_partitions_reduced(params_id):\n                self.param_dict[params_id].grad = None  # dead code\n\n    def flatten_and_print(self, message, tensors, start=0, n=5):\n        flatten_tensor = self.flatten(tensors)\n\n        def print_func():\n            logger.info(flatten_tensor.contiguous().view(-1).narrow(0, start, n))\n\n        self.sequential_execution(print_func, message)\n\n    def get_grads_to_reduce(self, i, partition_id):\n\n        def get_reducible_portion(key):\n            grad = self.param_dict[key].grad\n            total_elements = grad.numel()\n            start = self.grad_start_offset[i][partition_id][key]\n            num_elements = min(total_elements - start,\n                               self.partition_size[i] - self.grad_partition_insertion_offset[i][partition_id][key])\n            if not pg_correctness_test:\n                if num_elements == total_elements:\n                    return grad\n                else:\n                    return grad.contiguous().view(-1).narrow(0, int(start), int(num_elements))\n            else:\n                if num_elements == total_elements:\n                    return grad.clone()\n                else:\n                    return grad.clone().contiguous().view(-1).narrow(0, int(start), int(num_elements))\n\n        grads_to_reduce = []\n        for key in self.is_grad_computed[i][partition_id]:\n            grad = get_reducible_portion(key)\n            grads_to_reduce.append(grad)\n        return grads_to_reduce\n\n    def sequential_execution(self, function, message, group=None):\n        if group is None:\n            group = self.dp_process_group\n        if dist.get_rank(group=group) == 0:\n            logger.info(message)\n        for id in range(dist.get_world_size(group=group)):\n            if id == dist.get_rank(group=group):\n                function()\n            dist.barrier(group=group)\n\n    def set_none_gradients_to_zero(self, i, partition_id):\n        for param_id in self.is_grad_computed[i][partition_id]:\n            param = self.param_dict[param_id]\n            if param.grad is None:\n                param.grad = torch.zeros_like(param)\n\n    ######################Reduction Related Methods##############################\n    def allreduce_bucket(self, bucket, rank=None, log=None, divide=True, process_group=None):\n        tensor = self.flatten(bucket)\n\n        process_group = self.dp_process_group if process_group is None else process_group\n\n        tensor_to_allreduce = tensor\n\n        if pg_correctness_test or self.sequence_parallel_size > 1:\n            communication_data_type = torch.float32\n        else:\n            communication_data_type = self.communication_data_type\n\n        if communication_data_type != tensor.dtype:\n            tensor_to_allreduce = tensor.to(communication_data_type)\n\n        if divide:\n            tensor_to_allreduce.div_(dist.get_world_size(group=process_group) / float(self.sequence_parallel_size))\n\n        if rank is None:\n            #    \"All Reducing\"\n            dist.all_reduce(tensor_to_allreduce, group=process_group)\n        else:\n            global_rank = dist.get_global_rank(process_group, rank)\n            dist.reduce(tensor_to_allreduce, global_rank, group=process_group)\n\n        if communication_data_type != tensor.dtype and tensor is not tensor_to_allreduce:\n            if rank is None or rank == dist.get_rank(group=process_group):\n                tensor.copy_(tensor_to_allreduce)\n\n        return tensor\n\n    def _clear_previous_reduced_grads(self):\n        if self.previous_reduced_grads is not None:\n            for param in self.previous_reduced_grads:\n                self.clear_grad_attribute(param)\n            self.previous_reduced_grads = None\n\n    # if rank is specified do a reduction instead of an allreduce\n    def allreduce_and_copy(self, small_bucket, rank=None, log=None, divide=True, process_group=None):\n        process_group = self.dp_process_group if process_group is None else process_group\n        if self.overlap_comm:\n            if not get_accelerator().resolves_data_dependency():\n                get_accelerator().synchronize()\n            # It is safe to clear the previously reduced grads of other partitions\n            self._clear_previous_reduced_grads()\n            stream = self.reduction_stream\n        else:\n            stream = get_accelerator().current_stream()\n\n        with get_accelerator().stream(stream):\n            allreduced = self.allreduce_bucket(\n                small_bucket,\n                rank=rank,\n                log=log,\n                divide=divide,\n                process_group=process_group,\n            )\n            if rank is None or rank == dist.get_rank(group=self.dp_process_group):\n                for buf, synced in zip(small_bucket, self.unflatten(allreduced, small_bucket)):\n                    buf.copy_(synced)\n\n    def allreduce_no_retain(\n        self,\n        bucket,\n        numel_per_bucket=500000000,\n        rank=None,\n        log=None,\n        divide=True,\n        process_group=None,\n    ):\n        small_bucket = []\n        numel = 0\n        for tensor in bucket:\n            small_bucket.append(tensor)\n            numel = numel + tensor.numel()\n            if numel > numel_per_bucket:\n                self.allreduce_and_copy(small_bucket, rank=rank, log=None, divide=divide, process_group=process_group)\n                small_bucket = []\n                numel = 0\n\n        if len(small_bucket) > 0:\n            self.allreduce_and_copy(small_bucket, rank=rank, log=log, divide=divide, process_group=process_group)\n\n    # allows using reduction of gradients instead of using all_reduce\n\n    def buffered_reduce_fallback(self, rank, grads, elements_per_buffer=500000000, log=None):\n        split_buckets = split_half_float_double(grads)\n\n        for i, bucket in enumerate(split_buckets):\n            self.allreduce_no_retain(bucket, numel_per_bucket=elements_per_buffer, rank=rank, log=log)\n\n    #############################################################################\n    #############################################################################\n    #############################################################################\n\n    # views the tensor as multiple partitions and returns\n    # those partitions\n    def get_data_parallel_partitions(self, tensor, group_id):\n        partitions = []\n\n        dp = dist.get_world_size(group=self.real_dp_process_group[group_id])\n        # dp_id = dist.get_rank(group=self.real_dp_process_group[group_id])\n\n        total_num_elements = tensor.numel()\n\n        base_size = total_num_elements // dp\n        remaining = total_num_elements % dp\n\n        start = 0\n        for id in range(dp):\n            partition_size = base_size\n            if id < remaining:\n                partition_size = partition_size + 1\n            partitions.append(tensor.narrow(0, start, partition_size))\n            start = start + partition_size\n        return partitions\n\n    def get_partition_info(self, tensor_list, partition_size, partition_id):\n        params_in_partition = []\n        params_not_in_partition = []\n\n        start_index = partition_size * partition_id\n        end_index = partition_size * (partition_id + 1)\n\n        current_index = 0\n        first_offset = 0\n\n        for tensor in tensor_list:\n\n            tensor_size = tensor.numel()\n\n            if start_index <= current_index < end_index:\n                params_in_partition.append(tensor)\n\n            elif current_index < start_index < (current_index + tensor_size):\n                params_in_partition.append(tensor)\n\n                assert (first_offset == 0\n                        ), \"This can happen either zero or only once as this must be the first tensor in the partition\"\n                first_offset = start_index - current_index\n\n            else:\n                params_not_in_partition.append(tensor)\n\n            current_index = current_index + tensor_size\n\n        return params_in_partition, params_not_in_partition, first_offset\n\n    def zero_grad(self, set_to_none=True):\n        \"\"\"\n        Zero FP16 parameter grads.\n        \"\"\"\n        # FP32 grad should never exist.\n        # For speed, set model fp16 grad to None by default\n        # zero all pointers to grad tensors\n        for group in self.bit16_groups:\n            for p in group:\n                if set_to_none:\n                    p.grad = None  # epilogue and in step\n                    p.grad_accum = None\n                else:\n                    if p.grad is not None:\n                        p.grad.detach_()\n                        p.grad.zero_()\n\n    def _model_parallel_all_reduce(self, tensor, op):\n        \"\"\" Perform all reduce within model parallel group, if any.\n        \"\"\"\n        if self.model_parallel_group is None or self.model_parallel_world_size == 1:\n            pass\n        else:\n            dist.all_reduce(tensor=tensor, op=op, group=self.model_parallel_group)\n\n    def get_grad_norm_direct(self, gradients, params, norm_type=2):\n        \"\"\"Clips gradient norm of an iterable of parameters.\n\n        This is adapted from torch.nn.utils.clip_grad.clip_grad_norm_ and\n        added functionality to handle model parallel parameters. Note that\n        the gradients are modified in place.\n\n        Arguments:\n            parameters (Iterable[Tensor] or Tensor): an iterable of Tensors or a\n                single Tensor that will have gradients normalized\n            max_norm (float or int): max norm of the gradients\n            norm_type (float or int): type of the used p-norm. Can be ``'inf'`` for\n                infinity norm.\n\n        Returns:\n            Total norm of the parameters (viewed as a single vector).\n        \"\"\"\n        norm_type = float(norm_type)\n        all_norms = []\n        if norm_type == inf:\n            for g in gradients:\n                all_norms.append(g.data.abs().max().float())\n            total_norm = torch.stack(all_norms).max()\n            dist.all_reduce(total_norm, op=dist.ReduceOp.MAX, group=self.dp_process_group)\n\n            # Take max across all GPUs.\n            self._model_parallel_all_reduce(tensor=total_norm, op=dist.ReduceOp.MAX)\n        else:\n            # if dist.get_rank() == 0:\n            #    logger.info(f\"Total Norm beginning {total_norm}\")\n            for g, p in zip(gradients, params):\n                # Pipeline parallelism may replicate parameters. Avoid multi-counting.\n                if hasattr(p, PIPE_REPLICATED) and p.ds_pipe_replicated:\n                    continue\n                if is_model_parallel_parameter(p) or (self.model_parallel_rank == 0):\n                    all_norms.append(\n                        torch.norm(g.data.double().detach(), norm_type).to(get_accelerator().current_device_name()))\n            if len(all_norms) > 0:\n                total_norm = torch.stack(all_norms).square().sum().float()\n            else:\n                total_norm = torch.tensor(0.0, dtype=torch.float32).to(self.device)\n            # Sum across all model parallel Device.\n            dist.all_reduce(total_norm, op=dist.ReduceOp.SUM, group=self.dp_process_group)\n\n            self._model_parallel_all_reduce(tensor=total_norm, op=dist.ReduceOp.SUM)\n\n            total_norm = total_norm.pow(1. / norm_type)\n\n        norm_is_inf = total_norm.isinf()\n        norm_is_nan = total_norm.isnan()\n        inf_or_nan = norm_is_nan.logical_or(norm_is_inf)\n\n        err = torch.tensor(-1.0, device=self.device, dtype=torch.float)\n        total_norm = inf_or_nan * err + inf_or_nan.logical_not() * total_norm\n        return total_norm\n\n    # creates a flat fused tensor from the tensor list starting at the first_offset\n    # in the first tensor of the list. If there are not enough elements in the tensor\n    # list then the flat tensor will be padded with zeros\n    def get_flat_partition(self, tensor_list, first_offset, partition_size, dtype, device, return_tensor_list=False):\n        flat_tensor_list = []\n        current_size = 0\n\n        for i, tensor in enumerate(tensor_list):\n            grad_accum = self.get_param_gradient_attribute(tensor)\n            if grad_accum is None:\n                grad_accum = torch.zeros_like(tensor, dtype=dtype)\n\n            tensor = grad_accum\n            num_elements = tensor.numel()\n            tensor_offset = 0\n\n            # we need to offset to get to the right element\n            if i == 0 and first_offset > 0:\n                tensor_offset = first_offset\n                num_elements = num_elements - tensor_offset\n\n            # we dont need all elements of the tensor\n            if num_elements > (partition_size - current_size):\n                num_elements = partition_size - current_size\n\n            # we need a narrow view of the tensor based on the tensor offset and number of elements that\n            # we need from this tensor\n            if tensor_offset > 0 or num_elements < tensor.numel():\n                flat_tensor_list.append(tensor.contiguous().view(-1).narrow(0, int(tensor_offset), int(num_elements)))\n            else:\n                flat_tensor_list.append(tensor)\n\n            current_size = current_size + num_elements\n\n        # this means its the last partition and does not align with the dp boundary. We need to pad before flattening\n        if current_size < partition_size:\n            flat_tensor_list.append(torch.zeros(int(partition_size - current_size), dtype=dtype, device=device))\n\n        if return_tensor_list:\n            return flat_tensor_list\n\n        return self.flatten(flat_tensor_list)\n\n    def free_grad_in_param_list(self, param_list):\n        for p in param_list:\n            p.grad = None  # in step\n            p.grad_accum = None\n\n    def reset_cpu_buffers(self):\n        self.norm_for_param_grads = {}\n        self.local_overflow = False\n\n    def set_lr(self, lr):\n        \"\"\"Set the learning rate.\"\"\"\n        for param_group in self.optimizer.param_groups:\n            param_group[\"lr\"] = lr\n\n    def get_lr(self):\n        \"\"\"Return the current learning rate.\"\"\"\n        return self.optimizer.param_groups[0][\"lr\"]\n\n    def override_loss_scale(self, loss_scale):\n        if loss_scale != self.external_loss_scale:\n            logger.info(f'[deepspeed] setting loss scale from {self.external_loss_scale} -> {loss_scale}')\n        self.custom_loss_scaler = True\n        self.external_loss_scale = loss_scale\n\n    def scaled_global_norm(self, norm_type=2):\n        assert norm_type == 2, \"only L2 norm supported\"\n        norm_groups = []\n        for i, group in enumerate(self.bit16_groups):\n            if self.cpu_offload:\n                # complete complete_grad_norm_calculation_for_cpu_offload return python float, moving back to\n                # torch.tensor as else statement returns tensor as well\n                norm = torch.tensor(self.complete_grad_norm_calculation_for_cpu_offload(self.params_in_partition[i]),\n                                    device=self.device)\n                norm_groups.append(norm)\n            else:\n                norm_groups.append(self.get_grad_norm_direct(self.averaged_gradients[i], self.params_in_partition[i]))\n\n        if self.has_moe_layers:\n            self._average_expert_grad_norms(norm_groups)\n\n        # calculating L2 norm\n        return torch.norm(torch.stack(norm_groups), p=norm_type)\n\n    def get_bit16_param_group(self, group_no):\n        bit16_partitions = self.parallel_partitioned_bit16_groups[group_no]\n        partition_id = dist.get_rank(group=self.real_dp_process_group[group_no])\n        return [bit16_partitions[dist.get_rank(group=self.real_dp_process_group[group_no])]]\n\n    def _optimizer_step(self, group_no):\n        original_param_groups = self.optimizer.param_groups\n        self.optimizer.param_groups = [original_param_groups[group_no]]\n        # Disabling this as the C++ side copy & synchronize is not working correctly\n        #from deepspeed.ops.adam import DeepSpeedCPUAdam\n        #if type(self.optimizer) == DeepSpeedCPUAdam and self.dtype == torch.half:\n        #    self.optimizer.step(fp16_param_groups=[self.get_bit16_param_group(group_no)])\n        #else:\n        #    self.optimizer.step()\n        self.optimizer.step()\n        self.optimizer.param_groups = original_param_groups\n\n        # We need to link optimizer state after the first step() call\n        self._lazy_init_hp_params_optimizer_state()\n\n    def step(self, closure=None):\n        \"\"\"\n        Not supporting closure.\n        \"\"\"\n        self.micro_step_id = -1\n\n        see_memory_usage(f\"In step before checking overflow\")\n\n        # First compute norm for all group so we know if there is overflow\n        if self.dtype == torch.float16:\n            self.check_overflow()\n\n        prev_scale = self.loss_scale\n        self._update_scale(self.overflow)\n        if self.overflow:\n            see_memory_usage('After overflow before clearing gradients')\n            self.zero_grad(set_to_none=True)\n            if self.cpu_offload:\n                self.reset_cpu_buffers()\n            else:\n                self.averaged_gradients = {}\n\n            see_memory_usage('After overflow after clearing gradients')\n\n            for timer in OPTIMIZER_TIMERS:\n                self.timers(timer).start()\n                self.timers(timer).stop()\n            return\n\n        # Step 1:- Calculate gradient norm using bit-16 grads\n        see_memory_usage('Before norm calculation')\n        scaled_global_grad_norm = self.scaled_global_norm()\n        self._global_grad_norm = scaled_global_grad_norm / prev_scale\n        see_memory_usage('After norm before optimizer')\n\n        # Step 2:- run optimizer and upscaling simultaneously\n        for i, group in enumerate(self.bit16_groups):\n            self.timers(OPTIMIZER_GRADIENTS_TIMER).start()\n            partition_id = dist.get_rank(group=self.real_dp_process_group[i])\n            if self.cpu_offload:\n                single_grad_partition = self.single_partition_of_fp32_groups[i].grad\n                self.unscale_and_clip_grads([single_grad_partition], scaled_global_grad_norm)\n\n                self.timers(OPTIMIZER_GRADIENTS_TIMER).stop()\n                self.timers(OPTIMIZER_STEP_TIMER).start()\n                self._optimizer_step(i)\n\n                # Disabled, this is not currently working\n                #from deepspeed.ops.adam import DeepSpeedCPUAdam\n                #if not (type(self.optimizer) == DeepSpeedCPUAdam and self.dtype == torch.half):\n                #    bit16_partitions = self.parallel_partitioned_bit16_groups[i]\n                #    fp32_partition = self.single_partition_of_fp32_groups[i]\n                #    bit16_partitions[partition_id].data.copy_(fp32_partition.data)\n                bit16_partitions = self.parallel_partitioned_bit16_groups[i]\n                fp32_partition = self.single_partition_of_fp32_groups[i]\n                bit16_partitions[partition_id].data.copy_(\n                    fp32_partition.to(get_accelerator().current_device_name()).data)\n\n                self.timers(OPTIMIZER_STEP_TIMER).stop()\n            else:\n                # free gradients for all the parameters that are not updated by this process(ZeRO stage2)\n                self.free_grad_in_param_list(self.params_not_in_partition[i])\n\n                # create a flat gradients for parameters updated by this process\n                # If we are last partition, ensure we have same size grads and partition size, if not pad with zero tensors\n                if partition_id == dist.get_world_size(group=self.real_dp_process_group[i]) - 1:\n                    single_grad_partition = self.flatten_dense_tensors_aligned(\n                        self.averaged_gradients[i],\n                        int(self.partition_size[i])).to(self.single_partition_of_fp32_groups[i].dtype)\n                else:\n                    single_grad_partition = self.flatten(self.averaged_gradients[i]).to(\n                        self.single_partition_of_fp32_groups[i].dtype)\n                assert single_grad_partition.numel() == self.partition_size[i], \\\n                    \"averaged gradients have different number of elements that partition size {} {} {} {}\".format(\n                        single_grad_partition.numel(), self.partition_size[i], i, partition_id)\n\n                self.single_partition_of_fp32_groups[i].grad = single_grad_partition\n                # release all the gradient since we have already created a necessary copy in dp_grad_partition(ZeRO stage2)\n                self.free_grad_in_param_list(self.params_in_partition[i])\n\n                self.averaged_gradients[i] = None\n\n                self.unscale_and_clip_grads([single_grad_partition], scaled_global_grad_norm)\n\n                self.timers(OPTIMIZER_GRADIENTS_TIMER).stop()\n\n                # Step 3:- run the optimizer if no offloading\n                self.timers(OPTIMIZER_STEP_TIMER).start()\n                self._optimizer_step(i)\n                # Step 4:- get rid of the fp32 gradients. Not needed anymore\n                self.single_partition_of_fp32_groups[i].grad = None\n                del single_grad_partition\n                bit16_partitions = self.parallel_partitioned_bit16_groups[i]\n                fp32_partition = self.single_partition_of_fp32_groups[i]\n                bit16_partitions[partition_id].data.copy_(fp32_partition.data)\n                self.timers(OPTIMIZER_STEP_TIMER).stop()\n\n        see_memory_usage('After optimizer before all-gather')\n        if self.cpu_offload:\n            self.reset_cpu_buffers()\n\n        self.timers(OPTIMIZER_ALLGATHER_TIMER).start()\n        # Gather the updated weights from everyone.\n        # Then all partitions of the model parameters are updated and ready for next round forward.\n        all_gather_dp_groups(groups_flat=self.bit16_groups_flat,\n                             partitioned_param_groups=self.parallel_partitioned_bit16_groups,\n                             dp_process_group=self.real_dp_process_group,\n                             start_alignment_factor=self.nccl_start_alignment_factor,\n                             allgather_bucket_size=self.allgather_bucket_size)\n        self.timers(OPTIMIZER_ALLGATHER_TIMER).stop()\n\n        # TODO: we probably don't need this? just to be safe\n        for i in range(len(self.bit16_groups)):\n            self._update_model_bit16_weights(i)\n\n        self.timers.log(OPTIMIZER_TIMERS)\n        see_memory_usage('After zero_optimizer step')\n\n        return\n\n    @torch.no_grad()\n    def update_lp_params(self):\n        for i, (bit16_partitions, fp32_partition) in enumerate(\n                zip(self.parallel_partitioned_bit16_groups, self.single_partition_of_fp32_groups)):\n            partition_id = dist.get_rank(group=self.real_dp_process_group[i])\n            bit16_partitions[partition_id].data.copy_(fp32_partition.data)\n            # print_rank_0(f'update_lp_params {i=} {partition_id=}', force=True)\n            # if i == 0:\n            #     print_rank_0(f'{fp32_partition[:10]=}', force=True)\n        all_gather_dp_groups(groups_flat=self.bit16_groups_flat,\n                             partitioned_param_groups=self.parallel_partitioned_bit16_groups,\n                             dp_process_group=self.real_dp_process_group,\n                             start_alignment_factor=self.nccl_start_alignment_factor,\n                             allgather_bucket_size=self.allgather_bucket_size)\n\n    def _average_expert_grad_norms(self, norm_groups):\n        for i, norm in enumerate(norm_groups):\n            if self.is_moe_param_group[i]:\n                scaled_norm_tensor = norm * 1.0 / dist.get_world_size(group=self.real_dp_process_group[i])\n                if self.device == 'cpu':\n                    scaled_norm_tensor = scaled_norm_tensor.to(get_accelerator().current_device_name())\n                dist.all_reduce(scaled_norm_tensor, group=self.real_dp_process_group[i])\n                norm_groups[i] = scaled_norm_tensor.to(self.device)\n\n    def unscale_and_clip_grads(self, grad_groups_flat, total_norm):\n        # compute combined scale factor for this group\n        combined_scale = self.loss_scale\n        if self.clip_grad > 0.:\n            # norm is in fact norm*scale\n            clip = ((total_norm / self.loss_scale) + 1e-6) / self.clip_grad\n            clip = torch.clamp(clip, min=1.0)\n            combined_scale = clip * self.loss_scale\n\n        for grad in grad_groups_flat:\n            if isinstance(grad, list):\n                sub_partitions = grad\n                for g in sub_partitions:\n                    g.data.mul_(1. / combined_scale)\n            else:\n                grad.data.mul_(1. / combined_scale)\n\n    def _check_overflow(self, partition_gradients=True):\n        self.overflow = self.has_overflow(partition_gradients)\n\n    # `params` is a list / generator of torch.Variable\n    def has_overflow_serial(self, params):\n        invalid_grad_count = torch.zeros([1], dtype=torch.float, device=get_accelerator().current_device_name())\n        for p in params:\n            if p.grad is not None:\n                invalid_grad_count += self._has_inf_or_nan(p.grad)\n        return invalid_grad_count.bool()\n\n    def has_overflow_partitioned_grads_serial(self):\n        invalid_grad_count = torch.zeros([1], dtype=torch.float, device=get_accelerator().current_device_name())\n        for i in range(len(self.bit16_groups)):\n            for j, grad in enumerate(self.averaged_gradients[i]):\n                if grad is not None:\n                    invalid_grad_count += self._has_inf_or_nan(grad)\n        return invalid_grad_count.bool()\n\n    def has_overflow(self, partition_gradients=True):\n        if partition_gradients:\n            overflow = self.local_overflow if self.cpu_offload else self.has_overflow_partitioned_grads_serial()\n            overflow_gpu = get_accelerator().ByteTensor([overflow]) if self.cpu_offload else overflow.byte().to(\n                get_accelerator().current_device_name())\n            '''This will capture overflow across all data parallel and expert parallel process\n            Since expert parallel process are a subset of data parallel process'''\n            dist.all_reduce(overflow_gpu, op=dist.ReduceOp.MAX, group=self.dp_process_group)\n\n        else:\n            params = []\n            for group in self.bit16_groups:\n                for param in group:\n                    params.append(param)\n            overflow_gpu = self.has_overflow_serial(params).byte().to(get_accelerator().current_device_name())\n\n        # Since each model parallel GPU carries only part of the model,\n        # make sure overflow flag is synced across all the model parallel GPUs\n        self._model_parallel_all_reduce(tensor=overflow_gpu, op=dist.ReduceOp.MAX)\n\n        overflow = overflow_gpu[0].item()\n        return bool(overflow)\n\n    # `x` is a torch.Tensor\n    @staticmethod\n    def _has_inf_or_nan(x, j=None):\n        float_x = x.float()\n        nan = float_x.isnan()\n        inf = float_x.isinf()\n        inf_or_nan = nan.logical_or(inf)\n        return inf_or_nan.float().max()\n\n    def backward(self, loss, retain_graph=False):\n        \"\"\"\n        :attr:`backward` performs the following steps:\n\n        1. fp32_loss = loss.float()\n        2. scaled_loss = fp32_loss*loss_scale\n        3. scaled_loss.backward(), which accumulates scaled gradients into the ``.grad`` attributes of the model's fp16 leaves\n        \"\"\"\n        self.micro_step_id += 1\n\n        if self.contiguous_gradients:\n            self.ipg_buffer = []\n            buf_0 = torch.empty(int(self.reduce_bucket_size),\n                                dtype=self.dtype,\n                                device=get_accelerator().current_device_name())\n            self.ipg_buffer.append(buf_0)\n\n            # Use double buffers to avoid data access conflict when overlap_comm is enabled.\n            if self.overlap_comm:\n                buf_1 = torch.empty(int(self.reduce_bucket_size),\n                                    dtype=self.dtype,\n                                    device=get_accelerator().current_device_name())\n                self.ipg_buffer.append(buf_1)\n            self.ipg_index = 0\n\n        if self.custom_loss_scaler:\n            scaled_loss = self.external_loss_scale * loss\n            scaled_loss.backward()\n        else:\n            self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)\n\n        # Only for Stage 1, Mode 2\n        if self.use_grad_accum_attribute:\n            self.fill_grad_accum_attribute()\n\n    def check_overflow(self, partition_gradients=True):\n        self._check_overflow(partition_gradients)\n\n    def _update_scale(self, has_overflow=False):\n        self.loss_scaler.update_scale(has_overflow)\n\n    # Promote state so it can be retrieved or set via \"fp16_optimizer_instance.state\"\n    def _get_state(self):\n        return self.optimizer.state\n\n    def _set_state(self, value):\n        self.optimizer.state = value\n\n    state = property(_get_state, _set_state)\n\n    # Promote param_groups so it can be retrieved or set via \"fp16_optimizer_instance.param_groups\"\n    # (for example, to adjust the learning rate)\n    def _get_param_groups(self):\n        return self.optimizer.param_groups\n\n    def _set_param_groups(self, value):\n        self.optimizer.param_groups = value\n\n    param_groups = property(_get_param_groups, _set_param_groups)\n\n    # Promote loss scale so it can be retrieved or set via \"fp16_optimizer_instance.loss_scale\"\n    def _get_loss_scale(self):\n        if self.custom_loss_scaler:\n            return self.external_loss_scale\n        else:\n            return self.loss_scaler.cur_scale\n\n    def _set_loss_scale(self, value):\n        self.loss_scaler.cur_scale = value\n\n    loss_scale = property(_get_loss_scale, _set_loss_scale)\n    cur_scale = property(_get_loss_scale, _set_loss_scale)\n\n    # Return group tensor after removing paddings that are added for alignment to DP world size.\n    # This method works on the assumption that each group contains a single flattened tensor.\n    def _get_groups_without_padding(self, groups_with_padding):\n        groups_without_padding = []\n        for i, group in enumerate(groups_with_padding):\n            lean_length = group.numel() - self.groups_padding[i]\n            groups_without_padding.append(group[:lean_length])\n\n        return groups_without_padding\n\n    # Return optimizer state after removing paddings that are added for alignment.\n    def _get_state_without_padding(self, state_with_padding, padding):\n        lean_state = {}\n        for key, value in state_with_padding.items():\n            if torch.is_tensor(value):\n                lean_length = value.numel() - padding\n                lean_state[key] = value[:lean_length]\n            else:\n                lean_state[key] = value\n\n        return lean_state\n\n    # Return base optimizer states.\n    # This method assumes that each param group contains a single flattened tensor.\n    def _get_base_optimizer_state(self):\n        optimizer_groups_state = []\n        for i, group in enumerate(self.optimizer.param_groups):\n            p = group['params'][0]\n            lean_optimizer_state = self._get_state_without_padding(self.optimizer.state[p], self.groups_padding[i])\n            optimizer_groups_state.append(lean_optimizer_state)\n\n        return optimizer_groups_state\n\n    def state_dict(self):\n        \"\"\"\n        Returns a dict containing the current state of this :class:`FP16_Optimizer` instance.\n        This dict contains attributes of :class:`FP16_Optimizer`, as well as the state_dict\n        of the contained Pytorch optimizer.\n        Example::\n            checkpoint = {}\n            checkpoint['model'] = model.state_dict()\n            checkpoint['optimizer'] = optimizer.state_dict()\n            torch.save(checkpoint, \"saved.pth\")\n        \"\"\"\n        state_dict = {}\n        state_dict[LOSS_SCALER] = self.loss_scaler\n        state_dict['dynamic_loss_scale'] = self.dynamic_loss_scale\n        state_dict['overflow'] = self.overflow\n        state_dict[CLIP_GRAD] = self.clip_grad\n\n        if self.elastic_checkpoint:\n            state_dict[BASE_OPTIMIZER_STATE] = self._get_base_optimizer_state()\n\n            if \"step\" in self.optimizer.param_groups[0]:\n                # Assuming \"step\" is the only item that changes through training iterations\n                assert all(group[\"step\"] == self.optimizer.param_groups[0][\"step\"]\n                           for group in self.optimizer.param_groups), \"All param groups must have the same step value\"\n                state_dict[BASE_OPTIMIZER_STATE_STEP] = self.optimizer.param_groups[0][\"step\"]\n        else:\n            state_dict[BASE_OPTIMIZER_STATE] = self.optimizer.state_dict()\n\n        # Remove paddings for DP alignment to enable loading for other alignment values\n        fp32_groups_without_padding = self._get_groups_without_padding(self.single_partition_of_fp32_groups)\n        state_dict[SINGLE_PARTITION_OF_FP32_GROUPS] = fp32_groups_without_padding\n\n        state_dict[\n            ZERO_STAGE] = ZeroStageEnum.gradients if self.partition_gradients else ZeroStageEnum.optimizer_states\n        state_dict[GROUP_PADDINGS] = self.groups_padding\n        state_dict[PARTITION_COUNT] = self.partition_count\n\n        state_dict[DS_VERSION] = version\n        state_dict[PARAM_SLICE_MAPPINGS] = self._param_slice_mappings\n\n        return state_dict\n\n    # Restore base optimizer fp32 weights from elastic checkpoint by:\n    # 1) Merging fp32 weights from checkpoints of all partitions\n    # 2) Extracting fp32 weights for current partition from merged weights\n    # 3) Using extracted weights to update base optimizer weights directly.\n    def _restore_from_elastic_fp32_weights(self, all_state_dict):\n        merged_single_partition_of_fp32_groups = []\n\n        for i in range(len(self.single_partition_of_fp32_groups)):\n            partition_id = dist.get_rank(group=self.real_dp_process_group[i])\n            merged_partitions = [sd[SINGLE_PARTITION_OF_FP32_GROUPS][i] for sd in all_state_dict]\n            if self.is_moe_group(self.optimizer.param_groups[i]):\n                ranks = self.get_ep_ranks(group_name=self.optimizer.param_groups[i]['name'])\n                merged_partitions = [merged_partitions[i] for i in ranks]\n            flat_merged_partitions = self.flatten_dense_tensors_aligned(\n                merged_partitions,\n                self.nccl_start_alignment_factor * dist.get_world_size(group=self.real_dp_process_group[i]))\n            dp_partitions = self.get_data_parallel_partitions(flat_merged_partitions, i)\n            merged_single_partition_of_fp32_groups.append(dp_partitions[partition_id])\n\n        for current, saved in zip(self.single_partition_of_fp32_groups, merged_single_partition_of_fp32_groups):\n            current.data.copy_(saved.data)\n\n    # Restore base optimizer fp32 weights from ZeRO fp16 or bfloat16 weights\n    def _restore_from_bit16_weights(self):\n        for group_id, (bit16_partitions, fp32_partition) in enumerate(\n                zip(self.parallel_partitioned_bit16_groups, self.single_partition_of_fp32_groups)):\n            partition_id = dist.get_rank(group=self.real_dp_process_group[group_id])\n            fp32_partition.data.copy_(bit16_partitions[partition_id].data)\n\n    # Refresh the fp32 master params from the fp16 or bfloat16 copies.\n    def refresh_fp32_params(self):\n        self._restore_from_bit16_weights()\n\n    # Extract optimizer state for current partition from merged states of all partitions\n    def _partition_base_optimizer_state(self, state_key, all_partition_states, group_id):\n        partition_id = dist.get_rank(group=self.real_dp_process_group[group_id])\n        alignment = self.nccl_start_alignment_factor * dist.get_world_size(group=self.real_dp_process_group[group_id])\n        if torch.is_tensor(all_partition_states[0]):\n            flat_merged_partitions = self.flatten_dense_tensors_aligned(all_partition_states, alignment)\n            dp_partitions = self.get_data_parallel_partitions(flat_merged_partitions, group_id)\n            return dp_partitions[partition_id]\n        else:\n            # Assume non-tensor states are not partitioned and equal across ranks, so return first one\n            return all_partition_states[0]\n\n    def _restore_step_from_elastic_checkpoint(self, all_state_dict):\n        assert BASE_OPTIMIZER_STATE_STEP in all_state_dict[0]\n        assert all(sd[BASE_OPTIMIZER_STATE_STEP] == all_state_dict[0][BASE_OPTIMIZER_STATE_STEP]\n                   for sd in all_state_dict), \"State dicts of all partitions must have the same step value\"\n        return all_state_dict[0][BASE_OPTIMIZER_STATE_STEP]\n\n    def _restore_base_optimizer_state(self, base_optimizer_group_states, base_optimizer_state_step, group_paddings):\n        if type(base_optimizer_group_states) == dict:\n            base_optimizer_group_states = base_optimizer_group_states['state']\n\n        saved_keys = base_optimizer_group_states[0].keys()\n\n        for i, group in enumerate(self.optimizer.param_groups):\n            p = group['params'][0]\n            padding = 0 if group_paddings is None else group_paddings[i]\n            for key in saved_keys:\n                saved = base_optimizer_group_states[i][key]\n\n                if torch.is_tensor(saved):\n                    if key in self.optimizer.state[p]:\n                        dst_tensor = self.optimizer.state[p][key]\n                        src_tensor = _get_padded_tensor(saved, dst_tensor.numel())\n                        self.optimizer.state[p][key].data.copy_(src_tensor.data)\n                    else:\n                        self.optimizer.state[p][key] = _pad_tensor_by_size(\n                            saved, padding, torch.float32,\n                            torch.device('cpu') if self.cpu_offload else self.device)\n                else:\n                    self.optimizer.state[p][key] = saved\n\n        for param_group in self.optimizer.param_groups:\n            param_group['step'] = base_optimizer_state_step\n\n    def get_ep_ranks(self, rank=0, group_name=None):\n        from deepspeed.utils import groups\n        expert_parallel_size_ = groups._get_expert_parallel_world_size(group_name)\n        world_size = groups._get_data_parallel_world_size()\n        rank = groups._get_expert_parallel_rank(group_name)\n        ranks = range(rank, world_size, expert_parallel_size_)\n        return list(ranks)\n\n    # Restore base optimizer state from elastic checkpoint by\n    # 1) Merging optimizer state from checkpoints of all partitions\n    # 2) Extracting optimizer state for current partition from the merged state\n    # 3) Using the extracted value to directly update the base optimizer.\n    def _restore_elastic_base_optimizer_state(self, all_state_dict):\n        base_optimizer_group_states = []\n        for i in range(len(self.optimizer.param_groups)):\n            partition_states = {}\n            all_partition_group_states = [sd[BASE_OPTIMIZER_STATE][i] for sd in all_state_dict]\n\n            if self.is_moe_group(self.optimizer.param_groups[i]):\n                ranks = self.get_ep_ranks(group_name=self.optimizer.param_groups[i]['name'])\n                all_partition_group_states = [all_partition_group_states[i] for i in ranks]\n\n            for key in all_partition_group_states[0].keys():\n                all_partition_states = [all_states[key] for all_states in all_partition_group_states]\n                partition_states[key] = self._partition_base_optimizer_state(key, all_partition_states, i)\n            base_optimizer_group_states.append(partition_states)\n\n        self._restore_base_optimizer_state(base_optimizer_group_states,\n                                           self._restore_step_from_elastic_checkpoint(all_state_dict), None)\n\n    def load_state_dict(self,\n                        state_dict_list,\n                        load_optimizer_states=True,\n                        load_from_fp32_weights=False,\n                        checkpoint_folder=None,\n                        load_serial=None):\n        if checkpoint_folder:\n            self._load_universal_checkpoint(checkpoint_folder, load_optimizer_states, load_from_fp32_weights)\n        else:\n            self._load_legacy_checkpoint(state_dict_list, load_optimizer_states, load_from_fp32_weights)\n\n    def _load_universal_checkpoint(self, checkpoint_folder, load_optimizer_states, load_from_fp32_weights):\n        self.load_hp_checkpoint_state_from_checkpoint_dir(\"bit16_groups\", checkpoint_folder)\n\n    @property\n    def param_groups(self):\n        \"\"\"Forward the wrapped optimizer's parameters.\"\"\"\n        return self.optimizer.param_groups\n\n    def _load_global_state(self, sd):\n        self.loss_scaler = sd.get(LOSS_SCALER, self.loss_scaler)\n        self.dynamic_loss_scale = sd.get('dynamic_loss_scale', self.dynamic_loss_scale)\n        self.overflow = sd.get('overflow', self.overflow)\n        self.clip_grad = sd.get(CLIP_GRAD, self.clip_grad)\n\n        ckpt_version = sd.get(DS_VERSION, False)\n        assert ckpt_version, f\"Empty ds_version in checkpoint, not clear how to proceed\"\n        ckpt_version = pkg_version.parse(ckpt_version)\n\n        # zero stage 1 mode\n        if not self.partition_gradients:\n            required_version = pkg_version.parse(\"0.3.17\")\n            error_str = f\"ZeRO stage 1 changed in {required_version} and is not backwards compatible \" \\\n                \"with older stage 1 checkpoints. If you'd like to load an old ZeRO-1 checkpoint \" \\\n                \"please use an older version of DeepSpeed (<= 0.5.8) and set 'legacy_stage1': true in your zero config json.\"\n            assert required_version <= ckpt_version, f\"Old version: {ckpt_version} {error_str}\"\n\n    def _load_legacy_checkpoint(self, state_dict_list, load_optimizer_states=True, load_from_fp32_weights=False):\n        r\"\"\"Loading ZeRO checkpoint\n\n        Arguments:\n            state_dict_list: List of all saved ZeRO checkpoints, one for each saved partition.\n                Note that the number of saved partitions may differ from number of loading partitions to support\n                changing GPU count, specifically DP world size, between saving and loading checkpoints.\n            load_optimizer_states: Boolean indicating whether or not to load base optimizer states\n            load_from_fp32_weights: Boolean indicating whether to initialize fp32 master weights from fp32\n            copies in checkpoints (no precision loss) or from model's fp16 copies (with precision loss).\n        \"\"\"\n        \"\"\"\n        Loads a state_dict created by an earlier call to state_dict().\n        If ``fp16_optimizer_instance`` was constructed from some ``init_optimizer``,\n        whose parameters in turn came from ``model``, it is expected that the user\n        will call ``model.load_state_dict()`` before\n        ``fp16_optimizer_instance.load_state_dict()`` is called.\n        Example::\n            model = torch.nn.Linear(D_in, D_out).to(get_accelerator().device_name()).half()\n            optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n            optimizer = FP16_Optimizer(optimizer, static_loss_scale = 128.0)\n            ...\n            checkpoint = torch.load(\"saved.pth\")\n            model.load_state_dict(checkpoint['model'])\n            optimizer.load_state_dict(checkpoint['optimizer'])\n        \"\"\"\n\n        # I think it should actually be ok to reload the optimizer before the model.\n        dp_rank = dist.get_rank(group=self.dp_process_group)\n        current_rank_sd = state_dict_list[dp_rank]\n        self._load_global_state(current_rank_sd)\n\n        ckpt_is_rigid = isinstance(current_rank_sd[BASE_OPTIMIZER_STATE], dict)\n\n        # padding is always at the last rank/partition\n        # if DP=1024 and param-group elems=16 -> padding will be 1024-16 across all but one rank\n        # scenario-1 (shrink): saving w. 4 gpus -> loading w. 2 gpus\n        # scenario-2 (expand): saving w. 2 gpus -> loading w. 4 gpus\n        # if load_optimizer_states:\n        #     if new_dp_size:\n        #         self.strip_padding()\n        #         self.add_padding_w_new_dp_size()\n        #     self.optimizer.load_state_dict(current_rank_sd[BASE_OPTIMIZER_STATE])\n\n        if load_optimizer_states:\n            if ckpt_is_rigid:\n                # loading rigid ckpt into either rigid or elastic exec\n                self.optimizer.load_state_dict(current_rank_sd[BASE_OPTIMIZER_STATE])\n            else:\n                if self.elastic_checkpoint:\n                    # loading elastic into elastic exec\n                    self._restore_elastic_base_optimizer_state(state_dict_list)\n                else:\n                    # loading an elastic checkpoint into rigid exec\n                    self._restore_base_optimizer_state(current_rank_sd[BASE_OPTIMIZER_STATE],\n                                                       current_rank_sd[BASE_OPTIMIZER_STATE_STEP],\n                                                       current_rank_sd[GROUP_PADDINGS])\n\n        # At this point, the optimizer's references to the model's fp32 parameters are up to date.\n        # The optimizer's hyperparameters and internal buffers are also up to date.\n        # However, the fp32 master copies of the model's fp16 params stored by the optimizer are still\n        # out of date.  There are two options.\n        # 1:  Refresh the master params from the model's fp16 params.\n        # This requires less storage but incurs precision loss.\n        # 2:  Save and restore the fp32 master copies separately.\n        # We choose option 1 if changing DP degree and option 2 otherwise.\n        #\n        # Pytorch Optimizer.load_state_dict casts saved buffers (e.g. momentum) to the type and device\n        # of their associated parameters, because it's possible those buffers might not exist yet in\n        # the current optimizer instance.  In our case, as long as the current FP16_Optimizer has been\n        # constructed in the same way as the one whose state_dict we are loading, the same master params\n        # are guaranteed to exist, so we can just copy_() from the saved master params.\n\n        if load_from_fp32_weights:\n            # option 2 from above\n            if self.elastic_checkpoint and not ckpt_is_rigid:\n                self._restore_from_elastic_fp32_weights(state_dict_list)\n            else:\n                # For non-elastic checkpoint, simply copying from saved weights of current rank is sufficient.\n                for current, saved in zip(self.single_partition_of_fp32_groups,\n                                          current_rank_sd[SINGLE_PARTITION_OF_FP32_GROUPS]):\n                    src_tensor = _get_padded_tensor(saved, current.numel())\n                    current.data.copy_(src_tensor.data)\n        else:\n            # option 1 from above\n            self._restore_from_bit16_weights()\n\n        if load_optimizer_states:\n            self._link_all_hp_params()\n\n\ndef _handle_overflow(cpu_sum, x, i):\n    import math\n    rank = dist.get_rank()\n    if rank == 0:\n        t_i = -1\n        for v_i, v in enumerate(x.data.contiguous().view(-1)):\n            if not math.isfinite(float(v)):\n                t_i = v_i\n                break\n        logger.info(f\"rank {rank} detected overflow {cpu_sum} in tensor {i}:{t_i} shape {x.shape}\")\n\n\ndef estimate_zero2_model_states_mem_needs(total_params,\n                                          num_gpus_per_node=1,\n                                          num_nodes=1,\n                                          cpu_offload=True,\n                                          additional_buffer_factor=1.5):\n\n    total_gpus = num_nodes * num_gpus_per_node\n\n    if cpu_offload:\n        gpu_mem = 2 * total_params\n        cpu_mem = total_params * max(4 * total_gpus, 16) * additional_buffer_factor\n    else:\n        # GPU's total_params multipliers: 2 = params_16bit,\n        # 18 = 2_grads_16bit + 4_grads_32bit + 4_params_32bit + 8_optimizer_states_32bit(momentum and variance)\n        gpu_mem = 2 * total_params + int(18 * total_params / total_gpus)\n        cpu_mem = total_params * 4 * num_gpus_per_node * additional_buffer_factor\n\n    return int(cpu_mem), int(gpu_mem)\n\n\ndef model_to_params(model):\n    # shared params calculated only once\n    total_params = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters()).values())\n    return total_params\n\n\ndef estimate_zero2_model_states_mem_needs_all_live(model,\n                                                   num_gpus_per_node=1,\n                                                   num_nodes=1,\n                                                   additional_buffer_factor=1.5):\n    \"\"\"\n    Print out estimates on memory usage requirements for ZeRO 2 params, optim states and gradients\n    for a given ``model`` and hardware setup.\n\n    If you have an actual model object, use this function and everything will be derived\n    automatically.\n\n    If it's a hypothetical model, use ``estimate_zero2_model_states_mem_needs_all_cold`` where you have to pass\n    the ``total_params`` explicitly.\n\n    Args:\n        - ``model``: ``nn.Module`` object\n        - ``num_gpus_per_node``: how many gpus per node (defaults to 1)\n        - ``num_nodes``: how many nodes (defaults to 1),\n        - ``additional_buffer_factor``: estimation factor (defaults to 1.5):\n\n    \"\"\"\n\n    total_params = model_to_params(model)\n\n    estimate_zero2_model_states_mem_needs_all_cold(total_params=total_params,\n                                                   num_gpus_per_node=num_gpus_per_node,\n                                                   num_nodes=num_nodes,\n                                                   additional_buffer_factor=additional_buffer_factor)\n\n\ndef estimate_zero2_model_states_mem_needs_all_cold(total_params,\n                                                   num_gpus_per_node=1,\n                                                   num_nodes=1,\n                                                   additional_buffer_factor=1.5):\n    \"\"\"\n    Print out estimates on memory usage requirements for ZeRO 2 params, optim states and gradients\n    for a given ``model`` and hardware setup.\n\n    If it's a hypothetical model, use this function where you have to pass\n    the ``total_params`` and ``largest_layer_params`` explicitly.\n\n    If you have an actual model object, use ``estimate_zero2_model_states_mem_needs_all_live`` and everything\n    will be derived automatically.\n\n    Args:\n        - ``total_params``: total  model params\n        - ``num_gpus_per_node``: how many gpus per node (defaults to 1)\n        - ``num_nodes``: how many nodes (defaults to 1),\n        - ``additional_buffer_factor``: estimation factor (defaults to 1.5):\n\n    \"\"\"\n\n    def format_options(cpu_offload):\n        enabled = []\n        device = f'{OffloadDeviceEnum.cpu:4}' if cpu_offload else \"none\"\n        enabled.append(f\"offload_optimizer={device}\")\n        return \", \".join(enabled)\n\n    nodes_str = \"nodes\" if num_nodes > 1 else \"node\"\n    gpus_str = \"GPUs\" if num_gpus_per_node > 1 else \"GPU\"\n    print(\"Estimated memory needed for params, optim states and gradients for a:\\n\"\n          f\"HW: Setup with {num_nodes} {nodes_str}, {num_gpus_per_node} {gpus_str} per node.\\n\"\n          f\"SW: Model with {int(total_params/1e6)}M total params.\")\n    print(\"  per CPU  |  per GPU |   Options\")\n    for cpu_offload in [True, False]:\n        cpu_mem, gpu_mem = estimate_zero2_model_states_mem_needs(total_params=total_params,\n                                                                 num_gpus_per_node=num_gpus_per_node,\n                                                                 num_nodes=num_nodes,\n                                                                 cpu_offload=cpu_offload,\n                                                                 additional_buffer_factor=additional_buffer_factor)\n\n        options_str = format_options(cpu_offload=cpu_offload)\n        print(f\" {cpu_mem/2**30:7.2f}GB | {gpu_mem/2**30:6.2f}GB | {options_str}\")\n", "deepspeed/runtime/zero/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .partition_parameters import ZeroParamType\nfrom .partition_parameters import ZeroParamStatus\nfrom .partition_parameters import Init\nfrom .partition_parameters import GatheredParameters\nfrom .partition_parameters import register_external_parameter\n\nfrom .tiling import TiledLinear\nfrom .tiling import TiledLinearReturnBias\n\nfrom .mics import MiCS_Init\n", "deepspeed/runtime/zero/partitioned_param_profiler.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom dataclasses import dataclass\nfrom deepspeed.utils import log_dist\n\n\nclass PartitionedParameterProfiler(object):\n\n    @dataclass\n    class EventCounter:\n        name: str\n        count: int\n        num_elem: int\n\n        def reset(self):\n            self.count = 0\n            self.num_elem = 0\n\n        def increment(self, numel):\n            self.count += 1\n            self.num_elem += numel\n\n    def __init__(self, timers):\n        self.timers = timers\n        self.event_counters = {}\n\n    def reset_events(self):\n        for event_ctr in self.event_counters.values():\n            event_ctr.reset()\n\n    def start_event(self, name):\n        if self.timers is None:\n            return\n\n        if name not in self.event_counters:\n            self.event_counters[name] = __class__.EventCounter(name=name, count=0, num_elem=0)\n        self.timers(name).start()\n\n    def stop_event(self, name, num_elem):\n        if self.timers is None:\n            return\n        assert name in self.event_counters, f'unknown event {name}'\n        self.event_counters[name].increment(num_elem)\n        self.timers(name).stop()\n\n    def _log_timers(self):\n        if self.timers is None:\n            return\n        self.timers.log(names=list(self.event_counters.keys()))\n\n    def _log_event_counters(self):\n        for event_ctr in self.event_counters.values():\n            log_dist(\n                f'{event_ctr.name}: count = {event_ctr.count}, numel = {event_ctr.num_elem}',\n                #f'{event_ctr.name}: time = {self._log_timers()},count = {event_ctr.count}, numel = {event_ctr.num_elem}',\n                ranks=[0])\n\n    def log_events(self):\n        self._log_event_counters()\n        self._log_timers()\n", "deepspeed/runtime/zero/offload_config.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom enum import Enum\nfrom pathlib import Path\nfrom deepspeed.pydantic_v1 import Field, validator\nfrom deepspeed.runtime.config_utils import DeepSpeedConfigModel, pp_int\n\n\nclass OffloadDeviceEnum(str, Enum):\n    \"\"\" Enum for valid offload devices \"\"\"\n    none = \"none\"\n    cpu = \"cpu\"\n    nvme = \"nvme\"\n\n\nclass DeepSpeedZeroOffloadParamConfig(DeepSpeedConfigModel):\n    \"\"\" Set options for parameter offload. Valid only with stage 3. \"\"\"\n\n    device: OffloadDeviceEnum = \"none\"\n    \"\"\"\n    Device memory to offload model parameters. Supported options are `cpu` and\n    `nvme`.\n    \"\"\"\n\n    nvme_path: Path = None\n    \"\"\" Filesystem path for NVMe device for parameter offloading. \"\"\"\n\n    buffer_count: int = Field(5, ge=0)\n    \"\"\" Number of buffers in buffer pool for parameter offloading to NVMe. \"\"\"\n\n    buffer_size: int = Field(pp_int(1e8), ge=0)\n    \"\"\" Size of buffers in buffer pool for parameter offloading to NVMe. \"\"\"\n\n    max_in_cpu: int = Field(pp_int(1e9), ge=0)\n    \"\"\"\n    Number of parameter elements to maintain in CPU memory when offloading to\n    NVMe is enabled.\n    \"\"\"\n\n    pin_memory: bool = False\n    \"\"\"\n    Offload to page-locked CPU memory. This could boost throughput at the cost\n    of extra memory overhead.\n    \"\"\"\n\n\nclass DeepSpeedZeroOffloadOptimizerConfig(DeepSpeedConfigModel):\n    \"\"\" Set options for optimizer offload. Valid with stage 1, 2, and 3. \"\"\"\n\n    device: OffloadDeviceEnum = \"none\"\n    \"\"\"\n    Device memory to offload optimizer state. Supported options are `cpu` and\n    `nvme`. Optimizer computation is offload to CPU regardless of device option.\n    \"\"\"\n\n    nvme_path: Path = None\n    \"\"\" Filesystem path for NVMe device for optimizer state offloading. \"\"\"\n\n    buffer_count: int = Field(4, ge=0)\n    \"\"\"\n    Number of buffers in buffer pool for optimizer state offloading to NVMe.\n    This should be at least the number of states maintained per parameter by\n    the optimizer. For example, Adam optimizer has 4 states (parameter,\n    gradient, momentum, and variance).\n    \"\"\"\n\n    pin_memory: bool = False\n    \"\"\"\n    Offload to page-locked CPU memory. This could boost throughput at the cost\n    of extra memory overhead.\n    \"\"\"\n\n    pipeline_read: bool = False\n    \"\"\"\n    For tile-based optimizer step processing, overlap read of next tile with\n    computation of current tile. Used in ZeRO-Infinity.\n    \"\"\"\n\n    pipeline_write: bool = False\n    \"\"\"\n    For tile-based optimizer step processing, overlap write of previous tile\n    with computation of current tile.\n    \"\"\"\n\n    fast_init: bool = False\n    \"\"\" Enable fast optimizer initialization when offloading to NVMe. \"\"\"\n\n    @validator(\"pipeline_read\", \"pipeline_write\", always=True)\n    def set_pipeline(cls, field_value, values):\n        values[\"pipeline\"] = field_value or values.get(\"pipeline\", False)\n        return field_value\n\n    ratio: float = Field(1.0, ge=0.0, le=1.0)\n    \"\"\" Percentage of offloaded optimizer states to CPU Adam. Only valid with ZeRO Stage 3.\"\"\"\n", "deepspeed/runtime/zero/parameter_offload.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport sys\nimport torch\nfrom collections import OrderedDict\nfrom deepspeed.utils import z3_leaf_module\nfrom deepspeed.runtime.utils import see_memory_usage\nfrom deepspeed.runtime.zero.utils import apply_to_tensors_only, is_zero_param\nfrom deepspeed.runtime.zero.offload_config import OffloadDeviceEnum\nfrom deepspeed.runtime.zero.partition_parameters import _init_external_params\nfrom deepspeed.runtime.zero.partition_parameters import *\nfrom deepspeed.runtime.zero.partitioned_param_coordinator import PartitionedParameterCoordinator, InflightParamRegistry, iter_params\nfrom deepspeed.accelerator import get_accelerator\n\nFWD_MODULE_STACK = list()\n\n# ensure we only warn once, otherwise every iteration will trigger a warning\nwarned = False\n\n\n#for each tensor in outputs run the forward_function and register backward_function as hook\ndef _apply_forward_and_backward_to_tensors_only(module, forward_function, backward_function, outputs):\n    if type(outputs) is tuple:\n        touched_outputs = []\n        for output in outputs:\n            touched_output = _apply_forward_and_backward_to_tensors_only(module, forward_function, backward_function,\n                                                                         output)\n            touched_outputs.append(touched_output)\n        return tuple(touched_outputs)\n    elif type(outputs) is torch.Tensor:\n        forward_function(outputs)\n        if outputs.requires_grad:\n            outputs.register_hook(backward_function)\n        return outputs\n    else:\n        return outputs\n\n\nclass ZeROOrderedDict(OrderedDict):\n\n    def __init__(self, parent_module, *args, **kwargs):\n        \"\"\"A replacement for ``collections.OrderedDict`` to detect external ZeRO params.\n\n        Args:\n            parent_module (``collections.OrderedDict``): the collection to replace\n        \"\"\"\n\n        super().__init__(*args, **kwargs)\n        self._parent_module = parent_module\n        self._in_forward = False\n\n    def __getitem__(self, key):\n        param = super().__getitem__(key)\n\n        # Params can be registered as None (e.g., bias)\n        if param is None:\n            return param\n\n        if param.ds_status == ZeroParamStatus.NOT_AVAILABLE:\n            if self._parent_module._parameters._in_forward:\n                register_external_parameter(FWD_MODULE_STACK[-1], param)\n                param.all_gather()\n                print_rank_0(f'Registering external parameter from getter {key} ds_id = {param.ds_id}', force=False)\n\n        return param\n\n\ndef _inject_parameters(module, cls):\n    for module in module.modules():\n        if cls == ZeROOrderedDict:\n            new_param = cls(parent_module=module)\n        else:\n            new_param = cls()\n\n        for key, param in module._parameters.items():\n            new_param[key] = param\n        module._parameters = new_param\n\n\nclass DeepSpeedZeRoOffload(object):\n\n    def __init__(\n        self,\n        module,\n        timers,\n        ds_config,\n        overlap_comm=True,\n        prefetch_bucket_size=50000000,\n        max_reuse_distance=1000000000,\n        max_live_parameters=1000000000,\n        param_persistence_threshold=100000,\n        model_persistence_threshold=sys.maxsize,\n        dp_process_group=None,\n        offload_param_config=None,\n        mpu=None,\n        zero_param_parallel_group=None,\n        zero_quantized_weights=False,\n        zero_quantized_nontrainable_weights=False,\n    ):\n\n        see_memory_usage(\"DeepSpeedZeRoOffload initialize [begin]\", force=True)\n\n        print_rank_0(f\"initialized {__class__.__name__} with args: {locals()}\", force=False)\n\n        self.module = module\n        self.timers = timers\n        self.dtype = list(module.parameters())[0].dtype\n        self.dp_process_group = dp_process_group\n        self.offload_device = None\n        self.offload_param_pin_memory = False\n        self.zero_param_parallel_group = zero_param_parallel_group\n        self.zero_quantized_weights = zero_quantized_weights\n        self.zero_quantized_nontrainable_weights = zero_quantized_nontrainable_weights\n\n        if offload_param_config is not None and offload_param_config.device != OffloadDeviceEnum.none:\n            self.offload_device = offload_param_config.device\n            self.offload_param_pin_memory = offload_param_config.pin_memory\n\n        self._convert_to_zero_parameters(ds_config, module, mpu)\n\n        for m in module.modules():\n            _init_external_params(m)\n\n        _inject_parameters(module, ZeROOrderedDict)\n\n        self.param_numel_persistence_threshold = int(param_persistence_threshold)\n        self.model_persistence_threshold = int(model_persistence_threshold)\n        self.persistent_parameters = self.mark_persistent_parameters(self.param_numel_persistence_threshold,\n                                                                     self.model_persistence_threshold)\n\n        self.param_coordinators = {}\n        self._prefetch_bucket_sz = int(prefetch_bucket_size)\n        self._max_reuse_distance_in_numel = int(max_reuse_distance)\n        self._max_available_parameters_in_numel = int(max_live_parameters)\n        self.__allgather_stream = None if get_accelerator().is_synchronized_device() else get_accelerator().Stream(\n        ) if overlap_comm else get_accelerator().default_stream()\n\n        if not hasattr(module, \"ds_inflight_param_registry\"):\n            module.ds_inflight_param_registry = dict()\n            # we need two registries, one for training and one for eval. They will be used when creating PartitionedParameterCoordinator\n            module.ds_inflight_param_registry[True] = InflightParamRegistry()\n            module.ds_inflight_param_registry[False] = InflightParamRegistry()\n        self.__inflight_param_registry = module.ds_inflight_param_registry\n\n        self.forward_hooks = []\n        self.backward_hooks = []\n        self.setup_zero_stage3_hooks()\n        print_rank_0(\n            f'Created module hooks: forward = {len(self.forward_hooks)}, backward = {len(self.backward_hooks)}',\n            force=False)\n\n        see_memory_usage(\"DeepSpeedZeRoOffload initialize [end]\", force=True)\n\n    @instrument_w_nvtx\n    def partition_all_parameters(self):\n        \"\"\"Partitioning Parameters that were not partitioned usually if parameters\n        of modules whose input parameters do not require grad computation do not\n        trigger post call and will therefore will remain unpartitioned\"\"\"\n        self.get_param_coordinator(training=self.module.training).release_and_reset_all(self.module)\n        for param in iter_params(self.module, recurse=True):\n            if param.ds_status != ZeroParamStatus.NOT_AVAILABLE:\n                raise RuntimeError(f\"{param.ds_summary()} expected to be released\")\n\n    def get_param_coordinator(self, training):\n        if not training in self.param_coordinators:\n            self.param_coordinators[training] = PartitionedParameterCoordinator(\n                prefetch_bucket_sz=self._prefetch_bucket_sz,\n                max_reuse_distance_in_numel=self._max_reuse_distance_in_numel,\n                max_available_parameters_in_numel=self._max_available_parameters_in_numel,\n                allgather_stream=self.__allgather_stream,\n                inflight_param_registry=self.__inflight_param_registry[training],\n                prefetch_nvme=self.offload_device == OffloadDeviceEnum.nvme,\n                timers=self.timers,\n                zero_quantized_weights=self.zero_quantized_weights,\n                zero_quantized_nontrainable_weights=self.zero_quantized_nontrainable_weights,\n            )\n\n        return self.param_coordinators[training]\n\n    def empty_partition_cache(self):\n        self.partition_all_parameters()\n\n    def _convert_to_zero_parameters(self, ds_config, module, mpu):\n        non_zero_params = [p for p in module.parameters() if not is_zero_param(p)]\n        if non_zero_params:\n            zero_params = [p for p in module.parameters() if is_zero_param(p)]\n            if zero_params:\n                zero_params[0].convert_to_zero_parameters(param_list=non_zero_params)\n            else:\n                group = None\n                if mpu:\n                    group = mpu.get_data_parallel_group()\n\n                Init(module=module,\n                     data_parallel_group=group,\n                     dtype=self.dtype,\n                     config_dict_or_path=ds_config,\n                     remote_device=self.offload_device,\n                     pin_memory=self.offload_param_pin_memory,\n                     mpu=mpu,\n                     zero_param_parallel_group=self.zero_param_parallel_group,\n                     zero_quantized_weights=self.zero_quantized_weights,\n                     zero_quantized_nontrainable_weights=self.zero_quantized_nontrainable_weights)\n\n    def destroy(self):\n        self._remove_module_hooks()\n\n    def _remove_module_hooks(self):\n        num_forward_hooks = len(self.forward_hooks)\n        num_backward_hooks = len(self.backward_hooks)\n\n        for hook in self.forward_hooks:\n            hook.remove()\n\n        for hook in self.backward_hooks:\n            hook.remove()\n\n        print_rank_0(f'Deleted module hooks: forward = {num_forward_hooks}, backward = {num_backward_hooks}',\n                     force=False)\n\n    def setup_zero_stage3_hooks(self):\n        self.hierarchy = 0\n\n        #reset step if in inference mode\n        @instrument_w_nvtx\n        def _end_of_forward_hook(module, *args):\n\n            if not torch._C.is_grad_enabled():\n                self.get_param_coordinator(training=False).reset_step()\n\n        #likely one of them should be enough but just to be safe\n        self._register_hooks_recursively(self.module)\n        self.module.register_forward_hook(_end_of_forward_hook)\n\n        # Add top module to stack trace\n        global FWD_MODULE_STACK\n        FWD_MODULE_STACK.append(self.module)\n\n    def mark_persistent_parameters(self, param_threshold, model_threshold):\n        persistent_params = []\n        total_persistent_parameters = 0\n        params_count = 0\n        for name, param in self.module.named_parameters(recurse=True):\n            if param.ds_numel + total_persistent_parameters > model_threshold:\n                continue\n\n            if param.ds_numel <= param_threshold:\n                params_count += 1\n                param.ds_persist = True\n                persistent_params.append(param)\n                total_persistent_parameters += param.ds_numel\n\n        print_rank_0(\n            f\"Parameter Offload: Total persistent parameters: {total_persistent_parameters} in {params_count} params\",\n            force=True)\n\n        return persistent_params\n\n    def _register_hooks_recursively(self, module, count=[0]):\n        my_count = count[0]\n        module.id = my_count\n\n        #print(f\"{module.__class__} : {module.id}\")\n\n        if z3_leaf_module(module):\n            for param in module.parameters():\n                param.ds_z3_leaf_module = module\n        else:\n            for child in module.children():\n                count[0] = count[0] + 1\n                self._register_hooks_recursively(child, count=count)\n\n        @instrument_w_nvtx\n        def _pre_forward_module_hook(module, *args):\n            self.pre_sub_module_forward_function(module)\n\n        @instrument_w_nvtx\n        def _post_forward_module_hook(module, input, output):\n\n            global FWD_MODULE_STACK\n            FWD_MODULE_STACK.pop()\n            if output is None:\n                output = []\n            elif not isinstance(output, (list, tuple)):\n                if torch.is_tensor(output):\n                    output = [output]\n                else:\n                    #print(f'got UNKNOWN type {type(output)}')\n                    outputs = []\n                    output = output if isinstance(output, dict) else vars(output)\n                    for name, val in output.items():\n                        if not name.startswith('__') and torch.is_tensor(val):\n                            outputs.append(val)\n                    output = outputs\n\n            for item in filter(lambda item: is_zero_param(item) or hasattr(item, 'ds_param_alias'), output):\n                key = id(item) if hasattr(item, 'ds_id') else id(item.ds_param_alias)\n                actual_external_param = item if hasattr(item, 'ds_id') else item.ds_param_alias\n\n                if not any(key in m._external_params for m in FWD_MODULE_STACK):\n                    actual_external_param.is_external_param = True\n                    module_to_register = FWD_MODULE_STACK[-1]\n                    register_external_parameter(module_to_register, actual_external_param)\n                    print_rank_0(\n                        f'Registering dangling parameter for module {module_to_register.__class__.__name__}, ds_id = {actual_external_param.ds_id}.',\n                        force=False)\n\n                    # It's possible that the parameter was already external to the completed module. If so, remove it the\n                    # registration as it will be covered by the outer module instead.\n                    if key in module._external_params:\n                        print_rank_0(\n                            f'  Unregistering nested dangling parameter from module {module.__class__.__name__}, ds_id = {actual_external_param.ds_id}',\n                            force=False)\n                        unregister_external_parameter(module, actual_external_param)\n\n                    actual_external_param.all_gather()\n\n            self.post_sub_module_forward_function(module)\n\n        def _bwd_hook_unexpected_inputs_msg(value):\n            return f\"A module has unknown inputs or outputs type ({type(value)}) and the tensors embedded in it cannot be detected. \" \\\n                \"The ZeRO-3 hooks designed to trigger before or after backward pass of the module relies on knowing the input and \" \\\n                \"output tensors and therefore may not get triggered properly.\"\n\n        def _pre_backward_module_hook(module, inputs, output):\n\n            if not hasattr(module, \"pre_bwd_fn\"):\n\n                @instrument_w_nvtx\n                def _run_before_backward_function(sub_module):\n                    # some models (e.g. Albert) may run multiple forwards on the same layer in a loop\n                    # before doing backwards, so each backward will need a pre-fetch - using reference\n                    # counting to support this scenario\n                    #print(f\"COUNTER before: {sub_module.applied_pre_backward_ref_cnt}\")\n                    if sub_module.applied_pre_backward_ref_cnt > 0:\n                        self.pre_sub_module_backward_function(sub_module)\n                        sub_module.applied_pre_backward_ref_cnt -= 1\n                    #print(f\"COUNTER after: {sub_module.applied_pre_backward_ref_cnt}\")\n\n                class PreBackwardFunctionForModule(torch.autograd.Function):\n\n                    @staticmethod\n                    def forward(ctx, outputs):\n                        # Capture `module` and _run_before_backward_function\n                        ctx.module = module\n                        ctx.pre_backward_function = _run_before_backward_function\n                        if not hasattr(ctx.module, \"applied_pre_backward_ref_cnt\"):\n                            ctx.module.applied_pre_backward_ref_cnt = 0\n                        ctx.module.applied_pre_backward_ref_cnt += 1\n                        outputs = outputs.detach()\n                        return outputs\n\n                    @staticmethod\n                    def backward(ctx, *args):\n                        ctx.pre_backward_function(ctx.module)\n                        return args\n\n                module.pre_bwd_fn = PreBackwardFunctionForModule\n\n            return apply_to_tensors_only(module.pre_bwd_fn.apply,\n                                         output,\n                                         warning_msg_fn=_bwd_hook_unexpected_inputs_msg)\n\n        #This is an alternate to doing _post_backward_module_hook\n        #it uses tensor.register_hook instead of using torch.autograd.Function\n        def _alternate_post_backward_module_hook(module, inputs):\n            module.ds_grads_remaining = 0\n\n            #print(f\"Before Forward {module.__class__.__name__}\")\n\n            def _run_after_backward_hook(*unused):\n                module.ds_grads_remaining = module.ds_grads_remaining - 1\n                if module.ds_grads_remaining == 0:\n                    #print(f\"After backward {module.__class__.__name__}\")\n                    self.post_sub_module_backward_function(module)\n\n            def _run_before_forward_function(input):\n                if input.requires_grad:\n                    module.ds_grads_remaining += 1\n\n            return _apply_forward_and_backward_to_tensors_only(module, _run_before_forward_function,\n                                                               _run_after_backward_hook, inputs)\n\n        def _post_backward_module_hook(module, inputs):\n            module.ds_grads_remaining = 0\n\n            if not hasattr(module, \"post_bwd_fn\"):\n\n                @instrument_w_nvtx\n                def _run_after_backward_function(sub_module):\n                    if sub_module.ds_grads_remaining == 0:\n                        self.post_sub_module_backward_function(sub_module)\n\n                class PostBackwardFunctionModule(torch.autograd.Function):\n\n                    @staticmethod\n                    def forward(ctx, output):\n                        ctx.module = module\n                        if output.requires_grad:\n                            #TODO SOME TIMES post backward does not seem to be triggered debug in detail\n                            #Should only cause increase in memory not correctness issue\n                            #if output.grad_fn.__class__.__name__ == 'ViewBackward':\n                            #    ctx.view=True\n                            #    print(f\"Warning view tensor for input to module : {module.__class__.__name__}. Backward hooks may not trigger properly\")\n                            #assert len(module.parameters(recurse=False)), \"The input tensor to the module is a view, and autograd Function or register_hook is not triggered with view tensors.\"\n                            #if module.ds_grads_remaining == 0:\n                            #    print(f\"Before Forward: {ctx.module.__class__.__name__}\")\n                            module.ds_grads_remaining += 1\n                            ctx.post_backward_function = _run_after_backward_function\n                        output = output.detach()\n                        return output\n\n                    @staticmethod\n                    def backward(ctx, *args):\n                        ctx.module.ds_grads_remaining = ctx.module.ds_grads_remaining - 1\n                        if ctx.module.ds_grads_remaining == 0:\n                            ctx.post_backward_function(ctx.module)\n                        return args\n\n                module.post_bwd_fn = PostBackwardFunctionModule\n\n            return apply_to_tensors_only(module.post_bwd_fn.apply,\n                                         inputs,\n                                         warning_msg_fn=_bwd_hook_unexpected_inputs_msg)\n\n        # Pre forward hook\n        self.forward_hooks.append(module.register_forward_pre_hook(_pre_forward_module_hook))\n\n        # Post forward hook\n        self.forward_hooks.append(module.register_forward_hook(_post_forward_module_hook))\n\n        # Pre backward hook\n        self.backward_hooks.append(module.register_forward_hook(_pre_backward_module_hook))\n\n        # post backward hook\n        self.backward_hooks.append(module.register_forward_pre_hook(_post_backward_module_hook))\n\n    @torch.no_grad()\n    def pre_sub_module_forward_function(self, sub_module):\n        see_memory_usage(f\"Before sub module function {sub_module.__class__.__name__}\", force=False)\n\n        global FWD_MODULE_STACK\n        FWD_MODULE_STACK.append(sub_module)\n\n        param_coordinator = self.get_param_coordinator(training=sub_module.training)\n        param_coordinator.trace_prologue(sub_module)\n        if param_coordinator.is_record_trace():\n            param_coordinator.record_module(sub_module)\n        param_coordinator.fetch_sub_module(sub_module, forward=True)\n\n        see_memory_usage(f\"Before sub module function {sub_module.__class__.__name__} after fetch\", force=False)\n\n    @torch.no_grad()\n    def post_sub_module_forward_function(self, sub_module):\n        see_memory_usage(f\"After sub module function {sub_module.__class__.__name__} {sub_module.id} before release\",\n                         force=False)\n\n        param_coordinator = self.get_param_coordinator(training=sub_module.training)\n        param_coordinator.release_sub_module(sub_module)\n\n        see_memory_usage(f\"After sub module function {sub_module.__class__.__name__}  {sub_module.id} after release\",\n                         force=False)\n\n    @torch.no_grad()\n    def pre_sub_module_backward_function(self, sub_module):\n        assert sub_module.training, \"backward pass is invalid for module in evaluation mode\"\n        param_coordinator = self.get_param_coordinator(training=True)\n        param_coordinator.trace_prologue(sub_module)\n        if param_coordinator.is_record_trace():\n            param_coordinator.record_module(sub_module)\n        param_coordinator.fetch_sub_module(sub_module, forward=False)\n\n    @torch.no_grad()\n    def post_sub_module_backward_function(self, sub_module):\n        assert sub_module.training, \"backward pass is invalid for module in evaluation mode\"\n        see_memory_usage(\n            f\"After sub module backward function {sub_module.__class__.__name__} {sub_module.id} before release\",\n            force=False)\n\n        self.get_param_coordinator(training=True).release_sub_module(sub_module)\n\n        see_memory_usage(\n            f\"After sub module backward function {sub_module.__class__.__name__} {sub_module.id} after release\",\n            force=False)\n", "deepspeed/runtime/zero/partition_parameters.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport math\nimport os\nimport types\nfrom typing import Callable, Iterable\nfrom enum import Enum\nimport functools\nimport itertools\nfrom typing import List\nfrom collections import defaultdict\nimport logging\nimport torch\nfrom torch import Tensor\nfrom deepspeed import comm as dist\nfrom torch.nn import Module\nfrom torch.nn import Parameter\n\nfrom .linear import zero3_linear_wrap\n\nfrom deepspeed.utils import groups\nimport deepspeed\nfrom ..utils import see_memory_usage, get_only_unique_item\nfrom deepspeed.runtime.zero.config import DeepSpeedZeroConfig\nfrom deepspeed.runtime.zero.utils import assert_ints_same_as_other_ranks, is_zero_param\nfrom deepspeed.runtime.zero.offload_config import OffloadDeviceEnum\nfrom deepspeed.runtime.config_utils import get_config_default\nfrom deepspeed.utils import instrument_w_nvtx, logger\nfrom deepspeed.comm.comm import init_distributed\nfrom deepspeed.utils.debug import (debug_param2name_id_shape, debug_param2name_id_shape_device, debug_module2name,\n                                   debug_param2name_id, debug_param2name_id_shape_status)\nfrom deepspeed.accelerator import get_accelerator\nfrom ..swap_tensor.partitioned_param_swapper import AsyncPartitionedParameterSwapper, PartitionedParamStatus\nfrom deepspeed.inference.quantization.utils import _quantize_param, WEIGHT_QUANTIZATION_LAYERS, wrap_quantized_functional, wrap_load_from_state_dict\n\npartitioned_param_data_shape = [0]\nzero_init_context = 0\ntop_level_context = None\n\n\nclass NoGatherHandle:\n\n    def __init__(self, param: Parameter) -> None:\n        if param.ds_status != ZeroParamStatus.INFLIGHT:\n            raise RuntimeError(f\"expected param {param.ds_summary()} to be available\")\n\n        if hasattr(param.ds_tensor, \"ds_quant_scale\"):\n            param.data = Init.quantizer_module.dequantize(param.ds_tensor.data, param.ds_tensor.ds_quant_scale).to(\n                device=get_accelerator().current_device_name(), non_blocking=True).view(param.ds_shape)\n        else:\n            param.data = param.ds_tensor.data.to(device=get_accelerator().current_device_name(),\n                                                 non_blocking=True).view(param.ds_shape)\n        self.__param = param\n\n    def wait(self) -> None:\n        if not get_accelerator().resolves_data_dependency():\n            get_accelerator().current_stream().synchronize()\n        self.__param.ds_status = ZeroParamStatus.AVAILABLE\n\n\nclass NoGatherCoalescedHandle:\n\n    def __init__(self, params: List[Parameter]) -> None:\n        self.__params = params\n        self.__complete = False\n\n        for param in self.__params:\n            if param.ds_status != ZeroParamStatus.INFLIGHT:\n                raise RuntimeError(f\"expected param {param.ds_summary()} to not be available\")\n            if hasattr(param.ds_tensor, \"ds_quant_scale\"):\n                param.data = Init.quantizer_module.dequantize(param.ds_tensor.data, param.ds_tensor.ds_quant_scale).to(\n                    device=get_accelerator().current_device_name(), non_blocking=True).view(param.ds_shape)\n            else:\n                param.data = param.ds_tensor.data.to(device=get_accelerator().current_device_name(),\n                                                     non_blocking=True).view(param.ds_shape)\n\n    @instrument_w_nvtx\n    def wait(self) -> None:\n        if self.__complete:\n            return\n\n        if not get_accelerator().resolves_data_dependency():\n            get_accelerator().current_stream().synchronize()\n        for param in self.__params:\n            assert param.ds_status == ZeroParamStatus.INFLIGHT, f\"expected param {param.ds_summary()} to be inflight\"\n            param.ds_status = ZeroParamStatus.AVAILABLE\n\n        self.__complete = True\n\n\ndef _dist_allgather_fn(input_tensor: Tensor, output_tensor: Tensor, group=None):\n    return instrument_w_nvtx(dist.allgather_fn)(output_tensor, input_tensor, group=group, async_op=True)\n\n\ndef print_rank_0(message, debug=False, force=False):\n    rank = dist.get_rank()\n    if rank == 0 and (debug or force):\n        print(message)\n    # other variations\n    # - print for all ranks w/o interleaving\n    # printflock(f\"[{rank}] {message}\")\n    # - print to log file per rank\n    # log_rank_file(rank, message)\n\n\ndef debug_rank0(msg: str) -> None:\n    if dist.get_rank() == 0:\n        logger.debug(msg)\n\n\ndef _init_external_params(module):\n    if not hasattr(module, '_external_params'):\n        module._external_params = {}\n\n        def external_parameters(self):\n            return self._external_params.items()\n\n        def all_parameters(self):\n            return itertools.chain(self.named_parameters(self, recurse=False), external_parameters(self))\n\n        module.ds_external_parameters = types.MethodType(external_parameters, module)\n        module.all_parameters = types.MethodType(all_parameters, module)\n\n\ndef register_external_parameter(module, parameter):\n    \"\"\"Instruct DeepSpeed to coordinate ``parameter``'s collection and partitioning in\n    the forward and backward passes of ``module``.\n\n    This is used when a parameter is accessed outside of its owning module's\n    ``forward()``. DeepSpeed must know to collect it from its partitioned\n    state and when to release the memory.\n\n    .. note::\n        This is only applicable to training with ZeRO stage 3.\n\n    Args:\n        module (``torch.nn.Module``): The module that requires ``parameter`` in its forward pass.\n        parameter (``torch.nn.Parameter``): The parameter to register.\n\n    Raises:\n        RuntimeError: If ``parameter`` is not of type ``torch.nn.Parameter``.\n\n\n    Examples\n    ========\n\n    #. Register a weight that is used in another module's forward pass (line 6).\n       Parameter ``layer1.weight`` is used by ``layer2`` (line 11).\n\n        .. code-block:: python\n            :linenos:\n            :emphasize-lines: 6,11\n\n            class ModuleZ3(torch.nn.Module):\n                def __init__(self, *args):\n                    super().__init__(self, *args)\n                    self.layer1 = SomeLayer()\n                    self.layer2 = OtherLayer()\n                    deepspeed.zero.register_external_parameter(self, self.layer1.weight)\n\n                def forward(self, input):\n                    x = self.layer1(input)\n                    # self.layer1.weight is required by self.layer2.forward\n                    y = self.layer2(x, self.layer1.weight)\n                    return y\n    \"\"\"\n    if not isinstance(parameter, torch.nn.Parameter):\n        raise RuntimeError('Parameter is not a torch.nn.Parameter')\n\n    if not hasattr(module, '_external_params'):\n        _init_external_params(module)\n\n    key = id(parameter)\n    module._external_params[key] = parameter\n\n\ndef unregister_external_parameter(module, parameter):\n    \"\"\"Reverses the effects of :meth:`register_external_parameter`.\n\n    Args:\n        module (``torch.nn.Module``): The module to affect.\n        parameter (``torch.nn.Parameter``): The parameter to unregister.\n\n    Raises:\n        RuntimeError: If ``parameter`` is not of type ``torch.nn.Parameter``.\n        RuntimeError: If ``parameter`` is not a registered external parameter of ``module``.\n    \"\"\"\n    if not isinstance(parameter, torch.nn.Parameter):\n        raise RuntimeError('Parameter is not a torch.nn.Parameter')\n\n    if not hasattr(module, '_external_params') or id(parameter) not in module._external_params:\n        raise RuntimeError('Parameter is not a registered external parameter of module.')\n\n    key = id(parameter)\n    del module._external_params[key]\n\n\nclass ZeroParamType(Enum):\n\n    # same as regular pytorch parameters\n    NORMAL = 1\n\n    # parameters are partitioned across data parallel process\n    PARTITIONED = 2\n\n    # the parameter is held with a unique process rank\n    # and is not available on all other process\n    REMOTE = 3\n\n\nclass ZeroParamStatus(Enum):\n    # parameters are fully present and ready for use on all processes\n    AVAILABLE = 1\n\n    # parameters are either partitioned or remote in some or all process\n    NOT_AVAILABLE = 2\n\n    # parameters are being gathered.\n    INFLIGHT = 3\n\n\n_orig_torch_tensor = torch.tensor\n_orig_torch_empty = torch.empty\n_orig_torch_zeros = torch.zeros\n_orig_torch_ones = torch.ones\n_orig_torch_full = torch.full\n_orig_torch_arange = torch.arange\n_orig_torch_eye = torch.eye\n_orig_torch_randn = torch.randn\n\n\ndef zero_wrapper_for_fp_tensor_constructor(fn: Callable, target_fp_dtype: torch.dtype) -> Callable:\n\n    def wrapped_fn(*args, **kwargs) -> Tensor:\n        if kwargs.get(\"device\", None) is None:\n            kwargs['device'] = torch.device(get_accelerator().device_name(os.environ[\"LOCAL_RANK\"]))\n        tensor: Tensor = fn(*args, **kwargs)\n        if tensor.is_floating_point():\n            tensor.data = tensor.data.to(target_fp_dtype)\n\n        return tensor\n\n    return wrapped_fn\n\n\ndef get_new_tensor_fn_for_dtype(dtype: torch.dtype) -> Callable:\n\n    def new_tensor(cls, *args, **kwargs) -> Tensor:\n        device = torch.device(get_accelerator().device_name(os.environ[\"LOCAL_RANK\"]))\n        if not args:\n            args = (0, )\n        tensor = _orig_torch_empty(0, device=device).new_empty(*args, **kwargs)\n        if tensor.is_floating_point():\n            tensor = tensor.to(dtype)\n\n        return tensor\n\n    return new_tensor\n\n\n# https://stackoverflow.com/a/63851681/9201239\ndef get_all_subclasses(cls):\n    subclass_list = []\n\n    def recurse(cl):\n        for subclass in cl.__subclasses__():\n            subclass_list.append(subclass)\n            recurse(subclass)\n\n    recurse(cls)\n\n    return set(subclass_list)\n\n\n@instrument_w_nvtx\ndef free_param(param: Parameter) -> None:\n    \"\"\"Free underlying storage of a parameter.\"\"\"\n    assert not param.ds_active_sub_modules, param.ds_summary()\n    if get_accelerator().on_accelerator(param.data):\n        # need to make sure that we don't free the parameter while it is still\n        # being used for computation\n        if not get_accelerator().is_synchronized_device():\n            param.data.record_stream(get_accelerator().current_stream())\n    # param.data doesn't store anything meaningful in partitioned state\n    param.data = torch.empty(0, dtype=param.dtype, device=param.device)\n    param.ds_status = ZeroParamStatus.NOT_AVAILABLE\n\n\nreuse_buffers = False\ntemp_contiguous_tensor = None\nempty_buffers = {}\n\n\n# Inserts _post_init_method at the end of init method\n# for all sub classes of torch.nn.Module\nclass InsertPostInitMethodToModuleSubClasses(object):\n    num_module_parameters = 0\n    num_module_elements = 0\n\n    def __init__(self, enabled=True, mem_efficient_linear=True, ds_config=None, dtype=None):\n        self.mem_efficient_linear = mem_efficient_linear\n        self.enabled = enabled\n        self._set_dtype(ds_config, dtype)\n        assert self.dtype in [\n            torch.half, torch.bfloat16, torch.float\n        ], f\"Invalid data type {self.dtype}, allowed values are [torch.half, torch.bfloat16, torch.float]\"\n        self.wrapped_cls = set()\n        self.skip_init_depth = 0\n\n        self.quantized_initialization = None\n        if ds_config is not None and ds_config.weight_quantization_config and ds_config.weight_quantization_config.quantized_initialization:\n            self.quantized_initialization = ds_config.weight_quantization_config.quantized_initialization\n\n    def __enter__(self):\n        if not self.enabled:\n            return\n\n        global zero_init_context\n        if zero_init_context == 0:\n            self.patch_init_and_builtins()\n            global top_level_context\n            top_level_context = self\n\n        zero_init_context += 1\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        if not self.enabled:\n            return\n\n        global zero_init_context\n        zero_init_context -= 1\n\n        # Exiting the top level context\n        if zero_init_context == 0:\n            self.unpatch_init_and_builtins()\n            global top_level_context\n            top_level_context = None\n\n            if dist.get_rank() == 0:\n                billion_elems = InsertPostInitMethodToModuleSubClasses.num_module_elements / 1e9\n                num_params = InsertPostInitMethodToModuleSubClasses.num_module_parameters\n                logger.info(\n                    f\"finished initializing model - num_params = {num_params}, num_elems = {billion_elems:.2f}B\")\n\n        # Now that we cleaned up the metaclass injection, raise the exception.\n        if exc_type is not None:\n            return False\n\n    # To be implemented by inheriting classes\n    def _post_init_method(self, module):\n        pass\n\n    def _set_dtype(self, ds_config, dtype):\n        if ds_config is not None and dtype is None:\n            if ds_config.bfloat16_enabled and ds_config.fp16_enabled:\n                raise RuntimeError(\"bfloat16 and fp16 cannot be enabled at once\")\n\n            if ds_config.bfloat16_enabled:\n                self.dtype = torch.bfloat16\n            elif ds_config.fp16_enabled:\n                self.dtype = torch.half\n            else:\n                self.dtype = torch.float\n        else:\n            self.dtype = dtype or torch.float16 if get_accelerator().is_fp16_supported(\n            ) else torch.bfloat16 if get_accelerator().is_bf16_supported else torch.float32\n\n    def patch_init_and_builtins(self):\n\n        def apply_with_gather(orig_module_apply_fn: Callable) -> Callable:\n            \"\"\"many models make use of child modules like Linear or Embedding which\n            perform their own weight initialization in their __init__ methods,\n            but will then have more weight initialization in a parent module's __init__\n            method that modifies weights of child modules, which is typically done\n            using the Module.apply method.\n\n            since the Init context manager partitions child modules immediately after\n            they are initialized, without modifying apply we would entirely skip\n            any initialization done by parent modules.\n\n            to get around this issue, we wrap the function passed to Module.apply\n            so that the applied function is applied to child modules correctly.\n            \"\"\"\n\n            def get_wrapped_fn_to_apply(fn_to_apply: Callable) -> Callable:\n                if hasattr(fn_to_apply, \"wrapped\"):\n                    return fn_to_apply\n\n                @functools.wraps(fn_to_apply)\n                def wrapped_fn_to_apply(module_to_apply_fn_to: Module) -> None:\n                    \"\"\"gathers parameters before calling apply function. afterwards\n                    parameters are broadcasted to ensure consistency across all ranks\n                    then re-partitioned.\n\n                    takes the following steps:\n                    1. allgathers parameters for the current module being worked on\n                    2. calls the original function\n                    3. broadcasts root rank's parameters to the other ranks\n                    4. re-partitions the parameters\n                    \"\"\"\n\n                    # TODO Delay error checking for dangling partitioned parameters to post module init\n                    # raise RuntimeError(f\"not all parameters for {module_to_apply_fn_to.__class__.__name__}, \"\n                    #                    f\"were zero params, is it possible that the parameters were \"\n                    #                    f\"overwritten after they were initialized? \"\n                    #                    f\"params: {[p for p in module_to_apply_fn_to.parameters(recurse=False)]} \")\n\n                    params_to_apply_fn_to: Iterable[Parameter] = list(\n                        sorted([p for p in module_to_apply_fn_to.parameters(recurse=False) if is_zero_param(p)],\n                               key=lambda p: p.ds_id))\n\n                    for param in params_to_apply_fn_to:\n                        param.all_gather()\n\n                    fn_to_apply(module_to_apply_fn_to)\n\n                    for param in params_to_apply_fn_to:\n                        dist.broadcast(param.data, 0, group=param.ds_process_group)\n\n                    for param in params_to_apply_fn_to:\n                        param.partition(has_been_updated=True)\n\n                wrapped_fn_to_apply.wrapped = True\n\n                return wrapped_fn_to_apply\n\n            @functools.wraps(orig_module_apply_fn)\n            def wrapped_apply(module: Module, fn_to_apply: Callable) -> None:\n                orig_module_apply_fn(module, get_wrapped_fn_to_apply(fn_to_apply))\n\n            return wrapped_apply\n\n        def hook_for_skip_init(module):\n            # this function is intended for handling the logic of torch.nn.utils.skip_init\n            # skip_init:module_cls(*args, **kwargs).to_empty(device=final_device), where kwargs['device']='meta'\n            # the function call occurs between module_cls(*args, **kwargs) and to_empty(device=final_device).\n            def partition_after_empty_init(f):\n\n                @functools.wraps(f)\n                def wrapper(module, *args, **kwargs):\n                    _module = f(module, *args, **kwargs)\n                    # here is the post-hook for module.apply(empty_like...)\n                    # after module.apply(empty_like...), the module has completed its empty init on real device\n                    # since skip_init won't involve any computations or weight adjustments, we can directly utilize post_init\n                    self._post_init_method(_module)\n                    return _module\n\n                return wrapper\n\n            def post_wrapper_to_empty(f):\n                # append some wrapper restoration after to_empty() call\n                @functools.wraps(f)\n                def wrapper(*args, **kwargs):\n                    res = f(*args, **kwargs)\n                    # restore _apply hook\n                    for subclass in get_all_subclasses(torch.nn.modules.module.Module):\n                        _disable_class_apply(subclass)\n                    # self restore\n                    module.to_empty = f\n                    return res\n\n                return wrapper\n\n            def _enable_class_apply(cls):\n                cls._old_apply_of_skip_init_hook = cls._apply\n                cls._apply = partition_after_empty_init(cls._apply)\n\n            def _disable_class_apply(cls):\n                cls._apply = cls._old_apply_of_skip_init_hook\n\n            # add hooks for to_empty: apply_(empty_like)\n            for subclass in get_all_subclasses(torch.nn.modules.module.Module):\n                _enable_class_apply(subclass)\n\n            # add a restore hook when exiting skip_init\n            module.to_empty = post_wrapper_to_empty(module.to_empty)\n\n        def partition_after(f):\n\n            @functools.wraps(f)\n            def wrapper(module, *args, **kwargs):\n\n                # important logic: We want to run post_init only after child's __init__ is\n                # completed, and do nothing after __init__ of any of its parents and grandparents in\n                # the inheritance ancestry. This way the partitioning will need to happen only once\n                # when the whole object is ready to be partitioned and not before. This is because\n                # often the child module will need to tweak the weights - for example running a\n                # custom weights init function. So if a parent created the weights param, the child\n                # won't need to gather it in order to tweak it\n\n                print_rank_0(f'Before initializing {module.__class__.__name__}', force=False)\n\n                is_child_module = False\n                if not hasattr(module, \"_ds_child_entered\"):\n                    # child's __init__ was called, since parents all see the same object they can now skip post_init\n                    is_child_module = True\n                    setattr(module, \"_ds_child_entered\", True)\n\n                init_on_meta = 'device' in kwargs and kwargs['device'] == 'meta'\n                if init_on_meta:\n                    self.skip_init_depth += 1\n\n                f(module, *args, **kwargs)\n                if init_on_meta and self.skip_init_depth == 1:\n                    # check and handle the logic of empty_init\n                    hook_for_skip_init(module)\n                if is_child_module:\n                    # child's __init__ is done, now we can run a single post_init on the child object\n                    delattr(module, \"_ds_child_entered\")\n\n                    print_rank_0(f'Running post_init for {module.__class__.__name__}', force=False)\n                    if self.skip_init_depth == 0:\n                        self._post_init_method(module)\n\n                print_rank_0(f'After initializing followed by post init for {module.__class__.__name__}', force=False)\n                if init_on_meta:\n                    self.skip_init_depth -= 1\n\n            return wrapper\n\n        def _enable_class(cls):\n            cls._old_init = cls.__init__\n            cls.__init__ = partition_after(cls.__init__)\n\n        def _init_subclass(cls, **kwargs):\n            cls._old_init = cls.__init__\n            cls.__init__ = partition_after(cls.__init__)\n\n        # Replace .__init__() for all existing subclasses of torch.nn.Module recursively\n        for subclass in get_all_subclasses(torch.nn.modules.module.Module):\n            _enable_class(subclass)\n\n        # holding onto some methods so we can put them back the way they were in __exit__\n        torch.nn.modules.module.Module._old_init_subclass = torch.nn.modules.module.Module.__init_subclass__\n        torch.nn.modules.module.Module._old_apply = torch.nn.modules.module.Module.apply\n        torch.Tensor.__old_new__ = torch.Tensor.__new__\n\n        # Replace .__init__() for future subclasses of torch.nn.Module\n        torch.nn.modules.module.Module.__init_subclass__ = classmethod(_init_subclass)\n        if Init.override_module_apply:\n            torch.nn.modules.module.Module.apply = apply_with_gather(torch.nn.modules.module.Module._old_apply)\n\n        self._add_tensor_creation_wrappers()\n\n        if self.mem_efficient_linear:\n            print_rank_0(\n                \"nn.functional.linear has been overridden with a more memory efficient version. This will persist unless manually reset.\",\n                force=False)\n            self.linear_bk = torch.nn.functional.linear\n            torch.nn.functional.linear = zero3_linear_wrap\n\n            if self.quantized_initialization:\n                print_rank_0(\"nn.functional.linear has been overridden with quantized linear version.\", force=False)\n                torch.nn.functional.linear = wrap_quantized_functional(torch.nn.functional.linear)\n                torch.nn.functional.embedding = wrap_quantized_functional(torch.nn.functional.embedding)\n                for cls in WEIGHT_QUANTIZATION_LAYERS:\n                    cls._load_from_state_dict = wrap_load_from_state_dict(cls._load_from_state_dict)\n\n                logger.info(\"Enable Zero3 engine with INT4 quantization.\")\n\n        self.patched = True\n\n    def unpatch_init_and_builtins(self):\n        if self.patched:\n\n            def _disable_class(cls):\n                cls.__init__ = cls._old_init\n\n            for subclass in get_all_subclasses(torch.nn.modules.module.Module):\n                _disable_class(subclass)\n\n            # putting methods back the way we found them\n            torch.nn.modules.module.Module.__init_subclass__ = torch.nn.modules.module.Module._old_init_subclass\n            if Init.override_module_apply:\n                torch.nn.modules.module.Module.apply = torch.nn.modules.module.Module._old_apply\n\n            self._remove_tensor_creation_wrappers()\n\n            self.patched = False\n\n    def _add_tensor_creation_wrappers(self):\n        torch.Tensor.__new__ = get_new_tensor_fn_for_dtype(self.dtype)\n        torch.tensor = zero_wrapper_for_fp_tensor_constructor(_orig_torch_tensor, self.dtype)\n        torch.empty = zero_wrapper_for_fp_tensor_constructor(_orig_torch_empty, self.dtype)\n        torch.zeros = zero_wrapper_for_fp_tensor_constructor(_orig_torch_zeros, self.dtype)\n        torch.ones = zero_wrapper_for_fp_tensor_constructor(_orig_torch_ones, self.dtype)\n        torch.full = zero_wrapper_for_fp_tensor_constructor(_orig_torch_full, self.dtype)\n        torch.arange = zero_wrapper_for_fp_tensor_constructor(_orig_torch_arange, self.dtype)\n        torch.eye = zero_wrapper_for_fp_tensor_constructor(_orig_torch_eye, self.dtype)\n        torch.randn = zero_wrapper_for_fp_tensor_constructor(_orig_torch_randn, self.dtype)\n\n    def _remove_tensor_creation_wrappers(self):\n        torch.Tensor.__new__ = torch.Tensor.__old_new__\n        torch.tensor = _orig_torch_tensor\n        torch.empty = _orig_torch_empty\n        torch.zeros = _orig_torch_zeros\n        torch.ones = _orig_torch_ones\n        torch.full = _orig_torch_full\n        torch.arange = _orig_torch_arange\n        torch.eye = _orig_torch_eye\n        torch.randn = _orig_torch_randn\n\n\ndef shutdown_init_context():\n    \"\"\"\n    This function is used to initialize deepspeed engine inside the context of Init.\n    We need to remove the wrappers but keep the context.\n    \"\"\"\n    if top_level_context:\n        top_level_context.unpatch_init_and_builtins()\n\n\ndef restore_init_context():\n    \"\"\"\n    This function is used to restore the wrappers after deepspeed engine is initialized.\n    \"\"\"\n    if top_level_context:\n        top_level_context.patch_init_and_builtins()\n\n\nclass AllGatherHandle:\n\n    def __init__(self, handle, param: Parameter, quantization=None) -> None:\n        if param.ds_status != ZeroParamStatus.INFLIGHT:\n            raise RuntimeError(f\"expected param {param.ds_summary()} to be available\")\n\n        self.__handle = handle\n        self.__param = param\n        self.__quantization = quantization\n\n    def wait(self) -> None:\n        instrument_w_nvtx(self.__handle.wait)()\n        if self.__quantization:\n            instrument_w_nvtx(self.__quantization.quant_handle.wait)()\n            self.__param.data = self.__quantization.backend.dequantize(\n                self.__quantization.quantized_param, self.__quantization.scale_buffer).to(self.__param.device)\n        self.__param.ds_status = ZeroParamStatus.AVAILABLE\n\n\nclass AllGatherCoalescedHandle:\n\n    def __init__(\n        self,\n        allgather_handle,\n        params: List[Parameter],\n        partitions: List[Tensor],\n        world_size: int,\n        use_secondary_tensor=False,\n        quantization=None,\n    ) -> None:\n        self.allgather_handle = allgather_handle\n        self.params = params\n        self.partitions = partitions\n        self.world_size = world_size\n        self.use_secondary_tensor = use_secondary_tensor\n        self.complete = False\n        self.quantization = quantization\n\n        for param in self.params:\n            if param.ds_status != ZeroParamStatus.INFLIGHT:\n                raise RuntimeError(f\"expected param {param.ds_summary()} to not be available\")\n\n    @instrument_w_nvtx\n    def wait(self) -> None:\n        if self.complete:\n            return\n\n        instrument_w_nvtx(self.allgather_handle.wait)()\n\n        if self.quantization:\n            instrument_w_nvtx(self.quantization.quant_handle.wait)()\n            flat_tensor = self.quantization.backend.dequantize(\n                self.quantization.quantized_param, self.quantization.scale_buffer).to(self.params[0].device)\n\n            self.partitions: List[Parameter] = []\n            for i in range(self.world_size):\n                self.partitions.append(\n                    flat_tensor.narrow(0, self.quantization.partition_sz * i, self.quantization.partition_sz))\n\n        # split the single tensor out into individual tensors\n        param_offset = 0\n        for param in self.params:\n            assert param.ds_status == ZeroParamStatus.INFLIGHT, f\"expected param {param.ds_summary()} to be inflight\"\n            partitions: List[Tensor] = []\n            ds_tensor_numel = param.ds_tensor.ds_numel\n            if self.use_secondary_tensor:\n                ds_tensor_numel *= param.ds_secondary_tensor_num_of_groups\n            for rank in range(self.world_size):\n                param_start = rank * ds_tensor_numel\n                if param_start < param.ds_numel:\n                    part_to_copy = self.partitions[rank].narrow(0, param_offset,\n                                                                min(param.ds_numel - param_start, ds_tensor_numel))\n                    partitions.append(part_to_copy)\n            param.data = instrument_w_nvtx(torch.cat)(partitions).view(param.ds_shape)\n            param.ds_status = ZeroParamStatus.AVAILABLE\n\n            for part_to_copy in partitions:\n                if not get_accelerator().is_synchronized_device():\n                    part_to_copy.record_stream(get_accelerator().current_stream())\n\n            param_offset += ds_tensor_numel\n\n        self.complete = True\n\n\nclass MultipleAllGatherHandles:\n\n    def __init__(self, handles: List[AllGatherCoalescedHandle]):\n        self.handles = handles\n\n    def wait(self) -> None:\n        for handle in self.handles:\n            handle.wait()\n\n\nclass AllReduceCoalescedHandle:\n\n    def __init__(self, handle, params: List[Parameter]) -> None:\n        self.handle = handle\n        self.params = params\n        self.complete = False\n\n        for param in self.params:\n            if param.ds_status != ZeroParamStatus.INFLIGHT:\n                raise RuntimeError(f\"expected param {param.ds_summary()} to not be available\")\n\n    @instrument_w_nvtx\n    def wait(self) -> None:\n        if self.complete:\n            return\n\n        instrument_w_nvtx(self.handle.wait)()\n\n        for param in self.params:\n            assert param.ds_status == ZeroParamStatus.INFLIGHT, f\"expected param {param.ds_summary()} to be inflight\"\n            param.ds_status = ZeroParamStatus.AVAILABLE\n\n        self.complete = True\n\n\nclass QuantizationInfo:\n    # a placeholder object to store all quant related vars used in handles\n    def __init__(self) -> None:\n        self.quantized_param = None\n        self.backend = None\n        self.quant_handle = None\n        self.scale_buffer = None\n\n\nclass CUDAQuantizer:\n    async_flag = True\n    target_group_size = 8000  # the optimal size is 4k, so we set the target to be below 8k\n    group_size_cache = dict()\n    quantizer_cuda_module = None\n\n    def __init__(self) -> None:\n        if CUDAQuantizer.quantizer_cuda_module is None:\n            CUDAQuantizer.quantizer_cuda_module = deepspeed.ops.op_builder.QuantizerBuilder().load()\n\n    def quantize(self, param, groups=None):\n        if groups is None:\n            try:\n                groups = self.group_size_cache[param.numel()]\n            except KeyError:\n                groups = math.ceil(param.numel() / self.target_group_size)\n                while groups < param.numel():\n                    if param.numel() % (8 * groups) == 0:\n                        break\n                    groups += 1\n                while True:\n                    if param.numel() % (8 * groups * 2) == 0 and param.numel(\n                    ) / groups > self.target_group_size:  #hard limit of 16k group_size\n                        groups *= 2\n                    else:\n                        break\n                assert (\n                    param.numel() % (8 * groups) == 0\n                ), f\"Qantized weight requires the number of weights be a multiple of 8. Yet {param.numel()} cannot be divided by 8*{groups}\"\n                assert (param.numel() / groups < 16000), f\"{param.numel()} / {groups} is larger than 16k\"\n                assert param.numel(\n                ) > groups, f\"Adaptive grouping algorithm cannot find a group size for input tensor of size {param.numel()}\"\n                self.group_size_cache[param.numel()] = groups\n        return self.quantizer_cuda_module.quantize(param.to(get_accelerator().device_name()), groups, 8,\n                                                   self.quantizer_cuda_module.Symmetric)\n\n    def dequantize(self, quantized_param, scale):\n        return self.quantizer_cuda_module.dequantize(quantized_param, scale, scale.numel(), 8,\n                                                     self.quantizer_cuda_module.Symmetric)\n\n\ndef _no_gather_coalesced(params: Iterable[Parameter]) -> AllGatherCoalescedHandle:\n    for param in params:\n        if param.ds_status != ZeroParamStatus.NOT_AVAILABLE:\n            raise RuntimeError(f\"expect param.ds_status == ZeroParamStatus.NOT_AVAILABLE, got{param.ds_summary()}\")\n        param.ds_status = ZeroParamStatus.INFLIGHT\n\n    params = sorted(params, key=lambda p: p.ds_id)\n    if len(params) == 1:\n        param, = params\n        return NoGatherHandle(param)\n    return NoGatherCoalescedHandle(params)\n\n\n# Replaces all parameters in module with Scattered Parameters\nclass Init(InsertPostInitMethodToModuleSubClasses):\n    param_id = 0\n    param_persistence_threshold = get_config_default(DeepSpeedZeroConfig, \"param_persistence_threshold\")\n    model_persistence_threshold = get_config_default(DeepSpeedZeroConfig, \"model_persistence_threshold\")\n    num_persisted_parameters = 0\n    num_persisted_elements = 0\n    apply_param_persistence = False\n    override_module_apply = get_config_default(DeepSpeedZeroConfig, \"override_module_apply\")\n\n    def __init__(\n        self,\n        module=None,\n        data_parallel_group=None,\n        mem_efficient_linear=True,\n        remote_device=None,\n        pin_memory=False,\n        config_dict_or_path=None,\n        config=None,\n        enabled=True,\n        dtype=None,\n        mpu=None,\n        zero_param_parallel_group=None,\n        zero_quantized_weights=False,\n        zero_quantized_nontrainable_weights=False,\n        sequence_data_parallel_group=None,\n        param_swapper=None,\n    ):\n        \"\"\"A context to enable massive model construction for training with\n        ZeRO-3. Models are automatically partitioned (or, sharded) across the\n        system and converted to half precision.\n\n        Args:\n            module (``torch.nn.Module``, optional): If provided, partition the model as\n                if it was constructed in the context.\n            data_parallel_group (``deepspeed.comm`` process group, optional):\n                The group of processes to partition among. Defaults to all processes.\n            mem_efficient_linear (bool, optional): Replace\n                torch.nn.functional.linear with an implementation that allows\n                DeepSpeed to partition parameters. Defaults to ``True``.\n            remote_device (string, optional): The initial device to store model\n                weights e.g., ``cpu``, ``nvme``. Passing ``\"cpu\"`` will create the model in CPU\n                memory. The model may still be moved to GPU based on the\n                offload settings for training. Defaults to param offload device if a config is\n                defined, otherwise GPU.\n            pin_memory (bool, optional): Potentially increase performance by\n                using pinned memory for model weights. ``remote_device`` must be\n                ``\"cpu\"``. Defaults to pin_memory value in config, otherwise ``False``.\n            config_dict_or_path (dict or ``json file``, optional): If provided, provides configuration\n                for swapping fp16 params to NVMe.\n            config (dict or ``json file``, optional): Deprecated, use config_dict_or_path instead.\n            enabled (bool, optional): If ``False``, this context has no\n                effect. Defaults to ``True``.\n            dtype (``dtype``, optional): Can be used to change the data type of the parameters.\n                Supported options are ``torch.half`` and ``torch.float``. Defaults to ``None``\n            mpu (``object``, optional): A model parallelism unit object that implements get_{model,data}_parallel_{rank,group,world_size}.\n            zero_param_parallel_group(``object``, optional): Parallel (comm) group for dual partitioning of ZeRO params.\n            zero_quantized_weights (bool, optional): If ``True``, turn on quantized weights in all gather weights. Default is ``False``\n            zero_quantized_nontrainable_weights (bool, optional): If ``True``, nontrainable weights will be stored in quantized format. Default is ``False``\n            param_swapper (``deepspeed.runtime.swap_tensor.partitioned_param_swapper.AsyncPartitionedParameterSwapper``, optional): [Experimental] Use existing parameter swapper. Defaults to ``None``.\n                This argument will be removed in the near future.\n\n        This context accelerates model initialization and enables models that\n        are too large to allocate in their entirety in CPU memory. It has the\n        following effects:\n\n        #. allocates tensors to either GPU or CPU memory or NVMe\n        #. converts floating point tensors to half precision\n        #. immediately partitions tensors among the group of data-parallel devices\n        #. (*optional*) replaces ``torch.nn.functional.linear`` with a more\n           memory-efficient implementation\n\n        These modifications allow for models that exceed the size of local CPU/GPU\n        memory/NVMe, but fit within the total NVMe capacity (*i.e.*, aggregate CPU\n        or GPU memory or NVMe) across all nodes. Consider initializing a model with one\n        trillion parameters, whose weights occupy two terabytes (TB) in half\n        precision. The initial CPU allocation in full precision requires 4TB of\n        memory *per process*, and so a system with 8 GPUs per node would need 32TB of\n        CPU memory due to data-parallel redundancies. Instead, by immediately\n        partitioning tensors we remove the redundancies. The result is that\n        regardless of the number of GPUs, we still only require the original 4TB. This\n        allows for a linear increase in model size with the aggregate system memory.\n        For example, if a node has 1TB of memory and 8 GPUs, we could fit a trillion\n        parameter model with 4 nodes and 32 GPUs.\n\n        Important: If the fp16 weights of the model can't fit onto a single GPU memory\n        this feature must be used.\n\n        .. note::\n            Initializes ``deepspeed.comm`` if it has not already been done so.\n            See :meth:`deepspeed.init_distributed` for more information.\n\n        .. note::\n            Only applicable to training with ZeRO-3.\n\n        Examples\n        --------\n\n        #. Allocate a model and partition it among all processes:\n\n            .. code-block:: python\n\n                with deepspeed.zero.Init():\n                    model = MyLargeModel()\n\n\n        #. Allocate a model in pinned CPU memory and partition it among a subgroup of processes:\n\n            .. code-block:: python\n\n                with deepspeed.zero.Init(data_parallel_group=mpu.get_data_parallel_group(),\n                                         remote_device=\"cpu\",\n                                         pin_memory=True):\n                    model = MyLargeModel()\n\n\n        #. Partition an already-allocated model in CPU memory:\n\n            .. code-block:: python\n\n                model = deepspeed.zero.Init(module=model)\n        \"\"\"\n        if config is not None:\n            config_dict_or_path = config\n            logger.warning(\n                f'zero.Init: the `config` argument is deprecated. Please use `config_dict_or_path` instead.')\n        _ds_config = deepspeed.runtime.config.DeepSpeedConfig(config_dict_or_path,\n                                                              mpu) if config_dict_or_path is not None else None\n        if _ds_config is not None:\n            mem_efficient_linear = _ds_config.zero_config.memory_efficient_linear\n\n        super().__init__(enabled=enabled, mem_efficient_linear=mem_efficient_linear, ds_config=_ds_config, dtype=dtype)\n        if not dist.is_initialized():\n            init_distributed()\n            assert dist.is_initialized(), \"Parameters cannot be scattered without initializing deepspeed.comm\"\n\n        if data_parallel_group is None and sequence_data_parallel_group is None:\n            self.ds_process_group = dist.get_world_group()\n        elif sequence_data_parallel_group is not None:\n            self.ds_process_group = sequence_data_parallel_group\n        elif data_parallel_group is not None:\n            self.ds_process_group = data_parallel_group\n        else:  # both given\n            raise ValueError(\n                \"Both 'data_parallel_group' and 'sequence_data_parallel_group' were specified. Please provide only one of these arguments.\"\n            )\n\n        self.rank = dist.get_rank(group=self.ds_process_group)\n        self.dp_world_size = dist.get_world_size(group=self.ds_process_group)\n\n        self.zero_param_process_group = zero_param_parallel_group\n        if _ds_config is not None and _ds_config.zero_config.zero_hpz_partition_size > 1 and self.zero_param_process_group is None:\n            groups._create_zero_param_parallel_group(_ds_config.zero_config.zero_hpz_partition_size)\n            self.zero_param_process_group = groups._get_zero_param_intra_parallel_group()\n\n        self.num_ranks_in_param_group = self.dp_world_size\n        self.rank_in_group = self.rank\n        self.num_param_groups = 1\n\n        if self.zero_param_process_group is not None:\n            self.num_ranks_in_param_group = groups._get_zero_param_intra_parallel_group_world_size()\n            self.num_param_groups = int(self.dp_world_size / self.num_ranks_in_param_group)\n            self.rank_in_group = groups._get_zero_param_intra_parallel_rank_in_mygroup()\n            print_rank_0(f\"hpZeRO group size: {self.num_ranks_in_param_group}\", force=True)\n\n            logger.debug(\n                \"hpZeRO partition parameter my rank in world {} my rank in group {} ranks in my param partition group: {} \"\n                .format(self.rank, self.rank_in_group, groups._get_zero_param_intra_parallel_group_ranks()))\n\n        # Local device is the device where the parameters are consumed, must be default device.\n        # It is the device where parameters are fully instantiated using allgather\n        self.local_device = torch.device(get_accelerator().device_name(os.environ[\"LOCAL_RANK\"]))\n        get_accelerator().set_device(self.local_device)\n\n        self.quantized_weights = zero_quantized_weights\n        if _ds_config is not None and _ds_config.zero_config.zero_quantized_weights and not self.quantized_weights:\n            self.quantized_weights = _ds_config.zero_config.zero_quantized_weights\n        self.quantized_nontrainable_weights = zero_quantized_nontrainable_weights\n        if _ds_config is not None and _ds_config.zero_config.zero_quantized_nontrainable_weights and not self.quantized_nontrainable_weights:\n            self.quantized_nontrainable_weights = _ds_config.zero_config.zero_quantized_nontrainable_weights\n\n        self.module = module\n        if (self.quantized_weights or self.quantized_nontrainable_weights):\n            self.quantizer_module = CUDAQuantizer()\n            print_rank_0(f'Using quantizer for weights: {self.quantizer_module.__class__.__name__}', force=True)\n\n        if _ds_config is not None:\n            Init.override_module_apply = _ds_config.zero_config.override_module_apply\n\n            if _ds_config.zero_config.offload_param is not None:\n                remote_device = _ds_config.zero_config.offload_param.device\n                pin_memory = _ds_config.zero_config.offload_param.pin_memory\n\n        self._validate_remote_device(remote_device, _ds_config)\n\n        # Remote device is the device where parameter partitions are stored\n        # It can be same as local_device or it could be CPU or NVMe.\n        self.remote_device = self.local_device if remote_device in [None, OffloadDeviceEnum.none] else remote_device\n        self.pin_memory = pin_memory if (self.remote_device in [OffloadDeviceEnum.cpu, OffloadDeviceEnum.nvme\n                                                                ]) else False\n\n        # Enable fp16 param swapping to NVMe\n        if self.remote_device == OffloadDeviceEnum.nvme:\n            self.param_swapper = param_swapper or AsyncPartitionedParameterSwapper(_ds_config, self.dtype)\n        else:\n            self.param_swapper = None\n\n        # If we are provided an already-allocated module to prepare.\n        if module is not None:\n            assert isinstance(module, torch.nn.Module)\n            self._convert_to_zero_parameters(module.parameters(recurse=True))\n\n        self.use_all_gather_into_tensor = dist.has_all_gather_into_tensor()\n        if not self.use_all_gather_into_tensor:\n            logger.info(f\"all_gather_into_tensor API is not available in torch {torch.__version__}\")\n\n        self.use_all_reduce_for_fetch_params = get_config_default(DeepSpeedZeroConfig,\n                                                                  \"use_all_reduce_for_fetch_params\")\n        if _ds_config is not None:\n            self.use_all_reduce_for_fetch_params = _ds_config.zero_config.use_all_reduce_for_fetch_params\n\n    def _update_persist_config(self, ds_config):\n        Init.apply_param_persistence = True\n        Init.param_persistence_threshold = ds_config.zero_config.param_persistence_threshold\n        Init.model_persistence_threshold = ds_config.zero_config.model_persistence_threshold // self.num_partitions\n\n    def _zero_init_param(self, param):\n        self._convert_to_deepspeed_param(param)\n        if dist.get_world_group() == self.get_dp_process_group():\n            dist.broadcast(param.data, 0, self.get_dp_process_group())\n        else:\n            dist.broadcast(param.data, dist.get_global_rank(self.get_dp_process_group(), 0),\n                           self.get_dp_process_group())\n        param.partition()\n\n    def _convert_to_zero_parameters(self, param_list):\n        for param in param_list:\n            if is_zero_param(param):\n                continue\n\n            param.data = param.data.to(self.local_device)\n            self._zero_init_param(param)\n\n    def _validate_remote_device(self, remote_device, ds_config):\n        if ds_config is not None:\n            if remote_device in [None, OffloadDeviceEnum.cpu]:\n                if ds_config.zero_config.offload_param is not None:\n                    offload_param_device = ds_config.zero_config.offload_param.device\n                    assert offload_param_device != OffloadDeviceEnum.nvme, \\\n                        f\"'device' in DeepSpeed Config cannot be {offload_param_device} if remote device is {remote_device}.\"\n\n            if remote_device == OffloadDeviceEnum.nvme:\n                assert ds_config.zero_config.offload_param is not None, \\\n                f'\"offload_param\" must be defined in DeepSpeed Config if remote device is {OffloadDeviceEnum.nvme}.'\n\n                assert ds_config.zero_config.offload_param.nvme_path is not None, \\\n                f'\"nvme_path\" in DeepSpeed Config cannot be None if remote device is {OffloadDeviceEnum.nvme}'\n\n    def _post_init_method(self, module):\n        #see_memory_usage(f\"Before converting params in {module.__class__.__name__}\", force=False)\n        print_rank_0(f'Converting Params in {module.__class__.__name__}', force=False)\n        see_memory_usage(f\"Before converting and partitioning params in {module.__class__.__name__}\", force=False)\n\n        for name, param in module.named_parameters(recurse=False):\n            print_rank_0(f'Analyzing param {name} in {module.__class__.__name__}', force=False)\n            InsertPostInitMethodToModuleSubClasses.num_module_parameters += 1\n            InsertPostInitMethodToModuleSubClasses.num_module_elements += param.numel()\n            if not is_zero_param(param):\n                if not get_accelerator().on_accelerator(param):\n                    param.data = param.data.to(self.local_device)\n\n                if name == 'weight' and self.quantized_initialization and type(module) in WEIGHT_QUANTIZATION_LAYERS:\n                    _quantize_param(param, self.quantized_initialization)\n\n                self._zero_init_param(param)\n                print_rank_0(\n                    f\"Partitioning param {debug_param2name_id_shape(param)} module={debug_module2name(module)}\")\n\n        see_memory_usage(\n            f\"Param count {InsertPostInitMethodToModuleSubClasses.num_module_elements}. After converting and partitioning params in {module.__class__.__name__}\",\n            force=False)\n\n    def _convert_to_deepspeed_param(self, param):\n\n        # Partitioned, Normal, Remote\n        param.ds_param_type = ZeroParamType.PARTITIONED\n\n        # Replicated vs Partitioned vs Inflight\n        param.ds_status = ZeroParamStatus.AVAILABLE\n\n        # Stores the shape of the original tensor\n        param.ds_shape = param.shape\n\n        # Stores the number of elements in the original parameter without padding\n        param.ds_numel = param.numel()\n\n        # Stores the partitioned copy of the tensor\n        param.ds_tensor = None\n\n        # Keeps track of how many active sub-modules need this param at any given point in time\n        param.ds_active_sub_modules = set()\n\n        # If this flag is true, then the parameters are replicated throughput training\n        # And only partitioned before the step\n        if Init.apply_param_persistence and param.ds_numel <= Init.param_persistence_threshold and Init.num_persisted_elements + param.ds_numel <= Init.model_persistence_threshold:\n            param.ds_persist = True\n            Init.num_persisted_parameters += 1\n            Init.num_persisted_elements += param.ds_numel\n        else:\n            param.ds_persist = False\n\n        param.is_external_param = False\n\n        # The group that the parameter is scattered across.\n        param.ds_process_group = self.ds_process_group\n\n        # Stores the secondary partitioned copy of the tensor\n        param.ds_secondary_tensor = None\n\n        #Process group for secondary partition all (group) gather\n        param.ds_zero_param_process_group = self.zero_param_process_group\n        param.ds_secondary_tensor_group_size = self.num_ranks_in_param_group\n        param.ds_secondary_tensor_num_of_groups = self.num_param_groups\n\n        # This is set to the Async Param swapper if remote device is nvme\n        # else this is set to None\n        param.nvme_swapper = self.param_swapper\n\n        # DeepSpeed Param ID\n        param.ds_id = Init.param_id\n        Init.param_id += 1\n\n        def all_gather(param_list=None, async_op=False, hierarchy=0):\n            cls = param\n            if param_list is None:\n                param_list = [cls]\n            return self._all_gather(param_list, async_op=async_op, hierarchy=hierarchy)\n\n        def _all_gather_dtype(dtype, params, world_size, rank_in_group, ds_process_group):\n            partition_sz = sum(p.ds_tensor.ds_numel for p in params)\n\n            use_secondary_tensor = params[0].ds_secondary_tensor is not None\n\n            if use_secondary_tensor:\n                partition_sz = sum(p.ds_tensor.ds_numel * p.ds_secondary_tensor_num_of_groups for p in params)\n\n            flat_tensor = torch.empty(partition_sz * world_size,\n                                      dtype=dtype,\n                                      device=get_accelerator().current_device_name(),\n                                      requires_grad=False)\n\n            partitions: List[Parameter] = []\n            for i in range(world_size):\n                partitions.append(flat_tensor.narrow(0, partition_sz * i, partition_sz))\n\n            if use_secondary_tensor:\n                instrument_w_nvtx(\n                    torch.cat)([p.ds_secondary_tensor.to(get_accelerator().current_device_name()) for p in params],\n                               out=partitions[rank_in_group])\n            else:\n                instrument_w_nvtx(torch.cat)([p.ds_tensor.to(get_accelerator().current_device_name()) for p in params],\n                                             out=partitions[rank_in_group])\n            handle = _dist_allgather_fn(partitions[rank_in_group], flat_tensor, ds_process_group)\n            #Fix get_partition_dp_group(params[0]))\n\n            return AllGatherCoalescedHandle(\n                allgather_handle=handle,\n                params=params,\n                partitions=partitions,\n                world_size=world_size,\n                use_secondary_tensor=use_secondary_tensor,\n            )\n\n        @instrument_w_nvtx\n        def all_gather_coalesced(params: Iterable[Parameter],\n                                 safe_mode: bool = False,\n                                 quantize: bool = False) -> AllGatherCoalescedHandle:\n\n            # fetches from nvme if the partition is not available and in nvme\n            self._ensure_availability_of_partitioned_params(params)\n\n            if self.num_partitions == 1:\n                return _no_gather_coalesced(params)\n\n            for param in params:\n                if param.ds_status != ZeroParamStatus.NOT_AVAILABLE:\n                    raise RuntimeError(param.ds_summary())\n                param.ds_status = ZeroParamStatus.INFLIGHT\n\n            #use appropriate all gather process group\n            ds_process_group = self.ds_process_group\n            rank_in_group = self.rank\n            world_size = self.dp_world_size\n            use_secondary_tensor = params[0].ds_secondary_tensor is not None\n            if self.zero_param_process_group and use_secondary_tensor:\n                ds_process_group = self.zero_param_process_group  #intragroup\n                rank_in_group = self.rank_in_group\n                world_size = self.num_ranks_in_param_group\n\n            #pprint(dir(ds_process_group))\n            # ensure that each rank has params in same order. the allgather\n            # is done by flattening the parameter list into a single tensor that\n            # can be allgathered in a single call - this means that if each rank\n            # gives a list of the same parameters in a different order we will\n            # silently get incorrect parameter values, and have very difficult\n            # to debug correctness issues.\n            params = sorted(params, key=lambda p: p.ds_id)\n\n            if logger.isEnabledFor(logging.DEBUG):\n                debug_rank0(f\"-allgather_coalesced: {[p.ds_id for p in params]}\")\n\n            if safe_mode:\n                # ensure that same list (with same ordering) of parameters are\n                # being allgathered across all ranks, otherwise could mix\n                # data between tensors.\n                assert_ints_same_as_other_ranks([p.ds_id for p in params])\n                # ensure that tensors from each rank agree on the same ds_numel\n                # otherwise could mix data between tensors.\n                assert_ints_same_as_other_ranks([p.ds_tensor.ds_numel for p in params])\n\n            if len(params) == 1:\n                # have an opportunity to avoid some intermediate memory allocations\n                param = params[0]\n                buffer_size = math.ceil(param.ds_numel / world_size) * world_size\n                if use_secondary_tensor:\n                    buffer_size = param.ds_secondary_tensor.shape[0] * world_size  #make sure out is appropriately sized\n\n                param_ds_tensor = param.ds_secondary_tensor if use_secondary_tensor else param.ds_tensor\n                param_buffer = torch.empty(\n                    buffer_size,\n                    dtype=param_ds_tensor.dtype if not quantize else torch.int8,\n                    device=get_accelerator().current_device_name(),\n                    requires_grad=False,\n                )\n                if not quantize:\n                    handles = _dist_allgather_fn(\n                        param_ds_tensor.to(get_accelerator().current_device_name()),\n                        param_buffer,\n                        ds_process_group,\n                    )\n                    param.data = param_buffer.narrow(0, 0, param.ds_numel).view(param.ds_shape).to(param.device)\n                    return AllGatherHandle(handles, param)\n                else:\n                    if hasattr(param_ds_tensor, \"ds_quant_scale\"):\n                        scales = param_ds_tensor.ds_quant_scale\n                        quantized_param = param_ds_tensor.data\n                    else:\n                        quantized_param, scales = self.quantizer_module.quantize(param_ds_tensor)\n                    handle = _dist_allgather_fn(quantized_param.to(get_accelerator().current_device_name()),\n                                                param_buffer, ds_process_group)\n\n                    quant_scale_buffer = torch.empty(\n                        scales.numel() * world_size,\n                        dtype=scales.dtype,\n                        device=get_accelerator().current_device_name(),\n                        requires_grad=False,\n                    )\n                    quant_handle = _dist_allgather_fn(scales.to(get_accelerator().current_device_name()),\n                                                      quant_scale_buffer, ds_process_group)\n                    quant_info = QuantizationInfo()\n                    quant_info.quantized_param = param_buffer.narrow(0, 0, param.ds_numel).view(param.ds_shape).to(\n                        param.device)\n                    quant_info.backend = self.quantizer_module\n                    quant_info.quant_handle = quant_handle\n                    quant_info.scale_buffer = quant_scale_buffer\n                    return AllGatherHandle(handle, param, quantization=quant_info)\n\n            else:\n                if self.use_all_reduce_for_fetch_params and not quantize and not use_secondary_tensor:\n                    # Use all_reduce instead of all_gather to fetch the module params\n                    flat_buffer_size = sum(p.ds_numel_aligned for p in params)\n                    flat_tensor = torch.zeros(flat_buffer_size,\n                                              dtype=get_only_unique_item(p.ds_tensor.dtype for p in params),\n                                              device=get_accelerator().current_device_name(),\n                                              requires_grad=False)\n                    start_param = 0\n                    for param in params:\n                        param.data = flat_tensor.narrow(0, start_param, param.ds_numel).view(param.ds_shape)\n                        start = start_param + param.ds_tensor.ds_numel * self.get_partition_rank()\n                        flat_tensor.narrow(0, start, param.ds_tensor.ds_numel).copy_(param.ds_tensor)\n\n                        start_param += param.ds_numel\n\n                    handle = dist.all_reduce(flat_tensor, group=ds_process_group, async_op=True)\n\n                    return AllReduceCoalescedHandle(handle=handle, params=params)\n                else:\n                    if not quantize:\n                        dtype_params = defaultdict(list)\n                        for p in params:\n                            dtype_params[p.ds_tensor.dtype].append(p)\n                        handles = []\n                        for dtype, params in dtype_params.items():\n                            handles.append(\n                                _all_gather_dtype(dtype, params, world_size, rank_in_group, ds_process_group))\n\n                        return MultipleAllGatherHandles(handles)\n\n                    else:\n                        partition_sz = sum(p.ds_tensor.ds_numel for p in params)\n\n                        if use_secondary_tensor:\n                            partition_sz = sum(p.ds_tensor.ds_numel * p.ds_secondary_tensor_num_of_groups\n                                               for p in params)\n\n                        flat_tensor = torch.empty(partition_sz * world_size,\n                                                  dtype=torch.int8,\n                                                  device=get_accelerator().current_device_name(),\n                                                  requires_grad=False)\n\n                        if use_secondary_tensor:\n                            if hasattr(params[0].ds_secondary_tensor, \"ds_quant_scale\"):\n                                quantized_param = instrument_w_nvtx(torch.cat)([\n                                    p.ds_secondary_tensor.data.to(get_accelerator().current_device_name())\n                                    for p in params\n                                ])\n                                scales = instrument_w_nvtx(torch.cat)([\n                                    p.ds_secondary_tensor.ds_quant_scale.to(get_accelerator().current_device_name())\n                                    for p in params\n                                ])\n                            else:\n                                quantized_param, scales = self.quantizer_module.quantize(\n                                    instrument_w_nvtx(torch.cat)([\n                                        p.ds_secondary_tensor.to(get_accelerator().current_device_name())\n                                        for p in params\n                                    ]))\n                        else:\n                            if hasattr(params[0].ds_tensor, \"ds_quant_scale\"):\n                                quantized_param = instrument_w_nvtx(torch.cat)(\n                                    [p.ds_tensor.data.to(get_accelerator().current_device_name()) for p in params])\n                                scales = instrument_w_nvtx(torch.cat)([\n                                    p.ds_tensor.ds_quant_scale.to(get_accelerator().current_device_name())\n                                    for p in params\n                                ])\n                            else:\n                                quantized_param, scales = self.quantizer_module.quantize(\n                                    instrument_w_nvtx(torch.cat)(\n                                        [p.ds_tensor.to(get_accelerator().current_device_name()) for p in params]))\n                        quant_scale_buffer = torch.empty(\n                            scales.numel() * world_size,\n                            dtype=torch.float32,\n                            device=get_accelerator().current_device_name(),\n                            requires_grad=False,\n                        )\n                        handle = _dist_allgather_fn(quantized_param, flat_tensor, ds_process_group)\n                        quant_handle = _dist_allgather_fn(scales, quant_scale_buffer, ds_process_group)\n                        quant_info = QuantizationInfo()\n                        quant_info.quantized_param = flat_tensor\n                        quant_info.backend = self.quantizer_module\n                        quant_info.quant_handle = quant_handle\n                        quant_info.scale_buffer = quant_scale_buffer\n                        quant_info.partition_sz = partition_sz\n                        quant_info.world_size = world_size\n                        return AllGatherCoalescedHandle(\n                            allgather_handle=handle,\n                            params=params,\n                            partitions=None,\n                            world_size=world_size,\n                            use_secondary_tensor=use_secondary_tensor,\n                            quantization=quant_info,\n                        )\n\n        def partition(param_list=None, hierarchy=0, has_been_updated=False):\n            cls = param\n            print_rank_0(f\"{'--'*hierarchy}----Partitioning param {debug_param2name_id_shape_device(cls)}\",\n                         force=False)\n            if param_list is None:\n                param_list = [cls]\n            self._partition(param_list, has_been_updated=has_been_updated)\n\n        def reduce_gradients_at_owner(param_list=None, hierarchy=0):\n            cls = param\n            if param_list is None:\n                param_list = [cls]\n            print_rank_0(\n                f\"{'--'*hierarchy}----Reducing Gradients for param with ids {[param.ds_id for param in param_list]} to owner\"\n            )\n            self._reduce_scatter_gradients(param_list)\n\n        def partition_gradients(param_list=None, partition_buffers=None, hierarchy=0, accumulate=False):\n            cls = param\n            print_rank_0(\n                f\"{'--'*hierarchy}----Partitioning param gradient with id {debug_param2name_id_shape_device(cls)}\")\n            if param_list is None:\n                param_list = [cls]\n                if isinstance(partition_buffers, torch.Tensor):\n                    partition_buffers = [partition_buffers]\n\n            self._partition_gradients(param_list, partition_buffers=partition_buffers, accumulate=accumulate)\n\n        def aligned_size():\n            return self._aligned_size(param)\n\n        def padding_size():\n            return self._padding_size(param)\n\n        def partition_numel():\n            return self._partition_numel(param)\n\n        def item_override():\n            param.all_gather()\n            return param._orig_item()\n\n        def ds_summary(slf: torch.Tensor, use_debug_name: bool = False) -> dict:\n            return {\n                \"id\": debug_param2name_id(slf) if use_debug_name else slf.ds_id,\n                \"status\": slf.ds_status.name,\n                \"numel\": slf.numel(),\n                \"ds_numel\": slf.ds_numel,\n                \"shape\": tuple(slf.shape),\n                \"ds_shape\": tuple(slf.ds_shape),\n                \"requires_grad\": slf.requires_grad,\n                \"grad_shape\": tuple(slf.grad.shape) if slf.grad is not None else None,\n                \"persist\": slf.ds_persist,\n                \"active_sub_modules\": slf.ds_active_sub_modules,\n                \"ds_tensor.shape\": slf.ds_tensor.shape if slf.ds_tensor is not None else None\n            }\n\n        def convert_to_zero_parameters(param_list):\n            self._convert_to_zero_parameters(param_list)\n\n        def allgather_before(func: Callable) -> Callable:\n\n            def wrapped(*args, **kwargs):\n                param.all_gather()\n                return func(*args, **kwargs)\n\n            return wrapped\n\n        # Collectives for gathering and partitioning parameters\n        param.all_gather = all_gather\n        param.all_gather_coalesced = all_gather_coalesced\n        param.partition = partition\n\n        # Collective for averaging gradients\n        param.reduce_gradients_at_owner = reduce_gradients_at_owner\n        param.partition_gradients = partition_gradients\n\n        # Partitioning size utilities\n        param.aligned_size = aligned_size\n        param.padding_size = padding_size\n        param.partition_numel = partition_numel\n        param.ds_summary = types.MethodType(ds_summary, param)\n\n        param.item = allgather_before(param.item)\n\n        param.convert_to_zero_parameters = convert_to_zero_parameters\n\n    def _aligned_size(self, param):\n        return param.ds_numel + self._padding_size(param)\n\n    def _padding_size(self, param):\n        remainder = param.ds_numel % self.num_partitions\n        return (self.num_partitions - remainder) if remainder else 0\n\n    def _partition_numel(self, param):\n        return param.ds_tensor.ds_numel\n\n    def _ensure_availability_of_partitioned_params(self, params):\n        swap_in_list = []\n        swap_in_flight = []\n        for param in params:\n            if param.ds_tensor.status == PartitionedParamStatus.NOT_AVAILABLE:\n                assert param.ds_tensor.final_location == OffloadDeviceEnum.nvme and param.ds_status == ZeroParamStatus.NOT_AVAILABLE\n                swap_in_list.append(param)\n            if param.ds_tensor.status == PartitionedParamStatus.INFLIGHT:\n                assert param.ds_tensor.final_location == OffloadDeviceEnum.nvme and param.ds_status == ZeroParamStatus.NOT_AVAILABLE\n                swap_in_flight.append(param)\n        if len(swap_in_list) > 0:\n            swap_in_list[0].nvme_swapper.swap_in(swap_in_list, async_op=False)\n        elif len(swap_in_flight) > 0:\n            swap_in_flight[0].nvme_swapper.synchronize_reads()\n\n    @instrument_w_nvtx\n    def _all_gather(self, param_list, async_op=False, hierarchy=None):\n\n        # fetches from nvme if the partition is not available and in nvme\n        self._ensure_availability_of_partitioned_params(param_list)\n\n        handles = []\n        all_gather_list = []\n        for param in param_list:\n            if param.ds_status == ZeroParamStatus.NOT_AVAILABLE:\n                if async_op:\n                    handle = self._allgather_param(param, async_op=async_op, hierarchy=hierarchy)\n                    param.ds_status = ZeroParamStatus.INFLIGHT  # if async_op else ZeroParamStatus.AVAILABLE\n                    handles.append(handle)\n                else:\n                    all_gather_list.append(param)\n        # note: param_list may contain params that are already in flight / aviailable. So we need to use all_gather_list\n        if not async_op:\n            if len(all_gather_list) == 1:\n                ret_value = self._allgather_params(all_gather_list, hierarchy=hierarchy)\n            else:\n                all_gather_quantize_list = []\n                all_gather_nonquantize_list = []\n                for param in all_gather_list:\n                    if hasattr(param.ds_tensor,\n                               \"ds_quant_scale\") or (hasattr(param, \"ds_secondary_tensor\")\n                                                     and hasattr(param.ds_secondary_tensor, \"ds_quant_scale\")):\n                        all_gather_quantize_list.append(param)\n                    else:\n                        all_gather_nonquantize_list.append(param)\n                # _allgather_params_coalesced always return None\n                self._allgather_params_coalesced(all_gather_nonquantize_list, hierarchy, quantize=False)\n                self._allgather_params_coalesced(all_gather_quantize_list, hierarchy, quantize=True)\n            for param in all_gather_list:\n                param.ds_status = ZeroParamStatus.AVAILABLE\n            return None\n\n        return handles\n\n    def _partition(self, param_list, force=False, has_been_updated=False):\n        for param in param_list:\n            print_rank_0(f\"Before Partitioning Param {param.ds_id}\", force=False)\n            if self.zero_param_process_group is not None:\n                self._partition_param_sec(param)\n            self._partition_param(param, has_been_updated=has_been_updated)\n\n            param.ds_status = ZeroParamStatus.NOT_AVAILABLE\n            # if param.ds_tensor is not None:\n            #    assert id(param.data) == id(param.ds_tensor.data), \\\n            #    \"After the parameters are initially partitioned, make sure we are not recreating the partition.\"\n            #print_rank_0(f\"After Partitioning Param {param.ds_id} {param.ds_tensor.size()} {param.ds_tensor}\",force=False)\n    @instrument_w_nvtx\n    def _partition_param(self, param, buffer=None, has_been_updated=False):\n        assert param.ds_status is not ZeroParamStatus.INFLIGHT, f\" {param} Cannot partition a param in flight\"\n        global reuse_buffers\n        print_rank_0(f\"Param id {param.ds_id} status is {param.ds_status}\", force=False)\n        if param.ds_status is ZeroParamStatus.AVAILABLE:\n            print_rank_0(f\"Partitioning param id {param.ds_id} reuse buffers {reuse_buffers}\", force=False)\n            # if reuse_buffers and False:\n            #     numel = buffer.numel()\n            #     buffer = param.data.view(-1)\n            #     print_rank_0(\n            #         \"Returning buffer for param {param.ds_id} with numel {param.ds_numel} to empty buffers\",\n            #         force=False)\n            #     if numel in empty_buffers:\n            #         empty_buffers[numel].append(buffer)\n\n            # if deepspeed.comm.get_rank():\n            #    print(f\"Releasing {param.data.numel()}\")\n\n            if param.ds_tensor is not None and not has_been_updated:  ##param already partitioned\n\n                #print_rank_0(f\"Param  {param.ds_id} pri {param.ds_tensor.size()}  loc? {param.ds_tensor.final_location}\", force=True)\n                #param.data = param.ds_tensor.data\n\n                see_memory_usage(f'Before partitioning param {param.ds_id} {param.shape}', force=False)\n                # param.data does not store anything meaningful in partitioned state\n                free_param(param)\n                see_memory_usage(f'After partitioning param {param.ds_id} {param.shape}', force=False)\n\n                if param.ds_tensor.final_location == OffloadDeviceEnum.nvme:\n                    print_rank_0(f\"Param {param.ds_id} partition released since it exists in nvme\", force=False)\n                    param.nvme_swapper.remove_partition_and_release_buffers([param])\n                    print_rank_0(\n                        f\"after swap Param {param.ds_id} {param.ds_tensor.shape} partition released since it exists in nvme\",\n                        force=False)\n\n                return\n\n            tensor_size = self._aligned_size(param)\n            partition_size = tensor_size // self.num_partitions\n            if param.ds_tensor is None:\n                final_location = None\n                if self.remote_device == OffloadDeviceEnum.nvme and self.param_swapper.swappable_tensor(\n                        numel=partition_size):\n                    final_location = OffloadDeviceEnum.nvme\n                    buffer = self.param_swapper.get_buffer(param, partition_size)\n                    partitioned_tensor = torch.empty(0, dtype=param.dtype, device=buffer.device)\n                    partitioned_tensor.data = buffer.data\n                    print_rank_0(f\"ID {param.ds_id} Initializing partition for the first time for nvme offload.\")\n\n                else:\n                    if param.ds_persist:\n                        device = self.local_device\n                    elif self.remote_device == OffloadDeviceEnum.nvme:\n                        device = OffloadDeviceEnum.cpu\n                    else:\n                        device = self.remote_device\n\n                    partitioned_tensor = torch.empty(partition_size, dtype=param.dtype, device=device)\n                    # quantize the tensor if it's not trainable\n                    if not param.requires_grad and self.quantized_nontrainable_weights:\n                        partitioned_tensor, partitioned_tensor.ds_quant_scale = self.quantizer_module.quantize(\n                            partitioned_tensor)\n\n                    if device == OffloadDeviceEnum.cpu and self.pin_memory:\n                        partitioned_tensor = get_accelerator().pin_memory(partitioned_tensor)\n\n                partitioned_tensor.requires_grad = False\n                param.ds_tensor = partitioned_tensor\n                param.ds_tensor.ds_numel = partition_size\n                param.ds_tensor.status = PartitionedParamStatus.AVAILABLE\n                param.ds_tensor.final_location = final_location\n                param.ds_numel_aligned = tensor_size\n\n            start = partition_size * self.get_partition_rank()\n            end = start + partition_size\n\n            one_dim_param = param.contiguous().view(-1)\n\n            if start < param.ds_numel and end <= param.ds_numel:\n                src_tensor = one_dim_param.narrow(0, start, partition_size)\n\n                with torch.no_grad():\n                    # make sure param.ds_tensor requires_grad always be false,\n                    # otherwise, torch tracer will complain.\n                    param.ds_tensor.copy_(src_tensor)\n\n                #partitioned_tensor = src_tensor.clone().detach().to(self.remote_device)\n\n            else:\n                # partitioned_tensor = torch.zeros(partition_size,\n                #                                  dtype=param.dtype,\n                #                                  device=self.remote_device )\n\n                if start < param.ds_numel:\n                    elems_to_copy = param.ds_numel - start\n                    with torch.no_grad():\n                        # make sure param.ds_tensor requires_grad always be false,\n                        # otherwise, torch tracer will complain.\n                        param.ds_tensor.narrow(0, 0,\n                                               elems_to_copy).copy_(one_dim_param.narrow(0, start, elems_to_copy))\n\n            #print(f\"Remote device {self.remote_device}\")\n\n            #param.ds_tensor = partitioned_tensor\n\n            #param.data = param.ds_tensor.data\n\n            # param.data does not store anything meaningful in partitioned state\n\n            see_memory_usage(f'Before partitioning param {param.ds_id} {param.shape}', force=False)\n            free_param(param)\n            see_memory_usage(f'After partitioning param {param.ds_id} {param.shape}', force=False)\n\n            if param.ds_tensor.final_location == OffloadDeviceEnum.nvme:\n                self.param_swapper.swap_out_and_release([param])\n                print_rank_0(f\"ID {param.ds_id} Offloaded to nvme offload and buffers released.\")\n                see_memory_usage(f\"ID {param.ds_id} Offloaded to nvme offload and buffers released.\", force=False)\n\n            print_rank_0(f\"ID {param.ds_id} partitioned type {param.dtype} dev {param.device} shape {param.shape}\")\n\n    @instrument_w_nvtx\n    def _partition_param_sec(self, param, buffer=None, has_been_updated=False):\n        assert param.ds_status is not ZeroParamStatus.INFLIGHT, f\" {param} Cannot partition a param in flight\"\n        global reuse_buffers\n        ##support for NVME secondary param offload\n        #print_rank_0(f\"SEC Param id {param.ds_id} status is {param.ds_status}\", force=True)\n        if param.ds_status is ZeroParamStatus.AVAILABLE:\n            if param.ds_secondary_tensor is not None and not has_been_updated:  ##param already partitioned\n                return\n            #check padding\n            tensor_size = self._aligned_size(param)\n            partition_size = tensor_size // self.dp_world_size\n\n            secondary_partition_size = int(tensor_size // self.num_ranks_in_param_group)\n            if param.ds_secondary_tensor is None:\n                final_location = None\n                secondary_partitioned_tensor = torch.empty(secondary_partition_size,\n                                                           dtype=param.dtype,\n                                                           device=self.remote_device)\n\n                if self.pin_memory:\n                    secondary_partitioned_tensor = secondary_partitioned_tensor.pin_memory()\n                # quantize the tensor if it's not trainable\n                if not param.requires_grad and self.quantized_nontrainable_weights:\n                    secondary_partitioned_tensor, secondary_partitioned_tensor.ds_quant_scale = self.quantizer_module.quantize(\n                        secondary_partitioned_tensor)\n                secondary_partitioned_tensor.requires_grad = False\n                param.ds_secondary_tensor = secondary_partitioned_tensor\n                param.ds_secondary_tensor.ds_numel = secondary_partition_size\n                param.ds_secondary_tensor.status = PartitionedParamStatus.AVAILABLE\n                param.ds_secondary_tensor.final_location = final_location\n\n            #use rank in group for secondary tensor\n            secondary_start = secondary_partition_size * self.rank_in_group\n\n            secondary_end = secondary_start + secondary_partition_size\n\n            one_dim_param = param.contiguous().view(-1)\n\n            # ds_numel is unpadded, so the last chunk of the secondary tensor might not be secondary_partition_size\n            sec_numel = max(0, min(param.ds_numel - secondary_start, secondary_partition_size))\n\n            # copy from full tensor to secondary tensor\n            param.ds_secondary_tensor.narrow(0, 0,\n                                             sec_numel).copy_(one_dim_param.narrow(0, secondary_start, sec_numel))\n\n            # TODO: This is a temporary fix to avoid the issue that 2nd tensor all-gather happens before 2nd tensor partition is done\n            if not get_accelerator().resolves_data_dependency():\n                get_accelerator().current_stream().synchronize()\n\n            print_rank_0(f\"{param.ds_id} partitioned type {param.dtype} dev {param.device} shape {param.shape}\",\n                         force=False)\n\n    def _param_status(self, param):\n        if param.ds_tensor is not None:\n            print_rank_0(\n                f\"Param id {param.ds_id}, param status: {param.ds_status}, param numel {param.ds_numel}, partitioned numel {param.ds_tensor.numel()}, data numel {param.data.numel()}\"\n            )\n        else:\n            print_rank_0(\n                f\"Param id {param.ds_id}, param status: {param.ds_status}, param numel {param.ds_numel}, partitioned ds_tensor {param.ds_tensor}, data numel {param.data.numel()}\"\n            )\n\n    def _allgather_param(self, param, async_op=False, hierarchy=0):\n\n        partition_size = param.ds_tensor.ds_numel\n\n        tensor_size = partition_size * self.num_partitions\n        aligned_param_size = self._aligned_size(param)\n        assert tensor_size == aligned_param_size, f'param id {param.ds_id} aligned size {aligned_param_size} does not match tensor size {tensor_size}'\n\n        print_rank_0(\n            f\"{'--'* hierarchy}---- Before allocating allgather param {debug_param2name_id_shape_status(param)} partition size={partition_size}\"\n        )\n\n        see_memory_usage(\n            f'Before allocate allgather param {debug_param2name_id_shape_status(param)} partition_size={partition_size} ',\n            force=False)\n        flat_tensor = torch.zeros(aligned_param_size, dtype=param.dtype, device=param.device).view(-1)\n        see_memory_usage(\n            f'After allocate allgather param {debug_param2name_id_shape_status(param)} {aligned_param_size} {partition_size} ',\n            force=False)\n\n        if not get_accelerator().resolves_data_dependency():\n            get_accelerator().synchronize()\n\n        print_rank_0(\n            f\"{'--'* hierarchy}----allgather param with {debug_param2name_id_shape_status(param)} partition size={partition_size}\"\n        )\n        #        if not flat_tensor.numel() > 100000:\n        #            replicated_tensor = flat_tensor.narrow(0,\n        #                                                   0,\n        #                                                   param.ds_numel).view(param.ds_shape)\n        #            param.data = replicated_tensor.data\n        #            return None\n        if self.use_all_gather_into_tensor:\n            handle = dist.all_gather_into_tensor(flat_tensor,\n                                                 param.ds_tensor.to(get_accelerator().device_name()),\n                                                 group=self.get_partition_dp_group(param),\n                                                 async_op=async_op)\n        else:\n            partitions = []\n            for i in range(self.num_partitions):\n                partitions.append(flat_tensor.narrow(0, partition_size * i, partition_size))\n\n                if i == dist.get_rank(group=self.get_partition_dp_group(param)):\n                    partitions[i].data.copy_(param.ds_tensor.data, non_blocking=True)\n\n            handle = dist.all_gather(partitions,\n                                     partitions[self.get_partition_rank()],\n                                     group=self.get_partition_dp_group(param),\n                                     async_op=async_op)\n\n        replicated_tensor = flat_tensor.narrow(0, 0, param.ds_numel).view(param.ds_shape)\n        param.data = replicated_tensor.data\n        return handle\n\n    def _allgather_params_coalesced(self, param_list, hierarchy=0, quantize=False):\n        \"\"\" blocking call\n        avoid explicit memory copy in _allgather_params\n        \"\"\"\n        if len(param_list) == 0:\n            return\n\n        if self.num_partitions == 1:\n            handle = _no_gather_coalesced(param_list)\n            handle.wait()\n            return None\n\n        # collect local tensors and partition sizes\n        partition_sizes = []\n        local_tensors = []\n        if quantize:\n            quantize_scale_sizes = []\n            quantize_scale_tensors = []\n        for param in param_list:\n            partition_sizes.append(param.ds_tensor.ds_numel)\n            local_tensors.append(param.ds_tensor.to(get_accelerator().device_name()))\n            if quantize:\n                quantize_scale_sizes.append(param.ds_tensor.ds_quant_scale.numel())\n                quantize_scale_tensors.append(param.ds_tensor.ds_quant_scale.to(get_accelerator().device_name()))\n        # allocate memory for allgather params\n        allgather_params = []\n        if quantize:\n            allgather_quantize_scale = []\n        for psize in partition_sizes:\n            tensor_size = psize * self.num_partitions\n            flat_tensor = torch.empty(tensor_size, dtype=param_list[0].ds_tensor.dtype,\n                                      device=self.local_device).view(-1)\n            flat_tensor.requires_grad = False\n            allgather_params.append(flat_tensor)\n        if quantize:\n            for psize in quantize_scale_sizes:\n                tensor_size = psize * self.num_partitions\n                flat_tensor = torch.empty(tensor_size,\n                                          dtype=param_list[0].ds_tensor.ds_quant_scale.dtype,\n                                          device=self.local_device).view(-1)\n                flat_tensor.requires_grad = False\n                allgather_quantize_scale.append(flat_tensor)\n\n        # launch\n        launch_handles = []\n        launch_quantize_handles = []\n        for param_idx, param in enumerate(param_list):\n            input_tensor = local_tensors[param_idx].view(-1)\n\n            if self.use_all_gather_into_tensor:\n                # try the _all_gather_base from Pytorch master\n                h = dist.all_gather_into_tensor(allgather_params[param_idx],\n                                                input_tensor,\n                                                group=self.get_partition_dp_group(param),\n                                                async_op=True)\n                if quantize:\n                    quantize_handle = dist.all_gather_into_tensor(allgather_quantize_scale[param_idx],\n                                                                  quantize_scale_tensors[param_idx],\n                                                                  group=self.get_partition_dp_group(param),\n                                                                  async_op=True)\n                    launch_quantize_handles.append(quantize_handle)\n            else:\n                output_list = []\n                for i in range(self.num_partitions):\n                    psize = partition_sizes[param_idx]\n                    partition = allgather_params[param_idx].narrow(0, i * psize, psize)\n                    output_list.append(partition)\n                    if not get_accelerator().on_accelerator(partition):\n                        logger.warning(\n                            f'param {param_idx}, partition {i} is not on CUDA, partition shape {partition.size()}')\n\n                # back to old all_gather function\n                h = dist.all_gather(output_list, input_tensor, group=self.get_partition_dp_group(param), async_op=True)\n                if quantize:\n                    output_scale_list = []\n                    for i in range(self.num_partitions):\n                        psize = quantize_scale_sizes[param_idx]\n                        partition = allgather_quantize_scale[param_idx].narrow(0, i * psize, psize)\n                        output_scale_list.append(partition)\n                    quant_handle = dist.all_gather(output_scale_list,\n                                                   quantize_scale_tensors[param_idx],\n                                                   group=self.get_partition_dp_group(param),\n                                                   async_op=True)\n                    launch_quantize_handles.append(quant_handle)\n            launch_handles.append(h)\n\n        # Wait ensures the operation is enqueued, but not necessarily complete.\n        launch_handles[-1].wait()\n        if quantize:\n            for quant_handle in launch_quantize_handles:\n                quant_handle.wait()\n\n        # assign to param.data (not copy)\n        for i, param in enumerate(param_list):\n            gathered_tensor = allgather_params[i]\n            if quantize:\n                gathered_tensor = self.quantizer_module.dequantize(gathered_tensor, allgather_quantize_scale[i])\n            param.data = gathered_tensor.narrow(0, 0, param.ds_numel).view(param.ds_shape).data\n\n        # guarantee the communication to be completed\n        if not get_accelerator().resolves_data_dependency():\n            get_accelerator().synchronize()\n\n        return None\n\n    def _allgather_params(self, param_list, hierarchy=0):\n        if len(param_list) == 0:\n            return\n\n        partition_size = sum([param.ds_tensor.ds_numel for param in param_list])\n\n        tensor_size = partition_size * self.num_partitions\n        flat_tensor = torch.empty(tensor_size, dtype=param_list[0].ds_tensor.dtype, device=self.local_device)\n        flat_tensor.requires_grad = False\n        partitions = []\n        for i in range(self.num_partitions):\n            start = partition_size * i\n\n            partitions.append(flat_tensor.narrow(0, start, partition_size))\n\n            if i == self.get_partition_rank():\n                offset = 0\n                for param in param_list:\n                    param_numel = param.ds_tensor.ds_numel\n\n                    partitions[i].narrow(0, offset, param_numel).copy_(param.ds_tensor.data)\n\n                    offset += param_numel\n\n        if hasattr(param_list[0], 'ds_quant_scale'):\n            scale_size = sum([param.ds_tensor.ds_quant_scale.numel() for param in param_list])\n            scale_tensor_size = scale_size * self.world_size\n            flat_scale_tensor = torch.empty(scale_tensor_size,\n                                            dtype=param_list[0].ds_tensor.ds_quant_scale.dtype,\n                                            device=self.local_device)\n            flat_scale_tensor.requires_grad = False\n            scale_partitions = []\n            for i in range(self.world_size):\n                start = scale_tensor_size * i\n                scale_partitions.append(flat_scale_tensor.narrow(0, start, scale_tensor_size))\n                if i == self.rank:\n                    offset = 0\n                    for param in param_list:\n                        param_scale_numel = param.ds_tensor.ds_quant_scale.ds_numel\n\n                        scale_partitions[i].narrow(0, offset,\n                                                   param_scale_numel).copy_(param.ds_tensor.ds_quant_scale.data)\n\n                        offset += param_scale_numel\n\n        dist.all_gather_into_tensor(flat_tensor,\n                                    partitions[self.get_partition_rank()],\n                                    group=self.get_partition_dp_group(param),\n                                    async_op=False)\n        if hasattr(param_list[0], 'ds_quant_scale'):\n            dist.all_gather(flat_scale_tensor,\n                            param_list[0].ds_quant_scale,\n                            group=self.get_partition_dp_group(param),\n                            async_op=False)\n        param_offset = 0\n\n        for param in param_list:\n            param_partition_size = param.ds_tensor.ds_numel\n            param_size = param.ds_numel\n            replicated_tensor = torch.empty(param.ds_shape, dtype=param.ds_tensor.dtype, device=self.local_device)\n\n            for i in range(self.num_partitions):\n\n                start = i * partition_size\n\n                param_start = i * param_partition_size\n\n                if param_start < param_size:\n                    numel_to_copy = min(param_size - param_start, param_partition_size)\n\n                    part_to_copy = partitions[i].narrow(0, param_offset, numel_to_copy)\n\n                    replicated_tensor.view(-1).narrow(0, param_start, numel_to_copy).copy_(part_to_copy)\n            #param_offset += param.data.numel()\n            param_offset += param.ds_tensor.ds_numel\n            if hasattr(param_list[0], 'ds_quant_scale'):\n                replicated_tensor = self.quantizer_module.dequantize(replicated_tensor, flat_scale_tensor)\n            param.data = replicated_tensor.data\n\n        return None\n\n    def _reduce_scatter_gradients(self, param_list):\n        #print_rank_0([param.grad for param in param_list])\n        #assert any([param.grad is None for param in param_list]), \"None gradients cannot be reduce scattered\"\n\n        handles_and_reduced_partitions = []\n        for param in param_list:\n            assert param.grad.numel(\n            ) == param.ds_numel, f\"{param.grad.numel()} != {param.ds_numel} Cannot reduce scatter gradients whose size is not same as the params\"\n\n            handles_and_reduced_partitions.append(self._reduce_scatter_gradient(param))\n\n        for param, (handle, reduced_partition) in zip(param_list, handles_and_reduced_partitions):\n            if handle is not None:\n                handle.wait()\n\n            # some ranks may have partitions that are padded to go beyond the grad size.\n            # For these ranks the output of reduce scatter is a separate buffer and needs\n            # to be copied in\n            partition_size = param.ds_tensor.ds_numel\n            start = self.get_partition_rank() * partition_size\n            end = start + partition_size\n            #print_rank_0(\"REduce scatter was executed for param {param.ds_id}\")\n            if start < param.ds_numel < end:\n                elements = param.ds_numel - start\n                param.grad.view(-1).narrow(0, start, elements).copy_(reduced_partition.narrow(0, 0, elements))\n\n    def _reduce_scatter_gradient(self, param):\n\n        partition_size = param.ds_tensor.ds_numel\n        #output = torch.empty(partition_size, dtype=param.dtype, device=param.device)\n\n        total_size = partition_size * self.num_partitions\n        input_list = []\n\n        for i in range(self.num_partitions):\n\n            start = i * partition_size\n            end = start + partition_size\n\n            #print(\"before reduce scatter gradients\")\n            if start < param.ds_numel and end <= param.ds_numel:\n                input = param.grad.view(-1).narrow(0, start, partition_size)\n            else:\n                input = torch.zeros(partition_size, dtype=param.dtype, device=param.device)\n\n                if start < param.ds_numel:\n                    elements = param.ds_numel - start\n                    input.narrow(0, 0, elements).copy_(param.grad.view(-1).narrow(0, start, elements))\n            #print(\"after reduce scatter gradients\")\n            input_list.append(input)\n\n        rank = dist.get_rank(group=self.get_partition_dp_group(param))\n        handle = dist.reduce_scatter(input_list[rank],\n                                     input_list,\n                                     group=self.get_partition_dp_group(param),\n                                     async_op=True)\n\n        return handle, input_list[rank]\n\n    def _partition_gradients(self, param_list, partition_buffers=None, accumulate=False):\n        if partition_buffers is None:\n            partition_buffers = [None] * len(param_list)\n\n        for param, partition_buffer in zip(param_list, partition_buffers):\n            self._partition_gradient(param, partition_buffer=partition_buffer, accumulate=accumulate)\n\n    def _partition_gradient(self, param, partition_buffer=None, accumulate=False):\n\n        #import pdb;pdb.set_trace()\n        # param.grad=None\n        # param.grad.test()\n        print_rank_0(\n            f\"Partitioning param {param.ds_id} gradient of size {param.grad.numel()} type {param.grad.dtype} part_size {param.ds_tensor.ds_numel}\"\n        )\n        see_memory_usage(\"Before partitioning gradients\", force=False)\n        partition_size = param.ds_tensor.ds_numel\n\n        if partition_buffer is None:\n            assert not accumulate, \"No buffer to accumulate to\"\n            partition_buffer = torch.zeros(partition_size, dtype=param.dtype, device=param.device)\n        else:\n            assert partition_buffer.numel(\n            ) >= partition_size, f\"The partition buffer size {partition_buffer.numel()} should match the size of param.ds_tensor {partition_size}\"\n\n        rank = dist.get_rank(group=self.get_partition_dp_group(param))\n        start = partition_size * rank\n        end = start + partition_size\n\n        dest_tensor_full_buffer = partition_buffer.view(-1).narrow(0, 0, partition_size)\n\n        #print(\"before partition gradients\")\n        if start < param.ds_numel:\n            elements = min(param.ds_numel - start, partition_size)\n\n            dest_tensor = dest_tensor_full_buffer.narrow(0, 0, elements)\n            src_tensor = param.grad.view(-1).narrow(0, start, elements)\n\n            # just copy the grad partition to the buffer\n            if not accumulate:\n                dest_tensor.copy_(src_tensor)\n\n            # if source and destination are on same device,\n            # add to the provided buffer\n            elif src_tensor.device == dest_tensor.device:\n                dest_tensor.add_(src_tensor)\n\n            # if source and destination are on different device, copy first to src\n            # then add and move back to the destination. This seems to run faster\n            # when src is gpu and dest is cpu\n            # adding directly to cpu is very slow\n            else:\n                acc_tensor = torch.empty(src_tensor.numel(), dtype=param.dtype, device=param.device)\n\n                acc_tensor.copy_(dest_tensor)\n                acc_tensor.add_(src_tensor)\n                dest_tensor.copy_(acc_tensor)\n\n            # partition_buffer.view(-1).narrow(\n            #     0,\n            #     0,\n            #     elements).copy_(param.grad.view(-1).narrow(0,\n            #                                             start,\n            #                                             elements))\n\n        #print(\"after partition gradients\")\n        param.grad.data = dest_tensor_full_buffer.data\n        see_memory_usage(\"After partitioning gradients\", force=False)\n\n    def get_partition_dp_group(self, param):\n        return param.ds_process_group\n\n    def get_partition_rank(self):\n        \"\"\"subclass can overload to specify different relative rank in\n        parameter partition group\"\"\"\n        return self.rank\n\n    @property\n    def num_partitions(self):\n        return self.dp_world_size\n\n    def get_dp_process_group(self):\n        \"\"\" Return the communication group with all data-parallel ranks \"\"\"\n        return self.ds_process_group\n\n\nclass GatheredParameters:\n\n    def __init__(self, params, modifier_rank=None, fwd_module=None, enabled=True):\n        \"\"\"A context that collects parameters that were partitioned via a\n        :class:`deepspeed.zero.Init` context. The parameters are partitioned\n        again upon exit.\n\n        Args:\n            params (``torch.nn.Parameter``): A single parameter, or an iterable of parameters (list, tuple, generator) of parameters to collect.\n                It's assumed that all parameters are zero params.\n            modifier_rank (int, optional): If specified, this rank's parameter will be\n                broadcasted on exit from the context. This argument is required if ``params`` are\n                modified, so that all processes have a consistent view of the data. Defaults\n                to ``None``.\n            fwd_module (``torch.nn.Module``, optional): If specified, ``params`` will be\n                registered as external parameters of ``fwd_module``. See :meth:`deepspeed.zero.register_external_parameter`.\n            enabled (bool, optional): If ``False``, this context is a no-op. Defaults to ``True``.\n\n        Important: Make sure to use ``modifier_rank`` that is not ``None`` (e.g., ``modifier_rank=0``)\n        if you need the GPU memory allocated by gather to be released upon exit from the context manager.\n\n        Important: if ``params`` isn't an iterable of parameters or a single parameter it'll be silently ignored!\n\n        Examples\n        ========\n\n        #. Allocate a partitioned module, initialize its weight on rank 0, and update all\n           processes.\n\n            .. code-block:: python\n\n                with deepspeed.zero.Init():\n                    linear = torch.nn.Linear(1000,1000)\n\n                with deepspeed.zero.GatheredParameters(linear.weight,\n                                                       modifier_rank=0):\n                    if deepspeed.comm.get_rank() == 0:\n                        linear.weight.zero_()\n\n                with deepspeed.zero.GatheredParameters(linear.weight,\n                                                       modifier_rank=0):\n                    if deepspeed.comm.get_rank() == 0:\n                        linear.weight.zero_()\n\n        #. Collect a partitioned weight to pass to another module during\n           training. The parameter will be registered as an external parameter\n           and made available during the backward pass.\n\n            .. code-block:: python\n                :emphasize-lines: 6\n\n                def forward(self, input):\n                    x = self.layer1(input)\n\n                    # self.layer1.weight is required by self.layer2.forward\n                    with deepspeed.zero.GatheredParameters(self.layer1.weight,\n                                                           fwd_module=self):\n                        y = self.layer2(x, self.layer1.weight)\n                    return y\n\n\n        #. Pretrained model loading\n\n            .. code-block:: python\n\n                with deepspeed.zero.Init():\n                    model = MyModel()\n\n                state_dict = torch.load(model_path, map_location=\"cpu\")\n\n                def load(module: nn.Module, prefix=\"\"):\n                    # because zero3 puts placeholders in model params, this context\n                    # manager gathers (unpartitions) the params of the current layer, then loads from\n                    # the state dict and then re-partitions them again\n                    with deepspeed.zero.GatheredParameters(list(module.parameters(recurse=False)), modifier_rank=0):\n                        if deepspeed.comm.get_rank() == 0:\n                            module._load_from_state_dict(state_dict, prefix)\n\n                    for name, child in module._modules.items():\n                        if child is not None:\n                            load(child, prefix + name + \".\")\n\n                load(model, prefix=\"\")\n\n        If this approach is not used, then the full model will first be copied to each GPU. For models\n        bigger than the memory of a single GPU, this method is required.\n        \"\"\"\n\n        self.enabled = enabled\n        if not enabled:\n            return\n\n        if isinstance(params, Iterable) and not isinstance(params, torch.Tensor):\n            # deal with generators like model.parameters()\n            # must convert to list to be able to iterate more than once if we get a generator\n            params = list(params)\n        else:\n            # single param\n            params = [params]\n        # enable if at least one is zero-param, otherwise a noop\n        if not any(is_zero_param(p) for p in params):\n            self.enabled = False\n            return\n\n        self.params = [p for p in params if hasattr(p, \"ds_id\")]\n        self.params = sorted(\n            set(self.params), key=lambda x: x.ds_id\n        )  # remove the duplicates to prevent racing condition, we must also make sure the order is the same on all ranks otherwise we'll get deadlocks\n        self.src_rank = None\n        if modifier_rank is not None:\n            if self.params[0].ds_process_group == dist.get_world_group():\n                self.src_rank = modifier_rank\n            else:\n                # A group was specified; convert DP rank to global rank\n                self.src_rank = dist.get_global_rank(self.params[0].ds_process_group, modifier_rank)\n        self.fwd_module = fwd_module\n        if self.fwd_module is not None:\n            # is a no-op if already registered\n            for p in self.params:\n                register_external_parameter(self.fwd_module, p)\n\n    def __enter__(self):\n        if not self.enabled:\n            return\n        self.params[0].all_gather(param_list=self.params)\n\n    def __exit__(self, *exc):\n        if not self.enabled:\n            return\n        if self.src_rank is None:\n            self.params[0].partition(param_list=self.params, has_been_updated=False)\n            return\n\n        handles = [dist.broadcast(p.data, self.src_rank, group=p.ds_process_group, async_op=True) for p in self.params]\n        for h in handles:\n            h.wait()\n        self.params[0].partition(param_list=self.params, has_been_updated=True)\n", "deepspeed/runtime/fp16/fused_optimizer.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\"\"\"\nCopyright NVIDIA/apex\nThis file is adapted from FP16_Optimizer in NVIDIA/apex\n\"\"\"\n\nimport torch\nfrom torch._utils import _flatten_dense_tensors, _unflatten_dense_tensors\nfrom deepspeed.runtime.base_optimizer import DeepSpeedOptimizer\nfrom deepspeed.runtime.utils import get_global_norm, get_flattened_grad_norm, CheckOverflow, get_weight_norm, get_norm_with_moe_layers, is_model_parallel_parameter\nfrom deepspeed.runtime.fp16.loss_scaler import INITIAL_LOSS_SCALE, SCALE_WINDOW, MIN_LOSS_SCALE\nfrom deepspeed.utils import logger, log_dist\nfrom deepspeed.utils.torch import required_torch_version\nfrom deepspeed.checkpoint.constants import OPTIMIZER_STATE_DICT, CLIP_GRAD\nfrom deepspeed.accelerator import get_accelerator\nfrom deepspeed.moe.utils import is_moe_param_group\nfrom deepspeed.runtime.constants import PIPE_REPLICATED\nfrom deepspeed.utils.bwc import bwc_tensor_model_parallel_rank\n\nOVERFLOW_CHECK_TIMER = 'overflow_check'\nCOMPUTE_NORM_TIMER = 'compute_norm'\nUNSCALE_AND_CLIP_TIMER = 'unscale_and_clip'\nBASIC_STEP_TIMER = 'basic_step'\nUPDATE_FP16_TIMER = 'update_fp16'\n\nOVERFLOW_TIMERS = [COMPUTE_NORM_TIMER, OVERFLOW_CHECK_TIMER]\nSTEP_TIMERS = OVERFLOW_TIMERS + [UNSCALE_AND_CLIP_TIMER, BASIC_STEP_TIMER, UPDATE_FP16_TIMER]\n\n\nclass FP16_Optimizer(DeepSpeedOptimizer):\n    \"\"\"\n   FP16 Optimizer for training fp16 models. Handles loss scaling.\n\n   For usage example please see, TODO:  DeepSpeed V2 Tutorial\n    \"\"\"\n\n    def __init__(self,\n                 init_optimizer,\n                 deepspeed=None,\n                 static_loss_scale=1.0,\n                 dynamic_loss_scale=False,\n                 initial_dynamic_scale=2**32,\n                 dynamic_loss_args=None,\n                 verbose=True,\n                 mpu=None,\n                 clip_grad=0.0,\n                 fused_adam_legacy=False,\n                 has_moe_layers=False,\n                 timers=None):\n\n        self.fused_adam_legacy = fused_adam_legacy\n        self.timers = timers\n        self.deepspeed = deepspeed\n        self.has_moe_layers = has_moe_layers\n        self.using_pipeline = self.deepspeed.pipeline_parallelism\n        if not get_accelerator().is_available():\n            raise SystemError(\"Cannot use fp16 without accelerator.\")\n        self.optimizer = init_optimizer\n\n        # param flattened by groups\n        self.fp16_groups = []\n        self.fp16_groups_flat = []\n        self.fp32_groups_flat = []\n\n        self.flatten_grad_norm_mask_list = []\n        self.has_executed_step = False\n        self._global_grad_norm = 0.\n\n        # loop to deal with groups\n        for i, param_group in enumerate(self.optimizer.param_groups):\n            # push this group to list before modify\n            self.fp16_groups.append(param_group['params'])\n            # init fp16 weight buffer, flattened\n            self.fp16_groups_flat.append(_flatten_dense_tensors([p.clone().detach() for p in self.fp16_groups[i]]))\n            # set model fp16 weight to slices of flattened buffer\n            updated_params = _unflatten_dense_tensors(self.fp16_groups_flat[i], self.fp16_groups[i])\n            for p, q in zip(self.fp16_groups[i], updated_params):\n                p.data = q.data\n            # init master weight, flattened\n            self.fp32_groups_flat.append(self.fp16_groups_flat[i].clone().float().detach())\n            # modify optimizer of have flat master weight\n            self.fp32_groups_flat[i].requires_grad = True  # keep this in case internal optimizer uses it\n            param_group['params'] = [self.fp32_groups_flat[i]]\n\n        # we may have a way of fusing dynamic scale. Do not support for now\n        if dynamic_loss_scale:\n            self.dynamic_loss_scale = True\n            self.cur_iter = 0\n            self.last_overflow_iter = -1\n            self.scale_factor = 2\n\n            if dynamic_loss_args is None:\n                self.cur_scale = initial_dynamic_scale\n                self.scale_window = 1000\n                self.min_loss_scale = 1\n            else:\n                self.cur_scale = dynamic_loss_args[INITIAL_LOSS_SCALE]\n                self.scale_window = dynamic_loss_args[SCALE_WINDOW]\n                self.min_loss_scale = dynamic_loss_args[MIN_LOSS_SCALE]\n        else:\n            self.dynamic_loss_scale = False\n            self.cur_iter = 0\n            self.cur_scale = static_loss_scale\n        self.verbose = verbose\n\n        self.custom_loss_scaler = False\n        self.external_loss_scale = None\n\n        self.clip_grad = clip_grad\n        self.norm_type = 2\n\n        if required_torch_version(max_version=0.4):\n            self.clip_grad_norm = torch.nn.utils.clip_grad_norm\n        else:\n            self.clip_grad_norm = torch.nn.utils.clip_grad_norm_\n\n        #model parallel object\n        self.mpu = mpu\n\n        self.overflow = False\n        self.overflow_checker = CheckOverflow(self.fp16_groups, mpu=self.mpu, deepspeed=deepspeed)\n        self.initialize_optimizer_states()\n\n    def initialize_optimizer_states(self):\n        for i, group in enumerate(self.fp16_groups):\n            self.fp32_groups_flat[i].grad = torch.zeros(self.fp32_groups_flat[i].size(),\n                                                        device=self.fp32_groups_flat[i].device)\n\n        self.optimizer.step()\n\n        for i, group in enumerate(self.fp16_groups):\n            self.fp32_groups_flat[i].grad = None\n\n        return\n\n    def zero_grad(self, set_to_none=True):\n        \"\"\"\n        Zero FP16 parameter grads.\n        \"\"\"\n        # For speed, set model fp16 grad to None by default\n        for group in self.fp16_groups:\n            for p in group:\n                if set_to_none:\n                    p.grad = None\n                else:\n                    if p.grad is not None:\n                        p.grad.detach_()\n                        p.grad.zero_()\n\n    def step_fused_adam(self, closure=None):\n        \"\"\"\n        Not supporting closure.\n        \"\"\"\n\n        # First compute norm for all group so we know if there is overflow\n        grads_groups_flat = []\n        norm_groups = []\n        for i, group in enumerate(self.fp16_groups):\n            grads_groups_flat.append(\n                _flatten_dense_tensors([\n                    torch.zeros(p.size(), dtype=p.dtype, device=p.device) if p.grad is None else p.grad for p in group\n                ]))\n            norm_groups.append(get_weight_norm(grads_groups_flat[i], mpu=self.mpu))\n\n        self.overflow = self.overflow_checker.check_using_norm(norm_groups)\n        prev_scale = self.cur_scale\n        self._update_scale(self.overflow)\n\n        if self.overflow:\n            if self.verbose:\n                logger.info(\"[deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss \"\n                            \"scale: {}, reducing to {}\".format(prev_scale, self.cur_scale))\n            return self.overflow\n\n        scaled_grad_norm = get_global_norm(norm_list=norm_groups)\n\n        combined_scale = self.unscale_and_clip_grads(grads_groups_flat, scaled_grad_norm, apply_scale=False)\n\n        # Stash unscaled gradient norm\n        self._global_grad_norm = scaled_grad_norm / self.cur_scale\n\n        # norm is in fact norm*cur_scale\n        self.optimizer.step(grads=[[g] for g in grads_groups_flat],\n                            output_params=[[p] for p in self.fp16_groups_flat],\n                            scale=combined_scale,\n                            grad_norms=norm_groups)\n        # TODO: we probably don't need this? just to be safe\n        for i in range(len(norm_groups)):\n            updated_params = _unflatten_dense_tensors(self.fp16_groups_flat[i], self.fp16_groups[i])\n            for p, q in zip(self.fp16_groups[i], updated_params):\n                p.data = q.data\n        return self.overflow\n\n    def set_lr(self, lr):\n        \"\"\"Set the learning rate.\"\"\"\n        for param_group in self.optimizer.param_groups:\n            param_group[\"lr\"] = lr\n\n    def get_lr(self):\n        \"\"\"Return the current learning rate.\"\"\"\n        return self.optimizer.param_groups[0][\"lr\"]\n\n    def override_loss_scale(self, loss_scale):\n        if loss_scale != self.external_loss_scale:\n            logger.info(f'[deepspeed] setting loss scale from {self.external_loss_scale} -> {loss_scale}')\n        self.custom_loss_scaler = True\n        self.external_loss_scale = loss_scale\n\n    def _require_avoid_recompute_norm(self, p, tensor_model_parallel_rank):\n        # for filtering  replicated tensors from tensor\n        if hasattr(p, PIPE_REPLICATED) and p.ds_pipe_replicated:\n            return True\n        if (tensor_model_parallel_rank > 0) and not is_model_parallel_parameter(p):\n            return True\n\n    def _get_norm_mask_idx(self, group):\n        \"\"\"The function preserves the parallel information for norm\n        from unflattened gradients.\n\n        Args:\n            group (Iterable[Tensor] ): params group\n\n        Returns:\n            torch.Tensor: A 2D tensor containing index ranges for each group,\n                      where each row represents a [start index, end index].\n        \"\"\"\n        group_mask_idx_list = []\n        grad_flat_st_idx = 0\n        grad_flat_en_idx = 0\n\n        for p in group:\n            grad_flat_en_idx = grad_flat_st_idx + p.numel()\n            if p.grad is not None and self._require_avoid_recompute_norm(p, bwc_tensor_model_parallel_rank(self.mpu)):\n                # merge range\n                if len(group_mask_idx_list) > 0 and grad_flat_st_idx == group_mask_idx_list[-1][-1]:\n                    group_mask_idx_list[-1][-1] = grad_flat_en_idx\n                else:\n                    group_mask_idx_list.append([grad_flat_st_idx, grad_flat_en_idx])\n            grad_flat_st_idx = grad_flat_en_idx\n\n        return torch.tensor(group_mask_idx_list, device=get_accelerator().current_device_name())\n\n    def step(self, closure=None):\n        \"\"\"\n        Not supporting closure.\n        \"\"\"\n\n        if self.fused_adam_legacy:\n            return self.step_fused_adam()\n\n        # First determine if there is overflow.\n        self.timers(OVERFLOW_CHECK_TIMER).start()\n        fp16_params = []\n        for i, group in enumerate(self.fp16_groups):\n            fp16_params.extend([p for p in group if p.grad is not None])\n        self.overflow = self.overflow_checker.has_overflow(fp16_params)\n        self.timers(OVERFLOW_CHECK_TIMER).stop()\n        prev_scale = self.cur_scale\n        self._update_scale(self.overflow)\n        if self.overflow:\n            if self.verbose:\n                log_dist(\n                    \"Overflow detected. Skipping step. Attempted loss \"\n                    f\"scale: {prev_scale}, reducing to {self.cur_scale}\",\n                    ranks=[0])\n            # Clear gradients\n            for i, group in enumerate(self.fp16_groups):\n                for p in group:\n                    p.grad = None\n\n            self.timers.log(OVERFLOW_TIMERS)\n            return self.overflow\n\n        grads_groups_flat = []\n        non_experts_grads_for_norm = []\n        expert_grads_for_norm = {}\n        assert len(self.fp16_groups) == len(self.optimizer.param_groups)\n\n        for i, group in enumerate(self.fp16_groups):\n            data_type = self.fp32_groups_flat[i].dtype\n\n            grads_groups_flat.append(\n                _flatten_dense_tensors([\n                    torch.zeros(p.size(), dtype=data_type, device=p.device) if p.grad is None else p.grad.to(data_type)\n                    for p in group\n                ]))\n\n            self.fp32_groups_flat[i].grad = grads_groups_flat[i]\n            param_group = self.optimizer.param_groups[i]\n\n            # split expert and non_expert grads for norm\n            if self.has_moe_layers and is_moe_param_group(param_group):\n                if param_group['name'] not in expert_grads_for_norm:\n                    expert_grads_for_norm[param_group['name']] = []\n\n                expert_grads_for_norm[param_group['name']].append(self.fp32_groups_flat[i])\n            else:\n                # retrieves the required mask for calculating the norm of flat_grad\n                # perform this collect operation only once\n                if not self.has_executed_step:\n                    cur_flat_grad_norm_mask = self._get_norm_mask_idx(group)\n                    self.flatten_grad_norm_mask_list.append(cur_flat_grad_norm_mask)\n\n                non_experts_grads_for_norm.append(self.fp32_groups_flat[i])\n\n            for p in group:\n                p.grad = None\n\n        self.timers(COMPUTE_NORM_TIMER).start()\n\n        all_groups_norm = get_flattened_grad_norm(non_experts_grads_for_norm,\n                                                  mpu=self.mpu,\n                                                  grad_norm_mask=self.flatten_grad_norm_mask_list)\n\n        if self.has_moe_layers:\n            all_groups_norm = get_norm_with_moe_layers(all_groups_norm,\n                                                       mpu=self.mpu,\n                                                       expert_tensors=expert_grads_for_norm,\n                                                       norm_type=self.norm_type)\n\n        scaled_global_grad_norm = get_global_norm(norm_list=[all_groups_norm])\n        self.timers(COMPUTE_NORM_TIMER).stop()\n\n        # Stash unscaled gradient norm\n        self._global_grad_norm = scaled_global_grad_norm / self.cur_scale\n\n        self.timers(UNSCALE_AND_CLIP_TIMER).start()\n        self.unscale_and_clip_grads(grads_groups_flat, scaled_global_grad_norm)\n        self.timers(UNSCALE_AND_CLIP_TIMER).stop()\n\n        self.timers(BASIC_STEP_TIMER).start()\n        self.optimizer.step()\n        self.timers(BASIC_STEP_TIMER).stop()\n\n        #get rid of the fp32 gradients. Not needed anymore\n        for group in self.fp32_groups_flat:\n            group.grad = None\n\n        self.timers(UPDATE_FP16_TIMER).start()\n\n        for i in range(len(self.fp16_groups)):\n            updated_params = _unflatten_dense_tensors(self.fp32_groups_flat[i], self.fp16_groups[i])\n            for p, q in zip(self.fp16_groups[i], updated_params):\n                p.data.copy_(q.data)\n        self.has_executed_step = True\n        self.timers(UPDATE_FP16_TIMER).stop()\n\n        self.timers.log(STEP_TIMERS)\n\n        return self.overflow\n\n    def unscale_and_clip_grads(self, grad_groups_flat, total_norm, apply_scale=True):\n        # compute combined scale factor for this group\n        combined_scale = self.cur_scale\n        if self.clip_grad > 0.:\n            # norm is in fact norm*scale\n            clip = ((total_norm / self.cur_scale) + 1e-6) / self.clip_grad\n            if clip > 1:\n                combined_scale = clip * self.cur_scale\n\n        if apply_scale:\n            for grad in grad_groups_flat:\n                grad.data.mul_(1. / combined_scale)\n\n        return combined_scale\n\n    def backward(self, loss, create_graph=False, retain_graph=False):\n        \"\"\"\n        :attr:`backward` performs the following steps:\n\n        1. fp32_loss = loss.float()\n        2. scaled_loss = fp32_loss*loss_scale\n        3. scaled_loss.backward(), which accumulates scaled gradients into the ``.grad`` attributes of the model's fp16 leaves\n        \"\"\"\n        if self.custom_loss_scaler:\n            scaled_loss = self.external_loss_scale * loss\n            scaled_loss.backward()\n        else:\n            scaled_loss = (loss.float()) * self.cur_scale\n            scaled_loss.backward(create_graph=create_graph, retain_graph=retain_graph)\n\n    def _update_scale(self, skip):\n        if self.dynamic_loss_scale:\n            prev_scale = self.cur_scale\n            if skip:\n                self.cur_scale = max(self.cur_scale / self.scale_factor, self.min_loss_scale)\n                self.last_overflow_iter = self.cur_iter\n                if self.verbose:\n                    logger.info(f\"\\nGrad overflow on iteration {self.cur_iter}\")\n                    logger.info(f\"Reducing dynamic loss scale from {prev_scale} to {self.cur_scale}\")\n            else:\n                # Ensure self.scale_window updates since last overflow\n                stable_interval = (self.cur_iter - self.last_overflow_iter) - 1\n                if (stable_interval > 0) and (stable_interval % self.scale_window == 0):\n                    self.cur_scale *= self.scale_factor\n                    if self.verbose:\n                        logger.info(f\"No Grad overflow for {self.scale_window} iterations\")\n                        logger.info(f\"Increasing dynamic loss scale from {prev_scale} to {self.cur_scale}\")\n        else:\n            if skip:\n                logger.info(\"Grad overflow on iteration: %s\", self.cur_iter)\n                logger.info(\"Using static loss scale of: %s\", self.cur_scale)\n        self.cur_iter += 1\n        return\n\n    # Promote state so it can be retrieved or set via \"fp16_optimizer_instance.state\"\n    def _get_state(self):\n        return self.optimizer.state\n\n    def _set_state(self, value):\n        self.optimizer.state = value\n\n    state = property(_get_state, _set_state)\n\n    # Promote param_groups so it can be retrieved or set via \"fp16_optimizer_instance.param_groups\"\n    # (for example, to adjust the learning rate)\n    def _get_param_groups(self):\n        return self.optimizer.param_groups\n\n    def _set_param_groups(self, value):\n        self.optimizer.param_groups = value\n\n    param_groups = property(_get_param_groups, _set_param_groups)\n\n    def state_dict(self):\n        \"\"\"\n        Returns a dict containing the current state of this :class:`FP16_Optimizer` instance.\n        This dict contains attributes of :class:`FP16_Optimizer`, as well as the state_dict\n        of the contained Pytorch optimizer.\n        Example::\n            checkpoint = {}\n            checkpoint['model'] = model.state_dict()\n            checkpoint['optimizer'] = optimizer.state_dict()\n            torch.save(checkpoint, \"saved.pth\")\n        \"\"\"\n        state_dict = {}\n        state_dict['dynamic_loss_scale'] = self.dynamic_loss_scale\n        state_dict['cur_scale'] = self.cur_scale\n        state_dict['cur_iter'] = self.cur_iter\n        if state_dict['dynamic_loss_scale']:\n            state_dict['last_overflow_iter'] = self.last_overflow_iter\n            state_dict['scale_factor'] = self.scale_factor\n            state_dict['scale_window'] = self.scale_window\n        state_dict[OPTIMIZER_STATE_DICT] = self.optimizer.state_dict()\n        state_dict['fp32_groups_flat'] = self.fp32_groups_flat\n        state_dict[CLIP_GRAD] = self.clip_grad\n        return state_dict\n\n    # Refresh fp32 master params from fp16 copies\n    def refresh_fp32_params(self):\n        for current, saved in zip(self.fp32_groups_flat, self.fp16_groups_flat):\n            current.data.copy_(saved.data)\n\n    def load_state_dict(self, state_dict, load_optimizer_states=True):\n        \"\"\"\n        Loads a state_dict created by an earlier call to state_dict().\n        If ``fp16_optimizer_instance`` was constructed from some ``init_optimizer``,\n        whose parameters in turn came from ``model``, it is expected that the user\n        will call ``model.load_state_dict()`` before\n        ``fp16_optimizer_instance.load_state_dict()`` is called.\n        Example::\n            model = torch.nn.Linear(D_in, D_out).to(get_accelerator().device_name()).half()\n            optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n            optimizer = FP16_Optimizer(optimizer, static_loss_scale = 128.0)\n            ...\n            checkpoint = torch.load(\"saved.pth\")\n            model.load_state_dict(checkpoint['model'])\n            optimizer.load_state_dict(checkpoint['optimizer'])\n        \"\"\"\n        # I think it should actually be ok to reload the optimizer before the model.\n        self.dynamic_loss_scale = state_dict['dynamic_loss_scale']\n        self.cur_scale = state_dict['cur_scale']\n        self.cur_iter = state_dict['cur_iter']\n        if state_dict['dynamic_loss_scale']:\n            self.last_overflow_iter = state_dict['last_overflow_iter']\n            self.scale_factor = state_dict['scale_factor']\n            self.scale_window = state_dict['scale_window']\n        if load_optimizer_states:\n            self.optimizer.load_state_dict(state_dict[OPTIMIZER_STATE_DICT])\n        self.clip_grad = state_dict[CLIP_GRAD]\n        # At this point, the optimizer's references to the model's fp32 parameters are up to date.\n        # The optimizer's hyperparameters and internal buffers are also up to date.\n        # However, the fp32 master copies of the model's fp16 params stored by the optimizer are still\n        # out of date.  There are two options.\n        # 1:  Refresh the master params from the model's fp16 params.\n        # This requires less storage but incurs precision loss.\n        # 2:  Save and restore the fp32 master copies separately.\n        # We choose option 2.\n        #\n        # Pytorch Optimizer.load_state_dict casts saved buffers (e.g. momentum) to the type and device\n        # of their associated parameters, because it's possible those buffers might not exist yet in\n        # the current optimizer instance.  In our case, as long as the current FP16_Optimizer has been\n        # constructed in the same way as the one whose state_dict we are loading, the same master params\n        # are guaranteed to exist, so we can just copy_() from the saved master params.\n        for current, saved in zip(self.fp32_groups_flat, state_dict['fp32_groups_flat']):\n            current.data.copy_(saved.data)\n\n    def __repr__(self):\n        return repr(self.optimizer)\n\n    # Promote loss scale so it can be retrieved or set via \"fp16_optimizer_instance.loss_scale\"\n    def _get_loss_scale(self):\n        if self.custom_loss_scaler:\n            return self.external_loss_scale\n        else:\n            return self.cur_scale\n\n    def _set_loss_scale(self, value):\n        self.loss_scaler.cur_scale = value\n\n    loss_scale = property(_get_loss_scale, _set_loss_scale)\n", "deepspeed/runtime/fp16/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n'''Copyright The Microsoft DeepSpeed Team'''\n", "deepspeed/runtime/fp16/loss_scaler.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\"\"\"\nCopyright (c) 2019, NVIDIA CORPORATION.  All rights reserved.\n\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n\n     http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\nTaken and modified for DeepSpeed from:\n    https://github.com/NVIDIA/Megatron-LM/blob/master/fp16/loss_scaler.py\nCommit: 93ab4bea59dc5cbf97c079d313741866af4deac9\n\"\"\"\n\nimport torch\nfrom deepspeed import comm as dist\nfrom deepspeed.utils import logger\n\nINITIAL_LOSS_SCALE = 'init_scale'\nSCALE_WINDOW = 'scale_window'\nDELAYED_SHIFT = 'delayed_shift'\nCONSECUTIVE_HYSTERESIS = 'consecutive_hysteresis'\nMIN_LOSS_SCALE = 'min_scale'\n\n\n# item() is a recent addition, so this helps with backward compatibility.\ndef to_python_float(t):\n    if hasattr(t, 'item'):\n        return t.item()\n    return t[0]\n\n\nclass LossScalerBase:\n    \"\"\"LossScalarBase\n    Base class for a loss scaler\n    \"\"\"\n\n    def __init__(self, cur_scale):\n        self.cur_scale = cur_scale\n        self.dynamic = False\n\n    @property\n    def loss_scale(self):\n        return self.cur_scale\n\n    def scale_gradient(self, module, grad_in, grad_out):\n        return tuple(self.loss_scale * g for g in grad_in)\n\n    def update_scale(self, overflow):\n        pass\n\n    def backward(self, loss, retain_graph=False):\n        scaled_loss = loss * self.loss_scale\n        scaled_loss.backward(retain_graph=retain_graph)\n        # print(f'LossScalerBackward: {scaled_loss=}')\n\n\nclass LossScaler(LossScalerBase):\n    \"\"\"\n    Class that manages a static loss scale.  This class is intended to interact with\n    :class:`FP16_Optimizer`, and should not be directly manipulated by the user.\n\n    Use of :class:`LossScaler` is enabled via the ``static_loss_scale`` argument to\n    :class:`FP16_Optimizer`'s constructor.\n\n    Args:\n        scale (float, optional, default=1.0):  The loss scale.\n    \"\"\"\n\n    def __init__(self, scale=1):\n        super(LossScaler, self).__init__(scale)\n\n    # `params` is a list / generator of torch.Variable\n    def has_overflow(self, params):\n        return False\n\n    # `x` is a torch.Tensor\n    def _has_inf_or_nan(x):\n        return False\n\n\nclass DynamicLossScaler(LossScalerBase):\n    \"\"\"\n    Class that manages dynamic loss scaling.  It is recommended to use :class:`DynamicLossScaler`\n    indirectly, by supplying ``dynamic_loss_scale=True`` to the constructor of\n    :class:`FP16_Optimizer`.  However, it's important to understand how :class:`DynamicLossScaler`\n    operates, because the default options can be changed using the\n    the ``dynamic_loss_args`` argument to :class:`FP16_Optimizer`'s constructor.\n\n    Loss scaling is designed to combat the problem of underflowing gradients encountered at long\n    times when training fp16 networks.  Dynamic loss scaling begins by attempting a very high loss\n    scale.  Ironically, this may result in OVERflowing gradients.  If overflowing gradients are\n    encountered, :class:`DynamicLossScaler` informs :class:`FP16_Optimizer` that an overflow has\n    occurred.\n    :class:`FP16_Optimizer` then skips the update step for this particular iteration/minibatch,\n    and :class:`DynamicLossScaler` adjusts the loss scale to a lower value.\n    If a certain number of iterations occur without overflowing gradients detected,\n    :class:`DynamicLossScaler` increases the loss scale once more.\n    In this way :class:`DynamicLossScaler` attempts to \"ride the edge\" of\n    always using the highest loss scale possible without incurring overflow.\n\n    Args:\n        init_scale (float, optional, default=2**32):  Initial loss scale attempted by :class:`DynamicLossScaler.`\n        scale_factor (float, optional, default=2.0):  Factor used when adjusting the loss scale. If an overflow is encountered, the loss scale is readjusted to loss scale/``scale_factor``.  If ``scale_window`` consecutive iterations take place without an overflow, the loss scale is readjusted to loss_scale*``scale_factor``.\n        scale_window (int, optional, default=1000):  Number of consecutive iterations without an overflow to wait before increasing the loss scale.\n        consecutive_hysteresis (bool, optional, default=False): Whether to refill hysteresis if we reach an iteration that doesn't overflow\n    \"\"\"\n\n    def __init__(self,\n                 init_scale=2**32,\n                 scale_factor=2.,\n                 scale_window=1000,\n                 min_scale=1,\n                 delayed_shift=1,\n                 consecutive_hysteresis=False,\n                 raise_error_at_min_scale=True,\n                 dtype=torch.half):\n        super(DynamicLossScaler, self).__init__(init_scale)\n        self.cur_iter = 0\n        self.last_overflow_iter = -1\n        self.scale_factor = scale_factor\n        self.scale_window = scale_window\n        self.min_scale = min_scale\n        self.delayed_shift = delayed_shift\n        self.cur_hysteresis = delayed_shift\n        self.consecutive_hysteresis = consecutive_hysteresis\n        self.raise_error_at_min_scale = raise_error_at_min_scale\n        self.dynamic = True\n        self.dtype = dtype\n\n    # `params` is a list / generator of torch.Variable\n    def has_overflow_serial(self, params):\n        for p in params:\n            if p.grad is not None and self._has_inf_or_nan(p.grad.data):\n                return True\n\n        return False\n\n    # `x` is a torch.Tensor\n    def _has_inf_or_nan(x):\n        try:\n            # if x is half, the .float() incurs an additional deep copy, but it's necessary if\n            # Pytorch's .sum() creates a one-element tensor of the same type as x\n            # (which is true for some recent version of pytorch).\n            cpu_sum = float(x.float().sum())\n            # More efficient version that can be used if .sum() returns a Python scalar\n            # cpu_sum = float(x.sum())\n        except RuntimeError as instance:\n            # We want to check if inst is actually an overflow exception.\n            # RuntimeError could come from a different error.\n            # If so, we still want the exception to propagate.\n            if \"value cannot be converted\" not in instance.args[0]:\n                raise\n            return True\n        else:\n            if cpu_sum in [float('inf'), -float('inf')] or cpu_sum != cpu_sum:\n                return True\n            return False\n\n    # `overflow` is boolean indicating whether the gradient overflowed\n    def update_scale(self, overflow):\n        if overflow:\n            # self.cur_scale /= self.scale_factor\n            if self.delayed_shift == 1 or self.cur_hysteresis == 1:\n                if (self.cur_scale == self.min_scale) and self.raise_error_at_min_scale:\n                    raise Exception(\n                        \"Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.\")\n                else:\n                    next_scale = max(self.cur_scale / self.scale_factor, self.min_scale)\n                    if dist.get_rank() == 0:\n                        overflow_msg = f\"[deepspeed] OVERFLOW! Rank {dist.get_rank()} Skipping step.\"\n                        if self.dtype == torch.half:\n                            overflow_msg += f\" Attempted loss scale: {int(self.cur_scale)}, reducing to {int(next_scale)}\"\n                        logger.info(overflow_msg)\n                    self.cur_scale = next_scale\n            else:\n                if dist.get_rank() == 0:\n                    overflow_msg = f\"[deepspeed] OVERFLOW! Rank {dist.get_rank()} Skipping step.\"\n                    if self.dtype == torch.half:\n                        overflow_msg += f\" Attempted loss scale: {int(self.cur_scale)}, but hysteresis is {self.cur_hysteresis}. Reducing hysteresis to {self.cur_hysteresis-1}\"\n                    logger.info(overflow_msg)\n                self.cur_hysteresis -= 1\n            self.last_overflow_iter = self.cur_iter\n        else:\n            if self.consecutive_hysteresis:\n                if dist.get_rank() == 0:\n                    hysteresis_msg = f\"Consecutive hysteresis is enabled. Restoring hysteresis to {self.delayed_shift}\"\n                    logger.info(hysteresis_msg)\n                self.cur_hysteresis = self.delayed_shift\n            if (self.cur_iter - self.last_overflow_iter) % self.scale_window == 0:\n                if not self.consecutive_hysteresis:\n                    self.cur_hysteresis = self.delayed_shift\n                self.cur_scale *= self.scale_factor\n        self.cur_iter += 1\n\n\n# Although loss scaling is only defined for fp16, yet for backwards compatibility\n# we still create a scaler for other dtypes (fp32, bf16) which does not perform any scaling.\ndef CreateLossScaler(dtype, static_loss_scale, dynamic_scaling, dynamic_loss_args):\n    if dtype == torch.half and dynamic_scaling:\n        if dynamic_loss_args is None:\n            return DynamicLossScaler(dtype=dtype)\n        return DynamicLossScaler(dtype=dtype, **dynamic_loss_args)\n\n    loss_scale_value = static_loss_scale if dtype == torch.half else 1.0\n    return LossScaler(scale=loss_scale_value)\n\n\n##############################################################\n# Example usage below here -- assuming it's in a separate file\n##############################################################\n\"\"\"\nTO-DO separate out into an example.\nif __name__ == \"__main__\":\n    import torch\n    from torch.autograd import Variable\n    from dynamic_loss_scaler import DynamicLossScaler\n\n    # N is batch size; D_in is input dimension;\n    # H is hidden dimension; D_out is output dimension.\n    N, D_in, H, D_out = 64, 1000, 100, 10\n\n    # Create random Tensors to hold inputs and outputs, and wrap them in Variables.\n    x = Variable(torch.randn(N, D_in), requires_grad=False)\n    y = Variable(torch.randn(N, D_out), requires_grad=False)\n\n    w1 = Variable(torch.randn(D_in, H), requires_grad=True)\n    w2 = Variable(torch.randn(H, D_out), requires_grad=True)\n    parameters = [w1, w2]\n\n    learning_rate = 1e-6\n    optimizer = torch.optim.SGD(parameters, lr=learning_rate)\n    loss_scaler = DynamicLossScaler()\n\n    for t in range(500):\n        y_pred = x.mm(w1).clamp(min=0).mm(w2)\n        loss = (y_pred - y).pow(2).sum() * loss_scaler.loss_scale\n        print('Iter {} loss scale: {}'.format(t, loss_scaler.loss_scale))\n        print('Iter {} scaled loss: {}'.format(t, loss.data[0]))\n        print('Iter {} unscaled loss: {}'.format(t, loss.data[0] / loss_scaler.loss_scale))\n\n        # Run backprop\n        optimizer.zero_grad()\n        loss.backward()\n\n        # Check for overflow\n        has_overflow = DynamicLossScaler.has_overflow(parameters)\n\n        # If no overflow, unscale grad and update as usual\n        if not has_overflow:\n            for param in parameters:\n                param.grad.data.mul_(1. / loss_scaler.loss_scale)\n            optimizer.step()\n        # Otherwise, don't do anything -- ie, skip iteration\n        else:\n            print('fp16 dynamic loss scale overflow!')\n\n        # Update loss scale for next iteration\n        loss_scaler.update_scale(has_overflow)\n\n\"\"\"\n", "deepspeed/runtime/fp16/unfused_optimizer.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\"\"\"\nCopyright NVIDIA/apex\nThis file is adapted from FP16_Optimizer in NVIDIA/apex\n\"\"\"\n\nfrom deepspeed.moe.utils import split_params_grads_into_shared_and_expert_params\nimport torch\nfrom torch._utils import _flatten_dense_tensors\n\nfrom deepspeed.runtime.base_optimizer import DeepSpeedOptimizer\nfrom deepspeed.runtime.utils import get_global_norm, CheckOverflow, get_weight_norm\nfrom deepspeed.runtime.fp16.loss_scaler import INITIAL_LOSS_SCALE, SCALE_WINDOW, MIN_LOSS_SCALE\nfrom deepspeed.utils import logger\nfrom deepspeed.utils.torch import required_torch_version\nfrom deepspeed.checkpoint.constants import OPTIMIZER_STATE_DICT\nfrom deepspeed.accelerator import get_accelerator\nfrom deepspeed import comm as dist\n\n\nclass FP16_UnfusedOptimizer(DeepSpeedOptimizer):\n    \"\"\"\n    FP16 Optimizer without weight fusion to support LAMB optimizer\n\n    For usage example please see, TODO:  DeepSpeed V2 Tutorial\n    \"\"\"\n\n    def __init__(self,\n                 init_optimizer,\n                 deepspeed=None,\n                 static_loss_scale=1.0,\n                 dynamic_loss_scale=False,\n                 dynamic_loss_args=None,\n                 verbose=True,\n                 mpu=None,\n                 clip_grad=0.0,\n                 fused_lamb_legacy=False):\n\n        self.fused_lamb_legacy = fused_lamb_legacy\n        self._global_grad_norm = 0.\n\n        if dist.get_rank() == 0:\n            logger.info(f'Fused Lamb Legacy : {self.fused_lamb_legacy} ')\n\n        if not get_accelerator().is_available():\n            raise SystemError(\"Cannot use fp16 without accelerator.\")\n        self.optimizer = init_optimizer\n\n        # param groups\n        self.fp16_groups = []\n        self.fp32_groups = []\n\n        # loop to deal with groups\n        for i, param_group in enumerate(self.optimizer.param_groups):\n            #fp16 weights that represents the actual model weights\n            self.fp16_groups.append(param_group['params'])\n\n            #creating a fp32 copy of the weights that will be updated first then\n            #copied to fp16 weights\n            fp32_group = [p.clone().float().detach() for p in param_group['params']]\n\n            #in case the internal optimizer needs it\n            for p in fp32_group:\n                p.requires_grad = True\n\n            #setting the param groups in the optimizer to point to fp32\n            #note these are not the weights used by the model\n            #the model uses the fp16 version that we added to fp16_group\n            self.fp32_groups.append(fp32_group)\n            param_group['params'] = self.fp32_groups[i]\n\n        # we may have a way of fusing dynamic scale. Do not support for now\n        if dynamic_loss_scale:\n            self.dynamic_loss_scale = True\n            self.cur_iter = 0\n            self.last_overflow_iter = -1\n            self.scale_factor = 2.0\n            if dynamic_loss_args is None:\n                self.cur_scale = 1.0 * 2**16\n                self.scale_window = 1000\n                self.min_loss_scale = 0.25\n            else:\n                self.cur_scale = dynamic_loss_args[INITIAL_LOSS_SCALE]\n                self.scale_window = dynamic_loss_args[SCALE_WINDOW]\n                self.min_loss_scale = dynamic_loss_args[MIN_LOSS_SCALE]\n        else:\n            self.dynamic_loss_scale = False\n            self.cur_iter = 0\n            self.cur_scale = static_loss_scale\n\n        self.custom_loss_scaler = False\n        self.external_loss_scale = None\n\n        self.verbose = verbose\n\n        self.clip_grad = clip_grad\n        self.norm_type = 2\n\n        if required_torch_version(max_version=0.4):\n            self.clip_grad_norm = torch.nn.utils.clip_grad_norm\n        else:\n            self.clip_grad_norm = torch.nn.utils.clip_grad_norm_\n\n        self.mpu = mpu\n\n        self.overflow = False\n        self.overflow_checker = CheckOverflow(self.fp16_groups, mpu=self.mpu, deepspeed=deepspeed)\n\n        self.initialize_optimizer_states()\n\n    def zero_grad(self, set_to_none=True):\n        \"\"\"\n        Zero FP16 parameter grads.\n        \"\"\"\n        # FP32 grad should never exist outside of the step function\n        # For speed, set model fp16 grad to None by default\n        for group in self.fp16_groups:\n            for p in group:\n                if set_to_none:\n                    p.grad = None\n                else:\n                    if p.grad is not None:\n                        p.grad.detach_()\n                        p.grad.zero_()\n\n    def step_fused_lamb(self, closure=None):\n        \"\"\"\n        Not supporting closure.\n        \"\"\"\n        # First compute norm for all group so we know if there is overflow\n        grads_groups_flat = []\n        grads_groups = []\n        norm_groups = []\n        expert_norm_groups = []\n        for i, group in enumerate(self.fp16_groups):\n            grads = [\n                torch.zeros(p.size(), dtype=p.dtype, device=p.device) if p.grad is None else p.grad for p in group\n            ]\n            grads_groups.append(grads)\n            grads_groups_flat.append(_flatten_dense_tensors(grads))\n            grads_for_norm, expert_grads_for_norm = split_params_grads_into_shared_and_expert_params(group)\n            norm_group_value = 0.0\n            if len(grads_for_norm) > 0:\n                norm_group_value = get_weight_norm(_flatten_dense_tensors(grads_for_norm), mpu=self.mpu)\n            norm_groups.append(norm_group_value)\n            expert_norm_group_value = 0.0\n            if len(expert_grads_for_norm) > 0:\n                expert_norm_group_value = get_weight_norm(_flatten_dense_tensors(expert_grads_for_norm), mpu=self.mpu)\n            expert_norm_groups.append(expert_norm_group_value)\n\n        self.overflow = self.overflow_checker.check_using_norm(norm_groups + expert_norm_groups)\n        prev_scale = self.cur_scale\n\n        self._update_scale(self.overflow)\n        if self.overflow:\n            if self.verbose:\n                logger.info(\"[deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss \"\n                            \"scale: {}, reducing to {}\".format(prev_scale, self.cur_scale))\n            return self.overflow\n\n        self._global_grad_norm = get_global_norm(norm_list=norm_groups)\n        combined_scale = self.unscale_and_clip_grads(self._global_grad_norm, apply_scale=False)\n        self.optimizer.step(grads=grads_groups, output_params=self.fp16_groups, scale=combined_scale)\n\n        for fp32_group, fp16_group in zip(self.fp32_groups, self.fp16_groups):\n            for idx, (fp32_param, fp16_param) in enumerate(zip(fp32_group, fp16_group)):\n\n                #remove the fp32 grad\n                fp32_param.grad = None\n\n                #copy data from fp32 to fp16\n                fp16_param.data.copy_(fp32_param.data)\n\n        return self.overflow\n\n    def set_lr(self, lr):\n        \"\"\"Set the learning rate.\"\"\"\n        for param_group in self.optimizer.param_groups:\n            param_group[\"lr\"] = lr\n\n    def get_lr(self):\n        \"\"\"Return the current learning rate.\"\"\"\n        return self.optimizer.param_groups[0][\"lr\"]\n\n    def override_loss_scale(self, loss_scale):\n        if loss_scale != self.external_loss_scale:\n            logger.info(f'[deepspeed] setting loss scale from {self.external_loss_scale} -> {loss_scale}')\n        self.custom_loss_scaler = True\n        self.external_loss_scale = loss_scale\n\n    def step(self, closure=None):\n        \"\"\"\n        Not supporting closure.\n        \"\"\"\n\n        if self.fused_lamb_legacy:\n            return self.step_fused_lamb()\n\n        self.overflow = self.overflow_checker.check()\n        prev_scale = self.cur_scale\n\n        self._update_scale(self.overflow)\n        if self.overflow:\n            if self.verbose:\n                logger.info(\"[deepspeed] fp16 dynamic loss scale overflow! Skipping step. Attempted loss \"\n                            \"scale: {}, reducing to {}\".format(prev_scale, self.cur_scale))\n            return self.overflow\n\n        norm_groups = []\n        for i, group in enumerate(self.fp16_groups):\n            grads_for_norm, _ = split_params_grads_into_shared_and_expert_params(group)\n            norm_group_value = 0.0\n            if len(grads_for_norm) > 0:\n                norm_group_value = get_weight_norm(grads_for_norm, mpu=self.mpu)\n            norm_groups.append(norm_group_value)\n\n            # copying gradients to fp32 to work with fp32 parameters\n            for fp32_param, fp16_param in zip(self.fp32_groups[i], self.fp16_groups[i]):\n                if fp16_param.grad is None:\n                    fp32_param.grad = torch.zeros(fp16_param.size(), dtype=fp32_param.dtype, device=fp32_param.device)\n                else:\n                    fp32_param.grad = fp16_param.grad.to(fp32_param.dtype)\n\n        self._global_grad_norm = get_global_norm(norm_list=norm_groups)\n        self.unscale_and_clip_grads(self._global_grad_norm)\n\n        self.optimizer.step()\n\n        for fp32_group, fp16_group in zip(self.fp32_groups, self.fp16_groups):\n            for idx, (fp32_param, fp16_param) in enumerate(zip(fp32_group, fp16_group)):\n\n                #remove the fp32 grad\n                fp32_param.grad = None\n\n                #copy data from fp32 to fp16\n                fp16_param.data.copy_(fp32_param.data)\n\n        return self.overflow\n\n    def unscale_and_clip_grads(self, total_norm, apply_scale=True):\n        # compute combined scale factor for this group\n        combined_scale = self.cur_scale\n        if self.clip_grad > 0.:\n            # norm is in fact norm*scale\n            clip = ((total_norm / self.cur_scale) + 1e-6) / self.clip_grad\n            if clip > 1:\n                combined_scale = clip * self.cur_scale\n\n        if apply_scale:\n            for group in self.fp32_groups:\n                for param in group:\n                    if param.grad is not None:\n                        param.grad.data.mul_(1. / combined_scale)\n\n        return combined_scale\n\n    def backward(self, loss, create_graph=False, retain_graph=False):\n        \"\"\"\n        :attr:`backward` performs the following steps:\n\n        1. fp32_loss = loss.float()\n        2. scaled_loss = fp32_loss*loss_scale\n        3. scaled_loss.backward(), which accumulates scaled gradients into the ``.grad`` attributes of the model's fp16 leaves\n        \"\"\"\n        if self.custom_loss_scaler:\n            scaled_loss = self.external_loss_scale * loss\n            scaled_loss.backward()\n        else:\n            scaled_loss = (loss.float()) * self.cur_scale\n            scaled_loss.backward(create_graph=create_graph, retain_graph=retain_graph)\n\n    def _update_scale(self, skip):\n        if self.dynamic_loss_scale:\n            prev_scale = self.cur_scale\n            if skip:\n                self.cur_scale = max(self.cur_scale / self.scale_factor, self.min_loss_scale)\n                self.last_overflow_iter = self.cur_iter\n                if self.verbose:\n                    logger.info(\"Grad overflow on iteration: %s\", self.cur_iter)\n                    logger.info(f\"Reducing dynamic loss scale from {prev_scale} to {self.cur_scale}\")\n            else:\n                # Ensure self.scale_window updates since last overflow\n                stable_interval = (self.cur_iter - self.last_overflow_iter) - 1\n                if (stable_interval > 0) and (stable_interval % self.scale_window == 0):\n                    self.cur_scale *= self.scale_factor\n                    if self.verbose:\n                        logger.info(f\"No Grad overflow for {self.scale_window} iterations\")\n                        logger.info(f\"Increasing dynamic loss scale from {prev_scale} to {self.cur_scale}\")\n        else:\n            if skip:\n                logger.info(\"Grad overflow on iteration %s\", self.cur_iter)\n                logger.info(\"Using static loss scale of %s\", self.cur_scale)\n        self.cur_iter += 1\n        return\n\n    # Promote state so it can be retrieved or set via \"fp16_optimizer_instance.state\"\n    def _get_state(self):\n        return self.optimizer.state\n\n    def _set_state(self, value):\n        self.optimizer.state = value\n\n    state = property(_get_state, _set_state)\n\n    # Promote param_groups so it can be retrieved or set via \"fp16_optimizer_instance.param_groups\"\n    # (for example, to adjust the learning rate)\n    def _get_param_groups(self):\n        return self.optimizer.param_groups\n\n    def _set_param_groups(self, value):\n        self.optimizer.param_groups = value\n\n    param_groups = property(_get_param_groups, _set_param_groups)\n\n    # Promote loss scale so it can be retrieved or set via \"fp16_optimizer_instance.loss_scale\"\n    def _get_loss_scale(self):\n        if self.custom_loss_scaler:\n            return self.external_loss_scale\n        else:\n            return self.cur_scale\n\n    def _set_loss_scale(self, value):\n        self.loss_scaler.cur_scale = value\n\n    loss_scale = property(_get_loss_scale, _set_loss_scale)\n\n    def state_dict(self):\n        \"\"\"\n        Returns a dict containing the current state of this :class:`FP16_Optimizer` instance.\n        This dict contains attributes of :class:`FP16_Optimizer`, as well as the state_dict\n        of the contained Pytorch optimizer.\n        Example::\n            checkpoint = {}\n            checkpoint['model'] = model.state_dict()\n            checkpoint['optimizer'] = optimizer.state_dict()\n            torch.save(checkpoint, \"saved.pth\")\n        \"\"\"\n        state_dict = {}\n        state_dict['dynamic_loss_scale'] = self.dynamic_loss_scale\n        state_dict['cur_scale'] = self.cur_scale\n        state_dict['cur_iter'] = self.cur_iter\n        if state_dict['dynamic_loss_scale']:\n            state_dict['last_overflow_iter'] = self.last_overflow_iter\n            state_dict['scale_factor'] = self.scale_factor\n            state_dict['scale_window'] = self.scale_window\n        state_dict[OPTIMIZER_STATE_DICT] = self.optimizer.state_dict()\n        state_dict['fp32_groups'] = self.fp32_groups\n        return state_dict\n\n    # Refresh fp32 master params from fp16 copies\n    def refresh_fp32_params(self):\n        for current_group, saved_group in zip(self.fp32_groups, self.fp16_groups):\n            for current, saved in zip(current_group, saved_group):\n                current.data.copy_(saved.data)\n\n    def load_state_dict(self, state_dict, load_optimizer_states=True):\n        \"\"\"\n        Loads a state_dict created by an earlier call to state_dict().\n        If ``fp16_optimizer_instance`` was constructed from some ``init_optimizer``,\n        whose parameters in turn came from ``model``, it is expected that the user\n        will call ``model.load_state_dict()`` before\n        ``fp16_optimizer_instance.load_state_dict()`` is called.\n        Example::\n            model = torch.nn.Linear(D_in, D_out).to(get_accelerator().device_name()).half()\n            optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n            optimizer = FP16_Optimizer(optimizer, static_loss_scale = 128.0)\n            ...\n            checkpoint = torch.load(\"saved.pth\")\n            model.load_state_dict(checkpoint['model'])\n            optimizer.load_state_dict(checkpoint['optimizer'])\n        \"\"\"\n        # I think it should actually be ok to reload the optimizer before the model.\n        self.dynamic_loss_scale = state_dict['dynamic_loss_scale']\n        self.cur_scale = state_dict['cur_scale']\n        self.cur_iter = state_dict['cur_iter']\n        if state_dict['dynamic_loss_scale']:\n            self.last_overflow_iter = state_dict['last_overflow_iter']\n            self.scale_factor = state_dict['scale_factor']\n            self.scale_window = state_dict['scale_window']\n\n        if load_optimizer_states:\n            self.optimizer.load_state_dict(state_dict[OPTIMIZER_STATE_DICT])\n        # At this point, the optimizer's references to the model's fp32 parameters are up to date.\n        # The optimizer's hyperparameters and internal buffers are also up to date.\n        # However, the fp32 master copies of the model's fp16 params stored by the optimizer are still\n        # out of date.  There are two options.\n        # 1:  Refresh the master params from the model's fp16 params.\n        # This requires less storage but incurs precision loss.\n        # 2:  Save and restore the fp32 master copies separately.\n        # We choose option 2.\n        #\n        # Pytorch Optimizer.load_state_dict casts saved buffers (e.g. momentum) to the type and device\n        # of their associated parameters, because it's possible those buffers might not exist yet in\n        # the current optimizer instance.  In our case, as long as the current FP16_Optimizer has been\n        # constructed in the same way as the one whose state_dict we are loading, the same master params\n        # are guaranteed to exist, so we can just copy_() from the saved master params.\n        for current_group, saved_group in zip(self.fp32_groups, state_dict['fp32_groups']):\n            for current, saved in zip(current_group, saved_group):\n                current.data.copy_(saved.data)\n\n    def __repr__(self):\n        return repr(self.optimizer)\n\n    def initialize_optimizer_states(self):\n        for i, group in enumerate(self.fp16_groups):\n            for param in group:\n                param.grad = torch.zeros(param.size(),\n                                         dtype=param.dtype,\n                                         device=get_accelerator().current_device_name())\n\n        for i, group in enumerate(self.fp32_groups):\n            for param in group:\n                param.grad = torch.zeros(param.size(),\n                                         dtype=param.dtype,\n                                         device=get_accelerator().current_device_name())\n\n        self.optimizer.step()\n\n        for i, group in enumerate(self.fp16_groups):\n            for param in group:\n                param.grad = None\n\n        for i, group in enumerate(self.fp32_groups):\n            for param in group:\n                param.grad = None\n", "deepspeed/runtime/fp16/onebit/lamb.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport types\nimport torch\nimport numpy as np\nfrom deepspeed import comm as dist\nfrom deepspeed.utils.torch import required_torch_version\nfrom torch._utils import _flatten_dense_tensors, _unflatten_dense_tensors\nfrom deepspeed.accelerator import get_accelerator\n\n\nclass OnebitLamb(torch.optim.Optimizer):\n    \"\"\"Implements the 1-bit Lamb algorithm. Currently GPU-only.\n    For usage example please see https://www.deepspeed.ai/tutorials/onebit-lamb/\n    For technical details please see our paper https://arxiv.org/abs/2104.06069.\n\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups.\n        lr (float, optional): learning rate. (default: 1e-3)\n        freeze_step (int, optional): Number of steps for warmup (uncompressed)\n            stage before we start using compressed communication. (default 100000)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square. (default: (0.9, 0.999))\n        eps (float, optional): term added to the denominator to improve\n            numerical stability. (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        max_coeff(float, optional): maximum value of the lamb coefficient (default: 10.0)\n        min_coeff(float, optional): minimum value of the lamb coefficient (default: 0.01)\n        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n            algorithm from the paper `On the Convergence of Adam and Beyond`_\n            (default: False) NOT SUPPORTED in 1-bit Lamb!\n        eps_inside_sqrt (boolean, optional): in the 'update parameters' step,\n            adds eps to the bias-corrected second moment estimate before\n            evaluating square root instead of adding it to the square root of\n            second moment estimate as in the original paper. (default: False)\n        cuda_aware (boolean, required): Set True if the underlying MPI implementation\n            supports CUDA-Aware communication. (default: False)\n        comm_backend_name (string, optional): Set to 'mpi' if needed. (default: 'nccl')\n        coeff_beta (float, optional): coefficient used for computing\n            running averages of lamb coefficient (default: 0.9) note that you may want to\n            increase or decrease this beta depending on the freeze_step you choose, as\n            1/(1 - coeff_beta) should be smaller than or equal to freeze_step\n        factor_max (float, optional): maximum value of scaling factor to the frozen lamb\n            coefficient during compression stage (default: 4.0)\n        factor_min (float, optional): minimum value of scaling factor to the frozen lamb\n            coefficient during compression stage (default: 0.5)\n        factor_threshold (float, optional): threshold of how much the scaling factor can\n            fluctuate between steps (default: 0.1)\n    .. _Large Batch Optimization for Deep Learning\\\\: Training BERT in 76 minutes:\n        https://arxiv.org/abs/1904.00962\n    .. _Adam\\\\: A Method for Stochastic Optimization:\n        https://arxiv.org/abs/1412.6980\n    .. _On the Convergence of Adam and Beyond:\n        https://openreview.net/forum?id=ryQu7f-RZ\n    \"\"\"\n\n    def __init__(self,\n                 params,\n                 deepspeed=None,\n                 lr=1e-3,\n                 freeze_step=100000,\n                 bias_correction=True,\n                 betas=(0.9, 0.999),\n                 eps=1e-8,\n                 eps_inside_sqrt=False,\n                 weight_decay=0.,\n                 max_grad_norm=0.,\n                 max_coeff=10.0,\n                 min_coeff=0.01,\n                 amsgrad=False,\n                 cuda_aware=False,\n                 comm_backend_name='nccl',\n                 coeff_beta=0.9,\n                 factor_max=4.0,\n                 factor_min=0.5,\n                 factor_threshold=0.1):\n\n        if amsgrad:\n            raise RuntimeError('1-bit Lamb does not support the AMSGrad variant.')\n\n        defaults = dict(lr=lr,\n                        bias_correction=bias_correction,\n                        betas=betas,\n                        eps=eps,\n                        weight_decay=weight_decay,\n                        max_grad_norm=max_grad_norm,\n                        max_coeff=max_coeff,\n                        min_coeff=min_coeff)\n\n        super(OnebitLamb, self).__init__(params, defaults)\n        self.eps_mode = 0 if eps_inside_sqrt else 1\n        self.deepspeed = deepspeed\n        self.lamb_freeze_key = False\n        self.initialize = False\n        self.freeze_step = freeze_step\n        self.cuda_aware = cuda_aware\n        self.coeff_beta = coeff_beta\n        self.factor_max = factor_max\n        self.factor_min = factor_min\n        self.factor_threshold = factor_threshold\n        self.using_pipeline = False\n\n        self.comm_backend_name = comm_backend_name\n\n        assert dist.is_initialized(), \"Please initialize the torch distributed backend.\"\n        # Empty initializer. Set handle based on the comm backend as follows.\n        self.comm_backend_handle = None\n        if self.comm_backend_name == 'nccl':\n            assert (\n                required_torch_version(min_version=1.8)\n            ), \"Please use torch 1.8 or greater to enable NCCL backend in 1-bit Adam. Alternatively, please specify 'mpi' as the 'comm_backend_name' in config file to proceed with the MPI backend\"\n            from deepspeed.runtime.comm.nccl import NcclBackend\n            self.using_pipeline = hasattr(self.deepspeed, 'pipeline_enable_backward_allreduce')\n            self.comm_backend_handle = NcclBackend(self.deepspeed.mpu)\n        elif self.comm_backend_name == 'mpi':\n            from deepspeed.runtime.comm.mpi import MpiBackend\n            self.comm_backend_handle = MpiBackend(cuda_aware)\n        elif self.comm_backend_name == 'hccl':\n            from deepspeed.runtime.comm.hccl import HcclBackend\n            self.using_pipeline = hasattr(self.deepspeed, 'pipeline_enable_backward_allreduce')\n            self.comm_backend_handle = HcclBackend(self.deepspeed.mpu)\n        elif self.comm_backend_name == 'compressed':\n            from deepspeed.runtime.comm.compressed import CompressedBackend\n            self.using_pipeline = hasattr(self.deepspeed, 'pipeline_enable_backward_allreduce')\n            self.comm_backend_handle = CompressedBackend(self.deepspeed.mpu)\n\n        self.size = self.comm_backend_handle.size\n\n        self.divider = int(self.size * 8 / np.gcd(self.size, 8))\n\n        self.exp_avg_flat = []\n        self.dummy_exp_avg = {}\n        self.corrected_tensor_sizes = []\n        self.server_chunk_sizes = []\n        self.worker_errors = []\n        self.server_errors = []\n\n        self.lamb_coeffs = []\n\n    def step(self, closure=None, grads=None):\n        \"\"\"Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n            grads (list of tensors, optional): weight gradient to use for the\n                optimizer update. If gradients have type torch.half, parameters\n                are expected to be in type torch.float. (default: None)\n        \"\"\"\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        if grads is None:\n            grads_group = [None] * len(self.param_groups)\n        # backward compatibility\n        # assuming a list/generator of parameter means single group\n        elif isinstance(grads, types.GeneratorType):\n            grads_group = [grads]\n        elif type(grads[0]) != list:\n            grads_group = [grads]\n        else:\n            grads_group = grads\n\n        # remove the previous stats\n        del self.lamb_coeffs[:]\n\n        if self.lamb_freeze_key:\n            exp_avg_last_step = []\n            for group in self.param_groups:\n                exp_avg_last_step.append([self.state[p]['exp_avg'].detach().clone() for p in group['params']])\n            if 'scaling_coeff' not in self.state[self.param_groups[0]['params'][0]]:\n                # Compute the scaling_coeff for each momentum at the end of warmup stage.\n                # This is used to reduce compression error during compression stage.\n                momentum_scales = []\n                for group in self.param_groups:\n                    momentum_scales.append([(torch.linalg.norm(self.state[p]['exp_avg']) /\n                                             np.sqrt(torch.numel(self.state[p]['exp_avg']))).item()\n                                            for p in group['params']])\n                united_scale = sum([sum(x) for x in momentum_scales]) / sum([len(x) for x in momentum_scales])\n                for i, group in enumerate(self.param_groups):\n                    for j, p in enumerate(group['params']):\n                        self.state[p]['scaling_coeff'] = united_scale / momentum_scales[i][j]\n\n        for group, grads_this_group in zip(self.param_groups, grads_group):\n            if grads_this_group is None:\n                grads_this_group = [None] * len(group['params'])\n\n            bias_correction = 1 if group['bias_correction'] else 0\n\n            for p, grad in zip(group['params'], grads_this_group):\n                if p.grad is None and grad is None:\n                    continue\n                if grad is None:\n                    grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError('1-bit Lamb does not support sparse gradients')\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0 or (len(state) == 1 and 'scaling_coeff' in state.keys()):\n                    state['step'] = 0\n                    state['lamb_coeff_freeze'] = 0.0\n                    state['last_factor'] = 1.0\n                    # Exponential moving average of gradient values\n                    state['exp_avg'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n                    state['exp_avg_sq_fresh'] = torch.zeros_like(p.data)\n\n                if not self.initialize:\n                    self.lamb_freeze_key = True\n\n                exp_avg, exp_avg_sq, exp_avg_sq_fresh = state['exp_avg'], state['exp_avg_sq'], state[\n                    'exp_avg_sq_fresh']\n                beta1, beta2 = group['betas']\n                max_coeff = group['max_coeff']\n                min_coeff = group['min_coeff']\n\n                state['step'] += 1\n\n                if self.lamb_freeze_key is False:\n                    # warmup stage, baseline Lamb optimization\n                    exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                    if state['step'] == self.freeze_step:\n                        exp_avg_sq_fresh.data = exp_avg_sq.detach().clone()\n                    grad = None\n                    if self.initialize:\n                        weight_norm = p.data.pow(2).sum().sqrt()\n                        update = exp_avg / (exp_avg_sq.sqrt() + group['eps'])\n                        if group['weight_decay'] > 0.0:\n                            update += group['weight_decay'] * p.data\n                        update_norm = update.pow(2).sum().sqrt()\n                        lamb_coeff = 1.0\n                        if weight_norm != 0 and update_norm != 0:\n                            lamb_coeff = (weight_norm / update_norm).item()\n                            if lamb_coeff > max_coeff:\n                                lamb_coeff = max_coeff\n                            if lamb_coeff < min_coeff:\n                                lamb_coeff = min_coeff\n                        if lamb_coeff != 1.0:\n                            state['lamb_coeff_freeze'] = self.coeff_beta * state['lamb_coeff_freeze'] + (\n                                1 - self.coeff_beta) * lamb_coeff\n                        self.lamb_coeffs.append(lamb_coeff)\n                        with torch.no_grad():\n                            p.add_(-group['lr'] * lamb_coeff * update)\n                else:\n                    # compression stage, update each momentum locally, then\n                    # communicate based on the compressed_allreduce below\n                    if self.initialize:\n                        exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                        exp_avg.mul_(self.state[p]['scaling_coeff'])\n                    grad = None\n\n        # init fused momentum\n        if len(self.exp_avg_flat) == 0:\n            momentum_groups = []\n            tensor_size = 0\n            for group in self.param_groups:\n                for p in group['params']:\n                    momentum_groups.append(self.state[p]['exp_avg'])\n                    tensor_size += torch.numel(p.data)\n            corrected_tensor_size = tensor_size\n            if tensor_size % (self.size * self.divider) != 0:\n                difference = ((self.size * self.divider) - (tensor_size % (self.size * self.divider)))\n                corrected_tensor_size += difference\n                self.dummy_exp_avg[0] = torch.zeros(difference, device=momentum_groups[0].data.device)\n                momentum_groups.append(self.dummy_exp_avg[0])\n            self.corrected_tensor_sizes.append(corrected_tensor_size)\n            self.server_chunk_sizes.append(corrected_tensor_size // self.size)\n\n            self.exp_avg_flat.append(_flatten_dense_tensors([p.detach().clone() for p in momentum_groups]))\n            updated_params = _unflatten_dense_tensors(self.exp_avg_flat[0], momentum_groups)\n            for p, q in zip(momentum_groups, updated_params):\n                p.data = q.data\n\n        if self.initialize and len(self.worker_errors) == 0:\n            get_accelerator().empty_cache()\n            for i in range(len(self.exp_avg_flat)):\n                self.worker_errors.append(\n                    torch.zeros(self.corrected_tensor_sizes[i], device=self.exp_avg_flat[i].device))\n                self.server_errors.append(torch.zeros(self.server_chunk_sizes[i], device=self.exp_avg_flat[i].device))\n            get_accelerator().empty_cache()\n\n        if self.lamb_freeze_key:\n            if self.size > 1:\n                for i in range(len(self.exp_avg_flat)):\n                    if not self.initialize:\n                        get_accelerator().empty_cache()\n                        self.worker_errors.append(\n                            torch.zeros(self.corrected_tensor_sizes[i], device=self.exp_avg_flat[i].device))\n                        self.server_errors.append(\n                            torch.zeros(self.server_chunk_sizes[i], device=self.exp_avg_flat[i].device))\n                        get_accelerator().empty_cache()\n                        if dist.get_rank() == 0:\n                            print(\"Cupy Buffers Initialized Successfully.\")\n\n                        self.comm_backend_handle.compressed_allreduce(self.exp_avg_flat[i], self.worker_errors[0],\n                                                                      self.server_errors[0], self.deepspeed.local_rank)\n\n                        if dist.get_rank() == 0:\n                            print('Pop out errors', flush=True)\n                        del self.worker_errors[:]\n                        del self.server_errors[:]\n                    else:\n                        self.comm_backend_handle.compressed_allreduce(self.exp_avg_flat[i], self.worker_errors[i],\n                                                                      self.server_errors[i], self.deepspeed.local_rank)\n\n        if self.lamb_freeze_key and self.initialize:\n            for i, group in enumerate(self.param_groups):\n                bias_correction = 1 if group['bias_correction'] else 0\n\n                for j, p in enumerate(group['params']):\n                    state = self.state[p]\n                    exp_avg, exp_avg_sq, exp_avg_sq_fresh = state['exp_avg'], state['exp_avg_sq'], state[\n                        'exp_avg_sq_fresh']\n                    beta1, beta2 = group['betas']\n                    exp_avg.div_(self.state[p]['scaling_coeff'])\n                    # Because 1-bit compression cannot represent exact zero, it is required to\n                    # provide a momentum mask for those params that have constant exact zeros in their\n                    # momentums, otherwise the compression error would keep accumulating.\n                    # For example, for BERT pre-training seq 128, bert.embeddings.position_embeddings.weight\n                    # always have exact zeros in its momentum for row 129 to 512, because it only\n                    # learns up to seq length 128 while the model supports up to 512 seq length.\n                    # (See example in DeepSpeedExamples/bing_bert/deepspeed_train.py about how\n                    # to add this exp_avg_mask for BERT pre-training.)\n                    if 'exp_avg_mask' in group:\n                        if exp_avg.device != group['exp_avg_mask'].device:\n                            group['exp_avg_mask'] = group['exp_avg_mask'].to(device=exp_avg.device)\n                        exp_avg.mul_(group['exp_avg_mask'])\n\n                    grad_reconstruct = ((exp_avg - exp_avg_last_step[i][j] * beta1) / (1 - beta1))\n                    exp_avg_sq_fresh.mul_(beta2).addcmul_(1 - beta2, grad_reconstruct, grad_reconstruct)\n                    denom = exp_avg_sq.sqrt() + group['eps']\n                    update_prelim = exp_avg / denom\n\n                    if group['weight_decay'] > 0.0:\n                        update = update_prelim + group['weight_decay'] * p.data\n                    else:\n                        update = update_prelim\n\n                    lamb_coeff = 1.0\n                    update_norm = update.pow(2).sum().sqrt()\n                    denom_real = exp_avg_sq_fresh.sqrt() + group['eps']\n                    factor = (denom / denom_real).max().item()\n                    if group['weight_decay'] > 0.0:\n                        update_ratio = min(1.0, (update_prelim.pow(2).sum().sqrt() / update_norm).item())\n                        factor = factor * update_ratio + (1.0 - update_ratio)\n                    if factor > self.factor_max:\n                        factor = self.factor_max\n                    if factor < self.factor_min:\n                        factor = self.factor_min\n                    if factor > state['last_factor'] * (1.0 + self.factor_threshold):\n                        factor = state['last_factor'] * (1.0 + self.factor_threshold)\n                    if factor < state['last_factor'] * (1.0 - self.factor_threshold):\n                        factor = state['last_factor'] * (1.0 - self.factor_threshold)\n                    state['last_factor'] = factor\n                    lamb_coeff = state['lamb_coeff_freeze'] * factor\n                    self.lamb_coeffs.append(lamb_coeff)\n                    with torch.no_grad():\n                        p.add_(-group['lr'] * lamb_coeff * update)\n            del exp_avg_last_step[:]\n            exp_avg_last_step = None\n\n        if not self.initialize:\n            self.lamb_freeze_key = False\n            self.initialize = True\n            print(f\"Finished the initialization step at rank {dist.get_rank()}\")\n            return loss\n\n        if self.lamb_freeze_key is False:\n            if state['step'] >= self.freeze_step:\n                print('OnebitLamb - starting compressed communication')\n                self.lamb_freeze_key = True\n                if self.using_pipeline:\n                    self.deepspeed.pipeline_enable_backward_allreduce = False\n                else:\n                    self.deepspeed.enable_backward_allreduce = False\n\n        return loss\n\n    def load_state_dict(self, state_dict):\n        \"\"\"\n        Overrides load_state_dict() to add special handling when loading checkpoints\n        \"\"\"\n        # Because at different stage exp_avg_mask may change (e.g.,\n        # BERT pre-training seqlen 128 and 512 ), we don't use the exp_avg_mask\n        # in checkpoints but always use the one user provided in training script.\n        # (See example in DeepSpeedExamples/bing_bert/deepspeed_train.py.)\n        # Thus here we keep the exp_avg_mask unchanged when loading checkpoint\n        for i, group in enumerate(self.param_groups):\n            if 'exp_avg_mask' in group:\n                state_dict['param_groups'][i]['exp_avg_mask'] = group['exp_avg_mask']\n            elif 'exp_avg_mask' not in group and 'exp_avg_mask' in state_dict['param_groups'][i]:\n                state_dict['param_groups'][i].pop('exp_avg_mask')\n        super().load_state_dict(state_dict)\n        # need to reset the fused momentum since loading states will break the linking\n        del self.exp_avg_flat[:]\n        self.dummy_exp_avg.clear()\n        del self.corrected_tensor_sizes[:]\n        del self.server_chunk_sizes[:]\n        if self.state[self.param_groups[0]['params'][0]]['step'] < self.freeze_step:\n            if dist.get_rank() == 0:\n                print(\"Checkpoint loaded and OnebitLamb warmup stage starts/continues.\")\n            if self.lamb_freeze_key is True:\n                self.lamb_freeze_key = False\n                if self.using_pipeline:\n                    self.deepspeed.pipeline_enable_backward_allreduce = True\n                else:\n                    self.deepspeed.enable_backward_allreduce = True\n            for group in self.param_groups:\n                for p in group['params']:\n                    self.state[p]['lamb_coeff_freeze'] = 0.0\n                    self.state[p]['last_factor'] = 1.0\n                    if 'scaling_coeff' in self.state[p]:\n                        self.state[p].pop('scaling_coeff')\n        else:\n            if dist.get_rank() == 0:\n                print(\"Checkpoint loaded and OnebitLamb compression stage starts/continues.\")\n            if self.lamb_freeze_key is False:\n                self.lamb_freeze_key = True\n                if self.using_pipeline:\n                    self.deepspeed.pipeline_enable_backward_allreduce = False\n                else:\n                    self.deepspeed.enable_backward_allreduce = False\n        # We reset the compression errors when loading checkpoints for 3 reasons:\n        # 1) The worker and server error at each GPU are distinct, so in current implementation\n        # only rank 0's errors are saved in the checkpoint. Thus we have to reset the errors.\n        # If we want to save them correctly we need O(num_gpu*model_size) memory in order to\n        # gather all the error, which is a very large memory requirement. It's possible to save\n        # them in a distributed way, but it will make the checkpoint saving/loading much more complicated.\n        # 2) Even if we are able to save the compression errors correctly, you need to have the\n        # exact same number of GPUs in order to load them correctly.\n        # 3) We verified on BERT pre-training that occasionally resetting the compression error\n        # at checkpoint loading does not affect the convergence.\n        # However, please avoid frequent checkpoint loading which could break the error\n        # compensation mechanism thus affect the convergence.\n        del self.worker_errors[:]\n        del self.server_errors[:]\n\n    def get_lamb_coeffs(self):\n        return self.lamb_coeffs\n", "deepspeed/runtime/fp16/onebit/zoadam.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport types\nimport torch\nimport numpy as np\nfrom deepspeed.accelerator import get_accelerator\nfrom deepspeed.utils.torch import required_torch_version\nfrom deepspeed import comm as dist\n\n\nclass ZeroOneAdam(torch.optim.Optimizer):\n    \"\"\"Implements the 0/1 Adam algorithm. Currently GPU-only.\n    For usage example please see https://www.deepspeed.ai/tutorials/zero-one-adam/\n    For technical details please read https://arxiv.org/abs/2202.06009\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups.\n        lr (float, optional): learning rate. (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square. (default: (0.9, 0.999))\n        eps (float, optional): term added to the denominator to improve\n            numerical stability. (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        var_freeze_step (int, optional): The latest step to update the variance,\n            using the notation from https://arxiv.org/abs/2202.06009, it denotes the\n            max{i|i in T_v}. Note that this is different from the freeze step from the\n            1-bit Adam. The var_freeze_step is usually the end of the learning rate warmup\n            and thus does not require tuning. (default: 100000)\n        var_update_scaler (int, optional): The interval to update the variance. Note that\n            the update policy for variance follows an exponential rule, where var_update_scaler\n            denotes the kappa in the 0/1 Adam paper. (default: 16)\n        local_step_scaler (int, optional): The interval to scale the local steps interval\n            according to the learning rate policy. (default: 32678)\n        local_step_clipper (int, optional): The largest interval for local steps with\n            learning rate policy. This corresponds to the variable H in the 0/1 Adam paper.\n            (default: 16)\n        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n            algorithm from the paper `On the Convergence of Adam and Beyond`_\n            (default: False) NOT SUPPORTED in 0/1 Adam!\n        eps_inside_sqrt (boolean, optional): in the 'update parameters' step,\n            adds eps to the bias-corrected second moment estimate before\n            evaluating square root instead of adding it to the square root of\n            second moment estimate as in the original paper. (default: False)\n        cuda_aware (boolean, required): Set True if the underlying MPI implementation\n            supports CUDA-Aware communication. (default: False)\n        comm_backend_name (string, optional): Set to 'mpi' if needed. (default: 'nccl')\n    .. _Adam\\\\: A Method for Stochastic Optimization:\n        https://arxiv.org/abs/1412.6980\n    .. _On the Convergence of Adam and Beyond:\n        https://openreview.net/forum?id=ryQu7f-RZ\n    \"\"\"\n\n    def __init__(self,\n                 params,\n                 deepspeed=None,\n                 lr=1e-3,\n                 bias_correction=True,\n                 betas=(0.9, 0.999),\n                 eps=1e-8,\n                 eps_inside_sqrt=False,\n                 weight_decay=0.,\n                 max_grad_norm=0.,\n                 var_freeze_step=100000,\n                 var_update_scaler=16,\n                 local_step_scaler=32678,\n                 local_step_clipper=16,\n                 amsgrad=False,\n                 cuda_aware=False,\n                 comm_backend_name='nccl'):\n\n        if amsgrad:\n            raise RuntimeError('0/1 Adam does not support the AMSGrad variant.')\n\n        defaults = dict(lr=lr,\n                        bias_correction=bias_correction,\n                        betas=betas,\n                        eps=eps,\n                        weight_decay=weight_decay,\n                        max_grad_norm=max_grad_norm)\n\n        super(ZeroOneAdam, self).__init__(params, defaults)\n        self.eps_mode = 0 if eps_inside_sqrt else 1\n        self.deepspeed = deepspeed\n        self.initialize = False\n        self.cuda_aware = cuda_aware\n        self.using_pipeline = False\n\n        self.var_freeze_step = var_freeze_step\n        self.var_update_scaler = var_update_scaler\n        self.local_step_scaler = local_step_scaler\n        self.local_step_clipper = local_step_clipper\n        self.freeze_key = False\n        self.reinitial_error_buffer = False\n\n        self.comm_backend_name = comm_backend_name\n\n        assert dist.is_initialized(), \"Please initialize the torch distributed backend.\"\n        # Empty initializer. Set handle based on the comm backend as follows.\n        self.comm_backend_handle = None\n        if self.comm_backend_name == 'nccl':\n            assert (\n                required_torch_version(min_version=1.8)\n            ), \"Please use torch 1.8 or greater to enable NCCL backend in 0/1 Adam. Alternatively, please specify 'mpi' as the 'comm_backend_name' in config file to proceed with the MPI backend\"\n            from deepspeed.runtime.comm.nccl import NcclBackend\n            self.using_pipeline = hasattr(self.deepspeed, 'pipeline_enable_backward_allreduce')\n            self.comm_backend_handle = NcclBackend(self.deepspeed.mpu)\n        elif self.comm_backend_name == 'mpi':\n            from deepspeed.runtime.comm.mpi import MpiBackend\n            self.comm_backend_handle = MpiBackend(cuda_aware)\n        elif self.comm_backend_name == 'hccl':\n            from deepspeed.runtime.comm.hccl import HcclBackend\n            self.using_pipeline = hasattr(self.deepspeed, 'pipeline_enable_backward_allreduce')\n            self.comm_backend_handle = HcclBackend(self.deepspeed.mpu)\n        elif self.comm_backend_name == 'compressed':\n            from deepspeed.runtime.comm.compressed import CompressedBackend\n            self.using_pipeline = hasattr(self.deepspeed, 'pipeline_enable_backward_allreduce')\n            self.comm_backend_handle = CompressedBackend(self.deepspeed.mpu)\n        self.size = self.comm_backend_handle.size\n\n        self.divider = int(self.size * 8 / np.gcd(self.size, 8))\n\n    def step(self, closure=None, grads=None):\n        \"\"\"Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n            grads (list of tensors, optional): weight gradient to use for the\n                optimizer update. If gradients have type torch.half, parameters\n                are expected to be in type torch.float. (default: None)\n            output params (list of tensors, optional): A reduced precision copy\n                of the updated weights written out in addition to the regular\n                updated weights. Have to be of same type as gradients. (default: None)\n            scale (float, optional): factor to divide gradient tensor values\n                by before applying to weights. (default: 1)\n        \"\"\"\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        if grads is None:\n            grads_group = [None] * len(self.param_groups)\n        # backward compatibility\n        # assuming a list/generator of parameter means single group\n        elif isinstance(grads, types.GeneratorType):\n            grads_group = [grads]\n        elif type(grads[0]) != list:\n            grads_group = [grads]\n        else:\n            grads_group = grads\n\n        for group, grads_this_group in zip(self.param_groups, grads_group):\n            if grads_this_group is None:\n                grads_this_group = [None] * len(group['params'])\n\n            bias_correction = 1 if group['bias_correction'] else 0\n\n            for p, grad in zip(group['params'], grads_this_group):\n                if p.grad is None and grad is None:\n                    continue\n                if grad is None:\n                    grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError('0/1 Adam does not support sparse gradients')\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state['step'] = 0\n                    # Exponential moving average of gradient values\n                    state['exp_avg'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n\n                if not self.initialize or 'worker_error' not in state.keys():\n                    # Some scalars to help scale the variance update/local step policies\n                    state['var_interval'] = 1\n                    state['var_counter'] = 0\n                    state['local_step_interval'] = 1\n                    state['local_step_counter'] = 0\n                    state['lrs'] = 0\n                    state['tensor_size'] = torch.numel(p.data)\n                    state['corrected_tensor_size'] = state['tensor_size']\n\n                    if state['tensor_size'] % (self.size * self.divider) != 0:\n                        state['corrected_tensor_size'] += ((self.size * self.divider) - (state['tensor_size'] %\n                                                                                         (self.size * self.divider)))\n                    state['server_chunk_size'] = state['corrected_tensor_size'] // self.size\n                    get_accelerator().empty_cache()\n                    state['worker_error'] = torch.zeros(state['corrected_tensor_size'], device=p.device)\n                    state['server_error'] = torch.zeros(state['server_chunk_size'], device=p.device)\n                    # Accumulation of momentum, i.e., the u variable in the 0/1 Adam paper\n                    state['momentum_accumulator'] = torch.zeros_like(p.data)\n                    get_accelerator().empty_cache()\n                    # self.freeze_key = True\n                    if not self.initialize and dist.get_rank() == 0:\n                        print(\"Cupy Buffers Initialized Successfully.\")\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                comm_buffer = state['momentum_accumulator']\n                beta1, beta2 = group['betas']\n\n                state['step'] += 1\n\n                if self.initialize:\n                    if self.freeze_key is False:\n                        if state['step'] % state['var_interval'] == 0:\n                            exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                            exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n                        else:\n                            if self.size > 1:\n                                with torch.no_grad():\n                                    grad_onebit = self.comm_backend_handle.compressed_allreduce(\n                                        grad, state['worker_error'], state['server_error'], self.deepspeed.local_rank)\n                                    if 'exp_avg_mask' in group:\n                                        if grad_onebit.device != group['exp_avg_mask'].device:\n                                            group['exp_avg_mask'] = group['exp_avg_mask'].to(device=grad_onebit.device)\n                                        grad_onebit.mul_(group['exp_avg_mask'])\n                                    exp_avg.mul_(beta1).add_(1 - beta1, grad_onebit)\n                    else:\n                        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n                        state['lrs'] += group['lr']\n                    grad = None\n\n                if not self.initialize:\n                    if self.size > 1:\n                        comm_buffer.set_(\n                            self.comm_backend_handle.compressed_allreduce(comm_buffer, state['worker_error'],\n                                                                          state['server_error'],\n                                                                          self.deepspeed.local_rank))\n                        if 'exp_avg_mask' in group:\n                            if comm_buffer.device != group['exp_avg_mask'].device:\n                                group['exp_avg_mask'] = group['exp_avg_mask'].to(device=comm_buffer.device)\n                            comm_buffer.mul_(group['exp_avg_mask'])\n\n                if self.initialize:\n                    update = exp_avg / (exp_avg_sq.sqrt() + group['eps'])\n                    if group['weight_decay'] > 0.0:\n                        update += group['weight_decay'] * p.data\n                    with torch.no_grad():\n                        p.data.add_(-group['lr'] * update)\n                        if self.freeze_key is True:\n                            comm_buffer.add_(-group['lr'] * update)\n                    if state['step'] % state['local_step_interval'] == 0 and self.freeze_key:\n                        with torch.no_grad():\n                            p.data.add_(-1 * comm_buffer)\n                            comm_buffer.mul_(exp_avg_sq.sqrt() + group['eps'])\n                            if self.size > 1:\n                                comm_buffer.copy_(\n                                    self.comm_backend_handle.compressed_allreduce(comm_buffer, state['worker_error'],\n                                                                                  state['server_error'],\n                                                                                  self.deepspeed.local_rank))\n                                if 'exp_avg_mask' in group:\n                                    if comm_buffer.device != group['exp_avg_mask'].device:\n                                        group['exp_avg_mask'] = group['exp_avg_mask'].to(device=comm_buffer.device)\n                                    comm_buffer.mul_(group['exp_avg_mask'])\n                            exp_avg.zero_().add_(comm_buffer / state['lrs'], alpha=-1)\n                            p.data.add_(comm_buffer / (exp_avg_sq.sqrt() + group['eps']))\n                            comm_buffer.zero_()\n\n                            state['lrs'] = 0\n\n                    # According to 0/1 Adam theory, a fixed variance would allow more accurate estimation of momentum\n                    # However, in practice, we can also disable the manual freezing of variance, since the interval of\n                    # updating variance will increase exponentially, so that it has negligible effect on the estimation.\n                    if self.freeze_key is False:\n                        if state['step'] % state['var_interval'] == 0:\n                            state['var_counter'] += 1\n                            if state['var_counter'] == self.var_update_scaler:\n                                state['var_counter'] = 0\n                                state['var_interval'] *= 2\n                        if (state['step'] + 1) % state['var_interval'] == 0:\n                            if self.using_pipeline:\n                                self.deepspeed.pipeline_enable_backward_allreduce = True\n                            else:\n                                self.deepspeed.enable_backward_allreduce = True\n                        else:\n                            if self.using_pipeline:\n                                self.deepspeed.pipeline_enable_backward_allreduce = False\n                            else:\n                                self.deepspeed.enable_backward_allreduce = False\n                    else:\n                        state['local_step_counter'] += 1\n                        if state['local_step_counter'] == self.local_step_scaler:\n                            state['local_step_counter'] = 0\n                            state['local_step_interval'] = min(self.local_step_clipper,\n                                                               state['local_step_interval'] * 2)\n\n            if not self.initialize:\n                print('Pop out errors', flush=True)\n                self.freeze_key = False\n                state.pop('worker_error')\n                state.pop('server_error')\n\n        if not self.initialize:\n            self.initialize = True\n            print(f\"Finished the initialization step at rank {dist.get_rank()}\")\n            return loss\n\n        if self.state[self.param_groups[0]['params'][0]]['step'] > self.var_freeze_step:\n            self.freeze_key = True\n            if self.using_pipeline:\n                self.deepspeed.pipeline_enable_backward_allreduce = False\n            else:\n                self.deepspeed.enable_backward_allreduce = False\n\n        if self.freeze_key is True and self.reinitial_error_buffer is False:\n            # We need to reinitialize the error buffers when local step > 1 since\n            # the errors will be logged for different metrics (gradient vs. accumulated momentum).\n            for group in self.param_groups:\n                for p in group['params']:\n                    self.state[p]['worker_error'].zero_()\n                    self.state[p]['server_error'].zero_()\n            self.reinitial_error_buffer = True\n\n        return loss\n\n    def load_state_dict(self, state_dict):\n        \"\"\"\n        Overrides load_state_dict() to add special handling when loading checkpoints\n        \"\"\"\n        # Because at different stage exp_avg_mask may change (e.g.,\n        # BERT pre-training seqlen 128 and 512 ), we don't use the exp_avg_mask\n        # in checkpoints but always use the one user provided in training script.\n        # (See example in DeepSpeedExamples/bing_bert/deepspeed_train.py.)\n        # Thus here we keep the exp_avg_mask unchanged when loading checkpoint\n        for i, group in enumerate(self.param_groups):\n            if 'exp_avg_mask' in group:\n                state_dict['param_groups'][i]['exp_avg_mask'] = group['exp_avg_mask']\n            elif 'exp_avg_mask' not in group and 'exp_avg_mask' in state_dict['param_groups'][i]:\n                state_dict['param_groups'][i].pop('exp_avg_mask')\n        super().load_state_dict(state_dict)\n        if self.state[self.param_groups[0]['params'][0]]['step'] < self.var_freeze_step:\n            self.var_freeze_key = False\n            if (self.state[self.param_groups[0]['params'][0]]['step'] +\n                    1) % self.state[self.param_groups[0]['params'][0]]['var_interval'] == 0:\n                if self.using_pipeline:\n                    self.deepspeed.pipeline_enable_backward_allreduce = True\n                else:\n                    self.deepspeed.enable_backward_allreduce = True\n            else:\n                if self.using_pipeline:\n                    self.deepspeed.pipeline_enable_backward_allreduce = False\n                else:\n                    self.deepspeed.enable_backward_allreduce = False\n        else:\n            self.var_freeze_key = True\n            if self.using_pipeline:\n                self.deepspeed.pipeline_enable_backward_allreduce = False\n            else:\n                self.deepspeed.enable_backward_allreduce = False\n        self.reinitial_error_buffer = False\n        for group in self.param_groups:\n            for p in group['params']:\n                if 'worker_error' in self.state[p]:\n                    self.state[p].pop('worker_error')\n                if 'server_error' in self.state[p]:\n                    self.state[p].pop('server_error')\n                if 'momentum_accumulator' in self.state[p]:\n                    self.state[p].pop('momentum_accumulator')\n", "deepspeed/runtime/fp16/onebit/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .adam import OnebitAdam\nfrom .lamb import OnebitLamb\nfrom .zoadam import ZeroOneAdam\n", "deepspeed/runtime/fp16/onebit/adam.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport types\nimport torch\nimport numpy as np\nfrom deepspeed.accelerator import get_accelerator\nfrom deepspeed.utils.torch import required_torch_version\nfrom deepspeed import comm as dist\n\n\nclass OnebitAdam(torch.optim.Optimizer):\n    \"\"\"Implements the 1-bit Adam algorithm. Currently GPU-only.\n    For usage example please see https://www.deepspeed.ai/tutorials/onebit-adam/\n    For technical details please read https://arxiv.org/abs/2102.02888\n\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups.\n        lr (float, optional): learning rate. (default: 1e-3)\n        freeze_step (int, optional): Number of steps for warmup (uncompressed)\n            stage before we start using compressed communication. (default 100000)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square. (default: (0.9, 0.999))\n        eps (float, optional): term added to the denominator to improve\n            numerical stability. (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n            algorithm from the paper `On the Convergence of Adam and Beyond`_\n            (default: False) NOT SUPPORTED in 1-bit Adam!\n        eps_inside_sqrt (boolean, optional): in the 'update parameters' step,\n            adds eps to the bias-corrected second moment estimate before\n            evaluating square root instead of adding it to the square root of\n            second moment estimate as in the original paper. (default: False)\n        cuda_aware (boolean, required): Set True if the underlying MPI implementation\n            supports CUDA-Aware communication. (default: False)\n        comm_backend_name (string, optional): Set to 'mpi' if needed. (default: 'nccl')\n    .. _Adam\\\\: A Method for Stochastic Optimization:\n        https://arxiv.org/abs/1412.6980\n    .. _On the Convergence of Adam and Beyond:\n        https://openreview.net/forum?id=ryQu7f-RZ\n    \"\"\"\n\n    def __init__(self,\n                 params,\n                 deepspeed=None,\n                 lr=1e-3,\n                 freeze_step=100000,\n                 bias_correction=True,\n                 betas=(0.9, 0.999),\n                 eps=1e-8,\n                 eps_inside_sqrt=False,\n                 weight_decay=0.,\n                 max_grad_norm=0.,\n                 amsgrad=False,\n                 cuda_aware=False,\n                 comm_backend_name='nccl'):\n\n        if amsgrad:\n            raise RuntimeError('1-bit Adam does not support the AMSGrad variant.')\n\n        defaults = dict(lr=lr,\n                        bias_correction=bias_correction,\n                        betas=betas,\n                        eps=eps,\n                        weight_decay=weight_decay,\n                        max_grad_norm=max_grad_norm)\n\n        super(OnebitAdam, self).__init__(params, defaults)\n        self.eps_mode = 0 if eps_inside_sqrt else 1\n        self.comm_time = 0.0\n        self.step_time = 0.0\n        self.ave_step = 1\n        self.bk_time = 0.0\n\n        self.deepspeed = deepspeed\n        self.adam_freeze_key = False\n        self.initialize = False\n        self.freeze_step = freeze_step\n        self.cuda_aware = cuda_aware\n        self.using_pipeline = False\n\n        self.comm_backend_name = comm_backend_name\n\n        assert dist.is_initialized(), \"Please initialize the torch distributed backend.\"\n        # Empty initializer. Set handle based on the comm backend as follows.\n        self.comm_backend_handle = None\n        if self.comm_backend_name == 'nccl':\n            assert (\n                required_torch_version(min_version=1.8)\n            ), \"Please use torch 1.8 or greater to enable NCCL backend in 1-bit Adam. Alternatively, please specify 'mpi' as the 'comm_backend_name' in config file to proceed with the MPI backend\"\n            from deepspeed.runtime.comm.nccl import NcclBackend\n            self.using_pipeline = hasattr(self.deepspeed, 'pipeline_enable_backward_allreduce')\n            self.comm_backend_handle = NcclBackend(self.deepspeed.mpu)\n        elif self.comm_backend_name == 'mpi':\n            from deepspeed.runtime.comm.mpi import MpiBackend\n            self.comm_backend_handle = MpiBackend(cuda_aware)\n        elif self.comm_backend_name == 'hccl':\n            from deepspeed.runtime.comm.hccl import HcclBackend\n            self.using_pipeline = hasattr(self.deepspeed, 'pipeline_enable_backward_allreduce')\n            self.comm_backend_handle = HcclBackend(self.deepspeed.mpu)\n        elif self.comm_backend_name == 'compressed':\n            from deepspeed.runtime.comm.compressed import CompressedBackend\n            self.using_pipeline = hasattr(self.deepspeed, 'pipeline_enable_backward_allreduce')\n            self.comm_backend_handle = CompressedBackend(self.deepspeed.mpu)\n        self.size = self.comm_backend_handle.size\n\n        self.divider = int(self.size * 8 / np.gcd(self.size, 8))\n\n    def step(self, closure=None, grads=None):\n        \"\"\"Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n            grads (list of tensors, optional): weight gradient to use for the\n                optimizer update. If gradients have type torch.half, parameters\n                are expected to be in type torch.float. (default: None)\n            output params (list of tensors, optional): A reduced precision copy\n                of the updated weights written out in addition to the regular\n                updated weights. Have to be of same type as gradients. (default: None)\n            scale (float, optional): factor to divide gradient tensor values\n                by before applying to weights. (default: 1)\n        \"\"\"\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        gather_time = 0\n        allgather_time = 0\n        all_time = 0\n\n        if self.adam_freeze_key is False:\n            v_diff_buffer = 0.0\n\n        if grads is None:\n            grads_group = [None] * len(self.param_groups)\n        # backward compatibility\n        # assuming a list/generator of parameter means single group\n        elif isinstance(grads, types.GeneratorType):\n            grads_group = [grads]\n        elif type(grads[0]) != list:\n            grads_group = [grads]\n        else:\n            grads_group = grads\n\n        for group, grads_this_group in zip(self.param_groups, grads_group):\n            if grads_this_group is None:\n                grads_this_group = [None] * len(group['params'])\n\n            bias_correction = 1 if group['bias_correction'] else 0\n\n            for p, grad in zip(group['params'], grads_this_group):\n                if p.grad is None and grad is None:\n                    continue\n                if grad is None:\n                    grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError('1-bit Adam does not support sparse gradients')\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state['step'] = 0\n                    # Exponential moving average of gradient values\n                    state['exp_avg'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n\n                if not self.initialize or (self.adam_freeze_key and 'worker_error' not in state.keys()):\n                    state['tensor_size'] = torch.numel(p.data)\n                    state['corrected_tensor_size'] = state['tensor_size']\n\n                    if state['tensor_size'] % (self.size * self.divider) != 0:\n                        state['corrected_tensor_size'] += ((self.size * self.divider) - (state['tensor_size'] %\n                                                                                         (self.size * self.divider)))\n                    state['server_chunk_size'] = state['corrected_tensor_size'] // self.size\n                    get_accelerator().empty_cache()\n                    state['worker_error'] = torch.zeros(state['corrected_tensor_size'], device=p.device)\n                    state['server_error'] = torch.zeros(state['server_chunk_size'], device=p.device)\n                    get_accelerator().empty_cache()\n                    self.adam_freeze_key = True\n                    if not self.initialize and dist.get_rank() == 0:\n                        print(\"Cupy Buffers Initialized Successfully.\")\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                state['step'] += 1\n\n                if self.adam_freeze_key is False:\n                    exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                    exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                    grad = None\n                    if self.initialize:\n                        update = exp_avg / (exp_avg_sq.sqrt() + group['eps'])\n\n                else:\n                    if 'non_freeze' in group.keys() and group['non_freeze'] is True:\n                        dist.all_reduce(grad)\n                        grad.mul_(1 / dist.get_world_size())\n                        exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                        exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                        grad = None\n                    else:\n                        if self.initialize is True:\n                            exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                        grad = None\n\n                        if self.size > 1:\n                            exp_avg.set_(\n                                self.comm_backend_handle.compressed_allreduce(exp_avg, state['worker_error'],\n                                                                              state['server_error'],\n                                                                              self.deepspeed.local_rank))\n                        # Because 1-bit compression cannot represent exact zero, it is required to\n                        # provide a momentum mask for those params that have constant exact zeros in their\n                        # momentums, otherwise the compression error would keep accumulating.\n                        # For example, for BERT pre-training seq 128, bert.embeddings.position_embeddings.weight\n                        # always have exact zeros in its momentum for row 129 to 512, because it only\n                        # learns up to seq length 128 while the model supports up to 512 seq length.\n                        # (See example in DeepSpeedExamples/bing_bert/deepspeed_train.py.)\n                        if 'exp_avg_mask' in group:\n                            if exp_avg.device != group['exp_avg_mask'].device:\n                                group['exp_avg_mask'] = group['exp_avg_mask'].to(device=exp_avg.device)\n                            exp_avg.mul_(group['exp_avg_mask'])\n\n                    if self.initialize:\n                        update = exp_avg / (exp_avg_sq.sqrt() + group['eps'])\n\n                if self.initialize:\n                    if group['weight_decay'] > 0.0:\n                        update += group['weight_decay'] * p.data\n                    with torch.no_grad():\n                        p.add_(-group['lr'] * update)\n\n            if not self.initialize:\n                print('Pop out errors', flush=True)\n                state.pop('worker_error')\n                state.pop('server_error')\n\n        if not self.initialize:\n            self.adam_freeze_key = False\n            self.initialize = True\n            print(f\"Finished the initialization step at rank {dist.get_rank()}\")\n            return loss\n\n        if self.adam_freeze_key is False:\n            if state['step'] >= self.freeze_step:\n                print('OnebitAdam - starting compressed communication')\n                self.adam_freeze_key = True\n                if self.using_pipeline:\n                    self.deepspeed.pipeline_enable_backward_allreduce = False\n                else:\n                    self.deepspeed.enable_backward_allreduce = False\n\n        return loss\n\n    def load_state_dict(self, state_dict):\n        \"\"\"\n        Overrides load_state_dict() to add special handling when loading checkpoints\n        \"\"\"\n        # Because at different stage exp_avg_mask may change (e.g.,\n        # BERT pre-training seqlen 128 and 512 ), we don't use the exp_avg_mask\n        # in checkpoints but always use the one user provided in training script.\n        # (See example in DeepSpeedExamples/bing_bert/deepspeed_train.py.)\n        # Thus here we keep the exp_avg_mask unchanged when loading checkpoint\n        for i, group in enumerate(self.param_groups):\n            if 'exp_avg_mask' in group:\n                state_dict['param_groups'][i]['exp_avg_mask'] = group['exp_avg_mask']\n            elif 'exp_avg_mask' not in group and 'exp_avg_mask' in state_dict['param_groups'][i]:\n                state_dict['param_groups'][i].pop('exp_avg_mask')\n        super().load_state_dict(state_dict)\n        if self.state[self.param_groups[0]['params'][0]]['step'] < self.freeze_step:\n            if dist.get_rank() == 0:\n                print(\"Checkpoint loaded and OnebitAdam warmup stage starts/continues.\")\n            if self.adam_freeze_key is True:\n                self.adam_freeze_key = False\n                if self.using_pipeline:\n                    self.deepspeed.pipeline_enable_backward_allreduce = True\n                else:\n                    self.deepspeed.enable_backward_allreduce = True\n        else:\n            if dist.get_rank() == 0:\n                print(\"Checkpoint loaded and OnebitAdam compression stage starts/continues.\")\n            if self.adam_freeze_key is False:\n                self.adam_freeze_key = True\n                if self.using_pipeline:\n                    self.deepspeed.pipeline_enable_backward_allreduce = False\n                else:\n                    self.deepspeed.enable_backward_allreduce = False\n        # We reset the compression errors when loading checkpoints for 3 reasons:\n        # 1) The worker and server error at each GPU are distinct, so in current implementation\n        # only rank 0's errors are saved in the checkpoint. Thus we have to reset the errors.\n        # If we want to save them correctly we need O(num_gpu*model_size) memory in order to\n        # gather all the error, which is a very large memory requirement. It's possible to save\n        # them in a distributed way, but it will make the checkpoint saving/loading much more complicated.\n        # 2) Even if we are able to save the compression errors correctly, you need to have the\n        # exact same number of GPUs in order to load them correctly.\n        # 3) We verified on BERT pre-training that occasionally resetting the compression error\n        # at checkpoint loading does not affect the convergence.\n        # However, please avoid frequent checkpoint loading which could break the error\n        # compensation mechanism thus affect the convergence.\n        for group in self.param_groups:\n            for p in group['params']:\n                if 'worker_error' in self.state[p]:\n                    self.state[p].pop('worker_error')\n                if 'server_error' in self.state[p]:\n                    self.state[p].pop('server_error')\n", "deepspeed/runtime/checkpoint_engine/checkpoint_engine.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport os\n\n\nclass CheckpointEngine(object):\n\n    # init checkpoint engine for save/load\n    def __init__(self, config_params=None):\n        pass\n\n    def create(self, tag):\n        # create checkpoint on give tag for save/load.\n        pass\n\n    def makedirs(self, path, exist_ok=False):\n        os.makedirs(path, exist_ok=exist_ok)\n\n    def save(self, state_dict, path: str):\n        pass\n\n    def load(self, path: str, map_location=None):\n        pass\n\n    def commit(self, tag):\n        # to tell checkpoint services if all files are ready.\n        pass\n", "deepspeed/runtime/checkpoint_engine/nebula_checkpoint_engine.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport os\nimport torch\nimport torch_nebula\n\nfrom deepspeed.runtime.checkpoint_engine.checkpoint_engine import \\\n    CheckpointEngine\nfrom deepspeed.utils import logger, log_dist\nfrom deepspeed.nebula.constants import *\n\n\ndef _get_tag_from_path(path):\n    return os.path.basename(os.path.dirname(path))\n\n\nclass NebulaCheckpointEngine(CheckpointEngine):\n\n    def __init__(self, config_params=None):\n        super().__init__(config_params)\n        self.checkpoint = None\n        self.tag_flag = None\n        self.enable_nebula_load = config_params.enable_nebula_load\n        self.nebula_load_path = config_params.load_path\n        if self.nebula_load_path is None:\n            self.nebula_load_path = config_params.persistent_storage_path\n\n        nebula_config_params = {\n            NEBULA_PERSISTENT_STORAGE_PATH: config_params.persistent_storage_path,\n            NEBULA_PERSISTENT_TIME_INTERVAL: config_params.persistent_time_interval,\n            NEBULA_NUM_OF_VERSION_IN_RETENTION: config_params.num_of_version_in_retention,\n        }\n        torch_nebula.init(**nebula_config_params)\n\n    def create(self, tag):\n        log_dist(f\"[Nebula] Start Checkpoint for tag:{tag}\", ranks=[0])\n        # -2 means: customer needs to  explicitly tell nebula\n        # current checkpoint is complete by commit method.\n        self.checkpoint = torch_nebula.Checkpoint(tag, -2)\n\n    def save(self, state_dict, path: str):\n        log_dist(f\"[Nebula] Create dummy files for loading.\")\n        torch.save(\"\", path)\n\n        tag = _get_tag_from_path(path)\n        partition_name = os.path.basename(path)\n        logger.info(f\"[Nebula] Saving {partition_name} under tag {tag}...\")\n        self.checkpoint.save(partition_name, state_dict)\n        logger.info(f\"[Nebula] Saved {partition_name} under tag {tag}.\")\n        return None\n\n    def load(self, path: str, map_location=None):\n        tag = _get_tag_from_path(path)\n        first_load_flag = self.tag_flag is None or self.tag_flag == tag\n        if not self.enable_nebula_load and first_load_flag:\n            self.tag_flag = tag\n            logger.info(f\"[Nebula] Disable nebula load. Loading checkpoint from {path} ...\")\n            partition = torch.load(path, map_location=map_location)\n            logger.info(f\"[Nebula] Disable nebula load. Loaded checkpoint from {path} .\")\n            return partition\n\n        partition_name = os.path.basename(path)\n        logger.info(f\"[Nebula] Loading {path} under tag {tag} from nebula path {self.nebula_load_path}...\")\n\n        checkpoint = None\n        if tag in (None, 'latest', 'latest_universal'):\n            # In some cases, there is the inconsistent tag between deepspeed metadata (latest file)\n            # and nebula metadata, will lead to the failure on loading with deepspeed tag. Then we\n            # will try to load the valid latest checkpoint from nebula(tier3 > tier1). So, in summary\n            # when met failure loading for given tag, the loading priority would be like:\n            #               nebula tier3 latest > nebula tier1 latest.\n            checkpoint = torch_nebula.get_latest_checkpoint(persist_path=self.nebula_load_path)\n        else:\n            checkpoint = torch_nebula.get_checkpoint(tag=tag, persist_path=self.nebula_load_path)\n\n        if checkpoint is None or (checkpoint is not None and checkpoint.tag == ''):\n            logger.info(\n                f\"Unable to find valid checkpoint tag:{tag} from Nebula, try to get latest checkpoint again from nebula {self.nebula_load_path} path!\"\n            )\n            # nebula tier3 latest\n            checkpoint = torch_nebula.get_latest_checkpoint(persist_path=self.nebula_load_path)\n            if checkpoint is None or (checkpoint is not None and checkpoint.tag == ''):\n                logger.info(\n                    f\"Unable to find latest checkpoint from Nebula tier3, try to get latest checkpoint again from nebula tier1 path!\"\n                )\n                # nebula tier1 latest\n                checkpoint = torch_nebula.get_latest_checkpoint()\n                logger.warning(f\"Unable to find valid checkpoint from Nebula under tag:{tag}.\")\n                return None\n\n        tag = checkpoint.tag\n        self.tag_flag = -1\n        partition = checkpoint.load(partition_name, map_location=map_location)\n        logger.info(f\"[Nebula] Loaded {path} under tag {tag} from {self.nebula_load_path}.\")\n        return partition\n\n    def commit(self, tag):\n        # nebula commit will be call when all files under give tag are ready to be persisted in the async way.\n        logger.info(f\"[Nebula] all files for {tag} are saved in tier1. It is ready to start persisting\")\n        commit_rls = self.checkpoint.commit()\n        if not commit_rls:\n            logger.error(f\"[Nebula] failed to commit the checkpoint, please check the log.\")\n            return False\n        return commit_rls\n", "deepspeed/runtime/checkpoint_engine/torch_checkpoint_engine.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nfrom deepspeed.utils import logger, log_dist\nfrom deepspeed.runtime.checkpoint_engine.checkpoint_engine import \\\n    CheckpointEngine\n\n\nclass TorchCheckpointEngine(CheckpointEngine):\n\n    def __init__(self, config_params=None):\n        super().__init__(config_params)\n\n    def create(self, tag):\n        log_dist(f\"[Torch] Checkpoint {tag} is about to be saved!\", ranks=[0])\n\n    def save(self, state_dict, path: str):\n        logger.info(f\"[Torch] Saving {path}...\")\n        torch.save(state_dict, path)\n        logger.info(f\"[Torch] Saved {path}.\")\n        return None\n\n    def load(self, path: str, map_location=None):\n        logger.info(f\"[Torch] Loading checkpoint from {path}...\")\n        partition = torch.load(path, map_location=map_location)\n        logger.info(f\"[Torch] Loaded checkpoint from {path}.\")\n        return partition\n\n    def commit(self, tag):\n        logger.info(f\"[Torch] Checkpoint {tag} is ready now!\")\n        return True\n", "deepspeed/runtime/checkpoint_engine/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n'''Copyright The Microsoft DeepSpeed Team'''\n", "deepspeed/monitor/monitor.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\"\"\"\nSupport different forms of monitoring such as wandb and tensorboard\n\"\"\"\n\nfrom abc import ABC, abstractmethod\nimport deepspeed.comm as dist\n\n\nclass Monitor(ABC):\n\n    @abstractmethod\n    def __init__(self, monitor_config):\n        self.monitor_config = monitor_config\n\n    @abstractmethod\n    def write_events(self, event_list):\n        pass\n\n\nfrom .wandb import WandbMonitor\nfrom .tensorboard import TensorBoardMonitor\nfrom .csv_monitor import csvMonitor\nfrom .comet import CometMonitor\n\n\nclass MonitorMaster(Monitor):\n\n    def __init__(self, monitor_config):\n        super().__init__(monitor_config)\n        self.tb_monitor = None\n        self.wandb_monitor = None\n        self.csv_monitor = None\n        self.comet_monitor = None\n        self.enabled = monitor_config.enabled\n\n        if dist.get_rank() == 0:\n            if monitor_config.tensorboard.enabled:\n                self.tb_monitor = TensorBoardMonitor(monitor_config.tensorboard)\n            if monitor_config.wandb.enabled:\n                self.wandb_monitor = WandbMonitor(monitor_config.wandb)\n            if monitor_config.csv_monitor.enabled:\n                self.csv_monitor = csvMonitor(monitor_config.csv_monitor)\n            if monitor_config.comet.enabled:\n                self.comet_monitor = CometMonitor(monitor_config.comet)\n\n    def write_events(self, event_list):\n        if dist.get_rank() == 0:\n            if self.tb_monitor is not None:\n                self.tb_monitor.write_events(event_list)\n            if self.wandb_monitor is not None:\n                self.wandb_monitor.write_events(event_list)\n            if self.csv_monitor is not None:\n                self.csv_monitor.write_events(event_list)\n            if self.comet_monitor is not None:\n                self.comet_monitor.write_events(event_list)\n", "deepspeed/monitor/config.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Optional\n\nfrom deepspeed.pydantic_v1 import root_validator\nfrom deepspeed.runtime.config_utils import DeepSpeedConfigModel\n\n\ndef get_monitor_config(param_dict):\n    monitor_dict = {key: param_dict.get(key, {}) for key in (\"tensorboard\", \"wandb\", \"csv_monitor\", \"comet\")}\n    return DeepSpeedMonitorConfig(**monitor_dict)\n\n\nclass TensorBoardConfig(DeepSpeedConfigModel):\n    \"\"\"Sets parameters for TensorBoard monitor.\"\"\"\n\n    enabled: bool = False\n    \"\"\" Whether logging to Tensorboard is enabled. Requires `tensorboard` package is installed. \"\"\"\n\n    output_path: str = \"\"\n    \"\"\"\n    Path to where the Tensorboard logs will be written. If not provided, the\n    output path is set under the training script\u2019s launching path.\n    \"\"\"\n\n    job_name: str = \"DeepSpeedJobName\"\n    \"\"\" Name for the current job. This will become a new directory inside `output_path`. \"\"\"\n\n\nclass WandbConfig(DeepSpeedConfigModel):\n    \"\"\"Sets parameters for WandB monitor.\"\"\"\n\n    enabled: bool = False\n    \"\"\" Whether logging to WandB is enabled. Requires `wandb` package is installed. \"\"\"\n\n    group: str = None\n    \"\"\" Name for the WandB group. This can be used to group together runs. \"\"\"\n\n    team: str = None\n    \"\"\" Name for the WandB team. \"\"\"\n\n    project: str = \"deepspeed\"\n    \"\"\" Name for the WandB project. \"\"\"\n\n\nclass CSVConfig(DeepSpeedConfigModel):\n    \"\"\"Sets parameters for CSV monitor.\"\"\"\n\n    enabled: bool = False\n    \"\"\" Whether logging to local CSV files is enabled. \"\"\"\n\n    output_path: str = \"\"\n    \"\"\"\n    Path to where the csv files will be written. If not provided, the output\n    path is set under the training script\u2019s launching path.\n    \"\"\"\n\n    job_name: str = \"DeepSpeedJobName\"\n    \"\"\" Name for the current job. This will become a new directory inside `output_path`. \"\"\"\n\n\nclass CometConfig(DeepSpeedConfigModel):\n    \"\"\"\n    Sets parameters for Comet monitor. For logging data Comet uses\n    experiment object.\n    https://www.comet.com/docs/v2/api-and-sdk/python-sdk/reference/Experiment/\n    \"\"\"\n\n    enabled: bool = False\n    \"\"\" Whether logging to Comet is enabled. Requires `comet_ml` package is installed. \"\"\"\n\n    samples_log_interval: int = 100\n    \"\"\" Metrics will be submitted to Comet after processing every `samples_log_intervas` samples\"\"\"\n\n    project: Optional[str] = None\n    \"\"\"\n    Comet project name. Can be set through .comet.config file or environment variable COMET_PROJECT_NAME\n    https://www.comet.com/docs/v2/guides/experiment-management/configure-sdk/#explore-comet-configuration-options\n    \"\"\"\n\n    workspace: Optional[str] = None\n    \"\"\"\n    Comet workspace name. Can be set through .comet.config file or environment variable COMET_WORKSPACE\n    https://www.comet.com/docs/v2/guides/experiment-management/configure-sdk/#explore-comet-configuration-options\n    \"\"\"\n\n    api_key: Optional[str] = None\n    \"\"\"\n    Comet API key. Can be set through .comet.config file or environment variable COMET_API_KEY\n    https://www.comet.com/docs/v2/guides/experiment-management/configure-sdk/#explore-comet-configuration-options\n    \"\"\"\n\n    experiment_name: Optional[str] = None\n    \"\"\"\n    The name for comet experiment to be used for logging.\n    Can be set through .comet.config file or environment variable COMET_EXPERIMENT_NAME\n    https://www.comet.com/docs/v2/guides/experiment-management/configure-sdk/#explore-comet-configuration-options\n    \"\"\"\n\n    experiment_key: Optional[str] = None\n    \"\"\"\n    The key for comet experiment to be used for logging. Must be an alphanumeric string whose length is between 32 and 50 characters.\n    Can be set through .comet.config  or environment variable COMET_EXPERIMENT_KEY\n    https://www.comet.com/docs/v2/guides/experiment-management/configure-sdk/#explore-comet-configuration-options\n    \"\"\"\n\n    online: Optional[bool] = None\n    \"\"\"\n    If True, the data will be logged to Comet server, otherwise it will be stored locally in offline experiment\n    Defaults to True.\n    \"\"\"\n\n    mode: Optional[str] = None\n    \"\"\"\n    Control how the Comet experiment is started, 3 options are possible.:\n        - \"get\": Continue logging to an existing experiment identified by the `experiment_key` value.\n        - \"create\": Always creates of a new experiment, useful for HPO sweeps.\n        - \"get_or_create\" (default): Starts a fresh experiment if required, or persists logging to an existing one.\n    \"\"\"\n\n\nclass DeepSpeedMonitorConfig(DeepSpeedConfigModel):\n    \"\"\"Sets parameters for various monitoring methods.\"\"\"\n\n    tensorboard: TensorBoardConfig = {}\n    \"\"\" TensorBoard monitor, requires `tensorboard` package is installed. \"\"\"\n\n    comet: CometConfig = {}\n    \"\"\" Comet monitor, requires `comet_ml` package is installed \"\"\"\n\n    wandb: WandbConfig = {}\n    \"\"\" WandB monitor, requires `wandb` package is installed. \"\"\"\n\n    csv_monitor: CSVConfig = {}\n    \"\"\" Local CSV output of monitoring data. \"\"\"\n\n    @root_validator\n    def check_enabled(cls, values):\n        values[\"enabled\"] = values.get(\"tensorboard\").enabled or values.get(\"wandb\").enabled or values.get(\n            \"csv_monitor\").enabled or values.get(\"comet\").enabled\n        return values\n", "deepspeed/monitor/utils.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom packaging import version as pkg_version\n\n\ndef check_tb_availability():\n    try:\n        # torch.utils.tensorboard will fail if `tensorboard` is not available,\n        # see their docs for more details: https://pytorch.org/docs/1.8.0/tensorboard.html\n        import tensorboard  # noqa: F401 # type: ignore\n    except ImportError:\n        print('If you want to use tensorboard logging, please `pip install tensorboard`')\n        raise\n\n\ndef check_wandb_availability():\n    try:\n        import wandb  # noqa: F401 # type: ignore\n    except ImportError:\n        print(\n            'If you want to use wandb logging, please `pip install wandb` and follow the instructions at https://docs.wandb.ai/quickstart'\n        )\n        raise\n\n\ndef check_comet_availability():\n    try:\n        import comet_ml\n        comet_version = pkg_version.parse(comet_ml.__version__)\n        if comet_version < pkg_version.Version(\"3.41.0\"):\n            raise ImportError(\"`comet_ml` must have at least version 3.41.0\")\n    except ImportError:\n        print('If you want to use comet logging, please `pip install \"comet_ml>=3.41.0\"`')\n        raise\n", "deepspeed/monitor/comet.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import TYPE_CHECKING, Any, Tuple, List, Dict, Optional\n\nfrom .utils import check_comet_availability\nfrom .monitor import Monitor\n\nimport deepspeed.comm as dist\n\nif TYPE_CHECKING:\n    import comet_ml\n    from .config import CometConfig\n\nName = str\nValue = Any\nGlobalSamples = int\nEvent = Tuple[Name, Value, GlobalSamples]\n\n\nclass CometMonitor(Monitor):\n\n    def __init__(self, comet_config: \"CometConfig\"):\n        super().__init__(comet_config)\n        check_comet_availability()\n        import comet_ml\n\n        self.enabled = comet_config.enabled\n        self._samples_log_interval = comet_config.samples_log_interval\n        self._experiment: Optional[\"comet_ml.ExperimentBase\"] = None\n\n        if self.enabled and dist.get_rank() == 0:\n            self._experiment = comet_ml.start(\n                api_key=comet_config.api_key,\n                project=comet_config.project,\n                workspace=comet_config.workspace,\n                experiment_key=comet_config.experiment_key,\n                mode=comet_config.mode,\n                online=comet_config.online,\n            )\n\n            if comet_config.experiment_name is not None:\n                self._experiment.set_name(comet_config.experiment_name)\n\n        self._events_log_scheduler = EventsLogScheduler(comet_config.samples_log_interval)\n\n    @property\n    def experiment(self) -> Optional[\"comet_ml.ExperimentBase\"]:\n        return self._experiment\n\n    @property\n    def samples_log_interval(self) -> int:\n        return self._samples_log_interval\n\n    def write_events(self, event_list: List[Event]) -> None:\n        if not self.enabled or dist.get_rank() != 0:\n            return None\n\n        for event in event_list:\n            name = event[0]\n            value = event[1]\n            engine_global_samples = event[2]\n\n            if self._events_log_scheduler.needs_logging(name, engine_global_samples):\n                self._experiment.__internal_api__log_metric__(\n                    name=name,\n                    value=value,\n                    step=engine_global_samples,\n                )\n\n\nclass EventsLogScheduler:\n\n    def __init__(self, samples_log_interval: int):\n        self._samples_log_interval = samples_log_interval\n        self._last_logged_events_samples: Dict[str, int] = {}\n\n    def needs_logging(self, name: str, current_sample: int) -> bool:\n        if name not in self._last_logged_events_samples:\n            self._last_logged_events_samples[name] = current_sample\n            return True\n\n        last_logged_sample = self._last_logged_events_samples[name]\n        samples_delta = current_sample - last_logged_sample\n\n        if samples_delta >= self._samples_log_interval:\n            self._last_logged_events_samples[name] = current_sample\n            return True\n\n        return False\n", "deepspeed/monitor/tensorboard.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .utils import check_tb_availability\nfrom .monitor import Monitor\nimport os\n\nimport deepspeed.comm as dist\n\n\nclass TensorBoardMonitor(Monitor):\n\n    def __init__(self, tensorboard_config):\n        super().__init__(tensorboard_config)\n        check_tb_availability()\n\n        self.summary_writer = None\n        self.enabled = tensorboard_config.enabled\n        self.output_path = tensorboard_config.output_path\n        self.job_name = tensorboard_config.job_name\n\n        if self.enabled and dist.get_rank() == 0:\n            self.get_summary_writer()\n\n    def get_summary_writer(self, base=os.path.join(os.path.expanduser(\"~\"), \"tensorboard\")):\n        if self.enabled and dist.get_rank() == 0:\n            from torch.utils.tensorboard import SummaryWriter\n            if self.output_path is not None:\n                log_dir = os.path.join(self.output_path, self.job_name)\n            # NOTE: This code path currently is never used since the default output_path is an empty string and not None. Saving it in case we want this functionality in the future.\n            else:\n                if \"DLWS_JOB_ID\" in os.environ:\n                    infra_job_id = os.environ[\"DLWS_JOB_ID\"]\n                elif \"DLTS_JOB_ID\" in os.environ:\n                    infra_job_id = os.environ[\"DLTS_JOB_ID\"]\n                else:\n                    infra_job_id = \"unknown-job-id\"\n\n                summary_writer_dir_name = os.path.join(infra_job_id, \"logs\")\n                log_dir = os.path.join(base, summary_writer_dir_name, self.output_path)\n            os.makedirs(log_dir, exist_ok=True)\n            self.summary_writer = SummaryWriter(log_dir=log_dir)\n        return self.summary_writer\n\n    def write_events(self, event_list, flush=True):\n        if self.enabled and self.summary_writer is not None and dist.get_rank() == 0:\n            for event in event_list:\n                self.summary_writer.add_scalar(*event)\n            if flush:\n                self.summary_writer.flush()\n\n    def flush(self):\n        if self.enabled and self.summary_writer is not None and dist.get_rank() == 0:\n            self.summary_writer.flush()\n", "deepspeed/monitor/csv_monitor.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .monitor import Monitor\nimport os\n\nimport deepspeed.comm as dist\n\n\nclass csvMonitor(Monitor):\n\n    def __init__(self, csv_config):\n        super().__init__(csv_config)\n        self.filenames = []\n        self.enabled = csv_config.enabled\n        self.output_path = csv_config.output_path\n        self.job_name = csv_config.job_name\n        self.log_dir = self.setup_log_dir()\n\n    def setup_log_dir(self, base=os.path.join(os.path.expanduser(\"~\"), \"csv_monitor\")):\n        if self.enabled and dist.get_rank() == 0:\n            if self.output_path is not None:\n                log_dir = os.path.join(self.output_path, self.job_name)\n            # NOTE: This code path currently is never used since the default tensorboard_output_path is an empty string and not None. Saving it in case we want this functionality in the future.\n            else:\n                if \"DLWS_JOB_ID\" in os.environ:\n                    infra_job_id = os.environ[\"DLWS_JOB_ID\"]\n                elif \"DLTS_JOB_ID\" in os.environ:\n                    infra_job_id = os.environ[\"DLTS_JOB_ID\"]\n                else:\n                    infra_job_id = \"unknown-job-id\"\n\n                csv_monitor_dir_name = os.path.join(infra_job_id, \"logs\")\n                log_dir = os.path.join(base, csv_monitor_dir_name, self.job_name)\n            os.makedirs(log_dir, exist_ok=True)\n            return log_dir\n\n    def write_events(self, event_list):\n        if self.enabled and dist.get_rank() == 0:\n            import csv\n            # We assume each event_list element is a tensorboard-style tuple in the format: (log_name: String, value, step: Int)\n            for event in event_list:\n                log_name = event[0]\n                value = event[1]\n                step = event[2]\n\n                # Set the header to the log_name\n                # Need this check because the deepspeed engine currently formats log strings to separate with '/'\n                if '/' in log_name:\n                    record_splits = log_name.split('/')\n                    header = record_splits[len(record_splits) - 1]\n                else:\n                    header = log_name\n\n                # sanitize common naming conventions into filename\n                filename = log_name.replace('/', '_').replace(' ', '_')\n                fname = self.log_dir + '/' + filename + '.csv'\n\n                # Open file and record event. Insert header if this is the first time writing\n                with open(fname, 'a+') as csv_monitor_file:\n                    csv_monitor_writer = csv.writer(csv_monitor_file)\n                    if filename not in self.filenames:\n                        self.filenames.append(filename)\n                        csv_monitor_writer.writerow(['step', header])\n                    csv_monitor_writer.writerow([step, value])\n", "deepspeed/monitor/wandb.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .utils import check_wandb_availability\nfrom .monitor import Monitor\n\nimport deepspeed.comm as dist\n\n\nclass WandbMonitor(Monitor):\n\n    def __init__(self, wandb_config):\n        super().__init__(wandb_config)\n        check_wandb_availability()\n        import wandb\n\n        self.enabled = wandb_config.enabled\n        self.group = wandb_config.group\n        self.team = wandb_config.team\n        self.project = wandb_config.project\n\n        if self.enabled and dist.get_rank() == 0:\n            wandb.init(project=self.project, group=self.group, entity=self.team)\n\n    def log(self, data, step=None, commit=None, sync=None):\n        if self.enabled and dist.get_rank() == 0:\n            import wandb\n            return wandb.log(data, step=step, commit=commit, sync=sync)\n\n    def write_events(self, event_list):\n        if self.enabled and dist.get_rank() == 0:\n            for event in event_list:\n                label = event[0]\n                value = event[1]\n                step = event[2]\n                self.log({label: value}, step=step)\n", "deepspeed/monitor/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n'''Copyright The Microsoft DeepSpeed Team'''\n", "deepspeed/linear/config.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass LoRAConfig:\n    \"\"\"\n    Configuration settings for LoRAOptimizedLinear.\n\n    Attributes:\n        lora_r (int): LoRA attention dimension, also know as the rank. Defaults is 64.\n        lora_alpha (float): LoRA scaling factor, default is 16.\n        base_weight_sharding (int): The degree to which the base weights are sharded,\n            should typically be set to the data-parallel world size to maximize the memory\n            reduction benefits. Defaults to 1, which means this feature is disabled.\n    \"\"\"\n    lora_r: int = 64\n    lora_alpha: float = 16.\n    base_weight_sharding: int = 1\n\n\n@dataclass\nclass QuantizationConfig:\n    \"\"\"\n    Configuration settings for quantization for LoRAOptimizedLinear, QuantizedLinear,\n    and QuantizedParameter\n\n    Attributes:\n        q_bits (int): The number of bits used for quantization. Default is 8.\n        mantissa_bits (int): The number of bits reserved for the mantissa in fixed-point quantization. Default is 3.\n        group_size (int): The size of the group used for quantization. Default is 512.\n    \"\"\"\n    q_bits: int = 8\n    mantissa_bits: int = 3\n    group_size: int = 512\n", "deepspeed/linear/quantization.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport copy\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom typing import Optional\n\nfrom deepspeed.accelerator import get_accelerator\nfrom deepspeed.ops.fp_quantizer import Quantizer, FP_Quantize\nfrom .config import QuantizationConfig\n\n\nclass QuantizedParameter(nn.Parameter):\n    \"\"\"\n    Quantized parameter class that implements weight quantization. Weights\n    are stored in quantized form on GPUs, and can be dequantized on-the-fly when\n    needed by the model. The weights are actually quantized during any `.to(device)`.\n\n    Arguments:\n        data (Tensor): parameter tensor.\n        requires_grad (bool, optional): if the parameter requires gradient. Defaults\n            to False and is not supported to be True. Argument provided only for interface\n            compatibility with torch.nn.Parameter.\n        quantization_config (QuantizationConfig, optional):\n        quantizer (Quantizer, optional): Defaults to FP_Quantize but can be any quantizer\n            that implements deepspeed.ops.fp_quantizer.Quantizer. This argument is also\n            required since the quantizer is stashed in the Parameter itself, some models\n            may clone the Parameter by passing an attribute __dict__. For an example, see\n            tests/unit/linear/test_quant_param.py::TestQuantParam::test_hf_clone\n    \"\"\"\n\n    def __new__(\n        cls,\n        data: Optional[torch.Tensor] = None,\n        requires_grad: bool = False,  # quantized weights must be frozen\n        quantization_config: QuantizationConfig = None,\n        quantizer: Quantizer = None,\n    ):\n        if requires_grad:\n            raise ValueError(f\"requires_grad=True is not supported with QuantizedParameter\")\n        if data is None:\n            data = torch.empty(0)\n        self = torch.Tensor._make_subclass(cls, data, requires_grad)\n        self.quantization_config = QuantizationConfig() if quantization_config is None else quantization_config\n        if quantizer is not None:\n            self.quantizer = quantizer\n        else:\n            # if FPQuantizerBuilder is not compatible in this env this init will fail\n            self.quantizer = FP_Quantize(group_size=self.quantization_config.group_size)\n        self._ensure_quantized(self)\n        return self\n\n    def _ensure_quantized(self, tensor: torch.Tensor):\n        # If the tensor is on the accelerator and is not quantized, then quantize it in-place.\n        if get_accelerator().on_accelerator(tensor) and tensor.dtype != torch.int8:\n            with get_accelerator().stream(get_accelerator().current_stream(tensor.device)):\n                tensor.data = self.quantizer.quantize(tensor.data,\n                                                      q_bits=self.quantization_config.q_bits,\n                                                      q_mantisa_bits=self.quantization_config.mantissa_bits)\n            assert tensor.dtype == torch.int8\n\n    def dequantized(self) -> torch.Tensor:\n        \"\"\"\n        Return a tensor containing the dequantized weights of this parameter.\n        \"\"\"\n        if get_accelerator().on_accelerator(self.data) and self.data.dtype == torch.int8:\n            with get_accelerator().stream(get_accelerator().current_stream(self.data.device)):\n                return self.quantizer.dequantize(self.data,\n                                                 q_bits=self.quantization_config.q_bits,\n                                                 q_mantisa_bits=self.quantization_config.mantissa_bits)\n        return self.data\n\n    def __getstate__(self):\n        state = self.__dict__\n        state[\"data\"] = self.data\n        state[\"quantization_config\"] = self.quantization_config\n        state[\"requires_grad\"] = self.requires_grad\n        return state\n\n    def __setstate__(self, state):\n        self.quantizer = state[\"quantizer\"]\n        self.quantization_config = state[\"quantization_config\"]\n        self.data = state[\"data\"]\n        self.requires_grad = state[\"requires_grad\"]\n\n    def __deepcopy__(self, memo):\n        new_instance = type(self).__new__(type(self))\n        state = self.__getstate__()\n        new_instance.__setstate__(state)\n        new_instance.quantizer = copy.deepcopy(state[\"quantizer\"])\n        new_instance.quantization_config = copy.deepcopy(state[\"quantization_config\"])\n        new_instance.data = copy.deepcopy(state[\"data\"])\n        return new_instance\n\n    def __copy__(self):\n        new_instance = type(self).__new__(type(self))\n        state = self.__getstate__()\n        new_instance.__setstate__(state)\n        return new_instance\n\n    def cuda(self, device=None, non_blocking=False):\n        return self.to(device=\"cuda\" if device is None else device, non_blocking=non_blocking)\n\n    def to(self, *args, **kwargs):\n        \"\"\"\n        Move the parameter to the given device. Then, if the device is a cuda device,\n        quantize it.\n        \"\"\"\n        tensor = super().to(*args, **kwargs)\n        self._ensure_quantized(tensor)\n        return tensor\n\n\nclass QuantizedLinear(nn.Linear):\n    \"\"\"\n    Linear layer that implements weight quantization. Parameters\n    are stored via `QuantizedParameter` and are dequantized on-the-fly during any\n    forward pass.\n    \"\"\"\n\n    def __init__(self,\n                 input_dim: int,\n                 output_dim: int,\n                 bias: bool = False,\n                 quantization_config: QuantizationConfig = None,\n                 dtype=torch.bfloat16):\n        super().__init__(input_dim, output_dim, bias=bias, dtype=dtype)\n        assert dtype == torch.bfloat16, \"currently only supports bfloat16 dtype\"\n        self.weight = QuantizedParameter(self.weight.data, quantization_config=quantization_config)\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        return F.linear(input, self.weight.dequantized(), self.bias)\n", "deepspeed/linear/optimized_linear.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nimport math\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom dataclasses import is_dataclass\nfrom deepspeed.accelerator import get_accelerator\nimport deepspeed.comm as dist\n\nfrom .config import LoRAConfig, QuantizationConfig\nfrom .quantization import QuantizedParameter, QuantizedLinear\n\n\nclass OptimizedLinear(nn.Module):\n    \"\"\"\n    Optimized version of nn.Linear that adds features such as:\n      * LoRA w. base weight sharding\n      * FP [6,8,12] quantization\n\n    Arguments:\n        input_dim: Required: size of each input sample\n        output_dim: Required: size of each output sample\n        bias: Optional: If set to False, the layer will not learn an additive bias. Default: False\n        lora_config: Optional: LoRAConfig defining lora features and base-weight-sharding degree\n        quantization_config: Optional: QuantizationConfig defining quantization features\n        dtype: Optional: parameter dtype, only supports bfloat16 currently\n\n    Returns:\n        Returns a new nn.Module depending on the input config. Either native\n        torch.nn.Linear, QuantizedLinear, or the full-featured DSOptimizedLinear.\n    \"\"\"\n\n    def __new__(self,\n                input_dim: int,\n                output_dim: int,\n                bias: bool = False,\n                lora_config: LoRAConfig = None,\n                quantization_config: QuantizationConfig = None,\n                dtype=torch.bfloat16):\n\n        if quantization_config is not None and not is_dataclass(quantization_config):\n            raise ValueError(f\"Expecting QuantizationConfig but received {type(quantization_config)}\")\n        if lora_config is not None and not is_dataclass(lora_config):\n            raise ValueError(f\"Expecting LoRAConfig but received {type(lora_config)}\")\n        if lora_config is None and quantization_config is None:\n            # Everything disabled, fall back to normal nn.Linear\n            self = nn.Linear(input_dim, output_dim, bias=bias, dtype=dtype)\n\n        elif lora_config:\n            # lora enabled, quantization may or may not be\n            self = LoRAOptimizedLinear(input_dim=input_dim,\n                                       output_dim=output_dim,\n                                       bias=bias,\n                                       lora_config=lora_config,\n                                       quantization_config=quantization_config,\n                                       dtype=dtype)\n\n        elif quantization_config:\n            # only quantization enabled, no lora\n            self = QuantizedLinear(input_dim=input_dim,\n                                   output_dim=output_dim,\n                                   bias=bias,\n                                   quantization_config=quantization_config,\n                                   dtype=dtype)\n        return self\n\n\nclass LoRAOptimizedLinear(nn.Module):\n\n    def __init__(self,\n                 input_dim: int,\n                 output_dim: int,\n                 bias: bool = False,\n                 lora_config: LoRAConfig = None,\n                 quantization_config: QuantizationConfig = None,\n                 device=None,\n                 dtype=torch.bfloat16):\n        super().__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.bias = bias\n        self.lora_config = lora_config\n        self.quantization_config = quantization_config\n        device = get_accelerator().current_device_name() if device is None else device\n        assert self.lora_config is not None, \"DSOptimizedLinear requires a LoRA config\"\n\n        self.zero_shards = self.lora_config.base_weight_sharding\n        self.sharded_weight_size = int(float(self.input_dim) // self.zero_shards)\n        w = torch.nn.Parameter(torch.empty((self.output_dim, self.sharded_weight_size), dtype=dtype))\n        torch.nn.init.xavier_uniform_(w)\n\n        if self.quantization_config is not None:\n            assert dtype == torch.bfloat16, \"only bfloat16 is supported when using quantization\"\n            self.base_weight = QuantizedParameter(w, quantization_config=quantization_config)\n        else:\n            self.base_weight = w\n\n        self.base_weight.requires_grad = False\n\n        # Use RS lora for now.\n        self.lora_scaling_factor = self.lora_config.lora_alpha / math.sqrt(self.lora_config.lora_r)\n        # Keeping lora weights in bf16 precision for ease of training.\n        self.lora_weight_1 = nn.Linear(self.input_dim,\n                                       self.lora_config.lora_r,\n                                       bias=self.bias,\n                                       device=device,\n                                       dtype=dtype)\n        self.lora_weight_2 = nn.Linear(self.lora_config.lora_r,\n                                       self.output_dim,\n                                       bias=self.bias,\n                                       device=device,\n                                       dtype=dtype)\n        self.lora_weight_1.weight.requires_grad = True\n        self.lora_weight_2.weight.requires_grad = True\n\n    def full_weight(self):\n        # This assumes weights are evenly sharded across gpus. which might not be correct.\n        # in that case, we should flatten before all_gather.\n        local_weight = self.base_weight.dequantized() if isinstance(self.base_weight,\n                                                                    QuantizedParameter) else self.base_weight\n        tensor_list = [\n            torch.zeros_like(local_weight, device=local_weight.device, dtype=local_weight.dtype)\n            for _ in range(self.zero_shards)\n        ]\n        dist.all_gather(tensor_list, local_weight)\n        weight = nn.Parameter(torch.cat([tensor for tensor in tensor_list], dim=1))\n        return weight\n\n    def linear_without_F_linear(self, input, weight):\n        output = torch.mm(input.reshape(-1, input.shape[-1]), weight)\n        output = output.view(*input.shape[:-1], weight.shape[1])\n        return output\n\n    def forward(self, input_tensor):\n        # Gather the sharded base weight\n        if self.zero_shards > 1:\n            with torch.no_grad():\n                base_weight = self.full_weight()\n        elif self.quantization_config:\n            base_weight = self.base_weight.dequantized()\n        else:\n            base_weight = self.base_weight\n\n        base_weight_output = F.linear(input_tensor, base_weight)\n        lora_output = self.lora_weight_2(self.lora_weight_1(input_tensor))\n        return base_weight_output + self.lora_scaling_factor * lora_output\n", "deepspeed/linear/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .optimized_linear import OptimizedLinear\nfrom .config import LoRAConfig, QuantizationConfig\n", "deepspeed/inference/engine.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nimport time\nimport os\nfrom deepspeed import comm as dist\nfrom deepspeed.utils.logging import log_dist\n\nfrom torch.nn.modules import Module\nfrom packaging import version as pkg_version\nfrom deepspeed.runtime.checkpoint_engine.torch_checkpoint_engine import TorchCheckpointEngine\nfrom deepspeed.utils.timer import SynchronizedWallClockTimer\n\nfrom ..runtime.state_dict_factory import SDLoaderFactory\nfrom ..runtime.weight_quantizer import WeightQuantization\nfrom ..module_inject import replace_transformer_layer, generic_injection\nfrom ..comm.comm import init_distributed\nfrom ..pipe import PipelineModule\nfrom ..moe.utils import has_moe_layers\nfrom ..module_inject import LinearAllreduce, LinearLayer, Normalize, ReplaceWithTensorSlicing\nfrom deepspeed.accelerator import get_accelerator\nfrom ..module_inject.policy import TransformerPolicy\nfrom ..module_inject.auto_tp import AutoTP\n\nfrom ..module_inject.replace_policy import generic_policies\nfrom ..module_inject.auto_tp_model_utils import build_bloom_alibi_tensor, build_mpt_atten_bias_tensor, build_mpt_alibi_tensor, get_alibi_mask\nfrom ..ops.transformer.inference.ds_attention import DeepSpeedSelfAttention\nfrom ..model_implementations.transformers.ds_transformer import DeepSpeedTransformerInference\n\nDS_INFERENCE_ENABLED = False\nfrom torch import nn\n\nINFERENCE_MODEL_TIMER = \"model-forward-inference\"\n\n\nclass InferenceEngine(Module):\n    inference_mp_group = None\n    inference_ep_group = None\n    expert_mp_group = None\n\n    def __init__(self, model, config):\n        \"\"\"\n        Args:\n            model: torch.nn.Module\n            config: DeepSpeedInferenceConfig\n        \"\"\"\n        global DS_INFERENCE_ENABLED\n        DS_INFERENCE_ENABLED = True\n\n        super().__init__()\n\n        # Have to import here because inference_module is a global, but python\n        # globals only work at the module level and will not be updated unless\n        # we import it each time we init a new inference engine.\n        from ..model_implementations.transformers.ds_transformer import inference_module\n        if inference_module is not None:\n            self.destroy()\n\n        self.module = model\n        self._config = config\n\n        self._get_model_config_generate(config)  # keep for weird backward compatibility\n\n        # patch model generate with ours if model uses it\n        if hasattr(self.module, \"generate\"):\n            self.generate = self._generate\n\n        if hasattr(self.module, \"config\"):\n            TransformerPolicy.hf_model_config = self.module.config\n\n        if config.dtype == torch.half and not get_accelerator().is_fp16_supported():\n            raise ValueError(\"Type fp16 is not supported.\")\n\n        # todo: keep this self.injection_dict because we don't use to change config.injection_policy API\n        # todo: this will get changed when Molly's PR on auto injection dict is merged\n        self.injection_dict = config.injection_policy\n\n        # todo: refactor the mp_group and mp_size related in the next refactor\n        self.mp_group = config.tensor_parallel.tp_group\n        self.mpu = config.tensor_parallel.mpu\n\n        #self._validate_args(self.mpu, config.replace_with_kernel_inject)\n        self.quantize_merge_count = 1\n        self.quantization_scales = None\n\n        # these are not needed in the config as we are creating them ourselves in the inference engine\n        self.ep_group = None  # config.moe.ep_group\n        self.expert_mp_group = None  # config.moe.ep_mp_group\n\n        self.cuda_graph_created = False\n        self.checkpoint_engine = TorchCheckpointEngine()\n        quantization_setting = None\n        self._init_quantization_setting(\n            quantization_setting)  # todo: update with the new quant config for weight quant\n        self.model_profile_enabled = False\n        self._model_times = []\n\n        if not self.injection_dict and config.replace_with_kernel_inject:\n            # This is a hack to remove the prepare_mask function on HF side for BLOOM architecture\n            self.remove_mask_prepare_for_bloom()\n\n        if self.injection_dict or not config.replace_with_kernel_inject:\n            # This is a hack to redefine the alibi func due to TP\n            if config.tensor_parallel.tp_size > 1:\n                self.build_alibi_tensor()\n                self.build_attn_bias()\n\n        if get_accelerator().device_name() == 'cuda' and config.enable_cuda_graph:\n            assert pkg_version.parse(torch.__version__) >= pkg_version.parse(\"1.10\"), \\\n                \"If you want to use cuda graph, please upgrade torch to at least v1.10\"\n\n        # convert model to intended dtype\n        if config.dtype:\n            self._convert_to_dtype(config)\n\n        if self.mpu:\n            config.tensor_parallel.tp_size = dist.get_world_size(group=self.mpu.get_model_parallel_group())\n            self.mp_group = self.mpu.get_model_parallel_group()\n        elif config.tensor_parallel.tp_size > 1:\n            self._create_model_parallel_group(config)\n            config.tensor_parallel.tp_group = self.mp_group\n\n        if isinstance(self.module, torch.nn.Module):\n            moe, _ = has_moe_layers(self.module)\n        else:\n            moe = False\n\n        if moe and dist.get_world_size() > 1:\n            self._create_ep_parallel_group(config.moe.moe_experts)\n\n        # We only support three modes: 1) user specified policy for tensor-parallelism, 2) kernel injection (replace_with_kernel_inject), and 3) automatic tensor parallelism if tp_size > 1.\n        if self.injection_dict:\n            # 1. User specified Tensor Parallelism\n            assert not config.replace_with_kernel_inject, \"Cannot use both user specified injection policy and kernel injection\"\n            for client_module, injection_policy in self.injection_dict.items():\n\n                assert issubclass(client_module,\n                                  torch.nn.Module), f\"{client_module} is not a subclass of torch.nn.Module\"\n\n                # construct the tuple and pass that instead of a string or dict.\n                if isinstance(injection_policy, str):\n                    config.injection_policy_tuple = (injection_policy, )\n                else:\n                    config.injection_policy_tuple = injection_policy\n\n                layer_names = [name for name, _ in self.module.named_modules()]\n                for policy in config.injection_policy_tuple:\n                    if not any(name.endswith(policy) for name in layer_names):\n                        raise ValueError(f\"Injection policy layer'{policy}' not valid.\")\n\n                self._apply_injection_policy(config, client_module)\n        else:\n            if config.replace_with_kernel_inject:\n                # 2. DeepSpeed Kernel Injection\n                self._apply_injection_policy(config)\n            elif config.tensor_parallel.tp_size > 1:\n                # 3. Automatic Tensor Parallelism\n                parser_dict = AutoTP.tp_parser(model)\n                print(\"AutoTP: \", parser_dict)\n                for client_module, injection_policy in parser_dict:\n                    if isinstance(injection_policy, str):\n                        config.injection_policy_tuple = (injection_policy, )\n                    else:\n                        config.injection_policy_tuple = injection_policy\n                    self._apply_injection_policy(config, client_module)\n\n        device = get_accelerator().current_device_name()\n        # NOTE: This check assumes a Hugging Face hierarchy for the device type i.e. module.device.type\n        is_meta_device = hasattr(self.module, \"device\") and self.module.device.type == 'meta'\n        if is_meta_device:\n            self.module.to_empty(device=device)\n        else:\n            self.module.to(device)\n\n        if config.tensor_parallel.tp_size > 1:\n            _rng_state = get_accelerator().get_rng_state().to(get_accelerator().current_device_name())\n            dist.broadcast(_rng_state, 0)\n            get_accelerator().set_rng_state(_rng_state.cpu())\n\n        if config.tensor_parallel.tp_size > 1:\n            assert not config.enable_cuda_graph, \"Cuda graph is not supported for model parallelism\"\n\n        # Check if local CUDA graphs can be created in replacement modules\n        self.local_cuda_graph = self._local_cuda_graph_used(self.module)\n\n    def destroy(self):\n        # Have to import here because inference_module is a global, but python\n        # globals only work at the module level and will not be updated unless\n        # we import it each time we init a new inference engine.\n        from ..model_implementations.transformers.ds_transformer import inference_module\n        DeepSpeedTransformerInference.layer_id = 0\n        DeepSpeedSelfAttention.num_layers = 0\n        if inference_module is not None:\n            inference_module.release_workspace()\n            inference_module = None\n\n    def profile_model_time(self, use_cuda_events=True):\n        if not self.model_profile_enabled and not self._config.enable_cuda_graph:\n            self.module.register_forward_pre_hook(self._pre_forward_hook)\n            self.module.register_forward_hook(self._post_forward_hook)\n        self.model_profile_enabled = True\n        self.use_cuda_events = use_cuda_events\n        if self.use_cuda_events:\n            self.timers = SynchronizedWallClockTimer()\n\n    # todo: remove this once all the config dicts are centralized from top level pydantic config\n    def _get_model_config_generate(self, config):\n        # this is being passed to replace_transformer_layer(config=self.user_model_config_dict)\n        self.config = getattr(self.module, 'config', None) if config.config is None else config.config\n\n    def remove_mask_prepare_for_bloom(self):\n        if hasattr(self.module, 'transformer'):\n            if hasattr(self.module.transformer, '_prepare_attn_mask'):\n                self.module.transformer._prepare_attn_mask = lambda attention_mask, *args, **kwargs: attention_mask\n\n    def build_alibi_tensor(self):\n        if hasattr(self.module, 'transformer'):\n            if hasattr(self.module.transformer, 'build_alibi_tensor'):\n                self.module.transformer.build_alibi_tensor = build_bloom_alibi_tensor\n            if hasattr(self.module.transformer, 'build_mpt_alibi_tensor'):\n                self.module.transformer.build_mpt_alibi_tensor_orig = self.module.transformer.build_mpt_alibi_tensor\n                self.module.transformer.__class__.build_mpt_alibi_tensor = build_mpt_alibi_tensor\n        if hasattr(self.module, 'model'):\n            if hasattr(self.module.model, 'get_alibi_mask'):\n                self.module.model.get_alibi_mask_orig = self.module.model.get_alibi_mask\n                self.module.model.__class__.get_alibi_mask = get_alibi_mask\n\n    def build_attn_bias(self):\n        if hasattr(self.module, 'transformer'):\n            if hasattr(self.module.transformer, '_attn_bias'):\n                self.module.transformer._attn_bias_orig = self.module.transformer._attn_bias\n                self.module.transformer.__class__._attn_bias = build_mpt_atten_bias_tensor\n\n    def _pre_forward_hook(self, module, *inputs, **kwargs):\n        if self.use_cuda_events:\n            self.timers(INFERENCE_MODEL_TIMER).start()\n        else:\n            get_accelerator().synchronize()\n            self._start = time.time()\n\n    def _post_forward_hook(self, module, input, output):\n        if self.use_cuda_events:\n            self.timers(INFERENCE_MODEL_TIMER).stop()\n            elapsed_time = self.timers(INFERENCE_MODEL_TIMER).elapsed(reset=True)\n        else:\n            get_accelerator().synchronize()\n            self._end = time.time()\n            elapsed_time = (self._end - self._start) * 1e3  # convert seconds to ms\n        self._model_times.append(elapsed_time)\n\n    def _create_model_parallel_group(self, config):\n        # Call the init process\n        if InferenceEngine.inference_mp_group is None:\n            init_distributed()\n            local_rank = int(os.getenv('LOCAL_RANK', '0'))\n            get_accelerator().set_device(local_rank)\n\n            ranks = [i for i in range(config.tensor_parallel.tp_size)]\n            self.mp_group = dist.new_group(ranks)\n            InferenceEngine.inference_mp_group = self.mp_group\n        else:\n            self.mp_group = InferenceEngine.inference_mp_group\n\n    def _create_ep_parallel_group(self, moe_experts):\n        # Call the init process\n        self.ep_group = {}\n        self.expert_mp_group = {}\n        moe_experts = moe_experts if type(moe_experts) is list else [moe_experts]\n        for e in moe_experts:\n            self.ep_group.update({e: None})\n            self.expert_mp_group.update({e: None})\n        for moe_ep_size in self.ep_group.keys():\n            num_ep_groups = dist.get_world_size() // moe_ep_size\n            for i in range(num_ep_groups):\n                ep_cnt = i * moe_ep_size\n                size = dist.get_world_size() if moe_ep_size > dist.get_world_size() else moe_ep_size\n                ranks = list(range(ep_cnt, ep_cnt + size))\n                _ep_group = dist.new_group(ranks)\n                if dist.get_rank() in ranks:\n                    self.ep_group.update({moe_ep_size: _ep_group})\n\n            if dist.get_world_size() > moe_ep_size:\n                num_expert_mp_groups = dist.get_world_size() // num_ep_groups\n                expert_mp_size = dist.get_world_size() // moe_ep_size\n                for i in range(num_expert_mp_groups):\n                    expert_mp_comm_ranks = [i + nr * moe_ep_size for nr in range(expert_mp_size)]\n                    _expert_mp_group = dist.new_group(expert_mp_comm_ranks)\n                    if dist.get_rank() in expert_mp_comm_ranks:\n                        self.expert_mp_group.update({moe_ep_size: _expert_mp_group})\n\n    def _init_quantization_setting(self, quantization_setting):\n        self.quantize_bits = 8\n        self.mlp_extra_grouping = False\n        self.quantize_groups = 1\n        if type(quantization_setting) is tuple:\n            self.mlp_extra_grouping, \\\n            self.quantize_groups = quantization_setting\n        elif quantization_setting is not None:\n            self.quantize_groups = quantization_setting\n        log_dist(\n            f\"quantize_bits = {self.quantize_bits} \"\n            f\"mlp_extra_grouping = {self.mlp_extra_grouping}, \"\n            f\"quantize_groups = {self.quantize_groups}\", [0])\n\n    # TODO: remove this function and add this functionality to pydantic config checking\n    def _validate_args(self, mpu, replace_with_kernel_inject):\n        # TODO: to support SD pipeline we need to avoid this check for now\n        if replace_with_kernel_inject and not isinstance(self.module, Module):\n            raise ValueError(f\"model must be a torch.nn.Module, got {type(self.module)}\")\n        if not isinstance(self._config.tensor_parallel.tp_size, int) or self._config.tensor_parallel.tp_size < 1:\n            raise ValueError(f\"mp_size must be an int >= 1, got {self._config.tensor_parallel.tp_size}\")\n\n        if mpu:\n            methods = [\"get_model_parallel_group\", \"get_data_parallel_group\"]\n            for method in methods:\n                if not hasattr(mpu, method):\n                    raise ValueError(f\"mpu is missing {method}\")\n        if self._config.checkpoint is not None and not isinstance(self._config.checkpoint, (str, dict)):\n            raise ValueError(f\"checkpoint must be None, str or dict, got {type(self._config.checkpoint)}\")\n\n        supported_dtypes = [None, torch.half, torch.int8, torch.float]\n        if self._config.dtype not in supported_dtypes:\n            raise ValueError(f\"{self._config.dtype} not supported, valid dtype: {supported_dtypes}\")\n\n        if self.injection_dict is not None and not isinstance(self.injection_dict, dict):\n            raise ValueError(f\"injection_dict must be None or a dict, got: {self.injection_dict}\")\n\n    def load_model_with_checkpoint(self, r_module):\n        self.mp_replace = ReplaceWithTensorSlicing(\n            mp_group=self.mp_group, mp_size=self._config.tensor_parallel.tp_size)  #, out_dim=0, in_dim=1)\n        error_msgs = []\n\n        def load(module, state_dict, prefix):\n            args = (state_dict, prefix, {}, True, [], [], error_msgs)\n            if hasattr(module, 'weight'):\n                if module.weight.data.is_meta:\n                    # meta tensor cannot be casted or copied to, so we need to replace it with a normal tensor here\n                    module.weight = torch.nn.parameter.Parameter(data=torch.empty_like(module.weight.data,\n                                                                                       device=\"cpu\"),\n                                                                 requires_grad=module.weight.data.requires_grad)\n                if 'query_key_value' in prefix:\n                    module.weight = self.mp_replace.strided_copy(module.weight.data,\n                                                                 state_dict[prefix + 'weight'],\n                                                                 num_splits=3)\n                else:\n                    module.weight = self.mp_replace.copy(module.weight.data, state_dict[prefix + 'weight'])\n            else:\n                if module.norm.weight.data.is_meta:\n                    # meta tensor cannot be casted or copied to, so we need to replace it with a normal tensor here\n                    module.norm.weight = torch.nn.parameter.Parameter(\n                        data=torch.empty_like(module.norm.weight.data, device=\"cpu\"),\n                        requires_grad=module.norm.weight.data.requires_grad)\n                module.norm.weight = self.mp_replace.copy(module.norm.weight.data, state_dict[prefix + 'weight'])\n            if prefix + 'bias' in self.key_list:\n                if hasattr(module, 'norm'):\n                    if module.norm.bias.data.is_meta:\n                        # meta tensor cannot be casted or copied to, so we need to replace it with a normal tensor here\n                        module.norm.bias = torch.nn.parameter.Parameter(\n                            data=torch.empty_like(module.norm.bias.data, device=\"cpu\"),\n                            requires_grad=module.norm.bias.data.requires_grad)\n                    module.norm.bias = self.mp_replace.copy(module.norm.bias, state_dict[prefix + 'bias'])\n                else:\n                    if module.bias.data.is_meta:\n                        # meta tensor cannot be casted or copied to, so we need to replace it with a normal tensor here\n                        module.bias = torch.nn.parameter.Parameter(data=torch.empty_like(module.bias.data,\n                                                                                         device=\"cpu\"),\n                                                                   requires_grad=module.bias.data.requires_grad)\n                    data = state_dict[prefix + 'bias']\n                    data = data.to(get_accelerator().current_device_name())\n                    module.bias = self.mp_replace.copy(module.bias, data)\n\n        layer_policies = {\n            nn.Linear: load,\n            nn.Embedding: load,\n            nn.LayerNorm: load,\n            LinearLayer: load,\n            LinearAllreduce: load\n        }\n\n        def load_module_recursive(module, prefix='', level=0):\n            for name, child in module.named_children():\n                if child.__class__ in layer_policies:\n                    checking_key = prefix + name + '.'\n                    if not any(checking_key in item for item in self.key_list):\n                        continue\n                    if len(list(child.parameters())) > 0 and list(child.parameters())[0].numel() == 0:\n                        if len(child.weight.ds_shape) == 1:\n                            child = Normalize(dim=child.weight.ds_shape[-1], dtype=child.weight.dtype, eps=child.eps)\n                            setattr(module, name, child)\n                    load(child, self.sd, prefix + name + '.')\n                else:\n                    load_module_recursive(child, prefix if level == 0 else prefix + name + '.', level + 1)\n\n        load_module_recursive(r_module)\n\n        embedding_weight = None\n\n        for n, p in r_module.named_parameters():\n            if \"word_embeddings.\" in n or \"embed_tokens.\" in n or \"wte.\" in n:\n                embedding_weight = p\n        if embedding_weight is not None and hasattr(r_module, \"lm_head\") and hasattr(\n                r_module.lm_head, \"weight\") and r_module.lm_head.weight.is_meta:\n            r_module.lm_head.weight = embedding_weight\n\n    def _apply_injection_policy(self, config, client_module=None):\n        # client_module is only passed when using the injection_dict method.\n        checkpoint_dir = config.checkpoint\n        checkpoint = SDLoaderFactory.get_sd_loader_json(checkpoint_dir,\n                                                        self.checkpoint_engine) if checkpoint_dir is not None else None\n\n        generic_injection(self.module, dtype=config.dtype, enable_cuda_graph=config.enable_cuda_graph)\n\n        if isinstance(self.module, torch.nn.Module):\n            # config is our DeepSpeedInferenceConfig and self.config is the HF model config\n            replace_transformer_layer(client_module, self.module, checkpoint, config, self.config)\n\n    def _get_all_ckpt_names(self, checkpoints_path, tag):\n        ckpt_file_pattern = self._get_ckpt_name(checkpoints_path, tag, mp_placeholder=\"*\")\n        import glob\n\n        ckpt_files = glob.glob(ckpt_file_pattern)\n        ckpt_files.sort()\n        return ckpt_files\n\n    def _get_ckpt_name(self, checkpoints_path, tag, mp_placeholder=None):\n        if mp_placeholder is not None:\n            mp_rank_str = mp_placeholder\n        else:\n            mp_rank = 0 if self.mpu is None else self.mpu.get_model_parallel_rank()\n            mp_rank_str = \"{:02d}\".format(mp_rank)\n\n        ckpt_name = os.path.join(\n            checkpoints_path,\n            \"mp_rank_\" + mp_rank_str + \"_model_states.pt\",\n        )\n        return ckpt_name\n\n    def _load_checkpoint(self, load_dir, load_module_strict=True, tag=None):\n        is_pipe_parallel = isinstance(self.module, PipelineModule)\n        if is_pipe_parallel:\n            raise RuntimeError('pipeline parallelism is currently not supported in inference.')\n        if not isinstance(load_dir, dict) and os.path.isdir(load_dir):\n            if tag is None:\n                latest_path = os.path.join(load_dir, \"latest\")\n                if os.path.isfile(latest_path):\n                    with open(latest_path, \"r\") as fd:\n                        tag = fd.read().strip()\n\n            ckpt_list = self._get_all_ckpt_names(load_dir, tag)\n            sd_loader = SDLoaderFactory.get_sd_loader(ckpt_list, self.checkpoint_engine)\n        else:\n            sd_loader = SDLoaderFactory.get_sd_loader_json(load_dir, self.checkpoint_engine)\n\n        checkpoint = sd_loader['checkpoints']\n\n        if type(checkpoint) is list:\n            self.sd = torch.load(checkpoint[0], map_location='cpu')\n            self.key_list = list(self.sd.keys())\n\n            self.load_model_with_checkpoint(self.module)\n\n            for i in range(1, len(checkpoint)):\n                if not dist.is_initialized() or dist.get_rank() == 0:\n                    print(f\"loading checkpoint ({i})\")\n                self.sd = torch.load(checkpoint[i], map_location=get_accelerator().device_name())\n                self.key_list = list(self.sd.keys())\n                self.load_model_with_checkpoint(self.module)\n        else:\n            mp_rank = 0 if self.mpu is None else self.mpu.get_model_parallel_rank()\n\n            load_path, checkpoint, quantize_config = sd_loader.load(self._config.tensor_parallel.tp_size,\n                                                                    mp_rank,\n                                                                    is_pipe_parallel=is_pipe_parallel,\n                                                                    quantize=(self._config.dtype is torch.int8),\n                                                                    quantize_groups=self.quantize_groups,\n                                                                    mlp_extra_grouping=self.mlp_extra_grouping)\n\n            self.quantization_scales, self.quantize_merge_count = quantize_config\n\n            moe, _ = has_moe_layers(self.module)\n            if moe:\n                from deepspeed.runtime.engine import DeepSpeedEngine\n                old_moe_load = False\n                if not isinstance(checkpoint['num_experts'], list):\n                    old_moe_load = True\n                DeepSpeedEngine.load_moe_state_dict(load_dir,\n                                                    tag,\n                                                    state_dict=checkpoint[self._choose_module_key(checkpoint)],\n                                                    old_moe_load=old_moe_load,\n                                                    model=self.module,\n                                                    mpu=self.mpu,\n                                                    checkpoint_engine=self.checkpoint_engine)\n\n            self.module.load_state_dict(state_dict=checkpoint[self._choose_module_key(checkpoint)],\n                                        strict=load_module_strict)\n\n    def _choose_module_key(self, sd):\n        assert not ('module' in sd\n                    and 'model' in sd), \"checkpoint has both 'model' and 'module' keys, not sure how to proceed\"\n        assert 'module' in sd or 'model' in sd, \"checkpoint contains neither 'model' or 'module' keys, not sure how to proceed\"\n        if 'module' in sd:\n            return 'module'\n        elif 'model' in sd:\n            return 'model'\n\n    def _convert_to_dtype(self, config):\n        if not isinstance(self.module, torch.nn.Module):\n            return\n\n        if False:  #config.dtype is torch.int8 and self.quantization_scales is None:\n            quantizer = WeightQuantization(mlp_extra_grouping=self.mlp_extra_grouping)\n            model, self.quantization_scales = quantizer.model_quantize(self.module, self.injection_dict,\n                                                                       self.quantize_bits, self.quantize_groups)\n        elif config.dtype == torch.half:\n            self.module.half()\n        elif config.dtype == torch.bfloat16:\n            self.module.bfloat16()\n        elif config.dtype == torch.float:\n            self.module.float()\n\n    def _create_cuda_graph(self, *inputs, **kwargs):\n        # warmup to create the workspace and cublas handle\n        cuda_stream = get_accelerator().Stream()\n        cuda_stream.wait_stream(get_accelerator().current_stream())\n        with get_accelerator().stream(cuda_stream):\n            for i in range(3):\n                ret = self.module(*inputs, **kwargs)\n        get_accelerator().current_stream().wait_stream(cuda_stream)\n\n        # create cuda_graph and assign static_inputs and static_outputs\n        self._cuda_graphs = get_accelerator().create_graph()\n        self.static_inputs = inputs\n        self.static_kwargs = kwargs\n\n        with get_accelerator().capture_to_graph(self._cuda_graphs):\n            self.static_output = self.module(*self.static_inputs, **self.static_kwargs)\n\n        self.cuda_graph_created = True\n\n    def _graph_replay(self, *inputs, **kwargs):\n        for i in range(len(inputs)):\n            if torch.is_tensor(inputs[i]):\n                self.static_inputs[i].copy_(inputs[i])\n        for k in kwargs:\n            if torch.is_tensor(kwargs[k]):\n                self.static_kwargs[k].copy_(kwargs[k])\n        get_accelerator().replay_graph(self._cuda_graphs)\n        return self.static_output\n\n    def model_times(self):\n        assert self.model_profile_enabled, \"model profiling is not enabled\"\n        model_times = self._model_times\n        if self._config.enable_cuda_graph and len(self._model_times) == 0:\n            raise ValueError(\"Model times are empty and cuda graph is enabled. If \"\n                             \"this is a GPT-style model this combo is not supported. If this is a \"\n                             \"BERT-style model this is a bug, please report it. \"\n                             f\"Model type is: {type(self.module)}\")\n        self._model_times = []\n        return model_times\n\n    def _module_match(self, module):\n        for policy in generic_policies:\n            policy = policy()\n            if policy.match_replaced(module):\n                return True\n        return False\n\n    def _local_cuda_graph_used(self, module):\n        if isinstance(module, torch.nn.Module):\n            return False\n        else:\n            sub_module_cuda_graph = False\n            for name in module.__dict__.keys():\n                sub_module = getattr(module, name)\n\n                if self._module_match(sub_module) and hasattr(sub_module, \"enable_cuda_graph\"):\n                    sub_module_cuda_graph = True\n\n            return sub_module_cuda_graph\n\n    def forward(self, *inputs, **kwargs):\n        \"\"\"Execute forward propagation\n\n        Arguments:\n            *inputs: Variable length input list\n            **kwargs: variable length keyword arguments\n        \"\"\"\n        start = None\n        if self.model_profile_enabled and get_accelerator().device_name() == 'cuda' and self._config.enable_cuda_graph:\n            get_accelerator().synchronize()\n            start = time.time()\n\n        if get_accelerator().device_name() == 'cuda' and self._config.enable_cuda_graph and not self.local_cuda_graph:\n            if self.cuda_graph_created:\n                outputs = self._graph_replay(*inputs, **kwargs)\n            else:\n                self._create_cuda_graph(*inputs, **kwargs)\n                outputs = self._graph_replay(*inputs, **kwargs)\n\n        else:\n            outputs = self.module(*inputs, **kwargs)\n\n        if self.model_profile_enabled and self._config.enable_cuda_graph:\n            get_accelerator().synchronize()\n            duration = (time.time() - start) * 1e3  # convert seconds to ms\n            self._model_times.append(duration)\n\n        return outputs\n\n    def _generate(self, *inputs, **kwargs):\n        # Reset KV-cache at the beginning of generate\n        if hasattr(self.module, 'reset_cache'):\n            self.module.reset_cache()\n        num_beams = 1\n        if \"generation_config\" in kwargs:\n            gen_config = kwargs[\"generation_config\"]\n            num_beams = getattr(gen_config, \"num_beams\", 1)\n        if \"num_beams\" in kwargs:\n            num_beams = kwargs[\"num_beams\"]\n\n        if num_beams > 1:\n            raise NotImplementedError(\"DeepSpeed does not support `num_beams` > 1, if this is important to you please \"\n                                      \"add your request to: https://github.com/microsoft/DeepSpeed/issues/2506\")\n\n        if (\"input_ids\" in kwargs) and (kwargs[\"input_ids\"].dim() == 2):\n            for input_tensor in kwargs[\"input_ids\"]:\n                tensor_length = input_tensor.shape[-1]\n                if tensor_length > self._config.max_out_tokens:\n                    raise RuntimeError(\n                        f\"Input with size {tensor_length} exceeds maximum length of {self._config.max_out_tokens}. Please increase `max_tokens` in the DeepSpeed Inference Config.\"\n                    )\n\n        return self.module.generate(*inputs, **kwargs)\n", "deepspeed/inference/config.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nimport deepspeed\nfrom deepspeed.pydantic_v1 import Field, validator\nfrom deepspeed.runtime.config_utils import DeepSpeedConfigModel\nfrom deepspeed.runtime.zero.config import DeepSpeedZeroConfig\nfrom typing import Dict, Union\nfrom enum import Enum\n\n\nclass DtypeEnum(Enum):\n    # The torch dtype must always be the first value (so we return torch.dtype)\n    fp16 = torch.float16, \"torch.float16\", \"fp16\", \"float16\", \"half\"\n    fp32 = torch.float32, \"torch.float32\", \"fp32\", \"float32\", \"float\"\n    bf16 = torch.bfloat16, \"torch.bfloat16\", \"bf16\", \"bfloat16\", \"bfloat\"\n    int8 = torch.int8, \"torch.int8\", \"int8\"\n\n    # Copied from https://stackoverflow.com/a/43210118\n    # Allows us to use multiple values for each Enum index and returns first\n    # listed value when Enum is called\n    def __new__(cls, *values):\n        obj = object.__new__(cls)\n        # first value is canonical value\n        obj._value_ = values[0]\n        for other_value in values[1:]:\n            cls._value2member_map_[other_value] = obj\n        obj._all_values = values\n        return obj\n\n    def __repr__(self):\n        return \"<%s.%s: %s>\" % (\n            self.__class__.__name__,\n            self._name_,\n            \", \".join([repr(v) for v in self._all_values]),\n        )\n\n\nclass MoETypeEnum(str, Enum):\n    residual = \"residual\"\n    standard = \"standard\"\n\n\nclass DeepSpeedTPConfig(DeepSpeedConfigModel):\n    \"\"\" Configure tensor parallelism settings \"\"\"\n\n    enabled: bool = True\n    \"\"\" Turn tensor parallelism on/off. \"\"\"\n\n    tp_size: int = 1\n    \"\"\" Number of devices to split the model across using tensor parallelism. \"\"\"\n\n    mpu: object = None\n    \"\"\"\n    A model parallelism unit object that implements\n    ``get_{model,data}_parallel_{rank,group,world_size}()``.\n    \"\"\"\n\n    tp_group: object = None\n\n\nclass DeepSpeedMoEConfig(DeepSpeedConfigModel):\n    \"\"\" Sets parameters for MoE \"\"\"\n\n    enabled: bool = True\n    ep_size: int = 1\n    \"\"\"\n    The expert-parallelism size which is used for partitioning the experts\n    across the GPUs in the expert-parallel group.\n    \"\"\"\n\n    moe_experts: list = Field([1], alias=\"num_experts\")\n    \"\"\" The global number of experts used in an MoE layer. \"\"\"\n\n    type: MoETypeEnum = MoETypeEnum.standard\n    \"\"\"\n    Specify the type of MoE layer. We have two types of MoE layer: 'Standard'\n    and 'Residual'.\n    \"\"\"\n\n    ep_mp_group: object = None\n    ep_group: object = Field(None, alias=\"expert_group\")\n\n\nclass QuantTypeEnum(str, Enum):\n    asym = \"asymmetric\"\n    sym = \"symmetric\"\n\n\nclass BaseQuantConfig(DeepSpeedConfigModel):\n    enabled = True\n    num_bits = 8\n    q_type: QuantTypeEnum = QuantTypeEnum.sym\n    q_groups: int = 1\n\n\nclass WeightQuantConfig(BaseQuantConfig):\n    enabled = True\n    quantized_initialization: Dict = {}\n    post_init_quant: Dict = {}\n\n\nclass ActivationQuantConfig(BaseQuantConfig):\n    enabled = True\n\n\nclass QKVQuantConfig(DeepSpeedConfigModel):\n    enabled = True\n\n\nclass QuantizationConfig(DeepSpeedConfigModel):\n    enabled: bool = True\n    activation: ActivationQuantConfig = ActivationQuantConfig()\n    weight: WeightQuantConfig = WeightQuantConfig()\n    qkv: QKVQuantConfig = QKVQuantConfig()\n\n\n# todo: brainstorm on how to do ckpt loading for DS inference\nclass InferenceCheckpointConfig(DeepSpeedConfigModel):\n    checkpoint_dir: str = None\n    save_mp_checkpoint_path: str = None\n    base_dir: str = None\n\n\nclass DeepSpeedInferenceConfig(DeepSpeedConfigModel):\n    \"\"\" Sets parameters for DeepSpeed Inference Engine. \"\"\"\n\n    replace_with_kernel_inject: bool = Field(False, alias=\"kernel_inject\")\n    \"\"\"\n    Set to true to inject inference kernels for models such as, Bert, GPT2,\n    GPT-Neo and GPT-J.  Otherwise, the injection_dict provides the names of two\n    linear layers as a tuple:\n    `(attention_output projection, transformer output projection)`\n    \"\"\"\n\n    dtype: DtypeEnum = torch.float16\n    \"\"\"\n    Desired model data type, will convert model to this type.\n    Supported target types: `torch.half`, `torch.int8`, `torch.float`\n    \"\"\"\n\n    tensor_parallel: DeepSpeedTPConfig = Field({}, alias=\"tp\")\n    \"\"\"\n    Configuration for tensor parallelism used to split the model across several\n    GPUs. Expects a dictionary containing values for :any:`DeepSpeedTPConfig`.\n    \"\"\"\n\n    enable_cuda_graph: bool = False\n    \"\"\"\n    Use this flag for capturing the CUDA-Graph of the inference ops, so that it\n    can run faster using the graph replay method.\n    \"\"\"\n\n    use_triton: bool = False\n    \"\"\"\n    Use this flag to use triton kernels for inference ops.\n    \"\"\"\n\n    triton_autotune: bool = False\n    \"\"\"\n    Use this flag to enable triton autotuning.\n    Turning it on is better for performance but increase the 1st runtime for\n    autotuning.\n    \"\"\"\n\n    zero: DeepSpeedZeroConfig = {}\n    \"\"\"\n    ZeRO configuration to use with the Inference Engine. Expects a dictionary\n    containing values for :any:`DeepSpeedZeroConfig`.\n    \"\"\"\n\n    triangular_masking: bool = Field(True, alias=\"tm\")\n    \"\"\"\n    Controls the type of masking for attention scores in transformer layer.\n    Note that the masking is application specific.\n    \"\"\"\n\n    moe: Union[bool, DeepSpeedMoEConfig] = {}\n    \"\"\"\n    Specify if the type of Transformer is MoE. Expects a dictionary containing\n    values for :any:`DeepSpeedMoEConfig`.\n    \"\"\"\n\n    quant: QuantizationConfig = {}\n    \"\"\"\n    NOTE: only works for int8 dtype.\n    Quantization settings used for quantizing your model using the MoQ.  The\n    setting can be one element or a tuple. If one value is passed in, we\n    consider it as the number of groups used in quantization. A tuple is passed\n    in if we want to mention that there is extra-grouping for the MLP part of a\n    Transformer layer (e.g. (True, 8) shows we quantize the model using 8\n    groups for all the network except the MLP part that we use 8 extra\n    grouping). Expects a dictionary containing values for\n    :any:`QuantizationConfig`.\n    \"\"\"\n\n    #todo: refactor the following 3 into the new checkpoint_config\n    checkpoint: Union[str, Dict] = None\n    \"\"\"\n    Path to deepspeed compatible checkpoint or path to JSON with load policy.\n    \"\"\"\n\n    base_dir: str = \"\"\n    \"\"\"\n    This shows the root directory under which all the checkpoint files exists.\n    This can be passed through the json config too.\n    \"\"\"\n\n    set_empty_params: bool = False\n    \"\"\"\n    specifying whether the inference-module is created with empty or real Tensor\n    \"\"\"\n\n    save_mp_checkpoint_path: str = None\n    \"\"\"\n    The path for which we want to save the loaded model with a checkpoint. This\n    feature is used for adjusting the parallelism degree to help alleviate the\n    model loading overhead. It does not save any new checkpoint if no path is\n    passed.\n    \"\"\"\n\n    checkpoint_config: InferenceCheckpointConfig = Field({}, alias=\"ckpt_config\")\n    \"\"\"\n    TODO: Add docs. Expects a dictionary containing values for\n    :any:`InferenceCheckpointConfig`.\n    \"\"\"\n\n    return_tuple: bool = True\n    \"\"\"\n    Specify whether or not the transformer layers need to return a tuple or a\n    Tensor.\n    \"\"\"\n\n    training_mp_size: int = 1\n    \"\"\"\n    If loading a checkpoint this is the mp size that it was trained with, it\n    may be different than what the mp size that you want to use during\n    inference.\n    \"\"\"\n\n    replace_method: str = Field(\n        \"auto\",\n        deprecated=True,\n        deprecated_msg=\"This parameter is no longer needed, please remove from your call to DeepSpeed-inference\")\n\n    injection_policy: Dict = Field(None, alias=\"injection_dict\")\n    \"\"\"\n    Dictionary mapping a client nn.Module to its corresponding injection\n    policy. e.g., `{BertLayer : deepspeed.inference.HFBertLayerPolicy}`\n    \"\"\"\n\n    injection_policy_tuple: tuple = None\n    \"\"\" TODO: Add docs \"\"\"\n\n    config: Dict = Field(None, alias=\"args\")  # todo: really no need for this field if we can refactor\n\n    max_out_tokens: int = Field(1024, alias=\"max_tokens\")\n    \"\"\"\n    This argument shows the maximum number of tokens inference-engine can work\n    with, including the input and output tokens. Please consider increasing it\n    to the required token-length required for your use-case.\n    \"\"\"\n\n    min_out_tokens: int = Field(1, alias=\"min_tokens\")\n    \"\"\"\n    This argument communicates to the runtime the minimum number of tokens you\n    expect you will need to generate. This will cause the runtime to error\n    if it unable to provide this and provide context on the memory pressure\n    rather than seg-faulting or providing corrupted output.\n    \"\"\"\n\n    transposed_mode: bool = Field(False, alias=\"transposed_mode\")\n\n    mp_size: int = Field(1, deprecated=True, new_param=\"tensor_parallel.tp_size\")\n    \"\"\"\n    Desired model parallel size, default is 1 meaning no model parallelism.\n    Deprecated, please use the ``tensor_parallel` config to control model\n    parallelism.\n    \"\"\"\n    mpu: object = Field(None, deprecated=True, new_param=\"tensor_parallel.mpu\")\n    ep_size: int = Field(1, deprecated=True, new_param=\"moe.ep_size\")\n    ep_group: object = Field(None, alias=\"expert_group\", deprecated=True, new_param=\"moe.ep_group\")\n    ep_mp_group: object = Field(None, alias=\"expert_mp_group\", deprecated=True, new_param=\"moe.ep_mp_group\")\n    moe_experts: list = Field([1], deprecated=True, new_param=\"moe.moe_experts\")\n    moe_type: MoETypeEnum = Field(MoETypeEnum.standard, deprecated=True, new_param=\"moe.type\")\n\n    @validator(\"moe\")\n    def moe_backward_compat(cls, field_value, values):\n        if isinstance(field_value, bool):\n            return DeepSpeedMoEConfig(moe=field_value)\n        return field_value\n\n    @validator(\"use_triton\")\n    def has_triton(cls, field_value, values):\n        if field_value and not deepspeed.HAS_TRITON:\n            raise ValueError('Triton needs to be installed to use deepspeed with triton kernels')\n        return field_value\n\n    class Config:\n        # Get the str representation of the datatype for serialization\n        json_encoders = {torch.dtype: lambda x: str(x)}\n", "deepspeed/inference/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\nfrom .v2 import RaggedInferenceEngineConfig, DeepSpeedTPConfig\nfrom .v2.engine_v2 import InferenceEngineV2\nfrom .v2 import build_hf_engine, build_engine_from_ds_checkpoint\n", "deepspeed/inference/v2/scheduling_utils.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom enum import Enum\n\n\nclass SchedulingResult(Enum):\n\n    Success = 0\n    \"\"\"\n    The proposed batch is valid and can be scheduled.\n    \"\"\"\n\n    EngineSequenceLimitExceeded = 1\n    \"\"\"\n    The proposed batch would would overflow the number of concurrent sequences the engine may support.\n    \"\"\"\n\n    BatchSequenceLimitExceeded = 2\n    \"\"\"\n    The proposed batch contains more sequences than the engine was configured\n    to support in a single forwardp\n    \"\"\"\n\n    BatchTokenLimitExceeded = 3\n    \"\"\"\n    The proposed batch contains more tokens than the engine was configured\n    to support in a single forward.\n    \"\"\"\n\n    KVCacheLimitExceeded = 4\n    \"\"\"\n    The proposed batch would require more KV cache to be allocated than the engine\n    currently has available.\n    \"\"\"\n\n    SequenceTokenLimitExceeded = 5\n    \"\"\"\n    The proposed batch contains a sequence that is longer than the engine/model can support.\n    \"\"\"\n\n\nclass SchedulingError(RuntimeError):\n\n    result: SchedulingResult\n    \"\"\"\n    The failed result of the scheduling check. Guaranteed to not be SchedulingResult.Success.\n    \"\"\"\n\n    def __init__(self, result: SchedulingResult) -> None:\n        self.result = result\n        super().__init__(f\"Batch scheduling failed with result {result}\")\n", "deepspeed/inference/v2/logging.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport logging\n\nfrom deepspeed.utils.logging import LoggerFactory\n\ninf_logger = None\n\n\ndef inference_logger(level: int = logging.INFO) -> logging.Logger:\n    \"\"\"\n    Create the inference logger. NOTE: Logging is not cost free. On a 3960X,\n    there is a cost of about 6 us per call to a no-op logger, so this should\n    be used during setup only and not during the inference loop.\n\n    Args:\n        level (int, optional): The logging level. Defaults to logging.INFO.\n    \"\"\"\n    global inf_logger\n    if inf_logger is None:\n        inf_logger = LoggerFactory.create_logger(name=\"DS-Inference\", level=level)\n        inf_logger.debug(\"Inference logger created.\")\n    return inf_logger\n", "deepspeed/inference/v2/inference_parameter.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Dict\n\nimport torch\n\nCORE_PARAM = \"_ds_core_param_key\"\n\nSTR_TO_DTYPE = {\n    \"torch.float32\": torch.float32,\n    \"torch.float64\": torch.float64,\n    \"torch.float16\": torch.float16,\n    \"torch.bfloat16\": torch.bfloat16,\n    \"torch.int64\": torch.int64,\n    \"torch.int32\": torch.int32,\n    \"torch.int16\": torch.int16,\n    \"torch.int8\": torch.int8,\n    \"torch.uint8\": torch.uint8,\n    \"torch.bool\": torch.bool,\n}\n\n\nclass InferenceParameter(torch.Tensor):\n    \"\"\"\n    An extension of the torch.Tensor class to support our inference focused features. One important\n    thing to note here is that an InferenceParam can be used a torch.Tensor, but outputs of\n    torch.Tensor operations will not be InferenceParams.\n    \"\"\"\n\n    @staticmethod\n    def __new__(cls, tensor, *args, **kwargs):\n        new_tensor = super().__new__(cls, tensor, *args, **kwargs)\n        if hasattr(tensor, \"_aux_attrs\"):\n            setattr(new_tensor, \"_aux_attrs\", tensor.aux_attrs)\n        return new_tensor\n\n    def to(self, *args, **kwargs):\n        new_tensor = super().to(*args, **kwargs)\n        if hasattr(self, \"_aux_attrs\"):\n            setattr(new_tensor, \"_aux_attrs\", self.aux_attrs)\n        try:\n            _ = torch.device(args[0])\n            for name, attr in new_tensor.aux_attrs.items():\n                new_attr = attr.to(*args, **kwargs)\n                setattr(new_tensor, name, new_attr)\n                new_tensor.aux_attrs[name] = new_attr\n        except:\n            pass\n\n        return new_tensor\n\n    @classmethod\n    def initialize(cls, core_param: torch.Tensor, **kwargs) -> 'InferenceParameter':\n        \"\"\"\n        Create the inference parameter.\n        \"\"\"\n        param = InferenceParameter(core_param)\n        setattr(param, \"_aux_attrs\", kwargs)\n\n        for attr_name, attr in kwargs.items():\n            if hasattr(param, attr_name):\n                raise ValueError(f\"Attribute {attr_name} already exists on param.\")\n\n            if not isinstance(attr, torch.Tensor):\n                raise ValueError(f\"Attribute {attr_name} must be a tensor.\")\n\n            setattr(param, attr_name, attr)\n\n        return param\n\n    @classmethod\n    def initialize_raw(self, **kwargs) -> 'InferenceParameter':\n        \"\"\"\n        All kwargs must be torch.Tensors and must include the core parameter.\n        \"\"\"\n        if CORE_PARAM not in kwargs:\n            raise ValueError(f\"Must provide core parameter, with key {CORE_PARAM}.\")\n\n        return InferenceParameter.initialize(kwargs[CORE_PARAM], **kwargs)\n\n    @property\n    def aux_attrs(self) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Dictionary of auxiliary attributes.\n        \"\"\"\n        return self._aux_attrs\n", "deepspeed/inference/v2/config_v2.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Optional\nfrom deepspeed.pydantic_v1 import Field\nfrom deepspeed.runtime.config_utils import DeepSpeedConfigModel\nfrom .ragged import DSStateManagerConfig\n\n\nclass DeepSpeedTPConfig(DeepSpeedConfigModel):\n    \"\"\" Configure tensor parallelism settings \"\"\"\n\n    tp_size: int = 1\n    \"\"\" Number of devices to split the model across using tensor parallelism. \"\"\"\n\n\nclass QuantizationConfig(DeepSpeedConfigModel):\n    \"\"\" Configure tensor parallelism settings \"\"\"\n\n    quantization_mode: Optional[str] = None\n    \"\"\" The quantization mode in string format. The supported modes are as follows:\n        - 'wf6af16', weight-only quantization with FP6 weight and FP16 activation.\n    \"\"\"\n    # TODO: may reuse the constants in deepspeed/compression/constants.py\n\n\nclass RaggedInferenceEngineConfig(DeepSpeedConfigModel):\n    \"\"\" Sets parameters for DeepSpeed Inference Engine. \"\"\"\n\n    tensor_parallel: DeepSpeedTPConfig = Field({}, alias=\"tp\")\n    \"\"\"\n    Configuration for tensor parallelism used to split the model across several\n    GPUs. Expects a dictionary containing values for :any:`DeepSpeedTPConfig`.\n    \"\"\"\n\n    state_manager: DSStateManagerConfig = Field({}, alias=\"manager\")\n    \"\"\"\n    Configuration for managing persistent state\n    \"\"\"\n\n    quantization: QuantizationConfig = {}\n", "deepspeed/inference/v2/engine_v2.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport os\nimport json\nimport pickle\nfrom typing import Iterable, Tuple\n\nimport torch\n\nimport deepspeed.comm as dist\n\nfrom deepspeed.accelerator import get_accelerator\nfrom deepspeed.comm.comm import init_distributed\n\nfrom .model_implementations import InferenceV2Policy\nfrom .logging import inference_logger\nfrom .ragged import DSStateManager, RaggedBatchWrapper, PlaceholderSequenceDescriptor\nfrom .scheduling_utils import SchedulingError, SchedulingResult\nfrom .model_implementations.flat_model_helpers import make_param_filename, make_metadata_filename\nfrom .model_implementations.inference_model_base import DSInferenceModelBase\n\nfrom .config_v2 import RaggedInferenceEngineConfig\n\nINFERENCE_MODEL_TIMER = \"model-forward-inference\"\n\n\nclass InferenceEngineV2:\n\n    _config: RaggedInferenceEngineConfig\n    \"\"\"\n    Configuration of the inference engine.\n    \"\"\"\n\n    _model: DSInferenceModelBase\n    \"\"\"\n    Inference model supporting ragged inference.\n    \"\"\"\n\n    _state_manager: DSStateManager\n    \"\"\"\n    Persistent state manager for sequences and KV-cache.\n    \"\"\"\n\n    @property\n    def free_blocks(self) -> torch.Tensor:\n        \"\"\"\n        Number of free KV blocks. This is a tensor of shape [n_kv_cache_groups] where each\n        element is the number of free blocks in the corresponding KV cache group.\n        \"\"\"\n        return self._state_manager.free_blocks\n\n    @property\n    def n_kv_cache_groups(self) -> int:\n        \"\"\"\n        Number of KV cache groups.\n        \"\"\"\n        return self._state_manager.n_kv_cache_groups\n\n    def model(self) -> DSInferenceModelBase:\n        \"\"\"\n        The model implementation.\n        \"\"\"\n        return self._model\n\n    def __init__(self, policy: InferenceV2Policy, engine_config: RaggedInferenceEngineConfig) -> None:\n        \"\"\"\n        Create the Inference V2 engine.\n\n        Arguments:\n            policy (InferenceV2Policy): Policy for the model implementation. This policy object\n                will be used to build the model and load the checkpoint associated with it.\n            engine_config (RaggedInferenceEngineConfig): Configuration for the inference engine.\n        \"\"\"\n        self._config = engine_config\n        self._policy = policy\n        self._base_mp_group = self._initialize_tp_group()\n\n        # Build model from policy\n        inference_logger().info(\"Building model...\")\n        self._model = self._policy.build_model(self._config, self._base_mp_group)\n        inference_logger().info(\"Model built.\")\n\n        # Create state manager\n        self._batch = RaggedBatchWrapper(self._config.state_manager)\n        self._state_manager = DSStateManager(self._config.state_manager,\n                                             self._model.kv_cache_config(),\n                                             base_mp_group=self._base_mp_group)\n        self._model.set_state_manager(self._state_manager)\n\n    def _initialize_tp_group(self):\n        \"\"\"\n        Implementation of our TP group initialization.\n        \"\"\"\n        init_distributed()\n        local_rank = int(os.getenv(\"LOCAL_RANK\", 0))\n        get_accelerator().set_device(local_rank)\n\n        if local_rank >= self._config.tensor_parallel.tp_size:\n            raise RuntimeError(\"Local rank is greater than TP size, ensure that the TP config is correct.\")\n\n        ranks = list(range(self._config.tensor_parallel.tp_size))\n        return dist.new_group(ranks=ranks)\n\n    def put(self,\n            batch_uids: Iterable[int],\n            batch_tokens: Iterable[torch.Tensor],\n            do_checks: bool = True) -> torch.Tensor:\n        \"\"\"\n        Put a ragged batch onto the inference engine. This will perform one forward and return\n        a Tensor of the shape [len(batch_uids), *output_shape]. Logits for the non-final tokens\n        are not calculated.\n\n        Arguments:\n            batch_uids: Iterable of uids for the batch on the host\n            batch_tokens: Iterable of token tensors for the batch on the host\n            do_checks: Check schedulability when it is set to True. You can skip this check for better performance when it has already been completed.\n        \"\"\"\n\n        if do_checks:\n            token_lens = [len(tokens) for tokens in batch_tokens]\n            schedule_check = self.can_schedule(batch_uids, token_lens)\n            if schedule_check != SchedulingResult.Success:\n                raise SchedulingError(schedule_check)\n\n        self._batch.clear()\n        for uid, tokens in zip(batch_uids, batch_tokens):\n\n            host_seq_desc = self._state_manager.get_or_create_sequence(uid)\n            self._model.maybe_allocate_kv(host_seq_desc, tokens.numel())\n            host_seq_desc.pre_forward(tokens.numel())\n\n            # We can disable checks since we already validated schedulability.\n            self._batch.insert_sequence(host_seq_desc, tokens, do_checks=do_checks)\n\n        # Send all metadata to the device\n        self._batch.finalize()\n\n        # Prep all data structures for the actual forward (in anticipation of CG in the future)\n        # and also to amortize some of the costs in a more straightforward way.\n        self._model.prepare_batch(self._batch)\n\n        # Model implementation will pick up in the forward.\n        logits = self._model.forward(self._batch)\n\n        # We return one set of logits per sequence in the batch (saves cost on unembedding)\n        assert logits.shape[0] == self._batch.current_sequences\n\n        for uid in batch_uids:\n            host_seq_desc = self._state_manager.get_sequence(uid)\n            host_seq_desc.post_forward()  # Updates sequence metadata.\n            self._model.maybe_free_kv(host_seq_desc)\n\n        return logits\n\n    def query(self, uid: int, max_request_tokens: int, max_request_blocks) -> Tuple[int, torch.Tensor]:\n        \"\"\"\n        Determine the number of tokens and KV blocks to reserve for a given request. Given a UID\n        (this UID may not be recognized by the model yet), this will return the number of tokens\n        and blocks to reserve for the request.\n\n        Arguments:\n            uid (int): The UID of the sequence (as tracked by the scheduling entity). If\n                this is a new sequence (with a UID unknown to the inference engine), then\n                an empty placeholder is created to pass to the occupancy logic.\n            n_tokens (int): The number of tokens to hypothetically send.\n\n        Returns:\n            Tuple[int, Optional[int]]: Tuple of free kv blocks and the number of blocks\n                required to schedule the sequence.\n        \"\"\"\n        seq_desc = self._state_manager.get_sequence(uid)\n        if seq_desc is None:\n            if (self._state_manager.n_tracked_sequences == self._config.state_manager.max_tracked_sequences):\n                return (0, 0)\n            seq_desc = PlaceholderSequenceDescriptor()\n\n        req_tokens, req_blocks = self._model.get_kv_requirements(seq_desc, max_request_tokens, max_request_blocks)\n\n        return (req_tokens, req_blocks)\n\n    def can_schedule(self, uids: Iterable[int], lengths: Iterable[int]) -> SchedulingResult:\n        \"\"\"\n        Dry run a batch to determine if it can be scheduled. Placeholder sequences will be\n        created for any UIDs that are unknown to the inference engine.\n\n        Arguments:\n            uids (Iterable[int]): Iterable of UIDs for the batch\n            lengths (Iterable[int]): Iterable of lengths for each sequence of the batch. This lengths\n                corresponds to the number of tokens to send in the hypothetical forward; history\n                tokens will be determined via UID lookup and future tokens are disregarded.\n\n        Returns:\n            bool: True if the batch can be scheduled, False otherwise.\n        \"\"\"\n\n        cur_seqs = self._state_manager.n_tracked_sequences\n        free_blocks = self._state_manager.free_blocks\n        req_blocks = 0\n        batch_len = 0\n\n        if len(uids) > self._config.state_manager.max_ragged_sequence_count:\n            # Can only compose a batch from a limited number of sequences\n            return SchedulingResult.BatchSequenceLimitExceeded\n\n        for uid, length in zip(uids, lengths):\n            seq_desc = self._state_manager.get_sequence(uid)\n            if seq_desc is None:\n                cur_seqs += 1\n                seq_desc = PlaceholderSequenceDescriptor()\n\n            sched_len, sched_blocks = self._model.get_kv_requirements(seq_desc, length, free_blocks)\n\n            if sched_len != length:\n                # We ran out of KV cache\n                return SchedulingResult.KVCacheLimitExceeded\n\n            batch_len += length\n            free_blocks -= sched_blocks\n\n        if cur_seqs > self._config.state_manager.max_tracked_sequences:\n            # Would run out of tracking metadata\n            return SchedulingResult.EngineSequenceLimitExceeded\n\n        if batch_len > self._config.state_manager.max_ragged_batch_size:\n            # Would exceed the maximum batch size\n            return SchedulingResult.BatchTokenLimitExceeded\n\n        return SchedulingResult.Success\n\n    def get_remaining_block_capacity(self, uid: int) -> int:\n        \"\"\"\n        Get the remaining capacity of the last block already allocated.\n        \"\"\"\n        seq_desc = self._state_manager.get_sequence(uid)\n        if seq_desc is None:\n            return 0\n        return self._model.get_remaining_block_capacity(seq_desc)\n\n    def flush(self, uid: int) -> None:\n        \"\"\"\n        Remove all state associated with a sequence from the inference engine.\n\n        Arguments:\n            uid (int): The UID of the sequence to flush.\n        \"\"\"\n        self._state_manager.flush_sequence(uid)\n\n    def serialize(self, save_path: str) -> None:\n        \"\"\"\n        Serialize the model to a file.\n\n        Arguments:\n            path (str): Path to the file to serialize to.\n        \"\"\"\n        param_file_name = make_param_filename(save_path, self._model.tp_rank, self._model.tp_size)\n        metadata_file_name = make_metadata_filename(save_path, self._model.tp_rank, self._model.tp_size)\n\n        # Save the flattened parameters\n\n        torch.save(self._model.flattened_params, param_file_name)\n\n        json.dump(self._model.flattened_param_metadata.json(), open(metadata_file_name, \"w\"))\n\n        if self._model.tp_rank == 0:\n            pickle.dump(self._model._config, open(os.path.join(save_path, \"ds_model_config.pkl\"), \"wb\"))\n", "deepspeed/inference/v2/allocator.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom functools import reduce\nfrom typing import Iterable\nfrom collections import defaultdict\nimport torch\n\nfrom deepspeed.accelerator import get_accelerator\n\n\nclass Allocator:\n    cache = defaultdict(dict)\n\n    def empty_from(tensor: torch.Tensor, shape: Iterable[int]) -> torch.Tensor:\n        try:\n            return Allocator.cache[tensor][shape]\n        except KeyError:\n            shape_size = reduce(lambda x, y: x * y, shape)\n            if shape_size == 0:\n                raise ValueError(\"Cannot create empty tensor with size 0\")\n            Allocator.cache[tensor][shape] = tensor.flatten()[:shape_size].view(shape)\n            return Allocator.cache[tensor][shape]\n\n\nempty_from = Allocator.empty_from\n\n\ndef on_device(method) -> torch.Tensor:\n    \"\"\"\n    Wraps a method to ensure the returned tensor is on the current device.\n    \"\"\"\n\n    def wrapped(self, *args, **kwargs):\n        tensor = method(self, *args, **kwargs)\n        if isinstance(tensor, torch.Tensor):\n            return tensor.to(get_accelerator().current_device())\n        return tensor\n\n    return wrapped\n", "deepspeed/inference/v2/inference_utils.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Dict\n\nimport torch\n\nfrom enum import Enum, IntEnum\n\n\nclass NormTypeEnum(Enum):\n    LayerNorm: str = \"layer_norm\"\n    RMSNorm: str = \"rms_norm\"\n\n\nclass DtypeEnum(Enum):\n    # The torch dtype must always be the first value (so we return torch.dtype)\n    fp16 = torch.float16, \"torch.float16\", \"fp16\", \"float16\", \"half\"\n    fp32 = torch.float32, \"torch.float32\", \"fp32\", \"float32\", \"float\"\n    bf16 = torch.bfloat16, \"torch.bfloat16\", \"bf16\", \"bfloat16\", \"bfloat\"\n    int8 = torch.int8, \"torch.int8\", \"int8\"\n\n    # Copied from https://stackoverflow.com/a/43210118\n    # Allows us to use multiple values for each Enum index and returns first\n    # listed value when Enum is called\n    def __new__(cls, *values):\n        obj = object.__new__(cls)\n        # first value is canonical value\n        obj._value_ = values[0]\n        for other_value in values[1:]:\n            cls._value2member_map_[other_value] = obj\n        obj._all_values = values\n        return obj\n\n    def __repr__(self):\n        return \"<%s.%s: %s>\" % (\n            self.__class__.__name__,\n            self._name_,\n            \", \".join([repr(v) for v in self._all_values]),\n        )\n\n\nELEM_SIZES: Dict[torch.dtype, int] = {\n    torch.float16: 2,\n    torch.bfloat16: 2,\n    torch.float32: 4,\n    torch.float64: 8,\n    torch.int8: 1,\n    torch.uint8: 1,\n    torch.int16: 2,\n    torch.int32: 4,\n    torch.int64: 8,\n    torch.bool: 1,\n}\n\n\nclass ActivationType(IntEnum):\n    \"\"\"\n    Types of activations supported by DS-Inference\n    \"\"\"\n\n    GELU = 0\n\n    RELU = 1\n\n    SILU = 2\n\n    GEGLU = 3\n\n    ReGLU = 4\n\n    SiGLU = 5\n\n    IDENTITY = 6\n\n    InvalidType = -1\n\n\ndef is_gated(act_fn: ActivationType) -> bool:\n    \"\"\"\n    Return True if the given activation function is gated.\n    \"\"\"\n    if not isinstance(act_fn, ActivationType):\n        act_fn = ActivationType(act_fn)\n\n    return act_fn in [ActivationType.GEGLU, ActivationType.ReGLU, ActivationType.SiGLU]\n\n\ndef elem_size(dtype: torch.dtype) -> int:\n    \"\"\"\n    Return size in bytes of the given dtype.\n    \"\"\"\n    try:\n        return ELEM_SIZES[dtype]\n    except KeyError:\n        raise ValueError(\"Unknown dtype size for {}\".format(dtype))\n\n\ndef ceil_div(a: int, b: int) -> int:\n    \"\"\"\n    Return ceil(a / b).\n    \"\"\"\n    return -(-a // b)\n", "deepspeed/inference/v2/engine_factory.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport json\nimport logging\nimport os\nimport pickle\nfrom packaging import version\n\nfrom .engine_v2 import InferenceEngineV2\nfrom .config_v2 import RaggedInferenceEngineConfig\nfrom .checkpoint import HuggingFaceCheckpointEngine\nfrom .logging import inference_logger\nfrom .model_implementations import (\n    OPTPolicy,\n    Llama2Policy,\n    MistralPolicy,\n    MixtralPolicy,\n    FalconPolicy,\n    PhiPolicy,\n    QwenPolicy,\n    Qwen2Policy,\n)\nfrom .model_implementations.inference_policy_base import POLICIES, InferenceV2Policy\nfrom .model_implementations.flat_model_helpers import make_metadata_filename, ModelMetadata\n\n\ndef build_engine_from_ds_checkpoint(path: str,\n                                    engine_config: RaggedInferenceEngineConfig,\n                                    debug_level: int = logging.INFO) -> InferenceEngineV2:\n    \"\"\"\n    Creates an engine from a checkpoint saved by ``InferenceEngineV2``.\n\n    Arguments:\n        path: Path to the checkpoint. This does not need to point to any files in particular,\n            just the directory containing the checkpoint.\n        engine_config: Engine configuration. See ``RaggedInferenceEngineConfig`` for details.\n        debug_level: Logging level to use. Unless you are actively seeing issues, the recommended\n            value is ``logging.INFO``.\n\n    Returns:\n        Fully initialized inference engine ready to serve queries.\n    \"\"\"\n\n    inference_logger(level=debug_level)\n    # Load metadata, for grabbing the policy name we'll have all ranks just check for\n    # rank 0.\n    metadata_filename = make_metadata_filename(path, 0, engine_config.tensor_parallel.tp_size)\n    metadata = json.load(open(metadata_filename, \"r\"))\n    metadata = ModelMetadata.parse_raw(metadata)\n\n    # Get the policy\n    try:\n        policy_cls: InferenceV2Policy = POLICIES[metadata.policy]\n    except KeyError:\n        raise ValueError(f\"Unknown policy {metadata.policy} for model {path}\")\n\n    # Load the model config\n    model_config = pickle.load(open(os.path.join(path, \"ds_model_config.pkl\"), \"rb\"))\n    policy = policy_cls(model_config, inf_checkpoint_path=path)\n\n    return InferenceEngineV2(policy, engine_config)\n\n\ndef build_hf_engine(path: str,\n                    engine_config: RaggedInferenceEngineConfig,\n                    debug_level: int = logging.INFO) -> InferenceEngineV2:\n    \"\"\"\n    Build an InferenceV2 engine for HuggingFace models. This can accept both a HuggingFace\n    model name or a path to an Inference-V2 checkpoint.\n\n    Arguments:\n        path: Path to the checkpoint. This does not need to point to any files in particular,\n            just the directory containing the checkpoint.\n        engine_config: Engine configuration. See ``RaggedInferenceEngineConfig`` for details.\n        debug_level: Logging level to use. Unless you are actively seeing issues, the recommended\n            value is ``logging.INFO``.\n\n    Returns:\n        Fully initialized inference engine ready to serve queries.\n    \"\"\"\n\n    if os.path.exists(os.path.join(path, \"ds_model_config.pkl\")):\n        return build_engine_from_ds_checkpoint(path, engine_config, debug_level=debug_level)\n    else:\n        # Set up logging\n        inference_logger(level=debug_level)\n        # get HF checkpoint engine\n        checkpoint_engine = HuggingFaceCheckpointEngine(path)\n\n        # get model config from HF AutoConfig\n        model_config = checkpoint_engine.model_config\n\n        # get the policy\n        # TODO: generalize this to other models\n        if model_config.model_type == \"opt\":\n            if not model_config.do_layer_norm_before:\n                raise ValueError(\n                    \"Detected OPT-350m model. This model is not currently supported. If this is not the 350m model, please open an issue: https://github.com/microsoft/DeepSpeed-MII/issues\"\n                )\n            policy = OPTPolicy(model_config, checkpoint_engine=checkpoint_engine)\n        elif model_config.model_type == \"llama\":\n            policy = Llama2Policy(model_config, checkpoint_engine=checkpoint_engine)\n        elif model_config.model_type == \"mistral\":\n            # Ensure we're using the correct version of transformers for mistral\n            import transformers\n            assert version.parse(transformers.__version__) >= version.parse(\"4.34.0\"), \\\n                f\"Mistral requires transformers >= 4.34.0, you have version {transformers.__version__}\"\n            policy = MistralPolicy(model_config, checkpoint_engine=checkpoint_engine)\n        elif model_config.model_type == \"mixtral\":\n            # Ensure we're using the correct version of transformers for mistral\n            import transformers\n            assert version.parse(transformers.__version__) >= version.parse(\"4.36.1\"), \\\n                f\"Mistral requires transformers >= 4.36.1, you have version {transformers.__version__}\"\n            policy = MixtralPolicy(model_config, checkpoint_engine=checkpoint_engine)\n        elif model_config.model_type == \"falcon\":\n            policy = FalconPolicy(model_config, checkpoint_engine=checkpoint_engine)\n        elif model_config.model_type == \"phi\":\n            policy = PhiPolicy(model_config, checkpoint_engine=checkpoint_engine)\n        elif model_config.model_type == \"qwen\":\n            policy = QwenPolicy(model_config, checkpoint_engine=checkpoint_engine)\n        elif model_config.model_type == \"qwen2\":\n            policy = Qwen2Policy(model_config, checkpoint_engine=checkpoint_engine)\n        else:\n            raise ValueError(f\"Unsupported model type {model_config.model_type}\")\n\n        return InferenceEngineV2(policy, engine_config)\n", "deepspeed/inference/v2/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\nfrom .config_v2 import RaggedInferenceEngineConfig, DeepSpeedTPConfig\nfrom .engine_v2 import InferenceEngineV2\nfrom .engine_factory import build_hf_engine, build_engine_from_ds_checkpoint\n", "deepspeed/inference/v2/kernels/ds_kernel.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom abc import ABC, abstractmethod\n\n\nclass DSKernelBase(ABC):\n\n    @abstractmethod\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        If necessary trigger compilation and warmup\n        Autotuning of the kernel would happen at this stage to\n        eliminate any potential hangs that might occur mid-deployment\n        Validate that the desired run configuration is compatible.\n\n        It is not necessary to call super on this method.\n        \"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def __call__(self, *args, **kwargs):\n        \"\"\"\n        However the kernel needs to be called, it can be called here. Auto-tuning\n        should never be performed here.\n\n        All inputs/outputs should be passed as arguments to this function. No allocations\n        should be performed here.\n        \"\"\"\n        raise NotImplementedError()\n", "deepspeed/inference/v2/kernels/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .ds_kernel import DSKernelBase\n", "deepspeed/inference/v2/kernels/ragged_ops/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .atom_builder import *\nfrom .blocked_flash import *\nfrom .embed import *\nfrom .linear_blocked_kv_rotary import *\nfrom .logits_gather import *\nfrom .moe_gather import *\nfrom .moe_scatter import *\nfrom .top_k_gating import *\n", "deepspeed/inference/v2/kernels/ragged_ops/linear_blocked_kv_rotary/linear_blocked_kv_copy.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\n# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\n\nfrom ....inference_utils import DtypeEnum\nfrom ....ragged import RaggedBatchWrapper\nfrom deepspeed.ops.op_builder import RaggedOpsBuilder\nfrom ... import DSKernelBase\n\n\nclass LinearBlockedKVCopy(DSKernelBase):\n    \"\"\"\n    CUDA Kernel implementation that will perform rotary position embeddings on the queries and keys\n    before copying into a blocked KV cache.\n    \"\"\"\n\n    supported_dtypes = [DtypeEnum.fp16, DtypeEnum.bf16]\n    supported_head_sizes = [64, 80, 128]\n    supported_q_ratios = [1, 2, 4, 5, 8]\n\n    def __init__(self, head_size: int, n_q_heads: int, n_kv_heads: int, dtype: torch.dtype) -> None:\n        \"\"\"\n        Args:\n            head_size: The size of the attention head.\n            dtype: Data type for the input/output. Supported values are torch.float16 and torch.bfloat16.\n        \"\"\"\n\n        q_ratio = n_q_heads // n_kv_heads\n\n        if head_size not in LinearBlockedKVCopy.supported_head_sizes:\n            raise ValueError(\"Unsupported head size: {}, supported_head_sizes are {}\".format(\n                head_size, LinearBlockedKVCopy.supported_head_sizes))\n\n        if q_ratio not in LinearBlockedKVCopy.supported_q_ratios:\n            raise ValueError(\"Unsupported q_ratio: {}, supported_q_ratios are {}\".format(\n                q_ratio, LinearBlockedKVCopy.supported_q_ratios))\n\n        if not isinstance(dtype, DtypeEnum):\n            dtype = DtypeEnum(dtype)\n\n        if dtype not in LinearBlockedKVCopy.supported_dtypes:\n            raise ValueError(\"Unsupported data type: {}, supported_dtypes are {}\".format(\n                dtype, LinearBlockedKVCopy.supported_dtypes))\n\n        inf_module = RaggedOpsBuilder().load()\n        self.kernel = inf_module.linear_kv_copy\n        self.head_size = head_size\n        self.n_q_heads = n_q_heads\n        self.n_kv_heads = n_kv_heads\n\n    def __call__(self, kv_cache: torch.Tensor, qkv: torch.Tensor, ragged_batch: RaggedBatchWrapper) -> None:\n        \"\"\"\n        Perform rotary embeddings on the queries and keys before copying into a blocked KV cache.\n\n        Args:\n            kv_cache (torch.Tensor): Pre-allocated KV cache of [num_blocks, block_size, 2, n_kv_heads, head_size]\n            qkv: Input tensor of shape [num_tokens, head_size * (n_q_heads + 2 * n_kv_heads)]\n            ragged_batch: Wrapper for the ragged batch.\n        \"\"\"\n\n        q = qkv[:, :self.head_size * self.n_q_heads]\n        k = qkv[:, self.head_size * self.n_q_heads:self.head_size * (self.n_q_heads + self.n_kv_heads)]\n        v = qkv[:, self.head_size * (self.n_q_heads + self.n_kv_heads):]\n\n        self.kernel(kv_cache, q, k, v, ragged_batch.batch_metadata_buffer(), ragged_batch.inflight_seq_descriptors(),\n                    ragged_batch.tokens_to_seq(), ragged_batch.kv_ptrs())\n", "deepspeed/inference/v2/kernels/ragged_ops/linear_blocked_kv_rotary/blocked_trained_kv_rotary.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\n# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\n\nfrom ....inference_utils import DtypeEnum\nfrom deepspeed.ops.op_builder import RaggedOpsBuilder\nfrom ....ragged import RaggedBatchWrapper\nfrom ... import DSKernelBase\n\n\nclass BlockedTrainedRotaryEmbeddings(DSKernelBase):\n    \"\"\"\n    CUDA Kernel implementation that will perform rotary position embeddings on the queries and keys\n    before copying into a blocked KV cache.\n    \"\"\"\n\n    supported_dtypes = [DtypeEnum.fp16, DtypeEnum.bf16]\n    supported_head_sizes = [64, 80, 128]\n    supported_q_ratios = [1, 2, 4, 5, 8]\n\n    def __init__(self, head_size: int, n_q_heads: int, n_kv_heads: int, dtype: torch.dtype) -> None:\n        \"\"\"\n        Args:\n            head_size: The size of the attention head.\n            dtype: Data type for the input/output. Supported values are torch.float16 and torch.bfloat16.\n        \"\"\"\n\n        q_ratio = n_q_heads // n_kv_heads\n\n        if head_size not in BlockedTrainedRotaryEmbeddings.supported_head_sizes:\n            raise ValueError(\"Unsupported head size: {}, supported_head_sizes are {}\".format(\n                head_size, BlockedTrainedRotaryEmbeddings.supported_head_sizes))\n\n        if q_ratio not in BlockedTrainedRotaryEmbeddings.supported_q_ratios:\n            raise ValueError(\"Unsupported q_ratio: {}, supported_q_ratios are {}\".format(\n                q_ratio, BlockedTrainedRotaryEmbeddings.supported_q_ratios))\n\n        if not isinstance(dtype, DtypeEnum):\n            dtype = DtypeEnum(dtype)\n\n        if dtype not in BlockedTrainedRotaryEmbeddings.supported_dtypes:\n            raise ValueError(\"Unsupported data type: {}, supported_dtypes are {}\".format(\n                dtype, BlockedTrainedRotaryEmbeddings.supported_dtypes))\n\n        inf_module = RaggedOpsBuilder().load()\n        self.kernel = inf_module.kv_trained_rotary_embeddings\n        self.head_size = head_size\n        self.n_q_heads = n_q_heads\n        self.n_kv_heads = n_kv_heads\n\n    def __call__(self, kv_cache: torch.Tensor, qkv: torch.Tensor, ragged_batch: RaggedBatchWrapper,\n                 inverse_freqs: torch.Tensor) -> None:\n        \"\"\"\n        Perform rotary embeddings on the queries and keys before copying into a blocked KV cache.\n\n        Args:\n            kv_cache (torch.Tensor): Pre-allocated KV cache of [num_blocks, block_size, 2, n_kv_heads, head_size]\n            qkv: Input tensor of shape [num_tokens, head_size * (n_q_heads + 2 * n_kv_heads)]\n            ragged_batch: Wrapper for the ragged batch.\n            inverse_freqs: Inverse frequencies for the rotary embeddings. Shape [max_seq_len, rotary_dim // 2]\n        \"\"\"\n\n        q = qkv[:, :self.head_size * self.n_q_heads]\n        k = qkv[:, self.head_size * self.n_q_heads:self.head_size * (self.n_q_heads + self.n_kv_heads)]\n        v = qkv[:, self.head_size * (self.n_q_heads + self.n_kv_heads):]\n\n        self.kernel(kv_cache, q, k, v, inverse_freqs, ragged_batch.batch_metadata_buffer(),\n                    ragged_batch.inflight_seq_descriptors(), ragged_batch.tokens_to_seq(), ragged_batch.kv_ptrs())\n", "deepspeed/inference/v2/kernels/ragged_ops/linear_blocked_kv_rotary/blocked_kv_rotary.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\n\nfrom ....inference_utils import DtypeEnum\nfrom deepspeed.ops.op_builder import RaggedOpsBuilder\nfrom ....ragged import RaggedBatchWrapper\nfrom ... import DSKernelBase\n\n\nclass BlockedRotaryEmbeddings(DSKernelBase):\n    \"\"\"\n    CUDA Kernel implementation that will perform rotary position embeddings on the queries and keys\n    before copying into a blocked KV cache.\n    \"\"\"\n\n    supported_dtypes = [DtypeEnum.fp16, DtypeEnum.bf16]\n    supported_head_sizes = [64, 80, 128]\n    supported_q_ratios = [1, 2, 4, 5, 8, 16, 29, 35, 36, 71]\n\n    def __init__(self, head_size: int, n_q_heads: int, n_kv_heads: int, dtype: torch.dtype, rotary_dim: int,\n                 theta_base: float) -> None:\n        \"\"\"\n        Args:\n            head_size: The size of the attention head.\n            q_ratio: Ratio of q heads to kv heads (for GQA)\n            dtype: Data type for the input/output. Supported values are torch.float16 and torch.bfloat16.\n        \"\"\"\n\n        q_ratio = n_q_heads // n_kv_heads\n\n        if head_size not in BlockedRotaryEmbeddings.supported_head_sizes:\n            raise ValueError(\"Unsupported head size: {}, supported_head_sizes are {}\".format(\n                head_size, BlockedRotaryEmbeddings.supported_head_sizes))\n\n        if q_ratio not in BlockedRotaryEmbeddings.supported_q_ratios:\n            raise ValueError(\"Unsupported q_ratio: {}, supported_q_ratios are {}\".format(\n                q_ratio, BlockedRotaryEmbeddings.supported_q_ratios))\n\n        if not isinstance(dtype, DtypeEnum):\n            dtype = DtypeEnum(dtype)\n\n        if dtype not in BlockedRotaryEmbeddings.supported_dtypes:\n            raise ValueError(\"Unsupported data type: {}, supported_dtypes are {}\".format(\n                dtype, BlockedRotaryEmbeddings.supported_dtypes))\n\n        inf_module = RaggedOpsBuilder().load()\n        self.kernel = inf_module.kv_rotary_embeddings\n        self.head_size = head_size\n        self.n_q_heads = n_q_heads\n        self.n_kv_heads = n_kv_heads\n        self.rotary_dim = rotary_dim\n        self.theta_base = theta_base\n\n    def __call__(self, kv_cache: torch.Tensor, qkv: torch.Tensor, ragged_batch: RaggedBatchWrapper) -> None:\n        \"\"\"\n        Perform rotary embeddings on the queries and keys before copying into a blocked KV cache.\n\n        Args:\n            kv_cache (torch.Tensor): Pre-allocated KV cache of [num_blocks, block_size, 2, n_kv_heads, head_size]\n            qkv: Input tensor of shape [num_tokens, head_size * (n_q_heads + 2 * n_kv_heads)]\n            ragged_batch: Wrapper for the ragged batch.\n        \"\"\"\n\n        q = qkv[:, :self.head_size * self.n_q_heads]\n        k = qkv[:, self.head_size * self.n_q_heads:self.head_size * (self.n_q_heads + self.n_kv_heads)]\n        v = qkv[:, self.head_size * (self.n_q_heads + self.n_kv_heads):]\n\n        self.kernel(kv_cache, q, k, v, self.rotary_dim, self.theta_base, ragged_batch.batch_metadata_buffer(),\n                    ragged_batch.inflight_seq_descriptors(), ragged_batch.tokens_to_seq(), ragged_batch.kv_ptrs())\n", "deepspeed/inference/v2/kernels/ragged_ops/linear_blocked_kv_rotary/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .blocked_kv_rotary import *\nfrom .blocked_trained_kv_rotary import *\nfrom .linear_blocked_kv_copy import *\n", "deepspeed/inference/v2/kernels/ragged_ops/blocked_flash/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .blocked_flash import *\n", "deepspeed/inference/v2/kernels/ragged_ops/blocked_flash/blocked_flash.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\n\nfrom deepspeed.accelerator import get_accelerator\nfrom ....inference_utils import DtypeEnum\nfrom deepspeed.ops.op_builder import RaggedOpsBuilder\n\nfrom ... import DSKernelBase\n\n\ndef get_q_block_size(head_size: int) -> int:\n    \"\"\"\n    Returns the query block size required by the kernel given a head size.\n    \"\"\"\n    cc_major, cc_minor = torch.cuda.get_device_capability(get_accelerator().current_device())  #ignore-cuda\n\n    if cc_major < 8:\n        raise RuntimeError(\"Blocked attention requires CUDA compute capability >= 8.0\")\n\n    if head_size <= 64:\n        return 128\n    elif head_size <= 160:\n        if cc_minor != 0:\n            return 64\n        else:\n            return 128\n    elif head_size == 192:\n        return 128\n    elif head_size == 224:\n        if cc_minor != 0:\n            return 64\n        else:\n            return 128\n    else:\n        if cc_major == 8 and cc_minor == 0:\n            return 128\n        else:\n            return 64\n\n\ndef get_kv_block_size(head_size: int) -> int:\n    \"\"\"\n    Return preferred granulatity for blocked KV-cache implementation.\n    \"\"\"\n    cc_major, cc_minor = torch.cuda.get_device_capability(get_accelerator().current_device())  #ignore-cuda\n\n    if cc_major < 8:\n        raise RuntimeError(\"Blocked attention requires CUDA compute capability >= 8.0\")\n\n    if head_size <= 64:\n        return 128\n    elif head_size != 160 or cc_minor != 0:\n        return 64\n    else:\n        return 32\n\n\nclass BlockedFlashAttn(DSKernelBase):\n    \"\"\"\n    Modified implementation of flash-attn-2 tuned for inference on blocked KV-cache and wider\n    range of input sequence lengths.\n    \"\"\"\n\n    supported_dtypes = [DtypeEnum.fp16, DtypeEnum.bf16]\n\n    def __init__(self, head_size: int, dtype: DtypeEnum) -> None:\n        \"\"\"\n        Triggers any compilation of the kernels.\n        \"\"\"\n        if not isinstance(dtype, DtypeEnum):\n            dtype = DtypeEnum(dtype)\n\n        if dtype not in BlockedFlashAttn.supported_dtypes:\n            raise ValueError(\"Unsupported data type: {}, supported data types are {}\".format(\n                dtype, BlockedFlashAttn.supported_dtypes))\n\n        # For testing, need to revert to 32\n        if head_size % 16 != 0:\n            raise ValueError(\"Head size must be divisible by 32 (configured with {})\".format(head_size))\n\n        inf_module = RaggedOpsBuilder().load()\n        self.kernel = inf_module.flash_attn_by_atoms\n\n    def __call__(self, out: torch.Tensor, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, atoms: torch.Tensor,\n                 softmax_scale: float) -> torch.Tensor:\n        \"\"\"\n        Flash attention implementation atop a blocked KV-cache. Atoms should be pre-populated.\n        See attention_atom.h for further details on the structure of the information.\n\n        Arguments:\n            out (torch.Tensor): Output tensor of shape [tokens, hidden_size]\n            q (torch.Tensor): Query tensor of shape [tokens, hidden_size]\n            k (torch.Tensor): Key cache tensor of shape [n_blocks, block_size, n_heads_kv, head_size]. This Tensor only needs to be contiguous on the final dimension.\n            v (torch.Tensor): Value cache tensor of shape [n_blocks, block_size, n_heads_kv, head_size]. This Tensor only needs to be contiguous on the final dimension.\n            atoms (torch.Tensor): Atom information tensor of shape [num_atoms, 8] and type int32.\n                Not all data is readable in this format. See attention_atom.h for further details.\n            softmax_scale (float): Softmax scale factor.\n\n        Returns:\n            out (torch.Tensor): Output tensor of shape [tokens, hidden_size]\n        \"\"\"\n        self.kernel(out, q, k, v, atoms, softmax_scale, True)\n        return out\n", "deepspeed/inference/v2/kernels/ragged_ops/moe_scatter/moe_scatter.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\n\nfrom typing import Tuple\n\nfrom ... import DSKernelBase\nfrom ....inference_utils import DtypeEnum\nfrom deepspeed.ops.op_builder import RaggedOpsBuilder\n\n\nclass MoEScatter(DSKernelBase):\n    \"\"\"\n    CUDA implementation of MoE scatter\n    \"\"\"\n\n    supported_dtypes = [DtypeEnum.fp16, DtypeEnum.bf16]\n\n    def __init__(self, dtype: DtypeEnum, channels: int) -> None:\n\n        if not isinstance(dtype, DtypeEnum):\n            dtype = DtypeEnum(dtype)\n\n        if dtype not in MoEScatter.supported_dtypes:\n            raise RuntimeError(f\"Unsupported dtype {dtype}\")\n\n        if channels % 8 != 0:\n            raise RuntimeError(f\"Channels {channels} must be divisible by 8\")\n\n        inf_module = RaggedOpsBuilder().load()\n        self.kernel = inf_module.moe_scatter\n\n    def __call__(self, moe_input: torch.Tensor, expert_cumsum: torch.Tensor, mapped_slots: torch.Tensor,\n                 activations: torch.Tensor, expert_counts: torch.Tensor, assignments: torch.Tensor,\n                 offsets: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Scatters the hidden states such that the token stride for each expert's input is contiguous.\n\n        Arguments:\n            moe_input (torch.Tensor): The direct input for the MoE GEMM of shape [n_tokens * n_top_k, hidden_size].\n            expert_cumsum (torch.Tensor): The cumulative sum of the expert counts of shape [n_experts].\n            mapped_slots (torch.Tensor): The index of the token in the expert's input of shape [n_tokens, n_top_k].\n            hidden_states (torch.Tensor): The hidden states of shape [n_tokens, hidden_size].\n            expert_counts (torch.Tensor): The number of tokens assigned to each expert of shape [n_experts].\n            assignments (torch.Tensor): The expert assignments of shape [n_tokens, n_top_k].\n            offsets (torch.Tensor): The offsets into the expert for a given token of shape [n_tokens, n_top_K].\n\n        Returns:\n            Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: The MoE input (with scattered values), the cumsum of the offsets (for the MoE kernels themselves), and the assignments Tensor modified in place to show which row that token was mapped to in the input.\n        \"\"\"\n        self.kernel(moe_input, expert_cumsum, mapped_slots, activations, expert_counts, assignments, offsets)\n        return moe_input, expert_cumsum, mapped_slots\n", "deepspeed/inference/v2/kernels/ragged_ops/moe_scatter/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .moe_scatter import *\n", "deepspeed/inference/v2/kernels/ragged_ops/logits_gather/logits_gather.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\n\nfrom ... import DSKernelBase\nfrom deepspeed.ops.op_builder import RaggedOpsBuilder\nfrom ....inference_utils import elem_size\nfrom ....ragged import RaggedBatchWrapper\n\n\nclass RaggedLogitsGather(DSKernelBase):\n    \"\"\"\n    CUDA Kernel implementation for gather the hidden states of the final token\n    of each sequence. This is used to reduce the cost of the performing the unembedding.\n    \"\"\"\n\n    supported_dtypes = [torch.float16, torch.bfloat16, torch.float32]\n\n    def __init__(self, model_dim: int, fp_dtype: torch.dtype):\n        \"\"\"\n        Parameters:\n            fp_dtype (torch.dtype): Data type for the input/output. Supported values\n                are torch.float16, torch.bfloat16, and torch.float32.\n        \"\"\"\n        if fp_dtype not in RaggedLogitsGather.supported_dtypes:\n            raise ValueError(\"Unsupported data type: {}, supported_dtypes are {}\".format(\n                fp_dtype, RaggedLogitsGather.supported_dtypes))\n\n        if elem_size(fp_dtype) * model_dim % 16 != 0:\n            raise ValueError(\"Embedding dimension must be aligned to 16 bytes, got {}\".format(model_dim))\n\n        inf_module = RaggedOpsBuilder().load()\n        self.kernel = inf_module.gather_for_logits\n\n    def __call__(self, final_token_activations: torch.Tensor, all_activations: torch.Tensor,\n                 ragged_wrapper: RaggedBatchWrapper) -> torch.Tensor:\n        \"\"\"\n        Gather the hidden states of the final token of each sequence from `all_activations` into\n        `final_token_activations`.\n\n        Args:\n            final_token_activations (torch.Tensor): Output tensor of shape [num_seqs, model_dim]\n            all_activations (torch.Tensor): Input tensor of shape [num_tokens, model_dim]\n            ragged_wrapper (RaggedBatchWrapper): Wrapper for the ragged batch.\n        \"\"\"\n\n        self.kernel(final_token_activations, all_activations, ragged_wrapper.batch_metadata_buffer(),\n                    ragged_wrapper.inflight_seq_descriptors())\n        return final_token_activations\n", "deepspeed/inference/v2/kernels/ragged_ops/logits_gather/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .logits_gather import *\n", "deepspeed/inference/v2/kernels/ragged_ops/embed/embed.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Optional\n\nimport torch\n\nfrom ... import DSKernelBase\nfrom deepspeed.ops.op_builder import RaggedOpsBuilder\nfrom ....inference_utils import elem_size\nfrom ....ragged import RaggedBatchWrapper\n\n\nclass RaggedEmbeddingKernel(DSKernelBase):\n    \"\"\"\n    Ragged-aware CUDA kernel implementation for an embedding lookup. This will only lookup\n    the necessary tokens for a padded batch (i.e. if we are CGed and running with a slightly\n    larger batch size than the actual tokens).\n    \"\"\"\n\n    supported_dtypes = [torch.float16, torch.bfloat16, torch.float32]\n    supported_token_dtypes = [torch.int32, torch.int64]\n\n    def __init__(self, embed_dtype: torch.dtype, token_dtype: torch.dtype, embed_dim: int) -> None:\n        \"\"\"\n        Args:\n            fp_dtype (torch.dtype): Data type of the embedding table and output dtype.\n                Supported values are torch.float16, torch.bfloat16, and torch.float32.\n            token_dtype (torch.dtype): Data type of the token ids. Supported values are\n                torch.int32 and torch.int64.\n            embed_dim (int): Embedding dimension. Must be aligned to 16 bytes.\n        \"\"\"\n        if embed_dtype not in RaggedEmbeddingKernel.supported_dtypes:\n            raise ValueError(\"Unsupported embedding data type: {}, supported_dtypes are {}\".format(\n                embed_dtype, RaggedEmbeddingKernel.supported_dtypes))\n\n        if token_dtype not in RaggedEmbeddingKernel.supported_token_dtypes:\n            raise ValueError(\"Unsupported token data type: {}, supported_dtypes are {}\".format(\n                token_dtype, RaggedEmbeddingKernel.supported_token_dtypes))\n\n        if elem_size(embed_dtype) * embed_dim % 16 != 0:\n            raise ValueError(\"Embedding dimension must be aligned to 16 bytes, got {}\".format(embed_dim))\n\n        inf_module = RaggedOpsBuilder().load()\n        self.kernel = inf_module.ragged_embed\n\n    def __call__(self,\n                 embedded_tokens: torch.Tensor,\n                 ragged_wrapper: RaggedBatchWrapper,\n                 embedding_weight: torch.Tensor,\n                 position_embed_weight: Optional[torch.Tensor] = None,\n                 position_embed_offset: int = 0) -> torch.Tensor:\n        \"\"\"\n        Ragged aware embedding lookup.\n\n        Args:\n            embedded_tokens (torch.Tensor): Output tensor of shape [num_tokens, embed_dim]\n            ragged_wrapper (RaggedBatchWrapper): Wrapper for the ragged batch.\n            embedding_weight (torch.Tensor): Embedding table of shape [vocab_size, embed_dim]\n        \"\"\"\n        self.kernel(embedded_tokens, ragged_wrapper.input_ids(),\n                    embedding_weight, position_embed_weight, position_embed_offset,\n                    ragged_wrapper.batch_metadata_buffer(), ragged_wrapper.inflight_seq_descriptors(),\n                    ragged_wrapper.tokens_to_seq(), ragged_wrapper.kv_ptrs())\n        return embedded_tokens\n", "deepspeed/inference/v2/kernels/ragged_ops/embed/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .embed import RaggedEmbeddingKernel\n", "deepspeed/inference/v2/kernels/ragged_ops/top_k_gating/top_k_gating.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\n\nfrom typing import Tuple\n\nfrom ... import DSKernelBase\nfrom ....inference_utils import DtypeEnum\nfrom ....ragged import RaggedBatchWrapper\nfrom deepspeed.ops.op_builder import RaggedOpsBuilder\n\n\nclass RaggedTopKGating(DSKernelBase):\n    \"\"\"\n    CUDA implementation of top-1 gating. This will perform a softmax on the logits,\n    and return the scale as well as its idx within that expert's allocation.\n    \"\"\"\n\n    supported_logit_dtypes = [DtypeEnum.fp16, DtypeEnum.bf16, DtypeEnum.fp32]\n\n    def __init__(self, logit_dtype: DtypeEnum) -> None:\n\n        if not isinstance(logit_dtype, DtypeEnum):\n            logit_dtype = DtypeEnum(logit_dtype)\n\n        if logit_dtype not in RaggedTopKGating.supported_logit_dtypes:\n            raise RuntimeError(f\"Unsupported logit dtype {logit_dtype}\")\n\n        inf_module = RaggedOpsBuilder().load()\n        self.kernel = inf_module.top_k_gating\n\n    def __call__(self, expert_counts: torch.Tensor, scores: torch.Tensor, assignments: torch.Tensor,\n                 offsets: torch.Tensor, logits: torch.Tensor,\n                 batch: RaggedBatchWrapper) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Perform the ragged top_k_gating.\n\n        Arguments:\n            expert_counts (torch.Tensor): Tensor of 0s of shape [n_experts] to be filled with\n                number of tokens assigned to each expert. This must be filled with 0s else\n                the copy kernel will buffer overflow. In order to minimize the zero-fill cost,\n                it is recommended to write to 0 during the MoE output remapping.\n            scores (torch.Tensor): Preallocated output of shape [n_tokens, n_top_k] to place expert scaling\n                value.\n            expert_assignment (torch.Tensor): Preallocated output of shape [n_tokens, n_top_k] to place\n                which expert a token has been assigned to.\n            expert_offset (torch.Tensor): Preallocated output of shape [n_tokens, n_top_k] to place which\n                offset within an experts group a token is.\n            logits (torch.Tensor): Raw logits of gating function.\n            batch (RaggedBatchWrapper): Batch information for ragged tensor.\n\n        Returns:\n            tuple of (expert_counts, scores, expert_assignment, expert_offset)\n        \"\"\"\n        self.kernel(expert_counts, scores, assignments, offsets, logits, batch.batch_metadata_buffer())\n        return expert_counts, scores, assignments, offsets\n", "deepspeed/inference/v2/kernels/ragged_ops/top_k_gating/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .top_k_gating import RaggedTopKGating\n", "deepspeed/inference/v2/kernels/ragged_ops/atom_builder/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .atom_builder import *\n", "deepspeed/inference/v2/kernels/ragged_ops/atom_builder/atom_builder.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Tuple\n\nimport torch\n\nfrom ... import DSKernelBase\nfrom deepspeed.ops.op_builder import RaggedOpsBuilder\nfrom ....ragged import RaggedBatchWrapper\n\n\nclass AtomBuilder(DSKernelBase):\n    \"\"\"\n    C++ implementation to populate the attention atoms for the blocked attention\n    kernel.\n    \"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"\n        Triggers compilation of the C++ implementation.\n        \"\"\"\n        inf_module = RaggedOpsBuilder().load()\n        self.kernel = inf_module.build_atoms\n\n    def __call__(self, atoms: torch.Tensor, ragged_batch: RaggedBatchWrapper, q_block_size: int,\n                 kv_block_size: int) -> Tuple[torch.Tensor, int]:\n        \"\"\"\n        Populates the attention atoms for the blocked attention kernel.\n\n        Args:\n            atoms (torch.Tensor): Pre-allocated int32 tensor of shape [max_atoms, 8]\n            ragged_batch (torch.Tensor): Wrapper for the ragged batch.\n            q_block_size (int): The block size for the queries (as determined by the\n                attention implementation)\n            kv_block_size (int): The block size for the keys/values (as determined by the\n                attention implementation)\n\n        Returns:\n\n        \"\"\"\n        if atoms.device != torch.device(\"cpu\"):\n            raise RuntimeError(\"AtomBuilder must be called on tensors\")\n\n        n_atoms = self.kernel(atoms, ragged_batch.batch_metadata_buffer(on_device=False),\n                              ragged_batch.inflight_seq_descriptors(on_device=False),\n                              ragged_batch.kv_ptrs(on_device=False), q_block_size, kv_block_size)\n        return atoms, n_atoms\n", "deepspeed/inference/v2/kernels/ragged_ops/moe_gather/moe_gather.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\n\nfrom ... import DSKernelBase\nfrom ....inference_utils import DtypeEnum\nfrom deepspeed.ops.op_builder import RaggedOpsBuilder\n\n\nclass MoEGather(DSKernelBase):\n    \"\"\"\n    CUDA implementation of MoE gather. This will bring the tokens back\n    to their original indices and perform the output scaling.\n    \"\"\"\n\n    supported_dtypes = [DtypeEnum.fp16, DtypeEnum.bf16]\n\n    def __init__(self, dtype: DtypeEnum, channels: int, normalize_scores: bool = False) -> None:\n\n        if not isinstance(dtype, DtypeEnum):\n            dtype = DtypeEnum(dtype)\n\n        if dtype not in MoEGather.supported_dtypes:\n            raise RuntimeError(f\"Unsupported dtype {dtype}\")\n\n        if channels % 8 != 0:\n            raise RuntimeError(f\"Channels {channels} must be divisible by 8\")\n\n        inf_module = RaggedOpsBuilder().load()\n        self.kernel = inf_module.moe_gather\n        self.normalize_scores = normalize_scores\n\n    def __call__(self, layer_output: torch.Tensor, moe_output: torch.Tensor, scores: torch.Tensor,\n                 mapped_slots: torch.Tensor, expert_counts: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Reorders the moe_output tokens into their original order and scales them by their\n        gating scale. This will be a no-op for padded tokens.\n\n        Arguments:\n            layer_output (torch.Tensor): The output of the layer of shape [n_tokens, hidden_size]. This has been scaled appropriately.\n            moe_output (torch.Tensor): The output of the MoE of shape [n_tokens * n_top_k, hidden_size].\n            scores (torch.Tensor): The gating scores of shape [n_tokens].\n            mapped_slots (torch.Tensor): The index of the token in the expert's input of shape [n_tokens, n_top_k]. The indices of token ``i`` in layer_output is ``mapped_slots[i]``.\n            expert_counts (torch.Tensor): The number of tokens assigned to each expert of shape [n_experts]. This is passed to fuse the clearing of this data structure into the gather.\n\n        Returns:\n            layer_output\n        \"\"\"\n        self.kernel(layer_output, moe_output, scores, mapped_slots, expert_counts, self.normalize_scores)\n        return layer_output\n", "deepspeed/inference/v2/kernels/ragged_ops/moe_gather/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .moe_gather import *\n", "deepspeed/inference/v2/kernels/core_ops/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .bias_activations import *\nfrom .blas_kernels import *\nfrom .cuda_layer_norm import *\nfrom .cuda_rms_norm import *\nfrom .gated_activations import *\nfrom .cuda_linear import *\n", "deepspeed/inference/v2/kernels/core_ops/blas_kernels/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .blas_linear import *\n", "deepspeed/inference/v2/kernels/core_ops/blas_kernels/blas_linear.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\n\nfrom ....inference_utils import DtypeEnum\nfrom deepspeed.ops.op_builder import InferenceCoreBuilder\nfrom ... import DSKernelBase\n\n\nclass BlasLibLinear(DSKernelBase):\n    \"\"\"\n    Wrapper around the BLAS matmul kernel for FP16/BF16/FP32 for CUDA/RoCM.\n\n    Performs z = x @ y\n    \"\"\"\n\n    supported_dtypes = [DtypeEnum.fp16, DtypeEnum.bf16, DtypeEnum.fp32]\n\n    def __init__(self, fp_dtype: DtypeEnum):\n        \"\"\"\n        Parameters:\n            fp_dtype (torch.dtype): Data type for the input/output. Supported values\n                are torch.float16, torch.bfloat16, and torch.float32.\n        \"\"\"\n        fp_dtype = DtypeEnum(fp_dtype)\n        if fp_dtype not in BlasLibLinear.supported_dtypes:\n            raise ValueError(\"Unsupported data type: {}, supported_dtypes are {}\".format(\n                fp_dtype, BlasLibLinear.supported_dtypes))\n\n        self.inf_module = InferenceCoreBuilder().load()\n        self.inf_module.create_handle()\n        self.kernel = self.inf_module.blas_linear\n\n    def __call__(self, output: torch.Tensor, hidden_states: torch.Tensor, weights: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Matmul kernel as implemented by platform BLAS library. The input must be 2D or larger. If\n        n-dimensional, the leading dimensions are folded into each other:\n            2D: m = x.size(0)\n            3D: m = x.size(0) * x.size(1)\n            4D: m = x.size(0) * x.size(1) * x.size(2) (etc...)\n        All inputs should be contiguous.\n\n        Parameters:\n            output (torch.Tensor): Output tensor. Shape is of [*, out_features]\n            hidden_states (torch.Tensor): Input tensor. Shape is of [*, in_features]\n            weights (torch.Tensor): Input tensor. Shape is of [out_features, in_features]\n\n        Returns:\n            z (torch.Tensor): Output tensor. Shape is of [m, n]\n        \"\"\"\n        self.kernel(output, hidden_states, weights)\n        return output\n", "deepspeed/inference/v2/kernels/core_ops/gated_activations/gated_activation.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Optional\n\nimport torch\n\nfrom ... import DSKernelBase\nfrom ....inference_utils import ActivationType, elem_size\nfrom deepspeed.ops.op_builder import InferenceCoreBuilder\n\n\nclass CUDAGatedActivation(DSKernelBase):\n    \"\"\"\n    CUDA implementation of gated activation kernel. This kernel assumes that the input\n    tensor has gate and activation values in adjacent channels. The output tensor should\n    have half the dimensionality of the input tensor.\n    \"\"\"\n\n    supported_dtypes = [torch.float16, torch.bfloat16, torch.float32]\n    supported_act_fns = [ActivationType.GEGLU, ActivationType.ReGLU, ActivationType.SiGLU]\n\n    def __init__(self, channels: int, fp_dtype: torch.dtype, act_fn: ActivationType) -> None:\n        \"\"\"\n        Compile and validate for the gated activation function.\n\n        Args:\n            channels (int): Number of columns in the output tensor. Must be divisible to align\n                to 8 bytes.\n            fp_dtype (torch.dtype): Data type for the input/output/gamma. Supported values\n                are torch.float16, torch.bfloat16, and torch.float32.\n            act_fn (ActivationType): Activation function to use. Only GEGLU is supported.\n        \"\"\"\n        if fp_dtype not in CUDAGatedActivation.supported_dtypes:\n            raise ValueError(\"Unsupported data type: {}, supported_dtypes are {}\".format(\n                fp_dtype, CUDAGatedActivation.supported_dtypes))\n\n        act_fn = ActivationType(act_fn)\n        if act_fn not in CUDAGatedActivation.supported_act_fns:\n            raise ValueError(\"Unsupported activation function: {}, supported_act_fns are {}\".format(\n                act_fn, CUDAGatedActivation.supported_act_fns))\n\n        if elem_size(fp_dtype) * channels % 8 != 0:\n            raise ValueError(\"Channels must be divisible by 16 bytes\")\n\n        if elem_size(fp_dtype) * channels > 98304:\n            raise ValueError(\n                \"Kernel only compiled to support 98304 bytes per row, please file an issue if your model requires more.\"\n            )\n\n        self.inf_module = InferenceCoreBuilder().load()\n        self.act_fn = act_fn\n        self.kernel = self.inf_module.gated_activation\n\n    def __call__(self, output: torch.Tensor, input: torch.Tensor, bias: Optional[torch.Tensor] = None) -> None:\n        \"\"\"\n        Performs gated activation on the input tensor, writing the result to the output tensor.\n\n        Args:\n            output (torch.Tensor): Output tensor. Can be of [T, C // 2] or [B, S, C // 2]\n            input (torch.Tensor): Input tensor. Can be of [T, C] or [B, S, C]\n        \"\"\"\n        self.kernel(output, input, bias, self.act_fn.value)\n", "deepspeed/inference/v2/kernels/core_ops/gated_activations/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .gated_activation import *\n", "deepspeed/inference/v2/kernels/core_ops/cuda_rms_norm/rms_norm.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\n\nfrom .rms_norm_base import CUDARMSNormBase\n\n\nclass CUDARMSNorm(CUDARMSNormBase):\n    \"\"\"\n    Floating point layer norm kernel for CUDA/RoCM.\n\n    Performs: z = ln(x)\n    \"\"\"\n\n    def __call__(self, output_z: torch.Tensor, input_x: torch.Tensor, gamma: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        output_z may alias input_x directly. All Tensors should have the same shape.\n\n        Parameters:\n            output_z (torch.Tensor): Output tensor.\n            input_x (torch.Tensor): Input tensor.\n            gamma (torch.Tensor): Gamma tensor.\n        \"\"\"\n        self.inf_module.rms_norm(output_z, input_x, gamma, self.epsilon)\n        return output_z\n", "deepspeed/inference/v2/kernels/core_ops/cuda_rms_norm/rms_pre_norm.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Tuple\n\nimport torch\n\nfrom .rms_norm_base import CUDARMSNormBase\n\n\nclass CUDARMSPreNorm(CUDARMSNormBase):\n    \"\"\"\n    Floating point pre-LayerNorm kernel for CUDA/RoCM.\n\n    Performs: z_res = x_res + y_hid\n              z_hid = ln(z_hid)\n    \"\"\"\n\n    def __call__(self, z_res: torch.Tensor, z_hid: torch.Tensor, x_res: torch.Tensor, y_hid: torch.Tensor,\n                 gamma: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        z_res can alias x_res. All non-parameter input/output tensors\n        must have the same shape. z_hid can alias y_hid.\n\n        Parameters:\n            z_res (torch.Tensor): Output residual.\n            z_hid (torch.Tensor): Output hidden states.\n            x_res (torch.Tensor): Input residual.\n            y_hid (torch.Tensor): Input hidden states.\n            gamma (torch.Tensor): Gamma tensor.\n            beta (torch.Tensor): Beta tensor.\n\n        Returns:\n            output (torch.Tensor): Output tensor.\n        \"\"\"\n        self.inf_module.rms_pre_norm(z_hid, z_res, y_hid, x_res, gamma, self.epsilon)\n        return z_res, z_hid\n", "deepspeed/inference/v2/kernels/core_ops/cuda_rms_norm/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .rms_norm import CUDARMSNorm\nfrom .rms_pre_norm import CUDARMSPreNorm\n", "deepspeed/inference/v2/kernels/core_ops/cuda_rms_norm/rms_norm_base.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\n\nfrom ... import DSKernelBase\nfrom ....inference_utils import elem_size\nfrom deepspeed.ops.op_builder import InferenceCoreBuilder\n\n\nclass CUDARMSNormBase(DSKernelBase):\n    \"\"\"\n    Base class for CUDA LN kernels. They all same the same validation logic,\n    so we can share it here.\n    \"\"\"\n\n    supported_dtypes = [torch.float16, torch.bfloat16, torch.float32]\n\n    def __init__(self, channels: int, fp_dtype: torch.dtype, epsilon: float = 1e-5):\n        \"\"\"\n        Parameters:\n            channels (int): Number of channels in the input tensor. Must be divisible to align\n                to 16 bytes.\n            fp_dtype (torch.dtype): Data type for the input/output/gamma. Supported values\n                are torch.float16, torch.bfloat16, and torch.float32.\n        \"\"\"\n        if fp_dtype not in CUDARMSNormBase.supported_dtypes:\n            raise ValueError(\"Unsupported data type: {}, supported_dtypes are {}\".format(\n                fp_dtype, CUDARMSNormBase.supported_dtypes))\n\n        if elem_size(fp_dtype) * channels % 16 != 0:\n            raise ValueError(\"channels must be divisible by 16 bytes\")\n\n        self.inf_module = InferenceCoreBuilder().load()\n        self.epsilon = epsilon\n", "deepspeed/inference/v2/kernels/core_ops/bias_activations/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .bias_activation import *\n", "deepspeed/inference/v2/kernels/core_ops/bias_activations/bias_activation.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Optional\n\nimport torch\n\nfrom ....inference_utils import ActivationType, DtypeEnum\nfrom deepspeed.ops.op_builder import InferenceCoreBuilder\nfrom ... import DSKernelBase\n\n\nclass CUDABiasActivation(DSKernelBase):\n    \"\"\"\n    CUDA implementation of bias activation kernel. This kernel should be deprecated once\n    we are fusing the bias activation into the linear kernel in all scenarios.\n    \"\"\"\n\n    supported_dtypes = [DtypeEnum.fp16, DtypeEnum.bf16]\n    supported_act_fns = [ActivationType.IDENTITY, ActivationType.GELU, ActivationType.RELU, ActivationType.SILU]\n\n    def __init__(self, channels: int, dtype: DtypeEnum, act_fn: ActivationType) -> None:\n        \"\"\"\n        Compile and validate for the fused bias-activation kernel.\n\n        Parameters:\n            channels (int): Number of channels to expect in the activation.\n            dtype (torch.dtype): Data type for the input/output. Supported values\n                are DtypeEnum.fp16 and DtypeEnum.bf16.\n            act_fn (ActivationType): Activation function to use. Only IDENTITY, GELU, RELU, and SILU are supported.\n        \"\"\"\n\n        if channels % 8 != 0:\n            raise ValueError(\"channels must be divisible by 8\")\n\n        if DtypeEnum(dtype) not in CUDABiasActivation.supported_dtypes:\n            raise ValueError(\"Unsupported data type: {}, supported_dtypes are {}\".format(\n                dtype, CUDABiasActivation.supported_dtypes))\n\n        act_fn = ActivationType(act_fn)\n        if act_fn not in CUDABiasActivation.supported_act_fns:\n            raise ValueError(\"Unsupported activation function: {}, supported_act_fns are {}\".format(\n                act_fn, CUDABiasActivation.supported_act_fns))\n\n        inf_module = InferenceCoreBuilder().load()\n        self.kernel = inf_module.bias_activation\n        self.act_fn = act_fn\n\n    def __call__(self, activation: torch.Tensor, bias: Optional[torch.Tensor] = None) -> torch.Tensor:\n        \"\"\"\n        Add an optional bias and perform the non-linear activation function.\n\n        Parameters:\n            activation (torch.Tensor): Input tensor of shape [tokens, channels]\n            bias (torch.Tensor): Optional bias tensor of shape [channels]\n\n        Returns:\n            activation that has been updated in-place\n        \"\"\"\n        self.kernel(activation, bias, self.act_fn.value)\n", "deepspeed/inference/v2/kernels/core_ops/cuda_layer_norm/cuda_ln.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\n\nfrom .cuda_fp_ln_base import CUDAFPLNBase\n\n\nclass CUDAFPLN(CUDAFPLNBase):\n    \"\"\"\n    Floating point layer norm kernel for CUDA/RoCM.\n\n    Performs: z = ln(x)\n    \"\"\"\n\n    def __call__(self, output_z: torch.Tensor, input_x: torch.Tensor, gamma: torch.Tensor,\n                 beta: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        output_z may alias input_x directly. All Tensors should have the same shape.\n\n        Parameters:\n            output_z (torch.Tensor): Output tensor.\n            input_x (torch.Tensor): Input tensor.\n            gamma (torch.Tensor): Gamma tensor.\n            beta (torch.Tensor): Beta tensor.\n        \"\"\"\n        self.inf_module.layer_norm(output_z, input_x, gamma, beta, self.epsilon)\n        return output_z\n", "deepspeed/inference/v2/kernels/core_ops/cuda_layer_norm/cuda_fp_ln_base.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\n\nfrom ... import DSKernelBase\nfrom ....inference_utils import elem_size\nfrom deepspeed.ops.op_builder import InferenceCoreBuilder\n\n\nclass CUDAFPLNBase(DSKernelBase):\n    \"\"\"\n    Base class for CUDA LN kernels. They all same the same validation logic,\n    so we can share it here.\n    \"\"\"\n\n    supported_dtypes = [torch.float16, torch.bfloat16, torch.float32]\n\n    def __init__(self, channels: int, fp_dtype: torch.dtype, epsilon: float = 1e-5):\n        \"\"\"\n        Parameters:\n            channels (int): Number of channels in the input tensor. Must be divisible to align\n                to 16 bytes.\n            fp_dtype (torch.dtype): Data type for the input/output/gamma. Supported values\n                are torch.float16, torch.bfloat16, and torch.float32.\n        \"\"\"\n        if fp_dtype not in CUDAFPLNBase.supported_dtypes:\n            raise ValueError(\"Unsupported data type: {}, supported_dtypes are {}\".format(\n                fp_dtype, CUDAFPLNBase.supported_dtypes))\n\n        if elem_size(fp_dtype) * channels % 16 != 0:\n            raise ValueError(\"channels must be divisible by 16 bytes\")\n\n        self.inf_module = InferenceCoreBuilder().load()\n        self.epsilon = epsilon\n", "deepspeed/inference/v2/kernels/core_ops/cuda_layer_norm/cuda_post_ln.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\n\nfrom .cuda_fp_ln_base import CUDAFPLNBase\n\n\nclass CUDAFPPostLN(CUDAFPLNBase):\n    \"\"\"\n    Floating point post-LayerNorm kernel for CUDA/RoCM.\n\n    Performs: z = ln(x + y)\n    \"\"\"\n\n    def __call__(self, output_z: torch.Tensor, input_x: torch.Tensor, input_y: torch.Tensor, gamma: torch.Tensor,\n                 beta: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Either input_x or input_y can alias output_z.\n\n        Parameters:\n            output_z (torch.Tensor): Output tensor.\n            input_x (torch.Tensor): Input tensor.\n            input_y (torch.Tensor): Input tensor.\n            gamma (torch.Tensor): Gamma tensor.\n            beta (torch.Tensor): Beta tensor.\n\n        Returns:\n            output (torch.Tensor): Output tensor.\n        \"\"\"\n        self.inf_module.post_layer_norm(output_z, input_x, input_y, gamma, beta, self.epsilon)\n        return output_z\n", "deepspeed/inference/v2/kernels/core_ops/cuda_layer_norm/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .cuda_ln import *\nfrom .cuda_post_ln import *\nfrom .cuda_pre_ln import *\n", "deepspeed/inference/v2/kernels/core_ops/cuda_layer_norm/cuda_pre_ln.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Tuple\n\nimport torch\n\nfrom .cuda_fp_ln_base import CUDAFPLNBase\n\n\nclass CUDAFPPreLN(CUDAFPLNBase):\n    \"\"\"\n    Floating point pre-LayerNorm kernel for CUDA/RoCM.\n\n    Performs: z_res = x_res + y_hid\n              z_hid = ln(z_hid)\n    \"\"\"\n\n    def __call__(self, z_res: torch.Tensor, z_hid: torch.Tensor, x_res: torch.Tensor, y_hid: torch.Tensor,\n                 gamma: torch.Tensor, beta: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        z_res can alias x_res. All non-parameter input/output tensors\n        must have the same shape. z_hid can alias y_hid.\n\n        Parameters:\n            z_res (torch.Tensor): Output residual.\n            z_hid (torch.Tensor): Output hidden states.\n            x_res (torch.Tensor): Input residual.\n            y_hid (torch.Tensor): Input hidden states.\n            gamma (torch.Tensor): Gamma tensor.\n            beta (torch.Tensor): Beta tensor.\n\n        Returns:\n            output (torch.Tensor): Output tensor.\n        \"\"\"\n        self.inf_module.pre_layer_norm(z_res, z_hid, x_res, y_hid, gamma, beta, self.epsilon)\n        return z_res, z_hid\n", "deepspeed/inference/v2/kernels/core_ops/cuda_linear/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .cuda_linear import *\n", "deepspeed/inference/v2/kernels/core_ops/cuda_linear/cuda_linear.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\n\nfrom ....inference_utils import DtypeEnum\nfrom ....logging import inference_logger\nfrom deepspeed.ops.op_builder import InferenceCoreBuilder\nfrom ... import DSKernelBase\n\n\nclass CUDAWf6Af16Linear(DSKernelBase):\n    \"\"\"\n    Wrapper around the CUDA kernel of Wf6Af16 quantized linear.\n\n    Performs z = x @ y\n    \"\"\"\n    supported_dtypes = [DtypeEnum.fp16]\n\n    def __init__(self):\n        self.inf_module = InferenceCoreBuilder().load()\n        self.inf_module.create_handle()\n        self.kernel = self.inf_module.cuda_wf6af16_linear\n        # The split_k_map is profiled on A100-80G GPU for some common shapes.\n        # It is an array of dictionaries, where the array index is the tokens chunk id.\n        # The dictionary is the mapping from the output channel to the split-K size.\n        self.split_k_map = [\n            {  # tokens: [1, 64]\n                3072: 18,\n                4096: 13,\n                5120: 10,\n                6144: 9,\n                8192: 6,\n                10240: 5,\n                14336: 7,\n                28672: 7,\n                57344: 7\n            },\n            {  # tokens: [65:128]\n                3072: 9,\n                4096: 6,\n                5120: 5,\n                6144: 9,\n                8192: 3,\n                10240: 5,\n                14336: 7,\n                28672: 7,\n                57344: 6\n            },\n            {  # tokens: [129:192]\n                3072: 6,\n                4096: 4,\n                5120: 7,\n                6144: 3,\n                8192: 2,\n                10240: 5,\n                14336: 5,\n                28672: 5,\n                57344: 4\n            },\n            {  # tokens: [193:256]\n                3072: 9,\n                4096: 3,\n                5120: 5,\n                6144: 2,\n                8192: 5,\n                10240: 4,\n                14336: 8,\n                28672: 6,\n                57344: 4\n            },\n            {  # tokens: [257:320]\n                3072: 7,\n                4096: 5,\n                5120: 2,\n                6144: 5,\n                8192: 4,\n                10240: 1,\n                14336: 3,\n                28672: 3,\n                57344: 4\n            },\n            {  # tokens: [321:384]\n                3072: 3,\n                4096: 2,\n                5120: 5,\n                6144: 3,\n                8192: 1,\n                10240: 8,\n                14336: 3,\n                28672: 4,\n                57344: 3\n            },\n            {  # tokens: [385:448]\n                3072: 5,\n                4096: 7,\n                5120: 3,\n                6144: 5,\n                8192: 7,\n                10240: 3,\n                14336: 1,\n                28672: 1,\n                57344: 3\n            },\n            {  # tokens: [449:512]\n                3072: 2,\n                4096: 5,\n                5120: 4,\n                6144: 1,\n                8192: 5,\n                10240: 2,\n                14336: 6,\n                28672: 4,\n                57344: 1\n            },\n            {  # tokens: [513:576]\n                3072: 2,\n                4096: 3,\n                5120: 1,\n                6144: 1,\n                8192: 3,\n                10240: 3,\n                14336: 3,\n                28672: 1,\n                57344: 1\n            },\n            {  # tokens: [577:640]\n                3072: 5,\n                4096: 4,\n                5120: 1,\n                6144: 4,\n                8192: 2,\n                10240: 1,\n                14336: 1,\n                28672: 1,\n                57344: 1\n            },\n            {  # tokens: [641:704]\n                3072: 3,\n                4096: 1,\n                5120: 2,\n                6144: 2,\n                8192: 1,\n                10240: 2,\n                14336: 1,\n                28672: 1,\n                57344: 1\n            },\n            {  # tokens: [705:768]\n                3072: 3,\n                4096: 1,\n                5120: 3,\n                6144: 2,\n                8192: 1,\n                10240: 1,\n                14336: 1,\n                28672: 1,\n                57344: 1\n            }\n        ]\n\n    def __call__(self, output: torch.Tensor, hidden_states: torch.Tensor, weights_2bit: torch.Tensor,\n                 weights_4bit: torch.Tensor, scale: torch.Tensor, out_channels, tokens, in_channels) -> torch.Tensor:\n        \"\"\"\n        Matmul kernel of FP6 weight-only quantized linear. All inputs should be contiguous.\n        It does not support batched-matmul.\n\n        Parameters:\n            output (torch.Tensor): Output tensor. Shape is of [token_number, out_features]\n            hidden_states (torch.Tensor): Input tensor. Shape is of [token_number, in_features]\n            weights_2bit (torch.Tensor): Input tensor of the 2-bit slice. Shape is of [out_features*2/8, in_features]\n            weights_4bit (torch.Tensor): Input tensor of the 4-bit slice. Shape is of [out_features*4/8, in_features]\n            scale (torch.Tensor): Input tensor. Shape is of [out_features], since the scale is per output channel\n            out_channels (int): The number of output channels\n            tokens (int): The number of tokens\n            in_channels (int): The number of input channels\n        \"\"\"\n\n        if out_channels % 256 != 0 or in_channels % 64 != 0:\n            raise ValueError(\"The out and in channel should be multiple of 256 and 64 respectively.\")\n\n        # TODO: add a more general heuristic to determine the split-K.\n        split_k = -1  # not initialized\n        if tokens <= 768:\n            # Try to find the split-K from the pre-profiled map.\n            tokens_chunk_id = (tokens - 1) // 64\n            split_k = self.split_k_map[tokens_chunk_id].get(out_channels, -1)\n        if split_k == -1:\n            split_k = 1\n            inference_logger().warning(\n                f\"The split-K setting may be suboptimal for shape {tokens}x{in_channels}x{out_channels}...\")\n\n        workspace = self.get_workspace(out_channels, tokens, in_channels, split_k, torch.float, hidden_states.device)\n        self.kernel(output, hidden_states, weights_2bit, weights_4bit, scale, workspace, out_channels, tokens,\n                    in_channels, split_k)\n\n    def get_workspace(self, out_channels: int, tokens: int, in_channels: int, split_k: int, dtype,\n                      device) -> torch.Tensor:\n        \"\"\"\n        Allocate workspace for the kernel. The workspace is used to store the intermediate results of the matmul before\n        split-K. The split-K size is determined by the size of the matmul.\n        \"\"\"\n        workspace = torch.empty((split_k, out_channels, tokens), dtype=dtype, device=device)\n\n        return workspace\n", "deepspeed/inference/v2/kernels/cutlass_ops/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .mixed_gemm import *\nfrom .moe_gemm import *\n", "deepspeed/inference/v2/kernels/cutlass_ops/moe_gemm/moe_gemm.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\n\nfrom ... import DSKernelBase\nfrom ....inference_utils import ActivationType, DtypeEnum\nfrom deepspeed.ops.op_builder import InferenceCutlassBuilder\n\nfrom typing import Optional\n\n\nclass MoEGEMM(DSKernelBase):\n    \"\"\"\n    CUTLASS implementation of MoE GEMM.\n    \"\"\"\n\n    supported_dtypes = [DtypeEnum.fp16, DtypeEnum.bf16]\n    supported_act_fns = [ActivationType.GELU, ActivationType.SILU, ActivationType.RELU, ActivationType.IDENTITY]\n\n    def __init__(self, fp_dtype: DtypeEnum, act_fn: ActivationType) -> None:\n\n        if not isinstance(fp_dtype, DtypeEnum):\n            fp_dtype = DtypeEnum(fp_dtype)\n\n        if fp_dtype not in MoEGEMM.supported_dtypes:\n            raise ValueError(\"Unsupported data type: {}, supported_dtypes are {}\".format(\n                fp_dtype, MoEGEMM.supported_dtypes))\n\n        if act_fn not in MoEGEMM.supported_act_fns:\n            raise ValueError(\"Unsupported activation function: {}, supported_act_fns are {}\".format(\n                act_fn, MoEGEMM.supported_act_fns))\n\n        inf_module = InferenceCutlassBuilder().load()\n        self.kernel = inf_module.moe_gemm\n        self.act_fn = act_fn\n\n    def __call__(self,\n                 ordered_output: torch.Tensor,\n                 ordered_input: torch.Tensor,\n                 weights: torch.Tensor,\n                 total_rows_before_expert: torch.Tensor,\n                 biases: Optional[torch.Tensor] = None) -> None:\n        \"\"\"\n            Performs a MoE GEMM. Note that the stride between token inputs must be even (the distance between byte 1 of token 0 and token 1 must be the same as the distance between byte 1 of token 1 and token 2).\n\n            Arguments:\n                ordered_output (torch.Tensor): The output of the MoE GEMM of shape [n_tokens, out_neurons].\n                ordered_input (torch.Tensor): The direct input for the MoE GEMM of shape [n_tokens, in_neurons].\n                weights (torch.Tensor): The weights of shape [n_experts, in_neurons, out_neurons]. These weights must be contiguous.\n                total_rows_before_expert (torch.Tensor): The total number of rows before each expert of shape [n_experts].\n                biases (torch.Tensor): The biases of shape [n_experts, out_neurons]. These biases must be contiguous.\n\n            Returns:\n                ordered_output\n            \"\"\"\n        self.kernel(ordered_output, ordered_input, weights, biases, total_rows_before_expert, self.act_fn)\n        return ordered_output\n", "deepspeed/inference/v2/kernels/cutlass_ops/moe_gemm/mixed_moe_gemm.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\n\nfrom ... import DSKernelBase\nfrom ....inference_utils import ActivationType, DtypeEnum\nfrom deepspeed.ops.op_builder import InferenceCutlassBuilder\n\nfrom typing import Optional\n\n\nclass MixedMoEGEMM(DSKernelBase):\n    \"\"\"\n    CUTLASS implementation of MoE GEMM.\n    \"\"\"\n\n    supported_dtypes = [DtypeEnum.fp16, DtypeEnum.bf16]\n    supported_act_fns = [ActivationType.GELU, ActivationType.SILU, ActivationType.RELU, ActivationType.IDENTITY]\n\n    def __init__(self, fp_dtype: DtypeEnum, act_fn: ActivationType, num_bits: int) -> None:\n\n        if not isinstance(fp_dtype, DtypeEnum):\n            fp_dtype = DtypeEnum(fp_dtype)\n\n        if fp_dtype not in MixedMoEGEMM.supported_dtypes:\n            raise ValueError(\"Unsupported data type: {}, supported_dtypes are {}\".format(\n                fp_dtype, MixedMoEGEMM.supported_dtypes))\n\n        if act_fn not in MixedMoEGEMM.supported_act_fns:\n            raise ValueError(\"Unsupported activation function: {}, supported_act_fns are {}\".format(\n                act_fn, MixedMoEGEMM.supported_act_fns))\n\n        if num_bits != 4 and num_bits != 8:\n            raise ValueError(\"Unsupported num_bits: {}, supported num_bits are 4 and 8\".format(num_bits))\n\n        inf_module = InferenceCutlassBuilder().load()\n        self.num_bits = num_bits\n        self.kernel = inf_module.moe_gemm\n        self.act_fn = act_fn\n\n    def __call__(self,\n                 ordered_output: torch.Tensor,\n                 ordered_input: torch.Tensor,\n                 weights: torch.Tensor,\n                 scales: torch.Tensor,\n                 total_rows_before_expert: torch.Tensor,\n                 biases: Optional[torch.Tensor] = None) -> None:\n        \"\"\"\n            Performs a MoE GEMM. Note that the stride between token inputs must be even (the distance between byte 1 of token 0 and token 1 must be the same as the distance between byte 1 of token 1 and token 2).\n\n            Arguments:\n                ordered_output (torch.Tensor): The output of the MoE GEMM of shape [n_tokens, out_neurons].\n                ordered_input (torch.Tensor): The direct input for the MoE GEMM of shape [n_tokens, in_neurons].\n                weights (torch.Tensor): The weights of shape [n_experts, in_neurons, out_neurons]. These weights must be contiguous.\n                scales (torch.Tensor): The scales of shape [n_experts, out_neurons]. These scales must be contiguous.\n                total_rows_before_expert (torch.Tensor): The total number of rows before each expert of shape [n_experts].\n                biases (torch.Tensor): The biases of shape [n_experts, out_neurons]. These biases must be contiguous.\n\n            Returns:\n                ordered_output\n            \"\"\"\n        self.kernel(ordered_output, ordered_input, weights, scales, biases, total_rows_before_expert, self.num_bits,\n                    self.act_fn)\n        return ordered_output\n", "deepspeed/inference/v2/kernels/cutlass_ops/moe_gemm/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .mixed_moe_gemm import *\nfrom .moe_gemm import *\n", "deepspeed/inference/v2/kernels/cutlass_ops/mixed_gemm/mixed_gemm.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\n\nfrom ... import DSKernelBase\nfrom ....inference_utils import ActivationType, DtypeEnum\nfrom deepspeed.ops.op_builder import InferenceCutlassBuilder\n\nfrom typing import Optional\n\n\nclass MixedGEMM(DSKernelBase):\n    \"\"\"\n    CUTLASS implementation of MoE GEMM.\n    \"\"\"\n\n    supported_dtypes = [DtypeEnum.fp16, DtypeEnum.bf16]\n    supported_act_fns = [ActivationType.GELU, ActivationType.SILU, ActivationType.RELU, ActivationType.IDENTITY]\n\n    def __init__(self, fp_dtype: DtypeEnum, act_fn: ActivationType, num_bits: int) -> None:\n\n        if not isinstance(fp_dtype, DtypeEnum):\n            fp_dtype = DtypeEnum(fp_dtype)\n\n        if fp_dtype not in MixedGEMM.supported_dtypes:\n            raise ValueError(\"Unsupported data type: {}, supported_dtypes are {}\".format(\n                fp_dtype, MixedGEMM.supported_dtypes))\n\n        if act_fn not in MixedGEMM.supported_act_fns:\n            raise ValueError(\"Unsupported activation function: {}, supported_act_fns are {}\".format(\n                act_fn, MixedGEMM.supported_act_fns))\n\n        if num_bits != 4 and num_bits != 8:\n            raise ValueError(\"Unsupported num_bits: {}, supported num_bits are 4 and 8\".format(num_bits))\n\n        inf_module = InferenceCutlassBuilder().load()\n        self.num_bits = num_bits\n        self.kernel = inf_module.moe_gemm\n        self.act_fn = act_fn\n\n    def __call__(self,\n                 output: torch.Tensor,\n                 hidden_states: torch.Tensor,\n                 weights: torch.Tensor,\n                 scales: torch.Tensor,\n                 biases: Optional[torch.Tensor] = None) -> None:\n        \"\"\"\n            Performs a MoE GEMM. Note that the stride between token inputs must be even (the distance between byte 1 of token 0 and token 1 must be the same as the distance between byte 1 of token 1 and token 2).\n\n            Arguments:\n                output (torch.Tensor): The output of the MoE GEMM of shape [n_tokens, out_neurons].\n                hidden_states (torch.Tensor): The direct input for the MoE GEMM of shape [n_tokens, in_neurons].\n                weights (torch.Tensor): The weights of shape [in_neurons, out_neurons]. These weights must be contiguous.\n                scales (torch.Tensor): The scales of shape [out_neurons]. These scales must be contiguous.\n                biases (torch.Tensor): The biases of shape [out_neurons]. These biases must be contiguous.\n\n            Returns:\n                output\n            \"\"\"\n        self.kernel(output, hidden_states, weights, biases, self.num_bits, self.act_fn)\n        return output\n", "deepspeed/inference/v2/kernels/cutlass_ops/mixed_gemm/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .mixed_gemm import *\n", "deepspeed/inference/v2/model_implementations/parameter_base.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport weakref\nfrom abc import abstractmethod\nfrom typing import Type\n\nimport torch\n\n# Currently have dependency loops for the type hints.\nInferenceModel = Type[\"InferenceModel\"]\nLayerContainer = Type[\"LayerContainer\"]\n\nMAPPING_KEY = \"PARAM_MAPPING\"\n\n\ndef make_param_getter(clsname, param):\n    \"\"\"\n    Normal getter implementation for a property.\n    \"\"\"\n\n    def param_getter(self):\n        return getattr(self, f\"__{clsname}__{param}\")\n\n    return param_getter\n\n\ndef make_param_setter(clsname, param):\n    \"\"\"\n    Setter implementation that will call complete component to potentially\n    finalize the parameter.\n    \"\"\"\n\n    def param_setter(self, value):\n        setattr(self, f\"__{clsname}__{param}\", value)\n        self.complete_component()\n\n    return param_setter\n\n\ndef make_readonly_setter():\n    \"\"\"\n    Setter implementation that will raise an error if called.\n    \"\"\"\n\n    def paramlist_setter(self, value):\n        raise ValueError(\"Cannot set a ParametrizedList directly.\")\n\n    return paramlist_setter\n\n\nclass ParameterMetaclass(type):\n    \"\"\"\n    MetaClass for the ParameterBase base class. This class will parse the `src_params`\n    attribute and create properties for each of the dependencies. A dependency can either\n    be represented as a string, which is interpreted as a named Tensor, or a `ParametrizedList`\n    subclass.\n    \"\"\"\n\n    def __new__(cls, clsname, bases, attrs):\n\n        annotations = attrs.get(\"__annotations__\", {})\n        dependencies = {\n            name: annotation\n            for name, annotation in annotations.items() if issubclass(annotation, (torch.Tensor, ParametrizedList))\n        }\n        n_dependencies = len(dependencies)\n\n        # Create properties for each of our dependencies\n        for d_name, d_type in dependencies.items():\n            if issubclass(d_type, ParametrizedList):\n                assert hasattr(\n                    d_type, \"count_attr\"\n                ), \"ParametrizedList must have a count_attr attribute to access on the inference module.\"\n                attrs[d_name] = property(make_param_getter(clsname, d_name), make_readonly_setter())\n            else:  # torch.Tensor\n                attrs[d_name] = property(make_param_getter(clsname, d_name), make_param_setter(clsname, d_name))\n\n        new_cls = super().__new__(cls, clsname, bases, attrs)\n        new_cls.n_dependencies = n_dependencies\n\n        return new_cls\n\n    def __call__(cls, *args, **kwargs):\n        new_obj = super().__call__(*args, **kwargs)\n        new_obj.__init__(*args, **kwargs)\n\n        setattr(new_obj, \"dest_param\", None)\n\n        # Initialize our dependences to None/empty `ParametrizedList`s\n        for name, annotation in new_obj.__annotations__.items():\n            if issubclass(annotation, ParametrizedList):\n                #TODO(jeff): update assert with this, model implementation attribute does not align or missing wrt the ParametrizedList attributes\n                assert hasattr(\n                    new_obj.inference_model, annotation.count_attr\n                ), f\"new_obj={new_obj.__class__.__name__}, name={name}, annotation.count_attr={annotation.count_attr}\"\n                param_list = annotation(new_obj, getattr(new_obj.inference_model, annotation.count_attr))\n                setattr(new_obj, f\"__{new_obj.__class__.__name__}__{name}\", param_list)\n            else:  # torch.Tensor\n                setattr(new_obj, f\"__{new_obj.__class__.__name__}__{name}\", None)\n\n        return new_obj\n\n\nclass ParameterBase(metaclass=ParameterMetaclass):\n    \"\"\"\n    A ParameterBase allows us to consolidate tracking the dependencies of loading a parameter from\n    a checkpoint into a single object. This class should not be used directly, but rather subclassed\n    and the `src_params` attribute set to a list of strings and/or `ParametrizedList`s.\n    \"\"\"\n\n    # inference_model: InferenceModel\n    \"\"\"\n    Inference model that will provide context on how to shard and transform the parameter.\n    \"\"\"\n\n    #completed_components: int\n    \"\"\"\n    How many of the layer dependencies have been met. This is used to determine when the parameter\n    is ready to be finalized. A ParametrizedList counts as a single dependency for the purposes\n    of this counter.\n    \"\"\"\n\n    def __init__(self, model: InferenceModel, parent_container: LayerContainer) -> None:\n        \"\"\"\n        Direct constructor. This should not be called from client code.\n\n        Args:\n            model (InferenceModel): Inference model that will be used to shard and transform the\n                parameter in `finalize`.\n            parent_container (LayerContainer): The parent container that this parameter is a member\n                of. We will build a weakref to this container to call the finalization callback.\n        \"\"\"\n        self.inference_model = model\n        self.completed_components = 0\n        self.parent_container = weakref.ref(parent_container)\n\n    @abstractmethod\n    def finalize(self) -> torch.Tensor:\n        \"\"\"\n        Finalize the parameter after all of its source parameters have been set. This method\n        will be automatically called when all inputs have been set. It should return the Tensor\n        with all transformations performed on it.\n        \"\"\"\n        pass\n\n    def complete_component(self) -> None:\n        \"\"\"\n        Mark a component as completed. This should be called by the relevant setter of a direct\n        property or a ParametrizedList. This method will automatically call `finalize` when all\n        dependencies have been met and then call the finalization callback on the parent container.\n\n        Once the finalization callback has been called, the parameter will be replaced with the\n        `dst_param` attribute on the parent container, and this instance will be destroyed.\n        \"\"\"\n        self.completed_components += 1\n\n        if self.completed_components != self.n_dependencies:\n            return\n\n        finalized_param = self.finalize()\n        self.parent_container().finalization_callback(self, finalized_param)\n\n\nclass ParametrizedList:\n    \"\"\"\n    A ParametrizedList is a list of parameters that are dependencies\n    of a `ParameterBase` but may vary in length depending on the model\n    configuration (rather than architecture). For example, a MoE layer\n    may have different number of experts depending on the size of the model.\n\n    This class is used to manage these lists and provide integer indexing\n    of a single component rather than accessing names directly. For example,\n    it tends to be more natural to access the 8th expert with `experts[8]`\n    rather than a name like `expert_8`, especially as an attribute.\n\n    To inherit from this class, set static variables `name` and `count_attr`.\n\n    ```python\n    class MyParametrizedList(ParametrizedList):\n        count_attr: str = \"my_list_count\"\n    ```\n\n    In the above example, `my_list_count` should be an accessible attribute\n    of the inference model (i.e. via `self.inference_model.my_list_count`).\n\n    NOTE: There are some APIs in which this type cannot be used as if it is\n    just a list of Tensors. For example, `torch.cat(param_list)` will not work.\n    However, you can make it compatible with a tuple wrapper:\n        `torch.cat(tuple(param_list))`\n    \"\"\"\n\n    n_params: int\n    \"\"\"\n    Number of params this list contains.\n    \"\"\"\n\n    param: ParameterBase\n    \"\"\"\n    WeakRef to the owning parameter.\n    \"\"\"\n\n    def __init__(self, param: ParameterBase, n_params: int) -> None:\n        \"\"\"\n        Constructor. Should not be called from client code.\n\n        Args:\n            param (ParameterBase): The owning parameter.\n            n_params (int): The number of parameters this list contains. This should be\n        \"\"\"\n        self.n_params = n_params\n        self.set_params = 0\n        self.param = weakref.ref(param)\n        self._params = [None] * n_params\n\n    def __getitem__(self, index):\n        return self._params[index]\n\n    def __setitem__(self, index, value):\n        if self._params[index] is not None:\n            raise ValueError(\"Cannot set a parameter twice.\")\n\n        self._params[index] = value\n        self.set_params += 1\n\n        if self.set_params != self.n_params:\n            return\n\n        self.param().complete_component()\n\n    def __iter__(self):\n        return iter(self._params)\n\n\ndef ParamList(attr: str):\n    \"\"\"\n    Helper to create a subclass of ParametrizedList with the desired `count_attr`.\n\n    In this manner, we can annotate the type of a Parameter dependency with the\n    following:\n\n    ```python\n    class CustomParameter(ParameterBase):\n        dependency_list: ParamList(\"dependencies_count_name\")\n    ```\n\n    where \"dependencies_count_name\" is the name of the attribute on the inference model.\n    \"\"\"\n\n    class ParametrizedListInstance(ParametrizedList):\n        count_attr: str = attr\n\n    return ParametrizedListInstance\n", "deepspeed/inference/v2/model_implementations/inference_model_base.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom abc import ABC, abstractmethod\nfrom typing import Iterable, Optional, Tuple, Type\n\nimport torch\n\nimport deepspeed.comm as dist\nfrom ..ragged import DSStateManager, RaggedBatchWrapper\nfrom ..ragged.manager_configs import KVCacheConfig\nfrom ..ragged import DSSequenceDescriptor\nfrom ..model_implementations.layer_container_base import LayerContainer\nfrom ..config_v2 import RaggedInferenceEngineConfig\nfrom .flat_model_helpers import ModelMetadata\n\ntry:\n    from functools import cached_property\nexcept ImportError:\n\n    def cached_property(func):\n        return property(func)\n\n\n\"\"\"\nThis abstract class defines the interfaces that a model implementation should implement\nin order to include anything that may be called by the engine. Most models should be able\nto inherit from `DSInferenceTransformerModelBase` to reduce implementation work so it is recommended\nto begin there.\n\"\"\"\n\"\"\"\nPlaceholder for typing the model config, which can vary based on model implementation/\n\"\"\"\nDSModelImplementationConfig = Type['DSModelImplementationConfig']\n\"\"\"\nPlaceholder for typing the distributed comm object.\n\nTODO(cmikeh2): Replace when we have a more defined API for the inference communication system.\n\"\"\"\nMPType = Type[\"MPType\"]\n\n\nclass DSInferenceModelBase(torch.nn.Module, ABC):\n    \"\"\"\n    Implementation of a model for inference composable with ragged batching.\n    \"\"\"\n\n    _config: DSModelImplementationConfig\n    \"\"\"\n    Model-specific configuration. No abstraction surrounds this yet.\n    \"\"\"\n\n    _engine_config: RaggedInferenceEngineConfig\n    \"\"\"\n    Engine configuration.\n    \"\"\"\n\n    _base_mp_group: MPType\n    \"\"\"\n    Base communication group for Tensor-parallel inference.\n    \"\"\"\n\n    _non_transformer: Optional[LayerContainer]\n    \"\"\"\n    Abstract container for storing both embedding (pre-transformer) and unembedding (post-transformer)\n    parameters. This attribute should be None at model instantiation until the Policy sets\n    the model parameters. These parameters are grouped together since many model implementations\n    will tie the embedding and unembedding parameters together.\n    \"\"\"\n\n    _transformer: Optional[Iterable[LayerContainer]]\n    \"\"\"\n    List of abstract containers (1 per layer) for storing transformer (transformer)\n    parameters. This attribute should be None at model instantiation until the Policy\n    sets the model parameters.\n    \"\"\"\n\n    state_manager: Optional[DSStateManager]\n    \"\"\"\n    Since the state manager is lazy initialized, by the engine, it is not guaranteed to be present\n    until full initialization.\n    \"\"\"\n\n    def __init__(self, config: DSModelImplementationConfig, engine_config: RaggedInferenceEngineConfig,\n                 base_mp_group: MPType) -> None:\n        \"\"\"\n        Minimal initialization of the model.\n\n        Arguments:\n            config (DSModelImplementationConfig): Model-specific configuration. No assumptions\n                should be made about this config that are not closely tied to the specific\n                model implementation.\n            engine_config (RaggedInferenceEngineConfig): Engine configuration.\n            base_mp_group (MPType): Base communication group for Tensor-parallel inference.\n        \"\"\"\n        super().__init__()\n        self._config = config\n        self._engine_config = engine_config\n        self._base_mp_group = base_mp_group\n\n        # Set to None until the Policy sets the model parameters\n        self._non_transformer = None\n        self._transformer = None\n        self._flattened_param_buffer = None\n        self._flattened_param_metadata = None\n\n    @property\n    def config(self) -> DSModelImplementationConfig:\n        \"\"\"\n        The model config.\n        \"\"\"\n        return self._config\n\n    def set_parameters(self, transformer: Iterable[LayerContainer], non_transformer: LayerContainer,\n                       flattened_param_buffer: torch.Tensor, flattened_param_metadata: ModelMetadata):\n        \"\"\"\n        Set the model parameters for the embedding, transformer, and unembedding containers.\n        \"\"\"\n        self._transformer = transformer\n        self._non_transformer = non_transformer\n        self._flattened_param_buffer = flattened_param_buffer\n        self._flattened_param_metadata = flattened_param_metadata\n\n    def set_state_manager(self, state_manager: DSStateManager):\n        \"\"\"\n        Sets the state manager attribute. This is called by the inference engine after\n        the model is fully initialized.\n        \"\"\"\n        self.state_manager = state_manager\n\n    @cached_property\n    def tp_rank(self) -> int:\n        \"\"\"\n        The rank of the current process.\n\n        # TODO(cmikeh2): Kind of a hack right now, but this is too verbose to use at\n        the frequency we need.\n        \"\"\"\n        return dist.get_rank(group=self._base_mp_group)\n\n    @cached_property\n    def tp_size(self) -> int:\n        \"\"\"\n        The total number of processes.\n\n        # TODO(cmikeh2): Kind of a hack right now, but this is too verbose to use at\n        the frequency we need.\n        \"\"\"\n        return dist.get_world_size(group=self._base_mp_group)\n\n    @property\n    def model_config(self):\n        \"\"\"\n        The model config.\n        \"\"\"\n        return self._config\n\n    @property\n    def engine_config(self):\n        \"\"\"\n        The engine config.\n        \"\"\"\n        return self._engine_config\n\n    @property\n    def flattened_params(self) -> Optional[torch.Tensor]:\n        \"\"\"\n        The flattened parameter buffer.\n        \"\"\"\n        return self._flattened_param_buffer\n\n    @property\n    def flattened_param_metadata(self) -> Optional[ModelMetadata]:\n        \"\"\"\n        The flattened parameter metadata.\n        \"\"\"\n        return self._flattened_param_metadata\n\n    @abstractmethod\n    def get_kv_requirements(self, sequence: DSSequenceDescriptor, max_new_tokens: int,\n                            max_new_blocks: Tuple[int, ...]) -> Tuple[int, torch.Tensor]:\n        \"\"\"\n        Given a sequence and the number of new tokens in the sequence, determine the\n        number of new KV blocks needed to support the sequence. This method is\n        used to help the engine provide schedulability APIs and can be used as a helper\n        for ``maybe_allocate_kv``.\n\n        Args:\n            sequence (DSSequenceDescriptor): The sequence for which to allocate KV-storage.\n            max_new_tokens (int): Maximum number of tokens to hypothetically schedule.\n            max_new_blocks (int): Maximum number of blocks to hypothetically allocate.\n\n        Returns:\n            Tuple[int, torch.Tensor]: The tuple of number of tokens scheduled and number\n                of blocks allocated (per KV cache). In general, only one of these numbers will\n                match the corresponding input argument, but this is not guaranteed.\n        \"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def get_remaining_block_capacity(self, sequence: DSSequenceDescriptor) -> int:\n        raise NotImplementedError()\n\n    @abstractmethod\n    def maybe_allocate_kv(self, sequence: DSSequenceDescriptor, n_new_tokens: int) -> None:\n        \"\"\"\n        Given a sequence and the number of new tokens in the sequence, determine\n        whether or not additional KV-storage is needed and allocate it if so.\n\n        Args:\n            sequence (DSSequenceDescriptor): The sequence for which to allocate KV-storage.\n            n_new_tokens (int): The number of new tokens in the sequence.\n        \"\"\"\n        raise NotImplementedError()\n\n    @abstractmethod\n    def kv_cache_config(self) -> Tuple[KVCacheConfig, ...]:\n        \"\"\"\n        Return the KV-cache configuration for this model. This should be a tuple of one or more\n        KVCacheConfig objects (one for each distinct cache group).\n        \"\"\"\n        raise NotImplementedError()\n\n    @property\n    @abstractmethod\n    def max_sequence_length(self) -> int:\n        \"\"\"\n        The maximum sequence length supported by the model.\n        \"\"\"\n        ...\n\n    def maybe_free_kv(self, sequence: DSSequenceDescriptor) -> None:\n        \"\"\"\n        After completing a forward pass, determine whether or not the there are any KV blocks\n        that maybe freed since they are no longer in use.\n\n        Consider the following example:\n\n        We have a block size of 4 and a local window size of 8. At the beginning of the forward\n        pass there 10 tokens had been seen and the new forward has a size of 4. This would lend\n        itself to the following cache structure prior to the forward:\n            [[0, 1, 2*, 3*] [4*, 5*, 6*, 7*] [8*, 9*, x, x] [x x x x]]\n        Where x's denote empty cache locations and * denote values that are needed for attention\n        of the next open slot. After the forward, the cache would look like the following:\n            [[0, 1, 2, 3] [4, 5, 6*, 7*] [8*, 9*, 10*, 11*] [12* 13* x x]]\n        In this case, the first block is no longer needed since it is not needed for any future\n        local attention windows. This function would be responsible for freeing that block.\n\n        Default behavior assumes no local patterns that require freeing and in general should\n        be sufficient.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def prepare_batch(self, wrapped_batch: RaggedBatchWrapper) -> None:\n        \"\"\"\n        This will be called before each forward with the intent of building forward-specific metadata\n        about a batch. The intent here is to build data structures like attention atoms without necessarily\n        needing to implement graphable kernels to do so.\n\n        Abstract so as to force model implementations to opt out of doing anything here explicitly.\n        \"\"\"\n        raise NotImplementedError()\n\n    def forward(wrapped_batch: RaggedBatchWrapper) -> torch.Tensor:\n        \"\"\"\n        Complete a forward pass of the model. This interface should be graphable, so it\n        should not rely on the ability to use python control flow.\n        \"\"\"\n        raise NotImplementedError()\n", "deepspeed/inference/v2/model_implementations/flat_model_helpers.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Dict, Iterable, Tuple, Optional\nfrom os import path\n\nimport torch\n\nfrom deepspeed.accelerator import get_accelerator\nfrom deepspeed.ops.op_builder import RaggedUtilsBuilder\nfrom deepspeed.runtime.config_utils import DeepSpeedConfigModel\nfrom .layer_container_base import LayerContainer\nfrom ..inference_parameter import InferenceParameter, STR_TO_DTYPE\nfrom ..inference_utils import elem_size\n\n\ndef pad_to_aligned_offset(offset: int, alignment: int = 256) -> int:\n    \"\"\"\n    Pad the provided offset to a well-aligned value.\n    \"\"\"\n    return ((offset + alignment - 1) // alignment) * alignment\n\n\nclass TensorMetadata(DeepSpeedConfigModel):\n    \"\"\"\n    A class to represent a tensor specification.\n    \"\"\"\n    dtype: Optional[str]\n    shape: Optional[Tuple[int, ...]]\n    strides: Optional[Tuple[int, ...]]\n    offset: int\n\n\nclass ParameterMetadata(DeepSpeedConfigModel):\n    \"\"\"\n    A class to represent a parameter specification.\n    \"\"\"\n    core_param: TensorMetadata = None\n    aux_params: Dict[str, TensorMetadata] = {}\n\n\nclass LayerMetadata(DeepSpeedConfigModel):\n    \"\"\"\n    A class to represent a layer specification.\n    \"\"\"\n    params: Dict[str, ParameterMetadata] = {}\n\n\nclass ModelMetadata(DeepSpeedConfigModel):\n    \"\"\"\n    A class to represent a model specification.\n    \"\"\"\n    policy: str = \"\"\n    layers: Dict[str, LayerMetadata] = {}\n\n\ndef make_param_filename(base: str, rank: int, n_ranks: int) -> str:\n    \"\"\"\n    Make a filename for a parameter file.\n\n    Arguments:\n        rank: Rank of the file.\n        n_ranks: Total number of ranks.\n\n    Returns:\n        str: Filename.\n    \"\"\"\n    return path.join(base, f\"params_rank_{rank}_of_{n_ranks}.pt\")\n\n\ndef make_metadata_filename(base: str, rank: int, n_ranks: int) -> str:\n    \"\"\"\n    Make a filename for a metadata file.\n\n    Arguments:\n        rank: Rank of the file.\n        n_ranks: Total number of ranks.\n\n    Returns:\n        str: Filename.\n    \"\"\"\n    return path.join(base, f\"metadata_rank_{rank}_of_{n_ranks}.json\")\n\n\ndef make_model_config_filename(base: str) -> str:\n    \"\"\"\n    Make a filename for a model config file.\n\n    Arguments:\n        base: Base directory.\n\n    Returns:\n        str: Filename.\n    \"\"\"\n    return path.join(base, \"ds_model_config.json\")\n\n\ndef flatten_inference_model(\n    transformer_containers: Iterable[LayerContainer],\n    non_transformer_container: LayerContainer,\n    policy_name: str,\n) -> Tuple[torch.Tensor, ModelMetadata]:\n    \"\"\"\n    Flatten the underlying parameters into\n\n    Arguments:\n        transformer_containers: Iterable of layer containers corresponding to the transformer\n            parameters.\n        non_transformer_container: Layer container corresponding to the non-transformer parameters.\n        policy_name: The name of the policy class (typically accessed with `type(policy).__name__`).\n\n    Returns:\n        Iterable[Any]: Flattened list of parameters.\n    \"\"\"\n    alloc_fn = RaggedUtilsBuilder().load().allocate_view_on\n\n    total_size = 0\n    metadata = ModelMetadata(policy=policy_name)\n\n    def process_layer(layer_container: LayerContainer, l_name: str, cur_offset: int) -> int:\n        \"\"\"\n        Iterate over the parameters of a single container and collect metadata for the final\n        flattened buffer.\n\n        Arguments:\n            layer_container: The layer container to process.\n            l_name: The name of the layer container to key the metadata.\n            cur_offset: The current offset into the flattened buffer.\n\n        Captured Variables:\n            metadata: The metadata object to populate.\n\n        Returns:\n            int: The updated offset into the flattened buffer.\n        \"\"\"\n        try:\n            _ = layer_container.is_populated\n        except ValueError as e:\n            raise ValueError(f\"Layer container {l_name} is not populated.\") from e\n\n        layer_metadata = LayerMetadata()\n\n        for p_name in layer_container.annotation_attrs:\n            param = getattr(layer_container, p_name)\n            param_metadata = ParameterMetadata()\n\n            if param is None:\n                param_metadata.core_param = TensorMetadata(offset=-1)\n                layer_metadata.params[p_name] = param_metadata\n                continue\n\n            param_metadata.core_param = TensorMetadata(dtype=str(param.dtype),\n                                                       shape=param.shape,\n                                                       strides=param.stride(),\n                                                       offset=cur_offset)\n\n            cur_offset += pad_to_aligned_offset(elem_size(param.dtype) * param.numel())\n\n            for t_name, tensor in param.aux_attrs.items():\n                param_metadata.aux_params[t_name] = TensorMetadata(dtype=str(tensor.dtype),\n                                                                   shape=tensor.shape,\n                                                                   strides=tensor.stride(),\n                                                                   offset=cur_offset)\n\n                cur_offset += pad_to_aligned_offset(elem_size(tensor.dtype) * tensor.numel())\n\n            layer_metadata.params[p_name] = param_metadata\n\n        metadata.layers[l_name] = layer_metadata\n        return cur_offset\n\n    for i, layer in enumerate(transformer_containers):\n        l_name = f\"transformer_layer_{i}\"\n        total_size = process_layer(layer, l_name, total_size)\n\n    l_name = \"non_transformer\"\n    total_size = process_layer(non_transformer_container, l_name, total_size)\n\n    buffer = torch.empty(total_size, dtype=torch.uint8, device=get_accelerator().current_device())\n\n    def copy_layer(layer_container: LayerContainer, l_name: str) -> None:\n        \"\"\"\n        Local method for copying from the layer container to the flattened buffer.\n\n        Arguments:\n            layer_container: The layer container to copy from.\n            l_name: The name of the layer container to key the metadata.\n\n        Captured Variables:\n            buffer: The flattened buffer to copy into.\n            metadata: The metadata object to populate.\n        \"\"\"\n        l_metadata = metadata.layers[l_name]\n        for p_name in layer_container.annotation_attrs:\n            p_metadata = l_metadata.params[p_name]\n            param = getattr(layer_container, p_name)\n\n            if param is None:\n                continue\n\n            core_param = alloc_fn(param, buffer, p_metadata.core_param.offset)\n            core_param.copy_(param)\n\n            aux_params = {}\n\n            for t_name, tensor in param.aux_attrs.items():\n                t_view = alloc_fn(tensor, buffer, p_metadata.aux_params[t_name].offset)\n                aux_params[t_name] = t_view\n                t_view.copy_(tensor)\n\n            setattr(layer_container, p_name, InferenceParameter.initialize(core_param, **aux_params))\n\n    for i, layer in enumerate(transformer_containers):\n        l_name = f\"transformer_layer_{i}\"\n        copy_layer(layer, l_name)\n\n    l_name = \"non_transformer\"\n    copy_layer(non_transformer_container, l_name)\n\n    return buffer, metadata\n\n\ndef restore_inference_model(buffer: torch.Tensor, metadata: ModelMetadata,\n                            transformer_containers: Iterable[LayerContainer],\n                            non_transformer_container: LayerContainer) -> None:\n    \"\"\"\n    Restore the model from the buffer and metadata.\n\n    Arguments:\n        buffer: Buffer containing the model parameters.\n        metadata: Metadata for the model.\n        transformer_containers: Iterable of transformer layer containers.\n        non_transformer_container: Non-transformer layer container.\n    \"\"\"\n    alloc_fn = RaggedUtilsBuilder().load().allocate_view_like\n\n    def restore_layer(layer_container: LayerContainer, l_name: str) -> None:\n        \"\"\"\n        Local method for restoring a layer container from a flattened buffer. This\n        only constructs views for the parameters onto the buffer. No data movement\n        is performed.\n\n        Arguments:\n            layer_container: The layer container to restore.\n            l_name: The name of the layer container to key the metadata.\n\n        Captured Variables:\n            buffer: The flattened buffer to reconstruct views on top of.\n            metadata: The metadata object describing the each parameter in the model.\n        \"\"\"\n        l_metadata = metadata.layers[l_name]\n\n        for p_name in layer_container.annotation_attrs:\n            p_metadata = l_metadata.params[p_name]\n\n            if p_metadata.core_param.offset == -1:\n                layer_container.direct_injection(p_name, None)\n                continue\n\n            dummy_tensor = torch.empty([], dtype=STR_TO_DTYPE[p_metadata.core_param.dtype])\n            core_param = alloc_fn(p_metadata.core_param.shape, p_metadata.core_param.strides, dummy_tensor, buffer,\n                                  p_metadata.core_param.offset)\n\n            aux_params = {}\n\n            for t_name, t_metadata in p_metadata.aux_params.items():\n                dummy_tensor = torch.empty([], dtype=STR_TO_DTYPE[t_metadata.dtype])\n                t_view = alloc_fn(t_metadata.shape, t_metadata.strides, dummy_tensor, buffer, t_metadata.offset)\n\n                aux_params[t_name] = t_view\n\n            restored_param = InferenceParameter.initialize(core_param, **aux_params)\n            layer_container.direct_injection(p_name, restored_param)\n\n    for i, layer in enumerate(transformer_containers):\n        l_name = f\"transformer_layer_{i}\"\n        restore_layer(layer, l_name)\n\n    l_name = \"non_transformer\"\n    restore_layer(non_transformer_container, l_name)\n", "deepspeed/inference/v2/model_implementations/layer_container_base.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport re\nfrom typing import Type\n\nimport torch\n\nfrom deepspeed.accelerator import get_accelerator\nfrom .parameter_base import ParameterBase, ParametrizedList\nfrom ..inference_parameter import InferenceParameter\n\n# Currently have dependency loops for the type hints.\nInferenceModel = Type[\"InferenceModel\"]\nLayerContainer = Type[\"LayerContainer\"]\n\nMAPPING_KEY = \"PARAM_MAPPING\"\nPLIST_HELPERS = \"_ds_plist_strip_vals\"\n\n\ndef make_finalization_callback(all_names: str):\n    \"\"\"\n    Helper method for building the finalization callback for a LayerContainer. This\n    is not client code and should not be used or called directly.\n    \"\"\"\n\n    def finalization_callback(self, param: ParameterBase, finalized_param: torch.Tensor) -> None:\n        \"\"\"\n        Callback for when a parameter is finalized.\n        \"\"\"\n        self._finalized_params += 1\n\n        for name in all_names:\n            if getattr(self, name) is param:\n                setattr(self, name, finalized_param)\n\n    return finalization_callback\n\n\nclass LayerMetaclass(type):\n    \"\"\"\n    MetaClass for the LayerContainer base class. This class will parse the annotations\n    of the class that correspond to `ParameterBase` and create None initializers for each\n    as well as a finalization callback that for when each `ParameterBase` is finalized\n    and should be replaced with a Tensor.\n    \"\"\"\n\n    def __new__(cls, clsname, bases, attrs):\n\n        annotations = attrs.get(\"__annotations__\", {})\n\n        for base in bases:\n            # We'll pick up all annotations on any base classes. This will allow us to\n            # to use inheritance to share common parameter groups in base classes.\n            if hasattr(base, \"__annotations__\"):\n                annotations.update(base.__annotations__)\n\n            if hasattr(base, MAPPING_KEY):\n                if MAPPING_KEY not in attrs:\n                    # This is likely a fail state. If a parent has MAPPING KEY but the child does\n                    # not, then we're guaranteed only a subset of the parameters will be mapped.\n                    attrs[MAPPING_KEY] = {}\n                attrs[MAPPING_KEY].update(getattr(base, MAPPING_KEY))\n\n        all_names = [name for name, annotation in annotations.items() if issubclass(annotation, ParameterBase)]\n\n        if MAPPING_KEY in attrs:\n            # If we have a mapping key at all, then we will enter the validation mode for building\n            # helpers for mapping and ensuring we have complete mapping.\n\n            # First we'll build a flat list of every dependency for this layer.\n            all_deps = set()\n            for name in all_names:\n                parameter_deps = [\n                    name for name, annotation in annotations[name].__annotations__.items()\n                    if issubclass(annotation, (torch.Tensor, ParametrizedList))\n                ]\n\n                all_deps.update([f\"{name}.{dep}\" for dep in parameter_deps])\n\n            # Create static helper for doing the string processing only once.\n            attrs[PLIST_HELPERS] = []\n\n            # Iterate over all the mappings\n            for src_name, target_or_targets in attrs[MAPPING_KEY].items():\n                if isinstance(target_or_targets, str):\n                    target_or_targets = [target_or_targets]\n\n                actual_targets = []\n                for target_name in target_or_targets:\n                    base_dependency, dependency_attr = target_name.split(\".\")\n\n                    # Check for invalid mappings\n                    if base_dependency not in all_names:\n                        raise ValueError(\n                            \"Target parameter \\\"{}\\\" not found in this layer. Valid targets are {}\".format(\n                                base_dependency, all_names))\n                    if dependency_attr not in annotations[base_dependency].__annotations__:\n                        # This check is not universal (see below) if a single dependency is being\n                        # mapped to by a single row.\n                        raise ValueError(\n                            \"Target dependency \\\"{}\\\" not found on parameter \\\"{}\\\". Valid targets are {}\".format(\n                                dependency_attr, base_dependency, annotations[base_dependency].__annotations__.keys()))\n                    if target_name not in all_deps:\n                        raise ValueError(\n                            \"Target dependency \\\"{}\\\" was targeted with multiple mapping rules.\".format(target_name))\n\n                    # If we've made it this far, the dependency definitely exists.\n                    actual_targets.append(annotations[base_dependency].__annotations__[dependency_attr])\n\n                    all_deps.remove(target_name)\n\n                are_plists = [issubclass(target, ParametrizedList) for target in actual_targets]\n                if all(are_plists):\n                    # We can do direct sets on everything but ParametrizedLists, so we'll only explicitly\n                    # handle these here.\n                    # TODO(cmikeh2): SPLIT, error if more than 1\n                    glob_count = src_name.count(\"*\")\n                    if glob_count > 1:\n                        raise ValueError(\n                            \"ParametrizedList index inference can only work with a single glob: {}\".format(src_name))\n                    elif glob_count == 0:\n                        raise ValueError(\n                            \"Must have wildcard (*) in source name for ParametrizedList mapping: {}\".format(src_name))\n\n                    wildcard_idx = src_name.find(\"*\")\n                    prefix = src_name[:wildcard_idx]\n                    suffix = src_name[wildcard_idx + 1:]\n                    attrs[PLIST_HELPERS].append((prefix, suffix, target_or_targets))\n                elif any(are_plists):\n                    raise ValueError(\"Cannot mix ParametrizedLists and Tensors in a single mapping rule.\")\n\n            if len(all_deps) > 0:\n                raise ValueError(\n                    \"A parameter mapping was provided for {}, but the following dependencies were not mapped: {}\".\n                    format(clsname, all_deps))\n\n        attrs[\"finalization_callback\"] = make_finalization_callback(all_names)\n\n        new_obj = super().__new__(cls, clsname, bases, attrs)\n\n        setattr(new_obj, \"_n_params\", len(all_names))\n        setattr(new_obj, \"_annotation_attrs\", all_names)\n\n        return new_obj\n\n    def __call__(cls, *args, **kwargs):\n        instance = cls.__new__(cls, *args, **kwargs)\n        instance.__init__(*args, **kwargs)\n\n        for name, annotation in instance.__annotations__.items():\n            if issubclass(annotation, ParameterBase):\n                # TODO(cmikeh2): Do we want to make this a property\n                # It might also make sense to do this in the base class __init__\n                # but since it is tied with the changes made in __new__ it feels\n                # to me like it should be here.\n                setattr(instance, name, annotation(instance.inference_model, instance))\n\n        return instance\n\n\nclass LayerContainer(metaclass=LayerMetaclass):\n    \"\"\"\n    Abstract base class for containing model parameters.\n\n    This is primarily a guidance abstraction since we do not put any restrictions\n    on how the parameters are stored.\n\n    To use this class, annotate the class with `ParameterBase` subclasses and give them\n    names. As a checkpoint is loaded into this container, the `ParameterBase` instances\n    will be replaced with realized Tensors as soon as each of their dependencies are met.\n\n    To enable automatic mapping, add a static attribute `PARAM_MAPPING` to the class\n    definition. This should be a dictionary mapping from a source string to one or\n    more dependencies.\n\n    ```python\n    class MyLayer(LayerContainer):\n        PARAM_MAPPING = {\n            \"path.to.param.dependency\", \"container_param_1.dependency\",\n            \"path.to.param2.dependency\", \"container_param_2.dependency\",\n            \"path.to.param3.*.dependency\", \"container_param_3.list_dependency\"\n        }\n\n        ...\n    ```\n    \"\"\"\n\n    def __init__(self, model: InferenceModel) -> None:\n        \"\"\"\n        Initialization of the LayerContainer. This method does not need to be overridden\n        for any children classes.\n\n        Args:\n            model (InferenceModel): Inference model that will be used to shard and transform\n                parameters correctly, as well as provide specific information about the model\n                for `ParameterizedList`s that may be part of one of the member `ParameterBase`s.\n        \"\"\"\n        self.inference_model = model\n        self._finalized_params = 0\n\n    def _initialization_checker(self, check_device: bool = True) -> bool:\n        \"\"\"\n        Returns whether or not all parameters have been initialized and transformed by\n        the model. Once this returns True, all the `ParameterBase` instances will be\n        torch.Tensors.\n        \"\"\"\n        if self._finalized_params != self.n_params:\n            return False\n\n        for name in self._annotation_attrs:\n            tensor = getattr(self, name)\n            if tensor is None:\n                continue\n            elif not isinstance(tensor, InferenceParameter):\n                raise ValueError(\"Layer should be finalized, but {} ({}) is neither InferenceParameter or None\".format(\n                    name, type(tensor)))\n            elif check_device and tensor.device != torch.device(get_accelerator().current_device()):\n                raise RuntimeError(\"Layer should be finalized, but {} is not on device {}\".format(\n                    name,\n                    get_accelerator().current_device()))\n        return True\n\n    @property\n    def is_populated(self) -> bool:\n        \"\"\"\n        Returns whether or not all parameters have been populated by the checkpoint engine, but\n        does not validat the parameters are on the correct device.\n        \"\"\"\n        return self._initialization_checker(check_device=False)\n\n    @property\n    def is_initialized(self) -> bool:\n        \"\"\"\n        Returns whether or not all parameters have been initialized and transformed by\n        the model and are located on the appropriate device. Once this returns True, all\n        the `ParameterBase` instances ``InferenceParameter``s or explicitly set to ``None``.\n        \"\"\"\n        return self._initialization_checker()\n\n    @property\n    def n_params(self) -> int:\n        \"\"\"\n        The number of parameters this container holds. This is a read-only value\n        that is set by the metaclass.\n        \"\"\"\n        return self._n_params\n\n    @property\n    def annotation_attrs(self) -> list:\n        return self._annotation_attrs\n\n    @property\n    def mapping_params(self) -> dict:\n        return getattr(self.__class__, MAPPING_KEY, {})\n\n    @property\n    def plist_helpers(self) -> list:\n        return getattr(self.__class__, PLIST_HELPERS, [])\n\n    def direct_injection(self, name: str, tensor: InferenceParameter) -> None:\n\n        if name not in self._annotation_attrs:\n            raise ValueError(f\"Cannot directly inject {name}, not a valid parameter.\")\n\n        setattr(self, name, tensor)\n        self._finalized_params += 1\n\n    def set_dependency(self, dep_name: str, dep_value: torch.Tensor) -> None:\n        \"\"\"\n        Set dependency can be used for managing dependencies when a mapping is provided\n        in the class definition for the layer. The dep_name here should have any prefix\n        for transformer layers removed (such as model.layers.*.attn.qkv.weight -> attn.qkv.weight).\n\n        Args:\n            dep_name (str): The name of the dependency to set.\n            dep_value (torch.Tensor): The value to set the dependency to.\n        \"\"\"\n\n        def get_dep_name_target(dep_name: str) -> str:\n            \"\"\"\n            Helper method for getting the target name for a dependency from the\n            mapping params. Tries to match exact string first, then looks for\n            wildcards and attempts regex matching. Will return empty string if\n            no match found.\n            \"\"\"\n            if dep_name in self.mapping_params:\n                # If we have an exact match, it's a direct mapping and we can\n                # immediately set the value.\n                return self.mapping_params[dep_name]\n\n            matched_targets = []\n            for key, target in self.mapping_params.items():\n                regex_key = key.replace(\"*\", \".*\")\n                if re.match(regex_key, dep_name):\n                    matched_targets.append(target)\n            if len(matched_targets) > 1:\n                raise ValueError(f\"Multiple targets matched for dependency {dep_name}: {matched_targets}\")\n            if matched_targets:\n                return matched_targets[0]\n            return \"\"\n\n        if dep_name in self.mapping_params:\n            # If we have an exact match, it's a direct mapping and we can immediately set\n            # the value.\n            target = self.mapping_params[dep_name]\n\n            # Convert single targets to a list for consistency\n            if isinstance(target, str):\n                target = [target]\n\n            for target_name in target:\n                # Double setting doesn't set the attribute correctly, so we do a getattr then setattr\n                target_param_name, target_dependency_name = target_name.split(\".\")\n                target_param = getattr(self, target_param_name)\n                setattr(target_param, target_dependency_name, dep_value)\n            return\n\n        # Otherwise we need to map to one of the parameter lists.\n        for prefix, suffix, dests in self.plist_helpers:\n            if dep_name.startswith(prefix) and dep_name.endswith(suffix):\n                # We have a match, so we can set the value.\n                target_idx = int(dep_name[len(prefix):-len(suffix)])\n\n                # Convert single targets to a list for consistency\n                if isinstance(dests, str):\n                    dests = [dests]\n\n                for dest in dests:\n                    target_param_name, target_dependency_name = dest.split(\".\")\n                    target_param = getattr(self, target_param_name)\n                    target_dependency = getattr(target_param, target_dependency_name)\n                    target_dependency[target_idx] = dep_value\n                return\n\n        # TODO: Refactor this with the help of cmikeh2\n        # We should be able to combine this with the wildcard matching above.\n        target = get_dep_name_target(dep_name)\n        if target:\n            # Convert single targets to a list for consistency\n            if isinstance(target, str):\n                target = [target]\n\n            for target_name in target:\n                # Double setting doesn't set the attribute correctly, so we do a getattr then setattr\n                target_param_name, target_dependency_name = target_name.split(\".\")\n                target_param = getattr(self, target_param_name)\n                setattr(target_param, target_dependency_name, dep_value)\n            return\n\n        raise ValueError(\n            \"Could not find a mapping for dependency \\\"{}\\\". Check that it is included in the ``MAPPING_PARAMS``. See docstring for more on ``MAPPING_PARAMS``\"\n            .format(dep_name))\n", "deepspeed/inference/v2/model_implementations/inference_transformer_base.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom abc import abstractmethod\nfrom typing import Optional\n\nimport torch\n\nfrom deepspeed.accelerator import get_accelerator\nfrom ..config_v2 import RaggedInferenceEngineConfig\nfrom ..inference_utils import ActivationType, ceil_div, is_gated\nfrom ..model_implementations import *\nfrom ..model_implementations.sharding import *\nfrom ..modules.configs import (\n    DSEmbeddingsConfig,\n    DSLinearConfig,\n    DSMoEConfig,\n    DSNormConfig,\n    DSSelfAttentionConfig,\n    DSUnembedConfig,\n    NormTypeEnum,\n    PositionalEmbeddingType,\n    RotateHalfConfig,\n)\nfrom ..modules import heuristics\nfrom ..ragged import (\n    DSSequenceDescriptor,\n    KVCacheConfig,\n    RaggedBatchWrapper,\n)\nfrom .inference_model_base import (\n    DSInferenceModelBase,\n    DSModelImplementationConfig,\n    MPType,\n)\nfrom ..inference_parameter import InferenceParameter\n\ntry:\n    from functools import cached_property\nexcept ImportError:\n\n    def cached_property(func):\n        return property(func)\n\n\nclass DSTransformerModelBase(DSInferenceModelBase):\n    \"\"\"\n    Dimensioning properties\n    \"\"\"\n\n    @property\n    @abstractmethod\n    def num_layers(self) -> int:\n        \"\"\"\n        Number of the layers in the model\n        \"\"\"\n        ...\n\n    @property\n    @abstractmethod\n    def model_dim(self) -> int:\n        \"\"\"\n        Size of embedding projection and residuals.\n        \"\"\"\n        ...\n\n    @property\n    @abstractmethod\n    def vocab_size(self) -> int:\n        \"\"\"\n        Size of the vocabulary (including padding).\n        \"\"\"\n        ...\n\n    @property\n    @abstractmethod\n    def head_size(self) -> int:\n        \"\"\"\n        Size of each attention head.\n        \"\"\"\n        ...\n\n    @property\n    @abstractmethod\n    def n_heads(self) -> int:\n        \"\"\"\n        The number of query heads on the model. This should not take into account\n        any dimension reductions from model sharding.\n        \"\"\"\n        ...\n\n    @property\n    def n_heads_q(self) -> int:\n        \"\"\"\n        Alias to n_heads.\n        \"\"\"\n        return self.n_heads\n\n    @property\n    def n_heads_kv(self) -> int:\n        \"\"\"\n        The number of key and value heads on the model. For GQA or MQA, overload this attribute.\n        Otherwise it adopts MHA formulations and uses n_heads. This should not take into account\n        any dimension reductions from model sharding.\n        \"\"\"\n        return self.n_heads\n\n    @property\n    @abstractmethod\n    def intermediate_dim(self) -> int:\n        \"\"\"\n        The size of the (unsharded) intermediate projection dim. For a gated activation function\n        this is the size of the input to the second MLP layer. This should not take into account\n        any dimension reductions from model sharding.\n        \"\"\"\n        ...\n\n    @property\n    @abstractmethod\n    def positional_embedding_type(self) -> PositionalEmbeddingType:\n        \"\"\"\n        The type of positional embedding used by the model.\n        \"\"\"\n        ...\n\n    \"\"\"\n    Architectural properties\n    \"\"\"\n\n    @property\n    @abstractmethod\n    def activation_dtype(self) -> torch.dtype:\n        \"\"\"\n        The activation dtype of the model.\n        \"\"\"\n        ...\n\n    @property\n    @abstractmethod\n    def mlp_activation_fn(self) -> ActivationType:\n        \"\"\"\n        The activation function used in the MLP.\n        \"\"\"\n        ...\n\n    @property\n    @abstractmethod\n    def norm_type(self) -> NormTypeEnum:\n        \"\"\"\n        The type of normalization used in the model.\n        \"\"\"\n        ...\n\n    @property\n    @abstractmethod\n    def positional_embedding_config(self) -> Optional[RotateHalfConfig]:\n        \"\"\"\n        The positional embedding configuration for the model.\n        \"\"\"\n        ...\n\n    \"\"\"\n    Derived helpers\n    \"\"\"\n\n    @cached_property\n    def n_heads_q_local(self) -> int:\n        \"\"\"\n        Number of local heads post sharding.\n        \"\"\"\n        return get_local_heads(self.tp_rank, self.tp_size, self.n_heads_q, self.n_heads_kv)[0]\n\n    @cached_property\n    def n_heads_kv_local(self) -> int:\n        \"\"\"\n        Number of local heads post sharding.\n        \"\"\"\n        return get_local_heads(self.tp_rank, self.tp_size, self.n_heads_q, self.n_heads_kv)[1]\n\n    @property\n    def gated_mlp(self) -> bool:\n        \"\"\"\n        Return a boolean to determine whether the model uses a gated activation function.\n        \"\"\"\n        return is_gated(self.mlp_activation_fn)\n\n    \"\"\"\n    Method implementations\n    \"\"\"\n\n    def __init__(self, config: DSModelImplementationConfig, engine_config: RaggedInferenceEngineConfig,\n                 base_mp_group: MPType) -> None:\n        \"\"\"\n        Base implementation for initialization. By default, this will initialize\n        the traditional components of a transformer model:\n            - Embedding\n            - QKV projection\n            - Self attention\n            - Attention output projection\n            - Feed forward network\n            - Normalization\n            - Unembedding\n\n        Arguments:\n            config (DSModelImplementationConfig): Model-specific configuration. No assumptions\n                should be made about this config that are not closely tied to the specific\n                model implementation.\n            engine_config (RaggedInferenceEngineConfig): Engine configuration.\n            base_mp_group (MPType): Base communication group for Tensor-parallel inference.\n        \"\"\"\n        super().__init__(config, engine_config, base_mp_group)\n\n        self.make_norm_layer()\n        self.make_qkv_layer()\n        self.make_attn_layer()\n        self.make_attn_out_layer()\n        self.make_mlp_1_layer()\n        self.make_mlp_2_layer()\n        self.make_embedding_layer()\n        self.make_unembedding_layer()\n        self._kv_cache_config = None\n\n    ######### Embedding #########\n    def make_embedding_layer(self) -> None:\n        \"\"\"\n        Performs setup and creates embedding DSModule. This will set the `self.embed` attribute.\n        \"\"\"\n\n        embed_config = DSEmbeddingsConfig(\n            max_tokens=self._engine_config.state_manager.max_ragged_batch_size,\n            residual_dtype=self.activation_dtype,\n            embedding_dim=self.model_dim,\n        )\n\n        self.embed = heuristics.instantiate_embed(embed_config, self._engine_config)\n\n    def transform_embedding_param(self, param: torch.Tensor) -> InferenceParameter:\n        \"\"\"\n        Performs embedding sharding along the channels dimension.\n        \"\"\"\n        # Until we can do non-contiguous all-gather, we won't shard the embedding parameters.\n        param = param.to(self.activation_dtype.value)\n        return InferenceParameter.initialize(param)\n\n    ######### Unembedding #########\n    def make_unembedding_layer(self) -> None:\n        \"\"\"\n        Performs setup and creates an unembedding layer. This implementation assumes\n        normalization prior to the LM head projection. If this does not match the model's\n        implementation, override this method. This will set the ``self.unembed`` attribute.\n        \"\"\"\n        unembed_dim = sharded_unembed_dim(self.vocab_size, self.tp_rank, self.tp_size)\n\n        unembed_config = DSUnembedConfig(\n            max_tokens=self._engine_config.state_manager.max_ragged_batch_size,\n            max_sequences=self._engine_config.state_manager.max_ragged_sequence_count,\n            dtype=self.activation_dtype,\n            model_dim=self.model_dim,\n            vocab_size=unembed_dim,\n            norm_type=self.norm_type,\n        )\n\n        self.unembed = heuristics.instantiate_unembed(unembed_config, self._engine_config)\n\n        if self.tp_size > 1:\n            self._comm_logits = torch.empty(self.tp_size,\n                                            self._engine_config.state_manager.max_ragged_sequence_count,\n                                            unembed_dim,\n                                            device=get_accelerator().current_device(),\n                                            dtype=self.activation_dtype.value)\n            self._return_logits = torch.empty(self._engine_config.state_manager.max_ragged_sequence_count,\n                                              self.vocab_size,\n                                              device=get_accelerator().current_device(),\n                                              dtype=self.activation_dtype.value)\n\n    def transform_unembed_param(self, param: torch.Tensor) -> InferenceParameter:\n        \"\"\"\n        Performs sharding along the vocab dimension.\n        \"\"\"\n        param = shard_unembed_param(param, self.tp_rank, self.tp_size).to(self.activation_dtype.value)\n        return InferenceParameter.initialize(param)\n\n    ######### QKV #########\n    def make_qkv_layer(self) -> None:\n        \"\"\"\n        Instantiates the linear projection layer for the QKV linear layer. This sets the\n        `self.qkv` attribute.\n        \"\"\"\n        out_features = qkv_out_features(self.model_dim, self.tp_rank, self.tp_size, self.head_size, self.n_heads_q,\n                                        self.n_heads_kv)\n\n        linear_config = DSLinearConfig(\n            max_tokens=self._engine_config.state_manager.max_ragged_batch_size,\n            in_channels=self.model_dim,\n            out_channels=out_features,\n            input_dtype=self.activation_dtype,\n            output_dtype=self.activation_dtype,\n        )\n\n        self.qkv = heuristics.instantiate_linear(linear_config, self._engine_config)\n\n    def transform_qkv_param(self, param: torch.Tensor) -> InferenceParameter:\n        \"\"\"\n        Passes a QKV parameter to the underlying implementation for any necessary\n        transformations.\n\n        Args:\n            param (torch.Tensor): The parameter to transform. This may be either a bias or weight and should have\n                the shape (out_neurons, in_neurons)\n        \"\"\"\n        param = shard_qkv_param(param, self.tp_rank, self.tp_size, self.head_size, self.n_heads_q, self.n_heads_kv)\n        return self.qkv.transform_param(param)\n\n    ######### Attention #########\n    def make_attn_layer(self) -> None:\n        \"\"\"\n        Builds the attention layer for the model. This sets the `self.attn` attribute.\n        \"\"\"\n        softmax_scale = 1.0 / (self.head_size**0.5)\n\n        attn_config = DSSelfAttentionConfig(max_tokens=self._engine_config.state_manager.max_ragged_batch_size,\n                                            n_heads_q=self.n_heads_q_local,\n                                            n_heads_kv=self.n_heads_kv_local,\n                                            head_size=self.head_size,\n                                            max_sequences=self._engine_config.state_manager.max_ragged_sequence_count,\n                                            scale_factor=softmax_scale,\n                                            input_dtype=self.activation_dtype,\n                                            output_dtype=self.activation_dtype,\n                                            positional_embedding_type=self.positional_embedding_type,\n                                            positional_embedding_config=self.positional_embedding_config)\n\n        self.attn = heuristics.instantiate_attention(attn_config, self._engine_config)\n\n    def get_kv_requirements(self, sequence: DSSequenceDescriptor, max_new_tokens: int,\n                            max_new_blocks: int) -> Tuple[int, int]:\n        \"\"\"\n        See ``DSInferenceModelBase.get_kv_requirements`` for documentation.\n\n        This method assumes an autoregressive dense attention pattern. Override this method\n        if this does not match the model's attention pattern.\n        \"\"\"\n        total_tokens = sequence.seen_tokens + max_new_tokens\n        req_blocks = ceil_div(total_tokens, self.attn.kv_block_size)\n        block_lim = req_blocks - sequence.cur_allocated_blocks\n\n        if block_lim <= max_new_blocks:\n            return max_new_tokens, block_lim\n\n        token_capacity = (max_new_blocks +\n                          sequence.cur_allocated_blocks) * self.attn.kv_block_size - sequence.seen_tokens\n\n        return token_capacity, max_new_blocks\n\n    def get_remaining_block_capacity(self, sequence: DSSequenceDescriptor) -> int:\n        return sequence.seen_tokens % self.attn.kv_block_size\n\n    def maybe_allocate_kv(self, sequence: DSSequenceDescriptor, n_new_tokens: int) -> None:\n        \"\"\"\n        See ``DSInferenceModelBase.maybe_allocate_kv`` for documentation.\n\n        This method assumes an autoregressive dense attention pattern. Override this method\n        if this does not match the model's attention pattern.\n        \"\"\"\n        free_block = self.state_manager.free_blocks[0]\n        _, n_needed_blocks = self.get_kv_requirements(sequence, n_new_tokens, free_block)\n\n        if n_needed_blocks > 0:\n            new_blocks = self.state_manager.allocate_blocks(n_needed_blocks)\n            sequence.extend_kv_cache(new_blocks)\n\n    def kv_cache_config(self) -> Tuple[KVCacheConfig, ...]:\n        \"\"\"\n        See ``DSInferenceModelBase.kv_cache_config`` for documentation.\n\n        This method assumes an autoregressive dense attention pattern. Override this method\n        if this does not match the model's attention pattern.\n        \"\"\"\n        if self._kv_cache_config is None:\n            cache_shape = (self.num_layers, self.n_heads_kv_local, self.head_size)\n            max_blocks = ceil_div(self.max_sequence_length, self.attn.kv_block_size)\n            self._kv_cache_config = KVCacheConfig(block_size=self.attn.kv_block_size,\n                                                  cache_shape=cache_shape,\n                                                  cache_dtype=self.activation_dtype,\n                                                  max_blocks_per_allocation_group=max_blocks)\n        return (self._kv_cache_config, )\n\n    def prepare_batch(self, wrapped_batch: RaggedBatchWrapper) -> None:\n        \"\"\"\n        See ``DSInferenceModelBase.prepare_batch`` for documentation.\n\n        This method assumes an autoregressive dense attention pattern. Override this method\n        if this does not match the model's attention pattern.\n        \"\"\"\n        self.attn.build_atoms(wrapped_batch)\n\n    ######### Attention output #########\n    def make_attn_out_layer(self) -> None:\n        \"\"\"\n        Instantiates the linear projection layer for the attention output linear layer. This sets the\n        `self.attn_out` attribute.\n        \"\"\"\n        in_features = attn_out_in_features(self.model_dim, self.tp_rank, self.tp_size, self.head_size, self.n_heads_q,\n                                           self.n_heads_kv)\n\n        linear_config = DSLinearConfig(\n            max_tokens=self._engine_config.state_manager.max_ragged_batch_size,\n            in_channels=in_features,\n            out_channels=self.model_dim,\n            input_dtype=self.activation_dtype,\n            output_dtype=self.activation_dtype,\n        )\n\n        self.attn_out = heuristics.instantiate_linear(linear_config, self._engine_config)\n\n    def transform_attn_out_param(self, param: torch.Tensor) -> Optional[InferenceParameter]:\n        \"\"\"\n        Shards an attention output projection parameter and passes it to the underlying\n        implementation for any necessary transformations. This will return `None` for bias parameters\n        if they are not on TP rank 0.\n\n        Args:\n            param (torch.Tensor): The parameter to transform. This may be either a bias or weight and should have\n                the shape (out_neurons, in_neurons).\n        \"\"\"\n        param = shard_attn_out_param(param, self.tp_rank, self.tp_size, self.head_size, self.n_heads_q,\n                                     self.n_heads_kv)\n\n        if param is not None:\n            param = self.attn_out.transform_param(param)\n\n        return param\n\n    ######### MLP #########\n    def make_mlp_1_layer(self) -> None:\n        \"\"\"\n        Instantiates the linear projection layer for the first MLP in the feedforward network.\n        This sets the `self.mlp_1` attribute.\n        \"\"\"\n        shard_size = sharded_intermediate_dim(self.intermediate_dim, self.tp_size, self.tp_rank)\n\n        linear_config = DSLinearConfig(\n            max_tokens=self._engine_config.state_manager.max_ragged_batch_size,\n            in_channels=self.model_dim,\n            out_channels=shard_size,\n            activation=self.mlp_activation_fn,\n            input_dtype=self.activation_dtype,\n            output_dtype=self.activation_dtype,\n        )\n\n        self.mlp_1 = heuristics.instantiate_linear(linear_config, self._engine_config)\n\n    def transform_mlp_1_param(self, param: torch.Tensor) -> InferenceParameter:\n        \"\"\"\n        Shards the first MLP parameter and passes it to the underlying implementation\n        for any necessary transformations.\n\n        Args:\n            param (torch.Tensor): The parameter to transform. This may be either a bias or weight and should have\n                the shape (out_neurons, in_neurons).\n        \"\"\"\n        param = shard_mlp_1_param(param, self.tp_rank, self.tp_size, gated=self.gated_mlp)\n\n        return self.mlp_1.transform_param(param)\n\n    def make_mlp_2_layer(self) -> None:\n        \"\"\"\n        Instantiates the linear projection layer for the second MLP in the feedforward network.\n        This sets the `self.mlp_2` attribute.\n        \"\"\"\n        shard_size = sharded_intermediate_dim(self.intermediate_dim, self.tp_size, self.tp_rank)\n\n        linear_config = DSLinearConfig(\n            max_tokens=self._engine_config.state_manager.max_ragged_batch_size,\n            in_channels=shard_size,\n            out_channels=self.model_dim,\n            input_dtype=self.activation_dtype,\n            output_dtype=self.activation_dtype,\n        )\n\n        self.mlp_2 = heuristics.instantiate_linear(linear_config, self._engine_config)\n\n    def transform_mlp_2_param(self, param: torch.Tensor) -> Optional[InferenceParameter]:\n        \"\"\"\n        Shards the second MLP parameter and passes it to the underlying implementation\n        for any necessary transformations. This will return `None` for bias parameters\n        if they are not on TP rank 0.\n\n        Args:\n            param (torch.Tensor): The parameter to transform. This may be either a bias or weight and should have\n                the shape (out_neurons, in_neurons).\n        \"\"\"\n        param = shard_mlp_2_param(param, self.tp_rank, self.tp_size)\n\n        if param is not None:\n            param = self.mlp_2.transform_param(param)\n\n        return param\n\n    ######### Norm #########\n    def make_norm_layer(self) -> None:\n        \"\"\"\n        Instantiates the normalization layer for the model. This sets the `self.norm` attribute.\n\n        TODO(cmikeh2): In the future we'll distinguish between the different norm objects,\n        but for now we'll just use the same one for all of them.\n        \"\"\"\n        norm_config = DSNormConfig(\n            max_tokens=self._engine_config.state_manager.max_ragged_batch_size,\n            type=self.norm_type,\n            channels=self.model_dim,\n            residual_dtype=self.activation_dtype,\n            input_dtype=self.activation_dtype,\n            output_dtype=self.activation_dtype,\n        )\n\n        self.norm = heuristics.instantiate_pre_norm(norm_config, self._engine_config)\n\n    def transform_norm_param(self, param: torch.Tensor) -> InferenceParameter:\n        \"\"\"\n        Passes a normalization parameter to the underlying implementation for any\n        necessary transformations.\n\n        TODO(cmikeh2): In the future we'll distinguish between the different norm objects,\n        but for now we'll just use the same one for all of them.\n\n        Args:\n            param (torch.Tensor): The parameter to transform. This may be either a bias or weight and should have\n                shape (model_dim,)\n        \"\"\"\n        return self.norm.transform_param(param)\n\n\nclass DSMoETransformerModelBase(DSTransformerModelBase):\n\n    @property\n    def n_experts(self) -> int:\n        \"\"\"\n        Return the number of experts in the model.\n        \"\"\"\n        raise NotImplementedError(\"Attempted to access an unimplemented number of experts\")\n\n    @property\n    def n_top_k(self) -> int:\n        \"\"\"\n        Number of experts per token.\n        \"\"\"\n        raise NotImplementedError(\"Attempted to access an unimplemented number of experts per token\")\n\n    @property\n    def normalize_expert_scores(self) -> bool:\n        \"\"\"\n        Whether to normalize expert scores. If true, sum(expert_scores) = 1.\n        \"\"\"\n        raise NotImplementedError(\"Attempted to access an unimplemented normalization flag\")\n\n    def make_moe_layer(self) -> None:\n        \"\"\"\n        Instantiates the MoE layer for the model. This sets the `self.moe` attribute.\n        \"\"\"\n        sharded_dim = sharded_intermediate_dim(self.intermediate_dim, self.tp_size, self.tp_rank)\n\n        moe_config = DSMoEConfig(\n            max_tokens=self._engine_config.state_manager.max_ragged_batch_size,\n            model_dim=self.model_dim,\n            intermediate_features=sharded_dim,\n            activation=self.mlp_activation_fn,\n            n_experts=self.n_experts,\n            top_k=self.n_top_k,\n            input_dtype=self.activation_dtype,\n            output_dtype=self.activation_dtype,\n            normalize_scores=self.normalize_expert_scores,\n        )\n\n        self.moe = heuristics.instantiate_moe(moe_config, self._engine_config)\n\n    def transform_moe_gate_param(self, param: torch.Tensor) -> InferenceParameter:\n        \"\"\"\n        Passes a MoE gate parameter to the underlying implementation for any necessary transformations.\n\n        TODO(cmikeh2): This will need to be updated/overridden for expert parallelism.\n        \"\"\"\n        return self.moe.transform_gate_param(param)\n\n    def transform_moe_mlp_1_param(self, param: torch.Tensor) -> InferenceParameter:\n        \"\"\"\n        Shards the first MoE param and passes it to the underlying implementation. Since it's possible for an architecture\n        to have both MoE and non-MoE layers, this can't be overloaded on the MLP1 transform. Furthermore, since both\n        the MoE DSModule owns both MLP1 and MLP2, under certain sharding conditions it's not possible for the model implementation\n        to infer from the shape whether to perform a different transformation based on MLP1 or MLP2. This (and the below)\n        separations are intended to solve both these issues.\n\n        Args:\n            param (torch.Tensor): The parameter to transform. This should have shape (n_experts, out_neurons, in_neurons).\n        \"\"\"\n        param = shard_mlp_1_param(param, self.tp_rank, self.tp_size, gated=self.gated_mlp, is_moe=True)\n\n        return self.moe.transform_moe_mlp_1_param(param)\n\n    def transform_moe_mlp_2_param(self, param: torch.Tensor) -> Optional[torch.Tensor]:\n        \"\"\"\n        Shards the second MoE param and passes it to the underlying implementation. See the above for context on why this API\n        exists.\n\n        This will return `None` for expert bias params not on TP rank 0. NOTE(cmikeh2): Does it make sense to round-robin assign?\n        My intuition is that this will make debugging much more difficult for minimal memory reduction.\n\n        Args:\n            param (torch.Tensor): The parameter to transform. This should have shape (n_experts, out_neurons, in_neurons).\n        \"\"\"\n        param = shard_mlp_2_param(param, self.tp_rank, self.tp_size, is_moe=True)\n\n        if param is not None:\n            param = self.moe.transform_moe_mlp_2_param(param)\n\n        return param\n", "deepspeed/inference/v2/model_implementations/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .inference_model_base import DSInferenceModelBase\nfrom .inference_transformer_base import DSTransformerModelBase, DSMoETransformerModelBase\nfrom .inference_policy_base import InferenceV2Policy, ContainerMap\nfrom .sharding import *\n\n# Model Implementations\nfrom .llama_v2 import *\nfrom .opt import *\nfrom .mistral import *\nfrom .mixtral import *\nfrom .falcon import *\nfrom .phi import *\nfrom .qwen import *\nfrom .qwen_v2 import *\n", "deepspeed/inference/v2/model_implementations/inference_policy_base.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport json\nfrom abc import ABC, ABCMeta, abstractmethod\nfrom typing import Any, Iterable, List, Optional, Union\n\nimport torch\n\nfrom ..config_v2 import RaggedInferenceEngineConfig\nfrom ..checkpoint import CheckpointEngineBase\nfrom ..logging import inference_logger\nfrom .layer_container_base import LayerContainer\nfrom .inference_model_base import DSInferenceModelBase\nfrom .flat_model_helpers import (\n    flatten_inference_model,\n    make_param_filename,\n    make_metadata_filename,\n    ModelMetadata,\n    restore_inference_model,\n)\n\nPOLICIES = {}\n\n\nclass ContainerMap:\n\n    def __init__(self) -> None:\n        self._prefix_map = {}\n        self._transformer_params = None\n        self._non_transformer_params = None\n\n    @property\n    def transformer_params(self) -> Iterable[LayerContainer]:\n        return self._transformer_params\n\n    @property\n    def non_transformer_params(self) -> LayerContainer:\n        return self._non_transformer_params\n\n    def set_transformer_params(self, prefixes: Union[str, Iterable[str]], containers: List[LayerContainer]) -> None:\n        if not isinstance(containers, list):\n            raise ValueError(\n                f\"The transformer containers should be a list, of one container per layer, but got {type(containers)} instead.\"\n            )\n\n        self._transformer_prefixes = prefixes if isinstance(prefixes, list) else [prefixes]\n        self._transformer_params = containers\n\n    def set_non_transformer_params(self, container: LayerContainer) -> None:\n        self._non_transformer_params = container\n\n    def set_unmapped_params(self, prefixes: Union[str, Iterable[str]]) -> None:\n        self._unmapped_prefixes = prefixes\n\n    def map_param(self, name, parameter) -> None:\n        for unmapped_prefix in self._unmapped_prefixes:\n            if name.startswith(unmapped_prefix):\n                inference_logger().debug(f\"Ignoring: {name} for {unmapped_prefix}\")\n                return\n\n        for transformer_prefix in self._transformer_prefixes:\n            if name.startswith(transformer_prefix):\n                popped_name = name[len(transformer_prefix) + 1:]\n                layer_idx = popped_name.split(\".\")[0]\n                assert layer_idx.isdigit(\n                ), f\"expected name to start w. list index but got {layer_idx} instead, name={name}\"\n                layer_idx = int(layer_idx)\n                inference_logger().debug(\n                    f\"Setting: {'.'.join(popped_name.split('.')[1:])} for layer-idx={layer_idx} to {parameter.shape}\")\n                self._transformer_params[layer_idx].set_dependency(\".\".join(popped_name.split(\".\")[1:]), parameter)\n                return\n\n        try:\n            inference_logger().debug(f\"Setting: {name} to {parameter.shape}\")\n            self._non_transformer_params.set_dependency(name, parameter)\n        except ValueError:\n            # Catch the ValueError here from the non_transformer_params because we are knowingly\n            # calling it with something that may not match. This should allow us to raise a slightly more\n            # informative error message.\n            raise ValueError(f\"Cannot find container for {name}, please double check the Containers/ContainerMap\")\n\n    def validate(self) -> None:\n        if not self._non_transformer_params.is_initialized:\n            raise RuntimeError(\"Non-transformer parameters not fully initialized after checkpoint load.\")\n\n        for layer_idx, container in enumerate(self._transformer_params):\n            if not container.is_initialized:\n                raise RuntimeError(\n                    f\"Transformer container at index {layer_idx} not fully initialized after checkpoint load.\")\n\n\nclass PolicyMeta(ABCMeta):\n\n    def __new__(cls, name, bases, dct):\n        new_obj = super().__new__(cls, name, bases, dct)\n        if name != \"InferenceV2Policy\":\n            POLICIES[name] = new_obj\n        return new_obj\n\n\nclass InferenceV2Policy(ABC, metaclass=PolicyMeta):\n    \"\"\"\n    The InferenceV2Policy is the base class for all inference policies. An inference policy\n    is responsible for instantiating the inference model and mapping the parameters from the\n    checkpoint engine to the model itself.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_config: Any,\n        checkpoint_engine: Optional[CheckpointEngineBase] = None,\n        inf_checkpoint_path: Optional[str] = None,\n    ) -> None:\n        \"\"\"\n        Create the Policy with sufficient context to build the model. There are two supported\n        model creation mechanisms.\n\n        The first is the generalized ``checkpoint_engine`` which\n        will iterate over the parameters of the model and provide them to the policy. These in\n        turn will be sharded/transformed by the model implementation.\n\n        The second is used to re-create a previously serialized DeepSpeed inference model. These\n        checkpoints should not be used across different model backend configurations.\n\n        TODO(cmikeh2): Enforce this in code\n        \"\"\"\n        if checkpoint_engine is None and inf_checkpoint_path is None:\n            raise ValueError(\"Either checkpoint_engine or ds_checkpoint_path must be provided.\")\n\n        if checkpoint_engine is not None and inf_checkpoint_path is not None:\n            raise ValueError(\"Only one of checkpoint_engine or ds_checkpoint_path can be provided.\")\n\n        self._checkpoint_engine = checkpoint_engine\n        self._inf_checkpoint_path = inf_checkpoint_path\n        self._model_config = model_config\n\n    def build_model(self, engine_config: RaggedInferenceEngineConfig, mp_group: Any) -> DSInferenceModelBase:\n        \"\"\"\n        Completely instantiate the inference model. This will both create the ops needed to run the\n        model, as well as load the model parameters via the checkpoint engine. For more context\n        on each of these components please see ``instantiate_model`` and ``populate_model_parameters``.\n\n        Arguments:\n            engine_config: The config that has been used to instantiate the engine. This is used\n                to communicate to the model implementation the limits on batches (sequences/tokens)\n                and bound the size of intermediate buffers.\n            mp_group: Object to enable communication between tensor parallel ranks.\n\n        Returns:\n            DSInferenceModelBase: An implementation of the inference model abstraction that will be\n                run by the engine.\n        \"\"\"\n        self.model = self.instantiate_model(engine_config, mp_group)\n        self.populate_model_parameters()\n        return self.model\n\n    @abstractmethod\n    def instantiate_model(self, engine_config: RaggedInferenceEngineConfig) -> DSInferenceModelBase:\n        \"\"\"\n        Instantiate the inference model. Depending on the engine/model config, this could be where\n        different model implementations could be selected.\n\n        Arguments:\n            engine_config: The config that has been used to instantiate the engine. This is used\n                to communicate to the model implementation the limits on batches (sequences/tokens)\n                and bound the size of intermediate buffers.\n\n        Returns:\n            DSInferenceModelBase: An implementation of the inference model abstraction that will be\n                run by the engine.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def build_container_map(self) -> ContainerMap:\n        \"\"\"\n        Build a dictionary representing the structure of the string prefixes leading\n        to the parameters to be mapped to the container.\n\n        Returns:\n            ContainerMap: An instantiated mapping describing how checkpoint prefixes map\n                to ``LayerContainer`` instances.\n        \"\"\"\n        raise NotImplementedError()\n\n    def populate_model_parameters(self) -> None:\n        \"\"\"\n        This model will iterate over the parameters (as provided by the checkpoint engine) and\n        use the container map built by ``build_container_map`` to populate the model\n        \"\"\"\n\n        container_map = self.build_container_map()\n\n        if self._checkpoint_engine is not None:\n            for name, parameter in self._checkpoint_engine.parameters():\n                container_map.map_param(name, parameter)\n\n            buffer, metadata = flatten_inference_model(container_map.transformer_params,\n                                                       container_map.non_transformer_params, self.__class__.__name__)\n        else:\n\n            buffer_path = make_param_filename(self._inf_checkpoint_path, self.model.tp_rank, self.model.tp_size)\n            metadata_path = make_metadata_filename(self._inf_checkpoint_path, self.model.tp_rank, self.model.tp_size)\n\n            buffer = torch.load(buffer_path)\n            metadata = json.load(open(metadata_path, \"r\"))\n            metadata = ModelMetadata.parse_raw(metadata)\n\n            restore_inference_model(buffer, metadata, container_map.transformer_params,\n                                    container_map.non_transformer_params)\n\n        container_map.validate()\n\n        self.model.set_parameters(transformer=container_map.transformer_params,\n                                  non_transformer=container_map.non_transformer_params,\n                                  flattened_param_buffer=buffer,\n                                  flattened_param_metadata=metadata)\n", "deepspeed/inference/v2/model_implementations/mistral/model.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Iterable, Optional, Tuple\n\nimport torch\n\nimport deepspeed.comm as dist\n\nfrom ...allocator import empty_from\nfrom ...inference_utils import ActivationType, DtypeEnum\nfrom ...model_implementations import *\nfrom ...modules.configs import *\nfrom ...modules.interfaces import *\nfrom ...ragged import RaggedBatchWrapper\n\nfrom .container import MistralNonTransformerContainer, MistralTransformerContainer\n\n\nclass MistralInferenceModel(DSTransformerModelBase):\n    \"\"\"\n    Inference model implementation for ragged batching for Mistral models.\n    \"\"\"\n\n    _non_transformer: Optional[MistralNonTransformerContainer]\n    \"\"\"\n    Embed + unembed container. Specializing the type annotation.\n    \"\"\"\n\n    _transformer: Optional[Iterable[MistralTransformerContainer]]\n    \"\"\"\n    Per-layer transformer container. Specializing the type annotation.\n    \"\"\"\n    \"\"\"\n    Properties ineherited from `DSInferenceModelBase`\n    \"\"\"\n\n    @property\n    def max_sequence_length(self) -> int:\n        return self._config.max_seq_length\n\n    \"\"\"\n    Properties ineherited from `DSTransformerModelBase`\n    \"\"\"\n\n    @property\n    def num_layers(self) -> int:\n        return self._config.num_hidden_layers\n\n    @property\n    def model_dim(self) -> int:\n        return self._config.hidden_size\n\n    @property\n    def vocab_size(self) -> int:\n        return self._config.vocab_size\n\n    @property\n    def head_size(self) -> int:\n        return self.model_dim // self.n_heads\n\n    @property\n    def n_heads(self) -> int:\n        return self._config.num_attention_heads\n\n    @property\n    def intermediate_dim(self) -> int:\n        return self._config.intermediate_size\n\n    @property\n    def n_heads_kv(self) -> int:\n        return self._config.num_key_value_heads\n\n    @property\n    def activation_dtype(self) -> DtypeEnum:\n        if self._config.torch_dtype == torch.float16:\n            return DtypeEnum.fp16\n        elif self._config.torch_dtype == torch.bfloat16:\n            return DtypeEnum.bf16\n        else:\n            raise NotImplementedError(\"Only fp16 and bf16 are supported\")\n\n    @property\n    def mlp_activation_fn(self) -> ActivationType:\n        activation = self._config.hidden_act.lower()\n        if activation == \"gelu\":\n            return ActivationType.GEGLU\n        elif activation == \"relu\":\n            return ActivationType.ReGLU\n        elif activation == \"gegelu\":\n            return ActivationType.GEGLU\n        elif activation == \"silu\":\n            return ActivationType.SiGLU\n        else:\n            raise NotImplementedError(f\"Activation {activation} not supported\")\n\n    @property\n    def norm_type(self) -> NormTypeEnum:\n        return NormTypeEnum.RMSNorm\n\n    @property\n    def positional_embedding_type(self) -> PositionalEmbeddingType:\n        return PositionalEmbeddingType.rotate_half\n\n    @property\n    def positional_embedding_config(self) -> Optional[RotateHalfConfig]:\n        return RotateHalfConfig(theta_base=self._config.rope_theta)\n\n    \"\"\"\n    Forward implementations\n    \"\"\"\n\n    def _forward_embed(self, ragged_batch: RaggedBatchWrapper) -> torch.Tensor:\n        \"\"\"\n        Performs the embedding lookup prior to running the transformer of the model.\n\n        Arguments:\n            ragged_batch (RaggedBatchWrapper): The batch to embed.\n\n        Returns:\n            torch.Tensor: The embedded batch.\n        \"\"\"\n        embed = self.embed(ragged_batch, self._non_transformer.word_emb)\n\n        if embed.shape[-1] != self.model_dim:\n            raise ValueError(f\"Embedding output shape {embed.shape} does not match model_dim {self.model_dim}\")\n\n        return embed\n\n    def _forward_transformer(self, layer_idx: int, residual: torch.Tensor, hidden_states: torch.Tensor,\n                             ragged_batch_info: RaggedBatchWrapper) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Executes one (slightly offset) layer of the transformer. This implementation does a peak-ahead\n        optimization to fuse the layer norm of the next layer into the current layer.\n\n        Arguments:\n            layer_idx (int): The index of the layer to execute.\n            residual (torch.Tensor): The residual tensor from the previous layer.\n            hidden_states (torch.Tensor): The hidden states from the previous layer. This is the\n                hidden states after pre normalization.\n            ragged_batch_info (RaggedBatchWrapper): The batch metadata.\n        \"\"\"\n        # TODO(cmikeh2): Distribute ragged_batch_info to all modules\n\n        cur_params = self._transformer[layer_idx]\n        kv_cache = self.state_manager.get_cache(layer_idx)\n\n        hidden_states = self.qkv(hidden_states, cur_params.qkv_w, b=None)\n        hidden_states = self.attn(hidden_states, kv_cache, ragged_batch_info)\n        hidden_states = self.attn_out(hidden_states, cur_params.attn_out_w, b=None)\n\n        if self.tp_size > 1:\n            dist.all_reduce(hidden_states, group=self._base_mp_group)\n\n        residual, hidden_states = self.norm(residual, hidden_states, cur_params.mlp_norm_gamma, beta=None)\n\n        # Should be configurable in the future\n        hidden_states = self.mlp_1(hidden_states, cur_params.mlp_1_w, b=None)\n        hidden_states = self.mlp_2(hidden_states, cur_params.mlp_2_w, b=None)\n\n        if self.tp_size > 1:\n            dist.all_reduce(hidden_states, group=self._base_mp_group)\n\n        if layer_idx != self.num_layers - 1:\n            next_params = self._transformer[layer_idx + 1]\n            residual, hidden_states = self.norm(residual, hidden_states, next_params.attn_norm_gamma, beta=None)\n        else:\n            # On last layer, we just need to perform the residual add. Adding into the residual\n            # here is safe.\n            residual.add_(hidden_states)\n\n        return residual, hidden_states\n\n    def _forward_unembed(self, hidden_states: torch.Tensor, ragged_batch_info: RaggedBatchWrapper) -> torch.Tensor:\n        \"\"\"\n        Performs unembedding of the hidden states to logits. This will only sample the final\n        token of each sequence.\n        \"\"\"\n        logits = self.unembed(hidden_states,\n                              self._non_transformer.word_unembed,\n                              ragged_batch_info,\n                              gamma=self._non_transformer.final_norm)\n\n        if self.tp_size > 1:\n            comm_buffer = empty_from(self._comm_logits, (self.tp_size, logits.shape[0], logits.shape[1]))\n            full_logits = empty_from(self._return_logits, (logits.shape[0], self.vocab_size))\n\n            dist.all_gather_into_tensor(comm_buffer, logits, group=self._base_mp_group)\n\n            full_logits.copy_(comm_buffer.permute(1, 0, 2).reshape(logits.shape[0], self.vocab_size))\n\n            return full_logits\n        else:\n            return logits\n\n    def forward(self, wrapped_batch: RaggedBatchWrapper) -> torch.Tensor:\n\n        residual = self._forward_embed(wrapped_batch)\n\n        residual, hidden_states = self.norm(residual, None, self._transformer[0].attn_norm_gamma, beta=None)\n\n        for layer_idx in range(self.num_layers):\n            residual, hidden_states = self._forward_transformer(layer_idx, residual, hidden_states, wrapped_batch)\n\n        return self._forward_unembed(residual, wrapped_batch)\n", "deepspeed/inference/v2/model_implementations/mistral/container.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\n# Create a container object to save model-specific tensors using the policy file above.\n\nfrom deepspeed.inference.v2.model_implementations.common_parameters import *\nfrom deepspeed.inference.v2.model_implementations.layer_container_base import LayerContainer\n'''\n # HF Mistral model (mistralai/Mistral-7B-v0.1) looks like this:\nMistralForCausalLM(\n  (model): MistralModel(\n    (embed_tokens): Embedding(32000, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x MistralDecoderLayer(\n        (self_attn): MistralAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): MistralRotaryEmbedding()\n        )\n        (mlp): MistralMLP(\n          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): MistralRMSNorm()\n        (post_attention_layernorm): MistralRMSNorm()\n      )\n    )\n    (norm): MistralRMSNorm()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n)\n'''\n\n\nclass MistralTransformerContainer(LayerContainer):\n    \"\"\"\n        Transformer layer container for the Mistral model.\n    \"\"\"\n    qkv_w: UnfusedQKVParameter\n    attn_out_w: AttentionOutputParameter\n    mlp_1_w: GatedMLPParameter\n    mlp_2_w: MLP2Parameter\n    attn_norm_gamma: NormParameter\n    mlp_norm_gamma: NormParameter\n\n    PARAM_MAPPING = {\n        \"self_attn.q_proj.weight\": \"qkv_w.q_params\",\n        \"self_attn.k_proj.weight\": \"qkv_w.k_params\",\n        \"self_attn.v_proj.weight\": \"qkv_w.v_params\",\n        \"self_attn.o_proj.weight\": \"attn_out_w.params\",\n        \"mlp.gate_proj.weight\": \"mlp_1_w.gate_params\",\n        \"mlp.up_proj.weight\": \"mlp_1_w.up_params\",\n        \"mlp.down_proj.weight\": \"mlp_2_w.params\",\n        \"input_layernorm.weight\": \"attn_norm_gamma.params\",\n        \"post_attention_layernorm.weight\": \"mlp_norm_gamma.params\",\n    }\n\n\nclass MistralNonTransformerContainer(LayerContainer):\n    \"\"\"\n        Non-Transformer layer container for the Mistral model.\n    \"\"\"\n    word_emb: EmbeddingParameter\n    word_unembed: UnembedParameter\n    final_norm: NormParameter\n\n    PARAM_MAPPING = {\n        \"model.embed_tokens.weight\": \"word_emb.params\",\n        \"model.norm.weight\": \"final_norm.params\",\n        \"lm_head.weight\": \"word_unembed.params\",\n    }\n", "deepspeed/inference/v2/model_implementations/mistral/policy.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Any\n\nfrom ...config_v2 import RaggedInferenceEngineConfig\nfrom ..inference_policy_base import ContainerMap, InferenceV2Policy\nfrom .container import MistralNonTransformerContainer, MistralTransformerContainer\nfrom .model import MistralInferenceModel\n\n\nclass MistralPolicy(InferenceV2Policy):\n\n    def instantiate_model(self, engine_config: RaggedInferenceEngineConfig, mp_group: Any) -> MistralInferenceModel:\n        return MistralInferenceModel(config=self._model_config, engine_config=engine_config, base_mp_group=mp_group)\n\n    def build_container_map(self) -> ContainerMap:\n        map = ContainerMap()\n\n        transformer_containers = [MistralTransformerContainer(self.model) for _ in range(self.model.num_layers)]\n\n        map.set_transformer_params(['model.layers'], transformer_containers)\n\n        map.set_non_transformer_params(MistralNonTransformerContainer(self.model))\n\n        map.set_unmapped_params([])\n\n        return map\n", "deepspeed/inference/v2/model_implementations/mistral/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .policy import MistralPolicy\n", "deepspeed/inference/v2/model_implementations/qwen/model.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Iterable, Optional, Tuple\n\nimport torch\n\nimport deepspeed.comm as dist\n\nfrom ...allocator import empty_from\nfrom ...inference_utils import ActivationType, DtypeEnum\nfrom .. import *\nfrom ...modules.configs import *\nfrom ...modules.interfaces import *\nfrom ...modules import heuristics\nfrom ...ragged import RaggedBatchWrapper\n\nfrom .container import QwenNonTransformerContainer, QwenTransformerContainer\n\n\nclass QwenInferenceModel(DSTransformerModelBase):\n    \"\"\"\n    Inference model implementation for ragged batching for Llama-2 models.\n    \"\"\"\n\n    _non_transformer: Optional[QwenNonTransformerContainer]\n    \"\"\"\n    Embed + unembed container. Specializing the type annotation.\n    \"\"\"\n\n    _transformer: Optional[Iterable[QwenTransformerContainer]]\n    \"\"\"\n    Per-layer transformer container. Specializing the type annotation.\n    \"\"\"\n    \"\"\"\n    Properties ineherited from `DSInferenceModelBase`\n    \"\"\"\n\n    @property\n    def max_sequence_length(self) -> int:\n        return self._config.max_seq_length\n\n    \"\"\"\n    Properties ineherited from `DSTransformerModelBase`\n    \"\"\"\n\n    @property\n    def num_layers(self) -> int:\n        return self._config.num_hidden_layers\n\n    @property\n    def model_dim(self) -> int:\n        return self._config.hidden_size\n\n    @property\n    def vocab_size(self) -> int:\n        return self._config.vocab_size\n\n    @property\n    def head_size(self) -> int:\n        return self.model_dim // self.n_heads\n\n    @property\n    def n_heads(self) -> int:\n        return self._config.num_attention_heads\n\n    @property\n    def intermediate_dim(self) -> int:\n        return self._config.intermediate_size // 2\n\n    @property\n    def n_heads_kv(self) -> int:\n        return self._config.hidden_size // self._config.kv_channels\n\n    @property\n    def activation_dtype(self) -> DtypeEnum:\n        autoset_precision = self._config.bf16 + self._config.fp16 == 0\n        if autoset_precision:\n            return DtypeEnum.fp16\n        if self._config.fp16:\n            return DtypeEnum.fp16\n        elif self._config.bf16:\n            # TODO(ZonePG): bf16 inference results may be different from huggingface bf16,\n            # because in rms_norm, Qwen still use float() instead of bf16\n            return DtypeEnum.bf16\n        else:\n            raise NotImplementedError(\"Only fp16 and bf16 are supported\")\n\n    @property\n    def mlp_activation_fn(self) -> ActivationType:\n        return ActivationType.SiGLU\n\n    @property\n    def norm_type(self) -> NormTypeEnum:\n        return NormTypeEnum.RMSNorm\n\n    @property\n    def positional_embedding_type(self) -> PositionalEmbeddingType:\n        return PositionalEmbeddingType.rotate_half\n\n    @property\n    def positional_embedding_config(self) -> Optional[RotateHalfConfig]:\n        return RotateHalfConfig(theta_base=self._config.rotary_emb_base)\n\n    def make_norm_layer(self) -> None:\n        \"\"\"\n        Instantiates the normalization layer for the model. This sets the `self.norm` attribute.\n\n        TODO(cmikeh2): In the future we'll distinguish between the different norm objects,\n        but for now we'll just use the same one for all of them.\n        \"\"\"\n        norm_config = DSNormConfig(\n            max_tokens=self._engine_config.state_manager.max_ragged_batch_size,\n            type=self.norm_type,\n            channels=self.model_dim,\n            residual_dtype=self.activation_dtype,\n            input_dtype=self.activation_dtype,\n            output_dtype=self.activation_dtype,\n            eps=self._config.layer_norm_epsilon,\n        )\n\n        self.norm = heuristics.instantiate_pre_norm(norm_config, self._engine_config)\n\n    \"\"\"\n    Forward implementations\n    \"\"\"\n\n    def _forward_embed(self, ragged_batch: RaggedBatchWrapper) -> torch.Tensor:\n        \"\"\"\n        Performs the embedding lookup prior to running the transformer of the model.\n\n        Arguments:\n            ragged_batch (RaggedBatchWrapper): The batch to embed.\n\n        Returns:\n            torch.Tensor: The embedded batch.\n        \"\"\"\n        embed = self.embed(ragged_batch, self._non_transformer.word_emb)\n\n        if embed.shape[-1] != self.model_dim:\n            raise ValueError(f\"Embedding output shape {embed.shape} does not match model_dim {self.model_dim}\")\n\n        return embed\n\n    def _forward_transformer_layer(self, layer_idx: int, residual: torch.Tensor, hidden_states: torch.Tensor,\n                                   ragged_batch_info: RaggedBatchWrapper) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Executes one (slightly offset) layer of the transformer. This implementation does a peak-ahead\n        optimization to fuse the layer norm of the next layer into the current layer.\n\n        Arguments:\n            layer_idx (int): The index of the layer to execute.\n            residual (torch.Tensor): The residual tensor from the previous layer.\n            hidden_states (torch.Tensor): The hidden states from the previous layer. This is the\n                hidden states after pre normalization.\n            ragged_batch_info (RaggedBatchWrapper): The batch metadata.\n        \"\"\"\n        # TODO(cmikeh2): Distribute ragged_batch_info to all modules\n\n        cur_params = self._transformer[layer_idx]\n        kv_cache = self.state_manager.get_cache(layer_idx)\n\n        hidden_states = self.qkv(hidden_states, cur_params.qkv_w, b=cur_params.qkv_b)\n        hidden_states = self.attn(hidden_states, kv_cache, ragged_batch_info)\n        hidden_states = self.attn_out(hidden_states, cur_params.attn_out_w, b=None)\n\n        if self.tp_size > 1:\n            dist.all_reduce(hidden_states, group=self._base_mp_group)\n\n        residual, hidden_states = self.norm(residual, hidden_states, cur_params.mlp_norm_gamma, beta=None)\n\n        # Should be configurable in the future\n        hidden_states = self.mlp_1(hidden_states, cur_params.mlp_1_w, b=None)\n        hidden_states = self.mlp_2(hidden_states, cur_params.mlp_2_w, b=None)\n\n        if self.tp_size > 1:\n            dist.all_reduce(hidden_states, group=self._base_mp_group)\n\n        if layer_idx != self.num_layers - 1:\n            next_params = self._transformer[layer_idx + 1]\n            residual, hidden_states = self.norm(residual, hidden_states, next_params.attn_norm_gamma, beta=None)\n        else:\n            # On last layer, we just need to perform the residual add. Adding into the residual\n            # here is safe.\n            residual.add_(hidden_states)\n\n        return residual, hidden_states\n\n    def _forward_unembed(self, hidden_states: torch.Tensor, ragged_batch_info: RaggedBatchWrapper) -> torch.Tensor:\n        \"\"\"\n        Performs unembedding of the hidden states to logits. This will only sample the final\n        token of each sequence.\n        \"\"\"\n        logits = self.unembed(hidden_states,\n                              self._non_transformer.word_unembed,\n                              ragged_batch_info,\n                              gamma=self._non_transformer.final_norm)\n\n        if self.tp_size > 1:\n            comm_buffer = empty_from(self._comm_logits, (self.tp_size, logits.shape[0], logits.shape[1]))\n            full_logits = empty_from(self._return_logits, (logits.shape[0], self.vocab_size))\n\n            dist.all_gather_into_tensor(comm_buffer, logits, group=self._base_mp_group)\n\n            full_logits.copy_(comm_buffer.permute(1, 0, 2).reshape(logits.shape[0], self.vocab_size))\n\n            return full_logits\n        else:\n            return logits\n\n    def forward(self, wrapped_batch: RaggedBatchWrapper) -> torch.Tensor:\n\n        residual = self._forward_embed(wrapped_batch)\n\n        residual, hidden_states = self.norm(residual, None, self._transformer[0].attn_norm_gamma, beta=None)\n\n        for layer_idx in range(self.num_layers):\n            residual, hidden_states = self._forward_transformer_layer(layer_idx, residual, hidden_states,\n                                                                      wrapped_batch)\n\n        return self._forward_unembed(residual, wrapped_batch)\n", "deepspeed/inference/v2/model_implementations/qwen/container.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\n# Create a container object to save model-specific tensors using the policy file above.\n\nfrom ..common_parameters import *\nfrom ..layer_container_base import LayerContainer\n'''\n # HF Qwen model looks like this:\n\nQWenLMHeadModel(\n  (transformer): QWenModel(\n    (wte): Embedding(151936, 4096)\n    (drop): Dropout(p=0.0, inplace=False)\n    (rotary_emb): RotaryEmbedding()\n    (h): ModuleList(\n      (0-31): 32 x QWenBlock(\n        (ln_1): RMSNorm()\n        (attn): QWenAttention(\n          (c_attn): Linear(in_features=4096, out_features=12288, bias=True)\n          (c_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (ln_2): RMSNorm()\n        (mlp): QWenMLP(\n          (w1): Linear(in_features=4096, out_features=11008, bias=False)\n          (w2): Linear(in_features=4096, out_features=11008, bias=False)\n          (c_proj): Linear(in_features=11008, out_features=4096, bias=False)\n        )\n      )\n    )\n    (ln_f): RMSNorm()\n  )\n  (lm_head): Linear(in_features=4096, out_features=151936, bias=False)\n)\n'''\n\n\nclass QwenTransformerContainer(LayerContainer):\n    \"\"\"\n        Transformer layer container for the Qwen model.\n    \"\"\"\n    qkv_w: FusedQKVParameter\n    qkv_b: FusedQKVParameter\n    attn_out_w: AttentionOutputParameter\n    mlp_1_w: GatedMLPParameter\n    mlp_2_w: MLP2Parameter\n    attn_norm_gamma: NormParameter\n    mlp_norm_gamma: NormParameter\n\n    PARAM_MAPPING = {\n        \"attn.c_attn.weight\": \"qkv_w.params\",\n        \"attn.c_attn.bias\": \"qkv_b.params\",\n        \"attn.c_proj.weight\": \"attn_out_w.params\",\n        \"mlp.w1.weight\": \"mlp_1_w.up_params\",\n        \"mlp.w2.weight\": \"mlp_1_w.gate_params\",\n        \"mlp.c_proj.weight\": \"mlp_2_w.params\",\n        \"ln_1.weight\": \"attn_norm_gamma.params\",\n        \"ln_2.weight\": \"mlp_norm_gamma.params\",\n    }\n\n\nclass QwenNonTransformerContainer(LayerContainer):\n    \"\"\"\n        Non-Transformer layer container for the Qwen model.\n    \"\"\"\n    word_emb: EmbeddingParameter\n    word_unembed: UnembedParameter\n    final_norm: NormParameter\n\n    PARAM_MAPPING = {\n        \"transformer.wte.weight\": \"word_emb.params\",\n        \"transformer.ln_f.weight\": \"final_norm.params\",\n        \"lm_head.weight\": \"word_unembed.params\",\n    }\n", "deepspeed/inference/v2/model_implementations/qwen/policy.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Any\n\nfrom ...config_v2 import RaggedInferenceEngineConfig\nfrom ..inference_policy_base import ContainerMap, InferenceV2Policy\nfrom .container import QwenNonTransformerContainer, QwenTransformerContainer\nfrom .model import QwenInferenceModel\n\n\nclass QwenPolicy(InferenceV2Policy):\n\n    def instantiate_model(self, engine_config: RaggedInferenceEngineConfig, mp_group: Any) -> QwenInferenceModel:\n        return QwenInferenceModel(config=self._model_config, engine_config=engine_config, base_mp_group=mp_group)\n\n    def build_container_map(self) -> ContainerMap:\n        map = ContainerMap()\n\n        transformer_containers = [QwenTransformerContainer(self.model) for _ in range(self.model.num_layers)]\n\n        map.set_transformer_params(['transformer.h'], transformer_containers)\n\n        map.set_non_transformer_params(QwenNonTransformerContainer(self.model))\n\n        map.set_unmapped_params(['transformer.rotary_emb.inv_freq'])\n\n        return map\n", "deepspeed/inference/v2/model_implementations/qwen/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .policy import QwenPolicy\n", "deepspeed/inference/v2/model_implementations/qwen_v2/model.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Iterable, Optional, Tuple\n\nimport torch\n\nimport deepspeed.comm as dist\n\nfrom ...allocator import empty_from\nfrom ...inference_utils import ActivationType, DtypeEnum\nfrom .. import *\nfrom ...modules.configs import *\nfrom ...modules.interfaces import *\nfrom ...modules import heuristics\nfrom ...ragged import RaggedBatchWrapper\n\nfrom .container import Qwen2NonTransformerContainer, Qwen2TransformerContainer\n\n\nclass Qwen2InferenceModel(DSTransformerModelBase):\n    \"\"\"\n    Inference model implementation for ragged batching for Llama-2 models.\n    \"\"\"\n\n    _non_transformer: Optional[Qwen2NonTransformerContainer]\n    \"\"\"\n    Embed + unembed container. Specializing the type annotation.\n    \"\"\"\n\n    _transformer: Optional[Iterable[Qwen2TransformerContainer]]\n    \"\"\"\n    Per-layer transformer container. Specializing the type annotation.\n    \"\"\"\n    \"\"\"\n    Properties ineherited from `DSInferenceModelBase`\n    \"\"\"\n\n    @property\n    def max_sequence_length(self) -> int:\n        return self._config.max_seq_length\n\n    \"\"\"\n    Properties ineherited from `DSTransformerModelBase`\n    \"\"\"\n\n    @property\n    def num_layers(self) -> int:\n        return self._config.num_hidden_layers\n\n    @property\n    def model_dim(self) -> int:\n        return self._config.hidden_size\n\n    @property\n    def vocab_size(self) -> int:\n        return self._config.vocab_size\n\n    @property\n    def head_size(self) -> int:\n        return self.model_dim // self.n_heads\n\n    @property\n    def n_heads(self) -> int:\n        return self._config.num_attention_heads\n\n    @property\n    def intermediate_dim(self) -> int:\n        return self._config.intermediate_size\n\n    @property\n    def n_heads_kv(self) -> int:\n        return self._config.num_key_value_heads\n\n    @property\n    def activation_dtype(self) -> DtypeEnum:\n        # TODO(ZonePG): bf16 inference results may be different from huggingface bf16,\n        # because in rms_norm, Qwen still use float() instead of bf16\n        # if self._config.torch_dtype == torch.float16:\n        #     return DtypeEnum.fp16\n        # elif self._config.torch_dtype == torch.bfloat16:\n        #     return DtypeEnum.bf16\n        # else:\n        #     raise NotImplementedError(\"Only fp16 and bf16 are supported\")\n        return DtypeEnum.fp16\n\n    @property\n    def mlp_activation_fn(self) -> ActivationType:\n        return ActivationType.SiGLU\n\n    @property\n    def norm_type(self) -> NormTypeEnum:\n        return NormTypeEnum.RMSNorm\n\n    @property\n    def positional_embedding_type(self) -> PositionalEmbeddingType:\n        return PositionalEmbeddingType.rotate_half\n\n    @property\n    def positional_embedding_config(self) -> Optional[RotateHalfConfig]:\n        return RotateHalfConfig(theta_base=self._config.rope_theta)\n\n    def make_norm_layer(self) -> None:\n        \"\"\"\n        Instantiates the normalization layer for the model. This sets the `self.norm` attribute.\n\n        TODO(cmikeh2): In the future we'll distinguish between the different norm objects,\n        but for now we'll just use the same one for all of them.\n        \"\"\"\n        norm_config = DSNormConfig(\n            max_tokens=self._engine_config.state_manager.max_ragged_batch_size,\n            type=self.norm_type,\n            channels=self.model_dim,\n            residual_dtype=self.activation_dtype,\n            input_dtype=self.activation_dtype,\n            output_dtype=self.activation_dtype,\n            eps=self._config.rms_norm_eps,\n        )\n\n        self.norm = heuristics.instantiate_pre_norm(norm_config, self._engine_config)\n\n    \"\"\"\n    Forward implementations\n    \"\"\"\n\n    def _forward_embed(self, ragged_batch: RaggedBatchWrapper) -> torch.Tensor:\n        \"\"\"\n        Performs the embedding lookup prior to running the transformer of the model.\n\n        Arguments:\n            ragged_batch (RaggedBatchWrapper): The batch to embed.\n\n        Returns:\n            torch.Tensor: The embedded batch.\n        \"\"\"\n        embed = self.embed(ragged_batch, self._non_transformer.word_emb)\n\n        if embed.shape[-1] != self.model_dim:\n            raise ValueError(f\"Embedding output shape {embed.shape} does not match model_dim {self.model_dim}\")\n\n        return embed\n\n    def _forward_transformer_layer(self, layer_idx: int, residual: torch.Tensor, hidden_states: torch.Tensor,\n                                   ragged_batch_info: RaggedBatchWrapper) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Executes one (slightly offset) layer of the transformer. This implementation does a peak-ahead\n        optimization to fuse the layer norm of the next layer into the current layer.\n\n        Arguments:\n            layer_idx (int): The index of the layer to execute.\n            residual (torch.Tensor): The residual tensor from the previous layer.\n            hidden_states (torch.Tensor): The hidden states from the previous layer. This is the\n                hidden states after pre normalization.\n            ragged_batch_info (RaggedBatchWrapper): The batch metadata.\n        \"\"\"\n        # TODO(cmikeh2): Distribute ragged_batch_info to all modules\n\n        cur_params = self._transformer[layer_idx]\n        kv_cache = self.state_manager.get_cache(layer_idx)\n\n        hidden_states = self.qkv(hidden_states, cur_params.qkv_w, b=cur_params.qkv_b)\n        hidden_states = self.attn(hidden_states, kv_cache, ragged_batch_info)\n        hidden_states = self.attn_out(hidden_states, cur_params.attn_out_w, b=None)\n\n        if self.tp_size > 1:\n            dist.all_reduce(hidden_states, group=self._base_mp_group)\n\n        residual, hidden_states = self.norm(residual, hidden_states, cur_params.mlp_norm_gamma, beta=None)\n\n        # Should be configurable in the future\n        hidden_states = self.mlp_1(hidden_states, cur_params.mlp_1_w, b=None)\n        hidden_states = self.mlp_2(hidden_states, cur_params.mlp_2_w, b=None)\n\n        if self.tp_size > 1:\n            dist.all_reduce(hidden_states, group=self._base_mp_group)\n\n        if layer_idx != self.num_layers - 1:\n            next_params = self._transformer[layer_idx + 1]\n            residual, hidden_states = self.norm(residual, hidden_states, next_params.attn_norm_gamma, beta=None)\n        else:\n            # On last layer, we just need to perform the residual add. Adding into the residual\n            # here is safe.\n            residual.add_(hidden_states)\n\n        return residual, hidden_states\n\n    def _forward_unembed(self, hidden_states: torch.Tensor, ragged_batch_info: RaggedBatchWrapper) -> torch.Tensor:\n        \"\"\"\n        Performs unembedding of the hidden states to logits. This will only sample the final\n        token of each sequence.\n        \"\"\"\n        logits = self.unembed(hidden_states,\n                              self._non_transformer.word_unembed,\n                              ragged_batch_info,\n                              gamma=self._non_transformer.final_norm)\n\n        if self.tp_size > 1:\n            comm_buffer = empty_from(self._comm_logits, (self.tp_size, logits.shape[0], logits.shape[1]))\n            full_logits = empty_from(self._return_logits, (logits.shape[0], self.vocab_size))\n\n            dist.all_gather_into_tensor(comm_buffer, logits, group=self._base_mp_group)\n\n            full_logits.copy_(comm_buffer.permute(1, 0, 2).reshape(logits.shape[0], self.vocab_size))\n\n            return full_logits\n        else:\n            return logits\n\n    def forward(self, wrapped_batch: RaggedBatchWrapper) -> torch.Tensor:\n\n        residual = self._forward_embed(wrapped_batch)\n\n        residual, hidden_states = self.norm(residual, None, self._transformer[0].attn_norm_gamma, beta=None)\n\n        for layer_idx in range(self.num_layers):\n            residual, hidden_states = self._forward_transformer_layer(layer_idx, residual, hidden_states,\n                                                                      wrapped_batch)\n\n        return self._forward_unembed(residual, wrapped_batch)\n", "deepspeed/inference/v2/model_implementations/qwen_v2/container.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\n# Create a container object to save model-specific tensors using the policy file above.\n\nfrom ..common_parameters import *\nfrom ..layer_container_base import LayerContainer\n'''\n # HF Qwen2 model looks like this:\n\nQwen2ForCausalLM(\n  (model): Qwen2Model(\n    (embed_tokens): Embedding(151936, 1024)\n    (layers): ModuleList(\n      (0-23): 24 x Qwen2DecoderLayer(\n        (self_attn): Qwen2SdpaAttention(\n          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n          (rotary_emb): Qwen2RotaryEmbedding()\n        )\n        (mlp): Qwen2MLP(\n          (gate_proj): Linear(in_features=1024, out_features=2816, bias=False)\n          (up_proj): Linear(in_features=1024, out_features=2816, bias=False)\n          (down_proj): Linear(in_features=2816, out_features=1024, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): Qwen2RMSNorm()\n        (post_attention_layernorm): Qwen2RMSNorm()\n      )\n    )\n    (norm): Qwen2RMSNorm()\n  )\n  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n)\n'''\n\n\nclass Qwen2TransformerContainer(LayerContainer):\n    \"\"\"\n        Transformer layer container for the Qwen2 model.\n    \"\"\"\n    qkv_w: UnfusedQKVParameter\n    qkv_b: UnfusedQKVParameter\n    attn_out_w: AttentionOutputParameter\n    mlp_1_w: GatedMLPParameter\n    mlp_2_w: MLP2Parameter\n    attn_norm_gamma: NormParameter\n    mlp_norm_gamma: NormParameter\n\n    PARAM_MAPPING = {\n        \"self_attn.q_proj.weight\": \"qkv_w.q_params\",\n        \"self_attn.k_proj.weight\": \"qkv_w.k_params\",\n        \"self_attn.v_proj.weight\": \"qkv_w.v_params\",\n        \"self_attn.q_proj.bias\": \"qkv_b.q_params\",\n        \"self_attn.k_proj.bias\": \"qkv_b.k_params\",\n        \"self_attn.v_proj.bias\": \"qkv_b.v_params\",\n        \"self_attn.o_proj.weight\": \"attn_out_w.params\",\n        \"mlp.gate_proj.weight\": \"mlp_1_w.gate_params\",\n        \"mlp.up_proj.weight\": \"mlp_1_w.up_params\",\n        \"mlp.down_proj.weight\": \"mlp_2_w.params\",\n        \"input_layernorm.weight\": \"attn_norm_gamma.params\",\n        \"post_attention_layernorm.weight\": \"mlp_norm_gamma.params\",\n    }\n\n\nclass Qwen2NonTransformerContainer(LayerContainer):\n    \"\"\"\n        Non-Transformer layer container for the Qwen2 model.\n    \"\"\"\n    word_emb: EmbeddingParameter\n    word_unembed: UnembedParameter\n    final_norm: NormParameter\n\n    PARAM_MAPPING = {\n        \"model.embed_tokens.weight\": \"word_emb.params\",\n        \"model.norm.weight\": \"final_norm.params\",\n        \"lm_head.weight\": \"word_unembed.params\",\n    }\n", "deepspeed/inference/v2/model_implementations/qwen_v2/policy.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Any\n\nfrom ...config_v2 import RaggedInferenceEngineConfig\nfrom ..inference_policy_base import ContainerMap, InferenceV2Policy\nfrom .container import Qwen2NonTransformerContainer, Qwen2TransformerContainer\nfrom .model import Qwen2InferenceModel\n\n\nclass Qwen2Policy(InferenceV2Policy):\n\n    def instantiate_model(self, engine_config: RaggedInferenceEngineConfig, mp_group: Any) -> Qwen2InferenceModel:\n        return Qwen2InferenceModel(config=self._model_config, engine_config=engine_config, base_mp_group=mp_group)\n\n    def build_container_map(self) -> ContainerMap:\n        map = ContainerMap()\n\n        transformer_containers = [Qwen2TransformerContainer(self.model) for _ in range(self.model.num_layers)]\n\n        map.set_transformer_params(['model.layers'], transformer_containers)\n\n        map.set_non_transformer_params(Qwen2NonTransformerContainer(self.model))\n\n        map.set_unmapped_params(\n            [f'model.layers.{i}.self_attn.rotary_emb.inv_freq' for i in range(self.model.num_layers)])\n\n        return map\n", "deepspeed/inference/v2/model_implementations/qwen_v2/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .policy import Qwen2Policy\n", "deepspeed/inference/v2/model_implementations/common_parameters/norm_parameters.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\n\nfrom ...model_implementations.parameter_base import ParameterBase\n\"\"\"\nCommon Attention Output Parameter Patterns\n\"\"\"\n\n\nclass NormParameter(ParameterBase):\n    \"\"\"\n    Simple normalization container.\n    \"\"\"\n\n    params: torch.Tensor\n\n    def finalize(self) -> torch.Tensor:\n        return self.inference_model.transform_norm_param(self.params)\n", "deepspeed/inference/v2/model_implementations/common_parameters/mlp_parameters.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\n\nfrom ...model_implementations.parameter_base import ParameterBase\n\"\"\"\nMLP Parameter Containers\n\"\"\"\n\n\nclass MLP1Parameter(ParameterBase):\n    \"\"\"\n    First MLP projection weight container. This performs a straight pass-through to the\n    model implementation for transformation.\n    \"\"\"\n    params: torch.Tensor\n\n    def finalize(self) -> torch.Tensor:\n        # NOTE(cmikeh2): If we are gated but not in the format specified below, we should trigger a permutation here.\n        # I am not currently aware of any models that use this format (or how we should even detect it; probably should\n        # just be a different param entirely, but until then we'll just assume the format is correct).\n        return self.inference_model.transform_mlp_1_param(self.params)\n\n\nclass GatedMLPParameter(ParameterBase):\n    \"\"\"\n    Gated MLP projection container.\n    \"\"\"\n\n    gate_params: torch.Tensor\n    \"\"\"\n    Weight parameter for the gating matrix.\n    \"\"\"\n\n    up_params: torch.Tensor\n    \"\"\"\n    For lack of a better name, the non-gating weight parameters.\n    \"\"\"\n\n    def finalize(self) -> torch.Tensor:\n        \"\"\"\n        Our gated format (this is different from InferenceV1!) is to have the gate and activated neurons\n        interleaved. So if we have 4 output neurons (two effective neurons) with 4 input neurons, the finalized\n        parameter will look like:\n        [g0_0, g0_1, g0_2, g0_3]\n        [a0_0, a0_1, a0_2, a0_3]\n        [g1_0, g1_1, g1_2, g1_3]\n        [a1_0, a1_1, a1_2, a1_3]\n\n        As a reference, in inference v1, the format is:\n        [g0_0, g0_1, g0_2, g0_3]\n        [g1_0, g1_1, g1_2, g1_3]\n        [a0_0, a0_1, a0_2, a0_3]\n        [a1_0, a1_1, a1_2, a1_3]\n        \"\"\"\n        assert self.gate_params.shape[0] == self.up_params.shape[\n            0], \"Gated MLP parameters must have the same number of neurons.\"\n        total_neurons = self.gate_params.shape[0] + self.up_params.shape[0]\n\n        # flip the order if even with the correct tokenizer we get wrong output\n        #fused_param = torch.cat([self.up_params, self.gate_params], dim=-1).reshape(total_neurons, -1)\n        fused_param = torch.cat([self.gate_params, self.up_params], dim=-1).reshape(total_neurons, -1)\n        return self.inference_model.transform_mlp_1_param(fused_param)\n\n\nclass MLP2Parameter(ParameterBase):\n    \"\"\"\n    Second MLP projection weight container. This performs a straight pass-through to the\n    model implementation for transformation.\n    \"\"\"\n\n    params: torch.Tensor\n    \"\"\"\n    Full weight parameter.\n    \"\"\"\n\n    def finalize(self) -> torch.Tensor:\n        return self.inference_model.transform_mlp_2_param(self.params)\n", "deepspeed/inference/v2/model_implementations/common_parameters/qkv_parameters.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\n\nfrom ...model_implementations.parameter_base import ParameterBase\n\"\"\"\nCommon QKV Parameter Patterns\n\"\"\"\n\n\nclass FusedQKVParameter(ParameterBase):\n    \"\"\"\n    Traditional fused QKV parameters for QKV projection. This is functionally\n    a direct copy.\n\n    src_qkv_w shape: [3 * out_features, in_features]\n    qkv_w shape: [3 * out_features, in_features]\n    \"\"\"\n\n    params: torch.Tensor\n\n    def finalize(self) -> torch.Tensor:\n        return self.inference_model.transform_qkv_param(self.params)\n\n\nclass UnfusedQKVParameter(ParameterBase):\n    \"\"\"\n    QKV parameter container for unfused QKV projection.\n\n    src_param shapes: 3 x [out_features, in_features]\n    dst_param shape: [3 x out_features, in_features]\n    \"\"\"\n\n    q_params: torch.Tensor\n\n    k_params: torch.Tensor\n\n    v_params: torch.Tensor\n\n    def finalize(self):\n        fused_param = torch.cat([self.q_params, self.k_params, self.v_params], dim=0)\n        return self.inference_model.transform_qkv_param(fused_param)\n\n\ndef megatron_qkv_reshape(param: torch.Tensor, head_size: int, n_heads: int) -> torch.Tensor:\n    assert param.shape[0] == 3 * n_heads * head_size\n\n    all_heads = torch.chunk(param, chunks=3 * n_heads, dim=0)\n    q_heads = all_heads[::3]\n    k_heads = all_heads[1::3]\n    v_heads = all_heads[2::3]\n    return torch.cat([q_heads, k_heads, v_heads], dim=0)\n\n\nclass MegatronQKVParameter(ParameterBase):\n    \"\"\"\n    QKV parameter container for Megatron-style QKV projection. Megatron stores the parameter\n    as [n_heads, 3, head_size, in_features] whereas our inference system is built around\n    [3, n_heads, head_size, in_features]. This container handles the conversion.\n\n    Note: this container expects the model implementation to implement properties for\n    `head_size` and `n_heads`.\n\n    src_qkv_w shape: [3 * out_features, in_features]\n    qkv_w shape: [3 * out_features, in_features]\n    \"\"\"\n\n    params: torch.Tensor\n\n    def finalize(self) -> torch.Tensor:\n        head_size = self.inference_model.head_size\n        n_heads = self.inference_model.n_heads\n\n        transposed_param = megatron_qkv_reshape(self.params, head_size, n_heads)\n        return self.inference_model.transform_qkv_param(transposed_param)\n\n\ndef transform_gqa_megatron(src_param: torch.Tensor, head_size: int, n_q_heads: int, n_kv_heads: int) -> torch.Tensor:\n    assert src_param.shape[0] == (2 * n_kv_heads + n_q_heads) * head_size\n\n    head_ratio = n_q_heads // n_kv_heads\n\n    # Reshape to get the groups as the leading dimension\n    groups_leading_view = src_param.reshape(n_kv_heads, 2 + head_ratio, head_size, -1)\n    q_heads = groups_leading_view[:, :head_ratio, :, :].reshape(-1, groups_leading_view.shape[-1])\n    k_heads = groups_leading_view[:, head_ratio, :, :].reshape(-1, groups_leading_view.shape[-1])\n    v_heads = groups_leading_view[:, head_ratio + 1, :, :].reshape(-1, groups_leading_view.shape[-1])\n    # Squeeze will remove extra dimension for bias\n    return torch.cat([q_heads, k_heads, v_heads], dim=0).squeeze()\n\n\nclass GQAMegatronQKVParameter(ParameterBase):\n    \"\"\"\n    QKV parameter for Megatron-style QKV projection with GQA-style QKV projection. In this\n    storage format each of the groups is stored consecutively, so there will be multiple q_heads,\n    then one k head, and one v head.\n\n    Note: this container expects the model implementation to implement properties for\n    `head_size`, `n_q_heads`, and `n_kv_heads`.\n\n    src_qkv_w shape: [(2 * n_kv_heads + n_q_heads) * head_size, in_features]\n    qkv_w shape: [(2 * n_kv_heads + n_q_heads) * head_size, in_features]\n    \"\"\"\n\n    params: torch.Tensor\n\n    def finalize(self) -> torch.Tensor:\n        head_size = self.inference_model.head_size\n        n_q_heads = self.inference_model.n_heads_q\n        n_kv_heads = self.inference_model.n_heads_kv\n        transposed_param = transform_gqa_megatron(self.params, head_size, n_q_heads, n_kv_heads)\n        return self.inference_model.transform_qkv_param(transposed_param)\n", "deepspeed/inference/v2/model_implementations/common_parameters/moe_parameters.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\n\nfrom ...model_implementations.parameter_base import ParameterBase, ParamList\n\"\"\"\nMoe Parameters\n\nThese parameters are compatible with any model inheriting from ``DSMoETransformerModelBase``.\n\"\"\"\n\n\nclass MoEGatingWeightParameter(ParameterBase):\n    \"\"\"\n    Gating weight matrix.\n    \"\"\"\n\n    params: torch.Tensor\n    \"\"\"\n    Projection matrix from the input activations to the gate logits.\n    \"\"\"\n\n    def finalize(self) -> torch.Tensor:\n        return self.inference_model.transform_moe_gate_param(self.params)\n\n\nclass UnfusedMoEMLP1Parameter(ParameterBase):\n    \"\"\"\n    This container should be used when the experts are held in separate parameters\n    and need to be joined into a single group.\n    \"\"\"\n\n    experts: ParamList(\"n_experts\")  # noqa: F821\n\n    def finalize(self) -> torch.Tensor:\n        stacked_experts = torch.stack([p for p in self.experts], dim=0)\n        return self.inference_model.transform_moe_mlp_1_param(stacked_experts)\n\n\nclass UnfusedMoEMLP2Parameter(ParameterBase):\n    \"\"\"\n    This container should be used when the experts are held in separate parameters\n    and need to be joined into a single group.\n    \"\"\"\n\n    experts: ParamList(\"n_experts\")  # noqa: F821\n\n    def finalize(self) -> torch.Tensor:\n        stacked_experts = torch.stack([p for p in self.experts], dim=0)\n        return self.inference_model.transform_moe_mlp_2_param(stacked_experts)\n\n\nclass UnfusedMoEGatedMLPParameter(ParameterBase):\n    \"\"\"\n    MoE Parameter for a gated activation function in which the gating matrix is not\n    fused in the same parameter as the non-gating matrix.\n\n    This is a stacked version of the ``GatedMLPParameter``. Please see that class for more\n    documentation on the layout of the parameters.\n    \"\"\"\n\n    gating_experts: ParamList(\"n_experts\")  # noqa: F821\n\n    up_experts: ParamList(\"n_experts\")  # noqa: F821\n\n    def finalize(self) -> torch.Tensor:\n        transposed_experts = []\n        for gate, up in zip(self.gating_experts, self.up_experts):\n            assert gate.shape[0] == up.shape[0], \"Gated MLP parameters must have the same number of neurons.\"\n            total_neurons = gate.shape[0] + up.shape[0]\n            fused_expert = torch.cat([gate, up], dim=-1).reshape(total_neurons, -1)\n            transposed_experts.append(fused_expert)\n\n        stacked_experts = torch.stack(transposed_experts, dim=0)\n        return self.inference_model.transform_moe_mlp_1_param(stacked_experts)\n", "deepspeed/inference/v2/model_implementations/common_parameters/unembed_parameters.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\n\nfrom ...model_implementations.parameter_base import ParameterBase\n\"\"\"\nUnembedding containers.\n\"\"\"\n\n\nclass UnembedParameter(ParameterBase):\n    \"\"\"\n    Unembedding parameter. This will likely be mapped to the same original weight in the model as the\n    embedding, but we have a different preferred sharding approach.\n    \"\"\"\n\n    params: torch.Tensor\n    \"\"\"\n    Unembedding parameter of shape [vocab_size, model_dim].\n    \"\"\"\n\n    def finalize(self) -> torch.Tensor:\n        return self.inference_model.transform_unembed_param(self.params)\n", "deepspeed/inference/v2/model_implementations/common_parameters/embedding_parameters.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\n\nfrom ...model_implementations.parameter_base import ParameterBase\n\"\"\"\nEmbedding containers.\n\"\"\"\n\n\nclass EmbeddingParameter(ParameterBase):\n    \"\"\"\n    Embedding container. This should be safe to use for all types of embeddings (i.e. word, position,\n    and token type).\n    \"\"\"\n\n    params: torch.Tensor\n    \"\"\"\n    Vocabulary parameter of shape [vocab_size, model_dim].\n    \"\"\"\n\n    def finalize(self) -> torch.Tensor:\n        return self.inference_model.transform_embedding_param(self.params)\n", "deepspeed/inference/v2/model_implementations/common_parameters/attn_output_parameters.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\n\nfrom ...model_implementations.parameter_base import ParameterBase\n\"\"\"\nCommon Attention Output Parameter Patterns\n\"\"\"\n\n\nclass AttentionOutputParameter(ParameterBase):\n    \"\"\"\n    Attention output parameter container.\n\n    Note: The differentiation for something like GQA for this matrix is primarily\n    encompassed in the sharding logic, which is currently expected to be performed by\n    the model implementation.\n    \"\"\"\n\n    params: torch.Tensor\n    \"\"\"\n    Unsharded attention output parameter of shape [model_dim, model_dim]\n    \"\"\"\n\n    def finalize(self) -> torch.Tensor:\n        return self.inference_model.transform_attn_out_param(self.params)\n", "deepspeed/inference/v2/model_implementations/common_parameters/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .attn_output_parameters import *\nfrom .embedding_parameters import *\nfrom .mlp_parameters import *\nfrom .moe_parameters import *\nfrom .norm_parameters import *\nfrom .qkv_parameters import *\nfrom .unembed_parameters import *\nfrom .invfreq_parameters import *\n", "deepspeed/inference/v2/model_implementations/common_parameters/invfreq_parameters.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\n\nfrom ...model_implementations.parameter_base import ParameterBase\n\"\"\"\nCommon InvFreq Parameter Patterns\n\"\"\"\n\n\nclass InvFreqParameter(ParameterBase):\n\n    params: torch.Tensor\n\n    def finalize(self) -> torch.Tensor:\n        return self.params.to(self.inference_model.activation_dtype.value)\n", "deepspeed/inference/v2/model_implementations/mixtral/model.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Iterable, Optional, Tuple\n\nimport torch\n\nimport deepspeed.comm as dist\n\nfrom ...allocator import empty_from\nfrom ...config_v2 import RaggedInferenceEngineConfig\nfrom ...inference_utils import ActivationType, DtypeEnum\nfrom ...model_implementations import *\nfrom ...modules.configs import *\nfrom ...modules.interfaces import *\nfrom ...ragged import RaggedBatchWrapper\nfrom ..inference_model_base import (\n    DSModelImplementationConfig,\n    MPType,\n)\n\nfrom .container import MixtralNonTransformerContainer, MixtralTransformerContainer\n\n\nclass MixtralInferenceModel(DSMoETransformerModelBase):\n    \"\"\"\n    Inference model implementation for Mixtral models.\n    \"\"\"\n\n    _non_transformer: Optional[MixtralNonTransformerContainer]\n    \"\"\"\n    Embed + unembed container. Specializing the type annotation.\n    \"\"\"\n\n    _transformer: Optional[Iterable[MixtralTransformerContainer]]\n    \"\"\"\n    Per-layer transformer container. Specializing the type annotation.\n    \"\"\"\n    \"\"\"\n    Properties ineherited from `DSInferenceModelBase`\n    \"\"\"\n\n    @property\n    def max_sequence_length(self) -> int:\n        return self._config.max_position_embeddings\n\n    \"\"\"\n    Properties ineherited from `DSTransformerModelBase`\n    \"\"\"\n\n    @property\n    def num_layers(self) -> int:\n        return self._config.num_hidden_layers\n\n    @property\n    def model_dim(self) -> int:\n        return self._config.hidden_size\n\n    @property\n    def vocab_size(self) -> int:\n        return self._config.vocab_size\n\n    @property\n    def head_size(self) -> int:\n        return self.model_dim // self.n_heads\n\n    @property\n    def n_heads(self) -> int:\n        return self._config.num_attention_heads\n\n    @property\n    def intermediate_dim(self) -> int:\n        return self._config.intermediate_size\n\n    @property\n    def n_heads_kv(self) -> int:\n        return self._config.num_key_value_heads\n\n    @property\n    def activation_dtype(self) -> DtypeEnum:\n        if self._config.torch_dtype == torch.float16:\n            return DtypeEnum.fp16\n        elif self._config.torch_dtype == torch.bfloat16:\n            return DtypeEnum.bf16\n        else:\n            raise NotImplementedError(\"Only fp16 and bf16 are supported\")\n\n    @property\n    def mlp_activation_fn(self) -> ActivationType:\n        activation = self._config.hidden_act.lower()\n        if activation == \"gelu\":\n            return ActivationType.GEGLU\n        elif activation == \"relu\":\n            return ActivationType.ReGLU\n        elif activation == \"gegelu\":\n            return ActivationType.GEGLU\n        elif activation == \"silu\":\n            return ActivationType.SiGLU\n        else:\n            raise NotImplementedError(f\"Activation {activation} not supported\")\n\n    @property\n    def norm_type(self) -> NormTypeEnum:\n        return NormTypeEnum.RMSNorm\n\n    @property\n    def positional_embedding_type(self) -> PositionalEmbeddingType:\n        return PositionalEmbeddingType.rotate_half\n\n    @property\n    def positional_embedding_config(self) -> Optional[RotateHalfConfig]:\n        \"\"\"\n        The positional embedding configuration for the model.\n        \"\"\"\n        return RotateHalfConfig(theta_base=self._config.rope_theta)\n\n    \"\"\"\n    Inherited from `DSMoETransformerModelBase`\n    \"\"\"\n\n    @property\n    def n_experts(self) -> int:\n        return self._config.num_local_experts\n\n    @property\n    def n_top_k(self) -> int:\n        return self._config.num_experts_per_tok\n\n    @property\n    def normalize_expert_scores(self) -> bool:\n        return True\n\n    \"\"\"\n    Model implementation\n    \"\"\"\n\n    def __init__(self, config: DSModelImplementationConfig, engine_config: RaggedInferenceEngineConfig,\n                 base_mp_group: MPType) -> None:\n        \"\"\"\n        Base implementation for initialization. By default, this will initialize\n        the traditional components of a transformer model:\n            - Embedding\n            - QKV projection\n            - Self attention\n            - Attention output projection\n            - Feed forward network\n            - Normalization\n            - Unembedding\n\n        Arguments:\n            config (DSModelImplementationConfig): Model-specific configuration. No assumptions\n                should be made about this config that are not closely tied to the specific\n                model implementation.\n            engine_config (RaggedInferenceEngineConfig): Engine configuration.\n            base_mp_group (MPType): Base communication group for Tensor-parallel inference.\n        \"\"\"\n        super().__init__(config, engine_config, base_mp_group)\n\n        self.make_norm_layer()\n        self.make_qkv_layer()\n        self.make_attn_layer()\n        self.make_attn_out_layer()\n        self.make_moe_layer()\n        self.make_embedding_layer()\n        self.make_unembedding_layer()\n        self._kv_cache_config = None\n\n    def _forward_embed(self, ragged_batch: RaggedBatchWrapper) -> torch.Tensor:\n        \"\"\"\n        Performs the embedding lookup prior to running the transformer of the model.\n\n        Arguments:\n            ragged_batch (RaggedBatchWrapper): The batch to embed.\n\n        Returns:\n            torch.Tensor: The embedded batch.\n        \"\"\"\n        embed = self.embed(ragged_batch, self._non_transformer.word_emb)\n\n        if embed.shape[-1] != self.model_dim:\n            raise ValueError(f\"Embedding output shape {embed.shape} does not match model_dim {self.model_dim}\")\n\n        return embed\n\n    def _forward_transformer(self, layer_idx: int, residual: torch.Tensor, hidden_states: torch.Tensor,\n                             ragged_batch_info: RaggedBatchWrapper) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Executes one (slightly offset) layer of the transformer. This implementation does a peak-ahead\n        optimization to fuse the layer norm of the next layer into the current layer.\n\n        Arguments:\n            layer_idx (int): The index of the layer to execute.\n            residual (torch.Tensor): The residual tensor from the previous layer.\n            hidden_states (torch.Tensor): The hidden states from the previous layer. This is the\n                hidden states after pre normalization.\n            ragged_batch_info (RaggedBatchWrapper): The batch metadata.\n        \"\"\"\n        # TODO(cmikeh2): Distribute ragged_batch_info to all modules\n\n        cur_params = self._transformer[layer_idx]\n        kv_cache = self.state_manager.get_cache(layer_idx)\n\n        hidden_states = self.qkv(hidden_states, cur_params.qkv_w)\n        hidden_states = self.attn(hidden_states, kv_cache, ragged_batch_info)\n        hidden_states = self.attn_out(hidden_states, cur_params.attn_out_w)\n\n        if self.tp_size > 1:\n            dist.all_reduce(hidden_states, group=self._base_mp_group)\n\n        residual, hidden_states = self.norm(residual, hidden_states, cur_params.mlp_norm_gamma)\n\n        hidden_states = self.moe(hidden_states, ragged_batch_info, cur_params.moe_gate, cur_params.moe_mlp_1,\n                                 cur_params.moe_mlp_2)\n\n        if self.tp_size > 1:\n            dist.all_reduce(hidden_states, group=self._base_mp_group)\n\n        if layer_idx != self.num_layers - 1:\n            next_params = self._transformer[layer_idx + 1]\n            residual, hidden_states = self.norm(residual, hidden_states, next_params.attn_norm_gamma)\n        else:\n            # On last layer, we just need to perform the residual add. Adding into the residual\n            # here is safe.\n            residual.add_(hidden_states)\n\n        return residual, hidden_states\n\n    def _forward_unembed(self, hidden_states: torch.Tensor, ragged_batch_info: RaggedBatchWrapper) -> torch.Tensor:\n        \"\"\"\n        Performs unembedding of the hidden states to logits. This will only sample the final\n        token of each sequence.\n        \"\"\"\n        logits = self.unembed(hidden_states,\n                              self._non_transformer.word_unembed,\n                              ragged_batch_info,\n                              gamma=self._non_transformer.final_norm)\n\n        if self.tp_size > 1:\n            comm_buffer = empty_from(self._comm_logits, (self.tp_size, logits.shape[0], logits.shape[1]))\n            full_logits = empty_from(self._return_logits, (logits.shape[0], self.vocab_size))\n\n            dist.all_gather_into_tensor(comm_buffer, logits, group=self._base_mp_group)\n\n            full_logits.copy_(comm_buffer.permute(1, 0, 2).reshape(logits.shape[0], self.vocab_size))\n\n            return full_logits\n        else:\n            return logits\n\n    def forward(self, wrapped_batch: RaggedBatchWrapper) -> torch.Tensor:\n\n        residual = self._forward_embed(wrapped_batch)\n\n        residual, hidden_states = self.norm(residual, None, self._transformer[0].attn_norm_gamma, beta=None)\n\n        for layer_idx in range(self.num_layers):\n            residual, hidden_states = self._forward_transformer(layer_idx, residual, hidden_states, wrapped_batch)\n\n        return self._forward_unembed(residual, wrapped_batch)\n", "deepspeed/inference/v2/model_implementations/mixtral/container.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\n# Create a container object to save model-specific tensors using the policy file above.\n\nfrom deepspeed.inference.v2.model_implementations.common_parameters import *\nfrom deepspeed.inference.v2.model_implementations.layer_container_base import LayerContainer\n\n\nclass MixtralTransformerContainer(LayerContainer):\n\n    qkv_w: UnfusedQKVParameter\n    attn_out_w: AttentionOutputParameter\n    moe_gate: MoEGatingWeightParameter\n    moe_mlp_1: UnfusedMoEGatedMLPParameter\n    moe_mlp_2: UnfusedMoEMLP2Parameter\n    attn_norm_gamma: NormParameter\n    mlp_norm_gamma: NormParameter\n\n    PARAM_MAPPING = {\n        \"input_layernorm.weight\": \"attn_norm_gamma.params\",\n        \"post_attention_layernorm.weight\": \"mlp_norm_gamma.params\",\n        \"self_attn.q_proj.weight\": \"qkv_w.q_params\",\n        \"self_attn.k_proj.weight\": \"qkv_w.k_params\",\n        \"self_attn.v_proj.weight\": \"qkv_w.v_params\",\n        \"self_attn.o_proj.weight\": \"attn_out_w.params\",\n        \"block_sparse_moe.gate.weight\": \"moe_gate.params\",\n        \"block_sparse_moe.experts.*.w1.weight\": \"moe_mlp_1.gating_experts\",\n        \"block_sparse_moe.experts.*.w3.weight\": \"moe_mlp_1.up_experts\",\n        \"block_sparse_moe.experts.*.w2.weight\": \"moe_mlp_2.experts\",\n    }\n\n\nclass MixtralNonTransformerContainer(LayerContainer):\n\n    word_emb: EmbeddingParameter\n    word_unembed: UnembedParameter\n    final_norm: NormParameter\n\n    PARAM_MAPPING = {\n        \"model.embed_tokens.weight\": \"word_emb.params\",\n        \"lm_head.weight\": \"word_unembed.params\",\n        \"model.norm.weight\": \"final_norm.params\",\n    }\n", "deepspeed/inference/v2/model_implementations/mixtral/policy.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Any\n\nfrom ...config_v2 import RaggedInferenceEngineConfig\nfrom ..inference_policy_base import ContainerMap, InferenceV2Policy\nfrom .container import MixtralTransformerContainer, MixtralNonTransformerContainer\nfrom .model import MixtralInferenceModel\n\n\nclass MixtralPolicy(InferenceV2Policy):\n\n    def instantiate_model(self, engine_config: RaggedInferenceEngineConfig, mp_group: Any) -> MixtralInferenceModel:\n        return MixtralInferenceModel(config=self._model_config, engine_config=engine_config, base_mp_group=mp_group)\n\n    def build_container_map(self) -> ContainerMap:\n\n        map = ContainerMap()\n\n        transformer_containers = [MixtralTransformerContainer(self.model) for _ in range(self.model.num_layers)]\n\n        map.set_transformer_params(['model.layers'], transformer_containers)\n\n        map.set_non_transformer_params(MixtralNonTransformerContainer(self.model))\n\n        map.set_unmapped_params([])\n\n        return map\n", "deepspeed/inference/v2/model_implementations/mixtral/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .policy import MixtralPolicy\n", "deepspeed/inference/v2/model_implementations/phi/model.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Iterable, Optional, Tuple\n\nimport torch\n\nimport deepspeed.comm as dist\n\nfrom ...allocator import empty_from\nfrom ...inference_utils import ActivationType, DtypeEnum\nfrom .. import *\nfrom ...modules.configs import *\nfrom ...modules.interfaces import *\nfrom ...ragged import RaggedBatchWrapper\n\nfrom .containers import PhiNonTransformerContainer, PhiTransformerContainer\n\n\nclass PhiInferenceModel(DSTransformerModelBase):\n    \"\"\"\n    Inference model implementation for ragged batching for Llama-2 models.\n    \"\"\"\n\n    _non_transformer: Optional[PhiNonTransformerContainer]\n    \"\"\"\n    Embed + unembed container. Specializing the type annotation.\n    \"\"\"\n\n    _transformer: Optional[Iterable[PhiTransformerContainer]]\n    \"\"\"\n    Per-layer transformer container. Specializing the type annotation.\n    \"\"\"\n    \"\"\"\n    Properties inherited from `DSInferenceModelBase`\n    \"\"\"\n\n    @property\n    def max_sequence_length(self) -> int:\n        return self._config.max_seq_length\n\n    \"\"\"\n    Properties inherited from `DSTransformerModelBase`\n    \"\"\"\n\n    @property\n    def num_layers(self) -> int:\n        return self._config.num_hidden_layers\n\n    @property\n    def model_dim(self) -> int:\n        return self._config.hidden_size\n\n    @property\n    def vocab_size(self) -> int:\n        return self._config.vocab_size\n\n    @property\n    def head_size(self) -> int:\n        return self.model_dim // self.n_heads\n\n    @property\n    def n_heads(self) -> int:\n        return self._config.num_attention_heads\n\n    @property\n    def intermediate_dim(self) -> int:\n        return self._config.intermediate_size\n\n    @property\n    def n_heads_kv(self) -> int:\n        return self._config.num_key_value_heads\n\n    @property\n    def activation_dtype(self) -> DtypeEnum:\n        if self._config.torch_dtype == torch.float16:\n            return DtypeEnum.fp16\n        elif self._config.torch_dtype == torch.bfloat16:\n            return DtypeEnum.bf16\n        else:\n            raise NotImplementedError(\"Only fp16 and bf16 are supported\")\n\n    @property\n    def mlp_activation_fn(self) -> ActivationType:\n        return ActivationType.GELU\n\n    @property\n    def norm_type(self) -> NormTypeEnum:\n        return NormTypeEnum.LayerNorm\n\n    @property\n    def positional_embedding_type(self) -> PositionalEmbeddingType:\n        return PositionalEmbeddingType.rotate_half\n\n    @property\n    def positional_embedding_config(self) -> Optional[RotateHalfConfig]:\n        rotary_dim = int(self._config.partial_rotary_factor * self.head_size)\n        return RotateHalfConfig(rotate_dim=rotary_dim, theta_base=self._config.rope_theta)\n\n    \"\"\"\n    Forward implementations\n    \"\"\"\n\n    def _forward_embed(self, ragged_batch: RaggedBatchWrapper) -> torch.Tensor:\n        \"\"\"\n        Performs the embedding lookup prior to running the transformer of the model.\n\n        Arguments:\n            ragged_batch (RaggedBatchWrapper): The batch to embed.\n\n        Returns:\n            torch.Tensor: The embedded batch.\n        \"\"\"\n        embed = self.embed(ragged_batch, self._non_transformer.word_emb)\n\n        if embed.shape[-1] != self.model_dim:\n            raise ValueError(f\"Embedding output shape {embed.shape} does not match model_dim {self.model_dim}\")\n\n        return embed\n\n    def _forward_transformer_layer(self, layer_idx: int, residual: torch.Tensor, hidden_states: torch.Tensor,\n                                   ragged_batch_info: RaggedBatchWrapper) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Executes one (slightly offset) layer of the transformer. This implementation does a peak-ahead\n        optimization to fuse the layer norm of the next layer into the current layer.\n\n        Arguments:\n            layer_idx (int): The index of the layer to execute.\n            residual (torch.Tensor): The residual tensor from the previous layer.\n            hidden_states (torch.Tensor): The hidden states from the previous layer. This is the\n                hidden states after pre normalization.\n            ragged_batch_info (RaggedBatchWrapper): The batch metadata.\n        \"\"\"\n        cur_params = self._transformer[layer_idx]\n        kv_cache = self.state_manager.get_cache(layer_idx)\n\n        attn_ln_out = hidden_states\n        attn_hidden_state = self.qkv(attn_ln_out, cur_params.qkv_w, b=cur_params.qkv_b)\n        attn_hidden_state = self.attn(attn_hidden_state, kv_cache, ragged_batch_info)\n        attention_output = self.attn_out(attn_hidden_state, cur_params.attn_out_w, b=cur_params.attn_out_b)\n\n        mlp_ln_out = hidden_states\n        mlp_hidden_state = self.mlp_1(mlp_ln_out, cur_params.mlp_1_w, b=cur_params.mlp_1_b)\n        mlp_output = self.mlp_2(mlp_hidden_state, cur_params.mlp_2_w, b=cur_params.mlp_2_b)\n\n        mlp_output.add_(attention_output)\n\n        if self.tp_size > 1:\n            dist.all_reduce(mlp_output, group=self._base_mp_group)\n\n        if layer_idx != self.num_layers - 1:\n            next_params = self._transformer[layer_idx + 1]\n            residual, mlp_output = self.norm(residual, mlp_output, next_params.ln_gamma, beta=next_params.ln_beta)\n        else:\n            # On last layer, we just need to perform the residual add. Adding into the residual\n            # here is safe.\n            residual.add_(mlp_output)\n\n        return residual, mlp_output\n\n    def _forward_unembed(self, hidden_states: torch.Tensor, ragged_batch_info: RaggedBatchWrapper) -> torch.Tensor:\n        \"\"\"\n        Performs unembedding of the hidden states to logits. This will only sample the final\n        token of each sequence.\n        \"\"\"\n        logits = self.unembed(hidden_states,\n                              self._non_transformer.word_unembed_w,\n                              ragged_batch_info,\n                              bias=self._non_transformer.word_unembed_b,\n                              gamma=self._non_transformer.final_norm_gamma,\n                              beta=self._non_transformer.final_norm_beta)\n\n        if self.tp_size > 1:\n            comm_buffer = empty_from(self._comm_logits, (self.tp_size, logits.shape[0], logits.shape[1]))\n            full_logits = empty_from(self._return_logits, (logits.shape[0], self.vocab_size))\n\n            dist.all_gather_into_tensor(comm_buffer, logits, group=self._base_mp_group)\n\n            full_logits.copy_(comm_buffer.permute(1, 0, 2).reshape(logits.shape[0], self.vocab_size))\n\n            return full_logits\n        else:\n            return logits\n\n    def forward(self, wrapped_batch: RaggedBatchWrapper) -> torch.Tensor:\n        residual = self._forward_embed(wrapped_batch)\n\n        residual, hidden_states = self.norm(residual,\n                                            None,\n                                            gamma=self._transformer[0].ln_gamma,\n                                            beta=self._transformer[0].ln_beta)\n\n        for layer_idx in range(self.num_layers):\n            residual, hidden_states = self._forward_transformer_layer(layer_idx, residual, hidden_states,\n                                                                      wrapped_batch)\n\n        return self._forward_unembed(residual, wrapped_batch)\n", "deepspeed/inference/v2/model_implementations/phi/containers.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\n# Create a container object to save model-specific tensors using the policy file above.\n\nfrom ..common_parameters import *\nfrom ..layer_container_base import LayerContainer\n'''\n # HF Phi-2 model looks like this:\n\nPhiForCausalLM(\n  (model): PhiModel(\n    (embed_tokens): Embedding(51200, 2560)\n    (embed_dropout): Dropout(p=0.0, inplace=False)\n    (layers): ModuleList(\n      (0-31): 32 x PhiDecoderLayer(\n        (self_attn): PhiAttention(\n          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n          (dense): Linear(in_features=2560, out_features=2560, bias=True)\n          (rotary_emb): PhiRotaryEmbedding()\n        )\n        (mlp): PhiMLP(\n          (activation_fn): NewGELUActivation()\n          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n        )\n        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n        (resid_dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n)\n'''\n\n\nclass PhiTransformerContainer(LayerContainer):\n    \"\"\"\n        Transformer layer container for the Phi model.\n    \"\"\"\n    qkv_w: UnfusedQKVParameter\n    qkv_b: UnfusedQKVParameter\n    attn_out_w: AttentionOutputParameter\n    attn_out_b: AttentionOutputParameter\n    mlp_1_w: MLP1Parameter\n    mlp_1_b: MLP1Parameter\n    mlp_2_w: MLP2Parameter\n    mlp_2_b: MLP2Parameter\n    ln_gamma: NormParameter\n    ln_beta: NormParameter\n\n    PARAM_MAPPING = {\n        \"self_attn.q_proj.weight\": \"qkv_w.q_params\",\n        \"self_attn.k_proj.weight\": \"qkv_w.k_params\",\n        \"self_attn.v_proj.weight\": \"qkv_w.v_params\",\n        \"self_attn.q_proj.bias\": \"qkv_b.q_params\",\n        \"self_attn.k_proj.bias\": \"qkv_b.k_params\",\n        \"self_attn.v_proj.bias\": \"qkv_b.v_params\",\n        \"self_attn.dense.weight\": \"attn_out_w.params\",\n        \"self_attn.dense.bias\": \"attn_out_b.params\",\n        \"mlp.fc1.weight\": \"mlp_1_w.params\",\n        \"mlp.fc1.bias\": \"mlp_1_b.params\",\n        \"mlp.fc2.weight\": \"mlp_2_w.params\",\n        \"mlp.fc2.bias\": \"mlp_2_b.params\",\n        \"input_layernorm.weight\": \"ln_gamma.params\",\n        \"input_layernorm.bias\": \"ln_beta.params\",\n    }\n\n\nclass PhiNonTransformerContainer(LayerContainer):\n    \"\"\"\n        Non-Transformer layer container for the Phi model.\n    \"\"\"\n    word_emb: EmbeddingParameter\n    word_unembed_w: UnembedParameter\n    word_unembed_b: UnembedParameter\n    final_norm_gamma: NormParameter\n    final_norm_beta: NormParameter\n\n    PARAM_MAPPING = {\n        \"model.embed_tokens.weight\": \"word_emb.params\",\n        \"model.final_layernorm.weight\": \"final_norm_gamma.params\",\n        \"model.final_layernorm.bias\": \"final_norm_beta.params\",\n        \"lm_head.weight\": \"word_unembed_w.params\",\n        \"lm_head.bias\": \"word_unembed_b.params\",\n    }\n", "deepspeed/inference/v2/model_implementations/phi/policy.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Any\n\nfrom ...config_v2 import RaggedInferenceEngineConfig\nfrom ..inference_policy_base import ContainerMap, InferenceV2Policy\nfrom .containers import PhiNonTransformerContainer, PhiTransformerContainer\nfrom .model import PhiInferenceModel\n\n\nclass PhiPolicy(InferenceV2Policy):\n\n    def instantiate_model(self, engine_config: RaggedInferenceEngineConfig, mp_group: Any) -> PhiInferenceModel:\n        return PhiInferenceModel(config=self._model_config, engine_config=engine_config, base_mp_group=mp_group)\n\n    def build_container_map(self) -> ContainerMap:\n        map = ContainerMap()\n\n        trans_container_cls = PhiTransformerContainer\n        transformer_containers = [trans_container_cls(self.model) for _ in range(self.model.num_layers)]\n\n        map.set_transformer_params(['model.layers'], transformer_containers)\n\n        map.set_non_transformer_params(PhiNonTransformerContainer(self.model))\n\n        map.set_unmapped_params(\n            [f'model.layers.{i}.self_attn.rotary_emb.inv_freq' for i in range(self.model.num_layers)])\n\n        return map\n", "deepspeed/inference/v2/model_implementations/phi/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .policy import PhiPolicy\n", "deepspeed/inference/v2/model_implementations/falcon/model.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Iterable, Optional, Tuple\n\nimport torch\n\nimport deepspeed.comm as dist\n\nfrom ...allocator import empty_from\nfrom ...inference_utils import ActivationType, DtypeEnum\nfrom .. import *\nfrom ...modules.configs import *\nfrom ...modules.interfaces import *\nfrom ...ragged import RaggedBatchWrapper\n\nfrom .container import FalconNonTransformerContainer, FalconTransformerContainer\n\n\nclass FalconInferenceModel(DSTransformerModelBase):\n    \"\"\"\n    Inference model implementation for ragged batching for Llama-2 models.\n    \"\"\"\n\n    _non_transformer: Optional[FalconNonTransformerContainer]\n    \"\"\"\n    Embed + unembed container. Specializing the type annotation.\n    \"\"\"\n\n    _transformer: Optional[Iterable[FalconTransformerContainer]]\n    \"\"\"\n    Per-layer transformer container. Specializing the type annotation.\n    \"\"\"\n    \"\"\"\n    Properties inherited from `DSInferenceModelBase`\n    \"\"\"\n\n    @property\n    def max_sequence_length(self) -> int:\n        return self._config.max_seq_length\n\n    \"\"\"\n    Properties inherited from `DSTransformerModelBase`\n    \"\"\"\n\n    @property\n    def num_layers(self) -> int:\n        return self._config.num_hidden_layers\n\n    @property\n    def model_dim(self) -> int:\n        return self._config.hidden_size\n\n    @property\n    def vocab_size(self) -> int:\n        return self._config.vocab_size\n\n    @property\n    def head_size(self) -> int:\n        return self.model_dim // self.n_heads\n\n    @property\n    def n_heads(self) -> int:\n        return self._config.num_attention_heads\n\n    @property\n    def intermediate_dim(self) -> int:\n        return 4 * self._config.hidden_size\n\n    @property\n    def n_heads_kv(self) -> int:\n        return self._config.num_kv_heads if (self._config.new_decoder_architecture\n                                             or not self._config.multi_query) else 1\n\n    @property\n    def activation_dtype(self) -> DtypeEnum:\n        if self._config.torch_dtype == torch.float16:\n            return DtypeEnum.fp16\n        elif self._config.torch_dtype == torch.bfloat16:\n            return DtypeEnum.bf16\n        else:\n            raise NotImplementedError(\"Only fp16 and bf16 are supported\")\n\n    @property\n    def mlp_activation_fn(self) -> ActivationType:\n        return ActivationType.GELU\n\n    @property\n    def norm_type(self) -> NormTypeEnum:\n        return NormTypeEnum.LayerNorm\n\n    @property\n    def positional_embedding_type(self) -> PositionalEmbeddingType:\n        return PositionalEmbeddingType.rotate_half\n\n    @property\n    def positional_embedding_config(self) -> RotateHalfConfig:\n        \"\"\"\n        The positional embedding configuration for the model.\n        \"\"\"\n        return RotateHalfConfig()\n\n    \"\"\"\n    Forward implementations\n    \"\"\"\n\n    def _forward_embed(self, ragged_batch: RaggedBatchWrapper) -> torch.Tensor:\n        \"\"\"\n        Performs the embedding lookup prior to running the transformer of the model.\n\n        Arguments:\n            ragged_batch (RaggedBatchWrapper): The batch to embed.\n\n        Returns:\n            torch.Tensor: The embedded batch.\n        \"\"\"\n        embed = self.embed(ragged_batch, self._non_transformer.word_emb)\n\n        if embed.shape[-1] != self.model_dim:\n            raise ValueError(f\"Embedding output shape {embed.shape} does not match model_dim {self.model_dim}\")\n\n        return embed\n\n    def _forward_transformer_layer(self, layer_idx: int, residual: torch.Tensor, hidden_states: torch.Tensor,\n                                   ragged_batch_info: RaggedBatchWrapper) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Executes one (slightly offset) layer of the transformer. This implementation does a peak-ahead\n        optimization to fuse the layer norm of the next layer into the current layer.\n\n        Arguments:\n            layer_idx (int): The index of the layer to execute.\n            residual (torch.Tensor): The residual tensor from the previous layer.\n            hidden_states (torch.Tensor): The hidden states from the previous layer. This is the\n                hidden states after pre normalization.\n            ragged_batch_info (RaggedBatchWrapper): The batch metadata.\n        \"\"\"\n        assert self.config.parallel_attn, \"Only parallel attention implementation is supported\"\n\n        cur_params = self._transformer[layer_idx]\n        kv_cache = self.state_manager.get_cache(layer_idx)\n\n        attn_ln_out = hidden_states\n        attn_hidden_state = self.qkv(attn_ln_out, cur_params.qkv_w, b=None)\n        attn_hidden_state = self.attn(attn_hidden_state, kv_cache, ragged_batch_info)\n        attention_output = self.attn_out(attn_hidden_state, cur_params.attn_out_w, b=None)\n\n        if self.config.new_decoder_architecture:\n            residual, mlp_ln_out = self.norm(residual,\n                                             None,\n                                             gamma=cur_params.ln_mlp_gamma,\n                                             beta=cur_params.ln_mlp_beta)\n        else:\n            mlp_ln_out = hidden_states\n\n        mlp_hidden_state = self.mlp_1(mlp_ln_out, cur_params.mlp_1_w, b=None)\n        mlp_output = self.mlp_2(mlp_hidden_state, cur_params.mlp_2_w, b=None)\n\n        mlp_output.add_(attention_output)\n\n        if self.tp_size > 1:\n            dist.all_reduce(mlp_output, group=self._base_mp_group)\n\n        if layer_idx != self.num_layers - 1:\n            next_params = self._transformer[layer_idx + 1]\n            residual, mlp_output = self.norm(residual,\n                                             mlp_output,\n                                             next_params.ln_attn_gamma,\n                                             beta=next_params.ln_attn_beta)\n        else:\n            # On last layer, we just need to perform the residual add. Adding into the residual\n            # here is safe.\n            residual.add_(mlp_output)\n\n        return residual, mlp_output\n\n    def _forward_unembed(self, hidden_states: torch.Tensor, ragged_batch_info: RaggedBatchWrapper) -> torch.Tensor:\n        \"\"\"\n        Performs unembedding of the hidden states to logits. This will only sample the final\n        token of each sequence.\n        \"\"\"\n        logits = self.unembed(hidden_states,\n                              self._non_transformer.word_unembed,\n                              ragged_batch_info,\n                              gamma=self._non_transformer.final_norm_gamma,\n                              beta=self._non_transformer.final_norm_beta)\n\n        if self.tp_size > 1:\n            comm_buffer = empty_from(self._comm_logits, (self.tp_size, logits.shape[0], logits.shape[1]))\n            full_logits = empty_from(self._return_logits, (logits.shape[0], self.vocab_size))\n\n            dist.all_gather_into_tensor(comm_buffer, logits, group=self._base_mp_group)\n\n            full_logits.copy_(comm_buffer.permute(1, 0, 2).reshape(logits.shape[0], self.vocab_size))\n\n            return full_logits\n        else:\n            return logits\n\n    def forward(self, wrapped_batch: RaggedBatchWrapper) -> torch.Tensor:\n        residual = self._forward_embed(wrapped_batch)\n\n        residual, hidden_states = self.norm(residual,\n                                            None,\n                                            gamma=self._transformer[0].ln_attn_gamma,\n                                            beta=self._transformer[0].ln_attn_beta)\n\n        for layer_idx in range(self.num_layers):\n            residual, hidden_states = self._forward_transformer_layer(layer_idx, residual, hidden_states,\n                                                                      wrapped_batch)\n\n        return self._forward_unembed(residual, wrapped_batch)\n", "deepspeed/inference/v2/model_implementations/falcon/container.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\n# Create a container object to save model-specific tensors using the policy file above.\n\nfrom ..common_parameters import *\nfrom ..layer_container_base import LayerContainer\n'''\n # HF Falcon 7b model looks like this:\n\nFalconForCausalLM(\n  (transformer): FalconModel(\n    (word_embeddings): Embedding(65024, 4544)\n    (h): ModuleList(\n      (0-31): 32 x FalconDecoderLayer(\n        (self_attention): FalconAttention(\n          (maybe_rotary): FalconRotaryEmbedding()\n          (query_key_value): FalconLinear(in_features=4544, out_features=4672, bias=False)\n          (dense): FalconLinear(in_features=4544, out_features=4544, bias=False)\n          (attention_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (mlp): FalconMLP(\n          (dense_h_to_4h): FalconLinear(in_features=4544, out_features=18176, bias=False)\n          (act): GELU(approximate='none')\n          (dense_4h_to_h): FalconLinear(in_features=18176, out_features=4544, bias=False)\n        )\n        (input_layernorm): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n    (ln_f): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=4544, out_features=65024, bias=False)\n)\n'''\n\n\nclass FalconTransformerContainer(LayerContainer):\n    \"\"\"\n        Transformer layer container for the Falcon model.\n    \"\"\"\n    qkv_w: FusedQKVParameter\n    attn_out_w: AttentionOutputParameter\n    mlp_1_w: MLP1Parameter\n    mlp_2_w: MLP2Parameter\n    ln_attn_gamma: NormParameter\n    ln_attn_beta: NormParameter\n\n    PARAM_MAPPING = {\n        \"self_attention.query_key_value.weight\": \"qkv_w.params\",\n        \"self_attention.dense.weight\": \"attn_out_w.params\",\n        \"mlp.dense_h_to_4h.weight\": \"mlp_1_w.params\",\n        \"mlp.dense_4h_to_h.weight\": \"mlp_2_w.params\",\n        \"input_layernorm.weight\": \"ln_attn_gamma.params\",\n        \"input_layernorm.bias\": \"ln_attn_beta.params\",\n    }\n\n\nclass FalconNonTransformerContainer(LayerContainer):\n    \"\"\"\n        Non-Transformer layer container for the Falcon model.\n    \"\"\"\n    word_emb: EmbeddingParameter\n    word_unembed: UnembedParameter\n    final_norm_gamma: NormParameter\n    final_norm_beta: NormParameter\n\n    PARAM_MAPPING = {\n        \"transformer.word_embeddings.weight\": \"word_emb.params\",\n        \"transformer.ln_f.weight\": \"final_norm_gamma.params\",\n        \"transformer.ln_f.bias\": \"final_norm_beta.params\",\n        \"lm_head.weight\": \"word_unembed.params\",\n    }\n\n\n'''\n # HF Falcon 40b model looks like this:\n\n FalconForCausalLM(\n  (transformer): FalconModel(\n    (word_embeddings): Embedding(65024, 8192)\n    (h): ModuleList(\n      (0-59): 60 x FalconDecoderLayer(\n        (self_attention): FalconAttention(\n          (maybe_rotary): FalconRotaryEmbedding()\n          (query_key_value): FalconLinear(in_features=8192, out_features=9216, bias=False)\n          (dense): FalconLinear(in_features=8192, out_features=8192, bias=False)\n          (attention_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (mlp): FalconMLP(\n          (dense_h_to_4h): FalconLinear(in_features=8192, out_features=32768, bias=False)\n          (act): GELU(approximate='none')\n          (dense_4h_to_h): FalconLinear(in_features=32768, out_features=8192, bias=False)\n        )\n        (ln_attn): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)\n        (ln_mlp): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n    (ln_f): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=8192, out_features=65024, bias=False)\n)\n'''\n\n\nclass FalconNewArchTransformerContainer(LayerContainer):\n    \"\"\"\n        Transformer layer container for the Falcon model.\n    \"\"\"\n    qkv_w: GQAMegatronQKVParameter\n    attn_out_w: AttentionOutputParameter\n    mlp_1_w: MLP1Parameter\n    mlp_2_w: MLP2Parameter\n    ln_attn_gamma: NormParameter\n    ln_attn_beta: NormParameter\n    ln_mlp_gamma: NormParameter\n    ln_mlp_beta: NormParameter\n\n    PARAM_MAPPING = {\n        \"self_attention.query_key_value.weight\": \"qkv_w.params\",\n        \"self_attention.dense.weight\": \"attn_out_w.params\",\n        \"mlp.dense_h_to_4h.weight\": \"mlp_1_w.params\",\n        \"mlp.dense_4h_to_h.weight\": \"mlp_2_w.params\",\n        \"ln_attn.weight\": \"ln_attn_gamma.params\",\n        \"ln_attn.bias\": \"ln_attn_beta.params\",\n        \"ln_mlp.weight\": \"ln_mlp_gamma.params\",\n        \"ln_mlp.bias\": \"ln_mlp_beta.params\",\n    }\n", "deepspeed/inference/v2/model_implementations/falcon/policy.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Any\n\nfrom ...config_v2 import RaggedInferenceEngineConfig\nfrom ..inference_policy_base import ContainerMap, InferenceV2Policy\nfrom .container import FalconNonTransformerContainer, FalconTransformerContainer\nfrom .container import FalconNewArchTransformerContainer\nfrom .model import FalconInferenceModel\n\n\nclass FalconPolicy(InferenceV2Policy):\n\n    def instantiate_model(self, engine_config: RaggedInferenceEngineConfig, mp_group: Any) -> FalconInferenceModel:\n        return FalconInferenceModel(config=self._model_config, engine_config=engine_config, base_mp_group=mp_group)\n\n    def build_container_map(self) -> ContainerMap:\n        map = ContainerMap()\n\n        trans_container_cls = FalconNewArchTransformerContainer if self._model_config.new_decoder_architecture else FalconTransformerContainer\n        transformer_containers = [trans_container_cls(self.model) for _ in range(self.model.num_layers)]\n\n        map.set_transformer_params(['transformer.h'], transformer_containers)\n\n        map.set_non_transformer_params(FalconNonTransformerContainer(self.model))\n\n        map.set_unmapped_params(\n            [f'model.layers.{i}.self_attn.rotary_emb.inv_freq' for i in range(self.model.num_layers)])\n\n        return map\n", "deepspeed/inference/v2/model_implementations/falcon/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .policy import FalconPolicy\n", "deepspeed/inference/v2/model_implementations/llama_v2/model.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Iterable, Optional, Tuple\n\nimport torch\n\nimport deepspeed.comm as dist\n\nfrom ...allocator import empty_from\nfrom ...inference_utils import ActivationType, DtypeEnum\nfrom .. import *\nfrom ...modules.configs import *\nfrom ...modules.interfaces import *\nfrom ...ragged import RaggedBatchWrapper\n\nfrom .container import Llama2NonTransformerContainer, Llama2TransformerContainer\n\n\nclass Llama2InferenceModel(DSTransformerModelBase):\n    \"\"\"\n    Inference model implementation for ragged batching for Llama-2 models.\n    \"\"\"\n\n    _non_transformer: Optional[Llama2NonTransformerContainer]\n    \"\"\"\n    Embed + unembed container. Specializing the type annotation.\n    \"\"\"\n\n    _transformer: Optional[Iterable[Llama2TransformerContainer]]\n    \"\"\"\n    Per-layer transformer container. Specializing the type annotation.\n    \"\"\"\n    \"\"\"\n    Properties ineherited from `DSInferenceModelBase`\n    \"\"\"\n\n    @property\n    def max_sequence_length(self) -> int:\n        return self._config.max_seq_length\n\n    \"\"\"\n    Properties ineherited from `DSTransformerModelBase`\n    \"\"\"\n\n    @property\n    def num_layers(self) -> int:\n        return self._config.num_hidden_layers\n\n    @property\n    def model_dim(self) -> int:\n        return self._config.hidden_size\n\n    @property\n    def vocab_size(self) -> int:\n        return self._config.vocab_size\n\n    @property\n    def head_size(self) -> int:\n        return self.model_dim // self.n_heads\n\n    @property\n    def n_heads(self) -> int:\n        return self._config.num_attention_heads\n\n    @property\n    def intermediate_dim(self) -> int:\n        return self._config.intermediate_size\n\n    @property\n    def n_heads_kv(self) -> int:\n        return self._config.num_key_value_heads\n\n    @property\n    def activation_dtype(self) -> DtypeEnum:\n        if self._config.torch_dtype == torch.float16:\n            return DtypeEnum.fp16\n        elif self._config.torch_dtype == torch.bfloat16:\n            return DtypeEnum.bf16\n        else:\n            raise NotImplementedError(\"Only fp16 and bf16 are supported\")\n\n    @property\n    def mlp_activation_fn(self) -> ActivationType:\n        activation = self._config.hidden_act.lower()\n        # llama model family is special and is always gated so force gated versions of relu, gelu, silu\n        if activation == \"gelu\":\n            return ActivationType.GEGLU\n        elif activation == \"relu\":\n            return ActivationType.ReGLU\n        elif activation == \"gegelu\":\n            return ActivationType.GEGLU\n        elif activation == \"silu\":\n            return ActivationType.SiGLU\n        else:\n            raise NotImplementedError(f\"Activation {activation} not supported\")\n\n    @property\n    def norm_type(self) -> NormTypeEnum:\n        return NormTypeEnum.RMSNorm\n\n    @property\n    def positional_embedding_type(self) -> PositionalEmbeddingType:\n        return PositionalEmbeddingType.rotate_half\n\n    @property\n    def positional_embedding_config(self) -> Optional[RotateHalfConfig]:\n        return RotateHalfConfig(theta_base=self._config.rope_theta)\n\n    \"\"\"\n    Forward implementations\n    \"\"\"\n\n    def _forward_embed(self, ragged_batch: RaggedBatchWrapper) -> torch.Tensor:\n        \"\"\"\n        Performs the embedding lookup prior to running the transformer of the model.\n\n        Arguments:\n            ragged_batch (RaggedBatchWrapper): The batch to embed.\n\n        Returns:\n            torch.Tensor: The embedded batch.\n        \"\"\"\n        embed = self.embed(ragged_batch, self._non_transformer.word_emb)\n\n        if embed.shape[-1] != self.model_dim:\n            raise ValueError(f\"Embedding output shape {embed.shape} does not match model_dim {self.model_dim}\")\n\n        return embed\n\n    def _forward_transformer_layer(self, layer_idx: int, residual: torch.Tensor, hidden_states: torch.Tensor,\n                                   ragged_batch_info: RaggedBatchWrapper) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Executes one (slightly offset) layer of the transformer. This implementation does a peak-ahead\n        optimization to fuse the layer norm of the next layer into the current layer.\n\n        Arguments:\n            layer_idx (int): The index of the layer to execute.\n            residual (torch.Tensor): The residual tensor from the previous layer.\n            hidden_states (torch.Tensor): The hidden states from the previous layer. This is the\n                hidden states after pre normalization.\n            ragged_batch_info (RaggedBatchWrapper): The batch metadata.\n        \"\"\"\n        # TODO(cmikeh2): Distribute ragged_batch_info to all modules\n\n        cur_params = self._transformer[layer_idx]\n        kv_cache = self.state_manager.get_cache(layer_idx)\n\n        hidden_states = self.qkv(hidden_states, cur_params.qkv_w, b=None)\n        hidden_states = self.attn(hidden_states, kv_cache, ragged_batch_info)\n        hidden_states = self.attn_out(hidden_states, cur_params.attn_out_w, b=None)\n\n        if self.tp_size > 1:\n            dist.all_reduce(hidden_states, group=self._base_mp_group)\n\n        residual, hidden_states = self.norm(residual, hidden_states, cur_params.mlp_norm_gamma, beta=None)\n\n        # Should be configurable in the future\n        hidden_states = self.mlp_1(hidden_states, cur_params.mlp_1_w, b=None)\n        hidden_states = self.mlp_2(hidden_states, cur_params.mlp_2_w, b=None)\n\n        if self.tp_size > 1:\n            dist.all_reduce(hidden_states, group=self._base_mp_group)\n\n        if layer_idx != self.num_layers - 1:\n            next_params = self._transformer[layer_idx + 1]\n            residual, hidden_states = self.norm(residual, hidden_states, next_params.attn_norm_gamma, beta=None)\n        else:\n            # On last layer, we just need to perform the residual add. Adding into the residual\n            # here is safe.\n            residual.add_(hidden_states)\n\n        return residual, hidden_states\n\n    def _forward_unembed(self, hidden_states: torch.Tensor, ragged_batch_info: RaggedBatchWrapper) -> torch.Tensor:\n        \"\"\"\n        Performs unembedding of the hidden states to logits. This will only sample the final\n        token of each sequence.\n        \"\"\"\n        logits = self.unembed(hidden_states,\n                              self._non_transformer.word_unembed,\n                              ragged_batch_info,\n                              gamma=self._non_transformer.final_norm)\n\n        if self.tp_size > 1:\n            comm_buffer = empty_from(self._comm_logits, (self.tp_size, logits.shape[0], logits.shape[1]))\n            full_logits = empty_from(self._return_logits, (logits.shape[0], self.vocab_size))\n\n            dist.all_gather_into_tensor(comm_buffer, logits, group=self._base_mp_group)\n\n            full_logits.copy_(comm_buffer.permute(1, 0, 2).reshape(logits.shape[0], self.vocab_size))\n\n            return full_logits\n        else:\n            return logits\n\n    def forward(self, wrapped_batch: RaggedBatchWrapper) -> torch.Tensor:\n\n        residual = self._forward_embed(wrapped_batch)\n\n        residual, hidden_states = self.norm(residual, None, self._transformer[0].attn_norm_gamma, beta=None)\n\n        for layer_idx in range(self.num_layers):\n            residual, hidden_states = self._forward_transformer_layer(layer_idx, residual, hidden_states,\n                                                                      wrapped_batch)\n\n        return self._forward_unembed(residual, wrapped_batch)\n", "deepspeed/inference/v2/model_implementations/llama_v2/container.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\n# Create a container object to save model-specific tensors using the policy file above.\n\nfrom ..common_parameters import *\nfrom ..layer_container_base import LayerContainer\n'''\n # HF Llama model looks like this:\n\nLlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n    )\n    (norm): LlamaRMSNorm()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n)\n'''\n\n\nclass Llama2TransformerContainer(LayerContainer):\n    \"\"\"\n        Transformer layer container for the Llama-2 model.\n    \"\"\"\n    qkv_w: UnfusedQKVParameter\n    attn_out_w: AttentionOutputParameter\n    mlp_1_w: GatedMLPParameter\n    mlp_2_w: MLP2Parameter\n    attn_norm_gamma: NormParameter\n    mlp_norm_gamma: NormParameter\n\n    PARAM_MAPPING = {\n        \"self_attn.q_proj.weight\": \"qkv_w.q_params\",\n        \"self_attn.k_proj.weight\": \"qkv_w.k_params\",\n        \"self_attn.v_proj.weight\": \"qkv_w.v_params\",\n        \"self_attn.o_proj.weight\": \"attn_out_w.params\",\n        \"mlp.gate_proj.weight\": \"mlp_1_w.gate_params\",\n        \"mlp.up_proj.weight\": \"mlp_1_w.up_params\",\n        \"mlp.down_proj.weight\": \"mlp_2_w.params\",\n        \"input_layernorm.weight\": \"attn_norm_gamma.params\",\n        \"post_attention_layernorm.weight\": \"mlp_norm_gamma.params\",\n    }\n\n\nclass Llama2NonTransformerContainer(LayerContainer):\n    \"\"\"\n        Non-Transformer layer container for the Llama-2 model.\n    \"\"\"\n    word_emb: EmbeddingParameter\n    word_unembed: UnembedParameter\n    final_norm: NormParameter\n\n    PARAM_MAPPING = {\n        \"model.embed_tokens.weight\": \"word_emb.params\",\n        \"model.norm.weight\": \"final_norm.params\",\n        \"lm_head.weight\": \"word_unembed.params\",\n    }\n", "deepspeed/inference/v2/model_implementations/llama_v2/policy.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Any\n\nfrom ...config_v2 import RaggedInferenceEngineConfig\nfrom ..inference_policy_base import ContainerMap, InferenceV2Policy\nfrom .container import Llama2NonTransformerContainer, Llama2TransformerContainer\nfrom .model import Llama2InferenceModel\n\n\nclass Llama2Policy(InferenceV2Policy):\n\n    def instantiate_model(self, engine_config: RaggedInferenceEngineConfig, mp_group: Any) -> Llama2InferenceModel:\n        return Llama2InferenceModel(config=self._model_config, engine_config=engine_config, base_mp_group=mp_group)\n\n    def build_container_map(self) -> ContainerMap:\n        map = ContainerMap()\n\n        transformer_containers = [Llama2TransformerContainer(self.model) for _ in range(self.model.num_layers)]\n\n        map.set_transformer_params(['model.layers'], transformer_containers)\n\n        map.set_non_transformer_params(Llama2NonTransformerContainer(self.model))\n\n        map.set_unmapped_params(\n            [f'model.layers.{i}.self_attn.rotary_emb.inv_freq' for i in range(self.model.num_layers)])\n\n        return map\n", "deepspeed/inference/v2/model_implementations/llama_v2/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .policy import Llama2Policy\n", "deepspeed/inference/v2/model_implementations/sharding/mlp.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Optional\n\nimport torch\n\nfrom .types import ShardingType, DEFAULT_SHARD_GRANULARITY\nfrom .utils import shard_param, get_shard_endpoints\n\n\ndef shard_mlp_1_param(param: torch.Tensor,\n                      shard_rank: int,\n                      num_shards: int,\n                      gated: bool = False,\n                      is_moe: bool = False) -> torch.Tensor:\n    \"\"\"\n    Utility method for sharding an MLP 1 parameter. Both biases and weights are supported, as well\n    as for fused weights for MoE.\n\n    Args:\n        param (torch.Tensor): The parameter to shard.\n        shard_rank (int): Which shard of the partitioned tensor to return.\n        num_shards (int): The total number of shards the parameter is distributed across.\n        gated (bool): Whether or not the parameter is from a gated MLP.\n    \"\"\"\n    bias_dims = 2 if is_moe else 1\n\n    if gated:\n        return shard_param(param,\n                           ShardingType.OUTER_DIMENSION,\n                           shard_rank,\n                           num_shards,\n                           granularity=DEFAULT_SHARD_GRANULARITY * 2,\n                           bias_dims=bias_dims)\n    else:\n        return shard_param(param, ShardingType.OUTER_DIMENSION, shard_rank, num_shards, bias_dims=bias_dims)\n\n\ndef shard_mlp_2_param(param: torch.Tensor,\n                      shard_rank: int,\n                      num_shards: int,\n                      is_moe: bool = False) -> Optional[torch.Tensor]:\n    \"\"\"\n    Utility method for sharding an MLP 2 parameter.\n\n    Args:\n        param (torch.Tensor): The parameter to shard.\n        shard_rank (int): Which shard of the partitioned tensor to return.\n        num_shards (int): The total number of shards the parameter is distributed across.\n        is_moe (bool): Whether or not the parameter is from a MoE model.\n    \"\"\"\n    bias_dim_size = 2 if is_moe else 1\n\n    if len(param.shape) == bias_dim_size:\n        # We will do the bias addition on the 0th rank only rather than scale the parameter and\n        # implicitly reconstruct this in the distributed reduce.\n        return param if shard_rank == 0 else None\n\n    return shard_param(param, ShardingType.INNER_DIMENSION, shard_rank, num_shards)\n\n\ndef sharded_intermediate_dim(intermediate_size: int, num_shards: int, shard_rank: int) -> int:\n    \"\"\"\n    Utility method for getting the size of the intermediate dimension of a sharded MLP.\n\n    Args:\n        intermediate_size (int): The size of the intermediate dimension.\n        num_shards (int): The total number of shards the parameter is distributed across.\n        shard_rank (int): Which shard of the partitioned tensor to return.\n    \"\"\"\n    endpoints = get_shard_endpoints(intermediate_size, shard_rank, num_shards)\n    return endpoints[1] - endpoints[0]\n", "deepspeed/inference/v2/model_implementations/sharding/attn_out.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Optional\n\nimport torch\n\nfrom .types import ShardingType\nfrom .utils import shard_param, get_shard_endpoints\n\n\ndef shard_attn_out_param(param: torch.Tensor,\n                         shard_rank: int,\n                         num_shards: int,\n                         head_size: int,\n                         n_heads_q: Optional[int] = None,\n                         n_heads_kv: Optional[int] = None) -> Optional[torch.Tensor]:\n    \"\"\"\n    Utility method for sharding an attention output parameter.\n    \"\"\"\n    if len(param.shape) == 1:\n        # We will do the bias addition on the 0th rank only rather than scale the parameter and\n        # implicitly reconstruct this in the distributed reduce.\n        return param if shard_rank == 0 else None\n\n    assert n_heads_kv is None or (n_heads_q is not None\n                                  and n_heads_kv is not None), \"n_heads_kv should not be passed without n_heads_q\"\n\n    mha_sharding = n_heads_kv is None or n_heads_q == n_heads_kv\n\n    if mha_sharding:\n        return shard_param(param, ShardingType.INNER_DIMENSION, shard_rank, num_shards, granularity=head_size)\n    else:\n        assert param.shape[0] == head_size * n_heads_q, \"GQA param shape is not correct\"\n\n        # 32 KV heads, 16 shards for example\n        even_kv_sharding = n_heads_kv % num_shards == 0\n\n        # 8 KV heads, 16 shards for example\n        even_kv_distribution = num_shards % n_heads_kv == 0\n\n        assert even_kv_sharding or even_kv_distribution, \"No partitioning algorithm for this yet.\"\n\n        if even_kv_sharding:\n            # Same as original sharding scenario\n            return shard_param(param, ShardingType.INNER_DIMENSION, shard_rank, num_shards, granularity=head_size)\n        else:\n            # We will first do a sharding on the KV and Q to map to the one KV shard per group of Q.\n            q_sharding_degree = num_shards // n_heads_kv\n\n            kv_head = shard_rank // q_sharding_degree\n\n            q_sharding_rank = shard_rank % q_sharding_degree\n            q_factor = n_heads_q // n_heads_kv\n\n            q_chunk = param[..., q_factor * kv_head * head_size:q_factor * (kv_head + 1) * head_size]\n\n            return shard_param(q_chunk,\n                               ShardingType.INNER_DIMENSION,\n                               q_sharding_rank,\n                               q_sharding_degree,\n                               granularity=head_size)\n\n\ndef attn_out_in_features(out_features: int,\n                         shard_rank: int,\n                         num_shards: int,\n                         head_size: int,\n                         n_heads_q: Optional[int] = None,\n                         n_heads_kv: Optional[int] = None) -> int:\n    \"\"\"\n    Helper to calculate the expected output projection dimension of a QKV projection matrix.\n\n    Args:\n        in_features (int): The model dimension.\n        shard_rank (int): Which rank to return the corresponding size for.\n        num_shards (int): The total number of shards the parameter is distributed across.\n        head_size (int): The size of each attention head.\n        n_heads_q (int): The number of query heads on the model. This only needs to be passed if the number\n            of query and key/value heads are different. If passed without n_heads_kv, default\n            MHA partitioning will be used.\n        n_heads_kv (int): The number of key and value heads on the model. This only needs to be passed\n            if the number of query and key/value heads are different. This argument cannot be passed without\n            also passing n_heads_q (we want to explicitly opt into GQA sharding).\n    \"\"\"\n    assert n_heads_kv is None or (n_heads_q is not None\n                                  and n_heads_kv is not None), \"n_heads_kv should not be passed without n_heads_q\"\n\n    mha_sharding = n_heads_kv is None or n_heads_q == n_heads_kv\n\n    if mha_sharding:\n        endpoints = get_shard_endpoints(out_features, shard_rank, num_shards, granularity=head_size)\n        return endpoints[1] - endpoints[0]\n    else:\n        if n_heads_kv >= num_shards:\n            assert n_heads_kv % num_shards == 0, \"No partitioning algorithm for this yet.\"\n            n_local_groups = n_heads_kv // num_shards\n            group_size = n_heads_q // n_heads_kv\n\n            return n_local_groups * head_size * group_size\n        else:\n            assert num_shards % n_heads_kv == 0, \"No partitioning algorithm for this yet.\"\n            q_split_degree = num_shards // n_heads_kv\n            q_split_rank = shard_rank % q_split_degree\n            split_granularity = (n_heads_q // n_heads_kv) * head_size\n\n            q_endpoints = get_shard_endpoints(split_granularity, q_split_rank, q_split_degree, granularity=head_size)\n\n            return q_endpoints[1] - q_endpoints[0]\n", "deepspeed/inference/v2/model_implementations/sharding/unembed.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\n\nfrom .types import ShardingType\nfrom .utils import shard_param, get_shard_endpoints\n\n\ndef shard_unembed_param(param: torch.Tensor, shard_rank: int, num_shards: int) -> torch.Tensor:\n    \"\"\"\n    Utility method for sharding an unembed parameter. We shard unembeddings on the vocab dimension\n    with the expectation of an all-gather to produce the full results.\n\n    TODO(cmikeh2): Really ideal would be if MII could have access to the comm and we would do\n    an A2A and sharded sampling.\n\n    Args:\n        param (torch.Tensor): The parameter to shard. Should be of shape [vocab_size, model_dim]\n        shard_rank (int): Which shard of the partitioned tensor to return.\n        num_shards (int): The total number of shards the parameter is distributed across.\n\n    Returns:\n        torch.Tensor: The sharded parameter of shape [sharded_vocab_size, model_dim]\n    \"\"\"\n    return shard_param(param, ShardingType.OUTER_DIMENSION, shard_rank, num_shards, granularity=1)\n\n\ndef sharded_unembed_dim(vocab_size: int, shard_rank: int, num_shards: int) -> int:\n    \"\"\"\n    Utility method for determining the sharded vocab size of a sharded unembed parameter.\n\n    Args:\n        vocab_size (int): The size of the vocabulary.\n        shard_rank (int): Which shard of the partitioned tensor to return.\n        num_shards (int): The total number of shards the parameter is distributed across.\n    \"\"\"\n    start_idx, end_idx = get_shard_endpoints(vocab_size, shard_rank, num_shards, granularity=1)\n    return end_idx - start_idx\n", "deepspeed/inference/v2/model_implementations/sharding/qkv.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Optional\n\nimport torch\n\nfrom .types import ShardingType\nfrom .utils import shard_param, get_shard_endpoints\n\n\ndef shard_qkv_param(param: torch.Tensor,\n                    shard_rank: int,\n                    num_shards: int,\n                    head_size: int,\n                    n_heads_q: Optional[int] = None,\n                    n_heads_kv: Optional[int] = None) -> Optional[torch.Tensor]:\n    \"\"\"\n    Utility method for sharding a QKV parameter. Both biases and weights are supported. It is assumed\n    that the layout of the parameter is such that all Q heads, all K heads, and all V heads\n    are contiguous with respect to each other.\n\n    Args:\n        param (torch.Tensor): The parameter to shard.\n        shard_rank (int): Which shard of the partitioned tensor to return.\n        num_shards (int): The total number of shards the parameter is distributed across.\n        head_size (int): The size of each head.\n        n_heads_q (int): The number of query heads. This only needs to be passed if the number\n             of query and key/value heads are different. If passed without n_heads_kv, default\n             MHA partitioning will be used.\n        n_heads_kv (int): The number of key/value heads. This only needs to be passed if the number\n                of query and key/value heads are different. This argument should not be passed without\n                n_heads_q (we want to explicitly opt into GQA sharding).\n    \"\"\"\n    if n_heads_kv is not None and n_heads_q is None:\n        raise ValueError(\"n_heads_kv should not be passed without n_heads_q\")\n\n    if n_heads_q is None:\n        # Guaranteed to be in MHA\n        if param.shape[0] // 3 % head_size != 0:\n            raise ValueError(\"MHA param shape is not correct\")\n        n_heads_q = param.shape[0] // head_size // 3\n        mha_sharding = True\n    else:\n        mha_sharding = n_heads_q == n_heads_kv\n\n    if n_heads_q < num_shards:\n        raise ValueError(\"There must be at least as many query heads as there are shards.\")\n\n    if mha_sharding:\n        return shard_param(param,\n                           ShardingType.OUTER_DIMENSION,\n                           shard_rank,\n                           num_shards,\n                           num_concatenated_matrices=3,\n                           granularity=head_size)\n    else:\n        if n_heads_q % n_heads_kv != 0:\n            raise ValueError(\"Must be an even ratio between query and key/value heads.\")\n\n        if param.shape[0] != head_size * (n_heads_q + 2 * n_heads_kv):\n            raise ValueError(\"GQA param shape is not correct\")\n\n        # 32 KV heads, 16 shards for example\n        if n_heads_kv >= num_shards and n_heads_kv % num_shards != 0:\n            raise ValueError(\"Currently do not support uneven partitioning of KV heads for GQA.\")\n\n        # 8 KV heads, 16 shards for example\n        if n_heads_kv < num_shards and num_shards % n_heads_kv != 0:\n            raise ValueError(\"Currently do not support distributing KV heads across different numbers of shards.\")\n        else:\n            even_kv_sharding = n_heads_kv >= num_shards\n\n        if param is None:\n            return None\n\n        q_param = param[:head_size * n_heads_q]\n        kv_param = param[head_size * n_heads_q:]\n\n        if even_kv_sharding:\n            # This is equivalent to the original sharding algorithm since n_heads_q = C * n_heads_kv.\n            # If n_heads_kv % num_shards == 0, then n_heads_q % num_shards == 0.\n            q_param = shard_param(q_param, ShardingType.OUTER_DIMENSION, shard_rank, num_shards, granularity=head_size)\n            kv_param = shard_param(kv_param,\n                                   ShardingType.OUTER_DIMENSION,\n                                   shard_rank,\n                                   num_shards,\n                                   num_concatenated_matrices=2,\n                                   granularity=head_size)\n            return torch.cat([q_param, kv_param], dim=0)\n        else:\n            # We will first do a sharding on the KV and Q to map to the one KV shard per group of Q.\n            q_sharding_degree = num_shards // n_heads_kv\n\n            kv_head = shard_rank // q_sharding_degree\n            k_param = kv_param[kv_head * head_size:(kv_head + 1) * head_size]\n            v_param = kv_param[(n_heads_kv + kv_head) * head_size:(n_heads_kv + kv_head + 1) * head_size]\n\n            q_sharding_rank = shard_rank % q_sharding_degree\n            q_factor = n_heads_q // n_heads_kv\n\n            q_chunk = q_param[q_factor * kv_head * head_size:q_factor * (kv_head + 1) * head_size]\n\n            q_param = shard_param(q_chunk,\n                                  ShardingType.OUTER_DIMENSION,\n                                  q_sharding_rank,\n                                  q_sharding_degree,\n                                  granularity=head_size)\n\n            return torch.cat([q_param, k_param, v_param], dim=0)\n\n\ndef qkv_out_features(in_features: int,\n                     shard_rank: int,\n                     num_shards: int,\n                     head_size: int,\n                     n_heads_q: Optional[int] = None,\n                     n_heads_kv: Optional[int] = None) -> int:\n    \"\"\"\n    Helper to calculate the expected output projection dimension of a QKV projection matrix.\n\n    Args:\n        in_features (int): The model dimension.\n        shard_rank (int): Which rank to return the corresponding size for.\n        num_shards (int): The total number of shards the parameter is distributed across.\n        head_size (int): The size of each head.\n        n_heads_q (int): The number of query heads. This only needs to be passed if the number\n             of query and key/value heads are different. If passed without n_heads_kv, default\n             MHA partitioning will be used.\n        n_heads_kv (int): The number of key/value heads. This only needs to be passed if the number\n            of query and key/value heads are different. This argument cannot be passed without also\n            passing n_heads_q (we want to explicitly opt into GQA sharding).\n    \"\"\"\n    if n_heads_kv is not None and n_heads_q is None:\n        raise ValueError(\"n_heads_kv should not be passed without n_heads_q\")\n\n    mha_sharding = n_heads_kv is None or n_heads_q == n_heads_kv\n\n    if n_heads_q is not None and in_features != head_size * n_heads_q:\n        raise ValueError(\"in_features is not consistent with n_heads_q and head_size\")\n\n    if mha_sharding:\n        endpoints = get_shard_endpoints(in_features, shard_rank, num_shards, granularity=head_size)\n        return (endpoints[1] - endpoints[0]) * 3\n    else:\n        if n_heads_kv >= num_shards:\n            if n_heads_kv % num_shards != 0:\n                raise ValueError(\"The KV heads must be evenly distributed across the shards.\")\n\n            n_local_groups = n_heads_kv // num_shards\n            group_size = n_heads_q // n_heads_kv\n\n            return n_local_groups * head_size * (2 + group_size)\n        else:\n            if num_shards % n_heads_kv != 0:\n                raise ValueError(\"A shared KV head must always partition across the same number of shards.\")\n\n            q_split_degree = num_shards // n_heads_kv\n            q_split_rank = shard_rank % q_split_degree\n            split_granularity = (n_heads_q // n_heads_kv) * head_size\n\n            q_endpoints = get_shard_endpoints(split_granularity, q_split_rank, q_split_degree, granularity=head_size)\n\n            return (q_endpoints[1] - q_endpoints[0]) + 2 * head_size\n", "deepspeed/inference/v2/model_implementations/sharding/utils.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Optional, Tuple\n\nimport torch\n\nfrom .types import ShardingType, DEFAULT_SHARD_GRANULARITY\n\n\ndef get_shard_endpoints(dim_size: int,\n                        shard_rank: int,\n                        num_shards: int,\n                        granularity: int = DEFAULT_SHARD_GRANULARITY) -> Tuple[int, int]:\n    \"\"\"\n    Given a dimension to shard with size dim_size, return the start and end indices of the slice\n    that belong to the given rank.\n\n    The typical use of this is as an internal helper function, so see if there is a higher level\n    API that better suits the application.\n\n    Args:\n        dim_size (int): The size of the dimension to shard.\n        shard_rank (int): The rank of the shard to return.\n        num_shards (int): Total number of shards the dimension will be distributed across.\n        granularity (int): The minimum alignment of the shard endpoints. This is used to support\n            non-even head counts as well as align dimensions to cleaner GEMM boundaries.\n    \"\"\"\n    assert dim_size % granularity == 0, \"Dimension size must be divisible by granularity\"\n\n    total_chunks = dim_size // granularity\n    base_chunks_per_rank = total_chunks // num_shards\n    remainder_chunks = total_chunks % num_shards\n\n    start_chunk_id = shard_rank * base_chunks_per_rank + min(shard_rank, remainder_chunks)\n    end_chunk_id = start_chunk_id + base_chunks_per_rank + (1 if shard_rank < remainder_chunks else 0)\n\n    return start_chunk_id * granularity, end_chunk_id * granularity\n\n\ndef shard_param(param: Optional[torch.Tensor],\n                shard_mode: ShardingType,\n                shard_rank: int,\n                num_shards: int,\n                num_concatenated_matrices: int = 1,\n                granularity: int = 32,\n                bias_dims: int = 1) -> torch.Tensor:\n    \"\"\"\n    Utility for sharding a parameter. This will return the slice of the parameter that should\n    exist on the given shard_rank given the sharding configuration. The workflow here is\n    to find the minimum bounded Tensor to shard, get the slicing endpoints, and then concatenate\n    as needed.\n\n    The typical use of this is as an internal helper function, so see if there is a higher level\n    API that better suits the application.\n\n    Args:\n        param (torch.Tensor): The parameter to shard.\n        shard_mode (ShardingType): The type of sharding to apply. See ShardingType for more context.\n        shard_rank (int): The rank of the shard to return.\n        num_shards (int): Total number of shards the parameter will be distrbuted across.\n        num_concatenated_matrices (int): The number of matrices that have been concatenated together in the original\n            parameter. An example of this is a fused QKV projection matrix, where the `num_concatenated_matrices`\n            argument would be 3.\n        granularity (int): The minimum alignment of the shard endpoints. For attention projection matrices, this\n            should be set to the head size to support non-even sharding.\n        bias_dims (int): The number of dimensions that are considered bias dimensions. This is used to support\n            sharding of MoE and non-MoE biases on the same codepath.\n    \"\"\"\n    assert shard_rank < num_shards, \"Shard rank must be less than num_shards\"\n\n    # Easier to hide this inside of the sharding logic than to add checks in every model\n    # implementation.\n    if param is None:\n        return None\n\n    if num_shards == 1:\n        # Trivial case of no sharding.\n        return param\n\n    if shard_mode == ShardingType.OUTER_DIMENSION:\n\n        def get_matrices(dim_idx: int) -> torch.Tensor:\n            dim_size = param.size(dim_idx) // num_concatenated_matrices\n            start_channel_id, end_channel_id = get_shard_endpoints(dim_size, shard_rank, num_shards, granularity)\n            return torch.chunk(param, num_concatenated_matrices, dim=dim_idx), start_channel_id, end_channel_id\n\n        if param.ndim == bias_dims:\n            # Special case for bias parameters.\n            matrices, start_channel_id, end_channel_id = get_matrices(dim_idx=-1)\n            return torch.cat([mat[..., start_channel_id:end_channel_id] for mat in matrices], dim=-1)\n        else:\n            # General case for weight parameters. This assumes MoE parameters are stored in the format of\n            # [num_experts, out_features, in_features]\n            matrices, start_channel_id, end_channel_id = get_matrices(dim_idx=-2)\n            return torch.cat([mat[..., start_channel_id:end_channel_id, :] for mat in matrices], dim=-2)\n\n    elif shard_mode == ShardingType.INNER_DIMENSION:\n        dim_size = param.size(-1) // num_concatenated_matrices\n        start_channel_id, end_channel_id = get_shard_endpoints(dim_size, shard_rank, num_shards, granularity)\n        matrices = torch.chunk(param, num_concatenated_matrices, dim=-1)\n        return torch.cat([mat[..., start_channel_id:end_channel_id] for mat in matrices], dim=-1)\n", "deepspeed/inference/v2/model_implementations/sharding/attn.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Optional, Tuple\n\n\ndef get_local_heads(shard_rank: int,\n                    num_shards: int,\n                    n_heads_q: int,\n                    n_heads_kv: Optional[int] = None) -> Tuple[int, int]:\n    \"\"\"\n    Helper to determine the number of local heads of a given shard.\n\n    Args:\n        shard_rank (int): The rank of the shard.\n        num_shards (int): The total number of shards that attention is distributed over.\n        n_heads_q (int): The number of query heads.\n        n_heads_kv (int): The number of key/value heads. If not passed, it is assumed that\n            the number of query and key/value heads are the same.\n    \"\"\"\n    if n_heads_q < num_shards:\n        raise ValueError(\"There must be at least as many attention heads as there are shards.\")\n\n    if n_heads_kv is None or n_heads_kv == n_heads_q:\n        # MHA attention\n        base_heads = n_heads_q // num_shards\n        extra_heads = n_heads_q % num_shards\n\n        if shard_rank < extra_heads:\n            return (base_heads + 1), (base_heads + 1)\n        else:\n            return base_heads, base_heads\n    else:\n        # GQA attention\n        if n_heads_q % n_heads_kv != 0:\n            raise ValueError(\"Must be an even ratio between query and key/value heads.\")\n\n        if n_heads_kv < num_shards and num_shards % n_heads_kv != 0:\n            raise ValueError(\n                \"If splitting a group across multiple shards, we must be able to distribute the groups evenly.\")\n\n        if n_heads_kv >= num_shards and n_heads_kv % num_shards != 0:\n            raise ValueError(\"If parallelizing groups, must be able to evenly distribute them.\")\n\n        q_ratio = n_heads_q // n_heads_kv\n\n        if n_heads_kv >= num_shards:\n            local_kv_heads = n_heads_kv // num_shards\n            local_q_heads = local_kv_heads * q_ratio\n            return local_q_heads, local_kv_heads\n        else:\n            group_sharding_size = num_shards // n_heads_kv\n            group_rank_idx = shard_rank % group_sharding_size\n\n            base_heads = q_ratio // group_sharding_size\n            extra_heads = q_ratio % group_sharding_size\n\n            if group_rank_idx < extra_heads:\n                return (base_heads + 1), 1\n            else:\n                return base_heads, 1\n", "deepspeed/inference/v2/model_implementations/sharding/types.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom enum import Enum\n\nDEFAULT_SHARD_GRANULARITY = 32\n\n\nclass ShardingType(Enum):\n    # Inner dimension sharding corresponds to splitting the Tensor along the K-dimension\n    # of a matrix multiplication. This would be used for attention_output or MLP2.\n    INNER_DIMENSION = 1\n\n    # Outer dimension sharding corresponds to splitting the Tensor along the N-dimension\n    # of a matrix multiplication. This would be used for the QKV and MLP1 projections.\n    OUTER_DIMENSION = 0\n", "deepspeed/inference/v2/model_implementations/sharding/embedding.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\n\nfrom .types import ShardingType\nfrom .utils import shard_param, get_shard_endpoints\n\n\ndef shard_embedding_param(param: torch.Tensor, shard_rank: int, num_shards: int) -> torch.Tensor:\n    \"\"\"\n    Utility method for sharding an embedding parameter.\n\n    Args:\n        param (torch.Tensor): The parameter to shard. Should be of shape [vocab_size, model_dim]\n        shard_rank (int): Which shard of the partitioned tensor to return.\n        num_shards (int): The total number of shards the parameter is distributed across.\n    \"\"\"\n    return shard_param(param, ShardingType.INNER_DIMENSION, shard_rank, num_shards)\n\n\ndef sharded_embedding_dim(embedding_size: int, shard_rank: int, num_shards: int) -> int:\n    \"\"\"\n    Utility method for getting the size of the embedding dimension of a sharded embedding.\n\n    Args:\n        embedding_size (int): The size of the embedding.\n        shard_rank (int): Which shard of the partitioned tensor to return.\n        num_shards (int): The total number of shards the parameter is distributed across.\n    \"\"\"\n    start_idx, end_idx = get_shard_endpoints(embedding_size, shard_rank, num_shards)\n    return end_idx - start_idx\n", "deepspeed/inference/v2/model_implementations/sharding/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .attn import *\nfrom .attn_out import *\nfrom .embedding import *\nfrom .mlp import *\nfrom .qkv import *\nfrom .types import *\nfrom .unembed import *\n", "deepspeed/inference/v2/model_implementations/opt/model.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Iterable, Optional, Tuple\n\nimport torch\n\nimport deepspeed.comm as dist\n\nfrom ...allocator import empty_from\nfrom ...inference_utils import ActivationType, DtypeEnum\nfrom ...model_implementations import *\nfrom ...modules.configs import *\nfrom ...ragged import RaggedBatchWrapper\nfrom .container import OPTNonTransformerContainer, OPTTransformerContainer\n\nfrom ...modules.heuristics import instantiate_embed\n\n\nclass OPTInferenceModel(DSTransformerModelBase):\n    \"\"\"\n    Inference model implementation for ragged batching for OPT models.\n    \"\"\"\n\n    _non_transformer: Optional[OPTNonTransformerContainer]\n    \"\"\"\n    Embed + unembed container. Specializing the type annotation.\n    \"\"\"\n\n    _transformer: Optional[Iterable[OPTTransformerContainer]]\n    \"\"\"\n    Per-layer transformer container. Specializing the type annotation.\n    \"\"\"\n    \"\"\"\n    Properties ineherited from `DSInferenceModelBase`\n    \"\"\"\n\n    @property\n    def max_sequence_length(self) -> int:\n        return self._config.max_seq_length\n\n    \"\"\"\n    Properties ineherited from `DSTransformerModelBase`\n    \"\"\"\n\n    @property\n    def num_layers(self) -> int:\n        return self._config.num_hidden_layers\n\n    @property\n    def model_dim(self) -> int:\n        return self._config.hidden_size\n\n    @property\n    def vocab_size(self) -> int:\n        return self._config.vocab_size\n\n    @property\n    def head_size(self) -> int:\n        return self.model_dim // self.n_heads\n\n    @property\n    def n_heads(self) -> int:\n        return self._config.num_attention_heads\n\n    @property\n    def intermediate_dim(self) -> int:\n        return self._config.ffn_dim\n\n    @property\n    def activation_dtype(self) -> DtypeEnum:\n        if self._config.torch_dtype == torch.float16:\n            return DtypeEnum.fp16\n        elif self._config.torch_dtype == torch.bfloat16:\n            return DtypeEnum.bf16\n        else:\n            raise NotImplementedError(\"Only fp16 and bf16 are supported\")\n\n    @property\n    def mlp_activation_fn(self) -> ActivationType:\n        return ActivationType.RELU\n\n    @property\n    def norm_type(self) -> NormTypeEnum:\n        return NormTypeEnum.LayerNorm\n\n    @property\n    def positional_embedding_type(self) -> PositionalEmbeddingType:\n        return PositionalEmbeddingType.none\n\n    @property\n    def positional_embedding_config(self) -> Optional[RotateHalfConfig]:\n        return None\n\n    \"\"\"\n    Overrides of ``DSTransformerModelBase`` methods\n    \"\"\"\n\n    def make_embedding_layer(self) -> None:\n        \"\"\"\n        Performs setup and creates embedding DSModule. Since OPT includes trained\n        positional embeddings, we will override the base model implementation.\n        \"\"\"\n\n        embed_config = DSEmbeddingsConfig(max_tokens=self._engine_config.state_manager.max_ragged_batch_size,\n                                          residual_dtype=self.activation_dtype,\n                                          embedding_dim=self.model_dim,\n                                          positional_embedding=True,\n                                          positional_offset=2)\n\n        self.embed = instantiate_embed(embed_config, self._engine_config)\n\n    \"\"\"\n    Forward implementations\n    \"\"\"\n\n    def _forward_embed(self, ragged_batch: RaggedBatchWrapper) -> torch.Tensor:\n        embed = self.embed(ragged_batch, self._non_transformer.word_emb, self._non_transformer.word_emb_pos)\n        if embed.shape[-1] != self.model_dim:\n            raise ValueError(f\"Embedding output shape {embed.shape} does not match model_dim {self.model_dim}\")\n\n        return embed\n\n    def _forward_transformer_layer(self, layer_idx: int, residual: torch.Tensor, hidden_states: torch.Tensor,\n                                   ragged_batch_info: RaggedBatchWrapper) -> Tuple[torch.Tensor, torch.Tensor]:\n        # TODO(cmikeh2): Distribute ragged_batch_info to all modules\n\n        cur_params = self._transformer[layer_idx]\n        kv_cache = self.state_manager.get_cache(layer_idx)\n\n        hidden_states = self.qkv(hidden_states, cur_params.qkv_w, b=cur_params.qkv_b)\n        hidden_states = self.attn(hidden_states, kv_cache, ragged_batch_info)\n        hidden_states = self.attn_out(hidden_states, cur_params.attn_out_w, b=cur_params.attn_out_b)\n\n        if self.tp_size > 1:\n            dist.all_reduce(hidden_states, group=self._base_mp_group)\n\n        residual, hidden_states = self.norm(residual,\n                                            hidden_states,\n                                            cur_params.mlp_norm_gamma,\n                                            beta=cur_params.mlp_norm_beta)\n\n        # Should be configurable in the future\n        hidden_states = self.mlp_1(hidden_states, cur_params.mlp_1_w, b=cur_params.mlp_1_b)\n        hidden_states = self.mlp_2(hidden_states, cur_params.mlp_2_w, b=cur_params.mlp_2_b)\n\n        if self.tp_size > 1:\n            dist.all_reduce(hidden_states, group=self._base_mp_group)\n\n        if layer_idx != self.num_layers - 1:\n            next_params = self._transformer[layer_idx + 1]\n            residual, hidden_states = self.norm(residual,\n                                                hidden_states,\n                                                next_params.attn_norm_gamma,\n                                                beta=next_params.attn_norm_beta)\n        else:\n            # On last layer, we just need to perform the residual add. Adding into the residual\n            # here is safe.\n            residual.add_(hidden_states)\n\n        return residual, hidden_states\n\n    def _forward_unembed(self, hidden_states: torch.Tensor, ragged_batch_info: RaggedBatchWrapper) -> torch.Tensor:\n        logits = self.unembed(hidden_states,\n                              self._non_transformer.word_unembed,\n                              ragged_batch_info,\n                              gamma=self._non_transformer.final_norm_w,\n                              beta=self._non_transformer.final_norm_b)\n\n        if self.tp_size > 1:\n            comm_buffer = empty_from(self._comm_logits, (self.tp_size, logits.shape[0], logits.shape[1]))\n            full_logits = empty_from(self._return_logits, (logits.shape[0], self.vocab_size))\n\n            dist.all_gather_into_tensor(comm_buffer, logits, group=self._base_mp_group)\n\n            full_logits.copy_(comm_buffer.permute(1, 0, 2).reshape(logits.shape[0], self.vocab_size))\n\n            return full_logits\n        else:\n            return logits\n\n    def forward(self, wrapped_batch: RaggedBatchWrapper) -> torch.Tensor:\n\n        residual = self._forward_embed(wrapped_batch)\n\n        residual, hidden_states = self.norm(residual,\n                                            None,\n                                            self._transformer[0].attn_norm_gamma,\n                                            beta=self._transformer[0].attn_norm_beta)\n\n        for layer_idx in range(self.num_layers):\n            residual, hidden_states = self._forward_transformer_layer(layer_idx, residual, hidden_states,\n                                                                      wrapped_batch)\n\n        return self._forward_unembed(residual, wrapped_batch)\n", "deepspeed/inference/v2/model_implementations/opt/container.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\n# Create a container object to save model-specific tensors using the policy file above.\n\nfrom ..common_parameters import *\nfrom ..layer_container_base import LayerContainer\n'''\n # HF OPT model looks like this:\n\nOPTForCausalLM(\n  (model): OPTModel(\n    (decoder): OPTDecoder(\n      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (layers): ModuleList(\n        (0-11): 12 x OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n)\n\n'''\n\n\nclass OPTTransformerContainer(LayerContainer):\n    \"\"\"\n        Transformer layer container for the OPT model.\n    \"\"\"\n    qkv_w: UnfusedQKVParameter\n    qkv_b: UnfusedQKVParameter\n    attn_out_w: AttentionOutputParameter\n    attn_out_b: AttentionOutputParameter\n    mlp_1_w: MLP1Parameter\n    mlp_1_b: MLP1Parameter\n    mlp_2_w: MLP2Parameter\n    mlp_2_b: MLP2Parameter\n    attn_norm_beta: NormParameter\n    attn_norm_gamma: NormParameter\n    mlp_norm_beta: NormParameter\n    mlp_norm_gamma: NormParameter\n\n    PARAM_MAPPING = {\n        \"self_attn.q_proj.weight\": \"qkv_w.q_params\",\n        \"self_attn.q_proj.bias\": \"qkv_b.q_params\",\n        \"self_attn.k_proj.weight\": \"qkv_w.k_params\",\n        \"self_attn.k_proj.bias\": \"qkv_b.k_params\",\n        \"self_attn.v_proj.weight\": \"qkv_w.v_params\",\n        \"self_attn.v_proj.bias\": \"qkv_b.v_params\",\n        \"self_attn.out_proj.weight\": \"attn_out_w.params\",\n        \"self_attn.out_proj.bias\": \"attn_out_b.params\",\n        \"fc1.weight\": \"mlp_1_w.params\",\n        \"fc1.bias\": \"mlp_1_b.params\",\n        \"fc2.weight\": \"mlp_2_w.params\",\n        \"fc2.bias\": \"mlp_2_b.params\",\n        \"self_attn_layer_norm.weight\": \"attn_norm_gamma.params\",\n        \"self_attn_layer_norm.bias\": \"attn_norm_beta.params\",\n        \"final_layer_norm.weight\": \"mlp_norm_gamma.params\",\n        \"final_layer_norm.bias\": \"mlp_norm_beta.params\",\n    }\n\n\nclass OPTNonTransformerContainer(LayerContainer):\n    \"\"\"\n        Non-Transformer layer container for the OPT model.\n    \"\"\"\n    word_emb: EmbeddingParameter\n    word_emb_pos: EmbeddingParameter\n    word_unembed: UnembedParameter\n    final_norm_w: NormParameter\n    final_norm_b: NormParameter\n\n    PARAM_MAPPING = {\n        \"*decoder.embed_tokens.weight\": [\"word_emb.params\", \"word_unembed.params\"],\n        \"*decoder.embed_positions.weight\": \"word_emb_pos.params\",\n        \"*decoder.final_layer_norm.weight\": \"final_norm_w.params\",\n        \"*decoder.final_layer_norm.bias\": \"final_norm_b.params\",\n    }\n", "deepspeed/inference/v2/model_implementations/opt/policy.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Any\n\nfrom ...config_v2 import RaggedInferenceEngineConfig\nfrom ..inference_policy_base import ContainerMap, InferenceV2Policy\nfrom .container import OPTNonTransformerContainer, OPTTransformerContainer\nfrom .model import OPTInferenceModel\n\n\nclass OPTPolicy(InferenceV2Policy):\n\n    def instantiate_model(self, engine_config: RaggedInferenceEngineConfig, mp_group: Any) -> OPTInferenceModel:\n        return OPTInferenceModel(config=self._model_config, engine_config=engine_config, base_mp_group=mp_group)\n\n    def build_container_map(self) -> ContainerMap:\n        map = ContainerMap()\n\n        transformer_containers = [OPTTransformerContainer(self.model) for _ in range(self.model.num_layers)]\n\n        map.set_transformer_params(['model.decoder.layers', 'decoder.layers'], transformer_containers)\n\n        map.set_non_transformer_params(OPTNonTransformerContainer(self.model))\n\n        map.set_unmapped_params(['lm_head.weight'])\n\n        return map\n", "deepspeed/inference/v2/model_implementations/opt/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .policy import OPTPolicy\n", "deepspeed/inference/v2/checkpoint/base_engine.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom abc import ABC, abstractmethod\nfrom typing import Iterable, Tuple\n\nimport torch\n\n#from .huggingface_engine import HuggingFaceCheckpointEngine\n\nMEGATRON = 'megatron'\nHUGGINGFACE = 'huggingface'\n\n\nclass CheckpointEngineBase(ABC):\n    \"\"\"\n    Abstract interface for checkpoint engines to implement.\n\n    There is no ``__init__`` method here by design, since the creation of the checkpoint\n    engine will happen outside the policy/engine code. The tradeoff being made here is\n    that we will write different frontends for different checkpoint engines, but these\n    frontends can be tailored to the specific checkpoint engine/model source needs.\n    \"\"\"\n\n    @abstractmethod\n    def parameters(self) -> Iterable[Tuple[str, torch.Tensor]]:\n        \"\"\"\n        This method should create a generator of tuples of the form (name, parameter) for\n        all parameters in the model. The name should be the fully qualified name of the\n        parameter, and the parameter should be a torch.Tensor.\n\n        The expected use of a checkpoint engine is the following:\n        ```python\n        for name, parameter in checkpoint_engine.parameters():\n            container_map.map_param(name, parameter)\n        ```\n        For a concrete use example, see ``InferenceV2Policy``.\n        \"\"\"\n        ...\n", "deepspeed/inference/v2/checkpoint/huggingface_engine.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport os\nimport json\nimport torch\nfrom .base_engine import CheckpointEngineBase\nfrom typing import Iterable, Tuple\nfrom functools import partial\n\nfrom ..logging import inference_logger\n\n\nclass HuggingFaceCheckpointEngine(CheckpointEngineBase):\n\n    def __init__(self, model_name_or_path: str, auth_token: str = None) -> None:\n        super().__init__()\n        from transformers import AutoConfig, GenerationConfig\n\n        self.model_name_or_path = model_name_or_path\n        self.auth_token = auth_token\n        self.model_config = AutoConfig.from_pretrained(self.model_name_or_path)\n        # Define this property here so we can use it in the model implementation\n        if not hasattr(self.model_config, \"max_seq_length\"):\n            if hasattr(self.model_config, \"max_position_embeddings\"):\n                self.model_config.max_seq_length = self.model_config.max_position_embeddings\n            else:\n                generation_config = GenerationConfig.from_pretrained(self.model_name_or_path)\n                self.model_config.max_seq_length = generation_config.max_length\n        self._local_checkpoint_dir = None\n        self._all_ckpt_paths = self._fetch_checkpoint_files()\n\n    def _fetch_checkpoint_files(self):\n        \"\"\"\n        Fetch the checkpoint files from the HuggingFace Hub.\n        \"\"\"\n        # TODO(jeff): for models like llama-2 the user will have to provide an auth `token`,\n        # currently coming from the ckpt engine init but maybe a catch all kwargs for other\n        # snapshot download parameters would be more flexible.\n\n        from huggingface_hub import snapshot_download, list_repo_tree\n\n        def model_has_safetensors(model_name_or_path: str) -> bool:\n            if os.path.isdir(model_name_or_path):\n                file_list = os.listdir(model_name_or_path)\n            else:\n                file_list = [rf.path for rf in list_repo_tree(model_name_or_path)]\n            for f in file_list:\n                if f.endswith(\".safetensors\"):\n                    return True\n            return False\n\n        if os.path.isdir(self.model_name_or_path):\n            self._local_checkpoint_dir = self.model_name_or_path\n        else:\n            # We need to download the checkpoint files from HF\n            if model_has_safetensors(self.model_name_or_path):\n                # Prioritize downloading safetensors if they are available\n                allow_patterns = [\"*.safetensors\", \"*.json\"]\n            else:\n                # Fallback to bin files when safetensors are not present\n                allow_patterns = [\"*.bin\", \"*.json\", \"*.pt\"]\n            self._local_checkpoint_dir = snapshot_download(self.model_name_or_path,\n                                                           allow_patterns=allow_patterns,\n                                                           revision=None,\n                                                           token=self.auth_token)\n\n        assert os.path.isdir(\n            self._local_checkpoint_dir\n        ), f\"Checkpoint dir {self._local_checkpoint_dir} is not a directory, cannot load checkpoint.\"\n\n        # Set the appropriate file names based on whether we have safetensors or not\n        if model_has_safetensors(self._local_checkpoint_dir):\n            from safetensors.torch import load_file\n            model_param_json_fname = \"model.safetensors.index.json\"\n            model_file_fname = \"model.safetensors\"\n            self._checkpoint_load_fn = load_file\n        else:\n            model_param_json_fname = \"pytorch_model.bin.index.json\"\n            model_file_fname = \"pytorch_model.bin\"\n            self._checkpoint_load_fn = partial(torch.load, map_location=\"cpu\")\n\n        model_param_json = os.path.join(self._local_checkpoint_dir, model_param_json_fname)\n\n        if not os.path.isfile(model_param_json):\n            # We don't need any json as all such HF models will have pytorch_model.bin\n            all_checkpoint_files = [os.path.join(self._local_checkpoint_dir, model_file_fname)]\n        else:\n            param_map = json.load(open(model_param_json, \"r\"))\n\n            # weight_map -> { \"lm_head.weight\": \"pytorch_model-00002-of-00002.bin\", ... }\n            weight_map = param_map[\"weight_map\"]\n\n            # unique set of all checkpoint files\n            all_checkpoint_files = set(weight_map.values())\n\n            # get absolute path of all unique checkpoint files\n            all_checkpoint_files = [os.path.join(self._local_checkpoint_dir, f) for f in all_checkpoint_files]\n\n        return all_checkpoint_files\n\n    def parameters(self) -> Iterable[Tuple[str, torch.Tensor]]:\n        \"\"\"\n        Generator of model parameters (satisfies the CheckpointEngineBase interface).\n        \"\"\"\n        for checkpoint in self._all_ckpt_paths:\n            inference_logger().info(f\"Loading checkpoint: {checkpoint}\")\n            checkpoint_sd = self._checkpoint_load_fn(checkpoint)\n            param_keys = list(checkpoint_sd.keys())\n            for param_name in param_keys:\n                param = checkpoint_sd[param_name]\n                yield param_name, param\n\n            del checkpoint_sd\n\n\nif __name__ == \"__main__\":\n    # To test, add your auth_token here and run `python huggingface_engine.py`\n    engine = HuggingFaceCheckpointEngine(model_name_or_path=\"meta-llama/Llama-2-7b-hf\",\n                                         auth_token=\"hf_xxxxxxxxxxxxxxxxx\")\n    for name, param in engine.parameters():\n        print(name, param.shape)\n", "deepspeed/inference/v2/checkpoint/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .base_engine import CheckpointEngineBase\nfrom .in_memory_engine import InMemoryModelEngine\nfrom .huggingface_engine import HuggingFaceCheckpointEngine\n", "deepspeed/inference/v2/checkpoint/in_memory_engine.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Iterable, Tuple\nimport torch\n\nfrom .base_engine import CheckpointEngineBase\n\n\nclass InMemoryModelEngine(CheckpointEngineBase):\n    \"\"\"\n    This \"checkpoint\" engine uses the existing interface to enable loading parameters into an\n    inference model from a model already instantiated in memory. In general, this is not the\n    recommended way to use the inference engine, and should only be used when absolutely necessary.\n\n    The primary limitation of this approach is that the model must be fully instantiated in memory.\n    In a tensor parallel scenario, this means that the model is either replicated many times in host\n    memory. Currently, it is also recommended to only use this approach for models held in host memory.\n\n    In order to free the memory held by this copy of the model, we delete the model in the first call\n    to `parameters`, so it is not safe to make this call twice.\n    \"\"\"\n\n    def __init__(self, model: torch.nn.Module) -> None:\n        \"\"\"\n        Create virtual checkpoint engine for the provided module.\n\n        Args:\n            model (torch.nn.Module): Model to load parameters from.\n        \"\"\"\n        super().__init__()\n        self.model = model\n\n    def parameters(self) -> Iterable[Tuple[str, torch.Tensor]]:\n        for name, parameter in self.model.named_parameters():\n            yield name, parameter\n\n        del self.model\n", "deepspeed/inference/v2/ragged/sequence_descriptor.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import List, Tuple, Union\n\nimport torch\n\n\nclass BaseSequenceDescriptor:\n\n    @property\n    def seen_tokens(self) -> int:\n        \"\"\"\n        The number of tokens for this sequence that have completed a forward pass.\n        \"\"\"\n        raise NotImplementedError()\n\n    @property\n    def cur_allocated_blocks(self, cache_group: int = 0) -> int:\n        \"\"\"\n        The number of KV blocks currently allocated for this sequence.\n        \"\"\"\n        raise NotImplementedError()\n\n    @property\n    def kv_blocks_ptr(self, cache_group: int = 0) -> int:\n        \"\"\"\n        The pointer to the KV blocks for this sequence.\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass PlaceholderSequenceDescriptor(BaseSequenceDescriptor):\n    \"\"\"\n    The DummySequenceDescriptor is an empty object that allows us to perform schedulability\n    checks before formally tracking a sequence.\n    \"\"\"\n\n    def __init__(self, seen_tokens=0, cur_allocated_blocks=0, kv_blocks_ptr=0) -> None:\n        self._seen_tokens = seen_tokens\n        self._cur_allocated_blocks = cur_allocated_blocks\n        self._kv_blocks_ptr = kv_blocks_ptr\n\n    @property\n    def seen_tokens(self) -> int:\n        return self._seen_tokens\n\n    @property\n    def cur_allocated_blocks(self, cache_group: int = 0) -> int:\n        return self._cur_allocated_blocks\n\n    @property\n    def kv_blocks_ptr(self, cache_group: int = 0) -> int:\n        return self._kv_blocks_ptr\n\n\nclass DSSequenceDescriptor(BaseSequenceDescriptor):\n\n    _seen_tokens: int\n    \"\"\"\n    Number of tokens in the sequence that have completed a forward pass.\n    \"\"\"\n\n    _in_flight_tokens: int\n    \"\"\"\n    Number of tokens that have begun a forward pass but not yet completed it.\n    \"\"\"\n\n    _max_context: int\n    \"\"\"\n    Maximum number of tokens this sequence may eventually include. Currently unused but\n    may be used in future implementations for speculative caching.\n    \"\"\"\n\n    _num_allocation_groups: Tuple[int, ...]\n    \"\"\"\n    Number of unique allocation groups associated with the sequence for each cache group.\n    \"\"\"\n\n    _blocks_per_allocation_group: Tuple[torch.IntTensor, ...]\n    \"\"\"\n    Number of blocks allocated for each allocation group in each cache group.\n    \"\"\"\n\n    # Padded list of KV-cache IDs for the sequence.\n    _kv_cache_ids: Tuple[torch.Tensor, ...]\n    _kv_cache_ids_shadow: Tuple[torch.Tensor, ...]\n    \"\"\"\n    Padded list of KV-cache IDs for the sequence. The padded shape is [num_allocation_groups, max_blocks_per_allocation_group].\n    \"\"\"\n\n    # The location in the broader ID tensor where the KV-cache IDs for the sequence\n    # are stored. Used on flush.\n    _tracking_id: int\n\n    def __init__(self,\n                 tracking_id: int,\n                 kv_cache_ids: Tuple[torch.Tensor, ...],\n                 kv_cache_ids_shadow: Tuple[torch.Tensor, ...],\n                 max_context: int = -1) -> None:\n        \"\"\"\n        Create the metadata to track a single sequence in the system.\n\n        Arguments:\n            tracking_id (int): The slot in the tracking buffers used to track this sequence.\n            kv_cache_ids (Tuple[torch.Tensor, ...]): The KV-cache IDs for the sequence. The shape\n                of the tensor should be [num_allocation_groups, max_blocks_per_allocation_group].\n                There should be one tensor per cache group.\n            kv_cache_ids_shadow (Tuple[torch.Tensor, ...]): The shadow tensor for the KV-cache IDs.\n                This tensor should be allocated on the host and should have the same shape as the\n                tensor provided in ``kv_cache_ids``. There should be one tensor per cache group.\n            max_context (int): The maximum number of tokens this sequence may eventually include.\n                Currently unused but may be used in future implementations for speculative caching.\n        \"\"\"\n        self._tracking_id = tracking_id\n        self._kv_cache_ids = kv_cache_ids\n        self._kv_cache_ids_shadow = kv_cache_ids_shadow\n        self._max_context = max_context\n        self._n_cache_groups = len(kv_cache_ids)\n\n        self._seen_tokens = 0\n        self._in_flight_tokens = 0\n\n        self._num_allocation_groups = tuple(kv_cache_ids_shadow.shape[0]\n                                            for kv_cache_ids_shadow in kv_cache_ids_shadow)\n        self._blocks_per_allocation_group = tuple(\n            torch.zeros(num_groups, dtype=torch.int32, device=\"cpu\") for num_groups in self._num_allocation_groups)\n\n        for cache_group, kv_cache_ids in enumerate(kv_cache_ids):\n            assert self._num_allocation_groups[cache_group] == kv_cache_ids.shape[0]\n            assert len(kv_cache_ids.shape) == 2\n\n    @property\n    def seen_tokens(self) -> int:\n        \"\"\"\n        Number of tokens in the sequence that have completed a forward pass.\n        \"\"\"\n        return self._seen_tokens\n\n    @property\n    def in_flight_tokens(self) -> int:\n        \"\"\"\n        Number of tokens that have begun a forward pass but not yet completed it.\n        \"\"\"\n        return self._in_flight_tokens\n\n    @property\n    def max_context(self) -> int:\n        \"\"\"\n        Maximum number of tokens for this sequence. Currently unused.\n        \"\"\"\n        return self._max_context\n\n    @property\n    def tracking_id(self) -> int:\n        \"\"\"\n        Return the slot in the tracking buffers used to track this sequence.\n        \"\"\"\n        return self._tracking_id\n\n    @property\n    def cur_allocated_blocks(self, cache_group: int = 0) -> int:\n        \"\"\"\n        Returns the number of blocks currently allocated for this sequence in the specified cache group.\n\n        Arguments:\n            cache_group (int): The cache group to query.\n        \"\"\"\n        # Currently, there is only one allocation group.\n        # A shortcut is used here to bypass the overhead of sum().\n        if len(self._blocks_per_allocation_group) == 1:\n            return self._blocks_per_allocation_group[0].item()\n        return self._blocks_per_allocation_group[cache_group].sum().item()\n\n    def kv_cache_ids(self, cache_group: int = 0, on_device: bool = False) -> torch.Tensor:\n        \"\"\"\n        Returns the Tensor containing the block IDs for this sequence on the appropriate device\n        for the specified cache group.\n\n        Arguments:\n            cache_group (int): The cache group to query.\n            on_device (bool): Whether or not to return the Tensor on the device or on the host.\n        \"\"\"\n        if on_device:\n            return self._kv_cache_ids[cache_group]\n        else:\n            return self._kv_cache_ids_shadow[cache_group]\n\n    @property\n    def kv_blocks_ptr(self, cache_group: int = 0) -> int:\n        \"\"\"\n        Get the device pointer to the base of the KV-cache ids for the specified cache group and\n        sequence.\n\n        Arguments:\n            cache_group (int): The cache group to query.\n        \"\"\"\n        return self._kv_cache_ids[cache_group].data_ptr()\n\n    #TODO: this was previously a property but causing issues with PR-4668 need to consult w. Connor\n    def all_block_ids(self, cache_group: int = 0) -> torch.Tensor:\n        \"\"\"\n        Return the Tensor containing all block IDs for this sequence in the specified cache group.\n\n        Arguments:\n            cache_group (int): The cache group to query.\n        \"\"\"\n        block_ids = []\n        for allocation_group, num_blocks in zip(self._kv_cache_ids[cache_group],\n                                                self._blocks_per_allocation_group[cache_group]):\n            block_ids.append(allocation_group[:num_blocks])\n        return torch.cat(block_ids)\n\n    def pre_forward(self, num_tokens: int) -> None:\n        \"\"\"\n        Update the state of the sequence before a forward pass.\n\n        Arguments:\n            num_tokens (int): The number of tokens in the sequence that will be executed during the\n                next forward pass of the model.\n        \"\"\"\n        self._in_flight_tokens = num_tokens\n\n    def post_forward(self) -> None:\n        \"\"\"\n        Update the state of the sequence after a forward pass. This should be called after the forward\n        pass completes. NOTE: due to the asynchronous nature of the accelerator, this may be called\n        before the forward pass completes on the device itself.\n        \"\"\"\n        self._seen_tokens += self._in_flight_tokens\n        self._in_flight_tokens = 0\n\n    def extend_kv_cache(self, new_ids: Union[List[torch.IntTensor], torch.IntTensor], cache_group: int = 0) -> None:\n        \"\"\"\n        Extend the KV-cache for the sequence.\n\n        Arguments:\n            new_ids (Union[List[torch.IntTensor], torch.IntTensor]): For each allocation group, the IDs\n                to add to the KV-cache. If there is only one allocation group, a single tensor can be\n                provided. Otherwise, a list of tensors should be provided. The tensors do not need\n                to have the same shape.\n        \"\"\"\n        if isinstance(new_ids, torch.Tensor):\n            new_ids = [new_ids]\n\n        if len(new_ids) != self._num_allocation_groups[cache_group]:\n            raise ValueError(\n                f\"Only {len(new_ids)} allocation groups provided, expected {self._num_allocation_groups[cache_group]}\")\n\n        for group_id, new_group_ids in enumerate(new_ids):\n            new_blocks = new_group_ids.numel()\n\n            if new_blocks == 0:\n                # If we have multiple groups, it's possible to have an empty group.\n                continue\n\n            shadow_alloc_group = self._kv_cache_ids_shadow[cache_group][group_id]\n            alloc_group = self._kv_cache_ids[cache_group][group_id]\n            cur_blocks = self._blocks_per_allocation_group[cache_group][group_id]\n\n            shadow_alloc_group[cur_blocks:cur_blocks + new_blocks].copy_(new_group_ids)\n            alloc_group[cur_blocks:cur_blocks + new_blocks].copy_(shadow_alloc_group[cur_blocks:cur_blocks +\n                                                                                     new_blocks],\n                                                                  non_blocking=True)\n\n            self._blocks_per_allocation_group[cache_group][group_id] += new_blocks\n\n    def free_kv_cache(self, free_ids: Union[List[torch.IntTensor], torch.IntTensor], cache_group: int = 0) -> None:\n        \"\"\"\n        Free blocks from the KV-cache for the sequence.\n\n        Arguments:\n            free_ids (Union[List[torch.IntTensor], torch.IntTensor]): The ids of blocks to free\n                from the KV-cache. If there is only one allocation group, a single tensor can be\n                provided. Otherwise, a list of tensors should be provided. The tensors do not need\n                to have the same shape.\n        \"\"\"\n        raise NotImplementedError(\"Partial KV-cache freeing is not yet supported.\")\n", "deepspeed/inference/v2/ragged/blocked_allocator.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Iterable, Union\n\nimport torch\n\n\nclass BlockedAllocator:\n    \"\"\"\n    Allocator class for managing which blocks are free/used in the\n    blocked KV-cache. This is a simple allocator that uses a linked list\n    to keep track of which blocks are free/used. The cost of allocation/deallocation\n    is O(blocks), where blocks is the number of blocks to allocate/deallocate.\n\n    TODO(cmikeh2): Evaluate performance of this allocator and migrate\n    to C++ if necessary.\n    \"\"\"\n    # Number of blocks in the KV-cache(s).\n    _num_blocks: int\n\n    # Array of blocks, where each element is the next block in the linked list.\n    _blocks: torch.Tensor\n\n    # Index of the head of the linked list.\n    _head: int\n\n    # Number of free blocks in the KV-cache.\n    _free_blocks: int\n\n    def __init__(self, num_blocks: int) -> None:\n        \"\"\"\n        Initialize an allocator with `num_blocks` blocks. This requires at least\n        `num_blocks` * 4 bytes of host memory.\n\n        Parameters:\n            num_blocks (int): The number of blocks to allocate.\n        \"\"\"\n\n        if num_blocks < 1:\n            raise ValueError(f'Blocked KV-cache must have at least 1 block, provided {num_blocks}')\n\n        self._num_blocks = num_blocks\n        self._blocks = torch.arange(1, num_blocks + 1, dtype=torch.int32, device='cpu', pin_memory=True)\n        self._head = 0\n        self._free_blocks = num_blocks\n\n    def allocate(self, num_blocks: int) -> torch.Tensor:\n        \"\"\"\n        Allocate a list of blocks from the associated KV-caches. This will\n        return `num_blocks` blocks from the KV-cache if they are available,\n        or raise an exception if there are not enough free blocks.\n\n        Parameters:\n            num_blocks (int): The number of blocks to allocate.\n\n        Returns:\n            List[int]: The list of blocks allocated.\n        \"\"\"\n        if num_blocks > self._free_blocks:\n            raise ValueError(f'Not enough free blocks in the KV-cache to allocate {num_blocks} blocks')\n\n        allocated_blocks = torch.zeros(num_blocks, dtype=torch.int32)\n        for i in range(num_blocks):\n            allocated_blocks[i] = self._head\n            self._head = self._blocks[self._head].item()\n            self._blocks[allocated_blocks[i]] = -1  # Mark as used\n            self._free_blocks -= 1\n\n        return allocated_blocks\n\n    def free(self, blocks: Union[Iterable[int], int]) -> None:\n        \"\"\"\n        Return a list of blocks to the free pool. If a single invalid block is provided (i.e.,\n        one that is out of range of the allocator or is already free), then an exception is raised\n        and no blocks are freed.\n\n        Parameters:\n            blocks (Union[Iterable[int], int]): The list of blocks to free. If only one block\n                is to be freed, this can be alone as an integer.\n        \"\"\"\n        if isinstance(blocks, int):\n            blocks = [blocks]\n\n        for block in blocks:\n            # Parse all blocks for validity before mutating the list.\n            if block < 0 or block >= self._num_blocks:\n                raise ValueError(f'Invalid block {block} provided to free')\n\n            if self._blocks[block] != -1:\n                raise ValueError(f'Block {block} is already free')\n\n        for block in blocks:\n            self._blocks[block] = self._head\n            self._head = block\n            self._free_blocks += 1\n\n    @property\n    def free_blocks(self) -> int:\n        \"\"\"\n        Return the number of free blocks in the KV-cache.\n        \"\"\"\n        return self._free_blocks\n", "deepspeed/inference/v2/ragged/kv_cache.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport operator\nfrom functools import reduce\nfrom typing import Any, Iterable, Optional, Tuple\n\nimport torch\n\nimport deepspeed.comm as dist\nfrom deepspeed.comm.reduce_op import ReduceOp\n\nfrom deepspeed.accelerator import get_accelerator\nfrom ..inference_utils import elem_size\nfrom ..logging import inference_logger\nfrom .blocked_allocator import BlockedAllocator\nfrom .manager_configs import AllocationMode, KVCacheConfig, MemoryConfig\n\n\ndef split_kv(kv_cache: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Split a KV cache instance into its key and value components.\n\n    Parameters:\n        kv_cache (torch.Tensor): The KV-cache to split. This should be a 5D tensor with the\n            following shape: [num_blocks, block_size, 2, num_heads, head_size]\n\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor]: The key and value components of the KV-cache. Both\n            tensors will have the shape [num_blocks, block_size, num_heads, head_size].\n    \"\"\"\n    if kv_cache.ndim != 5:\n        raise ValueError(f\"KV-cache must have 5 dimensions, got {kv_cache.ndim}.\")\n\n    return kv_cache[:, :, 0, :, :], kv_cache[:, :, 1, :, :]\n\n\nclass BlockedKVCache:\n\n    _caches: Tuple[torch.Tensor, ...]\n    \"\"\"\n    Backing storage for all KV caches. This is a 6D tensor with the following shape:\n        (num_caches, num_blocks, block_size, 2, num_heads, head_size)\n    \"\"\"\n\n    _allocators: Tuple[BlockedAllocator, ...]\n    \"\"\"\n    Block allocator for tracking cache usage. This manages the GPU cache.\n    \"\"\"\n\n    _configs: Tuple[KVCacheConfig, ...]\n    \"\"\"\n    Configuration of the KV cache(s). See ``KVCacheConfig`` for more details. This enables the support\n    for different types/shapes of KV-caches (i.e. the alternating local and global attention in\n    GPT-Neo).\n    \"\"\"\n\n    def __init__(self,\n                 configs: Tuple[KVCacheConfig, ...],\n                 memory_config: MemoryConfig,\n                 mp_group: Optional[Any] = None,\n                 offload: bool = False) -> None:\n        \"\"\"\n        Create a container that will maintain the storage and allocations for a set of\n        blocked KV-caches.\n\n        Parameters:\n            config (KVCacheConfig): The configuration of the KV-cache.\n            slack (int): The amount of slack space to reserve in GPU memory for the cache.\n            enable_offload (bool): Whether to enable offloading of the cache to the host.\n            blocks (int): The number of blocks to pre-allocate for the cache. If this is set,\n                slack will be ignored.\n        \"\"\"\n        self._configs = configs\n        self._memory_config = memory_config\n        self._enable_offload = offload\n\n        if self._enable_offload:\n            raise NotImplementedError(\"Offloading of KV-caches is not yet supported.\")\n\n        if AllocationMode(self._memory_config.mode) is AllocationMode.RESERVE:\n            # TODO(cmikeh2): Change the weighting based on the type of the KV-cache\n\n            total_per_block_footprint = 0\n            for config in self._configs:\n                per_block_footprint = reduce(operator.mul, config.cache_shape, config.block_size)\n                per_block_footprint *= 2  # for key and value\n                total_per_block_footprint += per_block_footprint * elem_size(config.cache_dtype)\n\n            # Perform a dummy nccl call before calculating available memory, on some systems (H100) we've observed higher memory allocations from NCCL\n            if dist.get_world_size(group=mp_group) > 1:\n                dummy_tensor = torch.tensor(0, dtype=torch.int32, device=get_accelerator().current_device())\n                dist.all_reduce(dummy_tensor, op=ReduceOp.MIN, group=mp_group)\n\n            get_accelerator().empty_cache()\n            available_kv_memory = get_accelerator().available_memory() - self._memory_config.size\n            total_memory = get_accelerator().total_memory()\n\n            inference_logger().debug(\n                f\"Memory usage before KV-cache allocation: total_memory={total_memory}, available_kv_memory={available_kv_memory}, total_per_block_footprint={total_per_block_footprint}\"\n            )\n\n            if available_kv_memory < total_per_block_footprint:\n                raise ValueError(\n                    f\"Insufficient memory to allocate KV-caches. Required: {total_per_block_footprint}, Available: {available_kv_memory}\"\n                )\n\n            num_blocks = available_kv_memory // total_per_block_footprint\n\n            # In a multi-process setting, we need to ensure that all processes have the same\n            # KV cache capacity to ensure scheduling guarantees are equivalent on all ranks.\n            if dist.get_world_size(group=mp_group) > 1:\n                reduce_tensor = torch.tensor(num_blocks, dtype=torch.int32, device=get_accelerator().current_device())\n                dist.all_reduce(reduce_tensor, op=ReduceOp.MIN, group=mp_group)\n                num_blocks = reduce_tensor.item()\n\n                # This is ugly but don't want the fragmentation of the 8 byte Tensor maybe\n                # hanging around.\n                del reduce_tensor\n                get_accelerator().empty_cache()\n        else:  # AllocationMode.ALLOCATE\n            num_blocks = self._memory_config.size\n\n        caches = []\n        allocators = []\n\n        for cache_group_id, config in enumerate(self._configs):\n            num_caches = config.cache_shape[0]\n            num_heads = config.cache_shape[1]\n            head_size = config.cache_shape[2]\n\n            alloc_shape = (num_caches, num_blocks, config.block_size, 2, num_heads, head_size)\n            inference_logger().info(\n                f\"Allocating KV-cache {cache_group_id} with shape: {alloc_shape} consisting of {num_blocks} blocks.\")\n            caches.append(torch.empty(alloc_shape, dtype=config.cache_dtype,\n                                      device=get_accelerator().current_device()))\n            allocators.append(BlockedAllocator(num_blocks))\n\n        self._caches = tuple(caches)\n        self._allocators = tuple(allocators)\n\n    def reserve(self, num_blocks: int, cache_group: int = 0) -> torch.Tensor:\n        \"\"\"\n        Reserve a number of blocks from the cache. This will return a 1D tensor of\n        block_ids that have been marked as reserved.\n\n        Parameters:\n            num_blocks (int): The number of blocks to reserve.\n            cache_group (int): The cache group to reserve from. Default is 0.\n        \"\"\"\n        return self._allocators[cache_group].allocate(num_blocks)\n\n    def free(self, blocks: Iterable[int], cache_group: int = 0) -> None:\n        \"\"\"\n        Free a set of blocks from the cache. This will mark the blocks as free in the\n        allocator.\n\n        Parameters:\n            blocks (Iterable[int]): The blocks to free.\n            cache_group (int): The cache group to free from. Default is 0.\n        \"\"\"\n        self._allocators[cache_group].free(blocks)\n\n    def offload(self, blocks: Iterable[int], cache_group: int = 0) -> torch.Tensor:\n        \"\"\"\n        Offload KV-cache blocks from accelerator memory to the host.\n\n        Parameters:\n            blocks (Iterable[int]): The blocks to offload.\n            cache_group (int): The cache group to offload from. Default is 0.\n        \"\"\"\n        raise NotImplementedError(\"Offloading is not yet supported.\")\n\n    def restore(self, blocks: Iterable[int], cache_group: int = 0) -> torch.Tensor:\n        \"\"\"\n        Restore KV-cache blocks from the host to accelerator memory.\n\n        Parameters:\n            blocks (Iterable[int]): The blocks to restore.\n            cache_group (int): The cache group to restore to. Default is 0.\n        \"\"\"\n        raise NotImplementedError(\"Offloading is not yet supported.\")\n\n    def get_cache(self, cache_id: int, cache_group: int = 0) -> torch.Tensor:\n        \"\"\"\n        Get the tensor associated with the given cache ID.\n\n        Parameters:\n            cache_id (int): The ID of the cache tensor to get.\n            cache_group (int): The cache group to get from. Default is 0.\n        \"\"\"\n        return self._caches[cache_group][cache_id]\n\n    @property\n    def free_blocks(self) -> torch.Tensor:\n        \"\"\"\n        Return the number of free blocks in each cache\n        \"\"\"\n        return [allocator.free_blocks for allocator in self._allocators]\n\n    @property\n    def num_caches(self) -> int:\n        \"\"\"\n        Return the number of caches\n        \"\"\"\n        return len(self._caches)\n", "deepspeed/inference/v2/ragged/ragged_wrapper.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Optional\n\nimport torch\n\nfrom deepspeed.accelerator import get_accelerator\nfrom deepspeed.ops.op_builder import RaggedUtilsBuilder\n\nfrom .sequence_descriptor import DSSequenceDescriptor\nfrom .manager_configs import DSStateManagerConfig\n\n\ndef to_padded(original_size: int) -> int:\n    \"\"\"\n    Pad to a backend friendly granularity.\n    \"\"\"\n\n    def _pad_to_mul_of_pow2(val: int, pow_2_val: int) -> int:\n        return val + (pow_2_val - 1) & ~(pow_2_val - 1)\n\n    # TODO(cmikeh2): Tune this approach. This is mainly a placeholder right now.\n    granularity = 64 if original_size <= 512 else 128\n\n    return _pad_to_mul_of_pow2(original_size, granularity)\n\n\nclass RaggedBatchWrapper:\n    \"\"\"\n    Container for all the auxiliary Tensors used in the management of a ragged batch.\n\n    For each Tensor, we maintain a shadow Tensor on the host. This Tensor is what is\n    directly populated when constructing the ragged batch. The shadow Tensors, when possible,\n    should be allocated so as to support fast host-to-accelerator copies.\n    \"\"\"\n\n    # Tensors to populate the ragged batch into.\n    _input_ids_shadow: torch.Tensor\n    _input_ids: torch.Tensor\n    \"\"\"\n    Forward pass input buffer.\n    \"\"\"\n\n    _batch_metadata_storage: torch.Tensor\n    _batch_metadata_storage_shadow: torch.Tensor\n    \"\"\"\n    Holds the number of inflight sequences and tokens for the ragged batch.\n    \"\"\"\n\n    _token_to_seq_storage: torch.Tensor\n    _token_to_seq_storage_shadow: torch.Tensor\n    \"\"\"\n    Linear mapping for each of the tokens. Let's say we have 8 tokens in the batch,\n    with the sequence breakdown being [4, 1, 3]. Then, the mapping would be:\n    [0, 0, 0, 0, 1, 2, 2, 2]\n    \"\"\"\n\n    _inflight_seq_descriptors: torch.Tensor\n    _inflight_seq_descriptors_shadow: torch.Tensor\n    \"\"\"\n    For each sequence in the batch, we store the start token in the batch, the number of tokens\n    the number of tokens in the history of this sequence, and an unused 4th reserved for alignment.\n    For the above example this would give:\n    [[0, 4, H0, X], [4, 1, H1, X], [5, 3, H2, X]]\n    \"\"\"\n\n    # Holds the block ids for each sequence in the ragged batch.\n    _kv_ptrs: torch.Tensor\n    _kv_ptrs_shadow: torch.Tensor\n    \"\"\"\n    List of ptrs pointing to the GPU buffer that holds the KV-block ids for each sequence.\n    If there are multiple allocation groups associated with each of the sequences, then\n    then accessing the Nth cache will require accessing the Nth block id\n    \"\"\"\n\n    def __init__(self, config: DSStateManagerConfig) -> None:\n        \"\"\"\n        Convenience wrapper around the data structures used to represent a ragged\n        batch for inference. Only a single `RaggedBatchWrapper` should be used per\n        ragged inference engine.\n\n        The underlying data structures are implemented in `ragged_batch_descriptor.h`.\n        \"\"\"\n        self._config = config\n        self._input_ids = torch.zeros((self._config.max_ragged_batch_size),\n                                      dtype=torch.int64,\n                                      device=get_accelerator().current_device())\n\n        self._batch_metadata_storage = torch.zeros(2, dtype=torch.int32, device=get_accelerator().current_device())\n\n        self._token_to_seq_storage = torch.zeros((self._config.max_ragged_batch_size),\n                                                 dtype=torch.int32,\n                                                 device=get_accelerator().current_device())\n        self._inflight_seq_descriptors = torch.zeros((self._config.max_ragged_sequence_count, 4),\n                                                     dtype=torch.int32,\n                                                     device=get_accelerator().current_device())\n        self._kv_ptrs = torch.zeros((self._config.max_ragged_sequence_count),\n                                    dtype=torch.int64,\n                                    device=get_accelerator().current_device())\n\n        self._utils_module = RaggedUtilsBuilder().load()\n        host_alloc = self._utils_module.allocate_fast_host_buffer\n\n        self._input_ids_shadow = host_alloc(self._input_ids)\n        self._batch_metadata_storage_shadow = host_alloc(self._batch_metadata_storage)\n        self._token_to_seq_storage_shadow = host_alloc(self._token_to_seq_storage)\n        self._inflight_seq_descriptors_shadow = host_alloc(self._inflight_seq_descriptors)\n        self._kv_ptrs_shadow = host_alloc(self._kv_ptrs)\n\n        # Default behavior should be no padding\n        self._is_padded = False\n\n        self._current_tokens = 0\n        self._current_sequences = 0\n        self._batch_tokens = []\n        self._inflight_seq_descriptors_shadow_buf = []\n        self._kv_blocks_ptr_buf = []\n        self._token_to_seq_storage_shadow_buf = []\n\n    def clear(self) -> None:\n        \"\"\"\n        Clear the ragged batch. This will reset the number of tokens and sequences to 0.\n        \"\"\"\n        self._current_tokens = 0\n        self._current_sequences = 0\n        self._batch_tokens = []\n        self._inflight_seq_descriptors_shadow_buf = []\n        self._kv_blocks_ptr_buf = []\n        self._token_to_seq_storage_shadow_buf = []\n\n    def insert_sequence(self, seq_descriptor: DSSequenceDescriptor, tokens: torch.Tensor, do_checks=True) -> None:\n        \"\"\"\n        Incrementally insert a sequence into the ragged batch. This will update the\n        metadata for the ragged batch and the sequence.\n\n        Arguments:\n            seq_descriptor ()\n        \"\"\"\n        if tokens.device != torch.device(\"cpu\"):\n            # This doesn't really fall under schedulability, so we'll unconditionally check for it.\n            raise RuntimeError(f\"Expected tokens to be on host but found device '{tokens.device}'\")\n\n        if do_checks and self.current_sequences == self._config.max_ragged_sequence_count:\n            raise RuntimeError(f\"Ragged batch is full due to sequence limit: {self._config.max_ragged_sequence_count}\")\n\n        seq_tokens = tokens.numel()\n\n        if do_checks and self.current_tokens + seq_tokens > self._config.max_ragged_batch_size:\n            raise RuntimeError(f\"Ragged batch is full due to capacity limit: {self._config.max_ragged_batch_size})\")\n\n        # The values in _inflight_seq_descriptors_shadow_buf, _token_to_seq_storage_shadow_buf, _kv_blocks_ptr_buf, etc.,\n        # are ultimately stored in PyTorch tensors: _inflight_seq_descriptors_shadow, _token_to_seq_storage_shadow, _kv_ptrs_shadow, etc.\n        # However, we found it inefficient to iterate over and substitute values into tensor slices or to use copy/fill calls for this purpose.\n        # Therefore, we initially store the values in Python lists or primitive data types and then copy them collectively in the finalize() method,\n        # instead of updating the tensors directly in each iteration.\n        self._batch_tokens.append(tokens)\n        self._inflight_seq_descriptors_shadow_buf.append(self.current_tokens)\n        self._inflight_seq_descriptors_shadow_buf.append(seq_tokens)\n        self._inflight_seq_descriptors_shadow_buf.append(seq_descriptor.seen_tokens)\n        self._inflight_seq_descriptors_shadow_buf.append(0)  # alignment\n\n        self._token_to_seq_storage_shadow_buf.extend([self.current_sequences] * seq_tokens)\n\n        self._kv_blocks_ptr_buf.append(seq_descriptor.kv_blocks_ptr)\n\n        self._current_tokens += seq_tokens\n        self._current_sequences += 1\n\n    @property\n    def tensor_toks(self) -> torch.Tensor:\n        \"\"\"\n        The number of tokens in the in-flight ragged batch. This will not trigger\n        synchronization with the device.\n        \"\"\"\n        cur_toks = self.current_tokens\n        if self._is_padded:\n            return to_padded(cur_toks)\n        else:\n            return cur_toks\n\n    def finalize(self, padding: Optional[bool] = False) -> None:\n        \"\"\"\n        Completes construction of the ragged batch by flushing the host buffers to the device.\n        \"\"\"\n        cur_toks = self.current_tokens\n\n        # Batch-copy the values recorded in insert_sequence() into PyTorch tensors to enhance efficiency.\n        self._inflight_seq_descriptors_shadow.flatten()[:len(self._inflight_seq_descriptors_shadow_buf)].copy_(\n            torch.tensor(self._inflight_seq_descriptors_shadow_buf))\n        self._input_ids_shadow[:self.current_tokens].copy_(torch.cat(self._batch_tokens, dim=0))\n        self._token_to_seq_storage_shadow[:len(self._token_to_seq_storage_shadow_buf)].copy_(\n            torch.tensor(self._token_to_seq_storage_shadow_buf))\n        self._kv_ptrs_shadow[:len(self._kv_blocks_ptr_buf)].copy_(torch.tensor(self._kv_blocks_ptr_buf))\n        self._batch_metadata_storage_shadow.copy_(torch.tensor([cur_toks, self.current_sequences]))\n\n        if padding:\n            padded_toks = to_padded(cur_toks)\n            self._input_ids_shadow[cur_toks:padded_toks].fill_(-1)\n            self._token_to_seq_storage_shadow[cur_toks:padded_toks].fill_(-1)\n            self._is_padded = True\n        else:\n            padded_toks = cur_toks\n            self._is_padded = False\n\n        current_sequences = self.current_sequences\n\n        def _noblock_copy(dst: torch.Tensor, src: torch.Tensor) -> None:\n            dst.copy_(src, non_blocking=True)\n\n        _noblock_copy(self._input_ids[:padded_toks], self._input_ids_shadow[:padded_toks])\n        _noblock_copy(self._batch_metadata_storage, self._batch_metadata_storage_shadow)\n        _noblock_copy(self._token_to_seq_storage[:padded_toks], self._token_to_seq_storage_shadow[:padded_toks])\n        _noblock_copy(self._inflight_seq_descriptors[:current_sequences],\n                      self._inflight_seq_descriptors_shadow[:current_sequences])\n        _noblock_copy(self._kv_ptrs[:current_sequences], self._kv_ptrs_shadow[:current_sequences])\n\n    def input_ids(self, on_device: bool = True) -> torch.Tensor:\n        \"\"\"\n        The input ids tensor for the ragged batch. If the device Tensor is requested, the Tensor\n        is truncated to the number of tokens in the batch.\n        \"\"\"\n        if on_device:\n            return self._input_ids[:self.tensor_toks]\n        else:\n            return self._input_ids_shadow\n\n    def batch_metadata_buffer(self, on_device: bool = True) -> torch.Tensor:\n        \"\"\"\n        Buffer associated with the batch metadata tensor that can\n        be populated in preparation for passing a new input to the device.\n        \"\"\"\n        if on_device:\n            return self._batch_metadata_storage\n        else:\n            return self._batch_metadata_storage_shadow\n\n    def tokens_to_seq(self, on_device: bool = True) -> torch.Tensor:\n        \"\"\"\n        Mapping of token to which sequence it belongs to in the ragged batch. If the device Tensor\n        is requested, the Tensor is truncated to the number of tokens in the batch.\n        \"\"\"\n        if on_device:\n            return self._token_to_seq_storage[:self.tensor_toks]\n        else:\n            return self._token_to_seq_storage_shadow\n\n    def inflight_seq_descriptors(self, on_device: bool = True) -> torch.Tensor:\n        \"\"\"\n        Buffer associated with the metadata of each sequence in the ragged batch. If the device Tensor\n        is requested, the Tensor is truncated to the number of sequences in the batch.\n        \"\"\"\n        if on_device:\n            return self._inflight_seq_descriptors[:self.current_sequences]\n        else:\n            return self._inflight_seq_descriptors_shadow\n\n    def kv_ptrs(self, on_device: bool = True) -> torch.Tensor:\n        \"\"\"\n        Pointer to where the list of KV ids associated with a sequence are. If the device Tensor\n        is requested, the Tensor is truncated to the number of sequences in the batch.\n        \"\"\"\n        if on_device:\n            return self._kv_ptrs[:self.current_sequences]\n        else:\n            return self._kv_ptrs_shadow\n\n    def masks(self, on_device: bool = True) -> Optional[torch.Tensor]:\n        \"\"\"\n        Placeholder for supporting complex masks. Currently not supported.\n\n        Models that will need this will be BERT-like, not generative.\n        \"\"\"\n        return None\n\n    @property\n    def current_tokens(self) -> int:\n        \"\"\"\n        The number of tokens in the in-flight ragged batch. This will not trigger\n        synchronization with the device.\n        \"\"\"\n        return self._current_tokens\n\n    @property\n    def current_sequences(self) -> int:\n        \"\"\"\n        The number of sequences in the in-flight ragged batch. This will not trigger\n        synchronization with the device.\n        \"\"\"\n        return self._current_sequences\n", "deepspeed/inference/v2/ragged/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .kv_cache import split_kv\nfrom .manager_configs import (\n    AllocationMode,\n    DSStateManagerConfig,\n    KVCacheConfig,\n    MemoryConfig,\n)\nfrom .ragged_manager import DSStateManager\nfrom .ragged_wrapper import RaggedBatchWrapper\nfrom .sequence_descriptor import DSSequenceDescriptor, PlaceholderSequenceDescriptor\n", "deepspeed/inference/v2/ragged/ragged_manager.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nfrom typing import Any, Dict, Optional, Tuple\n\nfrom deepspeed.accelerator import get_accelerator\nfrom deepspeed.ops.op_builder import RaggedUtilsBuilder\nfrom deepspeed.utils.logging import logger\n\nfrom .blocked_allocator import BlockedAllocator\nfrom .kv_cache import BlockedKVCache\nfrom .manager_configs import DSStateManagerConfig, KVCacheConfig\nfrom .sequence_descriptor import DSSequenceDescriptor\n\n\nclass DSStateManager:\n    \"\"\"\n    Base abstract class for managing blocked KV caches. Will probably have a single\n    implementation for now.\n    \"\"\"\n\n    _config: DSStateManagerConfig\n    \"\"\"\n    Config for state management. See DSStateManagerConfig for more details. The arguments here\n    should come from the engine config.\n    \"\"\"\n\n    _kv_configs: Tuple[KVCacheConfig]\n    \"\"\"\n    Config for the KV cache. See KVCacheConfig for more details. These arguments should derive\n    from the model implementation.\n    \"\"\"\n\n    _kv_cache: BlockedKVCache\n    \"\"\"\n    Persistent KV cache store.\n    \"\"\"\n\n    # Container for tracking all sequences in the system.\n    _seqs: Dict[int, DSSequenceDescriptor]\n    \"\"\"\n    Container for tracking all sequences in the system.\n\n    TODO(cmikeh2): Evaluate if this has any performance implications.\n    \"\"\"\n\n    # Allocator for tracking sequences.\n    _tracking_allocator: BlockedAllocator\n    _all_block_ids: Tuple[torch.Tensor, ...]\n    _all_block_ids_shadow: Tuple[torch.Tensor, ...]\n\n    def __init__(self,\n                 config: DSStateManagerConfig,\n                 kv_configs: Tuple[KVCacheConfig, ...],\n                 base_mp_group: Optional[Any] = None) -> None:\n        \"\"\"\n        The key\n\n        Parameters:\n            block_size (int): The number of tokens to allocate in each block.\n        \"\"\"\n        self._config = config\n        self._kv_configs = kv_configs\n\n        # Load our helpers for host allocation.\n        self._ragged_utils = RaggedUtilsBuilder().load()\n\n        # Initialize the allocator for tracking sequences (so this doesn't need to be ad-hoc).\n        self._tracking_allocator = BlockedAllocator(self._config.max_tracked_sequences)\n\n        all_block_ids = []\n        all_block_ids_shadow = []\n\n        for cache_config in self._kv_configs:\n            # Storage to back tracking the KV cache allocation.\n            ids_shape = (\n                self._config.max_tracked_sequences,\n                cache_config.num_allocation_groups,\n                cache_config.max_blocks_per_allocation_group,\n            )\n\n            all_block_ids.append(torch.zeros(ids_shape, dtype=torch.int32, device=get_accelerator().current_device()))\n            all_block_ids_shadow.append(self._ragged_utils.allocate_fast_host_buffer(all_block_ids[-1]))\n\n        self._all_block_ids = tuple(all_block_ids)\n        self._all_block_ids_shadow = tuple(all_block_ids_shadow)\n\n        # Initialize the sequence container.\n        self._seqs = {}\n\n        # Finally initialize the KV cache.\n        self._kv_cache = BlockedKVCache(self._kv_configs,\n                                        self._config.memory_config,\n                                        mp_group=base_mp_group,\n                                        offload=self._config.offload)\n\n    def get_cache(self, cache_id: int, cache_group: int = 0) -> torch.Tensor:\n        \"\"\"\n        Return the Tensor associated with the given cache id in the specified cache group.\n\n        Arguments:\n            cache_group (str): The KV cache group.\n            cache_id (int): The cache id within that group.\n        \"\"\"\n        return self._kv_cache.get_cache(cache_id, cache_group=cache_group)\n\n    def flush_sequence(self, uid: int) -> None:\n        \"\"\"\n        Free all resources associated with the given sequence id.\n        \"\"\"\n        if uid not in self._seqs:\n            logger.warning(f\"Attempting to flush sequence {uid} which does not exist.\")\n            return\n\n        seq = self._seqs[uid]\n        for i in range(self.n_kv_cache_groups):\n            self._kv_cache.free(seq.all_block_ids(cache_group=i), cache_group=i)\n\n        self._tracking_allocator.free(seq.tracking_id)\n        del self._seqs[uid]\n\n    def get_sequence(self, uid: int) -> Optional[DSSequenceDescriptor]:\n        \"\"\"\n        Get the sequence descriptor for the given sequence id. If the sequence does not exist,\n        then None is returned.\n        \"\"\"\n        return self._seqs.get(uid, None)\n\n    def get_or_create_sequence(self, uid: int) -> DSSequenceDescriptor:\n        \"\"\"\n        Get the existing sequence descriptor for a given uid or initialize one if\n        it does not exist. NOTE: This will always return a valid sequence descriptor\n        if one may be allocated and should not be used from APIs that are attempting\n        to test the schedulability of a hypothetical batch.\n        \"\"\"\n        seq = self.get_sequence(uid)\n        if seq is not None:\n            return seq\n        else:\n            return self._create_sequence(uid)\n\n    def _create_sequence(self, uid: int) -> DSSequenceDescriptor:\n        \"\"\"\n        Create a new sequence descriptor for the given sequence id.\n        \"\"\"\n        if uid in self._seqs:\n            raise ValueError(f\"Sequence {uid} already exists.\")\n\n        try:\n            tracking_slot = self._tracking_allocator.allocate(1).item()\n        except ValueError:\n            raise RuntimeError(\n                f\"Unable to create tracking slot for sequence {uid} since the metadata buffers are full.\")\n\n        seq_block_ids = tuple(all_block_ids[tracking_slot] for all_block_ids in self._all_block_ids)\n        seq_block_ids_shadow = tuple(all_block_ids_shadow[tracking_slot]\n                                     for all_block_ids_shadow in self._all_block_ids_shadow)\n\n        self._seqs[uid] = DSSequenceDescriptor(tracking_slot,\n                                               seq_block_ids,\n                                               seq_block_ids_shadow,\n                                               max_context=self._config.max_context)\n        # TODO(cmikeh2): Debug call here might be unnecessary and is potentially on critical path.\n        logger.debug(f\"Created sequence {uid} with tracking slot {tracking_slot}.\")\n        return self._seqs[uid]\n\n    @property\n    def tracked_sequences(self) -> Dict[int, DSSequenceDescriptor]:\n        \"\"\"\n        Return the tracked sequences.\n        \"\"\"\n        return self._seqs\n\n    @property\n    def n_tracked_sequences(self) -> int:\n        \"\"\"\n        Return the number of sequences currently tracked.\n        \"\"\"\n        return len(self._seqs)\n\n    @property\n    def kv_block_size(self) -> int:\n        \"\"\"\n        Return the block size of the KV cache.\n        \"\"\"\n        return self._kv_config.block_size\n\n    @property\n    def n_kv_cache_groups(self) -> int:\n        \"\"\"\n        Return the number of KV caches.\n        \"\"\"\n        return self._kv_cache.num_caches\n\n    @property\n    def free_blocks(self) -> torch.Tensor:\n        \"\"\"\n        Return the number of free blocks in the KV cache.\n        \"\"\"\n        return self._kv_cache.free_blocks\n\n    def allocate_blocks(self, n_blocks: int, cache_group: int = 0) -> torch.Tensor:\n        return self._kv_cache.reserve(n_blocks, cache_group=cache_group)\n", "deepspeed/inference/v2/ragged/manager_configs.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom enum import Enum\nfrom typing import Tuple\n\nfrom deepspeed.pydantic_v1 import PositiveInt, validator\n\nfrom deepspeed.runtime.config_utils import DeepSpeedConfigModel\nfrom ..inference_utils import DtypeEnum\n\n\nclass KVCacheType(Enum):\n\n    DENSE = \"dense\"\n    \"\"\"\n    Dense KV-cache. This is the default type.\n    \"\"\"\n\n    LOCAL = \"local\"\n    \"\"\"\n    KV-cache that attends to only a local (trailing) window of tokens.\n    \"\"\"\n\n\nclass KVCacheConfig(DeepSpeedConfigModel):\n\n    type: KVCacheType = KVCacheType.DENSE\n    \"\"\"\n    Type of KV-cache to use. This may inform the allocator of the expected access/retention pattern\n    to enable more efficient memory management.\n    \"\"\"\n\n    block_size: int = 128\n    \"\"\"\n    Number of tokens that may be contained in each cache block.\n    \"\"\"\n\n    num_allocation_groups: PositiveInt = 1\n    \"\"\"\n    Allocation groups are assumed to be able to use the same allocation block size because\n    the allocation granularity is the same but the number of blocks required in each group\n    may differ.\n\n    As a concrete example, consider a model with alternating layers of local and global\n    attention (such as GPTNeo). The local attention layers do not require the same number\n    of cache blocks as the global layer. However, a static partitioning scheme is sub-optimal since the ratio of local to global KV-cache blocks is not constant across\n    the range of sequence lengths that may be encountered.\n\n    NOTE: In theory, this functionality could be used to do per-head and per-layer\n    KV-cache allocation, but it is likely the allocator will struggle with managing that\n    many blocks.\n\n    NOTE: This will need to be primarily understood and handled by the model implementation\n    itself, rather than the KV cache manager. However, I'd like to make this explicit.\n    \"\"\"\n\n    cache_shape: Tuple[PositiveInt, PositiveInt, PositiveInt]\n    \"\"\"\n    The shape of the cache per token. The first dimension is the number of individual\n    caches, the second is the number of heads, and the third is the head size. The number\n    of caches argument here is per allocation group.\n    \"\"\"\n\n    cache_dtype: DtypeEnum = DtypeEnum.fp16\n    \"\"\"\n    Data type of the KV-cache.\n    \"\"\"\n\n    max_blocks_per_allocation_group: PositiveInt = 64\n    \"\"\"\n    Maximum number of blocks that can be associated with an allocation group.\n    \"\"\"\n\n\n\"\"\"\nThe config above is a little confusing so let's use a couple of concrete examples of\nusage:\n\nModel 1: Llama-13B with a block size of 256\n\nLlama is uniform attention so we have a single allocation group. The cache shape is\n(40 layers, 40 heads, 128 head size)\n\n```python\nllama_kv_config = KVCacheConfig(block_size=256,\n                                num_allocation_groups=1,\n                                cache_shape=(40, 40, 128))\n```\n\nModel 2: GPTNeo-2.7B with a block size of 128\n\nGPTNeo has alternating local and global attention layers. We have two allocation groups.\nThere are 16 layers of each type with 20 heads apiece at 128 head size.\n\n```python\ngptneo_kv_config = KVCacheConfig(num_allocation_groups=2, cache_shape=(16, 20, 128))\n```\n\"\"\"\n\n\nclass AllocationMode(Enum):\n    \"\"\"\n    Helper class to describe memory allocation strategies for the KV-cache.\n    \"\"\"\n\n    RESERVE = \"reserve\"\n    \"\"\"\n    Reserve a small amount of memory for non-KV cache allocations.\n    \"\"\"\n\n    ALLOCATE = \"allocate\"\n    \"\"\"\n    Allocate an explicit number of KV blocks.\n    \"\"\"\n\n\nclass MemoryConfig(DeepSpeedConfigModel):\n\n    mode: AllocationMode = AllocationMode.RESERVE\n\n    size: PositiveInt = 1_000_000_000\n    \"\"\"\n    Parameter for each of the modes.\n\n    If mode is RESERVE, this is the amount of memory in bytes to reserve after allocating the\n    KV-cache. If in a tensor-parallel regime, this amount is guaranteed to be reserved on\n    all devices.\n\n    If mode is ALLOCATE, this is the number of blocks to allocate for the KV-cache. This may\n    require tuning for model/GPU setups.\n    \"\"\"\n\n\nclass DSStateManagerConfig(DeepSpeedConfigModel):\n\n    max_tracked_sequences: PositiveInt = 2048\n    \"\"\"\n    How many sequences this engine will track simultaneously. This limit should be greater\n    than the ``max_ragged_sequence_count``.\n    \"\"\"\n\n    max_ragged_batch_size: PositiveInt = 768\n    \"\"\"\n    The maximum number of tokens that can be contained in a single ragged batch. Passing\n    a larger value than this will raise an exception that must be handled by the runtime.\n    \"\"\"\n\n    max_ragged_sequence_count: PositiveInt = 512\n    \"\"\"\n    The maximum number of sequences that can compose a batch. This limitation is only\n    relevant under CUDA graphing scenarios currently, where the maximum number of blocks\n    is largely bound by the total number of sequences in the ragged batch. This number cannot\n    be larger than ``max_tracked_sequences`` or ``max_ragged_batch_size``.\n    \"\"\"\n\n    max_context: PositiveInt = 8192\n    \"\"\"\n    The maximum number of tokens (inclusive of generation) that can be contained in a single\n    sequence. Currently used to bound the size of the KV cache metadata.\n    \"\"\"\n\n    memory_config: MemoryConfig = MemoryConfig()\n    \"\"\"\n    Directive for how to manage the creation of the KV-cache. See MemoryConfig for more\n    details.\n    \"\"\"\n\n    offload: bool = False\n    \"\"\"\n    Enable tracking for offloading KV-cache to host memory. Currently unsupported.\n    \"\"\"\n\n    @validator(\"max_ragged_sequence_count\")\n    def max_ragged_sequence_count_validator(cls, v: int, values: dict):\n        # If the attributes below failed their validation they won't appear in the values dict.\n        if \"max_tracked_sequences\" in values and v > values[\"max_tracked_sequences\"]:\n            raise ValueError(\"max_ragged_sequence_count must be less than max_tracked_sequences\")\n        if \"max_ragged_batch_size\" in values and v > values[\"max_ragged_batch_size\"]:\n            raise ValueError(\"max_ragged_sequence_count must be less than max_ragged_batch_size\")\n        return v\n", "deepspeed/inference/v2/modules/heuristics.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom ..config_v2 import RaggedInferenceEngineConfig\nfrom ..inference_utils import NormTypeEnum\n\nfrom .module_registry import ConfigBundle\nfrom ..modules.configs import (\n    DSEmbeddingsConfig,\n    DSLinearConfig,\n    DSMoEConfig,\n    DSNormConfig,\n    DSSelfAttentionConfig,\n    DSUnembedConfig,\n)\nfrom ..modules.interfaces import (\n    DSEmbeddingBase,\n    DSEmbeddingRegistry,\n    DSLinearBase,\n    DSLinearRegistry,\n    DSMoEBase,\n    DSMoERegistry,\n    DSPostNormBase,\n    DSPostNormRegistry,\n    DSPreNormBase,\n    DSPreNormRegistry,\n    DSSelfAttentionBase,\n    DSSelfAttentionRegistry,\n    DSUnembedBase,\n    DSUnembedRegistry,\n)\n\n\ndef instantiate_attention(attention_config: DSSelfAttentionConfig,\n                          engine_config: RaggedInferenceEngineConfig) -> DSSelfAttentionBase:\n    \"\"\"\n    Choose an appropriate attention implementation based on the given configurations. This\n    method is currently a stub, but as more implementations may be developed  we can centralize\n    the logic for choosing between them here.\n\n    Arguments:\n        attention_config (DSSelfAttentionConfig): Configuration for the attention module.\n        engine_config (RaggedInferenceEngineConfig): Configuration for the inference engine.\n\n    Returns:\n        An attention module implementing the given configuration.\n    \"\"\"\n\n    # Currently, we only have one implementation, so we just return it.\n    config = ConfigBundle(name=\"dense_blocked_attention\", config=attention_config)\n    return DSSelfAttentionRegistry.instantiate_config(config)\n\n\ndef instantiate_embed(embed_config: DSEmbeddingsConfig, engine_config: RaggedInferenceEngineConfig) -> DSEmbeddingBase:\n    \"\"\"\n    Choose an appropriate embedding implementation based on the given configurations. This\n    method is currently a stub, but as more implementations may be developed  we can centralize\n    the logic for choosing between them here.\n\n    Arguments:\n        embed_config (DSEmbeddingsConfig): Configuration for the embedding module.\n        engine_config (RaggedInferenceEngineConfig): Configuration for the inference engine.\n\n    Returns:\n        An embedding module implementing the given configuration.\n    \"\"\"\n\n    # Currently, we only have one implementation, so we just return it.\n    config = ConfigBundle(name=\"ragged_embedding\", config=embed_config)\n    return DSEmbeddingRegistry.instantiate_config(config)\n\n\ndef instantiate_linear(linear_config: DSLinearConfig, engine_config: RaggedInferenceEngineConfig) -> DSLinearBase:\n    \"\"\"\n    Choose an appropriate linear implementation based on the given configurations. This\n    method is currently a stub, but as more implementations may be developed  we can centralize\n    the logic for choosing between them here.\n\n    Arguments:\n        linear_config (DSLinearConfig): Configuration for the linear module.\n        engine_config (RaggedInferenceEngineConfig): Configuration for the inference engine.\n\n    Returns:\n        A linear module implementing the given configuration.\n    \"\"\"\n\n    quantization_mode = engine_config.quantization.quantization_mode\n    if quantization_mode is None:\n        config = ConfigBundle(name=\"blas_fp_linear\", config=linear_config)\n    else:\n        # Currently, we only support ``quantized_wf6af16_linear`` on NVIDIA Ampere GPUs.\n        if quantization_mode == \"wf6af16\":\n            import torch\n            if not torch.cuda.is_available():  #ignore-cuda\n                raise ValueError(\"WF6AF16 quantization is only supported on CUDA\")\n            else:\n                is_rocm_pytorch = hasattr(torch.version, 'hip') and torch.version.hip is not None\n                if is_rocm_pytorch:\n                    raise ValueError(\"WF6AF16 quantization is only supported on NVIDIA GPUs\")\n                elif torch.cuda.get_device_properties(0).major != 8:  #ignore-cuda\n                    raise ValueError(\"WF6AF16 quantization is only supported on Ampere architectures\")\n            config = ConfigBundle(name=\"quantized_wf6af16_linear\", config=linear_config)\n        else:\n            raise ValueError(f\"Unsupported quantization mode: {quantization_mode}\")\n    return DSLinearRegistry.instantiate_config(config)\n\n\ndef instantiate_moe(moe_config: DSMoEConfig, engine_config: RaggedInferenceEngineConfig) -> DSMoEBase:\n    \"\"\"\n    Choose an appropriate MoE implementation based on the given configurations. This\n    method is currently a stub, but as more implementations may be developed  we can centralize\n    the logic for choosing between them here.\n\n    Arguments:\n        moe_config (DSMoEConfig): Configuration for the MoE module.\n        engine_config (RaggedInferenceEngineConfig): Configuration for the inference engine.\n\n    Returns:\n        A MoE module implementing the given configuration.\n    \"\"\"\n\n    moe_type = \"cutlass_multi_gemm_moe\"\n\n    if moe_type == \"cutlass_multi_gemm_moe\":\n        # TODO: Get this off an engine config\n        implementation_config = {\n            \"weight_dtype\": moe_config.input_dtype,\n        }\n\n    # Currently, we only have one implementation, so we just return it.\n    config = ConfigBundle(name=\"cutlass_multi_gemm_moe\",\n                          config=moe_config,\n                          implementation_config=implementation_config)\n    return DSMoERegistry.instantiate_config(config)\n\n\ndef instantiate_post_norm(norm_config: DSNormConfig, engine_config: RaggedInferenceEngineConfig) -> DSPostNormBase:\n    \"\"\"\n    Choose an appropriate post-norm implementation based on the given configurations. This\n    method is currently a stub, but as more implementations may be developed  we can centralize\n    the logic for choosing between them here.\n\n    Arguments:\n        norm_config (DSNormConfig): Configuration for the post-norm module.\n        engine_config (RaggedInferenceEngineConfig): Configuration for the inference engine.\n\n    Returns:\n        A post-norm module implementing the given configuration.\n    \"\"\"\n\n    # Currently, we only have one implementation, so we just return it.\n    config = ConfigBundle(name=\"cuda_post_ln\", config=norm_config)\n    return DSPostNormRegistry.instantiate_config(config)\n\n\ndef instantiate_pre_norm(norm_config: DSNormConfig, engine_config: RaggedInferenceEngineConfig) -> DSPreNormBase:\n    \"\"\"\n    Choose an appropriate pre-norm implementation based on the given configurations. Currently,\n    this will select between two CUDA implementations, one for LayerNorm and one for RMSNorm.\n\n    Arguments:\n        norm_config (DSNormConfig): Configuration for the pre-norm module.\n        engine_config (RaggedInferenceEngineConfig): Configuration for the inference engine.\n\n    Returns:\n        A pre-norm module implementing the given configuration.\n    \"\"\"\n    if NormTypeEnum(norm_config.type) == NormTypeEnum.LayerNorm:\n        module_name = \"cuda_pre_ln\"\n    elif NormTypeEnum(norm_config.type) == NormTypeEnum.RMSNorm:\n        module_name = \"cuda_pre_rms\"\n\n    config = ConfigBundle(name=module_name, config=norm_config)\n    return DSPreNormRegistry.instantiate_config(config)\n\n\ndef instantiate_unembed(unembed_config: DSUnembedConfig, engine_config: RaggedInferenceEngineConfig) -> DSUnembedBase:\n    \"\"\"\n    Choose an appropriate unembedding implementation based on the given configurations. This\n    method is currently a stub, but as more implementations may be developed  we can centralize\n    the logic for choosing between them here.\n\n    Arguments:\n        unembed_config (DSUnembedConfig): Configuration for the unembed module.\n        engine_config (RaggedInferenceEngineConfig): Configuration for the inference engine.\n\n    Returns:\n        An unembed module implementing the given configuration.\n    \"\"\"\n\n    # Currently, we only have one implementation, so we just return it.\n    config = ConfigBundle(name=\"ragged_unembed\", config=unembed_config)\n    return DSUnembedRegistry.instantiate_config(config)\n", "deepspeed/inference/v2/modules/module_registry.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom abc import ABC, abstractstaticmethod\nfrom typing import Any, Dict, Type\n\nfrom deepspeed.runtime.config_utils import DeepSpeedConfigModel\nfrom .ds_module import DSModuleBase\n\n\nclass ConfigBundle(DeepSpeedConfigModel):\n    \"\"\"\n    A config bundle is a collection of configs that are used to instantiate a model implementation.\n    \"\"\"\n    name: str\n    config: DeepSpeedConfigModel\n    implementation_config: Dict[str, Any] = {}\n\n\nclass DSModuleRegistryBase(ABC):\n    \"\"\"\n    Class holding logic for tracking the DSModule implementations of a given interface.\n    \"\"\"\n\n    @classmethod\n    def instantiate_config(cls, config_bundle: ConfigBundle) -> DSModuleBase:\n        \"\"\"\n        Given a DSModule key, attempt to instantiate\n        \"\"\"\n        if config_bundle.name not in cls.registry:\n            raise KeyError(f\"Unknown DSModule: {config_bundle.name}, cls.registry={cls.registry}\")\n\n        target_implementation = cls.registry[config_bundle.name]\n        if not target_implementation.supports_config(config_bundle.config):\n            raise ValueError(f\"Config {config_bundle.config} is not supported by {target_implementation}\")\n\n        return cls.registry[config_bundle.name](config_bundle.config, config_bundle.implementation_config)\n\n    @abstractstaticmethod\n    def associated_class() -> Type[DSModuleBase]:\n        \"\"\"\n        Return the class associated with this registry.\n        \"\"\"\n        raise NotImplementedError(\"Must associated a DSModule class with its registry.\")\n\n    @classmethod\n    def register_module(cls, child_class: DSModuleBase) -> None:\n        \"\"\"\n        Register a module with this registry.\n        \"\"\"\n        if not issubclass(child_class, cls.associated_class()):\n            raise TypeError(\n                f\"Can only register subclasses of {cls.associated_class()}, {child_class} does not inherit from {cls.associated_class()}\"\n            )\n        cls.registry[child_class.name()] = child_class\n        return child_class\n", "deepspeed/inference/v2/modules/ds_module.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom abc import ABC, abstractstaticmethod\nfrom typing import Any, Dict, Type\n\nimport torch\n\nfrom deepspeed.runtime.config_utils import DeepSpeedConfigModel\n\n\nclass DSModuleConfig(DeepSpeedConfigModel):\n\n    max_tokens: int\n\n\nclass DSModuleBase(torch.nn.Module, ABC):\n    \"\"\"\n    Base class for all DeepSpeed Inference modules. This class establishes\n    the basic attributes of a DSModule. Only abstract functionality modules should inherit\n    directly from this class, not specific implementations.\n    \"\"\"\n\n    @abstractstaticmethod\n    def name() -> str:\n        \"\"\"\n        Return a memorable, human-readable name for this module.\n\n        This will be used as a key in custom inference configurations and should only\n        be implemented by the children of functionality modules.\n        \"\"\"\n        ...\n\n    @abstractstaticmethod\n    def config_class() -> Type[DSModuleConfig]:\n        \"\"\"\n        Return the associated config class for this module.\n\n        This should be implemented (along with the config class) by an abstract functionality\n        module.\n        \"\"\"\n        ...\n\n    @abstractstaticmethod\n    def supports_config(config: DSModuleConfig) -> bool:\n        \"\"\"\n        Return whether or not this module supports the given config.\n\n        This should be implemented by the children of functionality modules and should report\n        whether it would be feasible to instantiate this module with the given config.\n        \"\"\"\n        ...\n\n    def __init__(self, config: DSModuleConfig, implementation_config: Dict[str, Any] = {}) -> None:\n        \"\"\"\n        Initialize the module with the given config.\n        \"\"\"\n        super().__init__()\n        self._config = config\n        self._implementation_config = implementation_config\n", "deepspeed/inference/v2/modules/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom . import implementations\nfrom . import interfaces\nfrom .module_registry import ConfigBundle\n", "deepspeed/inference/v2/modules/interfaces/moe_base.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom abc import abstractmethod\nfrom typing import Any, Dict, Optional, Type\n\nimport torch\n\nfrom deepspeed.runtime.config_utils import DeepSpeedConfigModel\nfrom ..ds_module import DSModuleBase\nfrom ..module_registry import DSModuleRegistryBase\nfrom ..configs import DSMoEConfig\nfrom ...inference_parameter import InferenceParameter\n\n\nclass DSMoEBase(DSModuleBase):\n    \"\"\"\n    Base mixing for MoE modules. The interface represented by this module is:\n\n    expert_assignments = gate(hidden_states)\n    intermediate = ragged_linear(hidden_states, expert_assignments)\n    output = ragged_linear(intermediate, expert_assignments)\n    \"\"\"\n\n    @staticmethod\n    def config_class() -> Type[DeepSpeedConfigModel]:\n        return DSMoEConfig\n\n    def __init__(self, config: DSMoEConfig, implementation_config: Dict[str, Any]) -> None:\n        super().__init__(config, implementation_config)\n\n    @abstractmethod\n    def transform_gate_param(self, param: torch.Tensor) -> InferenceParameter:\n        \"\"\"\n        Perform any necessary transformations of the gate parameter.\n\n        Args:\n            param (torch.Tensor): gate_w (shape: [num_experts, model_dim])\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def transform_moe_mlp_1_param(self, param: torch.Tensor) -> InferenceParameter:\n        \"\"\"\n        Perform any necessary transformations of the parameter. The specific component\n        being transformed should be inferred from the shape of the parameter.\n\n        Args:\n            param (torch.Tensor): One of either mlp_1_w, mlp_1_b\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def transform_moe_mlp_2_param(self, param: torch.Tensor) -> InferenceParameter:\n        \"\"\"\n        Perform any necessary transformations of the parameter. The specified component being\n        transformed should be inferred from the shape of the parameter. This interface is\n        separate from transform_moe_1_param because the two components may have identical\n        shapes.\n\n        Args:\n            param (torch.Tensor): One of either mlp_2_w or mlp_2_b\n        \"\"\"\n        ...\n\n    def forward(self,\n                hidden_states: torch.Tensor,\n                gate_w: torch.Tensor,\n                mlp_1_w: torch.Tensor,\n                mlp_2_w: torch.Tensor,\n                mlp_1_b: Optional[torch.Tensor] = None,\n                mlp_2_b: Optional[torch.Tensor] = None) -> torch.Tensor:\n        raise NotImplementedError()\n\n    @property\n    @abstractmethod\n    def output(self) -> torch.Tensor:\n        \"\"\"\n        Returns the pre-allocated, padded output Tensor.\n        \"\"\"\n        ...\n\n\nclass DSMoERegistry(DSModuleRegistryBase):\n    registry: Dict = {}\n\n    @staticmethod\n    def associated_class() -> Type[DSModuleBase]:\n        return DSMoEBase\n", "deepspeed/inference/v2/modules/interfaces/unembed_base.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Any, Dict, Optional, Type\n\nimport torch\n\nfrom deepspeed.runtime.config_utils import DeepSpeedConfigModel\nfrom ...ragged import RaggedBatchWrapper\nfrom ..ds_module import DSModuleBase\nfrom ..module_registry import DSModuleRegistryBase\nfrom ..configs import DSUnembedConfig\n\n\nclass DSUnembedBase(DSModuleBase):\n    \"\"\"\n    Base mixin for unmebedding modules. The interface represented by this module is:\n\n    if config.do_normalization\n        hidden = layer_norm(hidden)\n    logits = hidden @ projection\n    \"\"\"\n\n    @staticmethod\n    def config_class() -> Type[DeepSpeedConfigModel]:\n        return DSUnembedConfig\n\n    def __init__(self, config: DSUnembedConfig, implementation_config: Dict[str, Any]) -> None:\n        super().__init__(config, implementation_config)\n\n    def forward(self,\n                hidden_states: torch.Tensor,\n                vocab_embedding: torch.Tensor,\n                ragged_metadata: RaggedBatchWrapper,\n                gamma: Optional[torch.Tensor] = None,\n                beta: Optional[torch.Tensor] = None) -> torch.Tensor:\n        \"\"\"\n        Forward interface. Gamma and beta are optional parameters passed depending on\n        `self.config.do_normalization`.\n\n        Args:\n            hidden_states (torch.Tensor): Hidden states of shape [tokens, model_dim]\n            vocab_embedding (torch.Tensor): Embedding matrix of shape [vocab_size, model_dim]\n            ragged_metadata (RaggedBatchWrapper): Metadata for the ragged batch.\n            gamma (Optional[torch.Tensor]): Gamma parameter for layer norm.\n            beta (Optional[torch.Tensor]): Beta parameter for layer norm.\n\n        Returns:\n            torch.Tensor: Unembedded hidden states of shape [n_seqs, model_dim]\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass DSUnembedRegistry(DSModuleRegistryBase):\n    registry: Dict = {}\n\n    @staticmethod\n    def associated_class() -> Type[DSModuleBase]:\n        return DSUnembedBase\n", "deepspeed/inference/v2/modules/interfaces/post_norm_base.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom abc import abstractmethod\nfrom typing import Any, Dict, Optional, Tuple, Type\n\nimport torch\n\nfrom deepspeed.runtime.config_utils import DeepSpeedConfigModel\nfrom ..ds_module import DSModuleBase\nfrom ..configs.norm_config import DSNormConfig\nfrom ..module_registry import DSModuleRegistryBase\nfrom ...inference_parameter import InferenceParameter\n\n\nclass DSPostNormBase(DSModuleBase):\n    \"\"\"\n    Base MixIn for all Post-Normalization modules. The interface represented by this\n    module is:\n\n    residual, hidden_out = norm(residual + hidden_in)\n\n    If residual and hidden_out are the same data type, then they may alias each other.\n    Furthermore, residual should be updated in-place.\n    \"\"\"\n\n    @staticmethod\n    def config_class() -> Type[DeepSpeedConfigModel]:\n        return DSNormConfig\n\n    def __init__(self, config: DSNormConfig, implementation_config: Dict[str, Any]) -> None:\n        super().__init__(config, implementation_config)\n\n    @abstractmethod\n    def transform_param(self, param: torch.Tensor) -> InferenceParameter:\n        \"\"\"\n        Transform a gamma/beta parameter. It is assumed that both transformations are\n        the same.\n\n        Parameters:\n            param (torch.Tensor): Gamma or beta parameter.\n        \"\"\"\n        ...\n\n    def forward(self,\n                residual: torch.Tensor,\n                hidden_states: torch.Tensor,\n                gamma: torch.Tensor,\n                beta: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Parameters:\n            residual (torch.Tensor): Residual tensor.\n            hidden_states (torch.Tensor): Hidden states tensor.\n\n        Returns:\n            (torch.Tensor, torch.Tensor): Tuple of residual and hidden states.\n                Hidden states may alias with residual.\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass DSPostNormRegistry(DSModuleRegistryBase):\n    registry: Dict = {}\n\n    @staticmethod\n    def associated_class() -> Type[DSModuleBase]:\n        return DSPostNormBase\n", "deepspeed/inference/v2/modules/interfaces/attention_base.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Any, Dict, Optional, Tuple, Type\n\nimport torch\n\nfrom ...ragged import RaggedBatchWrapper\nfrom deepspeed.runtime.config_utils import DeepSpeedConfigModel\nfrom ..ds_module import DSModuleBase\nfrom ..module_registry import DSModuleRegistryBase\nfrom ..configs import DSSelfAttentionConfig\n\n\nclass DSSelfAttentionBase(DSModuleBase):\n    \"\"\"\n    Base mixin for all attention modules. The interface represented by this module\n    is broadly:\n\n    output = attention(query_key_value,\n                       Optional[kv_cache],\n                       Optional[attention_mask],\n                       Optional[attention_bias])\n    \"\"\"\n\n    @staticmethod\n    def config_class() -> Type[DeepSpeedConfigModel]:\n        return DSSelfAttentionConfig\n\n    def __init__(self, config: DSSelfAttentionConfig, implementation_config: Dict[str, Any]) -> None:\n        super().__init__(config, implementation_config)\n\n    @property\n    def kv_block_size(self) -> int:\n        \"\"\"\n        Return preferred granulatity for blocked KV-cache implementation.\n        \"\"\"\n        raise NotImplementedError()\n\n    @property\n    def q_block_size(self) -> int:\n        \"\"\"\n        Property to calculate blocking granularity for the query dimension.\n        This has no impact on the KV-cache structure, but will  affect the\n        number of attention atoms associated with a batch.\n        \"\"\"\n        raise NotImplementedError()\n\n    def build_atoms(self, ragged_batch: RaggedBatchWrapper) -> None:\n        \"\"\"\n        Build the atoms for this module. This is not a strict requirement for the class,\n        so this method is a no-op by default rather than abstract.\n        \"\"\"\n        pass\n\n    def forward(self,\n                q_k_v: torch.Tensor,\n                kv_cache: torch.Tensor,\n                batch: RaggedBatchWrapper,\n                attention_mask: Optional[torch.Tensor] = None,\n                attention_bias: Optional[torch.Tensor] = None,\n                inv_freqs: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Parameters:\n            q_k_v (torch.Tensor): Query, key, and value tensors. Expected shape is:\n                [\n                    batch,\n                    seq_len,\n                    2 * self._config.n_heads_kv + self._config.n_heads_q,\n                    self._config.head_size\n                ].\n            kv_cache (Optional[torch.Tensor]): Key and value cache tensor. Expected shape is\n                [\n                    2,\n                    batch,\n                    kv_cache_len,\n                    self._config.n_heads_kv,\n                    self._config.head_size\n                ]. If None, cache is disabled. The `kv_cache_len` dimension does not need to\n                be contiguous (it should expand stride by `max_out_tokens`).\n            batch (RaggedBatchWrapper): Ragged batch metadata.\n            attention_mask (Optional[torch.Tensor]): Attention mask tensor. If None, masking is\n                disabled. This will defer to the config in the case of conflicting information.\n                This means if the config class is implying causal attention, the mask will be ignored.\n            attention_bias (Optional[torch.Tensor]): Attention bias tensor. If None, bias is disabled.\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass DSSelfAttentionRegistry(DSModuleRegistryBase):\n    registry: Dict = {}\n\n    @staticmethod\n    def associated_class() -> Type[DSModuleBase]:\n        return DSSelfAttentionBase\n", "deepspeed/inference/v2/modules/interfaces/embedding_base.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom abc import abstractmethod\nfrom typing import Any, Dict, Optional, Type\n\nimport torch\n\nfrom deepspeed.runtime.config_utils import DeepSpeedConfigModel\nfrom ...ragged import RaggedBatchWrapper\nfrom ..ds_module import DSModuleBase\nfrom ..module_registry import DSModuleRegistryBase\nfrom ..configs import DSEmbeddingsConfig\nfrom ...inference_parameter import InferenceParameter\n\n\nclass DSEmbeddingBase(DSModuleBase):\n    \"\"\"\n    Base mixin for embedding modules. The interface represented by this module is:\n\n    hidden_out = embedding(input_ids) +\n                 position_embedding(position_ids) +\n                 token_type_embedding(token_type_ids)\n    with optional normalization.\n    \"\"\"\n\n    @staticmethod\n    def config_class() -> Type[DeepSpeedConfigModel]:\n        return DSEmbeddingsConfig\n\n    def __init__(self, config: DSEmbeddingsConfig, implementation_config: Dict[str, Any]) -> None:\n        super().__init__(config, implementation_config)\n\n    def transform_param(self, embed_param: torch.Tensor) -> InferenceParameter:\n        \"\"\"\n        Perform any necessary transformations on an embedding parameter. This module assumes\n        that all embedding parameters would require the same set of transformations.\n\n        Parameters:\n            embed_param (torch.Tensor): Embedding parameter. Shape is of [vocab_size, hidden_size]\n        \"\"\"\n        raise NotImplementedError()\n\n    @property\n    @abstractmethod\n    def output(self) -> torch.Tensor:\n        \"\"\"\n        Pre-allocated output Tensor. This currently needs to be exposed for gather operations\n        on the output.\n\n        TODO(cmikeh2): This is not ideal. We need a better abstraction for this, such as giving\n        access to the inference comm object to the DSModule.\n        \"\"\"\n        raise NotImplementedError()\n\n    def forward(self,\n                ragged_batch: RaggedBatchWrapper,\n                word_embeddings: torch.Tensor,\n                position_embeddings: Optional[torch.Tensor] = None,\n                token_type_ids: Optional[torch.Tensor] = None,\n                token_type_embeddings: Optional[torch.Tensor] = None) -> InferenceParameter:\n        \"\"\"\n        Parameters:\n            ragged_batch (torch.Tensor): Ragged batch of token ids + associated metadata.\n            word_embeddings (torch.Tensor): Word embeddings.\n            position_embeddings (torch.Tensor): Position embeddings. If passed, IDs will be\n                inferred from the ragged batch itself.\n            token_type_ids (torch.Tensor): Token type ids.\n            token_type_embeddings (torch.Tensor): Token type embeddings.\n\n        Returns:\n            torch.Tensor: Hidden states. This should be the sum of the relevant\n                encodings for the model.\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass DSEmbeddingRegistry(DSModuleRegistryBase):\n    registry: Dict = {}\n\n    @staticmethod\n    def associated_class() -> Type[DSModuleBase]:\n        return DSEmbeddingBase\n", "deepspeed/inference/v2/modules/interfaces/linear_base.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom abc import abstractmethod\nfrom typing import Any, Dict, Optional, Type\n\nimport torch\n\nfrom deepspeed.runtime.config_utils import DeepSpeedConfigModel\nfrom ..ds_module import DSModuleBase\nfrom ..module_registry import DSModuleRegistryBase\nfrom ..configs import DSLinearConfig\nfrom ...inference_parameter import InferenceParameter\n\n\nclass DSLinearBase(DSModuleBase):\n    \"\"\"\n    Base mixin for all Linear modules. The interface represented by this module\n    is:\n\n    hidden_out = activation(hidden_in * weight + bias)\n\n    The format and dtype of the weight and bias tensors are not defined and implementations\n    may compress as necessary. Must support a bias.\n    \"\"\"\n\n    @staticmethod\n    def config_class() -> Type[DeepSpeedConfigModel]:\n        return DSLinearConfig\n\n    def __init__(self, config: DSLinearConfig, implementation_config: Dict[str, Any]) -> None:\n        super().__init__(config, implementation_config)\n\n    @abstractmethod\n    def transform_param(self, param: torch.Tensor) -> InferenceParameter:\n        \"\"\"\n        Perform any necessary transformations of the parameters of this module.\n\n        Parameters:\n            param (torch.Tensor): Weight or bias tensor.\n        \"\"\"\n        ...\n\n    def forward(self, hidden_states: torch.Tensor, w: torch.Tensor, b: Optional[torch.Tensor] = None) -> torch.Tensor:\n        \"\"\"\n        Parameters:\n            hidden_states (torch.Tensor): Hidden states tensor. Expected shape is either\n                [batch, seq_len, in_channels] or [batch, in_channels].\n\n        Returns:\n            torch.Tensor: Output tensor. Tensor should have same number of dimensions as\n                input tensor.\n        \"\"\"\n        raise NotImplementedError()\n\n    @property\n    @abstractmethod\n    def output(self) -> torch.Tensor:\n        \"\"\"\n        Return the padded, pre-allocated output Tensor.\n        \"\"\"\n        ...\n\n\nclass DSLinearRegistry(DSModuleRegistryBase):\n    registry: Dict = {}\n\n    @staticmethod\n    def associated_class() -> Type[DSModuleBase]:\n        return DSLinearBase\n", "deepspeed/inference/v2/modules/interfaces/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .attention_base import DSSelfAttentionRegistry, DSSelfAttentionBase\nfrom .embedding_base import DSEmbeddingRegistry, DSEmbeddingBase\nfrom .linear_base import DSLinearRegistry, DSLinearBase\nfrom .moe_base import DSMoERegistry, DSMoEBase\nfrom .post_norm_base import DSPostNormRegistry, DSPostNormBase\nfrom .pre_norm_base import DSPreNormRegistry, DSPreNormBase\nfrom .unembed_base import DSUnembedRegistry, DSUnembedBase\n", "deepspeed/inference/v2/modules/interfaces/pre_norm_base.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom abc import abstractmethod\nfrom typing import Any, Dict, Optional, Tuple, Type\n\nimport torch\n\nfrom deepspeed.runtime.config_utils import DeepSpeedConfigModel\nfrom ..ds_module import DSModuleBase\nfrom ..configs.norm_config import DSNormConfig\nfrom ..module_registry import DSModuleRegistryBase\nfrom ...inference_parameter import InferenceParameter\n\n\nclass DSPreNormBase(DSModuleBase):\n    \"\"\"\n    Base mixin for all Pre-Normalization modules. The interface represented by this module\n    is:\n\n    if hidden_in is not None:\n        residual_out = residual + hidden_in\n    else:\n        residual_out = residual\n\n    hidden_out = normalize(residual_out)\n    return residual_out, hidden_out\n\n    Residual should be updated in-place.\n    \"\"\"\n\n    @staticmethod\n    def config_class() -> Type[DeepSpeedConfigModel]:\n        return DSNormConfig\n\n    def __init__(self, config: DSNormConfig, implementation_config: Dict[str, Any]):\n        super().__init__(config, implementation_config)\n\n    @abstractmethod\n    def transform_param(self, param: torch.Tensor) -> InferenceParameter:\n        \"\"\"\n        Transform a gamma/beta parameter. It is assumed that both transformations are\n        the same.\n\n        Parameters:\n            param (torch.Tensor): Gamma or beta parameter.\n        \"\"\"\n        ...\n\n    def forward(self,\n                residual: torch.Tensor,\n                hidden_states: Optional[torch.Tensor],\n                gamma: torch.Tensor,\n                beta: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Parameters:\n            residual (torch.Tensor): Residual tensor.\n            hidden_states (torch.Tensor): Hidden states tensor.\n\n        Returns:\n            (torch.Tensor, torch.Tensor): Tuple of residual and hidden states.\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass DSPreNormRegistry(DSModuleRegistryBase):\n    registry: Dict = {}\n\n    @staticmethod\n    def associated_class() -> Type[DSModuleBase]:\n        return DSPreNormBase\n", "deepspeed/inference/v2/modules/configs/embedding_config.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Optional\n\nfrom ...inference_utils import DtypeEnum, NormTypeEnum\nfrom ...modules.ds_module import DSModuleConfig\n\"\"\"\nTrying to define the space we need to support here right now:\n\nTypes of embeddings I've found so far:\n    1. Token embedding\n    2. Position embedding\n    3. Token type embedding\n    4. LN\n\nGPTNeo: 1, 2, 3 (shared with 1)\nGPTNeoX: 1\nGPTJ: 1, 3\nLLaMA: 1\nBERT: 1, 2, 3, 4\nGPT2: 1, 2, 3 (shared with 1)\n\nSidebar for OPT:\nOPT: 1, 2\n1 may not actually project to the actual hidden dimension according to the raw\ncode, but for the model configs we care about it does.\n2 has a weird offset associated with it that the others do not.\n\"\"\"\n\n\nclass DSEmbeddingsConfig(DSModuleConfig):\n    \"\"\"\n    Config class for DSEmbeddings.\n    \"\"\"\n\n    residual_dtype: DtypeEnum = DtypeEnum.fp16\n    \"\"\"\n    Data type the module should use for its output.\n    \"\"\"\n\n    embedding_dim: int\n    \"\"\"\n    Dimensionality of the embedding projections.\n    \"\"\"\n\n    positional_embedding: bool = False\n    \"\"\"\n    Whether the module should expect a positional embedding matrix. The shape of this\n    matrix should be of shape [max_seq_len + positional_offset, embedding_dim]\n    \"\"\"\n\n    positional_offset: int = 0\n    \"\"\"\n    Whether the linearized token IDs should be offset by a certain amount. For an example\n    of this, see the OPT model implementation.\n    \"\"\"\n\n    use_token_type: bool = False\n    \"\"\"\n    Whether the module should expect a token type embedding matrix.\n    \"\"\"\n\n    output_normalization: Optional[NormTypeEnum] = None\n    \"\"\"\n    If a the output of the embedding module should be normalized, specify here. See\n    ``inference.inference_utils.NormTypeEnum`` for supported values.\n    \"\"\"\n", "deepspeed/inference/v2/modules/configs/attention_configs.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom enum import Enum\nfrom typing import Dict, Optional\n\nfrom ...inference_utils import DtypeEnum\nfrom ...modules.ds_module import DSModuleConfig\nfrom deepspeed.runtime.config_utils import DeepSpeedConfigModel\n\n\nclass PositionalEmbeddingType(Enum):\n\n    # No positional embeddings\n    none = \"none\"\n\n    # Rotary positional embeddings - every half\n    rotate_half = \"rotate_half\"\n\n    # Rotary positional embeddings - every other\n    rotate_every_other = \"rotate_every_other\"\n\n    # Alibi\n    alibi = \"alibi\"\n\n\nclass RotateHalfConfig(DeepSpeedConfigModel):\n\n    use_trained_freqs: bool = False\n    \"\"\"\n    Whether to use a passed `trained_freqs` tensor for the attention implementation\n    or to use default synthesized frequencies.\n    \"\"\"\n\n    theta_base: float = 10_000.0\n    \"\"\"\n    Base for theta. This will only be used if `use_trained_freqs` is False.\n    \"\"\"\n\n    rotate_dim: Optional[int] = None\n    \"\"\"\n    How many neurons to rotate. If None, then all neurons will be rotated. Many external configs\n    will set this number to half the head dimension and then internally multiply by 2. To make it\n    more clear to understand what is happening (rotate_dim < head_dim -> then only partial rotation),\n    we do not do this multiplication internally.\n    \"\"\"\n\n\nclass MaskingType(Enum):\n\n    # No masking\n    none = \"none\"\n\n    # Causal masking\n    causal = \"causal\"\n\n    # Local masking\n    local = \"local\"\n\n    # Symmetric masking (this is a 1D tensor mask)\n    symmetric = \"symmetric\"\n\n    # Arbitrary masking (this would correspond to a 2D tensor mask)\n    asymmetric = \"asymmetric\"\n\n\nclass DSSelfAttentionConfig(DSModuleConfig):\n    \"\"\"\n    Config class for attention.\n    \"\"\"\n\n    # Number of query attention heads on this shard\n    n_heads_q: int\n\n    # Number of KV attention heads on this shard\n    n_heads_kv: int\n\n    # Size of each attention head\n    head_size: int\n\n    # Max number of sequences that may compose a ragged batch\n    max_sequences: int\n\n    # Scale factor for attention scores\n    scale_factor: float = 1.0\n\n    # Input data type\n    input_dtype: DtypeEnum = DtypeEnum.fp16\n\n    # Output data type\n    output_dtype: DtypeEnum = DtypeEnum.fp16\n\n    # Masking type\n    masking_type: MaskingType = MaskingType.causal\n\n    # Masking args\n    masking_args: Dict = {}\n\n    # Positional embedding type\n    positional_embedding_type: PositionalEmbeddingType = PositionalEmbeddingType.none\n\n    # Positional embedding args\n    positional_embedding_config: Optional[RotateHalfConfig] = None\n    \"\"\"\n    To extend this for the other positional embedding types, we would need to add\n    new configs for each type (as necessary) and annotate this with the\n    Union[RotateHalfConfig, OtherConfig, ...] type.\n    \"\"\"\n", "deepspeed/inference/v2/modules/configs/moe_config.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom ...inference_utils import ActivationType, DtypeEnum\nfrom ...modules.ds_module import DSModuleConfig\n\n\nclass DSMoEConfig(DSModuleConfig):\n    \"\"\"\n    Config class for DSMoEBase\n    \"\"\"\n\n    model_dim: int\n    \"\"\"\n    Size of input activation.\n    \"\"\"\n\n    intermediate_features: int\n    \"\"\"\n    Size of intermediate activation. Specifically, this is the number of input features\n    in the second linear layer. Depending on the activation function, the output of the first\n    linear layer may have increased dimensionality.\n    \"\"\"\n\n    n_experts: int\n    \"\"\"\n    Number of experts.\n    \"\"\"\n\n    top_k: int = 1\n    \"\"\"\n    top-k gating function (like top-1 or top-2)\n    \"\"\"\n\n    input_dtype: DtypeEnum = DtypeEnum.fp16\n    \"\"\"\n    Data type for the input activations.\n    \"\"\"\n\n    output_dtype: DtypeEnum = DtypeEnum.fp16\n    \"\"\"\n    Data type for the output activations.\n    \"\"\"\n\n    activation: ActivationType = ActivationType.IDENTITY\n    \"\"\"\n    Activation function of the first MLP1\n    \"\"\"\n\n    normalize_scores: bool = False\n    \"\"\"\n    Whether normalization is applied to the selected scores. If true, the module\n    should rescale the scores such that their sum is 1.0.\n    \"\"\"\n", "deepspeed/inference/v2/modules/configs/norm_config.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom ...inference_utils import DtypeEnum, NormTypeEnum\nfrom ...modules.ds_module import DSModuleConfig\n\n\nclass DSNormConfig(DSModuleConfig):\n    \"\"\"\n    Config class for both DSPreLN and DSPostLN.\n    \"\"\"\n\n    # Type of normalization\n    type: NormTypeEnum\n\n    # Number of channels in the model embedding\n    channels: int\n\n    # Data type of the residual input/outputs (we assume the residual must\n    # be the same data type for the entire model).\n    residual_dtype: DtypeEnum = DtypeEnum.fp16\n\n    # Data type of the hidden states input\n    input_dtype: DtypeEnum = DtypeEnum.fp16\n\n    # Data type of the hidden states output\n    output_dtype: DtypeEnum = DtypeEnum.fp16\n\n    # Epsilon value for numerical stability\n    eps: float = 1e-5\n", "deepspeed/inference/v2/modules/configs/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .attention_configs import (\n    DSSelfAttentionConfig,\n    PositionalEmbeddingType,\n    MaskingType,\n    RotateHalfConfig,\n)\nfrom .embedding_config import DSEmbeddingsConfig\nfrom .linear_config import DSLinearConfig\nfrom .moe_config import DSMoEConfig\nfrom .norm_config import DSNormConfig, NormTypeEnum\nfrom .unembed_config import DSUnembedConfig\n", "deepspeed/inference/v2/modules/configs/unembed_config.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom ...inference_utils import DtypeEnum, NormTypeEnum\nfrom ...modules.ds_module import DSModuleConfig\nfrom typing import Optional\n\n\nclass DSUnembedConfig(DSModuleConfig):\n    \"\"\"\n    Config class for DSUnembed\n    \"\"\"\n\n    dtype: DtypeEnum = DtypeEnum.fp16\n    \"\"\"\n    Expected data type.\n    \"\"\"\n\n    norm_type: Optional[NormTypeEnum] = None\n    \"\"\"\n    Whether the input to the unembed is normalized prior to the unembedding projection.\n    \"\"\"\n\n    model_dim: int\n    \"\"\"\n    Model embedding size.\n    \"\"\"\n\n    max_sequences: int\n    \"\"\"\n    Max sequences composing the ragged batch.\n    \"\"\"\n\n    vocab_size: int\n    \"\"\"\n    Local vocab size (the full vocab size may have been sharded across model parallel ranks)\n    \"\"\"\n", "deepspeed/inference/v2/modules/configs/linear_config.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom ...inference_utils import ActivationType, DtypeEnum\nfrom ...modules.ds_module import DSModuleConfig\n\n\nclass DSLinearConfig(DSModuleConfig):\n    \"\"\"\n    Config class for DSLinearBase.\n    \"\"\"\n\n    in_channels: int\n    \"\"\"\n    Number of input channels\n    \"\"\"\n\n    out_channels: int\n    \"\"\"\n    Number of output channels. NOTE: If this linear layer is using a gated activation function,\n    the value for ``out_channels`` passed here should refer to the number of channels after\n    gating (i.e., the expected weight shape before transformations will be ``[out_channels * 2, in_channels]``).\n    \"\"\"\n\n    activation: ActivationType = ActivationType.IDENTITY\n    \"\"\"\n    The activation function for this layer. See :class:`deepspeed.inference.inference_utils.ActivationType` for\n    supported activation functions.\n    \"\"\"\n\n    input_dtype: DtypeEnum = DtypeEnum.fp16\n    \"\"\"\n    The data type of the input tensor. See :class:`deepspeed.inference.inference_utils.DtypeEnum` for supported\n    data types.\n    \"\"\"\n\n    output_dtype: DtypeEnum = DtypeEnum.fp16\n    \"\"\"\n    The data type of the output tensor. See :class:`deepspeed.inference.inference_utils.DtypeEnum` for supported\n    data types.\n    \"\"\"\n", "deepspeed/inference/v2/modules/implementations/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\n# Imports for registering ops\nfrom .attention import *\nfrom .linear import *\nfrom .post_norm import *\nfrom .pre_norm import *\nfrom .embedding import *\nfrom .unembed import *\nfrom .moe import *\n", "deepspeed/inference/v2/modules/implementations/embedding/ragged_embedding.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Any, Dict, Optional\n\nimport torch\n\nfrom deepspeed.accelerator import get_accelerator\nfrom ....allocator import empty_from\nfrom ....inference_utils import DtypeEnum\nfrom ....kernels.ragged_ops import RaggedEmbeddingKernel\nfrom ....ragged import RaggedBatchWrapper\nfrom ...interfaces import DSEmbeddingBase, DSEmbeddingRegistry\nfrom ...configs import DSEmbeddingsConfig\n\n\n@DSEmbeddingRegistry.register_module\nclass DSRaggedEmbedding(DSEmbeddingBase):\n\n    @staticmethod\n    def name():\n        return 'ragged_embedding'\n\n    @staticmethod\n    def supports_config(config: DSEmbeddingsConfig) -> bool:\n\n        if DtypeEnum(config.residual_dtype) not in [DtypeEnum.fp16, DtypeEnum.bf16, DtypeEnum.fp32]:\n            return False\n\n        if config.use_token_type:\n            return False\n\n        if config.output_normalization is not None:\n            return False\n\n        try:\n            _ = RaggedEmbeddingKernel(config.residual_dtype, torch.int32, config.embedding_dim)\n        except ValueError:\n            return False\n\n        return True\n\n    def __init__(self, config: DSEmbeddingsConfig, implementation_config: Dict[str, Any]) -> None:\n        super().__init__(config, implementation_config)\n\n        self.embed_offset = self._config.positional_offset\n\n        # TODO(cmikeh2): How do we want to avoid the int32 vs int64 issue?\n        self._ragged_embed = RaggedEmbeddingKernel(self._config.residual_dtype, torch.int32,\n                                                   self._config.embedding_dim)\n\n        self._output = torch.empty((self._config.max_tokens, self._config.embedding_dim),\n                                   dtype=self._config.residual_dtype,\n                                   device=get_accelerator().current_device())\n\n    @property\n    def output(self) -> torch.Tensor:\n        return self._output\n\n    def forward(self,\n                ragged_batch: RaggedBatchWrapper,\n                word_embeddings: torch.Tensor,\n                position_embeddings: Optional[torch.Tensor] = None) -> torch.Tensor:\n        \"\"\"\n        Parameters:\n            ragged_batch (RaggedBatchWrapper): The input ids and associated ragged batch metadata.\n            word_embeddings (torch.Tensor): The word embedding table\n        \"\"\"\n        output = empty_from(self._output, (ragged_batch.tensor_toks, self._config.embedding_dim))\n        self._ragged_embed(output,\n                           ragged_batch,\n                           word_embeddings,\n                           position_embed_weight=position_embeddings,\n                           position_embed_offset=self.embed_offset)\n        return output\n", "deepspeed/inference/v2/modules/implementations/embedding/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .ragged_embedding import DSRaggedEmbedding\n", "deepspeed/inference/v2/modules/implementations/unembed/ragged_unembed.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Any, Dict, Optional\n\nimport torch\n\nfrom deepspeed.accelerator import get_accelerator\nfrom ....allocator import empty_from\nfrom ....inference_utils import DtypeEnum, ActivationType\nfrom ....kernels.core_ops import CUDAFPLN, BlasLibLinear, CUDARMSNorm, CUDABiasActivation\nfrom ....kernels.ragged_ops import RaggedLogitsGather\nfrom ....ragged import RaggedBatchWrapper\nfrom ...interfaces import DSUnembedBase, DSUnembedRegistry\nfrom ...configs import DSUnembedConfig\n\n\n@DSUnembedRegistry.register_module\nclass DSRaggedUnembed(DSUnembedBase):\n    \"\"\"\n    Ragged unembedding implementation. This implementation will gather only the last token\n    of each sequence in the ragged inflight batch and calculate the logits only for those rows.\n    \"\"\"\n\n    @staticmethod\n    def name():\n        return 'ragged_unembed'\n\n    @staticmethod\n    def supports_config(config: DSUnembedConfig):\n\n        if DtypeEnum(config.dtype) not in [DtypeEnum.fp16, DtypeEnum.bf16, DtypeEnum.fp32]:\n            return False\n\n        try:\n            _ = RaggedLogitsGather(config.model_dim, config.dtype)\n        except ValueError:\n            return False\n\n        if config.norm_type == 'rms_norm':\n            try:\n                _ = CUDARMSNorm(config.model_dim, config.dtype)\n            except ValueError:\n                return False\n        elif config.norm_type == 'layer_norm':\n            try:\n                _ = CUDAFPLN(config.model_dim, config.dtype)\n            except ValueError:\n                return False\n\n        return True\n\n    def __init__(self, config: DSUnembedConfig, implementation_config: Dict[str, Any]) -> None:\n        super().__init__(config, implementation_config)\n\n        self._logits_gather = RaggedLogitsGather(config.model_dim, self._config.dtype)\n\n        if self._config.norm_type == 'layer_norm':\n            self._norm = CUDAFPLN(self._config.model_dim, self._config.dtype)\n        elif self._config.norm_type == 'rms_norm':\n            self._norm = CUDARMSNorm(self._config.model_dim, self._config.dtype)\n        else:\n            self._norm = None\n\n        self._linear = BlasLibLinear(self._config.dtype)\n        # Here the activation kernel is being used to apply bias, hence the identity activation type!\n        self._act_fn = CUDABiasActivation(self._config.vocab_size, self._config.dtype, ActivationType.IDENTITY)\n\n        self._intermediate = torch.empty((self._config.max_sequences, self._config.model_dim),\n                                         dtype=self._config.dtype,\n                                         device=get_accelerator().current_device())\n\n        self._output = torch.empty((self._config.max_sequences, self._config.vocab_size),\n                                   dtype=self._config.dtype,\n                                   device=get_accelerator().current_device())\n\n    @property\n    def output(self) -> torch.Tensor:\n        return self._output\n\n    def forward(self,\n                hidden_states: torch.Tensor,\n                vocab_embedding: torch.Tensor,\n                ragged_metadata: RaggedBatchWrapper,\n                bias: Optional[torch.Tensor] = None,\n                gamma: Optional[torch.Tensor] = None,\n                beta: Optional[torch.Tensor] = None) -> torch.Tensor:\n        \"\"\"\n        Return final model logits.\n\n        Args:\n            hidden_states (torch.Tensor): The hidden states from the model. This is the output of the\n                final layer of the model.\n            vocab_embedding (torch.Tensor): The vocab embedding table.\n            raged_metadata (RaggedBatchWrapper): The ragged batch metadata.\n            gamma (Optional[torch.Tensor]): The gamma tensor for normalization.\n            beta (Optional[torch.Tensor]): The beta tensor for normalization.\n        \"\"\"\n\n        cut_down_hidden_states = empty_from(self._intermediate,\n                                            (ragged_metadata.current_sequences, self._config.model_dim))\n        self._logits_gather(cut_down_hidden_states, hidden_states, ragged_metadata)\n\n        if self._config.norm_type == 'rms_norm':\n            if gamma is None:\n                raise ValueError('RMS Normalization enabled but gamma not provided.')\n            self._norm(cut_down_hidden_states, cut_down_hidden_states, gamma)\n        elif self._config.norm_type == 'layer_norm':\n            if gamma is None or beta is None:\n                raise ValueError('Normalization enabled but gamma and/or beta not provided.')\n            self._norm(cut_down_hidden_states, cut_down_hidden_states, gamma, beta)\n\n        output = empty_from(self._output, (ragged_metadata.current_sequences, self._config.vocab_size))\n        self._linear(output, cut_down_hidden_states, vocab_embedding)\n        if bias is not None:\n            self._act_fn(output, bias)\n\n        return output\n", "deepspeed/inference/v2/modules/implementations/unembed/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .ragged_unembed import DSRaggedUnembed\n", "deepspeed/inference/v2/modules/implementations/moe/cutlass_multi_gemm.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Any, Dict, Optional, Tuple\n\nimport torch\n\nfrom deepspeed.accelerator import get_accelerator\nfrom ....allocator import empty_from\nfrom ....inference_utils import ActivationType, is_gated\nfrom ....kernels.core_ops import BlasLibLinear, CUDAGatedActivation\nfrom ....kernels.ragged_ops import (\n    MoEGather,\n    MoEScatter,\n    RaggedTopKGating,\n)\nfrom ....ragged import RaggedBatchWrapper\n\nfrom ...interfaces import DSMoEBase, DSMoERegistry\nfrom ...configs import DSMoEConfig\nfrom ....kernels.cutlass_ops import MoEGEMM\nfrom ....inference_parameter import InferenceParameter\n\n\n@DSMoERegistry.register_module\nclass DSMultiGemmMoE(DSMoEBase):\n    \"\"\"\n    MoE implementation based on the CUTLASS multi-GEMM.\n    \"\"\"\n\n    @staticmethod\n    def name():\n        return 'cutlass_multi_gemm_moe'\n\n    @staticmethod\n    def supports_config(config: DSMoEConfig) -> bool:\n        if config.input_dtype != config.output_dtype:\n            return False\n\n        if config.input_dtype != torch.float16 and config.input_dtype != torch.bfloat16:\n            return False\n\n        if config.top_k != 1 and config.top_k != 2:\n            return False\n\n        return True\n\n    def __init__(self, config: DSMoEConfig, implementation_config: Dict[str, Any]) -> None:\n        super().__init__(config, implementation_config)\n\n        # Convenience variables for frequently accessed items.\n        self.max_tokens = self._config.max_tokens\n        self.n_experts = self._config.n_experts\n        self.n_top_k = self._config.top_k\n        self.intermediate_dim = self._config.intermediate_features\n\n        moe_op_act_fn = ActivationType.IDENTITY if is_gated(self._config.activation) else self._config.activation\n\n        self._mlp_1 = MoEGEMM(fp_dtype=implementation_config['weight_dtype'], act_fn=moe_op_act_fn)\n        self._mlp_2 = MoEGEMM(fp_dtype=implementation_config['weight_dtype'], act_fn=ActivationType.IDENTITY)\n\n        if is_gated(self._config.activation):\n            self._activation = CUDAGatedActivation(self._config.model_dim, self._config.input_dtype,\n                                                   self._config.activation)\n        else:\n            self._activation = None\n\n        self._gate_proj = BlasLibLinear(self._config.input_dtype)\n        self._top_1_gate = RaggedTopKGating(config.input_dtype)\n        self._moe_scatter = MoEScatter(config.input_dtype, config.model_dim)\n        self._moe_gather = MoEGather(config.input_dtype, config.model_dim, config.normalize_scores)\n\n        self._create_buffers()\n\n    def _create_buffers(self):\n\n        # Gating buffers\n        self._logits = torch.empty((self._config.max_tokens, self.n_experts),\n                                   dtype=self._config.input_dtype,\n                                   device=get_accelerator().current_device())\n        self._expert_counts = torch.empty((self.n_experts, ),\n                                          dtype=torch.int32,\n                                          device=get_accelerator().current_device())\n        self._scores = torch.empty((self._config.max_tokens, self.n_top_k),\n                                   dtype=torch.float32,\n                                   device=get_accelerator().current_device())\n        self._assignments = torch.empty((self._config.max_tokens, self.n_top_k),\n                                        dtype=torch.int32,\n                                        device=get_accelerator().current_device())\n        self._offsets = torch.empty((self._config.max_tokens, self.n_top_k),\n                                    dtype=torch.int32,\n                                    device=get_accelerator().current_device())\n\n        # Scatter buffers\n        self._moe_input = torch.empty((self._config.max_tokens * self.n_top_k, self._config.model_dim),\n                                      dtype=self._config.input_dtype,\n                                      device=get_accelerator().current_device())\n        self._expert_cumsum = torch.empty((self._config.n_experts, ),\n                                          dtype=torch.int64,\n                                          device=get_accelerator().current_device())\n        self._mapped_slots = torch.empty((self._config.max_tokens, self.n_top_k),\n                                         dtype=torch.int32,\n                                         device=get_accelerator().current_device())\n\n        # GEMM Buffers\n        self._intermediate = torch.empty((self._config.max_tokens * self.n_top_k, self._config.intermediate_features),\n                                         dtype=self._config.output_dtype,\n                                         device=get_accelerator().current_device())\n        if self._activation is not None:\n            self._gated_intermediate = torch.empty(\n                (self._config.max_tokens * self.n_top_k, self._config.intermediate_features * 2),\n                dtype=self._config.output_dtype,\n                device=get_accelerator().current_device())\n\n        self._output_unordered = torch.empty((self._config.max_tokens * self.n_top_k, self._config.model_dim),\n                                             dtype=self._config.output_dtype,\n                                             device=get_accelerator().current_device())\n\n        # Gather buffer\n        self._output = torch.empty((self._config.max_tokens, self._config.model_dim),\n                                   dtype=self._config.output_dtype,\n                                   device=get_accelerator().current_device())\n\n    def transform_gate_param(self, param: torch.Tensor) -> InferenceParameter:\n        \"\"\"\n        Ensures gate param is going to match the activation data type.\n        \"\"\"\n        param = param.to(self._config.input_dtype)\n        return InferenceParameter.initialize(param)\n\n    def transform_moe_mlp_1_param(self, param: torch.Tensor) -> InferenceParameter:\n        \"\"\"\n        Converts param to same data type as input and output.\n\n        Parameters:\n            param (torch.Tensor): Weight or bias tensor.\n        \"\"\"\n        param = param.to(self._config.input_dtype)\n\n        if len(param.shape) == 3:\n            param = param.permute(0, 2, 1).contiguous()\n        return InferenceParameter.initialize(param)\n\n    def transform_moe_mlp_2_param(self, param: torch.Tensor) -> InferenceParameter:\n        \"\"\"\n        Converts param to same data type as input and output.\n\n        Parameters:\n            param (torch.Tensor): Weight or bias tensor.\n        \"\"\"\n        param = param.to(self._config.input_dtype)\n\n        if len(param.shape) == 3:\n            param = param.permute(0, 2, 1).contiguous()\n        return InferenceParameter.initialize(param)\n\n    @property\n    def output(self) -> torch.Tensor:\n        return self._output\n\n    def _gate(self, hidden_states: torch.Tensor, batch_metadata: RaggedBatchWrapper,\n              gate_w: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Helper function to isolate the logit for gating. This will take the hidden states\n        and produce the metadata + tensors for the CUTLASS ragged GEMMs. If the input has\n        been padded for CG, this will strip the padding for MoE.\n\n        Parameters:\n            hidden_states (torch.Tensor): Hidden states tensor. Expected shape is [n_tokens, model_dim].\n            batch_metadata (RaggedBatchWrapper): Batch metadata for the hidden states.\n            gate_w (torch.Tensor): Gate weight tensor. Expected shape is [num_experts, model_dim].\n\n        Returns:\n            Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]: The MoE input, the cumsum of the offsets (for the MoE kernels themselves), the scores, and the mapped slots (to recover the original order of the tokens)\n        \"\"\"\n\n        # Get views on the buffers for gating\n        logits = empty_from(self._logits, (hidden_states.shape[0], self._logits.shape[-1]))\n        scores = empty_from(self._scores, (hidden_states.shape[0], self.n_top_k))\n        assignments = empty_from(self._assignments, (hidden_states.shape[0], self.n_top_k))\n        offsets = empty_from(self._offsets, (hidden_states.shape[0], self.n_top_k))\n        mapped_slots = empty_from(self._mapped_slots, (hidden_states.shape[0], self.n_top_k))\n        moe_input = empty_from(self._moe_input, (hidden_states.shape[0] * self.n_top_k, self._moe_input.shape[-1]))\n\n        self._gate_proj(logits, hidden_states, gate_w)\n        self._expert_counts.zero_()\n        self._top_1_gate(self._expert_counts, scores, assignments, offsets, logits, batch_metadata)\n        self._moe_scatter(moe_input, self._expert_cumsum, mapped_slots, hidden_states, self._expert_counts,\n                          assignments, offsets)\n\n        return moe_input, self._expert_cumsum, scores, mapped_slots\n\n    def forward(self,\n                hidden_states: torch.Tensor,\n                batch_metadata: RaggedBatchWrapper,\n                gate_w: torch.Tensor,\n                mlp_1_w: torch.Tensor,\n                mlp_2_w: torch.Tensor,\n                mlp_1_b: Optional[torch.Tensor] = None,\n                mlp_2_b: Optional[torch.Tensor] = None) -> torch.Tensor:\n        \"\"\"\n        MoE forward pass built on top of CUTLASS multi-GEMM.\n\n        Parameters:\n            hidden_states (torch.Tensor): Hidden states tensor. Expected shape is [batch, seq_len, model_dim].\n            gate_w (torch.Tensor): Gate weight tensor. Expected shape is [num_experts, model_dim].\n        \"\"\"\n\n        moe_input, expert_cumsum, scores, mapped_slots = self._gate(hidden_states, batch_metadata, gate_w)\n\n        # Get views on the buffers for GEMM\n        intermediate = empty_from(self._intermediate,\n                                  (hidden_states.shape[0] * self.n_top_k, self._intermediate.shape[-1]))\n        output_unordered = empty_from(self._output_unordered,\n                                      (hidden_states.shape[0] * self.n_top_k, self._output_unordered.shape[-1]))\n        output = empty_from(self._output, (hidden_states.shape[0], self._output.shape[-1]))\n\n        if self._activation is not None:\n            gated_intermediate = empty_from(\n                self._gated_intermediate, (hidden_states.shape[0] * self.n_top_k, self._gated_intermediate.shape[-1]))\n            self._mlp_1(\n                gated_intermediate,\n                moe_input,\n                mlp_1_w,\n                expert_cumsum,\n                mlp_1_b,\n            )\n            self._activation(intermediate, gated_intermediate)\n        else:\n            self._mlp_1(\n                intermediate,\n                moe_input,\n                mlp_1_w,\n                expert_cumsum,\n                mlp_1_b,\n            )\n\n        self._mlp_2(\n            output_unordered,\n            intermediate,\n            mlp_2_w,\n            expert_cumsum,\n            mlp_2_b,\n        )\n\n        self._moe_gather(output, output_unordered, scores, mapped_slots, self._expert_counts)\n        return output\n", "deepspeed/inference/v2/modules/implementations/moe/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .cutlass_multi_gemm import DSMultiGemmMoE\n", "deepspeed/inference/v2/modules/implementations/pre_norm/cuda_pre_rms.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Any, Dict, Optional, Tuple\n\nimport torch\n\nfrom deepspeed.accelerator import get_accelerator\nfrom ...interfaces import DSPreNormBase, DSPreNormRegistry\nfrom ...configs import DSNormConfig, NormTypeEnum\nfrom ....kernels.core_ops import CUDARMSNorm, CUDARMSPreNorm\nfrom ....allocator import empty_from\nfrom ....inference_parameter import InferenceParameter\n\n\n@DSPreNormRegistry.register_module\nclass DSPreRMSCUDAModule(DSPreNormBase):\n\n    @staticmethod\n    def name():\n        return 'cuda_pre_rms'\n\n    @staticmethod\n    def supports_config(config: DSNormConfig):\n        type = NormTypeEnum(config.type)\n        if type != NormTypeEnum.RMSNorm:\n            return False\n\n        if len(set([config.residual_dtype, config.input_dtype, config.output_dtype])) != 1:\n            return False\n\n        try:\n            # Only need to check one since the support matrix for the two rms kernels is the same\n            _ = CUDARMSPreNorm(config.channels, config.residual_dtype)\n        except ValueError:\n            return False\n        return True\n\n    def __init__(self, config: DSNormConfig, implementation_config: Dict[str, Any]):\n        super().__init__(config, implementation_config)\n        self._fp_rms = CUDARMSNorm(self._config.channels, self._config.residual_dtype, epsilon=self._config.eps)\n        self._fp_rms_pre = CUDARMSPreNorm(self._config.channels, self._config.residual_dtype, epsilon=self._config.eps)\n\n        # Buffers for both the hidden and residual outputs\n        self._hidden_output = torch.empty((config.max_tokens, config.channels),\n                                          dtype=config.output_dtype,\n                                          device=get_accelerator().current_device())\n        self._residual_output = torch.empty((config.max_tokens, config.channels),\n                                            dtype=config.output_dtype,\n                                            device=get_accelerator().current_device())\n\n    def transform_param(self, param: torch.Tensor) -> InferenceParameter:\n        param = param.to(self._config.input_dtype)\n        return InferenceParameter.initialize(param)\n\n    def forward(self,\n                residual: torch.Tensor,\n                hidden_in: Optional[torch.Tensor],\n                gamma: torch.Tensor,\n                beta: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Since the CUDA FP only supports all data types being the same, we will alias the residual\n        with our output.\n\n        If hidden_in is None, that means we do not need to perform the residual add and will\n        only return the hidden output modified.\n        \"\"\"\n        assert beta is None, \"Beta is not supported for RMSNorm\"\n\n        hidden_out = empty_from(self._hidden_output, residual.shape)\n        if hidden_in is None:\n            self._fp_rms(hidden_out, residual, gamma)\n            residual_out = residual\n        else:\n            residual_out = empty_from(self._residual_output, residual.shape)\n            self._fp_rms_pre(residual_out, hidden_out, residual, hidden_in, gamma)\n        return residual_out, hidden_out\n", "deepspeed/inference/v2/modules/implementations/pre_norm/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .cuda_pre_ln import DSPreLNCUDAModule\nfrom .cuda_pre_rms import DSPreRMSCUDAModule\n", "deepspeed/inference/v2/modules/implementations/pre_norm/cuda_pre_ln.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Any, Dict, Optional, Tuple\n\nimport torch\n\nfrom deepspeed.accelerator import get_accelerator\nfrom ...interfaces import DSPreNormBase, DSPreNormRegistry\nfrom ...configs import DSNormConfig, NormTypeEnum\nfrom ....kernels.core_ops.cuda_layer_norm.cuda_pre_ln import CUDAFPPreLN\nfrom ....kernels.core_ops.cuda_layer_norm.cuda_ln import CUDAFPLN\nfrom ....allocator import empty_from\nfrom ....inference_parameter import InferenceParameter\n\n\n@DSPreNormRegistry.register_module\nclass DSPreLNCUDAModule(DSPreNormBase):\n\n    @staticmethod\n    def name():\n        return 'cuda_pre_ln'\n\n    @staticmethod\n    def supports_config(config: DSNormConfig):\n        type = NormTypeEnum(config.type)\n        if type != NormTypeEnum.LayerNorm:\n            return False\n\n        if len(set([config.residual_dtype, config.input_dtype, config.output_dtype])) != 1:\n            return False\n\n        try:\n            _ = CUDAFPPreLN(config.channels, config.residual_dtype)\n        except ValueError:\n            return False\n        return True\n\n    def __init__(self, config: DSNormConfig, implementation_config: Dict[str, Any]):\n        super().__init__(config, implementation_config)\n        self._fp_pre_ln = CUDAFPPreLN(self._config.channels, self._config.residual_dtype, epsilon=self._config.eps)\n        self._fp_ln = CUDAFPLN(self._config.channels, self._config.residual_dtype, epsilon=self._config.eps)\n\n        # Buffers for the hidden output (residual is updated in-place)\n        self._hidden_output = torch.empty((config.max_tokens, config.channels),\n                                          dtype=config.output_dtype,\n                                          device=get_accelerator().current_device())\n\n    def transform_param(self, param: torch.Tensor) -> InferenceParameter:\n        param = param.to(self._config.input_dtype)\n        return InferenceParameter.initialize(param)\n\n    def forward(self, residual: torch.Tensor, hidden_in: Optional[torch.Tensor], gamma: torch.Tensor,\n                beta: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Since the CUDA FP only supports all data types being the same, we will alias the residual\n        with our output.\n\n        If hidden_in is None, that means we do not need to perform the residual add and will\n        only return the hidden output modified.\n        \"\"\"\n        hidden_out = empty_from(self._hidden_output, residual.shape)\n        if hidden_in is None:\n            self._fp_ln(hidden_out, residual, gamma, beta)\n        else:\n            self._fp_pre_ln(residual, hidden_out, residual, hidden_in, gamma, beta)\n        return residual, hidden_out\n", "deepspeed/inference/v2/modules/implementations/linear/blas_fp_linear.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Any, Dict, Optional\n\nimport torch\n\nfrom deepspeed.accelerator import get_accelerator\nfrom ....allocator import empty_from\nfrom ....inference_utils import is_gated\nfrom ....kernels.core_ops import (\n    BlasLibLinear,\n    CUDABiasActivation,\n    CUDAGatedActivation,\n)\n\nfrom ...interfaces import DSLinearBase, DSLinearRegistry\nfrom ...configs import DSLinearConfig\nfrom ....inference_parameter import InferenceParameter\n\n\n@DSLinearRegistry.register_module\nclass BlasFPLinear(DSLinearBase):\n    \"\"\"\n    Linear DSModule based on BLAS library and standalone bias + activation kernel implementation.\n    \"\"\"\n\n    @staticmethod\n    def name():\n        return 'blas_fp_linear'\n\n    @staticmethod\n    def supports_config(config: DSLinearConfig) -> bool:\n        if config.input_dtype != config.output_dtype:\n            return False\n\n        if config.input_dtype != torch.float16 and config.input_dtype != torch.bfloat16:\n            return False\n\n        if is_gated(config.activation):\n            try:\n                _ = CUDAGatedActivation(config.out_channels, config.output_dtype, config.activation)\n            except ValueError:\n                return False\n        else:\n            try:\n                _ = CUDABiasActivation(config.out_channels, config.output_dtype, config.activation)\n            except ValueError:\n                return False\n\n        return True\n\n    def __init__(self, config: DSLinearConfig, implementation_config: Dict[str, Any]) -> None:\n        super().__init__(config, implementation_config)\n\n        self._linear_impl = BlasLibLinear(self._config.input_dtype)\n\n        if is_gated(config.activation):\n            self._is_gated = True\n            self._act_fn = CUDAGatedActivation(config.out_channels, config.output_dtype, config.activation)\n            self._double_buffer = torch.empty((config.max_tokens, config.out_channels * 2),\n                                              dtype=config.output_dtype,\n                                              device=get_accelerator().current_device())\n        else:\n            self._is_gated = False\n            self._act_fn = CUDABiasActivation(config.out_channels, config.output_dtype, config.activation)\n\n        self._output = torch.empty((config.max_tokens, config.out_channels),\n                                   dtype=config.output_dtype,\n                                   device=get_accelerator().current_device())\n\n    def transform_param(self, param: torch.Tensor) -> InferenceParameter:\n        \"\"\"\n        Converts param to same data type as input and output.\n\n        Parameters:\n            param (torch.Tensor): Weight or bias tensor.\n        \"\"\"\n        param = param.to(self._config.output_dtype)\n        return InferenceParameter.initialize(param)\n\n    def forward(self, hidden_states: torch.Tensor, w: torch.Tensor, b: Optional[torch.Tensor] = None) -> torch.Tensor:\n\n        output = empty_from(self._output, (hidden_states.shape[0], self._config.out_channels))\n\n        if self._is_gated:\n            staging_output = empty_from(self._double_buffer, (hidden_states.shape[0], self._config.out_channels * 2))\n            self._linear_impl(staging_output, hidden_states, w)\n            self._act_fn(output, staging_output, b)\n        else:\n            self._linear_impl(output, hidden_states, w)\n            self._act_fn(output, b)\n\n        return output\n\n    @property\n    def output(self) -> torch.Tensor:\n        \"\"\"\n        Return the padded, pre-allocated output Tensor.\n        \"\"\"\n        return self._output\n", "deepspeed/inference/v2/modules/implementations/linear/quantized_linear.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Any, Dict, Optional\n\nimport torch\n\nfrom deepspeed.accelerator import get_accelerator\nfrom deepspeed.ops.op_builder import InferenceCoreBuilder\nfrom ....allocator import empty_from\nfrom ....inference_utils import is_gated\nfrom ....kernels.core_ops import (\n    CUDAWf6Af16Linear,\n    CUDABiasActivation,\n    CUDAGatedActivation,\n)\n\nfrom ...interfaces import DSLinearBase, DSLinearRegistry\nfrom ...configs import DSLinearConfig\nfrom ....inference_parameter import InferenceParameter\n\n\ndef fp_quantize(input: torch.FloatTensor,\n                num_bits: int = 6,\n                exp_bits: int = 3,\n                min_value: torch.FloatTensor = None,\n                max_value: torch.FloatTensor = None,\n                group_size: int = -1):\n    \"\"\"\n    Args:\n        inputs (`torch.FloatTensor`)\n            The input which needs to be quantized\n        num_bits (int, >=4)\n            Number of bits to use for quantization\n        exp_bits:\n            fp exp_bits\n        min_value/max_vlue (torch.FloatTensor)\n            Used for static activation quantization\n        group_size (int) N\n            The quantization block size, each N numbers has its own scaling\n            factor and off-site. -1 means use the last dim as the group_size\n    Returns:\n        quantized_fake_fp6\n            The quantized weights, in fp16 format and contains fp6 value.\n        scales\n            Quantization scales\n    \"\"\"\n\n    try:\n        from qtorch.quant import float_quantize\n    except ImportError:\n        raise ImportError(\"Please install qtorch to use this function\")\n\n    assert (min_value is None and max_value is None) or (min_value is not None and max_value is not None)\n\n    assert input.dtype == torch.float16\n\n    orig_device = input.device\n    input = input.to(torch.float32).to(get_accelerator().current_device())\n    if num_bits == 6 and exp_bits == 3:  # this is default\n        q_range = 28\n    else:\n        raise NotImplementedError\n\n    man_bits = num_bits - exp_bits - 1\n    input_shape = input.shape\n\n    if group_size == -1:\n        group_size = input_shape[-1]\n    else:\n        # Only support per-channel quantization\n        raise NotImplementedError\n    num_groups = input.numel() // group_size\n    input = input.reshape(num_groups, -1)\n\n    if min_value is None:\n        max_input = torch.amax(torch.abs(input), dim=-1).view(num_groups, -1)\n    else:\n        max_input = torch.max(min_value.abs(), max_value)  # .view(-1)\n    scales = max_input / q_range  # q_range + 1\n    scales[scales == 0] = 1  # avoid zero scales\n    scaled_input = input / scales\n\n    quantized_fake_fp6 = float_quantize(scaled_input, exp_bits, man_bits, rounding=\"nearest\")\n\n    quantized_fake_fp6 = quantized_fake_fp6.reshape(input_shape).contiguous().to(torch.float16).to(orig_device)\n    scales = scales.to(torch.float16).to(orig_device)\n    # Now the dequantized value is quantized_fake_fp6 * scales\n\n    return quantized_fake_fp6, scales\n\n\n@DSLinearRegistry.register_module\nclass QuantizedWf6Af16Linear(DSLinearBase):\n    \"\"\"\n    Linear DSModule for FP6 weight-only quantization kernel, where weight is FP6\n    and activation is FP16.\n    \"\"\"\n\n    @staticmethod\n    def name():\n        return 'quantized_wf6af16_linear'\n\n    @staticmethod\n    def supports_config(config: DSLinearConfig) -> bool:\n        if config.input_dtype != config.output_dtype:\n            return False\n\n        # As for fp6 data items, they are packed and stored in a set of fp16\n        # tensors. E.g., 8 fp6 data items are stored in 3 fp16 tensor.\n        if config.input_dtype != torch.float16:\n            return False\n\n        if is_gated(config.activation):\n            try:\n                _ = CUDAGatedActivation(config.out_channels, config.output_dtype, config.activation)\n            except ValueError:\n                return False\n        else:\n            try:\n                _ = CUDABiasActivation(config.out_channels, config.output_dtype, config.activation)\n            except ValueError:\n                return False\n\n        return True\n\n    def __init__(self, config: DSLinearConfig, implementation_config: Dict[str, Any]) -> None:\n        super().__init__(config, implementation_config)\n\n        self._linear_impl = CUDAWf6Af16Linear()\n\n        if is_gated(config.activation):\n            # In the FP6 kernel implementation, the MatMul is W * A, where W is\n            # the weight and A is activation. M is the output channel size.\n            self.out_channels = self._config.out_channels * 2\n            self.in_channels = self._config.in_channels\n            self._is_gated = True\n            self._act_fn = CUDAGatedActivation(config.out_channels, config.output_dtype, config.activation)\n            self._double_buffer = torch.empty((config.max_tokens, config.out_channels * 2),\n                                              dtype=config.output_dtype,\n                                              device=get_accelerator().current_device())\n        else:\n            self.out_channels = self._config.out_channels\n            self.in_channels = self._config.in_channels\n            self._is_gated = False\n            self._act_fn = CUDABiasActivation(config.out_channels, config.output_dtype, config.activation)\n\n        self._output = torch.empty((config.max_tokens, config.out_channels),\n                                   dtype=config.output_dtype,\n                                   device=get_accelerator().current_device())\n\n        self.inf_module = InferenceCoreBuilder().load()\n        self.inf_module.create_handle()\n        self.preprocess_weight = self.inf_module.preprocess_weight\n\n        self.quantizer = fp_quantize\n\n    def transform_param(self, param: torch.Tensor) -> InferenceParameter:\n        \"\"\"\n        Converts param to same data type as input and output.\n\n        Parameters:\n            param (torch.Tensor): Weight or bias tensor.\n        \"\"\"\n        # It expects that the quantization scales are store in the attribute `scales`.\n\n        if param.ndim == 1:  # bias, do nothing\n            return InferenceParameter.initialize(param)\n\n        quantized_fake_fp6, scales = self.quantizer(param, num_bits=6, exp_bits=3)\n\n        # This is for debugging, will delete before release.\n        assert (quantized_fake_fp6.dtype == torch.float16)\n        assert quantized_fake_fp6.shape[0] == self.out_channels\n        assert scales.numel() == self.out_channels\n\n        weights_2bit, weights_4bit = self.preprocess_weight(quantized_fake_fp6)\n\n        return InferenceParameter.initialize(weights_2bit, weights_4bit=weights_4bit, scales=scales)\n\n    def forward(self, hidden_states: torch.Tensor, w: torch.Tensor, b: Optional[torch.Tensor] = None) -> torch.Tensor:\n        weights_2bit = w\n        weights_4bit = w.weights_4bit\n        scales = w.scales\n        output = empty_from(self._output, (hidden_states.shape[0], self._config.out_channels))\n        if self._is_gated:\n            staging_output = empty_from(self._double_buffer, (hidden_states.shape[0], self.out_channels))\n            self._linear_impl(staging_output, hidden_states, weights_2bit, weights_4bit, scales, self.out_channels,\n                              hidden_states.shape[0], self.in_channels)\n            self._act_fn(output, staging_output, b)\n        else:\n            self._linear_impl(output, hidden_states, weights_2bit, weights_4bit, scales, self.out_channels,\n                              hidden_states.shape[0], self.in_channels)\n            self._act_fn(output, b)\n\n        return output\n\n    @property\n    def output(self) -> torch.Tensor:\n        \"\"\"\n        Return the padded, pre-allocated output Tensor.\n        \"\"\"\n        return self._output\n", "deepspeed/inference/v2/modules/implementations/linear/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .blas_fp_linear import BlasFPLinear\nfrom .quantized_linear import QuantizedWf6Af16Linear, fp_quantize\n", "deepspeed/inference/v2/modules/implementations/attention/dense_blocked_attention.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Any, Dict, Optional\n\nimport torch\n\nfrom deepspeed.accelerator import get_accelerator\nfrom ....allocator import empty_from\nfrom ....inference_utils import DtypeEnum\nfrom ....kernels.ragged_ops import (\n    AtomBuilder,\n    BlockedFlashAttn,\n    BlockedRotaryEmbeddings,\n    BlockedTrainedRotaryEmbeddings,\n    get_q_block_size,\n    get_kv_block_size,\n    LinearBlockedKVCopy,\n)\nfrom ....ragged import RaggedBatchWrapper, split_kv\nfrom deepspeed.ops.op_builder import RaggedUtilsBuilder\n\nfrom ...interfaces import DSSelfAttentionBase, DSSelfAttentionRegistry\nfrom ...configs import DSSelfAttentionConfig, PositionalEmbeddingType, MaskingType\n\ntry:\n    from functools import cached_property\nexcept ImportError:\n\n    def cached_property(func):\n        return property(func)\n\n\n@DSSelfAttentionRegistry.register_module\nclass DSDenseBlockedAttention(DSSelfAttentionBase):\n    \"\"\"\n    Self attention implementation for dense, blocked self attention.\n    \"\"\"\n\n    @staticmethod\n    def name() -> str:\n        return 'dense_blocked_attention'\n\n    @staticmethod\n    def supports_config(config: DSSelfAttentionConfig) -> bool:\n\n        if config.input_dtype != config.output_dtype:\n            return False\n\n        if DtypeEnum(config.input_dtype) not in (DtypeEnum.fp16, DtypeEnum.bf16):\n            return False\n\n        if PositionalEmbeddingType(config.positional_embedding_type) not in [\n                PositionalEmbeddingType.none, PositionalEmbeddingType.rotate_half\n        ]:\n            return False\n\n        if MaskingType(config.masking_type) != MaskingType.causal:\n            return False\n\n        return True\n\n    def __init__(self, config: DSSelfAttentionConfig, implementation_config: Dict[str, Any]) -> None:\n        \"\"\"\n        Create the Attention DSModule.\n\n        Args:\n            config (DSSelfAttentionConfig): The self attention config for all attention DSModules.\n            implementation_config (Dict[str, Any]):\n                There are two (dependent) potential components in the implementtion config.\n\n                1. `trained_freqs` - If the embedding weights for RoPE are trained, the implementation\n                config should contain {'trained_freqs': True}. This will mean the implementation will\n                expect a `trained_freqs` tensor in the `forward` method and will not synthesize the\n                values internally.\n\n                2. `theta_base` - The base value for synthesized frequencies in the rotary embeddings.\n                This will only be used if `trained_freqs` is False or not present in the `implementation_config`. If this is not included, the default value of 10000.0 will be used.\n        \"\"\"\n        super().__init__(config, implementation_config)\n\n        embed_type = PositionalEmbeddingType(config.positional_embedding_type)\n        if embed_type == PositionalEmbeddingType.none:\n            self._kv_copy = LinearBlockedKVCopy(self._config.head_size, self._config.n_heads_q,\n                                                self._config.n_heads_kv, self._config.input_dtype)\n        elif embed_type == PositionalEmbeddingType.rotate_half:\n            rotary_config = config.positional_embedding_config\n            assert rotary_config is not None, \"Rotary config must be provided if using rotate_half as Positional Embedding Type.\"\n\n            if rotary_config.use_trained_freqs:\n                # Theta and rotary dim are effectively embedded into either the values (theta) or the shape (rotary_dim)\n                # of the trained_freqs tensor.\n                self._kv_copy = BlockedTrainedRotaryEmbeddings(self._config.head_size, self._config.n_heads_q,\n                                                               self._config.n_heads_kv, self._config.input_dtype)\n            else:\n                theta_base = rotary_config.theta_base\n                rotary_dim = rotary_config.rotate_dim if rotary_config.rotate_dim is not None else self._config.head_size\n                self._kv_copy = BlockedRotaryEmbeddings(self._config.head_size, self._config.n_heads_q,\n                                                        self._config.n_heads_kv, self._config.input_dtype, rotary_dim,\n                                                        theta_base)\n\n        self._softmax_scale = self._config.scale_factor\n\n        # TODO(cmikeh2): Attention kernel gets created here.\n        self._attn_kernel = BlockedFlashAttn(self._config.head_size, self._config.input_dtype)\n        self._atom_builder = AtomBuilder()\n\n        self.model_dim = self._config.head_size * self._config.n_heads_q\n        self._output = torch.empty((self._config.max_tokens, self._config.head_size * self._config.n_heads_q),\n                                   dtype=self._config.output_dtype,\n                                   device=get_accelerator().current_device())\n\n        # TODO(cmikeh2): Pre-allocate storage buffer for the attention atoms.\n        self._max_atoms = self._config.max_sequences\n        self._atoms = torch.empty((self._max_atoms, 8), dtype=torch.int32, device=get_accelerator().current_device())\n\n        alloc_func = RaggedUtilsBuilder().load().allocate_fast_host_buffer\n        self._atoms_shadow = alloc_func(self._atoms)\n        self._cur_atoms = 0\n\n    @cached_property\n    def kv_block_size(self) -> int:\n        \"\"\"\n        Return preferred granulatity for blocked KV-cache implementation.\n        \"\"\"\n        return get_kv_block_size(self._config.head_size)\n\n    @cached_property\n    def q_block_size(self) -> int:\n        \"\"\"\n        Property to calculate blocking granularity for the query dimension.\n        This has no impact on the KV-cache structure, but will  affect the\n        number of attention atoms associated with a batch.\n        \"\"\"\n        return get_q_block_size(self._config.head_size)\n\n    def build_atoms(self, ragged_batch: RaggedBatchWrapper) -> None:\n        \"\"\"\n        Build the atoms for the attention kernel.\n\n        Args:\n            ragged_batch (RaggedBatchWrapper): The input ids and associated ragged batch metadata.\n        \"\"\"\n        host_atoms, n_atoms = self._atom_builder(self._atoms_shadow, ragged_batch, self.q_block_size,\n                                                 self.kv_block_size)\n\n        self._cur_atoms = n_atoms\n        self._atoms[:n_atoms].copy_(host_atoms[:n_atoms], non_blocking=True)\n\n    def forward(self,\n                q_k_v: torch.Tensor,\n                kv_cache: torch.Tensor,\n                batch: RaggedBatchWrapper,\n                inv_freqs: Optional[torch.Tensor] = None) -> torch.Tensor:\n        \"\"\"\n        Forward implementation.\n\n        Args:\n            q_k_v (torch.Tensor): Query/Key/Value projection Tensor of shape\n                [n_heads, (n_heads_q + 2 * n_heads_kv) * head_size].\n            kv_cache (torch.Tensor): Blocked persistent cache of shape\n                [2, batch, block_size, n_heads_kv, head_size].\n            batch (RaggedBatchWrapper): The input ids and associated ragged batch metadata.\n            inv_freqs (Optional[torch.Tensor]): The inverse frequencies for the rotary embeddings if they\n                have been modified from synthesizable values.\n        \"\"\"\n        if inv_freqs is not None:\n            self._kv_copy(kv_cache, q_k_v, batch, inv_freqs)\n        else:\n            self._kv_copy(kv_cache, q_k_v, batch)\n\n        q = q_k_v[:, :self._config.head_size * self._config.n_heads_q]\n        output = empty_from(self._output, q.shape)\n        k_cache, v_cache = split_kv(kv_cache)\n\n        self._attn_kernel(output, q, k_cache, v_cache, self._atoms[:self._cur_atoms], self._softmax_scale)\n\n        return output\n", "deepspeed/inference/v2/modules/implementations/attention/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .dense_blocked_attention import DSDenseBlockedAttention\n", "deepspeed/inference/v2/modules/implementations/post_norm/cuda_post_ln.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom typing import Any, Dict, Tuple\n\nimport torch\n\nfrom deepspeed.accelerator import get_accelerator\nfrom ...interfaces import DSPostNormBase, DSPostNormRegistry\nfrom ...configs import DSNormConfig\nfrom ....kernels.core_ops.cuda_layer_norm.cuda_post_ln import CUDAFPPostLN\nfrom ....allocator import empty_from\nfrom ....inference_parameter import InferenceParameter\n\n\n@DSPostNormRegistry.register_module\nclass DSPostLNCUDAModule(DSPostNormBase):\n\n    @staticmethod\n    def name():\n        return 'cuda_post_ln'\n\n    @staticmethod\n    def supports_config(config: DSNormConfig):\n        if len(set([config.residual_dtype, config.input_dtype, config.output_dtype])) != 1:\n            return False\n\n        try:\n            _ = CUDAFPPostLN(config.channels, config.residual_dtype)\n        except ValueError:\n            return False\n        return True\n\n    def __init__(self, config: DSNormConfig, implementation_config: Dict[str, Any]):\n        super().__init__(config, implementation_config)\n        self._fp_post_ln = CUDAFPPostLN(self._config.channels, self._config.residual_dtype, epsilon=self._config.eps)\n\n        self._output = torch.empty((config.max_tokens, config.channels),\n                                   dtype=config.output_dtype,\n                                   device=get_accelerator().current_device())\n\n    def transform_param(self, param: torch.Tensor) -> InferenceParameter:\n        param = param.to(self._config.input_dtype)\n        return InferenceParameter.initialize(param)\n\n    def forward(self, residual: torch.Tensor, hidden_in: torch.Tensor, gamma: torch.Tensor,\n                beta: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Since the CUDA FP only supports all data types being the same, we will alias the residual\n        with our output.\n        \"\"\"\n        self._residual_output = empty_from(self._output, residual.shape)\n        self._fp_post_ln(residual, residual, hidden_in, gamma, beta)\n        return residual, residual\n", "deepspeed/inference/v2/modules/implementations/post_norm/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .cuda_post_ln import DSPostLNCUDAModule\n", "deepspeed/inference/quantization/quantization.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nfrom torch import nn\nfrom typing import Dict\nimport gc\nfrom deepspeed.inference.quantization import layers\nfrom .layers import QUANTIZATION_LAYER_MAPPINGS\nfrom .utils import get_AsyncPartitionedParameterSwapper, recursive_setattr\nfrom deepspeed.utils.logging import logger\nfrom collections import deque\nfrom transformers.utils.generic import ContextManagers\nfrom .quantization_context import QuantizationContext\nimport contextlib\n\n\ndef _init_group_wise_weight_quantization(model: nn.Module, ds_config: Dict) -> nn.Module:\n    \"\"\"[Experimental] Apply group-wise weight quantization to model. Replace layers module according to config_list\n\n    Args:\n        model (nn.Module): A nn.Module\n        ds_config (Dict, optional): The ds_config dictionary. use None for non-deepspeed managed model.\n\n    Returns:\n        nn.Module: Quantized nn.Module\n    \"\"\"\n\n    # global quantized_weight_registry\n\n    matched_module_list_by_key = {}\n    matched_module_count = 0\n\n    assert 'weight_quantization' in ds_config, 'Please provide quantization config in ds_config'\n    quantization_config = ds_config['weight_quantization']['post_init_quant']\n\n    # Return nvme swapper if exists, else return None.\n    # For nvme offloading we must use the same swapper here as model initialized.\n    nvme_swapper = get_AsyncPartitionedParameterSwapper(model)\n    is_zero3_enabled = 'zero_optimization' in ds_config and \\\n            'stage' in ds_config['zero_optimization'] and \\\n            ds_config['zero_optimization']['stage'] == 3\n    is_offloading_enabled = 'zero_optimization' in ds_config and \\\n                            'offload_param' in ds_config['zero_optimization']\n\n    layers.is_zero3_enabled = is_zero3_enabled\n\n    context_mgr = ContextManagers([QuantizationContext(config_dict_or_path=ds_config, param_swapper=nvme_swapper)]) \\\n                    if is_zero3_enabled else contextlib.suppress()\n    with context_mgr:\n        module_list = list(\n            filter(lambda named_module: type(named_module[1]) in QUANTIZATION_LAYER_MAPPINGS, model.named_modules()))\n\n        # Quantize small weight first then large.\n        if not is_offloading_enabled:\n            module_list.sort(key=lambda named_module: named_module[1].weight.ds_tensor.numel()\n                             if is_zero3_enabled else named_module[1].weight.numel())\n        module_list = deque(module_list)\n\n        while len(module_list) > 0:\n            # Use popleft to timely release module's memory of replaced module after each loop iteration\n            module_name, module = module_list.popleft()\n\n            matched_key = None\n            matched_quantization_config = None\n\n            for key, config in quantization_config.items():\n                if key in module_name:\n                    assert matched_key is None, f'{module_name} matched multiple quantization key word {matched_key} and {key}'\n                    matched_key = key\n                    matched_quantization_config = config\n\n            if matched_key is None:\n                continue\n\n            if is_zero3_enabled:\n                module.weight.all_gather()\n\n            assert module.weight.dtype == torch.float16, 'Model weight is expected in half.'\n\n            new_module = QUANTIZATION_LAYER_MAPPINGS[type(module)](matched_quantization_config, module)\n\n            if is_zero3_enabled:\n                module.weight.partition()\n\n            recursive_setattr(model, module_name, new_module)\n\n            if matched_key not in matched_module_list_by_key:\n                matched_module_list_by_key[matched_key] = []\n            matched_module_list_by_key[matched_key].append(module_name)\n            matched_module_count += 1\n\n            # Timely recycle memory to prevent OOM on large models\n            gc.collect()\n\n    # Clear registry after model construction.\n    layers.quantized_weight_registry.clear()\n\n    logger.info(\n        f'Group-wise weight quantization summary: convert {matched_module_count} node(s) to quantized implementation')\n    summary_str = '\\n'\n\n    for key, module_list in matched_module_list_by_key.items():\n        summary_str += f'Key: {key}, matched modules:\\n'\n        for module_name in module_list:\n            summary_str += f'\\t{module_name}\\n'\n    logger.info(summary_str)\n\n    return model\n", "deepspeed/inference/quantization/utils.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\nimport deepspeed\nfrom torch import Tensor\nfrom typing import Tuple\nimport torch.nn as nn\nfrom typing import Dict, Callable, Union\nfrom deepspeed.accelerator import get_accelerator\nimport functools\n\ndevice = get_accelerator().device_name() if get_accelerator().is_available() else 'cpu'\n\nquantizer_module = None\n\n\ndef get_quantizer_module():\n    global quantizer_module\n    if quantizer_module is None:\n        quantizer_module = deepspeed.ops.op_builder.QuantizerBuilder().load()\n    return quantizer_module\n\n\ndef tensor_clamp(tensor: Tensor, min, max) -> Tensor:\n    if tensor.device.type == 'cpu' and tensor.dtype == torch.float16:\n        # CPU does not support FP16 clamp\n        return tensor.to(dtype=torch.float32).clamp_(min, max).to(dtype=torch.float16)\n    else:\n        return tensor.clamp_(min, max)\n\n\ndef tensor_round(tensor: Tensor) -> Tensor:\n    if tensor.device.type == 'cpu' and tensor.dtype == torch.float16:\n        # CPU does not support FP16 round\n        return tensor.to(dtype=torch.float32).round_().to(dtype=torch.float16)\n    else:\n        return tensor.round_()\n\n\nclass Quantizer:\n\n    def __init__(self, config: Dict) -> None:\n        self.config = config\n        assert self.config['num_bits'] == 4 or self.config[\n            'num_bits'] == 8, 'Only INT4 and INT8 quantization is supported.'\n        assert self.config['symmetric'] == False, 'Only asymmetric quantization is supported at this moment.'\n\n    def quantize(self, tensor: Tensor) -> Tuple[Tensor, Tensor, Tensor]:\n        assert tensor.shape[self.config['group_dim']] % self.config['group_size'] == 0 \\\n            , f'Tensor shape: {tensor.shape} quantization config {self.config}'\n\n        tensor = torch.clone(tensor)\n\n        shape = tensor.shape\n        num_groups = shape[self.config['group_dim']] // self.config['group_size']\n        new_shape = (shape[:self.config['group_dim']] + (num_groups, self.config['group_size']) +\n                     shape[self.config['group_dim'] + 1:])\n        tensor = tensor.view(new_shape)\n\n        quantized_tensor, scale, min_value = self._quantize_int8(tensor)\n        quantized_tensor = quantized_tensor.view(shape)\n\n        if self.config['num_bits'] == 4:\n            return self._compress_uint8_to_uint4(quantized_tensor), scale, min_value\n        if self.config['num_bits'] == 8:\n            return quantized_tensor, scale, min_value\n\n        assert False, 'Unsupported quantization bits {}'.format(self.config['num_bits'])\n\n    def _quantize_int8(self, tensor: Tensor) -> Tuple[Tensor, Tensor, Tensor]:\n        q_range = 2**self.config['num_bits'] - 1\n        min_value = tensor.amin(dim=self.config['group_dim'] + 1, keepdim=True)\n        max_value = tensor.amax(dim=self.config['group_dim'] + 1, keepdim=True)\n\n        scale = q_range / (max_value - min_value)\n\n        tensor = tensor.sub_(min_value).mul_(scale)\n        tensor = tensor_round(tensor_clamp(tensor, 0, q_range)).to(torch.uint8)\n        return tensor, scale, min_value\n\n    def _compress_uint8_to_uint4(self, tensor: Tensor) -> Tensor:\n        assert tensor.shape[-1] % 2 == 0\n\n        new_data_shape = list(tensor.shape)\n        new_data_shape[-1] = new_data_shape[-1] // 2\n\n        data = torch.empty(new_data_shape, dtype=torch.uint8, device=tensor.device)\n        data = torch.bitwise_or(tensor[..., 0::2].bitwise_left_shift(4), tensor[..., 1::2])\n\n        return data\n\n\nclass DeQuantizer:\n\n    def __init__(self, config: Dict, dtype: torch.dtype) -> None:\n        self.config = config\n        self.dtype = dtype\n        assert self.config['num_bits'] == 4 or self.config[\n            'num_bits'] == 8, 'Only INT4 and INT8 quantization is supported.'\n        assert self.config['symmetric'] == False, 'Only asymmetric quantization is supported at this moment.'\n\n    def dequantize(self, tensor: Tensor, quant_scale: Tensor, quant_min: Tensor) -> Tensor:\n        # Use customized CUDA quantization kernel if possible.\n        if self.config['group_size'] % 8 == 0 and \\\n                (self.config['num_bits'] == 4 or self.config['num_bits'] == 8) and \\\n                self.config['group_dim'] == len(tensor.shape) - 1 and \\\n                    self.dtype == torch.float16 and device == get_accelerator().device_name():\n\n            last_dimension_size = self.config['group_size']\n            if self.config['num_bits'] == 4:\n                last_dimension_size = last_dimension_size // 2\n                quantized_tensor = get_quantizer_module().dequantize_int4_to_half_experimental(\n                    tensor.reshape(-1, last_dimension_size), quant_scale, quant_min,\n                    tensor.numel() // last_dimension_size, self.config['group_size'])\n                shape = list(tensor.shape)\n                shape[-1] = shape[-1] * 2\n            elif self.config['num_bits'] == 8:\n                # last_dimension_size = last_dimension_size // 2\n                quantized_tensor = get_quantizer_module().dequantize_int8_to_half_experimental(\n                    tensor.reshape(-1, last_dimension_size), quant_scale, quant_min,\n                    tensor.numel() // last_dimension_size, self.config['group_size'])\n                shape = list(tensor.shape)\n\n            return quantized_tensor.reshape(shape)\n\n        if self.config['num_bits'] == 4:\n            tensor = self._decompress_uint4_to_uint8(tensor)\n        elif self.config['num_bits'] != 8:\n            assert False, 'Unsupported quantization bits {}'.format(self.config['num_bits'])\n\n        shape = tensor.shape\n        num_groups = shape[self.config['group_dim']] // self.config['group_size']\n        new_shape = (shape[:self.config['group_dim']] + (num_groups, self.config['group_size']) +\n                     shape[self.config['group_dim'] + 1:])\n        tensor = tensor.view(new_shape)\n\n        dequantized_tensor = self._dequantize_int8(tensor, quant_scale, quant_min).view(shape)\n        return dequantized_tensor\n\n    def _dequantize_int8(self, tensor: Tensor, quant_scale: Tensor, quant_min: Tensor) -> Tensor:\n        assert tensor.dtype == torch.uint8\n        data = torch.zeros_like(tensor, dtype=self.dtype, device=tensor.device)\n        data = data.copy_(tensor)\n        data = data.div_(quant_scale).add_(quant_min)\n\n        return data\n\n    def _decompress_uint4_to_uint8(self, tensor: Tensor) -> Tensor:\n        new_data_shape = list(tensor.shape)\n        new_data_shape[-1] = new_data_shape[-1] * 2\n        data = torch.empty(new_data_shape, dtype=torch.uint8, device=tensor.device)\n        data[..., 0::2] = tensor.bitwise_right_shift(4)\n        data[..., 1::2] = tensor.bitwise_and(0xF)\n\n        return data\n\n\ndef get_AsyncPartitionedParameterSwapper(model: nn.Module):\n    for param_name, param in model.named_parameters():\n        if hasattr(param, 'nvme_swapper') and param.nvme_swapper is not None:\n            return param.nvme_swapper\n    return None\n\n\ndef recursive_setattr(model, module_name, module):\n    \"\"\"\n    Recursively set the attribute of a module.\n    Args:\n        model (`torch.nn.Module`)\n            The model to set the attribute in.\n        module_name (`str`)\n            The name of the module to set the attribute in.\n        module (`torch.nn.Module`)\n            The module to set the attribute to.\n    \"\"\"\n    split_list = module_name.split('.')\n    output = model\n    for name in split_list[:-1]:\n        output = getattr(output, name)\n    output.__setattr__(split_list[-1], module)\n\n\ndef concat_to_compat_param(quantized_weight: Tensor,\n                           quant_scale: Tensor,\n                           quant_min: Tensor,\n                           return_param: bool = True) -> Union[nn.Parameter, Tensor]:\n    shape_wieght = quantized_weight.shape\n    shape_scale = quant_scale.shape\n    shape_min = quant_min.shape\n\n    quantized_weight = torch.flatten(quantized_weight)\n    quant_scale = torch.flatten(quant_scale)\n    quant_min = torch.flatten(quant_min)\n\n    def deconcat_individual_tensors(shape_wieght: torch.Size, shape_scale: torch.Size,\n                                    shape_min: torch.Size) -> Callable:\n\n        def fn(compat_tensor: nn.Parameter) -> Tuple[Tensor, Tensor, Tensor]:\n            weight = torch.narrow(compat_tensor, 0, 0, shape_wieght.numel()).view(shape_wieght)\n            scale = torch.narrow(compat_tensor, 0, shape_wieght.numel(), shape_scale.numel()).view(shape_scale)\n            min_val = torch.narrow(compat_tensor, 0,\n                                   shape_wieght.numel() + shape_scale.numel(), shape_min.numel()).view(shape_min)\n\n            return weight, scale, min_val\n\n        return fn\n\n    compat_tensor = torch.concat([quantized_weight, quant_scale, quant_min])\n    if return_param:\n        compat_tensor = nn.Parameter(compat_tensor, requires_grad=False)\n    compat_tensor.deconcat = deconcat_individual_tensors(shape_wieght, shape_scale, shape_min)\n\n    return compat_tensor\n\n\ndef _quantize_param(param: nn.Parameter, quant_config: Dict):\n    assert not hasattr(param, 'weight_quantized'), 'Parameter has already been quantized.'\n    quantizer = Quantizer(quant_config)\n    dequantizer = DeQuantizer(quant_config, param.dtype)\n\n    quantized_weight, quant_scale, quant_min = quantizer.quantize(param.data)\n\n    quantized_weight = quantized_weight.view(param.dtype)\n    quant_scale = quant_scale.view(param.dtype)\n    quant_min = quant_min.view(param.dtype)\n\n    quantized_compat_tensor = concat_to_compat_param(quantized_weight, quant_scale, quant_min)\n    param.data = quantized_compat_tensor\n    param.deconcat = quantized_compat_tensor.deconcat\n\n    param.quantizer = quantizer\n    param.dequantizer = dequantizer\n    setattr(param, 'weight_quantized', True)\n\n\ndef wrap_quantized_functional(f):\n\n    @functools.wraps(f)\n    def wrapper(input: Tensor, weight: nn.Parameter, *args, **kwargs) -> Tensor:\n        if hasattr(weight, 'weight_quantized') and getattr(weight, 'weight_quantized'):\n            quantized_weight, quant_scale, quant_min = weight.deconcat(weight)\n            temp_dequantized_weight = weight.dequantizer.dequantize(quantized_weight.view(torch.uint8), quant_scale,\n                                                                    quant_min)\n            return f(input, temp_dequantized_weight, *args, **kwargs)\n        else:\n            return f(input, weight, *args, **kwargs)\n\n    return wrapper\n\n\ndef wrap_load_from_state_dict(f):\n\n    @functools.wraps(f)\n    def wrapper(model, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs):\n        replaced_old_value = None\n        key = None\n        # We may have nested wrappers if we launch multiple initialization context.\n        # Use state_dict_quantized flag to quantize state_dict only once\n        if hasattr(model.weight, 'weight_quantized') and getattr(\n                model.weight, 'weight_quantized') and not hasattr(model.weight, 'state_dict_quantized'):\n            setattr(model.weight, 'state_dict_quantized', True)\n            key = prefix + 'weight'\n            if key in state_dict:\n                quantized_weight, quant_scale, quant_min = model.weight.quantizer.quantize(state_dict[key])\n                quantized_weight = quantized_weight.view(model.weight.dtype)\n                quant_scale = quant_scale.view(model.weight.dtype)\n                quant_min = quant_min.view(model.weight.dtype)\n\n                replaced_old_value = state_dict[key]\n\n                state_dict[key] = concat_to_compat_param(quantized_weight, quant_scale, quant_min)\n\n        f(model, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\n\n        if replaced_old_value is not None:\n            state_dict[key] = replaced_old_value\n            delattr(model.weight, 'state_dict_quantized')\n\n    return wrapper\n\n\nWEIGHT_QUANTIZATION_LAYERS = (\n    nn.Linear,\n    nn.Embedding,\n)\n", "deepspeed/inference/quantization/quantization_context.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom deepspeed.runtime.zero import partition_parameters\nfrom deepspeed.runtime.swap_tensor.partitioned_param_swapper import AsyncPartitionedParameterSwapper\n\n\nclass QuantizationContext(partition_parameters.Init):\n\n    def __init__(self, config_dict_or_path, param_swapper: AsyncPartitionedParameterSwapper = None) -> None:\n        super().__init__(config_dict_or_path=config_dict_or_path, param_swapper=param_swapper)\n", "deepspeed/inference/quantization/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n", "deepspeed/inference/quantization/layers.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport torch\n\nfrom torch import nn\nfrom torch import Tensor\nfrom torch.nn import functional as F\nfrom .utils import Quantizer, DeQuantizer, concat_to_compat_param\nfrom typing import Tuple, Callable, Dict\nfrom deepspeed.runtime.zero import register_external_parameter\n\nquantized_weight_registry = {}\nis_zero3_enabled = False\n\n\n# deal with weight sharing\ndef get_quantized_weight_wrapper(model, pre_quant_weight: nn.Parameter, quantize_weight_fn: Callable) -> nn.Parameter:\n    if id(pre_quant_weight) in quantized_weight_registry:\n        compat_tensor = quantized_weight_registry[id(pre_quant_weight)]\n        if is_zero3_enabled:\n            register_external_parameter(model, compat_tensor)\n\n        return quantized_weight_registry[id(pre_quant_weight)]\n    else:\n        quantized_weights, quant_scale, quant_min = quantize_weight_fn()\n        quantized_weight_registry[id(pre_quant_weight)] = concat_to_compat_param(quantized_weights, quant_scale,\n                                                                                 quant_min)\n        return quantized_weight_registry[id(pre_quant_weight)]\n\n\ndef get_quantize_weight_fn(quantizer: Quantizer, pre_quant_weight: nn.Parameter) -> Callable:\n\n    def func() -> Tuple[nn.Parameter, Tensor, Tensor]:\n        quantized_weights, quant_scale, quant_min = quantizer.quantize(pre_quant_weight.data)\n        # A temporary hack as zero Zero3 assume all model weights has the same type. in all_gather_coalesced.get_only_unique_item\n        quantized_weights = quantized_weights.view(pre_quant_weight.dtype)\n        quant_scale = quant_scale.type(pre_quant_weight.dtype)\n        quant_min = quant_min.type(pre_quant_weight.dtype)\n        return quantized_weights, quant_scale, quant_min\n\n    return func\n\n\nclass QuantizedLinear(nn.Linear):\n\n    def __init__(self, config: Dict, pre_quant_layer: nn.Linear) -> None:\n        super(QuantizedLinear, self).__init__(in_features=pre_quant_layer.in_features,\n                                              out_features=pre_quant_layer.out_features,\n                                              bias=pre_quant_layer.bias is not None,\n                                              device=pre_quant_layer.weight.device,\n                                              dtype=pre_quant_layer.weight.dtype)\n        self.config = config\n\n        self.quantizer = Quantizer(config=config)\n        self.bias = pre_quant_layer.bias\n        self.weight = get_quantized_weight_wrapper(self, pre_quant_layer.weight,\n                                                   get_quantize_weight_fn(self.quantizer, pre_quant_layer.weight))\n\n        self.weight.dequantizer = DeQuantizer(config, pre_quant_layer.weight.dtype)\n\n    def forward(self, input: Tensor) -> Tensor:\n        quantized_weight, quant_scale, quant_min = self.weight.deconcat(self.weight)\n        temp_dequantized_weight = self.weight.dequantizer.dequantize(quantized_weight.view(torch.uint8), quant_scale,\n                                                                     quant_min)\n\n        # !!! Do not use torch.functional.linear(input, temp_dequantized_weight, self.bias) here as in zero3 torch.functional.linear is\n        # replaced by LinearFunctionForZeroStage3. Which assume weight is non-temporary.\n        # If weight is temp buffer there will be memory leak.\n        return torch._C._nn.linear(input, temp_dequantized_weight, self.bias)\n\n\nclass QuantizedEmbedding(nn.Embedding):\n\n    def __init__(self, config: Dict, pre_quant_layer: nn.Embedding) -> None:\n        super(QuantizedEmbedding, self).__init__(num_embeddings=pre_quant_layer.num_embeddings,\n                                                 embedding_dim=pre_quant_layer.embedding_dim,\n                                                 padding_idx=pre_quant_layer.padding_idx,\n                                                 max_norm=pre_quant_layer.max_norm,\n                                                 norm_type=pre_quant_layer.norm_type,\n                                                 scale_grad_by_freq=pre_quant_layer.scale_grad_by_freq,\n                                                 sparse=pre_quant_layer.sparse,\n                                                 _weight=pre_quant_layer.weight,\n                                                 device=pre_quant_layer.weight.device,\n                                                 dtype=pre_quant_layer.weight.dtype)\n\n        assert pre_quant_layer.max_norm is None, 'Not supported'\n        assert pre_quant_layer.norm_type == 2, 'Not supported'\n        assert pre_quant_layer.scale_grad_by_freq == False, 'Not supported'\n        assert pre_quant_layer.sparse == False, 'Not supported'\n\n        self.config = config\n        quantizer = Quantizer(config=config)\n\n        self.weight = get_quantized_weight_wrapper(self, pre_quant_layer.weight,\n                                                   get_quantize_weight_fn(quantizer, pre_quant_layer.weight))\n\n        self.weight.dequantizer = DeQuantizer(config, pre_quant_layer.weight.dtype)\n\n    def forward(self, input: Tensor) -> Tensor:\n        quantized_weight, quant_scale, quant_min = self.weight.deconcat(self.weight)\n        temp_dequantized_weight = self.weight.dequantizer.dequantize(quantized_weight.view(torch.uint8), quant_scale,\n                                                                     quant_min)\n\n        return F.embedding(input, temp_dequantized_weight, self.padding_idx, self.max_norm, self.norm_type,\n                           self.scale_grad_by_freq, self.sparse)\n\n\nQUANTIZATION_LAYER_MAPPINGS = {\n    nn.Linear: QuantizedLinear,\n    nn.Embedding: QuantizedEmbedding,\n}\n", "op_builder/stochastic_transformer.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .transformer import TransformerBuilder\n\n\nclass StochasticTransformerBuilder(TransformerBuilder):\n    BUILD_VAR = \"DS_BUILD_STOCHASTIC_TRANSFORMER\"\n    NAME = \"stochastic_transformer\"\n\n    def __init__(self):\n        super().__init__(name=self.NAME)\n\n    def absolute_name(self):\n        return f'deepspeed.ops.transformer.{self.NAME}_op'\n\n    def nvcc_args(self):\n        args = super().nvcc_args()\n        args.append('-D__STOCHASTIC_MODE__')\n        return args\n", "op_builder/fused_lion.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .builder import CUDAOpBuilder\n\nimport sys\n\n\nclass FusedLionBuilder(CUDAOpBuilder):\n    BUILD_VAR = \"DS_BUILD_FUSED_LION\"\n    NAME = \"fused_lion\"\n\n    def __init__(self):\n        super().__init__(name=self.NAME)\n\n    def absolute_name(self):\n        return f'deepspeed.ops.lion.{self.NAME}_op'\n\n    def sources(self):\n        return ['csrc/lion/fused_lion_frontend.cpp', 'csrc/lion/multi_tensor_lion.cu']\n\n    def include_paths(self):\n        return ['csrc/includes', 'csrc/lion']\n\n    def cxx_args(self):\n        args = super().cxx_args()\n        return args + self.version_dependent_macros()\n\n    def nvcc_args(self):\n        nvcc_flags = ['-O3'] + self.version_dependent_macros()\n        if not self.is_rocm_pytorch():\n            nvcc_flags.extend(\n                ['-allow-unsupported-compiler' if sys.platform == \"win32\" else '', '-lineinfo', '--use_fast_math'] +\n                self.compute_capability_args())\n        return nvcc_flags\n", "op_builder/all_ops.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport os\nimport pkgutil\nimport importlib\ntry:\n    # during installation time accelerator is visible, otherwise return deepspeed.accelerator\n    from accelerator import get_accelerator\nexcept ImportError:\n    from deepspeed.accelerator import get_accelerator\n\n# List of all available ops\n\n# reflect all builder names into __op_builders__\nop_builder_dir = get_accelerator().op_builder_dir()\nop_builder_module = importlib.import_module(op_builder_dir)\n__op_builders__ = []\n\nfor _, module_name, _ in pkgutil.iter_modules([os.path.dirname(op_builder_module.__file__)]):\n    # avoid self references\n    if module_name != 'all_ops' and module_name != 'builder':\n        module = importlib.import_module(\"{}.{}\".format(op_builder_dir, module_name))\n        for member_name in module.__dir__():\n            if member_name.endswith('Builder'):\n                # append builder to __op_builders__ list\n                builder = get_accelerator().create_op_builder(member_name)\n                __op_builders__.append(builder)\n\nALL_OPS = {op.name: op for op in __op_builders__ if op is not None}\naccelerator_name = get_accelerator()._name\n", "op_builder/cpu_lion.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .builder import TorchCPUOpBuilder\n\n\nclass CPULionBuilder(TorchCPUOpBuilder):\n    BUILD_VAR = \"DS_BUILD_CPU_LION\"\n    NAME = \"cpu_lion\"\n\n    def __init__(self):\n        super().__init__(name=self.NAME)\n\n    def absolute_name(self):\n        return f'deepspeed.ops.lion.{self.NAME}_op'\n\n    def sources(self):\n        return ['csrc/lion/cpu_lion.cpp', 'csrc/lion/cpu_lion_impl.cpp']\n\n    def libraries_args(self):\n        args = super().libraries_args()\n        return args\n\n    def include_paths(self):\n        return ['csrc/includes']\n", "op_builder/quantizer.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .builder import CUDAOpBuilder\n\n\nclass QuantizerBuilder(CUDAOpBuilder):\n    BUILD_VAR = \"DS_BUILD_QUANTIZER\"\n    NAME = \"quantizer\"\n\n    def __init__(self, name=None):\n        name = self.NAME if name is None else name\n        super().__init__(name=name)\n\n    def absolute_name(self):\n        return f'deepspeed.ops.quantizer.{self.NAME}_op'\n\n    def sources(self):\n        return [\n            'csrc/quantization/pt_binding.cpp',\n            'csrc/quantization/fake_quantizer.cu',\n            'csrc/quantization/quantize.cu',\n            'csrc/quantization/quantize_intX.cu',\n            'csrc/quantization/dequantize.cu',\n            'csrc/quantization/swizzled_quantize.cu',\n            'csrc/quantization/quant_reduce.cu',\n        ]\n\n    def include_paths(self):\n        return ['csrc/includes']\n\n    def extra_ldflags(self):\n        if not self.is_rocm_pytorch():\n            return ['-lcurand']\n        else:\n            return []\n", "op_builder/random_ltd.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .builder import CUDAOpBuilder\n\n\nclass RandomLTDBuilder(CUDAOpBuilder):\n    BUILD_VAR = \"DS_BUILD_RANDOM_LTD\"\n    NAME = \"random_ltd\"\n\n    def __init__(self, name=None):\n        name = self.NAME if name is None else name\n        super().__init__(name=name)\n\n    def absolute_name(self):\n        return f'deepspeed.ops.{self.NAME}_op'\n\n    def extra_ldflags(self):\n        if not self.is_rocm_pytorch():\n            return ['-lcurand']\n        else:\n            return []\n\n    def sources(self):\n        return [\n            'csrc/random_ltd/pt_binding.cpp', 'csrc/random_ltd/gather_scatter.cu',\n            'csrc/random_ltd/slice_attn_masks.cu', 'csrc/random_ltd/token_sort.cu'\n        ]\n\n    def include_paths(self):\n        includes = ['csrc/includes']\n        return includes\n", "op_builder/transformer_inference.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .builder import CUDAOpBuilder, installed_cuda_version\n\n\nclass InferenceBuilder(CUDAOpBuilder):\n    BUILD_VAR = \"DS_BUILD_TRANSFORMER_INFERENCE\"\n    NAME = \"transformer_inference\"\n\n    def __init__(self, name=None):\n        name = self.NAME if name is None else name\n        super().__init__(name=name)\n\n    def absolute_name(self):\n        return f'deepspeed.ops.transformer.inference.{self.NAME}_op'\n\n    def is_compatible(self, verbose=True):\n        try:\n            import torch\n        except ImportError:\n            self.warning(\"Please install torch if trying to pre-compile inference kernels\")\n            return False\n\n        cuda_okay = True\n        if not self.is_rocm_pytorch() and torch.cuda.is_available():\n            sys_cuda_major, _ = installed_cuda_version()\n            torch_cuda_major = int(torch.version.cuda.split('.')[0])\n            cuda_capability = torch.cuda.get_device_properties(0).major\n            if cuda_capability < 6:\n                self.warning(\"NVIDIA Inference is only supported on Pascal and newer architectures\")\n                cuda_okay = False\n            if cuda_capability >= 8:\n                if torch_cuda_major < 11 or sys_cuda_major < 11:\n                    self.warning(\"On Ampere and higher architectures please use CUDA 11+\")\n                    cuda_okay = False\n        return super().is_compatible(verbose) and cuda_okay\n\n    def filter_ccs(self, ccs):\n        ccs_retained = []\n        ccs_pruned = []\n        for cc in ccs:\n            if int(cc[0]) >= 6:\n                ccs_retained.append(cc)\n            else:\n                ccs_pruned.append(cc)\n        if len(ccs_pruned) > 0:\n            self.warning(f\"Filtered compute capabilities {ccs_pruned}\")\n        return ccs_retained\n\n    def sources(self):\n        return [\n            'csrc/transformer/inference/csrc/pt_binding.cpp',\n            'csrc/transformer/inference/csrc/gelu.cu',\n            'csrc/transformer/inference/csrc/relu.cu',\n            'csrc/transformer/inference/csrc/layer_norm.cu',\n            'csrc/transformer/inference/csrc/rms_norm.cu',\n            'csrc/transformer/inference/csrc/softmax.cu',\n            'csrc/transformer/inference/csrc/dequantize.cu',\n            'csrc/transformer/inference/csrc/apply_rotary_pos_emb.cu',\n            'csrc/transformer/inference/csrc/transform.cu',\n            'csrc/transformer/inference/csrc/pointwise_ops.cu',\n        ]\n\n    def extra_ldflags(self):\n        if not self.is_rocm_pytorch():\n            return ['-lcurand']\n        else:\n            return []\n\n    def include_paths(self):\n        return ['csrc/transformer/inference/includes', 'csrc/includes']\n", "op_builder/fused_lamb.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .builder import CUDAOpBuilder\n\nimport sys\n\n\nclass FusedLambBuilder(CUDAOpBuilder):\n    BUILD_VAR = 'DS_BUILD_FUSED_LAMB'\n    NAME = \"fused_lamb\"\n\n    def __init__(self):\n        super().__init__(name=self.NAME)\n\n    def absolute_name(self):\n        return f'deepspeed.ops.lamb.{self.NAME}_op'\n\n    def sources(self):\n        return ['csrc/lamb/fused_lamb_cuda.cpp', 'csrc/lamb/fused_lamb_cuda_kernel.cu']\n\n    def include_paths(self):\n        return ['csrc/includes']\n\n    def cxx_args(self):\n        args = super().cxx_args()\n        return args + self.version_dependent_macros()\n\n    def nvcc_args(self):\n        nvcc_flags = ['-O3'] + self.version_dependent_macros()\n        if self.is_rocm_pytorch():\n            ROCM_MAJOR, ROCM_MINOR = self.installed_rocm_version()\n            nvcc_flags += ['-DROCM_VERSION_MAJOR=%s' % ROCM_MAJOR, '-DROCM_VERSION_MINOR=%s' % ROCM_MINOR]\n        else:\n            nvcc_flags.extend(\n                ['-allow-unsupported-compiler' if sys.platform == \"win32\" else '', '-lineinfo', '--use_fast_math'] +\n                self.compute_capability_args())\n        return nvcc_flags\n", "op_builder/transformer.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .builder import CUDAOpBuilder\n\n\nclass TransformerBuilder(CUDAOpBuilder):\n    BUILD_VAR = \"DS_BUILD_TRANSFORMER\"\n    NAME = \"transformer\"\n\n    def __init__(self, name=None):\n        name = self.NAME if name is None else name\n        super().__init__(name=name)\n\n    def absolute_name(self):\n        return f'deepspeed.ops.transformer.{self.NAME}_op'\n\n    def extra_ldflags(self):\n        if not self.is_rocm_pytorch():\n            return ['-lcurand']\n        else:\n            return []\n\n    def sources(self):\n        return [\n            'csrc/transformer/ds_transformer_cuda.cpp', 'csrc/transformer/cublas_wrappers.cu',\n            'csrc/transformer/transform_kernels.cu', 'csrc/transformer/gelu_kernels.cu',\n            'csrc/transformer/dropout_kernels.cu', 'csrc/transformer/normalize_kernels.cu',\n            'csrc/transformer/softmax_kernels.cu', 'csrc/transformer/general_kernels.cu'\n        ]\n\n    def include_paths(self):\n        includes = ['csrc/includes']\n        return includes\n", "op_builder/fused_adam.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .builder import CUDAOpBuilder\n\nimport sys\n\n\nclass FusedAdamBuilder(CUDAOpBuilder):\n    BUILD_VAR = \"DS_BUILD_FUSED_ADAM\"\n    NAME = \"fused_adam\"\n\n    def __init__(self):\n        super().__init__(name=self.NAME)\n\n    def absolute_name(self):\n        return f'deepspeed.ops.adam.{self.NAME}_op'\n\n    def sources(self):\n        return ['csrc/adam/fused_adam_frontend.cpp', 'csrc/adam/multi_tensor_adam.cu']\n\n    def include_paths(self):\n        return ['csrc/includes', 'csrc/adam']\n\n    def cxx_args(self):\n        args = super().cxx_args()\n        return args + self.version_dependent_macros()\n\n    def nvcc_args(self):\n        nvcc_flags = ['-O3'] + self.version_dependent_macros()\n        if not self.is_rocm_pytorch():\n            nvcc_flags.extend(\n                ['-allow-unsupported-compiler' if sys.platform == \"win32\" else '', '-lineinfo', '--use_fast_math'] +\n                self.compute_capability_args())\n        return nvcc_flags\n", "op_builder/evoformer_attn.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .builder import CUDAOpBuilder, installed_cuda_version\nimport os\n\n\nclass EvoformerAttnBuilder(CUDAOpBuilder):\n    BUILD_VAR = \"DS_BUILD_EVOFORMER_ATTN\"\n    NAME = \"evoformer_attn\"\n\n    def __init__(self, name=None):\n        name = self.NAME if name is None else name\n        super().__init__(name=name)\n        self.cutlass_path = os.environ.get('CUTLASS_PATH')\n\n    def absolute_name(self):\n        return f'deepspeed.ops.{self.NAME}_op'\n\n    def extra_ldflags(self):\n        if not self.is_rocm_pytorch():\n            return ['-lcurand']\n        else:\n            return []\n\n    def sources(self):\n        src_dir = 'csrc/deepspeed4science/evoformer_attn'\n        return [f'{src_dir}/attention.cpp', f'{src_dir}/attention_back.cu', f'{src_dir}/attention_cu.cu']\n\n    def nvcc_args(self):\n        args = super().nvcc_args()\n        try:\n            import torch\n        except ImportError:\n            self.warning(\"Please install torch if trying to pre-compile kernels\")\n            return args\n        major = torch.cuda.get_device_properties(0).major  #ignore-cuda\n        minor = torch.cuda.get_device_properties(0).minor  #ignore-cuda\n        args.append(f\"-DGPU_ARCH={major}{minor}\")\n        return args\n\n    def is_compatible(self, verbose=True):\n        try:\n            import torch\n        except ImportError:\n            self.warning(\"Please install torch if trying to pre-compile kernels\")\n            return False\n        if self.cutlass_path is None:\n            self.warning(\"Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\")\n            return False\n        with open(f'{self.cutlass_path}/CHANGELOG.md', 'r') as f:\n            if '3.1.0' not in f.read():\n                self.warning(\"Please use CUTLASS version >= 3.1.0\")\n                return False\n        cuda_okay = True\n        if not self.is_rocm_pytorch() and torch.cuda.is_available():  #ignore-cuda\n            sys_cuda_major, _ = installed_cuda_version()\n            torch_cuda_major = int(torch.version.cuda.split('.')[0])\n            cuda_capability = torch.cuda.get_device_properties(0).major  #ignore-cuda\n            if cuda_capability < 7:\n                self.warning(\"Please use a GPU with compute capability >= 7.0\")\n                cuda_okay = False\n            if torch_cuda_major < 11 or sys_cuda_major < 11:\n                self.warning(\"Please use CUDA 11+\")\n                cuda_okay = False\n        return super().is_compatible(verbose) and cuda_okay\n\n    def include_paths(self):\n        includes = [f'{self.cutlass_path}/include', f'{self.cutlass_path}/tools/util/include']\n        return includes\n", "op_builder/fp_quantizer.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .builder import CUDAOpBuilder, installed_cuda_version\n\n\nclass FPQuantizerBuilder(CUDAOpBuilder):\n    BUILD_VAR = \"DS_BUILD_FP_QUANTIZER\"\n    NAME = \"fp_quantizer\"\n\n    def __init__(self, name=None):\n        name = self.NAME if name is None else name\n        super().__init__(name=name)\n\n    def absolute_name(self):\n        return f'deepspeed.ops.fp_quantizer.{self.NAME}_op'\n\n    def is_compatible(self, verbose=True):\n        try:\n            import torch\n        except ImportError:\n            self.warning(\"Please install torch if trying to pre-compile inference kernels\")\n            return False\n\n        cuda_okay = True\n        if not self.is_rocm_pytorch() and torch.cuda.is_available():  #ignore-cuda\n            sys_cuda_major, _ = installed_cuda_version()\n            torch_cuda_major = int(torch.version.cuda.split('.')[0])\n            cuda_capability = torch.cuda.get_device_properties(0).major  #ignore-cuda\n            if cuda_capability < 8:\n                self.warning(\"NVIDIA Inference is only supported on Ampere and newer architectures\")\n                cuda_okay = False\n            if cuda_capability >= 8:\n                if torch_cuda_major < 11 or sys_cuda_major < 11:\n                    self.warning(\"On Ampere and higher architectures please use CUDA 11+\")\n                    cuda_okay = False\n        return super().is_compatible(verbose) and cuda_okay\n\n    def filter_ccs(self, ccs):\n        ccs_retained = []\n        ccs_pruned = []\n        for cc in ccs:\n            if int(cc[0]) >= 8:\n                ccs_retained.append(cc)\n            else:\n                ccs_pruned.append(cc)\n        if len(ccs_pruned) > 0:\n            self.warning(f\"Filtered compute capabilities {ccs_pruned}\")\n        return ccs_retained\n\n    def sources(self):\n        return [\n            \"csrc/fp_quantizer/fp_quantize.cu\",\n            \"csrc/fp_quantizer/fp_quantize.cpp\",\n        ]\n\n    def extra_ldflags(self):\n        return ['-lcurand']\n\n    def include_paths(self):\n        return ['csrc/fp_quantizer/includes', 'csrc/includes']\n", "op_builder/inference_core_ops.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport os\n\nfrom .builder import CUDAOpBuilder, installed_cuda_version\n\n\nclass InferenceCoreBuilder(CUDAOpBuilder):\n    BUILD_VAR = \"DS_BUILD_INFERENCE_CORE_OPS\"\n    NAME = \"inference_core_ops\"\n\n    def __init__(self, name=None):\n        name = self.NAME if name is None else name\n        super().__init__(name=name)\n\n    def absolute_name(self):\n        return f'deepspeed.inference.v2.kernels{self.NAME}'\n\n    def is_compatible(self, verbose=True):\n        try:\n            import torch\n        except ImportError:\n            self.warning(\"Please install torch if trying to pre-compile inference kernels\")\n            return False\n\n        cuda_okay = True\n        if not self.is_rocm_pytorch() and torch.cuda.is_available():  #ignore-cuda\n            sys_cuda_major, _ = installed_cuda_version()\n            torch_cuda_major = int(torch.version.cuda.split('.')[0])\n            cuda_capability = torch.cuda.get_device_properties(0).major  #ignore-cuda\n            if cuda_capability < 6:\n                self.warning(\"NVIDIA Inference is only supported on Pascal and newer architectures\")\n                cuda_okay = False\n            if cuda_capability >= 8:\n                if torch_cuda_major < 11 or sys_cuda_major < 11:\n                    self.warning(\"On Ampere and higher architectures please use CUDA 11+\")\n                    cuda_okay = False\n        return super().is_compatible(verbose) and cuda_okay\n\n    def filter_ccs(self, ccs):\n        ccs_retained = []\n        ccs_pruned = []\n        for cc in ccs:\n            if int(cc[0]) >= 6:\n                ccs_retained.append(cc)\n            else:\n                ccs_pruned.append(cc)\n        if len(ccs_pruned) > 0:\n            self.warning(f\"Filtered compute capabilities {ccs_pruned}\")\n        return ccs_retained\n\n    def get_prefix(self):\n        ds_path = self.deepspeed_src_path(\"deepspeed\")\n        return \"deepspeed\" if os.path.isdir(ds_path) else \"..\"\n\n    def sources(self):\n        sources = [\n            \"inference/v2/kernels/core_ops/core_ops.cpp\",\n            \"inference/v2/kernels/core_ops/bias_activations/bias_activation.cpp\",\n            \"inference/v2/kernels/core_ops/bias_activations/bias_activation_cuda.cu\",\n            \"inference/v2/kernels/core_ops/cuda_layer_norm/layer_norm.cpp\",\n            \"inference/v2/kernels/core_ops/cuda_layer_norm/layer_norm_cuda.cu\",\n            \"inference/v2/kernels/core_ops/cuda_rms_norm/rms_norm.cpp\",\n            \"inference/v2/kernels/core_ops/cuda_rms_norm/rms_norm_cuda.cu\",\n            \"inference/v2/kernels/core_ops/gated_activations/gated_activation_kernels.cpp\",\n            \"inference/v2/kernels/core_ops/gated_activations/gated_activation_kernels_cuda.cu\",\n            \"inference/v2/kernels/core_ops/cuda_linear/linear_kernels.cpp\",\n            \"inference/v2/kernels/core_ops/cuda_linear/linear_kernels_cuda.cu\",\n        ]\n\n        prefix = self.get_prefix()\n        sources = [os.path.join(prefix, src) for src in sources]\n        return sources\n\n    def extra_ldflags(self):\n        return []\n\n    def include_paths(self):\n        sources = [\n            'inference/v2/kernels/core_ops/bias_activations',\n            'inference/v2/kernels/core_ops/blas_kernels',\n            'inference/v2/kernels/core_ops/cuda_layer_norm',\n            'inference/v2/kernels/core_ops/cuda_rms_norm',\n            'inference/v2/kernels/core_ops/gated_activations',\n            'inference/v2/kernels/core_ops/cuda_linear',\n            'inference/v2/kernels/includes',\n        ]\n\n        prefix = self.get_prefix()\n        sources = [os.path.join(prefix, src) for src in sources]\n\n        return sources\n", "op_builder/spatial_inference.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .builder import CUDAOpBuilder, installed_cuda_version\n\n\nclass SpatialInferenceBuilder(CUDAOpBuilder):\n    BUILD_VAR = \"DS_BUILD_SPATIAL_INFERENCE\"\n    NAME = \"spatial_inference\"\n\n    def __init__(self, name=None):\n        name = self.NAME if name is None else name\n        super().__init__(name=name)\n\n    def absolute_name(self):\n        return f'deepspeed.ops.spatial.{self.NAME}_op'\n\n    def is_compatible(self, verbose=True):\n        try:\n            import torch\n        except ImportError:\n            self.warning(\"Please install torch if trying to pre-compile inference kernels\")\n            return False\n\n        cuda_okay = True\n        if not self.is_rocm_pytorch() and torch.cuda.is_available():\n            sys_cuda_major, _ = installed_cuda_version()\n            torch_cuda_major = int(torch.version.cuda.split('.')[0])\n            cuda_capability = torch.cuda.get_device_properties(0).major\n            if cuda_capability >= 8:\n                if torch_cuda_major < 11 or sys_cuda_major < 11:\n                    self.warning(\"On Ampere and higher architectures please use CUDA 11+\")\n                    cuda_okay = False\n        return super().is_compatible(verbose) and cuda_okay\n\n    def sources(self):\n        return [\n            'csrc/spatial/csrc/opt_bias_add.cu',\n            'csrc/spatial/csrc/pt_binding.cpp',\n        ]\n\n    def include_paths(self):\n        return ['csrc/spatial/includes', 'csrc/includes']\n", "op_builder/ragged_utils.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport os\n\nfrom .builder import CUDAOpBuilder, installed_cuda_version\n\n\nclass RaggedUtilsBuilder(CUDAOpBuilder):\n    BUILD_VAR = \"DS_BUILD_RAGGED_OPS\"\n    NAME = \"ragged_ops\"\n\n    def __init__(self, name=None):\n        name = self.NAME if name is None else name\n        super().__init__(name=name)\n\n    def absolute_name(self):\n        return f'deepspeed.inference.v2.{self.NAME}'\n\n    def is_compatible(self, verbose=True):\n        try:\n            import torch\n        except ImportError:\n            self.warning(\"Please install torch if trying to pre-compile inference kernels\")\n            return False\n\n        cuda_okay = True\n        if not self.is_rocm_pytorch() and torch.cuda.is_available():  #ignore-cuda\n            sys_cuda_major, _ = installed_cuda_version()\n            torch_cuda_major = int(torch.version.cuda.split('.')[0])\n            cuda_capability = torch.cuda.get_device_properties(0).major  #ignore-cuda\n            if cuda_capability < 6:\n                self.warning(\"NVIDIA Inference is only supported on Pascal and newer architectures\")\n                cuda_okay = False\n            if cuda_capability >= 8:\n                if torch_cuda_major < 11 or sys_cuda_major < 11:\n                    self.warning(\"On Ampere and higher architectures please use CUDA 11+\")\n                    cuda_okay = False\n        return super().is_compatible(verbose) and cuda_okay\n\n    def filter_ccs(self, ccs):\n        ccs_retained = []\n        ccs_pruned = []\n        for cc in ccs:\n            if int(cc[0]) >= 6:\n                ccs_retained.append(cc)\n            else:\n                ccs_pruned.append(cc)\n        if len(ccs_pruned) > 0:\n            self.warning(f\"Filtered compute capabilities {ccs_pruned}\")\n        return ccs_retained\n\n    def get_prefix(self):\n        ds_path = self.deepspeed_src_path(\"deepspeed\")\n        return \"deepspeed\" if os.path.isdir(ds_path) else \"..\"\n\n    def sources(self):\n        sources = [\n            \"inference/v2/ragged/csrc/fast_host_buffer.cu\",\n            \"inference/v2/ragged/csrc/ragged_ops.cpp\",\n        ]\n\n        prefix = self.get_prefix()\n        sources = [os.path.join(prefix, src) for src in sources]\n        return sources\n\n    def extra_ldflags(self):\n        return []\n\n    def include_paths(self):\n        include_dirs = ['inference/v2/ragged/includes', 'inference/v2/kernels/includes']\n        prefix = self.get_prefix()\n        includes = [os.path.join(prefix, include_dir) for include_dir in include_dirs]\n\n        return includes\n", "op_builder/ragged_ops.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport os\n\nfrom .builder import CUDAOpBuilder, installed_cuda_version\n\n\nclass RaggedOpsBuilder(CUDAOpBuilder):\n    BUILD_VAR = \"DS_BUILD_RAGGED_DEVICE_OPS\"\n    NAME = \"ragged_device_ops\"\n\n    def __init__(self, name=None):\n        name = self.NAME if name is None else name\n        super().__init__(name=name)\n\n    def absolute_name(self):\n        return f'deepspeed.inference.v2.kernels.ragged_ops.{self.NAME}'\n\n    def is_compatible(self, verbose=True):\n        try:\n            import torch\n        except ImportError:\n            self.warning(\"Please install torch if trying to pre-compile inference kernels\")\n            return False\n\n        cuda_okay = True\n        if not self.is_rocm_pytorch() and torch.cuda.is_available():  #ignore-cuda\n            sys_cuda_major, _ = installed_cuda_version()\n            torch_cuda_major = int(torch.version.cuda.split('.')[0])\n            cuda_capability = torch.cuda.get_device_properties(0).major  #ignore-cuda\n            if cuda_capability < 6:\n                self.warning(\"NVIDIA Inference is only supported on Pascal and newer architectures\")\n                cuda_okay = False\n            if cuda_capability >= 8:\n                if torch_cuda_major < 11 or sys_cuda_major < 11:\n                    self.warning(\"On Ampere and higher architectures please use CUDA 11+\")\n                    cuda_okay = False\n        return super().is_compatible(verbose) and cuda_okay\n\n    def filter_ccs(self, ccs):\n        ccs_retained = []\n        ccs_pruned = []\n        for cc in ccs:\n            if int(cc[0]) >= 8:\n                # Blocked flash has a dependency on Ampere + newer\n                ccs_retained.append(cc)\n            else:\n                ccs_pruned.append(cc)\n        if len(ccs_pruned) > 0:\n            self.warning(f\"Filtered compute capabilities {ccs_pruned}\")\n        return ccs_retained\n\n    def get_prefix(self):\n        ds_path = self.deepspeed_src_path(\"deepspeed\")\n        return \"deepspeed\" if os.path.isdir(ds_path) else \"..\"\n\n    def sources(self):\n        sources = [\n            \"inference/v2/kernels/ragged_ops/ragged_ops.cpp\",\n            \"inference/v2/kernels/ragged_ops/atom_builder/atom_builder.cpp\",\n            \"inference/v2/kernels/ragged_ops/blocked_flash/blocked_flash.cpp\",\n            \"inference/v2/kernels/ragged_ops/embed/embed.cpp\",\n            \"inference/v2/kernels/ragged_ops/embed/embed_cuda.cu\",\n            \"inference/v2/kernels/ragged_ops/linear_blocked_kv_rotary/blocked_kv_rotary.cpp\",\n            \"inference/v2/kernels/ragged_ops/linear_blocked_kv_rotary/blocked_kv_rotary_cuda.cu\",\n            \"inference/v2/kernels/ragged_ops/logits_gather/logits_gather.cpp\",\n            \"inference/v2/kernels/ragged_ops/logits_gather/logits_gather_cuda.cu\",\n            \"inference/v2/kernels/ragged_ops/moe_scatter/moe_scatter.cpp\",\n            \"inference/v2/kernels/ragged_ops/moe_scatter/moe_scatter_cuda.cu\",\n            \"inference/v2/kernels/ragged_ops/moe_gather/moe_gather.cpp\",\n            \"inference/v2/kernels/ragged_ops/moe_gather/moe_gather_cuda.cu\",\n            \"inference/v2/kernels/ragged_ops/ragged_helpers/ragged_kernel_helpers.cpp\",\n            \"inference/v2/kernels/ragged_ops/top_k_gating/top_k_gating.cpp\",\n            \"inference/v2/kernels/ragged_ops/top_k_gating/top_k_gating_cuda.cu\",\n        ]\n\n        prefix = self.get_prefix()\n        sources = [os.path.join(prefix, src) for src in sources]\n        return sources\n\n    def extra_ldflags(self):\n        import dskernels\n        lib_path = dskernels.library_path()\n\n        prefix = self.get_prefix()\n        lib_path = os.path.join(prefix, lib_path)\n        lib_path = self.deepspeed_src_path(lib_path)\n\n        args = [f'-L{lib_path}', '-lblockedflash']\n        if self.jit_load:\n            args.append(f'-Wl,-rpath,{lib_path}')\n        return args\n\n    def include_paths(self):\n        sources = [\n            'inference/v2/kernels/includes',\n            'inference/v2/kernels/ragged_ops',\n            'inference/v2/kernels/ragged_ops/atom_builder',\n            'inference/v2/kernels/ragged_ops/blocked_flash',\n            'inference/v2/kernels/ragged_ops/embed',\n            'inference/v2/kernels/ragged_ops/includes',\n            'inference/v2/kernels/ragged_ops/linear_blocked_kv_rotary',\n            'inference/v2/kernels/ragged_ops/logits_gather',\n            'inference/v2/kernels/ragged_ops/moe_gather',\n            'inference/v2/kernels/ragged_ops/moe_scatter',\n            'inference/v2/kernels/ragged_ops/ragged_helpers',\n            'inference/v2/kernels/ragged_ops/top_k_gating',\n        ]\n\n        prefix = self.get_prefix()\n        sources = [os.path.join(prefix, src) for src in sources]\n        return sources\n", "op_builder/async_io.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport distutils.spawn\nimport subprocess\n\nfrom .builder import OpBuilder\n\n\nclass AsyncIOBuilder(OpBuilder):\n    BUILD_VAR = \"DS_BUILD_AIO\"\n    NAME = \"async_io\"\n\n    def __init__(self):\n        super().__init__(name=self.NAME)\n\n    def absolute_name(self):\n        return f'deepspeed.ops.aio.{self.NAME}_op'\n\n    def sources(self):\n        return [\n            'csrc/aio/py_lib/deepspeed_py_copy.cpp', 'csrc/aio/py_lib/py_ds_aio.cpp',\n            'csrc/aio/py_lib/deepspeed_py_aio.cpp', 'csrc/aio/py_lib/deepspeed_py_aio_handle.cpp',\n            'csrc/aio/py_lib/deepspeed_aio_thread.cpp', 'csrc/aio/common/deepspeed_aio_utils.cpp',\n            'csrc/aio/common/deepspeed_aio_common.cpp', 'csrc/aio/common/deepspeed_aio_types.cpp',\n            'csrc/aio/py_lib/deepspeed_pin_tensor.cpp'\n        ]\n\n    def include_paths(self):\n        return ['csrc/aio/py_lib', 'csrc/aio/common']\n\n    def cxx_args(self):\n        # -O0 for improved debugging, since performance is bound by I/O\n        CPU_ARCH = self.cpu_arch()\n        SIMD_WIDTH = self.simd_width()\n        import torch  # Keep this import here to avoid errors when building DeepSpeed wheel without torch installed\n        TORCH_MAJOR, TORCH_MINOR = map(int, torch.__version__.split('.')[0:2])\n        if TORCH_MAJOR >= 2 and TORCH_MINOR >= 1:\n            CPP_STD = '-std=c++17'\n        else:\n            CPP_STD = '-std=c++14'\n        return [\n            '-g',\n            '-Wall',\n            '-O0',\n            CPP_STD,\n            '-shared',\n            '-fPIC',\n            '-Wno-reorder',\n            CPU_ARCH,\n            '-fopenmp',\n            SIMD_WIDTH,\n            '-laio',\n        ]\n\n    def extra_ldflags(self):\n        return ['-laio']\n\n    def check_for_libaio_pkg(self):\n        libs = dict(\n            dpkg=[\"-l\", \"libaio-dev\", \"apt\"],\n            pacman=[\"-Q\", \"libaio\", \"pacman\"],\n            rpm=[\"-q\", \"libaio-devel\", \"yum\"],\n        )\n\n        found = False\n        for pkgmgr, data in libs.items():\n            flag, lib, tool = data\n            path = distutils.spawn.find_executable(pkgmgr)\n            if path is not None:\n                cmd = f\"{pkgmgr} {flag} {lib}\"\n                result = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n                if result.wait() == 0:\n                    found = True\n                else:\n                    self.warning(f\"{self.NAME}: please install the {lib} package with {tool}\")\n                break\n        return found\n\n    def is_compatible(self, verbose=True):\n        # Check for the existence of libaio by using distutils\n        # to compile and link a test program that calls io_submit,\n        # which is a function provided by libaio that is used in the async_io op.\n        # If needed, one can define -I and -L entries in CFLAGS and LDFLAGS\n        # respectively to specify the directories for libaio.h and libaio.so.\n        aio_compatible = self.has_function('io_pgetevents', ('aio', ))\n        if verbose and not aio_compatible:\n            self.warning(f\"{self.NAME} requires the dev libaio .so object and headers but these were not found.\")\n\n            # Check for the libaio package via known package managers\n            # to print suggestions on which package to install.\n            self.check_for_libaio_pkg()\n\n            self.warning(\n                \"If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\"\n            )\n        return super().is_compatible(verbose) and aio_compatible\n", "op_builder/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport sys\nimport os\nimport pkgutil\nimport importlib\n\nfrom .builder import get_default_compute_capabilities, OpBuilder\n\n# Do not remove, required for abstract accelerator to detect if we have a deepspeed or 3p op_builder\n__deepspeed__ = True\n\n# List of all available op builders from deepspeed op_builder\ntry:\n    import deepspeed.ops.op_builder  # noqa: F401 # type: ignore\n    op_builder_dir = \"deepspeed.ops.op_builder\"\nexcept ImportError:\n    op_builder_dir = \"op_builder\"\n\n__op_builders__ = []\n\nthis_module = sys.modules[__name__]\n\n\ndef builder_closure(member_name):\n    if op_builder_dir == \"op_builder\":\n        # during installation time cannot get builder due to torch not installed,\n        # return closure instead\n        def _builder():\n            from deepspeed.accelerator import get_accelerator\n            builder = get_accelerator().create_op_builder(member_name)\n            return builder\n\n        return _builder\n    else:\n        # during runtime, return op builder class directly\n        from deepspeed.accelerator import get_accelerator\n        builder = get_accelerator().get_op_builder(member_name)\n        return builder\n\n\n# reflect builder names and add builder closure, such as 'TransformerBuilder()' creates op builder wrt current accelerator\nfor _, module_name, _ in pkgutil.iter_modules([os.path.dirname(this_module.__file__)]):\n    if module_name != 'all_ops' and module_name != 'builder':\n        module = importlib.import_module(f\".{module_name}\", package=op_builder_dir)\n        for member_name in module.__dir__():\n            if member_name.endswith('Builder') and member_name != \"OpBuilder\" and member_name != \"CUDAOpBuilder\":\n                # assign builder name to variable with same name\n                # the following is equivalent to i.e. TransformerBuilder = \"TransformerBuilder\"\n                this_module.__dict__[member_name] = builder_closure(member_name)\n", "op_builder/inference_cutlass_builder.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\nimport os\n\nfrom .builder import CUDAOpBuilder, installed_cuda_version\n\n\nclass InferenceCutlassBuilder(CUDAOpBuilder):\n    BUILD_VAR = \"DS_BUILD_CUTLASS_OPS\"\n    NAME = \"cutlass_ops\"\n\n    def __init__(self, name=None):\n        name = self.NAME if name is None else name\n        super().__init__(name=name)\n\n    def absolute_name(self):\n        return f'deepspeed.inference.v2.kernels.cutlass_ops.{self.NAME}'\n\n    def is_compatible(self, verbose=True):\n        try:\n            import torch\n        except ImportError:\n            self.warning(\"Please install torch if trying to pre-compile inference kernels\")\n            return False\n\n        cuda_okay = True\n        if not self.is_rocm_pytorch() and torch.cuda.is_available():  #ignore-cuda\n            sys_cuda_major, _ = installed_cuda_version()\n            torch_cuda_major = int(torch.version.cuda.split('.')[0])\n            cuda_capability = torch.cuda.get_device_properties(0).major  #ignore-cuda\n            if cuda_capability < 6:\n                self.warning(\"NVIDIA Inference is only supported on Pascal and newer architectures\")\n                cuda_okay = False\n            if cuda_capability >= 8:\n                if torch_cuda_major < 11 or sys_cuda_major < 11:\n                    self.warning(\"On Ampere and higher architectures please use CUDA 11+\")\n                    cuda_okay = False\n        return super().is_compatible(verbose) and cuda_okay\n\n    def filter_ccs(self, ccs):\n        ccs_retained = []\n        ccs_pruned = []\n        for cc in ccs:\n            if int(cc[0]) >= 8:\n                # Only support Ampere and newer\n                ccs_retained.append(cc)\n            else:\n                ccs_pruned.append(cc)\n        if len(ccs_pruned) > 0:\n            self.warning(f\"Filtered compute capabilities {ccs_pruned}\")\n        return ccs_retained\n\n    def get_prefix(self):\n        ds_path = self.deepspeed_src_path(\"deepspeed\")\n        return \"deepspeed\" if os.path.isdir(ds_path) else \"..\"\n\n    def sources(self):\n        sources = [\n            \"inference/v2/kernels/cutlass_ops/cutlass_ops.cpp\",\n            \"inference/v2/kernels/cutlass_ops/mixed_gemm/mixed_gemm.cu\",\n            \"inference/v2/kernels/cutlass_ops/moe_gemm/moe_gemm.cu\",\n        ]\n\n        prefix = self.get_prefix()\n        sources = [os.path.join(prefix, src) for src in sources]\n        return sources\n\n    def extra_ldflags(self):\n        import dskernels\n        lib_path = dskernels.library_path()\n        prefix = self.get_prefix()\n        lib_path = os.path.join(prefix, lib_path)\n        lib_path = self.deepspeed_src_path(lib_path)\n\n        args = [f'-L{lib_path}', '-ldeepspeedft']\n        if self.jit_load:\n            args.append(f'-Wl,-rpath,{lib_path}')\n        return args\n\n    def include_paths(self):\n        sources = [\n            'inference/v2/kernels/includes',\n            'inference/v2/kernels/cutlass_ops/mixed_gemm',\n            'inference/v2/kernels/cutlass_ops/moe_gemm',\n            'inference/v2/kernels/cutlass_ops/shared_resources/',\n        ]\n\n        prefix = self.get_prefix()\n        sources = [os.path.join(prefix, src) for src in sources]\n        return sources\n", "op_builder/cpu_adam.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .builder import TorchCPUOpBuilder\n\n\nclass CPUAdamBuilder(TorchCPUOpBuilder):\n    BUILD_VAR = \"DS_BUILD_CPU_ADAM\"\n    NAME = \"cpu_adam\"\n\n    def __init__(self):\n        super().__init__(name=self.NAME)\n\n    def absolute_name(self):\n        return f'deepspeed.ops.adam.{self.NAME}_op'\n\n    def sources(self):\n        return ['csrc/adam/cpu_adam.cpp', 'csrc/adam/cpu_adam_impl.cpp']\n\n    def libraries_args(self):\n        args = super().libraries_args()\n        return args\n\n    def include_paths(self):\n        return ['csrc/includes']\n", "op_builder/sparse_attn.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .builder import OpBuilder\n\ntry:\n    from packaging import version as pkg_version\nexcept ImportError:\n    pkg_version = None\n\n\nclass SparseAttnBuilder(OpBuilder):\n    BUILD_VAR = \"DS_BUILD_SPARSE_ATTN\"\n    NAME = \"sparse_attn\"\n\n    def __init__(self):\n        super().__init__(name=self.NAME)\n\n    def absolute_name(self):\n        return f'deepspeed.ops.sparse_attention.{self.NAME}_op'\n\n    def sources(self):\n        return ['csrc/sparse_attention/utils.cpp']\n\n    def cxx_args(self):\n        return ['-O2', '-fopenmp']\n\n    def is_compatible(self, verbose=True):\n        # Check to see if llvm and cmake are installed since they are dependencies\n        #required_commands = ['llvm-config|llvm-config-9', 'cmake']\n        #command_status = list(map(self.command_exists, required_commands))\n        #deps_compatible = all(command_status)\n\n        if self.is_rocm_pytorch():\n            self.warning(f'{self.NAME} is not compatible with ROCM')\n            return False\n\n        try:\n            import torch\n        except ImportError:\n            self.warning(f\"unable to import torch, please install it first\")\n            return False\n\n        # torch-cpu will not have a cuda version\n        if torch.version.cuda is None:\n            cuda_compatible = False\n            self.warning(f\"{self.NAME} cuda is not available from torch\")\n        else:\n            major, minor = torch.version.cuda.split('.')[:2]\n            cuda_compatible = (int(major) == 10 and int(minor) >= 1) or (int(major) >= 11)\n            if not cuda_compatible:\n                self.warning(f\"{self.NAME} requires CUDA version 10.1+\")\n\n        TORCH_MAJOR = int(torch.__version__.split('.')[0])\n        TORCH_MINOR = int(torch.__version__.split('.')[1])\n        torch_compatible = (TORCH_MAJOR == 1 and TORCH_MINOR >= 5)\n        if not torch_compatible:\n            self.warning(\n                f'{self.NAME} requires a torch version >= 1.5 and < 2.0 but detected {TORCH_MAJOR}.{TORCH_MINOR}')\n\n        try:\n            import triton\n        except ImportError:\n            # auto-install of triton is broken on some systems, reverting to manual install for now\n            # see this issue: https://github.com/microsoft/DeepSpeed/issues/1710\n            self.warning(f\"please install triton==1.0.0 if you want to use sparse attention\")\n            return False\n\n        if pkg_version:\n            installed_triton = pkg_version.parse(triton.__version__)\n            triton_mismatch = installed_triton != pkg_version.parse(\"1.0.0\")\n        else:\n            installed_triton = triton.__version__\n            triton_mismatch = installed_triton != \"1.0.0\"\n\n        if triton_mismatch:\n            self.warning(f\"using untested triton version ({installed_triton}), only 1.0.0 is known to be compatible\")\n            return False\n\n        return super().is_compatible(verbose) and torch_compatible and cuda_compatible\n", "op_builder/builder.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport os\nimport sys\nimport time\nimport importlib\nfrom pathlib import Path\nimport subprocess\nimport shlex\nimport shutil\nimport tempfile\nimport distutils.ccompiler\nimport distutils.log\nimport distutils.sysconfig\nfrom distutils.errors import CompileError, LinkError\nfrom abc import ABC, abstractmethod\nfrom typing import List\n\nYELLOW = '\\033[93m'\nEND = '\\033[0m'\nWARNING = f\"{YELLOW} [WARNING] {END}\"\n\nDEFAULT_TORCH_EXTENSION_PATH = \"/tmp/torch_extensions\"\nDEFAULT_COMPUTE_CAPABILITIES = \"6.0;6.1;7.0\"\n\ntry:\n    import torch\nexcept ImportError:\n    print(f\"{WARNING} unable to import torch, please install it if you want to pre-compile any deepspeed ops.\")\nelse:\n    TORCH_MAJOR = int(torch.__version__.split('.')[0])\n    TORCH_MINOR = int(torch.__version__.split('.')[1])\n\n\nclass MissingCUDAException(Exception):\n    pass\n\n\nclass CUDAMismatchException(Exception):\n    pass\n\n\ndef installed_cuda_version(name=\"\"):\n    import torch.utils.cpp_extension\n    cuda_home = torch.utils.cpp_extension.CUDA_HOME\n    if cuda_home is None:\n        raise MissingCUDAException(\"CUDA_HOME does not exist, unable to compile CUDA op(s)\")\n    # Ensure there is not a cuda version mismatch between torch and nvcc compiler\n    output = subprocess.check_output([cuda_home + \"/bin/nvcc\", \"-V\"], universal_newlines=True)\n    output_split = output.split()\n    release_idx = output_split.index(\"release\")\n    release = output_split[release_idx + 1].replace(',', '').split(\".\")\n    # Ignore patch versions, only look at major + minor\n    cuda_major, cuda_minor = release[:2]\n    return int(cuda_major), int(cuda_minor)\n\n\ndef get_default_compute_capabilities():\n    compute_caps = DEFAULT_COMPUTE_CAPABILITIES\n    import torch.utils.cpp_extension\n    if torch.utils.cpp_extension.CUDA_HOME is not None and installed_cuda_version()[0] >= 11:\n        if installed_cuda_version()[0] == 11 and installed_cuda_version()[1] == 0:\n            # Special treatment of CUDA 11.0 because compute_86 is not supported.\n            compute_caps += \";8.0\"\n        else:\n            compute_caps += \";8.0;8.6\"\n    return compute_caps\n\n\n# list compatible minor CUDA versions - so that for example pytorch built with cuda-11.0 can be used\n# to build deepspeed and system-wide installed cuda 11.2\ncuda_minor_mismatch_ok = {\n    10: [\"10.0\", \"10.1\", \"10.2\"],\n    11: [\"11.0\", \"11.1\", \"11.2\", \"11.3\", \"11.4\", \"11.5\", \"11.6\", \"11.7\", \"11.8\"],\n    12: [\"12.0\", \"12.1\", \"12.2\", \"12.3\", \"12.4\", \"12.5\"],\n}\n\n\ndef assert_no_cuda_mismatch(name=\"\"):\n    cuda_major, cuda_minor = installed_cuda_version(name)\n    sys_cuda_version = f'{cuda_major}.{cuda_minor}'\n    torch_cuda_version = \".\".join(torch.version.cuda.split('.')[:2])\n    # This is a show-stopping error, should probably not proceed past this\n    if sys_cuda_version != torch_cuda_version:\n        if (cuda_major in cuda_minor_mismatch_ok and sys_cuda_version in cuda_minor_mismatch_ok[cuda_major]\n                and torch_cuda_version in cuda_minor_mismatch_ok[cuda_major]):\n            print(f\"Installed CUDA version {sys_cuda_version} does not match the \"\n                  f\"version torch was compiled with {torch.version.cuda} \"\n                  \"but since the APIs are compatible, accepting this combination\")\n            return True\n        elif os.getenv(\"DS_SKIP_CUDA_CHECK\", \"0\") == \"1\":\n            print(\n                f\"{WARNING} DeepSpeed Op Builder: Installed CUDA version {sys_cuda_version} does not match the \"\n                f\"version torch was compiled with {torch.version.cuda}.\"\n                \"Detected `DS_SKIP_CUDA_CHECK=1`: Allowing this combination of CUDA, but it may result in unexpected behavior.\"\n            )\n            return True\n        raise CUDAMismatchException(\n            f\">- DeepSpeed Op Builder: Installed CUDA version {sys_cuda_version} does not match the \"\n            f\"version torch was compiled with {torch.version.cuda}, unable to compile \"\n            \"cuda/cpp extensions without a matching cuda version.\")\n    return True\n\n\nclass OpBuilder(ABC):\n    _rocm_version = None\n    _rocm_gpu_arch = None\n    _rocm_wavefront_size = None\n    _is_rocm_pytorch = None\n    _is_sycl_enabled = None\n    _loaded_ops = {}\n\n    def __init__(self, name):\n        self.name = name\n        self.jit_mode = False\n        self.build_for_cpu = False\n        self.enable_bf16 = False\n        self.error_log = None\n\n    @abstractmethod\n    def absolute_name(self):\n        '''\n        Returns absolute build path for cases where the op is pre-installed, e.g., deepspeed.ops.adam.cpu_adam\n        will be installed as something like: deepspeed/ops/adam/cpu_adam.so\n        '''\n        pass\n\n    @abstractmethod\n    def sources(self):\n        '''\n        Returns list of source files for your op, relative to root of deepspeed package (i.e., DeepSpeed/deepspeed)\n        '''\n        pass\n\n    def hipify_extension(self):\n        pass\n\n    def sycl_extension(self):\n        pass\n\n    @staticmethod\n    def validate_torch_version(torch_info):\n        install_torch_version = torch_info['version']\n        current_torch_version = \".\".join(torch.__version__.split('.')[:2])\n        if install_torch_version != current_torch_version:\n            raise RuntimeError(\"PyTorch version mismatch! DeepSpeed ops were compiled and installed \"\n                               \"with a different version than what is being used at runtime. \"\n                               f\"Please re-install DeepSpeed or switch torch versions. \"\n                               f\"Install torch version={install_torch_version}, \"\n                               f\"Runtime torch version={current_torch_version}\")\n\n    @staticmethod\n    def validate_torch_op_version(torch_info):\n        if not OpBuilder.is_rocm_pytorch():\n            current_cuda_version = \".\".join(torch.version.cuda.split('.')[:2])\n            install_cuda_version = torch_info['cuda_version']\n            if install_cuda_version != current_cuda_version:\n                raise RuntimeError(\"CUDA version mismatch! DeepSpeed ops were compiled and installed \"\n                                   \"with a different version than what is being used at runtime. \"\n                                   f\"Please re-install DeepSpeed or switch torch versions. \"\n                                   f\"Install CUDA version={install_cuda_version}, \"\n                                   f\"Runtime CUDA version={current_cuda_version}\")\n        else:\n            current_hip_version = \".\".join(torch.version.hip.split('.')[:2])\n            install_hip_version = torch_info['hip_version']\n            if install_hip_version != current_hip_version:\n                raise RuntimeError(\"HIP version mismatch! DeepSpeed ops were compiled and installed \"\n                                   \"with a different version than what is being used at runtime. \"\n                                   f\"Please re-install DeepSpeed or switch torch versions. \"\n                                   f\"Install HIP version={install_hip_version}, \"\n                                   f\"Runtime HIP version={current_hip_version}\")\n\n    @staticmethod\n    def is_rocm_pytorch():\n        if OpBuilder._is_rocm_pytorch is not None:\n            return OpBuilder._is_rocm_pytorch\n\n        _is_rocm_pytorch = False\n        try:\n            import torch\n        except ImportError:\n            pass\n        else:\n            if TORCH_MAJOR > 1 or (TORCH_MAJOR == 1 and TORCH_MINOR >= 5):\n                _is_rocm_pytorch = hasattr(torch.version, 'hip') and torch.version.hip is not None\n                if _is_rocm_pytorch:\n                    from torch.utils.cpp_extension import ROCM_HOME\n                    _is_rocm_pytorch = ROCM_HOME is not None\n        OpBuilder._is_rocm_pytorch = _is_rocm_pytorch\n        return OpBuilder._is_rocm_pytorch\n\n    @staticmethod\n    def is_sycl_enabled():\n        if OpBuilder._is_sycl_enabled is not None:\n            return OpBuilder._is_sycl_enabled\n\n        _is_sycl_enabled = False\n        try:\n            result = subprocess.run([\"c2s\", \"--version\"], capture_output=True)\n        except:\n            pass\n        else:\n            _is_sycl_enabled = True\n\n        OpBuilder._is_sycl_enabled = _is_sycl_enabled\n        return OpBuilder._is_sycl_enabled\n\n    @staticmethod\n    def installed_rocm_version():\n        if OpBuilder._rocm_version:\n            return OpBuilder._rocm_version\n\n        ROCM_MAJOR = '0'\n        ROCM_MINOR = '0'\n        if OpBuilder.is_rocm_pytorch():\n            from torch.utils.cpp_extension import ROCM_HOME\n            rocm_ver_file = Path(ROCM_HOME).joinpath(\".info/version-dev\")\n            if rocm_ver_file.is_file():\n                with open(rocm_ver_file, 'r') as file:\n                    ROCM_VERSION_DEV_RAW = file.read()\n            elif \"rocm\" in torch.__version__:\n                ROCM_VERSION_DEV_RAW = torch.__version__.split(\"rocm\")[1]\n            else:\n                assert False, \"Could not detect ROCm version\"\n            assert ROCM_VERSION_DEV_RAW != \"\", \"Could not detect ROCm version\"\n            ROCM_MAJOR = ROCM_VERSION_DEV_RAW.split('.')[0]\n            ROCM_MINOR = ROCM_VERSION_DEV_RAW.split('.')[1]\n        OpBuilder._rocm_version = (int(ROCM_MAJOR), int(ROCM_MINOR))\n        return OpBuilder._rocm_version\n\n    @staticmethod\n    def get_rocm_gpu_arch():\n        if OpBuilder._rocm_gpu_arch:\n            return OpBuilder._rocm_gpu_arch\n        rocm_gpu_arch_cmd = \"/opt/rocm/bin/rocminfo | grep -o -m 1 'gfx.*'\"\n        try:\n            result = subprocess.check_output(rocm_gpu_arch_cmd, shell=True)\n            rocm_gpu_arch = result.decode('utf-8').strip()\n        except subprocess.CalledProcessError:\n            rocm_gpu_arch = \"\"\n        OpBuilder._rocm_gpu_arch = rocm_gpu_arch\n        return OpBuilder._rocm_gpu_arch\n\n    @staticmethod\n    def get_rocm_wavefront_size():\n        if OpBuilder._rocm_wavefront_size:\n            return OpBuilder._rocm_wavefront_size\n        rocm_wavefront_size_cmd = \"/opt/rocm/bin/rocminfo | grep -Eo -m1 'Wavefront Size:[[:space:]]+[0-9]+' | grep -Eo '[0-9]+'\"\n        try:\n            result = subprocess.check_output(rocm_wavefront_size_cmd, shell=True)\n            rocm_wavefront_size = result.decode('utf-8').strip()\n        except subprocess.CalledProcessError:\n            rocm_wavefront_size = \"32\"\n        OpBuilder._rocm_wavefront_size = rocm_wavefront_size\n        return OpBuilder._rocm_wavefront_size\n\n    def include_paths(self):\n        '''\n        Returns list of include paths, relative to root of deepspeed package (i.e., DeepSpeed/deepspeed)\n        '''\n        return []\n\n    def nvcc_args(self):\n        '''\n        Returns optional list of compiler flags to forward to nvcc when building CUDA sources\n        '''\n        return []\n\n    def cxx_args(self):\n        '''\n        Returns optional list of compiler flags to forward to the build\n        '''\n        return []\n\n    def is_compatible(self, verbose=True):\n        '''\n        Check if all non-python dependencies are satisfied to build this op\n        '''\n        return True\n\n    def extra_ldflags(self):\n        return []\n\n    def has_function(self, funcname, libraries, verbose=False):\n        '''\n        Test for existence of a function within a tuple of libraries.\n\n        This is used as a smoke test to check whether a certain library is available.\n        As a test, this creates a simple C program that calls the specified function,\n        and then distutils is used to compile that program and link it with the specified libraries.\n        Returns True if both the compile and link are successful, False otherwise.\n        '''\n        tempdir = None  # we create a temporary directory to hold various files\n        filestderr = None  # handle to open file to which we redirect stderr\n        oldstderr = None  # file descriptor for stderr\n        try:\n            # Echo compile and link commands that are used.\n            if verbose:\n                distutils.log.set_verbosity(1)\n\n            # Create a compiler object.\n            compiler = distutils.ccompiler.new_compiler(verbose=verbose)\n\n            # Configure compiler and linker to build according to Python install.\n            distutils.sysconfig.customize_compiler(compiler)\n\n            # Create a temporary directory to hold test files.\n            tempdir = tempfile.mkdtemp()\n\n            # Define a simple C program that calls the function in question\n            prog = \"void %s(void); int main(int argc, char** argv) { %s(); return 0; }\" % (funcname, funcname)\n\n            # Write the test program to a file.\n            filename = os.path.join(tempdir, 'test.c')\n            with open(filename, 'w') as f:\n                f.write(prog)\n\n            # Redirect stderr file descriptor to a file to silence compile/link warnings.\n            if not verbose:\n                filestderr = open(os.path.join(tempdir, 'stderr.txt'), 'w')\n                oldstderr = os.dup(sys.stderr.fileno())\n                os.dup2(filestderr.fileno(), sys.stderr.fileno())\n\n            # Workaround for behavior in distutils.ccompiler.CCompiler.object_filenames()\n            # Otherwise, a local directory will be used instead of tempdir\n            drive, driveless_filename = os.path.splitdrive(filename)\n            root_dir = driveless_filename[0] if os.path.isabs(driveless_filename) else ''\n            output_dir = os.path.join(drive, root_dir)\n\n            # Attempt to compile the C program into an object file.\n            cflags = shlex.split(os.environ.get('CFLAGS', \"\"))\n            objs = compiler.compile([filename], output_dir=output_dir, extra_preargs=self.strip_empty_entries(cflags))\n\n            # Attempt to link the object file into an executable.\n            # Be sure to tack on any libraries that have been specified.\n            ldflags = shlex.split(os.environ.get('LDFLAGS', \"\"))\n            compiler.link_executable(objs,\n                                     os.path.join(tempdir, 'a.out'),\n                                     extra_preargs=self.strip_empty_entries(ldflags),\n                                     libraries=libraries)\n\n            # Compile and link succeeded\n            return True\n\n        except CompileError:\n            return False\n\n        except LinkError:\n            return False\n\n        except:\n            return False\n\n        finally:\n            # Restore stderr file descriptor and close the stderr redirect file.\n            if oldstderr is not None:\n                os.dup2(oldstderr, sys.stderr.fileno())\n            if filestderr is not None:\n                filestderr.close()\n\n            # Delete the temporary directory holding the test program and stderr files.\n            if tempdir is not None:\n                shutil.rmtree(tempdir)\n\n    def strip_empty_entries(self, args):\n        '''\n        Drop any empty strings from the list of compile and link flags\n        '''\n        return [x for x in args if len(x) > 0]\n\n    def cpu_arch(self):\n        try:\n            from cpuinfo import get_cpu_info\n        except ImportError as e:\n            cpu_info = self._backup_cpuinfo()\n            if cpu_info is None:\n                return \"-march=native\"\n\n        try:\n            cpu_info = get_cpu_info()\n        except Exception as e:\n            self.warning(f\"{self.name} attempted to use `py-cpuinfo` but failed (exception type: {type(e)}, {e}), \"\n                         \"falling back to `lscpu` to get this information.\")\n            cpu_info = self._backup_cpuinfo()\n            if cpu_info is None:\n                return \"-march=native\"\n\n        if cpu_info['arch'].startswith('PPC_'):\n            # gcc does not provide -march on PowerPC, use -mcpu instead\n            return '-mcpu=native'\n        return '-march=native'\n\n    def is_cuda_enable(self):\n        try:\n            assert_no_cuda_mismatch(self.name)\n            return '-D__ENABLE_CUDA__'\n        except MissingCUDAException:\n            print(f\"{WARNING} {self.name} cuda is missing or is incompatible with installed torch, \"\n                  \"only cpu ops can be compiled!\")\n            return '-D__DISABLE_CUDA__'\n        return '-D__DISABLE_CUDA__'\n\n    def _backup_cpuinfo(self):\n        # Construct cpu_info dict from lscpu that is similar to what py-cpuinfo provides\n        if not self.command_exists('lscpu'):\n            self.warning(f\"{self.name} attempted to query 'lscpu' after failing to use py-cpuinfo \"\n                         \"to detect the CPU architecture. 'lscpu' does not appear to exist on \"\n                         \"your system, will fall back to use -march=native and non-vectorized execution.\")\n            return None\n        result = subprocess.check_output('lscpu', shell=True)\n        result = result.decode('utf-8').strip().lower()\n\n        cpu_info = {}\n        cpu_info['arch'] = None\n        cpu_info['flags'] = \"\"\n        if 'genuineintel' in result or 'authenticamd' in result:\n            cpu_info['arch'] = 'X86_64'\n            if 'avx512' in result:\n                cpu_info['flags'] += 'avx512,'\n            elif 'avx512f' in result:\n                cpu_info['flags'] += 'avx512f,'\n            if 'avx2' in result:\n                cpu_info['flags'] += 'avx2'\n        elif 'ppc64le' in result:\n            cpu_info['arch'] = \"PPC_\"\n\n        return cpu_info\n\n    def simd_width(self):\n        try:\n            from cpuinfo import get_cpu_info\n        except ImportError as e:\n            cpu_info = self._backup_cpuinfo()\n            if cpu_info is None:\n                return '-D__SCALAR__'\n\n        try:\n            cpu_info = get_cpu_info()\n        except Exception as e:\n            self.warning(f\"{self.name} attempted to use `py-cpuinfo` but failed (exception type: {type(e)}, {e}), \"\n                         \"falling back to `lscpu` to get this information.\")\n            cpu_info = self._backup_cpuinfo()\n            if cpu_info is None:\n                return '-D__SCALAR__'\n\n        if cpu_info['arch'] == 'X86_64':\n            if 'avx512' in cpu_info['flags'] or 'avx512f' in cpu_info['flags']:\n                return '-D__AVX512__'\n            elif 'avx2' in cpu_info['flags']:\n                return '-D__AVX256__'\n        return '-D__SCALAR__'\n\n    def command_exists(self, cmd):\n        if '|' in cmd:\n            cmds = cmd.split(\"|\")\n        else:\n            cmds = [cmd]\n        valid = False\n        for cmd in cmds:\n            result = subprocess.Popen(f'type {cmd}', stdout=subprocess.PIPE, shell=True)\n            valid = valid or result.wait() == 0\n\n        if not valid and len(cmds) > 1:\n            print(f\"{WARNING} {self.name} requires one of the following commands '{cmds}', but it does not exist!\")\n        elif not valid and len(cmds) == 1:\n            print(f\"{WARNING} {self.name} requires the '{cmd}' command, but it does not exist!\")\n        return valid\n\n    def warning(self, msg):\n        self.error_log = f\"{msg}\"\n        print(f\"{WARNING} {msg}\")\n\n    def deepspeed_src_path(self, code_path):\n        if os.path.isabs(code_path):\n            return code_path\n        else:\n            return os.path.join(Path(__file__).parent.parent.absolute(), code_path)\n\n    def builder(self):\n        from torch.utils.cpp_extension import CppExtension\n        include_dirs = [os.path.abspath(x) for x in self.strip_empty_entries(self.include_paths())]\n        return CppExtension(name=self.absolute_name(),\n                            sources=self.strip_empty_entries(self.sources()),\n                            include_dirs=include_dirs,\n                            extra_compile_args={'cxx': self.strip_empty_entries(self.cxx_args())},\n                            extra_link_args=self.strip_empty_entries(self.extra_ldflags()))\n\n    def load(self, verbose=True):\n        if self.name in __class__._loaded_ops:\n            return __class__._loaded_ops[self.name]\n\n        from deepspeed.git_version_info import installed_ops, torch_info, accelerator_name\n        from deepspeed.accelerator import get_accelerator\n        if installed_ops.get(self.name, False) and accelerator_name == get_accelerator()._name:\n            # Ensure the op we're about to load was compiled with the same\n            # torch/cuda versions we are currently using at runtime.\n            self.validate_torch_version(torch_info)\n            if torch.cuda.is_available() and isinstance(self, CUDAOpBuilder):\n                self.validate_torch_op_version(torch_info)\n\n            op_module = importlib.import_module(self.absolute_name())\n            __class__._loaded_ops[self.name] = op_module\n            return op_module\n        else:\n            return self.jit_load(verbose)\n\n    def jit_load(self, verbose=True):\n        if not self.is_compatible(verbose):\n            raise RuntimeError(\n                f\"Unable to JIT load the {self.name} op due to it not being compatible due to hardware/software issue. {self.error_log}\"\n            )\n        try:\n            import ninja  # noqa: F401 # type: ignore\n        except ImportError:\n            raise RuntimeError(f\"Unable to JIT load the {self.name} op due to ninja not being installed.\")\n\n        if isinstance(self, CUDAOpBuilder) and not self.is_rocm_pytorch():\n            self.build_for_cpu = not torch.cuda.is_available()\n\n        self.jit_mode = True\n        from torch.utils.cpp_extension import load\n\n        start_build = time.time()\n        sources = [os.path.abspath(self.deepspeed_src_path(path)) for path in self.sources()]\n        extra_include_paths = [os.path.abspath(self.deepspeed_src_path(path)) for path in self.include_paths()]\n\n        # Torch will try and apply whatever CCs are in the arch list at compile time,\n        # we have already set the intended targets ourselves we know that will be\n        # needed at runtime. This prevents CC collisions such as multiple __half\n        # implementations. Stash arch list to reset after build.\n        torch_arch_list = None\n        if \"TORCH_CUDA_ARCH_LIST\" in os.environ:\n            torch_arch_list = os.environ.get(\"TORCH_CUDA_ARCH_LIST\")\n            os.environ[\"TORCH_CUDA_ARCH_LIST\"] = \"\"\n\n        nvcc_args = self.strip_empty_entries(self.nvcc_args())\n        cxx_args = self.strip_empty_entries(self.cxx_args())\n\n        if isinstance(self, CUDAOpBuilder):\n            if not self.build_for_cpu and self.enable_bf16:\n                cxx_args.append(\"-DBF16_AVAILABLE\")\n                nvcc_args.append(\"-DBF16_AVAILABLE\")\n                nvcc_args.append(\"-U__CUDA_NO_BFLOAT16_OPERATORS__\")\n                nvcc_args.append(\"-U__CUDA_NO_BFLOAT162_OPERATORS__\")\n                nvcc_args.append(\"-U__CUDA_NO_BFLOAT16_CONVERSIONS__\")\n\n        if self.is_rocm_pytorch():\n            cxx_args.append(\"-D__HIP_PLATFORM_AMD__=1\")\n            os.environ[\"PYTORCH_ROCM_ARCH\"] = self.get_rocm_gpu_arch()\n            cxx_args.append('-DROCM_WAVEFRONT_SIZE=%s' % self.get_rocm_wavefront_size())\n\n        op_module = load(name=self.name,\n                         sources=self.strip_empty_entries(sources),\n                         extra_include_paths=self.strip_empty_entries(extra_include_paths),\n                         extra_cflags=cxx_args,\n                         extra_cuda_cflags=nvcc_args,\n                         extra_ldflags=self.strip_empty_entries(self.extra_ldflags()),\n                         verbose=verbose)\n\n        build_duration = time.time() - start_build\n        if verbose:\n            print(f\"Time to load {self.name} op: {build_duration} seconds\")\n\n        # Reset arch list so we are not silently removing it for other possible use cases\n        if torch_arch_list:\n            os.environ[\"TORCH_CUDA_ARCH_LIST\"] = torch_arch_list\n\n        __class__._loaded_ops[self.name] = op_module\n\n        return op_module\n\n\nclass CUDAOpBuilder(OpBuilder):\n\n    def compute_capability_args(self, cross_compile_archs=None):\n        \"\"\"\n        Returns nvcc compute capability compile flags.\n\n        1. `TORCH_CUDA_ARCH_LIST` takes priority over `cross_compile_archs`.\n        2. If neither is set default compute capabilities will be used\n        3. Under `jit_mode` compute capabilities of all visible cards will be used plus PTX\n\n        Format:\n\n        - `TORCH_CUDA_ARCH_LIST` may use ; or whitespace separators. Examples:\n\n        TORCH_CUDA_ARCH_LIST=\"6.1;7.5;8.6\" pip install ...\n        TORCH_CUDA_ARCH_LIST=\"6.0 6.1 7.0 7.5 8.0 8.6+PTX\" pip install ...\n\n        - `cross_compile_archs` uses ; separator.\n\n        \"\"\"\n        ccs = []\n        if self.jit_mode:\n            # Compile for underlying architectures since we know those at runtime\n            for i in range(torch.cuda.device_count()):\n                CC_MAJOR, CC_MINOR = torch.cuda.get_device_capability(i)\n                cc = f\"{CC_MAJOR}.{CC_MINOR}\"\n                if cc not in ccs:\n                    ccs.append(cc)\n            ccs = sorted(ccs)\n            ccs[-1] += '+PTX'\n        else:\n            # Cross-compile mode, compile for various architectures\n            # env override takes priority\n            cross_compile_archs_env = os.environ.get('TORCH_CUDA_ARCH_LIST', None)\n            if cross_compile_archs_env is not None:\n                if cross_compile_archs is not None:\n                    print(\n                        f\"{WARNING} env var `TORCH_CUDA_ARCH_LIST={cross_compile_archs_env}` overrides `cross_compile_archs={cross_compile_archs}`\"\n                    )\n                cross_compile_archs = cross_compile_archs_env.replace(' ', ';')\n            else:\n                if cross_compile_archs is None:\n                    cross_compile_archs = get_default_compute_capabilities()\n            ccs = cross_compile_archs.split(';')\n\n        ccs = self.filter_ccs(ccs)\n        if len(ccs) == 0:\n            raise RuntimeError(\n                f\"Unable to load {self.name} op due to no compute capabilities remaining after filtering\")\n\n        args = []\n        self.enable_bf16 = True\n        for cc in ccs:\n            num = cc[0] + cc[2]\n            args.append(f'-gencode=arch=compute_{num},code=sm_{num}')\n            if cc.endswith('+PTX'):\n                args.append(f'-gencode=arch=compute_{num},code=compute_{num}')\n\n            if int(cc[0]) <= 7:\n                self.enable_bf16 = False\n\n        return args\n\n    def filter_ccs(self, ccs: List[str]):\n        \"\"\"\n        Prune any compute capabilities that are not compatible with the builder. Should log\n        which CCs have been pruned.\n        \"\"\"\n        return ccs\n\n    def version_dependent_macros(self):\n        # Fix from apex that might be relevant for us as well, related to https://github.com/NVIDIA/apex/issues/456\n        version_ge_1_1 = []\n        if (TORCH_MAJOR > 1) or (TORCH_MAJOR == 1 and TORCH_MINOR > 0):\n            version_ge_1_1 = ['-DVERSION_GE_1_1']\n        version_ge_1_3 = []\n        if (TORCH_MAJOR > 1) or (TORCH_MAJOR == 1 and TORCH_MINOR > 2):\n            version_ge_1_3 = ['-DVERSION_GE_1_3']\n        version_ge_1_5 = []\n        if (TORCH_MAJOR > 1) or (TORCH_MAJOR == 1 and TORCH_MINOR > 4):\n            version_ge_1_5 = ['-DVERSION_GE_1_5']\n        return version_ge_1_1 + version_ge_1_3 + version_ge_1_5\n\n    def is_compatible(self, verbose=True):\n        return super().is_compatible(verbose)\n\n    def builder(self):\n        try:\n            if not self.is_rocm_pytorch():\n                assert_no_cuda_mismatch(self.name)\n            self.build_for_cpu = False\n        except MissingCUDAException:\n            self.build_for_cpu = True\n\n        if self.build_for_cpu:\n            from torch.utils.cpp_extension import CppExtension as ExtensionBuilder\n        else:\n            from torch.utils.cpp_extension import CUDAExtension as ExtensionBuilder\n        include_dirs = [os.path.abspath(x) for x in self.strip_empty_entries(self.include_paths())]\n        compile_args = {'cxx': self.strip_empty_entries(self.cxx_args())} if self.build_for_cpu else \\\n                       {'cxx': self.strip_empty_entries(self.cxx_args()), \\\n                        'nvcc': self.strip_empty_entries(self.nvcc_args())}\n\n        if not self.build_for_cpu and self.enable_bf16:\n            compile_args['cxx'].append(\"-DBF16_AVAILABLE\")\n            compile_args['nvcc'].append(\"-DBF16_AVAILABLE\")\n\n        if self.is_rocm_pytorch():\n            compile_args['cxx'].append(\"-D__HIP_PLATFORM_AMD__=1\")\n            #cxx compiler args are required to compile cpp files\n            compile_args['cxx'].append('-DROCM_WAVEFRONT_SIZE=%s' % self.get_rocm_wavefront_size())\n            #nvcc compiler args are required to compile hip files\n            compile_args['nvcc'].append('-DROCM_WAVEFRONT_SIZE=%s' % self.get_rocm_wavefront_size())\n            if self.get_rocm_gpu_arch():\n                os.environ[\"PYTORCH_ROCM_ARCH\"] = self.get_rocm_gpu_arch()\n\n        cuda_ext = ExtensionBuilder(name=self.absolute_name(),\n                                    sources=self.strip_empty_entries(self.sources()),\n                                    include_dirs=include_dirs,\n                                    libraries=self.strip_empty_entries(self.libraries_args()),\n                                    extra_compile_args=compile_args,\n                                    extra_link_args=self.strip_empty_entries(self.extra_ldflags()))\n\n        if self.is_rocm_pytorch():\n            # hip converts paths to absolute, this converts back to relative\n            sources = cuda_ext.sources\n            curr_file = Path(__file__).parent.parent  # ds root\n            for i in range(len(sources)):\n                src = Path(sources[i])\n                if src.is_absolute():\n                    sources[i] = str(src.relative_to(curr_file))\n                else:\n                    sources[i] = str(src)\n            cuda_ext.sources = sources\n        return cuda_ext\n\n    def hipify_extension(self):\n        if self.is_rocm_pytorch():\n            from torch.utils.hipify import hipify_python\n            hipify_python.hipify(\n                project_directory=os.getcwd(),\n                output_directory=os.getcwd(),\n                header_include_dirs=self.include_paths(),\n                includes=[os.path.join(os.getcwd(), '*')],\n                extra_files=[os.path.abspath(s) for s in self.sources()],\n                show_detailed=True,\n                is_pytorch_extension=True,\n                hipify_extra_files_only=True,\n            )\n\n    def cxx_args(self):\n        if sys.platform == \"win32\":\n            return ['-O2']\n        else:\n            return ['-O3', '-std=c++17', '-g', '-Wno-reorder']\n\n    def nvcc_args(self):\n        if self.build_for_cpu:\n            return []\n        args = ['-O3']\n        if self.is_rocm_pytorch():\n            ROCM_MAJOR, ROCM_MINOR = self.installed_rocm_version()\n            args += [\n                '-std=c++17', '-U__HIP_NO_HALF_OPERATORS__', '-U__HIP_NO_HALF_CONVERSIONS__',\n                '-U__HIP_NO_HALF2_OPERATORS__',\n                '-DROCM_VERSION_MAJOR=%s' % ROCM_MAJOR,\n                '-DROCM_VERSION_MINOR=%s' % ROCM_MINOR\n            ]\n        else:\n            try:\n                nvcc_threads = int(os.getenv(\"DS_NVCC_THREADS\", \"\"))\n                if nvcc_threads <= 0:\n                    raise ValueError(\"\")\n            except ValueError:\n                nvcc_threads = min(os.cpu_count(), 8)\n\n            cuda_major, _ = installed_cuda_version()\n            args += [\n                '-allow-unsupported-compiler' if sys.platform == \"win32\" else '', '--use_fast_math',\n                '-std=c++17' if cuda_major > 10 else '-std=c++14', '-U__CUDA_NO_HALF_OPERATORS__',\n                '-U__CUDA_NO_HALF_CONVERSIONS__', '-U__CUDA_NO_HALF2_OPERATORS__', f'--threads={nvcc_threads}'\n            ]\n            if os.environ.get('DS_DEBUG_CUDA_BUILD', '0') == '1':\n                args.append('--ptxas-options=-v')\n            args += self.compute_capability_args()\n        return args\n\n    def libraries_args(self):\n        if self.build_for_cpu:\n            return []\n\n        if sys.platform == \"win32\":\n            return ['cublas', 'curand']\n        else:\n            return []\n\n\nclass TorchCPUOpBuilder(CUDAOpBuilder):\n\n    def extra_ldflags(self):\n        if self.build_for_cpu:\n            return ['-fopenmp']\n\n        if not self.is_rocm_pytorch():\n            return ['-lcurand']\n\n        return []\n\n    def cxx_args(self):\n        import torch\n        args = []\n        if not self.build_for_cpu:\n            if not self.is_rocm_pytorch():\n                CUDA_LIB64 = os.path.join(torch.utils.cpp_extension.CUDA_HOME, \"lib64\")\n                if not os.path.exists(CUDA_LIB64):\n                    CUDA_LIB64 = os.path.join(torch.utils.cpp_extension.CUDA_HOME, \"lib\")\n            else:\n                CUDA_LIB64 = os.path.join(torch.utils.cpp_extension.ROCM_HOME, \"lib\")\n\n            args += super().cxx_args()\n            args += [\n                f'-L{CUDA_LIB64}',\n                '-lcudart',\n                '-lcublas',\n                '-g',\n            ]\n\n        CPU_ARCH = self.cpu_arch()\n        SIMD_WIDTH = self.simd_width()\n        CUDA_ENABLE = self.is_cuda_enable()\n        args += [\n            CPU_ARCH,\n            '-fopenmp',\n            SIMD_WIDTH,\n            CUDA_ENABLE,\n        ]\n\n        return args\n", "op_builder/cpu_adagrad.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .builder import TorchCPUOpBuilder\n\n\nclass CPUAdagradBuilder(TorchCPUOpBuilder):\n    BUILD_VAR = \"DS_BUILD_CPU_ADAGRAD\"\n    NAME = \"cpu_adagrad\"\n\n    def __init__(self):\n        super().__init__(name=self.NAME)\n\n    def absolute_name(self):\n        return f'deepspeed.ops.adagrad.{self.NAME}_op'\n\n    def sources(self):\n        return ['csrc/adagrad/cpu_adagrad.cpp']\n\n    def libraries_args(self):\n        args = super().libraries_args()\n        return args\n\n    def include_paths(self):\n        return ['csrc/includes']\n", "op_builder/xpu/no_impl.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .builder import SYCLOpBuilder\n\n\nclass NotImplementedBuilder(SYCLOpBuilder):\n    BUILD_VAR = \"DS_BUILD_NOT_IMPLEMENTED\"\n    NAME = \"deepspeed_not_implemented\"\n\n    def __init__(self, name=None):\n        name = self.NAME if name is None else name\n        super().__init__(name=name)\n\n    def absolute_name(self):\n        return f'deepspeed.ops.{self.NAME}_op'\n\n    def load(self, verbose=True):\n        raise ValueError(\"This op had not been implemented on XPU backend.\")\n\n    def sources(self):\n        return []\n\n    def cxx_args(self):\n        return []\n\n    def extra_ldflags(self):\n        return []\n\n    def include_paths(self):\n        return []\n", "op_builder/xpu/flash_attn.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\nfrom .builder import SYCLOpBuilder\n\n\nclass FlashAttentionBuilderObject():\n\n    def __init__(self):\n        pass\n\n    # general functions\n    def flash_attn_func_v2(self, q, k, v, dropout_p, softmax_scale, is_causal):\n        try:\n            import torch\n            import intel_extension_for_pytorch  # noqa\n            return torch.nn.functional.scaled_dot_product_attention(q,\n                                                                    k,\n                                                                    v,\n                                                                    dropout_p=dropout_p,\n                                                                    is_causal=is_causal,\n                                                                    scale=softmax_scale)\n        except ImportError:\n            raise ImportError(\n                \"Please install pytorch and intel_extension_for_pytorch to include scaled dot product attention.\")\n\n\nclass FlashAttentionBuilder(SYCLOpBuilder):\n    BUILD_VAR = \"DS_BUILD_FlashAttention\"\n    NAME = \"flash_attn\"\n\n    def __init__(self, name=None):\n        name = self.NAME if name is None else name\n        super().__init__(name=name)\n\n    def absolute_name(self):\n        return f'deepspeed.ops.{self.NAME}_op'\n\n    def sources(self):\n        return\n\n    def include_paths(self):\n        return []\n\n    def extra_ldflags(self):\n        return []\n\n    def cxx_args(self):\n        return []\n\n    def load(self):\n        return FlashAttentionBuilderObject()\n", "op_builder/xpu/packbits.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\nfrom .builder import SYCLOpBuilder\n\n\nclass PackbitsBuilder(SYCLOpBuilder):\n    BUILD_VAR = \"DS_BUILD_PACK_BITS\"\n    NAME = \"pack_bits\"\n\n    def __init__(self):\n        super().__init__(name=self.NAME)\n\n    def absolute_name(self):\n        return f'deepspeed.ops.{self.NAME}_op'\n\n    def sources(self):\n        return ['csrc/xpu/packbits/packing.cpp']\n\n    def include_paths(self):\n        return ['csrc/xpu/includes']\n\n    def cxx_args(self):\n        args = super().cxx_args()\n        return args + self.version_dependent_macros()\n", "op_builder/xpu/fused_adam.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\nfrom .builder import SYCLOpBuilder\n\n\nclass FusedAdamBuilder(SYCLOpBuilder):\n    BUILD_VAR = \"DS_BUILD_FUSED_ADAM\"\n    NAME = \"fused_adam\"\n\n    def __init__(self):\n        super().__init__(name=self.NAME)\n\n    def absolute_name(self):\n        return f'deepspeed.ops.adam.{self.NAME}_op'\n\n    def sources(self):\n        return ['csrc/xpu/adam/fused_adam_frontend.cpp', 'csrc/xpu/adam/multi_tensor_adam.dp.cpp']\n\n    def include_paths(self):\n        return ['csrc/xpu/includes', 'csrc/xpu/adam']\n\n    def cxx_args(self):\n        args = super().cxx_args()\n        return args + self.version_dependent_macros()\n", "op_builder/xpu/async_io.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport distutils.spawn\nimport subprocess\n\nfrom .builder import OpBuilder\n\n\nclass AsyncIOBuilder(OpBuilder):\n    BUILD_VAR = \"DS_BUILD_AIO\"\n    NAME = \"async_io\"\n\n    def __init__(self):\n        super().__init__(name=self.NAME)\n\n    def absolute_name(self):\n        return f'deepspeed.ops.aio.{self.NAME}_op'\n\n    def sources(self):\n        return [\n            'csrc/aio/py_lib/deepspeed_py_copy.cpp', 'csrc/aio/py_lib/py_ds_aio.cpp',\n            'csrc/aio/py_lib/deepspeed_py_aio.cpp', 'csrc/aio/py_lib/deepspeed_py_aio_handle.cpp',\n            'csrc/aio/py_lib/deepspeed_aio_thread.cpp', 'csrc/aio/common/deepspeed_aio_utils.cpp',\n            'csrc/aio/common/deepspeed_aio_common.cpp', 'csrc/aio/common/deepspeed_aio_types.cpp',\n            'csrc/aio/py_lib/deepspeed_pin_tensor.cpp'\n        ]\n\n    def include_paths(self):\n        return ['csrc/aio/py_lib', 'csrc/aio/common']\n\n    def cxx_args(self):\n        import torch\n        # -O0 for improved debugging, since performance is bound by I/O\n        CPU_ARCH = self.cpu_arch()\n        SIMD_WIDTH = self.simd_width()\n        TORCH_MAJOR, TORCH_MINOR = map(int, torch.__version__.split('.')[0:2])\n        if TORCH_MAJOR >= 2 and TORCH_MINOR >= 1:\n            CPP_STD = '-std=c++17'\n        else:\n            CPP_STD = '-std=c++14'\n        return [\n            '-g',\n            '-Wall',\n            '-O0',\n            CPP_STD,\n            '-shared',\n            '-fPIC',\n            '-Wno-reorder',\n            CPU_ARCH,\n            '-fopenmp',\n            SIMD_WIDTH,\n            '-laio',\n        ]\n\n    def extra_ldflags(self):\n        return ['-laio']\n\n    def check_for_libaio_pkg(self):\n        libs = dict(\n            dpkg=[\"-l\", \"libaio-dev\", \"apt\"],\n            pacman=[\"-Q\", \"libaio\", \"pacman\"],\n            rpm=[\"-q\", \"libaio-devel\", \"yum\"],\n        )\n\n        found = False\n        for pkgmgr, data in libs.items():\n            flag, lib, tool = data\n            path = distutils.spawn.find_executable(pkgmgr)\n            if path is not None:\n                cmd = f\"{pkgmgr} {flag} {lib}\"\n                result = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n                if result.wait() == 0:\n                    found = True\n                else:\n                    self.warning(f\"{self.NAME}: please install the {lib} package with {tool}\")\n                break\n        return found\n\n    def is_compatible(self, verbose=True):\n        # Check for the existence of libaio by using distutils\n        # to compile and link a test program that calls io_submit,\n        # which is a function provided by libaio that is used in the async_io op.\n        # If needed, one can define -I and -L entries in CFLAGS and LDFLAGS\n        # respectively to specify the directories for libaio.h and libaio.so.\n        aio_compatible = self.has_function('io_pgetevents', ('aio', ))\n        if verbose and not aio_compatible:\n            self.warning(f\"{self.NAME} requires the dev libaio .so object and headers but these were not found.\")\n\n            # Check for the libaio package via known package managers\n            # to print suggestions on which package to install.\n            self.check_for_libaio_pkg()\n\n            self.warning(\n                \"If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\"\n            )\n        return super().is_compatible(verbose) and aio_compatible\n", "op_builder/xpu/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .cpu_adam import CPUAdamBuilder\nfrom .cpu_adagrad import CPUAdagradBuilder\nfrom .fused_adam import FusedAdamBuilder\nfrom .async_io import AsyncIOBuilder\nfrom .inference import InferenceBuilder\nfrom .flash_attn import FlashAttentionBuilder\nfrom .no_impl import NotImplementedBuilder\nfrom .packbits import PackbitsBuilder\n", "op_builder/xpu/cpu_adam.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .builder import SYCLOpBuilder\n\n\nclass CPUAdamBuilder(SYCLOpBuilder):\n    BUILD_VAR = \"DS_BUILD_CPU_ADAM\"\n    NAME = \"cpu_adam\"\n\n    def __init__(self):\n        super().__init__(name=self.NAME)\n\n    def absolute_name(self):\n        return f'deepspeed.ops.adam.{self.NAME}_op'\n\n    def sources(self):\n        if self.build_for_cpu:\n            return ['csrc/xpu/adam/cpu_adam.cpp', 'csrc/xpu/adam/cpu_adam_impl.cpp']\n\n        return [\n            'csrc/xpu/adam/cpu_adam.cpp', 'csrc/xpu/adam/cpu_adam_impl.cpp',\n            'csrc/xpu/common/custom_cuda_kernel.dp.cpp'\n        ]\n\n    def include_paths(self):\n        return ['csrc/xpu/includes']\n", "op_builder/xpu/builder.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport os\nimport time\nimport importlib\n\ntry:\n    # is op_builder from deepspeed or a 3p version? this should only succeed if it's deepspeed\n    # if successful this also means we're doing a local install and not JIT compile path\n    from op_builder import __deepspeed__  # noqa: F401 # type: ignore\n    from op_builder.builder import OpBuilder\nexcept ImportError:\n    from deepspeed.ops.op_builder.builder import OpBuilder\n\n\nclass SYCLOpBuilder(OpBuilder):\n\n    def builder(self):\n        try:\n            from intel_extension_for_pytorch.xpu.cpp_extension import DPCPPExtension\n        except ImportError:\n            from intel_extension_for_pytorch.xpu.utils import DPCPPExtension\n        include_dirs = [os.path.abspath(x) for x in self.strip_empty_entries(self.include_paths())]\n        print(\"dpcpp sources = {}\".format(self.sources()))\n        dpcpp_ext = DPCPPExtension(name=self.absolute_name(),\n                                   sources=self.strip_empty_entries(self.sources()),\n                                   include_dirs=include_dirs,\n                                   extra_compile_args={\n                                       'cxx': self.strip_empty_entries(self.cxx_args()),\n                                   },\n                                   extra_link_args=self.strip_empty_entries(self.fixed_aotflags()))\n        return dpcpp_ext\n\n    def version_dependent_macros(self):\n        try:\n            from op_builder.builder import TORCH_MAJOR, TORCH_MINOR\n        except ImportError:\n            from deepspeed.ops.op_builder.builder import TORCH_MAJOR, TORCH_MINOR\n        # Fix from apex that might be relevant for us as well, related to https://github.com/NVIDIA/apex/issues/456\n        version_ge_1_1 = []\n        if (TORCH_MAJOR > 1) or (TORCH_MAJOR == 1 and TORCH_MINOR > 0):\n            version_ge_1_1 = ['-DVERSION_GE_1_1']\n        version_ge_1_3 = []\n        if (TORCH_MAJOR > 1) or (TORCH_MAJOR == 1 and TORCH_MINOR > 2):\n            version_ge_1_3 = ['-DVERSION_GE_1_3']\n        version_ge_1_5 = []\n        if (TORCH_MAJOR > 1) or (TORCH_MAJOR == 1 and TORCH_MINOR > 4):\n            version_ge_1_5 = ['-DVERSION_GE_1_5']\n        return version_ge_1_1 + version_ge_1_3 + version_ge_1_5\n\n    def cxx_args(self):\n        cxx_flags = [\n            '-fsycl', '-fsycl-targets=spir64_gen', '-g', '-gdwarf-4', '-O3', '-std=c++17', '-fPIC', '-DMKL_ILP64',\n            '-fno-strict-aliasing'\n        ]\n        if os.environ.get('USE_MKL_GEMM'):\n            cxx_flags.append('-DUSE_MKL_GEMM')\n        return cxx_flags\n\n    def extra_ldflags(self):\n        return [\n            '-fPIC', '-fsycl', '-fsycl-targets=spir64_gen', '-fsycl-max-parallel-link-jobs=8',\n            '-Xs \"-options -cl-poison-unsupported-fp64-kernels,cl-intel-enable-auto-large-GRF-mode\"',\n            '-Xs \"-device pvc\"', '-Wl,-export-dynamic'\n        ]\n\n    def fixed_aotflags(self):\n        return [\n            '-fsycl', '-fsycl-targets=spir64_gen', '-fsycl-max-parallel-link-jobs=8', '-Xs',\n            \"-options -cl-poison-unsupported-fp64-kernels,cl-intel-enable-auto-large-GRF-mode\", '-Xs', \"-device pvc\"\n        ]\n\n    def load(self, verbose=True):\n        from deepspeed.git_version_info import installed_ops, torch_info, accelerator_name  # noqa: F401\n        from deepspeed.accelerator import get_accelerator\n        if installed_ops.get(self.name, False) and accelerator_name == get_accelerator()._name:\n            return importlib.import_module(self.absolute_name())\n        else:\n            return self.jit_load(verbose)\n\n    def jit_load(self, verbose=True):\n        if not self.is_compatible(verbose):\n            raise RuntimeError(\n                f\"Unable to JIT load the {self.name} op due to it not being compatible due to hardware/software issue. {self.error_log}\"\n            )\n        try:\n            import ninja  # noqa: F401\n        except ImportError:\n            raise RuntimeError(f\"Unable to JIT load the {self.name} op due to ninja not being installed.\")\n\n        self.jit_mode = True\n        from intel_extension_for_pytorch.xpu.cpp_extension import load\n\n        start_build = time.time()\n        # Recognize relative paths as absolute paths for jit load\n\n        sources = [self.deepspeed_src_path(path) for path in self.sources()]\n        extra_include_paths = [self.deepspeed_src_path(path) for path in self.include_paths()]\n\n        # Torch will try and apply whatever CCs are in the arch list at compile time,\n        # we have already set the intended targets ourselves we know that will be\n        # needed at runtime. This prevents CC collisions such as multiple __half\n        # implementations. Stash arch list to reset after build.\n        '''\n        torch_arch_list = None\n        if \"TORCH_CUDA_ARCH_LIST\" in os.environ:\n            torch_arch_list = os.environ.get(\"TORCH_CUDA_ARCH_LIST\")\n            os.environ[\"TORCH_CUDA_ARCH_LIST\"] = \"\"\n        '''\n\n        op_module = load(\n            name=self.name,\n            sources=self.strip_empty_entries(sources),\n            extra_include_paths=self.strip_empty_entries(extra_include_paths),\n            extra_cflags=self.strip_empty_entries(self.cxx_args()),\n            # extra_cuda_cflags=self.strip_empty_entries(self.nvcc_args()),\n            extra_ldflags=self.strip_empty_entries(self.extra_ldflags()),\n            verbose=verbose)\n\n        build_duration = time.time() - start_build\n        if verbose:\n            print(f\"Time to load {self.name} op: {build_duration} seconds\")\n        '''\n        # Reset arch list so we are not silently removing it for other possible use cases\n        if torch_arch_list:\n            os.environ[\"TORCH_CUDA_ARCH_LIST\"] = torch_arch_list\n        '''\n        return op_module\n", "op_builder/xpu/inference.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\nfrom .builder import SYCLOpBuilder\n\n\nclass InferenceBuilder(SYCLOpBuilder):\n    BUILD_VAR = \"DS_BUILD_TRANSFORMER_INFERENCE\"\n    NAME = \"transformer_inference\"\n\n    def __init__(self, name=None):\n        name = self.NAME if name is None else name\n        super().__init__(name=name)\n\n    def absolute_name(self):\n        return f'deepspeed.ops.transformer.inference.{self.NAME}_op'\n\n    def sources(self):\n        return\n\n    def include_paths(self):\n        return []\n\n    def extra_ldflags(self):\n        return []\n\n    def cxx_args(self):\n        return []\n\n    def load(self):\n        try:\n            import intel_extension_for_pytorch.deepspeed\n            return intel_extension_for_pytorch.deepspeed.transformer_inference.transformer_inference\n        except ImportError:\n            raise ImportError(\"Please install intel-extension-for-pytorch >= 2.1.30 to include DeepSpeed kernels.\")\n", "op_builder/xpu/cpu_adagrad.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .builder import SYCLOpBuilder\n\n\nclass CPUAdagradBuilder(SYCLOpBuilder):\n    BUILD_VAR = \"DS_BUILD_CPU_ADAGRAD\"\n    NAME = \"cpu_adagrad\"\n\n    def __init__(self):\n        super().__init__(name=self.NAME)\n\n    def absolute_name(self):\n        return f'deepspeed.ops.adagrad.{self.NAME}_op'\n\n    def sources(self):\n        return ['csrc/xpu/adagrad/cpu_adagrad.cpp', 'csrc/xpu/common/custom_cuda_kernel.dp.cpp']\n\n    def include_paths(self):\n        return ['csrc/xpu/includes']\n", "op_builder/cpu/no_impl.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .builder import CPUOpBuilder\n\n\nclass NotImplementedBuilder(CPUOpBuilder):\n    BUILD_VAR = \"DS_BUILD_NOT_IMPLEMENTED\"\n    NAME = \"deepspeed_not_implemented\"\n\n    def __init__(self, name=None):\n        name = self.NAME if name is None else name\n        super().__init__(name=name)\n\n    def absolute_name(self):\n        return f'deepspeed.ops.comm.{self.NAME}_op'\n\n    def load(self, verbose=True):\n        raise ValueError(\"This op had not been implemented on CPU backend.\")\n\n    def sources(self):\n        return []\n", "op_builder/cpu/fused_adam.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .builder import CPUOpBuilder\n\n\nclass FusedAdamBuilder(CPUOpBuilder):\n    BUILD_VAR = \"DS_BUILD_FUSED_ADAM\"\n    NAME = \"fused_adam\"\n\n    def __init__(self):\n        super().__init__(name=self.NAME)\n\n    def absolute_name(self):\n        return f'deepspeed.ops.adam.{self.NAME}_op'\n\n    def sources(self):\n        return ['csrc/cpu/adam/fused_adam.cpp', 'csrc/adam/cpu_adam_impl.cpp']\n\n    def include_paths(self):\n        return ['csrc/includes']\n", "op_builder/cpu/comm.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport os\nfrom .builder import CPUOpBuilder\n\n\nclass CCLCommBuilder(CPUOpBuilder):\n    BUILD_VAR = \"DS_BUILD_CCL_COMM\"\n    NAME = \"deepspeed_ccl_comm\"\n\n    def __init__(self, name=None):\n        name = self.NAME if name is None else name\n        super().__init__(name=name)\n\n    def absolute_name(self):\n        return f'deepspeed.ops.comm.{self.NAME}_op'\n\n    def sources(self):\n        return ['csrc/cpu/comm/ccl.cpp', 'csrc/cpu/comm/shm.cpp']\n\n    def include_paths(self):\n        includes = ['csrc/cpu/includes']\n        return includes\n\n    def cxx_args(self):\n        return ['-O2', '-fopenmp']\n\n    def is_compatible(self, verbose=True):\n        # TODO: add soft compatibility check for private binary release.\n        #  a soft check, as in we know it can be trivially changed.\n        return super().is_compatible(verbose)\n\n    def extra_ldflags(self):\n        ccl_root_path = os.environ.get(\"CCL_ROOT\")\n        if ccl_root_path is None:\n            raise ValueError(\n                \"Didn't find CCL_ROOT, install oneCCL from https://github.com/oneapi-src/oneCCL and source its environment variable\"\n            )\n            return []\n        else:\n            return ['-lccl', f'-L{ccl_root_path}/lib']\n\n\nclass ShareMemCommBuilder(CPUOpBuilder):\n    BUILD_VAR = \"DS_BUILD_SHM_COMM\"\n    NAME = \"deepspeed_shm_comm\"\n\n    def __init__(self, name=None):\n        name = self.NAME if name is None else name\n        super().__init__(name=name)\n\n    def absolute_name(self):\n        return f'deepspeed.ops.comm.{self.NAME}_op'\n\n    def sources(self):\n        return ['csrc/cpu/comm/shm_interface.cpp', 'csrc/cpu/comm/shm.cpp']\n\n    def include_paths(self):\n        includes = ['csrc/cpu/includes']\n        return includes\n\n    def cxx_args(self):\n        return ['-O2', '-fopenmp']\n\n    def is_compatible(self, verbose=True):\n        # TODO: add soft compatibility check for private binary release.\n        #  a soft check, as in we know it can be trivially changed.\n        return super().is_compatible(verbose)\n", "op_builder/cpu/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n'''Copyright The Microsoft DeepSpeed Team'''\n\nfrom .comm import CCLCommBuilder, ShareMemCommBuilder\nfrom .fused_adam import FusedAdamBuilder\nfrom .cpu_adam import CPUAdamBuilder\nfrom .no_impl import NotImplementedBuilder\n", "op_builder/cpu/cpu_adam.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .builder import CPUOpBuilder\n\n\nclass CPUAdamBuilder(CPUOpBuilder):\n    BUILD_VAR = \"DS_BUILD_CPU_ADAM\"\n    NAME = \"cpu_adam\"\n\n    def __init__(self):\n        super().__init__(name=self.NAME)\n\n    def absolute_name(self):\n        return f'deepspeed.ops.adam.{self.NAME}_op'\n\n    def sources(self):\n        return ['csrc/adam/cpu_adam.cpp', 'csrc/adam/cpu_adam_impl.cpp']\n\n    def libraries_args(self):\n        args = super().libraries_args()\n        return args\n\n    def include_paths(self):\n        return ['csrc/includes']\n", "op_builder/cpu/builder.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport os\n\ntry:\n    # is op_builder from deepspeed or a 3p version? this should only succeed if it's deepspeed\n    # if successful this also means we're doing a local install and not JIT compile path\n    from op_builder import __deepspeed__  # noqa: F401 # type: ignore\n    from op_builder.builder import OpBuilder\nexcept ImportError:\n    from deepspeed.ops.op_builder.builder import OpBuilder\n\n\nclass CPUOpBuilder(OpBuilder):\n\n    def builder(self):\n        from torch.utils.cpp_extension import CppExtension as ExtensionBuilder\n        include_dirs = [os.path.abspath(x) for x in self.strip_empty_entries(self.include_paths())]\n        compile_args = {'cxx': self.strip_empty_entries(self.cxx_args())}\n\n        cpp_ext = ExtensionBuilder(name=self.absolute_name(),\n                                   sources=self.strip_empty_entries(self.sources()),\n                                   include_dirs=include_dirs,\n                                   libraries=self.strip_empty_entries(self.libraries_args()),\n                                   extra_compile_args=compile_args)\n\n        return cpp_ext\n\n    def cxx_args(self):\n        args = ['-O3', '-g', '-Wno-reorder']\n        CPU_ARCH = self.cpu_arch()\n        SIMD_WIDTH = self.simd_width()\n        args += [CPU_ARCH, '-fopenmp', SIMD_WIDTH]\n        return args\n\n    def libraries_args(self):\n        return []\n", "op_builder/npu/no_impl.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .builder import NPUOpBuilder\n\n\nclass NotImplementedBuilder(NPUOpBuilder):\n    BUILD_VAR = \"DS_BUILD_NOT_IMPLEMENTED\"\n    NAME = \"deepspeed_not_implemented\"\n\n    def __init__(self, name=None):\n        name = self.NAME if name is None else name\n        super().__init__(name=name)\n\n    def absolute_name(self):\n        return f'deepspeed.ops.comm.{self.NAME}_op'\n\n    def load(self, verbose=True):\n        raise ValueError(\"This op had not been implemented on NPU backend.\")\n\n    def sources(self):\n        return []\n\n    def cxx_args(self):\n        return []\n\n    def extra_ldflags(self):\n        return []\n\n    def include_paths(self):\n        return []\n", "op_builder/npu/cpu_lion.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .builder import NPUOpBuilder\n\n\nclass CPULionBuilder(NPUOpBuilder):\n    BUILD_VAR = \"DS_BUILD_CPU_LION\"\n    NAME = \"cpu_lion\"\n\n    def __init__(self):\n        super().__init__(name=self.NAME)\n\n    def absolute_name(self):\n        return f'deepspeed.ops.lion.{self.NAME}_op'\n\n    def sources(self):\n        return ['csrc/lion/cpu_lion.cpp', 'csrc/lion/cpu_lion_impl.cpp']\n\n    def include_paths(self):\n        args = super().include_paths()\n        args += ['csrc/includes']\n        return args\n", "op_builder/npu/fused_adam.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .builder import NPUOpBuilder\n\ntry:\n    import torch_npu\nexcept ImportError as e:\n    pass\n\n\nclass NPUFusedAdam:\n\n    @staticmethod\n    def multi_tensor_adam(chunk_size, noop_flag_buffer, tensor_lists, lr, beta1, beta2, epsilon, step, adam_w_mode,\n                          bias_correction, weight_decay, *args):\n        bias_correction1 = beta1**step\n        bias_correction2 = beta2**step\n\n        # iteration group['params']\n        for i in range(len(tensor_lists[0])):\n            grad_flat = tensor_lists[0][i]\n            param_flat = tensor_lists[1][i]\n            m_flat = tensor_lists[2][i]\n            v_flat = tensor_lists[3][i]\n\n            if adam_w_mode:\n                param_flat.data, m_flat, v_flat = torch_npu.npu_apply_adam_w(\n                    bias_correction1,\n                    bias_correction2,\n                    lr,\n                    weight_decay,\n                    beta1,\n                    beta2,\n                    epsilon,\n                    grad_flat,\n                    None,  # max_grad_norm\n                    False,  # amsgrad\n                    False,  # maximize\n                    out=(param_flat.data, m_flat, v_flat))\n            else:\n                param_flat.data, m_flat, v_flat = torch_npu.npu_apply_adam(\n                    bias_correction1,\n                    bias_correction2,\n                    lr,\n                    beta1,\n                    beta2,\n                    epsilon,\n                    grad_flat,\n                    False,  # use_locking\n                    False,  # use_nesterov\n                    out=(param_flat.data, m_flat, v_flat))\n\n\nclass FusedAdamBuilder(NPUOpBuilder):\n    BUILD_VAR = \"DS_BUILD_FUSED_ADAM\"\n    NAME = \"fused_adam\"\n\n    def __init__(self):\n        super().__init__(name=self.NAME)\n\n    def absolute_name(self):\n        return f'deepspeed.ops.adam.{self.NAME}_op'\n\n    def sources(self):\n        return []\n\n    def include_paths(self):\n        return []\n\n    def load(self, verbose=True):\n        return NPUFusedAdam\n", "op_builder/npu/async_io.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport distutils.spawn\nimport subprocess\n\nfrom .builder import NPUOpBuilder\n\n\nclass AsyncIOBuilder(NPUOpBuilder):\n    BUILD_VAR = \"DS_BUILD_AIO\"\n    NAME = \"async_io\"\n\n    def __init__(self):\n        super().__init__(name=self.NAME)\n\n    def absolute_name(self):\n        return f'deepspeed.ops.aio.{self.NAME}_op'\n\n    def sources(self):\n        return [\n            'csrc/aio/py_lib/deepspeed_py_copy.cpp', 'csrc/aio/py_lib/py_ds_aio.cpp',\n            'csrc/aio/py_lib/deepspeed_py_aio.cpp', 'csrc/aio/py_lib/deepspeed_py_aio_handle.cpp',\n            'csrc/aio/py_lib/deepspeed_aio_thread.cpp', 'csrc/aio/common/deepspeed_aio_utils.cpp',\n            'csrc/aio/common/deepspeed_aio_common.cpp', 'csrc/aio/common/deepspeed_aio_types.cpp',\n            'csrc/aio/py_lib/deepspeed_pin_tensor.cpp'\n        ]\n\n    def include_paths(self):\n        args = super().include_paths()\n        args += ['csrc/aio/py_lib', 'csrc/aio/common']\n        return args\n\n    def cxx_args(self):\n        args = super().cxx_args()\n        # -O0 for improved debugging, since performance is bound by I/O\n        CPU_ARCH = self.cpu_arch()\n        SIMD_WIDTH = self.simd_width()\n        import torch  # Keep this import here to avoid errors when building DeepSpeed wheel without torch installed\n        TORCH_MAJOR, TORCH_MINOR = map(int, torch.__version__.split('.')[0:2])\n        if TORCH_MAJOR >= 2 and TORCH_MINOR >= 1:\n            CPP_STD = '-std=c++17'\n        else:\n            CPP_STD = '-std=c++14'\n        return args + [\n            '-g',\n            '-Wall',\n            '-O0',\n            CPP_STD,\n            '-shared',\n            '-fPIC',\n            '-Wno-reorder',\n            CPU_ARCH,\n            '-fopenmp',\n            SIMD_WIDTH,\n            '-laio',\n        ]\n\n    def extra_ldflags(self):\n        args = super().extra_ldflags()\n        return args + ['-laio']\n\n    def check_for_libaio_pkg(self):\n        libs = dict(\n            dpkg=[\"-l\", \"libaio-dev\", \"apt\"],\n            pacman=[\"-Q\", \"libaio\", \"pacman\"],\n            rpm=[\"-q\", \"libaio-devel\", \"yum\"],\n        )\n\n        found = False\n        for pkgmgr, data in libs.items():\n            flag, lib, tool = data\n            path = distutils.spawn.find_executable(pkgmgr)\n            if path is not None:\n                cmd = f\"{pkgmgr} {flag} {lib}\"\n                result = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n                if result.wait() == 0:\n                    found = True\n                else:\n                    self.warning(f\"{self.NAME}: please install the {lib} package with {tool}\")\n                break\n        return found\n\n    def is_compatible(self, verbose=True):\n        # Check for the existence of libaio by using distutils\n        # to compile and link a test program that calls io_submit,\n        # which is a function provided by libaio that is used in the async_io op.\n        # If needed, one can define -I and -L entries in CFLAGS and LDFLAGS\n        # respectively to specify the directories for libaio.h and libaio.so.\n        aio_compatible = self.has_function('io_pgetevents', ('aio', ))\n        if verbose and not aio_compatible:\n            self.warning(f\"{self.NAME} requires the dev libaio .so object and headers but these were not found.\")\n\n            # Check for the libaio package via known package managers\n            # to print suggestions on which package to install.\n            self.check_for_libaio_pkg()\n\n            self.warning(\n                \"If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\"\n            )\n        return super().is_compatible(verbose) and aio_compatible\n", "op_builder/npu/__init__.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n'''Copyright The Microsoft DeepSpeed Team'''\n\nfrom .fused_adam import FusedAdamBuilder\nfrom .async_io import AsyncIOBuilder\nfrom .no_impl import NotImplementedBuilder\nfrom .cpu_adam import CPUAdamBuilder\nfrom .cpu_adagrad import CPUAdagradBuilder\nfrom .cpu_lion import CPULionBuilder\nfrom .inference import InferenceBuilder\n", "op_builder/npu/cpu_adam.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .builder import NPUOpBuilder\n\n\nclass CPUAdamBuilder(NPUOpBuilder):\n    BUILD_VAR = \"DS_BUILD_CPU_ADAM\"\n    NAME = \"cpu_adam\"\n\n    def __init__(self):\n        super().__init__(name=self.NAME)\n\n    def absolute_name(self):\n        return f'deepspeed.ops.adam.{self.NAME}_op'\n\n    def sources(self):\n        return ['csrc/adam/cpu_adam.cpp', 'csrc/adam/cpu_adam_impl.cpp']\n\n    def include_paths(self):\n        args = super().include_paths()\n        args += ['csrc/includes']\n        return args\n", "op_builder/npu/builder.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport re\nimport os\ntry:\n    import torch_npu\nexcept ImportError as e:\n    pass\n\ntry:\n    # is op_builder from deepspeed or a 3p version? this should only succeed if it's deepspeed\n    # if successful this also means we're doing a local install and not JIT compile path\n    from op_builder import __deepspeed__  # noqa: F401 # type: ignore\n    from op_builder.builder import OpBuilder\nexcept ImportError:\n    from deepspeed.ops.op_builder.builder import OpBuilder\n\n\nclass NPUOpBuilder(OpBuilder):\n    _ascend_path = None\n    _torch_npu_path = None\n    _cann_version = None\n\n    def __init__(self, name):\n        super().__init__(name)\n        self._ascend_path = self.installed_cann_path()\n        self._torch_npu_path = os.path.join(os.path.dirname(os.path.abspath(torch_npu.__file__)))\n        try:\n            self._cann_version = self.installed_cann_version(self.name)\n        except BaseException:\n            print(f\"{self.name} ascend_cann is missing, npu ops cannot be compiled!\")\n\n    def cann_defs(self):\n        if self._cann_version:\n            return '-D__ENABLE_CANN__'\n        return '-D__DISABLE_CANN__'\n\n    def installed_cann_path(self):\n        if \"ASCEND_HOME_PATH\" in os.environ or os.path.exists(os.environ[\"ASCEND_HOME_PATH\"]):\n            return os.environ[\"ASCEND_HOME_PATH\"]\n        return None\n\n    def installed_cann_version(self, name=\"\"):\n        ascend_path = self.installed_cann_path()\n        assert ascend_path is not None, \"CANN_HOME does not exist, unable to compile NPU op(s)\"\n        cann_version = \"\"\n        for dirpath, _, filenames in os.walk(os.path.realpath(ascend_path)):\n            if cann_version:\n                break\n            install_files = [file for file in filenames if re.match(r\"ascend_.*_install\\.info\", file)]\n            if install_files:\n                filepath = os.path.join(dirpath, install_files[0])\n                with open(filepath, \"r\") as f:\n                    for line in f:\n                        if line.find(\"version\") != -1:\n                            cann_version = line.strip().split(\"=\")[-1]\n                            break\n        return cann_version\n\n    def include_paths(self):\n        paths = super().include_paths()\n        paths += [os.path.join(self._ascend_path, 'include'), os.path.join(self._torch_npu_path, 'include')]\n        return paths\n\n    def cxx_args(self):\n        args = super().cxx_args()\n        args += ['-O3', '-std=c++17', '-g', '-Wno-reorder', '-fopenmp']\n        args += ['-fstack-protector-all', '-Wl,-z,relro,-z,now,-z,noexecstack', '-Wl,--disable-new-dtags,--rpath']\n        args += [\n            self.cann_defs(),\n            self.cpu_arch(),\n            self.simd_width(), '-L' + os.path.join(self._ascend_path, 'lib64'),\n            '-L' + os.path.join(self._torch_npu_path, 'lib')\n        ]\n        return args\n\n    def extra_ldflags(self):\n        flags = super().extra_ldflags()\n        flags += [\n            '-L' + os.path.join(self._ascend_path, 'lib64'), '-lascendcl',\n            '-L' + os.path.join(self._torch_npu_path, 'lib'), '-ltorch_npu'\n        ]\n        return flags\n", "op_builder/npu/inference.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom enum import IntEnum\nfrom .builder import NPUOpBuilder\n\ntry:\n    import torch\n    import torch_npu\nexcept ImportError as e:\n    pass\n\n\nclass ActivationFuncType(IntEnum):\n    UNKNOWN = 0\n    GELU = 1\n    ReLU = 2\n    GATED_GELU = 3\n    GATED_SILU = 4\n\n\nclass InferenceContext:\n    _workspace = None\n\n    _seed = 42\n    _curr_offset = 0\n    _stream = 0\n    _free_memory_size = 0\n    _num_tokens = 1\n    _attention_unfused_workspace_offset = 0\n    _workSpaceSize = 0\n\n    workSpaceSize = 0\n    kv_caches = None\n\n    @staticmethod\n    def reset_tokens(initial_tokens=1):\n        InferenceContext._num_tokens = initial_tokens\n\n    @staticmethod\n    def current_tokens():\n        return InferenceContext._num_tokens\n\n    @staticmethod\n    def GetWorkSpace():\n        return InferenceContext._workspace\n\n\nclass NPUInference:\n\n    @staticmethod\n    def layer_norm(inputs, gamma, beta, epsilon):\n        return torch.nn.functional.layer_norm(inputs, [inputs.shape[-1]], gamma, beta, eps=epsilon)\n\n    @staticmethod\n    def _qkv_gemm(inputs, weight, q_scale, bias, gamma, beta, eps, add_bias, q_int8, transpose):\n        inp_norm = torch.nn.functional.layer_norm(inputs, (inputs.shape[2], ), gamma, beta, eps)\n        weight = weight.t() if transpose else weight\n        tmp = torch.matmul(inp_norm, weight)\n        if add_bias:\n            tmp += bias\n        output = [tmp, inp_norm]\n        return output\n\n    @staticmethod\n    def qkv_gemm_fp16(inputs, weight, q_scale, bias, gamma, beta, eps, add_bias, q_int8, transpose):\n        return NPUInference._qkv_gemm(inputs, weight, q_scale, bias, gamma, beta, eps, add_bias, q_int8, transpose)\n\n    @staticmethod\n    def qkv_gemm_bf16(inputs, weight, q_scale, bias, gamma, beta, eps, add_bias, q_int8, transpose):\n        return NPUInference._qkv_gemm(inputs, weight, q_scale, bias, gamma, beta, eps, add_bias, q_int8, transpose)\n\n    @staticmethod\n    def qkv_gemm_fp32(inputs, weight, q_scale, bias, gamma, beta, eps, add_bias, q_int8, transpose):\n        return NPUInference._qkv_gemm(inputs, weight, q_scale, bias, gamma, beta, eps, add_bias, q_int8, transpose)\n\n    @staticmethod\n    def _bias_add_transform_0213(vals, bias, hidden_dim, seq_length, seq_offset, heads, num_kv, rotary_dim,\n                                 rotate_half, rotate_every_two, rope_theta):\n        bsz, _, _ = vals.shape\n        q = vals[..., :hidden_dim].reshape(bsz, seq_length, heads, -1)\n        k = vals[..., hidden_dim:hidden_dim + num_kv * (hidden_dim // heads)].reshape(bsz, seq_length, num_kv, -1)\n        v = vals[..., hidden_dim + num_kv * (hidden_dim // heads):]\n\n        if rotary_dim > 0 and rotate_every_two:\n            # sin, cos may use cache\n            seq_id = torch.arange(0, seq_length).to(\"npu\")\n            inv_freq = torch.arange(0, rotary_dim, 2) / rotary_dim\n            inv_freq = inv_freq.to(\"npu\")\n            inv_freq = 1.0 / torch.pow(rope_theta, inv_freq)\n            inv_freq = torch.outer(seq_id, inv_freq)\n            sin = inv_freq.sin()\n            cos = inv_freq.cos()\n            # shape: [bsz=1, seq_len, heads=1, rotary_dim]\n            sin = sin.view(-1, seq_length, 1, rotary_dim // 2).repeat_interleave(2, dim=-1)\n            cos = cos.view(-1, seq_length, 1, rotary_dim // 2).repeat_interleave(2, dim=-1)\n\n            q_pos, q_pass = q[..., :rotary_dim], q[..., rotary_dim:]\n            k_pos, k_pass = k[..., :rotary_dim], k[..., rotary_dim:]\n\n            q_pos = torch_npu.npu_rotary_mul(q_pos, cos, sin)\n            q = torch.cat([q_pos, q_pass], dim=-1)\n            k_pos = torch_npu.npu_rotary_mul(k_pos, cos, sin)\n            k = torch.cat([k_pos, k_pass], dim=-1)\n\n        output = q.reshape(bsz, seq_length, -1).contiguous()  # [b, s, H]\n        k_cache = k.reshape(bsz, seq_length, heads, -1).transpose(1, 2).contiguous()  # [b, n, s, d]\n        v_cache = v.reshape(bsz, seq_length, heads, -1).transpose(1, 2).contiguous()  # [b, n, s, d]\n        return output, k_cache, v_cache\n\n    @staticmethod\n    def _softmax_context(query_key_value, attn_mask, rotary_dim, rotate_half, rotate_every_two, heads, num_kv,\n                         norm_factor, triangular_masking, local_attention, window_size, no_masking, layer_id,\n                         num_layers, alibi, rope_theta):\n        bsz, seq_len, k = query_key_value.size()\n        k = k // (heads + 2 * (num_kv if num_kv > 0 else heads))\n        hidden_dim = heads * k\n\n        is_promt = seq_len > 1\n        if not InferenceContext.kv_caches:\n            InferenceContext.kv_caches = [[None, None] for _ in range(num_layers)]\n        if is_promt:\n            InferenceContext.reset_tokens(seq_len)\n            InferenceContext.kv_caches[layer_id] = [None, None]\n\n        soft_len = InferenceContext.current_tokens()\n        workspace = InferenceContext.GetWorkSpace()\n        seq_offset = 0 if is_promt else soft_len - 1\n\n        q, k, v = NPUInference._bias_add_transform_0213(vals=query_key_value,\n                                                        bias=None,\n                                                        hidden_dim=hidden_dim,\n                                                        seq_length=seq_len,\n                                                        seq_offset=seq_offset,\n                                                        heads=heads,\n                                                        num_kv=num_kv if num_kv > 0 else heads,\n                                                        rotary_dim=rotary_dim,\n                                                        rotate_half=rotate_half,\n                                                        rotate_every_two=rotate_every_two,\n                                                        rope_theta=rope_theta)\n\n        if not is_promt:\n            k_cache, v_cache = InferenceContext.kv_caches[layer_id]\n            if k_cache is not None:\n                k = torch.cat([k_cache, k], dim=2)\n                v = torch.cat([v_cache, v], dim=2)\n        InferenceContext.kv_caches[layer_id] = [k, v]\n        seq_len = k.shape[2]\n\n        layer_scale = max(1, layer_id) if len(alibi.size()) > 1 else 1.0\n        alpha = norm_factor * norm_factor / layer_scale\n\n        output = torch_npu.npu_fusion_attention(q,\n                                                k.transpose(1, 2).reshape(bsz, seq_len, -1).contiguous(),\n                                                v.transpose(1, 2).reshape(bsz, seq_len, -1).contiguous(),\n                                                heads,\n                                                \"BSH\",\n                                                pse=None,\n                                                padding_mask=None,\n                                                atten_mask=attn_mask.bool(),\n                                                scale=alpha,\n                                                pre_tockens=65536,\n                                                next_tockens=65536,\n                                                keep_prob=1,\n                                                inner_precise=0)[0]\n\n        return output, k, v\n\n    @staticmethod\n    def softmax_context_fp16(query_key_value, attn_mask, rotary_dim, rotate_half, rotate_every_two, heads, num_kv,\n                             norm_factor, triangular_masking, local_attention, window_size, no_masking, layer_id,\n                             num_layers, alibi, rope_theta):\n        return NPUInference._softmax_context(query_key_value, attn_mask, rotary_dim, rotate_half, rotate_every_two,\n                                             heads, num_kv, norm_factor, triangular_masking, local_attention,\n                                             window_size, no_masking, layer_id, num_layers, alibi, rope_theta)\n\n    @staticmethod\n    def softmax_context_bf16(query_key_value, attn_mask, rotary_dim, rotate_half, rotate_every_two, heads, num_kv,\n                             norm_factor, triangular_masking, local_attention, window_size, no_masking, layer_id,\n                             num_layers, alibi, rope_theta):\n        return NPUInference._softmax_context(query_key_value, attn_mask, rotary_dim, rotate_half, rotate_every_two,\n                                             heads, num_kv, norm_factor, triangular_masking, local_attention,\n                                             window_size, no_masking, layer_id, num_layers, alibi, rope_theta)\n\n    @staticmethod\n    def softmax_context_fp32(query_key_value, attn_mask, rotary_dim, rotate_half, rotate_every_two, heads, num_kv,\n                             norm_factor, triangular_masking, local_attention, window_size, no_masking, layer_id,\n                             num_layers, alibi, rope_theta):\n        return NPUInference._softmax_context(query_key_value, attn_mask, rotary_dim, rotate_half, rotate_every_two,\n                                             heads, num_kv, norm_factor, triangular_masking, local_attention,\n                                             window_size, no_masking, layer_id, num_layers, alibi, rope_theta)\n\n    @staticmethod\n    def _vector_matmul(input, weight, async_op, q_scale, q_int8, transposed_mode):\n        if transposed_mode:\n            return torch.matmul(input, weight.t())\n        return torch.matmul(input, weight)\n\n    @staticmethod\n    def vector_matmul_fp16(input, weight, async_op, q_scale, q_int8, transposed_mode):\n        return NPUInference._vector_matmul(input, weight, async_op, q_scale, q_int8, transposed_mode)\n\n    @staticmethod\n    def vector_matmul_bf16(input, weight, async_op, q_scale, q_int8, transposed_mode):\n        return NPUInference._vector_matmul(input, weight, async_op, q_scale, q_int8, transposed_mode)\n\n    @staticmethod\n    def vector_matmul_fp32(input, weight, async_op, q_scale, q_int8, transposed_mode):\n        return NPUInference._vector_matmul(input, weight, async_op, q_scale, q_int8, transposed_mode)\n\n    @staticmethod\n    def _mlp_gemm(input, residual, input_bias, weight_interm, weight_out, bias, gamma, beta, eps, pre_layer_norm,\n                  mlp_after_attn, interm_scale, out_scale, dtype, mlp_act_func_type, transpose):\n        if mlp_after_attn:\n            residual_add = torch.nn.functional.layer_norm(input + residual + input_bias, (input.shape[-1], ), gamma,\n                                                          beta, eps)\n        else:\n            residual_add = torch.nn.functional.layer_norm(input, (input.shape[-1], ), gamma, beta, eps)\n\n        weight_interm = weight_interm.t() if transpose else weight_interm\n        tmp = torch.matmul(residual_add, weight_interm)\n        if mlp_act_func_type == ActivationFuncType.GELU:\n            tmp = torch.nn.functional.gelu(tmp + bias)\n        elif mlp_act_func_type == ActivationFuncType.ReLU:\n            tmp = torch.nn.functional.relu(tmp + bias)\n        else:\n            raise Exception('Unsupported ActivationFuncType {}'.format(mlp_act_func_type))\n        output = torch.matmul(tmp, weight_out.t())\n        return output, residual_add\n\n    @staticmethod\n    def mlp_gemm_fp16(input, residual, input_bias, weight_interm, weight_out, bias, gamma, beta, eps, pre_layer_norm,\n                      mlp_after_attn, interm_scale, out_scale, dtype, mlp_act_func_type, transpose):\n        return NPUInference._mlp_gemm(input, residual, input_bias, weight_interm, weight_out, bias, gamma, beta, eps,\n                                      pre_layer_norm, mlp_after_attn, interm_scale, out_scale, dtype,\n                                      mlp_act_func_type, transpose)\n\n    @staticmethod\n    def mlp_gemm_bf16(input, residual, input_bias, weight_interm, weight_out, bias, gamma, beta, eps, pre_layer_norm,\n                      mlp_after_attn, interm_scale, out_scale, dtype, mlp_act_func_type, transpose):\n        return NPUInference._mlp_gemm(input, residual, input_bias, weight_interm, weight_out, bias, gamma, beta, eps,\n                                      pre_layer_norm, mlp_after_attn, interm_scale, out_scale, dtype,\n                                      mlp_act_func_type, transpose)\n\n    @staticmethod\n    def mlp_gemm_fp32(input, residual, input_bias, weight_interm, weight_out, bias, gamma, beta, eps, pre_layer_norm,\n                      mlp_after_attn, interm_scale, out_scale, dtype, mlp_act_func_type, transpose):\n        return NPUInference._mlp_gemm(input, residual, input_bias, weight_interm, weight_out, bias, gamma, beta, eps,\n                                      pre_layer_norm, mlp_after_attn, interm_scale, out_scale, dtype,\n                                      mlp_act_func_type, transpose)\n\n    @staticmethod\n    def _residual_add_bias(hidden_state, residual, attention_output, attention_bias, final_bias, mp_size,\n                           mlp_after_attn, add_bias, pre_layer_norm):\n        if mlp_after_attn:\n            if pre_layer_norm:\n                tmp = (residual.float() + attention_output.float() + attention_bias.float() +\n                       final_bias.float()) / mp_size + hidden_state.float()\n            else:\n                tmp = residual.float() + hidden_state.float() + final_bias.float()\n        else:\n            if add_bias:\n                residual += attention_bias.float()\n            tmp = hidden_state.float() + attention_output.float() + (residual.float() + final_bias.float()) / mp_size\n\n        input_dtype = hidden_state.dtype\n        residual.set_(tmp.to(input_dtype))\n\n    @staticmethod\n    def residual_add_bias_fp16(hidden_state, residual, attention_output, attention_bias, final_bias, mp_size,\n                               mlp_after_attn, add_bias, pre_layer_norm):\n        return NPUInference._residual_add_bias(hidden_state, residual, attention_output, attention_bias, final_bias,\n                                               mp_size, mlp_after_attn, add_bias, pre_layer_norm)\n\n    @staticmethod\n    def residual_add_bias_bf16(hidden_state, residual, attention_output, attention_bias, final_bias, mp_size,\n                               mlp_after_attn, add_bias, pre_layer_norm):\n        return NPUInference._residual_add_bias(hidden_state, residual, attention_output, attention_bias, final_bias,\n                                               mp_size, mlp_after_attn, add_bias, pre_layer_norm)\n\n    @staticmethod\n    def residual_add_bias_fp32(hidden_state, residual, attention_output, attention_bias, final_bias, mp_size,\n                               mlp_after_attn, add_bias, pre_layer_norm):\n        return NPUInference._residual_add_bias(hidden_state, residual, attention_output, attention_bias, final_bias,\n                                               mp_size, mlp_after_attn, add_bias, pre_layer_norm)\n\n\nclass InferenceBuilder(NPUOpBuilder):\n    BUILD_VAR = \"DS_BUILD_TRANSFORMER_INFERENCE\"\n    NAME = \"transformer_inference\"\n\n    def __init__(self):\n        super().__init__(name=self.NAME)\n\n    def absolute_name(self):\n        return f'deepspeed.ops.transformer.inference.{self.NAME}_op'\n\n    def sources(self):\n        return []\n\n    def include_paths(self):\n        return []\n\n    def load(self, verbose=True):\n        return NPUInference\n", "op_builder/npu/cpu_adagrad.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .builder import NPUOpBuilder\n\n\nclass CPUAdagradBuilder(NPUOpBuilder):\n    BUILD_VAR = \"DS_BUILD_CPU_ADAGRAD\"\n    NAME = \"cpu_adagrad\"\n\n    def __init__(self):\n        super().__init__(name=self.NAME)\n\n    def absolute_name(self):\n        return f'deepspeed.ops.adagrad.{self.NAME}_op'\n\n    def sources(self):\n        return ['csrc/adagrad/cpu_adagrad.cpp']\n\n    def include_paths(self):\n        args = super().include_paths()\n        args += ['csrc/includes']\n        return args\n", "op_builder/hpu/no_impl.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .builder import CPUOpBuilder\n\n\nclass NotImplementedBuilder(CPUOpBuilder):\n    BUILD_VAR = \"DS_BUILD_NOT_IMPLEMENTED\"\n    NAME = \"deepspeed_not_implemented\"\n\n    def __init__(self, name=None):\n        name = self.NAME if name is None else name\n        super().__init__(name=name)\n\n    def absolute_name(self):\n        return f'deepspeed.ops.comm.{self.NAME}_op'\n\n    def load(self, verbose=True):\n        raise ValueError(\"This op had not been implemented on HPU backend.\")\n\n    def sources(self):\n        return []\n", "op_builder/hpu/fused_adam.py": "# Copyright (c) 2023 Habana Labs, Ltd. an Intel Company\n# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\ntry:\n    # is op_builder from deepspeed or a 3p version? this should only succeed if it's deepspeed\n    # if successful this also means we're doing a local install and not JIT compile path\n    from op_builder import __deepspeed__  # noqa: F401 # type: ignore\n    from op_builder.builder import OpBuilder\nexcept ImportError:\n    from deepspeed.ops.op_builder.builder import OpBuilder\n\ntry:\n    import torch\n    import math\nexcept ImportError as e:\n    pass\n\n\nclass HPUFusedAdam:\n    htcore = None\n    is_lazy_mode = None\n\n    @staticmethod\n    def multi_tensor_adam(chunk_size, noop_flag_buffer, tensor_lists, lr, beta1, beta2, epsilon, step, adam_w_mode,\n                          bias_correction, weight_decay, *args):\n\n        if HPUFusedAdam.htcore is None:\n            from habana_frameworks.torch import core as htcore\n            from habana_frameworks.torch.utils.internal import is_lazy\n            HPUFusedAdam.htcore = htcore\n            HPUFusedAdam.is_lazy_mode = is_lazy()\n\n        htcore = HPUFusedAdam.htcore\n\n        htcore.step_closure._mark_step_if_lazy()\n        step_size = lr\n        if bias_correction:\n            bias_correction1 = 1.0 - pow(beta1, step)\n            bias_correction2 = 1.0 - pow(beta2, step)\n            step_size = step_size * math.sqrt(bias_correction2) / bias_correction1\n\n        neg_step = -step_size\n        neg_step_t = (torch.tensor([neg_step], dtype=torch.float,\n                                   requires_grad=False).to(tensor_lists[1][0].dtype).to(tensor_lists[1][0].device,\n                                                                                        non_blocking=True))\n\n        weight_decay = weight_decay if adam_w_mode else 0\n\n        # since lr is fed into the kernel as tensor, perform the scalar multiplication of wd here\n        # NOTE: TODO if lr is updated every step, then we need to convert it as tensor and\n        # perform weight decay unconditonally.\n        modified_wd = 1.0 - weight_decay * lr\n\n        if HPUFusedAdam.is_lazy_mode:\n            torch.ops.hpu.optimizer_adamw(\n                tensor_lists[0],\n                tensor_lists[1],\n                tensor_lists[2],\n                tensor_lists[3],\n                neg_step_t,\n                beta1,\n                beta2,\n                epsilon,\n                modified_wd,\n            )\n        else:\n            modified_wd_t = (torch.tensor([modified_wd], dtype=torch.float, requires_grad=False).to(\n                tensor_lists[1][0].dtype).to(tensor_lists[1][0].device, non_blocking=True))\n            torch.ops.hpu.optimizer_adamw(\n                tensor_lists[0],\n                tensor_lists[1],\n                tensor_lists[2],\n                tensor_lists[3],\n                neg_step_t,\n                beta1,\n                beta2,\n                epsilon,\n                modified_wd_t,\n                modified_wd != 1.0,\n            )\n\n        htcore.step_closure._mark_step_if_lazy()\n\n\nclass FusedAdamBuilder(OpBuilder):\n    BUILD_VAR = \"DS_BUILD_FUSED_ADAM\"\n    NAME = \"fused_adam\"\n\n    def __init__(self):\n        super().__init__(name=self.NAME)\n\n    def absolute_name(self):\n        return f'deepspeed.ops.adam.{self.NAME}_op'\n\n    def sources(self):\n        return []\n\n    def include_paths(self):\n        return []\n\n    def load(self, verbose=True):\n        return HPUFusedAdam\n", "op_builder/hpu/__init__.py": "# Copyright (c) 2023 Habana Labs, Ltd. an Intel Company\n# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n'''Copyright The Microsoft DeepSpeed Team'''\n\nfrom .cpu_adam import CPUAdamBuilder\nfrom .fused_adam import FusedAdamBuilder\nfrom .no_impl import NotImplementedBuilder\n", "op_builder/hpu/cpu_adam.py": "# Copyright (c) 2023 Habana Labs, Ltd. an Intel Company\n# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom .builder import CPUOpBuilder\n\n\nclass CPUAdamBuilder(CPUOpBuilder):\n    BUILD_VAR = \"DS_BUILD_CPU_ADAM\"\n    NAME = \"cpu_adam\"\n\n    def __init__(self):\n        super().__init__(name=self.NAME)\n\n    def absolute_name(self):\n        return f'deepspeed.ops.adam.{self.NAME}_op'\n\n    def sources(self):\n        return ['csrc/adam/cpu_adam.cpp', 'csrc/adam/cpu_adam_impl.cpp']\n\n    def libraries_args(self):\n        args = super().libraries_args()\n        return args\n\n    def include_paths(self):\n        return ['csrc/includes']\n", "op_builder/hpu/builder.py": "# Copyright (c) 2023 Habana Labs, Ltd. an Intel Company\n# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport os\n\ntry:\n    # is op_builder from deepspeed or a 3p version? this should only succeed if it's deepspeed\n    # if successful this also means we're doing a local install and not JIT compile path\n    from op_builder import __deepspeed__  # noqa: F401 # type: ignore\n    from op_builder.builder import OpBuilder\nexcept ImportError:\n    from deepspeed.ops.op_builder.builder import OpBuilder\n\n\nclass CPUOpBuilder(OpBuilder):\n\n    def builder(self):\n        from torch.utils.cpp_extension import CppExtension as ExtensionBuilder\n        include_dirs = [os.path.abspath(x) for x in self.strip_empty_entries(self.include_paths())]\n        compile_args = {'cxx': self.strip_empty_entries(self.cxx_args())}\n\n        cpp_ext = ExtensionBuilder(name=self.absolute_name(),\n                                   sources=self.strip_empty_entries(self.sources()),\n                                   include_dirs=include_dirs,\n                                   libraries=self.strip_empty_entries(self.libraries_args()),\n                                   extra_compile_args=compile_args)\n\n        return cpp_ext\n\n    def cxx_args(self):\n        args = ['-O3', '-g', '-Wno-reorder']\n        CPU_ARCH = self.cpu_arch()\n        SIMD_WIDTH = self.simd_width()\n        args += [CPU_ARCH, '-fopenmp', SIMD_WIDTH]\n        return args\n\n    def libraries_args(self):\n        return []\n", "scripts/check-torchdist.py": "#!/usr/bin/env python3\n# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom __future__ import annotations\n'''Copyright The Microsoft DeepSpeed Team'''\n\"\"\"\nChecks each file in sys.argv for the string \"torch.distributed\".\nModified from https://github.com/jlebar/pre-commit-hooks/blob/master/check_do_not_submit.py\n\"\"\"\n\nimport subprocess\nimport sys\n\n\ndef err(s: str) -> None:\n    print(s, file=sys.stderr)\n\n\n# There are many ways we could search for the string \"torch.distributed\", but `git\n# grep --no-index` is nice because\n#  - it's very fast (as compared to iterating over the file in Python)\n#  - we can reasonably assume it's available on all machines\n#  - unlike plain grep, which is slower and has different flags on MacOS versus\n#    Linux, git grep is always the same.\nres = subprocess.run(\n    [\"git\", \"grep\", \"-Hn\", \"--no-index\", r\"torch\\.distributed\", *sys.argv[1:]],\n    capture_output=True,\n)\nif res.returncode == 0:\n    err('Error: The string \"torch.distributed\" was found. Please replace all calls to torch.distributed with \"deepspeed.comm\"'\n        )\n    err(res.stdout.decode(\"utf-8\"))\n    sys.exit(1)\nelif res.returncode == 2:\n    err(f\"Error invoking grep on {', '.join(sys.argv[1:])}:\")\n    err(res.stderr.decode(\"utf-8\"))\n    sys.exit(2)\n", "scripts/replace_copyright.py": "#!/usr/bin/env python3\n# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\"\"\"\nUSAGE:\n$ python3 script/replace_copyright.py --repo_dir ./\n\"\"\"\n\nimport os\nimport argparse\n\nNEW_COPYRIGHT = (\"Copyright (c) Microsoft Corporation.\", \"SPDX-License-Identifier: Apache-2.0\", \"\", \"DeepSpeed Team\")\n\nPY_SL_COMMENT = \"#\"\nPY_ML_SINGLE = \"'''\"\nPY_ML_DOUBLE = '\"\"\"'\nPY_COMMENTS = (PY_SL_COMMENT, PY_ML_SINGLE, PY_ML_DOUBLE)\n\nC_SL_COMMENT = \"//\"\nC_ML_OPEN = \"/*\"\nC_ML_CLOSE = \"*/\"\nC_COMMENTS = (C_SL_COMMENT, C_ML_OPEN, C_ML_CLOSE)\n\nBASH_SL_COMMENT = \"#\"\nBASH_COMMENTS = (BASH_SL_COMMENT, )\n\nDELIM = \"|/-\\|/-\\|BARRIER|/-\\|/-\\|\"  # noqa: W605\n\n\ndef parser_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--repo_dir\", type=str, help=\"Repository directory\")\n    parser.add_argument(\"--python_style_ext\",\n                        type=str,\n                        nargs=\"+\",\n                        default=[\".py\"],\n                        help=\"File types to process with python-style comments\")\n    parser.add_argument(\"--bash_style_ext\",\n                        type=str,\n                        nargs=\"+\",\n                        default=[\".sh\"],\n                        help=\"File types to process with bash-style comments\")\n    parser.add_argument(\"--c_style_ext\",\n                        type=str,\n                        nargs=\"+\",\n                        default=[\n                            \".c\",\n                            \".cpp\",\n                            \".cu\",\n                            \".h\",\n                            \".hpp\",\n                            \".cuh\",\n                            \".cc\",\n                            \".hip\",\n                            \".tr\",\n                        ],\n                        help=\"File types to process with C-style comments\")\n    args = parser.parse_args()\n    return args\n\n\n# These get_header_* functions are ugly, but they work :)\ndef get_header_py(fp):\n    with open(fp, \"r\") as f:\n        lines = iter(l for l in f.readlines())\n\n    header = []\n    rest = []\n    in_multiline = False\n    multiline_type = None\n\n    while (l := next(lines, None)) is not None:\n        l = l.strip()\n        if l.startswith(PY_ML_SINGLE) or l.startswith(PY_ML_DOUBLE):\n            # Detected multiline comment\n            if in_multiline and multiline_type == l[:3]:\n                # Ended a multiline comment\n                in_multiline = False\n            else:\n                # Started a multiline comment\n                in_multiline = True\n                multiline_type = l[:3]\n            if l.endswith(multiline_type) and len(l) >= 6:\n                # Opened and closed multiline comment on single line\n                in_multiline = False\n        elif in_multiline and l.endswith(multiline_type):\n            # Ended a multiline comment\n            in_multiline = False\n        elif not (in_multiline or l.startswith(PY_SL_COMMENT) or l == \"\"):\n            # Not in a comment\n            rest += [l + \"\\n\"]\n            break\n        header.append(l)\n\n    rest += list(lines)\n\n    return header, rest\n\n\ndef get_header_c(fp):\n    with open(fp, \"r\") as f:\n        lines = iter(l for l in f.readlines())\n\n    header = []\n    rest = []\n    in_multiline = False\n\n    while (l := next(lines, None)) is not None:\n        l = l.strip()\n        if l.startswith(C_ML_OPEN):\n            # Detected multiline comment\n            if not l.endswith(C_ML_CLOSE):\n                # multiline comment not closed on same line\n                in_multiline = True\n        elif l.endswith(C_ML_CLOSE):\n            # Ended a multiline comment\n            in_multiline = False\n        elif not in_multiline or l.startswith(C_SL_COMMENT) or l.isspace():\n            # Not in a comment\n            rest += [l + \"\\n\"]\n            break\n        header.append(l)\n\n    rest += list(lines)\n\n    return header, rest\n\n\ndef get_header_bash(fp):\n    with open(fp, \"r\") as f:\n        lines = iter(l for l in f.readlines())\n\n    header = []\n    rest = []\n\n    while (l := next(lines, None)) is not None:\n        l = l.strip()\n        if not l.startswith(BASH_SL_COMMENT) or l.isspace():\n            # Not in a comment\n            rest += [l + \"\\n\"]\n            break\n        header.append(l)\n\n    rest += list(lines)\n\n    return header, rest\n\n\ndef remove_comments(line, comment_strs):\n    for cstr in comment_strs:\n        line = line.replace(cstr, \"\")\n    return line\n\n\ndef format_multiline_comment(text, comment_type):\n    if comment_type == PY_COMMENTS:\n        text = f\"\\n{comment_type[2]}\\n\" + \"\\n\".join(text) + f\"{comment_type[2]}\"\n    if comment_type == C_COMMENTS:\n        text = f\"\\n{comment_type[1]}\\n\" + \"\\n\".join(text) + f\"{comment_type[2]}\"\n    if comment_type == BASH_COMMENTS:\n        text = \"\\n\".join([f\"{comment_type[0]}{l}\" for l in text])\n    return text\n\n\ndef modify_file_header(fp, file_header, rest_of_file, preserve_text_store, comment_type):\n    header_text = \"\\n\".join(file_header)\n    if not (header_text.strip() == \"\" or header_text in preserve_text_store):\n        # Unique header, need to get user input\n        print(\"\\n\", DELIM, \"\\n\")\n        for idx, line in enumerate(file_header):\n            print(f\"{idx}: {line}\")\n        print(\"\\n\", DELIM, \"\\n\")\n        print(\"\\nIndicate the FIRST line of the Header to KEEP\")\n        print(\"(shebang #! lines will be automatically processed and should not be included).\")\n        keep_idx = input(\"Enter number (or leave blank if no lines should be preserved): \")\n        preserve_text_store[header_text] = file_header[int(keep_idx):] if keep_idx != \"\" else \"\"\n\n    # Identify any shebang lines in the file\n    shebang = \"\\n\".join([l for l in file_header if l.startswith(\"#!\")])\n    if shebang != \"\":\n        shebang += \"\\n\"\n\n    # Get the text we should preserve in this file and process to remove comment characters\n    text_to_preserve = preserve_text_store.get(header_text, [\"\"])\n    text_to_preserve = [remove_comments(l, comment_type) for l in text_to_preserve]\n\n    # Format the text we want to keep into a new multiline comment\n    if \"\".join(text_to_preserve) == \"\":\n        text_to_preserve = \"\"\n    else:\n        text_to_preserve = format_multiline_comment(text_to_preserve, comment_type)\n\n    # Generate the copyright text we will be adding\n    copyright_text = \"\\n\".join([f\"{comment_type[0]} {l}\" if l != \"\" else l for l in NEW_COPYRIGHT])\n\n    # Assemble the new header\n    new_header = shebang + copyright_text + text_to_preserve\n\n    # Write out the new file\n    new_file_contents = new_header + \"\\n\" + \"\".join(rest_of_file)\n    with open(fp, \"w\") as f:\n        f.write(new_file_contents)\n\n    return preserve_text_store  # Return so we can reuse for future files\n\n\ndef main(args):\n    preserve_text_store = {}  # Used to track header comments we should preserve\n    for root, dirs, fnames in os.walk(args.repo_dir):\n        # Walk across directory looking for all files with extensions we want to modify\n        for ext in args.python_style_ext:\n            fpaths = [os.path.join(root, fn) for fn in fnames if fn.endswith(ext)]\n            for fp in fpaths:\n                file_header, rest_of_file = get_header_py(fp)\n                preserve_text_store = modify_file_header(fp, file_header, rest_of_file, preserve_text_store,\n                                                         PY_COMMENTS)\n        for ext in args.c_style_ext:\n            fpaths = [os.path.join(root, fn) for fn in fnames if fn.endswith(ext)]\n            for fp in fpaths:\n                file_header, rest_of_file = get_header_c(fp)\n                preserve_text_store = modify_file_header(fp, file_header, rest_of_file, preserve_text_store,\n                                                         C_COMMENTS)\n        for ext in args.bash_style_ext:\n            fpaths = [os.path.join(root, fn) for fn in fnames if fn.endswith(ext)]\n            for fp in fpaths:\n                file_header, rest_of_file = get_header_bash(fp)\n                preserve_text_store = modify_file_header(fp, file_header, rest_of_file, preserve_text_store,\n                                                         BASH_COMMENTS)\n\n\nif __name__ == \"__main__\":\n    args = parser_args()\n    main(args)\n", "scripts/check-license.py": "#!/usr/bin/env python3\n# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom __future__ import annotations\n'''Copyright The Microsoft DeepSpeed Team'''\n\"\"\"\nModified from https://github.com/jlebar/pre-commit-hooks/blob/master/check_do_not_submit.py\n\"\"\"\n\nimport subprocess\nimport sys\n\n\ndef err(s: str) -> None:\n    print(s, file=sys.stderr)\n\n\nCOPYRIGHT = [\n    (r\"^# Copyright (c) Microsoft Corporation.$\", r\"^\\/\\/ Copyright (c) Microsoft Corporation.$\"),\n    (r\"^# SPDX-License-Identifier: Apache-2.0$\", r\"^\\/\\/ SPDX-License-Identifier: Apache-2.0$\"),\n    (r\"^# DeepSpeed Team$\", r\"^\\/\\/ DeepSpeed Team$\"),\n]\n\nsuccess = True\nfailures = []\nfor f in sys.argv[1:]:\n    for copyright_line in COPYRIGHT:\n        cmd = [\"git\", \"grep\", \"--quiet\"]\n        for line in copyright_line:\n            cmd.extend([\"-e\", line])\n        cmd.append(f)\n        res = subprocess.run(cmd, capture_output=True)\n        if res.returncode == 1:\n            success = False\n            failures.append(f)\n            break\n        elif res.returncode == 2:\n            err(f\"Error invoking grep on {', '.join(sys.argv[1:])}:\")\n            err(res.stderr.decode(\"utf-8\"))\n            sys.exit(2)\n\nif not success:\n    err(f'{failures}: Missing license at top of file')\n    err(res.stdout.decode(\"utf-8\"))\n    sys.exit(1)\n", "scripts/check-torchcuda.py": "#!/usr/bin/env python3\n# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom __future__ import annotations\n'''Copyright The Microsoft DeepSpeed Team'''\n\"\"\"\nChecks each file in sys.argv for the string \"torch.cuda\".\nModified from https://github.com/jlebar/pre-commit-hooks/blob/master/check_do_not_submit.py\n\"\"\"\n\nimport subprocess\nimport sys\n\n\ndef err(s: str) -> None:\n    print(s, file=sys.stderr)\n\n\nprint(*sys.argv[1:])\n\n# There are many ways we could search for the string \"torch.cuda\", but `git\n# grep --no-index` is nice because\n#  - it's very fast (as compared to iterating over the file in Python)\n#  - we can reasonably assume it's available on all machines\n#  - unlike plain grep, which is slower and has different flags on MacOS versus\n#    Linux, git grep is always the same.\nres = subprocess.run(\n    [\"git\", \"grep\", \"-Hn\", \"--no-index\", \"-e\", r\"torch\\.cuda\", \"--and\", \"--not\", \"-e\", \"#ignore-cuda\", *sys.argv[1:]],\n    capture_output=True,\n)\nif res.returncode == 0:\n    err('Error: The string \"torch.cuda\" was found.\\nPlease replace all calls to torch.cuda with \"get_accelerator()\" and add the following import line:\\n\\n    from deepspeed.accelerator import get_accelerator\\n\\nIf your code is mean to be cuda specific, please add the following comment in the line with torch.cuda:\\n\\n    #ignore-cuda\\n'\n        )\n    err(res.stdout.decode(\"utf-8\"))\n    sys.exit(1)\nelif res.returncode == 2:\n    err(f\"Error invoking grep on {', '.join(sys.argv[1:])}:\")\n    err(res.stderr.decode(\"utf-8\"))\n    sys.exit(2)\n\nres = subprocess.run(\n    [\"git\", \"grep\", \"-Hn\", \"--no-index\", r\"\\.cuda()\", *sys.argv[1:]],\n    capture_output=True,\n)\nif res.returncode == 0:\n    err('Error: The string \".cuda()\" was found. This implies convert a tensor to cuda tensor.  Please replace all calls to tensor.cuda() with \"tensor.to(get_accelerator().device_name())\" and add the following import line:\\nfrom deepspeed.accelerator import get_accelerator'\n        )\n    err(res.stdout.decode(\"utf-8\"))\n    sys.exit(1)\nelif res.returncode == 2:\n    err(f\"Error invoking grep on {', '.join(sys.argv[1:])}:\")\n    err(res.stderr.decode(\"utf-8\"))\n    sys.exit(2)\n\nfiles = []\nfor file in sys.argv[1:]:\n    if not file.endswith(\".cpp\"):\n        files.append(file)\n\nres = subprocess.run(\n    [\"git\", \"grep\", \"-Hn\", \"--no-index\", r\"\\.is_cuda\", *files],\n    capture_output=True,\n)\nif res.returncode == 0:\n    err('''\nError: The string \".is_cuda\" was found. This implies checking if a tensor is a cuda tensor.\n       Please replace all calls to \"tensor.is_cuda\" with \"get_accelerator().on_accelerator(tensor)\",\n       and add the following import line:\n       'from deepspeed.accelerator import get_accelerator'\n''')\n    err(res.stdout.decode(\"utf-8\"))\n    sys.exit(1)\nelif res.returncode == 2:\n    err(f\"Error invoking grep on {', '.join(files)}:\")\n    err(res.stderr.decode(\"utf-8\"))\n    sys.exit(2)\n", "scripts/check-extraindexurl.py": "#!/usr/bin/env python3\n# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nfrom __future__ import annotations\n'''Copyright The Microsoft DeepSpeed Team'''\n\"\"\"\nChecks each file in sys.argv for the string \"--extra-index-url\".\nModified from https://github.com/jlebar/pre-commit-hooks/blob/master/check_do_not_submit.py\n\"\"\"\n\nimport subprocess\nimport sys\n\n\ndef err(s: str) -> None:\n    print(s, file=sys.stderr)\n\n\nprint(*sys.argv[1:])\n\n# There are many ways we could search for the string \"--extra-index-url\", but `git\n# grep --no-index` is nice because\n#  - it's very fast (as compared to iterating over the file in Python)\n#  - we can reasonably assume it's available on all machines\n#  - unlike plain grep, which is slower and has different flags on MacOS versus\n#    Linux, git grep is always the same.\nres = subprocess.run(\n    [\"git\", \"grep\", \"-Hn\", \"--no-index\", \"-e\", r\"--extra-index-url\", *sys.argv[1:]],\n    capture_output=True,\n)\nif res.returncode == 0:\n    err('Error: The string \"--extra-index-url\" was found.\\nPlease replace all calls to --extra-index-url with \"--index-url\"'\n        )\n    err(res.stdout.decode(\"utf-8\"))\n    sys.exit(1)\nelif res.returncode == 2:\n    err(f\"Error invoking grep on {', '.join(sys.argv[1:])}:\")\n    err(res.stderr.decode(\"utf-8\"))\n    sys.exit(2)\n", "release/bump_patch_version.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport argparse\nfrom packaging import version as pkg_version\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\"--current_version\",\n                    type=str,\n                    help=\"The current version being published to help set the next version.\")\n\nargs = parser.parse_args()\n\ncurrent_version = pkg_version.parse(args.current_version)\n\nwith open('./version.txt', 'w') as fd:\n    fd.write(f'{current_version.major}.{current_version.minor}.{current_version.micro + 1}\\n')\n\nprint(f'{current_version} -> {current_version.major}.{current_version.minor}.{current_version.micro + 1}')\n", "release/check_release_version.py": "# Copyright (c) Microsoft Corporation.\n# SPDX-License-Identifier: Apache-2.0\n\n# DeepSpeed Team\n\nimport argparse\nfrom packaging import version as pkg_version\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\"--release_version\", type=str, help=\"The new version being published.\")\n\nargs = parser.parse_args()\n\nrelease_version = pkg_version.parse(args.release_version)\n\nwith open('./version.txt') as fd:\n    repo_version = pkg_version.parse(fd.read())\n\nassert repo_version == release_version, f\"{repo_version=} does not match {release_version=}, unable to proceed\"\n"}